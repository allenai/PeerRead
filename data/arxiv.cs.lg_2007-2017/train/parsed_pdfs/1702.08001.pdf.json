{
  "name" : "1702.08001.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Bayesian Nonparametric Feature and Policy Learning for Decision-Making",
    "authors" : [ "Jürgen Hahn", "Abdelhak M. Zoubir" ],
    "emails" : [ "jhahn@spg.tu-darmstadt.de", "zoubir@spg.tu-darmstadt.de" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Learning from demonstrations has gained increasing interest in the recent past, enabling an agent to learn how to make decisions by observing an experienced teacher. While many approaches have been proposed to solve this problem, there is only little work that focuses on reasoning about the observed behavior. We assume that, in many practical problems, an agent makes its decision based on latent features, indicating a certain action. Therefore, we propose a generative model for the states and actions. Inference reveals the number of features, the features, and the policies, allowing us to learn and to analyze the underlying structure of the observed behavior. Further, our approach enables prediction of actions for new states. Simulations are used to assess the performance of the algorithm based upon this model. Moreover, the problem of learning a driver’s behavior is investigated, demonstrating the performance of the proposed model in a real-world scenario.\nKey words: Bayesian nonparametrics, decision-making, learning from demonstrations, feature learning, imitation learning"
    }, {
      "heading" : "1 Introduction",
      "text" : "Decision-making plays a crucial role in many applications, such as robot learning, driver assistance systems, and recommender systems [40]. A fundamental question in decision-making is how an agent can learn to make optimal decisions. Learning from an experienced teacher provides a natural means to solve this problem, without the need of explicitly defining rules for the desired behavior. Further, observing a teacher may provide a deeper understanding of the decision-making process. Therefore, Learning From Demonstrations (LFD) [1] has gained a lot of interest in the recent past.\n∗jhahn@spg.tu-darmstadt.de †zoubir@spg.tu-darmstadt.de\nar X\niv :1\n70 2.\n08 00\n1v 1\n[ cs\n.L G\n] 2\n6 Fe\nAccording to [1], approaches for LFD can be grouped into (i) reward-based models and (ii) imitation learning. In reward-based models, it is assumed that the agent makes its decision based on a reward which is, in the context of LFD, learned from observations (as in Inverse Reinforcement Learning (IRL) [30]). In imitation learning, it is assumed that an experienced teacher can be observed. Thus, the policy, telling the agent how to act in a given situation, can be learned by understanding the direct relation between the teacher’s states and actions. Especially in reward-based approaches, problems defined on infinite spaces, where the state of an agent can take continuous values, are mostly intractable to solve and efficient approximations are needed. Only in the case of finite state and action spaces and known rewards, learning optimal policies has been solved [3]. A typical approach to facilitate this problem is the representation of the state space in a feature domain, c.f. [33, 5].\nHowever, decision-making has been viewed only from a limited feature-based perspective, where features are usually designed by experts and mainly serve the purpose of reducing the size of the state space. We argue that, in many practical systems, the agent makes its decision based on a compact representation of the observed data, which can be considered as a projection onto a feature space of the decision-making problem. As an example, consider a person driving a car. The driver’s observations consists of, i.a., the location, speed, and acceleration of his vehicle and the surrounding vehicles, the type of the road and the weather conditions. However, the driver makes his decision based on a subset of the available information, e.g., on the timeto-reach between his and the other road users’ cars. This idea has also been investigated from a psychological point of view by the concept of discovering latent causes in human behavior, which is related to learning state space representations [12].\nAssuming that a certain structure underlies the observations, we aim at inferring the latent features and build a feature-based representation of the states, yielding the following two advantages: first, the states can be represented compactly, rendering the decision-making problem much more efficient as compared to working in the original domain. Second, the features can be regarded as causes for the observed decisions, allowing for a deeper understanding of the observed behavior. In particular, we consider a LFD problem and propose a Bayesian nonparametric framework for feature learning in LFD. We assume that the policies depend on the features of the observed demonstrations. With this model, we are able to (i) significantly reduce the state space by (ii) learning the features as well as the number of features and (iii) provide a better understanding of what caused the teacher to take the observed actions.\nIn the next parts of this section, we formulate the problem and review the state-of-the-art for feature learning in LFD. In Section 2, we motivate and present the model for featurebased decision-making. The model is detailed in Section 3 and we explain how the number of latent features can be inferred from the data. Section 4 presents the inference scheme for the latent variables. The proposed algorithm is empirically evaluated with simulation experiments in Section 5, demonstrating its performance. Real data experiments are conducted in Section 6. We discuss our findings in Section 7 and end with a short conclusion in Section 8."
    }, {
      "heading" : "1.1 Problem formulation",
      "text" : "The goal of this work is to introduce a feature-based LFD framework. While this model can be used for teaching an agent given observations of the desired behavior, the focus of this work lies on the analysis of the observed behavior by means of the features. Since we consider a decisionmaking task, the investigated problem can be modeled by means of a Partially Observable Markov Decision Process (POMDP) [21, 42], which is defined by\n• a set of observations, Z,\n• a set of states, X ,\n• a finite set of Nu actions, U ,\n• a transition model, which describes the probability of entering a state after taking an action in the current state,\n• an observation model which explains how the observations are generated from the states,\n• a discount factor, which penalizes long-term rewards,\n• and a reward function, R.\nAs the main goal of this work is to provide a means to understand observed behavior, we consider an imitation learning approach and learn the relation between states and actions from the observations directly, where the reward and, hence, the discount factor, are not considered.\nWe assume that we (or an agent) have access to Nz noisy observations, zn ∈ Z, of the states, xn ∈ X , with n = 1, . . . , Nz. In particular, we consider Gaussian noise, i.e., zn, describes observations of the states, xn, with additive Gaussian noise. Further, we assume that the actions, un ∈ U , taken in the corresponding states, can be observed. The observations are assumed to be optimal in the sense that they represent the behavior of the agent seeking its goal, i.e., without any exploratory steps.\nA simple approach to the considered problem would be the use of a feature extraction technique such as Principal Component Analysis (PCA) [20] or Non-negative Matrix Factorization (NMF) [23] to learn features from the observed states. Following this solution, features cannot be jointly learned and shared between different actions. Moreover, learning discriminative features is not guaranteed. Therefore, we argue that the features and policies need to be learned jointly such that a trade-off between feature sharing, promoting a compact model, and discrimination capability is found based on the observations."
    }, {
      "heading" : "1.2 Relation to existing work",
      "text" : "In the following, we provide an overview about the current state of the art of feature learning for LFD. As IRL usually requires to solve an Markov Decision Process (MDP), which is usually done by means of a Reinforcement Learning (RL) algorithm, we start with an introduction in feature learning for RL."
    }, {
      "heading" : "1.2.1 Reinforcement Learning",
      "text" : "Large state and action spaces are especially problematic for value-based RL algorithms such as value-iteration [3] or Q-learning [48] since the value function, representing the expected accumulated reward, needs to be approximated. In early approaches, a set of basis functions, often referred to as features, is linearly weighted to represent the this function [5]. However, there exists only little work on learning these basis functions. Riedmiller [38] has proposed the Q-fitted value iteration where the value function is approximated by means of a neural network, where the features are learned in the layers of the network. In [29], this approach has been extended by replacing the neural network by a deep-layered counterpart. A different concept to employ features is proposed by Hutter with the Feature Reinforcement Learning framework [18]. In this framework, the goal is to learn a feature mapping from the agent’s history (comprised of actions, states, and rewards) to a MDP state, enabling decision learning for infinite state spaces [8]. An alternative framework for learning the latent structure of the state space is proposed in [9] which is based on Factored Markov Decision Processs (FMDPs) [4]. In the FMDP framework, it is assumed that the observed states can be represented compactly by exploiting the structure within the states, enabling efficient learning. This approach has been extended in [31] to an online approach, where the features are selected from a large set by means of Group LASSO. In these approaches, the inferred features mainly serve the purpose of dimensionality reduction. Therefore, the features do not necessarily possess a meaning that can be easily interpreted."
    }, {
      "heading" : "1.2.2 Inverse Reinforcement Learning",
      "text" : "IRL is concerned with the problem of learning the reward function from observed behavior [30]. As in RL, features are often used to linearly parameterize the reward function, e.g., in [30, 17]. Recent attempts have been made to consider a Deep Learning (DL) architecture [49] for IRL, providing means for nonlinear, hierarchical feature learning.\nA Bayesian nonparametric approach is proposed in [6], utilizing an Indian Buffet Process (IBP) to model feature activations. As the features of the reward function are assumed to be known, this approach can be understood rather as a feature selection than feature learning for IRL, where the number of features is inferred by the IBP. Different results on Bayesian nonparametrics for IRL, which is indirectly related to feature learning, are given in [27], where a partitioning of the state space is sought for, or [45], where complex behavior is decomposed into several, simpler behaviors that can be easily learned."
    }, {
      "heading" : "1.2.3 Imitation Learning",
      "text" : "Instead of estimating the reward as in IRL, imitation learning aims at inferring the underlying policy directly [2, 44]. Usually, handcrafted features are used, e.g., in [37, 39]. Attempts to introduce new features are made in [36] as an extension of the maximum margin planning algorithm which is proposed in [37]. As explained in [1], imitation learning can be considered\nas a supervised learning task. Thus, feature selection and learning techniques developed for classification and regression can also be used in imitation learning. An excellent overview is given in [16]. Although these models work well in practice, they might not be able to provide a deeper understanding of the observed behavior as they do not explicitly model the states and actions."
    }, {
      "heading" : "2 Choice of the Model",
      "text" : "In the first part of this section, we propose a feature model for LFD. In the second part, we explain the relevance of the transition model to the proposed framework. Since we assume a mixture of policies in this framework, we briefly discuss the intuition behind this assumption in the third part. Alternative models for feature learning for LFD are discussed in the forth part."
    }, {
      "heading" : "2.1 Feature model for learning from demonstrations",
      "text" : "We assume a linear latent feature model, similar to NMF [23] and PCA [20]. Thus, the noisy observations, zn ∈ R1×D, n = 1, . . . , Nz, are assumed to be composed of the latent features, F ∈ FK×D, and the feature coefficients, sn ∈ S1×K ,\nzn = snF + n, (1)\nwhere n represents Gaussian i.i.d. noise with variance σ 2 z and the states are given as xn = snF. The number of features is K and the dimension of the observations is D. Clearly, the feature space, F , depends on the application. In the following, we assume that the features are positivevalued, i.e., F = R+. Following [22], the feature matrix is composed of a binary activation matrix, A ∈ {0, 1}K×D, and a weighting matrix, W ∈ FK×D, where the relation is given by the Hadamard product,\nF = A W. (2)\nThe feature model in Eq. (2) can be easily extended to an infinite feature model by placing an IBP [13, 15] prior over A. The IBP assumes an infinite number of features, while the observed data can be explained by a finite number. This gives rise to a nonparametric model, where the number of features is implicitly modeled by means of the IBP. This is detailed in Section 3.\nA fundamental difference to most existing work on decision-making using feature representations is that we assume that the agent makes its decision based on features, where each feature attracts the agent to take a specific action. We consider a (latent) linear feature model where the feature coefficients depend on the observed state and determine the actions. For this reason, we refer to the feature coefficients also as substates."
    }, {
      "heading" : "2.2 Transition model",
      "text" : "In decision-making problems, the transition model explains which actions the agent can take in each state. Thus, the transition model acts as a constraint on the possible actions. As we focus on inferring the latent causes for the observed actions by means of features, the transition model is less relevant as it only provides additional information. Besides, the transition model is rarely known in practice. As we are given observations, we can, theoretically, infer the transition model. For this, we could either employ a parametric or a nonparametric model. Defining a parametric model for the transitions is not trivial, as the dynamics in the latent space may be highly nonlinear. Alternatively, a nonparametric model can be assumed. This, however, requires a large amount of observations for the estimation of the parameters, which is often not available. Assuming that the noise in the observations is low, we argue that we can reliably infer the substates from the corresponding observations, eliminating the need for a transition model."
    }, {
      "heading" : "2.3 Feature-based policy",
      "text" : "As each feature imposes its on own policy, the probability of the agent taking an action, u, in a substate, s, is a mixture of the feature policies, P (u |φk),\nP (u | s,Φ) ∝ K∑ k=1 skP (u |φk), (3)\nwhere φk, k = 1, . . . ,K, are the parameters of the feature policies and Φ = [φ1, . . . ,φK ]. The mixture of policies can be interpreted either as a stochastic or a deterministic policy. In the first case, the action to be taken should be sampled according to Eq. (3). In the second case, simply the most probable action is taken. Mixture polices have been investigated in multiobjective problems, where an agent aims at reaching several objectives, some of which can even be conflicting [32, 46, 41]. A stochastic policy is needed in this framework to ensure that all goals can be satisfied. Similar problems occur if the problem at hand is described by a POMDP, where the true state of the agent is unknown. The uncertainty about the state of the agent is expressed by beliefs over states. Acting according to a stochastic policy maximizes the expected return [21].\nIn case of single agents, it has been shown that deterministic policies are optimal solutions for MDPs [34]. As explained in Section 2.1, we argue that we can reliably infer the substates. Thus, we assume a deterministic policy in the following, where the probability in Eq. (3) expresses our confidence about the actions given the states.\nNote that we also assume that each feature imposes a deterministic policy such that we expect a strongly peaked distribution for the feature policies, expressing the confidence of the chosen action."
    }, {
      "heading" : "2.4 Alternative Feature-based Models",
      "text" : "Of course, there are other possibilities to incorporate feature learning in LFD. We briefly discuss two alternatives with their potential advantages and drawbacks. For completeness, as mentioned in the introduction, in the most trivial setup, one could simply cluster the states according to the observed actions and then learn the features. This, however, has the significant drawback that the features cannot be shared between the clusters."
    }, {
      "heading" : "2.4.1 Unique coefficient model",
      "text" : "Instead of assuming, as in our model, that the features determine the behavior of the agent, the feature coefficients can be clustered, where the clusters indicate the optimal actions given the coefficients. Thus, the clusters can be interpreted as latent substates. Supposing that the elements of the feature coefficients are binary-valued, we can convert each feature coefficient vector to a unique identifier, representing the cluster. Thus, instead of employing expensive clustering, a fast deterministic mapping from the binary coefficients to the cluster identifier can be used. However, as the identifiers must be unique for the cluster assignments, we can have at maximum 2K different clusters in this setup, i.e., the number of clusters (and, hence, possible actions) is strictly limited by the number of features. An advantage of this model is that the relation between substates and policies can be nonlinear. However, a significant drawback is that this model suffers severely from errors in the inference of the substates. Consider the case, where, for instance, due to noise, one element of the substate is incorrectly set. This substate is then assigned to a new cluster, for which the policy must be inferred from potentially little data, yielding highly varying policy estimates. As in our approach, an IBP prior can be placed over the feature activations to infer the number of latent features."
    }, {
      "heading" : "2.4.2 Clustering-based approach",
      "text" : "A clustering-based approach assumes that similar states can be grouped and result in the same behavior. This can also be understood as a single feature model, in which we assume that the observations can be described by a single feature, representing the clusters. Thus, the substates reduce to cluster indicators, i.e., they indicate which feature best represents the observed state. The number of latent states is then equal to the number of features. One possibility to infer this number is to utilize a Chinese Restaurant Process (CRP), giving rise to a Bayesian nonparametric model [27]. A similar model, where the state space is clustered according to the played actions, is proposed in [43]."
    }, {
      "heading" : "2.4.3 Relation to the proposed model",
      "text" : "Our model has the advantage, as opposed to the clustering-based approach and the unique coefficient model, that the features provide a means to understand the observed behavior. In contrast to the unique coefficient model, we neither require binary coefficients nor a clustering\nstep. Compared to the clustering-based approach, our model is able to significantly reduce the number of latent features and policies, as the features can be shared by different states. However, our model suffers from the assumption of a linear relation between features, substates, and policies. This problem is alleviated in the other approaches, as the features are decoupled from the policies. Note that our model becomes similar to the clustering-based model at the price of higher computational costs, if each substate is represented by only one feature."
    }, {
      "heading" : "3 Bayesian Nonparametric Model for Feature Learning",
      "text" : "In this section, we provide a general framework for Bayesian nonparametric feature learning for decision-making based on the model proposed in Section 2.1. In order to learn the structure, we assume that we are given a set of observations consisting of state-action pairs, D = {(z1, u1), . . . , (zNz , uNz)}."
    }, {
      "heading" : "3.1 Observation likelihood and noise variance",
      "text" : "The observations, zn, n = 1, . . . , Nz, are assumed to be conditionally independent. As we assume Gaussian noise in Eq. (1), the state likelihood can be expressed as\np(Z |W,A,S, σ2z ) = Nz∏ n=1 Nzn ( sn (A W) , σ2zI ) . (4)\nwith Z = [ z1 T . . . zNz T ]T and S = [ s1 T . . . sNz T ]T . The variance of the noise, σ2z , is assumed to be Inverse-Gamma distributed with hyperparameters ασ, βσ. Further, we place hyperpriors on ασ and βσ, following Gamma distributions with hyperparameters h (1) ασ , h (2) ασ , h (1) βσ , and h (2) βσ ."
    }, {
      "heading" : "3.2 Prior for the feature weights",
      "text" : "As explained above, the prior probability of the feature weights, W, depends on the problem at hand. We consider i.i.d. positive-valued feature weights, wk,d with k = 1, . . . ,K and d = 1, . . . , D, and assume an Exponential prior,\np(W | γw) = K∏ k=1 D∏ d=1 Expwk,d(γw) .\nThe scaling factor, γw, is assumed to be Inverse-Gamma distributed with hyperparameters αγ and βγ .\nIf the features are assumed to be real-valued, the prior can be modeled with a Gaussian\ndistribution with straightforward modifications."
    }, {
      "heading" : "3.3 Prior for the feature activations",
      "text" : "The feature activations are modeled by means of an IBP [13, 15], assuming an infinite number of features. In the following, we consider the two-parameter generalization [14] which allows to sample sparse as well as dense matrices.\nThe IBP is derived as follows. For a finite number of features, K?, the sums over the rows of the feature activation matrix, A? ∈ {0, 1}K?×D, are assumed to follow i.i.d. Binomial distributions. Placing a Beta prior with hyperparameters αaβaK? and βa over the parameter of the Binomial distribution and marginalizing over this parameter yields a Beta-Binomial distribution [13, 14]. Since we are interested in sampling from an infinite number of features, we consider the limit for K? →∞, resulting in the distribution of the activation matrix A [13, 14],\nP (A |αa, βa) := lim K?→∞ P (A? |αa, βa)\n= (αaβa) K∏ h∈{0,1}D\\0Kh! exp{−K̄} K∏ k=1 B(mk, D −mk + βa) , (5)\nwhere the number of occurrences of the binary vector h ∈ {0, 1}D is denoted by Kh and B is the Beta function. Note that A is a matrix with infinitely many rows. However, due to the sparsity assumption, only a finite number of the rows of the realizations contain active elements, denoted by K. Thus, we need to store only rows with active elements in memory, which can be understood as realizations of K features, where the average number of active rows is given by\nK̄ = αa ∑D d=1 βa βa+d−1 [13, 10]. The hyperparameters αa and βa reflect our prior belief about the number of features and the sparsity of the matrix [14]. The probability of activating an element increases with both hyperparameters. As the number of expected features grows linearly with αa, αa can be used to control the number of generated features. Considering the limits for βa → 0 and βa → ∞ shows that the expected number of features is limited to αaD. As the probability of an element of A being active increases, βa can be understood as a means to control the sparsity of the realizations. We infer αa and βa from the observations, assuming that they\nfollow Gamma distributions, i.e., αa ∼ Gaαa ( h (1) αA , h (2) αA ) and βa ∼ Gaβa ( h (1) βA , h (2) βA ) [22]."
    }, {
      "heading" : "3.4 Prior for the substates",
      "text" : "As formulated in Eq. (1), we assume that the observations are composed of a mixture of features weighted by the substates. Similar to a FMDP1, we restrict the domain of the substate elements, sn,k, with n = 1, . . . , Nz and k = 1, . . . ,K, to take values from a finite set, S = {s̆1, . . . s̆L}, where L denotes the number of elements, to simplify inference. For convenience, we assume equidistant elements in S and set s̆1 = 0 and s̆L = 1. Note that the limited range does not restrict the model, as the features can be scaled to fit the observations.\nFurther, we assume that the substates are sparse, meaning that each observation consists of\n1A fundamental difference between our model and a FMDP is that the substates in our model are, theoretically, not restricted to be finite as we do not need to enumerate over them.\nonly a few features such that only few polices determine the observed action. In particular, we consider a sparsity-promoting mixture prior on the substates, similar to a spike and slab model [28, 19]. The components are given by a Categorical distribution, where P (s = 0 | θs=0) is the sparsity component and P (s 6= 0 | θs6=0) is the weight component. Note that all categories, except s̆1 = 0, have equal probability. We place a Beta prior, p(θs | αs=02 , αs 6=0 2 ), with hyperparameters αs=0 and αs 6=0, over the mixture weights θs = {θs=0, θs6=0} with θs 6=0 = 1−θs=0. Marginalizing over θs=0 yields,\nP (S) = K∏ k=1 ∫ 1 0 Nz∏ n=1 ( P (sn,k = 0 | θs=0)δ(sn,k)\n+ P (sn,k 6= 0 | θs=0)(1− δ(sn,k) ) )\n× p(θ |αs=0, αs 6=0) dθs=0\n∝ K∏ k=1 BetaBinsk ( ms=0,k + αs=0,ms6=0,k + αs 6=0 ) ,\nwhere δ(s) returns one if s = 0 and zero otherwise, ms=0,k counts the zero elements in sk, and ms6=0,k = Nz −ms=0,k."
    }, {
      "heading" : "3.5 Mixture of policies and action likelihood",
      "text" : "Since we assume a finite set of actions, we consider a Categorical distribution for each mixture component,\nP (u |φk) = Catu(φk) ,\nsuch that the mixture model can be written as\nP (u |Φ, s) = 1 Zu K∑ k=1 skP (u |φk), (6)\nwith normalization constant Zu = ∑ u∈U ∑K k=1 skP (u |φk). The parameters of the policy of each mixture component follow Dirichlet distributions with identical hyperparameters, αφ,\np(Φ) = K∏ k=1 Dirφk(αφ, . . . , αφ) ,\nwhere we assume independent policies. We consider the hyperparameter, αφ, as a Gamma distributed variable with parameters h (1) φA and h (2) φA .\nAs explained, we require a data set containing observed actions, un, n = 1, . . . , Nz. Assuming\nthat the observed actions are independent, the likelihood is given as\nP (u |Φ,S) = Nz∏ n=1 1 Zun K∑ k=1 skP (un |φk)."
    }, {
      "heading" : "3.6 Joint posterior distribution",
      "text" : "The full joint posterior distribution can be factorized as\np(W,A,S,Φ, σ2z , γw, αa, βa, ασ, βσ, αφ |Z,u) ∝\np(Z |W,A,S, σ2z )P (u |S,Φ)p(σ2z |ασ, βσ)\n× p(S)p(W | γw)P (A |αa, βa)p(Φ |αφ) × p(γw|αγ , βγ)p(ασ)p(βσ)p(αa)p(βa)p(αφ).\n(7)\nThe conditional independences in this model are exploited in Section 4, where inference based on this model is explained. The structure of the posterior is illustrated as a graphical model in Fig. 1."
    }, {
      "heading" : "4 Inference",
      "text" : "We consider the problem of learning the latent variables and the prediction of optimal actions for new observations as a Bayesian inference problem. Thus, we are interested in the joint posterior distribution in Eq. (7). Since the joint posterior is not directly tractable, we represent it by samples generated by means of Gibbs sampling. Therefore, in the first part of this section, we\nderive the conditional distributions of the variables. After explaining the sampling scheme in the second part, in the third part, we detail how to predict actions for new observations using this model.\nFor convenience, we use the bar symbol (−) in what follows to denote the set of conditional variables, i.e., all variables except the one that shall be sampled. The set of all latent variables is denoted by\nΩ = {W,A,S,Φ, σ2z , γw, αa, βa, ασ, βσ, αφ} ∈ Ω,\nwhere Ω denotes the joint space of the latent variables."
    }, {
      "heading" : "4.1 Conditional distributions",
      "text" : ""
    }, {
      "heading" : "4.1.1 Sampling the noise variance",
      "text" : "The hyperpriors of ασ and βσ are conjugate to the prior of σ 2 z . Hence, the conditional p(σ 2 z | −) is also Inverse-Gamma distributed,\np(σ2z | −) ∝ IGaσ2z ( ασ + NzD\n2 +, βσ\n+ 1\n2 Nz∑ n=1 D∑ d=1\n( zn,d −\nK∑ k=1 sn,kak,dwk,d )2 . We use a Metropolis-Hastings algorithm with a Gamma proposal distribution to generate samples of the hyperparameters, ασ and βσ."
    }, {
      "heading" : "4.1.2 Sampling the substates",
      "text" : "Sampling the substates, S, is simple thanks to the assumption of a finite space of the elements. The conditional consists of the state and action likelihoods as wells as the (conditional) prior,\nP (sn,k | −) ∝ p(zn |W,A, sn, σ2z )P (un | sn,Φ)P (sn,k |S\\sn,k), (8)\nfor all sn,k ∈ S, where S\\sn,k denotes the elements of S without sn,k. As the prior for the substates follows a Beta-Binomial distribution, the conditional is simply given as\nP (sn,k |S\\sn,k) ∝ ms=0 + αs=0 for sn,k = 0ms6=0 + αs6=0 for sn,k 6= 0 , where ms=0 and ms 6=0 are defined in Section 3.4."
    }, {
      "heading" : "4.1.3 Sampling the feature weights",
      "text" : "Since the likelihood is Gaussian and the weights are i.i.d. following an Exponential distribution, the conditional of the kth row of the weight matrix, wk, is a truncated Gaussian distribution,\np(wk | −) ∝ p(Z |W,A,S, σ2z ) D∏ d=1 p(wk,d | γw)\n∝ T Nwk ( µwk ,Σwk ) ,\nwith T Nwk ( µwk ,Σwk ) denoting a truncated Gaussian, where the elements of wk are constraint to be positive-valued. The mean, µwk , and the covariance, Σwk , are given as\nΣ−1wk = 1\nσ2z Nz∑ n=1 s2n,kdiag(ak) (9)\nµwk = Σ −1 wk ( 1 σ2z Nz∑ n=1 sn,k ( zn − K∑ k′=1 k′ 6=k sn,k′ (ak′ wk′) ) ak − 1D 1 γw ) . (10)\nWe use the algorithm presented in [7] to sample from a truncated Gaussian distribution. Note that we neglect the dependency on the activations in Eq. (9) since the covariance would be infinite for ak,d = 0. This does not affect the sampling scheme, as the corresponding weight will be ignored due to ak,d = 0 anyway. Sampling the hyperparameter, γw, is fairly easy since the conditional of γw is an Inverse-Gamma distribution,\np(γw | −) ∝ IGaγw ( αγ + KD\n2 , βγ +\n1 2 K∑ k=1 D∑ d=1 wk,d\n) ."
    }, {
      "heading" : "4.1.4 Sampling the feature activations",
      "text" : "Sampling from the IBP consists of two steps: For each row, (i) the active columns are updated and then (ii) new features are proposed. In the first step, an element of the activation matrix, ak,d, is set active with probability\nP (ak,d | −) ∝ p(zd |Sfd, σ2z )P (ak,d |ak\\d), (11)\nwith ak\\d denoting the kth row of A without the dth element and fd the dth column of F. As A is assumed to follow an IBP, the conditional in Eq. (11) is [13, 14]\nP (ak,d = 1 |ak\\d) = mk\\d\nD + βa − 1 ,\nwhere mk\\d is the sum over ak\\d.\nIn the second step, K+ new features are proposed in a Metropolis step [10, 22]. The proposal\ndistribution, q(θ+ | θ), is independent of the previous sample, θ, as it consists of the priors for the substates, the feature weights, and the policies of the features,\nq(θ+ | θ) = q(θ+) = P (K+ | −)p(W | γw)P (S)p(Φ |αφ) (12)\nwith θ = {W,S,A,Φ} and θ+ = {W+,S+,A+,Φ+}, where W+,S+,A+, and Φ+ denote the proposed feature weights, activations, coefficients, and policies. The probability of adding K+ features is given as [13, 14]\nP (K+ | −) ∼ PoissonK+ (\nαaβa βa +D − 1\n) .\nAs the proposal distribution is independent of the previous sample, the acceptance ratio, r, is equal to the likelihood ratio between the new and existing features [26],\nr = P (Z | θ+) P (Z | θ) .\nSince the IBP tends to mix slowly, we augment this ratio with probability P+ of accepting a single new feature, resulting in a modified acceptance ratio which is derived in [22]. This increases the probability of proposing new features, leading to a faster convergence to the stationary distribution. The hyperparameters αa and βa are sampled as described in [22]."
    }, {
      "heading" : "4.1.5 Sampling the policies",
      "text" : "Sampling from the conditionals for the policies directly is difficult and would be computationally expensive due to the mixture model (Eq. (6)). An efficient approach is to introduce auxiliary variables, tn, n = 1, . . . , Nz, for each observation, indicating from which policy the observed action, un, has been generated [25]. Given the indicators, the mixture components φk, k = 1 . . . ,K, in Eq. (6) become conditionally independent of the mixture weights, sn, n = 1, . . . , Nz, which makes sampling the components straightforward. The sampling algorithm thus consists of two steps.\nFirst, the indicators are sampled according to\nP (tn = k |un,S,φ) ∝ skP (un |φk). (13)\nIn order to approximate the conditional for the policies, we draw Nt indicator samples from Eq. (13). Drawing the samples is easy, since the indicators follow Categorical distributions with tn ∈ {1, . . . ,K}. Second, given the indicators, the parameters of the kth feature policy, φk, is sampled from a Dirichlet-Multinomial distribution,\np(φk |u, t) = DirMultφk(mφk,1 + αφ, . . . ,mφk,Nu + αφ)\nwhere mφk,i, i = 1, . . . , Nu, counts the co-occurrences between the policy indicators, t, and the actions, un ∈ U . The hyperparameter, αφ, can be sampled in a Metropolis step, using a Gamma proposal distribution."
    }, {
      "heading" : "4.2 Sampling algorithm",
      "text" : "The Gibbs sampler is initialized with only one feature and the variables are sampled from their prior distributions. After several iterations of the Gibbs sampler, samples from the target distribution are generated.\nNote that there is the chance to generate new features in each iteration of the Gibbs sampler. Especially in scenarios with strong noise, different rows of the feature matrix may converge to similar realizations, increasing the number of features unnecessarily. We propose to merge features reducing the number of features, if they show a similarity larger than a prefixed threshold, Tcorr, where we keep the activations of both features and average the policies. The similarity is measured by means of the estimated correlation between the feature samples.\nA maximum-a-posteriori (MAP) estimator can be utilized, if we are interested in an estimate of the latent variables, ΩMAP, containing, i.a., estimates of the features, F̂MAP [35], and the policies, Φ̂MAP. The MAP estimator can be approximated by choosing the sample with the highest posterior probability. The posterior probability is calculated according to Eq. (7). For the prediction of actions for new observations, we can also use a Minimum Mean Squared Error (MMSE) estimator which provides better generalization capabilities. This is detailed in the next section."
    }, {
      "heading" : "4.3 Prediction of actions",
      "text" : "The proposed model can be used to learn the structure of the observed states as well as for the prediction of actions, given new observed states. For the prediction of an optimal action, u?, given a new observation, z?, we can evaluate the posterior predictive distribution, giving rise to a MMSE estimator,\nP (u? | z?,D) = ∫ S ∫ Ω P (u?, s? | z?,Ω)p(Ω | z?,D) dΩ ds?, (14)\nwhere we exploit that u? is independent of the data set containing the observations, D, given Ω. Eq. (14) shows that Ω depends on the new observations, z?. Thus, all variables would need to be inferred for each prediction, which would be computationally expensive. To remedy this issue, we assume that the observed data in D sufficiently represents the conditional distribution for Ω, such that we can ignore the dependency and simply infer Ω based on D, leaving only u? and s? to be inferred during prediction. The marginalization over Ω is approximated by Monte Carlo integration. For this, we need to draw samples of s? given the samples of Ω. The samples can be\ngenerated by drawing from the conditional in Eq. (8). Alternatively, we obtain a MAP estimate of u? by maximizing the posterior predictive distribution with respect to s? given ΩMAP.\nSince, as justified in Section 2.3, we assume a deterministic policy, the optimal action, u?opt, is the action that maximizes P (u? | z?,D). Thus, the posterior predictive distribution expresses our confidence about the actions and can be considered as the policy of the agent.\nNote that, especially when we are interested in the prediction of actions, a modification on the model can help to increase the prediction accuracy dramatically. Especially with highdimensional observation, the observation likelihood determines the posterior, where the effect of the action likelihood nearly vanishes, leading in the worst case to a neglect of the actions. A remedy consists in considering an action variable for each entry of the observation. However, we assume that these actions variables are identical. This increases the influence of the action log-likelihood by factor D, increasing the weight on the posterior significantly. The necessary modification in the inference algorithm are simple: First, for the conditional of Eq. (8), the action log-likelihood is multiplied by D. Second, the same modification is applied to the joint posterior distribution in Eq. (7). Sampling the policies is not affected as the actions for each substate are identical. We apply this modification on the model for the real data experiments in Section 6, as the observations in the experiments are high-dimensional."
    }, {
      "heading" : "5 Experimental Results",
      "text" : "In order to demonstrate the performance of the proposed method, we consider simulations as well as real data experiments. In this section, we evaluate the performance of the inference\nalgorithm by simulating observations with different SNRs and different latent numbers of features. For this, we simulate the ground truth values by drawing samples from distributions which are similar to the prior distributions of the variables. This is detailed in the following. The true hyperparameters of the features weights are set to ȟαγ = ȟβγ = 100. Thus, the true hyperparameter for the weights, γ̌w, as well as the true weights, W̌, are sampled from their prior distributions. An element of the true feature activation matrix, Ǎ, is activated with probability P (ǎk,d = 1) = 0.5. The true substates, šn, are simulated by drawing samples from a Dirichlet distribution with parameters 1K1K , resulting in a peaked distribution. The true noise variance, σ̌2z , is determined by the chosen SNR, where the signal power is estimated from X̌ = ŠF̌. The parameters of the ground truth policies, Φ̌, are chosen such that one action has high probability mass, reflecting deterministic policies, as justified in Section 2.3. In all simulations, we consider Nz = 100 observations of dimensionality D = 30 with Nu = 4 different actions. In order to evaluate the prediction performance of the model, we split the data set into a training and a test data set, leaving 80 observations for training and 20 for testing. We sweep the SNR from 10 dB to 30 dB with a step size of 5 dB and vary the number of features K within the set of {5, 7, 9, 12, 15, 18}. We set the hyperparameters of the algorithm according to Tab. 1. We organize the evaluation in three parts. First, we investigate the performance of the estimation of the features, the feature coefficients, and the reconstruction of the states. Second, we compare the estimated policies to the true policies and evaluate the prediction in terms of the accuracy, Au, which is the average rate of correctly predicted actions of the test data set. These estimates are obtained utilizing a MAP estimator. As for the prediction of actions we can also use the MMSE estimator, as detailed in Section 4.3, we also discuss the results obtained by this estimator. Third, we provide results for the inferred number of features and compare it with the true value."
    }, {
      "heading" : "5.1 Estimation of the features",
      "text" : "The quality of the MAP estimates is evaluated in terms of the Mean Squared Error (MSE) for the elements of the features, FMAP, the substates, SMAP, and the reconstructions, XMAP = SMAPFMAP. Further, we compute the RMSEs over 20 Monte Carlo runs, yielding RMSE measures for each estimated variable, F, S, and X. The results are shown in Fig. 2(a)–(c).\nWe obtain good results with low errors especially for a small latent number of features, almost independent of the SNR. With increasing K, we observe a strong growth of the error. The error becomes even more significant in case of strong noise. It is remarkable that the error of the estimated feature values as well as the feature coefficients behave similarly. Due to the linear relation between the features and the substates, the errors of the reconstructions also grows with the number of features."
    }, {
      "heading" : "5.2 Action prediction",
      "text" : "We evaluate the correctness of the estimated policies in terms of the RMSE of the Mean Absolute Deviation (MAD) over 20 Monte Carlo runs. As shown in Fig. 2(d), the error slightly grows with an increasing number of features. Again, the SNR has, compared to the number of features, only little effect on the accuracy of the policies.\nThe accuracy of the action prediction using the MAP and MMSE estimators are depicted in Fig. 2(e). In case of few features (K < 15) and low noise (SNR > 20 dB), we obtain highly accurate predictions with over 90 % accuracy. Only for strong noise and many latent features the accuracy drops slightly below 70 %. This observation can be explained by the relation between the action and substates, which are challenging to infer in case of many features.\nSince we assume in the simulation experiments that there is a fixed set of parameters that explains the observations, the MMSE estimator yields only minor improvements over the MAP estimator concerning the accuracy of predicted actions, as indicated by the dotted lines in Fig. 2(e)."
    }, {
      "heading" : "5.3 Estimation of the number of features",
      "text" : "Assuming that the observations have been generated by a fixed number of features, we evaluate how accurately the algorithm is able to infer this number. The results for the simulations are depicted in Fig. 2(f), showing the RMSE of the MAD over the 20 Monte Carlo runs. As can be observed, the MAP estimator is able to infer the correct number of features reliably, especially in case of few features. On the one hand, if the noise in the observations increases and the observations are based on many latent features, the error grows significantly. On the other hand, if the SNR is reasonably high, i.e., SNR > 15 dB, the results deviate on average by only four from the true number in case of 18 latent features. The error in estimating the number of features is probably due to the fact that some simulation examples can be explained by less features than used for their generation."
    }, {
      "heading" : "6 Real Data Experiments",
      "text" : "We consider the problem of analyzing a driver’s behavior, which is an important task for useradaptive driver assistance systems [24, 47], in order to demonstrate the performance of the proposed model in a real-world scenario. For this, we observe the surrounding of the vehicle and the actions taken by the driver, aiming at learning what caused the driver to make the observed decisions. Using the proposed model, we can also predict which action the driver is likely to take given a certain situation. Thus, we also investigate the predictive performance of our approach by randomly creating training and test data sets. For this, we consider real data provided by the KITTI Vision Benchmark Suite [11] containing several challenges in urban driving. We use the data for the tracking challenge as it contains time-sequential LIDAR measurements of different situations in public road traffic. A schematic plot of the setup is illustrated in Fig. 3.\nWe consider Scene 11 and 20 of the benchmark suite, which are detailed below in Section 6.1 and Section 6.2. Before running the proposed inference algorithm, we pre-process the data as follows. First, we apply a thresholding on the height values of the measurements such that we keep only samples above ground level, which is roughly 1.5 m below the LIDAR. Afterwards, we discretize the measurements to obtain positive-valued occupancy grids, where the values of the grid elements refer to the maximum measured height.\nAs observation in the nth time frame, we consider the joint occupancy grid of the current and the previous frame. This is required to implicitly include velocity information, which enables to decide, e.g., if the host vehicle is faster or slower than the vehicle in front. Thus, the minimum speed, vmin, between the host vehicle, H, and an obstacle, O, that can be resolved depends on the resolution, R, of the grid as follows,\nR ≤ | dn − dn−1 | = (tF,n − tF,n−1) | vH − vO | = 1\nfs vmin,\nwhere dn denotes the distance between the host vehicle and the obstacle in the nth time frame. The time stamp is denoted by tF,n and fs is the frequency at which the measurements are sampled (in the KITTI data set, fs = 10 Hz).\nWe consider the environment of the vehicle in the range from −3 m to 3 m in the lateral direction. This covers the lane of the vehicle plus parts of (if existent) neighboring lanes. In Scene 11, the longitudinal range is limited between −10 m and 30 m and for Scene 20 between −40 m and 40 m. The range in longitudinal direction gives a time window of more than 2 s to react to the observed situation when driving at 50 km/h, which is the speed limit in German cities.\nFor both scenes, we choose a grid size of 21 × 65 pixels, resulting in a spatial resolution in lateral direction of Rlat ≈ 0.3 m and in longitudinal direction of Rlong ≈ 0.6 m for Scene 11 (and Rlong ≈ 1.2 m for Scene 20). Thus, the minimum velocities that can be detected are\nvlat,min > 3 m/s in the lateral direction and vlong,min > 6 m/s in the longitudinal direction (and vlong,min > 12 m/s for Scene 20).\nWe labeled the scenes to obtain the observed actions by means of the measured acceleration of the host vehicle, obtained from the onboard inertial measurement unit. The ground truth for Scene 11 is depicted in Fig. 4(a) and for Scene 20 in Fig. 4(b). The set of actions, U , we consider in this experiment is comprised of acceleration (A), deceleration (D), lane change (right) (R), and moving at constant speed (C). However, due to the short duration of the sequences, usually fewer actions are observed in each scene.\nWe use the same settings for the hyperparameters as in Tab. 1. Only one of the two hyper-\nparameter for the IBP, h (2) αA , is modified to reduce the number of expected features, h (2) αA = 10. Further, we perform 500 iterations of the Gibbs sampler. In both data sets, after a mere 100 iterations, the sampler converges to the stationary distribution and produces samples from the target distribution."
    }, {
      "heading" : "6.1 Scene 11 - Traffic jam",
      "text" : "Scene 11 of the KITTI data set shows an urban scenario, in which the driver follows another vehicle. The vehicle in front cannot be overtaken due to a single-lane road. As shown in Fig. 4(a), the driver first accelerates. After a few seconds, the driver has to decelerate until the car stops due to a halt of the preceding car. When the vehicle in front starts moving again, the host vehicle accelerates.\nApplying the proposed algorithm to the observations results in 31 features using the MAP estimator. For the analysis of the features, we only visualize three of the most relevant features, which are selected according to the confidence in the corresponding feature policy.\nConsidering the current and the previous frames as inputs, we obtain feature estimates of consecutive frames. Observing the differences between the frames can be difficult as the features are likely to be highly similar due to the low temporal difference. Therefore, we show the feature of the current time frame and the difference of the features. A difference plot can be interpreted as the temporal gradient of two consecutive frames. Thus, from the depicted patterns we can draw conclusions about the relative speed of the obstacles. For instance, if the difference plot shows negative values left of positive values (red-blue pattern), the obstacle is faster than the host vehicle. In contrast, a blue-red pattern indicates a slower vehicle. The width of the bars reflect the magnitude of the relative velocities.\nThe features are illustrated in Fig. 5. In the figures, the x-axis corresponds to the longitudinal and the y-axis to the lateral direction. Shown is the top-view on the host vehicle, which is indicated by the red cross. The intensity reflects the measured heights at each pixel. As explained, the difference features are obtained by subtracting the features of the previous from the current time frame. Features A1 and A2 clearly show that the driver accelerates as long as the preceding car is significantly faster. A3, however, indicates acceleration though the preceding car decelerates. Since the car seems to be sufficiently far away, the driver has not yet decided to reduce the speed. Features D1 and D2 explain the deceleration of the driver with a slower car in front. In contrast, D3 shows that the preceding car accelerates. However, due to the low distance between both cars, the driver decides to decelerate. Features C1 and C2 show a vehicle in front of the driver’s car. The difference images reveal only little differences between the velocities of both vehicles, such that the driver maintains the current velocity. C3 represents an empty road, where, again, the driver does not need to adapt the speed of his car.\nIn order to evaluate the prediction performance, as in the simulations, 20% of the observations are used for testing while the rest is used for inferring the structure. Using the MAP estimator results in an accuracy of 74.32 %. The confusion matrix in Tab. 2 shows that, most notably, in some cases moving at constant speed and acceleration are confused. Further, deceleration is in few cases misclassified as acceleration.\nQuantifying the results of the state reconstruction is difficult, as a ground truth is not available. To provide at least an intuition about the quality of the reconstructions, we compute the MSE between the reconstructed states, based on the estimated substates and features, and the observations. This results in a MSE of approx. 0.0329. As the values of the observations are in a similar range as in the simulations, this result indicates low errors, resembling the results of the simulations. Still, the number has to be taken with care, as we compare the denoised reconstruction with noisy observation."
    }, {
      "heading" : "6.2 Scene 20 - Lane change",
      "text" : "Scene 20 shows a lane change maneuver on a two-lane road. As can be observed from the acceleration signals depicted in Fig. 4(b), the driver accelerates three times, depending on the current traffic situation. The lane change takes place from time frame 158 to 220.\nApplying the proposed algorithm reveals 17 features. Using these features to predict the actions of the test data set yields an accuracy of 73.33%. The confusion matrix in Tab. 3 shows that lane changes are reliably predicted. Acceleration maneuvers are in some cases misinterpreted as moving at constant speed or as a lane change. A third of the moving at constant speed observations are misclassified as lane changes. Comparing the reconstructed states with the noisy observation yields a MSE of 0.0027.\nFor illustration of the inferred features, Fig. 6 shows three out of the 17 features, each indicating a different action. The first feature (a) shows a lane change (right). As the driver is already performing the maneuver, the scene is rotated. The dark areas in the lower left corner represent vehicles behind the host car. The second feature (b) contains a vehicle behind the driver’s car and another vehicle on the left lane. Due to the high traffic density, the driver does not accelerate. In contrast, the third feature (c) shows a completely empty road, motivating the driver to accelerate the vehicle."
    }, {
      "heading" : "7 Discussion",
      "text" : "As shown in the simulation experiments, the algorithm based on the proposed model is able to reliably infer the number of features, the features and the policies. In case of strong noise, the algorithm finds several, almost equally probable explanations for the observations, resulting in variations of the MAP estimate. Especially when the number of features is high compared to the dimensionality of the observations, inferring the correct number of features can be challenging.\nThe real data experiments show that the model is able to provide deeper insights into the observations which may yield new conclusions about the observed behavior. As explained, in high-dimensional observations, the observation likelihood is likely to dominate the posterior leading to only little influence of the action likelihood and, hence, poor prediction performance.\nAs proposed in Section 4.3, reweighting the action likelihood, assuming an action variable for each entry of the observation, yields a significant performance increase, while the observed states can still be reliably reconstructed.\nAn advantage of the proposed generative model is that it can be modified and extended easily. This is helpful especially for a different assumption on the feature weights. For example, if real-valued features are expected, the corresponding prior can be changed to a Gaussian distribution. Of course, inference has to be adapted accordingly. Further, one can easily extend the proposed model to a semi-supervised learning approach, in which we add variables for states where the actions are not observed. Thus, the state representations can be learned from even more data resulting in more accurate estimates."
    }, {
      "heading" : "8 Conclusion",
      "text" : "We have presented a feature-based framework for learning from demonstrations that allows us to reason about the observed behavior and to predict actions for new states. A key assumption in the proposed model is that the observations are composed of latent features. Each feature imposes its own policy and contributes to the decision of the agent. To learn the structure of the behavior and to predict actions, we have considered a Bayesian nonparametric approach based on the Indian Buffet Process, which allows to infer the number of features and the features itself from the observed data. By means of this model, we are able to obtain a deeper understanding of the observed behavior as the features and their policies allow to reason about the observed decisions. The simulations show that the developed algorithm performs well. Only in scenarios with strong noise and many latent features, inference becomes challenging. Further, we have considered the task of learning a driver’s behavior. For this, we have applied our algorithm to real data obtained from the KITTI benchmark suite. The results reveal under which conditions the driver takes the observed actions. Further, prediction on a hold-out data set demonstrates that actions can be predicted reliably."
    } ],
    "references" : [ {
      "title" : "A survey of robot learning from demonstration",
      "author" : [ "B.D. Argall", "S. Chernova", "M. Veloso", "B. Browning" ],
      "venue" : "Robotics and Autonomous Systems,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2009
    }, {
      "title" : "Robot learning from demonstration",
      "author" : [ "C.G. Atkeson", "S. Schaal" ],
      "venue" : "In Proceedings of the 14th International Conference on Machine Learning,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1997
    }, {
      "title" : "Dynamic Programming",
      "author" : [ "R. Bellman" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1957
    }, {
      "title" : "Exploiting structure in policy construction",
      "author" : [ "C. Boutilier", "R. Dearden", "M. Goldszmidt" ],
      "venue" : "In Proceedings of the 14th International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1995
    }, {
      "title" : "Linear least-squares algorithms for temporal difference learning",
      "author" : [ "S.J. Bradtke", "A.G. Barto" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1996
    }, {
      "title" : "Nonparametric Bayesian inverse reinforcement learning for multiple reward functions",
      "author" : [ "J. Choi", "K.-E. Kim" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2012
    }, {
      "title" : "Fast simulation of truncated Gaussian distributions",
      "author" : [ "N. Chopin" ],
      "venue" : "Statistics and Computing,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2010
    }, {
      "title" : "Feature reinforcement learning: State of the art",
      "author" : [ "M. Daswani", "P. Sunehag", "M. Hutter" ],
      "venue" : "Workshop on Sequential Decision-making with Big Data. Association for the Advancement of Artificial Intelligence,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2014
    }, {
      "title" : "Learning the structure of factored Markov decision processes in reinforcement learning problems",
      "author" : [ "T. Degris", "O. Sigaud", "P.-H. Wuillemin" ],
      "venue" : "In Proceedings of the 23rd International Conference on Machine learning,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2006
    }, {
      "title" : "Accelerated sampling for the Indian buffet process",
      "author" : [ "F. Doshi-Velez", "Z. Ghahramani" ],
      "venue" : "In Proceedings of the 26th Annual International Conference on Machine Learning,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2009
    }, {
      "title" : "Are we ready for autonomous driving? The KITTI vision benchmark suite",
      "author" : [ "A. Geiger", "P. Lenz", "R. Urtasun" ],
      "venue" : "In Proceedings of the 25th IEEE International Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2012
    }, {
      "title" : "Discovering latent causes in reinforcement learning",
      "author" : [ "S.J. Gershman", "K.A. Norman", "Y. Niv" ],
      "venue" : "Current Opinion in Behavioral Sciences,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2015
    }, {
      "title" : "Infinite latent feature models and the Indian buffet process",
      "author" : [ "Z. Ghahramani", "T.L. Griffiths" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2005
    }, {
      "title" : "Bayesian nonparametric latent feature models",
      "author" : [ "Z. Ghahramani", "P. Sollich", "T.L. Griffiths" ],
      "venue" : "In Bayesian Statistics",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2007
    }, {
      "title" : "The Indian buffet process: An introduction and review",
      "author" : [ "T.L. Griffiths", "Z. Ghahramani" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2011
    }, {
      "title" : "An introduction to variable and feature selection",
      "author" : [ "I. Guyon", "A. Elisseeff" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2003
    }, {
      "title" : "Inverse reinforcement learning using expectation maximization in mixture models",
      "author" : [ "J. Hahn", "A.M. Zoubir" ],
      "venue" : "In Proceedings of the 40th IEEE International Conference on Acoustics, Speech and Signal Processing,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2015
    }, {
      "title" : "Feature reinforcement learning: Part I",
      "author" : [ "M. Hutter" ],
      "venue" : "Unstructured MDPs. Journal of Artificial General Intelligence,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2009
    }, {
      "title" : "Spike and slab variable selection: Frequentist and Bayesian strategies",
      "author" : [ "H. Ishwaran", "J.S. Rao" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2005
    }, {
      "title" : "Principal component analysis",
      "author" : [ "I.T. Jolliffe" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2002
    }, {
      "title" : "Planning and acting in partially observable stochastic domains",
      "author" : [ "L.P. Kaelbling", "M.L. Littman", "A.R. Cassandra" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1998
    }, {
      "title" : "Nonparametric Bayesian sparse factor models with application to gene expression modeling",
      "author" : [ "D. Knowles", "Z. Ghahramani" ],
      "venue" : "Annals of Applied Statistics,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2011
    }, {
      "title" : "Algorithms for non-negative matrix factorization",
      "author" : [ "D.D. Lee", "H.S. Seung" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2001
    }, {
      "title" : "Driver behavior and situation aware brake assistance for intelligent vehicles",
      "author" : [ "J.C. McCall", "M.M. Trivedi" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2007
    }, {
      "title" : "Finite Mixture Models",
      "author" : [ "G. McLachlan", "D. Peel" ],
      "venue" : null,
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2000
    }, {
      "title" : "Modeling dyadic data with binary latent factors",
      "author" : [ "E. Meeds", "Z. Ghahramani", "R.M. Neal", "S.T. Roweis" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2006
    }, {
      "title" : "Bayesian nonparametric inverse reinforcement learning",
      "author" : [ "B. Michini", "J.P. How" ],
      "venue" : "In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2012
    }, {
      "title" : "Bayesian variable selection in linear regression",
      "author" : [ "T.J. Mitchell", "J.J. Beauchamp" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 1988
    }, {
      "title" : "Humanlevel control through deep reinforcement learning",
      "author" : [ "V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis" ],
      "venue" : "Nature, 518(7540):529–533,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2015
    }, {
      "title" : "Algorithms for inverse reinforcement learning",
      "author" : [ "A.Y. Ng", "S.J. Russell" ],
      "venue" : "In Proceedings of the 17th International Conference on Machine Learning,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2000
    }, {
      "title" : "Online feature selection for model-based reinforcement learning",
      "author" : [ "T. Nguyen", "Z. Li", "T. Silander", "T.Y. Leong" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2013
    }, {
      "title" : "Policy gradient approaches for multi-objective sequential decision making",
      "author" : [ "S. Parisi", "M. Pirotta", "N. Smacchia", "L. Bascetta", "M. Restelli" ],
      "venue" : "In Proceedings of the International Joint Conference on Neural Networks,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2014
    }, {
      "title" : "Efficient training of artificial neural networks for autonomous navigation",
      "author" : [ "D.A. Pomerleau" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 1991
    }, {
      "title" : "Markov Decision Processes: Discrete Stochastic Dynamic Programming",
      "author" : [ "M.L. Puterman" ],
      "venue" : null,
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 1994
    }, {
      "title" : "Beam search based map estimates for the Indian buffet process",
      "author" : [ "P. Rai", "H. Daume" ],
      "venue" : "In Proceedings of the 28th International Conference on Machine Learning,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2011
    }, {
      "title" : "Boosting structured prediction for imitation learning",
      "author" : [ "N. Ratliff", "D. Bradley", "J.A. Bagnell", "J. Chestnutt" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2007
    }, {
      "title" : "Maximum margin planning",
      "author" : [ "N.D. Ratliff", "J.A. Bagnell", "M.A. Zinkevich" ],
      "venue" : "In Proceedings of the 23rd International Conference on Machine Learning,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2006
    }, {
      "title" : "Neural fitted Q iteration – first experiences with a data efficient neural reinforcement learning method",
      "author" : [ "M. Riedmiller" ],
      "venue" : "In Proceedings of the 16th European Conference on Machine Learning,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2005
    }, {
      "title" : "A reduction of imitation learning and structured prediction to no-regret online learning",
      "author" : [ "S. Ross", "G.J. Gordon", "D. Bagnell" ],
      "venue" : "In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2011
    }, {
      "title" : "An MDP-based recommender system",
      "author" : [ "G. Shani", "D. Heckerman", "R.I. Brafman" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2005
    }, {
      "title" : "Importance sampling for reinforcement learning with multiple objectives",
      "author" : [ "C.R. Shelton" ],
      "venue" : "PhD thesis, Massachusetts Institute of Technology,",
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 2001
    }, {
      "title" : "The optimal control of partially observable Markov processes over a finite horizon",
      "author" : [ "R.D. Smallwood", "E.J. Sondik" ],
      "venue" : "Operations Research,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 1973
    }, {
      "title" : "A Bayesian approach to policy recognition and state representation learning",
      "author" : [ "A. Šošić", "A.M. Zoubir", "H. Koeppl" ],
      "venue" : null,
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 2016
    }, {
      "title" : "Policy recognition via expectation maximization",
      "author" : [ "A. Sosic", "A.M. Zoubir", "H. Koeppl" ],
      "venue" : "In Proceedings of the 41st IEEE International Conference on Acoustics, Speech and Signal Processing,",
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 2016
    }, {
      "title" : "Bayesian nonparametric inverse reinforcement learning for switched Markov decision processes",
      "author" : [ "A. Surana", "K. Srivastava" ],
      "venue" : "In Proceedings of the 13th International Conference on Machine Learning and Applications,",
      "citeRegEx" : "45",
      "shortCiteRegEx" : "45",
      "year" : 2014
    }, {
      "title" : "Constructing stochastic mixture policies for episodic multiobjective reinforcement learning tasks",
      "author" : [ "P. Vamplew", "R. Dazeley", "E. Barker", "A. Kelarev" ],
      "venue" : "In Proceedings of the 22nd Australasian Joint Conference on Advances in Artificial Intelligence,",
      "citeRegEx" : "46",
      "shortCiteRegEx" : "46",
      "year" : 2009
    }, {
      "title" : "A forward collision warning algorithm with adaptation to driver behaviors",
      "author" : [ "J. Wang", "C. Yu", "S.E. Li", "L. Wang" ],
      "venue" : "IEEE Transactions on Intelligent Transportation Systems,",
      "citeRegEx" : "47",
      "shortCiteRegEx" : "47",
      "year" : 2016
    }, {
      "title" : "Learning from Delayed Rewards",
      "author" : [ "C.J.C.H. Watkins" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "48",
      "shortCiteRegEx" : "48",
      "year" : 1989
    }, {
      "title" : "Deep inverse reinforcement learning",
      "author" : [ "M. Wulfmeier", "P. Ondruska", "I. Posner" ],
      "venue" : "In NIPS Deep Reinforcement Learning Workshop,",
      "citeRegEx" : "49",
      "shortCiteRegEx" : "49",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 39,
      "context" : "Decision-making plays a crucial role in many applications, such as robot learning, driver assistance systems, and recommender systems [40].",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 0,
      "context" : "Therefore, Learning From Demonstrations (LFD) [1] has gained a lot of interest in the recent past.",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 0,
      "context" : "According to [1], approaches for LFD can be grouped into (i) reward-based models and (ii) imitation learning.",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 29,
      "context" : "In reward-based models, it is assumed that the agent makes its decision based on a reward which is, in the context of LFD, learned from observations (as in Inverse Reinforcement Learning (IRL) [30]).",
      "startOffset" : 193,
      "endOffset" : 197
    }, {
      "referenceID" : 2,
      "context" : "Only in the case of finite state and action spaces and known rewards, learning optimal policies has been solved [3].",
      "startOffset" : 112,
      "endOffset" : 115
    }, {
      "referenceID" : 32,
      "context" : "[33, 5].",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 4,
      "context" : "[33, 5].",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 11,
      "context" : "This idea has also been investigated from a psychological point of view by the concept of discovering latent causes in human behavior, which is related to learning state space representations [12].",
      "startOffset" : 192,
      "endOffset" : 196
    }, {
      "referenceID" : 20,
      "context" : "Since we consider a decisionmaking task, the investigated problem can be modeled by means of a Partially Observable Markov Decision Process (POMDP) [21, 42], which is defined by • a set of observations, Z, • a set of states, X , • a finite set of Nu actions, U , • a transition model, which describes the probability of entering a state after taking an action in the current state, • an observation model which explains how the observations are generated from the states, • a discount factor, which penalizes long-term rewards, • and a reward function, R.",
      "startOffset" : 148,
      "endOffset" : 156
    }, {
      "referenceID" : 41,
      "context" : "Since we consider a decisionmaking task, the investigated problem can be modeled by means of a Partially Observable Markov Decision Process (POMDP) [21, 42], which is defined by • a set of observations, Z, • a set of states, X , • a finite set of Nu actions, U , • a transition model, which describes the probability of entering a state after taking an action in the current state, • an observation model which explains how the observations are generated from the states, • a discount factor, which penalizes long-term rewards, • and a reward function, R.",
      "startOffset" : 148,
      "endOffset" : 156
    }, {
      "referenceID" : 19,
      "context" : "A simple approach to the considered problem would be the use of a feature extraction technique such as Principal Component Analysis (PCA) [20] or Non-negative Matrix Factorization (NMF) [23] to learn features from the observed states.",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 22,
      "context" : "A simple approach to the considered problem would be the use of a feature extraction technique such as Principal Component Analysis (PCA) [20] or Non-negative Matrix Factorization (NMF) [23] to learn features from the observed states.",
      "startOffset" : 186,
      "endOffset" : 190
    }, {
      "referenceID" : 2,
      "context" : "1 Reinforcement Learning Large state and action spaces are especially problematic for value-based RL algorithms such as value-iteration [3] or Q-learning [48] since the value function, representing the expected accumulated reward, needs to be approximated.",
      "startOffset" : 136,
      "endOffset" : 139
    }, {
      "referenceID" : 47,
      "context" : "1 Reinforcement Learning Large state and action spaces are especially problematic for value-based RL algorithms such as value-iteration [3] or Q-learning [48] since the value function, representing the expected accumulated reward, needs to be approximated.",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 4,
      "context" : "In early approaches, a set of basis functions, often referred to as features, is linearly weighted to represent the this function [5].",
      "startOffset" : 130,
      "endOffset" : 133
    }, {
      "referenceID" : 37,
      "context" : "Riedmiller [38] has proposed the Q-fitted value iteration where the value function is approximated by means of a neural network, where the features are learned in the layers of the network.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 28,
      "context" : "In [29], this approach has been extended by replacing the neural network by a deep-layered counterpart.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 17,
      "context" : "A different concept to employ features is proposed by Hutter with the Feature Reinforcement Learning framework [18].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 7,
      "context" : "In this framework, the goal is to learn a feature mapping from the agent’s history (comprised of actions, states, and rewards) to a MDP state, enabling decision learning for infinite state spaces [8].",
      "startOffset" : 196,
      "endOffset" : 199
    }, {
      "referenceID" : 8,
      "context" : "An alternative framework for learning the latent structure of the state space is proposed in [9] which is based on Factored Markov Decision Processs (FMDPs) [4].",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 3,
      "context" : "An alternative framework for learning the latent structure of the state space is proposed in [9] which is based on Factored Markov Decision Processs (FMDPs) [4].",
      "startOffset" : 157,
      "endOffset" : 160
    }, {
      "referenceID" : 30,
      "context" : "This approach has been extended in [31] to an online approach, where the features are selected from a large set by means of Group LASSO.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 29,
      "context" : "2 Inverse Reinforcement Learning IRL is concerned with the problem of learning the reward function from observed behavior [30].",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 29,
      "context" : ", in [30, 17].",
      "startOffset" : 5,
      "endOffset" : 13
    }, {
      "referenceID" : 16,
      "context" : ", in [30, 17].",
      "startOffset" : 5,
      "endOffset" : 13
    }, {
      "referenceID" : 48,
      "context" : "Recent attempts have been made to consider a Deep Learning (DL) architecture [49] for IRL, providing means for nonlinear, hierarchical feature learning.",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 5,
      "context" : "A Bayesian nonparametric approach is proposed in [6], utilizing an Indian Buffet Process (IBP) to model feature activations.",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 26,
      "context" : "Different results on Bayesian nonparametrics for IRL, which is indirectly related to feature learning, are given in [27], where a partitioning of the state space is sought for, or [45], where complex behavior is decomposed into several, simpler behaviors that can be easily learned.",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 44,
      "context" : "Different results on Bayesian nonparametrics for IRL, which is indirectly related to feature learning, are given in [27], where a partitioning of the state space is sought for, or [45], where complex behavior is decomposed into several, simpler behaviors that can be easily learned.",
      "startOffset" : 180,
      "endOffset" : 184
    }, {
      "referenceID" : 1,
      "context" : "3 Imitation Learning Instead of estimating the reward as in IRL, imitation learning aims at inferring the underlying policy directly [2, 44].",
      "startOffset" : 133,
      "endOffset" : 140
    }, {
      "referenceID" : 43,
      "context" : "3 Imitation Learning Instead of estimating the reward as in IRL, imitation learning aims at inferring the underlying policy directly [2, 44].",
      "startOffset" : 133,
      "endOffset" : 140
    }, {
      "referenceID" : 36,
      "context" : ", in [37, 39].",
      "startOffset" : 5,
      "endOffset" : 13
    }, {
      "referenceID" : 38,
      "context" : ", in [37, 39].",
      "startOffset" : 5,
      "endOffset" : 13
    }, {
      "referenceID" : 35,
      "context" : "Attempts to introduce new features are made in [36] as an extension of the maximum margin planning algorithm which is proposed in [37].",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 36,
      "context" : "Attempts to introduce new features are made in [36] as an extension of the maximum margin planning algorithm which is proposed in [37].",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 0,
      "context" : "As explained in [1], imitation learning can be considered",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 15,
      "context" : "An excellent overview is given in [16].",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 22,
      "context" : "1 Feature model for learning from demonstrations We assume a linear latent feature model, similar to NMF [23] and PCA [20].",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 19,
      "context" : "1 Feature model for learning from demonstrations We assume a linear latent feature model, similar to NMF [23] and PCA [20].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 21,
      "context" : "Following [22], the feature matrix is composed of a binary activation matrix, A ∈ {0, 1}K×D, and a weighting matrix, W ∈ FK×D, where the relation is given by the Hadamard product,",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 12,
      "context" : "(2) can be easily extended to an infinite feature model by placing an IBP [13, 15] prior over A.",
      "startOffset" : 74,
      "endOffset" : 82
    }, {
      "referenceID" : 14,
      "context" : "(2) can be easily extended to an infinite feature model by placing an IBP [13, 15] prior over A.",
      "startOffset" : 74,
      "endOffset" : 82
    }, {
      "referenceID" : 31,
      "context" : "Mixture polices have been investigated in multiobjective problems, where an agent aims at reaching several objectives, some of which can even be conflicting [32, 46, 41].",
      "startOffset" : 157,
      "endOffset" : 169
    }, {
      "referenceID" : 45,
      "context" : "Mixture polices have been investigated in multiobjective problems, where an agent aims at reaching several objectives, some of which can even be conflicting [32, 46, 41].",
      "startOffset" : 157,
      "endOffset" : 169
    }, {
      "referenceID" : 40,
      "context" : "Mixture polices have been investigated in multiobjective problems, where an agent aims at reaching several objectives, some of which can even be conflicting [32, 46, 41].",
      "startOffset" : 157,
      "endOffset" : 169
    }, {
      "referenceID" : 20,
      "context" : "Acting according to a stochastic policy maximizes the expected return [21].",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 33,
      "context" : "In case of single agents, it has been shown that deterministic policies are optimal solutions for MDPs [34].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 26,
      "context" : "One possibility to infer this number is to utilize a Chinese Restaurant Process (CRP), giving rise to a Bayesian nonparametric model [27].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 42,
      "context" : "A similar model, where the state space is clustered according to the played actions, is proposed in [43].",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 12,
      "context" : "3 Prior for the feature activations The feature activations are modeled by means of an IBP [13, 15], assuming an infinite number of features.",
      "startOffset" : 91,
      "endOffset" : 99
    }, {
      "referenceID" : 14,
      "context" : "3 Prior for the feature activations The feature activations are modeled by means of an IBP [13, 15], assuming an infinite number of features.",
      "startOffset" : 91,
      "endOffset" : 99
    }, {
      "referenceID" : 13,
      "context" : "In the following, we consider the two-parameter generalization [14] which allows to sample sparse as well as dense matrices.",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 12,
      "context" : "Placing a Beta prior with hyperparameters αaβa K? and βa over the parameter of the Binomial distribution and marginalizing over this parameter yields a Beta-Binomial distribution [13, 14].",
      "startOffset" : 179,
      "endOffset" : 187
    }, {
      "referenceID" : 13,
      "context" : "Placing a Beta prior with hyperparameters αaβa K? and βa over the parameter of the Binomial distribution and marginalizing over this parameter yields a Beta-Binomial distribution [13, 14].",
      "startOffset" : 179,
      "endOffset" : 187
    }, {
      "referenceID" : 12,
      "context" : "Since we are interested in sampling from an infinite number of features, we consider the limit for K →∞, resulting in the distribution of the activation matrix A [13, 14],",
      "startOffset" : 162,
      "endOffset" : 170
    }, {
      "referenceID" : 13,
      "context" : "Since we are interested in sampling from an infinite number of features, we consider the limit for K →∞, resulting in the distribution of the activation matrix A [13, 14],",
      "startOffset" : 162,
      "endOffset" : 170
    }, {
      "referenceID" : 12,
      "context" : "Thus, we need to store only rows with active elements in memory, which can be understood as realizations of K features, where the average number of active rows is given by K̄ = αa ∑D d=1 βa βa+d−1 [13, 10].",
      "startOffset" : 197,
      "endOffset" : 205
    }, {
      "referenceID" : 9,
      "context" : "Thus, we need to store only rows with active elements in memory, which can be understood as realizations of K features, where the average number of active rows is given by K̄ = αa ∑D d=1 βa βa+d−1 [13, 10].",
      "startOffset" : 197,
      "endOffset" : 205
    }, {
      "referenceID" : 13,
      "context" : "The hyperparameters αa and βa reflect our prior belief about the number of features and the sparsity of the matrix [14].",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 21,
      "context" : ", αa ∼ Gaαa ( h (1) αA , h (2) αA ) and βa ∼ Gaβa ( h (1) βA , h (2) βA ) [22].",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 27,
      "context" : "In particular, we consider a sparsity-promoting mixture prior on the substates, similar to a spike and slab model [28, 19].",
      "startOffset" : 114,
      "endOffset" : 122
    }, {
      "referenceID" : 18,
      "context" : "In particular, we consider a sparsity-promoting mixture prior on the substates, similar to a spike and slab model [28, 19].",
      "startOffset" : 114,
      "endOffset" : 122
    }, {
      "referenceID" : 6,
      "context" : "We use the algorithm presented in [7] to sample from a truncated Gaussian distribution.",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 12,
      "context" : "(11) is [13, 14]",
      "startOffset" : 8,
      "endOffset" : 16
    }, {
      "referenceID" : 13,
      "context" : "(11) is [13, 14]",
      "startOffset" : 8,
      "endOffset" : 16
    }, {
      "referenceID" : 9,
      "context" : "In the second step, K new features are proposed in a Metropolis step [10, 22].",
      "startOffset" : 69,
      "endOffset" : 77
    }, {
      "referenceID" : 21,
      "context" : "In the second step, K new features are proposed in a Metropolis step [10, 22].",
      "startOffset" : 69,
      "endOffset" : 77
    }, {
      "referenceID" : 12,
      "context" : "The probability of adding K features is given as [13, 14]",
      "startOffset" : 49,
      "endOffset" : 57
    }, {
      "referenceID" : 13,
      "context" : "The probability of adding K features is given as [13, 14]",
      "startOffset" : 49,
      "endOffset" : 57
    }, {
      "referenceID" : 25,
      "context" : "As the proposal distribution is independent of the previous sample, the acceptance ratio, r, is equal to the likelihood ratio between the new and existing features [26],",
      "startOffset" : 164,
      "endOffset" : 168
    }, {
      "referenceID" : 21,
      "context" : "Since the IBP tends to mix slowly, we augment this ratio with probability P of accepting a single new feature, resulting in a modified acceptance ratio which is derived in [22].",
      "startOffset" : 172,
      "endOffset" : 176
    }, {
      "referenceID" : 21,
      "context" : "The hyperparameters αa and βa are sampled as described in [22].",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 24,
      "context" : ", Nz, for each observation, indicating from which policy the observed action, un, has been generated [25].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 34,
      "context" : ", estimates of the features, F̂MAP [35], and the policies, Φ̂MAP.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 23,
      "context" : "We consider the problem of analyzing a driver’s behavior, which is an important task for useradaptive driver assistance systems [24, 47], in order to demonstrate the performance of the proposed model in a real-world scenario.",
      "startOffset" : 128,
      "endOffset" : 136
    }, {
      "referenceID" : 46,
      "context" : "We consider the problem of analyzing a driver’s behavior, which is an important task for useradaptive driver assistance systems [24, 47], in order to demonstrate the performance of the proposed model in a real-world scenario.",
      "startOffset" : 128,
      "endOffset" : 136
    }, {
      "referenceID" : 10,
      "context" : "For this, we consider real data provided by the KITTI Vision Benchmark Suite [11] containing several challenges in urban driving.",
      "startOffset" : 77,
      "endOffset" : 81
    } ],
    "year" : 2017,
    "abstractText" : "Learning from demonstrations has gained increasing interest in the recent past, enabling an agent to learn how to make decisions by observing an experienced teacher. While many approaches have been proposed to solve this problem, there is only little work that focuses on reasoning about the observed behavior. We assume that, in many practical problems, an agent makes its decision based on latent features, indicating a certain action. Therefore, we propose a generative model for the states and actions. Inference reveals the number of features, the features, and the policies, allowing us to learn and to analyze the underlying structure of the observed behavior. Further, our approach enables prediction of actions for new states. Simulations are used to assess the performance of the algorithm based upon this model. Moreover, the problem of learning a driver’s behavior is investigated, demonstrating the performance of the proposed model in a real-world scenario.",
    "creator" : "LaTeX with hyperref package"
  }
}