{
  "name" : "1204.4200.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Discrete Dynamical Genetic Programming in XCS",
    "authors" : [ "Richard J. Preen", "Larry Bull" ],
    "emails" : [ "richard2.preen@uwe.ac.uk", "larry.bull@uwe.ac.uk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n20 4.\n42 00\nv2 [\ncs .A\nI] 1\n8 O\nct 2\n01 4\nCategories and Subject Descriptors I.2.6 [Artificial Intelligence]: Learning—knowledge acquisition, parameter learning\nGeneral Terms Experimentation\nKeywords Learning Classifier Systems, Random Boolean Networks, Reinforcement Learning, Self-Adaptation, XCS"
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "Traditionally, learning classifier systems (LCS) [17] use a ternary encoding to generalise over the environmental inputs and to associate appropriate actions. A number of representations have previously been presented beyond this scheme however, including real numbers [41], LISP S-expressions [24], fuzzy logic [37] and neural networks [5]. To date, no temporally dynamic representation schemes have been used in LCS, a potentially important approach since temporal behaviour of such kinds is viewed as a significant aspect of cognition in general.\nIn this paper we explore the use of a dynamical system representation within XCS [40]—what is herein termed“dynamical genetic programming” (DGP). Traditional tree-based genetic programming (GP) [21] has been used within LCS both to calculate the action [1] and to represent the condition [24]. DGP uses a graph-based representation, each node\nProceedings of the Genetic and Evolutionary Computation Conference, GECCO’09, July 8–12, 2009, Montréal, Québec, Canada, pp. 1299–1306. ACM doi:10.1145/1569901.1570075.\nof which is constantly updated with asynchronous parallelism, and evolved using an open-ended, self-adaptive scheme. In the discrete case, each node is a Boolean function and therefore equivalent to a form of random Boolean network (RBN) (e.g., [20]). We show that XCS is able to solve a number of well-known immediate and delayed reward tasks using this temporally dynamic knowledge representation scheme."
    }, {
      "heading" : "2. RELATED WORK",
      "text" : "A number of representations have been presented by which to enable the evolution of computer programs, the most common being tree-based LISP S-expressions [24]. Other forms of GP include the use of machine code instructions (e.g., [4]) and finite state machines (e.g., [13]). Most relevant to the form of GP used in this paper is the small amount of prior work on graph-based representations. Teller and Veloso’s “neural programming” [35] uses a directed graph of connected nodes, each with functionality defined in the standard GP way, with recursive connections included. Significantly, each node is executed with synchronous parallelism for some number of cycles before an output node’s value is taken. Poli (e.g., [31]) presented a very similar scheme wherein the graph is placed over a 2D grid and executes its nodes synchronously in parallel. Other examples of graph-based GP typically contain sequentially updating nodes (e.g., [27]). Schmidt and Lipson [32] have recently demonstrated a number of benefits from graph encodings over traditional trees, such as reduced bloat and increased computational efficiency.\nAs noted above, tree-based S-expressions have been used within LCS. Recently, Wilson [42] has explored the use of a form of gene expression programming (GEP) [12] within LCS. Here the rules are represented as expression trees that are evaluated by assigning the environmental inputs to the tree’s terminals, evaluating the tree, and then comparing the result with a predetermined threshold. Whenever the threshold value is exceeded, the rule is added to the match set.\nThe most common form of discrete dynamical system is the cellular automaton (CA) [38], which consists of an array of cells (lattice of nodes) where the cells exist in states from a finite set and update their states with synchronous parallelism in discrete time. Traditionally, each cell calculates its next state depending upon its current state and the states of its closest neighbours. That is, CAs may be seen as a graph with a (typically) restricted topology. Packard [30] was the first to use evolutionary computing techniques to design CAs such that they exhibit a given emergent global behaviour. Following Packard, Mitchell et al. (e.g., [28])\nhave investigated the use of a genetic algorithm (GA) [16] to learn the rules of uniform binary CAs. As in Packard’s work, the GA produces the entries in the update table used by each cell, candidate solutions being evaluated with regard to their degree of success for the given task. Andre et al. [2] repeated Mitchell et al.’s work whilst using traditional GP to evolve the update rules. They report similar results. Sipper (e.g., [33]) presented a non-uniform, or heterogeneous, approach to evolving CAs. Each cell of a 1- or 2D CA is also viewed as a GA population member, mating only with its lattice neighbours and receiving an individual fitness. He shows an increase in performance over Mitchell et al.’s work by exploiting the potential for spatial heterogeneity in the tasks. Sipper and Ruppin [34] extended this approach to enable heterogeneity in the node connectivity, along with the node function; they evolved a form of random Boolean networks."
    }, {
      "heading" : "3. RANDOM BOOLEAN NETWORKS",
      "text" : "The discrete dynamical systems known as random Boolean networks (RBN) were originally introduced by Kauffman (see [20]) to explore aspects of biological genetic regulatory networks. Since then they have been used as a tool in a wide range of areas, such as self-organisation (e.g., [20]) and computation (e.g., [26]). An RBN typically consists of a network of N nodes, each performing a Boolean function with K inputs from other nodes in the network, all updating synchronously (see Figure 1). As such, RBN may be viewed as a generalisation of binary CAs. Since they have a finite number of possible states (2N ) and they use deterministic Boolean functions, the dynamics of RBN eventually fall into a basin of attraction. It is well-established that the value of K affects the emergent behaviour of RBN wherein attractors typically contain an increasing number of states with increasing K. 3 phases of behaviour are suggested: ordered when K = 1, with attractors consisting of 1 or a few states; chaotic when K > 3, with a very large number of states per attractor; and, a critical regime around K = 2, where similar states lie on trajectories that tend to neither diverge nor converge and 5–15% of nodes change state per attractor cycle (see [20] for discussions of this critical regime, e.g., with respect to perturbations). Analytical methods have been presented by which to determine the typical time taken to reach a basin of attraction and the number of states within such basins for a given degree of connectivity K (see [20]).\nClosely akin to the work described here, Kauffman [20] describes the use of simulated evolution to design RBN that must play a (mis)matching game wherein mutation is used to change connectivity, the Boolean functions, K and N . He reports the typical emergence of high fitness solutions with K=2 to 3, together with an increase in N over the initialised size. As noted above, traditional RBN consist of N nodes updating synchronously in discrete time steps, but asynchronous versions have also been presented, after [15], leading to a classification of the space of possible forms of RBN [14]. Asynchronous forms of CA have also been explored (e.g., [19]) wherein it is often suggested that asynchrony is a more realistic underlying assumption for many natural and artificial systems.\nAsynchronous logic devices are known to have the potential to consume less power and dissipate less heat [39], which may be exploitable during efforts towards hardware implementations of such systems. Asynchronous logic is also\nknown to have the potential for improved fault tolerance, particularly through delay insensitive schemes (e.g., [9]). This may also prove beneficial for hardware implementations.\nHarvey and Bossomaier [15] showed that asynchronous RBN exhibit either point attractors, as seen in asynchronous CAs, or “loose” attractors where “the network passes indefinitely through a subset of its possible states” [15] (as opposed to distinct cycles in the synchronous case). Thus the use of asynchrony represents another feature of RBN with the potential to significantly alter their underlying dynamics thereby offering another mechanism by which to aid the simulated evolutionary design process for a given task. Di Paolo [10] showed it is possible to evolve asynchronous RBN that exhibit rhythmic behaviour at equilibrium. Asynchronous CAs have also been evolved (e.g., [34])."
    }, {
      "heading" : "4. DISCRETE DGP-XCS",
      "text" : "To use asynchronous RBN as the rules within XCS, the following scheme is adopted. Each of an initial randomly created rule’s nodes has K randomly assigned connections, here 1 ≤ K ≤ 5. There are as many nodes N as input fields I for the given task and its outputs O, plus 1 other, as will be described, i.e., N = I +O + 1. The first connection of each input node is set to the corresponding locus of the input message. The other connections are assigned at random within the RBN as usual. In this way, the current input state is always considered along with the current state of the RBN itself per network update cycle by such nodes (see Figure 2). Nodes are initialised randomly each time the network is run to determine [M], etc. The population is initially empty and covering is applied to generate rules as in the standard XCS approach.\nMatching consists of executing each rule for T cycles based on the current input. The value of T is chosen to be a value typically within the basin of attraction of the RBN. Asynchrony is here implemented as a randomly chosen node being updated on a given cycle, with as many updates per overall network update cycle as there are nodes in the network before an equivalent cycle to 1 in the synchronous case is said to have occurred. See [14] for alternative schemes.\nIn this study, when well-known Boolean problems are explored there are only 2 possible actions and thus only 1 out-\nput node is required. Where well-known maze problems are explored there are 8 possible actions and accordingly 3 required output nodes. An extra “matching” node is also required to enable RBNs to (potentially) only match specific sets of inputs. If a given RBN has a logical ‘0’ on the match node, regardless of its output node’s state, the rule does not join [M] (see Figure 2). This scheme has also been exploited within neural LCS [5]. A ‘windowed approach’ is utilised where the output is decided by the most common state over the last W steps up to T . For example, if the last few states on a node updating prior to cycle T is 0101001 and W = 3, then the ending node’s state would be ‘0’ and not ‘1’. In this paper, W is set to 3. Thereafter, match set and action set processing proceeds as standard in XCS (the reader is referred to [8] for an algorithmic description of XCS).\nWhen covering is necessitated, a randomly constructed RBN is created and then executed for T cycles to determine the status of the match and output nodes. This procedure is repeated until an RBN is created that matches the environment state.\nParameter self-adaptation was first explored in LCS by Bull et al. [7] wherein the mutation rate is a locally evolving entity in itself; each rule has its own mutation rate µ Mutation only is used here and applied to the node’s truth table and connectivity map at rate µ. A node’s truth table is represented by a binary string and its connectivity by a list of K integers in the range [1, N ]. Since each node has\na given fixed K value, each node maintains a binary string of length 2K , which forms the entries in the look-up table for each of the possible 2K input states of that node, i.e., as in the aforementioned work of Packard [30] on evolving CAs, for example. These strings are subjected to mutation on reproduction at the self-adapting rate µ for that rule. Hence, within the RBN representation, evolution can define different Boolean functions for each node within a given network rule, along with its connectivity map. Specifically, each rule has its own mutation rate stored as a real number and initially seeded uniform randomly in the range [0, 1]. This parameter is passed to its offspring. The offspring then applies its mutation rate to itself using a Gaussian distribution, i.e., µ′ = µeN(0,1), before mutating the rest of the rule at the resulting rate.\nDue to the need for a possible different number of nodes within the rules for a given task, the DGP scheme is also of variable length. Once the truth table and connections have been mutated, a new randomly connected node is either added or the last added node is removed with the same probability µ. The latter case only occurs if the network currently consists of more than the initial number of nodes. Thus DGP is temporally dynamic both in the search process and the representation scheme. Evolving variable-length solutions via mutation only has previously been explored a number of times, e.g., [13]. Traditional GP can be seen to primarily rely upon recombination to search the space of possible tree sizes, although the standard mutation operator effectively increases or decreases tree size also. Whenever an offspring classifier is created and no changes occur to its RBN when undergoing mutation, the parent’s numerosity is increased and mutation rate set to the offspring’s."
    }, {
      "heading" : "5. EXPERIMENTATION",
      "text" : ""
    }, {
      "heading" : "5.1 Multiplexer",
      "text" : "We now apply this discrete version of DGP-XCS (dDGPXCS) to the well-known multiplexer task. These Boolean functions are defined for binary strings of length l = x+ 2x under which the x bits index into the remaining 2x bits, returning the value of the indexed bit. The correct classification to a randomly generated input results in a payoff of 1000, otherwise 0.\nFigure 3 shows the performance of the constructed system on the 6-bit multiplexer problem updated asynchronously with P = 800, ν = 5, θGA = 25, β = 0.2, pexpl = 1.0, T = 25, W = 3, and Ninit = 8 (6 inputs, 1 output, 1 match node). After Wilson [40], performance from exploit trials only is recorded (fraction of correct responses are shown), using a 50-point running average, averaged over 10 runs.\nFrom Figure 3a it can be seen that a near optimal solution is learnt around 35,000 trials and optimality is observed around trial 58,000. The parameter governing RBN mutation (see Figure 3a) declines rapidly until reaching a bottom around 40,000 trials, which is shortly after discovering an optimal solution. The number of (non-unique) rules initially grows rapidly, before declining to around 650. Furthermore, the average degree of connectivity K decreases fractionally, whilst, on average, each network grows approximately 1 extra node (see Figure 3b. This behaviour indicates that the evolutionary process is able to identify an appropriate typical topology with which to generate complex behaviour, i.e., in this case a computation. For other tasks, other values of\nK may prove beneficial; high K may be expected in random number generation, for example. It can be noted that a growth event under which a new node is added into an RBN is essentially neutral here since the new node receives inputs from the existing nodes (or itself) on addition but only provides inputs to other nodes after subsequent connectivity mutations. For comparative purposes, Figure 4 shows the performance with the same parameters on the 6- bit multiplexer when updated synchronously. It is shown that the performance is very similar regardless of the updating scheme and that there is thus apparently very little overhead when updating asynchronously, with the possible benefits mentioned above. Figure 2 provides an illustration of a rule generated whilst solving the 6-bit multiplexer problem when updated asynchronously. There is 1 new node in addition to the initial 8. The truth table shows to which state each node will transition, given each of the possible inputs. For example, the output node (node 1) has a truth table of ‘10’, which is synonymous with a NOT gate where if node 3 is in state ‘0’ then the output node will be set to ‘1’, and if node 3 is in state ‘1’ then the output node will be set to ‘0’. The truth table of node 3 is synonymous with an AND gate, etc.\nThe rule has a prediction of 1000 and an Error of 0, whilst\nhaving an experience of 822, showing that this is a highly accurate rule. Analysis of this RBN rule was undertaken by executing it for each of the 64 6-bit inputs. Each input was run 20 times with T = 25 and W = 3. The results show that for the majority of environment states the network will return a false match node, preventing it from being added to [M]. However, the network is general as the match node will always return true when the environment states are"
    }, {
      "heading" : "110000, 110010, 110100, 110110, 111000, 111010, 111100,",
      "text" : "and 111110. In all of those cases the output node always advocates action ‘0’. In addition, there are several environment states for which the match node will only sometimes return true. However, in all cases when the match node does permit the rule to be added to [M], the action advocated will always be consistent. There are 4 such additional environment states (010000, 010010, 011000, and 011010) for which the rule will match, albeit with a probability less than 50%.\nThe rule in Figure 2 was then re-run as before, however using a traditional synchronous updating scheme. The results of the match node and output nodes are extremely similar regardless of the updating mechanism. That is, XCS has evolved an RBN that is very robust to the random nature of the asynchronous updating, meaning it is accurate even for the relatively rare case of all nodes updating concurrently, i.e., the synchronous case."
    }, {
      "heading" : "5.2 Maze Environments",
      "text" : "In addition to the single-step multiplexer problems, dDGPXCS is applied to versions of 3 well-known multi-step maze environments, Woods 1 (see Figure 5a), Maze 4 (see Figure 5b), and Woods 101 (see Figure 5c).\nEach cell in the maze environments is encoded with 2 binary bits, where white space is represented as a ‘*’, obstacles as ‘O’, and food as ‘F’. Furthermore, actions are encoded in binary as shown in Figure 5d. The task is simply to find the shortest path to the food (F) given a random start point. Obstacles (O) represent cells that cannot be occupied. A teletransportation mechanism is employed whereby a trial is reset if the agent has not reached the goal state within 50 discrete movements. In Woods 1 the optimal number of steps to the food is 1.7, in Maze 4 optimal is 3.5 steps, and in Woods 101 it is 2.9. Figures 6a–6c show the performance\nof dDGP-XCS in the Woods 1 environment. The parameters used are identical to those applied in the aforementioned multiplexer experiments, except that Ninit = 20 (16 inputs, 3 outputs, 1 match node) (P = 800). As can be seen from Figure 6a, optimality is observed around 2,500 trials. This roughly matches the performance of neural XCS using self-adaptive constructivism (≈2,500 trials, P = 2000) [18] and faster than XCS using messy conditions (≈8,000 trials, P = 800) [22], XCS using stack-based GP conditions (≈10,000 trials, P = 1000) [23], and XCS with LISP Sexpression conditions (≈5,000 trials, P = 800) [24]. Figure 6b shows that there is an average of 745 (non-unique) rules evolved. In addition, Figure 6b shows that the mutation rate declines rapidly by 2,800 trials, shortly after the optimal solution is learnt. Figure 6c shows that on average the networks add 1 extra node (from the original 20) and the average number of connections decreases slightly. Figures 7a–7c present the performance of dDGP-XCS in the Maze 4 environment. The parameters used are identical to those in the Woods 1 environment, however a bigger population limit of P = 2000 is used, reflecting the larger search space. Optimality is observed around trial 23,000 (see Figure 7a), which is again similar to the performance observed using a neural XCS with self-adaptive constructivism (≈23,000 trials, P = 3000) [18]. The average number of rules evolved is around 1,800 (see Figure 7b). The average number of nodes in the networks also increases by almost 1, and the average number of connections declines slightly from 3 (see Figure 7c). The parameter governing RBN mutation (Figure 7b) declines rapidly after 4,000 trials, before finally stabilising after 15,000 trials.\nThe Woods 101 maze is a non-Markov environment containing 2 communicating aliasing states, i.e., 2 positions that border on the same non-aliasing state and are identically sensed, but require different optimal actions. Thus, to solve this maze optimally, a form of memory must be utilised (with at least 2 internal states). Optimal performance has previously been achieved in Woods 101 through the addition of a\nmemory register mechanism in XCS [25], a corporate XCS using rule-linkage [36], and a neural LCS using recurrent links [6]. Furthermore, in a proof of concept experiment, the cyclical directed graph from neural programming has been shown capable of representing rules with memory to solve Woods 101, however it was only found to do so twice in 50 experiments [3].\nThe simplest form of short-term memory is a fixed-length buffer containing the n most recent inputs; a common extension is to then apply a kernel function to the buffer to enable non-uniform sampling of the past values, e.g., an exponential decay of older inputs [29]. Simple forms of memory are static, i.e., the memory parameters are fixed in advance and the memory state is thus a predetermined function of\nthe input sequence. However, it is not clear that biological systems make use of such shift registers. Registers require some interface with the environment that buffers the input so that it can be presented simultaneously. They impose a rigid limit on the duration of patterns, defining the longest possible pattern and requiring that all input vectors be of the same length. Furthermore, such approaches struggle to distinguish relative temporal position from absolute temporal position [11].\nThe hypothesis of inherent content-addressable memory existing within synchronous RBN due to different possible routes to a basin of attraction [43] for the asynchronous case is here explored and extended by simply not resetting the node states on each step. A significant advantage of this approach is that each rule/network’s short-term memory is variable-length and adaptive, i.e., the networks can adjust the memory parameters, selecting within the limits of the capacity of the memory, what aspects of the input sequence are available for computing predictions [29]. In addition, as open-ended evolution is used, the maximum size of the shortterm memory is potentially also open-ended, increasing as the number of nodes within the network grows.\nHere, nodes are initialised at random for the initial random placing in the maze but thereafter they are not reset for each subsequent matching cycle. Consequently, each network processes the environmental input and the final node states then become the starting point for the next processing cycle, whereupon the network receives the new environmental input and places the network on a trajectory toward a (potentially) different locally stable limit point. A network given the same environmental input (i.e., the agent’s current maze perception) but with different initial node states (representing the agent’s history through the maze) may fall into a different basin of attraction (advocating a different action). Thus the rules’ dynamics are (potentially) constantly affected by the inputs as the system executes.\nFigures 8a–8c show the performance in the Woods 101 environment where all parameters used are identical to those applied in the previous Maze 4 environment. As can be seen from Figure 8a, dDGP-XCS, without node resets, is able to achieve optimal performance in Woods 101 after approximately 12,000 trials (this is slower than XCS using an explicit 1-bit memory register (≈7,000 trials, P = 800) [25]. Figure 8b shows the mutation rate and macro-classifiers. Figure 8c shows the average number of nodes and connections. Optimal performance is unattainable however when the nodes are reset randomly between matching (Figure 9), proving that the system is exploiting the potential for memory within asynchronous RBN here. The mechanism works within XCS because rules/RBN experience each input but need not match on each cycle. Hence for the ambiguous states they remain accurate for the payoff received on providing the action but do so having processed the previous input in an appropriate way, potentially without matching."
    }, {
      "heading" : "6. CONCLUSIONS",
      "text" : "In this paper a form of XCS has been presented with which to design asynchronous random Boolean networks. It has been shown that XCS is able to design ensembles of RBN that collectively solve a computational task under a reinforcement learning scheme. In particular, it has been shown possible to exploit the inherent dynamics of the representation scheme to solve a non-Markov maze, i.e., without extra\nmechanisms. Current research is exploring the possibilities of DGP as a general representation scheme by which to solve complex problems with LCS."
    }, {
      "heading" : "7. REFERENCES",
      "text" : "[1] M. Ahluwalia and L. Bull. A genetic programming\nclassifier system. In Proceedings of the Genetic and\nEvolutionary Computation Conference, GECCO ’99, pages 11–18, 1999.\n[2] D. Andre, J. R. Koza, F. H. Bennett, and M. Keane. Genetic Programming III. MIT Press, 1999.\n[3] G. C. Balan and S. Luke. A demonstration of neural programming applied to non-Markovian problems. In Proceedings of the Genetic and Evolutionary Computation Conference, GECCO ’04, pages 422–433, 2004.\n[4] W. Banzhaf. Genetic programming for pedestrians. In S. Forrest, editor, Proceedings of the Fifth International Conference on Genetic Algorithms. Morgan Kaufmann, 1993.\n[5] L. Bull. On using constructivism in neural classifier systems. In J. J. Merelo, P. Adamidis, and H.-G. Beyer, editors, Parallel Problem Solving from Nature: PPSN VII, volume 2439 of Lecture Notes in Computer Science, pages 558–567. Berlin: Springer-Verlag, 2002.\n[6] L. Bull and J. Hurst. A neural learning classifier system with self-adaptive constructivism. In Proceedings of the IEEE Congress on Evolutionary Computation, volume 2, pages 991–997, 2003.\n[7] L. Bull, J. Hurst, and A. Tomlinson. Self-adaptive mutation in classifier system controllers. In J. A. Meyer, A. Berthoz, D. Floreano, H. Roitblat, and S. W. Wilson, editors, From Animals to Animats 6, Proceedings of the Sixth International Conference on Simulation of Adaptive Behavior, pages 460–468. MIT Press, 2000.\n[8] M. V. Butz and S. W. Wilson. An algorithmic description of XCS. In Revised Papers from the Third International Workshop on Advances in Learning Classifier Systems, IWLCS ’00, pages 253–272. Berlin: Springer-Verlag, 2001.\n[9] J. Di and P. K. Lala. Cellular array-based delay-insensitive asynchronous circuits design and test for nanocomputing systems. Journal of Electronic Testing: Theory and Applications, 23(2–3):175–192, 2007.\n[10] E. A. Di Paolo. Rhythmic and non-rhythmic attractors in asynchronous random boolean networks. Biosystems, 59(3):185–195, 2001.\n[11] J. L. Elman. Finding structure in time. Cognitive Science, 14(2):179–211, 1990.\n[12] C. Ferreira. Gene Expression Programming. Berlin: Springer-Verlag, 2006.\n[13] L. J. Fogel, A. J. Owens, and M. J. Walsh. Artificial intelligence through a simulation of evolution. In Biophysics and Cybernetic Systems: Proceedings of the 2nd Cybernetic Sciences Symposium, pages 131–155, Washington, DC, USA, 1965. Spartan Book Co.\n[14] C. Gershenson. Classification of random boolean networks. In Proceedings of the 8th international conference on Artificial life, pages 1–8. MIT Press, 2002.\n[15] I. Harvey and T. Bossomaier. Time out of joint: Attractors in asynchronous random boolean networks. In Proceedings of the Fourth European Artificial Life Conference, pages 67–75. MIT Press, 1997.\n[16] J. H. Holland. Adaptation in Natural and Artificial Systems. University of Michigan Press, 1975.\n[17] J. H. Holland. Adaptation. In R. Rosen and F. M. Snell, editors, Progress in Theoretical Biology, volume 4, pages 263–293. Academic Press Inc., 1976.\n[18] G. D. Howard, L. Bull, and P.-L. Lanzi. Self-adaptive constructivism in neural XCS and XCSF. In Proceedings of the Genetic and Evolutionary Computation Conference, GECCO ’08, pages 1389–1396, 2008.\n[19] T. E. Ingerson and R. L. Buvel. Structure in asynchronous cellular automata. Physica D, 10(1–2):59–68, 1984.\n[20] S. A. Kauffman. The Origins of Order: Self-Organization and Selection in Evolution. Oxford, UK, 1993.\n[21] J. R. Koza. Genetic Programming. MIT Press, 1992.\n[22] P. L. Lanzi. Extending the representations of classifier conditions part i: From binary to messy coding. In Proceedings of the Genetic and Evolutionary Computation Conference, GECCO ’99, pages 337–344, 1999.\n[23] P. L. Lanzi. XCS with stack-based genetic programming. In Proceedings of the IEEE Congress on Evolutionary Computation, volume 2, pages 1186–1191, 2003.\n[24] P. L. Lanzi and A. Perrucci. Extending the representation of classifier conditions part ii: From messy coding to S-expressions. In Proceedings of the Genetic and Evolutionary Computation Conference, GECCO ’99, pages 345–352, 1999.\n[25] P. L. Lanzi and S. W. Wilson. Toward optimal classifier system performance in non-Markov environments. Evolutionary Computation, 8(4):393–418, 2000.\n[26] B. Mesot and C. Teuscher. Deducing local rules for solving global tasks with random boolean networks. Physica D, 211(1-2):88–106, 2005.\n[27] J. F. Miller. An empirical study of the efficiency of learning boolean functions using a cartesian genetic programming approach. In Proceedings of the Genetic and Evolutionary Computation Conference, GECCO ’99, pages 1135–1142, 1999.\n[28] M. Mitchell, P. T. Hraber, and J. P. Crutchfield.\nRevisiting the edge of chaos: Evolving cellular automata to perform computations. Complex Systems, 7:89–130, 1993.\n[29] M. C. Mozer. Neural net architectures for temporal sequence processing. In A. S. Weigend and N. A. Gershenfeld, editors, Time Series Prediction: Forecasting the Future and Understanding the Past, pages 243–264. Addison-Wesley, 1994.\n[30] N. Packard. Adaptation toward the edge of chaos. In J. Kelso, A. Mandell, and M. Shlesinger, editors, Dynamic Patterns in Complex Systems, pages 293–301. World Scientific, 1988.\n[31] J. C. F. Pujol and R. Poli. Efficient evolution of asymmetric recurrent neural networks using a PDGP-inspired two-dimensional representation. In Proceedings of the First European Workshop on Genetic Programming, pages 130–141. Berlin: Springer-Verlag, 1998.\n[32] M. Schmidt and H. Lipson. Comparison of tree and graph encodings as function of problem complexity. In Proceedings of the Genetic and Evolutionary Computation Conference, GECCO ’07, pages 1674–1679, 2007.\n[33] M. Sipper. Evolution of Parallel Cellular Machines. Berlin: Springer-Verlag, 1997.\n[34] M. Sipper and E. Ruppin. Co-evolving architectures for cellular machines. Physica D, 99(4):428–441, 1997.\n[35] A. Teller and M. Veloso. Neural programming and an internal reinforcement policy. In J. R. Koza, editor, Late Breaking Papers at the Genetic Programming 1996 Conference, pages 186–192. Stanford University, CA, USA, 1996.\n[36] A. Tomlinson. CXCS: Triggered linkage. Technical Report UWELCSG01-003, University of the West of England, Bristol, UK, 2001.\n[37] M. Valenzuela-Rendón. The fuzzy classifier system: A classifier system for continuously varying variables. In Proceedings of the Fourth International Conference on Genetic Algorithms, pages 346–353. Morgan Kaufmann, 1991.\n[38] J. Von Neumann. The Theory of Self-Reproducing Automata. University of Illinois, 1966.\n[39] T. Werner and V. Akella. Asynchronous processor survey. Computer (USA), 30(11):67–76, 1997.\n[40] S. W. Wilson. Classifier fitness based on accuracy. Evolutionary Computation, 3(2):149–175, 1995.\n[41] S. W. Wilson. Get real! XCS with continuous-valued inputs. In Learning Classifier Systems, From Foundations to Applications, pages 209–222. Berlin: Springer-Verlag, 2000.\n[42] S. W. Wilson. Classifier conditions using gene expression programming. In J. Bacardit, E. Bernado-Mansilla, M. V. Butz, T. Kovacs, X. Llora, and K. Takadama, editors, Learning Classifier Systems, pages 206–217. Berlin: Springer-Verlag, 2008.\n[43] A. Wuensche. Basins of attraction in network dynamics: A conceptual framework for biomolecular networks. In G. Schlosser and G. P. Wagner, editors, Modularity in Development and Evolution, pages 288–311. Chicago, University Press, 2004."
    } ],
    "references" : [ {
      "title" : "A genetic programming classifier system",
      "author" : [ "M. Ahluwalia", "L. Bull" ],
      "venue" : "Proceedings of the Genetic and  0  5000  10000  15000  20000  25000  0  5  10  15  20  25  30  35 dDGP-XCS (reset) Optimum (2.9) Trials  N  um  be  r  o  f  S  te  ps  to  G  oa  l Figure 9: dDGP-XCS Woods 101: Number of Steps to Goal with Nodes Reset (circle). Evolutionary Computation Conference, GECCO ’99, pages 11–18",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Genetic Programming III",
      "author" : [ "D. Andre", "J.R. Koza", "F.H. Bennett", "M. Keane" ],
      "venue" : "MIT Press",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "A demonstration of neural programming applied to non-Markovian problems",
      "author" : [ "G.C. Balan", "S. Luke" ],
      "venue" : "Proceedings of the Genetic and Evolutionary Computation Conference, GECCO ’04, pages 422–433",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Genetic programming for pedestrians",
      "author" : [ "W. Banzhaf" ],
      "venue" : "S. Forrest, editor, Proceedings of the Fifth International Conference on Genetic Algorithms. Morgan Kaufmann",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "On using constructivism in neural classifier systems",
      "author" : [ "L. Bull" ],
      "venue" : "J. J. Merelo, P. Adamidis, and H.-G. Beyer, editors, Parallel Problem Solving from Nature: PPSN VII, volume 2439 of Lecture Notes in Computer Science, pages 558–567. Berlin: Springer-Verlag",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "A neural learning classifier system with self-adaptive constructivism",
      "author" : [ "L. Bull", "J. Hurst" ],
      "venue" : "Proceedings of the IEEE Congress on Evolutionary Computation, volume 2, pages 991–997",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Self-adaptive mutation in classifier system controllers",
      "author" : [ "L. Bull", "J. Hurst", "A. Tomlinson" ],
      "venue" : "J. A. Meyer, A. Berthoz, D. Floreano, H. Roitblat, and S. W. Wilson, editors, From Animals to Animats 6, Proceedings of the Sixth International Conference on Simulation of Adaptive Behavior, pages 460–468. MIT Press",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "An algorithmic description of XCS",
      "author" : [ "M.V. Butz", "S.W. Wilson" ],
      "venue" : "Revised Papers from the Third International Workshop on Advances in Learning Classifier Systems, IWLCS ’00, pages 253–272. Berlin: Springer-Verlag",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Cellular array-based delay-insensitive asynchronous circuits design and test for nanocomputing systems",
      "author" : [ "J. Di", "P.K. Lala" ],
      "venue" : "Journal of Electronic Testing: Theory and Applications, 23(2–3):175–192",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Rhythmic and non-rhythmic attractors in asynchronous random boolean networks",
      "author" : [ "E.A. Di Paolo" ],
      "venue" : "Biosystems, 59(3):185–195",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Finding structure in time",
      "author" : [ "J.L. Elman" ],
      "venue" : "Cognitive Science, 14(2):179–211",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "Gene Expression Programming",
      "author" : [ "C. Ferreira" ],
      "venue" : "Berlin: Springer-Verlag",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Artificial intelligence through a simulation of evolution",
      "author" : [ "L.J. Fogel", "A.J. Owens", "M.J. Walsh" ],
      "venue" : "Biophysics and Cybernetic Systems: Proceedings of the 2nd Cybernetic Sciences Symposium, pages 131–155, Washington, DC, USA",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1965
    }, {
      "title" : "Classification of random boolean networks",
      "author" : [ "C. Gershenson" ],
      "venue" : "Proceedings of the 8th international conference on Artificial life, pages 1–8. MIT Press",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Time out of joint: Attractors in asynchronous random boolean networks",
      "author" : [ "I. Harvey", "T. Bossomaier" ],
      "venue" : "Proceedings of the Fourth European Artificial Life Conference, pages 67–75. MIT Press",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Adaptation in Natural and Artificial Systems",
      "author" : [ "J.H. Holland" ],
      "venue" : "University of Michigan Press",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1975
    }, {
      "title" : "Adaptation",
      "author" : [ "J.H. Holland" ],
      "venue" : "R. Rosen and F. M. Snell, editors, Progress in Theoretical Biology, volume 4, pages 263–293. Academic Press Inc.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1976
    }, {
      "title" : "Self-adaptive constructivism in neural XCS and XCSF",
      "author" : [ "G.D. Howard", "L. Bull", "P.-L. Lanzi" ],
      "venue" : "Proceedings of the Genetic and Evolutionary Computation Conference, GECCO ’08, pages 1389–1396",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Structure in asynchronous cellular automata",
      "author" : [ "T.E. Ingerson", "R.L. Buvel" ],
      "venue" : "Physica D, 10(1–2):59–68",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 1984
    }, {
      "title" : "The Origins of Order: Self-Organization and Selection in Evolution",
      "author" : [ "S.A. Kauffman" ],
      "venue" : "Oxford, UK",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Genetic Programming",
      "author" : [ "J.R. Koza" ],
      "venue" : "MIT Press",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Extending the representations of classifier conditions part i: From binary to messy coding",
      "author" : [ "P.L. Lanzi" ],
      "venue" : "Proceedings of the Genetic and Evolutionary Computation Conference, GECCO ’99, pages 337–344",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "XCS with stack-based genetic programming",
      "author" : [ "P.L. Lanzi" ],
      "venue" : "Proceedings of the IEEE Congress on Evolutionary Computation, volume 2, pages 1186–1191",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Extending the representation of classifier conditions part ii: From messy coding to S-expressions",
      "author" : [ "P.L. Lanzi", "A. Perrucci" ],
      "venue" : "Proceedings of the Genetic and Evolutionary Computation Conference, GECCO ’99, pages 345–352",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Toward optimal classifier system performance in non-Markov environments",
      "author" : [ "P.L. Lanzi", "S.W. Wilson" ],
      "venue" : "Evolutionary Computation, 8(4):393–418",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Deducing local rules for solving global tasks with random boolean networks",
      "author" : [ "B. Mesot", "C. Teuscher" ],
      "venue" : "Physica D, 211(1-2):88–106",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "An empirical study of the efficiency of learning boolean functions using a cartesian genetic programming approach",
      "author" : [ "J.F. Miller" ],
      "venue" : "Proceedings of the Genetic and Evolutionary Computation Conference, GECCO ’99, pages 1135–1142",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Revisiting the edge of chaos: Evolving cellular automata to perform computations",
      "author" : [ "M. Mitchell", "P.T. Hraber", "J.P. Crutchfield" ],
      "venue" : "Complex Systems, 7:89–130",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Neural net architectures for temporal sequence processing",
      "author" : [ "M.C. Mozer" ],
      "venue" : "A. S. Weigend and N. A. Gershenfeld, editors, Time Series Prediction: Forecasting the Future and Understanding the Past, pages 243–264. Addison-Wesley",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Adaptation toward the edge of chaos",
      "author" : [ "N. Packard" ],
      "venue" : "J. Kelso, A. Mandell, and M. Shlesinger, editors, Dynamic Patterns in Complex Systems, pages 293–301. World Scientific",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "Efficient evolution of asymmetric recurrent neural networks using a PDGP-inspired two-dimensional representation",
      "author" : [ "J.C.F. Pujol", "R. Poli" ],
      "venue" : "Proceedings of the First European Workshop on Genetic Programming, pages 130–141. Berlin: Springer-Verlag",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Comparison of tree and graph encodings as function of problem complexity",
      "author" : [ "M. Schmidt", "H. Lipson" ],
      "venue" : "Proceedings of the Genetic and Evolutionary Computation Conference, GECCO ’07, pages 1674–1679",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Evolution of Parallel Cellular Machines",
      "author" : [ "M. Sipper" ],
      "venue" : "Berlin: Springer-Verlag",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Co-evolving architectures for cellular machines",
      "author" : [ "M. Sipper", "E. Ruppin" ],
      "venue" : "Physica D, 99(4):428–441",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Neural programming and an internal reinforcement policy",
      "author" : [ "A. Teller", "M. Veloso" ],
      "venue" : "J. R. Koza, editor, Late Breaking Papers at the Genetic Programming 1996 Conference, pages 186–192. Stanford University, CA, USA",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "CXCS: Triggered linkage",
      "author" : [ "A. Tomlinson" ],
      "venue" : "Technical Report UWELCSG01-003, University of the West of England, Bristol, UK",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "The fuzzy classifier system: A classifier system for continuously varying variables",
      "author" : [ "M. Valenzuela-Rendón" ],
      "venue" : "Proceedings of the Fourth International Conference on Genetic Algorithms, pages 346–353. Morgan Kaufmann",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "The Theory of Self-Reproducing Automata",
      "author" : [ "J. Von Neumann" ],
      "venue" : "University of Illinois",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 1966
    }, {
      "title" : "Asynchronous processor survey",
      "author" : [ "T. Werner", "V. Akella" ],
      "venue" : "Computer (USA), 30(11):67–76",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Classifier fitness based on accuracy",
      "author" : [ "S.W. Wilson" ],
      "venue" : "Evolutionary Computation, 3(2):149–175",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Get real! XCS with continuous-valued inputs",
      "author" : [ "S.W. Wilson" ],
      "venue" : "Learning Classifier Systems, From Foundations to Applications, pages 209–222. Berlin: Springer-Verlag",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Classifier conditions using gene expression programming",
      "author" : [ "S.W. Wilson" ],
      "venue" : "J. Bacardit, E. Bernado-Mansilla, M. V. Butz, T. Kovacs, X. Llora, and K. Takadama, editors, Learning Classifier Systems, pages 206–217. Berlin: Springer-Verlag",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Basins of attraction in network dynamics: A conceptual framework for biomolecular networks",
      "author" : [ "A. Wuensche" ],
      "venue" : "G. Schlosser and G. P. Wagner, editors, Modularity in Development and Evolution, pages 288–311. Chicago, University Press",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "Traditionally, learning classifier systems (LCS) [17] use a ternary encoding to generalise over the environmental inputs and to associate appropriate actions.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 40,
      "context" : "A number of representations have previously been presented beyond this scheme however, including real numbers [41], LISP S-expressions [24], fuzzy logic [37] and neural networks [5].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 23,
      "context" : "A number of representations have previously been presented beyond this scheme however, including real numbers [41], LISP S-expressions [24], fuzzy logic [37] and neural networks [5].",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 36,
      "context" : "A number of representations have previously been presented beyond this scheme however, including real numbers [41], LISP S-expressions [24], fuzzy logic [37] and neural networks [5].",
      "startOffset" : 153,
      "endOffset" : 157
    }, {
      "referenceID" : 4,
      "context" : "A number of representations have previously been presented beyond this scheme however, including real numbers [41], LISP S-expressions [24], fuzzy logic [37] and neural networks [5].",
      "startOffset" : 178,
      "endOffset" : 181
    }, {
      "referenceID" : 39,
      "context" : "In this paper we explore the use of a dynamical system representation within XCS [40]—what is herein termed“dynamical genetic programming” (DGP).",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 20,
      "context" : "Traditional tree-based genetic programming (GP) [21] has been used within LCS both to calculate the action [1] and to represent the condition [24].",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 0,
      "context" : "Traditional tree-based genetic programming (GP) [21] has been used within LCS both to calculate the action [1] and to represent the condition [24].",
      "startOffset" : 107,
      "endOffset" : 110
    }, {
      "referenceID" : 23,
      "context" : "Traditional tree-based genetic programming (GP) [21] has been used within LCS both to calculate the action [1] and to represent the condition [24].",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 19,
      "context" : ", [20]).",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 23,
      "context" : "A number of representations have been presented by which to enable the evolution of computer programs, the most common being tree-based LISP S-expressions [24].",
      "startOffset" : 155,
      "endOffset" : 159
    }, {
      "referenceID" : 3,
      "context" : ", [4]) and finite state machines (e.",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 12,
      "context" : ", [13]).",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 34,
      "context" : "Teller and Veloso’s “neural programming” [35] uses a directed graph of connected nodes, each with functionality defined in the standard GP way, with recursive connections included.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 30,
      "context" : ", [31]) presented a very similar scheme wherein the graph is placed over a 2D grid and executes its nodes synchronously in parallel.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 26,
      "context" : ", [27]).",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 31,
      "context" : "Schmidt and Lipson [32] have recently demonstrated a number of benefits from graph encodings over traditional trees, such as reduced bloat and increased computational efficiency.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 41,
      "context" : "Recently, Wilson [42] has explored the use of a form of gene expression programming (GEP) [12] within LCS.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 11,
      "context" : "Recently, Wilson [42] has explored the use of a form of gene expression programming (GEP) [12] within LCS.",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 37,
      "context" : "The most common form of discrete dynamical system is the cellular automaton (CA) [38], which consists of an array of cells (lattice of nodes) where the cells exist in states from a finite set and update their states with synchronous parallelism in discrete time.",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 29,
      "context" : "Packard [30] was the first to use evolutionary computing techniques to design CAs such that they exhibit a given emergent global behaviour.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 27,
      "context" : ", [28])",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 15,
      "context" : "have investigated the use of a genetic algorithm (GA) [16] to learn the rules of uniform binary CAs.",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 1,
      "context" : "[2] repeated Mitchell et al.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 32,
      "context" : ", [33]) presented a non-uniform, or heterogeneous, approach to evolving CAs.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 33,
      "context" : "Sipper and Ruppin [34] extended this approach to enable heterogeneity in the node connectivity, along with the node function; they evolved a form of random Boolean networks.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 19,
      "context" : "The discrete dynamical systems known as random Boolean networks (RBN) were originally introduced by Kauffman (see [20]) to explore aspects of biological genetic regulatory networks.",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 19,
      "context" : ", [20]) and computation (e.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 25,
      "context" : ", [26]).",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 19,
      "context" : "3 phases of behaviour are suggested: ordered when K = 1, with attractors consisting of 1 or a few states; chaotic when K > 3, with a very large number of states per attractor; and, a critical regime around K = 2, where similar states lie on trajectories that tend to neither diverge nor converge and 5–15% of nodes change state per attractor cycle (see [20] for discussions of this critical regime, e.",
      "startOffset" : 353,
      "endOffset" : 357
    }, {
      "referenceID" : 19,
      "context" : "Analytical methods have been presented by which to determine the typical time taken to reach a basin of attraction and the number of states within such basins for a given degree of connectivity K (see [20]).",
      "startOffset" : 201,
      "endOffset" : 205
    }, {
      "referenceID" : 19,
      "context" : "Closely akin to the work described here, Kauffman [20] describes the use of simulated evolution to design RBN that must play a (mis)matching game wherein mutation is used to change connectivity, the Boolean functions, K and N .",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 14,
      "context" : "As noted above, traditional RBN consist of N nodes updating synchronously in discrete time steps, but asynchronous versions have also been presented, after [15], leading to a classification of the space of possible forms of RBN [14].",
      "startOffset" : 156,
      "endOffset" : 160
    }, {
      "referenceID" : 13,
      "context" : "As noted above, traditional RBN consist of N nodes updating synchronously in discrete time steps, but asynchronous versions have also been presented, after [15], leading to a classification of the space of possible forms of RBN [14].",
      "startOffset" : 228,
      "endOffset" : 232
    }, {
      "referenceID" : 18,
      "context" : ", [19]) wherein it is often suggested that asynchrony is a more realistic underlying assumption for many natural and artificial systems.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 38,
      "context" : "Asynchronous logic devices are known to have the potential to consume less power and dissipate less heat [39], which may be exploitable during efforts towards hardware implementations of such systems.",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 0,
      "context" : "1 1 1 0001 [1,3] Encoding",
      "startOffset" : 11,
      "endOffset" : 16
    }, {
      "referenceID" : 2,
      "context" : "1 1 1 0001 [1,3] Encoding",
      "startOffset" : 11,
      "endOffset" : 16
    }, {
      "referenceID" : 8,
      "context" : ", [9]).",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 14,
      "context" : "Harvey and Bossomaier [15] showed that asynchronous RBN exhibit either point attractors, as seen in asynchronous CAs, or “loose” attractors where “the network passes indefinitely through a subset of its possible states” [15] (as opposed to distinct cycles in the synchronous case).",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 14,
      "context" : "Harvey and Bossomaier [15] showed that asynchronous RBN exhibit either point attractors, as seen in asynchronous CAs, or “loose” attractors where “the network passes indefinitely through a subset of its possible states” [15] (as opposed to distinct cycles in the synchronous case).",
      "startOffset" : 220,
      "endOffset" : 224
    }, {
      "referenceID" : 9,
      "context" : "Di Paolo [10] showed it is possible to evolve asynchronous RBN that exhibit rhythmic behaviour at equilibrium.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 33,
      "context" : ", [34]).",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 13,
      "context" : "See [14] for alternative schemes.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 4,
      "context" : "This scheme has also been exploited within neural LCS [5].",
      "startOffset" : 54,
      "endOffset" : 57
    }, {
      "referenceID" : 7,
      "context" : "Thereafter, match set and action set processing proceeds as standard in XCS (the reader is referred to [8] for an algorithmic description of XCS).",
      "startOffset" : 103,
      "endOffset" : 106
    }, {
      "referenceID" : 6,
      "context" : "[7] wherein the mutation rate is a locally evolving entity in itself; each rule has its own mutation rate μ Mutation only is used here and applied to the node’s truth table and connectivity map at rate μ.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 29,
      "context" : ", as in the aforementioned work of Packard [30] on evolving CAs, for example.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 0,
      "context" : "Specifically, each rule has its own mutation rate stored as a real number and initially seeded uniform randomly in the range [0, 1].",
      "startOffset" : 125,
      "endOffset" : 131
    }, {
      "referenceID" : 12,
      "context" : ", [13].",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 39,
      "context" : "After Wilson [40], performance from exploit trials only is recorded (fraction of correct responses are shown), using a 50-point running average, averaged over 10 runs.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 17,
      "context" : "This roughly matches the performance of neural XCS using self-adaptive constructivism (≈2,500 trials, P = 2000) [18] and faster than XCS using messy conditions (≈8,000 trials, P = 800) [22], XCS using stack-based GP conditions (≈10,000 trials, P = 1000) [23], and XCS with LISP Sexpression conditions (≈5,000 trials, P = 800) [24].",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 21,
      "context" : "This roughly matches the performance of neural XCS using self-adaptive constructivism (≈2,500 trials, P = 2000) [18] and faster than XCS using messy conditions (≈8,000 trials, P = 800) [22], XCS using stack-based GP conditions (≈10,000 trials, P = 1000) [23], and XCS with LISP Sexpression conditions (≈5,000 trials, P = 800) [24].",
      "startOffset" : 185,
      "endOffset" : 189
    }, {
      "referenceID" : 22,
      "context" : "This roughly matches the performance of neural XCS using self-adaptive constructivism (≈2,500 trials, P = 2000) [18] and faster than XCS using messy conditions (≈8,000 trials, P = 800) [22], XCS using stack-based GP conditions (≈10,000 trials, P = 1000) [23], and XCS with LISP Sexpression conditions (≈5,000 trials, P = 800) [24].",
      "startOffset" : 254,
      "endOffset" : 258
    }, {
      "referenceID" : 23,
      "context" : "This roughly matches the performance of neural XCS using self-adaptive constructivism (≈2,500 trials, P = 2000) [18] and faster than XCS using messy conditions (≈8,000 trials, P = 800) [22], XCS using stack-based GP conditions (≈10,000 trials, P = 1000) [23], and XCS with LISP Sexpression conditions (≈5,000 trials, P = 800) [24].",
      "startOffset" : 326,
      "endOffset" : 330
    }, {
      "referenceID" : 17,
      "context" : "Optimality is observed around trial 23,000 (see Figure 7a), which is again similar to the performance observed using a neural XCS with self-adaptive constructivism (≈23,000 trials, P = 3000) [18].",
      "startOffset" : 191,
      "endOffset" : 195
    }, {
      "referenceID" : 24,
      "context" : "memory register mechanism in XCS [25], a corporate XCS using rule-linkage [36], and a neural LCS using recurrent links [6].",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 35,
      "context" : "memory register mechanism in XCS [25], a corporate XCS using rule-linkage [36], and a neural LCS using recurrent links [6].",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 5,
      "context" : "memory register mechanism in XCS [25], a corporate XCS using rule-linkage [36], and a neural LCS using recurrent links [6].",
      "startOffset" : 119,
      "endOffset" : 122
    }, {
      "referenceID" : 2,
      "context" : "Furthermore, in a proof of concept experiment, the cyclical directed graph from neural programming has been shown capable of representing rules with memory to solve Woods 101, however it was only found to do so twice in 50 experiments [3].",
      "startOffset" : 235,
      "endOffset" : 238
    }, {
      "referenceID" : 28,
      "context" : ", an exponential decay of older inputs [29].",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 10,
      "context" : "Furthermore, such approaches struggle to distinguish relative temporal position from absolute temporal position [11].",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 42,
      "context" : "The hypothesis of inherent content-addressable memory existing within synchronous RBN due to different possible routes to a basin of attraction [43] for the asynchronous case is here explored and extended by simply not resetting the node states on each step.",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 28,
      "context" : ", the networks can adjust the memory parameters, selecting within the limits of the capacity of the memory, what aspects of the input sequence are available for computing predictions [29].",
      "startOffset" : 183,
      "endOffset" : 187
    }, {
      "referenceID" : 24,
      "context" : "As can be seen from Figure 8a, dDGP-XCS, without node resets, is able to achieve optimal performance in Woods 101 after approximately 12,000 trials (this is slower than XCS using an explicit 1-bit memory register (≈7,000 trials, P = 800) [25].",
      "startOffset" : 238,
      "endOffset" : 242
    } ],
    "year" : 2014,
    "abstractText" : "A number of representation schemes have been presented for use within Learning Classifier Systems, ranging from binary encodings to neural networks. This paper presents results from an investigation into using a discrete dynamical system representation within the XCS Learning Classifier System. In particular, asynchronous random Boolean networks are used to represent the traditional condition-action production system rules. It is shown possible to use self-adaptive, open-ended evolution to design an ensemble of such discrete dynamical systems within XCS to solve a number of wellknown test problems.",
    "creator" : "LaTeX with hyperref package"
  }
}