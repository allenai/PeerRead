{
  "name" : "1303.3632.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Statistical Regression to Predict Total Cumulative CPU Usage of MapReduce Jobs",
    "authors" : [ "Nikzad Babaii Rizvandi", "Javid Taheri", "Reza Moraveji", "Albert Y. Zomaya" ],
    "emails" : [ "nikzad@it.usyd.edu.au" ],
    "sections" : [ {
      "heading" : null,
      "text" : "detection, and different data mining tasks, in both public and private clouds. Two of the challenging questions in such environments are (1) choosing suitable values for MapReduce configuration parameters –e.g., number of mappers, number of reducers, and DFS block size–, and (2) predicting the amount of resources that a user should lease from the service provider. Currently, the tasks of both choosing configuration parameters and estimating required resources are solely the users’ responsibilities. In this paper, we present an approach to provision the total CPU usage in clock cycles of jobs in MapReduce environment. For a MapReduce job, a profile of total CPU usage in clock cycles is built from the job past executions with different values of two configuration parameters e.g., number of mappers, and number of reducers. Then, a polynomial regression is used to model the relation between these configuration parameters and total CPU usage in clock cycles of the job. We also briefly study the influence of input data scaling on measured total CPU usage in clock cycles. This derived model along with the scaling result can then be used to provision the total CPU usage in clock cycles of the same jobs with different input data size. We validate the accuracy of our models using three realistic applications (WordCount, Exim MainLog parsing, and TeraSort). Results show that the predicted total CPU usage in clock cycles of generated resource provisioning options are less than 8% of the measured total CPU usage in clock cycles in our 20-node virtual Hadoop cluster.\nKeyword- total CPU usage in clock cycles, MapReduce, Hadoop, Resource provisioning, Configuration parameters, input data scaling"
    }, {
      "heading" : "1. Introduction",
      "text" : "Recently, businesses have started using MapReduce as a popular computation framework for processing large amount of data in both public and private clouds; e.g., many web-based service providers like Facebook is already utilizing MapReduce to analyse its core business as well as to extract information from their produced data. Therefore, understanding performance characteristics in MapReduce-style computations brings significant benefit to application developers in terms of improving application performance and resource utilization.\nOne of the regular user jobs –running experiments on MapReduce environment– is to frequently process and analysis almost relatively fixed-size data. For example, system administrators are always interested to frequently analysis system log files (such as Exim MainLog files[1]). As these log files are captured with fix sampling rate, their sizes do not usually change for specific period of times –e.g., for each month. Another example is Seismic imaging data where fix number of ultrasound senders/receivers produce earth underground information in a specific region; therefore, the size of output file –usually in the order of terabyte– is usually consistent [2]. The other example is to find a sequence matching between a new RNA and RNAs in a database [3], where the size of such databases (such as NCBI [4]) is almost unchanged over adjacent periods of time. These applications, which generally heavily consume resources, repeatedly show same execution pattern over their frequently deployments. As a result, any improvement in their resource utilisation can significantly improve the overall performance of such systems\nTwo typical performance questions in MapReduce environments are: (1) how to estimate the required resources for a job, and (2) how to automatically tweak/tune MapReduce configuration parameters to improve execution of a job; these two questions are important as they directly influence the performance of MapReduce jobs. Moreover, users are solely responsible to properly set these configuration parameters to achieve desirable performances. Although there are a few recent methodologies to estimate resource provisioning of MapReduce jobs (mostly on execution time prediction [5-8]), to best of our knowledge, there is no practice to study the dependency between performance of executing a job and the configuration parameters. The technique in this paper is our first attempt to study and model this dependency between two major configuration parameters –e.g., number of mappers, and number of reducers– and total CPU usage in clock cycles of jobs in MapReduce environment. Briefly, our contributions in this paper are:\n Study the influence of configuration parameters on the performance of executing a job (here, total CPU usage in clock cycles) in MapReduce environments.\n Model this dependency using polynomial regression to predict total CPU usage in clock cycles of the same job on the same input data size.\n Briefly study the influence of input data scaling on total CPU usage in clock cycles of jobs.\nThese enable a user to choose suitable values for configuration parameters, improve the performance of executing his job, and predict the total CPU usage in clock cycles of his job on different input sizes. It is worth noting that because our provisioning model is focused on the overall performance of an application, it cannot provide detailed information regarding its internal steps –e.g., identifying parts of an application that are more CPU usage in clock cycles compared with its other parts. Moreover, complexity degree of an application along with a proper model selection can significantly influence accuracy of our model; thus, results are expected to be less accurate for highly complex applications. It should be noted that all realistic jobs selected for provision validation are moderate/high CPU intensive jobs. This is because analysing of total CPU usage in clock cycles is the most important factor in CPU intensive jobs; while for I/O jobs, I/O utilization should be studied."
    }, {
      "heading" : "2. Related Work",
      "text" : "Early works on analysing/improving MapReduce performance started almost since 2005; such as an approach by Zaharia et al [7] that addressed problem of improving the performance of Hadoop for heterogeneous environments. Their approach was based on the critical assumption in Hadoop that only targets homogeneous cluster nodes; i.e., Hadoop assumes homogenous nodes to schedule its tasks and stragglers. A statistics-driven workload modelling was introduced in [9] to effectively evaluate design decisions in scaling, configuration and scheduling. Their framework was used to make practical suggestions to improve energy efficiency of MapReduce applications. Authors in [10] proposed a theoretical study on the MapReduce programming model which characterizes the features of mixed sequential and parallel processing in MapReduce. Performance prediction in MapReduce has been another important issue. In [11], the variation effect of Map and Reduce slots on the performance has been studied. Also, it was observed that different MapReduce applications may result in different CPU and I/O patterns. Then a fingerprint based method is utilized to predict the performance of a new MapReduce application based on the studied applications. The idea of pattern matching was used in [12] to find the similarity between CPU time patters of a new application and applications in database. Then it was concluded that if two applications show high similarity for several setting of configuration parameters it is very likely their optimal values of configuration parameters also be the same. Authors in [5] also used historical execution traces of applications on MapReduce environment for profiling and performance modelling and prediction. A modelling method was proposed in [7] to predict the total execution time of a MapReduce application; they used Kernel Canonical Correlation Analysis to obtain the correlation between the performance feature vectors extracted from MapReduce job logs, and map time, reduce time, and total execution time. These features were acknowledged as critical characteristics for establishing any scheduling decisions. Authors in [13, 14] reported a basic model for MapReduce computation utilizations. Here, at first, the map and reduce phases were modelled using dynamic linear programming independently; then, these phases were combined to build a global optimal strategy for MapReduce scheduling and resource allocation. Another study in [8] proposed a resource provisioning framework to predict how much resources a user job needs to be completed by a certain time. This work also studied the impact of failures on the\njob completion time. To the best of our knowledge, most of resource provisioning methodologies in MapReduce environment address predicting of execution time of jobs; there is no specific research on studying (1) CPU, memory, and I/O cost of such jobs, and (2) dependency between configuration parameters of MapReduce environment and performance of execution of jobs (e.g., execution time, CPU usage in clock cycles). In sections 3.1 and 4.2, the importance of these two issues will be further explained."
    }, {
      "heading" : "3. Application Modelling in MapReduce",
      "text" : "In commercial clouds (such as Amazon EC2), the problem of allocating appropriate number of machines for a proper time frame strongly depends on an application; user is responsible to set these values properly [15]. Thus, to estimate how much resource (in CPU cost, I/O cost, and Memory cost) a job requires in total enables user to make educated decisions to hire appropriate number of machines. In MapReduce environments, this problem becomes more important as number of machines cannot be changed after starting a job."
    }, {
      "heading" : "3.1. Profiling total CPU usage in clock cycles",
      "text" : "For each application, we generate a set of jobs –i.e., an experiment of application on MapReduce environment– with different values of two MapReduce configuration parameters –i.e., number of mappers and reducers– on a given platform. While running each job, the CPU usage in clock cycles of the job is gathered –as training data– to build a trace for future deployments; such data can be easily gathered through functions provided in XenAPI with almost no overhead. Within the system, we sampled the CPU usage in clock cycles of a job, for each machine, from the time the mappers start to the time all reducers finish with time interval of one second as\n*\uD835\uDC36\uD835\uDC610 , \uD835\uDC36\uD835\uDC611 , … , \uD835\uDC36\uD835\uDC61\uD835\uDC41+ (figure 2-left). Then, total CPU usage in clock cycles of the job is\ncalculated as (figure 2-right):\n\uD835\uDC5B\uD835\uDC50\uD835\uDC5D\uD835\uDC62 =∑(∑\uD835\uDC36\uD835\uDC61\uD835\uDC56,\uD835\uDC58\n\uD835\uDC41\n\uD835\uDC56=1\n) × \uD835\uDC53\uD835\uDC50\uD835\uDC59\uD835\uDC5C\uD835\uDC50\uD835\uDC58,\uD835\uDC58\n\uD835\uDC40\n\uD835\uDC58=1\nwhere \uD835\uDC40 , \uD835\uDC41, and \uD835\uDC53\uD835\uDC50\uD835\uDC59\uD835\uDC5C\uD835\uDC50\uD835\uDC58,\uD835\uDC58 are number of machines in cluster, number of CPU usage in clock cycles per seconds, and CPU clock frequency of k-th machine in cluster, respectively; for homogenous cluster , CPU clock frequency of all machines are the same. Total CPU usage in clock cycles is an independent metric from number of machines in cluster. This means total CPU usage in clock cycles of a job should not significantly change on two clusters with different number of the same machines and configuration. For a cluster with \uD835\uDC45 machines, and a job with \uD835\uDC47 execution time, the following statements should be almost correct:  A cluster with \uD835\uDC45\n2 machines, the same configuration, and with the same CPU clock\nfrequency should finish the job in 2\uD835\uDC47 time.  A cluster with \uD835\uDC45 machines, and the same configuration but half CPU clock\nfrequency should finish the job at 2\uD835\uDC47 time. Total CPU usage in clock cycles on a job on the same clusters, however, can change for different values of configuration parameters – the purpose of this study."
    }, {
      "heading" : "3.2. Total CPU usage in clock cycles model using polynomial regression",
      "text" : "The next step is to create a model for an application on MapReduce environment by characterizing the relationship between a set of MapReduce configuration parameters and CPU usage in clock cycles metric. The problem of such a modeling –based on linear regression– involves choosing of suitable coefficients for the model to better approximate a real system response time [16, 17].\nConsider the linear algebraic equations for different jobs of an application (\uD835\uDF11\uD835\uDC56) with different sets of two configuration parameters values as follows:\n{\n\uD835\uDC5B\uD835\uDC50\uD835\uDC5D\uD835\uDC62\uD835\uDF11\uD835\uDC56 (1) = \uD835\uDC4E0,\uD835\uDF11\uD835\uDC56 + \uD835\uDC4E1,\uD835\uDF11\uD835\uDC56\uD835\uDC40 (1) + \uD835\uDC4E2,\uD835\uDF11\uD835\uDC56(\uD835\uDC40 (1))2 + \uD835\uDC4E3,\uD835\uDF11\uD835\uDC56\uD835\uDC45 (1) + \uD835\uDC4E4,\uD835\uDF11\uD835\uDC56(\uD835\uDC45 (1))2 \uD835\uDC5B\uD835\uDC50\uD835\uDC5D\uD835\uDC62\uD835\uDF11\uD835\uDC56 (2) = \uD835\uDC4E0,\uD835\uDF11\uD835\uDC56 + \uD835\uDC4E1,\uD835\uDF11\uD835\uDC56\uD835\uDC40 (2) + \uD835\uDC4E2,\uD835\uDF11\uD835\uDC56(\uD835\uDC40 (2))2 + \uD835\uDC4E3,\uD835\uDF11\uD835\uDC56\uD835\uDC45 (2) + \uD835\uDC4E4,\uD835\uDF11\uD835\uDC56(\uD835\uDC45 (2))2\n⋮\n\uD835\uDC5B\uD835\uDC50\uD835\uDC5D\uD835\uDC62\uD835\uDF11\uD835\uDC56 (\uD835\uDC58) = \uD835\uDC4E0,\uD835\uDF11\uD835\uDC56 + \uD835\uDC4E1,\uD835\uDF11\uD835\uDC56\uD835\uDC40 (\uD835\uDC58) + \uD835\uDC4E2,\uD835\uDF11\uD835\uDC56(\uD835\uDC40 (\uD835\uDC58))2 + \uD835\uDC4E3,\uD835\uDF11\uD835\uDC56\uD835\uDC45 (\uD835\uDC58) + \uD835\uDC4E4,\uD835\uDF11\uD835\uDC56(\uD835\uDC45 (\uD835\uDC58))2\n(1)\nwhere \uD835\uDC5B\uD835\uDC50\uD835\uDC5D\uD835\uDC62\uD835\uDF11\uD835\uDC56 (\uD835\uDC56) is the actual value of total CPU usage in clock cycles of the application \uD835\uDF11\uD835\uDC56 in the j th job on MapReduce environment and S (j) = (M (j) , R (j) ) are the MapReduce configuration parameters; M (j) as the number of mappers, and R (j) as the number of reducers. Using the above definition, the approximation problem turns into estimating values of \uD835\uDC4E0,\uD835\uDF11\uD835̂\uDC56 , \uD835\uDC4E1,\uD835\uDF11\uD835̂\uDC56 , \uD835\uDC4E2,\uD835\uDF11\uD835̂\uDC56 , \uD835\uDC4E3,\uD835\uDF11\uD835̂\uDC56 , \uD835\uDC4E4,\uD835\uDF11\uD835̂\uDC56 to optimize a cost function between the approximation values and the actual values of total CPU usage in clock cycles. An approximated total CPU clock tick (\uD835\uDC5B\uD835\uDC50\uD835\uDC5D\uD835\uDC62\uD835\uDF11\uD835\uDC56 )̂ of the application for an unseen job with configuration parameters (\uD835\uDC40∗, \uD835\uDC45∗) is predicted as:\n\uD835\uDC5B\uD835\uDC50\uD835\uDC5D\uD835\uDC62\uD835\uDF11\uD835\uDC56  ̂ = \uD835\uDC4E0,\uD835\uDF11\uD835̂\uDC56 + \uD835\uDC4E1,\uD835\uDF11\uD835̂\uDC56\uD835\uDC40∗ + \uD835\uDC4E2,\uD835\uDF11\uD835̂\uDC56\uD835\uDC40∗ 2 + \uD835\uDC4E3,\uD835\uDF11\uD835̂\uDC56\uD835\uDC45∗ + \uD835\uDC4E4,\uD835\uDF11\uD835̂\uDC56\uD835\uDC45∗ 2 (2)\nThere are a variety of well-known mathematical methods in the literature to calculate the variables (\uD835\uDC4E0,\uD835\uDF11\uD835̂\uDC56 , \uD835\uDC4E1,\uD835\uDF11\uD835̂\uDC56 , \uD835\uDC4E2,\uD835\uDF11\uD835̂\uDC56 , \uD835\uDC4E3,\uD835\uDF11\uD835̂\uDC56 , \uD835\uDC4E4,\uD835\uDF11\uD835̂\uDC56 ). One widely used in many application domains is the Least Square Regression which calculates the parameters in Eqn.2 by minimizing the following error:\n\uD835\uDC52\uD835\uDC5F\uD835\uDC5F\uD835\uDC5C\uD835\uDC5F = √∑(\uD835\uDC5B\uD835\uDC50\uD835\uDC5D\uD835\uDC62\uD835\uDF11\uD835\uDC56 (\uD835\uDC57)̂ −\uD835\uDC5B\uD835\uDC50\uD835\uDC5D\uD835\uDC62\uD835\uDF11\uD835\uDC56 (\uD835\uDC57) )2\n\uD835\uDC3E\n\uD835\uDC57=1\nLeast Square Regression theory claims that if:\n\uD835\uDC3B\uD835\uDC5A\uD835\uDC5C\uD835\uDC51\uD835\uDC52\uD835\uDC59 = [ 1 \uD835\uDC40(1) (\uD835\uDC40(1))2 \uD835\uDC45(1) (\uD835\uDC45(1))2 1 \uD835\uDC40(2) (\uD835\uDC40(2))2 \uD835\uDC45(2) (\uD835\uDC45(2))2 ⋮ 1 \uD835\uDC40(\uD835\uDC58) (\uD835\uDC40(\uD835\uDC58))2 \uD835\uDC45(\uD835\uDC58) (\uD835\uDC45(\uD835\uDC58))2] , \uD835\uDC3B\uD835\uDC4E\uD835\uDC50\uD835\uDC61\uD835\uDC62\uD835\uDC4E\uD835\uDC59,\uD835\uDF11\uD835\uDC56 = [ \uD835\uDC5B\uD835\uDC50\uD835\uDC5D\uD835\uDC62\uD835\uDF11\uD835\uDC56\n(1)\n\uD835\uDC5B\uD835\uDC50\uD835\uDC5D\uD835\uDC62\uD835\uDF11\uD835\uDC56 (2)\n⋮\n\uD835\uDC5B\uD835\uDC50\uD835\uDC5D\uD835\uDC62\uD835\uDF11\uD835\uDC56 (\uD835\uDC58) ]\n,\n\uD835\uDC34 = [ \uD835\uDC4E0,\uD835\uDF11\uD835̂\uDC56 \uD835\uDC4E1,\uD835\uDF11\uD835̂\uDC56 ⋮\n\uD835\uDC4E4,\uD835\uDF11\uD835̂\uDC56]\n(3)\nthen the model satisfying the above error will be calculated as [17]:\n\uD835\uDC34 = (\uD835\uDC3B\uD835\uDC5A\uD835\uDC5C\uD835\uDC51\uD835\uDC52\uD835\uDC59 \uD835\uDC47 \uD835\uDC3B\uD835\uDC5A\uD835\uDC5C\uD835\uDC51\uD835\uDC52\uD835\uDC59 )−1\uD835\uDC3B\uD835\uDC5A\uD835\uDC5C\uD835\uDC51\uD835\uDC52\uD835\uDC59 \uD835\uDC47 \uD835\uDC3B\uD835\uDC4E\uD835\uDC50\uD835\uDC61\uD835\uDC62\uD835\uDC4E\uD835\uDC59,\uD835\uDF11\uD835\uDC56\n(4)\nwhere (. ) \uD835\uDC47 denotes a transpose matrix. The set of configuration parameters values \uD835\uDC4E0,\uD835\uDF11\uD835̂\uDC56 , \uD835\uDC4E1,\uD835\uDF11\uD835̂\uDC56 , \uD835\uDC4E2,\uD835\uDF11\uD835̂\uDC56 , \uD835\uDC4E3,\uD835\uDF11\uD835̂\uDC56 , \uD835\uDC4E4,\uD835\uDF11\uD835̂\uDC56 is the model that approximately describes the relationship between total CPU usage in clock cycles of an application to two MapReduce configuration parameters."
    }, {
      "heading" : "4. Experimental Validation",
      "text" : "In this section, we evaluate the effectiveness of our models using three realistic applications."
    }, {
      "heading" : "4.1. Experimental setting",
      "text" : "Three realistic applications are used to evaluate the effectiveness of our method. Our method has been implemented and evaluated on a private Cloud with the following specifications:\n Physical H/W: includes five servers, each one is an Intel Genuine with 3.00GHz clock, 1GB memory, 1GB cache and with 50GB of shared SAN hard disk.\n For virtualization, Xen cloud platform (XCP) has been used on top of the physical H/W. The XenAPI [18] provides functionality to directly manage virtual\nmachines inside XCP. It provides binding in high level languages like Java, C# and Python. Using these bindings, it was possible to measure the performance of all virtual machines in a datacentre and live-migrate them.\n Virtual nodes are implemented on top of the XCP. The number of virtual nodes is chosen 20 with Linux image (Debian). The virtual nodes run Hadoop version\n0.20.2 –i.e., Apache implementation of MapReduce developed in Java [19]. The XenAPI package is executed in background to monitor/extract the CPU utilization time series of applications (in the native system) [20]. For an experiment with a specific set of MapReduce configuration parameters values, statistics are gathered from ―running job‖ stage to the ―job completion‖ stage (arrows in Figure 2-left) with sampling time interval of one second. All CPU usages samples are then combined to form CPU utilization time series of an experiment.\nIn the training phase of our modelling, 64 \uD835\uDC60ets of jobs for each application are conduced where the number of mappers and reducers are integers with a value in ,4,8,12,16,20,24,28,32-; the size of input data is fixed to 12\uD835\uDC3A. To overcome temporal changes, each job is repeated ten times. Then in the prediction phase, the accuracy of the application model is evaluated with 30 new/unseen jobs on the same input data size where the number of mappers and reducers are randomly selected from the integers ,4…32-. Our benchmark applications are WordCount (used by leading researchers in Intel [21], IBM [6], MIT [22], and UC-Berkeley [7]), TeraSort (as a standard benchmark in the international TeraByte sort competition [23, 24] as well as many researchers in IBM [25, 26], Intel [21], INRIA [27] and UC-Berkeley [28]), and Exim Mainlog parsing [12, 29]. These benchmarks are used due to their striking differences as well as their popularity among MapReduce applications."
    }, {
      "heading" : "4.2 Evaluation Criteria",
      "text" : "We evaluate the accuracy of the fitted models, generated from regression based on a number of metrics [30]: Mean Absolute Percentage Error (MAPE), PRED(25) , Root Mean Squared Error (RMSE) and R2 Prediction Accuracy ."
    }, {
      "heading" : "4.2.1. Mean Absolute Percentage Error (MAPE)",
      "text" : "The Mean Absolute Percentage Error[30] for a prediction model is described as:\n\uD835\uDC40\uD835\uDC34 =\n∑ |\uD835\uDC5B\uD835\uDC50\uD835\uDC5D\uD835\uDC62\uD835\uDF11\n(\uD835\uDC56) − \uD835\uDC5B\uD835\uDC50\uD835\uDC5D\uD835\uDC62\uD835\uDF11 ̂ (\uD835\uDC56)|\n\uD835\uDC5B\uD835\uDC50\uD835\uDC5D\uD835\uDC62\uD835\uDF11\uD835\uDC56 (\uD835\uDC56)\n\uD835\uDC41 \uD835\uDC56=1\n\uD835\uDC41\nwhere \uD835\uDC5B\uD835\uDC50\uD835\uDC5D\uD835\uDC62\uD835\uDF11 (\uD835\uDC56) is the actual total CPU usage in clock cycles of application \uD835\uDF11\uD835\uDC58 , \uD835\uDC5B\uD835\uDC50\uD835\uDC5D\uD835\uDC62\uD835\uDF11 ( )̂ is the predicted total CPU usage in clock cycles and \uD835\uDC41 is the number of observations in the dataset. The smaller MAPE value indicates the better fit of the prediction model."
    }, {
      "heading" : "4.2.2. PRED(25)",
      "text" : "The measure PRED(25)[30] is given as:\n\uD835\uDC45 (25) = \uD835\uDC5C\uD835\uDC53 \uD835\uDC5C \uD835\uDC60\uD835\uDC52\uD835\uDC5F\uD835\uDC63\uD835\uDC4E\uD835\uDC61 \uD835\uDC5C\uD835\uDC5B\uD835\uDC60 \uD835\uDC64 \uD835\uDC61 \uD835\uDC5F\uD835\uDC52 \uD835\uDC4E\uD835\uDC61 \uD835\uDC63\uD835\uDC52 \uD835\uDC52\uD835\uDC5F\uD835\uDC5F\uD835\uDC5C\uD835\uDC5F \uD835\uDC52\uD835\uDC60\uD835\uDC60 \uD835\uDC61 \uD835\uDC4E\uD835\uDC5B 25\n\uD835\uDC5C\uD835\uDC53 \uD835\uDC61\uD835\uDC5C\uD835\uDC61\uD835\uDC4E \uD835\uDC5C \uD835\uDC60\uD835\uDC52\uD835\uDC5F\uD835\uDC63\uD835\uDC4E\uD835\uDC61 \uD835\uDC5C\uD835\uDC5B\uD835\uDC60\n2000\n2050\n2100\n2150\n2200\n2250\n2300\n4 8 12 16 20 24 28 32\nWordCount\nR=4 R=12 R=20 R=28\n5000\n5500\n6000\n6500\n7000\n4 8 12 16 20 24 28 32\nExim MainLog\nR=4 R=12 R=20 R=28\nTeraSort\nIt involves the percentage of observations whose prediction accuracy falls within 25% of the actual value. Closer value of PRED(25) to 1.0 implies a better fit of the prediction model."
    }, {
      "heading" : "4.2.3. Root Mean Squared Error (RMSE)",
      "text" : "The metric Root Mean Square Error (RMSE)[30] is given by:\n0 \uD835\uDC45\uD835\uDC40 = √ ∑ (\uD835\uDC5B\uD835\uDC50\uD835\uDC5D\uD835\uDC62\uD835\uDF11\n(\uD835\uDC56) − \uD835\uDC5B\uD835\uDC50\uD835\uDC5D\uD835\uDC62\uD835\uDF11 ̂ (\uD835\uDC56))2\uD835\uDC41\uD835\uDC56=1\n\uD835\uDC41 1\nMore effective prediction results from smaller RMSE value."
    }, {
      "heading" : "4.2.4. \uD835\uDC79\uD835\uDFD0 Prediction Accuracy",
      "text" : "The \uD835\uDC452 Prediction Accuracy[30] – commonly applied to Linear Regression models\nas a measure of the goodness-of-fit of the prediction model– is calculated as:\n0 \uD835\uDC452 = 1 − ∑ (\uD835\uDC5B\uD835\uDC50\uD835\uDC5D\uD835\uDC62\uD835\uDF11\n(\uD835\uDC56) − \uD835\uDC5B\uD835\uDC50\uD835\uDC5D\uD835\uDC62\uD835\uDF11 ̂ (\uD835\uDC56))2\uD835\uDC41\uD835\uDC56=1\n∑ (\uD835\uDC5B\uD835\uDC50\uD835\uDC5D\uD835\uDC62\uD835\uDF11 ̂ (\uD835\uDC56) − ∑\n\uD835\uDC5B\uD835\uDC50\uD835\uDC5D\uD835\uDC62\uD835\uDF11 ( )\n\uD835\uDC41 \uD835\uDC41 =1 ) \uD835\uDC41 \uD835\uDC56=1\n1\nFor a perfect prediction, \uD835\uDC452 = 1.0."
    }, {
      "heading" : "4.3. Results",
      "text" : "Total CPU usage in clock cycles and configuration parameters: As mentioned earlier, there is a strong dependency between total CPU usage in clock cycles and the number of mappers and reducers in MapReduce environments. Figure 3 shows the dependency between these two configuration parameters and the total CPU usage in clock cycles for different jobs. One observation from this figure is that these applications behave differently when their number of mappers and reducers are increased. For example, Exim MainLog parsing shows a smooth linear relation, whereas such relation for WordCount and TeraSort is much more complicated. Another observation is that variations of the studied configuration parameters slightly change the difference between the highest and lowest total CPU usage in clock cycles for WordCount (9.5%) and Exim MianLog parsing (15.5%), while this difference for TeraSort is significant (50%).\nPrediction accuracy: To test the accuracy of the model, we use our proposed model to predict total CPU usage in clock cycles of several new/unseen jobs of an application with randomly set values of the two configuration parameters. We then ran the jobs on the real system and collect their total CPU in clock cycles to determine the prediction error. Figures 4 and 5 show the prediction accuracies and MAPE prediction errors; Table 1 reflects RMSD, MAPE, R2 prediction accuracy, and PRED for these\napplications\nFigure 4. The actual total CPU usage verses the model prediction for studied jobs. The X-axis is job ID and Y-axis is total CPU usage (in tera clock cycle)\n1900\n2000\n2100\n2200\n2300\n1 3 5 7 9 11 13 15\nWordCount\nActual Predicted\n5000\n5500\n6000\n6500\n7000\n1 3 5 7 9 11 13 15\nExim MainLog\nActual Predicted\n400\n450\n500\n550\n600\n650\n700\n1 3 5 7 9 11 13 15\nTeraSort\nActual Predicted\n. We found that most prediction evolution criteria are well satisfied for both WordCount and Exim MainLog parsing; it showed accuracy of our model. Although the MAPR value is still reasonable for TeraSort (~7%), it shows low values for other criteria. An educated guess to explain this phenomenon could be related to the significant difference between the highest and lowest total CPU usage in clock cycles of TeraSort (in figure 3), indicating that our modelling technique based on two-degree polynomial regression fails to correctly model the total CPU usage in clock cycles for TeraSort; therefore a better model must be used for this application.\nInput data scaling: figure 6 shows how total CPU usage in clock cycles of our three applications scales with increasing of input data size. As can be seen, there is a linear relation between these two metrics. Thus, total CPU usage in clock cycles modelling –calculated through the idea of this paper – for a fixed-size input data can be used for other data sizes as well."
    }, {
      "heading" : "4.3. Discussion and future work",
      "text" : "Although the obtained model can successfully predict the level of total CPU usage in clock cycles required for a few MapReduce applications, it shows some drawbacks. First, the total CPU usage in clock cycles of a job is modelled by averaging total CPU usage in clock cycles of the whole job from several traces. Many applications show quite different behaviour between their Map and Reduce phases: in some cases the Map is compute intensive, in others the Reduce or even both. Taking this into account, we would like to extend our model to a finer granular one. To this end, we like to split this model to cover CPU usage in clock cycles of both phases separately; i.e., instead of using a uniform average, we rather prefer to rely on a weighted average that emphasizes the CPU usage in clock cycles in each stage of the MapReduce computation. Second, the two successful applications –WordCount and Exim MainLog parsing– in our evaluation have almost linear complexity; and thus, their polynomial regression produced acceptable results. Our experiments also show that if such regression model is applied to programs with higher complexity (like TeraSort), their results are mostly unacceptable. To this end, we also like to consider other models –mostly non-linear regression– for more complex applications and provide a suit of regression techniques to cover almost all classes of applications."
    }, {
      "heading" : "6. Conclusion",
      "text" : "In this work we proposed an accurate modelling technique to predict total CPU usage in clock cycles of jobs in MapReduce environment before their actual deployment on clusters and/or clouds. Such prediction can greatly help both application performance and effective resource utilization. To achieve this, we have presented an approach to model/profile total CPU usage in clock cycles of applications and applied polynomial regression model to identify correlation between two major MapReduce configuration parameters (number of mappers, and number of reducers) and the total CPU usage in clock cycles of an application. Our modelling technique can be used by both users/consumers (e.g., application developers) and service providers in the cloud for effective resource utilization. Evaluation results show that prediction error of total computation clock cycle of specific applications could be as less as 8%."
    }, {
      "heading" : "Acknowledgment",
      "text" : "Mr. N. Babaii Rizvandi’s work is supported by National ICT Australia (NICTA). Professor A.Y. Zomaya's work is supported by an Australian Research Council Grant LP0884070."
    } ],
    "references" : [ {
      "title" : "MapReduce Implementation of Prestack Kirchhoff Time Migration (PKTM) on Seismic Data",
      "author" : [ "N.B. Rizvandi", "A.J. Boloori", "N. Kamyabpour", "A. Zomaya" ],
      "venue" : "presented at the The 12th International Conference on Parallel and Distributed Computing, Applications and Technologies (PDCAT), Gwangju, Korea 2011.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Cloud-enabling Sequence Alignment with Hadoop MapReduce: A Performance Analysis",
      "author" : [ "K. Arumugam", "Y.S. Tan", "B.S. Lee", "R. Kanagasabai" ],
      "venue" : "presented at the 2012 4th International Conference on Bioinformatics and Biomedical Technology, 2012.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "An Analysis of Traces from a Production MapReduce Cluster",
      "author" : [ "S. Kavulya", "J. Tan", "R. Gandhi", "P. Narasimhan" ],
      "venue" : "presented at the Proceedings of the 2010 10th IEEE/ACM International Conference on Cluster, Cloud and Grid Computing, 2010. Figure 5. The error between actual total CPU usage in clock cycle and the model prediction. The X-axis is job ID while Y-axis is percentage of error  -20  -10  0  10  20 1 3 5 7 9 11 13 15  P  rc  e  n  ta  ge Error WordCount Exim MainLog TeraSort  14",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Towards Optimizing Hadoop Provisioning in the Cloud",
      "author" : [ "K. Kambatla", "A. Pathak", "H. Pucha" ],
      "venue" : "presented at the the 2009 conference on Hot topics in cloud computing, San Diego, California, 2009.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Improving MapReduce Performance in Heterogeneous Environments",
      "author" : [ "M. Zaharia", "A. Konwinski", "A.D.Joseph", "R. Katz", "I. Stoica" ],
      "venue" : "8th USENIX Symposium on Operating Systems Design and Implementation (OSDI 2008), pp. 29-42, 18 December 2008.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Resource Provisioning Framework for MapReduce Jobs with Performance Goals",
      "author" : [ "A. Verma", "L. Cherkasova", "R.H. Campbell" ],
      "venue" : "presented at the ACM/IFIP/USENIX 12th International Middleware Conference (Middleware), Lisbon, Portugal, 2011.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Statistical Workloads for Energy Efficient MapReduce",
      "author" : [ "Y. Chen", "A.S. Ganapathi", "A. Fox", "R.H. Katz", "D.A. Patterson" ],
      "venue" : "University of California at Berkeley,Technical Report No. UCB/EECS-2010-6, 2010.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A model of computation for MapReduce",
      "author" : [ "H. Karloff", "S. Suri", "S. Vassilvitskii" ],
      "venue" : "presented at the Proceedings of the Twenty-First Annual ACM-SIAM Symposium on Discrete Algorithms, Austin, Texas, 2010.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Towards optimizing hadoop provisioning in the cloud",
      "author" : [ "K. Kambatla", "A. Pathak", "H. Pucha" ],
      "venue" : "presented at the Proceedings of the 2009 conference on Hot topics in cloud computing, San Diego, California, 2009.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A Study on Using Uncertain Time Series Matching Algorithms in Map-Reduce Applications",
      "author" : [ "N.B. Rizvandi", "J. Taheri", "A.Y. Zomaya", "R. Moraveji" ],
      "venue" : "Concurrency and Computation: Practice and Experience, 2012.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Brief Announcement: Modelling MapReduce for Optimal Execution in the Cloud",
      "author" : [ "A. Wieder", "P. Bhatotia", "A. Post", "R. Rodrigues" ],
      "venue" : "presented at the Proceeding of the 29th ACM SIGACT-SIGOPS symposium on Principles of distributed computing, Zurich, Switzerland, 2010.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Conductor: orchestrating the clouds",
      "author" : [ "A. Wieder", "P. Bhatotia", "A. Post", "R. Rodrigues" ],
      "venue" : "presented at the 4th International Workshop on Large Scale Distributed Systems and Middleware, Zurich, Switzerland, 2010.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Bag-of-Tasks Scheduling under Budget Constraints \" presented at the IEEE",
      "author" : [ "A.-m. Oprescu", "T. Kielmann" ],
      "venue" : "Second International Conference on Cloud Computing Technology and Science (CloudCom),",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2010
    }, {
      "title" : "Profiling and Modeling Resource Usage of Virtualized Applications",
      "author" : [ "T. Wood", "L. Cherkasova", "K. Ozonat", "a. P. Shenoy" ],
      "venue" : "presented at the Proceedings of the ACM/IFIP/USENIX 9th International Middleware Conference, Leuven, Belgium, 2006.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "An Accurate Fir Approximation of Ideal Fractional Delay Filter with Complex Coefficients in Hilbert Space",
      "author" : [ "N.B. Rizvandi", "A. Nabavi", "S. Hessabi" ],
      "venue" : "Journal of Circuits, Systems, and Computers, vol. 14, pp. 497-506, 2005.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "The definitive guide to the xen hypervisor., first ed.",
      "author" : [ "D. Chisnall" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2007
    }, {
      "title" : "Optimizing MapReduce for Multicore Architectures",
      "author" : [ "a. Mao", "R. Morris", "M.F. Kaashoek" ],
      "venue" : "Massachusetts Institute of Technology2010.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Towards automatic optimization of MapReduce programs",
      "author" : [ "S. Babu" ],
      "venue" : "presented at the 1st ACM symposium on Cloud computing, Indianapolis, Indiana, USA, 2010.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A Simulation Approach to Evaluating Design Decisions in MapReduce Setups \" presented at the MASCOTS",
      "author" : [ "G. Wang", "A.R. Butt", "P. P", "K. Gupta" ],
      "venue" : null,
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2009
    }, {
      "title" : "Using realistic simulation for performance analysis of mapreduce setups",
      "author" : [ "G. Wang", "A.R. Butt", "P. Pandey", "K. Gupta" ],
      "venue" : "presented at the Proceedings of the 1st ACM workshop on Large-Scale system and application performance, Garching, Germany, 2009.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Optimizing intermediate data management in MapReduce computations",
      "author" : [ "D. Moise", "T.-T.-L. Trieu", "L. Boug", "#233", "G. Antoniu" ],
      "venue" : "presented at the Proceedings of the First International Workshop on Cloud Computing Platforms, Salzburg, Austria, 2011.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "MapReduce: Distributed Computing for Machine  Learning",
      "author" : [ "D. Gillick", "A. Faria", "J. DeNero" ],
      "venue" : "www.icsi.berkeley.edu/~arlo/publications/gillick_cs262a_proj.pdf2008.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "On using Pattern Matching Algorithms in MapReduce Applications",
      "author" : [ "N.B. Rizvandi", "J. Taheri", "A.Y. Zomaya" ],
      "venue" : "presented at the The 9th IEEE International Symposium on Parallel and Distributed Processing with Applications (ISPA), Busan, South Korea, 2011.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Empirical prediction models for adaptive resource provisioning in the cloud",
      "author" : [ "S. Islam", "J. Keung", "K. Lee", "A. Liu" ],
      "venue" : "Future Generation Comp. Syst., vol. 28, pp. 155-162, 2012. Figure 6. total CPU usage in clock cycles and scalability in input data size  0  50  100  150  200 250 0  2  4  6 8 To  ta  l  C  P  U  u  sa  ge  (  in  t  e  ra  c  lo  ck  c  yc  le  ) input data (in GB) Scalability in input data size WordCount Exim Terasort  16",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Another example is Seismic imaging data where fix number of ultrasound senders/receivers produce earth underground information in a specific region; therefore, the size of output file –usually in the order of terabyte– is usually consistent [2].",
      "startOffset" : 241,
      "endOffset" : 244
    }, {
      "referenceID" : 1,
      "context" : "The other example is to find a sequence matching between a new RNA and RNAs in a database [3], where the size of such databases (such as NCBI [4]) is almost unchanged over adjacent periods of time.",
      "startOffset" : 90,
      "endOffset" : 93
    }, {
      "referenceID" : 2,
      "context" : "Although there are a few recent methodologies to estimate resource provisioning of MapReduce jobs (mostly on execution time prediction [5-8]), to best of our knowledge, there is no practice to study the dependency between performance of executing a job and the configuration parameters.",
      "startOffset" : 135,
      "endOffset" : 140
    }, {
      "referenceID" : 3,
      "context" : "Although there are a few recent methodologies to estimate resource provisioning of MapReduce jobs (mostly on execution time prediction [5-8]), to best of our knowledge, there is no practice to study the dependency between performance of executing a job and the configuration parameters.",
      "startOffset" : 135,
      "endOffset" : 140
    }, {
      "referenceID" : 4,
      "context" : "Although there are a few recent methodologies to estimate resource provisioning of MapReduce jobs (mostly on execution time prediction [5-8]), to best of our knowledge, there is no practice to study the dependency between performance of executing a job and the configuration parameters.",
      "startOffset" : 135,
      "endOffset" : 140
    }, {
      "referenceID" : 5,
      "context" : "Although there are a few recent methodologies to estimate resource provisioning of MapReduce jobs (mostly on execution time prediction [5-8]), to best of our knowledge, there is no practice to study the dependency between performance of executing a job and the configuration parameters.",
      "startOffset" : 135,
      "endOffset" : 140
    }, {
      "referenceID" : 4,
      "context" : "Early works on analysing/improving MapReduce performance started almost since 2005; such as an approach by Zaharia et al [7] that addressed problem of improving the performance of Hadoop for heterogeneous environments.",
      "startOffset" : 121,
      "endOffset" : 124
    }, {
      "referenceID" : 6,
      "context" : "A statistics-driven workload modelling was introduced in [9] to effectively evaluate design decisions in scaling, configuration and scheduling.",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 7,
      "context" : "Authors in [10] proposed a theoretical study on the MapReduce programming model which characterizes the features of mixed sequential and parallel processing in MapReduce.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 8,
      "context" : "In [11], the variation effect of Map and Reduce slots on the performance has been studied.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 9,
      "context" : "The idea of pattern matching was used in [12] to find the similarity between CPU time patters of a new application and applications in database.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 2,
      "context" : "Authors in [5] also used historical execution traces of applications on MapReduce environment for profiling and performance modelling and prediction.",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 4,
      "context" : "A modelling method was proposed in [7] to predict the total execution time of a MapReduce application; they used Kernel Canonical Correlation Analysis to obtain the correlation between the performance feature vectors extracted from MapReduce job logs, and map time, reduce time, and total execution time.",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 10,
      "context" : "Authors in [13, 14] reported a basic model for MapReduce computation utilizations.",
      "startOffset" : 11,
      "endOffset" : 19
    }, {
      "referenceID" : 11,
      "context" : "Authors in [13, 14] reported a basic model for MapReduce computation utilizations.",
      "startOffset" : 11,
      "endOffset" : 19
    }, {
      "referenceID" : 5,
      "context" : "Another study in [8] proposed a resource provisioning framework to predict how much resources a user job needs to be completed by a certain time.",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 12,
      "context" : "In commercial clouds (such as Amazon EC2), the problem of allocating appropriate number of machines for a proper time frame strongly depends on an application; user is responsible to set these values properly [15].",
      "startOffset" : 209,
      "endOffset" : 213
    }, {
      "referenceID" : 13,
      "context" : "The problem of such a modeling –based on linear regression– involves choosing of suitable coefficients for the model to better approximate a real system response time [16, 17].",
      "startOffset" : 167,
      "endOffset" : 175
    }, {
      "referenceID" : 14,
      "context" : "The problem of such a modeling –based on linear regression– involves choosing of suitable coefficients for the model to better approximate a real system response time [16, 17].",
      "startOffset" : 167,
      "endOffset" : 175
    }, {
      "referenceID" : 14,
      "context" : "then the model satisfying the above error will be calculated as [17]:",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 15,
      "context" : "The XenAPI [18] provides functionality to directly manage virtual machines inside XCP.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 3,
      "context" : "Our benchmark applications are WordCount (used by leading researchers in Intel [21], IBM [6], MIT [22], and UC-Berkeley [7]), TeraSort (as a standard benchmark in the international TeraByte sort competition [23, 24] as well as many researchers in IBM [25, 26], Intel [21], INRIA [27] and UC-Berkeley [28]), and Exim Mainlog parsing [12, 29].",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 16,
      "context" : "Our benchmark applications are WordCount (used by leading researchers in Intel [21], IBM [6], MIT [22], and UC-Berkeley [7]), TeraSort (as a standard benchmark in the international TeraByte sort competition [23, 24] as well as many researchers in IBM [25, 26], Intel [21], INRIA [27] and UC-Berkeley [28]), and Exim Mainlog parsing [12, 29].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 4,
      "context" : "Our benchmark applications are WordCount (used by leading researchers in Intel [21], IBM [6], MIT [22], and UC-Berkeley [7]), TeraSort (as a standard benchmark in the international TeraByte sort competition [23, 24] as well as many researchers in IBM [25, 26], Intel [21], INRIA [27] and UC-Berkeley [28]), and Exim Mainlog parsing [12, 29].",
      "startOffset" : 120,
      "endOffset" : 123
    }, {
      "referenceID" : 17,
      "context" : "Our benchmark applications are WordCount (used by leading researchers in Intel [21], IBM [6], MIT [22], and UC-Berkeley [7]), TeraSort (as a standard benchmark in the international TeraByte sort competition [23, 24] as well as many researchers in IBM [25, 26], Intel [21], INRIA [27] and UC-Berkeley [28]), and Exim Mainlog parsing [12, 29].",
      "startOffset" : 207,
      "endOffset" : 215
    }, {
      "referenceID" : 18,
      "context" : "Our benchmark applications are WordCount (used by leading researchers in Intel [21], IBM [6], MIT [22], and UC-Berkeley [7]), TeraSort (as a standard benchmark in the international TeraByte sort competition [23, 24] as well as many researchers in IBM [25, 26], Intel [21], INRIA [27] and UC-Berkeley [28]), and Exim Mainlog parsing [12, 29].",
      "startOffset" : 251,
      "endOffset" : 259
    }, {
      "referenceID" : 19,
      "context" : "Our benchmark applications are WordCount (used by leading researchers in Intel [21], IBM [6], MIT [22], and UC-Berkeley [7]), TeraSort (as a standard benchmark in the international TeraByte sort competition [23, 24] as well as many researchers in IBM [25, 26], Intel [21], INRIA [27] and UC-Berkeley [28]), and Exim Mainlog parsing [12, 29].",
      "startOffset" : 251,
      "endOffset" : 259
    }, {
      "referenceID" : 20,
      "context" : "Our benchmark applications are WordCount (used by leading researchers in Intel [21], IBM [6], MIT [22], and UC-Berkeley [7]), TeraSort (as a standard benchmark in the international TeraByte sort competition [23, 24] as well as many researchers in IBM [25, 26], Intel [21], INRIA [27] and UC-Berkeley [28]), and Exim Mainlog parsing [12, 29].",
      "startOffset" : 279,
      "endOffset" : 283
    }, {
      "referenceID" : 21,
      "context" : "Our benchmark applications are WordCount (used by leading researchers in Intel [21], IBM [6], MIT [22], and UC-Berkeley [7]), TeraSort (as a standard benchmark in the international TeraByte sort competition [23, 24] as well as many researchers in IBM [25, 26], Intel [21], INRIA [27] and UC-Berkeley [28]), and Exim Mainlog parsing [12, 29].",
      "startOffset" : 300,
      "endOffset" : 304
    }, {
      "referenceID" : 9,
      "context" : "Our benchmark applications are WordCount (used by leading researchers in Intel [21], IBM [6], MIT [22], and UC-Berkeley [7]), TeraSort (as a standard benchmark in the international TeraByte sort competition [23, 24] as well as many researchers in IBM [25, 26], Intel [21], INRIA [27] and UC-Berkeley [28]), and Exim Mainlog parsing [12, 29].",
      "startOffset" : 332,
      "endOffset" : 340
    }, {
      "referenceID" : 22,
      "context" : "Our benchmark applications are WordCount (used by leading researchers in Intel [21], IBM [6], MIT [22], and UC-Berkeley [7]), TeraSort (as a standard benchmark in the international TeraByte sort competition [23, 24] as well as many researchers in IBM [25, 26], Intel [21], INRIA [27] and UC-Berkeley [28]), and Exim Mainlog parsing [12, 29].",
      "startOffset" : 332,
      "endOffset" : 340
    }, {
      "referenceID" : 23,
      "context" : "We evaluate the accuracy of the fitted models, generated from regression based on a number of metrics [30]: Mean Absolute Percentage Error (MAPE), PRED(25) , Root Mean Squared Error (RMSE) and R2 Prediction Accuracy .",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 23,
      "context" : "Mean Absolute Percentage Error (MAPE) The Mean Absolute Percentage Error[30] for a prediction model is described as:",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 23,
      "context" : "PRED(25) The measure PRED(25)[30] is given as:",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 23,
      "context" : "Root Mean Squared Error (RMSE) The metric Root Mean Square Error (RMSE)[30] is given by:",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 23,
      "context" : "R Prediction Accuracy The R Prediction Accuracy[30] – commonly applied to Linear Regression models as a measure of the goodness-of-fit of the prediction model– is calculated as:",
      "startOffset" : 47,
      "endOffset" : 51
    } ],
    "year" : 2013,
    "abstractText" : "recently, businesses have started using MapReduce as a popular computation framework for processing large amount of data, such as spam detection, and different data mining tasks, in both public and private clouds. Two of the challenging questions in such environments are (1) choosing suitable values for MapReduce configuration parameters –e.g., number of mappers, number of reducers, and DFS block size–, and (2) predicting the amount of resources that a user should lease from the service provider. Currently, the tasks of both choosing configuration parameters and estimating required resources are solely the users’ responsibilities. In this paper, we present an approach to provision the total CPU usage in clock cycles of jobs in MapReduce environment. For a MapReduce job, a profile of total CPU usage in clock cycles is built from the job past executions with different values of two configuration parameters e.g., number of mappers, and number of reducers. Then, a polynomial regression is used to model the relation between these configuration parameters and total CPU usage in clock cycles of the job. We also briefly study the influence of input data scaling on measured total CPU usage in clock cycles. This derived model along with the scaling result can then be used to provision the total CPU usage in clock cycles of the same jobs with different input data size. We validate the accuracy of our models using three realistic applications (WordCount, Exim MainLog parsing, and TeraSort). Results show that the predicted total CPU usage in clock cycles of generated resource provisioning options are less than 8% of the measured total CPU usage in clock cycles in our 20-node virtual Hadoop cluster. Keywordtotal CPU usage in clock cycles, MapReduce, Hadoop, Resource provisioning, Configuration parameters, input data scaling",
    "creator" : "Microsoft® Word 2010"
  }
}