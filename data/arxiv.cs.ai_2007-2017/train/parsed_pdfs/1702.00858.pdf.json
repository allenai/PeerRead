{
  "name" : "1702.00858.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "The Value of Inferring the Internal State of Traffic Participants for Autonomous Freeway Driving",
    "authors" : [ "Zachary N. Sunberg", "Christopher J. Ho", "Mykel J. Kochenderfer" ],
    "emails" : [ "zsunberg@stanford.edu.", "cho3@stanford.edu.", "mykel@stanford.edu." ],
    "sections" : [ {
      "heading" : null,
      "text" : "I. INTRODUCTION\nOne of the challenges in introducing autonomous automobiles is ensuring that they interact safely with human drivers. In order to navigate complex driving scenarios, human drivers routinely predict what other drivers will do and make driving decisions based on these predictions. Autonomous vehicles typically take an overly conservative approach, which can result in physical danger, reduced efficiency, and an uncomfortable experience. In a recent study, autonomous vehicles drove over 1.2 million miles without being legally responsible for any accidents. However, the autonomous vehicles actually had a higher accident rate than average for a conventional vehicle in the United States because of accidents for which they were not legally responsible [1]. This result suggests that there is significant room for improvement in autonomous-human vehicle interaction.\nOne approach to improve interaction would be to program ad-hoc logic for each situation into the vehicles. However, this approach is time-consuming and error prone, and edge cases that the programmers have not foreseen can present a safety risk. Furthermore, this approach limits the performance of the system to the capability of the human programmer. In contrast, artificial intelligence and machine\nThe authors are with the Department of Aeronautics and Astronautics, Stanford University. Toyota Research Institute (”TRI”) provided funds to assist the authors with their research but this article solely reflects the opinions and conclusions of its authors and not TRI or any other Toyota entity. Email: {zsunberg,cho3,mykel}@stanford.edu.\nlearning techniques have the potential to provide a more robust approach to such decision-making tasks. This paper explores techniques based on Markov decision processes (MDPs) and partially observable Markov decision processes (POMDPs) [2].\nPOMDPs are particularly well suited for modeling decisions for autonomous vehicles because it explicitly captures the limitations of the vehicle’s sensors in measuring the relevant state variables [3]–[5]. Though sensors can accurately measure many of the relevant variables pertaining to the physical state of the vehicles, the internal state (e.g., intentions and aggressiveness) of other drivers and road users can only be indirectly inferred [4]–[7]. The hypothesis explored in this paper is that inferring and planning with an estimate of the internal states of the traffic participants will improve safety and efficiency.\nDriving strategies derived from MDPs and POMDPs depend on several ingredients to be successful. First, an accurate stochastic model of the environment, including the behavior of other drivers is necessary. Though this paper uses a very simple model, there has been significant work on creating better models in recent years [8]–[10], and the POMDP planning methods that we use can easily be adapted to use these new models. POMDP methods also require a reward function. In some cases, constructing the reward function that balances different objectives (e.g., safety and efficiency) is straightforward. However, in other cases, human preferences can be difficult to quantify and encode into a reward function. For these cases, inverse reinforcement learning can be used to determine a suitable reward function based on data of how human drivers act [11], [12].\nBefore investing the effort required to develop and test a POMDP-based decision making system for real autonomous vehicles, it is important to quantify the potential performance improvement. This paper presents a method that involves comparing solutions obtained from several variations of Monte Carlo Tree Search [13]. For this research, we have chosen to investigate these ideas in the context of making lane changes on a freeway (a situation that has been anecdotally noted to be difficult [14]). We present a method for quantifying the performance gains that could result from perfect estimation of and planning with hidden behavior model parameters. In addition, we show that when model parameters are correlated, estimating the parameters online using physical measurements can greatly improve performance. Planning using a POMDP problem formulation that dynamically takes uncertainty into account can further improve performance.\nar X\niv :1\n70 2.\n00 85\n8v 1\n[ cs\n.A I]\n2 F\neb 2\n01 7"
    }, {
      "heading" : "II. MODEL",
      "text" : "The focus of this paper is on freeway driving. We investigate a scenario in which a vehicle must navigate from the rightmost to the leftmost lane of a four lane freeway as quickly as possible while maintaining safety and comfort.\nThroughout this section, x denotes position in the longitudinal direction, that is, the direction that the cars move along the road in meters, and y denotes position in the lateral direction, that is, the lane the car occupies in lane units. The problem can be stated as a discrete-time POMDP defined by the tuple (S,A, T,R,O,Z), which consists of • The state space, S: A system state,\ns = (q0, {(qi, θi)}i∈1..N ) ∈ S,\nconsists of the physical state of the ego vehicle (q0), and physical state and behavior model for each of the N other cars in the scene. The physical state,\nqi = (xi, yi, ẋi, ẏi),\nconsists of the car’s longitudinal and lateral position and velocity. The internal state (behavior model parameters), θi, is drawn from a set of behaviors Θ. • The action space, A: An action, u = (ẍe, ÿe) ∈ A, consists of the longitudinal acceleration and lateral velocity of the ego vehicle. The action space is discrete and pruned to prevent crashes (see Section II-C). • The state transition model, T : S × A × S → R: The value T (s, u, s′) is the probability of transitioning to state s′ given that action u is taken by the ego at state s. This function is implicitly defined by a generative model that consists of a state transition function, F (·), and a stochastic noise process (see Section II-B). • The reward model, R : S × S → R: The reward function, defined in Section II-D, rewards time spent in the target left lane and penalizes strong braking. • The observation space, O: An observation, o ∈ O consists of the physical states of all of the vehicles, that is o = {pi}i∈1..N . No information about the internal state is directly included in the observation. • The observation model, Z : S × O → R: The value Z(s′, o) is the probability of receiving observation o when the system transitions to state s′. In these experiments, the physical state is assumed to be known exactly, though it is not difficult to relax this assumption.\nThe remainder of this section elaborates on this model."
    }, {
      "heading" : "A. Driver Modeling",
      "text" : "The driver models for each car have two components: an acceleration model that governs the longitudinal motion and a lane change model that determines the lateral motion. In this paper, the acceleration model is the Intelligent Driver Model (IDM) [15], and the lane change model is the “Minimizing Overall Braking Induced by Lane change” (MOBIL) model [16]. Both of these models have a small number of parameters that determine the behavior of the drivers.\n1) IDM: The IDM Model was developed as a simple model for “microscopic” simulations of traffic flows and is able to reproduce some phenomena observed in real-world traffic flows. It determines the longitudinal acceleration for a human-driven car, ẍ, based on the desired distance gap to the preceding car, g, the absolute velocity, ẋ, and the velocity relative to the preceding car ∆ẋ. The longitudinal acceleration is governed by the following equation:\nẍIDM = a\n[ 1− ( ẋ\nẋ0\n)δ − ( g∗(ẋ,∆ẋ)\ng\n)2] , (1)\nwhere g∗ is the desired gap given by\ng∗(ẋ,∆ẋ) = g0 + T ẋ+ ẋ∆ẋ\n2 √ ab . (2)\nBrief descriptions and values for the parameters not defined here are provided later in Table I.\nA small amount of noise is also added to the acceleration\nẍ = ẍIDM + σvel ∆t w, (3)\nwhere w is zero-mean, normally distributed random variable with unit standard deviation. The w for each car and time step are independent of all others. Since w can take arbitrarily large values, crashes can technically occur. However, at each transition, if the additional noise is sufficiently large to cause a collision, w is artificially reduced to prevent collision. In practice this occurs very rarely.\n2) MOBIL: The MOBIL model makes the decision to change lanes based on maximizing the acceleration for the vehicle and its neighbors. When considering a lane change, MOBIL first ensures that the safety criterion ˜̈xfollow ≥ −bsafe, where ẍn will be the acceleration of the following car if the lane change is made and bsafe is the safe braking limit. It then makes the lane change if the following condition is met\n˜̈xc − ẍc + p ( ˜̈xn − ẍn + ˜̈xo − ẍo ) > ∆ath (4)\nwhere the quantities with tildes are calculated assuming that a lane change is made, the quantities with subscript c are quantities for the car making the lane change decision, those with n are for the new follower, and those with o are for the old follower. The parameter p ∈ [0, 1] is the politeness factor, which represents how much the driver values allowing other vehicles to increase their acceleration. The parameter ∆ath is the threshold acceleration increase to initiate a lane changing maneuver. Parameter values are listed in Table I."
    }, {
      "heading" : "B. Physical Dynamics",
      "text" : "The physical dynamics are simplified for the sake of computational efficiency. Time is divided into discrete steps of length ∆t. The longitudinal dynamics assume constant acceleration, and the lateral dynamics assume constant velocity over a time step, that is\nx′ = x+ ẋ∆t+ 1\n2 ẍ∆t2\nẋ′ = ẋ+ ẍ∆t y′ = y + ẏ∆t.\nThere is a physical limit to the braking acceleration, bmax. Lateral velocity is allowed to change instantly because cars on a freeway can achieve the lateral velocity needed for a lane change in time much shorter than ∆t by steering. If MOBIL determines that a lane change should be made, the lateral velocity, ẏ, is set to ẏlc. Lane changes are not allowed to reverse. Once a lane change has begun, ẏ remains constant until the lane change is completed (this is the reason that ẏ is part of the state). When a vehicle passes over the midpoint of a lane, lateral movement is immediately stopped so that lane changes always end at exactly the center of a lane.\nSince MOBIL only considers cars in adjacent lanes, there must be a coordination mechanism so that two cars do not converge into the same lane simultaneously. In order to accomplish this, if two cars begin changing into the same lane simultaneously, and the front vehicle is within g∗ of the rear vehicle, the rear vehicle’s lane change is canceled.\nIn order to reduce the computational demands of decisionmaking, only 50 m of road in front of the ego and 50 m behind are modeled. Thus, a model for vehicle entry into this section is needed. If there are fewer than Nmax vehicles on the road, a new vehicle is generated. First, a behavior for the new vehicle is drawn from Θ, and the initial speed is set to ẋ0 + σvelw0, where ẋ0 is the desired speed from the behavior model and w0 is a zero-mean, unit-variance, normally distributed random variable that is independent for each car. If this speed is greater than the ego’s speed, the new vehicle will appear at the back of the road section; if it is less, it will appear at the front. For each lane, g∗ is calculated, either for the new vehicle if the appearance is at the back or for the nearest following vehicle if the appearance is at the front. The new vehicle appears in the lane where the clearance to the nearest car is greatest. If no clearance is greater than g∗, the new vehicle does not appear.\nFor convenience, throughout this paper, the behavior described so far will be denoted compactly by the state transition function\ns′ = F (s, u, w). (5)"
    }, {
      "heading" : "C. Action Space for Crash-Free Driving",
      "text" : "At each time step, the planner for the ego must choose the longitudinal and lateral acceleration. For simplicity, the vehicle chooses from up to ten discrete actions. The vehicle may make an incremental decrease or increase in speed or maintain speed, and it may begin a left or right lane change or maintain the current lane. The combination of these adjustments make up nine of the actions. The final action is a braking action determined dynamically based on the speed and position of the vehicle ahead. At each time step, the maximum permitted acceleration, amax, is the maximum acceleration that the ego could take such that, if the vehicle ahead immediately begins braking at the physical limit, bmax, to a stop, the ego will still be able to stop before hitting it without exceeding physical braking limits itself. The braking action is (ẍe, ÿe) = (min{amax,−bnominal}, 0).\nThe inclusion of the dynamic braking action guarantees that there will always be an action available to the ego to\navoid a crash. At each step, the action space is pruned so that if ẍe > amax or if a lane change leads to a crash, that action is not considered. Since the IDM and MOBIL models are both crash-free [17], and actions that lead to crashes for the ego are not considered, no crashes occur in the simulation. Eliminating crashes in our model is justifiable because it is likely that in an actual autonomous vehicle a high-level planning system would be augmented with a low-level crash prevention system to increase safety and facilitate certification. In addition, it is difficult to model driver behavior in the extraordinary case of a crash."
    }, {
      "heading" : "D. Reward Function and Objectives",
      "text" : "The qualitative objectives in solving this problem are to reach the target lane as quickly as possible and increase the comfort and safety of both the ego and the other nearby vehicles. Thus, the following two metrics will be used to evaluate planning performance: 1) the average time taken for the ego to reach the target lane, and 2) the number of hard braking maneuvers that any vehicle undertakes during the time that it takes for the ego to reach the target lane. A hard braking maneuver occurs any time that ẍ < −bhard, where bhard is chosen to be an uncomfortably abrupt deceleration. The number of hard braking maneuvers is a proxy for both safety and comfort.\nIn order to encourage the planner to choose actions that will maximize these metrics, the reward function for the POMDP is defined as follows:\nR(s, s′) = 1(ye = ytarget)− λ N∑\ni=1\n1(ẋ′i − ẋi < −bhard∆t).\n(6) Thus, a reward is generated for each step in the target lane, and a cost is accrued for each braking maneuver. The weight λ balances the competing goals.\nE. Initial Scenes\nInitial scenes for the simulations are generated by beginning a simulation with only the ego on the road section. We then simulate 200 steps with the ego maintaining the current lane and using the IDM model with typical parameter values."
    }, {
      "heading" : "III. SOLUTION APPROACHES",
      "text" : "Monte Carlo tree search (MCTS) is one of the most widely used and effective methods for solving decisionmaking problems online [13]. MCTS creates a tree consisting of alternating levels of nodes corresponding to actions and states. Estimates of the value (expected discounted cumulative rewards) are maintained for each action node. In this paper, we consider four variants of MCTS to solve different versions of the problem. All of the variants make use of the upper confidence tree (UCT) [13] and double progressive widening (DPW) [18], [19] modifications to MCTS.\nWhen building the tree, UCT expands the action nodes that maximize an upper confidence estimate\nUCB(s, u) = Q̃(s, u) + c\n√ lnN(s)\nN(s, u) ,\nwhere Q̃(s, u) is the estimate of the action-value function obtained through rollout simulations and tree search, N(s, u) is the number of times action u has been tried from state s, N(s) = ∑ u∈AN(s, u), and c is the exploration constant. This efficiently balances exploration of the tree towards promising regions of the search space.\nDPW is used to govern the growth of the tree in large state spaces. If MCTS is applied to a large state space (such as the continuous state space of this lane changing problem) without a widening control mechanism in place, too many new states will be visited from each action node. This will result in a shallow tree that does not define a good policy. A DPW tree contains state-action nodes corresponding to each action that has been tried in each state. In order to control widening, DPW limits the number of children of a state-action node (s, u) to\nkN(s, u)α, (7)\nwhere k and α are tunable parameters. When there are fewer children than this limit, a new state is generated by simulating the dynamics and a child corresponding to the new state is added. As N(s, u) grows, so does the number of children, allowing for gradual widening of the tree. DPW also limits the number of actions that are considered from each state (“double” in DPW refers to limiting the widening at both state and action nodes), but this is less important for this problem because the action space is small.\nThe remainder of this section discusses three different approaches for planning with the internal states of the traffic participants."
    }, {
      "heading" : "A. Approach 1: Static Assumed Behavior (SAB)",
      "text" : "A performance baseline is established by planning as if all cars behave according to a single static “normal” internal state (see Table I). In this case, the problem is an MDP, which is solved using the MCTS-DPW algorithm."
    }, {
      "heading" : "B. Approach 2: Most Likely Model Predictive Control (MLMPC)",
      "text" : "Since information about the human’s internal state can be inferred by observing the car’s physical motion, performance superior to the SAB baseline can be achieved by estimating θ online. This is accomplished with a particle filter [20]. Filtering is independent for each car, but all of the behavior parameters for a given car are estimated jointly. There are two versions of the filter. In the first version, the behavior parameters are assumed to be uncorrelated, so a particle, θ̂ consists of values of all model parameters. In the second version, all parameters are assumed perfectly correlated (see Section IV-A), so a particle consists of only a single value, the “aggressiveness”.\nThe belief at a given time consists of the exactly known physical state, q, and a collection of M particles, {θ̂k}Mk=1 along with associated weights {W k}Mk=1. To update the belief when action u is taken, M new particles are sampled with probability proportional to the weights, and sampled noise values {ŵk}Mk=1 are used to generate new states according\nto ŝk′ = F ((q, θ̂k), u, ŵk). The new weights are determined by approximating the conditional probability of the particle given the observation:\nW k′ =    exp ( − (ẋ ′−ˆ̇x′)2 2σ2vel ) if y′ = ŷ′ γlane exp ( − (ẋ\n′−ˆ̇x′)2 2σ2vel\n) o.w.\n   ∝∼ Pr ( θ̂k ∣∣∣ o )\nwhere ẋ′ and y′ are taken from the observation, ˆ̇x′ and ˆ̇y′ are from ŝk′, the exponential expression is proportional to the Gaussian probability density function (from the acceleration noise), and γlane ∈ [0, 1] is a hand-tuned parameter that penalizes incorrect lane changes.\nIn order to prevent particle deprivation, during the resampling step, Gaussian noise with standard deviation proportional to the sample standard deviation of the current particle set is added to 10 % of the new samples.\nModel predictive control (MPC) is a widely used family of control techniques that use an imperfect model and feedback measurements to choose actions [21]. At each time step, a model predictive controller calculates a sequence of control actions that will maximize a reward function of the states visited up to a future horizon given that the system behaves according to a model. The first control action in this optimized sequence is executed, and the process is repeated after a new measurement is received.\nIn the most likely model predictive control (MLMPC) approach, we use this particle filter to estimate the internal state for each driver. At each step, the model used for MPC is the MDP that results from assuming that each driver has the internal state corresponding to the single particle with the highest weight. Each time a new observation is received, the particle filter is updated and MCTS-DPW determines the best action for the resulting MDP."
    }, {
      "heading" : "C. Approach 3: POMCP with Double Progressive Widening",
      "text" : "The MCTS algorithm has been extended to handle domains with state uncertainty in the partially observable Monte Carlo planning (POMCP) algorithm [22]. The root node of a POMCP tree corresponds to the current belief about the state maintained by the particle filter described above. In place of the state nodes of MCTS, POMCP uses history nodes that correspond to the sequence of actions and observations required to reach the node from the root. At each history node, POMCP represents the current belief using a collection of unweighted particles.\nIn order to address the challenges with the branching factor when using MCTS with continuous state spaces, we have adapted POMCP to use DPW. Equation (7) is used to limit the number of children of each action node. In cases when a new history node is not generated, a previously generated history node is selected and the next state is sampled from the particle collection corresponding to that node. The authors are not aware of any previous research that uses POMCP with DPW, so the algorithm’s general properties have not yet been thoroughly studied."
    }, {
      "heading" : "IV. RESULTS AND DISCUSSION",
      "text" : "The computational results from this study are designed to meet the two goals of 1) quantifying the size of the gap between the baseline control algorithm and the maximum potential lane change performance and 2) showing which cases internal state estimation and POMDP planning can approach the upper bound on performance. Experiments are carried out in three scenarios, each with a different distribution of internal states. In each of these scenarios, each of the three solution methods described in Section III are compared with an approximate upper performance bound obtained by planning with perfect knowledge of the behavior models."
    }, {
      "heading" : "A. Driver Model Distribution Scenarios",
      "text" : "We studied three internal state distribution scenarios. In all of these scenarios, drivers behave according to the IDM and MOBIL models presented in Section II-A, however the IDM and MOBIL parameter values are distributed differently.\nTable I shows typical parameter values for aggressive, timid, and normal drivers. The values are taken from [17], but some have been adjusted slightly so that the parameters for the normal driver are exactly half way between values for the timid and aggressive drivers. In all three of the scenarios, the marginal distributions of the parameters are uniformly distributed between the aggressive and timid values. The difference between the scenarios is the correlation of the parameter values. In Scenario 1, all of the parameters are independently distributed. In Scenario 2, all of the parameters are perfectly correlated so that all parameters are deterministic functions of the aggressiveness of the driver. Scenario 3 uses a distribution between these two extremes. In this scenario, values are drawn from a Gaussian copula with covariance matrix with 1 along the diagonal and a correlation, ρ ∈ (0, 1), elsewhere. The values are then scaled and translated to lie between the aggressive and normal limits. For Scenario 3, the value of ρ is 0.75, and Scenarios 1 and 2 can be thought of as limiting cases where ρ approaches 0 and 1, respectively. In Scenario 1, the first version of the particle filter, which estimates all of the model parameters jointly, is used, whereas in Scenarios 2 and 3, the second version of the particle filter that assumes fully correlated parameters is used, that is, it only estimates a single “aggressiveness” parameter for each car. The small inset plots in Fig. 1 illustrate the level of correlation by plotting sampled values of two of the parameters."
    }, {
      "heading" : "B. Performance Results",
      "text" : "Figure 1 shows the performance results from the simulations. Since the conservativeness of all of the solution techniques can be adjusted by changing λ in (6), 500 simulations were carried out using a reward function with λ set to the values in Table II. The average time taken to reach the target lane and the average number of braking actions per episode are plotted, and a linear interpolation between these points yields an approximation of the Pareto-optimal frontier.\nIn all cases, there is a significant performance gap between the upper bound and the baseline planning algorithm that assumes all cars behave according to the normal values of the driver model parameters. For example, in Scenario 1, if it is acceptable to have an average of 0.5 hard braking actions per episode, then the omniscient planner is able to reach the lane in an average of 9.0 s (50 %) less time than the baseline SAB planner. Alternatively, if the vehicle is required to complete the lane changing task in an average time of 10 s, then the average number of hard brakes per episode would be decreased by 0.40 (50 %) with perfect internal state knowledge. The performance gap between the baseline and the bound decreases slightly as the correlation, ρ, increases.\nPerhaps the most important result that can be gleaned from this data is the relationship between the correlation and the extent to which more advanced planning methods can close the gap to the bound. When the parameters are uncorrelated, POMCP is able to reduce the gap to approximately half its size, while MLMPC struggles to perform much better than the baseline. However, in the fully correlated scenario, both of the approaches that try to estimate the internal state are able to fully close the gap. The reason for this lies primarily in the difficulty in estimating the behavior parameters. In these tests, the human-driven cars spend most of their time near their desired speed, v0. This makes it relatively easy to estimate v0 using the particle filter, but difficult to measure\nthe other parameters that determine how the vehicle will react to the ego. In the fully correlated case, estimating v0 also gives the exact values of all other parameters, so POMCP and the MLMPC approach are essentially planning with known models. The fully correlated case is rather unrealistic, but the results from Scenario 3 indicate that even if the parameters are only partially correlated, most of the losses caused by not knowing the model can be recovered.\nThe gap between the POMCP and most likely behavior approaches also indicates the relative importance of the two tasks of estimation and planning when accounting for internal state uncertainty. Since the POMCP approach plans with the full distribution of possible internal states, it takes uncertainty and future feedback into account when planning. The MLMPC approach plans as if the behavior models are exactly known and does not incorporate possible future feedback. It could be called an “open loop” planner. Of course, when the model is easy to estimate with high certainty in the fully correlated scenario, there is little difference between the POMCP and MLMPC performance. However, in Scenario 1, where there is large uncertainty about the model, POMCP does show a significant improvement.\nThe Julia source code for the experiments can be found at https://github.com/sisl/Multilane.jl, and the solver software is part of the POMDPs.jl package (https://github.com/JuliaPOMDP/POMDPs.jl)."
    }, {
      "heading" : "V. CONCLUSION",
      "text" : "This paper has presented a method for quantifying the performance that could be gained by a planner omniscient to the internal state of other vehicles compared to a baseline. In the simplified case that we investigated, this gap proved to be significant. Moreover, we showed that the use of a simple particle filter can offer greatly improved performance, especially when internal state features are highly correlated, and planning using a POMDP model can further reduce the gap to the bound.\nThe method described in this paper is extremely flexible and adaptable. One only needs to define a generative model for states, rewards, and observations and possibly tune some algorithm parameters to adapt it to a new domain. This makes it useful as a preliminary analysis tool for deciding which control approach to implement on an actual vehicle.\nFor the particular case of freeway lane changing, the most important result from this research is that the distribution of behavior models in the driver population is a critical factor in determining the performance improvement that can be gained by estimating the internal state online and planning in a way that takes uncertainty in the model into account. If internal states that affect the performance goals are highly correlated with states that are easy to measure (e.g., speed), then the more advanced planning methods will offer a large benefit, but if internal states are not correlated, there is less advantage. This observation motivates the need for detailed understanding of the actual distribution of behavior models and internal states in the driver population, especially correlation between features.\nThere are many possible extensions to this work. It may be beneficial to try more advanced POMDP solvers such as DESPOT [23]. Thus far, only inter-vehicle internal state heterogeneity has been considered, but real drivers also change their behavior over time and in response to stimuli [24], and it may be useful to model this when planning. Moreover, it may be even more beneficial to take a driver’s specific intentions into account when planning [25]."
    } ],
    "references" : [ {
      "title" : "A preliminary analysis of real-world crashes involving self-driving vehicles",
      "author" : [ "B. Schoettle", "M. Sivak" ],
      "venue" : "University of Michigan Transportation Research Institute, Tech. Rep. UMTRI-2015-34, 2015.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Decision making under uncertainty: Theory and application",
      "author" : [ "M.J. Kochenderfer" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2015
    }, {
      "title" : "Solving continuous POMDPs: Value iteration with incremental learning of an efficient space representation",
      "author" : [ "S. Brechtel", "T. Gindele", "R. Dillmann" ],
      "venue" : "International Conference on Machine Learning (ICML), 2013.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Information gathering actions over human internal state",
      "author" : [ "D. Sadigh", "S.S. Sastry", "S.A. Seshia", "A. Dragan" ],
      "venue" : "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2016.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Intention-aware online POMDP planning for autonomous driving in a crowd",
      "author" : [ "H. Bai", "S. Cai", "N. Ye", "D. Hsu", "W.S. Lee" ],
      "venue" : "IEEE International Conference on Robotics and Automation (ICRA), 2015.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Improving human-in-the-loop decision making in multi-mode driver assistance systems using hidden mode stochastic hybrid systems",
      "author" : [ "C.-P. Lam", "A.Y. Yang", "K. Driggs-Campbell", "R. Bajcsy", "S.S. Sastry" ],
      "venue" : "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Identifying modes of intent from driver behaviors in dynamic environments",
      "author" : [ "K. Driggs-Campbell", "R. Bajcsy" ],
      "venue" : "IEEE International Conference on Intelligent Transportation Systems (ITSC), 2015.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Learning driver behavior models from traffic observations for decision making and planning",
      "author" : [ "T. Gindele", "S. Brechtel", "R. Dillmann" ],
      "venue" : "IEEE Intelligent Transportation Systems Magazine, vol. 7, no. 1, pp. 69–79, 2015.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A probabilistic framework for microscopic traffic propagation",
      "author" : [ "T.A. Wheeler", "P. Robbel", "M.J. Kochenderfer" ],
      "venue" : "IEEE International Conference on Intelligent Transportation Systems (ITSC), 2015.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Data-driven probabilistic modeling and verification of human driver behavior",
      "author" : [ "D. Sadigh", "K. Driggs-Campbell", "A. Puggelli", "V.S.W. Li", "R. Bajcsy", "A.L. Sangiovanni-Vincentelli", "S.S. Sastry", "S.A. Seshia" ],
      "venue" : "AAAI Spring Symposium on Formal Verification and Modeling in Human-Machine Systems, 2014.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Continuous inverse optimal control with locally optimal examples",
      "author" : [ "S. Levine", "V. Koltun" ],
      "venue" : "International Conference on Machine Learning (ICML), 2012.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Planning for autonomous cars that leverage effects on human actions",
      "author" : [ "D. Sadigh", "S.S. Sastry", "S.A. Seshia", "A. Dragan" ],
      "venue" : "Robotics: Science and Systems, 2016.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "A survey of Monte Carlo tree search methods",
      "author" : [ "C.B. Browne", "E. Powley", "D. Whitehouse", "S.M. Lucas", "P.I. Cowling", "P. Rohlfshagen", "S. Tavener", "D. Perez", "S. Samothrakis", "S. Colton" ],
      "venue" : "IEEE Transactions on Computational Intelligence and AI in Games, vol. 4, no. 1, pp. 1–43, 2012.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Humans are slamming into driverless cars and exposing a key flaw, [Online]. Available: http://bloom.bg/1Qw8fjB",
      "author" : [ "K. Naughton" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2015
    }, {
      "title" : "Congested traffic states in empirical observations and microscopic simulations",
      "author" : [ "M. Treiber", "A. Hennecke", "D. Helbing" ],
      "venue" : "Physical Review E, vol. 62, no. 2, pp. 1805–1824, 2 2000.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1805
    }, {
      "title" : "General lane-changing model MOBIL for car-following models",
      "author" : [ "A. Kesting", "M. Treiber", "D. Helbing" ],
      "venue" : "Transportation Research Record, vol. 1999, pp. 86–94, 2007.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Agents for traffic simulation",
      "author" : [ "——" ],
      "venue" : "Multi Agent Systems: Simulation and Applications, A. M. Uhrmacher and D. Weyns, Eds., CRC Press, 2009, ch. 11, pp. 325–356.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A comparison of Monte Carlo tree search and mathematical optimization for large scale dynamic resource allocation",
      "author" : [ "D. Bertsimas", "J.D. Griffith", "V. Gupta", "M.J. Kochenderfer", "V.V. Mišić", "R. Moss" ],
      "venue" : "ArXiv e-prints, May 2014. arXiv: 1405.5498.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Continuous upper confidence trees",
      "author" : [ "A. Couëtoux", "J.-B. Hoock", "N. Sokolovska", "O. Teytaud", "N. Bonnard" ],
      "venue" : "Learning and Intelligent Optimization, 2011.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Model predictive control: Theory and practicea survey",
      "author" : [ "C.E. Garcia", "D.M. Prett", "M. Morari" ],
      "venue" : "Automatica, vol. 25, no. 3, pp. 335–348, 1989.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Monte-Carlo planning in large POMDPs",
      "author" : [ "D. Silver", "J. Veness" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS), 2010.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "DESPOT: Online POMDP planning with regularization",
      "author" : [ "A. Somani", "N. Ye", "D. Hsu", "W.S. Lee" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS), 2013.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Heterogeneity in car-following behavior: Theory and empirics",
      "author" : [ "S. Ossen", "S.P. Hoogendoorn" ],
      "venue" : "Transportation Research Part C: Emerging Technologies, vol. 19, no. 2, pp. 182–195, 2011.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Driver intent inference at urban intersections using the intelligent driver model",
      "author" : [ "M. Liebner", "M. Baumann", "F. Klanner", "C. Stiller" ],
      "venue" : "Intelligent Vehicles Symposium, 2012.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "However, the autonomous vehicles actually had a higher accident rate than average for a conventional vehicle in the United States because of accidents for which they were not legally responsible [1].",
      "startOffset" : 195,
      "endOffset" : 198
    }, {
      "referenceID" : 1,
      "context" : "This paper explores techniques based on Markov decision processes (MDPs) and partially observable Markov decision processes (POMDPs) [2].",
      "startOffset" : 133,
      "endOffset" : 136
    }, {
      "referenceID" : 2,
      "context" : "POMDPs are particularly well suited for modeling decisions for autonomous vehicles because it explicitly captures the limitations of the vehicle’s sensors in measuring the relevant state variables [3]–[5].",
      "startOffset" : 197,
      "endOffset" : 200
    }, {
      "referenceID" : 4,
      "context" : "POMDPs are particularly well suited for modeling decisions for autonomous vehicles because it explicitly captures the limitations of the vehicle’s sensors in measuring the relevant state variables [3]–[5].",
      "startOffset" : 201,
      "endOffset" : 204
    }, {
      "referenceID" : 3,
      "context" : ", intentions and aggressiveness) of other drivers and road users can only be indirectly inferred [4]–[7].",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 6,
      "context" : ", intentions and aggressiveness) of other drivers and road users can only be indirectly inferred [4]–[7].",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 7,
      "context" : "uses a very simple model, there has been significant work on creating better models in recent years [8]–[10], and the POMDP planning methods that we use can easily be adapted to use these new models.",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 9,
      "context" : "uses a very simple model, there has been significant work on creating better models in recent years [8]–[10], and the POMDP planning methods that we use can easily be adapted to use these new models.",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 10,
      "context" : "For these cases, inverse reinforcement learning can be used to determine a suitable reward function based on data of how human drivers act [11], [12].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 11,
      "context" : "For these cases, inverse reinforcement learning can be used to determine a suitable reward function based on data of how human drivers act [11], [12].",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 12,
      "context" : "This paper presents a method that involves comparing solutions obtained from several variations of Monte Carlo Tree Search [13].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 13,
      "context" : "For this research, we have chosen to investigate these ideas in the context of making lane changes on a freeway (a situation that has been anecdotally noted to be difficult [14]).",
      "startOffset" : 173,
      "endOffset" : 177
    }, {
      "referenceID" : 14,
      "context" : "In this paper, the acceleration model is the Intelligent Driver Model (IDM) [15], and the lane change model is the “Minimizing Overall Braking Induced by Lane change” (MOBIL) model [16].",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 15,
      "context" : "In this paper, the acceleration model is the Intelligent Driver Model (IDM) [15], and the lane change model is the “Minimizing Overall Braking Induced by Lane change” (MOBIL) model [16].",
      "startOffset" : 181,
      "endOffset" : 185
    }, {
      "referenceID" : 0,
      "context" : "The parameter p ∈ [0, 1] is the politeness factor, which represents how much the driver values allowing other vehicles to increase their acceleration.",
      "startOffset" : 18,
      "endOffset" : 24
    }, {
      "referenceID" : 16,
      "context" : "Since the IDM and MOBIL models are both crash-free [17], and actions that lead to crashes for the ego are not considered, no crashes occur in the simulation.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 12,
      "context" : "Monte Carlo tree search (MCTS) is one of the most widely used and effective methods for solving decisionmaking problems online [13].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 12,
      "context" : "All of the variants make use of the upper confidence tree (UCT) [13] and double progressive widening (DPW) [18], [19] modifications to MCTS.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 17,
      "context" : "All of the variants make use of the upper confidence tree (UCT) [13] and double progressive widening (DPW) [18], [19] modifications to MCTS.",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 18,
      "context" : "All of the variants make use of the upper confidence tree (UCT) [13] and double progressive widening (DPW) [18], [19] modifications to MCTS.",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 0,
      "context" : "where ẋ′ and y′ are taken from the observation, ˆ̇ x′ and ˆ̇ y′ are from ŝk′, the exponential expression is proportional to the Gaussian probability density function (from the acceleration noise), and γlane ∈ [0, 1] is a hand-tuned parameter that penalizes incorrect lane changes.",
      "startOffset" : 209,
      "endOffset" : 215
    }, {
      "referenceID" : 19,
      "context" : "Model predictive control (MPC) is a widely used family of control techniques that use an imperfect model and feedback measurements to choose actions [21].",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 20,
      "context" : "Monte Carlo planning (POMCP) algorithm [22].",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 16,
      "context" : "The values are taken from [17],",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 21,
      "context" : "be beneficial to try more advanced POMDP solvers such as DESPOT [23].",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 22,
      "context" : "Thus far, only inter-vehicle internal state heterogeneity has been considered, but real drivers also change their behavior over time and in response to stimuli [24], and it may be useful to model this when planning.",
      "startOffset" : 160,
      "endOffset" : 164
    }, {
      "referenceID" : 23,
      "context" : "Moreover, it may be even more beneficial to take a driver’s specific intentions into account when planning [25].",
      "startOffset" : 107,
      "endOffset" : 111
    } ],
    "year" : 2017,
    "abstractText" : "Safe interaction with human drivers is one of the primary challenges for autonomous vehicles. In order to plan driving maneuvers effectively, the vehicle’s control system must infer and predict how humans will behave based on their latent internal state (e.g., intentions and aggressiveness). This research uses a simple model for human behavior with unknown parameters that make up the internal states of the traffic participants and presents a method for quantifying the value of estimating these states and planning with their uncertainty explicitly modeled. An upper performance bound is established by an omniscient Monte Carlo Tree Search (MCTS) planner that has perfect knowledge of the internal states. A baseline lower bound is established by planning with MCTS assuming that all drivers have the same internal state. MCTS variants are then used to solve a partially observable Markov decision process (POMDP) that models the internal state uncertainty to determine whether inferring the internal state offers an advantage over the baseline. Applying this method to a freeway lane changing scenario reveals that there is a significant performance gap between the upper bound and baseline. POMDP planning techniques come close to closing this gap, especially when important hidden model parameters are correlated with measurable parameters.",
    "creator" : "LaTeX with hyperref package"
  }
}