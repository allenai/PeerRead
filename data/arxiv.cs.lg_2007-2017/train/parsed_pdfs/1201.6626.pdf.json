{
  "name" : "1201.6626.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning RoboCup-Keepaway with Kernels",
    "authors" : [ "Tobias Jung", "Daniel Polani" ],
    "emails" : [ "tjung@informatik.uni-mainz.de", "d.polani@herts.ac.uk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n20 1.\n66 26\nv1 [\ncs .A\nI] 3\nKeywords: Reinforcement Learning, Least-squares Policy Iteration, Regularization Networks, RoboCup"
    }, {
      "heading" : "1. Introduction",
      "text" : "RoboCup simulated soccer has been conceived and is widely accepted as a common platform to address various challenges in artificial intelligence and robotics research. Here, we consider a subtask of the full problem, namely the keepaway problem. In keepaway we have two smaller teams: one team (the ‘keepers’) must try to maintain possession of the ball for as long as possible while staying within a small region of the full soccer field. The other team (the ‘takers’) tries to gain possession of the ball. Stone et al. (2005) initially formulated keepaway as benchmark problem for reinforcement learning (RL); the keepers must individually learn how to maximize the time they control the ball as a team against the team of opposing takers playing a fixed strategy. The central challenges to overcome are, for one, the high dimensionality of the state space (each observed state is a vector of 13 measurements), meaning that conventional approaches to function approximation in RL, like grid-based tilecoding, are infeasible; second, the stochasticity due to noise and the uncertainty in control due to the multi-agent nature imply that the dynamics of the\nc©2007 Tobias Jung and Daniel Polani.\nenvironment are both unknown and cannot be obtained easily. Hence we need model-free methods. Finally, the underlying soccer server expects an action every 100 msec, meaning that efficient methods are necessary that are able to learn in real-time.\nStone et al. (2005) successfully applied RL to keepaway, using the textbook approach with online Sarsa(λ) and tilecoding as underlying function approximator (Sutton and Barto, 1998). However, tilecoding is a local method and places parameters (i.e. basis functions) in a regular fashion throughout the entire state space, such that the number of parameters grows exponentially with the dimensionality of the space. In (Stone et al., 2005) this very serious shortcoming was adressed by exploiting problem-specific knowledge of how the various state variables interact. In particular, each state variable was considered independently from the rest. Here, we will demonstrate that one can also learn using the full (untampered) state information, without resorting to simplifying assumptions.\nIn this paper we propose a (non-parametric) kernel-based approach to approximate the value function. The rationale for doing this is that by representing the solution through the data and not by some basis functions chosen before the data becomes available, we can better adapt to the complexity of the unknown function we are trying to estimate. In particular, parameters are not ‘wasted’ on parts of the input space that are never visited. The hope is that thereby the exponential growth of parameters is bypassed. To solve the RL problem of optimal control we consider the framework of approximate policy iteration with the related least-squares based policy evaluation methods LSPE(λ) proposed by Nedić and Bertsekas (2003) and LSTD(λ) proposed by Boyan (1999). Least-squares based policy evaluation is ideally suited for the use with linear models and is a very sample-efficient variant of RL. In this paper we provide a unified and concise formulation of LSPE and LSTD; the approximated value function is obtained from a regularization network which is effectively the mean of the posterior obtained by GP regression (Rasmussen and Williams, 2006). We use the subset of regressors method (Smola and Schölkopf, 2000; Luo and Wahba, 1997) to approximate the kernel using a much reduced subset of basis functions. To select this subset we employ greedy online selection, similar to (Csató and Opper, 2001; Engel et al., 2003), that adds a candidate basis function based on its distance to the span of the previously chosen ones. One improvement is that we consider a supervised criterion for the selection of the relevant basis functions that takes into account the reduction of the cost in the original learning task in addition to reducing the error incurred from approximating the kernel. Since the per-step complexity during training and prediction depends on the size of the subset, making sure that no unnecessary basis functions are selected ensures more efficient usage of otherwise scarce resources. In this way learning in real-time (a necessity for keepaway) becomes possible.\nThis paper is structured in three parts: the first part (Section 2) gives a brief introduction on reinforcement learning and carrying out general regression with regularization networks. The second part (Section 3) describes and derives an efficient recursive implementation of the proposed approach, particularly suited for online learning. The third part describes the RoboCup-keepaway problem in more detail (Section 4) and contains the results we were able to achieve (Section 5). A longer discussion of related work is deferred to the end of the paper; there we compare the similarities of our work with that of Engel et al. (2003, 2005a,b)."
    }, {
      "heading" : "2. Background",
      "text" : "In this section we briefly review the subjects of RL and regularization networks."
    }, {
      "heading" : "2.1 Reinforcement Learning",
      "text" : "Reinforcement learning (RL) is a simulation-based form of approximate dynamic programming, e.g. see (Bertsekas and Tsitsiklis, 1996). Consider a discrete-time dynamical system with states S = {1, . . . , N} (for ease of exposition we assume the finite case). At each time step t, when the system is in state st, a decision maker chooses a control-action at (again, selected from a finite set A of admissible actions) which changes probabilistically the state of the system to st+1, with distribution P (st+1|st, at). Every such transition yields an immediate reward rt+1 = R(st+1|st, at). The ultimate goal of the decision-maker is to choose a course of actions such that the long-term performance, a measure of the cumulated sum of rewards, is maximized."
    }, {
      "heading" : "2.1.1 Model-free Q-value function and optimal control",
      "text" : "Let π denote a decision-rule (called the policy) that maps states to actions. For a fixed policy π we want to evaluate the state-action value function (Q-function) which for every state s is taken to be the expected infinite-horizon discounted sum of rewards obtained from starting in state s, choosing action a and then proceeding to select actions according to π:\nQπ(s, a) := Eπ\n\n\n\n∑\nt≥0\nγtrt+1|s0 = s, a0 = a\n\n\n\n∀s, a (1)\nwhere st+1 ∼ P (· |st, π(st)) and rt+1 = R(st+1|st, π(st)). The parameter γ ∈ (0, 1) denotes a discount factor.\nUltimately, we are not directly interested in Qπ; our true goal is optimal control, i.e. we seek an optimal policy π∗ = argmaxπ Q\nπ. To accomplish that, policy iteration interleaves the two steps policy evaluation and policy improvement: First, compute Qπk for a fixed policy πk. Then, once Q\nπk is known, derive an improved policy πk+1 by choosing the greedy policy with respect to Qπk , i.e. by by choosing in every state the action πk+1(s) = argmaxaQ\nπk(s, a) that achieves the best Q-value. Obtaining the best action is trivial if we employ the Q-notation, otherwise we would need the transition probabilities and reward function (i.e. a ‘model’).\nTo compute the Q-function, one exploits the fact that Qπ obeys the fixed-point relation Qπ = TπQ π, where Tπ is the Bellman operator\n( TπQ ) (s, a) := Es′∼P (· |s,a) { R(s′|s, a) + γQ(s′, π(s′)) } .\nIn principle, it is possible to calculate Qπ exactly by solving the corresponding linear system of equations, provided that the transition probabilities P (s′|s, a) and rewards R(s′|s, a) are known in advance and the number of states is finite and small.\nHowever, in many practical situations this is not the case. If the number of states is very large or infinite, one can only operate with an approximation of the Q-function, e.g. a linear approximation Q̃(s, a;w) = φm(s, a) Tw, where φm(s, a) is an m-dimensional feature\nvector and w the adjustable weight vector. To approximate the unknown expectation value one employs simulation (i.e. an agent interacts with the environment) to generate a large number of observed transitions. Figure 1 depicts the resulting approximate policy iteration framework: using only a parameterized Q̃ and sample transitions to emulate application of Tπ means that we can carry out the policy evaluation step only approximately. Also, using an approximation of Qπk to derive an improved policy from does not necessarily mean that the new policy actually is an improved one; oscillations in policy space are possible. In practice however, approximate policy iteration is a fairly sound procedure that either converges or oscillates with bounded suboptimality (Bertsekas and Tsitsiklis, 1996).\nInferring a parameter vector wk from sample transitions such that Q̃(· ;wk) is a good approximation to Qπk is therefore the central problem addressed by reinforcement learning. Chiefly two questions need to be answered:\n1. By what method do we choose the parametrisation of Q̃ and carry out regression?\n2. By what method do we learn the weight vector w of this approximation, given sample transitions?\nThe latter can be solved by the family of temporal difference learning, with TD(λ), initially proposed by Sutton (1988), being its most prominent member. Using a linearly parametrized value function, it was in shown in (Tsitsiklis and Roy, 1997) that TD(λ) converges against the true value function (under certain technical assumptions)."
    }, {
      "heading" : "2.1.2 Approximate policy evaluation with least-squares methods",
      "text" : "In what follows we will discuss three related algorithms for approximate policy evaluation that share most of the advantages of TD(λ) but converge much faster, since they are based on solving a least-squares problem in closed form, whereas TD(λ) is based on\nstochastic gradient descent. All three methods assume that an (infinitely) long1 trajectory of states and rewards is generated using a simulation of the system (e.g. an agent interacting with its environment). The trajectory starts from an initial state s0 and consists of tuples (s0, a0), (s1, a1), . . . and rewards r1, r2, . . . where action ai is chosen according to π and successor states and associated rewards are sampled from the underlying transition probabilities. From now on, to abbreviate these state-action tuples, we will understand xt as denoting xt := (st, at). Furthermore, we assume that the Q-function is parameterized by Q̃π(x;w) = φm(x) Tw and that w needs to be determined.\nThe LSPE(λ) method. The method λ-least squares policy evaluation LSPE(λ) was proposed by Nedić and Bertsekas (2003); Bertsekas et al. (2004) and proceeds by making incremental changes to the weights w. Assume that at time t (after having observed t transitions) we have a current weight vector wt and observe a new transition from xt to xt+1 with associated reward rt+1. Then we compute the solution ŵt+1 of the least-squares problem\nŵt+1 = argmin w\nt ∑\ni=0\n{\nφm(xi) Tw − φm(xi) Twt − t ∑\nk=i\n(λγ)k−id(xk,xk+1;wt)\n}2\n(2)\nwhere\nd(xk,xk+1;wt) := rk+1 + γφm(xk+1) Twt − φm(xk) Twt.\nThe new weight vector wt+1 is obtained by setting\nwt+1 = wt + ηt(ŵt+1 −wt) (3)\nwhere w0 is the initial weight vector and 0 < ηt ≤ 1 is a diminishing step size.\nThe LSTD(λ) method. The least-squares temporal difference method LSTD(λ) proposed by Bradtke and Barto (1996) for λ = 0 and by Boyan (1999) for general λ ∈ [0, 1] does not proceed by making incremental changes to the weight vector w. Instead, at time t (after having observed t transitions), the weight vector wt+1 is obtained by solving the fixed-point equation\nŵ = argmin w\nt ∑\ni=0\n{\nφm(xi) Tw − φm(xi)\nTŵ − t ∑\nk=i\n(λγ)k−id(xk,xk+1; ŵ)\n}2\n(4)\nfor ŵ, where\nd(xk,xk+1; ŵ) := rk+1 + γφm(xk+1) Tŵ − φm(xk) Tŵ,\nand setting wt+1 to this unique solution.\n1. If we are dealing with an episodic learning task with designated terminal states, we can generate an infinite trajectory in the following way: once an episode ends, we set the discount factor γ to zero and make a zero-reward transition from the terminal state to the start state of the next (following) episode.\nComparison of LSPE and LSTD. The similarities and differences between LSPE(λ) and LSTD(λ) are listed in Table 1. Both LSPE(λ) and LSTD(λ) converge to the same limit (see Bertsekas et al., 2004), which is also the limit to which TD(λ) converges (the initial iterates may be vastly different though). Both methods rely on the solution of a least-squares problem (either explicitly as is the case in LSPE or implicitly as is the case in LSTD) and can be efficiently implemented using recursive computations. Computational experiments in (Bertsekas and Ioffe, 1996) or (Lagoudakis and Parr, 2003) indicate that both approaches can perform much better than TD(λ).\nBoth methods LSPE and LSTD differ as far as their role in the approximate policy iteration framework is concerned. LSPE can take advantage of previous estimates of the weight vector and can hence be used in the context of optimistic policy iteration (OPI), i.e. the policy under consideration gets improved following very few observed transitions. For LSTD this is not possible; here a more rigid actor-critic approach is called for.\nBoth methods LSPE and LSTD also differ as far as their relation to standard regression with least-squares methods is concerned. LSPE directly minimizes a quadratic objective function. Using this function it will be possible to carry out ‘supervised’ basis selection, where for the selection of basis functions the reduction of the costs (the quantity we are trying to minimize) is taken into account. For LSTD this is not possible; here in fact we are solving a fixed point equation that employs least-squares only implicitly (to carry out the projection).\nThe BRM method. A third approach, related to LSTD(0) is the direct minimization of the Bellman residuals (BRM), as proposed in (Baird, 1995; Lagoudakis and Parr, 2003). Here, at time t, the weight vector wt+1 is obtained from solving the least-squares problem\nwt+1 = argmin w\nt ∑\ni=0\n{\nφm(xi) Tw −\n∑\ns′\nP (s′|si, π(si)) [ R(s′|si, π(si)) + γφm(s ′, π(s′))Tw ]\n}2\nUnfortunately, the transition probabilities can not be approximated by using single samples from the trajectory; one would need ‘doubled’ samples to obtain an unbiased estimate (see Baird, 1995). Thus this method would be only applicable for tasks with deterministic state transitions or known state dynamics; two conditions which are both violated in our application to RoboCup-keepaway. Nevertheless we will treat the deterministic case in first place during all our derivations, since LSPE and LSTD require only very minor changes to the resulting implementation. Using BRM with deterministic transitions amounts to\nsolving the least-squares problem\nwt+1 = argmin w\nt ∑\ni=0\n{\nφm(xi) Tw − ri+1 − γφm(xi+1)\nTw }2\n(5)"
    }, {
      "heading" : "2.2 Standard regression with regularization networks",
      "text" : "From the foregoing discussion we have seen that (approximate) policy evaluation can amount to a traditional function approximation problem. For this purpose we will here consider the family of regularization networks (Girosi et al., 1995), which are functionally equivalent to kernel ridge regression and Bayesian regression with Gaussian processes (Rasmussen and Williams, 2006). Here however, we will introduce them from the non-Bayesian regularization perspective as in (Smola and Schölkopf, 2000)."
    }, {
      "heading" : "2.2.1 Solving the full problem",
      "text" : "Given t training examples {xi, yi} t i=1 with inputs xi and observed outputs yi, to reconstruct the underlying function, one considers candidates from a function space Hk, where Hk is a reproducing kernel Hilbert space with reproducing kernel k (e.g. Wahba, 1990), and searches among all possible candidates for the function f ∈ Hk that achieves the minimum in the risk functional ∑\n(yi − f(xi)) 2 + σ2 ‖f‖Hk . The scalar σ 2 is a regularization parameter. Since solutions to this variational problem may be represented through the data alone (Wahba, 1990) as f(·) = ∑\nk(xi, ·)wi, the unknown weight vector w is obtained from solving the quadratic problem\nmin w∈Rt\n(Kw − y)T(Kw − y) + σ2wTKw (6)\nThe solution to (6) is w = (K+ σ2I)−1y, where y = ( y1, . . . yt ) T and K is the t× t kernel matrix [K]ij = k(xi,xj)."
    }, {
      "heading" : "2.2.2 Subset of regressor approximation",
      "text" : "Often, one is not willing to solve the full t-by-t problem in (6) when the number of training examples t is large and instead considers means of approximation. In the subset of regressors (SR) approach (Poggio and Girosi, 1990; Luo and Wahba, 1997; Smola and Schölkopf, 2000) one chooses a subset {x̃i} m i=1 of the data, with m ≪ t, and approximates the kernel for arbitrary x,x′ by taking\nk(x,x′) = km(x) TK−1mmkm(x ′). (7)\nHere km(x) denotes the m × 1 feature vector km(x) = ( k(x̃1,x), . . . , k(x̃m,x) )T\nand the m × m matrix Kmm is the submatrix [Kmm]ij = k(x̃i, x̃j) of the full kernel matrix K. Replacing the kernel in (6) by expression (7) gives\nmin w∈Rm (Ktmw − y) T(Ktmw − y) + σ 2wTKmmw\nwith solution\nwt = ( KTtmKtm + σ 2Kmm )−1 KTtmy (8)\nwhereKtm is the t×m submatrix [Ktm]ij = k(xi, x̃j) corresponding to them columns of the data points in the subset. Learning the weight vector wt from (8) costs O(tm\n2) operations. Afterwards, predictions for unknown test points x∗ are made by f(x∗) = km(x∗)\nTw at O(m) operations."
    }, {
      "heading" : "2.2.3 Online selection of the subset",
      "text" : "To choose the subset of relevant basis functions (termed the dictionary or set of basis vectors BV) many different approaches are possible; typically they can be distinguished as being unsupervised or supervised. Unsupervised approaches like random selection (Williams and Seeger, 2001) or the incomplete Cholesky decomposition (Fine and Scheinberg, 2001) do not use information about the task we want to solve, i.e. the response variable we wish to regress upon. Random selection does not use any information at all whereas incomplete Cholesky aims at reducing the error incurred from approximating the kernel matrix. Supervised choice of the subset does take into account the response variable and usually proceeds by greedy forward selection, using e.g. matching pursuit techniques (Smola and Bartlett, 2001).\nHowever, none of these approaches are directly applicable for sequential learning, since the complete set of basis function candidates must be known from the start. Instead, assume that the data becomes available only sequentially at t = 1, 2, . . . and that only one pass over the data set is possible, so that we cannot select the subset BV in advance. Working in the context of Gaussian process regression, Csató and Opper (2001) and later Engel et al. (2003) have proposed a sparse greedy online approximation: start from an empty set of BV and examine at every time step t if the new example needs to be included in BV or if it can be processed without augmenting BV. The criterion they employ to make that decision is an unsupervised one: at every time step t compute for the new data point xt the error\nδt = k(xt,xt)− km(xt) TK−1mmkm(xt) (9)\nincurred from approximating the new data point using the current BV. If δt exceeds a given threshold then it is considered as sufficiently different and added to the dictionary BV. Note that only the current number of elements in BV at a given time t is considered, the contribution from basis functions that will be added at a later time is ignored.\nIn this case, it might be instructive to visualize what happens to the t×m data matrix Ktm once BV is augmented. Adding the new element xt to BV means adding a new basis function (centered on xt) to the model and consequently adding a new associated column q = ( k(x1,xt), . . . , k(xt,xt) )T\nto Ktm. With sparse online approximation all t − 1 past entries in q are given by k(xi,xt) ≈ km(xi)\nTK−1mmkm(xt), i = 1 . . . , t− 1, which is exact for the m basis-elements and an approximation for the remaining t−m−1 non-basis elements. Hence, going from m to m+ 1 basis functions, we have that\nKt,m+1 = [ Ktm q ] =\n[\nKt−1,m Kt−1,mat km(xt) T k(xt,xt)\n]\n. (10)\nwhere at := K −1 mmkm(xt). The overall effect is that now we do not need to access the full data set any longer. All costly O(tm) operations that arise from adding a new column, i.e. adding a new basis function, computing the reduction of error during greedy\nforward selection of basis functions, or computing predictive variance with augmentation as in (Rasmussen and Quiñonero Candela, 2005), now become a more affordable O(m2).\nThis is exploited in (Jung and Polani, 2006); here a simple modification of the selection procedure is presented, where in addition to the unsupervised criterion from (9) the contribution to the reduction of the error (i.e. the objective function one is trying to minimize) is taken into account. Since the per-step complexity during training and then later during prediction critically depends on the size m of the subset BV, making sure that no unnecessary basis functions are selected ensures more efficient usage of otherwise scarce resources and makes learning in real-time (a necessity for keepaway) possible."
    }, {
      "heading" : "3. Policy evaluation with regularization networks",
      "text" : "We now present an efficient online implementation for least-squares-based policy evaluation (applicable to the methods LSPE, LSTD, BRM) to be used in the framework of approximate policy iteration (see Figure 1). Our implementation combines the aforementioned automatic selection of basis functions (from Section 2.2.3) with a recursive computation of the weight vector corresponding to the regularization network (from Section 2.2.2) to represent the underlying Q-function. The goal is to infer an approximation Q̃(· ;w) of Qπ, the unknown Q-function of some given policy π. The training examples are taken from an observed trajectory x0,x1,x2, . . . with associated rewards r1, r2, . . . where xi denotes state-action tuples xi := (si, ai) and action ai = π(si) is selected according to policy π."
    }, {
      "heading" : "3.1 Stating LSPE, LSTD and BRM with regularization networks",
      "text" : "First, express each of the three problems LSPE in eq. (2), LSTD in eq. (4) and BRM in eq. (5) in more compact matrix form using regularization networks from (8). Assume that the dictionary BV contains m basis functions. Further assume that at time t (after having observed t transitions) a new transition from xt to xt+1 under reward rt+1 is observed. From now on we will use a double index (also for vectors) to indicate the dependence in the number of examples t and the number of basis functions m. Define the matrices:\nKt+1,m :=\n\n \nkm(x0) T\n... km(xt) T\n\n  , Ht+1,m :=\n\n \nkm(x0) T − γkm(x1) T\n... km(xt) T − γkm(xt+1) T\n\n \nrt+1 :=\n\n  r1 ...\nrt+1\n\n  , Λt+1 :=\n\n    \n1 (λγ)1 · · · (λγ)t 0 . . .\n... ... . . . 1 (λγ)1 0 · · · 0 1\n\n    \n(11)\nwhere, as before, m× 1 vector km(·) denotes km(·) = ( k(·, x̃1), . . . , k(·, x̃m) )T ."
    }, {
      "heading" : "3.1.1 The LSPE(λ) method",
      "text" : "Then, for LSPE(λ), the least-squares problem (2) is stated as (wtm being the weight vector of the previous step):\nŵt+1,m = argmin w\n{\n∥ ∥Kt+1,mw −Kt+1,mwtm −Λt+1 ( rt+1 −Ht+1,mwtm )∥ ∥ 2\n+σ2(w −wtm) TKmm(w −wtm)\n}\nComputing the derivative wrt w and setting it to zero, one obtains for ŵt+1,m:\nŵt+1,m = wtm + ( KTt+1,mKt+1,m + σ 2Kmm )−1( ZTt+1,mrt+1 − Z T t+1,mHt+1,mwtm )\nwhere in the last line we have substituted Zt+1,m := Λ T t+1Kt+1,m. From (3) the next iterate wt+1,m for the weight vector in LSPE(λ) is thus obtained by\nwt+1,m = wtm + ηt(ŵt+1,m −wtm) = wtm + ηt ( KTt+1,mKt+1,m + σ 2Kmm )−1\n(\nZTt+1,mrt+1 − Z T t+1,mHt+1,mwtm )\n(12)"
    }, {
      "heading" : "3.1.2 The LSTD(λ) method",
      "text" : "Likewise, for LSTD(λ), the fixed point equation (4) is stated as:\nŵ = argmin w\n{\n∥ ∥Kt+1,mw −Kt+1,mŵ −Λt+1 ( rt+1 −Ht+1,mŵ ) ∥ ∥ 2\n+σ2wTKmmw } .\nComputing the derivative with respect to w and setting it to zero, one obtains\n(\nZTt+1,mHt+1,m + σ 2Kmm\n)\nŵ = ZTt+1,mrt+1.\nThus the solution wt+1,m to the fixed point equation in LSTD(λ) is given by:\nwt+1,m = ( ZTt+1,mHt+1,m + σ 2Kmm )−1 ZTt+1,mrt+1 (13)"
    }, {
      "heading" : "3.1.3 The BRM method",
      "text" : "Finally, for the case of BRM, the least-squares problem (5) is stated as:\nwt+1,m = argmin w\n{\n‖rt+1 −Ht+1,mw‖ 2 + σ2wTKmmw\n}\nThus again, one obtains the weight vector wt+1,m by\nwt+1,m = ( HTt+1,mHt+1,m + σ 2Kmm )−1 HTt+1,mrt+1 (14)"
    }, {
      "heading" : "3.2 Outline of the recursive implementation",
      "text" : "Note that all three methods amount to solving a very similar structured set of linear equations in eqs. (12),(13),(14). Overloading the notation these can be compactly stated as:\n• LSPE: solve\nwt+1,m = wtm + ηP −1 t+1,m(bt+1,m −At+1,mwtm) (12’)\nwhere\n– P−1t+1,m := (K T t+1,mKt+1,m + σ 2Kmm) −1\n– bt+1,m := Z T t+1,mrt+1\n– At+1,m := Z T t+1,mHt+1,m\n• LSTD: solve\nwt+1,m = P −1 t+1,mbt+1,m (13’)\nwhere\n– P−1t+1,m := (Z T t+1,mHt+1,m + σ 2Kmm) −1\n– bt+1,m := Z T t+1,mrt+1\n• BRM: solve\nwt+1,m = P −1 t+1,mbt+1,m (14’)\nwhere\n– P−1t+1,m := (H T t+1,mHt+1,m + σ 2Kmm) −1\n– bt+1,m := H T t+1,mrt+1\nEach time a new transitions from xt to xt+1 under reward rt+1 is observed, the goal is to recursively\n1. update the weight vector wtm, and\n2. possibly augment the model, adding a new basis function (centered on xt+1) to the set of currently selected basis functions BV.\nMore specifically, we will perform one or both of the following update operations:\n1. Normal step: Process (xt+1, rt+1) using the current fixed set of basis functions BV.\n2. Growing step: If the new example is sufficiently different from the previous examples in BV (i.e. the reconstruction error in (9) exceeds a given threshold) and strongly contributes to the solution of the problem (i.e. the decrease of the loss when adding the new basis function is greater than a given threshold) then the current example is added to BV and the number of basis functions in the model is increased by one.\nThe update operations work along the lines of recursive least squares (RLS), i.e. propagate forward the inverse2 of the m×m cross product matrix Ptm. Integral to the derivation of these updates are two well-known matrix identities for recursively computing the inverse of a matrix: (for matrices with compatible dimensions)\nif Bt+1 = Bt + bb T then B−1t+1 = B −1 t −\nB−1t bb TB−1t 1 + bTB−1t b (15)\nwhich is used when adding a row to the data matrix. Likewise,\nif Bt+1 =\n[\nBt b bT b∗\n]\nthen B−1t+1 =\n[\nB−1t 0 0 0\n]\n+ 1\n∆b\n[\n−B−1t b 1\n] [\n−B−1t b 1\n]T\n(16)\nwith ∆b = b ∗ − bTB−1t b. This second update is used when adding a column to the data matrix. An outline of the general implementation applicable to all three of the methods LSPE, LSTD, and BRM is sketched in Figure 2. To avoid unnecessary repetitions we will here only derive the update equations for the BRM method; the other two are obtained with very minor modifications and are summarized in the appendix."
    }, {
      "heading" : "3.3 Derivation of recursive updates for the case BRM",
      "text" : "Let t be the current time step, (xt+1, rt+1) the currently observed input-output pair and assume that from the past t examples {(xi, ri)} t i=1 the m examples {x̃i} m i=1 were selected into the dictionary BV. Consider the penalized least-squares problem that is BRM (restated here for clarity)\nmin w∈Rm\nJtm(w) = ‖rt −Htmw‖ 2 + σ2wTKmmw (17)\nwith Htm being the t×m data matrix and rt being the t× 1 vector of the observed output values from (11). Defining the m ×m cross product matrix Ptm = (H T tmHtm + σ\n2Kmm), the solution to (17) is given by\nwtm = P −1 tmH T tmrt.\nFinally, introduce the costs ξtm = Jtm(wtm). Assuming that {P −1 tm , wtm, ξtm} are known from previous computations, every time a new transition (xt+1, rt+1) is observed, we will perform one or both of the following update operations:\n3.3.1 Normal step: from {P−1tm ,wtm, ξtm} to {P −1 t+1,m,wt+1,m, ξt+1,m}\nWith ht+1 defined as ht+1 := ( km(xt)− γkm(xt+1) )T , one gets\nHt+1,m =\n[\nHtm hTt+1\n]\nand rt+1 =\n[\nrt rt+1\n]\n.\n2. A better alternative (from the standpoint of numerical implementation) would be to not propagate forward the inverse, but instead to work with the Cholesky factor. For this paper we chose the first method in the first place because it gives consistent update formulas for all three considered problems (note that for LSTD the cross-product matrix is not symmetric) and overall allows a better exposition. For details on the second way, see e.g. (Sayed, 2003).\nThus Pt+1,m = Ptm + ht+1h T t+1 and we obtain from (15) the well-known RLS updates\nP−1t+1,m = P −1 tm −\nP−1tmht+1h T t+1P −1 tm\n∆ (18)\nwith scalar ∆ = 1 + hTt+1P −1 tmht+1 and\nwt+1,m = wtm + ̺\n∆ P−1tmht+1 (19)\nwith scalar ̺ = rt+1 − h T t+1wtm. The costs become ξt+1,m = ξtm + ̺2\n∆ . The set of basis functions BV is not altered during this step. Operation complexity is O(m2).\n3.3.2 Growing step: from {P−1t+1,m,wt+1,m, ξt+1,m} to {P −1 t+1,m+1,wt+1,m+1, ξt+1,m+1}\nHow to add a BV. When adding a basis function (centered on xt+1) to the model, we augment the set BV with x̃m+1 (note that x̃m+1 is the same as xt+1 from above). Define kt+1 := km(x̃m+1), k ∗ t := k(xt,xt+1), and k ∗ t+1 := k(xt+1,xt+1). Adding a basis function means appending a new (t + 1) × 1 vector q to the data matrix and appending kt+1 as row/column to the penalty matrix Kmm, thus\nPt+1,m+1 = [ Ht+1,m q ]T [ Ht+1,m q ] + σ2 [\nKmm kt+1 kTt+1 k ∗ t+1\n]\n.\nInvoking (16) we obtain the updated inverse P−1t+1,m+1 via\nP−1t+1,m+1 =\n[\nP−1t+1,m 0\n0 0\n]\n+ 1\n∆b\n[\n−wb 1\n] [\n−wb 1\n]T\n(20)\nwhere simple vector algebra reveals that\nwb = P −1 t+1,m(H T t+1,mq+ σ 2kt+1) ∆b = q Tq+ σ2k∗t+1 − (H T t+1,mq+ σ 2kt+1) Twb. (21)\nWithout sparse online approximation this step would require us to recall all t past examples and would come at the undesirable price of O(tm) operations. However, we are going to get away with merely O(m) operations and only need to access the m past examples in the memorized BV. Due to the sparse online approximation, q is actually of the form qT = [\nHtmat+1 h ∗ t+1 ]T with h∗t+1 := k ∗ t −γk ∗ t+1 and at+1 = K −1 mmkt+1 (see Section 2.2.3).\nHence new information is injected only through the last component. Exploiting this special structure of q equation (21) becomes\nwb = at+1 + δh ∆ P−1tmht+1 ∆b = δ2h ∆ + σ2δh (22)\nwhere δh = h ∗ t+1 − h T t+1at+1. If we cache and reuse those terms already computed in the preceding step (see Section 3.3.1) then we can obtain wb,∆b in O(m) operations.\nTo obtain the updated coefficients wt+1,m+1 we postmultiply (20) by H T t+1,m+1rt+1 =\n[\nHTt+1,mrt+1 q Trt+1\n]T , getting\nwt+1,m+1 =\n[\nwtm 0\n]\n+ κ\n[\n−wb 1\n]\n(23)\nwhere scalar κ is defined by κ = rTt+1(q − Ht+1,mwb)/∆b. Again we can now exploit the special structure of q to show that κ is equal to\nκ = − δh̺\n∆b∆\nAnd again we can reuse terms computed in the previous step (see Section 3.3.1). Skipping the computations, we can show that the reduced (regularized) cost ξt+1,m+1 is recursively obtained from ξt+1,m via the expression:\nξt+1,m+1 = ξt+1,m − κ 2∆b. (24)\nFinally, each time we add an example to the BV set we must also update the inverse kernel matrix K−1mm needed during the computation of at+1 and δh. This can be done using the formula for partitioned matrix inverses (16):\nK−1m+1,m+1 =\n[\nK−1mm 0 0T 0\n]\n+ 1\nδ\n[\n−at+1 1\n] [\n−at+1 1\n]T\n. (25)\nWhen to add a BV. To decide whether or not the current example xt+1 should be added to the BV set, we employ the supervised two-part criterion from (Jung and Polani, 2006). The first part measures the ‘novelty’ of the current example: only examples that are ‘far’ from those already stored in the BV set are considered for inclusion. To this end we compute as in (Csató and Opper, 2001) the squared norm of the residual from projecting (in RKHS) the example onto the span of the current BV set, i.e. we compute, restated from (9), δ = k∗t+1 − k T t+1at+1. If δ < TOL1 for a given threshold TOL1, then xt+1 is well represented by the given BV set and its inclusion would not contribute much to reduce the error from approximating the kernel by the reduced set. On the other hand, if δ > TOL1 then xt+1 is not well represented by the current BV set and leaving it behind could incur a large error in the approximation of the kernel.\nAside from novelty, we consider as second part of the selection criterion the ‘usefulness’ of a basis function candidate. Usefulness is taken to be its contribution to the reduction of the regularized costs ξtm, i.e. the term κ\n2∆b from (24). Both parts together are combined into one rule: only if δ > TOL1 and δκ2∆b > TOL2, then the current example will become a new basis function and will be added to BV."
    }, {
      "heading" : "4. RoboCup-keepaway as RL benchmark",
      "text" : "The experimental work we carried out for this article uses the publicly available3 keepaway framework from (Stone et al., 2005), which is built on top of the standard RoboCup soccer simulator also used for official competitions (Noda et al., 1998). Agents in RoboCup are autonomous entities; they sense and act independently and asynchronously, run as individual processes and cannot communicate directly. Agents receive visual perceptions every 150 msec and may act once every 100 msec. The state description consists of relative distances and angles to visible objects in the world, such as the ball, other agents or fixed beacons for localization. In addition, random noise affects both the agents sensors as well as their actuators.\nIn keepaway, one team of ‘keepers’ must learn how to maximize the time they can control the ball within a limited region of the field against an opposing team of ‘takers’. Only the keepers are allowed to learn, the behavior of the takers is governed by a fixed set\n3. Sources are available from http://www.cs.utexas.edu/users/AustinVilla/sim/keepaway/ .\nof hand-coded rules. However, each keeper only learns individually from its own (noisy) actions and its own (noisy) perceptions of the world. The decision-making happens at an intermediate level using multi-step macro-actions; the keeper currently controlling the ball must decide between holding the ball or passing it to one of its teammates. The remaining keepers automatically try to position themselves such to best receive a pass. The task is episodic; it starts with the keepers controlling the ball and continues as long as neither the ball leaves the region nor the takers succeed in gaining control. Thus the goal for RL is to maximize the overall duration of an episode. The immediate reward is the time that passes between individual calls to the acting agent.\nFor our work, we consider as in (Stone et al., 2005) the special 3vs2 keepaway problem (i.e. three learning keepers against two takers) played in a 20x20m field. In this case the continuous state space has dimensionality 13, and the discrete action space consists of the three different actions hold, pass to teammate-1, pass to teammate-2 (see Figure 3). More generally, larger instantiations of keepaway would also be possible, like e.g. 4vs3, 5vs4 or more, resulting in even larger state- and action spaces."
    }, {
      "heading" : "5. Experiments",
      "text" : "In this section we are finally ready to apply our proposed approach to the keepaway problem. We implemented and compared two different variations of the basic algorithm in a policy iteration based framework: (a) Optimistic policy iteration using LSPE(λ) and (b) Actorcritic policy iteration using LSTD(λ). As baseline method we used Sarsa(λ) with tilecoding, which we re-implemented from (Stone et al., 2005) as faithfully as possible. Initially, we also tried to employ BRM instead of LSTD in the actor-critic framework. However, this set-up did not fare well in our experiments because of the stochastic state-transitions in keepaway (resulting in highly variable outcomes) and BRM’s inability to deal with this situation adequately. Thus, the results for BRM are not reported here.\nOptimistic policy iteration. Sarsa(λ) and LSPE(λ) paired with optimistic policy iteration is an on-policy learning method, meaning that the learning procedure estimates the Q-values from and for the current policy being executed by the agent. At the same time, the\nagent continually updates the policy according to the changing estimates of the Q-function. Thus policy evaluation and improvement are tightly interwoven. Optimistic policy iteration (OPI) is an online method that immediately processes the observed transitions as they become available from the agent interacting with the environment (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998).\nActor-critic. In contrast, LSTD(λ) paired with actor-critic is an off-policy learning method adhering with more rigor to the policy iteration framework. Here the learning procedure estimates the Q-values for a fixed policy, i.e. a policy that is not continually modified to reflect the changing estimates of Q. Instead, one collects a large number of state transitions under the same policy and estimates Q from these training examples. In OPI, where the most recent version of the Q-function is used to derive the next control action, only one network is required to represent Q and make the predictions. In contrast, the actor-critic framework maintains two instantiations of regularization networks: one (the actor) is used to represent the Q-function learned during the previous policy evaluation step and which is now used to represent the current policy, i.e. control actions are derived using its predictions. The second network (the critic) is used to represent the current Q-function and is updated regularly.\nOne advantage of the actor-critic approach is that we can reuse the same set of observed transitions to evaluate different policies, as proposed in (Lagoudakis and Parr, 2003). We maintain an ever-growing list of all transitions observed from the learning agent (irrespective of the policy), and use it to evaluate the current policy with LSTD(λ). To reflect the realtime nature of learning in RoboCup, where we can only carry out a very small amount of computations during one single function call to the agent, we evaluate the transitions in small batches (20 examples per step). Once we have completed evaluating all training examples in the list, the critic network is copied to the actor network and we can proceed to the next iteration, starting anew to process the examples, using this time a new policy.\nPolicy improvement and ε-greedy action selection. To carry out policy improvement, every time we need to determine a control action for an arbitrary state s∗, we choose the action a∗ that achieves the maximum Q-value; that is, given weights wk and a set of basis functions {x̃1, . . . , x̃m}, we choose\na∗ = argmax a Q̃(s∗, a;wk) = argmax a km(s ∗, a)Twk.\nSometimes however, instead of choosing the best (greedy) action, it is recommended to try out an alternative (non-greedy) action to ensure sufficient exploration. Here we employ the ε-greedy selection scheme; we choose a random action with a small probability ε ( ε = 0.01), otherwise we pick the greedy action with probability 1− ε. Taking a random action usually means to choose among all possible actions with equal probability.\nUnder the standard assumption for errors in Bayesian regression (e.g., see Rasmussen and Williams, 2006), namely that the observed target values differ from the true function values by an additive noise term (i.i.d. Gaussian noise with zero mean and uniform variance), it is also possible to obtain an expression for the ‘predictive variance’ which measures the uncertainty associated with value predictions. The availability of such confidence intervals (which is possible for the direct least-squares problems LSPE and also BRM) could be used, as suggested\nin (Engel et al., 2005a), to guide the choice of actions during exploration and to increase the overall performance. For the purpose of solving the keepaway problem however, our initial experiments showed no measurable increase in performance when including this additional feature.\nRemaining parameters. Since the kernel is defined for state-action tuples, we employ a product kernel k([s, a], [s′, a′]) = kS(s, s ′)kA(a, a ′) as suggested by Engel et al. (2005a). The action kernel kA(a, a ′) is taken to be the Kronecker delta, since the actions in keepaway are discrete and disparate. As state kernel kS(s, s ′) we chose the Gaussian RBF kS(s, s\n′) = exp(−h ‖s− s′‖2) with uniform length-scale h−1 = 0.2. The other parameters were set to: regularization σ2 = 0.1, discount factor for RL γ = 0.99, λ = 0.5, and LSPE step size ηt = 0.5. The novelty parameter for basis selection was set to TOL1 = 0.1. For the usefulness part we tried out different values to examine the effect supervised basis selection has; we started with TOL2 = 0 corresponding to the unsupervised case and then began increasing the tolerance, considering alternatively the settings TOL2 = 0.001 and TOL2 = 0.01. Since in the case of LSTD we are not directly solving a least-squares problem, we use the associated BRM formulation to obtain an expression for the error reduction in the supervised basis selection. Due to the very long runtime of the simulations (simulating one hour in the soccer server roughly takes one hour real time on a standard PC) we could not try out many different parameter combinations. The parameters governing RL were set according to our experiences with smaller problems and are in the range typically reported in the literature. The parameters governing the choice of the kernel (i.e. the length-scale of the Gaussian RBF) was chosen such that for the unsupervised case (TOL2 = 0) the number of selected basis functions approaches the maximum number of basis functions the CPU used for these the experiments was able to process in real-time. This number was determined to be ∼ 1400 (on a standard 2 GHz PC).\nResults. We evaluate every algorithm/parameter configuration using 5 independent runs. The learning curves for these runs are shown in Figure 4. The curves plot the average time the keepers are able to keep the ball (corresponding to the performance) against the simulated time the keepers were learning (roughly corresponding to the observed training examples). Additionally, two horizontal lines indicate the scores for the two benchmark policies random behavior and optimized hand-coded behavior used in (Stone et al., 2005).\nThe plots show that generally RL is able to learn policies that are at least as effective as the optimized hand-coded behavior. This is indeed quite an achievement, considering that the latter is the product of considerable manual effort. Comparing the three approaches Sarsa, LSPE and LSTD we find that the performance of LSPE is on par with Sarsa. The curves of LSTD tell a different story however; here we are outperforming Sarsa by 25% in terms of performance (in Sarsa the best performance is about 15 seconds, in LSTD the best performance is about 20 seconds). This gain is even more impressive when we consider the time scale at which this behavior is learned; just after a mere 2 hours we are already outperforming hand-coded control. Thus our approach needs far fewer state transitions to discover good behavior. The third observation shows the effectiveness of our proposed supervised basis function selection; here we show that our supervised approach performs as well as the unsupervised one, but requires significantly fewer basis functions to achieve that\nlevel of performance (∼ 700 basis functions at TOL2= 0.01 against 1400 basis functions at TOL2= 0).\nRegarding the unexpectedly weak performance of LSPE in comparison with LSTD, we conjecture that this strongly depends on the underlying architecture of policy iteration (i.e. OPI vs. actor-critic) as well as the specific learning problem. On a related number of experiments carried out with the octopus arm benchmark4 we made exactly the opposite observation (not discussed here in more detail, see Jung and Polani, 2007)."
    }, {
      "heading" : "6. Discussion and related work",
      "text" : "We have presented a kernel-based approach for least-squares based policy evaluation in RL using regularization networks as underlying function approximator. The key point is an efficient supervised basis selection mechanism, which is used to select a subset of relevant basis functions directly from the data stream. The proposed method was particularly devised with high-dimensional, stochastic control tasks for RL in mind; we prove its effectiveness using the RoboCup keepaway benchmark. Overall the results indicate that kernel-based online learning in RL is very well possible and recommendable. Even the rather few simulation runs we made clearly show that our approach is superior to convential function approximation in RL using grid-based tilecoding. What could be even more important is that the kernel-based approach only requires the setting of some fairly general parameters that do not depend on the specific control problem one wants to solve. On the other hand, using tilecoding or a fixed basis function network in high dimensions requires considerable manual effort on part of the programmer to carefully devise problem-specific features and manually choose suitable basis functions.\nEngel et al. (2003, 2005a) initially advocated using kernel-based methods in RL and proposed the related GPTD algorithm. Our method using regularization networks develops this idea further. Both methods have in common the online selection of relevant basis functions based on (Csató and Opper, 2001). As opposed to the unsupervised selection in GPTD, we use a supervised criterion to further reduce the number of relevant basis functions selected. A more fundamental difference is the policy evaluation method addressed by the respective formulation; GPTD models the Bellman residuals and corresponds to the BRM approach (see Section 2.1.2). Thus, in its original formulation GPTD can be only applied to RL problems with deterministic state transitions. In contrast, we provide a unified and concise formulation of LSTD and LSPE which can deal with stochastic state transitions as well. Another difference is the type of benchmark problem used to showcase the respective method; GPTD was demonstrated by learning to control a simulated octopus arm, which was posed as an 88-dimensional control problem (Engel et al., 2005b). Controlling the octopus arm is a deterministic control problem with known state transitions and was solved there using model-based RL. In contrast, 3vs2 keepaway is only a 13-dimensional problem; here however, we have to deal with stochastic and unknown state transitions and need to use model-free RL.\n4. From the ICML06 RL benchmarking page: http://www.cs.mcgill.ca/dprecup/workshops/ICML06/octopus.html"
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors wish to thank the anonymous reviewers for their useful comments and suggestions."
    }, {
      "heading" : "Appendix A. A summary of the updates",
      "text" : "Let xt+1 = (st+1, at+1) be the next state-action tuple and rt+1 be the reward assiociated with transition from the previous state st to st+1 under at. Define the abbreviations:\nkt := km(xt) kt+1 := km(xt+1) ht+1 := kt − γkt+1 k∗t := k(xt,xt+1) k ∗ t+1 := k(xt+1,xt+1) h ∗ t+1 := k ∗ t − γk ∗ t+1\nand at+1 := K −1 mmkt+1.\nA.1 Unsupervised basis selection\nWe want to test if xt+1 is well represented by the current basis functions in the dictionary or if we need to add xt+1 to the basis elements. Compute\nδ = k∗t+1 − k T t+1at+1. (9)\nIf δ < TOL1, then add xt+1 to the dictionary, execute the growing step (see below) and update\nK−1m+1,m+1 =\n[\nK−1mm 0 0T 0\n]\n+ 1\nδ\n[\n−at+1 1\n] [\n−at+1 1\n]T\n. (25)\nA.2 Recursive updates for BRM\n• Normal step {t,m} 7→ {t+ 1,m}:\n1.\nP−1t+1,m = P −1 tm −\nP−1tmht+1h T t+1P −1 tm\n∆ (18)\nwith ∆ = 1 + hTt+1P −1 tmht+1.\n2. wt+1,m = wtm + ̺\n∆ P−1tmht+1 (19)\nwith ̺ = rt+1 − h T t+1wtm.\n• Growing step {t+ 1,m} 7→ {t+ 1,m+ 1}\n1.\nP−1t+1,m+1 =\n[\nP−1t+1,m 0\n0 0\n]\n+ 1\n∆b\n[\n−wb 1\n] [\n−wb 1\n]T\n(20)\nwhere\nwb = at+1 + δh ∆ P−1tmht+1, ∆b = δ2h ∆ + σ2δh, δh = h ∗ t+1 − h T t+1at+1\n2.\nwt+1,m+1 =\n[\nwt+1,m 0\n]\n+ κ\n[\n−wb 1\n]\n(23)\nwhere κ = − δh̺∆b∆ .\n• Reduction of regularized cost when adding xt+1 (supervised basis selection):\nξt+1,m+1 = ξt+1,m − κ 2∆b (24)\nFor supervised basis selection we additionally check if κ2∆b > TOL2.\nA.3 Recursive updates for LSTD(λ)\n• Normal step {t,m} 7→ {t+ 1,m}:\n1. zt+1,m = (γλ)ztm + kt\n2.\nP−1t+1,m = P −1 tm −\nP−1tmzt+1,mh T t+1P −1 tm\n∆ (18’)\nwith ∆ = 1 + hTt+1P −1 tmzt+1,m.\n3. wt+1,m = wtm + ̺\n∆ P−1tmzt+1,m (19’)\nwith ̺ = rt+1 − h T t+1wtm.\n• Growing step {t+ 1,m} 7→ {t+ 1,m+ 1}\n1. zt+1,m+1 = [ zTt+1,m z ∗ t+1,m ]T\nwhere z∗t+1,m = (γλ)z T tmat+1 + k ∗ t .\n2.\nP−1t+1,m+1 =\n[\nP−1t+1,m 0\n0 0\n]\n+ 1\n∆b\n[\n−w (1) b\n1\n]\n[\n−w (2) b 1\n]\n(20’)\nwhere\nw (1) b = at+1 +\nδ(1)\n∆ P−1tmzt+1,m δ (1) = h∗t+1 − a T t+1ht+1\nw (2) b = a T t+1 + δ(2)\n∆ hTt+1P −1 tm δ (2) = z∗t+1,m − a T t+1zt+1,m\nand ∆b = δ(1)δ(2) ∆ + σ 2(k∗t+1 − k T t+1at+1).\n3.\nwt+1,m+1 =\n[\nwt+1,m 0\n]\n+ κ\n[\n−w (1) b\n1\n]\n(23’)\nwhere κ = − δ (2)̺ ∆b∆ .\nA.4 Recursive updates for LSPE(λ)\n• Normal step {t,m} 7→ {t+ 1,m}:\n1.\nzt+1,m = (γλ)ztm + kt+1\nAt+1,m = Atm + zt+1,mh T\nt+1\nbt+1,m = btm + zt+1,mrt+1\n2.\nP−1t+1,m = P −1 tm −\nP−1tmkt+1k T t+1P −1 tm\n∆ (18”)\nwith ∆ = 1 + kTt+1P −1 tmkt+1.\n3. wt+1,m = wtm + ηP −1 t+1,m(bt+1,m −At+1,mwtm) (19”)\n• Growing step {t+ 1,m} 7→ {t+ 1,m+ 1}\n1.\nzt+1,m+1 =\n[\nzt+1,m z∗t+1,m\n]\nbt+1,m+1 =\n[\nbt+1,m aTt+1btm + z ∗ t+1,mrt+1\n]\nAt+1,m+1 =\n[\nAt+1,m Atmat+1 + zt+1,mh ∗\naTt+1Atm + z ∗ t+1,mh T t+1 a T t+1Atmat+1 + z ∗ t+1,mh ∗\n]\nwhere z∗t+1,m = (γλ)z T tmat+1 + k ∗ t .\n2.\nP−1t+1,m+1 =\n[\nP−1t+1,m 0\n0 0\n]\n+ 1\n∆b\n[\n−wb 1\n] [\n−wb 1\n]T\n(20”)\nwhere\nwb = at+1 + δ\n∆ P−1tmkt+1, ∆b =\nδ2 ∆ + σ2δ, δ = k∗t − k T t at+1\nand ∆b = δ(1)δ(2) ∆ + σ 2(k∗t+1 − k T t+1at+1).\n3.\nwt+1,m+1 =\n[\nwt+1,m 0\n]\n+ κ\n[\n−w (1) b\n1\n]\n(23”)\nwhere κ = − δ (2)̺ ∆b∆ .\n• Reduction of regularized cost when adding xt+1 (supervised basis selection):\nξt+1,m+1 = ξt+1,m −∆ −1 b (c−w T b d) 2 (24”)\nwhere c = aTt+1(btm−Atmwtm)+z ∗ t+1,m(rt+1−h T t+1wtm) and d = bt+1,m−At+1,mwtm. For supervised basis selection we additionally check if ∆−1b (c−w T b d) 2 > TOL2."
    } ],
    "references" : [ {
      "title" : "Least-squares policy iteration",
      "author" : [ "M.G. Lagoudakis", "R. Parr" ],
      "venue" : "Dynamic Programming and Reinforcement Learning,",
      "citeRegEx" : "Lagoudakis and Parr.,? \\Q2003\\E",
      "shortCiteRegEx" : "Lagoudakis and Parr.",
      "year" : 2003
    }, {
      "title" : "Fundamentals of Adaptive Filtering",
      "author" : [ "A. Sayed" ],
      "venue" : "Wiley Interscience,",
      "citeRegEx" : "Sayed.,? \\Q2003\\E",
      "shortCiteRegEx" : "Sayed.",
      "year" : 2003
    }, {
      "title" : "Reinforcement Learning: An Introduction",
      "author" : [ "R. Sutton", "A. Barto" ],
      "venue" : "keepaway. Adaptive Behavior,",
      "citeRegEx" : "Sutton and Barto.,? \\Q1998\\E",
      "shortCiteRegEx" : "Sutton and Barto.",
      "year" : 1998
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "(2005) successfully applied RL to keepaway, using the textbook approach with online Sarsa(λ) and tilecoding as underlying function approximator (Sutton and Barto, 1998).",
      "startOffset" : 144,
      "endOffset" : 168
    }, {
      "referenceID" : 2,
      "context" : "(2005) successfully applied RL to keepaway, using the textbook approach with online Sarsa(λ) and tilecoding as underlying function approximator (Sutton and Barto, 1998). However, tilecoding is a local method and places parameters (i.e. basis functions) in a regular fashion throughout the entire state space, such that the number of parameters grows exponentially with the dimensionality of the space. In (Stone et al., 2005) this very serious shortcoming was adressed by exploiting problem-specific knowledge of how the various state variables interact. In particular, each state variable was considered independently from the rest. Here, we will demonstrate that one can also learn using the full (untampered) state information, without resorting to simplifying assumptions. In this paper we propose a (non-parametric) kernel-based approach to approximate the value function. The rationale for doing this is that by representing the solution through the data and not by some basis functions chosen before the data becomes available, we can better adapt to the complexity of the unknown function we are trying to estimate. In particular, parameters are not ‘wasted’ on parts of the input space that are never visited. The hope is that thereby the exponential growth of parameters is bypassed. To solve the RL problem of optimal control we consider the framework of approximate policy iteration with the related least-squares based policy evaluation methods LSPE(λ) proposed by Nedić and Bertsekas (2003) and LSTD(λ) proposed by Boyan (1999).",
      "startOffset" : 145,
      "endOffset" : 1505
    }, {
      "referenceID" : 2,
      "context" : "(2005) successfully applied RL to keepaway, using the textbook approach with online Sarsa(λ) and tilecoding as underlying function approximator (Sutton and Barto, 1998). However, tilecoding is a local method and places parameters (i.e. basis functions) in a regular fashion throughout the entire state space, such that the number of parameters grows exponentially with the dimensionality of the space. In (Stone et al., 2005) this very serious shortcoming was adressed by exploiting problem-specific knowledge of how the various state variables interact. In particular, each state variable was considered independently from the rest. Here, we will demonstrate that one can also learn using the full (untampered) state information, without resorting to simplifying assumptions. In this paper we propose a (non-parametric) kernel-based approach to approximate the value function. The rationale for doing this is that by representing the solution through the data and not by some basis functions chosen before the data becomes available, we can better adapt to the complexity of the unknown function we are trying to estimate. In particular, parameters are not ‘wasted’ on parts of the input space that are never visited. The hope is that thereby the exponential growth of parameters is bypassed. To solve the RL problem of optimal control we consider the framework of approximate policy iteration with the related least-squares based policy evaluation methods LSPE(λ) proposed by Nedić and Bertsekas (2003) and LSTD(λ) proposed by Boyan (1999). Least-squares based policy evaluation is ideally suited for the use with linear models and is a very sample-efficient variant of RL.",
      "startOffset" : 145,
      "endOffset" : 1542
    }, {
      "referenceID" : 0,
      "context" : "Computational experiments in (Bertsekas and Ioffe, 1996) or (Lagoudakis and Parr, 2003) indicate that both approaches can perform much better than TD(λ).",
      "startOffset" : 60,
      "endOffset" : 87
    }, {
      "referenceID" : 0,
      "context" : "A third approach, related to LSTD(0) is the direct minimization of the Bellman residuals (BRM), as proposed in (Baird, 1995; Lagoudakis and Parr, 2003).",
      "startOffset" : 111,
      "endOffset" : 151
    }, {
      "referenceID" : 1,
      "context" : "(Sayed, 2003).",
      "startOffset" : 0,
      "endOffset" : 13
    }, {
      "referenceID" : 2,
      "context" : "Optimistic policy iteration (OPI) is an online method that immediately processes the observed transitions as they become available from the agent interacting with the environment (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998).",
      "startOffset" : 179,
      "endOffset" : 235
    }, {
      "referenceID" : 0,
      "context" : "One advantage of the actor-critic approach is that we can reuse the same set of observed transitions to evaluate different policies, as proposed in (Lagoudakis and Parr, 2003).",
      "startOffset" : 148,
      "endOffset" : 175
    } ],
    "year" : 2012,
    "abstractText" : "We apply kernel-based methods to solve the difficult reinforcement learning problem of 3vs2 keepaway in RoboCup simulated soccer. Key challenges in keepaway are the highdimensionality of the state space (rendering conventional discretization-based function approximation like tilecoding infeasible), the stochasticity due to noise and multiple learning agents needing to cooperate (meaning that the exact dynamics of the environment are unknown) and real-time learning (meaning that an efficient online implementation is required). We employ the general framework of approximate policy iteration with least-squares-based policy evaluation. As underlying function approximator we consider the family of regularization networks with subset of regressors approximation. The core of our proposed solution is an efficient recursive implementation with automatic supervised selection of relevant basis functions. Simulation results indicate that the behavior learned through our approach clearly outperforms the best results obtained with tilecoding by Stone et al. (2005).",
    "creator" : "LaTeX with hyperref package"
  }
}