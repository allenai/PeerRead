{
  "name" : "1610.01283.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "EPOpt: Learning Robust Neural Network Policies Using Model Ensembles",
    "authors" : [ "Aravind Rajeswaran", "Sarvjeet Ghotra", "NITK Surathkal", "Sergey Levine", "Balaraman Ravindran" ],
    "emails" : [ "aravraj@cs.washington.edu", "sarvjeet.13it236@nitk.edu.in", "svlevine@eecs.berkeley.edu", "ravi@cse.iitm.ac.in" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Reinforcement learning methods with powerful function approximators such as deep neural networks (deep RL) have recently demonstrated remarkable success in a wide range of simulated tasks like games [1, 2], simulated robotic control problems [3, 4], and graphics [5]. However, high sample complexity poses a major challenge for directly applying these deep RL methods to real-world robotic control. Model-free methods like Q-learning, actor-critic, and policy gradients are known to suffer from long learning times [6], which is compounded when combined with expressive function approximators like deep neural networks. The challenge of gathering samples from the real world is further exacerbated by issues of safety for the agent and environment when sampling with partially learned policies which could be unstable [7]. Thus, model-free deep RL methods often require a prohibitively large number of potentially-dangerous samples for real-world control tasks.\nModel-based methods, where the target domain is approximated with a simulated source domain, provide an avenue to tackle the above challenges by learning policies using simulated data. The principal challenge with simulated training is the systematic discrepancy between source and\ntarget domains. We show that this discrepancy can be mitigated through two key ideas: (1) training on an ensemble of models in an adversarial fashion to learn policies that are robust not only to parametric model errors, but also to unmodeled effects; and (2) adaptation of the source domain distribution using data from target domain, to progressively make it a better approximation. This approach can be viewed as an instance of model-based Bayesian RL [8]; or as an instance of transfer learning from a collection of simulated source domains to a real-world target domain [9].\nStandard model-based RL methods typically operate by finding a maximum-likelihood estimate to the target dynamics model [10, 11], followed by policy optimization. This approach has two drawbacks: (a) For safety-critical systems, a good policy might be required to gather data from the target domain for model identification. Obtaining such a policy is not directly addressed. (b) RL algorithms exploit all aspects of model dynamics to find optimal policies, hence errors in point estimates of model parameters could lead to inadvertent or sub-optimal behaviors in target domain. Previously, Bayesian RL methods have been explored to address these drawbacks [12, 13]. Although such methods are highly general, they have not yet been demonstrated on highdimensional, continuous control tasks of the scale explored in this paper.\nIn this work, we propose the Ensemble Policy Optimization (EPOpt− ) algorithm, which uses an ensemble of source domains and a form of adversarial training to learn robust policies that generalize to a broad range of models. By robust, we primarily mean robustness to parametric model errors and near-optimal performance according to the following metrics: (a) Jumpstart: average initial performance in the target domain; (b) Worst-trajectory: return corresponding to worst-trajectory in the target domain. By adversarial training, we mean that model instances on which the policy performs poorly are sampled more often in order to encourage learning of policies that perform well for a wide range of model instances. This is in contrast to methods that learn policies which are highly optimized for specific model instances, but brittle under model perturbations. Further, we show that policies learned using EPOpt are robust even to effects not modeled in the source domain. We also present an approach for adapting the source domain ensembles using approximate Bayesian updates, in order to ar X iv :1\n61 0.\n01 28\n3v 1\n[ cs\n.L G\n] 5\nO ct\n2 01\n6\nprogressively make it a better approximation of the target domain. In contrast to standard system ID [14], the data for model learning is obtained through execution of a robust policy, and hence alleviates safety concerns during model identification. Thus, we are able to enjoy the benefits of both robustness and adaptation as outlined above. In addition, we leverage recent advances in fast physics simulators, deep learning, and policy search to overcome computational bottlenecks in policy learning for high-dimensional continuous control tasks. We evaluate the proposed methods on the hopper (12 dimensional state space; 3 dimensional action space) and half-cheetah (18 dimensional state space; 6 dimensional action space) benchmarks, further details about which are presented in section 5. Our experimental results suggest that (a) adversarial training on model ensembles produces robust policies which generalize much better than policies trained on the maximum-likelihood model from the ensemble; (b) policies trained on an ensemble perform better than policies trained on the best model within considered ensemble in presence of unmodeled effects."
    }, {
      "heading" : "2 Related Work",
      "text" : "The general problem of finding reliable policies using imprecise or inadequate model information is ubiquitous, and has been attempted many times under different assumptions and settings. Robust control is a branch of control theory which formally studies development of robust policies [15, 16]. However, typically no distribution assumption over source or target tasks is assumed, and a worst case analysis is performed. Much of the work in this community has been concentrated around linear systems or finite MDPs, which often cannot adequately model complexities of real-world tasks [17].\nThe broad field of model-based Bayesian RL maintains a belief over models for decision making under uncertainty [8, 13]. In Bayesian RL, through interaction with the target domain, the uncertainty is reduced to find the correct or closest model. Application of this idea in its full general form is difficult, and requires either restrictive assumptions like finite MDPs [18], Gaussian dynamics [19], or task specific innovations. Some previous methods have also suggested treating uncertain model parameters as unobserved state variables in a continuous POMDP framework, and solving the POMDP to get optimal explorationexploitation trade-off [20, 21]. While this approach is general, and allows automatic learning of epistemic actions, extending such methods to large continuous control tasks like those considered in this paper is difficult.\nUse of model ensembles to produce robust controllers has been explored recently in robotics. Mordatch et al. [22] use model based trajectory optimization and an ensemble with small finite set of models, whereas we follow a sampling based direct policy search approach over a continuous distribution of uncertain parameters and also show domain adaptation. Sampling based approaches can be applied easily to complex models and discrete MDPs which cannot be planned through easily.\nAnother related theme of work involves learning a policy subspace using the source domain distribution, in order\nto reduce sample complexity when interacting with the target domain. Kolter [23] identified that parameters of optimal policies to MDPs in the source distribution live in some low dimensional subspace, and hence policy search can be performed in this lower dimensional space with data from the target domain. Learning of parametrized skills [24] is also concerned with finding policies for a distribution of parametrized tasks. However, this setting is primarily geared towards situations where task parameters are revealed during test time, whereas our work is motivated by situation where parameters (e.g. friction) are unknown. A number of methods have also been suggested to reduce sample complexity when provided with either a baseline policy [25, 26], expert demonstration [27, 28], or approximate simulator [29, 30]. These are complimentary to our work, in the sense that our policy, which has good jumpstart performance, can be used to sample from the target domain and other off-policy methods could be explored for policy improvement."
    }, {
      "heading" : "3 Problem Formulation",
      "text" : "We consider parametrized Markov Decision Processes (MDPs), which are tuples of the form: M(p) ≡< S,A, Tp,Rp, γ, S0,p > where S, A are (continuous) states and actions respectively; Tp andRp are the state transition and reward functions, both parametrized by p; γ is the discount factor; and S0,p is the initial state distribution parametrized by p. Thus, we consider a set of MDPs with the same state and action spaces, with each MDP in the set instantiated by a parameter vector p. Each MDP in this set could potentially have different transition functions, rewards, and initial state distributions. We use transition functions of the form St+1 ≡ Tp(st, at) where Tp is a random process and St+1 is a random variable.\nWe distinguish between source and target MDPs usingM and W respectively. Our ultimate objective is to learn the optimal policy forW , and to do so we have access toM(p) for different choices of p. More concretely, we assume that we have a distribution (D) over the source domains (MDPs) generated by a distribution over the parameters P ≡ P(p) that capture our subjective belief about the parameters ofW . Let this distribution over parameters, P , be parametrized by ψ (e.g. mean, standard deviation). For example, M could be a hopping task with reward proportional to hopping velocity and falling down corresponds to a terminal state. For this task, p could correspond to parameters like torso mass, ground friction, and damping in joints, all of which affect the dynamics. Ideally, we would like the target domain to be in the same class ofM(p). However, in practice, there are likely to be unmodeled effects, and we analyze this setting in the experiments section.\nFor simplicity, we denote the source domain distribution withD ≡M(P ) and refer to it as the ensemble MDP. Here, D is both a distribution over MDPs as well as an MDP (p is considered a state with no transition dynamics i.e. pt+1 = pt; and initial p0 ∼ P). We wish to learn a policy at = π∗(st; θ) which will perform well for all (or most) M ∼ D. Note that this robust policy does not have an explicit dependence on p, and we require it to perform well on M∼ D without knowledge of p."
    }, {
      "heading" : "4 Learning protocol and EPOpt algorithm",
      "text" : "We follow the round-based (or episodic) learning protocol described in Algorithm 1, which is similar to Bayesian model-based RL. In each round, we interact with the target domain after computing a robust policy on the simulated source domain distribution. Following this, we update the source domain distribution using data from the target domain collected by executing the robust policy.\nAlgorithm 1: Robust model-based learning protocol 1 Input: ψ0, θ0 2 Compute robust policy π(θ0) on source domain\ndistribution D ≡M(P ) with P(p) = Pψ0 3 for round i = 0, 1, 2, . . . do 4 Interact withW to sample a trajectory τi = {st, at, rt, st+1}T−1t=0 using π(θi) 5 ψi+1 ← BeliefUpdate(ψi, τi) 6 θi+1 ← RobustPolicyUpdate(ψi+1, θi) 7 end\nRobust policy search We introduce the EPOpt algorithm for finding the robust policy for a given source domain distribution. EPOpt is a policy gradient based meta-algorithm which uses standard batch policy optimization methods as a subroutine. The basic idea is to sample a collection of models from the source domain distribution, sample trajectories from each of these models, and make a gradient update based on the sampled trajectories. We first define evaluation metrics for the parametrized policy, π(θ):\nηM(θ, p) = Eτ̃ [ T−1∑ t=0 γtrt(st, at) ∣∣∣∣ p ] ,\nηD(θ) = Ep∼P [ηM(θ, p)]\n= Ep∼P [ Eτ̂ [ T−1∑ t=0 γtrt(st, at) ∣∣∣∣ p ]]\n= Eτ [ T−1∑ t=0 γtrt(st, at) ] .\n(1)\nIn (1), ηM(θ, p) is the evaluation of π(θ) on the model M(p), with τ̃ being trajectories generated by M(p) and π(θ): τ̃ = {st, at, rt, st+1}T−1t=0 where st+1 ∼ Tp(st, at), s0 ∼ S0,p, rt ∼ Rp(st, at), and at ∼ π(st; θ). Similarly, ηD(θ) is the evaluation of π(θ) over the source domain distribution. The corresponding expectation is over trajectories τ generated by D and π(θ): τ = {st, at, rt, st+1}T−1t=0 , where st+1 ∼ Tpt(st, at), pt+1 = pt, s0 ∼ S0,p0 , rt ∼ Rpt(st, at), at ∼ π(st; θ), and p0 ∼ P . With this modified notation of trajectories, policy gradient methods can be employed to find the optimal policy parameters: θ∗ = argmaxθ ηD(θ) (e.g. [31], [32], [33]). For simplicity, we refer to such policy gradient methods as batch policy optimization.\nOptimizing ηD allows us to learn a policy that performs best in expectation over models in the source domain distribution. However, this does not necessarily lead to a robust policy, since there could be high variability in performance for different models in the distribution. To explicitly seek a robust policy, we use a softer version of max-min objective used in robust control:\nmax θ,y ∫ y −∞ ηM(θ, p)P(p)dp\ns.t. P (F(θ) ≤ y) = , (2)\nwhere F(θ) ≡ ηM(θ, P ) and P are random variables. is a hyperparameter which governs the level of relaxation from max-min objective. The interpretation is that (2) maximizes the expected return for the worst -percentile of MDPs in the source domain distribution. A related line of work is percentile optimization, where the -percentile value of return is directly optimized [34]. Most prior work in this area is confined to small and discrete MDPs, and finding the optimal policy under the chance constrained formulation is NP-hard for general MDPs. We also refer readers to [7] for a survey of related risk sensitive RL methods in the context of safety and robustness.\nWe adapt the previous policy gradient formulation to approximately optimize the objective in (2). The resulting algorithm, which we call EPOpt- , generalizes learning a policy using an ensemble of source MDPs which are sampled from a source domain distribution. In Algorithm 2, R(τk) ≡ ∑T−1 t=0 γ\ntrt,k denotes the discounted return obtained in trajectory sample τk. In line 7, we compute the −percentile value of returns from the N trajectories. In line 8, we find the subset of sampled trajectories which have returns lower thanQ , and make a gradient update using this subset in line 9.\nAlgorithm 2: EPOpt– 1 Input: ψ, θ0, niter, N , 2 for iteration i = 0, 1, 2, . . . niter do 3 for k = 1, 2, . . . N do 4 sample model parameters pk ∼ Pψ 5 sample a trajectory τk = {st, at, rt, st+1}T−1t=0 fromM(pk) using policy π(θi) 6 end 7 compute Q = percentile of {R(τk)}Nk=1 8 select sub-set T = {τk : R(τk) ≤ Q } 9 Update policy: θi+1 = BatchPolOpt(θi,T)\n10 end\nAdapting the source domain distribution\nIn line with model-based Bayesian RL, we can adapt the ensemble distribution after observing trajectory data from the target domain. The Bayesian update can be written as:\nP(P |τk) = 1\nZ × P(τk|P )× P(P )\n= 1 Z × T−1∏ t=0 P(St+1 = st+1|st, at, p)× P(P ),\n(3) where 1Z is the normalization constant required to make the probabilities sum to 1, St+1 is the random variable representing the next state, and st+1 is the observed transition from τk. We try to explain the target trajectory using the stochasticity in the state-transition function, which also models sensor errors. This provides the following expression for the likelihood:\nP(St+1|st, at, p) ≡ Tp(st, at). (4) In our experiments, we consider the case of deterministic simulator and known Gaussian sensor error model, but the approach is general. Based on the Gaussian transition model, we can simplify (4) as: P(St+1|st, at, p) ≡ N (xt+1,Σ) where xt+1 = f(st, at, p) is the next state predicted by the deterministic simulator.\nIn this work, we follow a bin-based approach to source domain adaptation, where the parameter range is discretized into bins, and uniform density is assumed within the bins. Thus, each bin is now associated with a probability, which can be updated in accordance with Bayes rule. Let bi denote the ith bin. According to Bayes rule,\nP(bi|τk) = 1\nZ × L(bi, τk)× P(bi), (5)\nwhere L(bi, τk) is the likelihood of models in bi generating τk. The likelihood is given by L(bi, τk) = EPbi [∏ t P(St+1 = st+1|st, at, p) ] where the expectation is over drawing parameters (p) from bi, which we take to be uniform. By sampling p from bi, we can estimate the likelihood, which allows us to update the bin probabilities according to (5)."
    }, {
      "heading" : "5 Experiments",
      "text" : "We evaluate the proposed EPOpt- algorithm on 2D hopper and half-cheetah simulated robotic tasks using the MuJoCo physics simulator [35]. Both tasks involves complex second order dynamics and direct torque control. The tasks were implemented using base code provided with OpenAI gym [36] and rllab [37]. We use TRPO [31] for our batch policy optimization subroutine. These tasks were chosen since falling down is a natural interpretation for failure and lack of robustness. Descriptions of these tasks are below: Hopper: The hopper task is to make a 2D planar hopper with three joints and 4 body parts hop forward as fast as possible [38]. This problem has a 12 dimensional state space and a 3 dimensional action space that corresponds to torques at the joints. We construct the source domain by considering a distribution over 4 parameters: torso mass, ground friction, armature (inertia), and damping of foot. Half Cheetah: The half-cheetah task [39] requires us to make a 2D cheetah with two legs run forward as fast as possible. The simulated robot has 8 body links with an 18 dimensional state space and a 6 dimensional action space that\ncorresponds to joint torques. Again, we construct the source domain using a distribution over the following parameters: torso and head mass, ground friction, damping, and armature (inertia) of foot joints.\nWe parametrize the stochastic policy using the scheme presented in Schulman et al. [31]. The policy is comprised of a Gaussian distribution, the mean of which is represented using a neural network with two hidden layers. Each hidden layer has 64 units, with a tanh non-linearity, and the final output layer is made of linear units. Normally distributed multivariate random variables (with diagonal covariance) are added to the output of this neural network, and we also learn the standard deviation of this distribution.\nIn the first experiment, we demonstrate the need for robustness using the hopper task, by learning policies for different model instances, followed by evaluating the learned policy on MDPs with perturbed parameters. Figure 2 illustrates the performance of control policies learned on different hopper configurations, which in this case correspond to different torso weights. We clearly see that there is no single task configuration, which if trained on, produces a policy that generalizes to a broad range of task parameters or configurations. Hence, an attractive approach to generate robust policies, meaning competent for multiple configurations, is to consider an ensemble of models and learn a single policy over such an ensemble. In Figure 2, we see that such a policy, trained over an ensemble, is robust and generalizes to all the parameters considered in the source distribution.\nNext, we analyze the robustness of policies trained using EPOpt for a range of parameters. Figures 3 and 4 compare the performance of policies obtained using: (a) batch policy optimization on the average or most-likely model of source domain distribution; (b) EPOpt( = 1) on source domain distribution – i.e. best policy in expectation over source domain distribution and (c) EPOpt( = 0.1) on source domain distribution – i.e. adversarially trained policy. The comparison is similar to what is done in robust control settings\nwhere a policy is trained on an initial source distribution (or range), and evaluated on the same range, without any adaptation. This analysis allows us to understand the quality of learned policy with regards to the jumpstart criteria. The\ntruncated Gaussian distribution described in Table 1 was used as the source domain distribution. It is observed that policies trained on the ensemble generalize better, especially when trained adverserially.\nTo analyze the robustness of the policies trained using EPOpt to unmodeled effects, our next experiment considers a setting where the torso mass variation is unmodeled. Specifically, the torso mass differs between source and target domains, with all models in the source domain having the same incorrect torso mass. The source domain distribution is obtained by varying the three other parameters in Table 1: ground friction, joint damping, and joint armature. Figure 5 indicates that EPOpt( = 0.1) policy is robust to\na broad range of torso masses even when its uncertainty is not considered. However, as expected, this policy is not as robust as the case when mass is also modeled as part of the source domain distribution.\nThe preceding experiments show that EPOpt can find robust policies, but the source domain distribution in these ex-\nperiments was chosen to be broad enough such that the target domain is not too far from high-density regions of the distribution. However, for real-world problems, we might not have the domain knowledge to identify a good source domain distribution in advance. In such settings, domain adaptation allows us to change the parameters of source distribution using data gathered from the target domain. Additionally, domain adaptation is helpful when the parameters of target domain could change over time – for example through wear and tear. To illustrate domain adaptation, we perform an experiment where the target domain is very far from the high density regions of the source domain, as depicted in Figure 6. We observe that progressively, the source domain becomes a better approximation of the target domain and consequently the performance improves. In this case, we initially observe a bimodal distribution, since the data from the target domain can be explained in two ways. When the mass is underestimated, under-estimating the ground friction allows for explaining the additional slippage of a higher mass body. Similarly, over-estimating the friction partially nullifies the effect of over-estimating the mass. Eventually, after 11 iterations (episodes), the source domain distribution identifies the correct bin with near-1 probability. Figure 7 depicts the learning curve, and we see that a robust policy with return more than 2000 can be discovered with just 3 trajectories from the target domain. Subsequently, the policy improves near monotonically, and we find a good policy with just 12 episodes worth of data from the target domain. Also, we learned a policy directly on the target domain, and observed that the final return achieved under this perfectly known model setting is approximately 3200, comparable to what is achieved by following the robust model-based learning protocol. Thus, maintaining a distribution over models and learning a robust policy, followed by domain adaptation allows us to learn both quickly and robustly.\nDuring our experiments with the EPOpt- algorithm, we found that directly optimizing the policy for a small value of leads to unstable learning. This is likely because, policy gradient methods try to increase the probability of better performing trajectories and penalize trajectories that perform poorly, and due to the adversarial nature, EPOpt- emphasizes penalizing poor trajectories more. This might constrain the initial exploration needed to find the better trajectories, and encourage stable learning. However, our experiments also indicate that choosing a low value for (≈ 0.1) leads to better generalization. This difficulty in training can be overcome by gradually reducing over multiple iterations till we get to the desired relaxation level. This scheme roughly corresponds to exploring extensively initially to find promising trajectories, and then rapidly reducing probability of those trajectories that do not generalize – i.e. perform poorly for some model instances."
    }, {
      "heading" : "6 Conclusions and Future Work",
      "text" : "In this paper, we presented the EPOpt- algorithm for training robust policies on ensembles of source domains. Our method provides for robust policy training by using a distribution of models at training time, and supports an adversarial training regime designed to provide good jumpstart\nand worst-case performance. We also describe how our approach can be combined with Bayesian model adaptation to adapt the source domain ensemble to a target domain using a small amount of target domain experience. Our algorithm can be used to train robust and generalizable policies in an\nensemble of simulated domains, and our experimental results demonstrate that the ensemble approach provides for policies that are robust to some unmodeled effects. Our experiments also demonstrate that Bayesian source ensemble adaptation can produce distributions over models that produce better policies on the target domain than more standard maximum likelihood estimation, particularly in the presence of unmodeled effects.\nAlthough our method exhibits good generalization performance and supports Bayesian adaptation, the adaptation algorithm we propose currently relies on discretization of the model parameter space, which quickly becomes intractable as the number of model parameters increases. In future, we plan to explore more sophisticated model ensemble parameterizations, such as flexible parametric probability distributions and nonparametric distributions. Such parametrizations and representations would provide more favorable performance with higher model parameter dimensionalities. One such promising venue is use of generalpurpose (Bayesian) neural network models, where the neural network parameters could be thought of as model parameters. These models could be pre-trained using physics based simulators like MuJoCo to get a practical initialization of neural network parameters. Such representations are likely useful when dealing with high dimensional inputs and parametrizations like simulated vision from rendered images or complex physics models, which are needed to train highly generalizable policies that can successfully transfer to physical robots acting in the real world."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors would like to thank Emo Todorov and Sham Kakade for insightful comments about the work. The authors would also like to thank Emo Todorov for the MuJoCo simulator. Aravind Rajeswaran and Balaraman Ravindran acknowledge financial support from ILDS, IIT Madras."
    } ],
    "references" : [ {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "Volodymyr Mnih" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2015
    }, {
      "title" : "Mastering the game of go with deep neural networks and tree",
      "author" : [ "David Silver" ],
      "venue" : "search. Nature,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2016
    }, {
      "title" : "Continuous control with deep reinforcement learning",
      "author" : [ "T.P. Lillicrap", "J.J. Hunt", "A. Pritzel", "N. Heess", "T. Erez", "Y. Tassa", "D. Silver", "D. Wierstra" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2015
    }, {
      "title" : "Interactive control of diverse complex characters with neural networks",
      "author" : [ "Igor Mordatch", "Kendall Lowrey", "Galen Andrew", "Zoran Popovic", "Emanuel V. Todorov" ],
      "venue" : "In NIPS",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2015
    }, {
      "title" : "Terrain-adaptive locomotion skills using deep reinforcement learning",
      "author" : [ "Xue Bin Peng", "Glen Berseth", "Michiel van de Panne" ],
      "venue" : "ACM Transactions on Graphics (Proc. SIGGRAPH",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2016
    }, {
      "title" : "On the Sample Complexity of Reinforcement Learning",
      "author" : [ "Sham Kakade" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2003
    }, {
      "title" : "A comprehensive survey on safe reinforcement learning",
      "author" : [ "Javier Garcı́a", "Fernando Fernández" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2015
    }, {
      "title" : "Bayesian reinforcement learning: A survey",
      "author" : [ "Mohammad Ghavamzadeh", "Shie Mannor", "Joelle Pineau", "Aviv Tamar" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2015
    }, {
      "title" : "Transfer learning for reinforcement learning domains: A survey",
      "author" : [ "Matthew E. Taylor", "Peter Stone" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2009
    }, {
      "title" : "Agnostic system identification for model-based reinforcement learning",
      "author" : [ "Stephane Ross", "Drew Bagnell" ],
      "venue" : "In ICML,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2012
    }, {
      "title" : "A survey on policy search for robotics",
      "author" : [ "Marc Peter Deisenroth", "Gerhard Neumann", "Jan Peters" ],
      "venue" : "Foundations and Trends in Robotics,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2013
    }, {
      "title" : "Model based bayesian exploration",
      "author" : [ "Richard Dearden", "Nir Friedman", "David Andre" ],
      "venue" : "In UAI,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1999
    }, {
      "title" : "Bayesian Reinforcement Learning, pages 359–386",
      "author" : [ "Nikos Vlassis", "Mohammad Ghavamzadeh", "Shie Mannor", "Pascal Poupart" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2012
    }, {
      "title" : "System Identification, pages 163–173",
      "author" : [ "Lennart Ljung" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1998
    }, {
      "title" : "Robust control of markov decision processes with uncertain transition matrices",
      "author" : [ "Arnab Nilim", "Laurent El Ghaoui" ],
      "venue" : "Operations Research,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2005
    }, {
      "title" : "Robust and Optimal Control",
      "author" : [ "Kemin Zhou", "John C. Doyle", "Keith Glover" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1996
    }, {
      "title" : "Reinforcement learning in robust markov decision processes",
      "author" : [ "Shiau Hong Lim", "Huan Xu", "Shie Mannor" ],
      "venue" : "In NIPS",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2013
    }, {
      "title" : "An analytic solution to discrete bayesian reinforcement learning",
      "author" : [ "Pascal Poupart", "Nikos A. Vlassis", "Jesse Hoey", "Kevin Regan" ],
      "venue" : "In ICML,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2006
    }, {
      "title" : "Bayesian reinforcement learning in continuous pomdps with application to robot navigation",
      "author" : [ "S. Ross", "B. Chaib-draa", "J. Pineau" ],
      "venue" : "In ICRA,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2008
    }, {
      "title" : "Design for an optimal probe",
      "author" : [ "Michael O. Duff" ],
      "venue" : "In ICML,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2003
    }, {
      "title" : "Point-based value iteration for continuous pomdps",
      "author" : [ "Josep M. Porta", "Nikos A. Vlassis", "Matthijs T.J. Spaan", "Pascal Poupart" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2006
    }, {
      "title" : "Ensemble- CIO: Full-body dynamic motion planning that transfers to physical humanoids",
      "author" : [ "I. Mordatch", "K. Lowrey", "E. Todorov" ],
      "venue" : "In IROS,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2015
    }, {
      "title" : "Learning and control with inaccurate models",
      "author" : [ "Zico Kolter" ],
      "venue" : "PhD thesis, Stanford University,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2010
    }, {
      "title" : "Learning parameterized skills",
      "author" : [ "Bruno Castro da Silva", "George Konidaris", "Andrew G. Barto" ],
      "venue" : "In ICML,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2012
    }, {
      "title" : "High-confidence off-policy evaluation",
      "author" : [ "Philip Thomas", "Georgios Theocharous", "Mohammad Ghavamzadeh" ],
      "venue" : "In AAAI Conference on Artificial Intelligence",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2015
    }, {
      "title" : "Approximately optimal approximate reinforcement learning",
      "author" : [ "Sham Kakade", "John Langford" ],
      "venue" : "In ICML,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2002
    }, {
      "title" : "Guided policy search",
      "author" : [ "Sergey Levine", "Vladlen Koltun" ],
      "venue" : "In ICML,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2013
    }, {
      "title" : "A survey of robot learning from demonstration",
      "author" : [ "Brenna D. Argall", "Sonia Chernova", "Manuela Veloso", "Brett Browning" ],
      "venue" : "Robotics and Autonomous Systems,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2009
    }, {
      "title" : "Integrating a partial model into model free reinforcement learning",
      "author" : [ "Aviv Tamar", "Dotan Di Castro", "Ron Meir" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2012
    }, {
      "title" : "Using inaccurate models in reinforcement learning",
      "author" : [ "Pieter Abbeel", "Morgan Quigley", "Andrew Y. Ng" ],
      "venue" : "In ICML,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2006
    }, {
      "title" : "Trust region policy optimization",
      "author" : [ "John Schulman", "Sergey Levine", "Philipp Moritz", "Michael Jordan", "Pieter Abbeel" ],
      "venue" : "In ICML,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2015
    }, {
      "title" : "A natural policy gradient",
      "author" : [ "Sham Kakade" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2001
    }, {
      "title" : "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning",
      "author" : [ "Ronald J. Williams" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 1992
    }, {
      "title" : "Percentile optimization for markov decision processes with parameter uncertainty",
      "author" : [ "Erick Delage", "Shie Mannor" ],
      "venue" : "Operations Research,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2010
    }, {
      "title" : "Mujoco: A physics engine for model-based control",
      "author" : [ "E. Todorov", "T. Erez", "Y. Tassa" ],
      "venue" : "In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2012
    }, {
      "title" : "Benchmarking deep reinforcement learning for continuous control",
      "author" : [ "Yan Duan", "Xi Chen", "Rein Houthooft", "John Schulman", "Pieter Abbeel" ],
      "venue" : "In ICML,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2016
    }, {
      "title" : "Infinite-horizon model predictive control for periodic tasks with contacts",
      "author" : [ "Tom Erez", "Yuval Tassa", "Emanuel Todorov" ],
      "venue" : "In Proceedings of Robotics: Science and Systems,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2011
    }, {
      "title" : "Real-time reinforcement learning by sequential actor-critics and experience replay",
      "author" : [ "Pawel Wawrzynski" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Reinforcement learning methods with powerful function approximators such as deep neural networks (deep RL) have recently demonstrated remarkable success in a wide range of simulated tasks like games [1, 2], simulated robotic control problems [3, 4], and graphics [5].",
      "startOffset" : 199,
      "endOffset" : 205
    }, {
      "referenceID" : 1,
      "context" : "Reinforcement learning methods with powerful function approximators such as deep neural networks (deep RL) have recently demonstrated remarkable success in a wide range of simulated tasks like games [1, 2], simulated robotic control problems [3, 4], and graphics [5].",
      "startOffset" : 199,
      "endOffset" : 205
    }, {
      "referenceID" : 2,
      "context" : "Reinforcement learning methods with powerful function approximators such as deep neural networks (deep RL) have recently demonstrated remarkable success in a wide range of simulated tasks like games [1, 2], simulated robotic control problems [3, 4], and graphics [5].",
      "startOffset" : 242,
      "endOffset" : 248
    }, {
      "referenceID" : 3,
      "context" : "Reinforcement learning methods with powerful function approximators such as deep neural networks (deep RL) have recently demonstrated remarkable success in a wide range of simulated tasks like games [1, 2], simulated robotic control problems [3, 4], and graphics [5].",
      "startOffset" : 242,
      "endOffset" : 248
    }, {
      "referenceID" : 4,
      "context" : "Reinforcement learning methods with powerful function approximators such as deep neural networks (deep RL) have recently demonstrated remarkable success in a wide range of simulated tasks like games [1, 2], simulated robotic control problems [3, 4], and graphics [5].",
      "startOffset" : 263,
      "endOffset" : 266
    }, {
      "referenceID" : 5,
      "context" : "Model-free methods like Q-learning, actor-critic, and policy gradients are known to suffer from long learning times [6], which is compounded when combined with expressive function approximators like deep neural networks.",
      "startOffset" : 116,
      "endOffset" : 119
    }, {
      "referenceID" : 6,
      "context" : "The challenge of gathering samples from the real world is further exacerbated by issues of safety for the agent and environment when sampling with partially learned policies which could be unstable [7].",
      "startOffset" : 198,
      "endOffset" : 201
    }, {
      "referenceID" : 7,
      "context" : "This approach can be viewed as an instance of model-based Bayesian RL [8]; or as an instance of transfer learning from a collection of simulated source domains to a real-world target domain [9].",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 8,
      "context" : "This approach can be viewed as an instance of model-based Bayesian RL [8]; or as an instance of transfer learning from a collection of simulated source domains to a real-world target domain [9].",
      "startOffset" : 190,
      "endOffset" : 193
    }, {
      "referenceID" : 9,
      "context" : "Standard model-based RL methods typically operate by finding a maximum-likelihood estimate to the target dynamics model [10, 11], followed by policy optimization.",
      "startOffset" : 120,
      "endOffset" : 128
    }, {
      "referenceID" : 10,
      "context" : "Standard model-based RL methods typically operate by finding a maximum-likelihood estimate to the target dynamics model [10, 11], followed by policy optimization.",
      "startOffset" : 120,
      "endOffset" : 128
    }, {
      "referenceID" : 11,
      "context" : "Previously, Bayesian RL methods have been explored to address these drawbacks [12, 13].",
      "startOffset" : 78,
      "endOffset" : 86
    }, {
      "referenceID" : 12,
      "context" : "Previously, Bayesian RL methods have been explored to address these drawbacks [12, 13].",
      "startOffset" : 78,
      "endOffset" : 86
    }, {
      "referenceID" : 13,
      "context" : "In contrast to standard system ID [14], the data for model learning is obtained through execution of a robust policy, and hence alleviates safety concerns during model identification.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 14,
      "context" : "Robust control is a branch of control theory which formally studies development of robust policies [15, 16].",
      "startOffset" : 99,
      "endOffset" : 107
    }, {
      "referenceID" : 15,
      "context" : "Robust control is a branch of control theory which formally studies development of robust policies [15, 16].",
      "startOffset" : 99,
      "endOffset" : 107
    }, {
      "referenceID" : 16,
      "context" : "Much of the work in this community has been concentrated around linear systems or finite MDPs, which often cannot adequately model complexities of real-world tasks [17].",
      "startOffset" : 164,
      "endOffset" : 168
    }, {
      "referenceID" : 7,
      "context" : "The broad field of model-based Bayesian RL maintains a belief over models for decision making under uncertainty [8, 13].",
      "startOffset" : 112,
      "endOffset" : 119
    }, {
      "referenceID" : 12,
      "context" : "The broad field of model-based Bayesian RL maintains a belief over models for decision making under uncertainty [8, 13].",
      "startOffset" : 112,
      "endOffset" : 119
    }, {
      "referenceID" : 17,
      "context" : "Application of this idea in its full general form is difficult, and requires either restrictive assumptions like finite MDPs [18], Gaussian dynamics [19], or task specific innovations.",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 18,
      "context" : "Application of this idea in its full general form is difficult, and requires either restrictive assumptions like finite MDPs [18], Gaussian dynamics [19], or task specific innovations.",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 19,
      "context" : "Some previous methods have also suggested treating uncertain model parameters as unobserved state variables in a continuous POMDP framework, and solving the POMDP to get optimal explorationexploitation trade-off [20, 21].",
      "startOffset" : 212,
      "endOffset" : 220
    }, {
      "referenceID" : 20,
      "context" : "Some previous methods have also suggested treating uncertain model parameters as unobserved state variables in a continuous POMDP framework, and solving the POMDP to get optimal explorationexploitation trade-off [20, 21].",
      "startOffset" : 212,
      "endOffset" : 220
    }, {
      "referenceID" : 21,
      "context" : "[22] use model based trajectory optimization and an ensemble with small finite set of models, whereas we follow a sampling based direct policy search approach over a continuous distribution of uncertain parameters and also show domain adaptation.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 22,
      "context" : "Kolter [23] identified that parameters of optimal policies to MDPs in the source distribution live in some low dimensional subspace, and hence policy search can be performed in this lower dimensional space with data from the target domain.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 23,
      "context" : "Learning of parametrized skills [24] is also concerned with finding policies for a distribution of parametrized tasks.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 24,
      "context" : "A number of methods have also been suggested to reduce sample complexity when provided with either a baseline policy [25, 26], expert demonstration [27, 28], or approximate simulator [29, 30].",
      "startOffset" : 117,
      "endOffset" : 125
    }, {
      "referenceID" : 25,
      "context" : "A number of methods have also been suggested to reduce sample complexity when provided with either a baseline policy [25, 26], expert demonstration [27, 28], or approximate simulator [29, 30].",
      "startOffset" : 117,
      "endOffset" : 125
    }, {
      "referenceID" : 26,
      "context" : "A number of methods have also been suggested to reduce sample complexity when provided with either a baseline policy [25, 26], expert demonstration [27, 28], or approximate simulator [29, 30].",
      "startOffset" : 148,
      "endOffset" : 156
    }, {
      "referenceID" : 27,
      "context" : "A number of methods have also been suggested to reduce sample complexity when provided with either a baseline policy [25, 26], expert demonstration [27, 28], or approximate simulator [29, 30].",
      "startOffset" : 148,
      "endOffset" : 156
    }, {
      "referenceID" : 28,
      "context" : "A number of methods have also been suggested to reduce sample complexity when provided with either a baseline policy [25, 26], expert demonstration [27, 28], or approximate simulator [29, 30].",
      "startOffset" : 183,
      "endOffset" : 191
    }, {
      "referenceID" : 29,
      "context" : "A number of methods have also been suggested to reduce sample complexity when provided with either a baseline policy [25, 26], expert demonstration [27, 28], or approximate simulator [29, 30].",
      "startOffset" : 183,
      "endOffset" : 191
    }, {
      "referenceID" : 30,
      "context" : "[31], [32], [33]).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 31,
      "context" : "[31], [32], [33]).",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 32,
      "context" : "[31], [32], [33]).",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 33,
      "context" : "A related line of work is percentile optimization, where the -percentile value of return is directly optimized [34].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 6,
      "context" : "We also refer readers to [7] for a survey of related risk sensitive RL methods in the context of safety and robustness.",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 34,
      "context" : "We evaluate the proposed EPOpt- algorithm on 2D hopper and half-cheetah simulated robotic tasks using the MuJoCo physics simulator [35].",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 35,
      "context" : "The tasks were implemented using base code provided with OpenAI gym [36] and rllab [37].",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 30,
      "context" : "We use TRPO [31] for our batch policy optimization subroutine.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 36,
      "context" : "Descriptions of these tasks are below: Hopper: The hopper task is to make a 2D planar hopper with three joints and 4 body parts hop forward as fast as possible [38].",
      "startOffset" : 160,
      "endOffset" : 164
    }, {
      "referenceID" : 37,
      "context" : "Half Cheetah: The half-cheetah task [39] requires us to make a 2D cheetah with two legs run forward as fast as possible.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 30,
      "context" : "[31].",
      "startOffset" : 0,
      "endOffset" : 4
    } ],
    "year" : 2017,
    "abstractText" : "Sample complexity and safety are major challenges when learning policies with reinforcement learning for real-world tasks – especially when the policies are represented using rich function approximators like deep neural networks. Modelbased methods where the real-world target domain is approximated using a simulated source domain provide an avenue to tackle the above challenges by augmenting real data with simulated data. However, discrepancies between the simulated source domain and the target domain pose a challenge for simulated training. We introduce the EPOpt algorithm, which uses an ensemble of simulated source domains and a form of adversarial training to learn policies that are robust and generalize to a broad range of possible target domains, including to unmodeled effects. Further, the probability distribution over source domains in the ensemble can be adapted using data from target domain and approximate Bayesian methods, to progressively make it a better approximation. Thus, learning on a model ensemble, along with source domain adaptation, provides the benefit of both robustness and learning/adaptation.",
    "creator" : "LaTeX with hyperref package"
  }
}