{
  "name" : "1705.08018.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Use of Knowledge Graph in Rescoring the N-best List in Automatic Speech Recognition",
    "authors" : [ "Ashwini Jaya Kumar", "Camilo Morales", "Maria-Esther Vidal", "Christoph Schmidt", "Sören Auer" ],
    "emails" : [ "ashwini.jaya.kumar@iais.fraunhofer.de" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Automatic speech recognition (ASR) is a system to convert spoken signal into it’s transcription (text). The performance of ASR is determined by how accurately the spoken words are recognised. The recognition accuracy is vital in any application where speech interface is required. Considering a question answering application, e.g.“What is the capital of Germany?”, in this sentence if an important entity is misrecognised by ASR, the whole meaning of the sentence is changed and thus affects the interconnected systems. An ASR system mainly consists of three main components, acoustic model, language model and decoder. Acoustic model is a sub-system which statistically connects the features extracted from speech signal to it’s phoneme representation. Language model assigns probability for sequence of words. It helps to constrain the search among alternative words. While the decoder helps to combine both the acoustic score and language model score to decode the sequence of words (or characters or phones). In the literature, recognition accuracy is improved by improving the acoustic model, the language model, at the decoder and at the output of ASR by rescoring the N-best list. N-best list is a list of hypothesis of a sentence by a recognition system. Subsequent re-ranking of the N-best list using various knowledge sources is called rescoring. Let S1 = s1, s2, s3, . . . , sn, . . . , sN be theN different form of a sentence S1 at the output of a speech recogniser. These best possible sentences are called N-best sentences.\nA knowledge graph is a connected graph of triples in < subject, predicate, object > form with entities as subjects/objects and predicate as the property/relation with which the entities are connected. A triple is a logical representation of the words and it’s relations.e.g. < subject, predicate, object >. A relation is a property with which the two entities are connected e.g. daughter of, employee of, president of, etc,.The words in a triple can be an entity, a real, discrete unit e.g. person, organisation, place, etc,.The use of external knowledge sources like knowledge graph to improve the recognition accuracy is not tested in any of the previous methods. In this paper, we are explaining the use of external knowledge resource to rescore the N-best list. We are using dbpedia knowledge graph as the external knowledge source.\nEntity linking is a method to link the triple (entities/relations) in a sentence to the RDF molecules of a knowledge graph. The N-best list of sentences and the knowledge graph is connected by linking the entities/relation in a sentence to the triple units in the knowledge base. Dbpedia spotlight, a tool to annotate the mentions of DBpedia is used here for entity/relations linking in the N-best sentences to the Dbpedia knowledge graph. The output of Dbpedia spotlight contains the link (uri) for a given entity/relations. e.g. for entity“Barrack Obama”, Dbpedia uri link looks like, http://dbpedia.org/page/Barack Obama. Dbpedia spotlight only gives the linking information. Next, we are trying to see whether these entities and relations are connected to each other in knowledge graph. If they are connected, then the difference of TrasE embeddings between that RDF molecule of entity1 and RDF molecule of entity2 is zero. In other words, we are applying modified viterbi algorithm across the RDF molecues of entities/relations in a sentence. Difference between TransE embeddings is used as a cost function. To get the TransE embeddings we need the RDF molecules i.e. the entity in a triple form e.g. < Barrack Obama,wife,Michael Obama >. Hence to get the corresponding RDF molecules, we are using Apache Jena, an API to create and read the Resource Descrption Framework (RDF) graphs. A sub-graph is created by extracting the RDF molecules containing the entities/relations in a given N-best sentences.\nTransE [8] is a method to model the relationships between entities by interpreting them as translations operating on the low-dimensional embeddings of the entities. Modeling mainly means extracting the local and global connectivity pattern between entities. It is easy to train and has shown to outperform all other state-of-the art in linking multi-relational databases or knowledge bases. In this work, TransE embeddings are used to model the sub-graph of Dbpedia.\nar X\niv :1\n70 5.\n08 01\n8v 1\n[ cs\n.C L\n] 2\n2 M\nay 2\n01 7"
    }, {
      "heading" : "2. Background",
      "text" : "In this section, different methods used to rescore the N-best list and is relevant to the work in this paper is discussed. A query specific search result is used as a feature to rescore the n-best list in [1]. The search result contains domain specific knowledge which is difficult to capture using simple n-gram based language model. The authors have reported that with the search domain knowledge, a disambiguate query can be classified and re-ranked in the N-best list which has resulted in improved recognition accuracy.\nThe standard selection criteria for a speech recognition hypothesis is maximising the posterior probability of a hypothesis W given the observation sequence X . In [3] an algorithm to rescore N-best list is proposed, where it approximates posterior probabilities using the N-best list and then the expected word error rate is computed for each hypothesis with respect to posterior probability distribution. In [1], various knowledge sources such as recogniser score, linguistic analysis, grammar construction, semantic discrimination score is used to rescore the N-best list. In semantic discrimination, it is only considered if the given words in a sentence are in triple form or not. Articulatory based feature is used as a knowledge source to rescore the N-best list in [4]. The manner and place of articulation is used here. In [5] the acoustic and lexical prosodic models are applied to each n-best hypothesis to obtain its prosody score, and combined with ASR scores to find the top hypothesis.\nLetw11, w12, w13, ..w1m..w1M1 be the words in 1st−best sentence s1 where N is the number of best sentences and M is the number of words in a sentence in the list. The recognition accuracy is usually measured for the 1st-best sentence in an N − best list generated by the decoder. Most often the erroneous word existing in the 1st-best might be in correct form in the later (N − 1)− best list of sentences. Most of the times the word sequences are picked such that they are in some context. Since the recogniser generates the most probable word sequences which is much closer to the given test sentence, the words in the n-best list are also closer to each other e.g. it is possible that w11 and w21 are same words. Also words w11, w12 are related similarly w21 and w22. There are methods which uses grammar, lexicon, language rules, syntactic and semantic properties to predict a sentence with most likely word sequences similar to the test sentence. Here we are trying to see the semantic relatedness between two words.\nTransE [8] aims at embedding entities and relationships in a relational database (knowledge graphs) into a lower dimensional vector space. It is an energy based model. In TransE relationships are represented as translations in the embedding space: in (h, l, t) the embedding of tail t should be close to the embedding of head h plus some vector that depends on the relationship l. The TransE algorithm is depicted in Figure 1."
    }, {
      "heading" : "3. Knowledge graph based N-best list rescoring",
      "text" : "Knowledge graphs can be considered as a rich information source which can be used to interpret a sentence in logical form or in a form understandable by a computer e.g. by semantically tagging the sentence.\nExample: Consider a sentence, “Chelsea Clinton is a daughter of Bill clinton and was born in Arkansas” is represented in logical/triple form as (< ChelseaClinton, daughter,Billclinton >) and (< ChelseaClinton, born,Arkansas >). The entities in\nthe above sentence are “Chelsea Clinton”,“Bill Clinton” and “Arkansas”, and connected with properties “daughter” and “born”.\nIt is shown in the above Example, how the entities and relations are connected in Dbpedia knowledge graph. This type of structured information storage makes it easy to retrieve the stored information and to store any new information. This type of knowledge source is useful in any language processing applications to do a logical interpretation of a sentence by understanding the entities and their relations. We are extending this knowledge graph concept to automatic speech recognition by using it to rescore the N-best list based on how the entities/relations in a sentence are semantically related.\nThe steps followed for N-best list rescoring is described below.\nSteps: N-best list rescoring 1: Obtaining the N-best list. 2: Annotating the mentions of Dbpedia in the N-best list 3: Obtaining the RDF molecules corresponding to annotated\nentities/relations in Step 2. 4: Calculating the semantic relatedness cost using TransE em-\nbeddings 5: Rescoring the N-best list based on the semantic relatedness\ncost computed in Step 4"
    }, {
      "heading" : "3.1. Obtaining the N-best list",
      "text" : "The N-best list of sentences are obtained from a kaldi based speech recogniser. A lattice can be defined as a labelled, weighted, directed acyclic graph (Weighted Finite State Transducers with word labels) [7]. The N best paths are computed through the lattice using Viterbi beam search algorithm with only a single tunable parameter: the pruning beam and outputs the result as a lattice, but with a special structure. The start state in the viterbi beam search algorithm will have (up to) n arcs out of it, each one to the start of a separate path [9]."
    }, {
      "heading" : "3.2. Annotating the mentions of Dbpedia in the N-best list",
      "text" : "DBpedia Spotlight is a tool for automatically annotating mentions of DBpedia resources in text, providing a solution for linking unstructured information sources to the Linked Open Data cloud through DBpedia. DBpedia Spotlight recognizes that names of concepts or entities have been mentioned (e.g. “Michael Jordan”), and subsequently matches these names to\nunique identifiers (e.g. dbpedia:Michael I. Jordan, the machine learning professor or dbpedia:Michael Jordan the basketball player) [6]. There is a web services for spotting, disambiguate and to annotate the entities/concepts. We are using only the annotation in this work. Annotation takes text as input, recognises entities/concepts to annotate and chooses an identifier in Dbpedia for each recognised entity/concept given the context."
    }, {
      "heading" : "3.3. Obtaining the RDF molecules corresponding to annotated entities/relations in Step 2",
      "text" : "The entities/relations are annotated and linked to the the knowledge graph using Dbpedia spotlight. We have to see the relatedness between the entities/relations in a sentence in the graph. The enities or relations are stored only in the triple form in the graph. The entities/relation could be connected to subject or object or property of some triple combination in the graph. Let’s call the entity/relations as the reference entity/relations. Using Apache Jena API, the othe triples which has the reference entity is shortlisted. The number is limited to 500. So here we get 500 RDF molecules for an entity. Like wise we obtain 500 RDF molecules for all the annotated entities/relations in a sentence."
    }, {
      "heading" : "3.4. Calculating the semantic relatedness cost using TransE embeddings",
      "text" : "The connection information stored in a triple is represented in vector space using TransE [8]. TransE aims at embedding entities and relationships in a relational database (knowledge graphs) into a lower dimensional vector space. In TransE relationships are represented as translations in the embedding space: in (h, l, t) the embedding of tail t should be close to the embedding of head h plus some vector that depends on the relationship l. If the two triples are similar then TransE encoding will have same vector values and hence the distance value will be zero. The TransE encoding takes care of the repeating subject or object or predicate in a triple. In other words the TransE encoding for the same subject in different RDF molecule will be same. The TransE embeddings are obtained for all the RDF molecules of the entities in a sentence. Here we are also using the term RDF molecule for triple.\nThe semantic relatedness cost is defined as the cost function to represent the relatedness between the entities in a sentence. Although it shows the relatedness between entities in a sentence, it is derived from the RDF molecules obtained from a knowledge graph. A list of RDF molecules are obtained for an entity in a sentence. A semantic relatedness measure in the knowledge graph gives an insight into the strengths in the connection between two entities of a sentence.\nLet M1t ,M2t ...Mnt ...MNt be the RDF molecule for entity et.\nWhere, T is the number of entities, N is the number of molecules for an entity t .Here T = 500\nδt = min(|Mnt+1 −Mnt |), 0 ≤ n ≤ N − 1, (1)\nβm = min(δt), 0 ≤ t ≤ T − 1, (2) Where M = number of sentences and βm is the semantic relatedness cost. Semantic relatedness cost is the summation of subject cost and object cost. If the distance is between subject of RDF molecule of the neighboring entities then it is called subject cost. Similarly if it is between the object of RDF molecule then\nit is called the object cost. Rescoring the n-best list is done based on all these three costs (i.e. Subject cost, Object cost and total cost) So if smaller is the value of cost function, higher is the relatedness."
    }, {
      "heading" : "3.5. Rescoring the N-best list based on the semantic relatedness cost computed in Step 4",
      "text" : "The semantic relatedness cost (SRC) computed for a sentence is used as a feature to rescore the N-best list. Lower the SRC, higher is the relatedness between the entities/relations in a sentence. Since the semantic relatedness cost is computed from the true information in the knowledge graph, it can be considered as a true cost."
    }, {
      "heading" : "4. Discussion and Conclusion",
      "text" : "A kaldi based speech recogniser is used in this work [9]. The training database is TED-LIUM (tedlium-1) with 118hours of training date, it consists of TED talks with cleaned automatic transcripts 1. Mel frequency cepstral coefficients are used as features to train a deep neural network based acoustic model and n-gram based language model. Decoding graph is created using the weighted finite state transducers [10]. The Nbest list of sentences are obtained with N = 30. The entity linking is made with a confidence score of 0.3 score on Dbpedia spotlight annotation tool. The Apache Jena API is used to fetch the RDF molecules from Dbpedia knowledge graph with LIMIT = 500. TransE is trained using the RDF molecules at sentence level fetched from Dbpedia.\nThe above discussed method showed average results on TED-LIUM audio corpus to rescore the N best list. The method is more suitable for factoid questions with entities and relations well annotated in the N best list."
    }, {
      "heading" : "5. Acknowledgement",
      "text" : "Parts of this work received funding from the European Unions Horizon 2020 research and innovation program under the Marie\n1http://www-lium.univ-lemans.fr/en/content/ted-lium-corpus\nSklodowska-Curie grant agreement No. 642795 (WDAqua project)."
    }, {
      "heading" : "6. References",
      "text" : "[1] Peng, Fuchun, Scott Roy, Ben Shahshahani, and Franoise Beau-\nfays. ”Search results based N-best hypothesis rescoring with maximum entropy classification.” In ASRU, pp. 422-427. 2013.\n[2] Rayner, Manny, David Carter, Vassilios Digalakis, and Patti Price. ”Combining knowledge sources to reorder n-best speech hypothesis lists.” In Proceedings of the workshop on Human Language Technology, pp. 217-221. Association for Computational Linguistics, 1994.\n[3] Stolcke, Andreas, Yochai Konig, and Mitchel Weintraub. ”Explicit word error minimization in n-best list rescoring.” In Eurospeech, vol. 97, pp. 163-166. 1997. Harvard\n[4] Li, Jinyu, Yu Tsao, and Chin-Hui Lee. ”A Study on Knowledge Source Integration for Candidate Rescoring in Automatic Speech Recognition.” In ICASSP (1), pp. 837-840. 2005.\n[5] Jeon, Je Hun, Wen Wang, and Yang Liu. ”N-best rescoring based on pitch-accent patterns.” In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pp. 732-741. Association for Computational Linguistics, 2011.\n[6] Daiber, Joachim, Max Jakob, Chris Hokamp, and Pablo N. Mendes. ”Improving efficiency and accuracy in multilingual entity extraction.” In Proceedings of the 9th International Conference on Semantic Systems, pp. 121-124. ACM, 2013.\n[7] Povey, Daniel, Mirko Hannemann, Gilles Boulianne, Luk Burget, Arnab Ghoshal, Milo Janda, Martin Karafit et al. ”Generating exact lattices in the WFST framework.” In 2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 4213-4216. IEEE, 2012.\n[8] Bordes, Antoine, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. ”Translating embeddings for modeling multi-relational data.” In Advances in Neural Information Processing Systems, pp. 2787-2795. 2013.\n[9] Povey, Daniel, Arnab Ghoshal, Gilles Boulianne, Lukas Burget, Ondrej Glembek, Nagendra Goel, Mirko Hannemann et al. ”The Kaldi speech recognition toolkit.” In IEEE 2011 workshop on automatic speech recognition and understanding, no. EPFL-CONF192584. IEEE Signal Processing Society, 2011.\n[10] Mohri, Mehryar, Fernando Pereira, and Michael Riley. ”Speech recognition with weighted finite-state transducers.” In Springer Handbook of Speech Processing, pp. 559-584. Springer Berlin Heidelberg, 2008."
    } ],
    "references" : [ {
      "title" : "Search results based N-best hypothesis rescoring with maximum entropy classification.",
      "author" : [ "Peng", "Fuchun", "Scott Roy", "Ben Shahshahani", "Franoise Beaufays" ],
      "venue" : "In ASRU,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2013
    }, {
      "title" : "Combining knowledge sources to reorder n-best speech hypothesis lists.",
      "author" : [ "Rayner", "Manny", "David Carter", "Vassilios Digalakis", "Patti Price" ],
      "venue" : "In Proceedings of the workshop on Human Language Technology,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1994
    }, {
      "title" : "Explicit word error minimization in n-best list rescoring.",
      "author" : [ "Stolcke", "Andreas", "Yochai Konig", "Mitchel Weintraub" ],
      "venue" : "In Eurospeech,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1997
    }, {
      "title" : "A Study on Knowledge Source Integration for Candidate Rescoring in Automatic Speech Recognition.",
      "author" : [ "Li", "Jinyu", "Yu Tsao", "Chin-Hui Lee" ],
      "venue" : "In ICASSP",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2005
    }, {
      "title" : "N-best rescoring based on pitch-accent patterns.",
      "author" : [ "Jeon", "Je Hun", "Wen Wang", "Yang Liu" ],
      "venue" : "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2011
    }, {
      "title" : "Improving efficiency and accuracy in multilingual entity extraction.",
      "author" : [ "Daiber", "Joachim", "Max Jakob", "Chris Hokamp", "Pablo N. Mendes" ],
      "venue" : "In Proceedings of the 9th International Conference on Semantic Systems,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2013
    }, {
      "title" : "Generating exact lattices in the WFST framework.",
      "author" : [ "Povey", "Daniel", "Mirko Hannemann", "Gilles Boulianne", "Luk Burget", "Arnab Ghoshal", "Milo Janda", "Martin Karafit" ],
      "venue" : "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2012
    }, {
      "title" : "Translating embeddings for modeling multi-relational data.",
      "author" : [ "Bordes", "Antoine", "Nicolas Usunier", "Alberto Garcia-Duran", "Jason Weston", "Oksana Yakhnenko" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2013
    }, {
      "title" : "The Kaldi speech recognition toolkit.",
      "author" : [ "Povey", "Daniel", "Arnab Ghoshal", "Gilles Boulianne", "Lukas Burget", "Ondrej Glembek", "Nagendra Goel", "Mirko Hannemann" ],
      "venue" : "In IEEE 2011 workshop on automatic speech recognition and understanding,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1925
    }, {
      "title" : "Speech recognition with weighted finite-state transducers.",
      "author" : [ "Mohri", "Mehryar", "Fernando Pereira", "Michael Riley" ],
      "venue" : "In Springer Handbook of Speech Processing,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "Semantic relatedness is computed using TransE [8], a method for low dimensional embedding of a triple in a knowledge graph.",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 7,
      "context" : "TransE [8] is a method to model the relationships between entities by interpreting them as translations operating on the low-dimensional embeddings of the entities.",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 0,
      "context" : "A query specific search result is used as a feature to rescore the n-best list in [1].",
      "startOffset" : 82,
      "endOffset" : 85
    }, {
      "referenceID" : 2,
      "context" : "In [3] an algorithm to rescore N-best list is proposed, where it approximates posterior probabilities using the N-best list and then the expected word error rate is computed for each hypothesis with respect to posterior probability distribution.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 0,
      "context" : "In [1], various knowledge sources such as recogniser score, linguistic analysis, grammar construction, semantic discrimination score is used to rescore the N-best list.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 3,
      "context" : "Articulatory based feature is used as a knowledge source to rescore the N-best list in [4].",
      "startOffset" : 87,
      "endOffset" : 90
    }, {
      "referenceID" : 4,
      "context" : "In [5] the acoustic and lexical prosodic models are applied to each n-best hypothesis to obtain its prosody score, and combined with ASR scores to find the top hypothesis.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 7,
      "context" : "TransE [8] aims at embedding entities and relationships in a relational database (knowledge graphs) into a lower dimensional vector space.",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 6,
      "context" : "A lattice can be defined as a labelled, weighted, directed acyclic graph (Weighted Finite State Transducers with word labels) [7].",
      "startOffset" : 126,
      "endOffset" : 129
    }, {
      "referenceID" : 8,
      "context" : "The start state in the viterbi beam search algorithm will have (up to) n arcs out of it, each one to the start of a separate path [9].",
      "startOffset" : 130,
      "endOffset" : 133
    }, {
      "referenceID" : 5,
      "context" : "Jordan, the machine learning professor or dbpedia:Michael Jordan the basketball player) [6].",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 7,
      "context" : "The connection information stored in a triple is represented in vector space using TransE [8].",
      "startOffset" : 90,
      "endOffset" : 93
    }, {
      "referenceID" : 8,
      "context" : "A kaldi based speech recogniser is used in this work [9].",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 9,
      "context" : "Decoding graph is created using the weighted finite state transducers [10].",
      "startOffset" : 70,
      "endOffset" : 74
    } ],
    "year" : 2017,
    "abstractText" : "With the evolution of neural network based methods, automatic speech recognition (ASR) field has been advanced to a level where building an application with speech interface is a reality. Inspite of these advances, building a real-time speech recogniser faces several problems such as low recognition accuracy, domain constraint and out-of-vocabulary words. The low recognition accuracy problem is addressed by improving the acoustic model, language model, decoder and by rescoring the N-best list at the output of decoder. We are considering the N-best list rescoring approach to improve the recognition accuracy. Most of the methods in literature uses the grammatical, lexical, syntactic and semantic connection between the words in a recognised sentence as a feature to rescore. In this paper, we have tried to see the semantic relatedness between the words in a sentence to rescore the N-best list. Semantic relatedness is computed using TransE [8], a method for low dimensional embedding of a triple in a knowledge graph. The novelty of the paper is the application of semantic web to automatic speech recognition.",
    "creator" : "LaTeX with hyperref package"
  }
}