{
  "name" : "1608.07734.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning Bayesian Networks without Assuming Missing at Random",
    "authors" : [ "Tameem Adel", "Cassio P. de Campos" ],
    "emails" : [ "tameem.hesham@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 8.\n07 73\n4v 1\n[ cs\n.A I]\n2 7\nA ug\nWe present new algorithms for learning Bayesian networks from data with missing values without the assumption that data are missing at random (MAR). An exact Bayesian network learning algorithm is obtained by recasting the problem into a standard Bayesian network learning problem without missing data. To the best of our knowledge, this is the first exact algorithm for this problem. As expected, the exact algorithm does not scale to large domains. We build on the exact method to create a new approximate algorithm using a hill-climbing technique. This algorithm scales to large domains so long as a suitable standard structure learning method for complete data is available. We perform a wide range of experiments to demonstrate the benefits of learning Bayesian networks without assuming MAR."
    }, {
      "heading" : "1. Introduction",
      "text" : "Missing entries in real-world data can exist due to various reasons. For instance, it can be due to damage of the device used to record feature values; a metal detector might fail to produce a signal denoting the existence of a metal due to a certain malfunction. Results can be incomplete in an industrial experiment due to mechanical breakdowns not necessarily related to the performed experiment (Little and Rubin, 1987). Recommendation data can have miss-\n∗Corresponding author Email address: tameem.hesham@gmail.com (Tameem Adel)\ning values since participants in the recommendation system did not rate all the available songs, films, books, etc. While data missingness in the above examples can mostly be assumed to be generated by a random process which depends only on the observed data, usually referred to as missing at random (MAR) (Little and Rubin, 1987; Rancoita et al., 2016), this assumption might miserably fail in other examples. People seeking for health insurance might refuse to give an answer to certain questions in order to reduce the costs, e.g. ‘do you smoke?’, and in many cases this can be seen as an indication of one specific answer. Similarly, a doctor may not perform all required tests on a patient after being confident of a certain outcome (e.g. to save time/resources), leading to missing values that are not missing at random. In cases when there is no available information about the missingness process, or cases where the MAR assumption cannot be assumed, then we say that data are not missing at random (NonMAR). NonMAR is a generalization of MAR, since we do not assume the MAR condition to fail, instead we assume that we cannot assert its validity (hence NonMAR as defined here shall not be confused with an assumption of certainly being not MAR).\nGiven a dataset with categorical random variables, the Bayesian network structure learning problem refers to finding the best network structure (a directed acyclic graph, or DAG) according to a score function based on the data (Heckerman et al., 1995). As well known, learning a Bayesian network from complete data is NP-complete (Chickering, 1996), and the task becomes even harder with incomplete data. In spite of that, the problem of learning a Bayesian network from incomplete data without assuming MAR belongs to the same complexity class, as we will show later on. Because of such result, we investigate and obtain a new exact algorithm for the problem, based on reformulating it into a standard structure learning without missing data. This is the first exact algorithm for the problem, to the best of our knowledge. In contrast to previous work, our algorithm performs both tasks, namely structure learning and data imputation, in a single shot rather than learning the Bayesian network and then dealing with the missing data, possibly in an iterative manner (Fried-\nman, 1998; Rancoita et al., 2016). Based on the optimization that is required to solve the problem and on the exact algorithm, we devise a hill-climbing approximate algorithm. The hill-climbing regards the completions of the missing values only, while the structure optimization is performed by any off-the-shelf algorithm for structure learning under complete data.\nPrevious work to learn the structure of Bayesian network from incomplete data has greatly focused on MAR data. The seminal algorithm in Friedman (1998) introduced an iterative method based on the Expectation-Maximization (EM) technique, referred to as structural EM. Implementation of structural EM begins with an initial graph structure, followed by steps where the probability distribution of variables with missing values is estimated by EM, alternated with steps in which the expectation of the score of each neighbouring graph is computed. After convergence, the graph maximizing the score is chosen. Many other algorithms have used ideas from structural EM and deal separately with the missing values and the structure optimization using complete data (Borchani et al., 2006; Leray and Francois, 2005; Meila and Jordan, 1998; Ramoni and Sebastiani, 1997; Riggelsen, 2006; Riggelsen and Feelders, 2005). In Rancoita et al. (2016), structures are learned from incomplete data using a structural EM whose maximization step is performed by an anytime method, and the ‘expectation’ step imputes the missing values using expected means, or modes, of the current estimated joint distribution. By using modes in each iteration, that is the closest work to ours. However, that algorithm works as a data augmentation approach related to MAR (Ramoni and Sebastiani, 1997) instead of considering NonMAR.\nWe perform experiments on a set of heterogeneous datasets. We base the evaluation on imputation accuracy in its pure form, as well as in the forms of classification accuracy and semi-supervised learning accuracy. Experiments show the improvements achieved by the proposed algorithms. As per the comparison between both of them, experiments demonstrate that accuracy levels achieved by the approximate algorithm are quite close to those achieved by the optimal learning algorithm, with the former being faster and more scalable."
    }, {
      "heading" : "2. Bayesian Network Structure Learning",
      "text" : "Let X = (X1, . . . , Xm) refer to a vector of categorical random variables, taking values in OX = ×iOXi , where OX represents the Cartesian product of the state space, OXi , of each Xi. Denote by D an n-instance dataset where each instance Du = (du,1, du,2, . . . , du,m) is such that du,i is either an observed value ou,i ∈ OXi or a special symbol denoting the entry is missing. Let Zu denote a completion for variables with missing values in instance u and zu,i for the missing value of Xi.\nA Bayesian network, M, is a probabilistic graphical model based on a structured dependency among random variables to represent a joint probability distribution in a compact and tractable manner. Here, it represents a joint probability distribution PrM over a collection of categorical random variables, X. We define a Bayesian network as a triple M = (G,X,P), where G = (VG , EG) is a directed acyclic graph (DAG) with VG a collection of m nodes associated to the random variables X (a node per variable), and EG a collection of arcs; P is a collection of conditional probabilities PrM(Xi|PAi) where PAi denotes the parents of Xi in the graph (PAi may be empty), corresponding to the relations of EG . In a Bayesian network, the Markov condition states that every variable is conditionally independent of its non-descendants given its parents. This structure induces a joint probability distribution by the expression PrM(X1, . . . , Xm) = ∏ i PrM(Xi|PAi). We define ri ≥ 2 as the number of values in OXi , i.e. ri = |OXi |, and rPAi as the number of possible realizations of the parent set, that is, rPAi = ∏ Xl∈PAi rl. Let R = maxi ri.\nGiven a complete dataset D with n instances, the structure learning problem in Bayesian networks is to find a DAG G that maximizes a given score function, that is, we look for G∗ = argmaxG∈G sD(G), with G the set of all DAGs over node set X. We consider here the score function sD to be the Bayesian Dirichlet Equivalent Uniform (BDeu) criterion (Buntine, 1991; Cooper and Herskovits, 1992) (other decomposable scores could be used too), so we have sD(G) = ∑\ni sD(Xi,PAi). We however have to deal with the missing part of the data.\nWithout assuming MAR, this can be done by completing the missing values in the best possible way (also known as optimistic completion):\n(G∗,Z∗) = argmax G∈G, Z∈Z sD(G,Z) = argmax G∈G, Z∈Z\n∑\ni\nsD(Xi,PAi;Z{Xi}∪PAi) (1)\nwhere Z = ×uOZu and sD(G,Z) is the score sD(G) evaluated for the complete data when its missing values are replaced by Z, while sD(Xi,PAi;Z{Xi}∪PAi) is the local score for a node Xi with parent set PAi (note that such computation only depends on the completion Z{Xi}∪PAi of the involved variables). We refer to this optimization task as the structure learning problem with NonMAR missing data (which can be also applied to MAR data, as explained in the Introduction).\nTheorem 1. The decision version associated to the structure learning problem with NonMAR missing data is NP-complete.\nProof. Hardness is obtained by realizing that this problem generalizes the structure learning problem without missing data, which is NP-hard Chickering (1996). Pertinence in NP holds since given G and Z, the score function sD can be computed in polynomial time.\nSince the problem is a combinatorial optimization over a discrete domain (both DAGs and completions of data are discrete entities), we could resort to enumerating all possible solutions. This is obviously infeasible for both: the number of DAGs grows super-exponentially in the number of variables and the number of completions grows exponentially in the number of missing values. We will now present an exact algorithm for the problem which transforms it into a standard structure learning problem, and later we modify the approach to perform approximate learning. In this respect, we define as a t-local optimal solution for Equation (1) a pair (G,Z) such that sD(G,Z) ≥ sD(G′,Z ′) for all G′ and all Z ′ with HD(Z,Z ′) ≤ t, where HD is the Hamming distance, that is, (G,Z) is optimal with respect to any other pair whose completion of the data has at most t elements different from Z. A global optimal solution is a ∞-local optimal solution."
    }, {
      "heading" : "2.1. Optimal (Exact) Learning Algorithm",
      "text" : "We assume that a standard structure learning algorithm for complete data is available to us, which is based on the framework of two main optimizations: (i) parent set identification and (ii) structure optimization. Step (i) concerns building a list of candidate parent sets for each variable, while Step (ii) optimizes the selection of a parent set for each variable in a way to maximize the total score while ensuring that the graph is a DAG. This latter step can be tackled by exact or approximate methods Bartlett and Cussens (2013); Scanagatta et al. (2015) (in our experiments we will employ an exact method such that we are sure that the quality of results is only affected/related to the proper treatment of the missing data, but for very large domains any approximate method could be used too).\nThe exact algorithm for solving Equation (1) is based on modifying the parent set identification step. This step has no known polynomial-time solution if we do not impose a maximum number of parents (Koivisto, 2006), so we will assume that such a bound k is given. We compute the candidate list by using one of the available approaches (de Campos and Ji, 2011; Scanagatta et al., 2015) to guide the search, but for each candidate to be evaluated, the corresponding variables in the dataset might contain missing values. The first part of the transformation is to create gadgets composed of some new artificial variables which will be related to the missing values and enables the inclusion of all possible replacements of missing values by augmenting the original domain.\nOver all the dataset, for each and every missing value, let us denote it by (u, i) for sample u and variableXi, we include artificial variablesX(u,i),1, . . . , X(u,i),ri . Each X(u,i),j has two parent set candidates: (i) X ∪ {X(u,i),1+(j mod ru)} with score zero (assuming all other score values are negative, without loss of generality) and (ii) ∅ with score −λ, with λ a large enough value (e.g. greater than the sum of all other absolute scores). We further illustrate the idea via an example for variable X1 with r1 = 3: Assume m = 3, r1 = 3 and there is one missing value at (u, 1). An artificial variable is included for each possible completion zu,1, resulting in a total of three new variables, X(u,1),1, X(u,1),2, X(u,1),3. The\nfollowing gadget, consisting of two parent set candidates per artificial variable, is added to the list of parent set scores (we know that only one parent set per variable will be chosen during the optimization phase later on):\ns(X(u,1),1, {X(u,1),2, X1, X2, X3}) = 0, s(X(u,1),1, ∅) = −λ, s(X(u,1),2, {X(u,1),3, X1, X2, X3}) = 0, s(X(u,1),2, ∅) = −λ, s(X(u,1),3, {X(u,1),1, X1, X2, X3}) = 0, s(X(u,1),3, ∅) = −λ.\nAccording to this gadget, each artificial variable will either have no parent variables or all other original variables as well as one other artificial variable as its set of parents. The case with no parents leaves open the opportunity to choose the variable representing such completion as a potential parent for all original variables. In contrast, the cases with all variables as parents disables such completion from being chosen as a parent by the original variables, otherwise it would create a cycle. Due to including one artificial variable as a parent of the next artificial variable, at least one parent set among those with score zero cannot be chosen (otherwise a cycle is formed), and because they are all very good scores when compared to −λ, all but one will certainly be chosen. There is one such gadget per missing value in the original dataset, so we spend time O(R ·m · C), where C is the number of missing values.\nFinally, we return to the computation of the score for a given variable and parent set. Let Xi be the variable of interest and PAi = {Xi1 , . . . , Xiq} for which the score must be evaluated. At this moment, we consider all possible completions Z{Xi}∪PAi and compute the scores sD(Xi,PAi;Z{Xi}∪PAi) for each one of them. In order to reduce the problem to a standard structure learning without missing data, we must index these scores somehow. This is made possible via the new artificial variables:\nsD(Xi,PAi;Z{Xi}∪PAi) = sD(Xi,PAi ∪ {X(u,i),zu,j : zu,j ∈ Z{Xi}∪PAi}) ,\nthat is, for each imputed missing value zu,j appearing for variable Xi or PAi we will have an extra parent within the parent set that tells which completion was used for that missing value, according to the completion Z{Xi}∪PAi . This idea is applied to every evaluation of the score of a parent set, for every possible\ncompletion Z{Xi}∪PAi , so the final list of candidates will include only parent sets for which the completion of the data is ‘known’ at the time that the score is computed. In order to ensure that the completions are compatible among different local score computations, the gadgets explained before are enough, since they force that a certain completion be chosen for each missing value.\nTheorem 2. The exact algorithm transforms the structure learning problem with NonMAR missing data into a standard structure learning without missing data in time O(R · m · C), plus time O(n · k · Rc) per parent set evaluation, where C is the total number of missing values and c is the maximum number of missing values appearing in the variable of interest or in variables in the parent set being evaluated (hence polynomial in all parameters but c).\nThere will be many score computations and entries in the list, exponential in the number of missing values involved. So the benefit of this approach is that usually only a few variables are involved in the score computation at the same time. The drawback is that it cannot handle datasets with many missing values for the same variable, since it is Rc times slower than the corresponding parent set evaluation without missing data. Next we address this issue by proposing an approximate method (the exact method is nevertheless useful in small domains and also important to check whether the approximate version achieves reasonable results)."
    }, {
      "heading" : "2.2. Approximate Algorithm",
      "text" : "Albeit locally to the variables involved in the evaluation of a parent set, the exact method considers all possible completions of the data. This is fine with a few missing values per variable, but if there are many missing values, in particular within the same variable, the exact method becomes computationally infeasible. We propose an approximate algorithm based on a hill-climbing idea. We start with an initial guess Z0 (or several different random guesses) for the completion of all missing values in the dataset. Then we execute the very same steps of the exact algorithm, but we restrict the completions only to those\nwhich are at most t elements different from the current guess Zh. There are at most (R · m)t completions Z ′h such that HD(Zh,Z ′ h) ≤ t. We proceed as with the exact method, but applying such constraint during the transformation that was explained in the previous section. After the transformation is done, the structure optimization is run and a new structure and new data completion Zh+1 is obtained. We repeat the process until convergence, that is, until Zh+1 = Zh.\nTheorem 3. The approximate algorithm transforms the structure learning problem with NonMAR missing data into a standard structure learning without missing data in time O(R ·m ·C), plus time O(n ·k ·(R ·m)t) per parent set evaluation (C is the total number of missing values and t is the amount of locality of the approximation, as previously defined), that is, polynomial in all parameters but t.\nThe outcome of the approximate learning algorithm is the network structure as well as the completion of all the missing data values. The approximate algorithm might lead to a locally optimal solution, but on the other hand it is much more scalable than the exact algorithm.\nTheorem 4. Provided that an optimal structure learning optimization algorithm is available, the approximate algorithm always converges to a t-local optimal solution.\nIf we want to scale to very large domains, we could also resort to an approximate structure learning optimization algorithm (e.g. (Scanagatta et al., 2015)). In this case, our NonMAR approximate algorithm could be used in domains with hundreds or even thousands of variables (using very small t), but we would lose the guarantee to converge to a t-local optimal solution (it would still be a local optimum, but we would have to define its locally also in terms of the graph structures)."
    }, {
      "heading" : "3. Experiments",
      "text" : "We perform experiments on simulated as well as real-world data. The main evaluation metric used is accuracy of the imputation of missing data values, either in the form of missing values spread throughout the data, or in the form of a binary classification problem where only the class variable can contain missing values. Most of our experiments are with binary data for the sake of exposition, even though the algorithms are general and can be used with any categorical data (as shown in the last experimental setting). To test significance, we perform a paired t-test with significance level at 5%. Throughout all tables of results, a result in bold refers to an accuracy value that is significantly better than its competitors, whereas showing two results belonging to the same experiment in bold means that each of them being significantly better than the rest of the competitors. For structure optimization, we use the exact solver referred to as Gobnilp (Bartlett and Cussens, 2013) with the code available from https://www.cs.york.ac.uk/aig/sw/gobnilp/. We perform comparisons among the two proposed NonMAR algorithms (exact and approximate) and the structural Expectation-Maximization (EM) algorithm (Friedman, 1998). We compare accuracy of the three algorithms based on the percentage of correct imputations over all missing values. As for the structural EM, we have used the implementation available at https://github.com/cassiopc/csdadataimputation (Rancoita et al., 2016). After convergence, we run the prediction of missing values using a most probable explanation query. We must emphasize that the task of Bayesian network structure learning with missing values is very challenging, as it is already challenging without missing values. Therefore, we have focused on real but controlled experiments where we can effectively run the algorithms and assert their quality. We use maximum number of parents, k = 3, and use t = 1."
    }, {
      "heading" : "3.1. Well-known Bayesian Networks",
      "text" : "We perform experiments using real but small data sets in order to compare both exact and approximate NonMAR algorithms. First, we employ the original\nBayesian network model for Breast Cancer (Almeida et al., 2014), which contains 8 binary variables, we simulate 100 data instances. That model has been learned from cancer patients of the University of Wisconsin Medical Hospital. Features (Bayesian network nodes) include breast density, mass density, architectural distortion and others, in addition to the diagnosis variable whose binary value refers to benign or malignant (DOrsi et al., 2003). We include two missing values per variable, resulting in a total of 16 missing values. These missing values are generated in a NonMAR manner by randomly removing values that are equal to each other, that is, during the generation we enforce that all missing values are zero, or that all missing values are one. Imputation results of the proposed exact NonMAR learning algorithm, approximate NonMAR algorithm and structural EM are displayed in the first row of Table 1 over 100 repetitions of the experiment.\nSecond, we use the Bayesian network that has been learned from the Prostate Cancer data by the Tree Augmented Naive Bayes (TAN) (Friedman et al., 1997), implemented by WEKA (Hall et al., 2009). The Prostate Cancer data were acquired during three different moments in time (Sarabando, 2011; Almeida et al., 2014), during a medical appointment, after performing auxiliary exams, and five years after a radical prostatectomy. It contains 11 binary variables and 100 instances are generated. Again, we randomly produce two NonMAR missing values per variable, resulting in a total of 22 missing values. Results are shown in the second row of Table 1. Third, the well-known ASIA network is used (Lauritzen and Speigelhalter, 1988). We generate 100 instances according to that model, which contains 8 binary nodes. Two missing values are randomly generated according to the same NonMAR missing data process. Imputation results are displayed in the third row of Table 1. Results indicate that the algorithms proposed here are significantly better than structural EM, which is expected since in this experiment data are not MAR (and structural EM assumes MAR). More interestingly, exact and approximate NonMAR algorithms are not significantly different, which supports the use of the (more efficient) approximate method for larger domains."
    }, {
      "heading" : "3.2. (LUng CAncer Simple set) LUCAS Dataset",
      "text" : "The LUCAS dataset contains data of the LUCAS causal Bayesian network (Fogelman-Soulie, 2008) with 11 binary variables, as well as the binary class variable, and contains 2000 instances. In this experiment we conduct an analysis of both MAR and NonMAR missing data, in order to understand whether the benefits that we have seen before are only significant for the NonMAR case. Thus, we carry out two experiments: (i) NonMAR setting by randomly generating missing values belonging all to the same data value (we repeat that to both zero and one values, one at a time); (ii) MAR setting by randomly generating missing values regardless of their respective original values. These simulations are repeated 100 times.\nFirst, we generate two missing values per variable (24 missing values). A comparison between the imputation accuracy values of the NonMAR approximate algorithm and structural EM is displayed in the first two rows of Table 2 (named ‘Spread All Over’). Surprisingly, our new algorithm is significantly better than structural EM even when missing data are MAR.\nSecond, we generate 20 missing class values and repeat the experiment to\nspan all instances such that each run involves missing values belonging to different instances (without replacement). For the NonMAR experiment, each run consists of 20 identical missing class values (that is, we only make missing values of the same class, and we repeated that for both classes). For the MAR case, there is no such restriction and missing class values are randomly generated. Hence, there are 100 runs in order to cover all 2000 instances. Results of the NonMAR approximate algorithm, structural EM and SVM using different kernels (for the sake of comparison with a state-of-the-art classifier) are displayed in the bottom rows of Table 2. Results of the new algorithm are significantly better when NonMAR data are used, while the same cannot be stated for the MAR case (accuracy of the new algorithm is nevertheless superior or equivalent to the others in the MAR case)."
    }, {
      "heading" : "3.3. SPECT Dataset",
      "text" : "The Single Proton Emission Computed Tomography (SPECT) dataset consists of binary data denoting partial diagnosis from SPECT images (Lichman, 2013). Each patient (data instance) is classified into one of two categories, normal and abnormal. The SPECT data consists of 267 instances and 23 variables in total (22 binary variables and a binary class variable). We generate NonMAR missing data with different proportions, always using only one specific value (missing data proportions over all the data are 3%, 5% and 10%). These randomly generated datasets are given as input to the NonMAR approximate algorithm as well as to structural EM. We note that there is a large discrepancy in the number of data values holding each of the two binary values: About 67% of the SPECT data has a value 0, whereas merely 33% of the data has a value 1. Due to that, we also investigate the average NonMAR imputation accuracy within each data value separately, and note as well that there is some discrepancy in such accuracy values. Imputation accuracy of the NonMAR approximate learning algorithm and structural EM are displayed in Table 3. The new NonMAR algorithm is significantly better in every scenario, as expected since in this experiment data are not missing at random."
    }, {
      "heading" : "3.4. Smoking Cessation Study Dataset",
      "text" : "The dataset used in this experiment is taken from a smoking cessation study as described in Gruder et al. (1993). It has been further utilized in other works, most notably Hedeker et al. (2007). The smoking cessation dataset is a binary dataset consisting of 489 patient records (instances) with the missing data being inherently therein, i.e. there is no need to simulate missing data. The dataset contains 4 variables including the class variable, which refers to smoking or non-smoking. All the missing values are located in the class variable. There is a total of 372 patient records with observed classes, consisting of 294 smoking\nand 78 non-smoking records, as well as 117 records with missing class labels. We do not have ground truth values for the missing values.\nThe experiment we perform here is a semi-supervised learning (SSL) experiment where we evaluate the performance of the algorithms as follows: (i) We hide the class labels of a portion of the observed labels; (ii) We apply the NonMAR approximate learning on the data consisting of the originally missing and artificially hidden labels as missing values, and the rest of the data as observed values. Clearly this is a SSL experiment where the training data consists of the records with observed labels as labeled instances, records with originally missing labels as unlabeled instances, and the test instances are the records with artificially hidden labels.\nThe evaluation metric is the accuracy of the test instances using a crossvalidation approach, as usually done in classification experiment. We compare the performance of the NonMAR approximate algorithm against an equivalent procedure using structural EM (labels are then chosen based on the posterior distribution), and also against a semi-supervised learner in the form of a Laplacian SVM (Melacci and Belkin, 2011) whose code is available online. Accuracies of the NonMAR approximate algorithm, structural EM, and the semi-supervised Laplacian SVM are displayed in Table 4. Results suggest that the NonMAR algorithm is a very promising approach for SSL."
    }, {
      "heading" : "3.5. Car Evaluation Dataset",
      "text" : "The Car Evaluation dataset (Blake and Merz, 1998; Lichman, 2013) contains 1728 instances and 7 variables consisting of 6 attributes and a class. The 6 attributes refer to the following: buying, maintenance, doors, persons, luggage boots and safety. The class variable refers to the car acceptability and can have exactly one of the following values: unacceptable, acceptable, good, very good. All variables are categorical with 3 or 4 states. The data were derived from a hierarchical decision model originally developed by Bohanec and Rajkovic (1988). Similar to Section 3.2, a NonMAR classification task is performed by involving missing values belonging all to one category of the class variable at a time (this is repeated for each label). Due to the class label unbalance (unacceptable: 1210 instances, acceptable: 384, good: 69, v-good: 65), we performed 10 experiments testing only the unacceptable and acceptable labels in five each, where there are 100 randomly chosen instances with a missing label (test set) in each experiment. The proposed NonMAR algorithm is compared to structural EM and to an SVM classifier. Classification results are\ndisplayed in Table 5. Again, the NonMAR algorithm is significantly better than the others."
    }, {
      "heading" : "4. Conclusions",
      "text" : "In this paper we discuss the Bayesian network structure learning problem with missing data. We make no assumptions about the missingness process, so data are not assumed to be missing at random. We define an optimization task to tackle the problem and propose a new exact algorithm for it which translates the task into a structure learning problem without missing data. Inspired by the exact procedure, we develop an approximate algorithm which employs structure optimization as a subcall. In our experiments, we decided to use an exact structure optimizer, so we can clearly see the differences in experimental results that are yield by the different treatments of the missing data. In spite of that, any existing algorithm can be easily plugged into our framework, so the proposed approximate method can scale to domains with hundreds or even thousands of variables. We intend to investigate such avenue in future work."
    } ],
    "references" : [ {
      "title" : "Expertbayes: Automatically refining manually built Bayesian networks",
      "author" : [ "E. Almeida", "P. Ferreira", "T. Vinhoza", "I. Dutra", "Y. Wu", "E. Burnside" ],
      "venue" : "Machine Learning and Applications (ICMLA)",
      "citeRegEx" : "Almeida et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Almeida et al\\.",
      "year" : 2014
    }, {
      "title" : "Advances in Bayesian network learning using integer programming",
      "author" : [ "M. Bartlett", "J. Cussens" ],
      "venue" : "Conference on Uncertainty in artificial intelligence (UAI)",
      "citeRegEx" : "Bartlett and Cussens,? \\Q2013\\E",
      "shortCiteRegEx" : "Bartlett and Cussens",
      "year" : 2013
    }, {
      "title" : "UCI machine learning repository of machine learning databases",
      "author" : [ "C. Blake", "C. Merz" ],
      "venue" : null,
      "citeRegEx" : "Blake and Merz,? \\Q1998\\E",
      "shortCiteRegEx" : "Blake and Merz",
      "year" : 1998
    }, {
      "title" : "Knowledge acquisition and explanation for multi-attribute decision making",
      "author" : [ "M. Bohanec", "V. Rajkovic" ],
      "venue" : "Intl. Workshop on Expert Systems and their Applications",
      "citeRegEx" : "Bohanec and Rajkovic,? \\Q1988\\E",
      "shortCiteRegEx" : "Bohanec and Rajkovic",
      "year" : 1988
    }, {
      "title" : "Learning Bayesian network equivalence classes from incomplete data",
      "author" : [ "H. Borchani", "N.B. Amor", "K. Mellouli" ],
      "venue" : "Lecture Notes in Comp",
      "citeRegEx" : "Borchani et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Borchani et al\\.",
      "year" : 2006
    }, {
      "title" : "Learning Bayesian networks is NP-complete",
      "author" : [ "D. Chickering" ],
      "venue" : "Learning from Data,",
      "citeRegEx" : "Chickering,? \\Q1996\\E",
      "shortCiteRegEx" : "Chickering",
      "year" : 1996
    }, {
      "title" : "A Bayesian method for the induction of probabilistic networks from data",
      "author" : [ "G. Cooper", "E. Herskovits" ],
      "venue" : "Machine Learning",
      "citeRegEx" : "Cooper and Herskovits,? \\Q1992\\E",
      "shortCiteRegEx" : "Cooper and Herskovits",
      "year" : 1992
    }, {
      "title" : "Efficient structure learning of Bayesian networks using constraints",
      "author" : [ "C. de Campos", "Q. Ji" ],
      "venue" : "Journal of Machine Learning Research (JMLR)",
      "citeRegEx" : "Campos and Ji,? \\Q2011\\E",
      "shortCiteRegEx" : "Campos and Ji",
      "year" : 2011
    }, {
      "title" : "BI-RADS: Mammography",
      "author" : [ "C. DOrsi", "L. Bassett", "W Berg" ],
      "venue" : "American College of Radiology",
      "citeRegEx" : "DOrsi et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "DOrsi et al\\.",
      "year" : 2003
    }, {
      "title" : "Mining massive data sets for security: Advances in data mining, search, social networks and text mining, and their applications to security",
      "author" : [ "F. Fogelman-Soulie" ],
      "venue" : null,
      "citeRegEx" : "Fogelman.Soulie,? \\Q2008\\E",
      "shortCiteRegEx" : "Fogelman.Soulie",
      "year" : 2008
    }, {
      "title" : "The Bayesian structural em algorithm",
      "author" : [ "N. Friedman" ],
      "venue" : "Conference on Uncertainty in artificial intelligence (UAI)",
      "citeRegEx" : "Friedman,? \\Q1998\\E",
      "shortCiteRegEx" : "Friedman",
      "year" : 1998
    }, {
      "title" : "Bayesian network classifiers",
      "author" : [ "N. Friedman", "D. Geiger", "M. Goldszmidt" ],
      "venue" : "Machine Learning",
      "citeRegEx" : "Friedman et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Friedman et al\\.",
      "year" : 1997
    }, {
      "title" : "Effects of social support and relapse prevention training as adjuncts to a televised smoking cessation intervention",
      "author" : [ "R. Warnecke", "R. Burzette", "T. Miller" ],
      "venue" : "J Consult Clin Psychol",
      "citeRegEx" : "J. et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "J. et al\\.",
      "year" : 1993
    }, {
      "title" : "The WEKA data mining software: An update",
      "author" : [ "M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I. Witten" ],
      "venue" : "SIGKDD Explor. Newsl",
      "citeRegEx" : "Hall et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Hall et al\\.",
      "year" : 2009
    }, {
      "title" : "Learning Bayesian networks: The combination of knowledge and statistical data",
      "author" : [ "D. Heckerman", "D. Geiger", "D. Chickering" ],
      "venue" : "Machine Learning",
      "citeRegEx" : "Heckerman et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Heckerman et al\\.",
      "year" : 1995
    }, {
      "title" : "Analysis of binary outcomes with missing data: missing = smoking, last observation carried forward",
      "author" : [ "D. Hedeker", "J. Mermelstein", "H. Demirtas" ],
      "venue" : null,
      "citeRegEx" : "Hedeker et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Hedeker et al\\.",
      "year" : 2007
    }, {
      "title" : "Parent assignment is hard for the mdl, aic, and nml",
      "author" : [ "M. Koivisto" ],
      "venue" : "costs. conference on Learning Theory (COLT)",
      "citeRegEx" : "Koivisto,? \\Q2006\\E",
      "shortCiteRegEx" : "Koivisto",
      "year" : 2006
    }, {
      "title" : "Local computations with probabilities on graphical structures and their application to expert systems",
      "author" : [ "S. Lauritzen", "D. Speigelhalter" ],
      "venue" : "Royal statistical Society B",
      "citeRegEx" : "Lauritzen and Speigelhalter,? \\Q1988\\E",
      "shortCiteRegEx" : "Lauritzen and Speigelhalter",
      "year" : 1988
    }, {
      "title" : "Bayesian network structural learning and incomplete data",
      "author" : [ "P. Leray", "O. Francois" ],
      "venue" : "Intl. and Interdisc. Conf. on Adaptive Knowledge Repr. and Reasoning (AKRR),",
      "citeRegEx" : "Leray and Francois,? \\Q2005\\E",
      "shortCiteRegEx" : "Leray and Francois",
      "year" : 2005
    }, {
      "title" : "Statistical analysis with missing data",
      "author" : [ "R. Little", "D. Rubin" ],
      "venue" : null,
      "citeRegEx" : "Little and Rubin,? \\Q1987\\E",
      "shortCiteRegEx" : "Little and Rubin",
      "year" : 1987
    }, {
      "title" : "Estimating dependency structure as a hidden variable",
      "author" : [ "M. Meila", "M. Jordan" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Meila and Jordan,? \\Q1998\\E",
      "shortCiteRegEx" : "Meila and Jordan",
      "year" : 1998
    }, {
      "title" : "Laplacian support vector machines trained in the primal",
      "author" : [ "S. Melacci", "M. Belkin" ],
      "venue" : "Journal of Machine Learning Research (JMLR)",
      "citeRegEx" : "Melacci and Belkin,? \\Q2011\\E",
      "shortCiteRegEx" : "Melacci and Belkin",
      "year" : 2011
    }, {
      "title" : "Learning Bayesian networks from incomplete databases",
      "author" : [ "M. Ramoni", "P. Sebastiani" ],
      "venue" : "Conference on Uncertainty in artificial intelligence (UAI)",
      "citeRegEx" : "Ramoni and Sebastiani,? \\Q1997\\E",
      "shortCiteRegEx" : "Ramoni and Sebastiani",
      "year" : 1997
    }, {
      "title" : "Bayesian network data imputation with application to survival tree analysis",
      "author" : [ "P. Rancoita", "M. Zaffalon", "E. Zucca", "F. Bertoni", "C. de Campos" ],
      "venue" : "Computational Statistics & Data Analysis",
      "citeRegEx" : "Rancoita et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Rancoita et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning Bayesian networks from incomplete data: An efficient method for generating approximate predictive distributions",
      "author" : [ "C. Riggelsen" ],
      "venue" : null,
      "citeRegEx" : "Riggelsen,? \\Q2006\\E",
      "shortCiteRegEx" : "Riggelsen",
      "year" : 2006
    }, {
      "title" : "Learning Bayesian network models from incomplete data using importance sampling",
      "author" : [ "C. Riggelsen", "A. Feelders" ],
      "venue" : null,
      "citeRegEx" : "Riggelsen and Feelders,? \\Q2005\\E",
      "shortCiteRegEx" : "Riggelsen and Feelders",
      "year" : 2005
    }, {
      "title" : "Um estudo do comportamento de redes Bayesianas no prognstico da sobrevivencia no cancro da prostata",
      "author" : [ "A. Sarabando" ],
      "venue" : "M.Sc. thesis, Universidade do Porto",
      "citeRegEx" : "Sarabando,? \\Q2011\\E",
      "shortCiteRegEx" : "Sarabando",
      "year" : 2011
    }, {
      "title" : "Learning Bayesian networks with thousands of variables",
      "author" : [ "M. Scanagatta", "C. de Campos", "G. Corani", "M. Zaffalon" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Scanagatta et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Scanagatta et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 19,
      "context" : "Results can be incomplete in an industrial experiment due to mechanical breakdowns not necessarily related to the performed experiment (Little and Rubin, 1987).",
      "startOffset" : 135,
      "endOffset" : 159
    }, {
      "referenceID" : 19,
      "context" : "While data missingness in the above examples can mostly be assumed to be generated by a random process which depends only on the observed data, usually referred to as missing at random (MAR) (Little and Rubin, 1987; Rancoita et al., 2016), this assumption might miserably fail in other examples.",
      "startOffset" : 191,
      "endOffset" : 238
    }, {
      "referenceID" : 23,
      "context" : "While data missingness in the above examples can mostly be assumed to be generated by a random process which depends only on the observed data, usually referred to as missing at random (MAR) (Little and Rubin, 1987; Rancoita et al., 2016), this assumption might miserably fail in other examples.",
      "startOffset" : 191,
      "endOffset" : 238
    }, {
      "referenceID" : 14,
      "context" : "Given a dataset with categorical random variables, the Bayesian network structure learning problem refers to finding the best network structure (a directed acyclic graph, or DAG) according to a score function based on the data (Heckerman et al., 1995).",
      "startOffset" : 227,
      "endOffset" : 251
    }, {
      "referenceID" : 5,
      "context" : "As well known, learning a Bayesian network from complete data is NP-complete (Chickering, 1996), and the task becomes even harder with incomplete data.",
      "startOffset" : 77,
      "endOffset" : 95
    }, {
      "referenceID" : 4,
      "context" : "Many other algorithms have used ideas from structural EM and deal separately with the missing values and the structure optimization using complete data (Borchani et al., 2006; Leray and Francois, 2005; Meila and Jordan, 1998; Ramoni and Sebastiani, 1997; Riggelsen, 2006; Riggelsen and Feelders, 2005).",
      "startOffset" : 152,
      "endOffset" : 301
    }, {
      "referenceID" : 18,
      "context" : "Many other algorithms have used ideas from structural EM and deal separately with the missing values and the structure optimization using complete data (Borchani et al., 2006; Leray and Francois, 2005; Meila and Jordan, 1998; Ramoni and Sebastiani, 1997; Riggelsen, 2006; Riggelsen and Feelders, 2005).",
      "startOffset" : 152,
      "endOffset" : 301
    }, {
      "referenceID" : 20,
      "context" : "Many other algorithms have used ideas from structural EM and deal separately with the missing values and the structure optimization using complete data (Borchani et al., 2006; Leray and Francois, 2005; Meila and Jordan, 1998; Ramoni and Sebastiani, 1997; Riggelsen, 2006; Riggelsen and Feelders, 2005).",
      "startOffset" : 152,
      "endOffset" : 301
    }, {
      "referenceID" : 22,
      "context" : "Many other algorithms have used ideas from structural EM and deal separately with the missing values and the structure optimization using complete data (Borchani et al., 2006; Leray and Francois, 2005; Meila and Jordan, 1998; Ramoni and Sebastiani, 1997; Riggelsen, 2006; Riggelsen and Feelders, 2005).",
      "startOffset" : 152,
      "endOffset" : 301
    }, {
      "referenceID" : 24,
      "context" : "Many other algorithms have used ideas from structural EM and deal separately with the missing values and the structure optimization using complete data (Borchani et al., 2006; Leray and Francois, 2005; Meila and Jordan, 1998; Ramoni and Sebastiani, 1997; Riggelsen, 2006; Riggelsen and Feelders, 2005).",
      "startOffset" : 152,
      "endOffset" : 301
    }, {
      "referenceID" : 25,
      "context" : "Many other algorithms have used ideas from structural EM and deal separately with the missing values and the structure optimization using complete data (Borchani et al., 2006; Leray and Francois, 2005; Meila and Jordan, 1998; Ramoni and Sebastiani, 1997; Riggelsen, 2006; Riggelsen and Feelders, 2005).",
      "startOffset" : 152,
      "endOffset" : 301
    }, {
      "referenceID" : 22,
      "context" : "However, that algorithm works as a data augmentation approach related to MAR (Ramoni and Sebastiani, 1997) instead of considering NonMAR.",
      "startOffset" : 77,
      "endOffset" : 106
    }, {
      "referenceID" : 9,
      "context" : "The seminal algorithm in Friedman (1998) introduced an iterative method based on the Expectation-Maximization (EM) technique, referred to as structural EM.",
      "startOffset" : 25,
      "endOffset" : 41
    }, {
      "referenceID" : 4,
      "context" : "Many other algorithms have used ideas from structural EM and deal separately with the missing values and the structure optimization using complete data (Borchani et al., 2006; Leray and Francois, 2005; Meila and Jordan, 1998; Ramoni and Sebastiani, 1997; Riggelsen, 2006; Riggelsen and Feelders, 2005). In Rancoita et al. (2016), structures are learned from incomplete data using a structural EM whose maximization step is performed by an anytime method, and the ‘expectation’ step imputes the missing values using expected means, or modes, of the current estimated joint distribution.",
      "startOffset" : 153,
      "endOffset" : 329
    }, {
      "referenceID" : 6,
      "context" : "We consider here the score function sD to be the Bayesian Dirichlet Equivalent Uniform (BDeu) criterion (Buntine, 1991; Cooper and Herskovits, 1992) (other decomposable scores could be used too), so we have sD(G) = ∑ i sD(Xi,PAi).",
      "startOffset" : 104,
      "endOffset" : 148
    }, {
      "referenceID" : 5,
      "context" : "Hardness is obtained by realizing that this problem generalizes the structure learning problem without missing data, which is NP-hard Chickering (1996). Pertinence in NP holds since given G and Z, the score function sD can be computed in polynomial time.",
      "startOffset" : 134,
      "endOffset" : 152
    }, {
      "referenceID" : 16,
      "context" : "This step has no known polynomial-time solution if we do not impose a maximum number of parents (Koivisto, 2006), so we will assume that such a bound k is given.",
      "startOffset" : 96,
      "endOffset" : 112
    }, {
      "referenceID" : 27,
      "context" : "We compute the candidate list by using one of the available approaches (de Campos and Ji, 2011; Scanagatta et al., 2015) to guide the search, but for each candidate to be evaluated, the corresponding variables in the dataset might contain missing values.",
      "startOffset" : 71,
      "endOffset" : 120
    }, {
      "referenceID" : 1,
      "context" : "This latter step can be tackled by exact or approximate methods Bartlett and Cussens (2013); Scanagatta et al.",
      "startOffset" : 64,
      "endOffset" : 92
    }, {
      "referenceID" : 1,
      "context" : "This latter step can be tackled by exact or approximate methods Bartlett and Cussens (2013); Scanagatta et al. (2015) (in our experiments we will employ an exact method such that we are sure that the quality of results is only affected/related to the proper treatment of the missing data, but for very large domains any approximate method could be used too).",
      "startOffset" : 64,
      "endOffset" : 118
    }, {
      "referenceID" : 27,
      "context" : "(Scanagatta et al., 2015)).",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 1,
      "context" : "For structure optimization, we use the exact solver referred to as Gobnilp (Bartlett and Cussens, 2013) with the code available from https://www.",
      "startOffset" : 75,
      "endOffset" : 103
    }, {
      "referenceID" : 10,
      "context" : "We perform comparisons among the two proposed NonMAR algorithms (exact and approximate) and the structural Expectation-Maximization (EM) algorithm (Friedman, 1998).",
      "startOffset" : 147,
      "endOffset" : 163
    }, {
      "referenceID" : 23,
      "context" : "com/cassiopc/csdadataimputation (Rancoita et al., 2016).",
      "startOffset" : 32,
      "endOffset" : 55
    }, {
      "referenceID" : 0,
      "context" : "Bayesian network model for Breast Cancer (Almeida et al., 2014), which contains 8 binary variables, we simulate 100 data instances.",
      "startOffset" : 41,
      "endOffset" : 63
    }, {
      "referenceID" : 8,
      "context" : "Features (Bayesian network nodes) include breast density, mass density, architectural distortion and others, in addition to the diagnosis variable whose binary value refers to benign or malignant (DOrsi et al., 2003).",
      "startOffset" : 196,
      "endOffset" : 216
    }, {
      "referenceID" : 11,
      "context" : "Second, we use the Bayesian network that has been learned from the Prostate Cancer data by the Tree Augmented Naive Bayes (TAN) (Friedman et al., 1997), implemented by WEKA (Hall et al.",
      "startOffset" : 128,
      "endOffset" : 151
    }, {
      "referenceID" : 13,
      "context" : ", 1997), implemented by WEKA (Hall et al., 2009).",
      "startOffset" : 29,
      "endOffset" : 48
    }, {
      "referenceID" : 26,
      "context" : "The Prostate Cancer data were acquired during three different moments in time (Sarabando, 2011; Almeida et al., 2014), during a medical appointment, after performing auxiliary exams, and five years after a radical prostatectomy.",
      "startOffset" : 78,
      "endOffset" : 117
    }, {
      "referenceID" : 0,
      "context" : "The Prostate Cancer data were acquired during three different moments in time (Sarabando, 2011; Almeida et al., 2014), during a medical appointment, after performing auxiliary exams, and five years after a radical prostatectomy.",
      "startOffset" : 78,
      "endOffset" : 117
    }, {
      "referenceID" : 17,
      "context" : "Third, the well-known ASIA network is used (Lauritzen and Speigelhalter, 1988).",
      "startOffset" : 43,
      "endOffset" : 78
    }, {
      "referenceID" : 9,
      "context" : "The LUCAS dataset contains data of the LUCAS causal Bayesian network (Fogelman-Soulie, 2008) with 11 binary variables, as well as the binary class variable, and contains 2000 instances.",
      "startOffset" : 69,
      "endOffset" : 92
    }, {
      "referenceID" : 15,
      "context" : "It has been further utilized in other works, most notably Hedeker et al. (2007). The smoking cessation dataset is a binary dataset consisting of 489 patient records (instances) with the missing data being inherently therein, i.",
      "startOffset" : 58,
      "endOffset" : 80
    }, {
      "referenceID" : 21,
      "context" : "We compare the performance of the NonMAR approximate algorithm against an equivalent procedure using structural EM (labels are then chosen based on the posterior distribution), and also against a semi-supervised learner in the form of a Laplacian SVM (Melacci and Belkin, 2011) whose code is available online.",
      "startOffset" : 251,
      "endOffset" : 277
    }, {
      "referenceID" : 2,
      "context" : "The Car Evaluation dataset (Blake and Merz, 1998; Lichman, 2013) contains 1728 instances and 7 variables consisting of 6 attributes and a class.",
      "startOffset" : 27,
      "endOffset" : 64
    }, {
      "referenceID" : 2,
      "context" : "The Car Evaluation dataset (Blake and Merz, 1998; Lichman, 2013) contains 1728 instances and 7 variables consisting of 6 attributes and a class. The 6 attributes refer to the following: buying, maintenance, doors, persons, luggage boots and safety. The class variable refers to the car acceptability and can have exactly one of the following values: unacceptable, acceptable, good, very good. All variables are categorical with 3 or 4 states. The data were derived from a hierarchical decision model originally developed by Bohanec and Rajkovic (1988). Similar to Section 3.",
      "startOffset" : 28,
      "endOffset" : 552
    } ],
    "year" : 2017,
    "abstractText" : "We present new algorithms for learning Bayesian networks from data with missing values without the assumption that data are missing at random (MAR). An exact Bayesian network learning algorithm is obtained by recasting the problem into a standard Bayesian network learning problem without missing data. To the best of our knowledge, this is the first exact algorithm for this problem. As expected, the exact algorithm does not scale to large domains. We build on the exact method to create a new approximate algorithm using a hill-climbing technique. This algorithm scales to large domains so long as a suitable standard structure learning method for complete data is available. We perform a wide range of experiments to demonstrate the benefits of learning Bayesian networks without assuming MAR.",
    "creator" : "LaTeX with hyperref package"
  }
}