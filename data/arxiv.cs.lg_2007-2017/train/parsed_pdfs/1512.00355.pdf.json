{
  "name" : "1512.00355.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Taxonomy grounded aggregation of classifiers with different label sets",
    "authors" : [ "Amrita Saha", "Sathish Indurthi", "Shantanu Godbole", "Subendhu Rongali", "Vikas C. Raykar" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "We describe the problem of aggregating the label predictions of diverse classifiers using a class taxonomy. Such a taxonomy may not have been available or referenced when the individual classifiers were designed and trained, yet mapping the output labels into the taxonomy is desirable to integrate the effort spent in training the constituent classifiers. A hierarchical taxonomy representing some domain knowledge may be different from, but partially mappable to, the label sets of the individual classifiers. We present a heuristic approach and a principled graphical model to aggregate the label predictions by grounding them into the available taxonomy. Our model aggregates the labels using the taxonomy structure as constraints to find the most likely hierarchically consistent class. We experimentally validate our proposed method on image and text classification tasks."
    }, {
      "heading" : "1 Introduction",
      "text" : "In several real-world classification problems (for example visual object recognition [12], text categorization [9, 19], web content classification [5], US Patent codes, ICD [1] codes of diseases [15] etc.) the classes to be predicted are naturally organized into a large predefined class hierarchy or a class taxonomy—typically a tree or a DAG (Directed Acyclic Graph). However most state-of-the-art results are obtained with flat classifiers which typically ignore the class hierarchy and treat each class separately. It could be that the taxonomy was not available while training the classi-\nPreliminary work. Under review by AISTATS 2016. Do not distribute.\nfiers or that it was not explicitly used. These hierarchy agnostic flat classifiers can be either multi-class, multi-label, or binary classifiers possibly trained on subsets of the classes. In this paper we address the problem of aggregating the output of multiple such flat classifiers using the pre-defined class taxonomy, since several applications need grounded references to such background taxonomies.\nAs a motivating example we consider the task of visual object recognition [12, 7]—given an image (or a region in the image) the task is to predict the most likely object in the image. Considerable amount of progress has been achieved in the computer vision community and various pre-trained state-of-the-art flat classifiers are available. Since the number of possible objects is quite large these classifiers are trained with different datasets and class labels1 (not necessarily mutually exclusive). Considerable amount of research effort and also CPU time(especially in the case of convolutional neural network based approaches [12, 24] which takes weeks to months to train) has been spent on training these classifiers. Hence we are interested in reusing multiple such classifiers on a given image, aggregating the scores and predicting the final label. However combining such pre-trained flat classifiers poses its own set of challenges which we list below and address in this paper.\nDifferent class labels—The classifiers are generally trained with different class labels based on the labeled dataset it was trained on. For example we may have one classifier trained with a label set {dog, cat, bird} while another classifier could have been trained with the label set {doberman, airplane, animal}. We will use the class taxonomy in order to ground the different label sets into a common space. For this domain we use the Wordnet [16] 2 , which is a DAG structured class\n1For example the CIFAR-100 dataset [11] has 100 class labels, the PASCAL-VOC datset [6] has 20 class labels, and the latest ImageNet ILSVRC challenge dataset [21] has 1000 class labels.\n2WordNet [16] is a large lexical database of english\nar X\niv :1\n51 2.\n00 35\n5v 1\n[ cs\n.A I]\n1 D\nec 2\n01 5\ntaxonomy organized by the hypernym-hyponym hierarchy. The Wordnet encodes our prior world knowledge that doberman is a dog and dog is an animal.\nHierarchically inconsistent predictions—Since the flat classifiers are trained ignoring the taxonomy, the class predictions may not be hierarchically consistent. Consider, for example, a classifier trained with class labels {doberman, dog, cat}. Since the class labels are not mutually exclusive, it is possible that the classifier may give a high score for doberman and a low score for dog, though the taxonomy implies that a doberman is a dog and hence dog should also receive a high score. The same problem persists across different classifiers—if one classifier predicts an instance as a dog and another as doberman, we need to aggregate the two classifiers in a hierarchically consistent way.\nDifferent classifier accuracies—The individual classifiers can have (unknown) different accuracies, which have to be accounted for when aggregating the classifiers. Moreover the reported accuracies are based on the classes on which it was trained on and also ignore the hierarchy. We model the classifier performance using the taxonomy and estimate the accuracies using a validation set.\nDepth of the predicted class label—By using the class taxonomy we can actually predict a class label which is not in the label sets used to train the classifiers. We generalize the notion of the label of an instance to a path in the taxonomy. The path starts at the root of the taxonomy and terminates at any class (not necessarily the leaf node) in the taxonomy. We propose strategies to decide where to terminate the path to give a final prediction.\nRelated work—There is a rich literature in the area of hierarchical classification (see [23] for a survey), which deals with training classifiers by explicitly accounting for the class hierarchy. However in this paper we are primarily concerned with aggregating pretrained flat classifiers in a hierarchically consistent way. We do not attempt to re-train any classifiers using the taxonomy.\nThe proposed algorithms can also be used to aggregate (hierarchical) labels collected from multiple annotators via crowdsourcing. While sophisticated techniques exist for binary, categorical and ordinal labels [20] to the best of our knowledge there are no methods for labels\nwhere nouns are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept (around 80k synsets). Synsets are interlinked by means of conceptualsemantic and lexical relations. The most frequently encoded relation among synsets is the super-subordinate relation (also called hyperonymy, hyponymy or IS-A relation). It links more general synsets like {dog} to increasingly specific ones like {doberman}.\ndefined by a taxonomy.\nAnother area of research related to our problem setting is that of integrating (or mapping) label-sets into each other as in the case of e-commerce catalog integration [2, 22, 18]. In catalog integration the problem is mapping a source product taxonomy (of a seller) containing textual descriptions of products on to a target master taxonomy (of the ecommerce site). Various techniques for this involve jointly learning class mappings with or without data labeled with both labelsets, and estimating and re-training the master taxonomy using statistics and/or structure of the source taxonomy. The specific formulation we propose is quite different from such a label-set mapping problem since we want to lazily combine only the predictions of differently trained classifiers into a background taxonomy. As in our motivating example of mapping object classification predictions to WordNet, we want to re-use the effort spent in creating and tuning existing classifiers and view them through the lens of WordNet to naturally describe objects in images as per its world knowledge.\nIn our problem setting the constituent classifiers are free to evolve and change, and have their own area of expertise. Thus ensemble meta-learning methods that learn accuracy estimates or dynamic model selection techniques do not apply to our setting.\nOrganization—§ 2 introduces the notation and the problem statement. We formally define the notion of class taxonomy and hierarchical consistency in a taxonomy. § 3 presents a heuristic solution based on propagating scores in the class taxonomy and generalizes the notion of the label of an instance to a path in the taxonomy and specifies when to terminate the path. § 4 presents the proposed graphical model. In § 5 we discuss some extensions and present an EM algorithm to estimate the parameters of the graphical model without having access to any validation set. § 6 experimentally validates the various approaches on a visual object recognition and a text classification dataset."
    }, {
      "heading" : "2 Notation and problem statement",
      "text" : "Class Taxonomies—In this paper we are concerned with classification problems where the class labels are organized hierarchically into class taxonomies. The hierarchy imposes a parent-child IS-A relation among the classes—an instance belonging in a specific class, also belongs in all its ancestor classes. Formally a class taxonomy [23] is defined as a pair (C,≺), where C = {c1, . . . , ck} is a finite set of k classes organized hierarchically with the IS-A relationship ≺. For any two classes ci and cj the relation ci ≺ cj means that\nci is a sub-class of cj . The relationship ≺ satisfies the following properties: (1) Asymmetry If ci ≺ cj then cj ⊀ ci, ∀ci, cj ∈ C. (2) Anti-reflexivity ci ⊀ ci, ∀ci ∈ C. (3) Transitivity If ci ≺ cj and cj ≺ ck then ci ≺ ck, ∀ci, cj , ck ∈ C. In graphs with cycles, only the transitivity property holds. In this paper we consider only hierarchies without cycles. The parent of a class c ∈ C is denoted as ↑ (c) and its child is denoted as ↓ (c). The descendants and ancestors are denoted ⇓ (c) and ⇑ (c) respectively.\nThe taxonomy can either be a tree (where the classes have a single parent) or a directed acyclic graph(DAG) (where the classes can have multiple parents). The Wordnet is a DAG taxonomy organized by the hypernym-hyponym hierarchy (See Figure 1 for an illustration).\nFlat pre-trained classifiers on taxonomy subsets—We have a set of m trained classifiers f1, . . . , fm. Each classifier f j is trained using a subset of kj ≤ k classes Cj = {c1, . . . , ckj}. We work with the assumption that these classifiers are flat multiclass classifiers 3 trained without the knowledge of the class hierarchy. It is also possible that the same classifier may have a category for a parent class (for example dog) and also a separate category for a descendant class as well (for example doberman). Let yj(c) = f j(x, c) be the score assigned to class c to an instance x by the classifier f j . Without loss of generality we assume that these scores are probabilities 4, that is, yj(c) = Pr(z(c) = 1|x, f j), where z(c) is the true binary label of the instance for class c in\n3The classifier can also be a binary classifier trained for a class c.\n4For real valued scores they can be converted to probabilities via the soft-max function or via some calibration techniques [25].\nthe taxonomy. We further note that the accuracies of the m classifiers are different. In the graphical model presented later (§ 4) we estimate the accuracies via a validation set and later present an EM algorithm (§ 5) to estimate them without using a validation set.\nWe implicity assume that the classifier label sets are mappable to some classes in the taxonomy. If this is not true then we can approximate the mappings using class mapping techniques [2, 22, 18].\nGeneralization of the instance label to paths in the taxonomy—Given a taxonomy the true label of any instance x is completely specified by one of the classes in the leaf nodes of the hierarchy. However in practice the class label may be specified by any class of the taxonomy higher than the leaf node. For example, the true label for an instance x maybe doberman (which is a leaf node in the Wordnet taxonomy), however the class label as specified by an annotator (or the classifier) could be one of its hypernyms, for example, dog.\nWe generalize the notion of the label of an instance x to a label path in the taxonomy. The path starts at the root of the taxonomy and terminates at any class in the taxonomy. Paths that end at the leaf node are completely specified. For any instance x with class label c (not necessarily the leaf node) we denote path(c) = {c, ↑ (c), ↑ (↑ (c)), . . . , } as the set of all classes starting from c to the root node. For a tree taxonomy there is one unique path from the terminal node c to the root. For a DAG taxonomy there could be multiple such paths 5.\nTaxonomy consistency—Since the classifiers are trained without the knowledge of the taxonomy, it is very likely that the scores may not be hierarchically consistent. For example, an instance may possibly have a higher score for doberman than dog. However from the taxonomy we know that a doberman is a dog, hence dog should have as high a score as doberman. We define the following notion of taxonomy consistency : For an instance x with true class label c, that is if z(c) = 1, taxonomy consistency implies z(cj) = 1 for all cj ∈⇑ (c), where ⇑ (c) are ancestors of c. This implies that Pr[z(cj) = 1|z(c) = 1] = 1 for all cj ∈⇑ (c).\n5For example, in the Wordnet hierarchy the following two hypernym paths can be found for the class doberman since dog has two parents domestic animal and canine.\npath1(doberman) = [doberman, pinscher, watchdog, working dog, dog, domestic animal, animal, organism,living thing, whole, object, physical entity, entity]\npath2(doberman) = [doberman, pinscher, watchdog, working dog, dog, canine, carnivore, placental, mammal, vertebrate, chordate, animal, organism, living thing, whole, object, physical entity, entity]\nProblem statement—Given a set of n instances x1, . . . ,xn, a taxonomy (C,≺), where C = {c1, . . . , ck} is a set of k classes organized hierarchically with the IS-A relationship ≺, a set of m trained classifiers f1, . . . , fm, each classifier f j is trained using a subset of kj ≤ k classes Cj = {c1, . . . , ckj}, and yji (c) be the score assigned to class c ∈ Cj for the instance xi by the classifier f j , the task is to aggregate the scores and estimate the best path(s) in the taxonomy pathi(c) for every instance xi such that the true class label c is taxonomically consistent. We could have multiple paths and each path need not necessarily end at a leaf node 6."
    }, {
      "heading" : "3 Score propagation in the class taxonomy",
      "text" : "We will first present a heuristic solution by propagating the classifier scores upward from a particular class to all its ancestors in the taxonomy by navigating the IS-A hierarchy upwards. The scores from multiple classifiers at a node are then aggregated by summing them up. The final path is then estimated by traversing the taxonomy from the root and terminating at a class based on the entropy of the children. Specifically (See Figure 2 for an illustration)\n• First construct an induced sub-graph with classes CI = {C1 ∪ . . . ∪ Cm} (which is the union of all classes from the m flat classifiers f1, . . . , fm) and all its ancestors.\n• Initialize the scores p(c) of each of the class-nodes c in the graph to 0.\n• For each classifier f j , add the scores yj(c) ∀ c ∈ Cj to p(c′) where c′ ∈ ⇑ (c) ∪ c.\n• Initialize path to null. Starting with the root node, recursively perform the following steps for every node in the path being constructed.\n– Add node c to path. – Let Yc be the set of scores p(c ′) ∀ c′ ∈↓ (c).\n– Calculate the entropy gc of the set Yc and normalize it based on the number of children. – if gc > θ (a pre defined threshold) or ↓ (c) is null, quit. – else select the node in ↓ (c) with the highest value of score p.\nBy calculating the entropy at a particular node, we are trying to check if we can go further down the taxonomy\n6This is the case when we do not have enough confidence to make a prediction till the leaf node. In such case we backoff to an ancestor.\nfor the instance. If the children have equal scores (if the impurity exceeds a threshold value θ.) then we do have have enough evidence to make a decision. Hence we back-off and terminate the path at this node."
    }, {
      "heading" : "4 The proposed probabilistic graphical model for aggregating classifiers",
      "text" : "The heuristic method assumes that all the classifiers have the same performance and then aggregates the scores. In this section we cast the label aggregation problem as an inference problem in an appropriately defined graphical model (for a given instance x). The proposed graphical model (or Bayesian network) has two kinds of nodes, discrete binary nodes corresponding to hierarchically organized classes in the taxonomy and continuous nodes corresponding to the m classifier scores (see Figure 3, which is the graphical model corresponding to the induced sub-graph in Figure 2).\nEach class c in the taxonomy corresponds to a binary discrete node z(c) which is the true (unknown) binary label of the instance x for class c in the taxonomy. All the discrete nodes z1, . . . , zK are organized hierarchically with the IS-A relationship ≺ defined by the class taxonomy—which defines the conditional independence assumptions of the graphical model. Each binary node zk is conditioned on its child ↓ (zk) node. Taxonomy consistency is ensured by appropriately set-\nting the conditional probability distribution as follows:\nPr[zk = 1|ch(zk)] = 1 if any ↓ (zk) = 1, (1)\nwhere ch(zk) are all the children of zk. The other entries Pr[zk = 1|ch(zk)] are estimated using the structure of the taxonomy 7.\nFor a classifier f j and a class ck ∈ Cj (which is in the classifier class set) we have a continuous node yjk which is conditioned on the discrete node zk.We assume that conditioned on the true label zk the classifier score y j k is normally distributed 8 , that is,\nyjk|zk = 1 ∼ N ( yjk|µ j 1k, (σ j 1k) 2 )\n(2)\nyjk|zk = 0 ∼ N ( yjk|µ j 0k, (σ j 0k) 2 )\n(3)\n7This is a reasonable estimate in the absence of any other information and works well in practice. If the actual counts from a corpus are available (for example the Brown corpus for WordNet) for each class in the taxonomy we can get more precise estimates.\n8This is a reasonable model for scores constructed as a linear (or non-linear) combination of many features. For probabilistic classifiers since the scores lie in the range [0, 1] we first apply a logit or the inverse softmax transformation to the scores which makes the scores approximately normal.\nwhere N ( x|µ, σ2 ) is a normal distribution with mean µ and variance σ2. The model parameters θjk ={ µj0k, σ j 0k, µ j 1k, σ j 1k } are estimated by using a validation set with known class labels. We further assume that conditioned on the true label zk the m classifiers are independent.\nHaving specified the graphical model and the model parameters the task is to find the most likely configuration of the class nodes zk given the values for the classifier nodes yjk. We actually compute the marginal probability for each of the class nodes and then traverse the taxonomy using the method described in § 3. The graphical model which we have described is a mixed discrete-gaussian network [3]. A mixed discretegaussian network consists of both discrete and continuous nodes. The models assumes that the conditional distribution of the continuous variables, given the discrete, is multivariate Gaussian. For such networks exact inference algorithms exist [13], which permits the local computation on the junction tree of exact probabilities, means and variances."
    }, {
      "heading" : "5 Extensions",
      "text" : "Discrete labels—We can also incorporate classifiers which produce discrete labels. Instead of the bi-normal distribution we have the following two parameters which define the conditional probability distribution at each classifier node: αjk := Pr[y j k = 1|zk = 1] and βjk := Pr[y j k = 0|zk = 0].\nThe EM algorithm for estimating classifier parameters—In § 3 we estimated the parameters by using a separate validation set. Sometimes we may not have access to a labeled validation set. This is especially true when we are interested in aggregating the crowdsourced labels where the goal is estimate the true labels. In such scenarios we can estimate the model parameters directly via the Expectation Maximization(EM) algorithm. The EM algorithm [4] is an efficient iterative procedure to estimate the parameters in presence of missing/hidden data. We will use the unknown hidden true label [z1, . . . , zK ] as the missing data in our case. Each iteration of the algorithm consists of two steps: an Expectation(E)-step and a Maximization(M)-step. The M-step involves maximization of a lower bound on the log-likelihood that is refined by the E-step. Specifically, in the E-step we obtain the marginal probabilities of all the class nodes given the current estimate of the model parameters and the observed classifier scores. In the M-step we re-estimate the model parameters given the class labels (marginal probabilities) for the taxonomy class nodes. The only difference being that when estimat-\ning the parameters of the bi-normal model we need to use these marginal probabilities instead of the binary labels (from the validation set). These two steps (the E- and the M-step) can be iterated till convergence.\nEntry level categories—The proposed algorithm returns a path in the taxonomy. The final label is computed by terminating the path based on the entropy. However for some domains we may want to terminate based on other domain specific criterion. For example, in object recognition task we may want to assign the label of the object to an ‘entry level’ category–the labels people will use to name an object, for example, the entry level category for rottweiler is dog. We can appropriately modify our termination strategy to account for this by suitable backing off the path using ideas in [17]."
    }, {
      "heading" : "6 Experimental Validation",
      "text" : "We experimentally validate our proposed algorithms on two domains, object recognition in images and text categorization. In both these domains we have a natural pre-defined taxonomy.\nDatasets—For visual object recognition we use a subset of images from the ImageNet ILSVRC2014 detection challenge dataset [21]. The dataset consists of the 200 basic categories that map to a total of 547 WordNet synsets (including the children). Out of this 547 node taxonomy we extract a sub-hierarchy rooted at the basic category animal resulting in a taxonomy of 297 nodes. The set of images under these 297 categories are completely non-overlapping and the aggregated set of 70096 images forms the final dataset for our experiments. For our text categorization experiments, we used the benchmark Reuters Corpus Volume 1 (RCV1) news articles dataset [14]. This is a hierarchical multi-labeled dataset where over 138 thousand articles are tagged along topics, industries, and regions facets. We used the 354 class industries taxonomy in our experiments.\nExperimental setup—We use 60% of the data for training, 10% for validation and 30% for testing. We randomly select a subset of classes from the taxonomy and train a multi-class flat classifier by completely ignoring the class taxonomy. For object detection, we take the activations of the sixth hidden layer of a deep convolutional neural network as features and then train a linear multi-class SVM with these features [8]. For the text categorization we used the standard token pre-processing available9 for this dataset with tfidf representation and trained a linear SVM. The clas-\n9http://www.jmlr.org/papers/volume5/lewis04a/ lyrl2004_rcv1v2_README.htm\nsifier is trained using the train split with the validation split being used to tune any hyper-parameters. We experimented with 10 flat classifiers each operating with different(with some overlap) class labels (see Table 1 for the number of classes). For each of the flat classifiers, if a training sample belongs to multiple labels (where one label is a parent of another) then during training-set construction, we randomly assign the instance to one of the classes in order to avoid confounding the classifier. The final model performance is evaluated on the test split which has a representation of all the classes in the taxonomy.\nEvaluation metrics—Accuracy is not a good metric for evaluating the performance of the various classifiers because of the hierarchical relations among classes. The classifiers operate only on a subset of the labels in the taxonomy, but the test set can possibly have instances from all the classes in the taxonomy. There is a rich literature on evaluation measures for hierarchical classification (see [10] for a review). For our evaluation we choose the Lowest Common Ancestor (LCA) based precision (PLCA), recall (RLCA) and F1-score (FLCA) measures as recommended in [10]. These measures are essentially hierarchical versions of the precision, recall, and F1-score based on LCA of the actual and the predicted class.\nResults—Table 1 shows the mean (and the standard deviation) hierarchical precision (PLCA), recall (RLCA) and F1-score (FLCA) on the test set for each of the individual classifiers and also our proposed heuristic(§ 3) and the graphical model(§ 4) based aggregation algorithms for both the visual object recognition and the text classification tasks. For the graphical model inference we used the junction tree algorithm from the Bayes Net Toolbox10. The appropriate thresholds for deciding the terminal class were tuned using the validation set. For the heuristic algorithm the path termination was based on the entropy. For the graphical model the terminal node was decided based on the marginal probabilities. For the visual object recognition dataset on all the three measures the performance of the aggregation algorithms are better than or equal to the best performing classifier in the ensemble, the graphical model based approach outperforming the heuristic approach. For the object recognition dataset we also show results for an alternate termination strategy described in § 5 which decides the terminal node by backing off to a suitable entry level class [17]. This alternate termination strategy gave further improvements. For the text classification dataset the graphical model outperforms the best classifier in terms of the precision by a large margin.\n10https://code.google.com/p/bnt/\nvisual object recognition: 18246 instances\nModel classes PLCA RLCA FLCA\nclassifier0 16 0.35 [0.27] 0.37 [0.27] 0.35 [0.26]\nclassifier1 10 0.33 [0.21] 0.35 [0.22] 0.33 [0.21]\nclassifier2 18 0.36 [0.28] 0.27 [0.22] 0.30 [0.21]\nclassifier3 14 0.32 [0.20] 0.33 [0.16] 0.31 [0.14]\nclassifier4 15 0.32 [0.14] 0.31 [0.16] 0.31 [0.14]\nclassifier5 10 0.31 [0.14] 0.31 [0.16] 0.30 [0.13]\nclassifier6 16 0.24 [0.11] 0.31 [0.14] 0.26 [0.11]\nclassifier7 11 0.21 [0.14] 0.20 [0.13] 0.20 [0.13]\nclassifier8 10 0.23 [0.13] 0.31 [0.16] 0.26 [0.13]\nclassifier9 14 0.24 [0.12] 0.33 [0.15] 0.27 [0.12]\nheuristic 0.40 [0.19] 0.38 [0.15] 0.38 [0.13]\ngraphical 0.44 [0.21] 0.39 [0.13] 0.41 [0.13]\nheuristic entry-level 0.57 [0.29] 0.37 [0.12] 0.44 [0.13] termination\ngraphical entry-level 0.58 [0.29] 0.38 [0.12] 0.46 [0.13] termination\ntext classification: 41395 instances\nModel classes PLCA RLCA FLCA\nclassifier0 10 0.29 [0.12] 0.32 [0.15] 0.30 [0.13]\nclassifier1 10 0.36 [0.19] 0.35 [0.20] 0.35 [0.19]\nclassifier2 10 0.49 [0.27] 0.54 [0.33] 0.51 [0.28]\nclassifier3 10 0.52 [0.36] 0.50 [0.34] 0.52 [0.34]\nclassifier4 10 0.50 [0.28] 0.44 [0.31] 0.47 [0.29]\nclassifier5 10 0.41 [0.25] 0.40 [0.26] 0.40 [0.25]\nclassifier6 10 0.46 [0.32] 0.49 [0.31] 0.47 [0.30]\nclassifier7 10 0.54 [0.34] 0.55 [0.33] 0.54 [0.33]\nclassifier8 10 0.55 [0.33] 0.54 [0.34] 0.53 [0.32]\nclassifier9 10 0.34 [0.15] 0.32 [0.15] 0.33 [0.15]\nheuristic 0.49 [0.27] 0.46 [0.30] 0.47 [0.28]\ngraphical 0.77 [0.27] 0.45 [0.28] 0.56 [0.24]\nTable 1: Experimental results(§ 6) The mean (and the standard deviation) hierarchical precision (PLCA), recall (RLCA) and F1-score (FLCA) measures on the test set for each of the individual classifiers and also our proposed heuristic(§ 3) and the graphical model(§ 4) based aggregation algorithms for both the visual object recognition and the text classification task. For each task and measure the best performing classifier is underlined and the proposed algorithms is in bold it the performance is better than the best individual classifier."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper we formulated the problem of aggregating labels from multiple flat classifiers into a possible different hierarchical taxonomy for the labels. This\nwas achieved without modifying or retraining the constituent classifiers in any way. We proposed two solutions, one based on a heuristic score propagation through the taxonomy and a more principled approach using a graphical model. The proposed algorithms were experimentally validated on two real world problems of visual object recognition and text categorization. We plan to extend this approach to taxonomies with cycles and curated knowledge graphs or ontologies. Our model implicity assumes that the classifier label sets are mappable to some classes in the taxonomy. We plan to integrate ideas from catalog integration [2] directly into our graphical model.\nReferences\n[1] http://www.cdc.gov/nchs/icd.htm.\n[2] Rakesh Agrawal and Ramakrishnan Srikant. On integrating catalogs. In Proceedings of the 10th international conference on World Wide Web, pages 603–612. ACM, 2001.\n[3] R. G Cowell, A. P. Dawid, S. L. Lauritzen, and D. J. Speigelhalter. Probabilistic networks and expert systems: Exact computational methods for Bayesian networks. Springer, 2006.\n[4] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society: Series B, 39(1):1–38, 1977.\n[5] S. Dumais and H. Chen. Hierarchical classification of web content. In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 256–263, 2000.\n[6] M. Everingham, L. Van Gool, C. Williams, J. Winn, and A. Zisserman. The Pascal Visual Object Classes (VOC) Challenge. International Journal of Computer Vision, 88(2):303–338, 2010.\n[7] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In Computer Vision and Pattern Recognition, 2014.\n[8] Donahue J., Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell. DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition. CoRR, abs/1310.1531, 2013.\n[9] D. Koller and M. Sahami. Hierarchically classifying documents using very few words. In Proceedings of the Fourteenth International Conference on Machine Learning, pages 170–178, 1997.\n[10] A. Kosmopoulos, I. Partalas, E. Gaussier, G. Paliouras, and I. Androutsopoulos. Evaluation measures for hierarchical classification: a unified view and novel approaches. Data Mining and Knowledge Discovery, 29(3):820–865, 2015.\n[11] A. Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.\n[12] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet Classification with Deep Convolutional Neural Networks. In Advances in neural information processing systems, pages 1097–1105, 2012.\n[13] S. L. Lauritzen. Propagation of probabilities, means, and variances in mixed graphical association models. Journal of the American Statistical Association, 87(420):1098–1108, 1992.\n[14] D. D. Lewis, Y. Yang, T. Rose, , and F. Li. RCV1: A New Benchmark Collection for Text Categorization Research. Journal of Machine Learning Research, 5:361–397, 2004.\n[15] L. V. Lita, S. Yu, R. S. Niculescu, and J. Bi. Large scale diagnostic code classification for medical patient records. In IJCNLP, pages 877–882, 2008.\n[16] G. A. Miller. WordNet: A Lexical Database for English. Communications of the ACM, 38(11):39–41, 1995.\n[17] V. Ordonez, J. Deng, Y. Choi, A. C. Berg, and T. L. Berg. From large scale image categorization to entrylevel categories. In IEEE International Conference on Computer Vision (ICCV), 2013.\n[18] Panagiotis Papadimitriou, Panayiotis Tsaparas, Ariel Fuxman, and Lise Getoor. Taci: Taxonomy-aware catalog integration. Knowledge and Data Engineering, IEEE Transactions on, 25(7):1643–1655, 2013.\n[19] I. Partalas, A. Kosmopoulos, N. Baskiotis, T. Artieres, G. Paliouras, E. Gaussier, I. Androutsopoulos, M.-R. Amini, and P. Galinari. Lshtc: A benchmark for large-scale text classification. arXiv preprint arXiv:1503.08581, 2015.\n[20] V. C. Raykar, S. Yu, L. H. Zhao, G. H. Valadez, C. Florin, L. Bogoni, and L. Moy. Learning from crowds. Journal of Machine Learning Research, 11:1297–1322, April 2010.\n[21] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 2015.\n[22] Sunita Sarawagi, Soumen Chakrabarti, and Shantanu Godbole. Cross-training: learning probabilistic mappings between topics. In Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 177–186. ACM, 2003.\n[23] C. N. Silla Jr. and A. A. Freitas. A survey of hierarchical classification across different application domains. Data Mining and Knowledge Discovery, 22(1-2):31– 72, 2011.\n[24] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going Deeper with Convolutions. CoRR, abs/1409.4842, 2014.\n[25] B. Zadrozny and C. Elkan. Transforming classifier scores into accurate multiclass probability estimates. In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 694–699, 2002."
    } ],
    "references" : [ {
      "title" : "On integrating catalogs",
      "author" : [ "Rakesh Agrawal", "Ramakrishnan Srikant" ],
      "venue" : "In Proceedings of the 10th international conference on World Wide Web,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2001
    }, {
      "title" : "Probabilistic networks and expert systems: Exact computational methods for Bayesian networks",
      "author" : [ "R. G Cowell", "A.P. Dawid", "S.L. Lauritzen", "D.J. Speigelhalter" ],
      "venue" : "Springer,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Maximum likelihood from incomplete data via the EM algorithm",
      "author" : [ "A.P. Dempster", "N.M. Laird", "D.B. Rubin" ],
      "venue" : "Journal of the Royal Statistical Society: Series B, 39(1):1–38,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1977
    }, {
      "title" : "Hierarchical classification of web content",
      "author" : [ "S. Dumais", "H. Chen" ],
      "venue" : "Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 256–263,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "The Pascal Visual Object Classes (VOC) Challenge",
      "author" : [ "M. Everingham", "L. Van Gool", "C. Williams", "J. Winn", "A. Zisserman" ],
      "venue" : "International Journal of Computer Vision, 88(2):303–338,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Rich feature hierarchies for accurate object detection and semantic segmentation",
      "author" : [ "R. Girshick", "J. Donahue", "T. Darrell", "J. Malik" ],
      "venue" : "Computer Vision and Pattern Recognition,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition",
      "author" : [ "Donahue J", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell" ],
      "venue" : "CoRR, abs/1310.1531,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2013
    }, {
      "title" : "Hierarchically classifying documents using very few words",
      "author" : [ "D. Koller", "M. Sahami" ],
      "venue" : "Proceedings of the Fourteenth International Conference on Machine Learning, pages 170–178,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Evaluation measures for hierarchical classification: a unified view and novel approaches",
      "author" : [ "A. Kosmopoulos", "I. Partalas", "E. Gaussier", "G. Paliouras", "I. Androutsopoulos" ],
      "venue" : "Data Mining and Knowledge Discovery, 29(3):820–865,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Learning multiple layers of features from tiny images",
      "author" : [ "A. Krizhevsky" ],
      "venue" : "Technical report,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Imagenet Classification with Deep Convolutional Neural Networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "Advances in neural information processing systems, pages 1097–1105,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Propagation of probabilities, means, and variances in mixed graphical association models",
      "author" : [ "S.L. Lauritzen" ],
      "venue" : "Journal of the American Statistical Association, 87(420):1098–1108,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "RCV1: A New Benchmark Collection for Text Categorization Research",
      "author" : [ "D.D. Lewis", "Y. Yang", "T. Rose", "F. Li" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2004
    }, {
      "title" : "Large scale diagnostic code classification for medical patient records",
      "author" : [ "L.V. Lita", "S. Yu", "R.S. Niculescu", "J. Bi" ],
      "venue" : "IJCNLP, pages 877–882,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "WordNet: A Lexical Database for English",
      "author" : [ "G.A. Miller" ],
      "venue" : "Communications of the ACM, 38(11):39–41,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "From large scale image categorization to entrylevel categories",
      "author" : [ "V. Ordonez", "J. Deng", "Y. Choi", "A.C. Berg", "T.L. Berg" ],
      "venue" : "IEEE International Conference on Computer Vision (ICCV),",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Taci: Taxonomy-aware catalog integration",
      "author" : [ "Panagiotis Papadimitriou", "Panayiotis Tsaparas", "Ariel Fuxman", "Lise Getoor" ],
      "venue" : "Knowledge and Data Engineering, IEEE Transactions on,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2013
    }, {
      "title" : "Lshtc: A benchmark for large-scale text classification",
      "author" : [ "I. Partalas", "A. Kosmopoulos", "N. Baskiotis", "T. Artieres", "G. Paliouras", "E. Gaussier", "I. Androutsopoulos", "M.-R. Amini", "P. Galinari" ],
      "venue" : "arXiv preprint arXiv:1503.08581,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Learning from crowds",
      "author" : [ "V.C. Raykar", "S. Yu", "L.H. Zhao", "G.H. Valadez", "C. Florin", "L. Bogoni", "L. Moy" ],
      "venue" : "Journal of Machine Learning Research, 11:1297–1322, April",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "ImageNet Large Scale Visual Recognition Challenge",
      "author" : [ "O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei" ],
      "venue" : "International Journal of Computer Vision (IJCV),",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Cross-training: learning probabilistic mappings between topics",
      "author" : [ "Sunita Sarawagi", "Soumen Chakrabarti", "Shantanu Godbole" ],
      "venue" : "In Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2003
    }, {
      "title" : "and A",
      "author" : [ "C.N. Silla Jr" ],
      "venue" : "A. Freitas. A survey of hierarchical classification across different application domains. Data Mining and Knowledge Discovery, 22(1-2):31– 72,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Going Deeper with Convolutions",
      "author" : [ "C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich" ],
      "venue" : "CoRR, abs/1409.4842,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Transforming classifier scores into accurate multiclass probability estimates",
      "author" : [ "B. Zadrozny", "C. Elkan" ],
      "venue" : "Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 694–699,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2002
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "In several real-world classification problems (for example visual object recognition [12], text categorization [9, 19], web content classification [5], US Patent codes, ICD [1] codes of diseases [15] etc.",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 7,
      "context" : "In several real-world classification problems (for example visual object recognition [12], text categorization [9, 19], web content classification [5], US Patent codes, ICD [1] codes of diseases [15] etc.",
      "startOffset" : 111,
      "endOffset" : 118
    }, {
      "referenceID" : 17,
      "context" : "In several real-world classification problems (for example visual object recognition [12], text categorization [9, 19], web content classification [5], US Patent codes, ICD [1] codes of diseases [15] etc.",
      "startOffset" : 111,
      "endOffset" : 118
    }, {
      "referenceID" : 3,
      "context" : "In several real-world classification problems (for example visual object recognition [12], text categorization [9, 19], web content classification [5], US Patent codes, ICD [1] codes of diseases [15] etc.",
      "startOffset" : 147,
      "endOffset" : 150
    }, {
      "referenceID" : 13,
      "context" : "In several real-world classification problems (for example visual object recognition [12], text categorization [9, 19], web content classification [5], US Patent codes, ICD [1] codes of diseases [15] etc.",
      "startOffset" : 195,
      "endOffset" : 199
    }, {
      "referenceID" : 10,
      "context" : "As a motivating example we consider the task of visual object recognition [12, 7]—given an image (or a region in the image) the task is to predict the most likely object in the image.",
      "startOffset" : 74,
      "endOffset" : 81
    }, {
      "referenceID" : 5,
      "context" : "As a motivating example we consider the task of visual object recognition [12, 7]—given an image (or a region in the image) the task is to predict the most likely object in the image.",
      "startOffset" : 74,
      "endOffset" : 81
    }, {
      "referenceID" : 10,
      "context" : "Considerable amount of research effort and also CPU time(especially in the case of convolutional neural network based approaches [12, 24] which takes weeks to months to train) has been spent on training these classifiers.",
      "startOffset" : 129,
      "endOffset" : 137
    }, {
      "referenceID" : 22,
      "context" : "Considerable amount of research effort and also CPU time(especially in the case of convolutional neural network based approaches [12, 24] which takes weeks to months to train) has been spent on training these classifiers.",
      "startOffset" : 129,
      "endOffset" : 137
    }, {
      "referenceID" : 14,
      "context" : "For this domain we use the Wordnet [16] 2 , which is a DAG structured class",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 9,
      "context" : "For example the CIFAR-100 dataset [11] has 100 class labels, the PASCAL-VOC datset [6] has 20 class labels, and the latest ImageNet ILSVRC challenge dataset [21] has 1000 class labels.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 4,
      "context" : "For example the CIFAR-100 dataset [11] has 100 class labels, the PASCAL-VOC datset [6] has 20 class labels, and the latest ImageNet ILSVRC challenge dataset [21] has 1000 class labels.",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 19,
      "context" : "For example the CIFAR-100 dataset [11] has 100 class labels, the PASCAL-VOC datset [6] has 20 class labels, and the latest ImageNet ILSVRC challenge dataset [21] has 1000 class labels.",
      "startOffset" : 157,
      "endOffset" : 161
    }, {
      "referenceID" : 14,
      "context" : "WordNet [16] is a large lexical database of english ar X iv :1 51 2.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 21,
      "context" : "Related work—There is a rich literature in the area of hierarchical classification (see [23] for a survey), which deals with training classifiers by explicitly accounting for the class hierarchy.",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 18,
      "context" : "While sophisticated techniques exist for binary, categorical and ordinal labels [20] to the best of our knowledge there are no methods for labels",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 0,
      "context" : "Another area of research related to our problem setting is that of integrating (or mapping) label-sets into each other as in the case of e-commerce catalog integration [2, 22, 18].",
      "startOffset" : 168,
      "endOffset" : 179
    }, {
      "referenceID" : 20,
      "context" : "Another area of research related to our problem setting is that of integrating (or mapping) label-sets into each other as in the case of e-commerce catalog integration [2, 22, 18].",
      "startOffset" : 168,
      "endOffset" : 179
    }, {
      "referenceID" : 16,
      "context" : "Another area of research related to our problem setting is that of integrating (or mapping) label-sets into each other as in the case of e-commerce catalog integration [2, 22, 18].",
      "startOffset" : 168,
      "endOffset" : 179
    }, {
      "referenceID" : 21,
      "context" : "Formally a class taxonomy [23] is defined as a pair (C,≺), where C = {c1, .",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 23,
      "context" : "For real valued scores they can be converted to probabilities via the soft-max function or via some calibration techniques [25].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 0,
      "context" : "If this is not true then we can approximate the mappings using class mapping techniques [2, 22, 18].",
      "startOffset" : 88,
      "endOffset" : 99
    }, {
      "referenceID" : 20,
      "context" : "If this is not true then we can approximate the mappings using class mapping techniques [2, 22, 18].",
      "startOffset" : 88,
      "endOffset" : 99
    }, {
      "referenceID" : 16,
      "context" : "If this is not true then we can approximate the mappings using class mapping techniques [2, 22, 18].",
      "startOffset" : 88,
      "endOffset" : 99
    }, {
      "referenceID" : 1,
      "context" : "The graphical model which we have described is a mixed discrete-gaussian network [3].",
      "startOffset" : 81,
      "endOffset" : 84
    }, {
      "referenceID" : 11,
      "context" : "For such networks exact inference algorithms exist [13], which permits the local computation on the junction tree of exact probabilities, means and variances.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 2,
      "context" : "The EM algorithm [4] is an efficient iterative procedure to estimate the parameters in presence of missing/hidden data.",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 15,
      "context" : "We can appropriately modify our termination strategy to account for this by suitable backing off the path using ideas in [17].",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 19,
      "context" : "Datasets—For visual object recognition we use a subset of images from the ImageNet ILSVRC2014 detection challenge dataset [21].",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 12,
      "context" : "For our text categorization experiments, we used the benchmark Reuters Corpus Volume 1 (RCV1) news articles dataset [14].",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 6,
      "context" : "For object detection, we take the activations of the sixth hidden layer of a deep convolutional neural network as features and then train a linear multi-class SVM with these features [8].",
      "startOffset" : 183,
      "endOffset" : 186
    }, {
      "referenceID" : 8,
      "context" : "There is a rich literature on evaluation measures for hierarchical classification (see [10] for a review).",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 8,
      "context" : "For our evaluation we choose the Lowest Common Ancestor (LCA) based precision (PLCA), recall (RLCA) and F1-score (FLCA) measures as recommended in [10].",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 15,
      "context" : "For the object recognition dataset we also show results for an alternate termination strategy described in § 5 which decides the terminal node by backing off to a suitable entry level class [17].",
      "startOffset" : 190,
      "endOffset" : 194
    }, {
      "referenceID" : 0,
      "context" : "We plan to integrate ideas from catalog integration [2] directly into our graphical model.",
      "startOffset" : 52,
      "endOffset" : 55
    } ],
    "year" : 2015,
    "abstractText" : "We describe the problem of aggregating the label predictions of diverse classifiers using a class taxonomy. Such a taxonomy may not have been available or referenced when the individual classifiers were designed and trained, yet mapping the output labels into the taxonomy is desirable to integrate the effort spent in training the constituent classifiers. A hierarchical taxonomy representing some domain knowledge may be different from, but partially mappable to, the label sets of the individual classifiers. We present a heuristic approach and a principled graphical model to aggregate the label predictions by grounding them into the available taxonomy. Our model aggregates the labels using the taxonomy structure as constraints to find the most likely hierarchically consistent class. We experimentally validate our proposed method on image and text classification tasks.",
    "creator" : "LaTeX with hyperref package"
  }
}