{
  "name" : "1507.03181.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "A Probabilistic Approach to Knowledge Translation",
    "authors" : [ "Shangpu Jiang", "Dejing Dou" ],
    "emails" : [ "shangpu@cs.uoregon.edu", "lowd@cs.uoregon.edu", "dou@cs.uoregon.edu" ],
    "sections" : [ {
      "heading" : "Introduction",
      "text" : "Knowledge acquisition is a critical process for building predictive or descriptive models for many applications. When domain expertise is available, knowledge can be constructed manually. When enough high-quality data is available, knowledge can be constructed automatically using data mining or machine learning tools. Both approaches can be difficult and expensive, so we would prefer to reuse or transfer knowledge from one application or system to another whenever possible. However, different applications or systems often have different semantics, which makes knowledge reuse or transfer a non-trivial task.\nAs a motivating example, suppose a new credit card company without historical data wants to use the classification model mined by a partner credit card company to determine whether the applicants of the new company are qualified or not. Since the two companies may use different schemas to store their applicants’ data (e.g., in one schema, we have annual income recorded as a numerical attribute, while in the other, we have salary as an attribute with discretized ranges), we cannot simply reuse the old classifier. Due to privacy\nand scalability concerns, we cannot transfer the collaborative company’s data to the new schema either. Therefore, we want to translate the classification model itself to the new schema, without using any data.\nIn this paper, we propose knowledge translation (KT) as a novel solution to translate knowledge across conceptually similar but semantically heterogeneous schemas or ontologies. For convenience, we refer to them generically as “schemas.” As shown in the previous example, KT is useful in situations where data translation/transfer is problematic due to privacy or scalability concerns.\nWe formally define knowledge translation as the task of converting knowledge KS in source schema S to equivalent knowledge KT in target schema T , where the correspondence between the schemas is given by some mapping MS,T . In general, one schema may have concepts that are more general or specific than the other, so an exact translation may not exist. We will therefore attempt to find the best translation, acknowledging that the best translation may still be a lossy approximation of the source knowledge.\nWe adopt a probabilistic approach to knowledge translation, in which the knowledge in the source schema, the mapping between the source and target schemas, and the resulting knowledge in the target schema are all represented as probability distributions. This gives us a consistent mathematical framework for handling uncertainty at every step in the process. This uncertainty is clearly necessary when the source knowledge is probabilistic, but it is also necessary when there is no exact mapping between the schemas, or when the correct mapping is uncertain. We propose to represent these probability distributions using Markov random fields, for propositional (non-relational) domains, and Markov logic networks, for relational domains. Given probability distributions for both the source knowledge and the schema mapping, we can combine them to define an implicit probability distribution in the target schema. Our goal is to find an explicit probability distribution in the target schema that is close to this implicit distribution in terms of the Kullback-Leibler divergence.\nOur main contributions are:\n• We formally define the problem of knowledge translation (KT), which allows knowledge to be reused when data is unavailable.\nar X\niv :1\n50 7.\n03 18\n1v 1\n[ cs\n.A I]\n1 2\nJu l 2\n01 5\n• We propose a novel probabilistic approach for KT by combining probabilistic graphical models with schema mappings.\n• We implement an experimental KT system and evaluate it on two real datasets We compare our data-free KT approach to baselines that use data from the source or target schema and show that we can obtain comparable accuracy without data.\nThe paper is organized as follows. We first summarize related work, such as semantic integration, distributed data mining, and transfer learning, and discuss their connections and distinctions with KT. We then show how Markov random fields and Markov logic networks can represent knowledge and mappings with uncertainty. Next, we present a variant of the MRF/Markov logic learning algorithm to solve the problem of knowledge translation. We then run experiments on synthetic and real datasets. Finally, we make a conclusion."
    }, {
      "heading" : "Related Work",
      "text" : "In this section, we compare the task of knowledge translation with some related work.\nSemantic Integration Data integration and exchange (e.g., (Lenzerini 2002)) are the mostly studied areas in semantic integration. The main task of data integration and exchange is to answer queries posed in terms of the global schema given source databases. The standard semantics of global query answering is to return the tuples in every possible database that is consistent with the global schema constraints and the mapping, i.e., the set of certain answers.\nA main difference between data integration/exchange and knowledge translation (KT) is that KT has probabilistic semantics for the translation process, that is, it defines a distribution of possible worlds in the target schema, instead of focusing only on the tuples that are in all the possible worlds (i.e., certain answers).\nDistributed Data Mining Efforts in distributed data mining (DDM) (see surveys in (Park and Kargupta 2002; Caragea et al. 2005)) have made considerable progress in mining distributed data resources without putting data in a centralized location. (Caragea et al. 2005) proposes a general DDM framework with two components: one sends statistical queries to local data sources, and the other uses the returned statistics to revise the current partial hypothesis and generate further queries.\nHeterogeneous DDM (Caragea et al. 2005) also handles the semantic heterogeneity between the global and local schemas, in particular, those containing attributes with different granularities called Attribute Value Taxonomy (AVT). Heterogeneous DDM requires local data resources and their mappings to the global schema to translate the statistics of queries. However, KT does not require data or statistics from either the source or the target. Instead, KT uses mappings to translate the generated/mined knowledge from the source directly.\nTransfer Learning Transfer learning (TL) has been a successful approach to knowledge reuse. In traditional machine learning, only one domain and one task is involved. When the amount of data is limited, it is desirable to use data from related domains or tasks. As long as the source and target data share some similarity (e.g., in the distribution or underlying feature representation), such knowledge can be used as a “prior” for the target task.\nMost transfer learning work focuses on the homogeneous case in which the source and target domain have identical attributes. The main exceptions are heterogeneous transfer learning (Yang et al. 2009) and relational transfer learning (e.g., TAMAR (Mihalkova, Huynh, and Mooney 2007), deep transfer (Davis and Domingos 2009)). Heterogeneous transfer learning deals with different representations of the data (e.g., text and images of an object). While it uses an implicit mapping of two feature spaces (e.g., through Flickr), KT uses an explicit mapping via FOL formulas. Relational transfer learning deals with two analogous domains (e.g., in movie and university domains, directors correspond to professors ). In contrast, KT focuses on a single domain with two different representations. Moreover, relational transfer learning only handles deterministic one-to-one matchings which can be inferred by using a small amount of target data, while KT does not use any target data, and relies on the provided explicit FOL mapping.\nDeductive Knowledge Translation Deductive knowledge translation (Dou, Qin, and Liu 2011) essentially tries to solve the same problem, but it only considers deterministic knowledge and mappings. Our KT work can handle knowledge and mappings with uncertainty, which is more general than the deterministic scenario deductive knowledge translation (Dou, Qin, and Liu 2011) can handle.\nSee Table 1 for a summary of the similarities and differences between our knowledge translation (KT) approach and related work."
    }, {
      "heading" : "Probabilistic Representations of Knowledge and Mappings",
      "text" : "To translate knowledge from one schema to another, we must have a representation of the knowledge and the mappings between the two schemas. In many cases, knowledge and mappings are uncertain. For example, the mined\nsource knowledge could be a probabilistic model, such as a Bayesian network. Mappings between two schemas may also be uncertain, either because a perfect alignment of the concepts does not exist, or because there is uncertainty about which alignment is the best. Therefore, we propose a probabilistic approach to knowledge translation."
    }, {
      "heading" : "Representation of Knowledge",
      "text" : "Our approach to knowledge translation requires that the source and target knowledge are probability distributions represented as log-linear models. In some cases, the source knowledge mined from the data may already be represented as a log-linear model, such as a Bayesian network used for fault diagnosis or Markov logic network modeling homophily in a social network. In other cases, we will need to convert the knowledge into this representation.\nFor mined knowledge represented as rules, including association rules, rule sets, and decision trees (which can be viewed as a special case of rule sets), we can construct a feature for each rule, with a weight corresponding to the confidence or probability of the rule. The rule weight has a closed-form solution based on the log odds that the rule is correct:\nwi = log p(fi) 1− p(fi) − log u(fi)\n1− u(fi) where p(fi) is the probability or confidence of the ith rule or formula and u(fi) is its probability under a uniform distribution. Relational rules in an ontology can similarly be converted to a Markov logic network by attaching weights representing their relative strengths or confidences.\nFor linear classifiers, such as linear support vector machines or perceptrons, we can substitute logistic regression, a probabilistic linear classifier.\nIn some cases, the knowledge we wish to translate takes the form of a conditional probability distribution, p(Y |X), or a predictive model that can be converted to a conditional probability distribution. This includes decision trees, neural networks, and other classifiers used in data mining and machine learning. The method we propose will rely on a full joint probability distribution over all variables. We can convert a conditional distribution into a joint distribution by assuming some prior distribution over the evidence, p(X), such as a uniform distribution."
    }, {
      "heading" : "Representation of Mappings",
      "text" : "The relationships between heterogeneous schemas can be represented as a mapping. We use probabilistic models to represent mappings. Consistent with the probabilistic representation of knowledge in a database schema, the attributes are considered as random variables for non-relational domains, and the attributes or relations are considered as firstorder random variables for relational domains. Let us denote the variables in the source as X = {X1, ..., XN} and those in the target as X ′ = {X ′1, ..., X ′M}. A mapping is the conditional distribution p(X ′|X).\nIn real cases, a mapping is often represented as a set of source-to-target correspondences\n{p(C ′i|Ci), i = 1, ..., I}\nwhere Ci ⊂ X and C ′i ⊂ X ′ are sets of variables in the source and target respectively. For the credit card company example, a mapping between the two schemas may include the correspondences of “age” and “age,” “salary” and “annual income,” etc.\nIn order to obtain a global mapping between the source and target schemas using the local correspondences, we make the following two assumptions:\n1. p(C ′i ∪C ′ j |X) = p(C ′ i|X)p(C ′ j |X), or, C ′ i ⊥ C ′ j |X; 2. p(C ′i|X) = p(C ′ i|Ci).\nFrom these two assumptions, it follows that: p(X ′|X) = ∏ i p(C ′i|X) = ∏ i p(C ′i|Ci)\nNote that these assumptions are not always correct, but they provide a good approximation of the global mapping when it is not available.\nThe weight of each formula can be estimated with the logodds. For example, we define a probabilistic source-to-target correspondence as qS →p qT , where qS and qT are queries (i.e., logical formulas) of source and target schemas or ontologies, and→p has probabilistic semantics:\nPr(qT |qS) = p Example 1 (Class correspondence). If x is a graduate student, then x is a student and older than 24 with probability 0.9, and vice versa.\nGrad(x)→0.9 Student(x) ∧ Age(x, y) ∧ (y ≥ 24) Grad(x)←0.9 Student(x) ∧ Age(x, y) ∧ (y ≥ 24)\nThis can be converted to 2.2 Grad(x)→ (Student(x) ∧ Age(x, y) ∧ (y ≥ 24)) 2.2 Grad(x)← (Student(x) ∧ Age(x, y) ∧ (y ≥ 24))"
    }, {
      "heading" : "Knowledge Translation",
      "text" : "In this section, we formalize the task of knowledge translation (KT) and propose a solution to this task. We have the source knowledge represented as a probabilistic model p(X) = p(X1, ...Xn) and a probabilistic mapping P (X ′|X). The probabilistic model in the target schema can be computed as\np(X ′) = ∑ X p(X)p(X ′|X) = ∑ X p(X) ∏ i p(C ′i|Ci)\n(1) Our goal is to find a compact probabilistic model in the target schema (i.e., the target knowledge) without using any source variables as latent variables. This requirement is due to both efficiency (when the knowledge is being used) and understandability consideration.\nWe also use a log-linear model q(X ′) to represent this compact model. A straight-forward objective is to minimize the Kullback-Leibler divergence\nq∗ = arg min q\nDKL [ p(X ′)‖q(X ′) ] = arg min\nq − ∑ X′ p(X ′) log q(X ′) (2)\nThe joint distribution p(X,X ′) is also a log-linear model (see Equation 1). The weights for a local correspondence can be computed as:\nθ̄(Ci,C ′ i) = log p(C ′ i|Ci) = log\nexp(θ(Ci,C ′ i))∑\nC′i exp θ(Ci,C\n′ i)\nwhere θ(Ci,C ′i) are the weights of the correspondence in the probabilistic mapping model. The computation of p(X ′) is therefore a standard inference task of the joint model p(X,X ′).\nParameter Learning The parameters of the target loglinear model that minimizes Equation 2 can be computed via standard optimization algorithms. A simple way to compute the objective is sampling: we first generate a sample from the source p(X), and then generate a sample of X ′ from p(X ′|X) conditioned on the source sample. In the relational domain (with Markov logic or other statistical relational models), each sample instance is a database, and we need to first decide the number of constants and create a set of ground variables with these constants.\nStructure Learning The structure of the target knowledge can also be learned via standard structure learning algorithms for Markov random fields or Markov logic networks. An alternative approach is to use heuristics to generate the structure first. For deterministic one-to-one correspondences, the independences in the target schema are the same as those in the source schema up to renaming. If the correspondences are non-deterministic, we may have less independences in the target schema, and we could have an extremely complex model with large cliques. Nonetheless, in realistic scenarios, the correspondences in a mapping are usually deterministic or nearly deterministic. Therefore, it is reasonable to pretend they are deterministic while inferring the target structure. In this way we trade off between the complexity and accuracy of the target knowledge.\nFirst of all, for Markov logic, we use first-order cliques instead of formulas as the source structure, so that it is consistent with the propositional case. We show the pseudocode of the structure translation in Algorithm 1. It is considered as a structure learning process. The first step (Line 1-8) is to remove the variables that do not have a correspondence in the target schema. This can be done by standard variable elimination (Koller and Friedman 2009; Poole 2003) without calculating parameters. However, exact variable elimination may create very large cliques and be very expensive, especially in Markov logic in the relational domains. Therefore, we approximate it by only merging two cliques at a time. For relational case, the merging involves a first-order unification operation (Russell and Norvig 2003; Poole 2003). When multiple most general unifiers exist, we simply include all the resulting new cliques. In the second step (Line 9-16), we replace each variable with the corresponding variables in the target schema. This also involves first-order unification in the relational case. If there are many-to-many correspondences, we may generate multiple target cliques from one source clique.\nAlgorithm 1 Structure Translation (MRFs or MLNs) Input: The source schema S, source structure (propositional or first-order cliques) Φ = {φi}, and mappingM. Output: The target structure Φ′M.\n1: for each variable (or first-order predicate) P ∈ S that does not appear inM do 2: Let ΦP denote all the cliques containing P 3: Remove ΦP from Φ 4: for each pair of cliques in ΦP do 5: Merge the two cliques and remove P 6: Insert the resulting clique back to Φ 7: end for 8: end for 9: for each clique φ ∈ Φ do\n10: for each variable P in φ do 11: Let P ′M be all possible correspondences of P 12: end for 13: Let φ′M denote all possible correspondences of φ 14: φ′M ← Cartesian product of P ′M 15: Add φ′M to Φ ′ M 16: end for\nExample 2. Given the source Markov logic:\nGrad(x)→ AgeOver25(x) AgeOver25(x)→ GoodCredit(x)\nand the mapping:\n2.2 Grad(x) ∨ Undergrad(x)↔ Student(x) 3.0 GoodCredit(x)↔ HighCreditScore(x)\nWe first eliminate AgeOver25(x) from the source structure because it does not occur in the mapping, and we get a new clique\n{Grad(x), GoodCredit(x)}\nThen we translate the clique based on the mapping, which gives\n{Student(x), HighCreditScore(x)}"
    }, {
      "heading" : "Experiments",
      "text" : "To evaluate our methods, we created two knowledge translation tasks: one on a non-relational domain (NBA) and one on a relational domain (University). In each knowledge translation task, we have 2 different database schemas as the source and target schemas and a dataset for each schema. The input of a knowledge translation system is the source knowledge and the mapping between the source and target schema. We obtained the source knowledge (i.e., a probabilistic model in the source) by performing a common learning algorithm on the source dataset, and created the probabilistic schema mapping manually. The output of a knowledge translation system is the target knowledge (i.e., a probabilistic model in terms of the target schema)."
    }, {
      "heading" : "Methods and Baselines",
      "text" : "We evaluate three different versions of our proposed probabilistic knowledge translation approach described in the previous section. All of them use the source knowledge and probabilistic mapping to generate a sampled approximation of the distribution in the target schema, and all of them use these samples to learn an explicit distribution in the target schema. The difference between them is their approach to knowledge structure. LS-KS (“learned structure”) learns the structure directly from the samples, which is the most flexible approach. TS-KS (“translated structure”) uses a heuristic translation of the structure from the source knowledge base. ES-KS (“empty structure”) is a simple baseline in which the target knowledge base is limited to a marginal distribution.\nWe also compare to several baselines that make use of additional data. When there is data DS in the source schema, we can use the probabilistic mapping to translate it to the target schema and learn models from the translated source data. LS-DS and MS-DS learn models from translated source data, using learned and manually specified structures, respectively. When there is data DT in the target schema, we can learn from this data directly. LS-DT and MS-DT learn models from target data with learned and manually specified structures respectively. These methods represent an unrealistic “best case” since they use data that is typically unavailable in knowledge translation tasks.\nWe evaluate our knowledge translation methods according to two criteria: the pseudo-log-likelihood (PLL) on the held-out target data, and PLL on the held-out translated source data. The advantage of the second measure is that it controls for differences between the source and target distributions. For relational domains, we use weighted pseudolog-likelihood (WPLL), where for each predicate r, the PLL of each of its groundings is weighted by the cr = 1/gr, where gr is the number of its groundings."
    }, {
      "heading" : "Non-Relational Domain (NBA)",
      "text" : "We collected information on basketball players in the National Basketball Association (NBA) from two websites, the NBA official website nba (as the source schema) and the Yahoo NBA website yahoo (as the target schema). The schemas of these two datasets both have the name, height, weight, position and team of each player. In these schemas, the values of position have a different granularity. Also, in nba, we discretize height and weight into 5 equalwidth ranges. In yahoo, we discretize them into 5 equalfrequency ranges (in order to make the mapping more challenging). The correspondences of these attributes are originally unit conversion formulas, e.g.,\nh′ = h× 39.3701\nAfter we discretize these attributes, we calculate the correspondence distribution of the ranges by making a simple assumption that each value range is uniformly distributed, e.g.,\np(h′ ∈ (73.5, 76.5]|h ∈ (1.858, 1.966]) = 0.706\nWe used the Libra Toolkit1 for creating the source knowledge and for performing the learning and inference subroutines required by the different knowledge translation approaches. We first left out 1/5 of the data instances in the source and target dataset as the testing sets. For the remaining source dataset, we used the decision tree structure learning (DTSL) (Lowd and Davis 2014) to learn the source knowledge. We used standard 4-fold cross validation to determine the parameters of the learning algorithm. The parameters include κ, prior, and mincount for decision tree learning, and l2 for weight learning.\nWe use Gibbs sampling for the sampling algorithm in the knowledge translation approaches. For LS-KS and TS-KS , we draw N samples from the source knowledge probability distribution. We then use the probabilistic mapping to draw 1 target sample for each source sample. For LS-DS , suppose we have NS instances in the source dataset. We use the probabilistic mapping to draw N/NS target samples for each source instance, such that the total number of target instances is also N .\nLS-KS and TS-KS both perform weight learning with an l2 prior. For structure translation with TS-KS , we only translate features for which the absolute value of the weight is greater than a threshold θ. These two parameters are tuned with cross validation over a partition of the samples.\nSee Figures 1 for learning curves comparing our methods to the baselines. We see that translated knowledge (LS-KS and TS-KS) is as accurate as knowledge learned from translated source data (LS-DS) on both the target data and the translated source data. This confirms that KT can be as accurate as data translation, but with the advantage of not requiring any data. We do not see a large difference between learning the structure (LS-KS) and heuristically translating the structure (TS-KS). As expected, the model learned directly on the target data (LS-DT ) has the best PLL on the target data, since it could observe the target distribution directly."
    }, {
      "heading" : "Relational Domain (University)",
      "text" : "We use the UW-CSE dataset2 and the UO-CIS dataset which we collected from the Computer and Information Science Department of the University of Oregon. The UW-CSE dataset was introduced by Richardson and Domingos (Richardson and Domingos 2006) and is widely used in statistical relational learning research. In this University domain, we have concepts such as persons, courses, and publications; attributes such as PhD student stage and course level; and relations such as advise, teach, and author. The schemas of the two databases differ in their granularities of concepts and attribute values. For example, UW-CSE graduate courses are marked as level 500, while UO-CIS has both graduate courses at level 600 and combined undegraduate/graduate courses at level 4/500.\nOur methods in this relational domain are similar to those\n1http://libra.cs.uoregon.edu/ 2http://alchemy.cs.washington.edu/data/\nuw-cse/.\nin the non-relational domain. We use Alchemy 3 for learning and inference in Markov logic networks. We obtain the source knowledge by manually creating the formulas in the source schema and then using the source data to learn the weights.\nWe use MC-SAT (Poon and Domingos 2006) as the sampling algorithm for these experiments. Since the behavior of a Markov logic network is highly sensitive to the number of constants, we want to keep the number of constants similar to the original dataset from which the model is learned. We set the number of constants of each type to be the average number over all training databases, multiplied by a scalar 1 2 for more efficient inference. For methods based on KS , we draw N samples from the source distribution and 1 target sample from each source sample and the mapping. For methods based on DS , we draw N samples based on the mapping. Here N does not have to be large, because each sample instance of a relational domain is itself a database. We set N to 1, 2 and 5 in our experiments. We set the l2 prior for weight learning to 10, based on cross-validation over samples.\nThe results are shown in Table 2. In general, learning MLN structure (LS-KS and LS-DS) did not work as well as their counterparts with manually specified structures (MSKS and MS-DS). From a single sample, the translated source data and manually specified structure (MS-DS) were more effective than knowledge translation with translated structure (TS-KS). However, as we increase the number of samples, the performance of TS-KS improves substantially. With 5 samples, the performance of TS-KS becomes competitive with that of MS-DS , again demonstrating that knowledge translation can achieve comparable results to data translation but without data. When evaluated on translated source data, TS-KS shows the same trend of improving with the number of samples, but its performance with 5 relational samples is slightly worse than MS-DS .\n3http://alchemy.cs.washington.edu/ alchemy1.html"
    }, {
      "heading" : "Conclusion",
      "text" : "Knowledge translation is an important task towards knowledge reuse where the knowledge in the source schema needs to be translated to a semantically heterogeneous target schema. Different from data integration and transfer learning, knowledge translation focuses on the scenario that the data may not be available in both the source and target. We propose a novel probabilistic approach for knowledge translation by combining probabilistic graphical models with schema mappings. We have implemented an experimental knowledge translation system and evaluated it on two real datasets for different prediction tasks. The results and comparison with baselines show that our approach can obtain comparable accuracy without data.\nThe proposed log-linear models, such as Markov random fields and Markov logic networks, already cover most of common types of knowledge used in data mining. In the future work, we will extend our approach to the knowledge types which are harder to represent as log-linear models, such as SVMs and nearest neighbor classifiers. It might require a specialized probabilistic representation.\nAcknowledgement This research is funded by NSF grant IIS-1118050."
    } ],
    "references" : [ {
      "title" : "Algorithms and software for collaborative discovery from autonomous, semantically heterogeneous, distributed information sources",
      "author" : [ "D. Caragea", "J. Zhang", "J. Bao", "J. Pathak", "V. Honavar" ],
      "venue" : "Proceedings of the 16th International Conference on Algorithmic Learning Theory,",
      "citeRegEx" : "Caragea et al\\.,? 2005",
      "shortCiteRegEx" : "Caragea et al\\.",
      "year" : 2005
    }, {
      "title" : "Deep transfer via secondorder Markov logic",
      "author" : [ "J. Davis", "P. Domingos" ],
      "venue" : "Proceedings of the 26th Annual International Conference on Machine Learning, ICML ’09, 217–224.",
      "citeRegEx" : "Davis and Domingos,? 2009",
      "shortCiteRegEx" : "Davis and Domingos",
      "year" : 2009
    }, {
      "title" : "Semantic translation for rule-based knowledge in data mining",
      "author" : [ "D. Dou", "H. Qin", "H. Liu" ],
      "venue" : "Proceedings of the 22nd International Conference on Database and Expert Systems Applications, volume Part II of DEXA’11, 74–89.",
      "citeRegEx" : "Dou et al\\.,? 2011",
      "shortCiteRegEx" : "Dou et al\\.",
      "year" : 2011
    }, {
      "title" : "Probabilistic Graphical Models: Principles and Techniques",
      "author" : [ "D. Koller", "N. Friedman" ],
      "venue" : "MIT Press.",
      "citeRegEx" : "Koller and Friedman,? 2009",
      "shortCiteRegEx" : "Koller and Friedman",
      "year" : 2009
    }, {
      "title" : "Data integration: A theoretical perspective",
      "author" : [ "M. Lenzerini" ],
      "venue" : "Proceedings of the Twenty-first ACM SIGMODSIGACT-SIGART Symposium on Principles of Database Systems, PODS ’02, 233–246.",
      "citeRegEx" : "Lenzerini,? 2002",
      "shortCiteRegEx" : "Lenzerini",
      "year" : 2002
    }, {
      "title" : "Improving markov network structure learning using decision trees",
      "author" : [ "D. Lowd", "J. Davis" ],
      "venue" : "Journal of Machine Learning Research 15(1):501–532.",
      "citeRegEx" : "Lowd and Davis,? 2014",
      "shortCiteRegEx" : "Lowd and Davis",
      "year" : 2014
    }, {
      "title" : "Mapping and revising Markov logic networks for transfer learning",
      "author" : [ "L. Mihalkova", "T. Huynh", "R.J. Mooney" ],
      "venue" : "Proceedings of the 22nd National Conference on Artificial Intelligence, volume 1 of AAAI’07, 608–614.",
      "citeRegEx" : "Mihalkova et al\\.,? 2007",
      "shortCiteRegEx" : "Mihalkova et al\\.",
      "year" : 2007
    }, {
      "title" : "Distributed data mining: Algorithms, systems, and applications",
      "author" : [ "Park", "B.-H.", "H. Kargupta" ],
      "venue" : "Ye, N., ed., The Handbook of Data Mining. Lawrence Erlbaum Associates. 341–358.",
      "citeRegEx" : "Park et al\\.,? 2002",
      "shortCiteRegEx" : "Park et al\\.",
      "year" : 2002
    }, {
      "title" : "First-order probabilistic inference",
      "author" : [ "D. Poole" ],
      "venue" : "Proceedings of the 18th International Joint Conference on Artificial Intelligence, IJCAI’03, 985–991. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc.",
      "citeRegEx" : "Poole,? 2003",
      "shortCiteRegEx" : "Poole",
      "year" : 2003
    }, {
      "title" : "Sound and efficient inference with probabilistic and deterministic dependencies",
      "author" : [ "H. Poon", "P. Domingos" ],
      "venue" : "Proceedings of the 21st National Conference on Artificial Intelligence, volume 1 of AAAI’06, 458–463.",
      "citeRegEx" : "Poon and Domingos,? 2006",
      "shortCiteRegEx" : "Poon and Domingos",
      "year" : 2006
    }, {
      "title" : "Markov logic networks",
      "author" : [ "M. Richardson", "P. Domingos" ],
      "venue" : "Machine Learning 62:107–136.",
      "citeRegEx" : "Richardson and Domingos,? 2006",
      "shortCiteRegEx" : "Richardson and Domingos",
      "year" : 2006
    }, {
      "title" : "Artificial Intelligence: A Modern Approach",
      "author" : [ "S.J. Russell", "P. Norvig" ],
      "venue" : "Pearson Education, 2 edition.",
      "citeRegEx" : "Russell and Norvig,? 2003",
      "shortCiteRegEx" : "Russell and Norvig",
      "year" : 2003
    }, {
      "title" : "Heterogeneous transfer learning for image clustering via the social web",
      "author" : [ "Q. Yang", "Y. Chen", "G.-R. Xue", "W. Dai", "Y. Yu" ],
      "venue" : "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the",
      "citeRegEx" : "Yang et al\\.,? 2009",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : ", (Lenzerini 2002)) are the mostly studied areas in semantic integration.",
      "startOffset" : 2,
      "endOffset" : 18
    }, {
      "referenceID" : 0,
      "context" : "Distributed Data Mining Efforts in distributed data mining (DDM) (see surveys in (Park and Kargupta 2002; Caragea et al. 2005)) have made considerable progress in mining distributed data resources without putting data in a centralized location.",
      "startOffset" : 81,
      "endOffset" : 126
    }, {
      "referenceID" : 0,
      "context" : "(Caragea et al. 2005) proposes a general DDM framework with two components: one sends statistical queries to local data sources, and the other uses the returned statistics to revise the current partial hypothesis and generate further queries.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 0,
      "context" : "Heterogeneous DDM (Caragea et al. 2005) also handles the semantic heterogeneity between the global and local schemas, in particular, those containing attributes with different granularities called Attribute Value Taxonomy (AVT).",
      "startOffset" : 18,
      "endOffset" : 39
    }, {
      "referenceID" : 12,
      "context" : "The main exceptions are heterogeneous transfer learning (Yang et al. 2009) and relational transfer learning (e.",
      "startOffset" : 56,
      "endOffset" : 74
    }, {
      "referenceID" : 1,
      "context" : ", TAMAR (Mihalkova, Huynh, and Mooney 2007), deep transfer (Davis and Domingos 2009)).",
      "startOffset" : 59,
      "endOffset" : 84
    }, {
      "referenceID" : 3,
      "context" : "This can be done by standard variable elimination (Koller and Friedman 2009; Poole 2003) without calculating parameters.",
      "startOffset" : 50,
      "endOffset" : 88
    }, {
      "referenceID" : 8,
      "context" : "This can be done by standard variable elimination (Koller and Friedman 2009; Poole 2003) without calculating parameters.",
      "startOffset" : 50,
      "endOffset" : 88
    }, {
      "referenceID" : 11,
      "context" : "For relational case, the merging involves a first-order unification operation (Russell and Norvig 2003; Poole 2003).",
      "startOffset" : 78,
      "endOffset" : 115
    }, {
      "referenceID" : 8,
      "context" : "For relational case, the merging involves a first-order unification operation (Russell and Norvig 2003; Poole 2003).",
      "startOffset" : 78,
      "endOffset" : 115
    }, {
      "referenceID" : 5,
      "context" : "For the remaining source dataset, we used the decision tree structure learning (DTSL) (Lowd and Davis 2014) to learn the source knowledge.",
      "startOffset" : 86,
      "endOffset" : 107
    }, {
      "referenceID" : 10,
      "context" : "The UW-CSE dataset was introduced by Richardson and Domingos (Richardson and Domingos 2006) and is widely used in statistical relational learning research.",
      "startOffset" : 61,
      "endOffset" : 91
    }, {
      "referenceID" : 9,
      "context" : "We use MC-SAT (Poon and Domingos 2006) as the sampling algorithm for these experiments.",
      "startOffset" : 14,
      "endOffset" : 38
    } ],
    "year" : 2015,
    "abstractText" : "In this paper, we focus on a novel knowledge reuse scenario where the knowledge in the source schema needs to be translated to a semantically heterogeneous target schema. We refer to this task as “knowledge translation” (KT). Unlike data translation and transfer learning, KT does not require any data from the source or target schema. We adopt a probabilistic approach to KT by representing the knowledge in the source schema, the mapping between the source and target schemas, and the resulting knowledge in the target schema all as probability distributions, specially using Markov random fields and Markov logic networks. Given the source knowledge and mappings, we use standard learning and inference algorithms for probabilistic graphical models to find an explicit probability distribution in the target schema that minimizes the Kullback-Leibler divergence from the implicit distribution. This gives us a compact probabilistic model that represents knowledge from the source schema as well as possible, respecting the uncertainty in both the source knowledge and the mapping. In experiments on both propositional and relational domains, we find that the knowledge obtained by KT is comparable to other approaches that require data, demonstrating that knowledge can be reused without data.",
    "creator" : "TeX"
  }
}