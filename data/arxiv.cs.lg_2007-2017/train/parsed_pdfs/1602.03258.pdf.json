{
  "name" : "1602.03258.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Interactive Bayesian Hierarchical Clustering",
    "authors" : [ "Sharad Vikram", "Sanjoy Dasgupta" ],
    "emails" : [ "SVIKRAM@CS.UCSD.EDU", "DASGUPTA@CS.UCSD.EDU" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Clustering is a basic tool of exploratory data analysis. There are a variety of efficient algorithms—including kmeans, EM for Gaussian mixtures, and hierarchical agglomerative schemes—that are widely used for discovering “natural” groups in data. Unfortunately, they don’t always find a grouping that suits the user’s needs.\nThis is inevitable. In any moderately complex data set, there are many different plausible grouping criteria. Should a collection of rocks be grouped according to value, or shininess, or geological properties? Should animal pictures be grouped according to the Linnaean taxonomy, or cuteness? Different users have different priorities, and an unsupervised algorithm has no way to magically guess these.\nAs a result, a rich body of work on constrained clustering has emerged. In this setting, a user supplies guidance, typically in the form of “must-link” or “cannot-link” constraints, pairs of points that must be placed together or apart. Introduced by Wagstaff & Cardie (2000), these constraints have since been incorporated into many differ-\nProceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s).\nent flat clustering procedures (Wagstaff et al., 2001; Bansal et al., 2004; Basu et al., 2004; Kulis et al., 2005; Biswas & Jacobs, 2014).\nIn this paper, we introduce constraints to hierarchical clustering, the recursive partitioning of a data set into successively smaller clusters to form a tree. A hierarchy has several advantages over a flat clustering. First, there is no need to specify the number of clusters in advance. Second, the tree captures cluster structure at multiple levels of granularity, simultaneously. As such, trees are particularly wellsuited for exploratory data analysis and the discovery of natural groups.\nThere are several well-established methods for hierarchical clustering, the most prominent among which are the bottom-up agglomerative methods such as average linkage (see, for instance, Chapter 14 of Hastie et al. (2009)). But they suffer from the same problem of under-specification that is the scourge of unsupervised learning in general. And, despite the rich literature on incorporating additional guidance into flat clustering, there has been relatively little work on the hierarchical case.\nWhat form might the user’s guidance take? The usual mustlink and cannot-link constraints make little sense when data has hierarchical structure. Among living creatures, for instance, should elephant and tiger be linked? At some level, yes, but at a finer level, no. A more straightforward assertion is that elephant and tiger should be linked in a cluster that does not include snake. We can write this as a triplet ({elephant,tiger},snake). We could also assert ({tiger,leopard},elephant). Formally, ({a, b}, c) stipulates that the hierarchy contains a subtree (that is, a cluster) containing a and b but not c.\nA wealth of research addresses learning taxonomies from triplets alone, mostly in the field of phylogenetics: see Felsenstein (2004) for an overview, and Aho et al. (1981) for a central algorithmic result. Let’s say there are n data items to be clustered, and that the user seeks a particular hierarchy T ∗ on these items. This T ∗ embodies at most( n 3 ) triplet constraints, possibly less if it is not binary. It was pointed out in Tamuz et al. (2011) that roughly n log n\nar X\niv :1\n60 2.\n03 25\n8v 3\n[ cs\n.L G\n] 2\n7 A\npr 2\n01 6\ncarefully-chosen triplets are enough to fully specify T ∗ if it is balanced. This is also a lower bound: there are nΩ(n) different labeled rooted trees, so each tree requires Ω(n log n) bits, on average, to write down—and each triple provides O(1) bits of information, since there are just three possible outcomes for each set of points a, b, c. Although n log n is a big improvement over n3, it is impractical for a user to provide this much guidance when the number of points is large. In such cases, a hierarchical clustering cannot be obtained on the basis of constraints alone; the geometry of the data must play a role.\nWe consider an interactive process during which a user incrementally adds constraints.\n• Starting with a pool of data X ⊆ Rd, the machine builds a candidate hierarchy T .\n• The set of constraints C is initially empty.\n• Repeat:\n– The machine presents the user with a small portion of T : specifically, its restriction to O(1) leaves S ⊂ X . We denote this T |S . – The user either accepts T |S , or provides a triplet constraint ({a, b}, c) that is violated by it.\n– If a triplet is provided, the machine adds it to C and modifies the tree T accordingly.\nIn realizing this scheme, a suitable clustering algorithm and querying strategy must be designed. Similar issues have been confronted in flat clustering—with must-link and cannot-link constraints—but the solutions are unsuitable for hierarchies, and thus a fresh treatment is warranted.\nTHE CLUSTERING ALGORITHM\nWhat is a method of hierarchical clustering that takes into account the geometry of the data points as well as userimposed constraints?\nWe adopt an interactive Bayesian approach. The learning procedure is uncertain about the intended tree and this uncertainty is captured in the form of a distribution over all possible trees. Initially, this distribution is informed solely by the geometry of the data but once interaction begins, it is also shaped by the growing set of constraints.\nThe nonparametric Bayes literature contains a variety of different distributional models for hierarchical clustering. We describe a general methodology for extending these to incorporate user-specified constraints. For concreteness, we focus on the Dirichlet diffusion tree (Neal, 2003), which has enjoyed empirical success. We show that triplet constraints are quite easily accommodated: when using a\nMetropolis-Hastings sampler, they can efficiently be enforced, and the state space remains strongly connected, assuring convergence to the unique stationary distribution.\nTHE QUERYING STRATEGY\nWhat is a good way to select the subsets S? A simple option is to pick them at random from X . We show that this strategy leads to convergence to the target tree T ∗. Along the way, we define a suitable distance function for measuring how close T is to T ∗.\nWe might hope, however, that a more careful choice of S would lead to faster convergence, in much the same way that intelligent querying is often superior to random querying in active learning. In order to do this, we show how the Bayesian framework allows us to quantify which portions of the tree are the most uncertain, and thereby to pick S that focuses on these regions.\nQuerying based on uncertainty sounds promising, but is dangerous because it is heavily influenced by the choice of prior, which is ultimately quite arbitrary. Indeed, if only such queries were used, the interactive learning process could easily converge to the wrong tree. We show how to avoid this situation by interleaving the two types of queries.\nFinally, we present a series of experiments that illustrate how a little interaction leads to significantly better hierarchical clusterings."
    }, {
      "heading" : "1.1. Other related work",
      "text" : "A related problem that has been studied in more detail (Zoller & Buhmann, 2000; Eriksson et al., 2011; Krishnamurthy et al., 2012) is that of building a hierarchical clustering where the only information available is pairwise similarities between points, but these are initially hidden and must be individually queried.\nIn another variant of interactive flat clustering (Balcan & Blum, 2008; Awasthi & Zadeh, 2010; Awasthi et al., 2014), the user is allowed to specify that individual clusters be merged or split. A succession of such operations can always lead to a target clustering, and a question of interest is how quickly this convergence can be achieved.\nFinally, it is worth mentioning the use of triplet constraints in learning other structures, such as Euclidean embeddings (Borg & Groenen, 2005)."
    }, {
      "heading" : "2. Bayesian hierarchical clustering",
      "text" : "The most basic form of hierarchical clustering is a rooted binary tree with the data points at its leaves. This is sometimes called a cladogram. Very often, however, the tree is adorned with additional information, for instance:\n1. An ordering of the internal nodes, where the root is assigned the lowest number and each node has a higher number than its parent.\nThis ordering uniquely specifies the induced kclustering (for any k): just remove the k − 1 lowestnumbered nodes and take the clusters to be the leafsets of the k resulting subtrees.\n2. Lengths on the edges.\nIntuitively, these lengths correspond to the amount of change (for instance, time elapsed) along the corresponding edges. They induce a tree metric on the nodes, and often, the leaves are required to be at the same distance from the root.\n3. Parameters at internal nodes.\nThese parameters are sometimes from the same space as the data, representing intermediate values on the way from the root to the leaves.\nMany generative processes for trees end up producing these more sophisticated structures, with the understanding that undesired additional information can simply be discarded at the very end. We now review some well-known distributions over trees and over hierarchical clusterings.\nLet’s start with cladograms on n leaves. The simplest distribution over these is the uniform. Another well-studied option is the Yule model, which can be described using either a top-down or bottom-up generative process. The top-down view corresponds to a continuous-time pure birth process: start with one lineage; each lineage persists for a random exponential(1) amount of time and then splits into two lineages; this goes on until there are n lineages. The bottom-up view is a coalescing process: start with n points; pick a random pair of them to merge; then repeat. Aldous (1995) has defined a one-parameter family of distributions over cladograms, called the beta-splitting model, that includes the uniform and the Yule model as special cases. It is a top-down generative process in which, roughly, each split is made by sampling from a Beta distribution to decide how many points go on each side. To move to arbitrary (not necessarily binary) splits, a suitable generalization is the Gibbs fragmentation tree (McCullagh et al., 2008).\nIn this paper, we will work with joint distributions over both tree structure and data. These are typically inspired by, or based directly upon, the simpler tree-only distributions described above. Our primary focus is the Dirichlet diffusion tree (Neal, 2003), which is specified by a birth process that we will shortly describe. However, our methodology applies quite generally. Other notable Bayesian approaches to hierarchical clustering include: Williams (2000), in which each node of the tree is annotated with a vector that\nis sampled from a Gaussian centered at its parent’s vector; Heller & Ghahramani (2005), that defines a distribution over flat clusterings and then specifies an agglomerative scheme for finding a good partition with respect to this distribution; Adams et al. (2008), in which data points are allowed to reside at internal nodes of the tree; Teh et al. (2008); Boyles & Welling (2012), in which the distribution over trees is specified by a bottom-up coalescing process; and Knowles & Ghahramani (2015), which generalizes the Dirichlet diffusion trees to allow non-binary splits."
    }, {
      "heading" : "2.1. The Dirichlet diffusion tree",
      "text" : "The Dirichlet diffusion tree (DDT) is a generative model for d-dimensional vectors x1, x2, . . . , xN . Data are generated sequentially via a continuous-time process, lasting from time t = 0 to t = 1, whereupon they reach their final value.\nThe first point, x1, is generated via a Brownian motion, beginning at the origin, i.e. X1(t + dt) = X1(t) + N (0, σ2Iddt) where X1(t) represents the value of x1 at time t. The next point, x2, follows the path created by x1 until it eventually diverges at a random time, according to a specified acquisition function a(t). When x2 diverges, it creates an internal node in the tree structure which contains both the time and value of x2 when it diverged. After divergence, it continues until t = 1 with an independent Brownian motion. In general, the i-th point follows the path created by the previous i−1 points. When it reaches a node, it will first sample one of two branches to enter, then either 1) diverge on the branch, whereupon a divergence time is sampled according to the acquisition function a(t), or 2) recursively continue to the next node. Each of these choices has a probability associated with it, according to various properties of the tree structure and choice of acquisition function (details can be found in Neal (2003)). Eventually, all points will diverge and continue independently, creating an internal node storing the time and intermediate value for each point at divergence. The DDT thus defines a binary tree over the data (see Figure 1 for an example). Furthermore, given a DDT with N points, it is possible to sample the possible divergence locations of a (N + 1)-th point, using the generative process.\nSampling the posterior DDT given data can be done with the Metropolis-Hastings (MH) algorithm, an MCMC method. The MH algorithm obtains samples from target distribution p(x) indirectly by instead sampling from a conditional “proposal” distribution q(x|x′), creating a Markov chain whose stationary distribution is p(x), assuming q satisfies some conditions. Our choice of proposal distribution modifies the DDT’s tree structure via a subtree-prune and regraft (SPR) move, which has the added benefit of extending to other distributions over hierarchies.\nTHE SUBTREE-PRUNE AND REGRAFT MOVE\nAn SPR move consists of first a prune then a regraft. Suppose T is a binary tree with n leaves. Let s be a non-root node in T selected uniformly at random and S be its corresponding subtree. To prune S from T , we remove s’s parent p from T , and replace p with s’s sibling.\nRegrafting selects a branch at random and attaches S to it as follows. Let (u, v) be the chosen branch (u is the parent of v). S is attached to the branch by creating a node p with children s and v and parent u (see Figure 2 for an example).\nThe MH proposal distribution for the DDT is an augmented SPR move, where the time and intermediate value at each node are sampled in addition to tree structure. The exchangeability of the DDT enables efficient sampling of regraft branches by simulating the generation process for a new point and returning the branch and time where it diverges. The intermediate values for the entire tree are sampled via an interleaved Gibbs sampling move, as all conditional distributions are Gaussian."
    }, {
      "heading" : "3. Adding interaction",
      "text" : "Impressive as the Dirichlet diffusion tree is, there is no reason to suppose that it will magically find a tree that suits the user’s needs. But a little interaction can be helpful in improving the outcome.\nLet T ∗ denote the target hierarchical clustering. It is not necessarily the case that the user would be able to write this down explicitly, but this is the tree that captures the distinctions he/she is able to make, or wants to make. Figure 3 (left) shows an example, for a small data set of 5 points.\nIn this case, the user does not wish to distinguish between points 1, 2, 3, but does wish to place them in a cluster that excludes point 4.\nWe could posit our goal as exactly recovering T ∗. But in many cases, it is good enough to find a tree that captures all the distinctions within T ∗ but also possibly has some extraneous distinctions, as in the right-hand side of Figure 3.\nFormally, given data set X , we say S ⊆ X is a cluster of tree T if there is some node of T whose descendant leaves are exactly S. We say T is a refinement of T ∗ if they have the same set of leaves, and moreover every cluster of T ∗ is also a cluster of T . This, then, is our goal: to find a refinement of the target clustering T ∗."
    }, {
      "heading" : "3.1. Triplets",
      "text" : "The user provides feedback in the form of triplets. The constraint ({a, b}, c) means that the tree should have a cluster containing a and b but not c. Put differently, the lowest common ancestor of a, b should be a strict descendant of the lowest common ancestor of a, b, c.\nLet ∆(T ) denote the set of all proper triplet constraints embodied in tree T . If T has n nodes, then |∆(T )| ≤ ( n 3 ) . For non-binary trees, it will be smaller than this number. Figure 3 (left), for instance, has no triplet involving 1, 2, 3.\nRefinement can be characterized in terms of triplets.\nLemma 3.1. Tree T is a refinement of tree T ′ if and only if ∆(T ′) ⊆ ∆(T ).\nProof. See supplement.\nIn particular, any triplet-querying scheme that converges to the full set of triplets of the target tree T ∗ is also guaranteed to produce trees that converge to a refinement of T ∗.\nWith this lemma in mind, it is natural to measure how close a tree T is to the target T ∗ with the following (asymmetric) distance function, which we call triplet distance (TD):\nTD(T ∗, T ) =\n∑ c∈∆(T∗) I(c /∈ ∆(T ))\n|∆(T ∗)| (1)\nwhere I is the indicator function. This distance is zero exactly when T is a refinement of T ∗, in which case we have reached our goal.\nA simple strategy for obtaining triplets would be to present the user with three randomly chosen data points and have the user pick the odd one out. This strategy has several drawbacks. First, some sets of three points have no triplet constraint (for instance, points 1, 2, 3 in Figure 3). Second, the chosen set of points might correspond to a triplet that has already been specified, or is implied by specified triplets. For example, knowledge of ({a, b}, c) and ({b, c}, d) implies ({a, c}, d). Enumerating the set of implied triplets is non-trivial for n > 3 triplets (Bryant & Steel, 1995), making it difficult to avoid these implied triplets in the first place.\nWe thus consider another strategy—rather than the user arranging three data points into a triplet, the user observes the hierarchy induced over some O(1)-sized subset S of the data and corrects an error in the tree by supplying a triplet. We call this is a subtree query. Finally, we note that in this work we only consider the realizable case where the triplets obtained from a user do not contain contradictory information and that there is a tree that satisfies all of them."
    }, {
      "heading" : "3.2. Finding a tree consistent with constraints",
      "text" : "We start with a randomly initialized hierarchy T over our data and show an induced subtree T |S to the user, obtaining the first triplet. The next step is constructing a new tree that satisfies the triplet. This begins the feedback cycle; a user provides a triplet given a subtree and the triplet is incorporated into a clustering algorithm, producing a new candidate tree. A starting point is an algorithm that returns a tree consistent with a set of triplets.\nThe simplest algorithm to solve this problem is the BUILD algorithm, introduced in Aho et al. (1981). Given a set of triplets C, BUILD will either return a tree that satisfies C, or error if no such tree exists. In BUILD, we first construct the Aho graph GC , which has a vertex for each data point and an undirected edge {a, b} for each triplet constraint ({a, b}, c). If GC is connected, there is no tree that satisfies all triplets. Otherwise, the top split of the tree is a partition of the connected components of GC : any split is fine as long as points in the same component stay together. Satisfied triplets are discarded, and BUILD then continues recursively on the left and right subtrees.\nBUILD satisfies triplet constraints but ignores the geometry of the data, whereas we wish to take both into account. By incorporating triplets into the posterior DDT sampler, we obtain high likelihood trees that still satisfy C."
    }, {
      "heading" : "3.3. Incorporating triplets into the sampler",
      "text" : "In this section, we present an algorithm to sample candidate trees from the posterior DDT, constrained by a triplet setC. It is based on the subtree prune and regraft move.\nThe SPR move is of particular interest because we can efficiently enforce triplets to form a constrained-SPR move, resulting in a sampler that only produces trees that satisfy a set of triplets. A constrained-SPR move is defined as an SPR move that assigns zero probability to any resulting trees that would violate a set of triplets. Restricting the neighborhood of an SPR move runs the risk of partitioning the state space, losing the convergence guarantees of the Metropolis-Hastings algorithm. Fortunately, a constrainedSPR move does not compromise strong connectivity. For any realizable triplet set C, we prove the constrained-SPR move Markov chain’s aperiodicity and irreducibility.\nConsider the Markov chain on the state space of rooted binary trees that is induced by the constrained sampler.\nLemma 3.2. The constrained-SPR Markov chain is aperi-\nodic.\nProof. A sufficient condition for aperiodicity is the existent of a “self-loop” in the transition matrix: a non-zero probability of a state transitioning to itself. Supposed we have pruned a subtree already. When regrafting, the ordinary SPR move has a non-zero probability of choosing any branch, and a constrained-SPR move cannot regraft to branches that would violate triplets. Since the current tree in the Markov chain satisfies triplet set C, there is a nonzero probability of regrafting to the same location. We thus have an aperiodic Markov chain.\nLemma 3.3. A constrained-SPR Markov chain is irreducible.\nProof. (sketch) To show irreducibility, we show that a tree T has an non-zero probability of reaching an arbitrary tree T ′ via constrained-SPR moves where both T and T ′ satisfy a set of triplets C. Our proof strategy is to construct a canonical tree TC , and show that there exists a non-zero probability path from T to TC , and therefore from T ′ to TC . We then show that for a given constrained-SPR move, the reverse move has a non-zero probability. Thus, there exists a path from T to TC to T ′, satisfying irreducibility.\nRecall that the split at a node in a binary tree that satisfies triplet set C corresponds to a binary partition of the Aho graph at the node (see Section 3.2). TC is a tree such that every node in TC is in canonical form. A node is in canonical form if it is a leaf node, or, the partition of the Aho graph at that node can be written as (l, r). l is the single connected component containing the point with the minimum data index, and r is the rest of the components.\nTo convert a particular node s into canonical form, we first perform “grouping”, which puts l into a single descendant of s via constrained-SPR moves. We then make two constrained-SPR moves to convert the partition at s into the\nform (l, r) (see Figure 5). We convert all nodes into canonical form recursively, turning an arbitrary tree T into TC .\nFinally, the reverse constrained-SPR move has a non-zero probability. Suppose we perform a constrained-SPR move on tree T1, converting it into T2 by detaching subtree s and attaching it to branch (u, v). A constrained-SPR move on T2 can select s for pruning and can regraft it to form T1 with a non-zero probability since T1 satisfies the same constraint set as C. For a full proof, please refer to the supplement.\nThe simplest possible scheme for a constrained-SPR move would be rejection sampling. The Metropolis-Hastings algorithm for the DDT would be the same as in the unconstrained case, but any trees violating C would have accept probability 0. Although this procedure is correct, it is impractical. As the number of triplets grows larger, more trees will be rejected and the sampler will slow down over time.\nTo efficiently sample a tree that satisfies a set of triplets C, we modify the regraft in the ordinary SPR move. The constrained-SPR move must assign zero probability to any regraft branches that would result in a tree that violates C. This is accomplished by generating the path from the root in the same manner as sampling a branch, but avoiding paths that would resulted in violated triplets.\nDESCRIPTION OF CONSTRAINED-SPR SAMPLER\nRecall that in the DDT’s sampling procedure for regraft branches, a particle at a node picks a branch, and either diverges from that branch or recursively samples the node’s child. Let s be the root of the subtree we are currently grafting back onto tree T , let C be the triplet set, and let leaves(u) denote the descendant-leaves of node u. Suppose we are are currently at node u, deciding whether to diverge at the branch (u, v) or to recursively sample v. Consider any triplet ({a, b}, c) ∈ C. If all—or none—of a, b, c are in leaves(s), then the triplet is unaffected by\nthe graft, and can be ignored. Otherwise, some checks are needed:\n1. c ∈ leaves(s) Then we know a, b 6∈ leaves(s). If a and b are split across v’s children, we are banned from sampling v.\n2. a ∈ leaves(s) but b, c 6∈ leaves(s) If both b and c are in leaves(v), we are required to sample v. If just c is in leaves(v), we are banned from both diverging at (u, v) and sampling v. Otherwise we can either diverge at (u, v) or sample v.\n(The case where b ∈ leaves(s) is symmetric to case 2.) If we choose to sample v, we remove constraints from our current set C that are now satisfied, and continue recursively. This defines a procedure by which we can sample a divergence branch that does not violate constraints.\nWhile the constrained-SPR sampler can produce a set of trees given a set of static constraints, the BUILD algorithm is useful in adding new triplets into the sampler. Suppose we have been sampling trees with constrained-SPR moves with satisfying triplet set C and we obtain a new triplet u = ({a, b}, c) from a user query. We take the current tree T and find the least common ancestor (call it z) of a and b. We then call BUILD(C + {u}) on just the nodes in leaves(z), and we substitute the resulting subtree at position z in tree T ."
    }, {
      "heading" : "3.4. Intelligent subset queries",
      "text" : "We now have a method to sample a constrained distribution over candidate trees. Given a particular candidate tree T , our first strategy for subtree querying is to pick a random subset S of the leaves of constant size, and show the user the induced subtree over the subset, T |S . We call this random subtree querying. But can we use a set of trees\nproduced by the sampler to make better subtree queries? If tree structure is ambiguous in a particular region of data, i.e. there are several hierarchies that could explain a particular configuration of data, the MH algorithm will sample over these different configurations. A query over points in these ambiguous regions may help our algorithm converge to a better tree faster. By looking for these regions in our samples, we can choose query subsets S for which the tree structure is highly variable, and hopefully the resulting triplet from the user will reduce the ambiguity.\nMore precisely, we desire a notion of tree variance. Given a set of trees T , what is the variance over a given subset of the data S? We propose using the notion of tree distance as a starting point. For a given tree T , the tree distance between two nodes a and b, denoted treedistT (a, b), is the number of edges of T needed to get from a to b. Consider two leaves u and v. If the tree structure around them is static, we expect the tree distance between u and v to change very little, as the surrounding tree will not change. However, if there is ambiguity in the surrounding structure, the tree distance will be more variable. Given a subset of data S and a set of trees T , the tree distance variance (TDV) of the trees over the subset is defined as:\nTDV(T , S) = max u,v∈S VarT∈T [treedistT |S (u, v)] (2)\nThis measure of variance is the max of the variance of tree distance between any two points in the subset. Computing this requires O(|T ||S|2 log |S|) time, and since since |S| is constant, it is not prohibitively expensive.\nGiven a set of trees from the sampler T , we now select a high-variance subtree by instantiating L random subsets of constant size, S1, . . . , SL and picking argmaxlTDV(T , Sl). We call this active subtree querying. Although using tree variance will help reconcile ambiguity in the tree structure, if a set of samples from a tree all violate the same triplet, it is unlikely that active querying\nwill recover that triplet. Thus, interleaving random querying and active querying will hopefully help the algorithm converge quickly, while avoiding local optima."
    }, {
      "heading" : "4. Experiments",
      "text" : "We evaluated the convergence properties of five different querying schemes. In a “simple query”, a user is presented with three random data and picks an odd one out. In a “smart query”, a user is unrealistically shown the entire candidate tree and reports a violated triplet. In a “random query”, the user is shown the induced candidate tree over a random subset of the data. In an “active query”, the user is shown a high variance subtree using tree-distance variance. Finally, in an “interleaved query”, the user is alternatively shown a random subtree and a high variance subtree. In each experiment, T ∗ was known, so user queries were simulated by picking a triplet violated by the root split of the queried tree, and if no such triplet existed, recursing on a child. Each scheme was evaluated on four different datasets. The first dataset, MNIST (Lecun et al., 1998), is an 10-way image classification dataset where the data are 28 x 28 images of digits. The target tree T ∗ is simply the K-way classification tree over the data. The second dataset is Fisher Iris, a 3-way flower classification problem, where each of 150 flowers has five features. The third dataset, Zoo (Lichman, 2013), is a set of 93 animals and 15 binary morphological features for each of animals, the target tree being the induced binary tree from the Open Tree of Life (Hinchliff et al., 2015). The fourth dataset is 20 Newsgroups (Joachims, 1997), a corpus of text articles on 20 different subjects. We use the first 10 principal components as features in this classification problem. All datasets were modeled with DDT’s with acquisition function a(t) = 1/(1 − t) and Brownian motion parameter σ2 estimated from data. To better visualize the different convergence rates of the querying schemes, MNIST and 20 Newsgroups were subsampled to 150 random points.\nFor each dataset and querying scheme, we instantiated a SPR sampler with no constraints. Every one hundred iterations of the sampler, we performed a query. In subtree queries, we used subsets of size |S| = 10 and in active querying, the highest-variance subset was chosen from L = 20 different random subsets. As baselines, we measured the triplet distance of the vanilla DDT and the average linkage tree. Finally, results were averaged over four runs of each sampler. The triplet distances for Fisher Iris and MNIST can be seen in Figure 6. Results for the other datasets can be found in the supplement. Although unrealistic due to the size of the tree shown to the user, the smart query performed the best, achieving minimum error with the least amount of queries. Interleaved followed next, followed by active, random, and simple. In general, the vanilla\nDDT performed the worst, and the average linkage score varied on each dataset, but in all cases, the subtree querying schemes performed better than both the vanilla DDT and average linkage.\nIn three datasets (MNIST, Fisher Iris and Zoo), interactive methods achieve higher data likelihood than the vanilla DDT. Initially, the sampler is often restructuring the tree with new triplets and data likelihood is unlikely to rise. However, over time as less triplets are reported, the data likelihood increases rapidly. We thus conjecture that triplet constraints may help the MH algorithm find better optima."
    }, {
      "heading" : "5. Future Work",
      "text" : "We are interested in studying the non-realizable case, i.e. when there does not exist a tree that satisfies triplet set C. We would also like to better understand the effect of constraints on searching for optima using MCMC methods."
    }, {
      "heading" : "A. Proof Details",
      "text" : "Lemma 3.1. Tree T is a refinement of tree T ′ if and only if ∆(T ′) ⊆ ∆(T ).\nProof. Suppose, first, that T is a refinement of T ′. Pick any triplet ({a, b}, c) ∈ ∆(T ′). Then there is a node in T ′ whose descendants include a, b but not c. By the definition of refinement, T contains a node with the same descendants. Hence the constraint ({a, b}, c) holds for T as well.\nConversely, say ∆(T ′) ⊆ ∆(T ). Pick any cluster S′ of T ′; it consists of the descendants of some node in T ′. Consider the set of all triplet constraints consisting of two nodes of S′ and one node outside S′. Since these constraints also hold for T , it follows that the lowest common ancestor of S′ in T must have exactly S′ as its set of descendants. Thus S′ is also a cluster of T .\nLemma 3.3. A constrained-SPR Markov chain is irreducible.\nProof. To prove irreducibility, we show that there is a non-zero probability of moving from state T to T ′, both of which satisfy C. We accomplish this by first defining a canonical tree TC given a triplet set C and showing that we can reach TC from T using constrained-SPR moves. We then show that for every constrained-SPR move, there exists an equivalent reverse move that undoes it with non-zero probability. This proves that that from TC we can reach T ′, creating a path from T to TC to T ′.\nA binary tree T can be entirely defined by the bipartitions made over the data at each node. Let Gn be the Aho graph for node n. For a binary tree that satisfies a set of triplets, the split over the data at each node n must be a bipartition of the connected components of Gn. We define a particular node to be in canonical form if either a) it is a leaf, or b) the bipartition over Gn at that node can be written as (l, r), where l exactly matches a single, particular connected component of Gn, and r is the rest of the connected components. The particular component l is the connected component in Gn with the minimum data index inside it. Note that we treat the children of nodes as unordered. A canonical tree TC is one such that every node in the tree is in canonical form. To convert an arbitrary tree T that satisfies C into TC , we first convert the root node of T into canonical form using constrained-SPR moves.\nLet s be the root of T and let l be the set of points that ought to be in their own partition according to Gs. In order for s not to be in canonical form, l must be in a partition with data from other connected components in Gs, which we will call o. The bipartition if s were in canonical form would be (l, r) and the current non-canonical bipartition can thus be written as (l + o, r − o).\nWe first examine t, the child of the root that contains l + o. In general, the data from l and the data from o could be split over the children of t, so the partition at t can be written as (l1 + o1, l2 + o2) where l = l1 + l2 and o = o1 + o2. This is visualized in the first tree of Figure 5. We first group the data from l into their own “pure” subtree of t as follows. Let u be the root of the lowest non-pure subtree of t that has data from l in both of its children. There exist two subtrees that are descendants of u that contain data from l (one on the left and one on the right). Those two subtrees must be pure, and furthermore, they are both free to move within u via constrained-SPR moves because they are in different connected components in Gu. Thus, we can perform a constrained-SPR move to merge these two pure subtrees together into a larger pure subtree. We can repeat this process for t until all nodes from l are in their own pure subtree of t. The partition of t can thus be written as (l + o1, o2), since the pure subtree may be several levels down from t. This grouping process is visualized in Figure S1 and the results can be seen in the second tree in Figure 5.\nWe now perform a constrained-SPR move to detach the pure subtree of l and regraft it to the edge between s and t. This is a permissible move since l is its own connected component in Gs. We now have the third tree in Figure S2. We now perform a final constrained-SPR move, moving the subtree of o to the opposite side of s, creating the proper canonical partition of (l, r). To entirely convert T into TC , we need to recurse and convert every node in T into canonical form.\nEvery constrained-SPR move has an associated reverse constrained-SPR move that performs the opposite transition. The reverse constrained-SPR move selects the same subtree as the forward one and prunes it, and just regrafts the subtree to its original location before the forward move. We know that this regraft has non-zero probability because the original tree did not violate constraints. Thus, since any arbitrary T can be converted into TC and since each move has a non-zero probability reverse move, TC can be converted into an arbitrary tree T ′ and we have a non-zero probability path to convert T into T ′.\nFigure S1. The process of grouping the data in u that belong to l into their own pure subtree. u is the lowest node of t (see Figure 5) that has data from l in both of its children.\nFigure S2. The process of converting s into canonical form. We first group nodes from l into their own pure subtree, then perform two constrained-SPR moves to put s into canonical form."
    }, {
      "heading" : "B. Additional Results",
      "text" : "(a) Zoo (b) 20 Newsgroups\nFigure S3. The average of four runs of constrained-SPR samplers for the Zoo dataset and the 20 Newsgroups dataset, using 5 different querying schemes. A query was made every 100 iterations."
    } ],
    "references" : [ {
      "title" : "Treestructured stick breaking for hierarchical data",
      "author" : [ "R.P. Adams", "Z. Ghahramani", "M.I. Jordan" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Adams et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Adams et al\\.",
      "year" : 2008
    }, {
      "title" : "Inferring a tree from lowest common ancestors with an application to the optimization of relational expressions",
      "author" : [ "A.V. Aho", "Y. Sagiv", "T.G. Szymanski", "J.D. Ullman" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "Aho et al\\.,? \\Q1981\\E",
      "shortCiteRegEx" : "Aho et al\\.",
      "year" : 1981
    }, {
      "title" : "Probability distributions on cladograms",
      "author" : [ "D. Aldous" ],
      "venue" : "Random Discrete Structures (IMA Volumes in Mathematics and its Applications",
      "citeRegEx" : "Aldous,? \\Q1995\\E",
      "shortCiteRegEx" : "Aldous",
      "year" : 1995
    }, {
      "title" : "Supervised clustering",
      "author" : [ "P. Awasthi", "R.B. Zadeh" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Awasthi and Zadeh,? \\Q2010\\E",
      "shortCiteRegEx" : "Awasthi and Zadeh",
      "year" : 2010
    }, {
      "title" : "Local algorithms for interactive clustering",
      "author" : [ "P. Awasthi", "Balcan", "M.-F", "K. Voevodski" ],
      "venue" : "In Proceedings of the 31st International Conference on Machine Learning,",
      "citeRegEx" : "Awasthi et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Awasthi et al\\.",
      "year" : 2014
    }, {
      "title" : "Clustering with interactive feedback",
      "author" : [ "Balcan", "M.-F", "A. Blum" ],
      "venue" : "Notes in Computer Science),",
      "citeRegEx" : "Balcan et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Balcan et al\\.",
      "year" : 2008
    }, {
      "title" : "Active semisupervision for pairwise constrained clustering",
      "author" : [ "S. Basu", "A. Banerjee", "R. Mooney" ],
      "venue" : "In SIAM International Conference on Data Mining,",
      "citeRegEx" : "Basu et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Basu et al\\.",
      "year" : 2004
    }, {
      "title" : "Active image clustering with pairwise constraints from humans",
      "author" : [ "A. Biswas", "D. Jacobs" ],
      "venue" : "International Journal of Computer Vision,",
      "citeRegEx" : "Biswas and Jacobs,? \\Q2014\\E",
      "shortCiteRegEx" : "Biswas and Jacobs",
      "year" : 2014
    }, {
      "title" : "Modern Multidimensional Scaling: Theory and Applications",
      "author" : [ "I. Borg", "P.J.F. Groenen" ],
      "venue" : null,
      "citeRegEx" : "Borg and Groenen,? \\Q2005\\E",
      "shortCiteRegEx" : "Borg and Groenen",
      "year" : 2005
    }, {
      "title" : "The time-marginalized coalescent prior for hierarchical clustering",
      "author" : [ "L. Boyles", "M. Welling" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Boyles and Welling,? \\Q2012\\E",
      "shortCiteRegEx" : "Boyles and Welling",
      "year" : 2012
    }, {
      "title" : "Extension operations on sets of leaf-labeled trees",
      "author" : [ "D Bryant", "M. Steel" ],
      "venue" : "Advances in Applied Mathematics,",
      "citeRegEx" : "Bryant and Steel,? \\Q1995\\E",
      "shortCiteRegEx" : "Bryant and Steel",
      "year" : 1995
    }, {
      "title" : "Active clustering: robust and efficient hierarchical clustering using adaptively selected similarities",
      "author" : [ "B. Eriksson", "G. Dasarathy", "A. Singh", "R. Nowak" ],
      "venue" : "In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Eriksson et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Eriksson et al\\.",
      "year" : 2011
    }, {
      "title" : "Bayesian hierarchical clustering",
      "author" : [ "K. Heller", "Z. Ghahramani" ],
      "venue" : "In Proceedings of the 22nd International Conference on Machine Learning,",
      "citeRegEx" : "Heller and Ghahramani,? \\Q2005\\E",
      "shortCiteRegEx" : "Heller and Ghahramani",
      "year" : 2005
    }, {
      "title" : "A probabilistic analysis of the rocchio algorithm with tfidf for text categorization",
      "author" : [ "Joachims", "Thorsten" ],
      "venue" : "In Proceedings of the Fourteenth International Conference on Machine Learning,",
      "citeRegEx" : "Joachims and Thorsten.,? \\Q1997\\E",
      "shortCiteRegEx" : "Joachims and Thorsten.",
      "year" : 1997
    }, {
      "title" : "Pitman-Yor diffusion trees for Bayesian hierarchical clustering",
      "author" : [ "D.A. Knowles", "Z. Ghahramani" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "Knowles and Ghahramani,? \\Q2015\\E",
      "shortCiteRegEx" : "Knowles and Ghahramani",
      "year" : 2015
    }, {
      "title" : "Efficient active algorithms for hierarchical clustering",
      "author" : [ "A. Krishnamurthy", "S. Balakrishnan", "M. Xu", "A. Singh" ],
      "venue" : "In Proceedings of the 29th International Conference on Machine Learning,",
      "citeRegEx" : "Krishnamurthy et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krishnamurthy et al\\.",
      "year" : 2012
    }, {
      "title" : "Semisupervised graph clustering: a kernel approach",
      "author" : [ "B. Kulis", "S. Basu", "I. Dhillon", "R. Mooney" ],
      "venue" : "In Proceedings of the 22nd International Conference on Machine Learning,",
      "citeRegEx" : "Kulis et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Kulis et al\\.",
      "year" : 2005
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Lecun", "Yann", "Bottou", "Lon", "Bengio", "Yoshua", "Haffner", "Patrick" ],
      "venue" : "In Proceedings of the IEEE,",
      "citeRegEx" : "Lecun et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Lecun et al\\.",
      "year" : 1998
    }, {
      "title" : "Density modeling and clustering using Dirichlet diffusion trees",
      "author" : [ "R.M. Neal" ],
      "venue" : "Bayesian Statistics",
      "citeRegEx" : "Neal,? \\Q2003\\E",
      "shortCiteRegEx" : "Neal",
      "year" : 2003
    }, {
      "title" : "Adaptively learning the crowd kernel",
      "author" : [ "O. Tamuz", "C. Liu", "S. Belongie", "O. Shamir", "A.T. Kalai" ],
      "venue" : "In Proceedings of the 28th International Conference on Machine Learning,",
      "citeRegEx" : "Tamuz et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Tamuz et al\\.",
      "year" : 2011
    }, {
      "title" : "Bayesian agglomerative clustering with coalescents",
      "author" : [ "Y.W. Teh", "III", "H. Daume", "D.M. Roy" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Teh et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Teh et al\\.",
      "year" : 2008
    }, {
      "title" : "Clustering with instance-level constraints",
      "author" : [ "K. Wagstaff", "C. Cardie" ],
      "venue" : "In Proceedings of the 17th International Conference on Machine Learning,",
      "citeRegEx" : "Wagstaff and Cardie,? \\Q2000\\E",
      "shortCiteRegEx" : "Wagstaff and Cardie",
      "year" : 2000
    }, {
      "title" : "Constrained k-means clustering with background knowledge",
      "author" : [ "K. Wagstaff", "C. Cardie", "S. Rogers", "S. Schroedl" ],
      "venue" : "In Proceedings of the 18th International Conference on Machine Learning,",
      "citeRegEx" : "Wagstaff et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Wagstaff et al\\.",
      "year" : 2001
    }, {
      "title" : "A MCMC approach to hierarchical mixture modeling",
      "author" : [ "C.K.I. Williams" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Williams,? \\Q2000\\E",
      "shortCiteRegEx" : "Williams",
      "year" : 2000
    }, {
      "title" : "Active learning for hierarchical pairwise data clustering",
      "author" : [ "T. Zoller", "J.M. Buhmann" ],
      "venue" : "In Proceedings of the 15th International Conference on Pattern Recognition,",
      "citeRegEx" : "Zoller and Buhmann,? \\Q2000\\E",
      "shortCiteRegEx" : "Zoller and Buhmann",
      "year" : 2000
    } ],
    "referenceMentions" : [ {
      "referenceID" : 22,
      "context" : "ent flat clustering procedures (Wagstaff et al., 2001; Bansal et al., 2004; Basu et al., 2004; Kulis et al., 2005; Biswas & Jacobs, 2014).",
      "startOffset" : 31,
      "endOffset" : 137
    }, {
      "referenceID" : 6,
      "context" : "ent flat clustering procedures (Wagstaff et al., 2001; Bansal et al., 2004; Basu et al., 2004; Kulis et al., 2005; Biswas & Jacobs, 2014).",
      "startOffset" : 31,
      "endOffset" : 137
    }, {
      "referenceID" : 16,
      "context" : "ent flat clustering procedures (Wagstaff et al., 2001; Bansal et al., 2004; Basu et al., 2004; Kulis et al., 2005; Biswas & Jacobs, 2014).",
      "startOffset" : 31,
      "endOffset" : 137
    }, {
      "referenceID" : 1,
      "context" : "A wealth of research addresses learning taxonomies from triplets alone, mostly in the field of phylogenetics: see Felsenstein (2004) for an overview, and Aho et al. (1981) for a central algorithmic result.",
      "startOffset" : 154,
      "endOffset" : 172
    }, {
      "referenceID" : 19,
      "context" : "It was pointed out in Tamuz et al. (2011) that roughly n log n ar X iv :1 60 2.",
      "startOffset" : 22,
      "endOffset" : 42
    }, {
      "referenceID" : 18,
      "context" : "For concreteness, we focus on the Dirichlet diffusion tree (Neal, 2003), which has enjoyed empirical success.",
      "startOffset" : 59,
      "endOffset" : 71
    }, {
      "referenceID" : 11,
      "context" : "A related problem that has been studied in more detail (Zoller & Buhmann, 2000; Eriksson et al., 2011; Krishnamurthy et al., 2012) is that of building a hierarchical clustering where the only information available is pairwise similarities between points, but these are initially hidden and must be individually queried.",
      "startOffset" : 55,
      "endOffset" : 130
    }, {
      "referenceID" : 15,
      "context" : "A related problem that has been studied in more detail (Zoller & Buhmann, 2000; Eriksson et al., 2011; Krishnamurthy et al., 2012) is that of building a hierarchical clustering where the only information available is pairwise similarities between points, but these are initially hidden and must be individually queried.",
      "startOffset" : 55,
      "endOffset" : 130
    }, {
      "referenceID" : 4,
      "context" : "In another variant of interactive flat clustering (Balcan & Blum, 2008; Awasthi & Zadeh, 2010; Awasthi et al., 2014), the user is allowed to specify that individual clusters be merged or split.",
      "startOffset" : 50,
      "endOffset" : 116
    }, {
      "referenceID" : 2,
      "context" : "Aldous (1995) has defined a one-parameter family of distributions over cladograms, called the beta-splitting model, that includes the uniform and the Yule model as special cases.",
      "startOffset" : 0,
      "endOffset" : 14
    }, {
      "referenceID" : 18,
      "context" : "Our primary focus is the Dirichlet diffusion tree (Neal, 2003), which is specified by a birth process that we will shortly describe.",
      "startOffset" : 50,
      "endOffset" : 62
    }, {
      "referenceID" : 17,
      "context" : "Our primary focus is the Dirichlet diffusion tree (Neal, 2003), which is specified by a birth process that we will shortly describe. However, our methodology applies quite generally. Other notable Bayesian approaches to hierarchical clustering include: Williams (2000), in which each node of the tree is annotated with a vector that is sampled from a Gaussian centered at its parent’s vector; Heller & Ghahramani (2005), that defines a distribution over flat clusterings and then specifies an agglomerative scheme for finding a good partition with respect to this distribution; Adams et al.",
      "startOffset" : 51,
      "endOffset" : 269
    }, {
      "referenceID" : 17,
      "context" : "Our primary focus is the Dirichlet diffusion tree (Neal, 2003), which is specified by a birth process that we will shortly describe. However, our methodology applies quite generally. Other notable Bayesian approaches to hierarchical clustering include: Williams (2000), in which each node of the tree is annotated with a vector that is sampled from a Gaussian centered at its parent’s vector; Heller & Ghahramani (2005), that defines a distribution over flat clusterings and then specifies an agglomerative scheme for finding a good partition with respect to this distribution; Adams et al.",
      "startOffset" : 51,
      "endOffset" : 420
    }, {
      "referenceID" : 0,
      "context" : "Other notable Bayesian approaches to hierarchical clustering include: Williams (2000), in which each node of the tree is annotated with a vector that is sampled from a Gaussian centered at its parent’s vector; Heller & Ghahramani (2005), that defines a distribution over flat clusterings and then specifies an agglomerative scheme for finding a good partition with respect to this distribution; Adams et al. (2008), in which data points are allowed to reside at internal nodes of the tree; Teh et al.",
      "startOffset" : 395,
      "endOffset" : 415
    }, {
      "referenceID" : 0,
      "context" : "Other notable Bayesian approaches to hierarchical clustering include: Williams (2000), in which each node of the tree is annotated with a vector that is sampled from a Gaussian centered at its parent’s vector; Heller & Ghahramani (2005), that defines a distribution over flat clusterings and then specifies an agglomerative scheme for finding a good partition with respect to this distribution; Adams et al. (2008), in which data points are allowed to reside at internal nodes of the tree; Teh et al. (2008); Boyles & Welling (2012), in which the distribution over trees is specified by a bottom-up coalescing process; and Knowles & Ghahramani (2015), which generalizes the Dirichlet diffusion trees to allow non-binary splits.",
      "startOffset" : 395,
      "endOffset" : 508
    }, {
      "referenceID" : 0,
      "context" : "Other notable Bayesian approaches to hierarchical clustering include: Williams (2000), in which each node of the tree is annotated with a vector that is sampled from a Gaussian centered at its parent’s vector; Heller & Ghahramani (2005), that defines a distribution over flat clusterings and then specifies an agglomerative scheme for finding a good partition with respect to this distribution; Adams et al. (2008), in which data points are allowed to reside at internal nodes of the tree; Teh et al. (2008); Boyles & Welling (2012), in which the distribution over trees is specified by a bottom-up coalescing process; and Knowles & Ghahramani (2015), which generalizes the Dirichlet diffusion trees to allow non-binary splits.",
      "startOffset" : 395,
      "endOffset" : 533
    }, {
      "referenceID" : 0,
      "context" : "Other notable Bayesian approaches to hierarchical clustering include: Williams (2000), in which each node of the tree is annotated with a vector that is sampled from a Gaussian centered at its parent’s vector; Heller & Ghahramani (2005), that defines a distribution over flat clusterings and then specifies an agglomerative scheme for finding a good partition with respect to this distribution; Adams et al. (2008), in which data points are allowed to reside at internal nodes of the tree; Teh et al. (2008); Boyles & Welling (2012), in which the distribution over trees is specified by a bottom-up coalescing process; and Knowles & Ghahramani (2015), which generalizes the Dirichlet diffusion trees to allow non-binary splits.",
      "startOffset" : 395,
      "endOffset" : 651
    }, {
      "referenceID" : 18,
      "context" : "Each of these choices has a probability associated with it, according to various properties of the tree structure and choice of acquisition function (details can be found in Neal (2003)).",
      "startOffset" : 174,
      "endOffset" : 186
    }, {
      "referenceID" : 1,
      "context" : "The simplest algorithm to solve this problem is the BUILD algorithm, introduced in Aho et al. (1981). Given a set of triplets C, BUILD will either return a tree that satisfies C, or error if no such tree exists.",
      "startOffset" : 83,
      "endOffset" : 101
    }, {
      "referenceID" : 17,
      "context" : "The first dataset, MNIST (Lecun et al., 1998), is an 10-way image classification dataset where the data are 28 x 28 images of digits.",
      "startOffset" : 25,
      "endOffset" : 45
    } ],
    "year" : 2016,
    "abstractText" : "Clustering is a powerful tool in data analysis, but it is often difficult to find a grouping that aligns with a user’s needs. To address this, several methods incorporate constraints obtained from users into clustering algorithms, but unfortunately do not apply to hierarchical clustering. We design an interactive Bayesian algorithm that incorporates user interaction into hierarchical clustering while still utilizing the geometry of the data by sampling a constrained posterior distribution over hierarchies. We also suggest several ways to intelligently query a user. The algorithm, along with the querying schemes, shows promising results on real data.",
    "creator" : "LaTeX with hyperref package"
  }
}