{
  "name" : "1511.04401.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Symbol Grounding Association in Multimodal Sequences with Missing Elements",
    "authors" : [ "Federico Raue", "Andreas Dengel", "Thomas M. Breuel", "Marcus Liwicki" ],
    "emails" : [ "federico.raue@dfki.de", "andreas.dengel@dfki.de", "tmb@cs.uni-kl.de", "liwicki@cs.uni-kl.de" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "A striking feature of the human brain is to associate abstract concepts with the sensory input signals, such as visual and audio. As a result of this multimodal association, a concept can be located and associated from a representation of one modality (visual) to another representation of a different modality (audio), and vice versa. For example, the abstract concept “ball” from the sentence “John plays with a ball” can be associated to several instances of different spherical shapes (visual input) and sound waves (audio input). Several fields, such as Neuroscience, Psychology, and Artificial Intelligence, are interested to determine all factors that are involved in binding semantic concepts to the physical world. This scenario is known as Symbol Grounding Problem (Harnad, 1990) and is still an open problem (Steels, 2008).\nWith this in mind, infants start learning the association when they are acquiring the language(s) in a multimodal scenario. Gershkoff-Stowe and Smith (2004) found the initial\nar X\niv :1\n51 1.\n04 40\n1v 4\n[ cs\n.C V\n] 1\n6 D\nset of words in infants is mainly nouns, such as dad, mom, cat, and dog. In contrast, the language development can be limited by the lack of stimulus (Andersen, Dunlea, & Kekelis, 1993; Spencer, 2000), i.e., deafness, blindness. Asano et al. found two different patterns in the brain activity of infants depending on the semantic correctness between a visual and an audio stimulus. In simpler terms, the brain activity is pattern ‘A’ if the visual and audio signals represent the same semantic concept. Otherwise, the pattern is ‘B’.\nRelated work has been proposed in different multimodal scenarios inspired by the Symbol Grounding Problem. Yu and Ballard (2004) explored a framework that learns the association between objects and their spoken names in day-to-day tasks. Nakamura et al. (2011) introduced a multimodal categorization applied to robotics. Their framework exploited the relation of concepts in different modalities (visual, audio and haptic) using a Multimodal latent Dirichlet allocation. Previous approaches has focused on feature engineering, and the segmentation and the classification tasks are considered as independent modules.\nThis paper is focusing on a model for segmentation and classification tasks in the objectword association scenario. Moreover, we are interested in multimodal sequences that represent a semantic concept sequence with the constraint that not all elements can be on both modalities. For instance, one modality sequence (text lines of digits) is represented by ‘2 4 5’, and the other modality (spoken words) is represented by ‘four six five’. Also, the association problem is different from the traditional setup where the association is fixed via a pre-defined coding scheme of the classes (e.g. 1-of-K scheme) before training. We explain the difference between common approaches for multimodal machine learning and our problem setup in Section 1.1.\nIn this work, we investigate the benefits of exploiting the alignment between elements that are common in the multimodal sequence and still agree in a similar representation via coding scheme. Note that our work is an extension of Raue et al. (2015) where both modalities represent the same semantic sequence (no missing elements). Similarly to Raue et al. (2015), the model was implemented by two Long Short-Term Memories (LSTMs) that their output vectors were aligned in the time axis using Dynamic Time Warping (DTW) (Berndt & Clifford, 1994). However, in this work, the modalities may have missing elements. Our contributions in this paper are the following\n• We propose a novel model for the cognitive multimodal association task. Moreover, our model handles multimodal sequences where the semantic concepts can be in one or both modalities. In addition, a max pooling operation in the time-axis is added to the architecture for exploiting the cross-modality of the shared semantic concepts.\n• We evaluate the presented model in two scenarios. In the first scenario, the missing semantic concepts can be in any modality. In the second scenario, the semantic concepts are missing only in one modality. For example, the visual sequence ‘1 2 3 4 5 6’ and the audio sequence ‘two four’. The semantic concepts in the audio modality are shared with the visual sequence. In contrast, some semantic concepts in the visual sequence are missed in the audio sequence. In both cases, our model performances better that the model proposed by Raue et al. (2015).\nThis paper is organized as follows. We shortly describe Long Short-Term Memory networks, mainly sequence classification in unsegmented inputs, in Section 2. The original\nend-to-end model for object-word association is explained in Section 3. The presented model for handling missing elements is presented in Section 4. A generated dataset of multimodal sequences with missing elements is described in Section 5. In Section 6, we compare the performance of the proposed extension against the original model and a single LSTM network trained on one modality in the traditional setup (pre-defined coding scheme)."
    }, {
      "heading" : "1.1 Multimodal Tasks in Machine Learning",
      "text" : "Machine Learning has been applied successfully to several scenarios where multimodal relation between input samples is exploited. In the following, we want to indicate the differences between previous multimodal tasks and our work. Multimodal Feature Fusion: The task is to combine features of different modalities for creating a better feature. In this manner, the generated feature exploits the best qualities of each modality. Recently, Deep Boltzmann Machines learns how to combine different modalities in unsupervised environment (Srivastava & Salakhutdinov, 2012; Sohn, Shang, & Lee, 2014). Image Captioning: The task is to generate a textual description given images as input. This can be seen as a machine translation from images to captions. Convolutional Neural Networks (CNN) in combination with LSTM have been already applied in cross-modality scenario, which translates images to textual captions (Vinyals, Toshev, Bengio, & Erhan, 2014; Karpathy, Joulin, & Li, 2014). New Task - Cognitive Object-Word Association: In this work, we are interested in the cross-modality association between objects (visual) and words (audio). Moreover, our\nscenario is motivated by symbol grounding. With this in mind, two definitions are introduced for explanation purposes: semantic concepts (SeC) and symbolic features (SyF). We explain these definitions through the following example. A classification problem is usually defined by an input that is associated with a class (or a semantic concept) and this class is represented by a vector (or a symbolic feature). That relation is fixed and it is chosen externally (outside of the network). In contrast, the presented task includes the relation between the class and its corresponding vector representation as a learned parameter. At the end, the model not only learns to associate objects and words but also learns the symbolic structure between the semantic concepts and the symbolic features. Figure 1 shows examples of each component and the differences between the traditional setup and our task. It can be seen that our task involves only three components, whereas the traditional setup uses only two components (red box)."
    }, {
      "heading" : "2. Long Short-Term Memory (LSTM)",
      "text" : "Long Short-Term Memory (LSTM) is a recurrent neural network, which is capable to learn long sequences without the vanishing gradient problem (Hochreiter & Schmidhuber, 1997; Hochreiter, 1998). This architecture incorporated to Recurrent Neural Networks the concept of gates and memory cells, which are defined by\nit = σ(Wxixt +Whiht−1 + bi) (1)\nft = σ(Wxfxt +Whfht−1 + bf ) (2)\not = σ(Wxoxt +Whoht−1 + bo) (3)\ngt = tanh(Wxcxt +Whcht−1 + bc) (4)\nct = ftct−1 + itgt (5)\nht = ot tanh(ct) (6)\nzt = Whzht + bz (7)\nwhere xt ∈ Rn is the input vector at time t, W∗ and b∗ are the weight matrices and bias, respectively.\nLSTM has been succesfully applied to several scenarios, such as, image captioning (Karpathy et al., 2014), texture classification (Byeon, Breuel, Raue, & Liwicki, 2015), and machine translation (Sutskever, Vinyals, & Le, 2014). In this work, we are interested in a particular method, which LSTM is able to sequence classification in unsegmented input samples. This has been applied mainly in one-dimensional tasks (i.e., speech recognition (Graves, Fernández, Gomez, & Schmidhuber, 2006) and OCR (Breuel, Ul-Hasan, Al-Azawi, & Shafait, 2013). In more detail, Graves et al. (2006) introduced Connectionist Temporal Classification (CTC). Their idea was to add an extra class (blank class (b)) to the set of classes for aligning two sequences. One sequence is LSTM output vectors, and the other sequence is obtained by a forward-backward propagation of probabilities similar to Hidden Markov Models (HMM) (Rabiner, 1989).\nyt = CTC training(fw, bw). (8)\nCTC-forward-backward step requires two recursive variables forward (fw) and backward (bw) for generating the target vector yt. Similar to HMM training, the motivation is to exploit the past context and future context at time t. An LSTM is trained by Backpropagation Trough Time (BPTT) (Werbos, 1990), and the loss function is defined by\nloss function(t) = zt − yt (9)\nThe final step of the CTC training is to predict the label sequence given an unknown input sequence. This step is called decoding, and two methods have been proposed: Best Path Decoding and Prefix Search Decoding. Please refer to the original paper for more information (Graves et al., 2006)."
    }, {
      "heading" : "3. Multimodal Symbolic Association",
      "text" : "In Section 1, we mentioned that this work is an extension of Raue et al. (2015). They have introduced a Symbolic Association scenario where their model learns to associate multimodal sequences and learns the semantic binding between Semantic Concepts (SeC ) and vectorial representation (SyF ). The initial assumption is that two different modalities (visual and audio) represent the exactly the same semantic sequence1. In other words, there is a one-to-one relation between the representation of the same semantic concepts in both modalities, i.e., visual and audio. For example, the semantic concept sequence ‘1 2 3’ is visually represented by a text line of digits and auditory represented by the waveforms of the spoken words ‘one two three’. Furthermore, the model is implemented using two parallel LSTM networks and an EM-style training rule. The model exploits recent results of LSTM in segmentation and classification for sequences in one-dimension, and the EM-training rule is applied for learning the agreement under the proposed constraints.\nThe multimodal scenario is defined by two parallel multimodal sequences (visual and audio), and each modality represents the same semantic sequence. More formally, xv,t1 ∈ Rn and xa,t2 ∈ Rm are the input vectors for visual and audio modalities, respectively. In addition, the semantic sequence is a sequence of semantic concepts s1, . . . , sk where k is the number of semantic concepts in the sequence, and each semantic concepts is selected from a vocabulary s1, . . . , sC ∈ SeC where C is the size of the vocabulary. Moreover, two bidirectional LSTM are defined for each modality LSTMv and LSTMa with output vector size zv,t1 , za,t2 ∈ RC .\nInitially, each input sequence xv,t1 and xa,t2 is fed to their respectively LSTMv and LSTMa. Consequently, each output zv,t1 and za,t2 is post-processed for finding the most likely symbolic features for each semantic concept si in the sequence. For instance, the semantic concept ‘duck’ can be represented by the index ‘4’ (via one-hot coding vector). With this in mind, two sets of weighing concept vectors (γi,βi ∈ RC where i = 1, . . . , C) are assigned to each LSTM (more details in Section 3.1). Afterwards, LSTM output of each modality in combination with the found representation is fed to CTC-forward-backward step (c.f. CTC layer in Section 2). Until this step, both LSTM networks have been forward propagated independently in each modality. For exploiting multimodal sequence, both CTC-forward-backward steps (yv,t1 and ya,t2) are aligned between each other in the\n1. A semantic sequence is a set of semantic concepts.\ntime-axis by Dynamic Time Warping (DTW) (Berndt & Clifford, 1994) (more details in Section 3.2). Therefore, the latent variables of one modality can be used to train the other modality, and vice versa. Figure 2 illustrates the training algorithm with an example."
    }, {
      "heading" : "3.1 Statistical Constraint for Semantic Binding",
      "text" : "In this symbolic association scenario, one important constraint is related to semantic concepts, which are not biding to vectorial representations before training. As mentioned, the vocabulary is a set of semantic concepts s1, . . . , sC ∈ SeC. With this in mind, a set of concept vectors γ1, . . . ,γC ∈ RC is defined for learning the mapping between the semantic concepts and the output vectors. Note that two or more concepts cannot have the same representation. This component is trained in a EM-style algorithm. For explanation purposes, it is described considering only one LSTM. However, it can be applied to two LSTM networks independently.\nE-step predicts the mapping between semantic concepts in the sequence and the symbolic representation given the LSTM output and the concept vectors. This is defined by\nẑi = 1\nT T∑ t=1 power(zt,γi), i = 1, . . . , C (10)\nwhere zt is the LSTM output vector at time t, γi is the concept vector, T is the number of timesteps of the sequence, and power(zt,γi) is the element-wise power operation between zt and γi. Then, a matrix is assembled by concatenating ẑ1, . . . , ẑC . This matrix can be used for determining the mapping between each semantic concepts and their representation\nẐ = [ẑ1, . . . , ẑC ] (11)\ne1, . . . , eC = g(Ẑ) (12)\nwhere g(Ẑ) is a row-column elimination, and e1, . . . , eC ∈ RC are column vectors of a permutation of the identity matrix. For simplicity, the column vector ei can represent j-th identity vector where i and j can or cannot be the same. In other words, the column vector e4 can represent the 1-st identity vector (e.g., e4 = [1 0 0 . . .]\nT ). The row-column elimination procedure ranks all values in the matrix. Next, the position (col, row), where the maximum value is found, and determines the row-th identity column vector ecol. For example, the maximum value is found at (2, 5), and its correspondence vector is e2 = [0 0 0 0 1 0 . . .]\nT . Finally, all values of the previous column and the previous row are set to zero. This columnrow elimination is applied C times. As are result, the vectors e1, . . . , eC are the mapping between semantic concepts (columns) and their representation (rows).\nE-step updates the concept vectors given the LSTM output and the target statistical distribution. Hence, the cost function is defined by\ncosti = (ẑi − 1\nC ei)\n2, i = 1, . . . , C (13)\nγi = γi − α ∗ ∇γicosti, i = 1, . . . , C (14)\nwhere ei is a column vector of the identity matrix that represent the semantic concept, α is the learning rate, and ∇γicosti is the derivative w.r.t γi"
    }, {
      "heading" : "3.2 Dynamic Time Warping (DTW)",
      "text" : "This module exploits that both modalities represent the same semantic concept. In addition, this component converts from one modality to another modality in the time-axis because the monotonic behavior of LSTM networks in one-dimension. Moreover, both output sequences of the CTC-forward-backward training are aligned against each other based on Dynamic Time Warping (DTW) (Berndt & Clifford, 1994). The DTW matrix is calculated with the following path\nDTW [i, j] = dist[i, j] +min  DTW [i− 1, j − 1] DTW [i− 1, j] DTW [i, j − 1]\n(15)\nwhere dist[i, j] is the Euclidean distance between output vectors at timestep i of LSTMv and at timestep j of LSTMa. Afterwards, the loss function of one modality can use the other modality as a target, and vice versa. This is defined by\nloss functionv(t1) = ya,t2 [t2 → t1]− zv,t1 (16)\nloss functiona(t2) = yv,t1 [t1 → t2]− za,t2 (17)\nwhere zv,t1 and za,t2 are LSTM output vector at time t1 and t2, yv,t1 and ya,t2 are the CTC-forward-backward steps, and (source)→ (target) is the alignment function from time source of one modality to time target of the other modality via DTW path."
    }, {
      "heading" : "4. Handling Missing Elements",
      "text" : "In this paper, we are interested in the multimodal association inspired by the symbol grounding problem for the case that the multimodal sequences have some of the semantic concepts shared between modalities but not all of them. Hence, we have update the problem definition (cf. Section 3). Two sequences of different modalities xv,t1 ∈ Rn and xa,t2 ∈ Rm where v,a represent the visual and audio modalities, t1, t2 represent the timestep of each sequence. In addition, each sequence input is associated to a semantic sequence Sv = {s1, . . . , sk} and Sa = {s1, . . . , sp} where Sv ∩ Sa = {s1, . . . , sg} 1 ≤ g ≤ min(k, p). As a result, semantic sequences Sv and Sa have some semantic concepts that are shared between modalities. In other words, there is not a one-to-one relation like in the original model. In addition, we want to exploit the shared semantic concepts via max pooling. Our contribution is to find the best representation for updating LSTM weights. This is mainly the alignment in the time-axis (Section 3.2). As a consequence, a new loss function is proposed\nloss functionv(t1) = poolingv(ya,t2 [t2 → t1],yv,t1)− zv,t1 (18)\nloss functiona(t2) = poolinga(yv,t1 [t1 → t2],ya,t2)− za,t2 (19)\nwhere\npoolingv = { max(ya,t2 [t2 → t1],yv,t1) for all shared semantic concepts yv,t1 non-shared elements in the visual modality\n(20)\npoolinga = { max(yv,t1 [t1 → t2],ya,t2) for all shared semantic concepts ya,t2 non-shared elements in the audio modality\n(21)\nwhere max is the element-wise maximum operation. The intuition behind is to give the shared semantic concept the best representation in the multimodal sequence. In contrast, the non-shared elements kept the distribution obtained from the CTC-forward-backward step."
    }, {
      "heading" : "5. Experimental Design",
      "text" : ""
    }, {
      "heading" : "5.1 Datasets",
      "text" : "We generated several multimodal datasets where the elements of the sequence are missing in one or both modalities, but the relative order between the elements is the same. For example, a visual semantic concept sequence can be represented by a text line of digits “2\n4 7”, and an audio semantic concept sequence can be represented by “two seven”. In this case, we assumed a simplified scenario of symbol grounding, where the continuity of semantic concepts is different on each modality. The visual component is a horizontal arrangement of isolated objects, and the audio component is a spoken semantic concepts of some elements of the visual component, and vice versa. We want to point out that the visual component is similar to a panorama view. The procedure for generating the multimodal datasets is explained.\nGenerating Semantic Sequences: Two scenarios are considered for generating the semantic sequences for each modality: missing elements in both modalities and in one modality. For the first scenario, we generated a sequence of ten semantic concepts. Later, we randomly remove between zero and five elements each sequence. As a result, two different sequences for two different modalities were obtained with few common elements between them. For the second scenario, we follow a similar procedure. In that case, one sequence always has a sequence with ten semantic concepts and the other sequence has missing elements. In addition, our vocabulary has 30 semantic concepts in Spanish: oso, bote, botella, bol, caja, carro, gato, queso, cigarrillo, gaseosa, bebida, pato, cara, comida, hamburguesa, higiene, liquido, loción, cebolla, pimentón, pera, redondo, sanduche, cuchara, té, teléfono, tomate, florero, veh́ıculo, madera.\nVisual Component: We used a subset of 30 objects from COIL-100 (Nene, Nayar, & Murase, 1996) that is a standard dataset of 100 isolated objects. Each isolated object has 72 views at different angles. After selecting the object for the sequence, each object was converted to gray scale and rescaled to 32 x 32 pixels. Later, one random isolated object for each semantic sequences was selected and all objects were stacked horizontally. In addition, a random noise was added to the final image. While the odd angles were used for training, the even angles were used for testing.\nAudio Component: We recorded each semantic concept two times from twelve different subjects who are Spanish native speakers (five female and seven male speakers). Afterwards, the isolated semantic concepts were concatenated for creating audio sequences. The voices were divided into nine voices for training (four females and five males) and three voices for testing (one female and two males).\nTraining and Testing Multimodal Datasets: We generated three different datasets for evaluating our model. The first dataset has missing elements in both modalities. While the second dataset has ten semantic concepts in the visual component, the audio modality has a fixed number of missing elements. In other words, all audio sequences only have one missing element if we compare against the visual component. The third dataset has a similar idea with respect to the second dataset. In this case, we are testing an audio sequence with ten semantic concepts, but the visual component has a fixed number of missing elements. All of the three multimodal datasets have 50,000 sequences for training and 30,000 sequences for testing. One example of the dataset is shown in Figure 3."
    }, {
      "heading" : "5.2 Input Features and LSTM setup",
      "text" : "We did not apply any pre-processing step for the visual component. In contrast, the audio component was converted to Mel-Frequency Cepstral Coefficient (MFCC) using HTK\ntoolkit2. The audio representation is a vector of 123 components: a Fourier filter-bank with 40 coefficients (plus energy), including the first and second derivatives. Afterwards, all audio and visual components were normalized to have zero mean and standard deviation one.\nIn addition, the proposed extension was compared against the original model in (Raue et al., 2015). Also, we compared the extension against LSTM with CTC layer and a predefined coding scheme. The parameters of the visual LSTM were: 40 memory cells, learning rate 1e-4, and momentum 0.9. On the other hand, the audio LSTM had 100 memory cells, and the learning rate and momentum are the same as in the visual LSTM. In addition, the learning rate in the statistical constraint was set to 0.001."
    }, {
      "heading" : "6. Results and Discussion",
      "text" : "As mentioned previously, the assumption of the original model was to represent the same semantic concept sequence in both modalities. In other words, a one-to-one relation exists between modalities. In contrast, in this work, our assumption is more challenging because the semantic concept in one modality can be or cannot be present in the other modality. We evaluate the multimodal association task using Association Accuracy (AAcc), which is defined by the following equation\nAAcc = ∑N i=1 LCS(outputa,i, outputv,i, gta,i, gtv,i)∑N\ni=1 LCS(gta,i, gtv,i) , (22)\nwhere LCS is the length of the longest common sequence, outputa,i and outputv,i are the output classification of each modality, gta,i and gtv,i are the ground-truth labels of each modality, and N is the number of elements in the dataset. In other words, we are evaluating the association between the common elements. Our model not only learns the association\n2. http://htk.eng.cam.ac.uk\nbut also learns to classify each modality. With this in mind, we also reported the Label Error Rate (LER) as a performance metric, which is defined by\nLER = 1\nN N∑ i=1 ED(outputi, gti) |gti| (23)\nwhere outputi is the output classification, gti is the ground-truth, and ED(outputi, gti) is the edit distance between the output classification and the ground-truth. In addition, we selected randomly 10,000 sequences from the training set and 3,000 sequences from the testing. We did this selection five times and reported the average results.\nTable 1 summarizes the performance of LSTM trained with a pre-defined coding scheme, the original model, and the presented extension. Those results are divided into two parts as follows. First, the proposed extension handles missing elements in multimodal sequences better than the original model. It can be inferred that the max operation keeps the strongest of the common semantic concepts between modalities. Note that the representations are used for updating the weights in the backward step. Second, the proposed extension reaches similar results to the standard LSTM. In this case, LSTM was trained in each modality independently. As a reminder, we mentioned two setups for classification tasks: the traditional setup and the setup used in this work. We want to point out that the visual LSTM boost the performance of audio sequences compared to LSTM. As a result, our model reaches lower Label Error Rate in the audio sequences than the standard LSTM trained only in audio sequence.\nAnother outcome in this work is the conformity of the symbolic structure in both modalities, even with missing elements. Figure 4 shows examples of the coding scheme agreement. It can be observed that both LSTM networks learn to segment and classify the object-word relation in unsegmented multimodal sequences. Moreover, the common concepts in both modalities are represented by a similar symbolic feature and located at the right position in the sequence. For example, the semantic concept “redondo” (first element at the visual component and second element at the audio component) is represented by the index “27” in\nboth modalities3. Note that not only the common elements, but also the missing elements are classified correctly.\nIn addition to the considerations we made so far, we were also interested in the robustness of the presented model against the number of missing elements. This, we generated\n3. There are some cases that represent one semantic concept with two different coding vectors for each network. However, both networks retrieve correctly the same concept regardless of the different coding scheme.\nseveral datasets where one modality has ten semantic concepts, and the other has only fixed number of missing elements from the ten semantic concepts. Figure 5 shows the Association Accuracy of the original model and the presented model for handling missing elements. First, the original model (red dashed line) decreases its performance when the number of missing elements is increased in both modalities. These results were expected because the original model relies on one-to-one relation between modalities. Second, we recognize that the presented model (blue solid line) shows a better performance compared to the original model (red dotted line) in both modalities. Thus, we may conclude that the presented model does not reduce its performance even if 50% of elements are missing in one of the modalities. In more detail, Figure 6 shows that the cross-modality learning reduces the Label Error Rate of the network applied to the audio modality."
    }, {
      "heading" : "7. Conclusions",
      "text" : "In summary, we have presented a solution inspired by the symbol grounding problem for the object-word association problem. Additionally, the model relies on multimodal sequences (visual and audio) where the semantic elements can be presented in one or both modalities. Further work is planned for more realistic scenarios where the visual component is not clearly segmentable. Moreover, we are interested to extend the word-association problem between a two-dimensional image and speech. With this in mind, we will incorporate visual attention mechanism in synchronization with speech. Finally, the human language development relies on how abstract concepts are associated with the real world through the sensory input, and the scenario of the symbol grounding problem can be seen as simple. However, many questions remain still open (Needham, Santos, Magee, Devin, Hogg, & Cohn, 2005; Steels, 2008)."
    } ],
    "references" : [ {
      "title" : "The impact of input: language acquisition in the visually impaired",
      "author" : [ "E.S. Andersen", "A. Dunlea", "L. Kekelis" ],
      "venue" : "First Language,",
      "citeRegEx" : "Andersen et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Andersen et al\\.",
      "year" : 1993
    }, {
      "title" : "Using Dynamic Time Warping to Find Patterns in Time",
      "author" : [ "D.J. Berndt", "J. Clifford" ],
      "venue" : null,
      "citeRegEx" : "Berndt and Clifford,? \\Q1994\\E",
      "shortCiteRegEx" : "Berndt and Clifford",
      "year" : 1994
    }, {
      "title" : "High-performance ocr for printed english and fraktur using lstm networks",
      "author" : [ "T. Breuel", "A. Ul-Hasan", "M. Al-Azawi", "F. Shafait" ],
      "venue" : "In Document Analysis and Recognition (ICDAR),",
      "citeRegEx" : "Breuel et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Breuel et al\\.",
      "year" : 2013
    }, {
      "title" : "Scene labeling with lstm recurrent neural networks",
      "author" : [ "W. Byeon", "T.M. Breuel", "F. Raue", "M. Liwicki" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Byeon et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Byeon et al\\.",
      "year" : 2015
    }, {
      "title" : "Shape and the first hundred nouns",
      "author" : [ "L. Gershkoff-Stowe", "L.B. Smith" ],
      "venue" : "Child development,",
      "citeRegEx" : "Gershkoff.Stowe and Smith,? \\Q2004\\E",
      "shortCiteRegEx" : "Gershkoff.Stowe and Smith",
      "year" : 2004
    }, {
      "title" : "Connectionist temporal classification",
      "author" : [ "A. Graves", "S. Fernández", "F. Gomez", "J. Schmidhuber" ],
      "venue" : "In Proceedings of the 23rd international conference on Machine learning - ICML",
      "citeRegEx" : "Graves et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2006
    }, {
      "title" : "The symbol grounding problem",
      "author" : [ "S. Harnad" ],
      "venue" : "Physica D: Nonlinear Phenomena,",
      "citeRegEx" : "Harnad,? \\Q1990\\E",
      "shortCiteRegEx" : "Harnad",
      "year" : 1990
    }, {
      "title" : "The Vanishing Gradient Problem During Learning Recurrent Neural Nets and Problem Solutions",
      "author" : [ "S. Hochreiter" ],
      "venue" : "International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems,",
      "citeRegEx" : "Hochreiter,? \\Q1998\\E",
      "shortCiteRegEx" : "Hochreiter",
      "year" : 1998
    }, {
      "title" : "Long Short-Term Memory",
      "author" : [ "S. Hochreiter", "J. Schmidhuber" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Hochreiter and Schmidhuber,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber",
      "year" : 1997
    }, {
      "title" : "Deep fragment embeddings for bidirectional image sentence mapping",
      "author" : [ "A. Karpathy", "A. Joulin", "F.F.F. Li" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Karpathy et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Karpathy et al\\.",
      "year" : 2014
    }, {
      "title" : "Grounding of word meanings in latent dirichlet allocation-based multimodal concepts",
      "author" : [ "T. Nakamura", "T. Araki", "T. Nagai", "N. Iwahashi" ],
      "venue" : "Advanced Robotics,",
      "citeRegEx" : "Nakamura et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Nakamura et al\\.",
      "year" : 2011
    }, {
      "title" : "Protocols from perceptual observations",
      "author" : [ "C.J. Needham", "P.E. Santos", "D.R. Magee", "V. Devin", "D.C. Hogg", "A.G. Cohn" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Needham et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Needham et al\\.",
      "year" : 2005
    }, {
      "title" : "Columbia object image library (coil-100)",
      "author" : [ "S. Nene", "S. Nayar", "H. Murase" ],
      "venue" : "Tech. rep.",
      "citeRegEx" : "Nene et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Nene et al\\.",
      "year" : 1996
    }, {
      "title" : "A tutorial on hidden markov models and selected applications in speech recognition",
      "author" : [ "L.R. Rabiner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "Rabiner,? \\Q1989\\E",
      "shortCiteRegEx" : "Rabiner",
      "year" : 1989
    }, {
      "title" : "Symbol Grounding in Multimodal Sequences using Recurrent Neural Network",
      "author" : [ "F. Raue", "W. Byeon", "T. Breuel", "M. Liwicki" ],
      "venue" : "In Workshop Cognitive Computation: Integrating Neural and Symbolic Approaches at NIPS",
      "citeRegEx" : "Raue et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Raue et al\\.",
      "year" : 2015
    }, {
      "title" : "Improved multimodal deep learning with variation of information",
      "author" : [ "K. Sohn", "W. Shang", "H. Lee" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Sohn et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sohn et al\\.",
      "year" : 2014
    }, {
      "title" : "Looking without listening: is audition a prerequisite for normal development of visual attention during infancy",
      "author" : [ "P.E. Spencer" ],
      "venue" : "Journal of deaf studies and deaf education,",
      "citeRegEx" : "Spencer,? \\Q2000\\E",
      "shortCiteRegEx" : "Spencer",
      "year" : 2000
    }, {
      "title" : "Multimodal learning with deep boltzmann machines",
      "author" : [ "N. Srivastava", "R.R. Salakhutdinov" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Srivastava and Salakhutdinov,? \\Q2012\\E",
      "shortCiteRegEx" : "Srivastava and Salakhutdinov",
      "year" : 2012
    }, {
      "title" : "The symbol grounding problem has been solved, so whats next ?. Symbols, Embodiment and Meaning",
      "author" : [ "L. Steels" ],
      "venue" : null,
      "citeRegEx" : "Steels,? \\Q2008\\E",
      "shortCiteRegEx" : "Steels",
      "year" : 2008
    }, {
      "title" : "Sequence to sequence learning with neural networks. In Advances in neural information processing",
      "author" : [ "I. Sutskever", "O. Vinyals", "Q.V. Le" ],
      "venue" : null,
      "citeRegEx" : "Sutskever et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Show and tell: A neural image caption generator",
      "author" : [ "O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan" ],
      "venue" : "arXiv preprint arXiv:1411.4555",
      "citeRegEx" : "Vinyals et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2014
    }, {
      "title" : "Backpropagation through time: what it does and how to do it",
      "author" : [ "P.J. Werbos" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "Werbos,? \\Q1990\\E",
      "shortCiteRegEx" : "Werbos",
      "year" : 1990
    }, {
      "title" : "A multimodal learning interface for grounding spoken language in sensory perceptions",
      "author" : [ "C. Yu", "D.H. Ballard" ],
      "venue" : "ACM Transactions on Applied Perception (TAP),",
      "citeRegEx" : "Yu and Ballard,? \\Q2004\\E",
      "shortCiteRegEx" : "Yu and Ballard",
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "This scenario is known as Symbol Grounding Problem (Harnad, 1990) and is still an open problem (Steels, 2008).",
      "startOffset" : 51,
      "endOffset" : 65
    }, {
      "referenceID" : 18,
      "context" : "This scenario is known as Symbol Grounding Problem (Harnad, 1990) and is still an open problem (Steels, 2008).",
      "startOffset" : 95,
      "endOffset" : 109
    }, {
      "referenceID" : 4,
      "context" : "Gershkoff-Stowe and Smith (2004) found the initial",
      "startOffset" : 0,
      "endOffset" : 33
    }, {
      "referenceID" : 16,
      "context" : "In contrast, the language development can be limited by the lack of stimulus (Andersen, Dunlea, & Kekelis, 1993; Spencer, 2000), i.",
      "startOffset" : 77,
      "endOffset" : 127
    }, {
      "referenceID" : 14,
      "context" : "In contrast, the language development can be limited by the lack of stimulus (Andersen, Dunlea, & Kekelis, 1993; Spencer, 2000), i.e., deafness, blindness. Asano et al. found two different patterns in the brain activity of infants depending on the semantic correctness between a visual and an audio stimulus. In simpler terms, the brain activity is pattern ‘A’ if the visual and audio signals represent the same semantic concept. Otherwise, the pattern is ‘B’. Related work has been proposed in different multimodal scenarios inspired by the Symbol Grounding Problem. Yu and Ballard (2004) explored a framework that learns the association between objects and their spoken names in day-to-day tasks.",
      "startOffset" : 113,
      "endOffset" : 590
    }, {
      "referenceID" : 10,
      "context" : "Nakamura et al. (2011) introduced a multimodal categorization applied to robotics.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 10,
      "context" : "Nakamura et al. (2011) introduced a multimodal categorization applied to robotics. Their framework exploited the relation of concepts in different modalities (visual, audio and haptic) using a Multimodal latent Dirichlet allocation. Previous approaches has focused on feature engineering, and the segmentation and the classification tasks are considered as independent modules. This paper is focusing on a model for segmentation and classification tasks in the objectword association scenario. Moreover, we are interested in multimodal sequences that represent a semantic concept sequence with the constraint that not all elements can be on both modalities. For instance, one modality sequence (text lines of digits) is represented by ‘2 4 5’, and the other modality (spoken words) is represented by ‘four six five’. Also, the association problem is different from the traditional setup where the association is fixed via a pre-defined coding scheme of the classes (e.g. 1-of-K scheme) before training. We explain the difference between common approaches for multimodal machine learning and our problem setup in Section 1.1. In this work, we investigate the benefits of exploiting the alignment between elements that are common in the multimodal sequence and still agree in a similar representation via coding scheme. Note that our work is an extension of Raue et al. (2015) where both modalities represent the same semantic sequence (no missing elements).",
      "startOffset" : 0,
      "endOffset" : 1375
    }, {
      "referenceID" : 10,
      "context" : "Nakamura et al. (2011) introduced a multimodal categorization applied to robotics. Their framework exploited the relation of concepts in different modalities (visual, audio and haptic) using a Multimodal latent Dirichlet allocation. Previous approaches has focused on feature engineering, and the segmentation and the classification tasks are considered as independent modules. This paper is focusing on a model for segmentation and classification tasks in the objectword association scenario. Moreover, we are interested in multimodal sequences that represent a semantic concept sequence with the constraint that not all elements can be on both modalities. For instance, one modality sequence (text lines of digits) is represented by ‘2 4 5’, and the other modality (spoken words) is represented by ‘four six five’. Also, the association problem is different from the traditional setup where the association is fixed via a pre-defined coding scheme of the classes (e.g. 1-of-K scheme) before training. We explain the difference between common approaches for multimodal machine learning and our problem setup in Section 1.1. In this work, we investigate the benefits of exploiting the alignment between elements that are common in the multimodal sequence and still agree in a similar representation via coding scheme. Note that our work is an extension of Raue et al. (2015) where both modalities represent the same semantic sequence (no missing elements). Similarly to Raue et al. (2015), the model was implemented by two Long Short-Term Memories (LSTMs) that their output vectors were aligned in the time axis using Dynamic Time Warping (DTW) (Berndt & Clifford, 1994).",
      "startOffset" : 0,
      "endOffset" : 1489
    }, {
      "referenceID" : 14,
      "context" : "In both cases, our model performances better that the model proposed by Raue et al. (2015).",
      "startOffset" : 72,
      "endOffset" : 91
    }, {
      "referenceID" : 7,
      "context" : "Long Short-Term Memory (LSTM) is a recurrent neural network, which is capable to learn long sequences without the vanishing gradient problem (Hochreiter & Schmidhuber, 1997; Hochreiter, 1998).",
      "startOffset" : 141,
      "endOffset" : 191
    }, {
      "referenceID" : 9,
      "context" : "LSTM has been succesfully applied to several scenarios, such as, image captioning (Karpathy et al., 2014), texture classification (Byeon, Breuel, Raue, & Liwicki, 2015), and machine translation (Sutskever, Vinyals, & Le, 2014).",
      "startOffset" : 82,
      "endOffset" : 105
    }, {
      "referenceID" : 13,
      "context" : "One sequence is LSTM output vectors, and the other sequence is obtained by a forward-backward propagation of probabilities similar to Hidden Markov Models (HMM) (Rabiner, 1989).",
      "startOffset" : 161,
      "endOffset" : 176
    }, {
      "referenceID" : 5,
      "context" : "In more detail, Graves et al. (2006) introduced Connectionist Temporal Classification (CTC).",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 21,
      "context" : "An LSTM is trained by Backpropagation Trough Time (BPTT) (Werbos, 1990), and the loss function is defined by",
      "startOffset" : 57,
      "endOffset" : 71
    }, {
      "referenceID" : 5,
      "context" : "Please refer to the original paper for more information (Graves et al., 2006).",
      "startOffset" : 56,
      "endOffset" : 77
    }, {
      "referenceID" : 14,
      "context" : "In Section 1, we mentioned that this work is an extension of Raue et al. (2015). They have introduced a Symbolic Association scenario where their model learns to associate multimodal sequences and learns the semantic binding between Semantic Concepts (SeC ) and vectorial representation (SyF ).",
      "startOffset" : 61,
      "endOffset" : 80
    }, {
      "referenceID" : 14,
      "context" : "Figure 2: General overview of the original model proposed by Raue et al. (2015) and the contributions of this paper.",
      "startOffset" : 61,
      "endOffset" : 80
    }, {
      "referenceID" : 14,
      "context" : "In addition, the proposed extension was compared against the original model in (Raue et al., 2015).",
      "startOffset" : 79,
      "endOffset" : 98
    }, {
      "referenceID" : 14,
      "context" : "00 Original Model (Raue et al. (2015)) 12.",
      "startOffset" : 19,
      "endOffset" : 38
    }, {
      "referenceID" : 18,
      "context" : "However, many questions remain still open (Needham, Santos, Magee, Devin, Hogg, & Cohn, 2005; Steels, 2008).",
      "startOffset" : 42,
      "endOffset" : 107
    } ],
    "year" : 2016,
    "abstractText" : "In this paper, we extend a symbolic association framework to being able to handle missing elements in multimodal sequences. The general scope of the work is the symbolic associations of object-word mappings as it happens in language development on infants. In other words, two different representations of the same abstract concepts can be associated in both directions. This scenario has been long interested in Artificial Intelligence, Psychology, and Neuroscience. In this work, we extend a recent approach for multimodal sequences (visual and audio) to also cope with missing elements in one or both modalities. Our approach uses two parallel Long Short-Term Memories (LSTMs) with a learning rule based on EM-algorithm. It aligns both LSTM outputs via Dynamic Time Warping (DTW). We propose to include an extra step for the combination with max operation for exploiting the common elements between both sequences. The intuition behind is that the combination acts as a condition selector for choosing the best representation from both LSTMs. We evaluated the proposed extension in the following scenarios: missing elements in one modality (visual or audio) and missing elements in both modalities (visual and audio). The performance of our extension reaches better results than the original model and similar results to individual LSTM trained in each modality.",
    "creator" : "TeX"
  }
}