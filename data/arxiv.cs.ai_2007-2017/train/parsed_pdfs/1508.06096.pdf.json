{
  "name" : "1508.06096.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Unsatisfiable Cores and Lower Bounding for Constraint Programming",
    "authors" : [ "Nicholas Downing", "Peter J. Stuckey" ],
    "emails" : [ "ndowning@csse.unimelb.edu.au", "tfeydy@csse.unimelb.edu.au", "pjs@csse.unimelb.edu.au" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Earlier work on unsatisfiable cores for Maximum Satisfiability (MAXSAT) has shown that it is advantageous to consider soft constraints to be hard constraints initially, solve the problem using a modern SAT solver, and use the resulting evidence of infeasibility to see which (temporarily hard) constraints are conflicting with each other, and soften them again only as necessary [12].\nIn this paper we extend the unsatisfiable cores algorithm from MAXSAT to Constraint Programming (CP). CP handles soft-constraint problems as minimization problems where the objective is a count of violations, the counts being derived from either reified primitive constraints (whose enforcement is controlled by an auxiliary variable) or soft global constraints (for examples of soft global constraints and their propagation algorithms see Van Hoeve [28]).\nOne reason to expect that unsatisfiable cores will help in CP is that propagation solving relies on eliminating impossible (partial) solutions, but unfortunately when most constraints are soft then most solutions cannot be ruled out definitively and so propagation has little effect. Making as many constraints as\n? NICTA is funded by the Australian Government as represented by the Department of Broadband, Communications and the Digital Economy and the Australian Research Council.\nar X\niv :1\n50 8.\n06 09\n6v 1\n[ cs\n.L O\n] 2\n5 A\nug 2\npossible hard, should improve the propagation behaviour. Conversely, for the approach to be successful the solver needs to be able to prove infeasibility, if this is easy to do repeatedly then unsatisfiable cores will be highly effective, but if it requires a lot of search then it should be put off for as long as possible!\nWe work in the context of a Lazy Clause Generation (LCG) solver, because the LCG solver can easily ‘explain’ why failures occurred, which is useful because it tells us which (temporarily) hard constraints should be made soft again. LCG is a hybrid approach to CP that uses a traditional ‘propagation and search’ constraint solver as the outer layer which guides the solution process, plus an inner layer which lazily decomposes CP to Boolean satisfiability (SAT) and applies learning SAT solver technology to reduce search [24,25].\nThe contributions of this paper are:\n– We translate the basic unsatisfiable core approach of SAT to CP solving. – We extend the basic unsatisfiable core approach to a nested version which\nmore aggressively makes soft constraints hard. – We discuss how we can use the unsatisfiable cores generated to improve the\nestimation of the objective function in CP search. – We give experiments showing that for some CP optimization problems the\nunsatisfiable-core approach is significantly better than branch and bound."
    }, {
      "heading" : "2 Lazy Clause Generation",
      "text" : "We give a brief description of propagation-based solving and LCG, for more details see [25]. We consider problems consisting of constraints C over integer variables x1, . . ., xn, each with a given finite domain Dorig(xi). A feasible solution is a valuation θ to the variables, which satisfies all constraints C, and lies in the domain Dorig = Dorig(x1)× . . .×Dorig(xn), i.e. θ(xi) ∈ Dorig(xi).\nA propagation solver maintains a domain restriction D(xi) ⊆ Dorig(xi) for each variable and considers only solutions that lie within D = D(x1) × . . . × D(xn). Solving interleaves propagation, which repeatedly applies propagators to remove unsupported values, and search which splits the domain of some variable and considers the resulting sub-problems. This continues until all variables are fixed (success) or failure is detected (backtrack and try another subproblem). A singleton domain D where all variables are fixed corresponds to a valuation θD where θD(xi) = vi when D(xi) = {vi}, i ∈ 1..n.\nLazy clause generation is implemented by introducing Boolean variables for each potential value of a CP variable, named [xi = j], and for each bound, [xi ≥ j]. Negating them gives [xi 6= j] and [xi ≤ j − 1]. Fixing such a literal modifies D(xi) to make the corresponding fact true, and vice versa. Hence the literals give an alternate Boolean representation of the domain, which supports reasoning. Lazy clause generation makes use of clauses to record nogoods. A clause is a disjunction of literals, which we will often treat as a set of literals.\nThe high-level solving algorithm LCG, including propagation, search, and nogood generation, is shown as Algorithm 1. It is a standard CP branch-andbound search, except that propagation (Propagate) must return a nogood N as shown, explaining any failures that are detected by propagation. Propagation\nAlgorithm 1 CP branch-and-bound with clause learning and backjumping\n1: function LCG(C,D, c,y) % initial constraints, domains and objective 2: S ← [] % empty stack of domains per decision level 3: θ ← none % best solution found so far, initially none 4: loop 5: % the call below updates the implication graph and N , not shown explicitly 6: D← Propagate(C,D) 7: if D = ∅ then 8: % failure, nogood is N → false where decision level(l) > 0 for all l ∈ N 9: if N = ∅ then % conflict occurred at level 0 10: return θ % no further improvement possible 11: else 12: (L,M)← Analyze(N) % make 1UIP nogood L→ l where M = {l} 13: pop S until reaching highest decision level of literals in L or 0 14: pop D from S % backjump 15: C← Learn(C, L,M) % add redundant constraint to problem 16: end if 17: else if D is a singleton domain then 18: % found solution, record it and restart with tighter objective constraint 19: θ ← θD 20: pop S until reaching decision level 0 21: pop D from S 22: C← C ∪ {cTy < cT θ(y)} 23: else 24: % reached a fixed point, execute the user’s programmed search strategy 25: push D onto S 26: D← Decide(D) 27: end if 28: end loop 29: end function\n30: function Analyze(N) 31: conflict level← maxn∈N decision level(n) 32: while there are multiple n ∈ N with decision level(n) = conflict level do 33: let L→ l be the most recent unprocessed propagation at conflict level 34: if no such propagations remain unprocessed then break end if 35: if l ∈ N then N ← (N − {l}) ∪ L end if 36: end while 37: return ({n : n ∈ N, decision level(n) < conflict level}, {¬n : n ∈ N, decision level(n) = conflict level}) 38: end function\n39: function Learn(C, L, {l}) 40: return C ∪ {L→ l} 41: end function\nmust also record an implication graph showing the reasons for each propagation step, and for each literal which is fixed, its decision level as the value of |S| at the time of fixing. Conflict analysis derives new redundant constraints to avoid repeated search, and, as a side effect, modifies the backtracking procedure to backjump or restart solving at an appropriate point close to the failure [24].\nThe Analyze procedure reduces the information from the failure nogood N → false, and the implication graph, into a 1UIP nogood, which can be learnt as a new redundant constraint. It considers propagations at the conflict level, which is the highest level of any literal in N . Working in reverse chronological order, for each propagation L → l, where the propagated literal l occurs in N , this literal is replaced by its reason giving (N −{l})∪L. The process stops when there is at most one literal in N whose decision level is the conflict level, leaving a clause which propagates to fix that literal to its opposite value."
    }, {
      "heading" : "3 Soft Constraint Optimization",
      "text" : "Soft constraints are constraints which should be respected if possible. When not all soft constraints can hold simultaneously we attach a cost to each violation. In the resulting optimization problem the overall cost is to be minimized. Soft constraints may be intensional or extensional. An intensional constraint is an equation or predicate capturing the desired relationship between variables, whereas an extensional constraint is written as a table with columns for the variables of interest, explicitly listing the allowed or disallowed tuples.\nSpecialized solvers have been highly successful for soft-constraint problems in extensional form. All of these solvers attempt to discover conflicts between soft constraints, or unsatisfiable soft constraints, by using what are essentially lookahead approaches, followed by appropriate reformulation that exposes the increased lower bound on solution cost due to the conflict or violation.\nFor WCSP, in which each extensional table contains a weight column giving the cost to be paid if the row holds, the best solver seems to be toolbar/toulbar2 [13,17]. It is based on a branch-and-bound search with consistency notions, where (loosely speaking) a variable is consistent if the costs of the minimum cost value(s) have been subtracted from the tables involving the variable and moved into the lower bound, thus fathoming unpromising branches.\nFor MAXSAT, good solvers in a recent evaluation [3] included akmaxsat [16] and MaxSatz [20] variants. Essentially they use lookahead, with unit propagation and failed literal detection, to improve the lower bound [18,19]. In restricted cases they use MAXSAT resolution, in which conflicting clauses are replaced by a unified clause plus compensation clauses [4]. WCSP solvers are also highly effective on MAXSAT, since MAXSAT is a special case of WCSP.\nRecently there has also been considerable interest in decomposing MAXSAT to SAT, usually with an unsatisfiable-core approach [2,12,22,23]. Because they use learning instead of lookahead (and other improvements such as activity-based search [24]), they have a considerable advantage over the previously-described approaches. On the other hand they do not employ reformulation, and not all problems are suitable for unsatisfiable-core searches.\nPseudo-Boolean Optimization (PBO) is also promising for MAXSAT, which is a special case of PBO. In particular the Weighted Boolean Optimization (WBO) framework [21] is an application of PBO to soft-constraint problems, using some of the specialized techniques discussed above. Another option is decomposition to SAT via the PBO solver MiniSAT+ [9], which could be useful if unsatisfiability-based methods aren’t applicable to a particular problem.\nIn this research we extend certain of the above techniques to intensional soft constraints. Modelling with intensional constraints has many advantages, (i) it is much easier since constraints are expressed in a natural way, (ii) it handles more constraints, since decomposition to extensional form is not always practical, and (iii) it can be more efficient, since propagation is a reasoning task as opposed to an expensive table traversal. It also has some disadvantages, (i) propagators must be implemented for each type of intensional constraint, and (ii) due to the many ways that constraints can interact, reformulation is difficult.\nWe consider solving combinatorial constrained optimization problems with pseudo-Boolean objective (COPPBO). A COPPBO (x,y,D,C, c) consists of a vector x of general variables xi, i ∈ 1..m, a vector of y of Boolean variables yi, i ∈ 1..n which appear in the objective, an initial finite domain D = D(x1)× . . .×D(xm)×D(y1)× . . .×D(ym), a set C of constraints Ci, i ∈ 1..k and an objective z = cTy to be minimized, where c consists of positive constant weighting factors12. COPPBO problems encompass MAXSAT, partial MAXSAT, weighted partial MAXSAT, WBO and PBO.\nAn important class of COPPBO problems are soft constraint optimization problems (x,D,H,S, c) given by a set of hard constraints H and a vector of soft constraints S, with corresponding weight vector c such that ci is the cost of violating soft constraint Si, i ∈ 1..n. The aim is to find a solution θ ∈ D to the variables [x y], which minimizes z = cTy, subject to H ∪ {¬yi → Si : i ∈ 1..n}, where y consists of introduced relaxation variables for the constraints in S. Note that a CP system supporting constraint Si can be straightforwardly extended to support the softened form ¬yi → Si through half-reification [10]."
    }, {
      "heading" : "4 Basic Unsatisfiable Cores Algorithm",
      "text" : "An unsatisfiable core is a clause which contains only literals in y. This clause forces an objective variable to be true, and must add some positive value to the objective. Note that clauses containing a literal ¬yi are not unsatisfiable cores. The unsatisfiable cores approach to optimization originally arose for solving a MAXSAT problem, which is, given a set of soft clauses S, find a solution which satisfies the maximum number of soft clauses.\nThe basic unsatisfiable core solver consists of the procedures in Algorithm 2, which are called by the high level solver of Algorithm 1, and essentially modify the decision procedure based on information from conflict analysis. Each attempt fixes all unfixed variables in y, that have never appeared in an unsatisfiable core,\n1 In calculating cTy we take false = 0 and true = 1. 2 We make the coefficients ci positive by negating Boolean literals if necessary.\nAlgorithm 2 Basic unsatisfiable core algorithm (relative to Algorithm 1)\nRequire: F = {y1, . . . , yn} initially 1: function Learn(C, L,M) where |M | > 1 (otherwise fall back to Algorithm 1) 2: F ← F −M % remove candidates that have appeared in an unsatisfiable core 3: return C % unchanged constraint set, to avoid risk of learning duplicates 4: end function\nRequire: F = set of yi literals which have never appeared in an unsatisfiable core 5: function Decide(D(x1)× . . .×D(xm)×D(y1)× . . .×D(yn)) 6: if decision level = 0 and exists yi ∈ F with D(yi) = {false, true} then 7: for all such yi restrict the corresponding domain D(yi) to {false} 8: else 9: restrict some other domain according to user’s programmed search 10: end if 11: return D(x1)× . . .×D(xm)×D(y1)× . . .×D(yn) 12: end function\nto false, and solves the resulting problem. This either finds a solution (which should be of low cost), or it detects that the problem is unsatisfiable. In a learning solver such as a SAT or LCG solver, by fixing the y-variables as a (possibly) multiple decision in an artificial first decision level, failure occurring at this level generates, as a side effect, a new unsatisfiable core. This continues until solutions are found, or the original problem is proved unsatisfiable.\nWe have to modify the standard LCG solver to allow multiple simultaneous decisions when branching in procedure Decide. The Analyze procedure returns generalized 1UIP nogoods L → M where L is a set of literals treated as a conjunction, and M a set treated as a disjunction, e.g. a∧b→ c∨d. The code for Analyze in Algorithm 1 already handles this case, as line 34 will cause the loop to exit when only decisions remain. This line is unnecessary for search without multiple simultaneous decisions. Note also that, for the basic unsatisfiable core algorithm, we will only ever generate generalized 1UIP nogoods where L = ∅, but we will use the more general form in the next section.\nAlgorithm 2 keeps track of F , the set of optimization variables that have never appeared in an unsatisfiable core. The new post-analysis handler Learn(C, L,M) handles the case where |M | > 1, by removing variables in M from F . Unlike the 1UIP case with |M | = 1, it does not learn the new nogood L→M since this may already be in the clause database, because two or more literals in M may have been set false simultaneously (propagating the database precludes single wrong decisions that violate a clause, but this does not extend to multiple decisions).\nExample 1. The MAXSAT instance (all constraints soft) in the left column is entered as on the right, with objective z = y1 + y2 + y3 + y4 to be minimized.\nC1 ≡ ¬b C2 ≡ a ∨ b y1 ∨ ¬b y2 ∨ a ∨ b C3 ≡ ¬a C4 ≡ a y3 ∨ ¬a y4 ∨ a\nHere C3 and C4 are clearly in conflict, but given the choice it is better to relax C3 so that C1 and C2 can be satisfied simultaneously. At the top level the solver creates the multiple decision (simultaneously) y1 = false, . . ., y4 = false and\nsolves. This fails with unsatisfiable core y3 ∨ y4, which is the generalized 1UIP nogood resulting from the implication graph in Figure 1(a).\nOn the next attempt it sets only y1 = false, y2 = false, because y3, y4 have appeared in an unsatisfiable core. The only possible solution, under these assumptions, is b = false, a = true, y3 = true, y4 = false with cost 1.\nAlthough the initial solutions found by the unsatisfiable core algorithm are frequently of low cost, the algorithm does not replace the need for a traditional branch-and-bound approach (or equivalently the solving of a series of satisfaction problems with tighter and tighter bounds on the cost). When we have a (best known) solution θ, the objective bound cTy < cT θ(y) needs to be propagated. This is standard for CP systems, although MAXSAT solvers typically use SAT decompositions of the objective using BDDs or sorting networks [22].\nExample 2. Consider the trivial instance (original on left, reified on right), C1 ≡ ¬a C2 ≡ ¬b y1 ∨ ¬a y2 ∨ ¬b C3 ≡ a ∨ b C4 ≡ ¬c y3 ∨ a ∨ b y4 ∨ ¬c The only unsatisfiable core is y1∨ y2∨ y3, discovered from the implication graph in Figure 1(b). The solver will never try c = true because y4 does not participate in any unsatisfiable core. After discovering the core, search will set y4 = false and propagate ¬c. Search will continue finding solutions with a = b = true at cost 2 or any other valuation to a, b at cost 1. Unsatisfiable-core algorithms do not distinguish between these cases.\nSuperficially the algorithm is a static search which fixes the variables in y to false before proceeding, but is quite different because (i) any propagation resulting from fixing the y-variables is done after all are fixed, which is enormously more efficient than one-by-one, especially when global constraints are involved, and (ii) static search backtracks only as far as necessary, so it will try all values for the tuple (y1, . . . , yn) in lexical order, which is impractical for large n.\nIn the best MAXSAT implementations [22,23] the relaxation variables for a clause were not created until the clause had appeared in an unsatisfiable subset, with the information about conflicting clauses being extracted from proof traces rather than from the presence of their relaxation literals in a learnt clause. This was sensible for MAXSAT considering that MAXSAT problems are ‘tall’,\ni.e. they have more clauses than variables, hence more relaxation variables than ordinary variables, unless relaxation variables are created lazily.\nOn the other hand CP has much less reliance on decomposition due to its richer constraint library and problems are frequently ‘wide’, i.e. they have more variables than constraints, particularly when there is extensive use of global constraints. So relatively few relaxation literals are required and it was not worth going to the trouble of reformulating the problem ‘on the fly’, so we opted for the simpler approach of the artificial decision level, which allows conflict analysis to collect relaxation literals into unsatisfiable cores in a natural way."
    }, {
      "heading" : "5 Nested Unsatisfiable Core Algorithm",
      "text" : "With the previously outlined approach to unsatisfiable cores it can happen that all or most variables in y are covered by one or more unsatisfiable cores. This is particularly likely in problems with a lot of structure or symmetry such that each variable is involved in a similar set of constraints. In such cases, once all unsatisfiable cores have been enumerated, all soft constraints have reverted to being treated as soft, and the problem is no easier than originally.\nHence we define a nested version of the algorithm in which soft constraints can be made hard during search rather than only at the top level. This involves two new concepts, (i) active unsatisfiable cores which describe a conflicting set of soft constraints where we do not know which will be violated, as opposed to inactive where a particular violation is known and has already been penalized, and (ii) contingent unsatisfiable cores which are only unsatisfiable cores in the current subproblem, that is, under the current search assumptions.\nNote that in the clausal view a contingent unsatisfiable core is simply any clause where only yi literals remain, because other literals are already false. Then at least one of these yi must be true, i.e. it is an unsatisfiable core. It is initially active but goes inactive when at least one of these yi becomes true.\nIn the revised algorithm, an objective variable yi may be set false initially by a multiple decision, but then reverts to unconstrained when search discovers it is part of an unsatisfiable core and backtracks. It will no longer be set to false since it appears in an active unsatisfiable core. But it may again be set false at a deeper decision level if all the unsatisfiable cores involving yi go inactive.\nExample 3. Referring to Example 2, given the unsatisfiable core y1 ∨ y2 ∨ y3, on the next attempt y4 has not been involved in any core and so is set to false on level 1, hence c = false. On level 2, suppose search tries a = true hence y1 = true and reaches a fixed point. Since y1 ∨ y2 ∨ y3 is now satisfied, there is no evidence that y2 or y3 needs to be true, and a new multiple decision is made as level 3 which sets y2 = y3 = false. Solving continues, finding b = false and cost 1.\nExample 4. Consider minimizing a+ b+ c subject to the clauses\na ∨ b ∨ d b ∨ c ∨ ¬d ¬a ∨ d,\nwhich arises from the partial MAXSAT instance with soft constraints ¬a,¬b,¬c and hard constraints as above. Falsifying a, b, c returns the unsatisfiable core\nAlgorithm 3 Nested unsatisfiable core algorithm (relative to Algorithm 1)\nRequire: ai = count of active contingent unsatisfiable cores involving yi, initially 0 Require: Yi = stack of all contingent unsatisfiable core involving yi, initially empty\nRequire: When literal yi goes true due to propagation or decision, 1: for {L→M} in stack Yi where {L→M} is still active do 2: mark {L→M} as inactive, and for all yj ∈M decrement aj 3: end for\n4: function Learn(C, L,M) where |M | > 1 (otherwise fall back to Algorithm 1) 5: for all yi ∈M push the clause {L→M} onto stack Yi and increment ai 6: return C % unchanged constraint set, to avoid risk of learning duplicates 7: end function\n8: function Decide(D(x1)× . . .×D(xm)×D(y1)× . . .×D(yn)) 9: if exists yi with ai = 0 and D(yi) = {false, true} then\n10: for all such yi restrict the corresponding domain D(yi) to {false} 11: else 12: restrict some other domain according to user’s programmed search 13: end if 14: return D(x1)× . . .×D(xm)×D(y1)× . . .×D(yn) 15: end function\na∨ b∨ c, so that on the next attempt, nothing can be fixed at the top level. This is exactly the problem that the nested algorithm tries to address.\nSince a multiple decision is not applicable the ordinary decision procedure sets a = true on level 1, resulting in d = true, and reaches a fixed point. Because a ∨ b ∨ c is now satisfied, level 2 is a multiple decision setting b = c = false, but this immediately fails with reason b∨ c∨¬d, because d was already true. Solving backtracks past the multiple decision and records the new unsatisfiable core b∨c (contingent on d = true), then continues, using the ordinary decision procedure, eventually finding a solution of cost 2 (with a and either b or c = true).\nThe benefit of this approach is that even if all objective literals eventually appear in an unsatisfiable core, we still aggressively set them to false as soon as all the active (contingent) unsatisfiable cores they appear in are satisfied. Conversely, it may be there are too many contingent unsatisfiable cores to be enumerated by this method, leading to thrashing in the solver.\nThe revised solver replaces Algorithm 2 with Algorithm 3 which maintains activity counts ai for each variable yi. When a new core becomes active through discovery in conflict analysis, the counts for all variables involved are incremented. When the core goes inactive, the counts are decremented, based on a cross-referencing data structure consisting of stacks Yi. The counts ai and stacks Yi must be trailed so that they are reset correctly on backjumping. The Decide procedure only considers yi variables with zero activity counts as candidates for a multiple decision, or if none exist it makes a normal search decision."
    }, {
      "heading" : "6 Notification-based Nested Algorithm",
      "text" : "In the algorithms discussed to this point, we did not ‘learn’ unsatisfiable cores by adding them to the constraint store, because we did not want to risk learning duplicates, leading to duplicated propagation work. This is frustrating for the nested algorithm since contingent unsatisfiable cores are discarded on backtracking, even though they are valid nogoods which may capture new information.\nExample 5. In Example 4, the first unsatisfiable core is a ∨ b ∨ c which holds new information not explicit in the original formulation, that can help future search. The next (contingent) unsatisfiable core is b∨ c∨¬d which is a duplicate of a clause in the original problem. Other than searching the constraint store or using a hashing scheme, we have no way of distinguishing between these cases.\nFurthermore, with the previously-defined algorithms, clausal constraints from the constraint store are not used as contingent unsatisfiable cores except when they are violated by a multiple decision. To tackle this problem we modify the clausal propagators to issue a notification whenever an ordinary clause becomes an active contingent unsatisfiable core, that is, when the last of the non-yi literals in the clause goes false.\nIn essence we treat the clause C = X ∨ Y , where Y are the yi literals in C and X the others, as X ∨ l and ¬l ∨ Y , where l is a new literal. When l is set true by unit propagation, the clause C becomes a contingent unsatisfiable core. This is preferable because any multiple decisions made by the unsatisfiable core algorithms take into account all current information and do not lead to trivial failures. Then it is safe to learn all clauses resulting from conflict analysis.\nExample 6. Revisiting Example 4, the first unsatisfiable core a∨ b∨ c has to be derived in the usual way by conflict analysis resulting from a multiple decision, but on the second attempt, as soon as d is set true by propagation from a, the original clause b∨ c∨¬d becomes an active contingent unsatisfiable core and is immediately recorded against the literals b and c as evidence that one of them must be true. This avoids the failed search with b = c = false."
    }, {
      "heading" : "7 Enhanced Lower Bounding",
      "text" : "At each node of the branch and bound tree we need to check the linear constraint cTy < cT θ(y) where θ is the current best solution and fathom (backtrack) when the constraint detects failure. This compares the current solution cost, or if we only have a partial solution then a conservative lower bound on the solution cost, with a conservative upper bound on the optimal solution cost. Usually the conservative lower bound is obtained by taking all unfixed yi as false.\nInformation from unsatisfiable cores can be used to strengthen the lower bound. Suppose that some subset e.g. y2, y3 and y5 are unfixed but we have the unsatisfiable core y2 ∨ y3 ∨ y5. Then clearly it is too conservative to assume that all are false and we can increase the estimate by at least min(c2, c3, c5),\nAlgorithm 4 Disjoint unsatisfiable core-based bound-strengthening\nfunction Disjoint(cTy < u, {G1, . . . , G`}) for i in 1..` do\nαi ← minyj∈Gi cj for all yj in Gi do cj ← cj − αi end for u← u− αi\nend for return cTy < u\nend function\nhopefully leading to earlier fathoming. This is the basis of disjoint inconsistent subformula approaches which have been used for MAXSAT [19].\nExample 7. Referring to Example 2, after a finding the first solution of cost 1, the unsatisfiable core y1 ∨ y2 ∨ y3 implies that any solution will have cost ≥ 1 so no further improvement is possible. Solving can then terminate immediately.\nNote that our treatment is somewhat more formal as LCG solvers require reasons for fathoming. Thus it is inadequate just to increase the estimate, we have to derive a globally true but tighter objective constraint. Suppose we have an objective constraint L ≡ ∑n i=1 ciyi < u where u is the current upper bound, and a collection of active unsatisfiable cores C1, . . ., C`. Then considering core Ci as a linear constraint Gi ≡ ∑ l∈Ci −l ≤ −1, we can produce a tightened objective constraint as a linear combination of L and Gi, i ∈ 1..`.\nExample 8. Suppose the problem is minimizing 2y1 +3y2 +3y3 +5y4 with u = 7 so that the objective constraint is U ≡ 2y1 + 3y2 + 3y3 + 5y4 < 7. Unsatisfiable core C1 ≡ y1∨y3∨y4 gives the linear constraint G1 ≡ −y1−y3−y5 ≤ −1. Then a strengthened objective constraint is obtained as the Fourier elimination [11] of y1, by summing the constraints U\n′ ≡ U+2G1 ≡ 3y2 +y3 +3y5 < 5. Unsatisfiable core C2 ≡ y2 ∨ y3 ∨ y4 gives G2 = −y2 − y3 − y4 ≤ 1 and eliminates y3 giving U ′′ ≡ U ′ +G2 ≡ 2y2 + 2y5 < 4, which is tighter while still globally true.\nWe use two methods to strengthen the objective constraint U , (i) a heuristic method and (ii) an exact method. Note that the tightened inequality is contingent upon the non-yi literals of any contingent unsatisfiable cores that contributed to tightening, so whenever we have to explain the actions of the tightened propagator U we have to add these literals to the explanation.\nDisjoint Unsatisfiable Core-based Lower Bounding: The heuristic method, shown as Algorithm 4, starts by taking a working version of the objective upperbound constraint U . Then for each Gi it eliminates from U , by the FourierMotzkin method [11], the cheapest variable that could be made true to satisfy Gi, using the current working coefficients as costs. This means adding the assumed cost of satisfying Gi into the estimate, then adjusting the assumed costs used from then on, to avoid double-counting. Example 8 illustrates the algorithm.\nThe above algorithm is well known for weighted MAXSAT. We define an incremental version that modifies the objective constraint each time new active\nunsatisfiable cores are discovered. When unsatisfiable cores go inactive they are removed from the tightened inequality, which is necessary for correctness, since the tightened bound constraint is a consequence of the original bound constraint, not equivalent. After removing an inactive core, we found it essential to reexamine subsequently-added cores to see if they can contribute any additional strength.\nExample 9. Continuing Example 8, suppose later y1 = true, then U ′′ does not propagate, but U would have since it then becomes 3y2 + 3y3 + 5y4 < 5 which immediately sets y4 = false. We roll back to U where we eliminated the first core involving y1, and now eliminate the remaining active core C2 ≡ y2 ∨ y3 ∨ y4 to obtain U ′′′ ≡ U + 3G2 ≡ 2y1 + 2y4 < 4, which propagates y4 = false.\nLinear Programming-based Lower Bounding: The strengthening procedure is an optimization problem over the coefficients αi discussed above, which can be solved to optimality using a Linear Programming (LP) solver to give the best possible fathoming based on the information available at each node. The LP has to be updated at each node with the latest set of unsatisfiable cores, noting that inactive cores can be left in, since it can see they are already satisfied. The LP can also contain other constraints at the modeller’s discretion.\nWe define an LP constraint linear program(G, c,y, z) which given a set of linear (in)equalities G over the variables y, an objective cTy and an upper bound z on the objective (based on the best solution found so far), enforces that all (in)equalities are satisfied and cTy ≤ z. It executes when the bounds on y are tightened, and verifies that G can still hold, if not it detects failure with an explanation. It also verifies that there is still objective slack, if not it fathoms with an explanation. Then it prunes the y if possible, based on the slack.\nThe linear program propagator works similarly to the bound-strengthening procedure described previously, in that it derives new linear constraints L +∑\ni∈1..` αiGi, choosing the vector α which minimizes the RHS of the resulting constraint. For the LP case the vector α and indeed the coefficients and RHS of the strengthened constraint are available directly in the dual solution after minimizing cTy subject to G. Thus the standard procedure of deriving explanations from dual solutions or unbounded dual rays is applicable [1,6,8]."
    }, {
      "heading" : "8 Experiments",
      "text" : "To evaluate whether unsatisfiable cores are useful for CP and whether our extensions are helping, we compared standard branch-and-bound with the solvers described in Sections 4, 5 and 6. With each of these solvers we tried standard lower bounding and each of the strengthened lower bounds from Section 7.\nThe basic solver is a state-of-the-art LCG solver, Chuffed . We use activitybased search (VSIDS) for all experiments [24]. For maximization problems we negate the objective, so all are minimization. The experiments were run on a cluster of AMD 6-core Opteron 4184 at 2.8 GHz with time limit 3600s, memory limit 2 Gbytes per core, and memory-outs treated as timeouts. Data files are available from http://www.csse.unimelb.edu.au/~pjs/unsat_core.\nWe tried the following combinatorial benchmarks: psm (pattern set mining, 16 instances) is given a set of training items each a vector of Booleans, find some vector which characterizes all items, or if k > 1 find the best k vectors [14]; photo (30 instances) is given a set of people and soft constraints on who they stand next to, place them in a line for a photo; rlfap (radio link frequency assignment, 16 instances) is assigning frequencies to channels with soft constraints that minimize interference [5]; roster (20 instances) is finding a cyclic roster for a single worker over a number of weeks with soft work pattern constraints; and sugiyama (5 instances) is a graph layout problem on layered graphs with soft no-edge-crossings constraints [27].\nTable 1 shows for each solver: ‘opt’ number of instances optimized, ‘s’ mean solving time (taking the timeout for instances which timed out), ‘sol’ number of instances for which any solution was found, and ‘obj’ mean objective (taking the objective from the worst solver for instances where no objective is available, noting that this may be too generous). The solver which solves the most instances, falling back to comparing times and so on, is highlighted.\nOn these problems we see that with standard bound-estimation, we see that unsatisfiable cores is usually better than the unmodified solver but the improvement is not particularly dramatic. The best version of the unsatisfiable cores algorithm depends on the problem. Adding notification nearly always improves the nested algorithm, though sugiyama is an exception which indicates an unusual constraint structure (perhaps there are too many unsatisfiable cores).\nIf the extra information from unsatisfiable cores is also used for boundstrengthening then dramatic improvements are possible. The best solver overall is the nested algorithm with notification and (initially empty) LP-based bound\nstrengthening. We also see good results from the nested or basic algorithm and disjoint-based bound strengthening (without notification). The trade-off is that disjoint-based bounding is faster but sacrifices pruning power. Adding notification always helps LP-based strengthening, because the LP receives more information, but never helps disjoint-based strengthening, because overlapping unsatisfiable cores slow down the algorithm without adding strength.\nWe then tried some much more difficult industrial problems: ctt (curriculumbased timetabling, 32 instances) is finding a weekly repeating timetable for a university subject to various kinds of soft availability constraints and soft no-clashes constraints [7]; stein (Steiner network, 13 instances) is designing a connected network given a set nodes and arcs with a fixed (building) cost per arc [15]; fcnf (fixed-charge network flow, 60 instances) is designing a connected network with fixed (building) and also variable (operating) costs per arc [26]; and nsp (nurse scheduling problem, 32 instances) is designing a roster for a hospital ward based on the shift preferences of each individual nurse [29].\nAll of these problems use global constraints taken from gcc, sequence, regular and network flow. On ctt and nsp we utilize a hybrid search which considers each course/day sequentially, with activity-based search within courses/days.\nFurthermore the objective in ctt and nsp is not pseudo-Boolean as some yi are general integer. Then the unsatisfiable-cores algorithm expresses that e.g. all nurses should receive their first preference until we have evidence that this should be relaxed to the second preference for certain nurses and so on. We make the appropriate change to the multiple-decision algorithm, omitted earlier for clarity. Then unsatisfiable cores e.g. [y1 ≥ 3] ∨ [y2 ≥ 2] say nurse 1 cannot receive 1st or 2nd preference simultaneously with nurse 2 receiving 1st preference. For bounding we add a linearization e.g. 1/2(y1− 1) + (y2− 1) ≥ 1, into the LP. We only consider LP-based bounding in this experiment, not disjoint-based.\nThese problems are extremely difficult to prove infeasible and so unsatisfiablecores approaches were not highly successful initially (‘std’/‘empty LP’ columns). We addressed this with a hybrid CP/MIP approach, by adding a linear decom-\nposition of all CP constraints, including globals, as a redundant linear program propagator (‘redundant LP’ column). We also evaluated leaving out the original CP constraints and using only the LP, so that Chuffed becomes similar to a learning MIP solver such as SCIP using unsatisfiable cores (‘LP only’ column), but missing advanced MIP cutting planes and rounding heuristics.\nTable 2 shows the results of these experiments. We add the column ‘inf’ as number of instances proved infeasible due to hard constraints. With the enhanced globality due to the linear program propagator, the unsatisfiable-cores algorithm, in particular the nested algorithm with notification, is a big improvement for CP. On nsp (which has preferences), CP is useless without unsatisfiable cores.\nFinally we consider MIP (without unsatisfiable cores) as an alternative to CP for solving soft-constraint problems. Table 3 compare CPLEX 12.0 and SCIP 2.1.1 with the best solver from each of the experiments described above. On the combinatorial benchmarks CP is clearly superior and much improved with our techniques. MIP was really only effective on the industrial problems. Given that timetabling and design problems require globality and that MIP employs many specialized and/or proprietary techniques, MIP should be the first resort for such problems. On the other hand CP with all our techniques is significantly improved and starts to look competitive, indeed on stein we beat MIP. On stein the ‘cuts’ added for LP-based bounding enforce the connectivity of the network and are similar to [15,26], but our approach is much more generic."
    }, {
      "heading" : "9 Conclusion",
      "text" : "We have translated unsatisfiable-core methods from MAXSAT to solve CP optimization problems with pseudo-Boolean objectives, by making use of the facility of LCG solvers to generate unsatisfiable cores. This provides one of the first approaches to soft intensionally defined constraint problems beyond branch and bound that we are aware of, apart from PBO/WBO [9,21] which support intensionally-defined linear constraints only.\nTo gain the maximum advantage from unsatisfiable cores we needed to extend the method to generate and use unsatisfiable cores in the middle of search. This approach to optimization can be substantially better than the traditional branch and bound approach on intensionally defined optimization problems. With our extensions we saw a clear synergy between (i) aggressively assuming that soft\nconstraints hold, and (ii) using the resulting information about unsatisfiable or conflicting soft constraints, for enhanced fathoming."
    } ],
    "references" : [ {
      "title" : "Conflict analysis in mixed integer programming",
      "author" : [ "T. Achterberg" ],
      "venue" : "Discrete Optimization 4(1), 4–20",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Solving (Weighted) Partial MaxSAT through Satisfiability Testing",
      "author" : [ "C. Ansótegui", "M. Bonet", "J. Levy" ],
      "venue" : "Kullmann, O. (ed.) Proc. SAT2009, LNCS, vol. 5584, pp. 427–440. Springer Berlin / Heidelberg",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Resolution for Max-SAT",
      "author" : [ "M.L. Bonet", "J. Levy", "F. Manyà" ],
      "venue" : "AI 171(8-9), 606–618",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Radio Link Frequency Assignment",
      "author" : [ "B. Cabon", "S. de Givry", "L. Lobjois", "T. Schiex", "J.P. Warners" ],
      "venue" : "Constraints 4, 79–89",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Efficient Intelligent Backtracking Using Linear Programming",
      "author" : [ "B. Davey", "N. Boland", "P. Stuckey" ],
      "venue" : "IJOC 14(4), 373–386",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Neighborhood Portfolio Approach for Local Search Applied to Timetabling Problems",
      "author" : [ "L. Di Gaspero", "A. Schaerf" ],
      "venue" : "JMMA 5, 65–89",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Explaining Flow-Based Propagation",
      "author" : [ "N. Downing", "T. Feydy", "P.J. Stuckey" ],
      "venue" : "Beldiceanu, N., Jussien, N., Pinson, E. (eds.) Proc. CPAIOR2012. LNCS, vol. 7298, pp. 146–162. Springer Verlag, Nantes, France",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Translating Pseudo-Boolean Constraints into SAT",
      "author" : [ "N. Eén", "N. Sörensson" ],
      "venue" : "JSAT 2(1-4), 1–26",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Half reification and flattening",
      "author" : [ "T. Feydy", "Z. Somogyi", "P. Stuckey" ],
      "venue" : "Lee, J. (ed.) Proc. CP2011, LNCS, vol. 6876, pp. 286–301. Springer Berlin / Heidelberg",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "On Solving the Partial MAX-SAT Problem",
      "author" : [ "Z. Fu", "S. Malik" ],
      "venue" : "Biere, A., Gomes, C. (eds.) Proc. SAT2006, LNCS, vol. 4121, pp. 252–265. Springer Berlin / Heidelberg",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Existential arc consistency: Getting closer to full arc consistency in weighted CSPs",
      "author" : [ "S. de Givry", "F. Heras", "M. Zytnicki", "J. Larrosa" ],
      "venue" : "Proc. IJCAI05. pp. 193–198. AAAI Press",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Itemset mining: A constraint programming perspective",
      "author" : [ "T. Guns", "S. Nijssen", "L.D. Raedt" ],
      "venue" : "AI 175(12-13), 1951–1983",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Solving Steiner tree problems in graphs to optimality",
      "author" : [ "T. Koch", "A. Martin" ],
      "venue" : "Networks 32(3), 207–232",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Improved Exact Solver for the Weighted MAX-SAT Problem",
      "author" : [ "A. Kuegel" ],
      "venue" : "Pragmatics of SAT workshop, SAT2010",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "In the quest of the best form of local consistency for Weighted CSP",
      "author" : [ "J. Larrosa", "T. Schiex" ],
      "venue" : "Proc. IJCAI03. pp. 239–244. AAAI Press",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Exploiting Unit Propagation to Compute Lower Bounds in Branch and Bound Max-SAT Solvers",
      "author" : [ "C. Li", "F. Manyà", "J. Planes" ],
      "venue" : "van Beek, P. (ed.) Proc. CP2005, LNCS, vol. 3709, pp. 403–414. Springer Berlin / Heidelberg",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Detecting disjoint inconsistent subformulas for computing lower bounds for Max-SAT",
      "author" : [ "C.M. Li", "F. Manyà", "J. Planes" ],
      "venue" : "Proc. AAAI06. pp. 86–91. AAAI Press",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "IncWMaxSatz. In: Max-SAT",
      "author" : [ "H. Lin", "K. Su", "C.M. Li", "J. Argelich" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2008
    }, {
      "title" : "Algorithms for Weighted Boolean Optimization",
      "author" : [ "V. Manquinho", "J. Marques-Silva", "J. Planes" ],
      "venue" : "Kullmann, O. (ed.) Proc. SAT2009, LNCS, vol. 5584, pp. 495– 508. Springer Berlin / Heidelberg",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Algorithms for Maximum Satisfiability using Unsatisfiable Cores",
      "author" : [ "J. Marques-Silva", "J. Planes" ],
      "venue" : "Proc. DATE2008. pp. 408–413",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "On using unsatisfiability for solving maximum satisfiability",
      "author" : [ "J. Marques-Silva", "J. Planes" ],
      "venue" : "CoRR abs/0712.1097",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Chaff: engineering an efficient SAT solver",
      "author" : [ "M. Moskewicz", "C. Madigan", "Y. Zhao", "L. Zhang", "S. Malik" ],
      "venue" : "Proc. DAC2001. pp. 530–535",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Propagation via lazy clause generation",
      "author" : [ "O. Ohrimenko", "P. Stuckey", "M. Codish" ],
      "venue" : "Constraints 14, 357–391",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A branch-and-cut algorithm for the single-commodity, uncapacitated, fixed-charge network flow problem",
      "author" : [ "F. Ortega", "L.A. Wolsey" ],
      "venue" : "Networks 41(3), 143–158",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Methods for Visual Understanding of Hierarchical System Structures",
      "author" : [ "K. Sugiyama", "S. Tagawa", "M. Toda" ],
      "venue" : "Systems, Man and Cybernetics, IEEE Trans. 11(2), 109–125",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 1981
    }, {
      "title" : "Operations Research Techniques in Constraint Programming",
      "author" : [ "W.J. Van Hoeve" ],
      "venue" : "Ph.D. thesis, Centrum voor Wiskunde en Informatica, The Netherlands",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "NSPLib – A Nurse Scheduling Problem Library: A tool to evaluate (meta-)heuristic procedures",
      "author" : [ "M. Vanhoucke", "B. Maenhout" ],
      "venue" : "Proc. ORAHS2005",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "Earlier work on unsatisfiable cores for Maximum Satisfiability (MAXSAT) has shown that it is advantageous to consider soft constraints to be hard constraints initially, solve the problem using a modern SAT solver, and use the resulting evidence of infeasibility to see which (temporarily hard) constraints are conflicting with each other, and soften them again only as necessary [12].",
      "startOffset" : 379,
      "endOffset" : 383
    }, {
      "referenceID" : 25,
      "context" : "CP handles soft-constraint problems as minimization problems where the objective is a count of violations, the counts being derived from either reified primitive constraints (whose enforcement is controlled by an auxiliary variable) or soft global constraints (for examples of soft global constraints and their propagation algorithms see Van Hoeve [28]).",
      "startOffset" : 348,
      "endOffset" : 352
    }, {
      "referenceID" : 21,
      "context" : "LCG is a hybrid approach to CP that uses a traditional ‘propagation and search’ constraint solver as the outer layer which guides the solution process, plus an inner layer which lazily decomposes CP to Boolean satisfiability (SAT) and applies learning SAT solver technology to reduce search [24,25].",
      "startOffset" : 291,
      "endOffset" : 298
    }, {
      "referenceID" : 22,
      "context" : "LCG is a hybrid approach to CP that uses a traditional ‘propagation and search’ constraint solver as the outer layer which guides the solution process, plus an inner layer which lazily decomposes CP to Boolean satisfiability (SAT) and applies learning SAT solver technology to reduce search [24,25].",
      "startOffset" : 291,
      "endOffset" : 298
    }, {
      "referenceID" : 22,
      "context" : "We give a brief description of propagation-based solving and LCG, for more details see [25].",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 21,
      "context" : "Conflict analysis derives new redundant constraints to avoid repeated search, and, as a side effect, modifies the backtracking procedure to backjump or restart solving at an appropriate point close to the failure [24].",
      "startOffset" : 213,
      "endOffset" : 217
    }, {
      "referenceID" : 10,
      "context" : "For WCSP, in which each extensional table contains a weight column giving the cost to be paid if the row holds, the best solver seems to be toolbar/toulbar2 [13,17].",
      "startOffset" : 157,
      "endOffset" : 164
    }, {
      "referenceID" : 14,
      "context" : "For WCSP, in which each extensional table contains a weight column giving the cost to be paid if the row holds, the best solver seems to be toolbar/toulbar2 [13,17].",
      "startOffset" : 157,
      "endOffset" : 164
    }, {
      "referenceID" : 13,
      "context" : "For MAXSAT, good solvers in a recent evaluation [3] included akmaxsat [16] and MaxSatz [20] variants.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 17,
      "context" : "For MAXSAT, good solvers in a recent evaluation [3] included akmaxsat [16] and MaxSatz [20] variants.",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 15,
      "context" : "Essentially they use lookahead, with unit propagation and failed literal detection, to improve the lower bound [18,19].",
      "startOffset" : 111,
      "endOffset" : 118
    }, {
      "referenceID" : 16,
      "context" : "Essentially they use lookahead, with unit propagation and failed literal detection, to improve the lower bound [18,19].",
      "startOffset" : 111,
      "endOffset" : 118
    }, {
      "referenceID" : 2,
      "context" : "In restricted cases they use MAXSAT resolution, in which conflicting clauses are replaced by a unified clause plus compensation clauses [4].",
      "startOffset" : 136,
      "endOffset" : 139
    }, {
      "referenceID" : 1,
      "context" : "Recently there has also been considerable interest in decomposing MAXSAT to SAT, usually with an unsatisfiable-core approach [2,12,22,23].",
      "startOffset" : 125,
      "endOffset" : 137
    }, {
      "referenceID" : 9,
      "context" : "Recently there has also been considerable interest in decomposing MAXSAT to SAT, usually with an unsatisfiable-core approach [2,12,22,23].",
      "startOffset" : 125,
      "endOffset" : 137
    }, {
      "referenceID" : 19,
      "context" : "Recently there has also been considerable interest in decomposing MAXSAT to SAT, usually with an unsatisfiable-core approach [2,12,22,23].",
      "startOffset" : 125,
      "endOffset" : 137
    }, {
      "referenceID" : 20,
      "context" : "Recently there has also been considerable interest in decomposing MAXSAT to SAT, usually with an unsatisfiable-core approach [2,12,22,23].",
      "startOffset" : 125,
      "endOffset" : 137
    }, {
      "referenceID" : 21,
      "context" : "Because they use learning instead of lookahead (and other improvements such as activity-based search [24]), they have a considerable advantage over the previously-described approaches.",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 18,
      "context" : "In particular the Weighted Boolean Optimization (WBO) framework [21] is an application of PBO to soft-constraint problems, using some of the specialized techniques discussed above.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 7,
      "context" : "Another option is decomposition to SAT via the PBO solver MiniSAT+ [9], which could be useful if unsatisfiability-based methods aren’t applicable to a particular problem.",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 8,
      "context" : "Note that a CP system supporting constraint Si can be straightforwardly extended to support the softened form ¬yi → Si through half-reification [10].",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 19,
      "context" : "This is standard for CP systems, although MAXSAT solvers typically use SAT decompositions of the objective using BDDs or sorting networks [22].",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 19,
      "context" : "In the best MAXSAT implementations [22,23] the relaxation variables for a clause were not created until the clause had appeared in an unsatisfiable subset, with the information about conflicting clauses being extracted from proof traces rather than from the presence of their relaxation literals in a learnt clause.",
      "startOffset" : 35,
      "endOffset" : 42
    }, {
      "referenceID" : 20,
      "context" : "In the best MAXSAT implementations [22,23] the relaxation variables for a clause were not created until the clause had appeared in an unsatisfiable subset, with the information about conflicting clauses being extracted from proof traces rather than from the presence of their relaxation literals in a learnt clause.",
      "startOffset" : 35,
      "endOffset" : 42
    }, {
      "referenceID" : 16,
      "context" : "This is the basis of disjoint inconsistent subformula approaches which have been used for MAXSAT [19].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 0,
      "context" : "Thus the standard procedure of deriving explanations from dual solutions or unbounded dual rays is applicable [1,6,8].",
      "startOffset" : 110,
      "endOffset" : 117
    }, {
      "referenceID" : 4,
      "context" : "Thus the standard procedure of deriving explanations from dual solutions or unbounded dual rays is applicable [1,6,8].",
      "startOffset" : 110,
      "endOffset" : 117
    }, {
      "referenceID" : 6,
      "context" : "Thus the standard procedure of deriving explanations from dual solutions or unbounded dual rays is applicable [1,6,8].",
      "startOffset" : 110,
      "endOffset" : 117
    }, {
      "referenceID" : 21,
      "context" : "We use activitybased search (VSIDS) for all experiments [24].",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 11,
      "context" : "We tried the following combinatorial benchmarks: psm (pattern set mining, 16 instances) is given a set of training items each a vector of Booleans, find some vector which characterizes all items, or if k > 1 find the best k vectors [14]; photo (30 instances) is given a set of people and soft constraints on who they stand next to, place them in a line for a photo; rlfap (radio link frequency assignment, 16 instances) is assigning frequencies to channels with soft constraints that minimize interference [5]; roster (20 instances) is finding a cyclic roster for a single worker over a number of weeks with soft work pattern constraints; and sugiyama (5 instances) is a graph layout problem on layered graphs with soft no-edge-crossings constraints [27].",
      "startOffset" : 232,
      "endOffset" : 236
    }, {
      "referenceID" : 3,
      "context" : "We tried the following combinatorial benchmarks: psm (pattern set mining, 16 instances) is given a set of training items each a vector of Booleans, find some vector which characterizes all items, or if k > 1 find the best k vectors [14]; photo (30 instances) is given a set of people and soft constraints on who they stand next to, place them in a line for a photo; rlfap (radio link frequency assignment, 16 instances) is assigning frequencies to channels with soft constraints that minimize interference [5]; roster (20 instances) is finding a cyclic roster for a single worker over a number of weeks with soft work pattern constraints; and sugiyama (5 instances) is a graph layout problem on layered graphs with soft no-edge-crossings constraints [27].",
      "startOffset" : 506,
      "endOffset" : 509
    }, {
      "referenceID" : 24,
      "context" : "We tried the following combinatorial benchmarks: psm (pattern set mining, 16 instances) is given a set of training items each a vector of Booleans, find some vector which characterizes all items, or if k > 1 find the best k vectors [14]; photo (30 instances) is given a set of people and soft constraints on who they stand next to, place them in a line for a photo; rlfap (radio link frequency assignment, 16 instances) is assigning frequencies to channels with soft constraints that minimize interference [5]; roster (20 instances) is finding a cyclic roster for a single worker over a number of weeks with soft work pattern constraints; and sugiyama (5 instances) is a graph layout problem on layered graphs with soft no-edge-crossings constraints [27].",
      "startOffset" : 750,
      "endOffset" : 754
    }, {
      "referenceID" : 5,
      "context" : "We then tried some much more difficult industrial problems: ctt (curriculumbased timetabling, 32 instances) is finding a weekly repeating timetable for a university subject to various kinds of soft availability constraints and soft no-clashes constraints [7]; stein (Steiner network, 13 instances) is designing a connected network given a set nodes and arcs with a fixed (building) cost per arc [15]; fcnf (fixed-charge network flow, 60 instances) is designing a connected network with fixed (building) and also variable (operating) costs per arc [26]; and nsp (nurse scheduling problem, 32 instances) is designing a roster for a hospital ward based on the shift preferences of each individual nurse [29].",
      "startOffset" : 255,
      "endOffset" : 258
    }, {
      "referenceID" : 12,
      "context" : "We then tried some much more difficult industrial problems: ctt (curriculumbased timetabling, 32 instances) is finding a weekly repeating timetable for a university subject to various kinds of soft availability constraints and soft no-clashes constraints [7]; stein (Steiner network, 13 instances) is designing a connected network given a set nodes and arcs with a fixed (building) cost per arc [15]; fcnf (fixed-charge network flow, 60 instances) is designing a connected network with fixed (building) and also variable (operating) costs per arc [26]; and nsp (nurse scheduling problem, 32 instances) is designing a roster for a hospital ward based on the shift preferences of each individual nurse [29].",
      "startOffset" : 395,
      "endOffset" : 399
    }, {
      "referenceID" : 23,
      "context" : "We then tried some much more difficult industrial problems: ctt (curriculumbased timetabling, 32 instances) is finding a weekly repeating timetable for a university subject to various kinds of soft availability constraints and soft no-clashes constraints [7]; stein (Steiner network, 13 instances) is designing a connected network given a set nodes and arcs with a fixed (building) cost per arc [15]; fcnf (fixed-charge network flow, 60 instances) is designing a connected network with fixed (building) and also variable (operating) costs per arc [26]; and nsp (nurse scheduling problem, 32 instances) is designing a roster for a hospital ward based on the shift preferences of each individual nurse [29].",
      "startOffset" : 547,
      "endOffset" : 551
    }, {
      "referenceID" : 26,
      "context" : "We then tried some much more difficult industrial problems: ctt (curriculumbased timetabling, 32 instances) is finding a weekly repeating timetable for a university subject to various kinds of soft availability constraints and soft no-clashes constraints [7]; stein (Steiner network, 13 instances) is designing a connected network given a set nodes and arcs with a fixed (building) cost per arc [15]; fcnf (fixed-charge network flow, 60 instances) is designing a connected network with fixed (building) and also variable (operating) costs per arc [26]; and nsp (nurse scheduling problem, 32 instances) is designing a roster for a hospital ward based on the shift preferences of each individual nurse [29].",
      "startOffset" : 700,
      "endOffset" : 704
    }, {
      "referenceID" : 12,
      "context" : "On stein the ‘cuts’ added for LP-based bounding enforce the connectivity of the network and are similar to [15,26], but our approach is much more generic.",
      "startOffset" : 107,
      "endOffset" : 114
    }, {
      "referenceID" : 23,
      "context" : "On stein the ‘cuts’ added for LP-based bounding enforce the connectivity of the network and are similar to [15,26], but our approach is much more generic.",
      "startOffset" : 107,
      "endOffset" : 114
    }, {
      "referenceID" : 7,
      "context" : "This provides one of the first approaches to soft intensionally defined constraint problems beyond branch and bound that we are aware of, apart from PBO/WBO [9,21] which support intensionally-defined linear constraints only.",
      "startOffset" : 157,
      "endOffset" : 163
    }, {
      "referenceID" : 18,
      "context" : "This provides one of the first approaches to soft intensionally defined constraint problems beyond branch and bound that we are aware of, apart from PBO/WBO [9,21] which support intensionally-defined linear constraints only.",
      "startOffset" : 157,
      "endOffset" : 163
    } ],
    "year" : 2017,
    "abstractText" : "Constraint Programming (CP) solvers typically tackle optimization problems by repeatedly finding solutions to a problem while placing tighter and tighter bounds on the solution cost. This approach is somewhat naive, especially for soft-constraint optimization problems in which the soft constraints are mostly satisfied. Unsatisfiable-core approaches to solving soft constraint problems in SAT (e.g. MAXSAT) force all soft constraints to be hard initially. When solving fails they return an unsatisfiable core, as a set of soft constraints that cannot hold simultaneously. These are reverted to soft and solving continues. Since lazy clause generation solvers can also return unsatisfiable cores we can adapt this approach to constraint programming. We adapt the original MAXSAT unsatisfiable core solving approach to be usable for constraint programming and define a number of extensions. Experimental results show that our methods are beneficial on a broad class of CP-optimization benchmarks involving soft constraints, cardinality or preferences.",
    "creator" : "LaTeX with hyperref package"
  }
}