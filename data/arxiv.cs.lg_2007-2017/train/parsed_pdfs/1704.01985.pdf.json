{
  "name" : "1704.01985.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Recognizing Multi-talker Speech with Permutation Invariant Training",
    "authors" : [ "Dong Yu", "Xuankai Chang", "Yanmin Qian" ],
    "emails" : [ "dongyu@ieee.org,", "xuank@sjtu.edu.cn,", "yanminqian@sjtu.edu.cn" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Thanks to the significant progresses made in the recent years [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], the ASR systems now surpassed the threshold for adoption in many real-world scenarios and enabled services such as Microsoft Cortana, Apple’s Siri and Google Now, where close-talk microphones are commonly used.\nHowever, the current ASR systems still perform poorly when far-field microphones are used. This is because many difficulties hidden by close-talk microphones now surface under distant recognition scenarios. For example, the signal to noise ratio (SNR) between the target speaker and the interfering noises is much lower than that when close-talk microphones are used. As a result, the interfering signals, such as background noise, reverberation, and speech from other talkers, become so distinct that they can no longer be ignored.\nIn this paper, we aims at solving the speech recognition problem when multi-talkers speak at the same time and only a single channel of mixed speech is available. Many attempts have been made to attack this problem. Before the deep learning era, the most famous and effective model is the factorial GMM-HMM [21], which outperformed human in the 2006 monaural speech separation and recognition challenge [22]. The factorial GMM-HMM, however, requires the test speakers to be seen during training so that the interactions between them can be properly modeled. Recently, several deep learning based techniques have been proposed to solve this problem\nDong Yu and Yanmin Qian are the corresponding authors. 1. Part of the work was done while at Microsoft Research. 2. Xuankai Chang and Yanmin Qian was supported by the Shanghai Sailing Program No. 16YF1405300, the China NSFC projects (No. 61573241 and No. 61603252) and the Interdisciplinary Program (14JCZ03) of Shanghai Jiao Tong University in China. Experiments have been carried out on the PI supercomputer at Shanghai Jiao Tong University.\n[23, 24, 25, 26, 19, 20]. The core issue that these techniques try to address is the label ambiguity or permutation problem (refer to Section 3 for details).\nIn Weng et al. [23] a deep learning model was developed to recognize the mixed speech directly. To solve the label ambiguity problem, Weng et al. assigned the senone labels of the talker with higher instantaneous energy to output one and the other to output two. This, although addresses the label ambiguity problem, causes frequent speaker switch across frames. To deal with the speaker switch problem, a two-speaker joint-decoder with a speaker switching penalty was used to trace speakers. This approach has two limitations. First, energy, which is manually picked, may not be the best information to assign labels under all conditions. Second, the frame switching problem introduces burden to the decoder.\nIn Hershey et al. [24, 25] the multi-talker mixed speech is first separated into multiple streams. An ASR engine is then applied to these streams independently to recognize speech. To separate the speech streams, they proposed a technique called deep clustering (DPCL). They assume that each time-frequency bin belongs to only one speaker and can be mapped into a shared embedding space. The model is optimized so that in the embedding space the time-frequency bins belong to the same speaker are closer and those of different speakers are farther away. During evaluation, a clustering algorithm is used upon embeddings to generate a partition of the time-frequency bins, i.e., speech separation and recognition are two separate components.\nChen et al. [26] proposed a similar technique called deep attractor network (DANet). Following DPCL, their approach also learns a high-dimensional embedding of the acoustic signals. Different from DPCL, however, it creates cluster centers, called attractor points, in the embedding space to pull together the time-frequency bins corresponding to the same source. The main limitation of DANet is the requirement to estimate attractor points during evaluation time and to form frequency-bin clusters based on these points.\nIn Yu et al. [19] and Kolbak et al.[20], a simpler yet equally effective technique named permutation invariant training (PIT) was proposed to attack the speaker independent multi-talker speech separation problem. In PIT, the source targets are treated as a set (i.e., order is irrelevant). During training, PIT first determines the output-target assignment with the minimum error at the utterance level based on the forward-pass result. It then minimizes the error given the assignment. This strategy elegantly solved the label permutation problem and speaker tracing problem in one shot. However, in these original works PIT was used to separate speech streams from mixed speech. For this reason, a frequency-bin mask was first estimated and then used to reconstruct each stream. The minimum mean square error (MMSE) between the true and reconstructed streams was used as the criterion to optimize model parameters.\nIn this paper, we propose the PIT-ASR model that can\nar X\niv :1\n70 4.\n01 98\n5v 4\n[ cs\n.S D\n] 1\n9 Ju\nn 20\n17\ndirectly recognize multiple streams of speech given just the single-channel mixed speech, without first separating it into speech streams. Different from [19, 20], we define PIT over the cross entropy (CE) between the true and estimated senone posterior probabilities. We evaluate our approach on the artificially mixed AMI data and demonstrate that the proposed approach is very promising.\nThe rest of the paper is organized as follows. In Section 2 we describe the speaker independent multi-talker mixed speech recognition problem. In Section 3 we apply PIT-ASR to directly recognize multi-streams of speech. We report experimental results in Section 4 and conclude the paper in Section 5."
    }, {
      "heading" : "2. Problem Setup",
      "text" : "In this paper, we assume that a linearly mixed singlemicrophone signal y[n] = ∑S s=1 xs[n] is known, where xs[n], s = 1, · · · , S are S streams of speech sources. Our goal is to separate and recognize these streams.\nHowever, given only the mixed speech y[n], the problem of recognizing all streams is under-determined because there are an infinite number of possible xs[n] (and thus recognition results) combinations that lead to the same y[n]. Fortunately, speech is not random signal. It has patterns that we may learn from a training set of pairs y and `s, s = 1, · · · , S, where `s is the senone label sequence for stream s.\nIn the single speaker case, where S = 1, the learning problem can be casted as a simple supervised optimization problem, in which the input to the model is some feature representation of y and the output is simply the senone posterior probability conditioned on the input. The model can be optimized to minimize the cross entropy between the senone label and the estimated posterior probability.\nWhen S > 1, however, it is no longer a simple supervised optimization problem due to the label ambiguity or permutation problem. Because speech sources are symmetric given the mixture (i.e., x1 + x2 equals to x2 + x1 and both x1 and x2 have the same characteristics), there is no pre-determined way to assign the correct target to the corresponding output layer. Interested readers can find additional information in [19, 20] on how training progresses to nowhere when the conventional supervised approach is used for the multi-talker speech separation."
    }, {
      "heading" : "3. Permutation Invariant Training",
      "text" : "To address the label ambiguity problem, we propose a novel model based on the permutation invariant training (PIT) [19, 20]. Note that, DPCL [24, 25] and DANet [26] are alternative solutions to the label ambiguity problem when the goal is speech source separation. However, these two techniques are not suitable for direct recognition of multiple streams of speech because of the clustering step required during separation, and the assumption that each time-frequency bin belongs to only one speaker.\nFormally, given some feature representation Y of the mixed speech y, our model will compute\nH0 = Y (1)\nHfi = RNN f i (Hi−1), i = 1, · · · , N (2)\nHbi = RNN b i (Hi−1), i = 1, · · · , N (3)\nHi = Stack(H f i ,H b i ), i = 1, · · · , N (4) Hso = Linear(HN ), s = 1, · · · , S (5) Os = Softmax(Hso), s = 1, · · · , S (6)\nusing a deep bidirectional recurrent neural network (RNN), where H0 is the input, Hi, i = 1, · · · , N is the i-th hidden layer in an N -hidden-layer network, RNNfi and RNN b i are the forward and backward RNNs at hidden layer i, respectively, Hso, s = 1, · · · , S is the excitation at output layer for each speech stream s, and Os, s = 1, · · · , S is the output layer for stream s. Note that, in this model each output layer represents an estimate of the senone posterior probability for a speech stream. No additional clustering or speaker tracing is needed. Although various RNN structures can be used, in this study we used long short-term memory (LSTM) RNNs. It’s clear that nothing is special in the forward computation.\nThe key is in the training process. We need to assign the correct label to each output layer for each training sample, i.e., deal with the label ambiguity problem, and to make sure the posterior probability for the same speaker is always associated with the same output layer across frames. PIT [19, 20], which is originally designed for speech separation, is extended here to guarantee these properties. More specifically, we minimize the objective function\nJ = 1\nS min s′∈permu(S) ∑ s ∑ t CE(`s ′ t ,O s t ), s = 1, · · · , S (7)\nwhere permu(S) is a permutation of 1, · · · , S. The model is illustrated in Figure 1. We note two important ingredients in this objective function. First, we compute the average CE for each possible assignment of labels, pick the one with minimum CE, and optimize for that assignment. In other words, it automatically finds the appropriate assignment no matter how the labels are ordered. Second, the CE is computed over the whole sequence for each assignment. This forces all the frames\nof the same speaker to be aligned with the same output layer. This strategy elegantly solves the label permutation problem and speaker tracing problem in one shot. Note, the computational cost associated with label assignment is negligible compared to the network forward computation during training, and no label assignment (thus no cost) is needed during evaluation."
    }, {
      "heading" : "4. Experimental Results",
      "text" : "To evaluate the proposed approach, a series of experiments were performed on an artificially mixed AMI corpus, and only twotalker mixed scenario is focused here."
    }, {
      "heading" : "4.1. Experimental data",
      "text" : "The AMI IHM (close-talk) data is used, which contains about 80 hours and 8 hours in training and evaluation sets respectively [27, 28], and the two-talker mixed speech is artificially generated with the sentences in the corpus. For the better and clear definition, we defined high energy (High E) and low energy (Low E) speakers within each two-talker mixed speech, and thus generated five different SNR conditions (i.e. 0dB, 5dB, 10dB, 15dB, 20dB) based on the energy ratio of the two-talkers. We set a rule to make the length of the selected mixed speech pair comparable, so most speech duration in this new corpus is twotalker overlapped. All the utterance-pairs are randomly chosen from two different speakers, and the shorter one will be padded with small noise at the front and end to get the same length as the longer one.\nFigure 2 gives one spectrogram comparison of the original single-talker clean speech and the 0db two-talker mixed-speech in this new AMI corpus. It is observed that there is a huge difference within the two-talker and single-talker spectrograms. Two clean signals are sufficiently overlapped in the mixed speech and hard to separate them from each other."
    }, {
      "heading" : "4.2. Baseline setup",
      "text" : "In this work, all the neural networks were built using the latest Microsoft Cognitive Toolkit (CNTK) [29] and the decoding systems were built based on Kaldi [30]. We first followed the officially released kaldi recipe to build an LDA-MLLT-SAT GMM-HMM model. This model uses 39-dim MFCC feature and has roughly 4K tied-states and 80K Gaussians. We then used this acoustic model to generate the senone alignment for neural network training. We trained the DNN and BLSTMRNN baseline systems with the original AMI IHM data. 80- dimensional log filter bank features with CMVN were used to train the baselines. The DNN has 6 hidden layers each of which contains 2048 Sigmoid neurons. The input feature for DNN contains 11 frames contextual window. The BLSTM-RNN has 3 bidirectional LSTM layers which are followed by the softmax layer. Each BLSTM layer has 512 memory cells. The input to the BLSTM-RNN is a single acoustic frame. All the models explored here are optimized with cross-entropy criterion. The DNN is optimized using SGD method with 256 minibatch size, and the BLSTM-RNN is trained using SGD and BPTT with 4 full-length utterances parallel processing.\nFor decoding, we used a 50K-word dictionary and a trigram language model interpolated from the ones created using the AMI transcripts and the Fisher English corpus. The performance of these two baselines on the original single-speaker AMI corpus are presented in Table 1, and they are still comparable with other works [28] even without using adapted fMLLR feature. It is noted that adding more BLSTM layers did not\nshow substantial WER reduction in the baseline.\nTo test the baseline results on the two-talker mixed speech, the above baseline BLSTM-RNN model is utilized to decode the mixed speech directly. In scoring we compare the decoding outputs with the individual reference of two speakers respectively to obtain two-talkers’ WERs, and the results are illustrated in Table 2. It is observed that the ability of the singlespeaker model is very limited on the multi-talker mixed speech, and there is very large degradation in all conditions. The performance drop is increased very fast with the lower SNR, and the WERs for the low energy speaker even are all around 100.0%. These results demonstrate the huge challenge of the multi-talker speech recognition.\nTable 2: WER (%) of the baseline BLSTM-RNN system on twotalker mixed AMI IHM speech\nSNR Condition High E Spk Low E Spk\n0db 85.0 100.5 5db 68.8 110.2 10db 51.9 114.9 15db 39.3 117.6 20db 32.1 118.7"
    }, {
      "heading" : "4.3. Evaluation on PIT-ASR models",
      "text" : "The experimental results on the proposed PIT-ASR model is described here. All the mixed data under the different SNR conditions are pooled together for training. The individual senone alignments for the two-talkers in each mixed speech utterance are from the single-speaker baseline alignment. For compatibility, the alignment of the shorter utterance within the mixed speech is padded with the silence state at the front and the end. The PIT-ASR model is composed of 4 bidirectional LSTM layers with 768 memory cells in each layer, and 40-dimensional log filter bank feature is used for the PIT-ASR model. The model was trained with 8 parallel utterances in the same minibatch, and the gradient was clipped with the threshold of 0.0003 to guarantee the training stability.\nTwo outputs of the PIT-ASR model are both used in decoding to obtain the hypotheses for two talkers. For scoring, we evaluated the hypotheses on the pairwise score mode against the two references, and made the better WER as the final assignment for each utterance.\nThe results are shown in Table 3. The PIT-ASR model achieves very large gains on both talkers compared with baseline results in Table 2 for all SNR conditions. The degradation increases slowly with the lower SNR for the high energy speaker, and the improvement is huge for the low energy speaker. In 0dB SNR scenario, the performances of two speakers are very close, and obtain 40.0% relative improvement for both high and low energy speakers. In 20dB SNR, the WER of the high energy speaker is still significantly better than the baseline, and even approaches the original single-speaker decoding in Table 1.\nTo give a better understanding on the results comparison,\nTable 3: WER (%) of the propsoed PIT-ASR model on two-talker mixed AMI IHM speech\nSNR Condition High E WER Low E WER\n0db 49.74 56.88 5db 40.31 60.31\n10db 34.38 65.52 15db 31.24 73.04 20db 29.68 80.83\nthe results of one 0db two-talker mixed speech utterance using different models are illustrated in Figure 3 and 4. For the baseline using BLSTM-RNN decoding with the mixed speech directly, the hypotheses are erroneous and most outputs are wrong. In contrast, lots of words can be recognized correctly by the proposed PIT-ASR model for both speakers, and it seems that the PIT-ASR framework can do the speech separation implicitly."
    }, {
      "heading" : "5. Conclusion",
      "text" : "In this paper, we proposed a novel technique for direct recognition of multiple speech streams given the single channel of mixed speech, without first separating them. Our technique is based on permutation invariant training, which was originally developed for separation of multiple speech streams. Our experiments on artificially mixed AMI data showed that the proposed approach is very promising.\nThere are many possible ways to further improve the recognition accuracy. For example, we only explored log filter bank features. It is well known that finer frequency resolution can help better separate speech streams. In addition, we only used acoustic information in this work. Further accuracy improvement can be achieved by feeding language model information back from the decoder to the speech separation component, and by jointly considering all streams of speech when making decoding decision. Although we discussed and evaluated our proposed approach on single channel mixed speech, the technique can be applied to multi-channel condition and can exploit beamforming results to achieve better results."
    }, {
      "heading" : "6. References",
      "text" : "[1] D. Yu and L. Deng, Automatic speech recognition: A deep learn-\ning approach. Springer, 2014.\n[2] D. Yu, L. Deng, and G. E. Dahl, “Roles of pre-training and fine-tuning in context-dependent dbn-hmms for real-world speech recognition,” in NIPS 2010 Workshop on Deep Learning and Unsupervised Feature Learning, 2010.\n[3] G. E. Dahl, D. Yu, L. Deng, and A. Acero, “Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition,” IEEE Transactions on Audio, Speech and Language Processing, vol. 20, no. 1, pp. 30–42, 2012.\n[4] F. Seide, G. Li, and D. Yu, “Conversational speech transcription using context-dependent deep neural networks.” in INTERSPEECH, 2011, pp. 437–440.\n[5] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath et al., “Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,” IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82–97, 2012.\n[6] O. Abdel-Hamid, A.-r. Mohamed, H. Jiang, and G. Penn, “Applying convolutional neural networks concepts to hybrid NN-HMM model for speech recognition,” in ICASSP, 2012, pp. 4277–4280.\n[7] O. Abdel-Hamid, A.-r. Mohamed, H. Jiang, L. Deng, G. Penn, and D. Yu, “Convolutional neural networks for speech recognition,” IEEE/ACM Transactions on Audio, Speech and Language Processing, vol. 22, no. 10, pp. 1533–1545, 2014.\n[8] T. N. Sainath, O. Vinyals, A. Senior, and H. Sak, “Convolutional, long short-term memory, fully connected deep neural networks,” in ICASSP, 2015, pp. 4580–4584.\n[9] M. Bi, Y. Qian, and K. Yu, “Very deep convolutional neural networks for LVCSR,” in INTERSPEECH, 2015.\n[10] Y. Qian, M. Bi, T. Tan, and K. Yu, “Very deep convolutional neural networks for noise robust speech recognition,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 12, pp. 2263–2276, 2016.\n[11] Y. Qian and P. C. Woodland, “Very deep convolutional neural networks for robust speech recognition,” in SLT, 2016, pp. 481–488.\n[12] V. Mitra and H. Franco, “Time-frequency convolutional networks for robust speech recognition,” in ASRU, 2015, pp. 317–323.\n[13] V. Peddinti, D. Povey, and S. Khudanpur, “A time delay neural network architecture for efficient modeling of long temporal contexts,” in INTERSPEECH, 2015.\n[14] T. Sercu, C. Puhrsch, B. Kingsbury, and Y. LeCun, “Very deep multilingual convolutional neural networks for LVCSR,” in ICASSP, 2016.\n[15] D. Amodei, R. Anubhai, E. Battenberg, C. Case, J. Casper, B. Catanzaro, J. Chen, M. Chrzanowski, A. Coates, G. Diamos et al., “Deep speech 2: End-to-end speech recognition in English and Mandarin,” arXiv preprint arXiv:1512.02595, 2015.\n[16] S. Zhang, C. Liu, H. Jiang, S. Wei, L. Dai, and Y. Hu, “Feedforward sequential memory networks: A new structure to learn long-term dependency,” arXiv preprint arXiv:1512.08301, 2015.\n[17] D. Yu, W. Xiong, J. Droppo, A. Stolcke, G. Ye, J. Li, and G. Zweig, “Deep convolutional neural networks with layer-wise context expansion and attention,” in INTERSPEECH, 2016.\n[18] W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, and G. Zweig, “Achieving human parity in conversational speech recognition,” arXiv preprint arXiv:1610.05256, 2016.\n[19] D. Yu, M. Kolbk, Z.-H. Tan, and J. Jensen, “Permutation invariant training of deep models for speaker-independent multi-talker speech separation,” in ICASSP, 2017.\n[20] M. Kolbk, D. Yu, Z.-H. Tan, and J. Jensen, “Multi-talker speech separation and tracing with permutation invariant training of deep recurrent neural networks,” IEEE/ACM Transactions on Audio, Speech and Language Processing, submitted, 2017.\n[21] Z. Ghahramani and M. I. Jordan, “Factorial hidden markov models,” Machine learning, vol. 29, no. 2-3, pp. 245–273, 1997.\n[22] M. Cooke, J. R. Hershey, and S. J. Rennie, “Monaural speech separation and recognition challenge,” Computer Speech and Language, vol. 24, no. 1, pp. 1–15, 2010.\n[23] C. Weng, D. Yu, M. L. Seltzer, and J. Droppo, “Deep neural networks for single-channel multi-talker speech recognition,” IEEE/ACM Transactions on Audio, Speech and Language Processing, vol. 23, no. 10, pp. 1670–1679, 2015.\n[24] J. R. Hershey, Z. Chen, J. L. Roux, and S. Watanabe, “Deep clustering: Discriminative embeddings for segmentation and separation,” in Proc. IEEE Int. Conf. Acoust. Speech Signal Process (ICASSP)., 2016, pp. 31–35.\n[25] Y. Isik, J. L. Roux, Z. Chen, S. Watanabe, and J. R. Hershey, “Single-Channel Multi-Speaker Separation Using Deep Clustering,” in Proc. Annual Conference of International Speech Communication Association (INTERSPEECH), 2016, pp. 545–549.\n[26] Z. Chen, Y. Luo, and N. Mesgarani, “Deep attractor network for single-microphone speaker separation,” in ICASSP, 2017.\n[27] T. Hain, L. Burget, J. Dines, P. N. Garner, F. Grézl, A. E. Hannani, M. Huijbregts, M. Karafiat, M. Lincoln, and V. Wan, “Transcribing meetings with the amida systems,” IEEE Transactions on Audio, Speech, and Language Processing, vol. 20, no. 2, pp. 486– 498, 2012.\n[28] P. Swietojanski, A. Ghoshal, and S. Renals, “Hybrid acoustic models for distant and multichannel large vocabulary speech recognition,” in Proceedings of ASRU, 2013, pp. 285–290.\n[29] D. Yu, A. Eversole, M. Seltzer, K. Yao, Z. Huang, B. Guenter, O. Kuchaiev, Y. Zhang, F. Seide, H. Wang et al., “An introduction to computational networks and the computational network toolkit,” Microsoft Technical Report MSR-TR-2014–112, 2014.\n[30] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz et al., “The kaldi speech recognition toolkit,” in IEEE 2011 workshop on automatic speech recognition and understanding, no. EPFLCONF-192584. IEEE Signal Processing Society, 2011."
    } ],
    "references" : [ {
      "title" : "Automatic speech recognition: A deep learning approach",
      "author" : [ "D. Yu", "L. Deng" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2014
    }, {
      "title" : "Roles of pre-training and fine-tuning in context-dependent dbn-hmms for real-world speech recognition",
      "author" : [ "D. Yu", "L. Deng", "G.E. Dahl" ],
      "venue" : "NIPS 2010 Workshop on Deep Learning and Unsupervised Feature Learning, 2010.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition",
      "author" : [ "G.E. Dahl", "D. Yu", "L. Deng", "A. Acero" ],
      "venue" : "IEEE Transactions on Audio, Speech and Language Processing, vol. 20, no. 1, pp. 30–42, 2012.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Conversational speech transcription using context-dependent deep neural networks.",
      "author" : [ "F. Seide", "G. Li", "D. Yu" ],
      "venue" : "in INTER- SPEECH,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2011
    }, {
      "title" : "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups",
      "author" : [ "G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath" ],
      "venue" : "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82–97, 2012.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Applying convolutional neural networks concepts to hybrid NN-HMM model for speech recognition",
      "author" : [ "O. Abdel-Hamid", "A.-r. Mohamed", "H. Jiang", "G. Penn" ],
      "venue" : "ICASSP, 2012, pp. 4277–4280.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Convolutional neural networks for speech recognition",
      "author" : [ "O. Abdel-Hamid", "A.-r. Mohamed", "H. Jiang", "L. Deng", "G. Penn", "D. Yu" ],
      "venue" : "IEEE/ACM Transactions on Audio, Speech and Language Processing, vol. 22, no. 10, pp. 1533–1545, 2014.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Convolutional, long short-term memory, fully connected deep neural networks",
      "author" : [ "T.N. Sainath", "O. Vinyals", "A. Senior", "H. Sak" ],
      "venue" : "ICASSP, 2015, pp. 4580–4584.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Very deep convolutional neural networks for LVCSR",
      "author" : [ "M. Bi", "Y. Qian", "K. Yu" ],
      "venue" : "INTERSPEECH, 2015.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Very deep convolutional neural networks for noise robust speech recognition",
      "author" : [ "Y. Qian", "M. Bi", "T. Tan", "K. Yu" ],
      "venue" : "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 12, pp. 2263–2276, 2016.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Very deep convolutional neural networks for robust speech recognition",
      "author" : [ "Y. Qian", "P.C. Woodland" ],
      "venue" : "SLT, 2016, pp. 481–488.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Time-frequency convolutional networks for robust speech recognition",
      "author" : [ "V. Mitra", "H. Franco" ],
      "venue" : "ASRU, 2015, pp. 317–323.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A time delay neural network architecture for efficient modeling of long temporal contexts",
      "author" : [ "V. Peddinti", "D. Povey", "S. Khudanpur" ],
      "venue" : "INTERSPEECH, 2015.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Very deep multilingual convolutional neural networks for LVCSR",
      "author" : [ "T. Sercu", "C. Puhrsch", "B. Kingsbury", "Y. LeCun" ],
      "venue" : "ICASSP, 2016.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Deep speech 2: End-to-end speech recognition in English and Mandarin",
      "author" : [ "D. Amodei", "R. Anubhai", "E. Battenberg", "C. Case", "J. Casper", "B. Catanzaro", "J. Chen", "M. Chrzanowski", "A. Coates", "G. Diamos" ],
      "venue" : "arXiv preprint arXiv:1512.02595, 2015.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Feedforward sequential memory networks: A new structure to learn long-term dependency",
      "author" : [ "S. Zhang", "C. Liu", "H. Jiang", "S. Wei", "L. Dai", "Y. Hu" ],
      "venue" : "arXiv preprint arXiv:1512.08301, 2015.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Deep convolutional neural networks with layer-wise context expansion and attention",
      "author" : [ "D. Yu", "W. Xiong", "J. Droppo", "A. Stolcke", "G. Ye", "J. Li", "G. Zweig" ],
      "venue" : "INTERSPEECH, 2016.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Achieving human parity in conversational speech recognition",
      "author" : [ "W. Xiong", "J. Droppo", "X. Huang", "F. Seide", "M. Seltzer", "A. Stolcke", "D. Yu", "G. Zweig" ],
      "venue" : "arXiv preprint arXiv:1610.05256, 2016.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Permutation invariant training of deep models for speaker-independent multi-talker speech separation",
      "author" : [ "D. Yu", "M. Kolbk", "Z.-H. Tan", "J. Jensen" ],
      "venue" : "ICASSP, 2017.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Multi-talker speech separation and tracing with permutation invariant training of deep recurrent neural networks",
      "author" : [ "M. Kolbk", "D. Yu", "Z.-H. Tan", "J. Jensen" ],
      "venue" : "IEEE/ACM Transactions on Audio, Speech and Language Processing, submitted, 2017.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Factorial hidden markov models",
      "author" : [ "Z. Ghahramani", "M.I. Jordan" ],
      "venue" : "Machine learning, vol. 29, no. 2-3, pp. 245–273, 1997.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Monaural speech separation and recognition challenge",
      "author" : [ "M. Cooke", "J.R. Hershey", "S.J. Rennie" ],
      "venue" : "Computer Speech and Language, vol. 24, no. 1, pp. 1–15, 2010.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Deep neural networks for single-channel multi-talker speech recognition",
      "author" : [ "C. Weng", "D. Yu", "M.L. Seltzer", "J. Droppo" ],
      "venue" : "IEEE/ACM Transactions on Audio, Speech and Language Processing, vol. 23, no. 10, pp. 1670–1679, 2015.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Deep clustering: Discriminative embeddings for segmentation and separation",
      "author" : [ "J.R. Hershey", "Z. Chen", "J.L. Roux", "S. Watanabe" ],
      "venue" : "Proc. IEEE Int. Conf. Acoust. Speech Signal Process (ICASSP)., 2016, pp. 31–35.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Single-Channel Multi-Speaker Separation Using Deep Clustering",
      "author" : [ "Y. Isik", "J.L. Roux", "Z. Chen", "S. Watanabe", "J.R. Hershey" ],
      "venue" : "Proc. Annual Conference of International Speech Communication Association (INTERSPEECH), 2016, pp. 545–549.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Deep attractor network for single-microphone speaker separation",
      "author" : [ "Z. Chen", "Y. Luo", "N. Mesgarani" ],
      "venue" : "ICASSP, 2017.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Transcribing meetings with the amida systems",
      "author" : [ "T. Hain", "L. Burget", "J. Dines", "P.N. Garner", "F. Grézl", "A.E. Hannani", "M. Huijbregts", "M. Karafiat", "M. Lincoln", "V. Wan" ],
      "venue" : "IEEE Transactions on Audio, Speech, and Language Processing, vol. 20, no. 2, pp. 486– 498, 2012.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Hybrid acoustic models for distant and multichannel large vocabulary speech recognition",
      "author" : [ "P. Swietojanski", "A. Ghoshal", "S. Renals" ],
      "venue" : "Proceedings of ASRU, 2013, pp. 285–290.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "An introduction to computational networks and the computational network toolkit",
      "author" : [ "D. Yu", "A. Eversole", "M. Seltzer", "K. Yao", "Z. Huang", "B. Guenter", "O. Kuchaiev", "Y. Zhang", "F. Seide", "H. Wang" ],
      "venue" : "Microsoft Technical Report MSR-TR-2014–112, 2014.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "The kaldi speech recognition toolkit",
      "author" : [ "D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz" ],
      "venue" : "IEEE 2011 workshop on automatic speech recognition and understanding, no. EPFL- CONF-192584. IEEE Signal Processing Society, 2011.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Thanks to the significant progresses made in the recent years [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], the ASR systems now surpassed the threshold for adoption in many real-world scenarios and enabled services such as Microsoft Cortana, Apple’s Siri and Google Now, where close-talk microphones are commonly used.",
      "startOffset" : 62,
      "endOffset" : 133
    }, {
      "referenceID" : 1,
      "context" : "Thanks to the significant progresses made in the recent years [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], the ASR systems now surpassed the threshold for adoption in many real-world scenarios and enabled services such as Microsoft Cortana, Apple’s Siri and Google Now, where close-talk microphones are commonly used.",
      "startOffset" : 62,
      "endOffset" : 133
    }, {
      "referenceID" : 2,
      "context" : "Thanks to the significant progresses made in the recent years [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], the ASR systems now surpassed the threshold for adoption in many real-world scenarios and enabled services such as Microsoft Cortana, Apple’s Siri and Google Now, where close-talk microphones are commonly used.",
      "startOffset" : 62,
      "endOffset" : 133
    }, {
      "referenceID" : 3,
      "context" : "Thanks to the significant progresses made in the recent years [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], the ASR systems now surpassed the threshold for adoption in many real-world scenarios and enabled services such as Microsoft Cortana, Apple’s Siri and Google Now, where close-talk microphones are commonly used.",
      "startOffset" : 62,
      "endOffset" : 133
    }, {
      "referenceID" : 4,
      "context" : "Thanks to the significant progresses made in the recent years [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], the ASR systems now surpassed the threshold for adoption in many real-world scenarios and enabled services such as Microsoft Cortana, Apple’s Siri and Google Now, where close-talk microphones are commonly used.",
      "startOffset" : 62,
      "endOffset" : 133
    }, {
      "referenceID" : 5,
      "context" : "Thanks to the significant progresses made in the recent years [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], the ASR systems now surpassed the threshold for adoption in many real-world scenarios and enabled services such as Microsoft Cortana, Apple’s Siri and Google Now, where close-talk microphones are commonly used.",
      "startOffset" : 62,
      "endOffset" : 133
    }, {
      "referenceID" : 6,
      "context" : "Thanks to the significant progresses made in the recent years [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], the ASR systems now surpassed the threshold for adoption in many real-world scenarios and enabled services such as Microsoft Cortana, Apple’s Siri and Google Now, where close-talk microphones are commonly used.",
      "startOffset" : 62,
      "endOffset" : 133
    }, {
      "referenceID" : 7,
      "context" : "Thanks to the significant progresses made in the recent years [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], the ASR systems now surpassed the threshold for adoption in many real-world scenarios and enabled services such as Microsoft Cortana, Apple’s Siri and Google Now, where close-talk microphones are commonly used.",
      "startOffset" : 62,
      "endOffset" : 133
    }, {
      "referenceID" : 8,
      "context" : "Thanks to the significant progresses made in the recent years [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], the ASR systems now surpassed the threshold for adoption in many real-world scenarios and enabled services such as Microsoft Cortana, Apple’s Siri and Google Now, where close-talk microphones are commonly used.",
      "startOffset" : 62,
      "endOffset" : 133
    }, {
      "referenceID" : 9,
      "context" : "Thanks to the significant progresses made in the recent years [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], the ASR systems now surpassed the threshold for adoption in many real-world scenarios and enabled services such as Microsoft Cortana, Apple’s Siri and Google Now, where close-talk microphones are commonly used.",
      "startOffset" : 62,
      "endOffset" : 133
    }, {
      "referenceID" : 10,
      "context" : "Thanks to the significant progresses made in the recent years [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], the ASR systems now surpassed the threshold for adoption in many real-world scenarios and enabled services such as Microsoft Cortana, Apple’s Siri and Google Now, where close-talk microphones are commonly used.",
      "startOffset" : 62,
      "endOffset" : 133
    }, {
      "referenceID" : 11,
      "context" : "Thanks to the significant progresses made in the recent years [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], the ASR systems now surpassed the threshold for adoption in many real-world scenarios and enabled services such as Microsoft Cortana, Apple’s Siri and Google Now, where close-talk microphones are commonly used.",
      "startOffset" : 62,
      "endOffset" : 133
    }, {
      "referenceID" : 12,
      "context" : "Thanks to the significant progresses made in the recent years [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], the ASR systems now surpassed the threshold for adoption in many real-world scenarios and enabled services such as Microsoft Cortana, Apple’s Siri and Google Now, where close-talk microphones are commonly used.",
      "startOffset" : 62,
      "endOffset" : 133
    }, {
      "referenceID" : 13,
      "context" : "Thanks to the significant progresses made in the recent years [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], the ASR systems now surpassed the threshold for adoption in many real-world scenarios and enabled services such as Microsoft Cortana, Apple’s Siri and Google Now, where close-talk microphones are commonly used.",
      "startOffset" : 62,
      "endOffset" : 133
    }, {
      "referenceID" : 14,
      "context" : "Thanks to the significant progresses made in the recent years [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], the ASR systems now surpassed the threshold for adoption in many real-world scenarios and enabled services such as Microsoft Cortana, Apple’s Siri and Google Now, where close-talk microphones are commonly used.",
      "startOffset" : 62,
      "endOffset" : 133
    }, {
      "referenceID" : 15,
      "context" : "Thanks to the significant progresses made in the recent years [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], the ASR systems now surpassed the threshold for adoption in many real-world scenarios and enabled services such as Microsoft Cortana, Apple’s Siri and Google Now, where close-talk microphones are commonly used.",
      "startOffset" : 62,
      "endOffset" : 133
    }, {
      "referenceID" : 16,
      "context" : "Thanks to the significant progresses made in the recent years [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], the ASR systems now surpassed the threshold for adoption in many real-world scenarios and enabled services such as Microsoft Cortana, Apple’s Siri and Google Now, where close-talk microphones are commonly used.",
      "startOffset" : 62,
      "endOffset" : 133
    }, {
      "referenceID" : 17,
      "context" : "Thanks to the significant progresses made in the recent years [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], the ASR systems now surpassed the threshold for adoption in many real-world scenarios and enabled services such as Microsoft Cortana, Apple’s Siri and Google Now, where close-talk microphones are commonly used.",
      "startOffset" : 62,
      "endOffset" : 133
    }, {
      "referenceID" : 18,
      "context" : "Thanks to the significant progresses made in the recent years [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], the ASR systems now surpassed the threshold for adoption in many real-world scenarios and enabled services such as Microsoft Cortana, Apple’s Siri and Google Now, where close-talk microphones are commonly used.",
      "startOffset" : 62,
      "endOffset" : 133
    }, {
      "referenceID" : 19,
      "context" : "Thanks to the significant progresses made in the recent years [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], the ASR systems now surpassed the threshold for adoption in many real-world scenarios and enabled services such as Microsoft Cortana, Apple’s Siri and Google Now, where close-talk microphones are commonly used.",
      "startOffset" : 62,
      "endOffset" : 133
    }, {
      "referenceID" : 20,
      "context" : "Before the deep learning era, the most famous and effective model is the factorial GMM-HMM [21], which outperformed human in the 2006 monaural speech separation and recognition challenge [22].",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 21,
      "context" : "Before the deep learning era, the most famous and effective model is the factorial GMM-HMM [21], which outperformed human in the 2006 monaural speech separation and recognition challenge [22].",
      "startOffset" : 187,
      "endOffset" : 191
    }, {
      "referenceID" : 22,
      "context" : "[23, 24, 25, 26, 19, 20].",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 23,
      "context" : "[23, 24, 25, 26, 19, 20].",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 24,
      "context" : "[23, 24, 25, 26, 19, 20].",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 25,
      "context" : "[23, 24, 25, 26, 19, 20].",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 18,
      "context" : "[23, 24, 25, 26, 19, 20].",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 19,
      "context" : "[23, 24, 25, 26, 19, 20].",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 22,
      "context" : "[23] a deep learning model was developed to recognize the mixed speech directly.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "[24, 25] the multi-talker mixed speech is first separated into multiple streams.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 24,
      "context" : "[24, 25] the multi-talker mixed speech is first separated into multiple streams.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 25,
      "context" : "[26] proposed a similar technique called deep attractor network (DANet).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "[19] and Kolbak et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "[20], a simpler yet equally effective technique named permutation invariant training (PIT) was proposed to attack the speaker independent multi-talker speech separation problem.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "Different from [19, 20], we define PIT over the cross entropy (CE) between the true and estimated senone posterior probabilities.",
      "startOffset" : 15,
      "endOffset" : 23
    }, {
      "referenceID" : 19,
      "context" : "Different from [19, 20], we define PIT over the cross entropy (CE) between the true and estimated senone posterior probabilities.",
      "startOffset" : 15,
      "endOffset" : 23
    }, {
      "referenceID" : 18,
      "context" : "Interested readers can find additional information in [19, 20] on how training progresses to nowhere when the conventional supervised approach is used for the multi-talker speech separation.",
      "startOffset" : 54,
      "endOffset" : 62
    }, {
      "referenceID" : 19,
      "context" : "Interested readers can find additional information in [19, 20] on how training progresses to nowhere when the conventional supervised approach is used for the multi-talker speech separation.",
      "startOffset" : 54,
      "endOffset" : 62
    }, {
      "referenceID" : 18,
      "context" : "To address the label ambiguity problem, we propose a novel model based on the permutation invariant training (PIT) [19, 20].",
      "startOffset" : 115,
      "endOffset" : 123
    }, {
      "referenceID" : 19,
      "context" : "To address the label ambiguity problem, we propose a novel model based on the permutation invariant training (PIT) [19, 20].",
      "startOffset" : 115,
      "endOffset" : 123
    }, {
      "referenceID" : 23,
      "context" : "Note that, DPCL [24, 25] and DANet [26] are alternative solutions to the label ambiguity problem when the goal is speech source separation.",
      "startOffset" : 16,
      "endOffset" : 24
    }, {
      "referenceID" : 24,
      "context" : "Note that, DPCL [24, 25] and DANet [26] are alternative solutions to the label ambiguity problem when the goal is speech source separation.",
      "startOffset" : 16,
      "endOffset" : 24
    }, {
      "referenceID" : 25,
      "context" : "Note that, DPCL [24, 25] and DANet [26] are alternative solutions to the label ambiguity problem when the goal is speech source separation.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 18,
      "context" : "PIT [19, 20], which is originally designed for speech separation, is extended here to guarantee these properties.",
      "startOffset" : 4,
      "endOffset" : 12
    }, {
      "referenceID" : 19,
      "context" : "PIT [19, 20], which is originally designed for speech separation, is extended here to guarantee these properties.",
      "startOffset" : 4,
      "endOffset" : 12
    }, {
      "referenceID" : 26,
      "context" : "The AMI IHM (close-talk) data is used, which contains about 80 hours and 8 hours in training and evaluation sets respectively [27, 28], and the two-talker mixed speech is artificially generated with the sentences in the corpus.",
      "startOffset" : 126,
      "endOffset" : 134
    }, {
      "referenceID" : 27,
      "context" : "The AMI IHM (close-talk) data is used, which contains about 80 hours and 8 hours in training and evaluation sets respectively [27, 28], and the two-talker mixed speech is artificially generated with the sentences in the corpus.",
      "startOffset" : 126,
      "endOffset" : 134
    }, {
      "referenceID" : 28,
      "context" : "In this work, all the neural networks were built using the latest Microsoft Cognitive Toolkit (CNTK) [29] and the decoding systems were built based on Kaldi [30].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 29,
      "context" : "In this work, all the neural networks were built using the latest Microsoft Cognitive Toolkit (CNTK) [29] and the decoding systems were built based on Kaldi [30].",
      "startOffset" : 157,
      "endOffset" : 161
    }, {
      "referenceID" : 27,
      "context" : "The performance of these two baselines on the original single-speaker AMI corpus are presented in Table 1, and they are still comparable with other works [28] even without using adapted fMLLR feature.",
      "startOffset" : 154,
      "endOffset" : 158
    } ],
    "year" : 2017,
    "abstractText" : "In this paper, we propose a novel technique for direct recognition of multiple speech streams given the single channel of mixed speech, without first separating them. Our technique is based on permutation invariant training (PIT) for automatic speech recognition (ASR). In PIT-ASR, we compute the average cross entropy (CE) over all frames in the whole utterance for each possible output-target assignment, pick the one with the minimum CE, and optimize for that assignment. PIT-ASR forces all the frames of the same speaker to be aligned with the same output layer. This strategy elegantly solves the label permutation problem and speaker tracing problem in one shot. Our experiments on artificially mixed AMI data showed that the proposed approach is very promising.",
    "creator" : "LaTeX with hyperref package"
  }
}