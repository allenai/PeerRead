{
  "name" : "1610.09269.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Hierarchical Clustering via Spreading Metrics",
    "authors" : [ "Aurko Roy", "Sebastian Pokutta" ],
    "emails" : [ "aurko@gatech.edu", "sebastian.pokutta@isye.gatech.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "( log3/2 n ) times the cost of the optimal solution. We improve this by giving an O(log n)-\napproximation algorithm for this problem. Our main technical ingredients are a combinatorial characterization of ultrametrics induced by this cost function, deriving an Integer Linear Programming (ILP) formulation for this family of ultrametrics, and showing how to iteratively round an LP relaxation of this formulation by using the idea of sphere growing which has been extensively used in the context of graph partitioning. We also prove that our algorithm returns an O(log n)-approximate hierarchical clustering for a generalization of this cost function also studied in [Dasgupta, 2016]. Experiments show that the hierarchies found by using the ILP formulation as well as our rounding algorithm often have better projections into flat clusters than the standard linkage based algorithms. We conclude with constant factor inapproximability results for this problem: 1) no polynomial size LP or SDP can achieve a constant factor approximation for this problem and 2) no polynomial time algorithm can achieve a constant factor approximation under the assumption of the Small Set Expansion hypothesis."
    }, {
      "heading" : "1 Introduction",
      "text" : "Hierarchical clustering is an important method in cluster analysis where a data set is recursively partitioned into clusters of successively smaller size. They are typically represented by rooted trees where the root corresponds to the entire data set, the leaves correspond to individual data points and the intermediate nodes correspond to a cluster of its descendant leaves. Such a hierarchy represents several possible flat clusterings of the data at various levels of granularity; indeed every pruning of this tree returns a possible clustering. Therefore in situations where the number of desired clusters is not known beforehand, a hierarchical clustering scheme is often preferred to flat clustering. The most popular algorithms for hierarchical clustering are bottoms-up agglomerative algorithms like single linkage, average linkage and complete linkage. In terms of theoretical guarantees these algorithms are known to correctly recover a ground truth clustering if the similarity function on the data satisfies corresponding stability properties (see, e.g., [Balcan et al., 2008]). Often, however, one wishes to think of a good\nar X\niv :1\n61 0.\n09 26\n9v 1\n[ cs\n.L G\n] 2\n8 O\nclustering as optimizing some kind of cost function rather than recovering a hidden “ground truth”. This is the standard approach in the classical clustering setting where popular objectives are k-means, k-median, minsum and k-center (see Chapter 14, [Friedman et al., 2001]). However as pointed out by [Dasgupta, 2016] for a lot of popular hierarchical clustering algorithms including linkage based algorithms, it is hard to pinpoint explicitly the cost function that these algorithms are optimizing. Moreover, much of the existing cost function based approaches towards hierarchical clustering evaluate a hierarchy based on a cost function for flat clustering, e.g., assigning the k-means or k-median cost to a pruning of this tree. Motivated by this, [Dasgupta, 2016] introduced a cost function for hierarchical clustering where the cost takes into account the entire structure of the tree rather than just the projections into flat clusterings. This cost function is shown to recover the intuitively correct hierarchies on several synthetic examples like planted partitions and cliques. In addition, a top-down graph partitioning algorithm is presented that outputs a tree with cost at most O(αn log n) times the cost of the optimal tree and where αn is the approximation guarantee of the Sparsest Cut subroutine used. Thus using the Leighton-Rao algorithm [Leighton and Rao, 1988, Leighton and Rao, 1999] or the AroraRao-Vazirani algorithm [Arora et al., 2009] gives an approximation factor of O ( log2 n ) and O ( log3/2 n\n) respectively. In this work we give a polynomial time algorithm to recover a hierarchical clustering of cost at most O(log n) times the cost of the optimal clustering according to this cost function. We also analyze a generalization of this cost function studied by [Dasgupta, 2016] and show that our algorithm still gives an O(log n) approximation in this setting. We do this by viewing the cost function in terms of the ultrametric it induces on the data, writing a convex relaxation for it and concluding by analyzing a popular rounding scheme used in graph partitioning algorithms. We also implement the integer program, its LP relaxation, and the rounding algorithm and test it on some synthetic and real world data sets to compare the cost of the rounded solutions to the true optimum as well as to compare its performance to other hierarchical clustering algorithms used in practice. Our experiments suggest that the hierarchies found by this algorithm are often better than the ones found by linkage based algorithms as well as the k-means algorithm in terms of the error of the best pruning of the tree compared to the ground truth."
    }, {
      "heading" : "1.1 Related Work",
      "text" : "The immediate precursor to this work is [Dasgupta, 2016] where the cost function for evaluating a hierarchical clustering was introduced. Prior to this there has been a long line of research on hierarchical clustering in the context of phylogenetics and taxonomy (see, e.g., [Jardine and Sibson, 1971, Sneath et al., 1973, Felsenstein and Felenstein, 2004]). Several authors have also given theoretical justifications for the success of the popular linkage based algorithms for hierarchical clustering (see, e.g. [Jardine and Sibson, 1968, Zadeh and Ben-David, 2009, Ackerman et al., 2010]). In terms of cost functions, one approach has been to evaluate a hierarchy in terms of the k-means or k-median cost that it induces (see [Dasgupta and Long, 2005]). The cost function and the top-down algorithm in [Dasgupta, 2016] can also be seen as a theoretical justification for several graph partitioning heuristics that are used in practice. Besides this prior work on hierarchical clustering we are also motivated by the long line of work in the classical clustering setting where a popular strategy is to study convex relaxations of these problems and to round an optimal fractional solution into an integral one with the aim of getting a good approximation to the cost function. A long line of work (see, e.g., [Charikar et al., 1999, Jain and Vazirani, 2001, Jain et al., 2003, Charikar and Li, 2012]) has employed this approach on LP relaxations for the k-median problem, including [Li and Svensson, 2013] which gives the best known approximation factor of 1 + √ 3 + ε. Similarly, a few authors have studied LP and SDP relaxations for the k-means problem (see, e.g., [Peng and Xia, 2005, Peng and Wei, 2007, Awasthi et al., 2015]), while one of the best known algorithms for kernel k-means and spectral clustering is due to [Recht et al., 2012] which approximates the nonnegative matrix factorization (NMF) problem by LPs.\nLP relaxations for hierarchical clustering have also been studied in [Ailon and Charikar, 2005] where the objective is to fit a tree metric to a data set given pairwise dissimilarities. While the LP relaxation and rounding algorithm in [Ailon and Charikar, 2005] is similar in flavor, the result is incomparable to ours (see Section 7 for a discussion). Another work that is indirectly related to our approach is [Di Summa et al., 2015] where the authors study an ILP to obtain a closest ultrametric to arbitrary functions on a discrete set. Our approach is to give a combinatorial characterization of the ultrametrics induced by the cost function of [Dasgupta, 2016] which allows us to use the tools from [Di Summa et al., 2015] to model the problem as an ILP. The natural LP relaxation of this ILP turns out to be closely related to LP relaxations considered before for several graph partitioning problems (see, e.g., [Leighton and Rao, 1988, Leighton and Rao, 1999, Even et al., 1999, Krauthgamer et al., 2009]) and we use a rounding technique studied in this context to round this LP relaxation. Recently, we became aware of independent work by [Charikar and Chatziafratis, 2016] obtaining similar results for hierarchical clustering. In particular [Charikar and Chatziafratis, 2016] improve the approximation factor to O (√ log n ) by showing how to round a spreading metric SDP relaxation for this cost function. The analysis of this rounding procedure also enabled them to show that the top-down heuristic of [Dasgupta, 2016] actually returns an O( √ log n) approximate clustering rather than an O ( log3/2 n\n) approximate clustering. They also analyzed a very similar LP relaxation using the divide-and-conquer approximation algorithms using spreading metrics paradigm of [Even et al., 2000] together with a result of [Bartal, 2004] to show an O(log n) approximation. Finally, they also gave similar constant factor inapproximability results for this problem."
    }, {
      "heading" : "1.2 Contribution",
      "text" : "While studying convex relaxations of optimization problems is fairly natural, for the cost function introduced in [Dasgupta, 2016] however, it is not immediately clear how one would go about writing such a relaxation. Our first contribution is to give a combinatorial characterization of the family of ultrametrics induced by this cost function on hierarchies. Inspired by the approach in [Di Summa et al., 2015] where the authors study an integer linear program for finding the closest ultrametric, we are able to formulate the problem of finding the minimum cost hierarchical clustering as an integer linear program. Interestingly and perhaps unsurprisingly, the specific family of ultrametrics induced by this cost function give rise to linear constraints studied before in the context of finding balanced separators in weighted graphs. We then show how to round an optimal fractional solution using the sphere growing technique first introduced in [Leighton and Rao, 1988] (see also [Garg et al., 1996, Even et al., 1999, Charikar et al., 2003]) to recover a tree of cost at most O(log n) times the optimal tree for this cost function. The generalization of this cost function involves scaling every pairwise distances by an arbitrary strictly increasing function f satisfying f (0) = 0. We modify the integer linear program for this general case and show that the rounding algorithm still finds a hierarchical clustering of cost at most O(log n) times the optimal clustering in this setting. We also show a constant factor inapproximability result for this problem for any polynomial sized LP and SDP relaxations and under the assumption of the Small Set Expansion hypothesis. We conclude with an experimental study of the integer linear program and the rounding algorithm on some synthetic and real world data sets to show that the approximation algorithm often recovers clusters close to the true optimum (according to this cost function) and that its projections into flat clusters often has a better error rate than the linkage based algorithms and the k-means algorithm."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "A similarity based clustering problem consists of a dataset V of n points and a similarity function κ : V × V → R≥0 such that κ(i, j) is a measure of the similarity between i and j for any i, j ∈ V. We will assume that the similarity function is symmetric i.e., κ(i, j) = κ(j, i) for every i, j ∈ V. Note that we do not make any assumptions about the points in V coming from an underlying metric space. For a given instance of a clustering problem we have an associated weighted complete graph Kn with vertex set V and weight function\ngiven by κ. A hierarchical clustering of V is a tree T with a designated root r and with the elements of V as its leaves, i.e., leaves(T) = V. For any set S ⊆ V we denote the lowest common ancestor of S in T by lca(S). For pairs of points i, j ∈ V wewill abuse the notation for the sake of simplicity and denote lca({i, j}) simply by lca(i, j). For a node v of T we denote the subtree of T rooted at v by T[v]. The following cost function was introduced by [Dasgupta, 2016] to measure the quality of the hierarchical clustering T\ncost(T) := ∑ {i,j}∈E(Kn) κ(i, j) |leaves(T[lca(i, j)])| . (1)\nThe intuition behind this cost function is as follows. Let T be a hierarchical clustering with designated root r so that r represents the whole data set V. Since leaves(T) = V, every internal node v ∈ T represents a cluster of its descendant leaves, with the leaves themselves representing singleton clusters of V. Starting from r and going down the tree, every distinct pair of points i, j ∈ V will be eventually separated at the leaves. If κ(i, j) is large, i.e., i and j are very similar to each other then we would like them to be separated as far down the tree as possible if T is a good clustering of V. This is enforced in the cost function (1): if κ(i, j) is large then the number of leaves of lca(i, j) should be small i.e., lca(i, j) should be far from the root r of T. Such a cost function is not unique however; see Section 7 for some other cost functions of a similar flavor. Note that while requiring κ to be non-negative might seem like an artificial restriction, cost function (1) breaks down when all the κ(i, j) < 0, since in this case the trivial clustering r, T∗ where T∗ is the star graph with V as its leaves is always the minimizer. Therefore in the rest of this work we will assume that κ ≥ 0. This is not a restriction compared to [Dasgupta, 2016], since the Sparsest Cut algorithm used as a subroutine also requires this assumption. Let us now briefly recall the notion of an ultrametric.\nDefinition 2.1 (Ultrametric). An ultrametric on a set X of points is a distance function d : X × X → R satisfying the following properties for every x, y, z ∈ X\n1. Nonnegativity: d(x, y) ≥ 0 with d(x, y) = 0 iff x = y\n2. Symmetry: d(x, y) = d(y, x)\n3. Strong triangle inequality: d(x, y) ≤ max{d(y, z), d(z, x)}\nUnder the cost function (1), one can interpret the tree T as inducing an ultrametric dT on V given by dT(i, j) := |leaves(T[lca (i, j)])| − 1. This is an ultrametric since dT(i, j) = 0 iff i = j and for any triple i, j, k ∈ V we have dT(i, j) ≤ max{dT(i, k), dT(j, k)}. The following definition introduces the notion of non-trivial ultrametrics. These turn out to be precisely the ultrametrics that are induced by tree decompositions of V corresponding to cost function (1), as we will show in Corollary 3.4.\nDefinition 2.2. An ultrametric d on a set of points V is non-trivial if the following conditions hold.\n1. For every non-empty set S ⊆ V, there is a pair of points i, j ∈ S such that d(i, j) ≥ |S| − 1.\n2. For any t if St is an equivalence class ofV under the relation i ∼ j iff d(i, j) ≤ t, thenmaxi,j∈St d(i, j) ≤ |St| − 1.\nNote that for an equivalence class St where d(i, j) ≤ t for every i, j ∈ St it follows from Condition 1 that t ≥ |St| − 1. Thus in the case when t = |St| − 1 the two conditions imply that the maximum distance between any two points in S is t and that there is a pair i, j ∈ S for which this maximum is attained. The following lemma shows that non-trivial ultrametrics behave well under restrictions to equivalence classes St of the form i ∼ j iff d(i, j) ≤ t.\nLemma 2.3. Let d be a non-trivial ultrametric on V and let St ⊆ V be an equivalence class under the relation i ∼ j iff d(i, j) ≤ t. Then d restricted to St is a non-trivial ultrametric on St.\nProof. Clearly d restricted to St is an ultrametric on St and sowe need to establish that it satisfies Conditions 1 and 2 of Definition 2.2. Let S ⊆ St be any set. Since d is a non-trivial ultrametric on V it follows that there is a pair i, j ∈ S with d(i, j) ≥ |S| − 1, and so d restricted to St satisfies Condition 1. If S′r is an equivalence class in St under the relation i ∼ j iff d(i, j) ≤ r then clearly S′r = St if r > t. Since d is a non-trivial ultrametric on V, it follows that maxi,j∈S′r d(i, j) = maxi,j∈St d(i, j) ≤ |St| − 1 = |S′r| − 1. Thus we may assume that r ≤ t. Consider an i ∈ S′r and let j ∈ V be such that d(i, j) ≤ r. Since r ≤ t and i ∈ St, it follows that j ∈ St and so j ∈ S′r. In other words S′r is an equivalence class in V under the relation i ∼ j iff d(i, j) ≤ r. Since d is an ultrametric on V it follows that maxi,j∈S′r d(i, j) ≤ |S′r| − 1. Thus d restricted to St satisfies Condition 2.\nThe intuition behind the two conditions in Definition 2.2 is as follows. Condition 1 imposes a certain lower bound by ruling out trivial ultrametrics where, e.g., d(i, j) = 1 for every distinct pair i, j ∈ V. On the other hand Condition 2 discretizes and imposes an upper bound on d by restricting its range to the set {0, 1, . . . , n− 1} (see Lemma 2.4). This rules out the other spectrum of triviality where for example d(i, j) = n for every distinct pair i, j ∈ V with |V| = n.\nLemma 2.4. Let d be a non-trivial ultrametric on the set V as in Definition 2.2. Then the range of d is contained in the set {0, 1, . . . , n− 1} with |V| = n.\nProof. We will prove this by induction on |V|. The base case when |V| = 1 is trivial. Therefore, we now assume that |V| > 1. By Condition 1 there is a pair i, j ∈ V such that d(i, j) ≥ n − 1. Let t = maxi,j∈V d(i, j), then the only equivalence class under the relation i ∼ j iff d(i, j) ≤ t is V. By Condition 2 it follows that maxi,j∈V d(i, j) = t = n− 1. Let V1, . . . Vm denote the set of equivalence classes of V under the relation i ∼ j iff d(i, j) ≤ n − 2. Note that m > 1 as there is a pair i, j ∈ V with d(i, j) = n − 1, and therefore each Vl ( V. By Lemma 2.3, d restricted to each of these Vi’s is a non-trivial ultrametric on those sets. The claim then follows immediately: for any i, j ∈ V either i, j ∈ Vl for some Vl in which case by the induction hypothesis d(i, j) ∈ {0, 1, . . . , |Vl | − 1}, or i ∈ Vl and j ∈ Vl′ for l 6= l′ in which case d(i, j) = n− 1."
    }, {
      "heading" : "3 Ultrametrics and Hierarchical Clusterings",
      "text" : "We start with the following easy lemma about the lowest common ancestors of subsets of V in a hierarchical clustering T of V.\nLemma 3.1. Let S ⊆ V with |S| ≥ 2. If r = lca(S) then there is a pair i, j ∈ S such that lca(i, j) = r.\nProof. We will proceed by induction on |S|. If |S| = 2 then the claim is trivial and so we may assume |S| > 2. Let i ∈ S be an arbitrary point and let r′ = lca(S \\ {i}). We claim that r = lca(i, r′). Clearly the subtree rooted at lca(i, r′) contains S and since T[r] is the smallest such tree it follows that r ∈ T[lca(i, r′)]. Conversely, T[r] contains S \\ {i} and so r′ ∈ T[r] and since i ∈ T[r], it follows that lca(i, r′) ∈ T[r]. Thus we conclude that r = lca(i, r′). If lca(i, r′) = r′, then we are done by the induction hypothesis. Thus we may assume that i /∈ T[r′]. Consider any j ∈ S such that j ∈ T[r′]. Then we have that lca(i, j) = r as lca(i, r′) = r and j ∈ T[r′] and i /∈ T[r′].\nWe will now show that non-trivial ultrametrics on V as in Definition 2.2 are exactly those that are induced by hierarchical clusterings on V under cost function (1). The following lemma shows the forward direction: the ultrametric dT induced by any hierarchical clustering T is non-trivial.\nLemma 3.2. Let T be a hierarchical clustering on V and let dT be the ultrametric on V induced by it. Then dT is non-trivial.\nProof. Let S ⊆ V be arbitrary and r = lca(S), then T[r] has at least |S| leaves. By Lemma 3.1 there must be a pair i, j ∈ S such that r = lca(i, j) and so dT(i, j) ≥ |S| − 1. This satisfies Condition 1 of non-triviality. For any t, let St be a non-empty equivalence class under the relation i ∼ j iff dT(i, j) ≤ t. Since dT satisfies Condition 1 it follows that |St| − 1 ≤ t. Let us assume for the sake of contradiction that there is a pair i, j ∈ St such that dT(i, j) > |St| − 1. Let r = lca(St); using the definition of dT it follows that t + 1 ≥ |leaves (T[r])| > |St| since i, j ∈ St. Let k ∈ leaves (T[r]) \\ St be an arbitrary point, then for every l ∈ St it follows that dT(k, l) ≤ |leaves(T[r])| − 1 ≤ t since the subtree rooted at r contains both k and l. This is a contradiction to St being an equivalence class under i ∼ j iff dT(i, j) ≤ t since k /∈ St. Thus dT also satisfies Condition 2 of Definition 2.2.\nThe following crucial lemma shows the converse: every non-trivial ultrametric on V is realized by a hierarchical clustering T of V.\nLemma 3.3. For every non-trivial ultrametric d on V there is a hierarchical clustering T on V such that for any pair i, j ∈ V we have\ndT(i, j) = |leaves(T[lca (i, j)])| − 1 = d(i, j).\nMoreover this hierarchy can be constructed in time O ( n3 ) by Algorithm 1 where |V| = n.\nProof. The proof is by induction on n. The base case when n = 1 is straightforward. We now suppose that the statement is true for sets of size < n. Note that i ∼ j iff d(i, j) ≤ n− 2 is an equivalence relation on V and thus partitions V into m equivalence classes V1, . . . , Vm. We first observe that m > 1 since by Condition 1 there is a pair of points i, j ∈ V such that d(i, j) ≥ n− 1 and in particular |V|l < n for every l ∈ {1, . . . , m}. By Lemma 2.3, d restricted to any Vl is a non-trivial ultrametric on Vl and there is a pair of points i, j ∈ Vl such that d(i, j) = |Vl | − 1 by Conditions 1 and 2. Therefore by the induction hypothesis we construct trees T1, . . . , Tm such that for every l ∈ {1, . . . , m} we have leaves(Tl) = Vl . Further for any pair of points i, j ∈ Vl for some l ∈ {1, . . . , m}, we also have d(i, j) = dTl (i, j). We construct the tree T as follows: we first add a root r and then connect the root rl of Tl to r for every l ∈ {1, . . . , m}. Consider a pair of points i, j ∈ V. If i, j ∈ Vl for some l ∈ {1, . . . , m} then we are done since dTl (i, j) = dT(i, j) as lca(i, j) ∈ Tl . If i ∈ Vl and j ∈ Vl′ for some l 6= l′ then d(i, j) = n − 1 since d(i, j) ≥ n− 1 by definition of the equivalence relation and the range of d lies in {0, 1, . . . , n− 1} by Lemma 2.4. Moreover i and j are leaves in Tl and Tl′ respectively, and thus by construction of T we have lca(i, j) = r, i.e., dT(i, j) = n− 1 and so the claim follows. Algorithm 1 simulates this inductive argument can be easily implemented to run in time O ( n3 ) .\nLemmas 3.2 and 3.3 together imply the following corollary about the equivalence of hierarchical clusterings and non-trivial ultrametrics.\nCorollary 3.4. There is a bijection between the set of hierarchical clusterings T on V and the set of nontrivial ultrametrics d on V satisfying the following conditions.\n1. For every hierarchical clustering T on V, there is a non-trivial ultrametric dT defined as dT(i, j) := |leaves T[lca(i, j)]| − 1 for every i, j ∈ V.\n2. For every non-trivial ultrametric d on V, there is a hierarchical clustering T on V such that for every i, j ∈ V we have |leaves T[lca(i, j)]| − 1 = d(i, j).\nMoreover this bijection can be computed in O(n3) time, where |V| = n.\nInput: Data set V of n points, non-trivial ultrametric d : V ×V → R≥0 Output: Hierarchical clustering T of V with root r\n1 r ← arbitrary choice of designated root in V 2 X ← {r} 3 E← ∅ 4 if n = 1 then 5 T ← (X, E) 6 return r, T 7 else 8 Partition V into {V1, . . . Vm} under the equivalence relation i ∼ j iff d(i, j) < n− 1 9 for l ∈ {1, . . . , m} do\n10 Let rl , Tl be output of Algorithm 1 on Vl , d|Vl 11 X ← X ∪V(Tl) 12 E← E ∪ {r, rl} 13 end 14 T ← (X, E) 15 return r, T 16 end\nAlgorithm 1: Hierarchical clustering of V from non-trivial ultrametric\nTherefore to find the hierarchical clustering of minimum cost, it suffices to minimize 〈κ, d〉 over non-trivial ultrametrics d : V × V → {0, . . . , n − 1}, where V is the data set. Note that the cost of the ultrametric dT corresponding to a tree T is an affine offset of cost(T). In particular, we have 〈κ, dT〉 = cost(T) − ∑{i,j}∈E(Kn) κ(i, j). A natural approach is to formulate this problem as an Integer Linear Program (ILP) and then study LP or SDP relaxations of it. We consider the following ILP for this problem that ismotivated by [Di Summa et al., 2015]. We have the variables x1ij, . . . , x n−1 ij for every distinct pair i, j ∈ V with xtij = 1 if and only if d(i, j) ≥ t. For any positive integer n, let [n] := {1, 2, . . . , n}.\nmin n−1 ∑ t=1 ∑ {i,j}∈E(Kn) κ(i, j)xtij (ILP-ultrametric)\ns.t. xtij ≥ xt+1ij ∀i, j ∈ V, t ∈ [n− 2] (2) xtij + x t jk ≥ xtik ∀i, j, k ∈ V, t ∈ [n− 1] (3)\n∑ i,j∈S xtij ≥ 2 ∀t ∈ [n− 1], S ⊆ V, |S| = t + 1 (4)\n∑ i,j∈S\nx|S|ij ≤ |S| ∑ i,j∈S\nxtij + ∑ i∈S j/∈S\n( 1− xtij ) ∀t ∈ [n− 1], S ⊆ V (5) xtij = x t ji ∀i, j ∈ V, t ∈ [n− 1] (6) xtii = 0 ∀i ∈ V, t ∈ [n− 1] (7) xtij ∈ {0, 1} ∀i, j ∈ V, t ∈ [n− 1] (8)\nConstraints (2) and (7) follow from the interpretation of the variables xtij: if d(i, j) ≥ t, i.e., xtij = 1 then\nclearly d(i, j) ≥ t− 1 and so xt−1ij = 1. Furthermore, for any i ∈ V we have d(i, i) = 0 and so xtii = 0 for every t ∈ [n− 1]. Note that constraint (3) is the same as the strong triangle inequality (Definition 2.1) since the variables xtij are in {0, 1}. Constraint 6 ensures that the ultrametric is symmetric. Constraint 4 ensures the ultrametric satisfies Condition 1 of non-triviality: for every S ⊆ V of size t + 1 we know that there must be points i, j ∈ S such that d(i, j) = d(j, i) ≥ t or in other words xtij = xtji = 1. Constraint 5 ensures that the ultrametric satisfies Condition 2 of non-triviality. To see this note that the constraint is active only when ∑i,j∈S xtij = 0 and ∑i∈S,j/∈S(1− xtij) = 0. In other words d(i, j) ≤ t− 1 for every i, j ∈ S and S is a maximal such set since if i ∈ S and j /∈ S then d(i, j) ≥ t. Thus S is an equivalence class under the relation i ∼ j iff d(i, j) ≤ t− 1 and so for every i, j ∈ S we have d(i, j) ≤ |S| − 1 or equivalently x|S|ij = 0. The ultrametric d represented by a feasible solution xtij is given by d(i, j) = ∑ n−1 t=1 x t ij.\nDefinition 3.5. For any { xtij | t ∈ [n− 1], i, j ∈ V } let Et be defined as Et := { {i, j} | xtij = 0 } . Note that if xtij is feasible for ILP-ultrametric then Et ⊆ Et+1 for any t since xtij ≥ xt+1ij . The sets {Et}n−1t=1 induce a natural sequence of graphs {Gt}n−1t=1 where Gt = (V, Et) with V being the data set. For a fixed t ∈ {1, . . . , n − 1} it is instructive to study the combinatorial properties of the so called layer-t problem, where we restrict ourselves to the constraints corresponding to that particular t and drop constraints (2) and (5) since they involve different layers in their expression.\nmin ∑ {i,j}∈E(Kn) κ(i, j)xtij (ILP-layer)\ns.t. xtij + x t jk ≥ xtik ∀i, j, k ∈ V (9)\n∑ i,j∈S xtij ≥ 2 ∀S ⊆ V, |S| = t + 1 (10)\nxtij = x t ji ∀i, j ∈ V (11) xtii = 0 ∀i ∈ V (12) xtij ∈ {0, 1} ∀i, j ∈ V (13)\nThe following lemma provides a combinatorial characterization of feasible solutions to the layer-t problem.\nLemma 3.6. Let Gt = (V, Et) be the graph as in Definition 3.5 corresponding to a solution xtij to the layer-t problem ILP-layer. Then Gt is a disjoint union of cliques of size ≤ t. Moreover this exactly characterizes all feasible solutions of ILP-layer.\nProof. We first note that Gt = (V, Et)must be a disjoint union of cliques since if {i, j} ∈ Et and {j, k} ∈ Et then {i, k} ∈ Et since xtik ≤ xtij + xtjk = 0 due to constraint (9). Suppose there is a clique in Gt of size > t. Choose a subset S of this clique of size t + 1. Then ∑i,j∈S xtij = 0 which violates constraint (10). Conversely, let Et be a subset of edges such that Gt = (V, Et) is a disjoint union of cliques of size≤ t. Let xtij = 0 if {i, j} ∈ Et and 1 otherwise. Clearly xtij = xtji by definition. Suppose xtij violates constraint (9), so that there is a pair i, j, k ∈ V such that xtik = 1 but xtij = xtjk = 0. However this implies that Gt is not a disjoint union of cliques since {i, j}, {j, k} ∈ Et but {i, k} /∈ Et. Suppose xtij violates constraint (10) for some set S of size t + 1. Therefore for every i, j ∈ S, we have xtij = 0 since xtij = xtji for every i, j ∈ V and so S must be a clique of size t + 1 in Gt which is a contradiction.\nBy Lemma 3.6 the layer-t problem is to find a subset Et ⊆ E(Kn) of minimum weight under κ, such that the complement graph Gt = (V, Et) is a disjoint union of cliques of size ≤ t. Note that this implies that\nthe number of components in the complement graph is ≥ dn/te.The converse however, is not necessarily true: when t = n − 1 then the layer t-problem is the minimum (weighted) cut problem whose partitions may have size larger than 1. Our algorithmic approach is to solve an LP relaxation of ILP-ultrametric and then round the solution to obtain a feasible solution to ILP-ultrametric. The rounding however proceeds iteratively in a layer-wise manner and so we need to make sure that the rounded solution satisfies the interlayer constraints (2) and (5). The following lemma gives a combinatorial characterization of solutions that satisfy these two constraints.\nLemma 3.7. For every t ∈ [n− 1], let xtij be feasible for the layer-t problem ILP-layer. Let Gt = (V, Et) be the graph as in Definition 3.5 corresponding to xtij, so that by Lemma 3.6, Gt is a disjoint union of cliques Kt1, . . . , K t lt each of size at most t. Then x t ij is feasible for ILP-ultrametric if and only if the following conditions hold.\nNested cliques For any s ≤ t every clique Ksp for some p ∈ [ls] in Gs is a subclique of some clique Ktq in Gt where q ∈ [lt].\nRealization If ∣∣∣Ktp∣∣∣ = s for some s ≤ t, then Gs contains Ktp as a component clique, i.e., Ksq = Ktp for some\nq ∈ [ls].\nProof. Since xtij is feasible for the layer-t problem ILP-layer it is feasible for ILP-ultrametric if and only if it satisfies constraints (2) and (5). The solution xtij satisfies constraint (2) if and only if Et ⊆ Et+1 by definition and so Condition Nested cliques follows. Let us now assume that xtij is feasible for ILP-ultrametric, so that by the above argument Condition Nested cliques is satisfied. Note that every clique Ktp in the clique decomposition of Gt corresponds to an equivalence class St under the relation i ∼ j iff xtij = 0. Moreover, by Lemma 3.6 we have |St| ≤ t. Constraint (5) implies that x|St|ij = 0 for every i, j ∈ St. In other words, if |St| = s ≤ t, then xsij = 0 for every i, j ∈ St and so St is a subclique of some clique Ksq in the clique decomposition of Gs. However by Condition Nested cliques, Ksq must be a subclique of a clique Ktp′ in the clique decomposition of Gt, since s ≤ t. However, as Ktp ∩ Ktp′ = St and the clique decomposition decomposes Gt into a disjoint union of cliques, it follows that St ⊆ Ksq ⊆ Ktp′ = Ktp = St and so Ksq = Ktp. Therefore Condition Realization is satisfied. Conversely, suppose that xtij satisfies Conditions Nested cliques and Realization, so that by the argument in the paragraph above xtij satisfies constraint (2). Let us assume for the sake of contradiction that for a set S ⊆ V and a t ∈ [n− 1] constraint (5) is violated, i.e.,\n∑ i,j∈S\nx|S|ij > |S| ∑ i,j∈S\nxtij + ∑ i∈S j/∈S\n( 1− xtij ) . Since xtij ∈ {0, 1} it follows that xtij = 0 for every i, j ∈ S and xtij = 1 for every i ∈ S, j /∈ S so that S is a clique in Gt. Note that |S| < t since ∑i,j∈S x|S|ij > 0. This contradicts Condition Realization however, since S is clearly not a clique in G|S|.\nThe combinatorial interpretation of the individual layer-t problems allow us to simplify the formulation of ILP-ultrametric by replacing the constraints for sets of a specific size (constraint (4)) by a global constraint about all sets (constraint (14)).\nLemma 3.8. We may replace constraint (4) of ILP-ultrametric by the following equivalent constraint\n∑ j∈S xtij ≥ |S| − t ∀t ∈ [n− 1], S ⊆ V, i ∈ S. (14)\nProof. Let xtij be a feasible solution to ILP-ultrametric. Note that if |S| ≤ t then the constraints are redundant since xtij ∈ {0, 1}. Thus we may assume that |S| > t and let i be any vertex in S. Let us suppose for the sake of a contradiction that ∑j∈S xtij < |S| − t. This implies that there is a t sized subset S′ ⊆ S \\ {i} such that for every j ∈ S′ we have xtij′ = 0. In other words {i, j′} is an edge in Gt = (V, Et) for every j′ ∈ S′ and since Gt is a disjoint union of cliques (constraint (3)), this implies the existence of a clique of size t + 1. Thus by Lemma 3.6, xtij could not have been a feasible solution to ILP-ultrametric. Conversely, suppose xtij is feasible for the modified ILP where constraint (4) is replaced by constraint (14). Then again Gt = (V, Et) is a disjoint union of cliques since xtij satisfies constraint (3). Assume for contradiction that constraint (4) is violated: there is a set S of size t + 1 such that ∑i,j∈S xtij < 2. Note that this implies that ∑i,j xtij = 0 since x t ij = x t ji for every i, j ∈ V and t ∈ [n − 1]. Fix any i ∈ S, then ∑j∈S xtij < 1 = |S| − t since xtij = xtji by constraint (6), a violation of constraint (14). Thus xtij is feasible for ILP-ultrametric since it satisfies every other constraint by assumption."
    }, {
      "heading" : "4 Rounding an LP relaxation",
      "text" : "In this section we consider the following natural LP relaxation for ILP-ultrametric. We keep the variables xtij for every t ∈ [n − 1] and i, j ∈ V but relax the integrality constraint on the variables as well as drop constraint (5).\nmin n−1 ∑ t=1 ∑ {i,j}∈E(Kn) κ(i, j)xtij (LP-ultrametric)\ns.t. xtij ≥ xt+1ij ∀i, j ∈ V, t ∈ [n− 2] (15) xtij + x t jk ≥ xtik ∀i, j, k ∈ V, t ∈ [n− 1] (16)\n∑ j∈S xtij ≥ |S| − t ∀t ∈ [n− 1], S ⊆ V, i ∈ S (17)\nxtij = x t ji ∀i, j ∈ V, t ∈ [n− 1] (18) xtii = 0 ∀i, j ∈ V, t ∈ [n− 1] (19) 0 ≤ xtij ≤ 1 ∀i, j ∈ V, t ∈ [n− 1] (20)\nA feasible solution xtij to LP-ultrametric induces a sequence {dt}t∈[n−1] of distance metrics over V defined as dt(i, j) := xtij. Constraint 17 enforces an additional structure on this metric: informally points in a “large enough” subset S should be spread apart according to the metric dt. Metrics of type dt are called spreading metrics and were first studied in [Even et al., 1999, Even et al., 2000] in relation to graph partitioning problems. The following lemma gives a technical interpretation of spreading metrics (see, e.g., [Even et al., 1999, Even et al., 2000, Krauthgamer et al., 2009]); we include a proof for completeness.\nLemma 4.1. Let xtij be feasible for LP-ultrametric and for a fixed t ∈ [n− 1], let dt be the induced spreading metric. Let i ∈ V be an arbitrary vertex and let S ⊆ V be a set with i ∈ S such that |S| > (1+ ε)t for some ε > 0. Then maxj∈S dt(i, j) > ε1+ε .\nProof. For the sake of a contradiction suppose that for every j ∈ S we have dt(i, j) = xtij ≤ ε1+ε . This implies that xtij violates constraint (17) leading to a contradiction:\n∑ j∈S\nxtij ≤ ε\n1 + ε |S| < |S| − t,\nwhere the last inequality follows from |S| > (1 + ε)t.\nThe following lemma shows that we can optimize over LP-ultrametric in polynomial time. Lemma 4.2. An optimal solution to LP-ultrametric can be computed in time polynomial in n and log ( maxi,j κ(i, j) ) .\nProof. Weargue in the standard fashion via the application of the Ellipsoidmethod (see e.g., [Schrijver, 1998]). As such it suffices to verify that the encoding length of the numbers is small (which is indeed the case here) and that the constraints can be separated in polynomial time in the size of the input, i.e., in n and the logarithm of the absolute value of the largest coefficient. Since constraints of type (15), (16), (18), and (19) are polynomially many in n, we only need to check separation for constraints of type (17). Given a claimed solution xtij we can check constraint (17) by iterating over all t ∈ [n− 1], vertices i ∈ V, and sizes m of the set S from t+ 1 to n. For a fixed t, i, and set size m sort the vertices in V \\ {i} in increasing order of distance from i (according to the metric dt) and let S be the first m vertices in this ordering. If ∑j∈S x t ij < m− t then clearly xtij is not feasible for LP-ultrametric, so we may assume that ∑j∈S x t ij ≥ m− t. Moreover this is the only set to check: for any set S ⊆ V containing i such that |S| = m, ∑j∈S xtij ≥ ∑j∈S xtij ≥ m− t. Thus for a fixed t ∈ [n− 1], i ∈ V and set size m, it suffices to check that xtij satisfies constraint (17) for this subset S.\nFrom now on we will simply refer to a feasible solution to LP-ultrametric by the sequence of spreading metrics {dt}t∈[n−1] it induces. The following definition introduces the notion of an open ball BU (i, r, t) of radius r centered at i ∈ V according to the metric dt and restricted to the set U ⊆ V.\nDefinition 4.3. Let {dt | t ∈ [n− 1]} be the sequence of spreading metrics feasible for LP-ultrametric. Let U ⊆ V be an arbitrary subset of V. For a vertex i ∈ U, r ∈ R, and t ∈ [n− 1] we define the open ball BU (i, r, t) of radius r centered at i as\nBU (i, r, t) := {j ∈ U | dt(i, j) < r} ⊆ U.\nIf U = V then we denote BU (i, r, t) simply by B (i, r, t).\nRemark 4.4. For every pair i, j ∈ V we have dt(i, j) ≥ dt+1(i, j) by constraint (15). Thus for any subset U ⊆ V, i ∈ U, r ∈ R, and t ∈ [n− 2], it holds BU (i, r, t) ⊆ BU (i, r, t + 1).\nTo round LP-ultrametric to get a feasible solution for ILP-ultrametric, we will use the technique of sphere growingwhich was introduced in [Leighton and Rao, 1988] to show anO(log n) approximation for the maximum multicommodity flow problem. Recall from Lemma 3.6 that a feasible solution to ILP-layer consists of a decomposition of the graph Gt into a set of disjoint cliques of size at most t. One way to obtain such a decomposition is to choose an arbitrary vertex, grow a ball around this vertex until the expansion of this ball is below a certain threshold, chop off this ball and declare it as a partition and then recurse on the remaining vertices. This is the main idea behind sphere growing, and the parameters are chosen depending on the constraints of the specific problem (see, e.g., [Garg et al., 1996, Even et al., 1999, Charikar et al., 2003] for a few representative applications of this technique). The first step is to associate to every ball BU (i, r, t) a\nvolume vol (BU (i, r, t)) and a boundary ∂BU (i, r, t) so that its expansion is defined. For any t ∈ [n− 1] and U ⊆ V we denote by γUt the value of the layer-t objective for solution dt restricted to the set U, i.e.,\nγUt := ∑ i,j∈U i<j κ(i, j)dt(i, j).\nWhen U = V we refer to γUt simply by γt. Since κ : V × V → R≥0, it follows that γUt ≤ γt for any U ⊆ V. We are now ready to define the volume, boundary, and expansion of a ball BU (i, r, t). We use the definition of [Even et al., 1999] modified for restrictions to arbitrary subsets U ⊆ V.\nDefinition 4.5. [Even et al., 1999] Let U be an arbitrary subset of V. For a vertex i ∈ U, radius r ∈ R≥0, and t ∈ [n− 1], let BU (i, r, t) be the ball of radius r as in Definition 4.3. Then we define its volume as\nvol (BU (i, r, t)) := γUt\nn log n + ∑\nj,k∈BU(i,r,t) j<k κ(j, k)dt(j, k) + ∑ j∈BU(i,r,t) k/∈BU(i,r,t)\nk∈U\nκ(j, k) (r− dt(i, j)) .\nThe boundary of the ball ∂BU (i, r, t) is the partial derivative of volume with respect to the radius:\n∂BU (i, r, t) := ∂ vol (BU (i, r, t))\n∂r = ∑\nj∈BU(i,r,t) k/∈BU(i,r,t)\nk∈U\nκ(j, k).\nThe expansion φ(BU (i, r, t)) of the ball BU (i, r, t) is defined as the ratio of its boundary to its volume, i.e.,\nφ (BU (i, r, t)) := ∂BU (i, r, t)\nvol (BU (i, r, t)) .\nThe following lemma shows that the volume of a ball BU (i, r, t) is differentiable with respect to r in the interval (0, ∆] except at finitely many points (see e.g., [Even et al., 1999]).\nLemma 4.6. Let BU (i, r, t) be the ball corresponding to a set U ⊆ V, vertex i ∈ U, radius r ∈ R and t ∈ [n− 1]. Then vol (BU (i, r, t)) is differentiable with respect to r in the interval (0, ∆] except at finitely many points.\nProof. Note that for any fixed U ⊆ V, vol (BU (i, r, t)) is a monotone non-decreasing function in r since for a pair j, k ∈ U such that j ∈ BU (i, r, t) and k /∈ BU (i, r, t) we have r − dt(i, j) ≤ dt(j, k) otherwise r− dt(i, j) > dt(j, k) so that r > dt(i, j)+ dt(j, k) ≥ dt(i, k), a contradiction to the fact that k /∈ BU (i, r, t). Therefore adding the vertex k to the ball centered at i is only going to increase its volume as r− dt(i, j) ≤ dt(j, k) (see Definition 4.3). Thus vol (BU (i, r, t)) is differentiable with respect to r in the interval (0, ∆] except at finitely many points which correspond to a new vertex from U being added to the ball.\nThe following theorem establishes that the rounding procedure of Algorithm 2 ensures that the cliques in Ct are “small” and that the cost of the edges removed to form them are not too high. It also shows that Algorithm 2 can be implemented to run in time polynomial in n.\nTheorem 4.7. Let mε := ⌊ n−1\n1+ε\n⌋ as in Algorithm 2 and let { xtij | t ∈ [mε], i, j ∈ V } be the output of Al-\ngorithm 2 run on a feasible solution {dt}t∈[n−1] of LP-ultrametric and any choice of ε ∈ (0, 1). For any\nInput: Data set V, {dt}t∈[n−1] : V ×V, ε > 0, κ : V ×V → R≥0 Output: A solution set of the form { xtij ∈ {0, 1} | t ∈ [⌊ n−1 1+ε ⌋] , i, j ∈ V } 1 mε ← ⌊ n−1 1+ε\n⌋ 2 t← mε 3 Ct+1 ← {V} 4 ∆← ε1+ε 5 while t ≥ 1 do 6 Ct ← ∅ 7 for U ∈ Ct+1 do 8 if |U| ≤ (1 + ε)t then 9 Ct ← Ct ∪ {U}\n10 Go to line 7 11 end 12 while U 6= ∅ do 13 Let i be arbitrary in U 14 Let r ∈ (0, ∆] be s.t. φ (BU (i, r, t)) ≤ 1∆ log ( vol(BU(i,∆,t)) vol(BU(i,0,t))\n) 15 Ct ← Ct ∪ {BU (i, r, t)} 16 U ← U \\ BU (i, r, t) 17 end 18 end 19 xtij = 1 if i ∈ U1 ∈ Ct, j ∈ U2 ∈ Ct and U1 6= U2, else xtij = 0 20 t← t− 1 21 end 22 return { xtij | t ∈ [mε], i, j ∈ V\n} Algorithm 2: Iterative rounding algorithm to find a low cost ultrametric\nt ∈ [mε], we have that xtij is feasible for the layer-b(1 + ε) tc problem ILP-layer and there is a constant c(ε) > 0 depending only on ε such that\n∑ {i,j}∈E(Kn) κ(i, j)xtij ≤ c(ε)(log n)γt.\nMoreover, Algorithm 2 can be implemented to run in time polynomial in n.\nProof. We first show that for a fixed t, the constructed solution xtij is feasible for the layer-b(1+ ε)tc problem ILP-layer. Let Ct be as in Algorithm 2 so that xtij = 1 if i, j belong to different sets in Ct and xtij = 0 otherwise. Let Gt = (V, Et) be as in Definition 3.5 corresponding to xtij. Note that for any t ∈ [mε], every Vi ∈ Ct is a clique in Gt by construction (line 19) and for every distinct pair Vi, Vj ∈ Ct we have Vi ∩Vj = ∅ (lines 15 and 16). Therefore by Lemma 3.6, it suffices to prove that for any Vi ∈ Ct, it holds |Vi| ≤ b(1 + ε)tc. If Vi is added to Ct in line 9 then there is nothing to prove. Thus let us assume that Vi is of the form BU (i, r, t) for some U ⊆ V as in line 14 so that φ (BU (i, r, t)) ≤ 1 ∆ log ( vol(BU(i,∆,t)) vol(BU(i,0,t)) ) . Note that by Lemma 4.1 it suffices to show that there is such an r ∈ (0, ∆]. This property follows from the rounding scheme due to [Even et al., 1999] as we will explain now. By Lemma 4.6 vol (BU (i, r, t)) is differentiable everywhere in the interval (0, ∆] except at finitely many points X. Let the set of discontinuous points be X = {x1, x2, . . . , xk−1} with x0 = 0 < x1 < x2 . . . xk−1 < xk = ∆. We claim that there must be an r ∈ (0, ∆] \\ X such that φ (BU (i, r, t)) ≤ 1∆ log ( vol(BU(i,∆,t)) vol(BU(i,0,t)) ) . Let us assume for the sake of a contradiction that for every r ∈ (0, ∆] \\ X we have φ (BU (i, r, t)) > 1 ∆ log ( vol(BU(i,∆,t)) vol(BU(i,0,t)) ) . However integrating both sides from 0 to ∆ results in a contradiction:\n∫ ∆ r=0 φ (BU (i, r, t)) dr = ∫ ∆ r=0 ∂BU (i, r, t) vol (BU (i, r, t)) dr (21)\n= k\n∑ i=1 ∫ xi r=xi−1 ∂BU (i, r, t) vol (BU (i, r, t)) dr (22)\n= k\n∑ i=1 ∫ xi r=xi−1 d (vol (BU (i, r, t))) vol (BU (i, r, t))\n(23)\n≤ log vol (BU (i, ∆, t))− log vol (BU (i, 0, t)) (24) = ∫ ∆\nr=0\n1 ∆\nlog (\nvol (BU (i, ∆, t)) vol (BU (i, 0, t))\n) dr, (25)\nwhere line 24 follows since f is monotonic increasing. For any t ∈ [mε] the set Ct is a disjoint partition of V with balls of the form BU (i, r, t′) for some t′ ≥ t and U ⊆ Ul ∈ Ct′+1: this is easily seen by induction since Cmε+1 is initialized as V. Further, a cluster Vi is added to Ct either in line 15 in which case it is a ball of the form BU (i, r, t) for some U ∈ Ct+1, i ∈ U, and r ∈ R or it is added in line 9 in which case it must have been a ball BU (i′, r′, t′) for some t′ > t, U ⊆ Ul ∈ Ct′+1, i′ ∈ V, and r′ ∈ R. Note that for any t′ ≥ t and U ⊆ V, it holds γUt′ ≤ γUt since for every pair i, j ∈ V we have κ(i, j) ≥ 0 and dt(i, j) ≥ dt′(i, j) because of constraint (15). Moreover, for any subset U ⊆ V we have γUt ≤ γt since κ, dt ≥ 0. We claim that for any t ∈ [mε] the total volume of the balls in Ct is at most ( 2 + 1log n ) γt. First note that the affine term γ U t′\nn log n in the volume of a ball BU (i, r, t′) in Ct is upper bounded by γt\nn log n and appears at most n times. Next we claim that the contribution to the total volume from the term involving the edges inside and crossing a ball BU (i, r, t′) ∈ Ct is at most 2γt. This is because the balls are disjoint, r − dt′(i, k) ≤ dt′(j, k) ≤ dt(j, k) for the crossing edges of a ball BU (i, r, t′) ∈ Ct and a crossing edge contributes to the\nvolume of at most 2 balls in Ct. Note that for any U ⊆ V, i ∈ U, and r ∈ R≥0 we have vol (BU (i, r, t)) ∈[ γUt n log n , ( 1 + 1n log n ) γUt ] . Using this observation and the stopping condition of line 14 it follows that\n∑ {i,j}∈E(Kn) κ(i, j)xtij = ∑ {i,j}∈E(Kn):\ni,j separated in Ct\nκ(i, j)\n= 1 2 ∑BU(i,r,t′)∈Ct :\nt′≥t U⊆Ul∈Ct′+1\n∑ j∈BU(i,r,t′) k/∈BU(i,r,t′) κ(j, k)\n︸ ︷︷ ︸ Since κ is symmetric\n= 1 2 ∑BU(i,r,t′)∈Ct :\nt′≥t U⊆Ul∈Ct′+1\n∂BU ( i, r, t′ )\n= 1 2 ∑BU(i,r,t′)∈Ct :\nt′≥t U⊆Ul∈Ct′+1\nφ ( BU ( i, r, t′ )) vol ( BU ( i, r, t′ ))\n≤ ∑ BU(i,r,t′)∈Ct :\nt′≥t U⊆Ul∈Ct′+1\n1 2∆\nlog (\nvol (BU (i, ∆, t′)) vol (BU (i, 0, t′))\n) vol ( BU ( i, r, t′ ))\n≤ 1 2∆ (log (n log n + 1))︸ ︷︷ ︸ via interval bounds ∑ BU(i,r,t′)∈Ct :\nt′≥t U⊆Ul∈Ct′+1\nvol ( BU ( i, r, t′ ))\n≤ 1 + ε 2ε\n(log (n log n + 1)) ( 2 + 1\nlog n ) γt︸ ︷︷ ︸\ncontribution of affine term ≤ γtlog n contribution of edge terms ≤ 2γt\n≤ c(ε)(log n)γt,\nfor some constant c(ε) > 0 depending only on ε. For the run time of Algorithm 2 note that the loop in line 5 runs for at most n− 1 steps, while the loop in line 7 runs for at most n steps. For a set U ⊆ V, to compute the ball BU (i, r, t) of least radius r such that φ (BU (i, r, t)) ≤ 1∆ log ( vol(BU(i,∆,t)) vol(BU(i,0,t)) ) , sort the vertices in U \\ {i} in increasing order of distance from i\naccording to dt. Let the vertices in U \\ {i} in this sorted order be { j1, . . . , j|U|−1 } . Then it suffices to check the expansion of the balls {i} and {i} ∪ {j1, . . . , jk} for every k ∈ [|U| − 1]. It is straightforward to see that all the other steps in Algorithm 2 run in time polynomial in n.\nRemark 4.8. A discrete version of the volumetric argument for region growing can be found in [Gupta, 2005].\nWe are now ready to prove the main theorem showing that we can obtain a low cost non-trivial ultrametric from Algorithm 2.\nTheorem 4.9. Let {xtij | t ∈ [mε] , i, j ∈ V} be the output of Algorithm 2 on an optimal solution {dt}t∈[n−1] of LP-ultrametric for any choice of ε ∈ (0, 1). Define the sequence { ytij } for every t ∈ [n− 1] and i, j ∈ V\nas\nytij := { xbt/(1+ε)cij if t > 1 + ε 1 if t ≤ 1 + ε.\nThen ytij is feasible for ILP-ultrametric and satisfies\nn−1 ∑ t=1 ∑ {i,j}∈E(Kn) κ(i, j)ytij ≤ (2c(ε) log n)OPT\nwhereOPT is the optimal solution to ILP-ultrametric and c(ε) is the constant in the statement of Theorem 4.9.\nProof. Note that by Theorem 4.7 for every t ∈ [mε], xtij is feasible for the layer-b(1 + ε)tc problem ILPlayer and that there is a constant c(ε) > 0 such that for every t ∈ [mε], we have ∑{i,j}∈E(Kn) κ(i, j)xtij ≤ (c(ε) log n) γt. Let ytij be as in the statement of the theorem. The graph Gt = (V, Et) as in Definition 3.5 corresponding to ytij for t ≤ 1+ ε consists of isolated vertices, i.e., cliques of size 1: By definition ytij is feasible for the layer-t problem ILP-layer. The collection C1 corresponding to x1ij consists of cliques of size at most 1 + ε, however since 0 < ε < 1 it follows that the cliques in C1 are isolated vertices and so x1ij = 1 for every {i, j} ∈ E(Kn). Thus ∑i,j κ(i, j)ytij = ∑i,j κ(i, j)x 1 ij ≤ (c(ε) log n) γ1 for t ≤ 1 + ε by Theorem 4.7. Moreover for every t > 1+ ε, we have ∑i,j κ(i, j)ytij ≤ (c(ε) log n)γbt/(1+ε)c again by Theorem 4.7. We claim that ytij is feasible for ILP-ultrametric. The solution ytij corresponds to the collection Cb t1+ε c for t > 1 + ε or to the collection C1 for t ≤ 1 + ε from Algorithm 2. For any t < mε, every ball BU (i, r, t) ∈ Ct comes from the refinement of a ball BU′ (i′, r′, t′) for some i′ ∈ V, r′ ≥ r, t′ ≥ t and U′ ⊇ U. Thus ytij satisfies Condition Nested cliques of Lemma 3.7. On the other hand line 8 ensures that if |BU (i, r, t)| = b(1 + ε)sc for some U ⊆ V and s < t then BU (i, r, t) also appears as a ball in Cs. Therefore ytij also satisfies Condition Realization of Lemma 3.7 and so is feasible for ILP-ultrametric. The cost of ytij is at most\nn−1 ∑ t=1 ∑ {i,j}∈E(Kn)\nκ(i, j)ytij ≤ (c(ε) log n) (\nγ1 + n−1 ∑ t=2 γbt/(1+ε)c\n)\n≤ 2c(ε) log n n−1 ∑ t=1 γt ≤ 2c(ε) log n OPT,\nwhere we use the fact that ∑n−1t=1 γt = OPT(LP) ≤ OPT since LP-ultrametric is a relaxation of ILPultrametric.\nTheorem 4.9 implies the following corollary where we put everything together to obtain a hierarchical clustering of V in time polynomial in n with |V| = n. Let T denote the set of all possible hierarchical clusterings of V.\nCorollary 4.10. Given a data set V of n points and a similarity function κ : V × V → R≥0, Algorithm 3 returns a hierarchical clustering T of V satisfying\ncost(T) ≤ O (log n) min T′∈T cost(T′).\nMoreover Algorithm 3 runs in time polynomial in n and log ( maxi,j∈V κ(i, j) ) .\nInput: Data set V of n points, similarity function κ : V ×V → R≥0 Output: Hierarchical clustering of V\n1 Solve LP-ultrametric to obtain optimal sequence of spreading metrics {dt | dt : V ×V → [0, 1]} 2 Fix a choice of ε ∈ (0, 1) 3 mε ← ⌊ n−1 1+ε ⌋ 4 Let { xtij | t ∈ [mε] } be the output of Algorithm 2 on V, κ, {dt}t∈[n−1]\n5 Let ytij := { xbt/(1+ε)cij if t > 1 + ε 1 if t ≤ 1 + ε for every t ∈ [n− 1], i, j ∈ E(Kn) 6 d(i, j)← ∑n−1t=1 ytij for every i, j ∈ E(Kn) 7 d(i, i)← 0 for every i ∈ V 8 Let r, T be the output of Algorithm 1 on V, d 9 return r, T\nAlgorithm 3: Hierarchical clustering of V for cost function (1)\nProof. Let T̂ be the optimal hierarchical clustering according to cost function (1). By Corollary 3.4 and Theorem 4.9 we can find a hierarchical clustering T satisfying\n∑ {i,j}∈E(Kn) κ(i, j)(|leaves(T[lca(i, j)])| − 1) ≤ O(log n)  ∑ {i,j}∈E(Kn) κ(i, j) (∣∣∣leaves(T̂[lca(i, j)])∣∣∣− 1)  . Let K := ∑{i,j}∈E(Kn) κ(i, j). Then it follows from the above expression that cost(T) ≤ O(log n) cost(T̂)− O(log n)K + K ≤ O(log n) cost(T̂).\nWe can find an optimal solution to LP-ultrametric due to Lemma 4.2 using the Ellipsoid algorithm in time polynomial in n and log ( maxi,j∈V κ(i, j) ) . Algorithm 2 runs in time polynomial in n due to Theorem 4.7.\nFinally, Algorithm 1 runs in time O ( n3 ) due to Lemma 3.3."
    }, {
      "heading" : "5 Generalized Cost Function",
      "text" : "In this sectionwe study the following natural generalization of cost function (1) also introduced by [Dasgupta, 2016] where the distance between the two points is scaled by a function f : R≥0 → R≥0, i.e.,\ncost f (T) := ∑ {i,j}∈E(Kn) κ(i, j) f (|leaves T[lca(i, j)]|) . (26)\nIn order that cost function (26) makes sense, f should be strictly increasing and satisfy f (0) = 0. Possible choices for f could be { x2, ex − 1, log(1 + x) } . The top-down heuristic in [Dasgupta, 2016] finds the optimal hierarchical clustering up to an approximation factor of cn log n with cn being defined as\ncn := 3αn max 1≤n′≤n f (n′) f (dn′/3e)\nand where αn is the approximation factor from the Sparsest Cut algorithm used. A naive approach to solving this problem using the ideas of Algorithm 2 would be to replace the objective function of ILP-ultrametric by\n∑ {i,j}∈E(Kn) κ(i, j) f ( n−1 ∑ t=1 xtij ) .\nThis makes the corresponding analogue of LP-ultrametric non-linear however, and for a general κ and f it is not clear how to compute an optimum solution in polynomial time. One possible solution is to assume that f is convex and use the Frank-Wolfe algorithm to compute an optimum solution. That still leaves the problem of how to relate f ( ∑n−1t=1 x t ij ) to ∑n−1t=1 f ( xtij ) as one would have to do to get a corresponding version of Theorem 4.9. The following simple observation provides an alternate way of tackling this problem.\nObservation 5.1. Let d : V × V → R be an ultrametric and f : R≥0 → R≥0 be a strictly increasing function such that f (0) = 0. Define the function f (d) : V ×V → R as f (d)(i, j) := f (d(i, j)). Then f (d) is also an ultrametric on V.\nTherefore by Corollary 3.4 to find a minimum cost hierarchical clustering T of V according to the cost function (26), it suffices to minimize 〈κ, d〉 where d is the f -image of a non-trivial ultrametric as in Definition 2.2. The following lemma lays down the analogue of Conditions 1 and 2 from Definition 2.2 that the f -image of a non-trivial ultrametric satisfies.\nLemma 5.2. Let f : R≥0 → R≥0 be a strictly increasing function satisfying f (0) = 0. An ultrametric d on V is the f -image of a non-trivial ultrametric on V iff\n1. for every non-empty set S ⊆ V, there is a pair of points i, j ∈ S such that d(i, j) ≥ f (|S| − 1),\n2. for any t if St is an equivalence class ofV under the relation i ∼ j iff d(i, j) ≤ t, thenmaxi,j∈St d(i, j) ≤ f (|St| − 1).\nProof. If d is the f -image of a non-trivial ultrametric d′ on V then clearly d satisfies Conditions 1 and 2. Conversely, let d be an ultrametric on V satisfying Conditions 1 and 2. Note that f is strictly increasing and V is a finite set and thus f−1 exists and is strictly increasing as well, with f−1(0) = 0. Define d′ as d′(i, j) := f−1(d(i, j)) for every i, j ∈ V. By Observation 5.1 d′ is an ultrametric on V satisfying Conditions 1 and 2 of Definition 2.2 and so d′ is a non-trivial ultrametric on V.\nLemma 5.2 allows us to write the analogue of ILP-ultrametric for finding the minimum cost ultrametric that is the f -image of a non-trivial ultrametric on V. Note that by Lemma 2.4 the range of such an ultrametric is the set { f (0), f (1), . . . , f (n− 1)}. We have the binary variables xtij for every distinct pair i, j ∈ V and t ∈ [n− 1], where xtij = 1 if d(i, j) ≥ f (t) and xtij = 0 if d(i, j) < f (t).\nmin n−1 ∑ t=1 ∑ {i,j}∈E(Kn) κ(i, j) ( f (t)− f (t− 1)) xtij (f-ILP-ultrametric)\ns.t. xtij ≥ xt+1ij ∀i, j ∈ V, t ∈ [n− 2] (27) xtij + x t jk ≥ xtik ∀i, j, k ∈ V, t ∈ [n− 1] (28)\n∑ i,j∈S xtij ≥ 2 ∀t ∈ [n− 1], S ⊆ V, |S| = t + 1 (29)\n∑ i,j∈S\nx|S|ij ≤ |S| ∑ i,j∈S\nxtij + ∑ i∈S j/∈S\n( 1− xtij ) ∀t ∈ [n− 1], S ⊆ V (30) xtij = x t ji ∀i, j ∈ V, t ∈ [n− 1] (31) xtii = 0 ∀i ∈ V, t ∈ [n− 1] (32) xtij ∈ {0, 1} ∀i, j ∈ V, t ∈ [n− 1] (33)\nIf xtij is a feasible solution to f-ILP-ultrametric then the ultrametric represented by it is defined as\nd(i, j) := n−1 ∑ t=1 ( f (t)− f (t− 1))xtij.\nConstraint (29) ensures that d satisfies Condition 1 of Lemma 5.2, since for every S ⊆ V of size t + 1 we have a pair i, j ∈ S such that d(i, j) ≥ f (t). Similarly constraint (30) ensures that d satisfies Condition 2 of Lemma 5.2 since it is active if and only if S is an equivalence class of V under the relation i ∼ j iff d(i, j) < f (t). In this case Condition 2 of Lemma 5.2 requires maxi,j∈S d(i, j) ≤ f (|S| − 1) or in other words x|S|ij = 0 for every i, j ∈ S. Similar to ILP-layer we define an analogous layer-t problem where we fix a choice of t ∈ [n− 1] and drop the constraints that relate the different layers to each other.\nmin ∑ {i,j}∈E(Kn) κ(i, j) ( f (t)− f (t− 1)) xtij (f-ILP-layer)\ns.t. xtij + x t jk ≥ xtik ∀i, j, k ∈ V (34)\n∑ i,j∈S xtij ≥ 2 ∀S ⊆ V, |S| = t + 1 (35)\nxtij = x t ji ∀i, j ∈ V (36) xtii = 0 ∀i ∈ V (37) xtij ∈ {0, 1} ∀i, j ∈ V (38)\nNote that f-ILP-ultrametric and f-ILP-layer differ from ILP-ultrametric and ILP-layer respectively only in the objective function. Therefore Lemmas 3.6 and 3.7 also give a combinatorial characterization of the set of feasible solutions to f-ILP-layer and f-ILP-ultrametric respectively. Similarly, by Lemma 3.8 we may replace constraint (29) by the following equivalent constraint over all subsets of V\n∑ j∈S xtij ≥ |S| − t ∀t ∈ [n− 1], S ⊆ V, i ∈ S.\nThis provides the analogue of LP-ultrametric in which we drop constraint (30) and enforce it in the rounding procedure.\nmin n−1 ∑ t=1 ∑ {i,j}∈E(Kn) κ(i, j) ( f (t)− f (t− 1)) xtij (f-LP-ultrametric)\ns.t. xtij ≥ xt+1ij ∀i, j ∈ V, t ∈ [n− 2] (39) xtij + x t jk ≥ xtik ∀i, j, k ∈ V, t ∈ [n− 1] (40)\n∑ j∈S xtij ≥ |S| − t ∀t ∈ [n− 1], S ⊆ V, i ∈ S (41)\nxtij = x t ji ∀i, j ∈ V, t ∈ [n− 1] (42) xtii = 0 ∀i ∈ V, t ∈ [n− 1] (43) 0 ≤ xtij ≤ 1 ∀i, j ∈ V, t ∈ [n− 1] (44)\nSince f-LP-ultrametric differs fromLP-ultrametric only in the objective function, it follows fromLemma 4.2 that an optimum solution to f-LP-ultrametric can be computed in time polynomial in n. As before, a feasible solution xtij of f-LP-ultrametric induces a sequence {dt}t∈[n−1] of spreading metrics on V defined as dt(i, j) := xtij. Note that in contrast to the ultrametric d, the spreading metrics {dt}t∈[n−1] are independent of the function f . Let BU (i, r, t) be a ball of radius r centered at i ∈ U for some set U ⊆ V as in Definition 4.3. For a subset U ⊆ V, let γUt be defined as before to be the value of the layer-t objective corresponding to a solution dt of f-LP-ultrametric restricted to U, i.e.,\nγUt := ∑ i,j∈U i<j ( f (t)− f (t− 1)) κ(i, j)dt(i, j).\nAs before, we denote γVt by γt. We will associate a volume vol (BU (i, r, t)) and a boundary ∂BU (i, r, t) to the ball BU (i, r, t) as in Section 4.\nDefinition 5.3. Let U be an arbitrary subset of V. For a vertex i ∈ U, radius r ∈ R≥0, and t ∈ [n− 1], let BU (i, r, t) be the ball of radius r as in Definition 4.3. Then we define its volume as\nvol (BU (i, r, t)) := γUt\nn log n + ( f (t)− f (t− 1))  ∑j,k∈BU(i,r,t) j<k κ(j, k)dt(j, k) + ∑ j∈BU(i,r,t) k/∈BU(i,r,t)\nk∈U\nκ(j, k) (r− dt(i, j))  .\nThe boundary of the ball ∂BU (i, r, t) is the partial derivative of volume with respect to the radius:\n∂BU (i, r, t) := ( f (t)− f (t− 1)) (\n∂ vol (BU (i, r, t)) ∂r\n) = ( f (t)− f (t− 1))  ∑j∈BU(i,r,t) k/∈BU(i,r,t)\nk∈U\nκ(j, k)  .\nThe expansion φ (BU (i, r, t)) of the ball BU (i, r, t) is defined as the ratio of its boundary to its volume, i.e.,\nφ (BU (i, r, t)) := ∂BU (i, r, t)\nvol (BU (i, r, t)) .\nNote that the expansion φ (BU (i, r, t)) ofDefinition 5.3 is the same as inDefinition 4.5 since the ( f (t)− f (t− 1)) term cancels out. Thus one could run Algorithm 2 with the same notion of volume as in Definition 4.5, however in that case the analogous versions of Theorems 4.7 and 4.9 do not follow as naturally. The following is then a simple corollary of Theorem 4.7.\nCorollary 5.4. Let mε := ⌊ n−1\n1+ε\n⌋ as in Algorithm 2. Let { xtij | t ∈ [n− 1], i, j ∈ V } be the output of Al-\ngorithm 2 using the notion of volume, boundary and expansion as in Definition 5.3, on a feasible solution to f-LP-ultrametric and any choice of ε ∈ (0, 1). For any t ∈ [mε], we have that xtij is feasible for the layer-b(1 + ε)tc problem f-ILP-layer and there is a constant c(ε) > 0 depending only on ε such that\n∑ {i,j}∈E(Kn) κ(i, j) ( f (t)− f (t− 1)) xtij ≤ (c(ε) log n) γt.\nCorollary 5.4 allows us to prove the analogue of Theorem 4.9, i.e., we can use Algorithm 2 to get an ultrametric that is an f -image of a non-trivial ultrametric and whose cost is at most O(log n) times the cost of an optimal hierarchical clustering according to cost function (26).\nTheorem 5.5. Let {xtij | t ∈ [mε] , i, j ∈ V} be the output of Algorithm 2 using the notion of volume, boundary, and expansion as in Definition 5.3 on an optimal solution {dt}t∈[n−1] of f-LP-ultrametric for any choice of ε ∈ (0, 1). Define the sequence { ytij } for every t ∈ [n− 1] and i, j ∈ V as\nytij := { xbt/(1+ε)cij if t > 1 + ε 1 if t ≤ 1 + ε.\nThen ytij is feasible for f-ILP-ultrametric and there is a constant c(ε) > 0 such that\nn−1 ∑ t=1 ∑ {i,j}∈E(Kn) κ(i, j) ( f (t)− f (t− 1)) ytij ≤ (c(ε) log n)OPT\nwhere OPT is the optimal solution to f-ILP-ultrametric.\nProof. Immediate from Corollary 5.4 and Theorem 4.9.\nFinally we put everything together to obtain the corresponding Algorithm 4 that outputs a hierarchical clustering of V of cost at most O (log n) times the optimal clustering according to cost function (26).\nCorollary 5.6. Given a data set V of n points and a similarity function κ : V×V → R, Algorithm 4 returns a hierarchical clustering T of V satisfying\ncost f (T) ≤ O (an + log n) min T′∈T cost f (T′),\nwhere an := maxn′∈[n] f (n′)− f (n′ − 1). Moreover Algorithm 4 runs in time polynomial in n, log f (n) and log ( maxi,j∈V κ(i, j) ) .\nProof. Let T̂ be an optimal hierarchical clustering according to cost function (26). ByCorollary 3.4, Lemma 5.2 and Theorem 5.5 it follows that we can find a hierarchical clustering T satisfying\n∑ {i,j}∈E(Kn) κ(i, j) f (|leaves(T[lca(i, j)]| − 1) ≤ O(log n)  ∑ {i,j}∈E(Kn) κ(i, j) f (∣∣∣leaves(T̂[lca(i, j)]∣∣∣− 1)  . Recall that cost f (T) := ∑{i,j}∈E(Kn) κ(i, j) f (|leaves(T[lca(i, j)]|). Let K := ∑{i,j}∈E(Kn) κ(i, j). Note that for any hierarchical clustering T′ we have K ≤ cost f (T′) since f is an increasing function. From the above expression we infer that\ncost f (T)− anK ≤ ∑ {i,j}∈E(Kn) κ(i, j) f (|leaves(T[lca(i, j)]| − 1) ≤ O(log n) cost f (T̂),\nand so cost f (T) ≤ O(log n) cost f (T̂) + anK ≤ O(an + log n) cost f (T̂). We can find an optimal solution to f-LP-ultrametric due to Lemma 4.2 using the Ellipsoid algorithm in time polynomial in n, log f (n), and log ( maxi,j∈V κ(i, j) ) . Note the additional log f (n) in the running time since now we need to binary\nsearch over the interval [ 0, maxi,j∈V κ(i, j) · f (n) · n ] . Algorithm 2 runs in time polynomial in n due to\nTheorem 4.7. Finally, Algorithm 1 runs in time O ( n3 ) due to Lemma 3.3.\nInput: Data set V of n points, similarity function κ : V ×V → R≥0, f : R≥0 → R≥0 strictly increasing with f (0) = 0\nOutput: Hierarchical clustering of V 1 Solve f-LP-ultrametric to obtain optimal sequence of spreading metrics {dt | dt : V ×V → [0, 1]} 2 Fix a choice of ε ∈ (0, 1) 3 mε ← ⌊ n−1 1+ε\n⌋ 4 Let { xtij | t ∈ [mε] } be the output of Algorithm 2 on V, κ, {dt}t∈[n−1]\n5 Let ytij := { xbt/(1+ε)cij if t > 1 + ε 1 if t ≤ 1 + ε for every t ∈ [n− 1], i, j ∈ E(Kn) 6 d(i, j)← ∑n−1t=1 ( f (t)− f (t− 1)) ytij for every i, j ∈ E(Kn) 7 d(i, i)← 0 for every i ∈ V 8 Let r, T be the output of Algorithm 1 on V, f−1(d) 9 return r, T\nAlgorithm 4: Hierarchical clustering of V for cost function (26)"
    }, {
      "heading" : "6 Experiments",
      "text" : "Finally, we describe the experiments we performed. For small data sets ILP-ultrametric and f-ILP-ultrametric describe integer programming formulations that allow us to compute the exact optimal hierarchical clustering for cost functions (1) and (26) respectively. We implement f-ILP-ultrametric where one can plug in any strictly increasing function f satisfying f (0) = 0. In particular, setting f (x) = x gives us ILP-ultrametric. We use the Mixed Integer Programming (MIP) solver Gurobi 6.5 [Gurobi Optimization, 2015]. Similarly, we also implement Algorithms 1, 2, and 4 using Gurobi as our LP solver. Note that Algorithm 4 needs to fix a parameter choice ε ∈ (0, 1). In Sections 4 and 5 we did not discuss the effect of the choice of the parameter ε in detail. In particular, we need to choose an ε small enough such that for every U ⊆ V encountered in Algorithm 2, vol (BU (i, ∆, t)) is of the same sign as vol (BU (i, 0, t)) for every t ∈ [n − 1], so that log (\nvol(BU(i,∆,t)) vol(BU(i,0,t))\n) is defined. In our experiments we start with a particular value of ε (say 0.5) and halve it\ntill the volumes have the same sign. For the sake of exposition, we limit ourselves to the following choices for the function f {\nx, x2, log(1 + x), ex − 1 } .\nBy Lemma 4.2 we can optimize over f-LP-ultrametric in time polynomial in n using the Ellipsoid method. In practice however, we use the dual simplex method where we separate triangle inequality constraints (40) and spreading constraints (41) to obtain fast computations. For the similarity function κ : V × V → R we limit ourselves to using cosine similarity and the Gaussian kernel with σ = 1. They are defined formally below.\nDefinition 6.1 (Cosine similarity). Given a data set V ∈ Rm for some m ≥ 0, the cosine similarity κcos is defined as κcos(x, y) := 〈x,y〉‖x‖‖y‖ .\nSince the LP rounding Algorithm 2 assumes that κ ≥ 0 in practice we implement 1+ κcos rather than κcos. Definition 6.2 (Gaussian kernel). Given a data set V ∈ Rm for some m ≥ 0, the Gaussian kernel κgauss with standard deviation σ is defined as κgauss(x, y) := exp ( − ‖x−y‖ 2\n2σ2\n) .\nThe main aim of our experiments was to answer the following two questions.\n1. How good is the hierarchal clustering obtained from Algorithm 4 as opposed to the true optimal output by f-ILP-ultrametric?\n2. How good does Algorithm 4 perform compared to other hierarchical clustering methods?\nFor the first question, we are restricted to working with small data sets since computing an optimum solution to f-ILP-ultrametric is expensive. In this case we consider synthetic data sets of small size and samples of some data sets from the UCI database [Lichman, 2013]. The synthetic data sets we consider are mixtures of Gaussians in various small dimensional spaces. Figure 1 shows a comparison of the cost of the hierarchy (according to cost function (26)) returned by solving f-ILP-ultrametric and by Algorithm 4 for various forms of f when the similarity function is κcos and κgauss. Note that we normalize the cost of the tree returned by f-ILP-ultrametric and Algorithm 4 by the cost of the trivial clustering r, T∗ where T∗ is the star graph with V as its leaves and r as the internal node. In other words dT∗(i, j) = n− 1 for every distinct pair i, j ∈ V and so the normalized cost of any tree lies in the interval (0, 1]. For the study of the second question, we consider some of the popular algorithms for hierarchical clustering are single linkage, average linkage, complete linkage, and Ward’s method [Ward Jr, 1963]. To get a numerical handle on how good a hierarchical clustering T of V is, we prune the tree to get the best k flat clusters and measure its error relative to the target clustering. We use the following notion of error also known as Classification Error that is standard in the literature for hierarchical clustering (see, e.g., [Meilă and Heckerman, 2001]). Note that we may think of a flat k-clustering of the data V as a function h mapping elements of V to a label set L := {1, . . . , k}. Let Sk denote the group of permutations on k letters. Definition 6.3 (Classification Error). Given a proposed clustering h : V → L its classification error relative to a target clustering g : V → L is denoted by err (g, h) and is defined as\nerr (g, h) := min σ∈Sk\n[ Pr\nx∈V [h(x) 6= σ(g(x))\n] .\nWe compare the error of Algorithm 4 with the various linkage based algorithms that are commonly used for hierarchical clustering, as well as Ward’s method and the k-means algorithm. We test Algorithm 4 most extensively for f (x) = x while doing a smaller number of tests for f (x) ∈ { x2, log(1 + x), ex − 1 } . Note that both Ward’s method and the k-means algorithm work on the squared Euclidean distance ‖x− y‖22 between two points x, y ∈ V, i.e., they both require an embedding of the data points into a normed vector space which provides extra information that can be potentially exploited. For the linkage based algorithms\nwe use the same notion of similarity 1 + κcos or κgauss that we use for Algorithm 4. For comparison we use a mix of synthetic data sets as well as the Wine, Iris, Soybean-small, Digits, Glass, and Wdbc data sets from the UCI repository [Lichman, 2013]. For some of the larger data sets, we sample uniformly at random a smaller number of data points and take the average of the error over the different runs. Figures 2, 3, 4, and 5 show that the hierarchical clustering returned by Algorithm 4 with f (x) ∈ { x, x2, log(1 + x), ex − 1 } often has better projections into flat clusterings than the other algorithms. This is especially true when we compare it to the linkage based algorithms, since they use the same pairwise similarity function as Algorithm 4, as opposed to Ward’s method and k-means."
    }, {
      "heading" : "7 Discussion",
      "text" : "In this work we have studied the cost functions (1) and (26) for hierarchical clustering given a pairwise similarity function over the data and shown an O(log n) approximation algorithm for this problem. As briefly mentioned in Section 2 however, such a cost function is not unique. Further, there is an intimate connection between hierarchical clusterings and ultrametrics over discrete sets which points to other directions for for-\nmulating a cost function over hierarchies. In particular we briefly mention the related notion of hierarchically well-separated trees (HST) as defined in [Bartal, 1996] (see also [Bartal et al., 2001, Bartal et al., 2003]). A k-HST for k ≥ 1 is a tree T such that each vertex u ∈ T has a label ∆(u) ≥ 0 such that ∆(u) = 0 if and only if u is a leaf of T. Further, if u is a child of v in T then ∆(u) ≤ ∆(v)/k. It is well known that any ultrametric d on a finite set V is equivalent to a 1-HST where V is the set of leaves of T and d(i, j) = ∆ (lca(i, j)) for every i, j ∈ V. Thus in the special case when ∆(u) = |leaves T[u]| − 1 we get the cost function (1), while if ∆(u) = f (|leaves T[u]| − 1) for a strictly increasing function f with f (0) = 0 then we get cost function (26). It turns out this assumption on ∆ enables us to prove the combinatorial results of Section 3 and give a O(log n) approximation algorithm to find the optimal cost tree according to these cost functions. It is an interesting problem to investigate cost functions and algorithms for hierarchical clustering induced by other families of ∆ that arise from a k-HST on V, i.e., if the cost of T is defined as\ncost∆(T) := ∑ {i,j}∈E(Kn) κ(i, j)∆ (lca(i, j)) . (45)\nNote that not all choices of∆ lead to ameaningful cost function. For example, choosing∆(u) = diam (T[u])− 1 gives rise to the following cost function\ncost(T) := ∑ {i,j}∈E(Kn) κ(i, j)distT(i, j) (46)\nwhere distT(i, j) is the length of the unique path from i to j in T. In this case, the trivial clustering r, T∗ where T∗ is the star graph with V as its leaves and r as the root is always a minimizer; in other words, there is no incentive for spreading out the hierarchical clustering. Also worth mentioning is a long line of related work on fitting tree metrics to metric spaces (see e.g., [Ailon and Charikar, 2005, Räcke, 2008, Fakcharoenphol et al., 2003]). In this setting, the data points V are assumed to come from a metric space dV and the objective is to find a hierarchical clustering T so as to minimize ‖dV − dT‖p. If the points in V lie on the unit sphere and the similarity function κ is the cosine similarity κcos(i, j) = 1− dV(i, j)/2, then the problem of fitting a tree metric with p = 2 minimizes the same objective as cost function (46). Since dV ≤ 1 in this case, the minimizer is the trivial tree r, T∗ (as remarked above). In general, when the points in V are not constrained to lie on the unit sphere, the two problems are incomparable."
    }, {
      "heading" : "8 Acknowledgments",
      "text" : "Research reported in this paper was partially supported by NSF CAREER award CMMI-1452463 and NSF grant CMMI-1333789. We would like to thank Kunal Talwar and Mohit Singh for helpful discussions and anonymous reviewers for helping improve the presentation of this paper."
    }, {
      "heading" : "A Hardness of finding the optimal hierarchical clustering",
      "text" : "In this section we study the hardness of finding the optimal hierarchical clustering according to cost function (1). We show that under the assumption of the Small Set Expansion (SSE) hypothesis there is no constant factor approximation algorithm for this problem. We also show that no polynomial sized Linear Program (LP) or Semidefinite Program (SDP) can give a constant factor approximation for this problem without the need for any complexity theoretic assumptions. Both these results make use of the similarity of this problem with the minimum linear arrangement problem. To show hardness under Small Set Expansion, we make use of the result of [Raghavendra et al., 2012] showing that there is no constant factor approximation algorithm for the Minimum Linear Arrangement problem under the assumption of SSE. To show the LP and SDP inapproximability results, we make use of the reduction framework of [Braun et al., 2015] together with the NP-hardness proof for Minimum Linear Arrangement due to [Garey et al., 1976]. We also note that both these hardness results hold even for unweighted graphs (i.e., when κ ∈ {0, 1}).\nNote that the individual layer-t problem f-ILP-layer for t = bn/2c is equivalent to the minimum bisection problem for which the best known approximation is O(log n) due to [Räcke, 2008], while the best known bicriteria approximation is O (√ log n ) due to [Arora et al., 2009] and improving these approximation factors is a major open problem. However it is not clear if an improved approximation algorithm for hierarchical clustering under cost function (1) would imply an improved algorithm for every layer-t problem, which is why a constant factor inapproximability result is of interest. We start by recalling the definition of an optimization problem in the framework of [Braun et al., 2015].\nDefinitionA.1 (Optimization problem). [Braun et al., 2015] An optimization problem is a tupleP = (S , I, val) consisting of a set S of feasible solutions, a set I of instances, and a real-valued objective called measure val : I× S → R. We shall use valI (s) for the objective value of a feasible solution s ∈ S for an instance I ∈ I.\nSince we are interested in the integrality gaps of LP and SDP relaxations for an optimization problemP = (S , I, val), we represent the approximation gap by two functions C, S : I→ R where C is the completeness guarantee while S is the soundness guarantee. Note that the ratio C/S represents the approximation factor for the problem P . We recall below the formal definition of an LP relaxation of P that achieves a (C, S)approximation guarantee. We assume without loss of generality that P is a maximization problem.\nDefinition A.2 (LP formulation of an optimization problem). [Braun et al., 2015] Let P = (S , I, val) be an optimization problem, and C, S : I→ R. Then let IS := {I ∈ I |max valI ≤ S(I)} denote the set of sound instances, i.e., for which the soundness guarantee S is an upper bound on the maximum. A (C, S)approximate LP formulation of P consists of a linear program Ax ≤ b with x ∈ Rr for some r and the following realizations:\nFeasible solutions as vectors xs ∈ Rr for every s ∈ S satisfying\nAxs ≤ b for all s ∈ S , (47)\ni.e., the system Ax ≤ b is a relaxation of conv (xs | s ∈ S).\nInstances as affine functions wI : Rr → R for all I ∈ IS satisfying\nwI (xs) = valI (s) for all s ∈ S , (48)\ni.e., the linearization wI of valI is required to be exact on all xs with s ∈ S .\nAchieving (C, S) approximation guarantee by requiring\nmax {wI (x) | Ax ≤ b} ≤ C(I) for all I ∈ IS, (49)\nThe size of the formulation is the number of inequalities in Ax ≤ b. Finally, the (C, S)-approximate LP formulation complexity fcLP(P , C, S) of P is the minimal size of all its LP formulations.\nOne can similarly define a (C, S)-approximate SDP formulation for a problem P where instead of a LP, we now have a SDP relaxation A(X) = b with X ∈ Sr+ and where Sr+ denotes the space of r × r positive semidefinite matrices. The size of such an SDP formulation is measured by the dimension r and fcSDP(P , C, S) is defined as the minimum size of an SDP formulation achieving (C, S)-approximation for problemP . Belowwe recall the precise notion of a reduction between two problems as in [Braun et al., 2015].\nDefinition A.3 (Reduction). [Braun et al., 2015] Let P1 = (S1, I1, val) and P2 = (S2, I2, val) be optimization problems with guarantees C1, S1 and C2, S2, respectively. Let τ1 = +1 if P1 is a maximization problem, and τ1 = −1 if P1 is a minimization problem. Similarly, let τ2 = ±1 depending on whether P2 is a maximization problem or a minimization problem. A reduction from P1 to P2 respecting the guarantees consists of 1. two mappings: ∗ : I1 → I2 and ∗ : S1 → S2 translating instances and feasible solutions indepen-\ndently;\n2. two nonnegative I1 × S1 matrices M1, M2 subject to the conditions\nτ1 [C1(I1)− valI1(s1)] = τ2 [ C2(I∗1 )− valI∗1 (s ∗ 1) ]\nM1(I1, s1) + M2(I1, s1) (50-complete) τ2 OPT (I∗1 ) ≤ τ2S2(I∗1 ) if τ1 OPT (I1) ≤ τ1S1(I1). (50-sound)\nThe matrices M1 and M2 control the parameters of the reduction relating the integrality gap of relaxations for P1 to the integrality gap of corresponding relaxations for P2. For a matrix A, let rk+ A and rkpsd A denote the nonnegative rank and psd rank of A respectively. The following theorem is a restatement of Theorem 3.2 from [Braun et al., 2015] ignoring constants.\nTheorem A.4. [Braun et al., 2015] Let P1 and P2 be optimization problems with a reduction from P1 to P2 respecting the completeness guarantees C1, C2 and soundness guarantees S1, S2 of P1 and P2, respectively. Then\nfcLP(P1, C1, S1) ≤ rk+ M2 + rk+ M1 + rk+ M1 · fcLP(P2, C2, S2), (51) fcSDP(P1, C1, S1) ≤ rkpsd M2 + rkpsd M1 + rkpsd M1 · fcSDP(P2, C2, S2), (52)\nwhere M1 and M2 are the matrices in the reduction as in Definition A.3.\nTherefore to obtain a lower bound for problem P2, it suffices to find a source problem P1 and matrices M1 and M2 of low nonnegative rank and low psd rank, satisfying Definition A.3. Below, we cast the hierarchical clustering problem (HCLUST) as an optimization problem. We also recall a different formulation of cost function (1) due to [Dasgupta, 2016] that will be useful in the analysis of the reduction.\nDefinition A.5 (HCLUST as optimization problem). The minimization problem HCLUST of size n consists of\ninstances similarity function κ : E(Kn)→ R≥0 feasible solutions hierarchical clustering r, T of V(Kn)\nmeasure valκ(T) = ∑{i,j}∈E(Kn) κ(i, j) |leaves(T[lca(i, j)])|. Wewill alsomake use of the following alternate interpretation of cost function (1) given by [Dasgupta, 2016]. Let κ : V × V → R≥0 be an instance of HCLUST. For a subset S ⊆ V, a split S1, . . . , Sk is a partition of S into k disjoint pieces. For a binary split S1, S2 we can define κ(S1, S2) := ∑i∈S1,j∈S2 κ(i, j). This can be extended to k-way splits in the natural way:\nκ(S1, . . . , Sk) := ∑ 1≤i≤j≤k κ(Si, Sj).\nThen the cost of a tree T is the sum over all the internal nodes of the splitting costs at the nodes, as follows.\ncost(T) = ∑ splits S→(S1,...,Sk) in T |S| κ(S1, . . . , Sk).\nWe now briefly recall the MAXCUT problem.\nDefinition A.6 (MAXCUT as optimization problem). Themaximization problemMAXCUT of size n consists of\ninstances all graphs G with V(G) ⊆ [n]\nfeasible solutions all subsets X of [n]\nmeasure valG(X) = |δG(X)|.\nSimilarly, the Minimum Linear Arrangement problem can be phrased as an optimization problem as follows.\nDefinition A.7 (MLA as optimization problem). The minimization problem MLA of size n consists of\ninstances weight function w : E(Kn)→ R≥0 feasible solutions all permutations π : V(Kn)→ [n]\nmeasure valw(π) := ∑{i,j}∈E(Kn) w(i, j) |π(i)− π(j)|.\nWe now describe the reduction from MAXCUT to HCLUST which is a modification of the reduction from MAXCUT to MLA due to [Garey et al., 1976]. Note that an instance of MAXCUT maps to an unweighted instance of HCLUST, i.e., κ ∈ {0, 1}.\nMapping instances Given an instance G = (V, E) of MAXCUT of size n, let r = n4 andU = {u1, u2, . . . , ur}. The instance κ of HCLUST is on the graph with vertex set V ′ := V ∪U and has weights in {0, 1}. For any distinct pair i, j ∈ V ′, if {i, j} ∈ E then we define κ(i, j) := 0 and otherwise we set κ(i, j) := 1.\nMapping solutions Given a cut X ⊆ V of MAXCUT we map it to the clustering r, T of V ′ where the root r has the following children: n4 leaves corresponding to U, and 2 internal vertices corresponding to X and X. The internal vertices for X and X are split into |X| and\n∣∣X∣∣ leaves respectively at the next level.\nThe following lemma relates the LP and SDP formulations for MAXCUT and MLA.\nLemma A.8. For any completeness and soundness guarantee (C, S), we have the following\nfcLP (MAXCUT, C, S) ≤ fcLP ( HCLUST, C′, S′ ) + O(n2)\nfcSDP (MAXCUT, C, S) ≤ fcSDP ( HCLUST, C′, S′ ) + O(n2).\nwhere C′ := (n 4+n)3−(n4+n) 3 − C(n4 + n) and S′ := (n 4+n+1 3 )− Sn4.\nProof. To show completeness, we analyze the cost of the tree T that a cut X maps to, using the alternate interpretation of the cost function (1) due to [Dasgupta, 2016] (see above). Let H be the graph on vertex set V ′ induced by κ, i.e. {i, j} ∈ E(H) iff κ(i, j) = 1. Let H denote the complement graph of H and let κ be the similarity function induced by it, i.e., κ(i, j) = 1 iff {i, j} 6∈ E(H) and κ(i, j) = 0 otherwise. For a hierarchical clustering T of V ′, we denote by costH(T) and costH(T) the cost of T induced by κ and κ respectively, i.e., costH(T) := ∑{i,j}∈E(H) |leaves(T[lca(i, j)])| and costH(T) := ∑{i,j}6∈E(H) |leaves(T[lca(i, j)])|. Let X := V ′ \\ X. The cost of the tree T that the cut X maps to, is given by\ncost(T) = costH(T)\n=\n( n + n4 )3 − (n + n4) 3 − costH(T)\n=\n( n + n4 )3 − (n + n4) 3\n− ∑ splits S→(S1,...,Sk) in T |S| κ(S1, . . . , Sk)\n=\n( n + n4 )3 − (n + n4) 3 − ( n + n4 ) valG(X)− ( |X| |E[X]|+ ∣∣X∣∣ ∣∣E[X]∣∣) , where E[X] and E[X] are the edges of E(H) induced on the set X and X respectively. Therefore, we have the following completeness relationship between the two problems\nC− valG(X) = 1\nn + n4\n( cost(T)− ( (n + n4)3 − (n + n4)\n3 − C(n + n4)\n)) + |X| |E[X]|+ ∣∣X∣∣ ∣∣E[X]∣∣ n4 + n .\nWenowdefine thematrices M1 and M2 as M1(H, X) := 1n+n4 and M2(H, X) := |X| |E[X]|+ ∣∣X∣∣ ∣∣E[X]∣∣.\nClearly, M1 has O(1) nonnegative rank and psd rank. We claim that the nonnegative rank of M2 is at most 2(n2). The vectors vH ∈ R2( n 2) corresponding to the instances H is defined as the concatenation [uH, wH ] of two vectors uH, wH ∈ R( n 2). Both the vectors uH, wH encode the edges of H scaled by n4 + n, i.e., uH({i, j}) = wH({i, j}) = 1/(n4 + n) iff {i, j} ∈ E(H) and 0 otherwise. The vectors vX ∈ R2( n 2) corresponding to the solutions are also defined as the concatenation [uX, wX] of two vectors uX, wX ∈ Rn. The vector uX encodes the vertices in X scaled by |X| i.e., uX({i, j}) = |X| iff i, j ∈ X and 0 otherwise. The vector wX encodes the vertices in X scaled by\n∣∣X∣∣ i.e., wX({i, j}) = ∣∣X∣∣ iff i, j ∈ X and 0 otherwise. Clearly, we have M2(H, X) = 〈vH, vX〉 and so the nonnegative (and psd) rank of M2 is at most 2(n2). Soundness follows due to the analysis in [Garey et al., 1976] and by noting that the cost of a linear arrangement obtained by projecting the leaves of T is a lower bound on cost(T). By the analysis in [Garey et al., 1976] if the optimal value OPT(G) of MAXCUT is at most S, then the optimal value of MLA on V ′, κ is at least (n\n4+n+1 3 ) − Sn4. Therefore, it follows that the optimal value of HCLUST on V ′, κ is also at least\n(n 4+n+1\n3 )− Sn4.\nThe constant factor inapproximability result for HCLUST now follows due to the following theorems.\nTheorem A.9 ([Chan et al., 2013, Theorem 3.2]). For any ε > 0 there are infinitely many n such that\nfcLP\n( MAXCUT, 1− ε, 1\n2 +\nε\n6\n) ≥ nΩ(log n/ log log n).\nTheorem A.10 ([Braun et al., 2015, Theorem 7.1]). For any δ, ε > 0 there are infinitely many n such that\nfcSDP\n( MAXCUT,\n4 5 − ε, 3 4 + δ\n) = nΩ(log n/ log log n). (53)\nThus we have the following corollary about the LP and SDP inapproximability for the problem HCLUST.\nCorollary A.11 (LP and SDP hardness for HCLUST). For any constant c ≥ 1, HCLUST is LP-hard and SDP-hard with an inapproximability factor of c.\nProof. Straightforward by using Theorems A.9 and A.10 together with Lemma A.8 and by choosing n large enough.\nThe following lemma shows that a minor modification of the argument in [Raghavendra et al., 2012] also implies a constant factor inapproximability result under the Small Set Expansion (SSE) hypothesis. Note that this reduction is also true for unit capacity graphs, i.e., κ ∈ {0, 1}. We briefly recall the formulation of the Small Set Expansion hypothesis. Informally, given a graph G = (V, E) the problem is to decide whether all “small” sets in the graph are expanding. Let d(i) denote the degree of a vertex i ∈ V. For a subset S ⊆ V let µ(S) := |S| / |V| be the volume of S, and let φ(S) := E(S, S)/ ∑i∈S d(i) be the expansion of S. Then the SSE problem is defined as follows.\nDefinition A.12 (Small set expansion (SSE) hypothesis [Raghavendra et al., 2012]). For every constant η > 0, there exists sufficiently small δ > 0 such that given a graph G = (V, E), it is NP-hard to decide the following cases,\nCompleteness there exists a subset S ⊆ V with volume µ(S) = δ and expansion φ(S) ≤ η,\nSoundness every subset S ⊆ V of volume µ(S) = δ has expansion φ(S) ≥ 1− η.\nUnder this assumption, [Raghavendra et al., 2012] proved the following amplification result about the expansion of small sets in the graph.\nTheorem A.13 (Theorem 3.5 [Raghavendra et al., 2012]). For all q ∈ N and ε′, γ > 0 it is SSE-hard to distinguish the following for a given graph H = (VH, EH)\nCompleteness There exist disjoint sets S1, . . . , Sq ⊆ VH satisfying µ(Si) = 1q and φ(Si) ≤ ε′ + o(ε′) for all i ∈ [n],\nSoundness For all sets S ⊆ VH we have φ(S) ≥ φG(1− ε′/2)(µ(S))− γ/µ(S),\nwhere φG(1− ε′/2)(µ(S)) is the expansion of sets of volume µ(S) in the infinite Gaussian graph G(1− ε′/2).\nThe following lemma establishes that it is SSE-hard to approximateHCLUST to within any constant factor. The argument closely parallels Corollary A.5 of [Raghavendra et al., 2012] where it was shown that it is SSEhard to approximate MLA to within any constant factor.\nLemma A.14. Let G = (V, E) be a graph on V with κ induced by the edges E i.e., κ(i, j) = 1 iff {i, j} ∈ E and 0 otherwise. Then it is SSE-hard to distinguish between the following two cases\nCompleteness There exists a hierarchical clustering T of V with cost(T) ≤ εn |E|, Soundness Every hierarchical clustering T of V satisfies cost(T) ≥ c√εn |E|\nfor some constant c not depending on n.\nProof. Apply Theorem A.13 on the graph G with the following choice of parameters: q = d2/εe, ε′ = ε/3 and γ = ε. Suppose there exist S1, . . . , Sq ⊆ V satisfying φ(Si) ≤ ε′+ o(ε′) and |Si| = |V| /q ≤ ε |V| /2. Then consider the tree r, T with the root r having q children corresponding to each Si, and each Si being further separated into |Si| leaves at the next level. We claim that cost(T) ≤ εn |E|. We analyze this using the alternate interpretation of cost function (1) (see above). Every crossing edge between Si, Sj for distinct i, j ∈ [q] incurs a cost of n, but by assumption there are at most ε |E| /2 such edges. Further, any edge in Si incurs a cost nq ≤ εn/2 and thus their contribution is upper bounded by εn |E|. The analysis for soundness follows by the argument of Corollary A.5 in [Raghavendra et al., 2012]. In particular, if for every S ⊆ V we have φ(S) ≥ φG(1− ε′/2)(µ(S))− γ/µ(S) then the cost of the optimal linear arrangement on G is at most √ εn |E|. Since the cost of any tree (including the optimal tree) is at least the cost of the linear arrangement induced by projecting the leaf vertices, the claim about soundness follows."
    } ],
    "references" : [ {
      "title" : "Characterization of linkagebased clustering",
      "author" : [ "Ackerman et al", "M. 2010] Ackerman", "S. Ben-David", "D. Loker" ],
      "venue" : "In COLT,",
      "citeRegEx" : "al. et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2010
    }, {
      "title" : "Fitting tree metrics: Hierarchical clustering and phylogeny",
      "author" : [ "Ailon", "Charikar", "N. 2005] Ailon", "M. Charikar" ],
      "venue" : "In 46th Annual IEEE Symposium on Foundations of Computer Science",
      "citeRegEx" : "Ailon et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Ailon et al\\.",
      "year" : 2005
    }, {
      "title" : "Expander flows, geometric embeddings and graph partitioning",
      "author" : [ "Arora et al", "S. 2009] Arora", "S. Rao", "U. Vazirani" ],
      "venue" : "Journal of the ACM (JACM),",
      "citeRegEx" : "al. et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2009
    }, {
      "title" : "Relax, no need to round: Integrality of clustering formulations",
      "author" : [ "Awasthi et al", "P. 2015] Awasthi", "A.S. Bandeira", "M. Charikar", "R. Krishnaswamy", "S. Villar", "R. Ward" ],
      "venue" : "In Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science,",
      "citeRegEx" : "al. et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2015
    }, {
      "title" : "A discriminative framework for clustering via similarity functions",
      "author" : [ "Balcan et al", "2008] Balcan", "M.-F", "A. Blum", "S. Vempala" ],
      "venue" : "In Proceedings of the fortieth annual ACM symposium on Theory of computing,",
      "citeRegEx" : "al. et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2008
    }, {
      "title" : "Y",
      "author" : [ "Bartal" ],
      "venue" : "(1996). Probabilistic approximation of metric spaces and its algorithmic applications. In Foundations of Computer Science,",
      "citeRegEx" : "Bartal. 1996",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "M",
      "author" : [ "Y. Bartal", "B. Bollobás", "Mendel" ],
      "venue" : "(2001). A ramsey-type theorem for metric spaces and its applications for metrical task systems and related problems. In Foundations of Computer Science,",
      "citeRegEx" : "Bartal et al.. 2001",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Onmetric ramsey-type phenomena",
      "author" : [ "Bartal et al", "Y. 2003] Bartal", "N. Linial", "M. Mendel", "A. andNaor" ],
      "venue" : "In Proceedings of the thirty-fifth annual ACM symposium on Theory of computing,",
      "citeRegEx" : "al. et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2003
    }, {
      "title" : "Strong reductions for extended formulations",
      "author" : [ "Braun et al", "G. 2015] Braun", "S. Pokutta", "A. Roy" ],
      "venue" : "CoRR, abs/1512.04932. 29,",
      "citeRegEx" : "al. et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2015
    }, {
      "title" : "Approximate constraint satisfaction requires large lp relaxations",
      "author" : [ "Chan et al", "S.O. 2013] Chan", "J. Lee", "P. Raghavendra", "D. Steurer" ],
      "venue" : "In Foundations of Computer Science (FOCS),",
      "citeRegEx" : "al. et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2013
    }, {
      "title" : "Approximate hierarchical clustering via sparsest cut and spreading metrics",
      "author" : [ "Charikar", "Chatziafratis", "M. 2016] Charikar", "V. Chatziafratis" ],
      "venue" : "arXiv preprint arXiv:1609.09548",
      "citeRegEx" : "Charikar et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Charikar et al\\.",
      "year" : 2016
    }, {
      "title" : "A constant-factor approximation algorithm for the k-median problem",
      "author" : [ "Charikar et al", "M. 1999] Charikar", "S. Guha", "É. Tardos", "D.B. Shmoys" ],
      "venue" : "In Proceedings of the thirty-first annual ACM symposium on Theory of computing,",
      "citeRegEx" : "al. et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 1999
    }, {
      "title" : "A",
      "author" : [ "M. Charikar", "V. Guruswami", "Wirth" ],
      "venue" : "(2003). Clustering with qualitative information. In Foundations of Computer Science,",
      "citeRegEx" : "Charikar et al.. 2003",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "A dependent lp-rounding approach for the k-median problem",
      "author" : [ "Charikar", "Li", "M. 2012] Charikar", "S. Li" ],
      "venue" : "In Automata, Languages, and Programming,",
      "citeRegEx" : "Charikar et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Charikar et al\\.",
      "year" : 2012
    }, {
      "title" : "Performance guarantees for hierarchical clustering",
      "author" : [ "Dasgupta", "Long", "S. 2005] Dasgupta", "P.M. Long" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Dasgupta et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Dasgupta et al\\.",
      "year" : 2005
    }, {
      "title" : "Finding the closest ultrametric",
      "author" : [ "Di Summa et al", "M. 2015] Di Summa", "D. Pritchard", "L. Sanità" ],
      "venue" : "Discrete Applied Mathematics,",
      "citeRegEx" : "al. et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2015
    }, {
      "title" : "Fast approximate graph partitioning algorithms",
      "author" : [ "Even et al", "G. 1999] Even", "J. Naor", "S. Rao", "B. Schieber" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "al. et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 1999
    }, {
      "title" : "Divide-and-conquer approximation algorithms via spreading metrics",
      "author" : [ "Even et al", "G. 2000] Even", "J.S. Naor", "S. Rao", "B. Schieber" ],
      "venue" : "Journal of the ACM (JACM),",
      "citeRegEx" : "al. et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2000
    }, {
      "title" : "A tight bound on approximating arbitrary metrics by tree metrics",
      "author" : [ "Fakcharoenphol et al", "J. 2003] Fakcharoenphol", "S. Rao", "K. Talwar" ],
      "venue" : "In Proceedings of the thirty-fifth annual ACM symposium on Theory of computing,",
      "citeRegEx" : "al. et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2003
    }, {
      "title" : "The elements of statistical learning, volume 1. Springer series in statistics Springer, Berlin",
      "author" : [ "Friedman et al", "J. 2001] Friedman", "T. Hastie", "R. Tibshirani" ],
      "venue" : null,
      "citeRegEx" : "al. et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2001
    }, {
      "title" : "Some simplified np-complete graph problems",
      "author" : [ "Garey et al", "M.R. 1976] Garey", "D.S. Johnson", "L. Stockmeyer" ],
      "venue" : "Theoretical computer science, 1(3):237–267",
      "citeRegEx" : "al. et al\\.,? \\Q1976\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 1976
    }, {
      "title" : "Approximatemax-flowmin-(multi) cut theorems and their applications",
      "author" : [ "Garg et al", "N. 1996] Garg", "V.V. Vazirani", "M. andYannakakis" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "al. et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 1996
    }, {
      "title" : "Greedy facility location algorithms analyzed using dual fitting with factor-revealing lp",
      "author" : [ "Jain et al", "K. 2003] Jain", "M. Mahdian", "E. Markakis", "A. Saberi", "V.V. andVazirani" ],
      "venue" : "Journal of the ACM (JACM),",
      "citeRegEx" : "al. et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2003
    }, {
      "title" : "Approximation algorithms for metric facility location and k-median problems using the primal-dual schema and lagrangian relaxation",
      "author" : [ "Jain", "Vazirani", "K. 2001] Jain", "V.V. Vazirani" ],
      "venue" : "Journal of the ACM (JACM),",
      "citeRegEx" : "Jain et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Jain et al\\.",
      "year" : 2001
    }, {
      "title" : "The construction of hierarchic and nonhierarchic classifications",
      "author" : [ "Jardine", "Sibson", "N. 1968] Jardine", "R. Sibson" ],
      "venue" : "The Computer Journal,",
      "citeRegEx" : "Jardine et al\\.,? \\Q1968\\E",
      "shortCiteRegEx" : "Jardine et al\\.",
      "year" : 1968
    }, {
      "title" : "Partitioning graphs into balanced components",
      "author" : [ "Krauthgamer et al", "R. 2009] Krauthgamer", "J.S. Naor", "R. Schwartz" ],
      "venue" : "In Proceedings of the twentieth Annual ACM-SIAM Symposium on Discrete Algorithms,",
      "citeRegEx" : "al. et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2009
    }, {
      "title" : "S",
      "author" : [ "T. Leighton", "Rao" ],
      "venue" : "(1988). An approximate max-flow min-cut theorem for uniform multicommodity flow problems with applications to approximation algorithms. In Foundations of Computer Science,",
      "citeRegEx" : "Leighton and Rao. 1988",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "Multicommodity max-flow min-cut theorems and their use in designing approximation algorithms",
      "author" : [ "Leighton", "Rao", "T. 1999] Leighton", "S. Rao" ],
      "venue" : "Journal of the ACM (JACM),",
      "citeRegEx" : "Leighton et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Leighton et al\\.",
      "year" : 1999
    }, {
      "title" : "Approximating k-median via pseudoapproximation",
      "author" : [ "Li", "Svensson", "S. 2013] Li", "O. Svensson" ],
      "venue" : "In Proceedings of the forty-fifth annual ACM symposium on Theory of computing,",
      "citeRegEx" : "Li et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2013
    }, {
      "title" : "An experimental comparison of model-based clustering methods",
      "author" : [ "Meilă", "Heckerman", "M. 2001] Meilă", "D. Heckerman" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "Meilă et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Meilă et al\\.",
      "year" : 2001
    }, {
      "title" : "Approximating k-means-type clustering via semidefinite programming",
      "author" : [ "Peng", "Wei", "J. 2007] Peng", "Y. Wei" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "Peng et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2007
    }, {
      "title" : "A new theoretical framework for k-means-type clustering",
      "author" : [ "Peng", "Xia", "J. 2005] Peng", "Y. Xia" ],
      "venue" : "In Foundations and advances in data mining,",
      "citeRegEx" : "Peng et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2005
    }, {
      "title" : "Reductions between expansion problems",
      "author" : [ "Raghavendra et al", "P. 2012] Raghavendra", "D. Steurer", "M. Tulsiani" ],
      "venue" : "In Computational Complexity (CCC),",
      "citeRegEx" : "al. et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2012
    }, {
      "title" : "Factoring nonnegative matrices with linear programs",
      "author" : [ "Recht et al", "B. 2012] Recht", "C. Re", "J. Tropp", "V. Bittorf" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "al. et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2012
    }, {
      "title" : "Numerical taxonomy. The principles and practice of numerical classification",
      "author" : [ "Sneath et al", "P.H. 1973] Sneath", "Sokal", "R. R" ],
      "venue" : null,
      "citeRegEx" : "al. et al\\.,? \\Q1973\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 1973
    }, {
      "title" : "Hierarchical grouping to optimize an objective function",
      "author" : [ "Ward Jr.", "J.H. 1963] Ward Jr." ],
      "venue" : "Journal of the American statistical association,",
      "citeRegEx" : "Jr and Jr,? \\Q1963\\E",
      "shortCiteRegEx" : "Jr and Jr",
      "year" : 1963
    }, {
      "title" : "A uniqueness theorem for clustering",
      "author" : [ "Zadeh", "Ben-David", "R.B. 2009] Zadeh", "S. Ben-David" ],
      "venue" : "In Proceedings of the twenty-fifth conference on uncertainty in artificial intelligence,",
      "citeRegEx" : "Zadeh et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Zadeh et al\\.",
      "year" : 2009
    }, {
      "title" : "Wewill alsomake use of the following alternate interpretation of cost function (1) given by [Dasgupta, 2016",
      "author" : [ "leaves(T[lca(i" ],
      "venue" : null,
      "citeRegEx" : "leaves.T.lca.i and j...|.,? \\Q2016\\E",
      "shortCiteRegEx" : "leaves.T.lca.i and j...|.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 26,
      "context" : "We then show how to round an optimal fractional solution using the sphere growing technique first introduced in [Leighton and Rao, 1988] (see also [Garg et al.",
      "startOffset" : 112,
      "endOffset" : 136
    }, {
      "referenceID" : 26,
      "context" : "To round LP-ultrametric to get a feasible solution for ILP-ultrametric, we will use the technique of sphere growingwhich was introduced in [Leighton and Rao, 1988] to show anO(log n) approximation for the maximum multicommodity flow problem.",
      "startOffset" : 139,
      "endOffset" : 163
    }, {
      "referenceID" : 5,
      "context" : "In particular we briefly mention the related notion of hierarchically well-separated trees (HST) as defined in [Bartal, 1996] (see also [Bartal et al.",
      "startOffset" : 111,
      "endOffset" : 125
    } ],
    "year" : 2016,
    "abstractText" : "We study the cost function for hierarchical clusterings introduced by [Dasgupta, 2016] where hierarchies are treated as first-class objects rather than deriving their cost from projections into flat clusters. It was also shown in [Dasgupta, 2016] that a top-down algorithm returns a hierarchical clustering of cost at most O (αn log n) times the cost of the optimal hierarchical clustering, where αn is the approximation ratio of the Sparsest Cut subroutine used. Thus using the best known approximation algorithm for Sparsest Cut due to Arora-Rao-Vazirani, the top-down algorithm returns a hierarchical clustering of cost at most O ( log3/2 n ) times the cost of the optimal solution. We improve this by giving an O(log n)approximation algorithm for this problem. Our main technical ingredients are a combinatorial characterization of ultrametrics induced by this cost function, deriving an Integer Linear Programming (ILP) formulation for this family of ultrametrics, and showing how to iteratively round an LP relaxation of this formulation by using the idea of sphere growing which has been extensively used in the context of graph partitioning. We also prove that our algorithm returns an O(log n)-approximate hierarchical clustering for a generalization of this cost function also studied in [Dasgupta, 2016]. Experiments show that the hierarchies found by using the ILP formulation as well as our rounding algorithm often have better projections into flat clusters than the standard linkage based algorithms. We conclude with constant factor inapproximability results for this problem: 1) no polynomial size LP or SDP can achieve a constant factor approximation for this problem and 2) no polynomial time algorithm can achieve a constant factor approximation under the assumption of the Small Set Expansion hypothesis.",
    "creator" : "LaTeX with hyperref package"
  }
}