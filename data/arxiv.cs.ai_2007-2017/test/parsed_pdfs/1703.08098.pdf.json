{
  "name" : "1703.08098.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "An overview of embedding models of entities and relationships for knowledge base completion",
    "authors" : [ "Dat Quoc Nguyen" ],
    "emails" : [ "dat.nguyen@students.mq.edu.au" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: Knowledge base completion, link prediction, embedding model, triple classification, entity prediction."
    }, {
      "heading" : "1 Introduction",
      "text" : "Knowledge bases (KBs), such as WordNet (Miller, 1995), YAGO (Suchanek et al., 2007), Freebase (Bollacker et al., 2008) and DBpedia (Lehmann et al., 2015), represent relationships between entities as triples (head entity, relation, tail entity). Even very large knowledge bases are still far from complete (Socher et al., 2013; West et al., 2014). Knowledge base completion or link prediction systems (Nickel et al., 2016a) predict which triples not in a knowledge base are likely to be true (Taskar et al., 2004; Bordes et al., 2011).\nEmbedding models for KB completion associate entities and/or relations with dense feature vectors or matrices. Such models obtain the state-of-theart performances (Bordes et al., 2013; Wang et al., 2014; Guu et al., 2015; Nguyen et al., 2016a; Nguyen et al., 2016b) and generalize to large KBs (Krompaß et al., 2015). This article serves as a brief overview of embedding models for KB completion, with up-to-date experimental results on\ntwo standard evaluation tasks: i) the entity prediction task—which is also referred to as the link prediction task (Bordes et al., 2013)—and ii) the triple classification task (Socher et al., 2013)."
    }, {
      "heading" : "2 Embedding models for KB completion",
      "text" : ""
    }, {
      "heading" : "2.1 General idea",
      "text" : "Let E denote the set of entities and R the set of relation types. Denote by G the knowledge base consisting of a set of correct triples (h, r, t), such that h, t ∈ E and r ∈ R. For each triple (h, r, t), the embedding models define a score function f(h, r, t) of its implausibility. Their goal is to choose f such that the score f(h, r, t) of a plausible triple (h, r, t) is smaller than the score f(h′, r′, t′) of an implausible triple (h′, r′, t′).\nTable 1 summarizes different score functions f(h, r, t) and the optimization algorithms used to estimate model parameters, e.g., vanilla Stochastic Gradient Descent (SGD), SGD+AdaGrad (Duchi et al., 2011), SGD+AdaDelta (Zeiler, 2012) and L-BFGS (Liu and Nocedal, 1989). To learn model parameters (i.e. entity vectors, relation vectors or matrices), the embedding models minimize an objective function. A common objective function is the following margin-based function:\nL = ∑\n(h,r,t)∈G (h′,r,t′)∈G′\n(h,r,t)\n[γ + f(h, r, t)− f(h′, r, t′)]+\nwhere [x]+ = max(0, x), γ is the margin hyperparameter, and\nG′(h,r,t) = {(h ′, r, t) | h′ ∈ E , (h′, r, t) /∈ G}\n∪ {(h, r, t′) | t′ ∈ E , (h, r, t′) /∈ G}\nis the set of incorrect triples generated by corrupting the correct triple (h, r, t) ∈ G.\nar X\niv :1\n70 3.\n08 09\n8v 2\n[ cs\n.C L\n] 2\n8 M\nar 2\n01 7"
    }, {
      "heading" : "2.2 Specific models",
      "text" : "The Unstructured model (Bordes et al., 2012) assumes that the head and tail entity vectors are similar. As the Unstructured model does not take the relationship into account, it cannot distinguish different relation types. The Structured Embedding (SE) model (Bordes et al., 2011) extends the Unstructured model by assuming that the head and tail entities are similar only in a relation-dependent subspace, where each relation is represented by two different matrices. Futhermore, the SME model (Bordes et al., 2012) uses four different matrices to project entity and relation vectors into a subspace. The TransE model (Bordes et al., 2013) is inspired by models such as the Word2Vec Skipgram model (Mikolov et al., 2013) where relationships between words often correspond to translations in latent feature space.\nThe TransH model (Wang et al., 2014) associates each relation with a relation-specific hyper-\nplane and uses a projection vector to project entity vectors onto that hyperplane. TransD (Ji et al., 2015) and TransR/CTransR (Lin et al., 2015b) extend the TransH model by using two projection vectors and a matrix to project entity vectors into a relation-specific space, respectively. Similar to TransR, TransR-FT (Feng et al., 2016a) also uses a matrix to project head and tail entity vectors. TEKE H (Wang and Li, 2016) extends TransH to incorporate rich context information in an external text corpus. lppTransD (Yoon et al., 2016) extends TransD to additionally use two projection vectors for representing each relation. STransE (Nguyen et al., 2016b) and TranSparse (Ji et al., 2016) can be viewed as direct extensions of the TransR model, where head and tail entities are associated with their own projection matrices. Unlike STransE, TranSparse uses adaptive sparse matrices, whose sparse degrees are defined based on the number of entities linked by relations.\nDISTMULT (Yang et al., 2015) is based on the Bilinear model (Nickel et al., 2011; Bordes et al., 2012; Jenatton et al., 2012) where each relation is represented by a diagonal rather than a full matrix. The neural tensor network (NTN) model (Socher et al., 2013) uses a bilinear tensor operator to represent each relation while ER-MLP (Dong et al., 2014) and ProjE (Shi and Weninger, 2017) could be viewed as simplified versions of NTN. Such quadratic forms are also used to model entities and relations in KG2E (He et al., 2015), TransG (Xiao et al., 2016), ComplEx (Trouillon et al., 2016), TATEC (Garcı́a-Durán et al., 2016) and RSTE (Tay et al., 2017). In addition, HolE (Nickel et al., 2016b) uses circular correlation—a compositional operator—which could be interpreted as a compression of the tensor product.\nRecent research has shown that relation paths between entities in KBs provide richer context information and improve the performance of embedding models for KB completion (Luo et al., 2015; Lin et al., 2015a; Liang and Forbus, 2015; Garcı́aDurán et al., 2015; Guu et al., 2015; Toutanova et al., 2016; Nguyen et al., 2016a).\nLuo et al. (2015) constructed relation paths between entities and viewing entities and relations in the path as pseudo-words, and then applied Word2Vec algorithms (Mikolov et al., 2013) to produce pre-trained vectors for these pseudowords. Luo et al. (2015) showed that using these pre-trained vectors for initialization helps to improve the performance of the TransE (Bordes et al., 2013), SME (Bordes et al., 2012) and SE (Bordes et al., 2011). In addition, Liang and Forbus (2015) used the implausibility score produced by SME to compute the weights of relation paths.\nFurthermore, RTransE (Garcı́a-Durán et al., 2015), PTransE (Lin et al., 2015a) and TransECOMP (Guu et al., 2015) are extensions of the TransE model. These models similarly represent a relation path by a vector which is the sum of the vectors of all relations in the path, whereas in the Bilinear-COMP model (Guu et al., 2015) and the PRUNED-PATHS model (Toutanova et al., 2016), each relation is a matrix and so it represents the relation path by matrix multiplication. The neighborhood mixture model TransE-NMM (Nguyen et al., 2016a) can be also viewed as a three-relation path model as it takes into account the neighborhood entity and relation information of both head and tail entities in each triple.\nOther relation path-based models: The Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) is a random walk inference technique which was proposed to predict a new relationship between two entities in KBs. Lao et al. (2011) used PRA to estimate the probability of an unseen triple as a combination of weighted random walks that follow different paths linking the head entity and tail entity in the KB. Gardner et al. (2014) made use of an external text corpus to increase the connectivity of the KB used as the input to PRA. Gardner and Mitchell (2015) improved PRA by proposing a subgraph feature extraction technique to make the generation of random walks in KBs more efficient and expressive, while Wang et al. (2016) extended PRA to couple the path ranking of multiple relations. PRA can also be used in conjunction with first-order formula in the discriminative Gaifman model (Niepert, 2016). In addition, Neelakantan et al. (2015) used a recurrent neural network to learn vector representations of PRA-style relation paths between entities in the KB. Other randomwalk based learning algorithms for KB completion can be also found in Feng et al. (2016b), Liu et al. (2016) and Wei et al. (2016).\nSee other methods for learning from KBs and multi-relational data in Nickel et al. (2016a)."
    }, {
      "heading" : "3 Evaluation tasks",
      "text" : "Two standard tasks are proposed to evaluate embedding models for KB completion including: the entity prediction task, i.e. link prediction (Bordes et al., 2013), and the triple classification task (Socher et al., 2013).\nCommonly, the WN18 and FB15k datasets (Bordes et al., 2013) are used for entity prediction evaluation, while the WN11 and FB13 datasets\n(Socher et al., 2013) are used for triple classification. WN18 and WN11 are derived from the large lexical KB WordNet (Miller, 1995). FB15k and FB13 are derived from the large real-world fact KB FreeBase (Bollacker et al., 2008). Information about these datasets is given in Table 2."
    }, {
      "heading" : "3.1 Entity prediction",
      "text" : ""
    }, {
      "heading" : "3.1.1 Task description",
      "text" : "The entity prediction task, i.e. link prediction (Bordes et al., 2013), predicts the head or the tail entity given the relation type and the other entity, i.e. predicting h given (?, r, t) or predicting t given (h, r, ?) where ? denotes the missing element. The results are evaluated using a ranking induced by the function f(h, r, t) on test triples.\nEach correct test triple (h, r, t) is corrupted by replacing either its head or tail entity by each of the possible entities in turn, and then these candidates are ranked in ascending order of their implausibility score. This is called as the “Raw” setting\nprotocol. Furthermore, the “Filtered” setting protocol, described in Bordes et al. (2013), filters out before ranking any corrupted triples that appear in the KB. Ranking a corrupted triple appearing in the KB (i.e. a correct triple) higher than the original test triple is also correct, but is penalized by the “Raw” score, thus the “Filtered” setting provides a clearer view on the ranking performance.\nIn addition to the mean rank and the Hits@10 (i.e., the proportion of test triples for which the target entity was ranked in the top 10 predictions), which were originally used in the entity prediction task (Bordes et al., 2013), recent work also reports the mean reciprocal rank (MRR) which is commonly used in information retrieval. In both “Raw” and “Filtered” settings, mean rank is always greater or equal to 1 and the lower mean rank indicates better entity prediction performance. MRR and Hits@10 scores always range from 0.0 to 1.0, and higher score reflects better prediction result."
    }, {
      "heading" : "3.1.2 Main results",
      "text" : "Table 3 lists the entity prediction results of KB completion models on the WN18 and FB15k datasets. The first 18 rows report the performance of the models that do not exploit information about alternative paths between head and tail entities. The next 5 rows report results of the models that exploit information about relation paths. The last 3 rows present results for the models which make use of textual mentions derived from a large external corpus. It is clear that the models with the additional external corpus information obtained the best results. Table 3 also shows that the models employing path information generally achieve better results than models that do not use such information. In terms of models not exploiting path information or external information, the ComplEx model obtains highest scores on FB15k. In addition, the STransE model performs better than its closely related models SE, TransE, TransR, CTransR, TransD and TranSparse on both WN18 and FB15k."
    }, {
      "heading" : "3.2 Triple classification",
      "text" : ""
    }, {
      "heading" : "3.2.1 Task description",
      "text" : "The triple classification task was first introduced by Socher et al. (2013), and since then it has been used to evaluate various embedding models. The aim of this task is to predict whether a triple (h, r, t) is correct or not. For classification, a relation-specific threshold θr is set for each relation type r. If the implausibility score of an unseen test triple (h, r, t) is smaller than θr then the triple will be classified as correct, otherwise incorrect. Following Socher et al. (2013), the relationspecific thresholds are determined by maximizing the micro-averaged accuracy, which is a per-triple average, on the validation set."
    }, {
      "heading" : "3.2.2 Main results",
      "text" : "Table 4 presents the triple classification results of KB completion models on the WN11 and FB13 datasets.1 The first 6 rows report the performance of models that use TransE to initialize the entity and relation vectors. The last 11 rows present the accuracy of models with randomly initialized parameters. Note that there are higher results\n1In fact, as shown in tables 3 and 4, TransE obtains a very competitive performance on entity prediction and triple classification tasks. The similar observation is also found by Garcı́a-Durán et al. (2015), Lin et al. (2015a) and Garcı́aDurán et al. (2016). The reason is due to a careful grid search.\nreported for NTN, Bilinear-COMP and TransECOMP when entity vectors are initialized by averaging the pre-trained word vectors (Mikolov et al., 2013; Pennington et al., 2014). It is not surprising as many entity names in WordNet and FreeBase are lexically meaningful. It is possible for all other embedding models to utilize the pre-trained word vectors as well. However, as pointed out by Wang et al. (2014) and Guu et al. (2015), averaging the pre-trained word vectors for initializing entity vectors is an open problem and it is not always useful since entity names in many domain-specific KBs are not lexically meaningful."
    }, {
      "heading" : "4 Conclusions and further discussion",
      "text" : "This article presented an overview of embedding models of entity and relationships for KB completion. The article also provided update-to-date experimental results of the embedding models on the entity prediction and triple classification tasks.\nIt would be interesting to further explore those models for a new application where triple-style data is available. For example, Vu et al. (2017) extended the STransE model (Nguyen et al., 2016b) for the search personalization task in information retrieval, to model a user-oriented relationship between query and document."
    } ],
    "references" : [ {
      "title" : "Freebase: A Collaboratively Created Graph Database for Structuring Human Knowledge",
      "author" : [ "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor" ],
      "venue" : "In Proceedings of the 2008 ACM SIGMOD Interna-",
      "citeRegEx" : "Bollacker et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Bollacker et al\\.",
      "year" : 2008
    }, {
      "title" : "Learning Structured Embeddings of Knowledge Bases",
      "author" : [ "Jason Weston", "Ronan Collobert", "Yoshua Bengio" ],
      "venue" : "In Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Bordes et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2011
    }, {
      "title" : "A Semantic Matching Energy Function for Learning with Multirelational Data",
      "author" : [ "Xavier Glorot", "Jason Weston", "Yoshua Bengio" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Bordes et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2012
    }, {
      "title" : "Translating Embeddings for Modeling Multi-relational Data",
      "author" : [ "Nicolas Usunier", "Alberto Garcia-Duran", "Jason Weston", "Oksana Yakhnenko" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Bordes et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2013
    }, {
      "title" : "Knowledge vault: A web-scale approach to probabilistic knowledge fusion",
      "author" : [ "Dong et al.2014] Xin Dong", "Evgeniy Gabrilovich", "Geremy Heitz", "Wilko Horn", "Ni Lao", "Kevin Murphy", "Thomas Strohmann", "Shaohua Sun", "Wei Zhang" ],
      "venue" : null,
      "citeRegEx" : "Dong et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2014
    }, {
      "title" : "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization",
      "author" : [ "Duchi et al.2011] John Duchi", "Elad Hazan", "Yoram Singer" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Duchi et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2011
    }, {
      "title" : "2016a. Knowledge graph embedding by flexible translation",
      "author" : [ "Feng et al.2016a] Jun Feng", "Minlie Huang", "Mingdong Wang", "Mantong Zhou", "Yu Hao", "Xiaoyan Zhu" ],
      "venue" : "In Principles of Knowledge Representation and Reasoning: Proceedings of the Fifteenth",
      "citeRegEx" : "Feng et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Feng et al\\.",
      "year" : 2016
    }, {
      "title" : "2016b. GAKE: Graph Aware Knowledge Embedding",
      "author" : [ "Feng et al.2016b] Jun Feng", "Minlie Huang", "Yang Yang", "xiaoyan zhu" ],
      "venue" : "In Proceedings of COLING 2016,",
      "citeRegEx" : "Feng et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Feng et al\\.",
      "year" : 2016
    }, {
      "title" : "Composing Relationships with Translations",
      "author" : [ "Antoine Bordes", "Nicolas Usunier" ],
      "venue" : "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Garcı́a.Durán et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Garcı́a.Durán et al\\.",
      "year" : 2015
    }, {
      "title" : "Combining Two and Three-Way Embedding Models for Link Prediction in Knowledge",
      "author" : [ "Antoine Bordes", "Nicolas Usunier", "Yves Grandvalet" ],
      "venue" : null,
      "citeRegEx" : "Garcı́a.Durán et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Garcı́a.Durán et al\\.",
      "year" : 2016
    }, {
      "title" : "Efficient and Expressive Knowledge Base Completion Using Subgraph Feature Extraction",
      "author" : [ "Gardner", "Mitchell2015] Matt Gardner", "Tom Mitchell" ],
      "venue" : "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "Gardner et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gardner et al\\.",
      "year" : 2015
    }, {
      "title" : "Incorporating Vector Space Similarity in Random Walk Inference over Knowledge Bases",
      "author" : [ "Gardner et al.2014] Matt Gardner", "Partha P. Talukdar", "Jayant Krishnamurthy", "Tom M. Mitchell" ],
      "venue" : "In Proceedings of the 2014 Conference on Empirical Methods",
      "citeRegEx" : "Gardner et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Gardner et al\\.",
      "year" : 2014
    }, {
      "title" : "Traversing Knowledge Graphs in Vector Space",
      "author" : [ "Guu et al.2015] Kelvin Guu", "John Miller", "Percy Liang" ],
      "venue" : "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Guu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Guu et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning to Represent Knowledge Graphs with Gaussian Embedding",
      "author" : [ "He et al.2015] Shizhu He", "Kang Liu", "Guoliang Ji", "Jun Zhao" ],
      "venue" : "In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management,",
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "A latent factor model for highly multirelational data",
      "author" : [ "Nicolas L. Roux", "Antoine Bordes", "Guillaume R Obozinski" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Jenatton et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Jenatton et al\\.",
      "year" : 2012
    }, {
      "title" : "Knowledge Graph Embedding via Dynamic Mapping Matrix",
      "author" : [ "Ji et al.2015] Guoliang Ji", "Shizhu He", "Liheng Xu", "Kang Liu", "Jun Zhao" ],
      "venue" : "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th In-",
      "citeRegEx" : "Ji et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ji et al\\.",
      "year" : 2015
    }, {
      "title" : "Knowledge Graph Completion with Adaptive Sparse Transfer Matrix",
      "author" : [ "Ji et al.2016] Guoliang Ji", "Kang Liu", "Shizhu He", "Jun Zhao" ],
      "venue" : "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Ji et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ji et al\\.",
      "year" : 2016
    }, {
      "title" : "Type-Constrained Representation Learning in Knowledge Graphs",
      "author" : [ "Stephan Baier", "Volker Tresp" ],
      "venue" : "In Proceedings of the 14th International Semantic Web Conference,",
      "citeRegEx" : "Krompaß et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Krompaß et al\\.",
      "year" : 2015
    }, {
      "title" : "Relational retrieval using a combination of path-constrained random walks",
      "author" : [ "Lao", "Cohen2010] Ni Lao", "William W. Cohen" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Lao et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Lao et al\\.",
      "year" : 2010
    }, {
      "title" : "Random Walk Inference and Learning in a Large Scale Knowledge Base",
      "author" : [ "Lao et al.2011] Ni Lao", "Tom Mitchell", "William W. Cohen" ],
      "venue" : "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Lao et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Lao et al\\.",
      "year" : 2011
    }, {
      "title" : "DBpedia - A Large-scale",
      "author" : [ "Lehmann et al.2015] Jens Lehmann", "Robert Isele", "Max Jakob", "Anja Jentzsch", "Dimitris Kontokostas", "Pablo N. Mendes", "Sebastian Hellmann", "Mohamed Morsey", "Patrick van Kleef", "Sören Auer", "Christian Bizer" ],
      "venue" : null,
      "citeRegEx" : "Lehmann et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lehmann et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning Plausible Inferences from Semantic Web Knowledge by Combining Analogical Generalization with Structured Logistic Regression",
      "author" : [ "Liang", "Forbus2015] Chen Liang", "Kenneth D. Forbus" ],
      "venue" : "In Proceedings of the Twenty-Ninth AAAI Con-",
      "citeRegEx" : "Liang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Liang et al\\.",
      "year" : 2015
    }, {
      "title" : "Modeling Relation Paths for Representation Learning of Knowledge Bases",
      "author" : [ "Lin et al.2015a] Yankai Lin", "Zhiyuan Liu", "Huanbo Luan", "Maosong Sun", "Siwei Rao", "Song Liu" ],
      "venue" : "In Proceedings of the 2015 Conference on Empirical Methods in Nat-",
      "citeRegEx" : "Lin et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning Entity and Relation Embeddings for Knowledge Graph Completion",
      "author" : [ "Lin et al.2015b] Yankai Lin", "Zhiyuan Liu", "Maosong Sun", "Yang Liu", "Xuan Zhu" ],
      "venue" : "In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence Learn-",
      "citeRegEx" : "Lin et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2015
    }, {
      "title" : "On the Limited Memory BFGS Method for Large Scale Optimization",
      "author" : [ "Liu", "Nocedal1989] D.C. Liu", "J. Nocedal" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "Liu et al\\.,? \\Q1989\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 1989
    }, {
      "title" : "Hierarchical Random Walk Inference in Knowledge Graphs",
      "author" : [ "Liu et al.2016] Qiao Liu", "Liuyi Jiang", "Minghao Han", "Yao Liu", "Zhiguang Qin" ],
      "venue" : "In Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Infor-",
      "citeRegEx" : "Liu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2016
    }, {
      "title" : "Context-Dependent Knowledge Graph Embedding",
      "author" : [ "Luo et al.2015] Yuanfei Luo", "Quan Wang", "Bin Wang", "Li Guo" ],
      "venue" : "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Luo et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Luo et al\\.",
      "year" : 2015
    }, {
      "title" : "Distributed Representations of Words and Phrases and their Compositionality",
      "author" : [ "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "WordNet: A Lexical Database for English",
      "author" : [ "George A. Miller" ],
      "venue" : "Communications of the ACM,",
      "citeRegEx" : "Miller.,? \\Q1995\\E",
      "shortCiteRegEx" : "Miller.",
      "year" : 1995
    }, {
      "title" : "Compositional Vector Space Models for Knowledge Base Completion",
      "author" : [ "Benjamin Roth", "Andrew McCallum" ],
      "venue" : "In Proceedings of the 53rd Annual Meeting of the Association",
      "citeRegEx" : "Neelakantan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Neelakantan et al\\.",
      "year" : 2015
    }, {
      "title" : "Neighborhood Mixture Model for Knowledge Base Completion",
      "author" : [ "Kairit Sirts", "Lizhen Qu", "Mark Johnson" ],
      "venue" : "In Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning,",
      "citeRegEx" : "Nguyen et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2016
    }, {
      "title" : "2016b. STransE: a novel embedding model of entities and relationships in knowledge bases",
      "author" : [ "Kairit Sirts", "Lizhen Qu", "Mark Johnson" ],
      "venue" : "In Proceedings of the 2016 Conference of the North American Chapter",
      "citeRegEx" : "Nguyen et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2016
    }, {
      "title" : "A Three-Way Model for Collective Learning on Multi-Relational Data",
      "author" : [ "Volker Tresp", "Hans-Peter Kriegel" ],
      "venue" : "In Proceedings of the 28th International Conference on Machine Learning,",
      "citeRegEx" : "Nickel et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Nickel et al\\.",
      "year" : 2011
    }, {
      "title" : "2016a. A Review of Relational Machine Learning for Knowledge Graphs",
      "author" : [ "Kevin Murphy", "Volker Tresp", "Evgeniy Gabrilovich" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "Nickel et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Nickel et al\\.",
      "year" : 2016
    }, {
      "title" : "Holographic embeddings of knowledge graphs",
      "author" : [ "Lorenzo Rosasco", "Tomaso Poggio" ],
      "venue" : "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Nickel et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Nickel et al\\.",
      "year" : 2016
    }, {
      "title" : "Discriminative Gaifman Models",
      "author" : [ "Mathias Niepert" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Niepert.,? \\Q2016\\E",
      "shortCiteRegEx" : "Niepert.",
      "year" : 2016
    }, {
      "title" : "Glove: Global Vectors for Word Representation",
      "author" : [ "Richard Socher", "Christopher Manning" ],
      "venue" : "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Pennington et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "ProjE: Embedding Projection for Knowledge Graph Completion",
      "author" : [ "Shi", "Weninger2017] Baoxu Shi", "Tim Weninger" ],
      "venue" : "In Proceedings of the 31st AAAI Conference on Artificial Intelligence",
      "citeRegEx" : "Shi et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Shi et al\\.",
      "year" : 2017
    }, {
      "title" : "Reasoning With Neural Tensor Networks for Knowledge Base Completion",
      "author" : [ "Danqi Chen", "Christopher D Manning", "Andrew Ng" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Socher et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "YAGO: A Core of Semantic Knowledge",
      "author" : [ "Gjergji Kasneci", "Gerhard Weikum" ],
      "venue" : "In Proceedings of the 16th International Conference on World Wide Web,",
      "citeRegEx" : "Suchanek et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Suchanek et al\\.",
      "year" : 2007
    }, {
      "title" : "Link Prediction in Relational Data",
      "author" : [ "Taskar et al.2004] Ben Taskar", "Ming fai Wong", "Pieter Abbeel", "Daphne Koller" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Taskar et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Taskar et al\\.",
      "year" : 2004
    }, {
      "title" : "Random Semantic Tensor Ensemble for Scalable Knowledge Graph Link Prediction",
      "author" : [ "Tay et al.2017] Yi Tay", "Anh Tuan Luu", "Siu Cheung Hui", "Falk Brauer" ],
      "venue" : "In Proceedings of the Tenth ACM International Conference on Web Search and Data Min-",
      "citeRegEx" : "Tay et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Tay et al\\.",
      "year" : 2017
    }, {
      "title" : "Observed Versus Latent Features for Knowledge Base and Text Inference",
      "author" : [ "Toutanova", "Chen2015] Kristina Toutanova", "Danqi Chen" ],
      "venue" : "In Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality,",
      "citeRegEx" : "Toutanova et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Toutanova et al\\.",
      "year" : 2015
    }, {
      "title" : "Compositional Learning of Embeddings for Relation Paths in Knowledge Base and Text",
      "author" : [ "Victoria Lin", "Wen-tau Yih", "Hoifung Poon", "Chris Quirk" ],
      "venue" : "In Proceedings of the 54th Annual Meeting",
      "citeRegEx" : "Toutanova et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Toutanova et al\\.",
      "year" : 2016
    }, {
      "title" : "Complex Embeddings for Simple Link Prediction",
      "author" : [ "Johannes Welbl", "Sebastian Riedel", "Éric Gaussier", "Guillaume Bouchard" ],
      "venue" : "In Proceedings of the 33nd International Conference on Machine Learning,",
      "citeRegEx" : "Trouillon et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Trouillon et al\\.",
      "year" : 2016
    }, {
      "title" : "Search Personalization with Embeddings",
      "author" : [ "Vu et al.2017] Thanh Vu", "Dat Quoc Nguyen", "Mark Johnson", "Dawei Song", "Alistair Willis" ],
      "venue" : "In Proceedings of the 39th European Conference on Information Retrieval",
      "citeRegEx" : "Vu et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Vu et al\\.",
      "year" : 2017
    }, {
      "title" : "Text-Enhanced Representation Learning for Knowledge Graph",
      "author" : [ "Wang", "Li2016] Zhigang Wang", "Juan-Zi Li" ],
      "venue" : "In Proceedings of the TwentyFifth International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "Wang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "Knowledge Graph Embedding by Translating on Hyperplanes",
      "author" : [ "Wang et al.2014] Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen" ],
      "venue" : "In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Wang et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2014
    }, {
      "title" : "Knowledge Base Completion via Coupled Path Ranking",
      "author" : [ "Wang et al.2016] Quan Wang", "Jing Liu", "Yuanfei Luo", "Bin Wang", "Chin-Yew Lin" ],
      "venue" : "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Wang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "Mining Inference Formulas by Goal-Directed Random Walks",
      "author" : [ "Wei et al.2016] Zhuoyu Wei", "Jun Zhao", "Kang Liu" ],
      "venue" : "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Wei et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wei et al\\.",
      "year" : 2016
    }, {
      "title" : "Knowledge Base Completion via Search-based Question Answering",
      "author" : [ "West et al.2014] Robert West", "Evgeniy Gabrilovich", "Kevin Murphy", "Shaohua Sun", "Rahul Gupta", "Dekang Lin" ],
      "venue" : "In Proceedings of the 23rd International Conference on World",
      "citeRegEx" : "West et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "West et al\\.",
      "year" : 2014
    }, {
      "title" : "TransG : A Generative Model for Knowledge Graph Embedding",
      "author" : [ "Xiao et al.2016] Han Xiao", "Minlie Huang", "Xiaoyan Zhu" ],
      "venue" : "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Xiao et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Xiao et al\\.",
      "year" : 2016
    }, {
      "title" : "SSP: semantic space projection for knowledge graph embedding with text descriptions",
      "author" : [ "Xiao et al.2017] Han Xiao", "Minlie Huang", "Xiaoyan Zhu" ],
      "venue" : "In Proceedings of the 31st AAAI Conference on Artificial Intelligence",
      "citeRegEx" : "Xiao et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Xiao et al\\.",
      "year" : 2017
    }, {
      "title" : "Embedding Entities and Relations for Learning and Inference in Knowledge Bases",
      "author" : [ "Yang et al.2015] Bishan Yang", "Wen-tau Yih", "Xiaodong He", "Jianfeng Gao", "Li Deng" ],
      "venue" : "In Proceedings of the International Conference on Learning Representations",
      "citeRegEx" : "Yang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2015
    }, {
      "title" : "A Translation-Based Knowledge Graph Embedding Preserving Logical Property of Relations",
      "author" : [ "Yoon et al.2016] Hee-Geun Yoon", "Hyun-Je Song", "Seong-Bae Park", "Se-Young Park" ],
      "venue" : "In Proceedings of the 2016 Conference of the North Amer-",
      "citeRegEx" : "Yoon et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Yoon et al\\.",
      "year" : 2016
    }, {
      "title" : "ADADELTA: An Adaptive Learning Rate Method. CoRR, abs/1212.5701",
      "author" : [ "Matthew D. Zeiler" ],
      "venue" : null,
      "citeRegEx" : "Zeiler.,? \\Q2012\\E",
      "shortCiteRegEx" : "Zeiler.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 28,
      "context" : "Knowledge bases (KBs), such as WordNet (Miller, 1995), YAGO (Suchanek et al.",
      "startOffset" : 39,
      "endOffset" : 53
    }, {
      "referenceID" : 39,
      "context" : "Knowledge bases (KBs), such as WordNet (Miller, 1995), YAGO (Suchanek et al., 2007), Freebase (Bollacker et al.",
      "startOffset" : 60,
      "endOffset" : 83
    }, {
      "referenceID" : 0,
      "context" : ", 2007), Freebase (Bollacker et al., 2008) and DBpedia (Lehmann et al.",
      "startOffset" : 18,
      "endOffset" : 42
    }, {
      "referenceID" : 20,
      "context" : ", 2008) and DBpedia (Lehmann et al., 2015), represent relationships between entities as triples (head entity, relation, tail entity).",
      "startOffset" : 20,
      "endOffset" : 42
    }, {
      "referenceID" : 38,
      "context" : "Even very large knowledge bases are still far from complete (Socher et al., 2013; West et al., 2014).",
      "startOffset" : 60,
      "endOffset" : 100
    }, {
      "referenceID" : 50,
      "context" : "Even very large knowledge bases are still far from complete (Socher et al., 2013; West et al., 2014).",
      "startOffset" : 60,
      "endOffset" : 100
    }, {
      "referenceID" : 40,
      "context" : ", 2016a) predict which triples not in a knowledge base are likely to be true (Taskar et al., 2004; Bordes et al., 2011).",
      "startOffset" : 77,
      "endOffset" : 119
    }, {
      "referenceID" : 1,
      "context" : ", 2016a) predict which triples not in a knowledge base are likely to be true (Taskar et al., 2004; Bordes et al., 2011).",
      "startOffset" : 77,
      "endOffset" : 119
    }, {
      "referenceID" : 3,
      "context" : "Such models obtain the state-of-theart performances (Bordes et al., 2013; Wang et al., 2014; Guu et al., 2015; Nguyen et al., 2016a; Nguyen et al., 2016b) and generalize to large KBs (Krompaß et al.",
      "startOffset" : 52,
      "endOffset" : 154
    }, {
      "referenceID" : 47,
      "context" : "Such models obtain the state-of-theart performances (Bordes et al., 2013; Wang et al., 2014; Guu et al., 2015; Nguyen et al., 2016a; Nguyen et al., 2016b) and generalize to large KBs (Krompaß et al.",
      "startOffset" : 52,
      "endOffset" : 154
    }, {
      "referenceID" : 12,
      "context" : "Such models obtain the state-of-theart performances (Bordes et al., 2013; Wang et al., 2014; Guu et al., 2015; Nguyen et al., 2016a; Nguyen et al., 2016b) and generalize to large KBs (Krompaß et al.",
      "startOffset" : 52,
      "endOffset" : 154
    }, {
      "referenceID" : 17,
      "context" : ", 2016b) and generalize to large KBs (Krompaß et al., 2015).",
      "startOffset" : 37,
      "endOffset" : 59
    }, {
      "referenceID" : 3,
      "context" : "This article serves as a brief overview of embedding models for KB completion, with up-to-date experimental results on two standard evaluation tasks: i) the entity prediction task—which is also referred to as the link prediction task (Bordes et al., 2013)—and ii) the",
      "startOffset" : 234,
      "endOffset" : 255
    }, {
      "referenceID" : 38,
      "context" : "triple classification task (Socher et al., 2013).",
      "startOffset" : 27,
      "endOffset" : 48
    }, {
      "referenceID" : 5,
      "context" : ", vanilla Stochastic Gradient Descent (SGD), SGD+AdaGrad (Duchi et al., 2011), SGD+AdaDelta (Zeiler, 2012) and L-BFGS (Liu and Nocedal, 1989).",
      "startOffset" : 57,
      "endOffset" : 77
    }, {
      "referenceID" : 55,
      "context" : ", 2011), SGD+AdaDelta (Zeiler, 2012) and L-BFGS (Liu and Nocedal, 1989).",
      "startOffset" : 22,
      "endOffset" : 36
    }, {
      "referenceID" : 2,
      "context" : "The Unstructured model (Bordes et al., 2012) assumes that the head and tail entity vectors are similar.",
      "startOffset" : 23,
      "endOffset" : 44
    }, {
      "referenceID" : 1,
      "context" : "The Structured Embedding (SE) model (Bordes et al., 2011) extends the Unstructured model by assuming that the head and tail entities are similar only in a relation-dependent subspace, where each relation is represented by two different matrices.",
      "startOffset" : 36,
      "endOffset" : 57
    }, {
      "referenceID" : 2,
      "context" : "Futhermore, the SME model (Bordes et al., 2012) uses four different matrices to project entity and relation vectors into a subspace.",
      "startOffset" : 26,
      "endOffset" : 47
    }, {
      "referenceID" : 3,
      "context" : "The TransE model (Bordes et al., 2013) is inspired by models such as the Word2Vec Skipgram model (Mikolov et al.",
      "startOffset" : 17,
      "endOffset" : 38
    }, {
      "referenceID" : 27,
      "context" : ", 2013) is inspired by models such as the Word2Vec Skipgram model (Mikolov et al., 2013) where relationships between words often correspond to translations in latent feature space.",
      "startOffset" : 66,
      "endOffset" : 88
    }, {
      "referenceID" : 47,
      "context" : "The TransH model (Wang et al., 2014) associates each relation with a relation-specific hyperplane and uses a projection vector to project entity vectors onto that hyperplane.",
      "startOffset" : 17,
      "endOffset" : 36
    }, {
      "referenceID" : 15,
      "context" : "TransD (Ji et al., 2015) and TransR/CTransR (Lin et al.",
      "startOffset" : 7,
      "endOffset" : 24
    }, {
      "referenceID" : 54,
      "context" : "lppTransD (Yoon et al., 2016) extends TransD to additionally use two projection vectors for representing each relation.",
      "startOffset" : 10,
      "endOffset" : 29
    }, {
      "referenceID" : 16,
      "context" : ", 2016b) and TranSparse (Ji et al., 2016) can be viewed as direct extensions of the TransR model, where head and tail entities are associated with their own projection matrices.",
      "startOffset" : 24,
      "endOffset" : 41
    }, {
      "referenceID" : 53,
      "context" : "DISTMULT (Yang et al., 2015) is based on the Bilinear model (Nickel et al.",
      "startOffset" : 9,
      "endOffset" : 28
    }, {
      "referenceID" : 32,
      "context" : ", 2015) is based on the Bilinear model (Nickel et al., 2011; Bordes et al., 2012; Jenatton et al., 2012) where each relation is represented by a diagonal rather than a full matrix.",
      "startOffset" : 39,
      "endOffset" : 104
    }, {
      "referenceID" : 2,
      "context" : ", 2015) is based on the Bilinear model (Nickel et al., 2011; Bordes et al., 2012; Jenatton et al., 2012) where each relation is represented by a diagonal rather than a full matrix.",
      "startOffset" : 39,
      "endOffset" : 104
    }, {
      "referenceID" : 14,
      "context" : ", 2015) is based on the Bilinear model (Nickel et al., 2011; Bordes et al., 2012; Jenatton et al., 2012) where each relation is represented by a diagonal rather than a full matrix.",
      "startOffset" : 39,
      "endOffset" : 104
    }, {
      "referenceID" : 38,
      "context" : "The neural tensor network (NTN) model (Socher et al., 2013) uses a bilinear tensor operator to represent each relation while ER-MLP (Dong et al.",
      "startOffset" : 38,
      "endOffset" : 59
    }, {
      "referenceID" : 4,
      "context" : ", 2013) uses a bilinear tensor operator to represent each relation while ER-MLP (Dong et al., 2014) and ProjE (Shi and Weninger, 2017) could be viewed as simplified versions of NTN.",
      "startOffset" : 80,
      "endOffset" : 99
    }, {
      "referenceID" : 13,
      "context" : "Such quadratic forms are also used to model entities and relations in KG2E (He et al., 2015), TransG (Xiao et al.",
      "startOffset" : 75,
      "endOffset" : 92
    }, {
      "referenceID" : 51,
      "context" : ", 2015), TransG (Xiao et al., 2016), ComplEx (Trouillon et al.",
      "startOffset" : 16,
      "endOffset" : 35
    }, {
      "referenceID" : 44,
      "context" : ", 2016), ComplEx (Trouillon et al., 2016), TATEC (Garcı́a-Durán et al.",
      "startOffset" : 17,
      "endOffset" : 41
    }, {
      "referenceID" : 9,
      "context" : ", 2016), TATEC (Garcı́a-Durán et al., 2016) and RSTE (Tay et al.",
      "startOffset" : 15,
      "endOffset" : 43
    }, {
      "referenceID" : 41,
      "context" : ", 2016) and RSTE (Tay et al., 2017).",
      "startOffset" : 17,
      "endOffset" : 35
    }, {
      "referenceID" : 27,
      "context" : "(2015) constructed relation paths between entities and viewing entities and relations in the path as pseudo-words, and then applied Word2Vec algorithms (Mikolov et al., 2013) to produce pre-trained vectors for these pseudo-",
      "startOffset" : 152,
      "endOffset" : 174
    }, {
      "referenceID" : 3,
      "context" : "(2015) showed that using these pre-trained vectors for initialization helps to improve the performance of the TransE (Bordes et al., 2013), SME (Bordes et al.",
      "startOffset" : 117,
      "endOffset" : 138
    }, {
      "referenceID" : 2,
      "context" : ", 2013), SME (Bordes et al., 2012) and SE (Bordes et al.",
      "startOffset" : 13,
      "endOffset" : 34
    }, {
      "referenceID" : 1,
      "context" : ", 2012) and SE (Bordes et al., 2011).",
      "startOffset" : 15,
      "endOffset" : 36
    }, {
      "referenceID" : 23,
      "context" : "Luo et al. (2015) showed that using these pre-trained vectors for initialization helps to improve the performance of the TransE (Bordes et al.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 1,
      "context" : "(2015) showed that using these pre-trained vectors for initialization helps to improve the performance of the TransE (Bordes et al., 2013), SME (Bordes et al., 2012) and SE (Bordes et al., 2011). In addition, Liang and Forbus (2015) used the implausibility score produced by SME to compute the weights of relation paths.",
      "startOffset" : 118,
      "endOffset" : 233
    }, {
      "referenceID" : 8,
      "context" : "Furthermore, RTransE (Garcı́a-Durán et al., 2015), PTransE (Lin et al.",
      "startOffset" : 21,
      "endOffset" : 49
    }, {
      "referenceID" : 12,
      "context" : ", 2015a) and TransECOMP (Guu et al., 2015) are extensions of the TransE model.",
      "startOffset" : 24,
      "endOffset" : 42
    }, {
      "referenceID" : 12,
      "context" : "These models similarly represent a relation path by a vector which is the sum of the vectors of all relations in the path, whereas in the Bilinear-COMP model (Guu et al., 2015) and the PRUNED-PATHS model (Toutanova et al.",
      "startOffset" : 158,
      "endOffset" : 176
    }, {
      "referenceID" : 43,
      "context" : ", 2015) and the PRUNED-PATHS model (Toutanova et al., 2016), each relation is a matrix and so it represents the relation path by matrix multiplication.",
      "startOffset" : 35,
      "endOffset" : 59
    }, {
      "referenceID" : 8,
      "context" : "Furthermore, RTransE (Garcı́a-Durán et al., 2015), PTransE (Lin et al., 2015a) and TransECOMP (Guu et al., 2015) are extensions of the TransE model. These models similarly represent a relation path by a vector which is the sum of the vectors of all relations in the path, whereas in the Bilinear-COMP model (Guu et al., 2015) and the PRUNED-PATHS model (Toutanova et al., 2016), each relation is a matrix and so it represents the relation path by matrix multiplication. The neighborhood mixture model TransE-NMM (Nguyen et al., 2016a) can be also viewed as a three-relation path model as it takes into account the neighborhood entity and relation information of both head and tail entities in each triple. Other relation path-based models: The Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) is a random walk inference technique which was proposed to predict a new relationship between two entities in KBs. Lao et al. (2011) used PRA to estimate the probability of an unseen triple as a combination of weighted random walks that follow different paths linking the head entity and tail entity in the KB.",
      "startOffset" : 22,
      "endOffset" : 928
    }, {
      "referenceID" : 8,
      "context" : "Furthermore, RTransE (Garcı́a-Durán et al., 2015), PTransE (Lin et al., 2015a) and TransECOMP (Guu et al., 2015) are extensions of the TransE model. These models similarly represent a relation path by a vector which is the sum of the vectors of all relations in the path, whereas in the Bilinear-COMP model (Guu et al., 2015) and the PRUNED-PATHS model (Toutanova et al., 2016), each relation is a matrix and so it represents the relation path by matrix multiplication. The neighborhood mixture model TransE-NMM (Nguyen et al., 2016a) can be also viewed as a three-relation path model as it takes into account the neighborhood entity and relation information of both head and tail entities in each triple. Other relation path-based models: The Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) is a random walk inference technique which was proposed to predict a new relationship between two entities in KBs. Lao et al. (2011) used PRA to estimate the probability of an unseen triple as a combination of weighted random walks that follow different paths linking the head entity and tail entity in the KB. Gardner et al. (2014) made use of an external text corpus to increase the connectivity of the KB used as the input to PRA.",
      "startOffset" : 22,
      "endOffset" : 1128
    }, {
      "referenceID" : 8,
      "context" : "Furthermore, RTransE (Garcı́a-Durán et al., 2015), PTransE (Lin et al., 2015a) and TransECOMP (Guu et al., 2015) are extensions of the TransE model. These models similarly represent a relation path by a vector which is the sum of the vectors of all relations in the path, whereas in the Bilinear-COMP model (Guu et al., 2015) and the PRUNED-PATHS model (Toutanova et al., 2016), each relation is a matrix and so it represents the relation path by matrix multiplication. The neighborhood mixture model TransE-NMM (Nguyen et al., 2016a) can be also viewed as a three-relation path model as it takes into account the neighborhood entity and relation information of both head and tail entities in each triple. Other relation path-based models: The Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) is a random walk inference technique which was proposed to predict a new relationship between two entities in KBs. Lao et al. (2011) used PRA to estimate the probability of an unseen triple as a combination of weighted random walks that follow different paths linking the head entity and tail entity in the KB. Gardner et al. (2014) made use of an external text corpus to increase the connectivity of the KB used as the input to PRA. Gardner and Mitchell (2015) improved PRA by proposing a subgraph feature extraction technique to make the generation of random walks in KBs more efficient and expressive, while Wang et al.",
      "startOffset" : 22,
      "endOffset" : 1257
    }, {
      "referenceID" : 8,
      "context" : "Furthermore, RTransE (Garcı́a-Durán et al., 2015), PTransE (Lin et al., 2015a) and TransECOMP (Guu et al., 2015) are extensions of the TransE model. These models similarly represent a relation path by a vector which is the sum of the vectors of all relations in the path, whereas in the Bilinear-COMP model (Guu et al., 2015) and the PRUNED-PATHS model (Toutanova et al., 2016), each relation is a matrix and so it represents the relation path by matrix multiplication. The neighborhood mixture model TransE-NMM (Nguyen et al., 2016a) can be also viewed as a three-relation path model as it takes into account the neighborhood entity and relation information of both head and tail entities in each triple. Other relation path-based models: The Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) is a random walk inference technique which was proposed to predict a new relationship between two entities in KBs. Lao et al. (2011) used PRA to estimate the probability of an unseen triple as a combination of weighted random walks that follow different paths linking the head entity and tail entity in the KB. Gardner et al. (2014) made use of an external text corpus to increase the connectivity of the KB used as the input to PRA. Gardner and Mitchell (2015) improved PRA by proposing a subgraph feature extraction technique to make the generation of random walks in KBs more efficient and expressive, while Wang et al. (2016) extended PRA to couple the path ranking of multiple relations.",
      "startOffset" : 22,
      "endOffset" : 1425
    }, {
      "referenceID" : 35,
      "context" : "model (Niepert, 2016).",
      "startOffset" : 6,
      "endOffset" : 21
    }, {
      "referenceID" : 29,
      "context" : "In addition, Neelakantan et al. (2015) used a recurrent neural network to learn vector representations of PRA-style relation paths between entities in the KB.",
      "startOffset" : 13,
      "endOffset" : 39
    }, {
      "referenceID" : 6,
      "context" : "can be also found in Feng et al. (2016b), Liu et al.",
      "startOffset" : 21,
      "endOffset" : 41
    }, {
      "referenceID" : 6,
      "context" : "can be also found in Feng et al. (2016b), Liu et al. (2016) and Wei et al.",
      "startOffset" : 21,
      "endOffset" : 60
    }, {
      "referenceID" : 6,
      "context" : "can be also found in Feng et al. (2016b), Liu et al. (2016) and Wei et al. (2016). See other methods for learning from KBs and multi-relational data in Nickel et al.",
      "startOffset" : 21,
      "endOffset" : 82
    }, {
      "referenceID" : 6,
      "context" : "can be also found in Feng et al. (2016b), Liu et al. (2016) and Wei et al. (2016). See other methods for learning from KBs and multi-relational data in Nickel et al. (2016a).",
      "startOffset" : 21,
      "endOffset" : 174
    }, {
      "referenceID" : 3,
      "context" : "link prediction (Bordes et al., 2013), and the triple classification task (Socher et al.",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 38,
      "context" : ", 2013), and the triple classification task (Socher et al., 2013).",
      "startOffset" : 44,
      "endOffset" : 65
    }, {
      "referenceID" : 3,
      "context" : "Commonly, the WN18 and FB15k datasets (Bordes et al., 2013) are used for entity prediction evaluation, while the WN11 and FB13 datasets",
      "startOffset" : 38,
      "endOffset" : 59
    }, {
      "referenceID" : 1,
      "context" : "SE (Bordes et al., 2011) 1011 68.",
      "startOffset" : 3,
      "endOffset" : 24
    }, {
      "referenceID" : 2,
      "context" : "8 Unstructured (Bordes et al., 2012) 315 35.",
      "startOffset" : 15,
      "endOffset" : 36
    }, {
      "referenceID" : 2,
      "context" : "3 SME (Bordes et al., 2012) 545 65.",
      "startOffset" : 6,
      "endOffset" : 27
    }, {
      "referenceID" : 47,
      "context" : "8 TransH (Wang et al., 2014) 401 73.",
      "startOffset" : 9,
      "endOffset" : 28
    }, {
      "referenceID" : 13,
      "context" : "2 KG2E (He et al., 2015) 342 80.",
      "startOffset" : 7,
      "endOffset" : 24
    }, {
      "referenceID" : 15,
      "context" : "0 TransD (Ji et al., 2015) 224 79.",
      "startOffset" : 9,
      "endOffset" : 26
    }, {
      "referenceID" : 54,
      "context" : "3 lppTransD (Yoon et al., 2016) 283 80.",
      "startOffset" : 12,
      "endOffset" : 31
    }, {
      "referenceID" : 16,
      "context" : "7 TranSparse (Ji et al., 2016) 223 80.",
      "startOffset" : 13,
      "endOffset" : 30
    }, {
      "referenceID" : 9,
      "context" : "5 TATEC (Garcı́a-Durán et al., 2016) - - - - - - - - - 58 76.",
      "startOffset" : 8,
      "endOffset" : 36
    }, {
      "referenceID" : 38,
      "context" : "7 NTN (Socher et al., 2013) - - - - - - - 66.",
      "startOffset" : 6,
      "endOffset" : 27
    }, {
      "referenceID" : 53,
      "context" : "25 DISTMULT (Yang et al., 2015) - - - - - - - 94.",
      "startOffset" : 12,
      "endOffset" : 31
    }, {
      "referenceID" : 44,
      "context" : "35 ComplEx (Trouillon et al., 2016) - - 0.",
      "startOffset" : 11,
      "endOffset" : 35
    }, {
      "referenceID" : 32,
      "context" : "524 RESCAL (Nickel et al., 2011) [*] - - 0.",
      "startOffset" : 11,
      "endOffset" : 32
    }, {
      "referenceID" : 3,
      "context" : "354 TransE (Bordes et al., 2013) [*] - - 0.",
      "startOffset" : 11,
      "endOffset" : 32
    }, {
      "referenceID" : 8,
      "context" : "RTransE (Garcı́a-Durán et al., 2015) - - - - - - - - - 50 76.",
      "startOffset" : 8,
      "endOffset" : 36
    }, {
      "referenceID" : 35,
      "context" : "8 Gaifman (Niepert, 2016) - - - - - - 352 93.",
      "startOffset" : 10,
      "endOffset" : 25
    }, {
      "referenceID" : 25,
      "context" : "2 Hiri (Liu et al., 2016) - - - - - - - 90.",
      "startOffset" : 7,
      "endOffset" : 25
    }, {
      "referenceID" : 52,
      "context" : "0 SSP (Xiao et al., 2017) 168 81.",
      "startOffset" : 6,
      "endOffset" : 25
    }, {
      "referenceID" : 38,
      "context" : "The results for NTN (Socher et al., 2013) listed in this table are taken from Yang et al.",
      "startOffset" : 20,
      "endOffset" : 41
    }, {
      "referenceID" : 32,
      "context" : "The results for NTN (Socher et al., 2013) listed in this table are taken from Yang et al. (2015) since NTN was originally evaluated only for triple classification.",
      "startOffset" : 21,
      "endOffset" : 97
    }, {
      "referenceID" : 29,
      "context" : "[*]: Results from the implementation of Nickel et al. (2016b) because these results are higher than those previously published in Bordes et al.",
      "startOffset" : 40,
      "endOffset" : 62
    }, {
      "referenceID" : 1,
      "context" : "(2016b) because these results are higher than those previously published in Bordes et al. (2013).",
      "startOffset" : 76,
      "endOffset" : 97
    }, {
      "referenceID" : 38,
      "context" : "(Socher et al., 2013) are used for triple classifica-",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 28,
      "context" : "WN18 and WN11 are derived from the large lexical KB WordNet (Miller, 1995).",
      "startOffset" : 60,
      "endOffset" : 74
    }, {
      "referenceID" : 0,
      "context" : "FB15k and FB13 are derived from the large real-world fact KB FreeBase (Bollacker et al., 2008).",
      "startOffset" : 70,
      "endOffset" : 94
    }, {
      "referenceID" : 3,
      "context" : "link prediction (Bordes et al., 2013), predicts the head or the tail entity given the relation type and the other entity, i.",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 1,
      "context" : "Furthermore, the “Filtered” setting protocol, described in Bordes et al. (2013), filters out",
      "startOffset" : 59,
      "endOffset" : 80
    }, {
      "referenceID" : 3,
      "context" : ", the proportion of test triples for which the target entity was ranked in the top 10 predictions), which were originally used in the entity prediction task (Bordes et al., 2013), recent work also reports the mean reciprocal rank (MRR) which is commonly used in information retrieval.",
      "startOffset" : 157,
      "endOffset" : 178
    }, {
      "referenceID" : 38,
      "context" : "The triple classification task was first introduced by Socher et al. (2013), and since then it has been used to evaluate various embedding mod-",
      "startOffset" : 55,
      "endOffset" : 76
    }, {
      "referenceID" : 38,
      "context" : "Following Socher et al. (2013), the relationspecific thresholds are determined by maximizing the micro-averaged accuracy, which is a per-triple average, on the validation set.",
      "startOffset" : 10,
      "endOffset" : 31
    }, {
      "referenceID" : 27,
      "context" : "reported for NTN, Bilinear-COMP and TransECOMP when entity vectors are initialized by averaging the pre-trained word vectors (Mikolov et al., 2013; Pennington et al., 2014).",
      "startOffset" : 125,
      "endOffset" : 172
    }, {
      "referenceID" : 36,
      "context" : "reported for NTN, Bilinear-COMP and TransECOMP when entity vectors are initialized by averaging the pre-trained word vectors (Mikolov et al., 2013; Pennington et al., 2014).",
      "startOffset" : 125,
      "endOffset" : 172
    }, {
      "referenceID" : 8,
      "context" : "The similar observation is also found by Garcı́a-Durán et al. (2015), Lin et al.",
      "startOffset" : 41,
      "endOffset" : 69
    }, {
      "referenceID" : 8,
      "context" : "The similar observation is also found by Garcı́a-Durán et al. (2015), Lin et al. (2015a) and Garcı́aDurán et al.",
      "startOffset" : 41,
      "endOffset" : 89
    }, {
      "referenceID" : 8,
      "context" : "The similar observation is also found by Garcı́a-Durán et al. (2015), Lin et al. (2015a) and Garcı́aDurán et al. (2016). The reason is due to a careful grid search.",
      "startOffset" : 41,
      "endOffset" : 120
    }, {
      "referenceID" : 8,
      "context" : "The similar observation is also found by Garcı́a-Durán et al. (2015), Lin et al. (2015a) and Garcı́aDurán et al. (2016). The reason is due to a careful grid search. reported for NTN, Bilinear-COMP and TransECOMP when entity vectors are initialized by averaging the pre-trained word vectors (Mikolov et al., 2013; Pennington et al., 2014). It is not surprising as many entity names in WordNet and FreeBase are lexically meaningful. It is possible for all other embedding models to utilize the pre-trained word vectors as well. However, as pointed out by Wang et al. (2014) and Guu et al.",
      "startOffset" : 41,
      "endOffset" : 572
    }, {
      "referenceID" : 8,
      "context" : "The similar observation is also found by Garcı́a-Durán et al. (2015), Lin et al. (2015a) and Garcı́aDurán et al. (2016). The reason is due to a careful grid search. reported for NTN, Bilinear-COMP and TransECOMP when entity vectors are initialized by averaging the pre-trained word vectors (Mikolov et al., 2013; Pennington et al., 2014). It is not surprising as many entity names in WordNet and FreeBase are lexically meaningful. It is possible for all other embedding models to utilize the pre-trained word vectors as well. However, as pointed out by Wang et al. (2014) and Guu et al. (2015), averaging the pre-trained word vectors for initializing entity vectors is an open problem and it is not always useful since entity names in many domain-specific KBs are not lexically meaningful.",
      "startOffset" : 41,
      "endOffset" : 594
    }, {
      "referenceID" : 15,
      "context" : "TransD (Ji et al., 2015) 86.",
      "startOffset" : 7,
      "endOffset" : 24
    }, {
      "referenceID" : 16,
      "context" : "TranSparse-S (Ji et al., 2016) 86.",
      "startOffset" : 13,
      "endOffset" : 30
    }, {
      "referenceID" : 16,
      "context" : "TranSparse-US (Ji et al., 2016) 86.",
      "startOffset" : 14,
      "endOffset" : 31
    }, {
      "referenceID" : 38,
      "context" : "NTN (Socher et al., 2013) 70.",
      "startOffset" : 4,
      "endOffset" : 25
    }, {
      "referenceID" : 47,
      "context" : "TransH (Wang et al., 2014) 78.",
      "startOffset" : 7,
      "endOffset" : 26
    }, {
      "referenceID" : 13,
      "context" : "KG2E (He et al., 2015) 85.",
      "startOffset" : 5,
      "endOffset" : 22
    }, {
      "referenceID" : 12,
      "context" : "Bilinear-COMP (Guu et al., 2015) 77.",
      "startOffset" : 14,
      "endOffset" : 32
    }, {
      "referenceID" : 12,
      "context" : "TransE-COMP (Guu et al., 2015) 80.",
      "startOffset" : 12,
      "endOffset" : 30
    }, {
      "referenceID" : 51,
      "context" : "TransG (Xiao et al., 2016) 87.",
      "startOffset" : 7,
      "endOffset" : 26
    }, {
      "referenceID" : 54,
      "context" : "lppTransD (Yoon et al., 2016) 86.",
      "startOffset" : 10,
      "endOffset" : 29
    }, {
      "referenceID" : 3,
      "context" : "TransE (Bordes et al., 2013) [*] 85.",
      "startOffset" : 7,
      "endOffset" : 28
    }, {
      "referenceID" : 30,
      "context" : "[*]: TransE results from the implementation of Nguyen et al. (2016a) because these results are higher than those first reported in Wang et al.",
      "startOffset" : 47,
      "endOffset" : 69
    }, {
      "referenceID" : 30,
      "context" : "[*]: TransE results from the implementation of Nguyen et al. (2016a) because these results are higher than those first reported in Wang et al. (2014).",
      "startOffset" : 47,
      "endOffset" : 150
    }, {
      "referenceID" : 43,
      "context" : "For example, Vu et al. (2017) extended the STransE model (Nguyen et al.",
      "startOffset" : 13,
      "endOffset" : 30
    } ],
    "year" : 2017,
    "abstractText" : "Knowledge bases of real-world facts about entities and their relationships are useful resources for a variety of natural language processing tasks. However, because knowledge bases are typically incomplete, it is useful to be able to perform knowledge base completion or link prediction, i.e., predict whether a relationship not in the knowledge base is likely to be true. This article presents an overview of embedding models of entities and relationships for knowledge base completion.",
    "creator" : "LaTeX with hyperref package"
  }
}