{"conference": "ICLR 2017 conference submission", "title": "Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks", "abstract": "There is a lot of research interest in encoding variable length sentences into fixed length vectors, in a way that preserves the sentence meanings. Two common methods include representations based on averaging word vectors, and representations based on the hidden states of recurrent neural networks such as LSTMs. The sentence vectors are used as features for subsequent machine learning tasks or for pre-training in the context of deep learning. However, not much is known about the properties that are encoded in these sentence representations and about the language information they capture. We propose a framework that facilitates better understanding of the encoded representations. We define prediction tasks around isolated aspects of sentence structure (namely sentence length, word content, and word order), and score representations by the ability to train a classifier to solve each prediction task when using the representation as input. We demonstrate the potential contribution of the approach by analyzing different sentence representation mechanisms. The analysis sheds light on the relative strengths of different sentence embedding methods with respect to these low level prediction tasks, and on the effect of the encoded vector\u2019s dimensionality on the resulting representations.", "reviews": [{"is_meta_review": true, "comments": "This paper presents a set of experiments investigating what kinds of information are captured in common unsupervised approaches to sentence representation learning. The results are non-trivial and somewhat surprising. For example, they show that it is possible to reconstruct word order from bag of words representations, and they show that LSTM sentence autoencoders encode interpretable features even for randomly permuted nonsense sentences.\n\nEffective unsupervised sentence representation learning is an important and largely unsolved problem in NLP, and this kind of work seems like it should be straightforwardly helpful towards that end. In addition, the experimental paradigm presented here is likely more broadly applicable to a range of representation learning systems. Some of the results seem somewhat strange, but I see no major technical concerns, and think that that they are informative. I recommend acceptance.\n\nOne minor red flag: \n- The massive drop in CBOW performance in Figures 1b and 4b are not explained, and seem implausible enough to warrant serious further investigation. Can you be absolutely certain that those results would appear with a different codebase and different random seed implementing the same model? Fortunately, this point is largely orthogonal to the major results of the paper.\n\nTwo writing comments:\n- I agree that the results with word order and CBOW are surprising, but I think it's slightly misleading to say that CBOW is predictive of word order. It doesn't represent word order at all, but it's possible to probabilistically reconstruct word order from the information that it does encode.\n- Saying that \"LSTM auto-encoders are more effective at encoding word order than word content\" doesn't really make sense. These two quantities aren't comparable.", "IS_META_REVIEW": true}, {"TITLE": "ICLR committee final decision", "OTHER_KEYS": "ICLR 2017 pcs", "comments": "The area chair agrees with the reviewers and think this paper would be of interest to the ICLR audience. There is clearly more to be done in this area, but the authors do a good job shedding some light on what sentence embeddings can encode. We need more work like this that helps us understand what neural networks can model.", "IS_META_REVIEW": false, "DATE": "06 Feb 2017", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Review", "is_meta_review": false, "comments": "\nThe authors present a methodology for analyzing sentence embedding techniques by checking how much the embeddings preserve information about sentence length, word content, and word order. They examine several popular embedding methods including autoencoding LSTMs, averaged word vectors, and skip-thought vectors. The experiments are thorough and provide interesting insights into the representational power of common sentence embedding strategies, such as the fact that word ordering is surprisingly low-entropy conditioned on word content.\n\nExploring what sort of information is encoded in representation learning methods for NLP is an important and under-researched area. For example, the tide of word-embeddings research was mostly stemmed after a thread of careful experimental results showing most embeddings to be essentially equivalent, culminating in \"Improving Distributional Similarity with Lessons Learned from Word Embeddings\" by Levy, Goldberg, and Dagan. As representation learning becomes even more important in NLP this sort of research will be even more important.\n\nWhile this paper makes a valuable contribution in setting out and exploring a methodology for evaluating sentence embeddings, the evaluations themselves are quite simple and do not necessarily correlate with real-world desiderata for sentence embeddings (as the authors note in other comments, performance on these tasks is not a normative measure of embedding quality). For example, as the authors note, the ability of the averaged vector to encode sentence length is trivially to be expected given the central limit theorem (or more accurately, concentration inequalities like Hoeffding's inequality).\n\nThe word-order experiments were interesting. A relevant citation for this sort of conditional ordering procedure is \"Generating Text with Recurrent Neural Networks\" by Sutskever, Martens, and Hinton, who refer to the conversion of a bag of words into a sentence as \"debagging.\"\n\nAlthough this is just a first step in better understanding of sentence embeddings, it is an important one and I recommend this paper for publication.", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Experimental analysis of unsupervised sentence embeddings", "is_meta_review": false, "comments": "This paper analyzes various unsupervised sentence embedding approaches by means of a set of auxiliary prediction tasks. By examining how well classifiers can predict word order, word content, and sentence length, the authors aim to assess how much and what type of information is captured by the different embedding models. The main focus is on a comparison between and encoder-decoder model (ED) and a permutation-invariant model, CBOW. (There is also an analysis of skip-thought vectors, but since it was trained on a different corpus it is hard to compare).\n\nThere are several interesting and perhaps counter-intuitive results that emerge from this analysis and the authors do a nice job of examining those results and, for the most part, explaining them. However, I found the discussion of the word-order experiment rather unsatisfying. It seems to me that the appropriate question should have been something like, 'How well does model X do compared to the theoretical upper bound which can be deduced from natural language statistics?' This is investigated from one angle in Section 7, but I would have preferred to the effect of natural language statistics discussed up front rather than presented as the explanation to a 'surprising' observation. I had a similar reaction to the word-order experiments.\n\nMost of the interesting results, in my opinion, are about the ED model. It is fascinating that the LSTM encoder does not seem to rely on natural-language ordering statistics -- it seems like doing so should be a big win in terms of per-parameter expressivity. I also think that it's strange that word content accuracy begins to drop for high-dimensional embeddings. I suppose this could be investigated by handicapping the decoder.\n\nOverall, this is a very nice paper investigating some aspects of the information content stored in various types of sentence embeddings. I recommend acceptance.", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Interesting analytic results on unsupervised sentence encoders", "is_meta_review": false, "comments": "This paper presents a set of experiments investigating what kinds of information are captured in common unsupervised approaches to sentence representation learning. The results are non-trivial and somewhat surprising. For example, they show that it is possible to reconstruct word order from bag of words representations, and they show that LSTM sentence autoencoders encode interpretable features even for randomly permuted nonsense sentences.\n\nEffective unsupervised sentence representation learning is an important and largely unsolved problem in NLP, and this kind of work seems like it should be straightforwardly helpful towards that end. In addition, the experimental paradigm presented here is likely more broadly applicable to a range of representation learning systems. Some of the results seem somewhat strange, but I see no major technical concerns, and think that that they are informative. I recommend acceptance.\n\nOne minor red flag: \n- The massive drop in CBOW performance in Figures 1b and 4b are not explained, and seem implausible enough to warrant serious further investigation. Can you be absolutely certain that those results would appear with a different codebase and different random seed implementing the same model? Fortunately, this point is largely orthogonal to the major results of the paper.\n\nTwo writing comments:\n- I agree that the results with word order and CBOW are surprising, but I think it's slightly misleading to say that CBOW is predictive of word order. It doesn't represent word order at all, but it's possible to probabilistically reconstruct word order from the information that it does encode.\n- Saying that \"LSTM auto-encoders are more effective at encoding word order than word content\" doesn't really make sense. These two quantities aren't comparable. ", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "11 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Encoding of sentence length in embedding norm", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "is_meta_review": false}, {"TITLE": "Questions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "is_meta_review": false}, {"TITLE": "Detail questions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_META_REVIEW": false, "DATE": "19 Nov 2016", "is_meta_review": false}, {"TITLE": "applications of task (c)", "OTHER_KEYS": "Jiaqi Mu", "comments": "We (J. Mu and P. Viswanath) enjoyed the tour-de-force comparison of a vast variety of sentence representation algorithms all in one compact manuscript. Of particular interest to us were the three  \"synthetic\" tasks introduced here:  (a) to what extent the sentence representation encodes its length; (b) to what extent the sentence representation encodes the identities of words within it and (c) to what extent the sentence representation encodes word order.  \n\nThe best part of these tasks is that they are very well defined and labeling does not need any (expert) supervision at all and can be done over the entire corpus too. We have a reasonable intuition on why tasks (a) and (b) might be interesting/relevant for downstream tasks: the length of a sentence could be a proxy for the amount of content in the sentence; testing of a word within a sentence could be a type of test for the topic embedded in the sentence. \n\nBut we aren't so clear as to what might be the sense in which task (c) could be useful for downstream applications. One instance where this matters seems to be cause-effect relationships. For example,  the order of 'Mary' and 'John' is critical in 'Mary stole an apple from John.' Such a pair of words tend to be  named entities, however. \n\nThe tests presented in this manuscript worked with a random pair of words (and not just named entities or scenarios where cause-effect relationship mattered). We would love to hear what the authors think about the use cases of task (c) and our conjecture that they are particularly relevant in cause-effect scenarios. ", "IS_META_REVIEW": false, "DATE": "17 Nov 2016", "is_meta_review": false}], "SCORE": 8, "authors": "Yossi Adi, Einat Kermany, Yonatan Belinkov, Ofer Lavi, Yoav Goldberg", "KEYWORDS": "A method for analyzing sentence embeddings on a fine-grained level using auxiliary prediction tasks", "accepted": true, "id": ""}
