{
  "name" : "1707.07344.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Event Coreference Resolution by Iteratively Unfolding Inter-dependencies among Events",
    "authors" : [ "Prafulla Kumar Choubey", "Ruihong Huang" ],
    "emails" : [ "huangrh)@tamu.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Event coreference resolution is the task of identifying event mentions and clustering them such that each cluster represents a unique real world event. The capability of resolving links among coreferring event identities is vital for information aggregation and many NLP applications, including topic detection and tracking, information extraction, question answering and text summarization (Humphreys et al., 1997; Allan et al., 1998; Daniel et al., 2003; Narayanan and Harabagiu, 2004; Mayfield et al., 2009; Zhang et al., 2015). Yet, studies on event coreference are few com-\npared to the well-studied entity coreference resolution.\nEvent mentions that refer to the same event can occur both within a document (WD) and across multiple documents (CD). One common practice (Lee et al., 2012) to approach CD coreference task is to resolve event coreference in a megadocument created by concatenating topic-relevant documents, which essentially does not distinguish WD and CD event links.\nHowever, intuitively, recognizing CD coreferent event pairs requires stricter evidence compared to WD event linking because it is riskier to link two event mentions from two distinct documents rather than the same document. In a perfect scenario where all WD event mentions are properly clustered and their participants and arguments are combined within a cluster, CD clustering can be performed with ease as sufficient evidences are collected through initial WD clustering. Therefore, another very common practice for event coreference is to first group event mentions within a document and then group WD clusters across documents (Yang et al., 2015).\nNonetheless, WD coreference chains are equally hard to resolve. Event mentions in the same document can look very dissimilar (”killed/ VB” and ”murder/ NN”), have event arguments (i.e., participants and spatio-temporal information of an event (Bejan and Harabagiu, 2010)) partially or entirely omitted, or appear in distinct contexts compared to their antecedent event mentions, partially to avoid repetitions. Under this irresolute state, approaching WD and CD individually is incompetent.\nWhile CD coreference resolution is overall difficult, we observe that some CD coreferent event mentions, especially the ones that appear at the beginning of documents, share sufficient contexts and are relatively easier to resolve. At the same\nar X\niv :1\n70 7.\n07 34\n4v 1\n[ cs\n.C L\n] 2\n3 Ju\nl 2 01\n7\ntime, many of them bear sufficient differences that can bring in new information and further lead to more WD merges and consequently more CD merges.\nGuided by these observations, we present an event coreference approach that exploits interdependencies among event mentions within an event chain both within a document and across documents, by sequentially applying WD and CD merges in an alternating manner until no more merge can be made. We combine argument features of event mentions after each CD (WD) merge in order to resolve more difficult WD (CD) merges in the following iterations. Furthermore, our model uses two distinct pairwise classifiers that are separately trained with features intrinsic to each type. Specifically, the WD classifier uses features based on event mentions and their arguments while the CD classifier relies on features characterizing surrounding contexts of event mentions as well.\nWe further exploit second-order interdependencies across event clusters in order to resolve additional WD and CD coreferent event pairs. Intuitively, if two event mentions are related to the same set of events, it is likely that the two event mentions refer to the same real world event, even when their word forms and local contexts are distinct. Specifically, we merge event clusters if their event mentions are tightly associated (i.e., having the same dependency relations) or loosely associated (i.e., co-occurring in the same sentential context) with enough (i.e., passing a threshold) other events that are known coreferent.\nExperimental results on the benchmark event coreference dataset, ECB+ (Cybulska and Vossen, 2014b,a), show that our model extensively exploits inter-dependencies between events and outperforms the state-of-the-art methods for both WD and CD event coreference resolution."
    }, {
      "heading" : "2 Related Work",
      "text" : "Different approaches, focusing on either of WD or CD coreference chains, have been proposed for event coreference resolution. Works specific to WD event coreference includes pairwise classifiers (Ahn, 2006; Chen et al., 2009) graph based clustering method (Chen and Ji, 2009), information propagation (Liu et al., 2014), and markov logic networks Lu et al. (2016). As to only CD event coreference, Cybulska and Vossen (2015a)\ncreated pairwise classifiers using features indicating granularities of event slots and in another work (2015b), grouped events based on compatibilities of event contexts.\nLike this work, several studies have considered both WD and CD event coreference resolution task together. However to simplify the problem, they (Lee et al., 2012; Bejan and Harabagiu, 2010, 2014) created a meta-document by concatenating topic-relevant documents and treated both as an identical task. Most recently, Yang et al. (2015) applied a two-level clustering model that first groups event mentions within a document and then groups WD clusters across documents in a joint inference process. Our approach advances these works and emphasizes on different natures of WD and CD clusters along with the benefits of distinguishing WD merges from CD merges and exploiting their mutual dependencies.\nIterative models, in general, have been applied to both entity coreference resolution (Singh et al., 2009; Clark and Manning, 2015, 2016; Wiseman et al., 2016) and prior event coreference resolution (Lee et al., 2012) works, which gradually build clusters and enable later merges to benefit from earlier ones. Especially, Lee et al. (2012) used an iterative model to jointly build entity and event clusters and showed the advantages of information flow between entity and event clusters through semantic role features. Our model, by alternating between WD and CD merges, allows the multi-level flow of first order interdependencies. Moreover, additional cross cluster merges based on 2nd order interdependencies effectively exploits the semantic relations among events, in contrast to only semantic roles (between events and arguments) used in previous work."
    }, {
      "heading" : "3 System Overview and A Worked Example",
      "text" : "Inter-dependencies among event mentions can be effectively exploited by conducting sequential WD and CD merges in an iterative manner. In addition, recognizing second order relations between event chains relies on adequate number of event mentions that are already linked. Therefore, our model conducts event coreference in two stages. In the first stage, it iteratively conducts WD and CD merges as suggested by pairwise WD and CD merging classifiers respectively. Argument features of individual event mentions are propagated\nwithin a cluster after each merge operation. In the second stage, it explores second order relations across event clusters w.r.t context event mentions in order to carefully generate candidate event clusters and perform further merging.\nThe example in Figure 1 illustrates the two stages of our proposed approach. It shows two iterations of WD and CD merges. In iteration 1, relatively easy coreferent event mentions were linked, including the two shooting and two trial event mentions in doc1 and doc2 as well as the event mentions presented, trial and murder across the two documents. Argument propagation was conducted after each merge and murder’s argument ”mother of 12” in doc1 is combined with the murder event in doc2 after iteration 1. Then in iteration 2, more merges were made by recognizing additional coreferent event mentions including event mentions in one document (e.g., murdering and killed in doc2) and event mentions across the two documents (e.g., shooting in doc1 and shootout in doc2). Next, two additional merges were made by leveraging second-order inter-dependencies. Specifically, both the event mentions released in doc1 and presented in doc2 are in the same dependency relation (“nmod”)\nwith a mention of the trial event cluster, therefore, a new merge was made between clusters containing the two mentions. Following this, the event mentions court hearing in doc1 and trial in doc2 were identified to have multiple coreferent events in their sentential contexts, therefore, the clusters containing these two event mentions were merged as well."
    }, {
      "heading" : "4 Detailed System Description:",
      "text" : "Exploiting Interdependencies between Events"
    }, {
      "heading" : "4.1 Document Clustering",
      "text" : "Our approach starts with a pre-processing step that clusters input documents (D) into a set of document clusters (C). This is meant to reduce search space and mitigate errors (Lee et al., 2012). In our experiments, we used the Affinity Propagation algorithm (Buitinck et al., 2013) on tf − idf vectors, where terms are only proper nouns and verbs (excludes reporting and auxiliary verbs) in the document. While it is interesting to understand the influences of wrong document clusters to event coreference, this algorithm yielded perfect document clusters on the benchmark ECB+\ndataset (Cybulska and Vossen, 2014b,a). This is consistent with the prior study (Lee et al., 2012) on the related ECB dataset (Bejan and Harabagiu, 2010) 1, which shows that document clustering in the ECB dataset is trivial.\nAlgorithm 1: input: set of Documents D\nWithin-Document Classifier: ΘWD Cross-Document Classifier: ΘCD\n// clusters of event mentions\n1 EM = {} // clusters of Documents 2 C = ClusterDocument(D) 3 for each document cluster c in C do 4 EM′ = {Singleton Clusters}\n// Iterative WD and CD Merging\n6 while iterate do 7 iterate = False 8 for each two clusters E1, E2 ∈ EM′ s.t. ∃e1 ∈ E1, e2 ∈ E2, (e1, e2) ∈ a Doc, and score(ΘWD, e1, e2) > 0.60 do 9 Merge(E1, E2, EM′) 10 iterate = True 11 if not iterate break 12 iterate = False 13 for each two clusters E1, E2 ∈ EM′ s.t. ∃e1 ∈ E1, e2 ∈ E2, (e1, e2) /∈ a Doc, and score(ΘCD, e1, e2) > 0.90 do 14 Merge(E1, E2, EM′) 15 iterate = True\n// Exploiting Second-Order Inter-\ndependencies Across Event Chains 16 while ∃ two clusters E1, E2 ∈ EM′ s.t. GovernorModifierRelated(E1, E2,ΘCD) do 17 EM′ = Merge(E1, E2, EM′) 18 while ∃ two clusters E1, E2 ∈ EM′ s.t. ContextSimilarity(E1, E2,ΘCD) do 19 EM′ = Merge(E1, E2, EM′) 20 EM = EM + EM′ 21 output: EM"
    }, {
      "heading" : "4.2 Iterative WD and CD Merging",
      "text" : "We iteratively conduct WD merges and CD merges until no more merge can be done. We train pairwise classifiers for identifying event clusters to merge. Specifically for WD merges as indicated in lines 8-10 in Algorithm 1, we iteratively go through pairs of clusters that contain a pair\n1The ECB+ dataset is an extended version of the ECB dataset. Both datasets have documents for the same 43 topics.\nof within-document event mentions, one mention from each cluster. If the similarity score between the two event mentions is above a tuned threshold of 0.6 2, we merge the two clusters. Similarly, for CD merges described in lines 13-15 of Algorithm 1, we iteratively go through pairs of clusters that contain a pair of cross-document event mentions and merge the two clusters if the similarity score between the two event mentions is above another tuned threshold of 0.9 3. Following each cluster pair merge, arguments are combined for the two merged clusters."
    }, {
      "heading" : "4.3 Merging by Exploiting Second-Order Inter-dependencies Across Event Chains",
      "text" : "Intuitively, two event mentions that share events in their contexts are likely to be coreferent. Similarly, if their context events are coreferent, the two events are likely to be coreferent as well.\nFirst, if two event mentions are in the same dependency relation with two other event mentions that are known coreferent, then the first two event mentions are likely to describe the same real world event as well. In steps 16-17 of Algorithm 1, we perform event cluster merges by collecting evidence pertaining to dependency relations. The subroutine GovernorModifierRelated (E1, E2,ΘCD) checks whether two event mentions e1 and e2, from clusters E1 and E2 respectively, have a related event e3 from another cluster E3, such that E3 /∈ {E1, E2} and pairs (e1, e3), (e2, e3) are linked with the same dependency relation. Note that observing shared event mentions in the contexts will increase the likelihood that the two event mentions are coreferent, but we can not sufficiently infer the coreference relation yet, we still need to look at features describing the event mentions. Therefore, if the condition was satisfied, the subroutine eventually makes merges based on the CD confidence score assigned to the event pair (e1, e2) but using a lower threshold of 0.8.\nIn addition, seeing coreferent event mentions in the sentential contexts of two events will increase the likelihood that the two events are coreferent as well. Then as shown in steps 18-19, we fur-\n2all tunings are performed on Validation dataset (topics 23-25)\n3Note that these high threshold for WD- and CD- classifiers are meant to retain high precision and avoid error propagation in subsequent stages. Output from each classifier is a number bounded in [0,1].\nther use context events co-occurring in the same sentence as another parameter to perform additional clustering. Subroutine ContextSimilarity (E1, E2,ΘCD) generates a context vector (CV) for each event cluster and check whether cosine similarity between context vectors of two clusters E1 and E2 (cos( ~CV1, ~CV2)) is above 0.7. Specifically, we define context clusters for an event mention as the different event clusters that have event mentions co-occurring in the same sentence. Then the context vector of an event cluster has an entry for each of its context clusters, with the value to be the number of sentences where event mentions from the two clusters co-occur. This subroutine also makes merges based on the CD confidence score using the same lower threshold of 0.8."
    }, {
      "heading" : "5 Distinguishing WD and CD Merging",
      "text" : "We implement two distinct pairwise classifiers to effectively utilize the distributional variations in WD and CD clusters. The first classifier (WD) is used for calculating a similarity between two event mentions within a document and recognizing coreferent event mention pairs. The second classifier (CD) is used for calculating a similarity between two event mentions across two documents and then identifying coreferent event mention pairs across documents. Both classifiers were implemented as neural nets (Chollet, 2015). The architectures of the two classifiers are shown in Figure 2.\nWD Classifier: the neural network based WD classifier essentially inherits the features that have\nbeen shown effective in previous event coreference studies (Ahn, 2006; Chen et al., 2009), including both features for event words and features for their arguments. Specifically, the classifier includes a common neural layer shared by two event mentions to embed event lemma and partsof-speech features. Then the classifier calculates cosine similarity and euclidean distance between two event embeddings, one per event mention. In addition, the classifier includes a neural layer component to embed event arguments that are overlapped between the two event mentions. Its output layer takes the calculated cosine similarity and euclidean distance between event mention embeddings as well as the embedding of the overlapped event arguments as input, and output a confidence score to indicate the similarity of the two event mentions.\nCD Classifier: the CD classifier mimics the WD classifier except that the CD classifier contains an additional LSTM layer (Hochreiter and Schmidhuber, 1997) to embed context words. The LSTM layer is shared by both event mentions in order to calculate context word embeddings for both event mentions. Specifically, three words to each side of an event word together with the event word itself are used to calculate the context embedding for each event mention. The classifier then calculates cosine similarity and euclidean distance between two context embeddings as well. The output neural net layer will take two sets of cosine similarity and euclidean distance scores that have been calculated w.r.t. context embed-\ndings and event word embeddings, as well as the embedding of the overlapped event arguments as input, and further calculate a confidence score indicating the similarity of two event mentions across documents."
    }, {
      "heading" : "5.1 Characteristics of WD and CD Event Linking",
      "text" : "In order to further understand characteristics of within- and cross-document event linking, we trained two classifiers having the same CD classifier architecture (Figure 2(b)) but with different sets of event pairs, within-document or crossdocument event pairs, then analyzed the impacts of features on each type of event linking by comparing the neural net learned weights for each feature. Table 1 shows the comparisons of feature weights.\nWe can see that within-document event linking mainly relies on the euclidean distance and cosine similarity scores calculated using event word features, with a reasonable amount of weight assigned to overlapped arguments’ embedding as well. However, only very small weights were assigned to the similarity and distance scores calculated using context embeddings. In contrast, in the classifier trained with cross-doc coreferent event mention pairs, the highest weight was assigned to the cosine similarity score calculated using context embeddings of two event mentions. Additionally, both the cosine similarity score calculated using event word embeddings and the overlapped argument features were assigned high weights as well. The comparisons clearly demonstrate the significantly different nature of WD and CD event coreference."
    }, {
      "heading" : "5.2 Neural Net Classifiers and Training",
      "text" : "In both WD and CD classifiers, we use neural network layer with 60 neurons for embedding event word features and another layer with 1 neuron\nfor embedding argument features. Additionally, in CD classifier, we use an LSTM layer with 30 neurons to embed context features. Dropout of 0.25 was applied to both the event word neural net layer and the context layer. We used sigmoid activation function for the dense layers and tanh activation for the LSTM layer. We used 300- dimensional word embeddings and one hot 374 dimensional pos tag embeddings in all our experiments. Therefore, input to word embedding layer is a 337-dimensional vector and to LSTM layer is 300*7 dimensional vectors.\nWe train both classifiers using the ECB+ corpus (Cybulska and Vossen, 2014b,a). We train the WD classifier using all pairs of WD event mentions that are in an annotated event chain as positive instances and using all pairs of WD event mentions that are not in an annotated event chain as negative instances. However, there are significantly more CD coreferent event mention pairs annotated in the ECB+ corpus, therefore, we randomly sampled 70% of all the CD coreferent event mention pairs as positive instances and randomly sampled from non-coreferent CD event mention pairs as negative instances. Specifically, number of negative instances are kept 5 times of positive instances.\nNote that the pairwise classifiers will be used throughout the iterative merging stage. However, after each merge, argument propagation is conducted to enrich features for each event mention in the merged cluster and the number of arguments of an event mention will grow after several merges. In order to account for the growing number of arguments in iterative merging, we augment arguments for each event mention in training instances with arguments derived from other event mentions in the same pair. The augmenting was performed randomly for only 50% of event mentions."
    }, {
      "heading" : "6 Evaluation",
      "text" : "We perform all the experiments on the ECB+ corpus (Cybulska and Vossen, 2014b,a), which is an extension to the earlier EventCorefBank (ECB) (Bejan and Harabagiu, 2010) dataset. We have adopted the settings used in Yang et al. (2015). We divide the dataset into training set (topics 1-20), validation set (topics 21-23) and test\n4Corresponding to the unique 36 POS tags based on the Stanford POS tagger (Toutanova et al., 2003) and an additional ’padding’.\nset (topics 24-43). Table 2 shows the distribution of the corpus.\nWe used event mentions identified by CRF based event extractor used in Yang et al. (2015) and extracted event arguments by applying state-of-the-art semantic role labeling system (SwiRL (Surdeanu et al., 2007)). In addition, we used the Stanford parser (Chen and Manning, 2014) for generating dependency relations, partsof-speech tags and lemmas. We use pre-trained Glove vectors (Pennington et al., 2014)5 for word representation and one-hot vectors for parts-ofspeech tags.\nWe evaluate our model using four commonly adopted event coreference evaluation metrics, namely, MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005) and CoNLL F1 (Pradhan et al., 2014). We used the publicly available official implementation of revised coreference scorer (v8.01).6"
    }, {
      "heading" : "6.1 Baseline Systems",
      "text" : "We compare our iterative event coreference resolution model with five baseline systems.\nLEMMA: The Lemma match baseline links event mentions within- or cross- documents which have the same lemmatized head word. It is often considered a strong baseline for this task.\nHDDCRP (Yang et al., 2015): The second baseline is the supervised Hierarchical Distance Dependent Bayesian Model, the most recent event coreference system evaluated on the same ECB+ dataset. This model uses distances between event mentions, generated using a feature-rich learnable distance function, as Bayesian priors for single pass non-parametric clustering.\nHDP-LEX7: A reimplementation of the unsupervised hierarchical bayesian model by Bejan\n5 Trained on 840 billion tokens of Common Crawl data, http://nlp.stanford.edu/projects/glove/\n6 https://github.com/conll/reference-coreference-scorers 7 The results were taken from the paper Yang et al. (2015).\nand Harabagiu (2010, 2014). Agglomerative 7: A Reimplementation of twostep agglomerative clustering model, WD clustering followed by CD clustering (Chen et al., 2009).\nWe have trained our systems using the same ECB+ dataset and the same set of event mentions as these prior systems."
    }, {
      "heading" : "6.2 Our Systems",
      "text" : "We evaluate several variation systems of our proposed model.\nCommon Classifier (WD or CD): the system implementing only the first stage of iterative WD & CD merging. In addition, the same neural net classifier with the architecture as shown in Figure 2(a) (the WD classifier) or in Figure 2(b) (the CD classifier) was applied for both WD and CD merging. The neural net classifiers were trained using all coreferent event mention pairs including both within-document and cross-document ones.\nWD and CD Classifiers: distinguishes WD from CD merges by using two distinct classifiers (Figure 2(a), 2(b)) in the first stage of the algorithm.\n+ 2nd Order Relations: after iterative WD and CD merges within each individual chain as suggested by pairwise classifiers (the first stage), further merges (the second stage) were conducted leveraging second order event inter-dependencies across event chains."
    }, {
      "heading" : "6.3 Results",
      "text" : "Table 3 shows the comparison results for both within-document and cross-document event coreference resolution. In the first stage of iterative merging, using two distinct WD and CD classifiers for corresponding WD and CD merges yields clear improvements for both WD and CD event coreference resolution tasks, compared with using one common classifier for both types of merges. In addition, the second stage of iterative merging further improves both WD and CD event coreference resolution performance stably by leveraging second order event inter-dependencies. The improvements are consistent when measured using various coreference resolution evaluation metrics.\nOur full model achieved more than 8% of improvements when compared with the lemma matching baseline, using the CoNLL F1-score for both WD and CD coreference resolution tasks. Furthermore, it outperforms state-of-the-art HDDCRP model for both WD and CD event coreference resolution by 2.1% and 4.9% respectively."
    }, {
      "heading" : "7 Discussion and Analysis",
      "text" : "Stage I: The first stage of our algorithm, iterative WD and CD merging, went for three iterations (See Table 4). Our analysis of merges in each iteration shows that most of the merges in the initial iteration are between event mentions with the same lemma or shared arguments. In the second and third iterations, more merges were between event mentions with synonymous lemmas or shared arguments that have been accumulated in previous iterations. Example merges between synonymous event mentions include (nominate, nominations), (die, death), (murder, killing), (hit, strike), (attack, bomb) etc.\nStage II: It is even more intriguing to discuss the clusters that were merged in stage 2 of merging, that leverages second order event interdependencies across event chains. We found that al-\nmost all of the 81 merges happening in the second stage are between event mentions that are quite dissimilar including (take over, replace), (unveil, announce), (win, victory, comeback), (downtime, problem, outage), (cut, damage), (spark, trigger) etc. Most interestingly, two event pairs which are antonymous to each other, (win, beat) and (defeat, victory), were also correctly merged.\nErrors: while our iterative algorithm has gradually resolved coreference relations between event mentions that are synonyms or distant by surface forms, many coreference links were overlooked and many unrelated events were wrongly predicted as coreferential. We analyzed our system’s final predictions in order to identify the most common sources of errors.\nMissed Coreference Links: We found that many event mentions have few or no argument in their local context, and our event coreference resolution system often failed to link these event mentions with their coreferential mentions. For instance, in the following event mention pairs that were overlooked by the system, (operations, raids), (operations, sweep), (suicide, hang), (prosecution, jail), and (participating, role), one or both event mentions do not have an argument in their local context. This is mainly because the base WD and CD classifiers heavily rely on features extracted from the local context of two event men-\ntions, including event words and event arguments, in resolving the coreference relation. For these event mentions having few arguments identified, the iterative algorithm may get stuck from the beginning.\nWhile it is a grand challenge to further resolve coreferential relations between event mentions that do not have sufficient local features, these missed coreference links easily break a long and influential event chain into several sub-chains, which makes event coreference resolution results less useful for many potential applications, such as text summarization.\nWrongly Predicted Coreference Links: The majority of this type of errors are between noncoreferent event mentions that have the same lemma. This is especially common among reporting event mentions and light verb mentions. For instance, we found that 24 non-coreferent event clusters corresponding to reporting events, e.g., said, told and reported, and 13 non-coreferent clusters corresponding to light verbs, e.g., take, give and get, were incorrectly merged by the system."
    }, {
      "heading" : "8 Conclusions and Future Work",
      "text" : "We presented a novel approach for event coreference resolution that extensively exploits event inter-dependencies between event mentions in the same chain and event mentions across chains. The approach iteratively conducts WD and CD merges followed by further merges leveraging second order event inter-dependencies across chains. We further distinguish WD and CD merges using two distinct classifiers that capture differences of within- and cross-document event clusters in feature distributions. Our system was shown effective in both WD and CD event coreference and has outperformed the previous best event coreference system in both tasks.\nNote that our approach is flexible to incorporate different strategies for conducting WD and CD merges. In the future, we plan to continue to investigate the distinct characteristics of WD and CD coreferent event mentions in order to further improve event coreference performance. Especially, we are interested in including additional discourse-level features for improving WD coreference merge performance, such as, features indicating the distance between two event mentions in a document."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We want to thank our anonymous reviewers for providing insightful review comments."
    } ],
    "references" : [ {
      "title" : "The stages of event extraction",
      "author" : [ "David Ahn." ],
      "venue" : "Proceedings of the Workshop on Annotating and Reasoning about Time and Events, pages 1–8. Association for Computational Linguistics.",
      "citeRegEx" : "Ahn.,? 2006",
      "shortCiteRegEx" : "Ahn.",
      "year" : 2006
    }, {
      "title" : "Topic detection and tracking pilot study final report",
      "author" : [ "James Allan", "Jaime G Carbonell", "George Doddington", "Jonathan Yamron", "Yiming Yang" ],
      "venue" : null,
      "citeRegEx" : "Allan et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Allan et al\\.",
      "year" : 1998
    }, {
      "title" : "Algorithms for scoring coreference chains",
      "author" : [ "Amit Bagga", "Breck Baldwin." ],
      "venue" : "The first international conference on language resources and evaluation workshop on linguistics coreference, volume 1, pages 563–566.",
      "citeRegEx" : "Bagga and Baldwin.,? 1998",
      "shortCiteRegEx" : "Bagga and Baldwin.",
      "year" : 1998
    }, {
      "title" : "Unsupervised event coreference resolution with rich linguistic features",
      "author" : [ "Cosmin Adrian Bejan", "Sanda Harabagiu." ],
      "venue" : "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1412–1422. Association for Com-",
      "citeRegEx" : "Bejan and Harabagiu.,? 2010",
      "shortCiteRegEx" : "Bejan and Harabagiu.",
      "year" : 2010
    }, {
      "title" : "Unsupervised event coreference resolution",
      "author" : [ "Cosmin Adrian Bejan", "Sanda Harabagiu." ],
      "venue" : "Computational Linguistics, 40(2):311–347.",
      "citeRegEx" : "Bejan and Harabagiu.,? 2014",
      "shortCiteRegEx" : "Bejan and Harabagiu.",
      "year" : 2014
    }, {
      "title" : "A fast and accurate dependency parser using neural networks",
      "author" : [ "Danqi Chen", "Christopher D Manning." ],
      "venue" : "EMNLP, pages 740–750.",
      "citeRegEx" : "Chen and Manning.,? 2014",
      "shortCiteRegEx" : "Chen and Manning.",
      "year" : 2014
    }, {
      "title" : "Graph-based event coreference resolution",
      "author" : [ "Zheng Chen", "Heng Ji." ],
      "venue" : "Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing, pages 54–57. Association for Computational Linguistics.",
      "citeRegEx" : "Chen and Ji.,? 2009",
      "shortCiteRegEx" : "Chen and Ji.",
      "year" : 2009
    }, {
      "title" : "A pairwise event coreference model, feature impact and evaluation for event coreference resolution",
      "author" : [ "Zheng Chen", "Heng Ji", "Robert Haralick." ],
      "venue" : "Proceedings of the workshop on events in emerging text types, pages 17–22. Association for Computa-",
      "citeRegEx" : "Chen et al\\.,? 2009",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2009
    }, {
      "title" : "Keras",
      "author" : [ "François Chollet." ],
      "venue" : "https://github. com/fchollet/keras.",
      "citeRegEx" : "Chollet.,? 2015",
      "shortCiteRegEx" : "Chollet.",
      "year" : 2015
    }, {
      "title" : "Entitycentric coreference resolution with model stacking",
      "author" : [ "Kevin Clark", "Christopher D Manning" ],
      "venue" : null,
      "citeRegEx" : "Clark and Manning.,? \\Q2015\\E",
      "shortCiteRegEx" : "Clark and Manning.",
      "year" : 2015
    }, {
      "title" : "Improving coreference resolution by learning entitylevel distributed representations",
      "author" : [ "Kevin Clark", "Christopher D Manning" ],
      "venue" : null,
      "citeRegEx" : "Clark and Manning.,? \\Q2016\\E",
      "shortCiteRegEx" : "Clark and Manning.",
      "year" : 2016
    }, {
      "title" : "Guidelines for ecb+ annotation of events and their coreference",
      "author" : [ "Agata Cybulska", "Piek Vossen." ],
      "venue" : "Technical report, Technical report, Technical Report NWR-2014-1, VU University Amsterdam.",
      "citeRegEx" : "Cybulska and Vossen.,? 2014a",
      "shortCiteRegEx" : "Cybulska and Vossen.",
      "year" : 2014
    }, {
      "title" : "Using a sledgehammer to crack a nut? lexical diversity and event coreference resolution",
      "author" : [ "Agata Cybulska", "Piek Vossen." ],
      "venue" : "LREC, pages 4545– 4552.",
      "citeRegEx" : "Cybulska and Vossen.,? 2014b",
      "shortCiteRegEx" : "Cybulska and Vossen.",
      "year" : 2014
    }, {
      "title" : "Translating granularity of event slots into features for event coreference resolution",
      "author" : [ "Agata Cybulska", "Piek Vossen." ],
      "venue" : "Proceedings of the 3rd Workshop on EVENTS at the NAACL-HLT, pages 1–",
      "citeRegEx" : "Cybulska and Vossen.,? 2015a",
      "shortCiteRegEx" : "Cybulska and Vossen.",
      "year" : 2015
    }, {
      "title" : "bag of events approach to event coreference resolution",
      "author" : [ "Agata Cybulska", "Piek Vossen." ],
      "venue" : "supervised classification of event templates. IJCLA, page 11.",
      "citeRegEx" : "Cybulska and Vossen.,? 2015b",
      "shortCiteRegEx" : "Cybulska and Vossen.",
      "year" : 2015
    }, {
      "title" : "Sub-event based multi-document summarization",
      "author" : [ "Naomi Daniel", "Dragomir Radev", "Timothy Allison." ],
      "venue" : "Proceedings of the HLT-NAACL 03 on Text summarization workshop-Volume 5, pages 9–16. Association for Computational Linguistics.",
      "citeRegEx" : "Daniel et al\\.,? 2003",
      "shortCiteRegEx" : "Daniel et al\\.",
      "year" : 2003
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation, 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Event coreference for information extraction",
      "author" : [ "Kevin Humphreys", "Robert Gaizauskas", "Saliha Azzam." ],
      "venue" : "Proceedings of a Workshop on Operational Factors in Practical, Robust Anaphora Resolution for Unrestricted Texts, pages 75–81. Associa-",
      "citeRegEx" : "Humphreys et al\\.,? 1997",
      "shortCiteRegEx" : "Humphreys et al\\.",
      "year" : 1997
    }, {
      "title" : "Joint entity and event coreference resolution across documents",
      "author" : [ "Heeyoung Lee", "Marta Recasens", "Angel Chang", "Mihai Surdeanu", "Dan Jurafsky." ],
      "venue" : "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and",
      "citeRegEx" : "Lee et al\\.,? 2012",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2012
    }, {
      "title" : "Supervised withindocument event coreference using information propagation",
      "author" : [ "Zhengzhong Liu", "Jun Araki", "Eduard H Hovy", "Teruko Mitamura." ],
      "venue" : "LREC, pages 4539–4544.",
      "citeRegEx" : "Liu et al\\.,? 2014",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2014
    }, {
      "title" : "Joint inference for event coreference resolution",
      "author" : [ "Jing Lu", "Deepak Venugopal", "Vibhav Gogate", "Vincent Ng" ],
      "venue" : null,
      "citeRegEx" : "Lu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2016
    }, {
      "title" : "On coreference resolution performance metrics",
      "author" : [ "Xiaoqiang Luo." ],
      "venue" : "Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 25–32. Association for Computational Linguistics.",
      "citeRegEx" : "Luo.,? 2005",
      "shortCiteRegEx" : "Luo.",
      "year" : 2005
    }, {
      "title" : "Cross-document coreference resolution: A key technology for learning by reading",
      "author" : [ "James Mayfield", "David Alexander", "Bonnie J Dorr", "Jason Eisner", "Tamer Elsayed", "Tim Finin", "Clayton Fink", "Marjorie Freedman", "Nikesh Garera", "Paul McNamee" ],
      "venue" : null,
      "citeRegEx" : "Mayfield et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Mayfield et al\\.",
      "year" : 2009
    }, {
      "title" : "Question answering based on semantic structures",
      "author" : [ "Srini Narayanan", "Sanda Harabagiu." ],
      "venue" : "Proceedings of the 20th international conference on Computational Linguistics, page 693. Association for Computational Linguistics.",
      "citeRegEx" : "Narayanan and Harabagiu.,? 2004",
      "shortCiteRegEx" : "Narayanan and Harabagiu.",
      "year" : 2004
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D Manning." ],
      "venue" : "EMNLP, volume 14, pages 1532– 1543.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Scoring coreference partitions of predicted mentions: A reference implementation",
      "author" : [ "Sameer Pradhan", "Xiaoqiang Luo", "Marta Recasens", "Eduard Hovy", "Vincent Ng", "Michael Strube." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association",
      "citeRegEx" : "Pradhan et al\\.,? 2014",
      "shortCiteRegEx" : "Pradhan et al\\.",
      "year" : 2014
    }, {
      "title" : "Bi-directional joint inference for entity resolution and segmentation using imperatively-defined factor graphs",
      "author" : [ "Sameer Singh", "Karl Schultz", "Andrew McCallum." ],
      "venue" : "Machine Learning and Knowledge Discovery in Databases, pages 414–429.",
      "citeRegEx" : "Singh et al\\.,? 2009",
      "shortCiteRegEx" : "Singh et al\\.",
      "year" : 2009
    }, {
      "title" : "Combination strategies for semantic role labeling",
      "author" : [ "Mihai Surdeanu", "Lluı́s Màrquez", "Xavier Carreras", "Pere R Comas" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Surdeanu et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Surdeanu et al\\.",
      "year" : 2007
    }, {
      "title" : "Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network",
      "author" : [ "K. Toutanova", "D. Klein", "C. Manning", "Y. Singer." ],
      "venue" : "Proceedings of HLT-NAACL 2003.",
      "citeRegEx" : "Toutanova et al\\.,? 2003",
      "shortCiteRegEx" : "Toutanova et al\\.",
      "year" : 2003
    }, {
      "title" : "A modeltheoretic coreference scoring scheme",
      "author" : [ "Marc Vilain", "John Burger", "John Aberdeen", "Dennis Connolly", "Lynette Hirschman." ],
      "venue" : "Proceedings of the 6th conference on Message understanding, pages 45–52. Association for Computational",
      "citeRegEx" : "Vilain et al\\.,? 1995",
      "shortCiteRegEx" : "Vilain et al\\.",
      "year" : 1995
    }, {
      "title" : "Learning global features for coreference resolution",
      "author" : [ "Sam Wiseman", "Alexander M Rush", "Stuart M Shieber." ],
      "venue" : "Proceedings of NAACL-HLT, pages 994–1004.",
      "citeRegEx" : "Wiseman et al\\.,? 2016",
      "shortCiteRegEx" : "Wiseman et al\\.",
      "year" : 2016
    }, {
      "title" : "A hierarchical distance-dependent bayesian model for event coreference resolution",
      "author" : [ "Bishan Yang", "Claire Cardie", "Peter Frazier." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 3:517– 528.",
      "citeRegEx" : "Yang et al\\.,? 2015",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2015
    }, {
      "title" : "Cross-document event coreference resolution based on cross-media features",
      "author" : [ "Tongtao Zhang", "Hongzhi Li", "Heng Ji", "ShihFu Chang." ],
      "venue" : "EMNLP, pages 201–206.",
      "citeRegEx" : "Zhang et al\\.,? 2015",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 18,
      "context" : "One common practice (Lee et al., 2012) to approach CD coreference",
      "startOffset" : 20,
      "endOffset" : 38
    }, {
      "referenceID" : 31,
      "context" : "within a document and then group WD clusters across documents (Yang et al., 2015).",
      "startOffset" : 62,
      "endOffset" : 81
    }, {
      "referenceID" : 3,
      "context" : ", participants and spatio-temporal information of an event (Bejan and Harabagiu, 2010)) partially or entirely omitted, or appear in distinct contexts compared to their antecedent event mentions, partially to avoid repetitions.",
      "startOffset" : 59,
      "endOffset" : 86
    }, {
      "referenceID" : 0,
      "context" : "Works specific to WD event coreference includes pairwise classifiers (Ahn, 2006; Chen et al., 2009) graph based clustering method (Chen and Ji, 2009), information propagation (Liu et al.",
      "startOffset" : 69,
      "endOffset" : 99
    }, {
      "referenceID" : 7,
      "context" : "Works specific to WD event coreference includes pairwise classifiers (Ahn, 2006; Chen et al., 2009) graph based clustering method (Chen and Ji, 2009), information propagation (Liu et al.",
      "startOffset" : 69,
      "endOffset" : 99
    }, {
      "referenceID" : 6,
      "context" : ", 2009) graph based clustering method (Chen and Ji, 2009), information propagation (Liu et al.",
      "startOffset" : 38,
      "endOffset" : 57
    }, {
      "referenceID" : 19,
      "context" : ", 2009) graph based clustering method (Chen and Ji, 2009), information propagation (Liu et al., 2014), and markov logic networks Lu et al.",
      "startOffset" : 83,
      "endOffset" : 101
    }, {
      "referenceID" : 0,
      "context" : "Works specific to WD event coreference includes pairwise classifiers (Ahn, 2006; Chen et al., 2009) graph based clustering method (Chen and Ji, 2009), information propagation (Liu et al., 2014), and markov logic networks Lu et al. (2016). As to only CD event coreference, Cybulska and Vossen (2015a) created pairwise classifiers using features indicating granularities of event slots and in another work",
      "startOffset" : 70,
      "endOffset" : 238
    }, {
      "referenceID" : 0,
      "context" : "Works specific to WD event coreference includes pairwise classifiers (Ahn, 2006; Chen et al., 2009) graph based clustering method (Chen and Ji, 2009), information propagation (Liu et al., 2014), and markov logic networks Lu et al. (2016). As to only CD event coreference, Cybulska and Vossen (2015a) created pairwise classifiers using features indicating granularities of event slots and in another work",
      "startOffset" : 70,
      "endOffset" : 300
    }, {
      "referenceID" : 18,
      "context" : "However to simplify the problem, they (Lee et al., 2012; Bejan and Harabagiu, 2010, 2014) created a meta-document by concatenating topic-relevant documents and treated both as an identical task.",
      "startOffset" : 38,
      "endOffset" : 89
    }, {
      "referenceID" : 3,
      "context" : ", 2012; Bejan and Harabagiu, 2010, 2014) created a meta-document by concatenating topic-relevant documents and treated both as an identical task. Most recently, Yang et al. (2015) applied a two-level clustering model that first groups event mentions within a document and then groups WD clusters across documents in a joint inference process.",
      "startOffset" : 8,
      "endOffset" : 180
    }, {
      "referenceID" : 26,
      "context" : "to both entity coreference resolution (Singh et al., 2009; Clark and Manning, 2015, 2016; Wiseman et al., 2016) and prior event coreference resolution (Lee et al.",
      "startOffset" : 38,
      "endOffset" : 111
    }, {
      "referenceID" : 30,
      "context" : "to both entity coreference resolution (Singh et al., 2009; Clark and Manning, 2015, 2016; Wiseman et al., 2016) and prior event coreference resolution (Lee et al.",
      "startOffset" : 38,
      "endOffset" : 111
    }, {
      "referenceID" : 18,
      "context" : ", 2016) and prior event coreference resolution (Lee et al., 2012) works, which gradually build clusters and enable later merges to benefit from",
      "startOffset" : 47,
      "endOffset" : 65
    }, {
      "referenceID" : 18,
      "context" : "Especially, Lee et al. (2012) used an iterative model to jointly build entity and event clusters and showed the advantages of information flow between entity and event clusters through semantic role features.",
      "startOffset" : 12,
      "endOffset" : 30
    }, {
      "referenceID" : 18,
      "context" : "This is meant to reduce search space and mitigate errors (Lee et al., 2012).",
      "startOffset" : 57,
      "endOffset" : 75
    }, {
      "referenceID" : 18,
      "context" : "consistent with the prior study (Lee et al., 2012) on the related ECB dataset (Bejan and Harabagiu, 2010) 1, which shows that document clustering in the ECB dataset is trivial.",
      "startOffset" : 32,
      "endOffset" : 50
    }, {
      "referenceID" : 3,
      "context" : ", 2012) on the related ECB dataset (Bejan and Harabagiu, 2010) 1, which shows that document clustering in the ECB dataset is trivial.",
      "startOffset" : 35,
      "endOffset" : 62
    }, {
      "referenceID" : 8,
      "context" : "Both classifiers were implemented as neural nets (Chollet, 2015).",
      "startOffset" : 49,
      "endOffset" : 64
    }, {
      "referenceID" : 0,
      "context" : "classifier essentially inherits the features that have been shown effective in previous event coreference studies (Ahn, 2006; Chen et al., 2009), in-",
      "startOffset" : 114,
      "endOffset" : 144
    }, {
      "referenceID" : 7,
      "context" : "classifier essentially inherits the features that have been shown effective in previous event coreference studies (Ahn, 2006; Chen et al., 2009), in-",
      "startOffset" : 114,
      "endOffset" : 144
    }, {
      "referenceID" : 16,
      "context" : "CD Classifier: the CD classifier mimics the WD classifier except that the CD classifier contains an additional LSTM layer (Hochreiter and Schmidhuber, 1997) to embed context words.",
      "startOffset" : 122,
      "endOffset" : 156
    }, {
      "referenceID" : 3,
      "context" : "We perform all the experiments on the ECB+ corpus (Cybulska and Vossen, 2014b,a), which is an extension to the earlier EventCorefBank (ECB) (Bejan and Harabagiu, 2010) dataset.",
      "startOffset" : 140,
      "endOffset" : 167
    }, {
      "referenceID" : 3,
      "context" : "We perform all the experiments on the ECB+ corpus (Cybulska and Vossen, 2014b,a), which is an extension to the earlier EventCorefBank (ECB) (Bejan and Harabagiu, 2010) dataset. We have adopted the settings used in Yang et al. (2015). We divide the dataset into training set (topics 1-20), validation set (topics 21-23) and test",
      "startOffset" : 141,
      "endOffset" : 233
    }, {
      "referenceID" : 28,
      "context" : "Corresponding to the unique 36 POS tags based on the Stanford POS tagger (Toutanova et al., 2003) and an additional ’padding’.",
      "startOffset" : 73,
      "endOffset" : 97
    }, {
      "referenceID" : 27,
      "context" : "(2015) and extracted event arguments by applying state-of-the-art semantic role labeling system (SwiRL (Surdeanu et al., 2007)).",
      "startOffset" : 103,
      "endOffset" : 126
    }, {
      "referenceID" : 30,
      "context" : "We used event mentions identified by CRF based event extractor used in Yang et al. (2015) and extracted event arguments by applying state-of-the-art semantic role labeling system (SwiRL (Surdeanu et al.",
      "startOffset" : 71,
      "endOffset" : 90
    }, {
      "referenceID" : 5,
      "context" : "we used the Stanford parser (Chen and Manning, 2014) for generating dependency relations, partsof-speech tags and lemmas.",
      "startOffset" : 28,
      "endOffset" : 52
    }, {
      "referenceID" : 24,
      "context" : "We use pre-trained Glove vectors (Pennington et al., 2014)5 for word representation and one-hot vectors for parts-of-",
      "startOffset" : 33,
      "endOffset" : 58
    }, {
      "referenceID" : 29,
      "context" : "We evaluate our model using four commonly adopted event coreference evaluation metrics, namely, MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005) and CoNLL F1 (Pradhan et al.",
      "startOffset" : 100,
      "endOffset" : 121
    }, {
      "referenceID" : 2,
      "context" : ", 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005) and CoNLL F1 (Pradhan et al.",
      "startOffset" : 12,
      "endOffset" : 37
    }, {
      "referenceID" : 21,
      "context" : ", 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005) and CoNLL F1 (Pradhan et al.",
      "startOffset" : 45,
      "endOffset" : 56
    }, {
      "referenceID" : 25,
      "context" : ", 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005) and CoNLL F1 (Pradhan et al., 2014).",
      "startOffset" : 70,
      "endOffset" : 92
    }, {
      "referenceID" : 31,
      "context" : "HDDCRP (Yang et al., 2015): The second baseline is the supervised Hierarchical Distance Dependent Bayesian Model, the most recent event coreference system evaluated on the same ECB+ dataset.",
      "startOffset" : 7,
      "endOffset" : 26
    }, {
      "referenceID" : 7,
      "context" : "Agglomerative 7: A Reimplementation of twostep agglomerative clustering model, WD clustering followed by CD clustering (Chen et al., 2009).",
      "startOffset" : 119,
      "endOffset" : 138
    }, {
      "referenceID" : 30,
      "context" : "com/conll/reference-coreference-scorers 7 The results were taken from the paper Yang et al. (2015). and Harabagiu (2010, 2014).",
      "startOffset" : 80,
      "endOffset" : 99
    }, {
      "referenceID" : 28,
      "context" : "6 HDDCRP Yang et al. (2015) 40.",
      "startOffset" : 9,
      "endOffset" : 28
    }, {
      "referenceID" : 3,
      "context" : "7 HDP-LEX Bejan and Harabagiu (2010) 43.",
      "startOffset" : 10,
      "endOffset" : 37
    }, {
      "referenceID" : 3,
      "context" : "7 HDP-LEX Bejan and Harabagiu (2010) 43.7 65.6 52.5 63.5 75.5 69.0 60.2 34.8 44.1 55.2 Agglomerative Chen et al. (2009) 40.",
      "startOffset" : 10,
      "endOffset" : 120
    }, {
      "referenceID" : 28,
      "context" : "9 HDDCRP Yang et al. (2015) 67.",
      "startOffset" : 9,
      "endOffset" : 28
    }, {
      "referenceID" : 3,
      "context" : "8 HDP-LEX Bejan and Harabagiu (2010) 67.",
      "startOffset" : 10,
      "endOffset" : 37
    }, {
      "referenceID" : 3,
      "context" : "8 HDP-LEX Bejan and Harabagiu (2010) 67.6 74.7 71.0 39.1 50.0 43.9 71.4 66.2 68.7 61.2 Agglomerative Chen et al. (2009) 67.",
      "startOffset" : 10,
      "endOffset" : 120
    } ],
    "year" : 2017,
    "abstractText" : "We introduce a novel iterative approach for event coreference resolution that gradually builds event clusters by exploiting inter-dependencies among event mentions within the same chain as well as across event chains. Among event mentions in the same chain, we distinguish withinand cross-document event coreference links by using two distinct pairwise classifiers, trained separately to capture differences in feature distributions of withinand crossdocument event clusters. Our event coreference approach alternates between WD and CD clustering and combines arguments from both event clusters after every merge, continuing till no more merge can be made. And then it performs further merging between event chains that are both closely related to a set of other chains of events. Experiments on the ECB+ corpus show that our model outperforms state-of-the-art methods in joint task of WD and CD event coreference resolution.",
    "creator" : "LaTeX with hyperref package"
  }
}