{
  "name" : "1303.5741.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Formal Model of Uncertainty for Possibilistic Rules",
    "authors" : [ "Arthur Ramer", "Leslie Lander" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Formal Model of Uncertainty for Possibilistic Rules\nArthur Ramer Leslie Lander University of Oklahoma, Norman, OK 79019 SUNY-Binghamton, Binghamton, NY 19902-6000\nOVERVIEW\nGiven a universe of discourse X -a domain of possible outcomes-an experiment may consist of selecting one of its elements, subject to the operation of chance, or of observing the elements, subject to imprecision.\nA priori uncertainty about the actual result of the ex periment may be quantified, representing either the likelihood of the choice of x E X or the degree to which any such x E X would be suitable as a description of the outcome. The former case corresponds to a prob ability distribution, while the latter gives a possibility assignment on X.\nThe study of such assignments and their properties falls within the purview of possibility theory [DP88, Y80, Z78]. It, like probability theory, assigns values between 0 and 1 to express likelihoods of outcomes. Here, however, the similarity ends. Possibility theory uses the maximum and minimum functions to com bine uncertainties, whereas probability theory uses the plus and times operations. This leads to very dissim ilar theories in terms of analy tical framework, even though they share several semantic concepts. One of the shared concepts consists of expressing quantita tively the uncertainty associated with a given distribu tion. In probability theory its value corresponds to the gain of information that would result from conduct ing an experiment and ascertaining an actual result. This gain of information can equally well be viewed as a decrease in uncertainty about the outcome of an experiment. In this case the standard measure of in formation, and thus uncertainty, is Shannon entropy [AD75, G77]. It enjoys several advantages-it is char acterized uniquely by a few, very natural properties, and it can be conveniently used in decision processes. This application is based on the principle of maximum entropy; it has become a popular method of relating decisions to uncertainty.\nThis paper demonstrates that an equally integrated theory can be built on the foundation of possibility theory. We first show how to define measures of in-\nformation and uncertainty for possibility assignments. Next we construct an information-based metric on the space of all possibility distributions defined on a given domain. It allows us to capture the notion of prox imity in information content among the distributions. Lastly, we show that all the above constructions can be carried out for 'continuous distributions'-possibility assignments on arbitrary measurable domains. We consider this step very significant-finite domains of discourse are but approximations of the real-life infi nite domains. If possibility theory is to represent real world situations, it must handle continuous distribu tions both directly and through finite approximations.\nIn the last section we discuss a principle of maximum uncertainty for possibility distributions. We show how such a principle could be formalized as an inference rule. We also suggest it could be derived as a conse quence of simple assumptions about combining infor mation.\nWe would like to mention that possibility assignments can be viewed as fuzzy sets and that every fuzzy set gives rise to an assignment of possibilities. This corre spondence has far reaching consequences in logic and in control theory. Our treatment here is independent of any special interpretation; in particular we speak of possibility distributions and possibility measures, defining them as measurable mappings into the inter val [0, 1]. Our presentation is intended as a self-contained, albeit terse summary. Topics discussed were selected with care, to demonstrate both the completeness and a cer tain elegance of the theory. Proofs are not included; we only offer illustrative examples.\n1 POSSIBILITY DISTRIBUTIONS\nAND MEASURES\n1.1 DISCRETE DOMAINS\nWe use the model of possibility theory introduced in [Z78]. The domain of discourse can be any finite or\n296 Ramer and Lander\nfinitely measurable set. Here we discuss a finite do main X and define a possibility distribution as a func tion 1r : X -+ [0, 1] such that maxxEX 1r(x) = 1. It expresses an assignment of possibility values 1r( x) to elementary events x E X. We extend it to arbitrary subsets Y C X putting 1r(Y) = maxxEY 1r(x). 1 Given two domains X and Y and two independent possibility assignments 1r 1 :X-+ [0, 1], 1r2 : Y-+ [0, 1] we define a joint distribution"
    }, {
      "heading" : "1r1 181 1r2 : (x, y) >-+ min(1r1(x), 1r2(y)).",
      "text" : "Given an arbitrary assignment 1r on a product space X X Y we define its marginal assignments 1r1 on X and 1r11 on Y as\n1r1(x) = max 1r(x, y), yEY 1r11(y) = max 1r(x, y). xEX It is also convenient to define an extension of 1r from its domain X to a larger set Y :J X. We put 1ry (y) = 1r(y), y E X and 1ry (y) = 0 otherwise. Lastly, given X= {x 1, ... , Xn} and a permutation s of {1 , ... , n}, we define a possibility assignment s ( 1r)\ns(1r)(x;) = 1r(x,(;))·\n1.2 CONTINUOUS DOMAINS\nThis structure generalizes to an arbitrary X endowed with a finite (Lebesgue) measure. The assignment be comes a measurable function f : X -+ [0, 1], defining the possibility of Y C X as supxEY f(x). By analogy with probability theory, we term such structures con tinuous possibility assignments or distributions. Joint, marginal and extended distributions are now defined using sup and inf instead of max and min. Lastly, we generalize permutations of X to measure-preserving transformations s, putting s(/)(x) = f(s(x)). A trans formation corresponding to sorting discrete values is of particular interest. For f defined on X we want f to be a descending equivalent of f, defined on a real interval of the same measure as X. For definite ness, we can make the origin the left end-point of the interval and have j decrease monotonically. Measure preserving implies that j 'stays' above any given value a, 0 :$ a :$ 1, over tlte same space as the orig inal function 2 and leads to a classical construction [HLP34].\nWe put P(y) = M{x: J(z) :;:: y}, where M is a stan dard measure on [0, 1] and define f(x) = p-1(x). As an illustration let us consider two examples.\nExample\nf(x) = { 2x, 2- 2:t, 0 :$ X:$ 0.5, otherwise. 'The fuzzy interpretation is obtained by treating the pair (X, 1r) as a fuzzy subset of X and {z : 1r(z) ;::: a} as its a-cuts.\n2 All a-cuts [DP88] of are of the same size (have the same measure) as a-cuts of j.\nHere P(y) = 1-y and i{x) = 1- x. It is immediate that f(x) 2: a over the set of the same measure as the set where f(x) 2: a.\nExample f(x) = 4(x- !)2 = 4x2-4x + 1. Now P(y) represents the combined length of the inter vals where f(x) is 2: y.\nSince {x: f(x) 2: y} = [0, l�.fo] U ]1+2.fo, 1], we have P(y) = 1-JY. Therefore y = (1-P(y))2 and i{x) = (1- x)2•"
    }, {
      "heading" : "2 INFORMATION FUNCTIONS IN POSSIBILITY THEORY",
      "text" : "2.1 UNCERTAINTY\nThe structure outlined above provides the possibilistic context for the quantification of the notions of uncer tainty and information. We view the mapping \" as assigning a degree of assurance or certainty that an element of X is the outcome of an experiment. That experiment would consist of selecting x E X as a rep resentative (perhaps unique) object of discourse. A priori we know only the distribution ?rj to determine x E X means to remove uncertainty about the result, thus entailing a gain of information. We would be particularly interested in quantifying that gain of in formation, averaged over the complete distribution 1r. That would also express the overall value of uncer tainty inherent in the complete distribution 1!\". Ac cordingly, we intend to define an information function I which assigns a nonnegative real value to an arbi trary distribution 1r. Following established principles of information theory, [AD75, G77] we stipulate that such an information function satisfies certain standard properties. Specifically, we require\nadditivity I(1r1 181 1r2) = subadditivity I( 1r) :$ symmetry I(s(1r)) = expansibility I( 1ry) = I(7r!) + I(7r2) I( 1r') + I( 1r\") I(7r) I(7r)\nIt turns out that these properties essentially character ize the admissible information functions [KM87,RL87]. Here we discuss the discrete case of X = {x1, . • • , xn}· Let Pl 2: P2 2: . . . 2: Pn be a descending sequence formed from the values 1r(xt), ... , 7r(xn)· Then, up to a multiplicative constant:\nTheorem All information functions on X are of the form\n= I:n,;;-1 1 (r(p;)- r(f5Hd) logi\n= I:7=2 r(p;)'V logi where r is a nondecreasing mapping of [0, 1] onto it self. I(1r) is continuous (as a functional on the space of distributions) iff r is a continuous deformation of [0, 1].\nThe formula can be derived from functional equations representing the properties of information. In partic ular, the presence of log i comes from additivity, while the differences r(p;) - r(Pi+l) reflect the use of the max and min operations.\nBy analogy with Shannon theory we may also impose a linear interpolation property on I(1r) [KM87]. We then obtain a particularly simple expression, named U-uncertainty [HK82]\nU(1r) = L(.P;- Pi+d logi = Liiivlogi. It follows by taking r to be the identity mapping and we shall continue to do that in the remainder of the paper; however, all the results can be extended to an arbitrary r.\nWe observe that the distribution which carries the highest uncertainty value consists of assigning possi bility 1 to all the events in X. It states that, a priori, every event is fully possible. This distribution, carry ing no prior information, can be considered the most uninformed one.\n2.2 INFORMATION DISTANCE\nU-uncertainty serves to define various information dis tances [HK83, R90] between two distributions 1r and p defined on the same domain X. If 1r(x) S: p(x), we put\ng(1r,p) = U(p)- U(1r). For the general case, given 1r and p, we first define their lattice meet and join\nWe then put\n1r 1\\ p: x >--+ min(1r(x) , p(x)), 1r V p: x >--+ max(1r(x), p(x)).\nG(?r,p) = g(1r, 1r v p) + g(p, 1r v p), H ( 11\", p) = g( 11\" 1\\ p, 11\") + g( 1r 1\\ p, P)'\nK(1r,p) = max(g(7r,7rV p),g(p,1rV p)). These functions have several attractive properties:\nTheorem Both G and K define metric distances on the space of all possibility distributions (on a given domain). H is additive in both arguments\nH(1r1 1811r2, P1 181 P2) = H(1r1, pi) + H(1r2, P2).\n3 DESIGN OF CONTINUOUS\nPOSSIBILITY INFOR MATION\nWe shall now extend the previous definitions to arbi trary measurable domains. To avoid technical com plications, we consider only the special, albeit typical case where X is the unit interval. Now a possibility distribution is a function f : [0, 1] -+ [0, 1] such that\nFormal Model of Uncertainty for Possibilistic Rules 297\nsupxE[O,lJ f(x) = 1. Although in a variety of practical situations it is sufficient to consider only continuous functions, we do not make that restriction.\nAs a first step the discrete formula U ( x) = I:; p;'<Jlog i suggests forming an expression like f01 i{x)dln x, where j is a suitable 'decreasing sorted' equivalent of /, while din x substitutes \\/log x. The latter quantity simply represents x-1dx, while for j we use a descend ing rearrangement of f. Using this definition we can consider J; i';l dx as a candidate expression for the value of information. Un fortunately, i{x) is equal to 1 at 0, and the integral above diverges. A solution can be found through a technique that has been used in probability theory [ G 77], which is to use the information distance be tween a given density and the uniform one. In possi bility theory we consider a constant function f (x) = 1 as representing a uniform distribution. It is also the most 'uninformed' one-its discrete form clearly at tains maximum U-uncertainty. Our final formula becomes\nI(!) = { 1 1- j{x) dx. lo x\nThis integral is well defined and avoids the annoying singularity at 0. We demonstrate its use on a class of polynomial functions.\nExample Let us consider possibility distributions represented by f(x) = x\", n = 0, 1, .. .. Writing J, = I(x\") and remembering that x\" = (1- x)\", let us first compute J,- Jn-1\n[ 1 �(1_-�(�1_-_x)�\")-�(1_-�(�1_-_x �)\"_- l� ) dx = lo x\nt (1-x)n-1 - (1-x)\" dx = t (1-x)\"-ldx = .!. Jo x Jo n As J0 = 0 we find that J, = 1 + ! + · · · + � = H,, the n1h harmonic number.\n4 PR OPERTIES OF\nCONTINUOUS INFORMATION\nMEA SURES\nWe summarize the properties of I(!) in the next two theorems; /, /1, . . . stand for continuous distributions, h 181 h for their min--product and f' and !\" for the projections off when it is defined on a product space. Theorem I(!) is\nadditive superadditive symmetric expansible I(h 181 h) = I(h) +I( h) I(!) S: I(!') +I(!\") I(s(f)) = I(!) I(fy) = I(!)\nSuperadditivity of I (replacing subadditivity of U) is due to the minus sign in the formula that defines it.\nUsing I (!) we can define continuous extensions of the information distances g, G, H and K. As in the dis crete case, G and K are metric distances, while H is additive in both arguments. We shall demonstrate additivity of I with an example.\nExample We use as an example f = g = za, a 2: 0. We put h(z, y) = min(za, ya) and find h(t) = (1- y't)a. Then I(h) = J; 1-(1�0)\" dt which, after the substitution u = Vt becomes {1 1- (1; u)a ·2udu = 2 t 1-(1- u)a du = 2I(f). lo u lo u\nTheorem I(!) can be approximated as a limit of U(pn), where the Pn are discrete distributions approx imating f. 3\nThis theorem confirms that we are justified using dis crete possibility distributions in uncertainty compu tations. Their information values approximate con sistently an idealized value of a putative continuous distribution. Already a non-trivial example is offered by a linear function.\nExample We select f(zJ = 1-z and approximate it using the values at �, ;; , . . . 1. The approximating distributions are 1r(n) = (n�1, n�2, • • • 1), thus\n�n-i+1 n-i . 1� . 1 .L) - -- ) lnt = - L.)nt =-Inn! n n n n\nFrom Stirling's formula U(1r(n)) � Inn - 1 and I(1r(n)) =Inn- U(1r(n)) = 1 which agrees with I (f).\n5 PRINCIPLE OF MAXIMUM\nUNCERTAINTY\nThe decision rule forming the principle of maximum uncertainty can be stated independently of any spe cific theory used to capture the notions of random ness, vagueness or imprecision [G77, SJ80, J82]. We only need to assume that such randomness, vagueness or imprecision is expressed in the form of a numerical information function. The rule can be enhanced if, in addition, an information distance function is available.\nThe principle offers a method of selecting a distribu tion subject to certain constraints, usually presented as systems of linear equations on the parameters of an unknown distribution. Such constraints define a set of\n3For sufficiently uniform approximations.\nadmissible distributions, and the choice from among those, the reasoning continues, should be made with out introducing extraneous information, or should be as 'uninformed' as possible. Thus we should select a distribution of the maximum uncertainty value.\nA variation of the rule occurs when we are given a 'prior' distribution and are required to replace it with a 'posterior' distribution, subject to admissibility cri teria. Now we select the distribution for which the distance from the current one ('prior') reaches a min imum. The earlier case can be viewed as selecting a distribution closest to a hypothetical 'least informed' distribution.\nIn possibility theory such a principle would state that, given a prior assignment of possibility values and cer tain constraints on the posterior assignemnt, we should select the latter as the closest admissible assignment. The proximity here is expressed through the possibilis tic information distance. If there is no known or as sumed prior assignemnt, we should consider the dis tance from the most 'uninformed' possibility distribu tion, which is given by assigning a constant value 1 to every element of the domain of discourse. It clearly has the highest value of U-uncertainty; it also agrees with the intuitive perception that, in the absence of constraints, every choice should be accorded maximum possibility.\nA similar method of determining distributions holds valid in probability theory [SJ80]. There it can be also shown that any reasonable selection based on maxi mization must be based on an information measure. Our current research aims to show that also in possi bility theory decisions based on information measures stand privileged.\nREFERENCES\nAD75 ACZEL,J . and DAROCZY,Z., 1975, On mea sures of information and their characterization, Academic Press, New York.\nDP88 DUBOIS, D. and PRADE,H., 1988, Possibility theory, Plenum Press, New York .\nDP87 DUBOIS,D. and PRADE,H., 1987, Properties of measures of information in evidence and possi bility theories, Fuzzy Sets Syst., 24(2).\nG11 GUIASU, S., 1977, Information Theory and Ap plications, McGraw Hill, New York .\nHK83 HIGASHI, M . and KLIR, G., 1983, On the no tion of distance representing information close ness, Int. J. Gen. Syst., 9(1).\nHK82 HIGASHI, M . and KLIR,G., 1982, Measures of uncertainty and information based on possibility distributions Int. J. Gen. Syst., 8(3).\nHLP34 HARDY,G., LITTLEWOOD,J. and POLYA,G., 1934, Inequalities,\nCambridge University Press, Cambridge .\nJ82 JAYNES, E., 1982, On the rationale of maximum entropy methods, Proc. IEEE, 70.\nKM87 KLIR,G. and MARIANO,M., 1987, On the uniqueness of possibilistic measure of uncertainty and information, Fuzzy Sets Syst., 24(2).\nR90 RAMER, A., 1990, Structure of possibilistic in formation metrics and distances, Int. J. Gen. Syst., 17(1), 18(1).\nR89 RAMER, A., 1989, Concepts of fuzzy informa tion measures on continuous domains, Int. J. Gen. Syst., 17(2-3).\nRL87 RAMER, A . and LANDER,L., 1987, Classifica tion of possibilistic uncertainty and information functions, Fuzzy Sets Syst., 24(2).\nSJ80 SHORE,J. and JOHNSON,R., 1980, Axiomatic derivation of the principle of maximum entropy and the principle of minimum cross-entropy, IEEE Trans.Inf. Theory, IT-26.\nY80 YAGER, Y., 1980, Aspects of possibilistic uncer tainty, Int. J. Man-Machine Studies, 12.\nZ78 ZADEH,L., 1978, Fuzzy sets as a basis for a the ory of possibility, Fuzzy Sets Syst., 3(1)."
    } ],
    "references" : [ {
      "title" : "Information Theory and Ap­",
      "author" : [ "S. G11 GUIASU" ],
      "venue" : null,
      "citeRegEx" : "GUIASU,? \\Q1977\\E",
      "shortCiteRegEx" : "GUIASU",
      "year" : 1977
    }, {
      "title" : "On the no­ tion of distance representing information close­",
      "author" : [ "HK83 HIGASHI", "G. KLIR" ],
      "venue" : "ness, Int. J. Gen. Syst.,",
      "citeRegEx" : "HIGASHI et al\\.,? \\Q1983\\E",
      "shortCiteRegEx" : "HIGASHI et al\\.",
      "year" : 1983
    }, {
      "title" : "Measures of uncertainty and information based on possibility distributions Int",
      "author" : [ "M HIGASHI" ],
      "venue" : "KLIR,G.,",
      "citeRegEx" : "HIGASHI,? \\Q1982\\E",
      "shortCiteRegEx" : "HIGASHI",
      "year" : 1982
    }, {
      "title" : "On the rationale of maximum entropy methods",
      "author" : [ "E. J82 JAYNES" ],
      "venue" : "Proc. IEEE,",
      "citeRegEx" : "JAYNES,? \\Q1982\\E",
      "shortCiteRegEx" : "JAYNES",
      "year" : 1982
    }, {
      "title" : "Structure of possibilistic in­ formation metrics and distances",
      "author" : [ "A. R90 RAMER" ],
      "venue" : "Int. J. Gen. Syst.,",
      "citeRegEx" : "RAMER,? \\Q1990\\E",
      "shortCiteRegEx" : "RAMER",
      "year" : 1990
    }, {
      "title" : "Concepts of fuzzy informa­ tion measures on continuous domains",
      "author" : [ "A. R89 RAMER" ],
      "venue" : "Int. J. Gen. Syst.,",
      "citeRegEx" : "RAMER,? \\Q1989\\E",
      "shortCiteRegEx" : "RAMER",
      "year" : 1989
    }, {
      "title" : "Aspects of possibilistic uncer­ tainty",
      "author" : [ "Y. Y80 YAGER" ],
      "venue" : "Int. J. Man-Machine Studies,",
      "citeRegEx" : "YAGER,? \\Q1980\\E",
      "shortCiteRegEx" : "YAGER",
      "year" : 1980
    } ],
    "referenceMentions" : [ ],
    "year" : 2011,
    "abstractText" : "The study of such assignments and their properties falls within the purview of possibility theory [DP88, Y80, Z78]. It, like probability theory, assigns values between 0 and 1 to express likelihoods of outcomes. Here, however, the similarity ends. Possibility theory uses the maximum and minimum functions to com­ bine uncertainties, whereas probability theory uses the plus and times operations. This leads to very dissim­ ilar theories in terms of analy tical framework, even though they share several semantic concepts. One of the shared concepts consists of expressing quantita­ tively the uncertainty associated with a given distribu­ tion. In probability theory its value corresponds to the gain of information that would result from conduct­ ing an experiment and ascertaining an actual result. This gain of information can equally well be viewed as a decrease in uncertainty about the outcome of an experiment. In this case the standard measure of in­ formation, and thus uncertainty, is Shannon entropy [AD75, G77]. It enjoys several advantages-it is char­ acterized uniquely by a few, very natural properties, and it can be conveniently used in decision processes. This application is based on the principle of maximum entropy; it has become a popular method of relating decisions to uncertainty.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}