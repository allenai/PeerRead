{
  "name" : "1706.02613.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "In recent years, deep learning achieves state-of-the-art results in various different tasks, however, neural networks are mostly trained using supervised learning, where a massive amount of labeled data is required. While collecting unlabeled data is relatively easy given the amount of data available on the web, providing accurate labeling is usually an expensive task. In order to overcome this problem, data science becomes an art of extracting labels out of thin air. Some popular approaches to labeling are crowdsourcing, where the labeling is not done by experts, and mining available meta-data, such as text that is linked to an image in a webpage. Unfortunately, this gives rise to a problem of abundant noisy labels - labels may often be corrupted [19], which might deteriorate the performance of neural-networks [12].\nLet us start with an intuitive explanation as to why noisy labels are problematic. Common neural network optimization algorithms start with a random guess of what the classifier should be, and then iteratively update the classifier based on stochastically sampled examples from a given dataset, optimizing a given loss function such as the hinge loss or the logistic loss. In this process, wrong predictions lead to an update of the classifier that would hopefully result in beter classification performance. While at the beginning of the training process the predictions are likely to be wrong, as the classifier improves it will fail on less and less examples, thus making fewer and fewer\nar X\niv :1\n70 6.\n02 61\n3v 1\n[ cs\n.L G\n] 8\nJ un\nupdates. On the other hand, in the presence of noisy labels, as the classifier improves the effect of the noise increases - the classifier may give correct predictions, but will still have to update due to wrong labeling. Thus, in an advanced stage of the training process the majority of the updates may actually be due to wrongly labeled examples, and therefore will not allow the classifier to further improve.\nTo tackle this problem, we propose to decouple the decision of “when to update” from the decision of “how to update”. As mentioned before, in the presence of noisy labels, if we update only when the classifier’s prediction differs from the available label, then at the end of the optimization process, these few updates will probably be mainly due to noisy labels. We would therefore like a different update criterion, that would let us decide whether it is worthy to update the classifier based on a given example. We would like to preserve the behaviour of performing many updates at the beginning of the training process but only a few updates when we approach convergence. To do so, we suggest to train two predictors, and perform update steps only in case of disagreement between them. This way, when the predictors get better, the “area” of their disagreement gets smaller, and updates are performed only on examples that lie in the disagreement area, therefore preserving the desired behaviour of the standard optimization process. On the other hand, since we do not perform an update based on disagreement with the label (which may be due to a problem in the label rather than a problem in the predictor), this method keeps the effective amount of noisy labels seen throughout the training process at a constant rate.\nThe idea of deciding “when to update” based on a disagreement between classifiers is closely related to approaches for active learning and selective sampling - a setup in which the learner does not have unlimited access to labeled examples, but rather has to query for each instance’s label, provided at a given cost (see for example [34]). Specifically, the well known query-by-committee algorithm maintains a version space of hypotheses and at each iteration, decides whether to query the label of a given instance by sampling two hypotheses uniformly at random from the version space [14, 35]. Naturally, maintaining the version space of deep networks seems to be intractable. Our algorithm maintains only two deep networks. The difference between them stems from the random initialization. Therefore, unlike the original query-by-committee algorithm, that samples from the version space at every iteration, we sample from the original hypotheses class only once (at the initialization), and from there on, we update these two hypotheses using the backpropagation rule, when they disagree on the label. To the best of our knowledge, this algorithm was not proposed/analyzed previously, not in the active learning literature and especially not as a method for dealing with noisy labels.\nTo show that this method indeed improves the robustness of deep learning to noisy labels, we conduct an experiment that aims to study a real-world scenario of acquiring noisy labels for a given dataset. We consider the task of gender classification based on images. We did not have a dedicated dataset for this task. Instead, we relied on the Labeled Faces in the Wild (LFW) dataset, which contains images of different people along with their names, but with no information about their gender. To find the gender for each image, we use an online service to match a gender to a given name (as is suggested by [25]), a method which is naturally prone to noisy labels (due to unisex names). Applying our algorithm to an existing neural network architecture reduces the\neffect of the noisy lables, achieving better results than similar available approaches, when tested on a clean subset of the data. We also performed a controlled experiment, in which the base algorithm is the perceptron, and show that using our approach leads to a noise resilient algorithm, which can handle an extermely high label noise rates of up to 40%. The controlled experiments are detailed in Appendix B.\nIn order to provide theoretical gurantees for our meta algorithm, we need to tackle two questions: 1. does this algoirthm converge? and if so, how quickly? and 2. does it converge to an optimum? We give a positive answer to the first question, when the base algorithm is the perceptron and the noise is label flip with a constant probability. Specifically, we prove that the expected number of iterations required by the resulting algorithm equals (up to a constant factor) to that of the perceptron in the noise-free setting. As for the second question, clearly, the convergence depends on the initialisation of the two predictors. For example, if we initialize the two predictors to be the same predictor, the algorithm will not perform any updates. Furthermore, we derive lower bounds on the quality of the solution even if we initialize the two predictors at random. In particular, we show that for some distributions, the algorithm’s error will be bounded away from zero, even in the case of linearly separable data. This raises the question of whether a better initialization procedure may be helpful. Indeed, we show that for the same distribution mentioned above, even if we add random label noise, if we initialize the predictors by performing few vanilla perceptron iterations, then the algorithm performs much better. Despite this worst case pessimism, we show that empirically, when working with natural data, the algorithm converges to a good solution. We leave a formal investigation of distribution dependent upper bounds to future work."
    }, {
      "heading" : "2 Related Work",
      "text" : "The effects of noisy labels was vastly studied in many different learning algorithms (see for example the survey in [13]), and various solutions to this problem have been proposed, some of them with theoretically provable bounds, including methods like statistical queries, boosting, bagging and more [3, 7, 8, 21, 23, 26, 27, 29, 31]. Our focus in this paper is on the problem of noisy labels in the context of deep learning. Recently, there have been several works aiming at improving the resilience of deep learning to noisy labels. To the best of our knowledge, there are four main approaches. The first changes the loss function. The second adds a layer that tries to mimic the noise behaviour. The third groups examples into buckets. The fourth tries to clean the data as a preprocessing step. Beyond these approaches, there are methods that assume a small clean data set and another large, noisy, or even unlabeled, data set [1, 6, 30, 38]. We now list some specific algorithms from these families.\n[33] proposed to change the cross entropy loss function by adding a regularization term that takes into account the current prediction of the network. This method is inspired by a technique called minimum entropy regularization, detailed in [16, 17]. It was also found to be effective by [12], which suggested a further improvement of this method by effectively increasing the weight of the regularization term during the training procedure.\n[28] suggested to use a probablilstic model that models the conditional probability of seeing a wrong label, where the correct label is a latent variable of the model. While [28] assume that the probablity of label-flips between classes is known in advance, a follow-up work by [36] extends this method to a case were these probabilities are unknown. An improved method, that takes into account the fact that some instances might be more likely to have a wrong label, has been proposed recently in [15]. In particular, they add another softmax layer to the network, that can use the output of the last hidden layer of the network in order to predict the probability of the label being flipped. Unfortunately, their method involves optimizing the biases of the additional softmax layer by first training it on a simpler setup (without using the last hidden layer), which implies two-phase training that further complicates the optimization process. It is worth noting that there are some other works that suggest methods that are very similar to [15, 36], with a slightly different objective or training method [5, 20], or otherwise suggest a complicated process which involves estimation of the class-dependent noise probabilities [32]. Another method from the same family is the one described in [37], who suggests to differentiate between “confusing” noise, where some features of the example make it hard to label, or otherwise a completely random label noise, where the mislabeling has no clear reason.\n[39] suggested to train the network to predict labels on a randomly selected group of images from the same class, instead of classifying each image individually. In their method, a group of images is fed as an input to the network, which merges their inner representation in a deeper level of the network, along with an attention model added to each image, and producing a single prediction. Therefore, noisy labels may appear in groups with correctly labeled examples, thus diminishing their impact. The final setup is rather complicated, involving many hyper-parameters, rather than providing a simple plug-and-play solution to make an existing architecture robust to noisy labels.\nFrom the family of preprocessing methods, we mention [4, 10], that try to eliminate instances that are suspected to be mislabeled. Our method shares a similar motivation of disregarding contaminated instances, but without the cost of complicating the training process by a preprocessing phase.\nIn our experiment we test the performance of our method against methods that are as simple as training a vanilla version of neural network. In particular, from the family of modified loss function we chose the two variants of the regularized cross entropy loss suggested by [33] (soft and hard bootsrapping). From the family of adding a layer that models the noise, we chose to compare to one of the models suggested in [15] (which is very similar to the model proposed by [36]), because this model does not require any assumptions or complication of the training process. We find that our method outperformed all of these competing methods, while being extremely simple to implement.\nFinally, as mentioned before, our “when to update” rule is closely related to approaches for active learning and selective sampling, and in particular to the query-bycommittee algorithm. In [14] a thorough analysis is provided for various base algorithms implementing the query-by-committe update rule, and particularly they analyze the perceptron base algorithm under some strong distributional assumptions. In other works, an ensemble of neural networks is trained in an active learning setup to improve the generalization of neural networks [2, 11, 22]. Our method could be seen\nas a simplified member of ensemble methods. As mentioned before, our motivation is very different than the active learning scenario, since our main goal is dealing with noisy labels, rather than trying to reduce the number of label queries. To the best of our knowledge, the algorithm we propose was not used or analyzed in the past for the purpose of dealing with noisy labels in deep learning."
    }, {
      "heading" : "3 Method",
      "text" : "As mentioned before, to tackle the problem of noisy labels, we suggest to change the update rule commonly used in deep learning optimization algorithms in order to decouple the decision of “when to update” from “how to update”. In our approach, the decision of “when to update” does not depend on the label. Instead, it depends on a disagreement between two different networks. This method could be generally thought of as a meta-algorithm that uses two base classifiers, performing updates according to a base learning algorithm, but only on examples for which there is a disagreement between the two classifiers.\nTo put this formally, let X be an instance space and Y be the label space, and assume we sample examples from a distribution D̃ over X × Y , with possibly noisy labels. We wish to train a classifier h, coming from a hypothesis class H. We rely on an update rule, U , that updates h based on its current value as well as a mini-batch of b examples. The meta algorithm receives as input a pair of two classifiers, h1, h2 ∈ H, the update rule, U , and a mini batch size, b. A pseudo-code is given in Algorithm 1.\nNote that we do not specify how to initialize the two base classifiers, h1, h2. When using deep learning as the base algorithm, the easiest approach is maybe to perform a random initialization. Another approach is to first train the two classifiers while following the regular “when to update” rule (which is based on the label y), possibly training each classifier on a different subset of the data, and switching to the suggested update rule only in an advanced stage of the training process. We later show that the second approach is preferable.\nAt the end of the optimization process, we can simply return one of the trained classifiers. If a small accurately labeled test data is availble, we can choose to return the classifier with the better accuracy on the clean test data.\nAlgorithm 1 Update by Disagreement input:\nan update rule U batch size b two initial predictors h1, h2 ∈ H\nfor t = 1, 2, . . . , N do draw mini-batch (x1, y1), . . . , (xb, yb) ∼ D̃b let S = {(xi, yi) : h1(xi) 6= h2(xi)} h1 ← U(h1, S) h2 ← U(h2, S) end for"
    }, {
      "heading" : "4 Theoretical analysis",
      "text" : "Since a convergence analysis for deep learning is beyond our reach even in the noisefree setting, we focus on analyzing properties of our algorithm for linearly separable data, which is corrupted by random label noise, and while using the perceptron as a base algorithm.\nLet X = {x ∈ Rd : ‖x‖ ≤ 1}, Y = {±1}, and let D be a probability distribution over X × Y , such that there exists w∗ for which D({(x, y) : y〈w∗, x〉 < 1}) = 0. The distribution we observe, denoted D̃, is a noisy version of D. Specifically, to sample (x, ỹ) ∼ D̃ one should sample (x, y) ∼ D and output (x, y) with probability 1−µ and (x,−y) with probability µ. Here, µ is in [0, 1/2).\nFinally, let H be the class of linear classifiers, namely, H = {x 7→ sign(〈w, x〉) : w ∈ Rd}. We use the perceptron’s update rule with mini-batch size of 1. That is, given the classifier wt ∈ Rd, the update on example (xt, yt) ∈ X × Y is: wt+1 = U(wt, (xt, yt)) := wt + yt xt.\nAs mentioned in the introduction, to provide a full theoretical analysis of this algorithm, we need to account for two questions:\n1. does this algoirthm converge? and if so, how quickly?\n2. does it converge to an optimum?\nTheorem 1 below provides a positive answer for the first question. It shows that the number of updates of our algorithm is only larger by a constant factor (that depends on the initial vectors and the amount of noise) relatively to the bound for the vanilla perceptron in the noise-less case.\nTheorem 1 Suppose that the “Update by Disagreement” algorithm is run on a sequence of random N examples from D̃, and with initial vectors w(1)0 , w (2) 0 . Denote K = maxi ‖w(i)0 ‖. Let T be the number of updates performed by the “Update by Disagreement” algorithm. Then, E[T ] ≤ 3 (4K+1)(1−2µ)2 ‖w\n∗‖2 where the expectation is w.r.t. the randomness of sampling from D̃.\nProof It will be more convenient to rewrite the algorithm as follows. We perform N iterations, where at iteration t we receive (xt, ỹt), and update w (i) t+1 = w (i) t + τt ỹt xt , where\nτt =\n{ 1 if sign(〈w(1)t , xt〉) 6= sign(〈w (2) t , xt〉)\n0 otherwise\nObserve that we can write ỹt = θtyt, where (xt, yt) ∼ D, and θt is a random variables with P[θt = 1] = 1−µ and P[θt = −1] = µ. We also use the notation vt = yt〈w∗, xt〉 and ṽt = θtvt. Our goal is to upper bound T̄ := E[T ] = E[ ∑ t τt].\nWe start with showing that\nE [ N∑ t=1 τtṽt ] ≥ (1− 2µ)T (1)\nIndeed, since θt is independent of τt and vt, we get that:\nE[τtṽt] = E[τtθtvt] = E[θt] · E[τtvt] = (1− 2µ)E[τtvt] ≥ (1− 2µ)E[τt]\nwhere in the last inequality we used the fact that vt ≥ 1 with probability 1 and τt is non-negative. Summing over t we obtain that Equation 1 holds.\nNext, we show that for i ∈ {1, 2},\n‖w(i)t ‖2 ≤ ‖w (i) 0 ‖2 + N∑ t=1 τt(2‖w(2)0 − w (1) 0 ‖+ 1) (2)\nIndeed, since the update of w(1)t+1 and w (2) t+1 is identical, we have that ‖w (1) t+1−w (2) t+1‖ = ‖w(1)0 −w (2) 0 ‖ for every t. Now, whenever τt = 1 we have that either yt〈w (1) t−1, xt〉 ≤ 0 or yt〈w(2)t−1, xt〉 ≤ 0. Assume w.l.o.g. that yt〈w (1) t−1, xt〉 ≤ 0. Then,\n‖w(1)t ‖2 = ‖w (1) t−1 + ytxt‖2 = ‖w (1) t−1‖2 + 2yt〈w (1) t−1, xt〉+ ‖xt‖2 ≤ ‖w (1) t−1‖2 + 1\nSecond,\n‖w(2)t ‖2 = ‖w (2) t−1 + ytxt‖2 = ‖w (2) t−1‖2 + 2yt〈w (2) t−1, xt〉+ ‖xt‖2\n≤ ‖w(2)t−1‖2 + 2yt〈w (2) t−1 − w (1) t−1, xt〉+ ‖xt‖2\n≤ ‖w(2)t−1‖2 + 2 ‖w (2) t−1 − w (1) t−1‖+ 1 = ‖w (2) t−1‖2 + 2 ‖w (2) 0 − w (1) 0 ‖+ 1\nTherefore, the above two equations imply ∀i ∈ {1, 2}, ‖w(i)t ‖2 ≤ ‖w (i) t−1‖2+2 ‖w (2) 0 − w (1) 0 ‖+ 1. Summing over t we obtain that Equation 2 holds.\nEquipped with Equation 1 and Equation 2 we are ready to prove the theorem. Denote K = maxi ‖w(i)0 ‖ and note that ‖w (2) 0 − w (1) 0 ‖ ≤ 2K. We prove the theorem by providing upper and lower bounds on E[〈w(i)t , w∗〉]. Combining the update rule with Equation 1 we get:\nE[〈w(i)t , w∗〉] = 〈w (i) 0 , w ∗〉+ E [ N∑ t=1 τt ṽt ] ≥ 〈w(i)0 , w∗〉+ (1− 2µ)T̄ ≥ −K ‖w∗‖+ (1− 2µ)T̄\nTo construct an upper bound, first note that Equation 2 implies that\nE[‖w(i)t ‖2] ≤ ‖w (i) 0 ‖2 + (2‖w (2) 0 − w (1) 0 ‖+ 1)T̄ ≤ K2 + (4K + 1) T̄\nUsing the above and Jensen’s inequality, we get that\nE[〈w(i)t , w∗〉] ≤ E[‖w (i) t ‖ ‖w∗‖] ≤ ‖w∗‖ √ E[‖w(i)t ‖2] ≤ ‖w∗‖ √ K2 + (4K + 1)T̄\nComparing the upper and lower bounds, we obtain that −K ‖w∗‖+ (1− 2µ)T̄ ≤ ‖w∗‖ √ K2 + (4K + 1)T̄\nUsing √ a+ b ≤ √ a+ √ b, the above implies that\n(1− 2µ)T̄ − ‖w∗‖ √ (4K + 1) √ T̄ − 2K ‖w∗‖ ≤ 0\nDenote α = ‖w∗‖ √ (4K + 1), then the above also implies that (1− 2µ)T̄ −α √ T̄ − α ≤ 0. Denote β = α/(1 − 2µ), using standard algebraic manipulations, the above implies that T̄ ≤ β + β2 + β1.5 ≤ 3β2 , where we used the fact that ‖w∗‖ must be at least 1 for the separability assumption to hold, hence β ≥ 1. This concludes our proof.\nThe above theorem tells us that our algorithm converges quickly. We next address the second question, regarding the quality of the point to which the algorithm converges. As mentioned in the introduction, the convergence must depend on the initial predictors. Indeed, if w(1)0 = w (2) 0 , then the algorithm will not make any updates. The next question is what happens if we initialize w(1)0 and w (2) 0 at random. The lemma below shows that this does not suffice to ensure convergence to the optimum, even if the data is linearly separable without noise. The proof for this lemma is given in Appendix A.\nLemma 1 Fix some δ ∈ (0, 1) and let d be an integer greater than 40 log(1/δ). There exists a distribution over Rd × {±1}, which is separable by a weight vector w∗ for which ‖w∗‖2 = d, such that running the “Update by Disagreement” algorithm, with the perceptron as the underlying update rule, and with every coordinate of w(1)0 , w (2) 0 initialized according to any symmetric distribution over R, will yield a solution whose error is at least 1/8, with probability of at least 1− δ.\nTrying to circumvent the lower bound given in the above lemma, one may wonder what would happen if we will initialize w(1)0 , w (2) 0 differently. Intuitively, maybe noisy labels are not such a big problem at the beginning of the learning process. Therefore, we can initialize w(1)0 , w (2) 0 by running the vanilla perceptron for several iterations, and only then switch to our algorithm. Trivially, for the distribution we constructed in the proof of Lemma 1, this approach will work just because in the noise-free setting, both w(1)0 and w (2) 0 will converge to vectors that give the same predictions as w\n∗. But, what would happen in the noisy setting, when we flip the label of every example with probability of µ? The lemma below shows that the error of the resulting solution is likely to be order of µ3. Here again, the proof is given in Appendix A.\nLemma 2 Consider a vectorw∗ ∈ {±1}d and the distribution D̃ over Rd×{±1} such that to sample a pair (x, ỹ) we first choose x uniformly at random from {e1, . . . , ed}, set y = 〈w∗, ei〉, and set ỹ = y with probability 1 − µ and ỹ = −y with probability µ. Let w(1)0 , w (2) 0 be the result of running the vanilla perceptron algorithm on random examples from D̃ for any number of iterations. Suppose that we run the “Update by Disagreement” algorithm for an additional arbitrary number of iterations. Then, the error of the solution is likely to be Ω(µ3).\nTo summarize, we see that without making additional assumptions on the data distribution, it is impossible to prove convergence of our algorithm to a good solution. In the next section we show that for natural data distributions, our algorithm converges to a very good solution."
    }, {
      "heading" : "5 Experiments",
      "text" : "We now demonstrate the merit of our suggested meta-algorithm using empirical evaluation. Our main experiment is using our algorithm with deep networks in a real-world scenario of noisy labels. In particular, we use a hypothesis class of deep networks and a Stochastic Gradient Descent with momentum as the basis update rule. The task is classifying face images according to gender. As training data, we use the Labeled Faces in the Wild (LFW) dataset for which we had a labeling of the name of the face, but we did not have gender labeling. To construct gender labels, we used an external service that provides gender labels based on names. This process resulted in noisy labels. We show that our method leads to state-of-the-art results on this task, compared to competing noise robustness methods. We also performed controlled experiments to demonstrate our algorithm’s performance with linear classification with varying levels of noise. Due to the lack of space, these results are detailed in Appendix B."
    }, {
      "heading" : "5.1 Deep Learning",
      "text" : "We have applied our algorithm with a Stochastic Gradient Descent (SGD) with momentum as the base update rule on the task of labeling images of faces according to gender. The images were taken from the Labeled Faces in the Wild (LFW) benchmark [18]. This benchmark consists of 13,233 images of 5,749 different people collected from the web, labeled with the name of the person in the picture. Since the gender of each subject is not provided, we follow the method of [25] and use a service that determines a person’s gender by their name (if it is recognized), along with a confidence level. This method gives rise to “natural” noisy labels due to “unisex” names, and therefore allows us to experiment with a real-world setup of dataset with noisy labels.\nWe have constructed train and test sets as follows. We first took all the individuals on which the gender service gave 100% confidence. We divided this set at random into\nthree subsets of equal size, denoted N1, N2, N3. We denote by N4 the individuals on which the confidence level is in [90%, 100%), and by N5 the individuals on which the confidence level is in [0%, 90%). Needless to say that all the setsN1, . . . , N5 have zero intersection with each other.\nWe repeated each experiment three times, where in every time we used a different Ni as the test set, for i ∈ {1, 2, 3}. Suppose N1 is the test set, then for the training set we used two configurations:\n1. A dataset consisting of all the images that belong to names in N2, N3, N4, N5, where unrecognized names were labeled as male (since the majority of the subjects in the LFW dataset are males).\n2. A dataset consisting of all the images that belong to names in N2, N3, N4.\nWe use a network architecture suggested by [24], using an available tensorflow implementation1. It should be noted that we did not change any parameters of the network architecture or the optimization process, and use the default parameters in the implementation. Since the amount of male and female subjects in the dataset is not balanced, we use an objective of maximizing the balanced accuracy [9] - the average accuracy obtained on either class.\nTraining is done for 30,000 iterations on 128 examples mini-batch. In order to make the networks disagreement meaningful, we initialize the two networks by training both of them normally (updating on all the examples) until iteration #5000, where we switch to training with the “Update by Disagreement” rule. Due to the fact that we are not updating on all examples, we decrease the weight of batches that had less than 10% of the original examples in the original batch to stabilize the gradients. The exact code with the implementation details will be posted online.\nWe inspect the balanced accuracy on our test data during the training process, comparing our method to a vanilla neural network training, as well as to soft and hard bootstrapping described in [33] and to the s-model described in [15], all of which are using the same network architecture. We use the initialization parameters for [15, 33] that were suggested in the original papers. We show that while in other methods, the accuracy effectively decreases during the training process due to overfitting the noisy labels, in our method this effect is less substantial, allowing the network to keep improving.\nWe study two different scenarios, one in which a small clean test data is available for model selection, and therefore we can choose the iteration with best test accuracy, and a more realistic scenario in which there is no clean test data at hand. For the first scenario, we observe the balanced accuracy of the best available iteration. For the second scenario, we observe the balanced accuracy of the last iteration.\nAs can be seen in Figure 2 and the supplementary results listed in Table 1 in Appendix B, our method outperforms the other methods in both situations. This is true for both datasets, although, as expected, the improvement in performance is less substantial on the cleaner dataset.\nThe second best algorithm is the s-model described in [15]. Since our method can be applied to any base algorithm, we also applied our method on top of the s-model.\n1https://github.com/dpressel/rude-carnie.\nThis yields even better performance, especially when the data is less noisy, where we obtain a significant improvement."
    }, {
      "heading" : "6 Discussion",
      "text" : "We have described an extremely simple approach for supervised learning in the presence of noisy labels. The basic idea is to decouple the “when to update” rule from the “how to update” rule. We achieve this by maintaining two predictors, and update based on their disagreement. We have shown that this simple approach leads to state-of-theart results.\nOur theoretical analysis shows that the approach leads to fast convergence rate when the underlying update rule is the perceptron. We have also shown that proving that the method converges to an optimal solution must rely on distributional assumptions. There are several immediate open questions that we leave to future work. First, suggesting distributional assumptions that are likely to hold in practice and proving that the algorithm converges to an optimal solution under these assumptions. Second, extending the convergence proof beyond linear predictors. While obtaining absolute convergence guarantees seems beyond reach at the moment, coming up with oracle based convergence guarantees may be feasible.\nAcknowledgements: This research is supported by the European Research Council (TheoryDL project)."
    }, {
      "heading" : "A Proofs",
      "text" : "Proof of Lemma 1: Let the distribution over instances be concentrated uniformly over the vectors of the standard basis, e1, . . . , ed. Let w∗ be any vector in {±1}d. Fix some i. Then, with probability 1/4 over the choice ofw(1)0 , w (2) 0 , we have that the signs of 〈w (1) 0 , ei〉, 〈w (2) 0 , ei〉 agree with each other, but disagree with 〈w∗, ei〉. It is easy to see that the i’th coordinate of w(1) and w(2) will never be updated. Therefore, no matter how many iterations we will perform, the solution will be wrong on ei. It follows that the probability of error is lower bounded by the random variable 1d ∑d i=1 Zi, the Zi are i.i.d. Bernoulli variables with P[Zi = 1] = 1/4. Using Chernoff’s inequality,\nP\n[ 1\nd d∑ i=1 Zi < 1/8\n] ≤ exp(−dC) ,\nwhere C = 3112 . It follows that if d ≥ log(1/δ)/C then with probability of at least 1− δ we will have that the error of the solution is at least 1/8.\nProof of Lemma 2: Let wt be a random vector indicating the vector of the perceptron after t iterations. Fix some i and w.l.o.g. assume that w∗i = 1. The value of wt at the i’th coordinate is always in the set {−1, 0, 1}. Furthermore, it alters its value like a Markov chain with a transition matrix of\nP = µ 1− µ 0µ 0 1− µ 0 µ 1− µ  It is easy to verify that the stationary distribution over {−1, 0, 1} is\nπ =\n( µ2\nµ+ (1− µ)2 , µ(1− µ) µ+ (1− µ)2 , (1− µ)2 µ+ (1− µ)2\n) .\nNow, the probability that our algorithm will fail on the i’th coordinate is lower bounded by the probability that the i’th coordinate of both w(1), w(2) will be 0 and then our algorithm will see a flipped label. This would happen with probability of order of µ3 for a small µ."
    }, {
      "heading" : "B Experimental Results",
      "text" : "We show our algorithm’s performance in two controlled setups, using a perceptron based algorithm. In the first setup we test we run our algorithm on synthetic data that is generated by randomly sampling instances from the unit ball in Rd, with different probabilities for random label-flips. In the second setup we test our performance on a binary classification task based on the MNIST dataset, again with random labelflips with different probabilities. We show that in both scenarios, our adaptation of the perceptron algorithm results in resilience for large noise probabilities, unlike the vanilla perceptron algorithm which fails to converge on even small amounts of noise.\nB.1 Linear Classification on Synthetic Data To test the performance of the suggested perceptron-like algorithm, we use synthetic data in various dimensions, generated in the following process:\n1. Randomly choose w∗ ∈ Rd with a given norm ‖w∗‖ = 103\n(a) In each iteration, draw vectors x ∈ Rd from the uniform distribution on the unit ball until |〈w∗, x〉| ≥ 1, and then set y = sign(〈w∗, x〉).\n(b) With probability µ < 0.5, flip the sign of y.\nThe above was performed for different values of µ, and repeated 5 times for each setup. In Figure 3 we depict the average performance over the 5 runs. As can be seen, our algorithm greatly improves the noise resilience of the vanilla perceptron.\nB.2 Linear Classification on MNIST Data Noisy Labels Here we use a binary classification task of discriminating between the digits 4 and 7, from the MNIST dataset.\nWe tested the performance of the above algorithm against the regular perceptron algorithm with various levels of noise.\nB.3 Deep Learning Detailed Results The table below details the results of the LFW experiment, showing the balanced accuracy of all the different methods for dealing with noisy labels. We show the results on the best iteration and on the last iteration. We observe that our method outperforms other alternative, and combining it with the s-model of [15] results in an even better improvement."
    } ],
    "references" : [ {
      "title" : "Two-view feature generation model for semisupervised learning",
      "author" : [ "Rie Kubota Ando", "Tong Zhang" ],
      "venue" : "In Proceedings of the 24th international conference on Machine learning,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2007
    }, {
      "title" : "Training connectionist networks with queries and selective sampling",
      "author" : [ "Les E Atlas", "David A Cohn", "Richard E Ladner", "Mohamed A El-Sharkawi", "Robert J Marks", "ME Aggoune", "DC Park" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1989
    }, {
      "title" : "The power of localization for efficiently learning linear separators with noise",
      "author" : [ "Pranjal Awasthi", "Maria Florina Balcan", "Philip M Long" ],
      "venue" : "In Proceedings of the 46th Annual ACM Symposium on Theory of Computing,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2014
    }, {
      "title" : "Decontamination of training samples for supervised pattern recognition methods. In Joint IAPR International Workshops on Statistical Techniques in Pattern Recognition (SPR) and Structural and Syntactic Pattern Recognition (SSPR)",
      "author" : [ "Ricardo Barandela", "Eduardo Gasca" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2000
    }, {
      "title" : "Training deep neural-networks based on unreliable labels",
      "author" : [ "Alan Joseph Bekker", "Jacob Goldberger" ],
      "venue" : "In Acoustics, Speech and Signal Processing (ICASSP),",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2016
    }, {
      "title" : "Combining labeled and unlabeled data with cotraining",
      "author" : [ "Avrim Blum", "Tom Mitchell" ],
      "venue" : "In Proceedings of the eleventh annual conference on Computational learning theory,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1998
    }, {
      "title" : "Label-noise robust logistic regression and its applications",
      "author" : [ "Jakramate Bootkrajang", "Ata Kabán" ],
      "venue" : "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2012
    }, {
      "title" : "Boosting in the presence of label noise",
      "author" : [ "Jakramate Bootkrajang", "Ata Kabán" ],
      "venue" : "arXiv preprint arXiv:1309.6818,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2013
    }, {
      "title" : "The balanced accuracy and its posterior distribution",
      "author" : [ "Kay Henning Brodersen", "Cheng Soon Ong", "Klaas Enno Stephan", "Joachim M Buhmann" ],
      "venue" : "In Pattern recognition (ICPR),",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2010
    }, {
      "title" : "Identifying mislabeled training data",
      "author" : [ "Carla E. Brodley", "Mark A. Friedl" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1999
    }, {
      "title" : "Improving generalization with active learning",
      "author" : [ "David Cohn", "Les Atlas", "Richard Ladner" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1994
    }, {
      "title" : "On the robustness of convnets to training on noisy labels",
      "author" : [ "David Flatow", "Daniel Penner" ],
      "venue" : "penner_report.pdf,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2017
    }, {
      "title" : "Classification in the presence of label noise: a survey",
      "author" : [ "Benoı̂t Frénay", "Michel Verleysen" ],
      "venue" : "IEEE transactions on neural networks and learning systems,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2014
    }, {
      "title" : "Selective sampling using the query by committee algorithm",
      "author" : [ "Yoav Freund", "H Sebastian Seung", "Eli Shamir", "Naftali Tishby" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1997
    }, {
      "title" : "Training deep neural networks using a noise adaptation layer",
      "author" : [ "Jacob Goldberger", "Ehud Ben-Reuven" ],
      "venue" : "Under review for ICLR,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2017
    }, {
      "title" : "Entropy regularization. Semi-supervised learning, pages 151–168",
      "author" : [ "Yves Grandvalet", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2006
    }, {
      "title" : "Semi-supervised learning by entropy minimization",
      "author" : [ "Yves Grandvalet", "Yoshua Bengio" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2004
    }, {
      "title" : "Labeled faces in the wild: A database for studying face recognition in unconstrained environments",
      "author" : [ "Gary B Huang", "Manu Ramesh", "Tamara Berg", "Erik Learned-Miller" ],
      "venue" : "Technical report,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2007
    }, {
      "title" : "Quality management on amazon mechanical turk",
      "author" : [ "Panagiotis G Ipeirotis", "Foster Provost", "Jing Wang" ],
      "venue" : "In Proceedings of the ACM SIGKDD workshop on human computation,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2010
    }, {
      "title" : "Probabilistic learning from mislabelled data for multimedia content recognition",
      "author" : [ "Pravin Kakar", "Alex Yong-Sang Chia" ],
      "venue" : "In Multimedia and Expo (ICME),",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2015
    }, {
      "title" : "Efficient noise-tolerant learning from statistical queries",
      "author" : [ "Michael Kearns" ],
      "venue" : "Journal of the ACM (JACM),",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1998
    }, {
      "title" : "Neural network ensembles, cross validation, and active learning",
      "author" : [ "Anders Krogh", "Jesper Vedelsby" ],
      "venue" : "Advances in neural information processing systems,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1995
    }, {
      "title" : "Design of robust neural network classifiers",
      "author" : [ "Jan Larsen", "L Nonboe", "Mads Hintz-Madsen", "Lars Kai Hansen" ],
      "venue" : "In Acoustics, Speech and Signal Processing,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1998
    }, {
      "title" : "Age and gender classification using convolutional neural networks",
      "author" : [ "Gil Levi", "Tal Hassner" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2015
    }, {
      "title" : "Evaluation of face recognition apis and libraries",
      "author" : [ "Philip Masek", "Magnus Thulin" ],
      "venue" : "Master’s thesis, University of Gothenburg,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2015
    }, {
      "title" : "An empirical comparison of three boosting algorithms on real data sets with artificial class noise",
      "author" : [ "Ross A McDonald", "David J Hand", "Idris A Eckley" ],
      "venue" : "In International Workshop on Multiple Classifier Systems,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2003
    }, {
      "title" : "Learning from binary labels with instance-dependent corruption",
      "author" : [ "Aditya Krishna Menon", "Brendan van Rooyen", "Nagarajan Natarajan" ],
      "venue" : "arXiv preprint arXiv:1605.00751,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2016
    }, {
      "title" : "Learning to label aerial images from noisy data",
      "author" : [ "Volodymyr Mnih", "Geoffrey E Hinton" ],
      "venue" : "In Proceedings of the 29th International Conference on Machine Learning",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2012
    }, {
      "title" : "Learning with noisy labels",
      "author" : [ "Nagarajan Natarajan", "Inderjit S Dhillon", "Pradeep K Ravikumar", "Ambuj Tewari" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2013
    }, {
      "title" : "Analyzing the effectiveness and applicability of co-training",
      "author" : [ "Kamal Nigam", "Rayid Ghani" ],
      "venue" : "In Proceedings of the ninth international conference on Information and knowledge management,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2000
    }, {
      "title" : "Loss factorization, weakly supervised learning and label noise robustness",
      "author" : [ "Giorgio Patrini", "Frank Nielsen", "Richard Nock", "Marcello Carioni" ],
      "venue" : "arXiv preprint arXiv:1602.02450,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2016
    }, {
      "title" : "Making neural networks robust to label noise: a loss correction approach",
      "author" : [ "Giorgio Patrini", "Alessandro Rozza", "Aditya Menon", "Richard Nock", "Lizhen Qu" ],
      "venue" : "arXiv preprint arXiv:1609.03683,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2016
    }, {
      "title" : "Training deep neural networks on noisy labels with bootstrapping",
      "author" : [ "Scott Reed", "Honglak Lee", "Dragomir Anguelov", "Christian Szegedy", "Dumitru Erhan", "Andrew Rabinovich" ],
      "venue" : "arXiv preprint arXiv:1412.6596,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2014
    }, {
      "title" : "Active learning literature survey",
      "author" : [ "Burr Settles" ],
      "venue" : "University of Wisconsin, Madison,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2010
    }, {
      "title" : "Query by committee",
      "author" : [ "H Sebastian Seung", "Manfred Opper", "Haim Sompolinsky" ],
      "venue" : "In Proceedings of the fifth annual workshop on Computational learning theory,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 1992
    }, {
      "title" : "Training convolutional networks with noisy labels",
      "author" : [ "Sainbayar Sukhbaatar", "Joan Bruna", "Manohar Paluri", "Lubomir Bourdev", "Rob Fergus" ],
      "venue" : "arXiv preprint arXiv:1406.2080,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2014
    }, {
      "title" : "Learning from massive noisy labeled data for image classification",
      "author" : [ "Tong Xiao", "Tian Xia", "Yi Yang", "Chang Huang", "Xiaogang Wang" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2015
    }, {
      "title" : "Semi-supervised learning literature survey",
      "author" : [ "Xiaojin Zhu" ],
      "venue" : "Computer Sciences TR 1530,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 18,
      "context" : "Unfortunately, this gives rise to a problem of abundant noisy labels - labels may often be corrupted [19], which might deteriorate the performance of neural-networks [12].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 11,
      "context" : "Unfortunately, this gives rise to a problem of abundant noisy labels - labels may often be corrupted [19], which might deteriorate the performance of neural-networks [12].",
      "startOffset" : 166,
      "endOffset" : 170
    }, {
      "referenceID" : 33,
      "context" : "The idea of deciding “when to update” based on a disagreement between classifiers is closely related to approaches for active learning and selective sampling - a setup in which the learner does not have unlimited access to labeled examples, but rather has to query for each instance’s label, provided at a given cost (see for example [34]).",
      "startOffset" : 334,
      "endOffset" : 338
    }, {
      "referenceID" : 13,
      "context" : "Specifically, the well known query-by-committee algorithm maintains a version space of hypotheses and at each iteration, decides whether to query the label of a given instance by sampling two hypotheses uniformly at random from the version space [14, 35].",
      "startOffset" : 246,
      "endOffset" : 254
    }, {
      "referenceID" : 34,
      "context" : "Specifically, the well known query-by-committee algorithm maintains a version space of hypotheses and at each iteration, decides whether to query the label of a given instance by sampling two hypotheses uniformly at random from the version space [14, 35].",
      "startOffset" : 246,
      "endOffset" : 254
    }, {
      "referenceID" : 24,
      "context" : "To find the gender for each image, we use an online service to match a gender to a given name (as is suggested by [25]), a method which is naturally prone to noisy labels (due to unisex names).",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 12,
      "context" : "The effects of noisy labels was vastly studied in many different learning algorithms (see for example the survey in [13]), and various solutions to this problem have been proposed, some of them with theoretically provable bounds, including methods like statistical queries, boosting, bagging and more [3, 7, 8, 21, 23, 26, 27, 29, 31].",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 2,
      "context" : "The effects of noisy labels was vastly studied in many different learning algorithms (see for example the survey in [13]), and various solutions to this problem have been proposed, some of them with theoretically provable bounds, including methods like statistical queries, boosting, bagging and more [3, 7, 8, 21, 23, 26, 27, 29, 31].",
      "startOffset" : 301,
      "endOffset" : 334
    }, {
      "referenceID" : 6,
      "context" : "The effects of noisy labels was vastly studied in many different learning algorithms (see for example the survey in [13]), and various solutions to this problem have been proposed, some of them with theoretically provable bounds, including methods like statistical queries, boosting, bagging and more [3, 7, 8, 21, 23, 26, 27, 29, 31].",
      "startOffset" : 301,
      "endOffset" : 334
    }, {
      "referenceID" : 7,
      "context" : "The effects of noisy labels was vastly studied in many different learning algorithms (see for example the survey in [13]), and various solutions to this problem have been proposed, some of them with theoretically provable bounds, including methods like statistical queries, boosting, bagging and more [3, 7, 8, 21, 23, 26, 27, 29, 31].",
      "startOffset" : 301,
      "endOffset" : 334
    }, {
      "referenceID" : 20,
      "context" : "The effects of noisy labels was vastly studied in many different learning algorithms (see for example the survey in [13]), and various solutions to this problem have been proposed, some of them with theoretically provable bounds, including methods like statistical queries, boosting, bagging and more [3, 7, 8, 21, 23, 26, 27, 29, 31].",
      "startOffset" : 301,
      "endOffset" : 334
    }, {
      "referenceID" : 22,
      "context" : "The effects of noisy labels was vastly studied in many different learning algorithms (see for example the survey in [13]), and various solutions to this problem have been proposed, some of them with theoretically provable bounds, including methods like statistical queries, boosting, bagging and more [3, 7, 8, 21, 23, 26, 27, 29, 31].",
      "startOffset" : 301,
      "endOffset" : 334
    }, {
      "referenceID" : 25,
      "context" : "The effects of noisy labels was vastly studied in many different learning algorithms (see for example the survey in [13]), and various solutions to this problem have been proposed, some of them with theoretically provable bounds, including methods like statistical queries, boosting, bagging and more [3, 7, 8, 21, 23, 26, 27, 29, 31].",
      "startOffset" : 301,
      "endOffset" : 334
    }, {
      "referenceID" : 26,
      "context" : "The effects of noisy labels was vastly studied in many different learning algorithms (see for example the survey in [13]), and various solutions to this problem have been proposed, some of them with theoretically provable bounds, including methods like statistical queries, boosting, bagging and more [3, 7, 8, 21, 23, 26, 27, 29, 31].",
      "startOffset" : 301,
      "endOffset" : 334
    }, {
      "referenceID" : 28,
      "context" : "The effects of noisy labels was vastly studied in many different learning algorithms (see for example the survey in [13]), and various solutions to this problem have been proposed, some of them with theoretically provable bounds, including methods like statistical queries, boosting, bagging and more [3, 7, 8, 21, 23, 26, 27, 29, 31].",
      "startOffset" : 301,
      "endOffset" : 334
    }, {
      "referenceID" : 30,
      "context" : "The effects of noisy labels was vastly studied in many different learning algorithms (see for example the survey in [13]), and various solutions to this problem have been proposed, some of them with theoretically provable bounds, including methods like statistical queries, boosting, bagging and more [3, 7, 8, 21, 23, 26, 27, 29, 31].",
      "startOffset" : 301,
      "endOffset" : 334
    }, {
      "referenceID" : 0,
      "context" : "Beyond these approaches, there are methods that assume a small clean data set and another large, noisy, or even unlabeled, data set [1, 6, 30, 38].",
      "startOffset" : 132,
      "endOffset" : 146
    }, {
      "referenceID" : 5,
      "context" : "Beyond these approaches, there are methods that assume a small clean data set and another large, noisy, or even unlabeled, data set [1, 6, 30, 38].",
      "startOffset" : 132,
      "endOffset" : 146
    }, {
      "referenceID" : 29,
      "context" : "Beyond these approaches, there are methods that assume a small clean data set and another large, noisy, or even unlabeled, data set [1, 6, 30, 38].",
      "startOffset" : 132,
      "endOffset" : 146
    }, {
      "referenceID" : 37,
      "context" : "Beyond these approaches, there are methods that assume a small clean data set and another large, noisy, or even unlabeled, data set [1, 6, 30, 38].",
      "startOffset" : 132,
      "endOffset" : 146
    }, {
      "referenceID" : 32,
      "context" : "[33] proposed to change the cross entropy loss function by adding a regularization term that takes into account the current prediction of the network.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "This method is inspired by a technique called minimum entropy regularization, detailed in [16, 17].",
      "startOffset" : 90,
      "endOffset" : 98
    }, {
      "referenceID" : 16,
      "context" : "This method is inspired by a technique called minimum entropy regularization, detailed in [16, 17].",
      "startOffset" : 90,
      "endOffset" : 98
    }, {
      "referenceID" : 11,
      "context" : "It was also found to be effective by [12], which suggested a further improvement of this method by effectively increasing the weight of the regularization term during the training procedure.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 27,
      "context" : "[28] suggested to use a probablilstic model that models the conditional probability of seeing a wrong label, where the correct label is a latent variable of the model.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 27,
      "context" : "While [28] assume that the probablity of label-flips between classes is known in advance, a follow-up work by [36] extends this method to a case were these probabilities are unknown.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 35,
      "context" : "While [28] assume that the probablity of label-flips between classes is known in advance, a follow-up work by [36] extends this method to a case were these probabilities are unknown.",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 14,
      "context" : "An improved method, that takes into account the fact that some instances might be more likely to have a wrong label, has been proposed recently in [15].",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 14,
      "context" : "It is worth noting that there are some other works that suggest methods that are very similar to [15, 36], with a slightly different objective or training method [5, 20], or otherwise suggest a complicated process which involves estimation of the class-dependent noise probabilities [32].",
      "startOffset" : 97,
      "endOffset" : 105
    }, {
      "referenceID" : 35,
      "context" : "It is worth noting that there are some other works that suggest methods that are very similar to [15, 36], with a slightly different objective or training method [5, 20], or otherwise suggest a complicated process which involves estimation of the class-dependent noise probabilities [32].",
      "startOffset" : 97,
      "endOffset" : 105
    }, {
      "referenceID" : 4,
      "context" : "It is worth noting that there are some other works that suggest methods that are very similar to [15, 36], with a slightly different objective or training method [5, 20], or otherwise suggest a complicated process which involves estimation of the class-dependent noise probabilities [32].",
      "startOffset" : 162,
      "endOffset" : 169
    }, {
      "referenceID" : 19,
      "context" : "It is worth noting that there are some other works that suggest methods that are very similar to [15, 36], with a slightly different objective or training method [5, 20], or otherwise suggest a complicated process which involves estimation of the class-dependent noise probabilities [32].",
      "startOffset" : 162,
      "endOffset" : 169
    }, {
      "referenceID" : 31,
      "context" : "It is worth noting that there are some other works that suggest methods that are very similar to [15, 36], with a slightly different objective or training method [5, 20], or otherwise suggest a complicated process which involves estimation of the class-dependent noise probabilities [32].",
      "startOffset" : 283,
      "endOffset" : 287
    }, {
      "referenceID" : 36,
      "context" : "Another method from the same family is the one described in [37], who suggests to differentiate between “confusing” noise, where some features of the example make it hard to label, or otherwise a completely random label noise, where the mislabeling has no clear reason.",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 3,
      "context" : "From the family of preprocessing methods, we mention [4, 10], that try to eliminate instances that are suspected to be mislabeled.",
      "startOffset" : 53,
      "endOffset" : 60
    }, {
      "referenceID" : 9,
      "context" : "From the family of preprocessing methods, we mention [4, 10], that try to eliminate instances that are suspected to be mislabeled.",
      "startOffset" : 53,
      "endOffset" : 60
    }, {
      "referenceID" : 32,
      "context" : "In particular, from the family of modified loss function we chose the two variants of the regularized cross entropy loss suggested by [33] (soft and hard bootsrapping).",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 14,
      "context" : "From the family of adding a layer that models the noise, we chose to compare to one of the models suggested in [15] (which is very similar to the model proposed by [36]), because this model does not require any assumptions or complication of the training process.",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 35,
      "context" : "From the family of adding a layer that models the noise, we chose to compare to one of the models suggested in [15] (which is very similar to the model proposed by [36]), because this model does not require any assumptions or complication of the training process.",
      "startOffset" : 164,
      "endOffset" : 168
    }, {
      "referenceID" : 13,
      "context" : "In [14] a thorough analysis is provided for various base algorithms implementing the query-by-committe update rule, and particularly they analyze the perceptron base algorithm under some strong distributional assumptions.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 1,
      "context" : "In other works, an ensemble of neural networks is trained in an active learning setup to improve the generalization of neural networks [2, 11, 22].",
      "startOffset" : 135,
      "endOffset" : 146
    }, {
      "referenceID" : 10,
      "context" : "In other works, an ensemble of neural networks is trained in an active learning setup to improve the generalization of neural networks [2, 11, 22].",
      "startOffset" : 135,
      "endOffset" : 146
    }, {
      "referenceID" : 21,
      "context" : "In other works, an ensemble of neural networks is trained in an active learning setup to improve the generalization of neural networks [2, 11, 22].",
      "startOffset" : 135,
      "endOffset" : 146
    }, {
      "referenceID" : 17,
      "context" : "The images were taken from the Labeled Faces in the Wild (LFW) benchmark [18].",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 24,
      "context" : "Since the gender of each subject is not provided, we follow the method of [25] and use a service that determines a person’s gender by their name (if it is recognized), along with a confidence level.",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 23,
      "context" : "We use a network architecture suggested by [24], using an available tensorflow implementation1.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 8,
      "context" : "Since the amount of male and female subjects in the dataset is not balanced, we use an objective of maximizing the balanced accuracy [9] - the average accuracy obtained on either class.",
      "startOffset" : 133,
      "endOffset" : 136
    }, {
      "referenceID" : 32,
      "context" : "We inspect the balanced accuracy on our test data during the training process, comparing our method to a vanilla neural network training, as well as to soft and hard bootstrapping described in [33] and to the s-model described in [15], all of which are using the same network architecture.",
      "startOffset" : 193,
      "endOffset" : 197
    }, {
      "referenceID" : 14,
      "context" : "We inspect the balanced accuracy on our test data during the training process, comparing our method to a vanilla neural network training, as well as to soft and hard bootstrapping described in [33] and to the s-model described in [15], all of which are using the same network architecture.",
      "startOffset" : 230,
      "endOffset" : 234
    }, {
      "referenceID" : 14,
      "context" : "We use the initialization parameters for [15, 33] that were suggested in the original papers.",
      "startOffset" : 41,
      "endOffset" : 49
    }, {
      "referenceID" : 32,
      "context" : "We use the initialization parameters for [15, 33] that were suggested in the original papers.",
      "startOffset" : 41,
      "endOffset" : 49
    }, {
      "referenceID" : 14,
      "context" : "The second best algorithm is the s-model described in [15].",
      "startOffset" : 54,
      "endOffset" : 58
    } ],
    "year" : 2017,
    "abstractText" : "Deep learning requires data. A useful approach to obtain data is to be creative and mine data from various sources, that were created for different purposes. Unfortunately, this approach often leads to noisy labels. In this paper, we propose a meta algorithm for tackling the noisy labels problem. The key idea is to decouple “when to update” from “how to update”. We demonstrate the effectiveness of our algorithm by mining data for gender classification by combining the Labeled Faces in the Wild (LFW) face recognition dataset with a textual genderizing service, which leads to a noisy dataset. While our approach is very simple to implement, it leads to state-of-the-art results. We analyze some convergence properties of the proposed algorithm.",
    "creator" : "LaTeX with hyperref package"
  }
}