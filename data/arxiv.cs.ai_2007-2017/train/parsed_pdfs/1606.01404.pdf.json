{
  "name" : "1606.01404.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Generating Natural Language Inference Chains",
    "authors" : [ "Vladyslav Kolesnyk" ],
    "emails" : [ "vladyslav.kolesnyk.12@ucl.ac.uk,", "t.rocktaschel@cs.ucl.ac.uk", "s.riedel@cs.ucl.ac.uk" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The ability to determine entailment or contradiction between natural language text is essential\nfor improving the performance in a wide range of natural language processing tasks. Recognizing Textual Entailment (RTE) is a task primarily designed to determine whether two natural language sentences are independent, contradictory or in an entailment relationship where the second sentence (the hypothesis) can be inferred from the first (the premise). Although systems that perform well in RTE could potentially be used to improve question answering, information extraction, text summarization and machine translation [6], only in few of such downstream NLP tasks sentence-pairs are actually available. Usually, only a single source sentence (e.g. a question that needs to be answered or a source sentence that we want to translate) is present and models need to come up with their own hypotheses and commonsense knowledge inferences.\nThe release of the large Stanford Natural Language Inference (SNLI) corpus [2] allowed end-to-end differentiable neural networks to outperform feature-based classifiers on the RTE task [3, 4, 12, 15, 17].\nIn this work, we go a step further and investigate how well recurrent neural networks can produce true hypotheses given a source sentence. Furthermore, we qualitatively demonstrate that by only training on input-output pairs and recursively generating entailed sentence we can generate natural language inference chains (see Figure 1 for an example). Note that every inference step is interpretable as it is mapping one natural language sentence to another one.\nOur contributions are fourfold: (i) we propose\nar X\niv :1\n60 6.\n01 40\n4v 1\n[ cs\n.C L\n] 4\nJ un\n2 01\nan entailment generation task based on the SNLI corpus (§2.1), (ii) we investigate a sequence-tosequence model and find that 82% of generated sentences are correct (§3.2), (iii) we demonstrate the ability to generate natural language inference chains trained solely from entailment pairs (§3.3), and finally (iv) we can also generate sentences with more specific information by swapping source and target sentences during training (§3.4)."
    }, {
      "heading" : "2 Method",
      "text" : "In the section, we briefly introduce the entailment generation task and our sequence-to-sequence model."
    }, {
      "heading" : "2.1 Entailment Generation",
      "text" : "To create the entailment generation dataset, we simply filter the Stanford Natural Language Inference corpus for sentence-pairs of the entailment class. This results in a training set of 183, 416 sentence pairs, a development set of 3, 329 pairs and a test of 3, 368 pairs. Instead of a classification task, we can now use this dataset for a sequence transduction task."
    }, {
      "heading" : "2.2 Sequence-to-Sequence",
      "text" : "Sequence-to-sequence recurrent neural networks [14] have been successfully employed for many sequence transduction tasks in NLP such as machine translation [1, 5], constituency parsing [16], sentence summarization [13] and question answering [7]. They consist of two recurrent neural networks (RNNs): an encoder that maps an input sequence of words into a dense vector representation, and a decoder that conditioned on that vector representation generates an output sequence. Specifically, we use long short-term memory (LSTM) RNNs [8] for encoding and decoding. Furthermore, we experiment with wordby-word attention [1], which allows the decoder to search in the encoder outputs to circumvent\nthe LSTM’s memory bottleneck. We use greedy decoding at test time. The success of LSTMs with attention in sequence transduction tasks makes them a natural choice as a baseline for entailment generation, and we leave the investigation of more advanced models to future work."
    }, {
      "heading" : "2.3 Optimization and Hyperparameters",
      "text" : "We use stochastic gradient descent with a minibatch size of 64 and the ADAM optimizer [9] with a first momentum coefficient of 0.9 and a second momentum coefficient of 0.999. Word embeddings are initialized with pre-trained word2vec vectors [10]. Out-of-vocabulary words (10.5%) are randomly initialized by sampling values uniformly from [− √ 3, √\n3] and optimized during training. Furthermore, we clip gradients using a norm of 5.0. We stop training after 25 epochs."
    }, {
      "heading" : "3 Experiments and Results",
      "text" : "We present results for various tasks: (i) given a premise, generate a sentence that can be inferred from the premise, (ii) construct inference chains by recursively generating sentences, and (iii) given a sentence, create a premise that would entail this sentence, i.e., make a more descriptive sentence by adding specific information."
    }, {
      "heading" : "3.1 Quantitative Evaluation",
      "text" : "We train an LSTM with and without attention on the training set. After training, we take the best model in terms of BLEU score [11] on the development set and calculate the BLEU score on the test set. To our surprise, we found that using attention yields only a marginally higher BLEU score (43.1 vs. 42.8). We suspect that this is due to the fact that generating entailed sentences has a larger space of valid target sequences, which makes the use of BLEU problematic and penalizes correct solutions. Hence, we manually annotated 100 random test sentences and decided whether the generated sentence can indeed be inferred from the source sentence. We found that sentences generated by an LSTM with attention are substantially more accurate (82% accuracy) than those generated from an LSTM baseline (71.7%). To gain more insights into the model’s capabilities, we turn to a thorough qualitative\nanalysis of the attention LSTM model in the remainder of this paper."
    }, {
      "heading" : "3.2 Example Generations",
      "text" : "Figure 2 shows examples of generated sentences from the development set. Syntactic simplification of the input sentence seems to be the most common approach. The model removes certain parts of the premise such as adjectives, resulting in a more abstract sentence (see Figure 2.1).\nFigure 2.2 demonstrates that the system can recognize the number of subjects in the sentence and includes this information in the generated sentence. However, we did not observe such ’counting’ behavior for more than four subjects, indicating that the system memorized frequency patterns from the training set.\nFurthermore, we found predictions that hint to common-sense assumptions: if a sentence talks about a father holding a newborn baby, it is most likely that the newborn baby is his own child (Example 2.3).\nLimitations Two reappearing limitations of the proposed model are related to dealing with words that have a very different meaning but similar word2vec embeddings (e.g. colors), as well as ambiguous words. For instance, ’bar’ in\nFigure 3.1 refers to pole vault and not a place\nin which you can have a drink. Substituting one color by another one (Figure 3.2) is a common mistake.\nOut-of-Corpus Examples The SNLI corpus might not reflect the variety of sentences that can be encountered in downstream NLP tasks. In Figure 4 we present generated sentences for randomly selected examples of out-of-domain textual resources. They demonstrate that the model generalizes well to out-of-domain sentences, making it a potentially very useful component for improving systems for question answering, information extraction, sentence summarization etc."
    }, {
      "heading" : "3.3 Inference Chain Generation",
      "text" : "Next, we test how well the model can generate inference chains by repeatedly passing generated output sentences as inputs to the model. We stop once a sentence has already been generated in the chain. Figure 5 shows that this works well despite that the model was only trained on sentence-pairs.\nFurthermore, by generating inference chains for all sentences in the development set we construct an entailment graph. In that graph we found that sentences with shared semantics are eventually mapped to the same sentence that captures the shared meaning.\nA visualization of the topology of the entailment graph is shown in Figure 6. Note that there are several long inference chains, as well as large clusters of sentences (nodes) that are mapped (links) to the same shared meaning."
    }, {
      "heading" : "3.4 Inverse Inference",
      "text" : "By swapping the source and target sequences for training, we can train a model that given a sentence invents additional information to generate a new sentence (Figure 7). We believe this might prove useful to increase the language variety and\ncomplexity of AI unit tests such as the Facebook bAbI task [18], but we leave this for future work."
    }, {
      "heading" : "4 Conclusion and Future Work",
      "text" : "We investigated the ability of sequence-tosequence models to generate entailed sentences from a source sentence. To this end, we trained an attentive LSTM on entailment-pairs of the SNLI corpus. We found that this works well and generalizes beyond in-domain sentences. Hence, it could become a useful component for improving the performance of other NLP systems.\nWe were able to generate natural language inference chains by recursively generating sentences from previously inferred ones. This allowed us to construct an entailment graph for sentences of the SNLI development corpus. In this graph, the shared meaning of two related sentences is represented by the first natural language sentence that connects both sentences. Every inference step is interpretable as it maps a natural language sentence to another one.\nTowards high-quality data augmentation, we experimented with reversing the generation task. We found that this enabled the model to learn to invent specific information.\nFor future work, we want to integrate the presented model into larger architectures to improve the performance of downstream NLP tasks such as information extraction and question answering. Furthermore, we plan to use the model for data augmentation to train expressive neural networks on tasks where only little annotated data is available. Another interesting research direction is to investigate methods for increasing the diversity of the generated sentences."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank Guillaume Bouchard for suggesting the reversed generation task, and Dirk Weissenborn, Isabelle Augenstein and Matko Bosnjak for comments on drafts of this paper. This work was supported by Microsoft Research through its PhD Scholarship Programme, an Allen Distinguished Investigator Award, and a Marie Curie Career Integration Award."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "D. Bahdanau", "K. Cho", "Y. Bengio" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2015
    }, {
      "title" : "A large annotated corpus for learning natural language inference",
      "author" : [ "S.R. Bowman", "G. Angeli", "C. Potts", "C.D. Manning" ],
      "venue" : "In EMNLP,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2015
    }, {
      "title" : "A fast unified model for parsing and sentence understanding",
      "author" : [ "S.R. Bowman", "J. Gauthier", "A. Rastogi", "R. Gupta", "C.D. Manning", "C. Potts" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2016
    }, {
      "title" : "Long short-term memory-networks for machine reading",
      "author" : [ "J. Cheng", "L. Dong", "M. Lapata" ],
      "venue" : "arXiv preprint arXiv:1601.06733,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2016
    }, {
      "title" : "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "author" : [ "K. Cho", "B. Van Merriënboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio" ],
      "venue" : "arXiv preprint arXiv:1406.1078,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2014
    }, {
      "title" : "The pascal recognising textual entailment challenge. In Machine learning challenges. evaluating predictive uncertainty, visual object classification, and recognising tectual entailment",
      "author" : [ "I. Dagan", "O. Glickman", "B. Magnini" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2006
    }, {
      "title" : "Teaching machines to read and comprehend",
      "author" : [ "K.M. Hermann", "T. Kocisky", "E. Grefenstette", "L. Espeholt", "W. Kay", "M. Suleyman", "P. Blunsom" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2015
    }, {
      "title" : "Long  short-term memory",
      "author" : [ "S. Hochreiter", "J. Schmidhuber" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1997
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "D. Kingma", "J. Ba" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2015
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2013
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu" ],
      "venue" : "In ACL,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2002
    }, {
      "title" : "Reasoning about entailment with neural attention",
      "author" : [ "T. Rocktäschel", "E. Grefenstette", "K.M. Hermann", "T. Kočiskỳ", "P. Blunsom" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2016
    }, {
      "title" : "A neural attention model for abstractive sentence summarization",
      "author" : [ "A.M. Rush", "S. Chopra", "J. Weston" ],
      "venue" : "In EMNLP,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2015
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "I. Sutskever", "O. Vinyals", "Q.V. Le" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2014
    }, {
      "title" : "Order-embeddings of images and language",
      "author" : [ "I. Vendrov", "R. Kiros", "S. Fidler", "R. Urtasun" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2016
    }, {
      "title" : "Grammar as a foreign language",
      "author" : [ "O. Vinyals", "L. Kaiser", "T. Koo", "S. Petrov", "I. Sutskever", "G. Hinton" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2015
    }, {
      "title" : "Learning Natural Language Inference with LSTM",
      "author" : [ "S. Wang", "J. Jiang" ],
      "venue" : "In NAACL,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2016
    }, {
      "title" : "Towards ai-complete question answering: A set of prerequisite toy tasks",
      "author" : [ "J. Weston", "A. Bordes", "S. Chopra", "T. Mikolov" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "Although systems that perform well in RTE could potentially be used to improve question answering, information extraction, text summarization and machine translation [6], only in few of such downstream NLP tasks sentence-pairs are actually available.",
      "startOffset" : 166,
      "endOffset" : 169
    }, {
      "referenceID" : 1,
      "context" : "The release of the large Stanford Natural Language Inference (SNLI) corpus [2] allowed end-to-end differentiable neural networks to outperform feature-based classifiers on the RTE task [3, 4, 12, 15, 17].",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 2,
      "context" : "The release of the large Stanford Natural Language Inference (SNLI) corpus [2] allowed end-to-end differentiable neural networks to outperform feature-based classifiers on the RTE task [3, 4, 12, 15, 17].",
      "startOffset" : 185,
      "endOffset" : 203
    }, {
      "referenceID" : 3,
      "context" : "The release of the large Stanford Natural Language Inference (SNLI) corpus [2] allowed end-to-end differentiable neural networks to outperform feature-based classifiers on the RTE task [3, 4, 12, 15, 17].",
      "startOffset" : 185,
      "endOffset" : 203
    }, {
      "referenceID" : 11,
      "context" : "The release of the large Stanford Natural Language Inference (SNLI) corpus [2] allowed end-to-end differentiable neural networks to outperform feature-based classifiers on the RTE task [3, 4, 12, 15, 17].",
      "startOffset" : 185,
      "endOffset" : 203
    }, {
      "referenceID" : 14,
      "context" : "The release of the large Stanford Natural Language Inference (SNLI) corpus [2] allowed end-to-end differentiable neural networks to outperform feature-based classifiers on the RTE task [3, 4, 12, 15, 17].",
      "startOffset" : 185,
      "endOffset" : 203
    }, {
      "referenceID" : 16,
      "context" : "The release of the large Stanford Natural Language Inference (SNLI) corpus [2] allowed end-to-end differentiable neural networks to outperform feature-based classifiers on the RTE task [3, 4, 12, 15, 17].",
      "startOffset" : 185,
      "endOffset" : 203
    }, {
      "referenceID" : 13,
      "context" : "Sequence-to-sequence recurrent neural networks [14] have been successfully employed for many sequence transduction tasks in NLP such as machine translation [1, 5], constituency parsing [16], sentence summarization [13] and question answering [7].",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 0,
      "context" : "Sequence-to-sequence recurrent neural networks [14] have been successfully employed for many sequence transduction tasks in NLP such as machine translation [1, 5], constituency parsing [16], sentence summarization [13] and question answering [7].",
      "startOffset" : 156,
      "endOffset" : 162
    }, {
      "referenceID" : 4,
      "context" : "Sequence-to-sequence recurrent neural networks [14] have been successfully employed for many sequence transduction tasks in NLP such as machine translation [1, 5], constituency parsing [16], sentence summarization [13] and question answering [7].",
      "startOffset" : 156,
      "endOffset" : 162
    }, {
      "referenceID" : 15,
      "context" : "Sequence-to-sequence recurrent neural networks [14] have been successfully employed for many sequence transduction tasks in NLP such as machine translation [1, 5], constituency parsing [16], sentence summarization [13] and question answering [7].",
      "startOffset" : 185,
      "endOffset" : 189
    }, {
      "referenceID" : 12,
      "context" : "Sequence-to-sequence recurrent neural networks [14] have been successfully employed for many sequence transduction tasks in NLP such as machine translation [1, 5], constituency parsing [16], sentence summarization [13] and question answering [7].",
      "startOffset" : 214,
      "endOffset" : 218
    }, {
      "referenceID" : 6,
      "context" : "Sequence-to-sequence recurrent neural networks [14] have been successfully employed for many sequence transduction tasks in NLP such as machine translation [1, 5], constituency parsing [16], sentence summarization [13] and question answering [7].",
      "startOffset" : 242,
      "endOffset" : 245
    }, {
      "referenceID" : 7,
      "context" : "Specifically, we use long short-term memory (LSTM) RNNs [8] for encoding and decoding.",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 0,
      "context" : "Furthermore, we experiment with wordby-word attention [1], which allows the decoder to search in the encoder outputs to circumvent the LSTM’s memory bottleneck.",
      "startOffset" : 54,
      "endOffset" : 57
    }, {
      "referenceID" : 8,
      "context" : "We use stochastic gradient descent with a minibatch size of 64 and the ADAM optimizer [9] with a first momentum coefficient of 0.",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 9,
      "context" : "Word embeddings are initialized with pre-trained word2vec vectors [10].",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 10,
      "context" : "After training, we take the best model in terms of BLEU score [11] on the development set and calculate the BLEU score on the test set.",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 17,
      "context" : "complexity of AI unit tests such as the Facebook bAbI task [18], but we leave this for future work.",
      "startOffset" : 59,
      "endOffset" : 63
    } ],
    "year" : 2016,
    "abstractText" : "The ability to reason with natural language is a fundamental prerequisite for many NLP tasks such as information extraction, machine translation and question answering. To quantify this ability, systems are commonly tested whether they can recognize textual entailment, i.e., whether one sentence can be inferred from another one. However, in most NLP applications only single source sentences instead of sentence pairs are available. Hence, we propose a new task that measures how well a model can generate an entailed sentence from a source sentence. We take entailment-pairs of the Stanford Natural Language Inference corpus and train an LSTM with attention. On a manually annotated test set we found that 82% of generated sentences are correct, an improvement of 10.3% over an LSTM baseline. A qualitative analysis shows that this model is not only capable of shortening input sentences, but also inferring new statements via paraphrasing and phrase entailment. We then apply this model recursively to input-output pairs, thereby generating natural language inference chains that can be used to automatically construct an entailment graph from source sentences. Finally, by swapping source and target sentences we can also train a model that given an input sentence invents additional information to generate a new sentence.",
    "creator" : "LaTeX with hyperref package"
  }
}