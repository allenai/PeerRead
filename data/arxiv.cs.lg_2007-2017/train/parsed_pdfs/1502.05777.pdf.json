{
  "name" : "1502.05777.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Spike Event Based Learning in Neural Networks",
    "authors" : [ "J. A. Henderson", "T. A. Gibson", "J. Wiles" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "A scheme is derived for learning connectivity in spiking neural networks. The scheme learns instantaneous firing rates that are conditional on the activity in other parts of the network. The scheme is independent of the choice of neuron dynamics or activation function, and network architecture. It involves two simple, online, local learning rules that are applied only in response to occurrences of spike events. This scheme provides a direct method for transferring ideas between the fields of deep learning and computational neuroscience. This learning scheme is demonstrated using a layered feedforward spiking neural network trained self-supervised on a prediction and classification task for moving MNIST images collected using a Dynamic Vision Sensor.\nKeywords: Spiking Neural Networks, Learning, Vision, Prediction"
    }, {
      "heading" : "1. Introduction",
      "text" : "Methods in deep learning for training neural networks (NNs) have been very successfully applied to a range of datasets, performing tasks at levels approaching human performance, such as image classification [1], object detection [1] and speech recognition [2, 3, 4]. Along with these experimental successes, the field of deep learning is rapidly developing theoretical frameworks in representation learning [5, 6, 7] including understanding the benefits of different types of non-linearities in neuron activation functions [8], disentanglement of inputs by projecting onto hidden layer manifolds, model averaging with techniques like maxout and dropout [9, 10] and assisting generalization through corruption of input with denoising autoencoders [11].\nThese types of experimental and theoretical work are necessary to effectively build and understand systems like brains that are capable of learning to solve real world problems. Many of the successes of deep learning are a\nPreprint submitted to arχiv February 23, 2015\nar X\niv :1\n50 2.\n05 77\n7v 1\n[ cs\n.N E\n] 2\n0 Fe\nb 20\nresult of a broad inspiration from biology; however, there is a large gap in understanding how the principles of deep learning are related to those of the brain. Some elements of deep learning may well inspire discoveries in brain function. Equally, deep learning systems are still inferior to the brain in aspects such as memory, thus efforts to develop models that bridge between deep learning and neuroscience are likely to be mutually beneficial.\nThe neuron models commonly used in deep learning are abstracted away from neuron models that are used in computational neuroscience to model biological neurons. Spiking is a salient feature of biological neurons that is not typically present in deep learning networks. It is not yet understood why the brain uses spiking dynamics; for the purposes of machine learning it would be useful to know what if any advantages spiking dynamics confers spike based NN learning algorithms over other types of NN learning algorithms, rather than advantages that are otherwise useful in implementing algorithms in biology such as energy efficiency and robustness. Dynamical systems like spiking networks appear more naturally suited to processing continuous time temporal data than state machines, as deep networks are usually implemented, but this idea is yet to be demonstrated experimentally on machine learning tasks.\nIn an effort to bridge this gap in understanding between spike, and nonspike based NN learning systems, and develop systems for processing event based, continuous time data, this paper develops a scheme for learning connectivity in a spiking neural network (SNN). The scheme is based upon learning conditional instantaneous firing rates, linking it to many of the statistical frameworks previously developed in deep learning that are based on conditional probabilities. However, our scheme is fundamentally different to most methods used in deep learning as the learning rules are based solely on the activity of the neurons in the network and are the same, independent of the choice of neuron dynamics or activation function unlike gradient descent methods [12], and they can be implemented online and do not require periods of statistical sampling from the model unlike energy based methods [13]. In addition, the learning scheme is local, meaning that modifying a connection only requires knowledge of the activity of the neurons it connects, not neurons from a distant part of the network, unlike gradient descent and energy based methods [12, 13]. From a perspective of biological plausibility, this means neurons do not have to make assumptions about, or communicate to each other their dynamics or activation function and associated parameters in order to correctly learn, and the system can be run online without\ninterrupting processing with periods of sampling for learning. This paper describes a general scheme for event based learning in SNNs. This scheme is demonstrated on a network similar to that commonly used in deep learning, specifically, a feedforward layered network architecture with rectified linear units, and piecewise constant temporal connectivity. Dropout is utilized to show that many ideas from deep learning can be directly imported into SNNs using this learning scheme. An event based dataset of moving MNIST digits collected using an DVS camera [14, 15, 16] is used to train the network for both prediction and classification tasks."
    }, {
      "heading" : "2. Learning Theory",
      "text" : "We begin by developing a method for learning the connectivity of a supervised output neuron. The discussion will be framed in reference to learning in a network operating continuously in time with temporally delayed connectivity and temporally encoded input signals since spiking neurons are usually modelled as dynamical systems; however, the results are also applicable to networks operating in discrete timesteps (as is usual in implementations of SNNs with current standard computer architecture), with or without temporally delayed connectivity and temporally encoded inputs, so they can also be applied to traditional artificial neural networks performing static image classification, for example.\nFigure 1 shows a general network containing input neurons whose activity is determined by an external source, hidden neurons, and supervised output neurons. At present we do not assume any particular connection architecture, nor do we specify the dynamics or activation functions of the individual neurons.\nWe first consider learning the input, Qo(t), that a supervised output neuron o receives from the network at time t. We need to identify the mathematical quantity that o should learn - the quantity that Qo(t) should approximate. Note that Qo(t) may be calculated from two sets of quantities only. The first quantities are the weights W , that connect the activity of the network to o, potentially including self connections and connections that have a temporal delay. We call these connections weights as this is the terminology usually used in machine learning; however, these connections can also be imagined as propagator functions whose value varies with temporal delay. The second quantity is the history of activity of the neurons in the network, H, which since we are primarily interested in SNNs and event based learning,\nwe model as a set of one dimensional Dirac delta functions in time that may be normalized to non unit integral to allow a neuron’s spike strength to be a real value, instead of only binary as in many SNN models. However, since it allows the use of more intuitive terminology of spike rates, instead of spike strength rates, we assume that a spike with real valued strength is equivalent to a sum of simultaneous unit strength spikes and possibly one partial unit spike. Alternatively, this equivalence holds if spike strengths are restricted to a unit value and simultaneous spikes are not allowed. In any case the mathematical description and quantitative results are unchanged aside from a possible conversion function if simultaneous spikes are not considered to combine additively into a single real valued spike and vice-versa.\nAssuming W is fixed after learning, the only time varying quantity that can be used in calculating Qo(t) is H(t), so we re-parametrize Qo(t) to Qo(H). We then propose that a sensible output of the network to o is Q(o|H), the mean conditional instantaneous spike rate (during training) of supervised output neuron o, given activity H in the network. Integrating Q over a time period gives an expected number of spikes. Thus, in the case of a network operating in discretized time, as is common in the implementation of artificial SNNs in code, Q(o|H) can be interpreted as an expected number of spikes o given H, where the integral over a small discrete timestep is understood. That is, if we observe activity H in the network n times during training, and o spikes no times during timesteps coinciding with those n occurrences of H, then after training if we observe H again, the output that should have been learnt and produced by the network at that timestep is no/n. If only single unit strength spikes are allowed at each timestep, then this output can be interpreted as P (o|H), the probability of o spiking during the given timestep, given H. This interpretation is important in connecting the focus on probability distributions in machine learning with the focus on spike and rate coded networks in computational neuroscience [17].\nOf course if H includes the full history of the network’s activity from inception, then only one sample trajectory of H will be observed and used for learning. However, we assume that W approaches zero as the connection time delay becomes large, meaning that only some recent history of activity in the network is used in calculating Q(o|H). Thus, a variety of different H will be observed during training. In any practical network W will be finitely parametrized, so for any particular Hk the parameters modified in learning Q(o|Hk) will in effect learn for both a range of other H that are slight perturbations of Hk and also use the same parameters, as well as very\ndifferent H that only share a portion of the same parameters. If we assume the neurons in the network are spiking neurons, then there are only four events that occur within the network at which to apply learning rules that modify Qo(H). They are (i) when an input neuron spikes, (ii) when a hidden neuron spikes, (iii) when a supervised output neuron spikes due to supervision, and (iv) when a supervised output neuron spikes due to its own dynamics or activation function. Modification could also be made continuously at all times, at randomly generated time points, or according to a temporally periodic function; however, we proceed concentrating on the spiking events.\nLet D be a function that is applied to Qo(H) when H occurs (a combination of events (ii)), and let U be a function that is applied to Qo(H) for each supervised spike o (an event (i)) that co-occurs with H (see Fig. 2). After some period of training time t, we have a series of iterated applications of D and U applied to the initial value Q0o(H)\nQto(H) = (D ◦ U ◦ ... ◦ U) ◦ ... ◦ (D ◦ U ◦ ... ◦ U) ◦Q0o(H), (1)\nwhere the brackets group operations for each occurrence of H. To find a relation between U and D, suppose now that the initial value Q0o(H) = Q(o|H) as we desire. Clearly, we also require that Qto(H) ≈ Q(o|H), meaning that the application of the learning rules D and U do not cause the output to significantly deviate from the desired value. Note that we cannot require strict equality due to the stochastic nature of the event occurrences. If we choose\nU = DJ(Q t′ o (H)), (2)\nwhere superscripts indicate composition, not exponentiation and J is an unknown function still to be determined, then with the initial value Q0o(H) = Q(o|H), Eq. (1) becomes\nQto(H) = ( D ◦DJ(Qt ′′ o (H)) ◦ ... ◦DJ(Qt ′′ o (H)) ) ◦...◦ ( D ◦DJ(Qt ′ o (H)) ◦ ... ◦DJ(Qt ′ o (H)) ) Q(o|H).\n(3) Let N be the number of occurrences of H. For Qto(H) ≈ Q(o|H), we\nrequire that all the applications of D and U approximately cancel, i.e. N + ∑ J(Qt ′\no (H)) ≈ 0. (4)\nThe expected number of applications of U is NQ(o|H), and we require that Qto(H) ≈ Q(o|H) at all points in this sequence of applications of D and U , so we have\nJ(Qo(H))NQ(o|H) ≈ −N. (5)\nHowever, since we do not know Q(o|H) a priori we use the network’s current estimate Qo(H) instead and set\nJ(Qo(H)) = − 1\nQo(H) . (6)\nUsing Eq. (2) we now have the following relation between the function D that is applied when H occurs, and the function U that is applied when o spikes due to supervision\nU = D− 1 Qo . (7)\nThis requires that D has a unique inverse, and D−1 can be generalized in such a way as to be applied a fractional number of times.\nIn the above we required that Qto(H) ≈ Q(o|H) at all points in a sequence of applications of D and the U . This implies that any single application of either D or U when Qo(H) ≈ Q(o|H), can only change Qo(H) by a small (but not necessarily fixed) amount\nQo − U ≤ U(Qo) ≤ Qo + U , (8)\nQo − D ≤ D(Qo) ≤ Qo + D, (9)\nwhich using (7) leads to the relation\nD = Qo U . (10)\nThe required range of Qo is [0,∞). To ensure that U remains small as Qo → 0, we require D be chosen so that in the limQ→0, DQ remains finite. Alternatively it would be possible to insert noise spikes, for example Poisson noise with rate m into the supervision to fix a minimum target value of Qo to m, hence bounding Qo > 0 and eliminating the divergence in Eq. (10). After learning this noise can be stopped and subtracted from the learnt value of Q(o|H). In most cases the maximum value of Q will be finite, and hence D and U can be chosen to give sufficiently small changes.\nTo avoid Qo converging to an unwanted value, this learning scheme must have only a single globally stable fixed point Qo = Q(o|H). This means that\nU(Q) and D(Q) cannot both have fixed points at any Q. We therefore adjust Eqs. (8) and (9) to\nQo − U ≤ U(Qo) < Qo or Qo < U(Qo) ≤ Qo + U , (11)\nand\nQo < D(Qo) ≤ Qo + D or Qo − D ≤ D(Qo) < Qo. (12)\nWe choose between either the two left, or two right options in (11) and (12) by considering the stability of the fixed point Q(o|H) for each of these choices. Taking equalities in the above equations and using Eq. (10), the total change to Qo after N applications of D and an expected Q(o|H)N applications of U is\n∆ ≈ ±NQo U ∓Q(o|H)N U . (13)\nIf Qo > Q(o|H) we require ∆ < 0, and if Qo < Q(o|H) we require ∆ > 0. This implies the following choice for our learning rule restrictions\nQo − D ≤ D(Qo) < Qo, (14)\nQo < U(Qo) ≤ Qo + U , (15)\nthat is, D slightly decreases Qo and U slightly increases Qo."
    }, {
      "heading" : "3. Application to Learning Layers of Autoencoders",
      "text" : "We now outline a demonstration of this learning scheme. A standard method for training an unsupervised deep feedforward network is to train each pair of layers successively as autoencoders [6] so that each layer encodes the activity of the layer below it, see Fig. 3. The learning rules described in Sec. 2 can be used to learn layers of autoencoders by replacing the supervised output neuron o , with an input neuron i that self-supervises, and by reversing the direction of connectivity so that i learns to output Q(i|H), where H is now the future activity of the hidden neurons in the layer above i, since causality is reversed from the previous case; the input layer causes activity in the hidden layer above, see Fig. 2.\nUsing this method, the hidden layers learn so that by observing a period of hidden layer activity, the activity of the layer below at the beginning of\nthat observation can be inferred. The activity of the hidden layer and the connectivity between layers acts like, and encodes a short term memory.\nIn this demonstration we also include a layer of prediction neurons that predict the activity of the input layer at a specified time period in the future. These neurons are supervised by the activity of the input layer with the corresponding prediction time period delay, see Fig. 2. We also include a layer of digit classification neurons that are trained as for a supervised output neuron o, see Fig. 2.\nSo far we have not needed to specify the dynamics or activation function of the hidden neurons in the network in order to develop this learning scheme. Spiking neuron models in computational neuroscience are often dynamical systems modeled using differential equations [18]. In contrast, neurons in machine learning are typically characterized by an activation function of the neuron’s input [6]. Any of these types of neuron models could be employed here; however, we choose rectified linear units (ReLUs) that are commonly used in deep learning networks [6]. The form we use here is\nA(I) = I, I > 0,\n= 0, I ≤ 0, (16)\nsee Fig. 3c."
    }, {
      "heading" : "3.1. Weight Update Rules",
      "text" : "We have so far developed rules for learning a value Qo to approximate Q(o|H); however, we have not yet discussed rules for modifying W that are necessary for implementation in a network. Before these rules can be determined, the formula for calculating Qo from H and W needs to be chosen. The most common choice is to use the product of H and W summed across all neurons in the layer below and integrated across time in the case of time delay connections. We use this same choice here\nQo(t) = ∑ j ∫ t 0 hj(t ′)wj(t− t′)dt′, (17)\nwhere hj is a hidden neuron connected to o and wj is the corresponding connection between them. Other choices are possible and may have advantages over this choice, though this is left for future investigation. We also need to choose a parametrization for W . A wide variety of choices could\nbe made here such as sums of continuous functions, or convolution kernels acting across different sets of j, as is done in convolutional neural networks by modifying (17) to include a convolution across j as well as t. However as a first demonstration of this learning scheme we make a simpler choice of using a piecewise constant function (see Fig. 3b) that is easy to conceptualize and produces simple learning rules for the weight parameter updates\nwj(t) = K∑ k=1 ωk [S(t+ (k − 1)τ)− S(t+ kτ)] , (18)\nwhere S is the Heaviside step function, τ defines the width of each of the K pieces of wj, and the ωk are modified by learning. We simplify this notation to use wjk = ωk [S(t+ (k − 1)τ)− S(t+ kτ)] , (19) where wjk are effectively the time delayed weights in the network. For time delays greater than τK, the connectivity weight is zero, meaning that only activity histories H of length τK are used in calculating Qo.\nAssuming H is composed of spikes modeled as delta functions, Eq. (17) becomes a sum of weights multiplied by the numbers of spikes\nQo(t) = ∑ wjkhjt′ , (20)\nThe following simple and fast weight update rules satisfy Eqs. (14) and (15), though other choices are possible. A weight update rule d that implements D when H occurs is d(w) = w − hQ, (21) and a corresponding weight update rule u that implements U when supervision spikes o occur is\nu(w) = w + ho, (22)\nwhere is a hyperparameter of the learning rules and should be chosen to be appropriately small. These learning rules cause Qo to fluctuate within a small range of Q(o|H) and it may be useful to change with time to allow a initial period of fast convergence from the initialization point, and then a reduced fluctuation error once Qo ≈ Q(o|H). Again, these rules are not specific to the ReLUs that we demonstrate with, these neurons could be replaced with sigmoid units, for example, without changing these weight update rules.\nWe use the same weight update rules for learning to predict the activity of the input layer from the activity of the hidden layers, where during learning the prediction neurons are supervised by the future input, see Fig. 2."
    }, {
      "heading" : "3.2. DVS MNIST Event Based Dataset",
      "text" : "To demonstrate this learning scheme we use a dataset collected using a Dynamic Vision Sensor (DVS) [16]. The DVS is a type of video camera that collects event data, unlike conventional video cameras that collect frame data. In the camera an event is triggered by the light intensity impinging upon a pixel changing above a threshold amount. Upon such an event, the camera outputs the pixel coordinates, a timestamp (in µs) and the polarity of the change in intensity.\nThe MNIST database [15] has been used extensively in the development of deep learning [6]. With the view of linking this work to previous work in deep learning, we demonstrate this learning scheme using a DVS version of the MNIST database [14, 15] in which the handwritten digits are displayed and moved on an LCD screen that is being recorded by a DVS camera. In this dataset the light intensity changes collected by the DVS camera are primarily edges of the moving MNIST digits; however, in general the camera also captures other scene changes such as changes in illumination. The resulting dataset is noisy. Viewing the recorded data reveals that the edges are often blurred, and the number of events captured is not uniform across a digit’s edges. The dataset also appears to contain some events that are not related to the movement of the MNIST digit on the LCD screen; however, these events are relatively few in number. The dataset contains recordings of 1000 handwritten digits for each integer from 0 to 9. We use the first 900 entries for training and the last 100 entries for testing. The DVS’s 128× 128 array of pixels is cropped down to 23× 23 pixels with each of these pixels mapped onto two input neurons, one for each polarity of light intensity change. The input training sequence was formed from a random selection of the individual MNIST digit sequences each separated by 15 timesteps or 75 ms of no input. Each individual MNIST event sequence has a duration of about 77 timesteps or about 2.3 s.\nEach pair of neurons have five ωk parameters encoding weights for connection delays kτ of width 30 ms corresponding to the network’s execution timesteps of 30 ms. In this demonstration we predict 15 timesteps or 450ms into the future. An additional ten output neurons are used to classify the current input as a digit from zero to nine. All connection weights ω between layers were initialized to small random values to the range [0, ] where was initially set to 1× 10−5. The connection weights for the prediction and classification neurons were all initialized to zero and used an initial value of of 2.5×10−6, corresponding to the value for the between layer connections di-\nvided by the number of layers, since the prediction and classification neurons connect to all layers. The connections between hidden layers were trained one layer at a time for one pass through the training dataset, corresponding to 8.8× 105 timesteps. After each pass through the hidden layers was decreased by half for all connections and training was repeated, beginning at the first hidden layer. Note that the initial value, decay and decay period for are not heavily optimized. The prediction and classification weights from all hidden layers were trained at every timestep. To demonstrate that many ideas used in deep learning are directly transferable to a spiking neural network that learns using this scheme, during training we use 50% dropout [10] for each hidden layer.\nThe operation of the trained network is demonstrated in Fig. 4. The hidden layers are very active since in this demonstration the neurons have a threshold fixed at zero. Including a learnable threshold would produce a more sparse representation whilst also reducing the required cpu time as the network’s operation and learning are both dependent on the number of events that occur. The inference of the noisy input is significantly better than the prediction since the inference involves a memory of the input whereas the prediction does not. However, a smoothed version of the future input is usually identifiable in the prediction. The inference and predictions are often poor when the digit changes direction as the edges at these points are weak and the data are particularly noisy. The classification output correctly classifies the input digit 87.41% of the time. The classification error as a function of training time is shown in Fig. 5.\nFigure 6 shows receptive fields of neurons from the first hidden layer and predictive fields of neurons from all layers. Both positive (excitatory) and negative (inhibitory) weights are learnt. Initially all neurons are active and the small random weight vectors converge toward a time averaged input vector. Upon converging toward the time averaged input, the weight vectors are nearly identical; however, differences due to the small random initialization breaks their symmetry and the weights of different neurons begin to diverge toward other more specific features of the input. This process continues as these features themselves are further split into other even more specific features. After learning is stopped, some of the receptive fields are tuned toward responding to a small number of pixels, while others respond to a distributed pattern of pixels. Predictive fields tend to be composed of larger patches of the sensory field indicating that the encoding of the prediction is distributed across many neurons. Without dropout, denoising autoencoding\nor another regularization method, the connectivity between hidden layers forms an identity mapping, with each neuron connecting only to a single neuron in the previous layer."
    }, {
      "heading" : "4. Summary",
      "text" : "This paper introduces an event based learning scheme for neural networks. The scheme does not depend on the specific form of the neuronal dynamics or activation function, and while this paper focuses on training spiking neural networks, this scheme may also be used to train traditional artificial neural networks, especially those that involve discontinuous activation functions that defeat gradient descent methods. The scheme may also\nbe applied to networks of neurons containing biologically inspired dynamics. Future work in this direction may inform theories of dynamics and learning in the brain. The broad applicability of this learning scheme provides an avenue to directly apply ideas from both deep learning and computational neuroscience and thus strengthen and inform the theoretical progress in both fields."
    } ],
    "references" : [ {
      "title" : "L",
      "author" : [ "O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg" ],
      "venue" : "Fei-Fei, Imagenet large scale visual recognition challenge ",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition",
      "author" : [ "G. Dahl", "D. Yu", "L. Deng", "A. Acero" ],
      "venue" : "Audio, Speech, and Language Processing, IEEE Transactions on 20 (1) ",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "New types of deep neural network learning for speech recognition and related applications: an overview",
      "author" : [ "L. Deng", "G. Hinton", "B. Kingsbury" ],
      "venue" : "in: Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups",
      "author" : [ "G. Hinton", "L. Deng", "D. Yu", "G. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T. Sainath", "B. Kingsbury" ],
      "venue" : "Signal Processing Magazine, IEEE 29 (6) ",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Representation learning: A review and new perspectives",
      "author" : [ "Y. Bengio", "A. Courville", "P. Vincent" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on 35 (8) ",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Deep Learning",
      "author" : [ "Y. Bengio", "I. Goodfellow", "A. Courville" ],
      "venue" : "MIT Press ",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Deep learning in neural networks: An overview",
      "author" : [ "J. Schmidhuber" ],
      "venue" : "Neural Networks 61 (0) ",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Rectified linear units improve restricted boltzmann machines",
      "author" : [ "V. Nair", "G.E. Hinton" ],
      "venue" : "in: Proceedings of the 27th International Conference on Machine Learning (ICML-10)",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Maxout networks",
      "author" : [ "I.J. Goodfellow", "D. Warde-Farley", "M. Mirza", "A. Courville", "Y. Bengio" ],
      "venue" : "ICML 28 (3) ",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Dropout: A simple way to prevent neural networks from overfitting",
      "author" : [ "N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov" ],
      "venue" : "J. Mach. Learn. Res. 15 (1) ",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion",
      "author" : [ "P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.-A. Manzagol" ],
      "venue" : "J. Mach. Learn. Res. 11 ",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Learning representations by back-propagating errors",
      "author" : [ "D. Rumelhart", "G. Hinton", "R. Williams" ],
      "venue" : "Nature 323 (6088) ",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1986
    }, {
      "title" : "A fast learning algorithm for deep belief nets",
      "author" : [ "G.E. Hinton", "S. Osindero", "Y.-W. Teh" ],
      "venue" : "Neural Comput. 18 (7) ",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner" ],
      "venue" : "Proceedings of the IEEE 86 (11) ",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "B",
      "author" : [ "T. Serrano-Gotarredona" ],
      "venue" : "Linares-Barranco, A 128 × 128 1.5% contrast sensitivity 0.9% FPN 3 μs latency 4 mW asynchronous frame-free dynamic vision sensor using transimpedance preamplifiers, Solid-State Circuits, IEEE Journal of 48 (3) ",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Spiking activity propagation in neuronal networks: reconciling different perspectives on neural coding",
      "author" : [ "A. Kumar", "S. Rotter", "A. Aertsen" ],
      "venue" : "Nat. Rev. Neurosc. 11 (9) ",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Dynamical Systems in Neuroscience",
      "author" : [ "E. Izhikevich" ],
      "venue" : "MIT Press",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Methods in deep learning for training neural networks (NNs) have been very successfully applied to a range of datasets, performing tasks at levels approaching human performance, such as image classification [1], object detection [1] and speech recognition [2, 3, 4].",
      "startOffset" : 207,
      "endOffset" : 210
    }, {
      "referenceID" : 0,
      "context" : "Methods in deep learning for training neural networks (NNs) have been very successfully applied to a range of datasets, performing tasks at levels approaching human performance, such as image classification [1], object detection [1] and speech recognition [2, 3, 4].",
      "startOffset" : 229,
      "endOffset" : 232
    }, {
      "referenceID" : 1,
      "context" : "Methods in deep learning for training neural networks (NNs) have been very successfully applied to a range of datasets, performing tasks at levels approaching human performance, such as image classification [1], object detection [1] and speech recognition [2, 3, 4].",
      "startOffset" : 256,
      "endOffset" : 265
    }, {
      "referenceID" : 2,
      "context" : "Methods in deep learning for training neural networks (NNs) have been very successfully applied to a range of datasets, performing tasks at levels approaching human performance, such as image classification [1], object detection [1] and speech recognition [2, 3, 4].",
      "startOffset" : 256,
      "endOffset" : 265
    }, {
      "referenceID" : 3,
      "context" : "Methods in deep learning for training neural networks (NNs) have been very successfully applied to a range of datasets, performing tasks at levels approaching human performance, such as image classification [1], object detection [1] and speech recognition [2, 3, 4].",
      "startOffset" : 256,
      "endOffset" : 265
    }, {
      "referenceID" : 4,
      "context" : "Along with these experimental successes, the field of deep learning is rapidly developing theoretical frameworks in representation learning [5, 6, 7] including understanding the benefits of different types of non-linearities in neuron activation functions [8], disentanglement of inputs by projecting onto hidden layer manifolds, model averaging with techniques like maxout and dropout [9, 10] and assisting generalization through corruption of input with denoising autoencoders [11].",
      "startOffset" : 140,
      "endOffset" : 149
    }, {
      "referenceID" : 5,
      "context" : "Along with these experimental successes, the field of deep learning is rapidly developing theoretical frameworks in representation learning [5, 6, 7] including understanding the benefits of different types of non-linearities in neuron activation functions [8], disentanglement of inputs by projecting onto hidden layer manifolds, model averaging with techniques like maxout and dropout [9, 10] and assisting generalization through corruption of input with denoising autoencoders [11].",
      "startOffset" : 140,
      "endOffset" : 149
    }, {
      "referenceID" : 6,
      "context" : "Along with these experimental successes, the field of deep learning is rapidly developing theoretical frameworks in representation learning [5, 6, 7] including understanding the benefits of different types of non-linearities in neuron activation functions [8], disentanglement of inputs by projecting onto hidden layer manifolds, model averaging with techniques like maxout and dropout [9, 10] and assisting generalization through corruption of input with denoising autoencoders [11].",
      "startOffset" : 140,
      "endOffset" : 149
    }, {
      "referenceID" : 7,
      "context" : "Along with these experimental successes, the field of deep learning is rapidly developing theoretical frameworks in representation learning [5, 6, 7] including understanding the benefits of different types of non-linearities in neuron activation functions [8], disentanglement of inputs by projecting onto hidden layer manifolds, model averaging with techniques like maxout and dropout [9, 10] and assisting generalization through corruption of input with denoising autoencoders [11].",
      "startOffset" : 256,
      "endOffset" : 259
    }, {
      "referenceID" : 8,
      "context" : "Along with these experimental successes, the field of deep learning is rapidly developing theoretical frameworks in representation learning [5, 6, 7] including understanding the benefits of different types of non-linearities in neuron activation functions [8], disentanglement of inputs by projecting onto hidden layer manifolds, model averaging with techniques like maxout and dropout [9, 10] and assisting generalization through corruption of input with denoising autoencoders [11].",
      "startOffset" : 386,
      "endOffset" : 393
    }, {
      "referenceID" : 9,
      "context" : "Along with these experimental successes, the field of deep learning is rapidly developing theoretical frameworks in representation learning [5, 6, 7] including understanding the benefits of different types of non-linearities in neuron activation functions [8], disentanglement of inputs by projecting onto hidden layer manifolds, model averaging with techniques like maxout and dropout [9, 10] and assisting generalization through corruption of input with denoising autoencoders [11].",
      "startOffset" : 386,
      "endOffset" : 393
    }, {
      "referenceID" : 10,
      "context" : "Along with these experimental successes, the field of deep learning is rapidly developing theoretical frameworks in representation learning [5, 6, 7] including understanding the benefits of different types of non-linearities in neuron activation functions [8], disentanglement of inputs by projecting onto hidden layer manifolds, model averaging with techniques like maxout and dropout [9, 10] and assisting generalization through corruption of input with denoising autoencoders [11].",
      "startOffset" : 479,
      "endOffset" : 483
    }, {
      "referenceID" : 11,
      "context" : "However, our scheme is fundamentally different to most methods used in deep learning as the learning rules are based solely on the activity of the neurons in the network and are the same, independent of the choice of neuron dynamics or activation function unlike gradient descent methods [12], and they can be implemented online and do not require periods of statistical sampling from the model unlike energy based methods [13].",
      "startOffset" : 288,
      "endOffset" : 292
    }, {
      "referenceID" : 12,
      "context" : "However, our scheme is fundamentally different to most methods used in deep learning as the learning rules are based solely on the activity of the neurons in the network and are the same, independent of the choice of neuron dynamics or activation function unlike gradient descent methods [12], and they can be implemented online and do not require periods of statistical sampling from the model unlike energy based methods [13].",
      "startOffset" : 423,
      "endOffset" : 427
    }, {
      "referenceID" : 11,
      "context" : "In addition, the learning scheme is local, meaning that modifying a connection only requires knowledge of the activity of the neurons it connects, not neurons from a distant part of the network, unlike gradient descent and energy based methods [12, 13].",
      "startOffset" : 244,
      "endOffset" : 252
    }, {
      "referenceID" : 12,
      "context" : "In addition, the learning scheme is local, meaning that modifying a connection only requires knowledge of the activity of the neurons it connects, not neurons from a distant part of the network, unlike gradient descent and energy based methods [12, 13].",
      "startOffset" : 244,
      "endOffset" : 252
    }, {
      "referenceID" : 13,
      "context" : "An event based dataset of moving MNIST digits collected using an DVS camera [14, 15, 16] is used to train the network for both prediction and classification tasks.",
      "startOffset" : 76,
      "endOffset" : 88
    }, {
      "referenceID" : 14,
      "context" : "An event based dataset of moving MNIST digits collected using an DVS camera [14, 15, 16] is used to train the network for both prediction and classification tasks.",
      "startOffset" : 76,
      "endOffset" : 88
    }, {
      "referenceID" : 15,
      "context" : "This interpretation is important in connecting the focus on probability distributions in machine learning with the focus on spike and rate coded networks in computational neuroscience [17].",
      "startOffset" : 184,
      "endOffset" : 188
    }, {
      "referenceID" : 5,
      "context" : "A standard method for training an unsupervised deep feedforward network is to train each pair of layers successively as autoencoders [6] so that each layer encodes the activity of the layer below it, see Fig.",
      "startOffset" : 133,
      "endOffset" : 136
    }, {
      "referenceID" : 16,
      "context" : "Spiking neuron models in computational neuroscience are often dynamical systems modeled using differential equations [18].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 5,
      "context" : "In contrast, neurons in machine learning are typically characterized by an activation function of the neuron’s input [6].",
      "startOffset" : 117,
      "endOffset" : 120
    }, {
      "referenceID" : 5,
      "context" : "Any of these types of neuron models could be employed here; however, we choose rectified linear units (ReLUs) that are commonly used in deep learning networks [6].",
      "startOffset" : 159,
      "endOffset" : 162
    }, {
      "referenceID" : 14,
      "context" : "DVS MNIST Event Based Dataset To demonstrate this learning scheme we use a dataset collected using a Dynamic Vision Sensor (DVS) [16].",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 13,
      "context" : "The MNIST database [15] has been used extensively in the development of deep learning [6].",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 5,
      "context" : "The MNIST database [15] has been used extensively in the development of deep learning [6].",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 13,
      "context" : "With the view of linking this work to previous work in deep learning, we demonstrate this learning scheme using a DVS version of the MNIST database [14, 15] in which the handwritten digits are displayed and moved on an LCD screen that is being recorded by a DVS camera.",
      "startOffset" : 148,
      "endOffset" : 156
    }, {
      "referenceID" : 9,
      "context" : "To demonstrate that many ideas used in deep learning are directly transferable to a spiking neural network that learns using this scheme, during training we use 50% dropout [10] for each hidden layer.",
      "startOffset" : 173,
      "endOffset" : 177
    } ],
    "year" : 2015,
    "abstractText" : "A scheme is derived for learning connectivity in spiking neural networks. The scheme learns instantaneous firing rates that are conditional on the activity in other parts of the network. The scheme is independent of the choice of neuron dynamics or activation function, and network architecture. It involves two simple, online, local learning rules that are applied only in response to occurrences of spike events. This scheme provides a direct method for transferring ideas between the fields of deep learning and computational neuroscience. This learning scheme is demonstrated using a layered feedforward spiking neural network trained self-supervised on a prediction and classification task for moving MNIST images collected using a Dynamic Vision Sensor.",
    "creator" : "LaTeX with hyperref package"
  }
}