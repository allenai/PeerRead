{
  "name" : "1606.04216.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Spreadsheet Probabilistic Programming",
    "authors" : [ "Mike Wu", "Yura Perov", "Frank Wood", "Hongseok Yang" ],
    "emails" : [ "mike@invrea.com", "yura@invrea.com", "frank@invrea.com", "hongseok@invrea.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Spreadsheets are the de facto lingua franca of data analysis [Panko, 2008]. They are the principle what-if simulation and decision-making tool for millions of users [Scaffidi et al., 2005, Chan and Storey, 1996]. Spreadsheet users often translate internal beliefs and expert domain knowledge into simulations in the form of spreadsheet programs, without necessarily even realising they are programming. A common spreadsheet simulation is one in which assumptions are set apart, often on a separate worksheet, and a dependent forward-simulation is specified, for example a sequence of dividend payments given a simulation of the finances and decision making policy of a corporate entity. These simulations are used to make predictions for decision-making under uncertainty; for instance an investment decision based on the distribution of an internal rate of return calculation. The usual way stochasticity is injected into such simulations is by manually varying the values of assumptions to reflect uncertainty in held beliefs about their values. Model checking is implicit as all subcomputations may be plotted and “eye-balled” to assess their realism; unrealistic simulators are simply reprogrammed immediately. Conditioning is manual in the sense that constraining said models to reflect observed actuals relies upon the spreadsheet user manually editing the spreadsheet, replacing previously simulated cells with observed actual values. Our probabilistic programming approach to spreadsheet modeling introduces a novel approach to this latter procedure via the notion of observation, but remains compatible with existing usage paradigms.\nThe principle contribution of this paper is the idea that automatic Bayesian model inversion in spreadsheet computation is possible and derives from the connection between spreadsheets, programming languages, and, consequently, probabilistic programming. The design for introducing the notion of observation in the spreadsheet framework is novel, so too are the algorithms which enable our native implementations. Our abstract spreadsheet programming language also allows us to formalise connections between language expressivity and inference algorithm formal requirements in a way that further solidifies the footings of the machine learning probabilistic programming literature\nar X\niv :1\n60 6.\n04 21\n6v 1\n[ cs\n.A I]\n1 4\nJu n\n20 16\n[Pfeffer, 2001, Goodman et al., 2008, Pfeffer, 2009, Wingate et al., 2011, Wood et al., 2014, Paige and Wood, 2014, van de Meent et al., 2015], particularly that part which advocates variational inference [Wingate and Weber, 2013, Mansinghka et al., 2014, Kucukelbir et al., 2014], and particularly black-box variational inference [Ranganath et al., 2013], for probabilistic programming."
    }, {
      "heading" : "2 Abstract Spreadsheet Language",
      "text" : "We start by formalizing the syntax and semantics of our spreadsheet language and proving important properties of the language. These properties enable us to safely employ certain inference algorithms as discussed later in this paper.\nIntuitively, spreadsheets are finite maps from references of table cells to program expressions, which specify how to calculate the value of the current cell using those of other cells. Table 1 shows a grammar for these expressions e associated with cells. The grammar uses c for constant numbers (such as 1.0 and −2.4), r for references of cells, primOp for primitive operators (such as + and log), and blackOp for user-defined black-box operators. Typically these black-box operators are external custom functions, such as Excel VBA functions, and they may be stochastic and model unknown probability distributions. According to the grammar, an expression can be a constant c, the value of a cell r, or the result of applying deterministic or stochastic opertions opl(e1, ..., en). These applications are annotated with unique labels l, which we will use to name random variables associated with spreadsheets. An expression can also be the conditional statement if e1 e2 e3, which executes e2 or e3 depending on whether the evaluation of e1 gives a non-zero value or not. The last possibility is the actual statement actual(c, erpl(e1, . . . , en)), which states that a random variable with the distribution erpl(e1, . . . , en) is observed and has the value c. The erp here is also annotated with a unique label l. Note that these labels will typically not be part of any concrete instantiation of this abstract language, but, due to the properties that follow, can easily be added at compile time in a single pass over the spreadsheet. Also note that actual is novel to spreadsheet languages but closely corresponds to the notion of observation in the probabilistic programming literature Mansinghka et al. [2014], Wood et al. [2014].\nLet Expr be the set of all expressions this grammar can generate. Formally, a spreadsheet is a finite map f from references of cells to expressions in Expr . We write\nf : Ref → Expr\nto denote a spreadsheet f whose domain is Ref . Note that Ref is finite since f is a finite map. Ref consists of cells used in the spreadsheet and f describes expressions associated with these cells.\nWe say that a spreadsheet f : Ref → Expr is well-formed if the following directed graph G with vertex set V and edge set E does not have a cycle:1\nG = (V,E), V = Ref , E = {(r, r′) | r, r′ ∈ Ref and r occurs in expression f(r′)}.\nIntuitively, this acyclicity condition means the absence of a circular dependency among reference cells in a spreadsheet. In this paper, we consider only well-formed spreadsheets.\n1Formally, this acyclicity means that the transitive closure E+ of E does not relate any r ∈ V to itself:\nE1 = E, En+1 = {(r, r′) | (r, r′′) ∈ E and (r′′, r′) ∈ E for some r′′}, E+ = ⋃ n≥1 En.\nOne useful consequence of our well-formedness condition is that we can compute a total order of all cell references of a spreadsheet that respects the dependency relationship. This can be achieved by the well-known topological-sort algorithm, which enumerates vertices of a given finite directed acyclic graph (V,E) to a sequence [v1, v2, . . . , vn] such that for every edge (v, v′) ∈ E, the vertex v appears before v′ in the sequence. Lemma 2.1. For every spreadsheet f : Ref → Expr , there exists an enumeration [r1, . . . , rn] of all references in Ref such that for all r, r′ ∈ Ref , if r occurs in f(r′), it appears before r′ in the enumeration. This enumeration can be computed by topological sort.\nSimple yet important properties of well-formed spreadsheets are that they always terminate and that they use bounded numbers of random variables. These two properties enable us to show that such spreadsheets are probabilistic models with acyclic dependencies, and that we can safely perform inference over spreadsheet calculations using algorithms developed for such models. The properties hold because well-formedness bans circular dependency and expressions used in these spreadsheets do not have loop or recursion. In the rest of this section, we formally prove these properties. We use a fixed well-formed spreadsheet f : Ref → Expr , assume the enumeration [r1, . . . , rn] of Ref generated by the topological sort as described by the previous lemma, and write ri ≺ rj for ri, rj ∈ Ref when ri appears before rj in this enumeration. Define a state ρ to be a function from a subset of Ref , denoted dom(ρ), to numbers such that\n∀r, r′ ∈ Ref . (r ≺ r′ ∧ r′ ∈ dom(ρ)) =⇒ r ∈ dom(ρ). A state ρ represents a partially-evaluated spreadsheet, and specifies the values of evaluated cells. The condition for ρ just means that the evaluation occurs according to the total order ≺. The formal semantics of spreadsheets is defined in terms of two evaluation relations, one for entire spreadsheets and the other for expressions. Let p, q real numbers, Λ a finite map from labels to real numbers sequencies, L a sequence of labels, ρ, ρ′ spreadsheet states, and e, e′ expressions. These relations have the following forms\nρ p,q,Λ,L−−−−−→f ρ′ and e ⇓ρ c, (p, q,Λ, L).\nThe first relates two spreadsheet states ρ and ρ′, and describes that evaluating f one step from ρ results in ρ′. The tuple (p, q,Λ, L) is bookkeeping about this evaluation: during the evaluation, |L|-many values are sampled from applications with labels in L, the total log density of these samples according to their proposal distributions is q, the gradients of the densities of these proposals\nwith respect to their parameters form a map Λ, and the log density of the samples according to the target joint distributions is p. This single step evaluation computes the value of a cell r so that {r} = dom(ρ′) \\ dom(ρ). The second relation specifies similar information about expressions. It says that e evaluates to a number c possibly in multiple steps (rather than in one step), and that the tuple (p, q,Λ, L) records the very information that we have just described for ρ, but this time for this multi-step evaluation of the expression e. The rules for deriving these evaluation relations are given in Table 1. Each rule says that if the conditions above the bar hold, so does the statement below the bar. The rule for a reference r says that the expression r gets evaluated by the simple look up of the spreadsheet state ρ. The bookkeeping part, often denoted by a symbol w = (p, q,Λ, L), in this case is a tuple of two zeros, the empty finite function ∅, and the empty sequence []. According to its rule, the evaluation of primOpl(e1, ...., en) first executes all of its parameters e1, ..., en to get (c1, w1), ..., (cn, wn), and then combines these results. ci’s get combined by the primitive operator primOp, denoted c in the rule, and w1, . . . , wn by the ⊕ operator that is defined as follows: (p, q,Λ, L) ⊕ (p′, q′,Λ′, L′) = (p + p′, q + q′,Λ′′, L′′) where L′′ = concat(L′, L′′), the concatenation of L′ and L′′, and\nΛ′′(λ) = { Λ(λ) if (λ ∈ dom(Λ)), Λ′(λ) else if (λ ∈ dom(Λ′)), undefined otherwise.\nThe case of the black-box operator is similar except that the resulting number c is sampled according to the operator, the bookkeeping part records the use of this random variable by adding l to the end of concat(L1, ..., Ln), and its Λ component becomes ⊥, which represents the absence of information on gradient. This ⊥ is an annihilator. When it gets combined with another Λ′ (from both directions) in ⊕, the result is always ⊥. The rule for the erp application is the most complex, but follows the similar pattern. According to this rule, the evaluation of erpl(e1, ..., en) first runs its arguments and obtains (c1, w1), ...(cn, wn). Then, it looks up a proposal distribution Q at the label l, which has a parameters vector λ. The evaluation gets a sample c from Q, and computes the log densities p of the prior erp(c1, ..., cn) and q of the proposal Q(c1, ..., cn;λ), as well as the gradient g of Q(c1, ..., cn;λ) with respect to λ. These p, q, the singleton map from l to g, and the label l are all added to the bookkeeping of this evaluation. The meaning of the remaining rules for ⇓ρ follow suit. We have only one rule for→f . It says that the evaluation of f at ρ first picks the next unevaluated cell r, then executes the expression stored at r, and incorporates the result (c, (p, q,Λ, L)) of this execution by associating r with c in ρ, and recording (p, q,Λ, L) on top of→f . Theorem 2.2 (Termination). All well-formed spreadsheets terminate. Technically, this means that for every well-formed spreadsheet f , there is no infinite sequence\nρ1 p1,q1,Λ1,L1−−−−−−−→f ρ2 p2,q2,Λ2,L2−−−−−−−→f ρ3 . . . ρk pk,qk,Λk,Lk−−−−−−−−→f ρk+1 . . .\nand that for every state ρ and expression e, there is no infinite derivation tree with the conclusion (e ⇓ρ c, w) for some c, w.\nThis theorem holds because if ρ p,q,Λ,L−−−−−→f ρ′, then dom(ρ′) is strictly larger than dom(ρ), and in every rule for (e ⇓ρ c, w), all assumptions are about subexpressions of e not equal to e itself. Theorem 2.3 (Bounded Number of Random Variables). Let f : Ref → Expr be a well-formed spreadsheet, and let L = {l | l is a label used in f(r) for some r ∈ Ref }. Then, there are |L| or less random variables that cover all random variables used by the executions of f .\nTo see why this theorem holds, let ρ1 be the empty spreadsheet state ∅. Then, by the definitions of our evaluation relations, whenever we have\nρ1 p1,q1,Λ1,L1−−−−−−−→f ρ2 . . . ρi pi,qi,Λm,Li−−−−−−−→f ρi+1 . . . ρm pm,qm,Λm,Lm−−−−−−−−−→f ρm+1\nfor dom(ρm+1) = Ref , the concatenation of L1, . . . , Lm does not contain any label more than once. Furthermore, all of its labels are included in the set L in the theorem. The claim of the theorem follows from this observation.\nTable 2 establishes notation for an acyclic graphG = (V,E) generated by a well-formed spreadsheet f : Ref → Expr ; VO for the set of observed vertices, that is, references of cells containing actual statements, which are enumerated in sequence [vo1 , ..., voi ] according to the total order ≺ ,and four types of vertex sets: Vai and V∗i for certain predecessors of the observed vertex voi , Vr for vertices not affecting observed vertices during the evaluation of a spreadsheet, and V∗r for the immediate predecessors of these vertices.\nAlgorithm 1 Spreadsheet Sequential Monte Carlo Input: program f : Ref → Expr , joint distribution P , proposal distribution Q, number of particles S, graph\nG = (V,E), subgraphs VO , { Vai}, {V∗i}, Vr , V∗r . Variables: state ρ, particles weights {ws}Ss=1, temporary log likelihoods T , database of cells values {Ds : dom(f)→ im(ρ)}Ss=1, temporary database { Dtmps } for resampling.\n1: // Step 1 : Compute the first Actual. 2: for s = 1 to S do 3: Reset ρ = ∅. Set Ds = ∅. i = 1. Tp = 0;Tq = 0 4: for r ∈ Vai following the total order do 5: ρ\np,q,∅,L−−−−→f ρ′ evaluates r s.t. {r} = dom(ρ′) \\ dom(ρ) 6: Tp += p; Tq += q; ρ = ρ′ 7: end for 8: ρ′\np,q,∅,L−−−−→f ρ′′ evaluates voi . Tp += p; Tq += q; ws = exp(Tp − Tq) 9: for each r ∈ (Vai ∪ voi), Ds(r) = ρ′′(r)\n10: end for 11: // Step 2 : Resample and copy particles. 12: for s = 1 to S do 13: z ∼ categorical(norm({ws})). 14: for each r ∈ {Vai ∪ {voi}} , Dtmps (r) = Ds(r) and wtmps = ws 15: end for 16: D = Dtmp . {ws} = { wtmps } . Set all ws to 1S ∑S s=1 ws. 17: // Step 3 : Compute remaining Actuals. 18: for i = 2 to ‖VO‖ − 1 do 19: for s = 1 to S do 20: Reset ρ = ∅. for each r ∈ V∗i, ρ = ρ[r : Ds(r)] 21: Repeat lines 3–9. 22: end for 23: Repeat Step 2. 24: end for 25: // Step 4 : Propagate changes to other latent cells. 26: for s = 1 to S do 27: Reset ρ = ∅. for each r ∈ V∗r, ρ = ρ[r : Ds(r)] 28: for r ∈ Vr following the total order do 29: ρ\np,q,∅,L−−−−→f ρ′ evaluates r. ρ = ρ′ 30: end for 31: for each r ∈ Vr, Ds(r) = ρ′(r) 32: end for 33: // Step 5 : Outpute posterior distribution. 34: For some chosen r̄ ∈ V , output a histogram given {Ds(r̄)}Ss=1."
    }, {
      "heading" : "3 Spreadsheet Inference",
      "text" : "Having proven that a spreadsheet terminates and knowing that there exists a total order for the cells in a spreadsheet, we can safely employ algorithms based on sequential Monte Carlo (SMC)\nAlgorithm 2 Spreadsheet Black-Box Inference (follows Algorithm 1 from Ranganath et al. [2013]) Input: program f : Ref → Expr , joint distribution P , distribution Q, number of particles S, graph G =\n(V,E), convergence constant ε, bound on iterations tmax , number of samples per iteration S, learning rate parameter γ, number of stochastic operators in the program Lerp∗ . Variables: state ρ, free parameters λ(l) of the distribution Q for a particular random choice l, joint log probability of a particular sample Tp, joint log probability Tq for variational distributions Q, vector of gradients for Ql, number of applications for a random choice Tt(l), learning rate η, change ∆λ(l) in λ(l) for a random choice l, matrices G(l) for AdaGrad algorithm.\n1: for each label l ∈ Lerp∗ do 2: Change erpl to the respective distribution Ql with n parameters λ(l). 3: Initialize λ(l) = 0. G(l) = 0. 4: end for each 5: Set t = 0. 6: repeat 7: t = t+ 1 8: for s = 1 to S do 9: Tp = 0;Tq = 0. for each l ∈ Lerp∗ , TΛ(l) = 0 and Tt(l) = 0.\n10: Reset ρ = ∅. 11: for r ∈ V in the sorted total order do 12: ρ\np,q,Λ,L−−−−−→f ρ′ evaluates r s.t. {r} = dom(ρ′) \\ dom(ρ) 13: for each label l ∈ L do 14: TΛ(l) += Λ; Tt(l) += 1 15: end for each 16: Tp += p; Tq += q; ρ = ρ′ 17: end for 18: end for 19: λprev = λ 20: for each l ∈ Lerp∗ s.t. Tt(l) > 0 do 21: ∆λ(l) = 1\nTt(l) TΛ(l) · (Tp − Tq) ; G(l) += ∆λ(l)⊗∆λ(l) 22: η = γ diag( ∑t i=1 G(l)) − 1 2 ; λ(l) = λ(l) + η∆λ(l) 23: end for each 24: until ‖λ− λprev‖2 < ε or t > tmax 25: For some chosen l ∈ Lerp∗ , return q(λ(l)).\nfor posterior inference over execution paths of spreadsheet programs written in our spreadsheet language.\nAlgorithm 1 gives a detailed implementation of a version of SMC, the inner loop of the particle independent Metropolis Hastings (PIMH)-like algorithm Andrieu et al. [2010] we implemented in the Excel spreadsheet engine. Our SMC algorithm relies the Excel engine to provide ρ p,q,∅,L−−−−→f ρ′, namely, to compute the value c of a new cell r ∈ dom(ρ′) \\ dom(ρ) and log scores p, q of erp and proposal distributions respectively. The trick is to make it do so repeatedly for all particles for observations cells voj and their corresponding preceding cells Vaj preserving the total order. We resample particles after each evaluated observation. By nature of resampling, particles are not independent and semantically need to be evaluated “in parallel.” Our implementation is single-threaded and simulates parallelisation by switching between different states of ρ. For every spreadsheet state ρ obtained in this repeated evaluation and selected references r, the algorithm stores and reuses ρ(r), if r is in dom(ρ), in a database Ds(r) that is indexed by particle number and cell reference.\nUp to the first cell containing an observation expression references are evaluated according to the total order, likelihoods are incorporated into weights, and bindings are saved into the database (Alg. 1 lines 3–9). After the first observation, the weights are normalized, and the stored bindings D are resampled accordingly. For the rest of the observations vo2 , . . . , vo|VO| , the same procedure is repeated with the exception that directly preceding cells V∗j for the cells we need to evaluate Vaj must be restored to the state to ensure the evaluation of voj . This is done by rebinding the references based on values stored in D (Alg. 1 lines 19–23). After the observations, changes in each particle are propagated to Vr (Step 4). Lastly, the posterior distribution for a reference r̄ can be estimated with the final values stored in {Ds}Ss=1.\nWe do not run sequential Monte Carlo once, but instead we do M independent SMC runs with S1, . . . , SM particles. This improves particles diversity and helps with the problem of sample impoverishment. In order to join these independent SMC islands P̂1 = ∑S s=1 w\n1 sδD1s (D), P̂2 =∑S′\ns=1 w 2 sδD2s (D), . . . into an unbiased posterior approximation, we weight our isolated particle filters by their evidence estimates Ẑj = 1Sj ∑Sj s=1 w j s which are saved for each SMC run. That we can do this follows directly from the PIMH results in [Andrieu et al., 2010] where instead of doing MH on ratios of evidence estimates we simply do importance sampling with weights proportional to the evidence estimates.\nAdditionally, knowing that the spreadsheet graphical model has a finite number of random variables allows us to implement black-box variational inference (BBVI). For each random primitive l in the spreadsheet, BBVI associates a mean field approximation factor Ql. In our implementation we provide normal Gaussian(µ, σ), uniform continuous Between(a, b), categorical Choice((val1, . . . , valn), (p1, . . . , pn)) and one-parameter distribution Near(val > 0) ∼ Gaussian(val, 0.1 ∗ val) random variables. To approximate them, we use variational families Gaussian(λ1, expλ2), (b − a) Beta(expλ1, expλ2) + a, Choice((val1, . . . , valn), 1∑n\ni=1 expλi (expλ1, . . . , expλn)) and Gaussian(λ1, expλ2) corre-\nspondigly. Algorithm 2 describes black-box inference in further detail."
    }, {
      "heading" : "4 Experiments",
      "text" : "To demonstrate practicality of implementing both SMC and BBVI inference natively in a spreadsheet engine, we demonstrate correctness via a regression example and show that both can perform inference over spreadsheets that include user-defined functions. The Excel-syntax abstract spreadsheet language implementation allows users to use random primitives (=GAUSSIAN(·), =CHOICE(·), and =BETWEEN(·)) and “observe” cells via a syntax =ACTUAL(data, model, parameters) shown in each of the examples that follow. SMC and BBVI are both implemented in VB and are deployed as Excel Add-In’s, meaning that inference functionality can be added to existing spreadsheets. For the examples below, all SMC runs used 5000 particles in islands of 500 and all BBVI runs used 10 samples with 1000 iterations.\nTo illustrate correctness and our novel Excel syntax Figure 3 shows an Excel regression model for US GDP growth versus years from 1950 to 1983. Figures 2 and 3 show the highest probable values of a selection of cells after inference, and their formulas respectively. Notice that overlapping cells in columns B, C and D imply that the formulas in Figure 3 underly the cells with the same labels in Figure 2. This manner of displaying Excel formulae and values is used throughout the experiments section.\nThe estimated posterior distributions for the slope of the linear model for SMC and BBVI in comparison to the ground truth (GT). BBVI and the ground truth distributions are close to identical (µBBV I = 0.098, µGT = 0.099, σBBV I = 0.019, σGT = 0.018) and SMC offers a good approximation (yielding 0.098 for the posterior mean slope).\nAnother example, shown in Figure 4, illustrates the use of a blackOp primitive, here actually the IRR (internal rate of return) function in Excel, to perform the kind of analysis suggested in the introduction, namely to make an invest or not decision based on the IRR of a cash flow arising from dividend yields, and stock price movements. Because IRR hides an optimization it serves as the kind of blackOp primitive for which no knowledge is available to the spreadsheet about its\ninternal workings. From an end-user perspective, supporting inference over programs with such primitives is important since a significant portion of Excel functionality comes from custom userdefined functions and other similar blackOp functions. Although not shown, one inference objective that we could compute is to examine the distribution of the IRR in B12 given the effect of observing a dividend in B6 and a stock price in B7 under the specified model.\nWhile our experiments were performed using a prototype, which is quite slow, the approach can be integrated inside any spreadsheet engine since, as shown, it uses only built-in functionality of such engines. By doing such software engineering effort, inference will be much faster and users will be able to run models with many actuals."
    }, {
      "heading" : "5 Related Work",
      "text" : "Existing work on using variational Bayes Wingate and Weber [2013], Mansinghka et al. [2014] and specifically black box variational Bayes in probabilistic programming Kucukelbir et al. [2014] inspired this work. The formalism we introduce in this paper provides some theoretical justification for some of this prior art but also raises questions, particularly having to do with stochastic optimization in infinite dimensions.\nHere both BBVI and SMC rely upon repeated re-execution of the program guided by proposal distributions. Having proved termination of all programs written in our abstract spreadsheet language means that we can be assured that the computation performed in the inner loop of our inference algorithm will terminate every time, and, as a result, we can rely on our inference algorithm to terminate too. Prior probabilistic programming inference work reposed on SMC, notably [Wood et al., 2014, Paige and Wood, 2014, Mansinghka et al., 2014, van de Meent et al., 2015].\nThere are discernible differences between our approach and Tabular, a probabilistic programming language for Excel created by Microsoft Research [Gordon et al., 2014]. Tabular is similarly restricted to our abstract spreadsheet language in the sense that the random choices made in all possible execution paths are finitely enumerable. The most significant difference between our approach and that of Tabular is that the latter sits “on the side” of Excel with execution of its supported inference algorithms performed by a separate runtime, not the Excel engine itself. Furthermore, Tabular does not allow black-box user-programmed primitives owing to their incompatibility with, for instance, EP [Minka, 2001] inference (aka Infer.Net Minka et al.). We note, however, that progress towards support for black-box factors in EP is in motion [Heess et al., 2013, Jitkrittum et al.]."
    }, {
      "heading" : "6 Discussion",
      "text" : "We have demonstrated that Bayesian model inversion via both sequential Monte Carlo and black box variational inference are natively implementable in a spreadsheet engine and, moreover, safe in the sense of being on theoretically sound footing. Implementation in additional spreadsheet engines is ongoing work. This could bring about a transition from deterministic to probabilistic, conditioned spreadsheet computation which, in turn, could fundamentally impact the way spreadsheets are developed and used for data analysis and modeling in the future."
    } ],
    "references" : [ {
      "title" : "Particle Markov chain Monte Carlo methods",
      "author" : [ "Christophe Andrieu", "Arnaud Doucet", "Roman Holenstein" ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
      "citeRegEx" : "Andrieu et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Andrieu et al\\.",
      "year" : 2010
    }, {
      "title" : "The use of spreadsheets in organizations: Determinants and consequences",
      "author" : [ "Yolande E Chan", "Veda C Storey" ],
      "venue" : "Information & Management,",
      "citeRegEx" : "Chan and Storey.,? \\Q1996\\E",
      "shortCiteRegEx" : "Chan and Storey.",
      "year" : 1996
    }, {
      "title" : "Church: a language for generative models",
      "author" : [ "Noah D. Goodman", "Vikash K. Mansinghka", "Daniel M. Roy", "Keith Bonawitz", "Joshua B. Tenenbaum" ],
      "venue" : "In Proc. of Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Goodman et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Goodman et al\\.",
      "year" : 2008
    }, {
      "title" : "Tabular: a schema-driven probabilistic programming language",
      "author" : [ "Andrew D Gordon", "Thore Graepel", "Nicolas Rolland", "Claudio Russo", "Johannes Borgstrom", "John Guiver" ],
      "venue" : "In ACM SIGPLAN Notices,",
      "citeRegEx" : "Gordon et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Gordon et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning to pass expectation propagation messages",
      "author" : [ "Nicolas Heess", "Daniel Tarlow", "John Winn" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Heess et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Heess et al\\.",
      "year" : 2013
    }, {
      "title" : "Fully automatic variational inference of differentiable probability models",
      "author" : [ "Alp Kucukelbir", "Rajesh Ranganath", "Andrew Gelman", "David Blei" ],
      "venue" : "In NIPS Workshop on Probabilistic Programming,",
      "citeRegEx" : "Kucukelbir et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kucukelbir et al\\.",
      "year" : 2014
    }, {
      "title" : "Venture: a higher-order probabilistic programming platform with programmable inference. arXiv, page",
      "author" : [ "Vikash Mansinghka", "Daniel Selsam", "Yura Perov" ],
      "venue" : null,
      "citeRegEx" : "Mansinghka et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mansinghka et al\\.",
      "year" : 2014
    }, {
      "title" : "A family of algorithms for approximate Bayesian inference",
      "author" : [ "Thomas P Minka" ],
      "venue" : "PhD thesis, Massachusetts Institute of Technology,",
      "citeRegEx" : "Minka.,? \\Q2001\\E",
      "shortCiteRegEx" : "Minka.",
      "year" : 2001
    }, {
      "title" : "A compilation target for probabilistic programming languages",
      "author" : [ "Brooks Paige", "Frank Wood" ],
      "venue" : "In JMLR; ICML",
      "citeRegEx" : "Paige and Wood.,? \\Q2014\\E",
      "shortCiteRegEx" : "Paige and Wood.",
      "year" : 2014
    }, {
      "title" : "Spreadsheet errors: What we know. what we think we can do",
      "author" : [ "Raymond R Panko" ],
      "venue" : "arXiv preprint arXiv:0802.3457,",
      "citeRegEx" : "Panko.,? \\Q2008\\E",
      "shortCiteRegEx" : "Panko.",
      "year" : 2008
    }, {
      "title" : "IBAL: A probabilistic rational programming language",
      "author" : [ "Avi Pfeffer" ],
      "venue" : "In IJCAI,",
      "citeRegEx" : "Pfeffer.,? \\Q2001\\E",
      "shortCiteRegEx" : "Pfeffer.",
      "year" : 2001
    }, {
      "title" : "Figaro: An object-oriented probabilistic programming language. Charles River Analytics",
      "author" : [ "Avi Pfeffer" ],
      "venue" : "Technical Report,",
      "citeRegEx" : "Pfeffer.,? \\Q2009\\E",
      "shortCiteRegEx" : "Pfeffer.",
      "year" : 2009
    }, {
      "title" : "Black box variational inference",
      "author" : [ "Rajesh Ranganath", "Sean Gerrish", "David M Blei" ],
      "venue" : "arXiv preprint arXiv:1401.0118,",
      "citeRegEx" : "Ranganath et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Ranganath et al\\.",
      "year" : 2013
    }, {
      "title" : "Estimating the numbers of end users and end user programmers",
      "author" : [ "Christopher Scaffidi", "Mary Shaw", "Brad Myers" ],
      "venue" : "In Visual Languages and Human-Centric Computing,",
      "citeRegEx" : "Scaffidi et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Scaffidi et al\\.",
      "year" : 2005
    }, {
      "title" : "Particle Gibbs with Ancestor Sampling for Probabilistic Programs",
      "author" : [ "Jan-Willem van de Meent", "Hongseok Yang", "Vikash Mansinghka", "Frank Wood" ],
      "venue" : "In Artificial Intelligence and Statistics,",
      "citeRegEx" : "Meent et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Meent et al\\.",
      "year" : 2015
    }, {
      "title" : "Automated variational inference in probabilistic programming",
      "author" : [ "David Wingate", "Theophane Weber" ],
      "venue" : "arXiv preprint arXiv:1301.1299,",
      "citeRegEx" : "Wingate and Weber.,? \\Q2013\\E",
      "shortCiteRegEx" : "Wingate and Weber.",
      "year" : 2013
    }, {
      "title" : "Lightweight implementations of probabilistic programming languages via transformational compilation",
      "author" : [ "David Wingate", "Andreas Stuhlmueller", "Noah D Goodman" ],
      "venue" : "In International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Wingate et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Wingate et al\\.",
      "year" : 2011
    }, {
      "title" : "A new approach to probabilistic programming inference",
      "author" : [ "Frank Wood", "Jan-Willem van de Meent", "Vikash Mansinghka" ],
      "venue" : "In Artificial Intelligence and Statistics,",
      "citeRegEx" : "Wood et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Wood et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "Spreadsheets are the de facto lingua franca of data analysis [Panko, 2008].",
      "startOffset" : 61,
      "endOffset" : 74
    }, {
      "referenceID" : 12,
      "context" : ", 2014], and particularly black-box variational inference [Ranganath et al., 2013], for probabilistic programming.",
      "startOffset" : 58,
      "endOffset" : 82
    }, {
      "referenceID" : 6,
      "context" : "Also note that actual is novel to spreadsheet languages but closely corresponds to the notion of observation in the probabilistic programming literature Mansinghka et al. [2014], Wood et al.",
      "startOffset" : 153,
      "endOffset" : 178
    }, {
      "referenceID" : 6,
      "context" : "Also note that actual is novel to spreadsheet languages but closely corresponds to the notion of observation in the probabilistic programming literature Mansinghka et al. [2014], Wood et al. [2014].",
      "startOffset" : 153,
      "endOffset" : 198
    }, {
      "referenceID" : 12,
      "context" : "Algorithm 2 Spreadsheet Black-Box Inference (follows Algorithm 1 from Ranganath et al. [2013]) Input: program f : Ref → Expr , joint distribution P , distribution Q, number of particles S, graph G = (V,E), convergence constant ε, bound on iterations tmax , number of samples per iteration S, learning rate parameter γ, number of stochastic operators in the program L ∗ .",
      "startOffset" : 70,
      "endOffset" : 94
    }, {
      "referenceID" : 0,
      "context" : "Algorithm 1 gives a detailed implementation of a version of SMC, the inner loop of the particle independent Metropolis Hastings (PIMH)-like algorithm Andrieu et al. [2010] we implemented in",
      "startOffset" : 150,
      "endOffset" : 172
    }, {
      "referenceID" : 0,
      "context" : "That we can do this follows directly from the PIMH results in [Andrieu et al., 2010] where instead of doing MH on ratios of evidence estimates we simply do importance sampling with weights proportional to the evidence estimates.",
      "startOffset" : 62,
      "endOffset" : 84
    }, {
      "referenceID" : 13,
      "context" : "Existing work on using variational Bayes Wingate and Weber [2013], Mansinghka et al.",
      "startOffset" : 41,
      "endOffset" : 66
    }, {
      "referenceID" : 5,
      "context" : "Existing work on using variational Bayes Wingate and Weber [2013], Mansinghka et al. [2014] and specifically black box variational Bayes in probabilistic programming Kucukelbir et al.",
      "startOffset" : 67,
      "endOffset" : 92
    }, {
      "referenceID" : 5,
      "context" : "[2014] and specifically black box variational Bayes in probabilistic programming Kucukelbir et al. [2014] inspired this work.",
      "startOffset" : 81,
      "endOffset" : 106
    }, {
      "referenceID" : 3,
      "context" : "There are discernible differences between our approach and Tabular, a probabilistic programming language for Excel created by Microsoft Research [Gordon et al., 2014].",
      "startOffset" : 145,
      "endOffset" : 166
    }, {
      "referenceID" : 7,
      "context" : "Furthermore, Tabular does not allow black-box user-programmed primitives owing to their incompatibility with, for instance, EP [Minka, 2001] inference (aka Infer.",
      "startOffset" : 127,
      "endOffset" : 140
    } ],
    "year" : 2016,
    "abstractText" : "Spreadsheet workbook contents are simple programs. Because of this, probabilistic programming techniques can be used to perform Bayesian inversion of spreadsheet computations. What is more, existing execution engines in spreadsheet applications such as Microsoft Excel can be made to do this using only built-in functionality. We demonstrate this by developing a native Excel implementation of both a particle Markov Chain Monte Carlo variant and black-box variational inference for spreadsheet probabilistic programming. The resulting engine performs probabilistically coherent inference over spreadsheet computations, notably including spreadsheets that include user-defined black-box functions. Spreadsheet engines that choose to integrate the functionality we describe in this paper will give their users the ability to both easily develop probabilistic models and maintain them over time by including actuals via a simple user-interface mechanism. For spreadsheet end-users this would mean having access to efficient and probabilistically coherent probabilistic modeling and inference for use in all kinds of decision making under uncertainty.",
    "creator" : "LaTeX with hyperref package"
  }
}