{
  "name" : "1202.2773.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Decentralized Multi-agent Plan Repair in Dynamic Environments∗",
    "authors" : [ "Antońın Komenda", "Peter Novák", "Michal Pěchouček" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "The contribution of the presented paper is threefold. Firstly, we formally introduce the multi-agent plan repair problem and formally present the core hypothesis underlying our work. Secondly, we propose three algorithms for multi-agent plan repair reducing the problem to specialized instances of the multi-agent planning problem. Finally, we present results of experimental validation confirming the core hypothesis of the paper."
    }, {
      "heading" : "1 Motivation",
      "text" : "Classical planning and multi-agent planning based on classical planning are approaches to constructing autonomous agents and teams of agents, which attempt to achieve their objectives in an environment. The result of the planning process is traditionally a plan, a sequence of actions the agent should perform in order to achieve a given goal. When the agent is situated in a dynamic environment,\n∗This is the full version of an extended abstract published in Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2012), Conitzer, Winikoff, Padgham, and van der Hoek (eds.), June, 4–8, 2012, Valencia, Spain.\nar X\niv :1\n20 2.\n27 73\nv1 [\ncs .A\nI] 1\noccurrence of various unexpected events the environment generates might lead to the plan invalidation, a failure. A straightforward solution to this problem is to invoke a planning algorithm and compute a new plan from the state the agent found itself in after the failure to a state conforming with its original objective.\nPlanning, as well as replanning, in the case of a failure occurrence, is a costly procedure, especially in terms of its time complexity. It is relatively straightforward to see, that in many cases, however, a relatively minor fix to the original plan would resolve the failure possibly at a lower cost. Because it is not clear what exactly are the planning domains and types of dynamic environments which would allow for such a repair approach, it can be argued that non-informed plan repair attempts can in many cases even raise the overall complexity of the approach in comparison to replanning. This would be due to futile attempts to repair the failed plan before inevitably falling back to replanning.\nIn general, plan repair can be seen as planning with re-use of fragments of the old plan. There is a number of works, empirically demonstrating that plan repair in various domains performs better than replanning (e.g., [8, 2, 5]). However, in [10], Nebel and Koehler theoretically analyzed plan re-use (plan repair), and conclude that in general it does not bring any benefit over replanning in terms of computational time complexity.\nIn situated multi-agent systems, however, the time complexity is often not of the primary importance. In such systems, often it is the communication complexity which is a higher priority concern. Consider application domains, such as e.g., undersea operations by teams of coordinated autonomous underwater vehicles. While the state-of-the-art technology allows to employ relatively powerful computers on board of such robots, the communication links are extremely constrained and expensive; wireless networks cannot be deployed and communication is performed mostly using acoustic signaling. In such applications, it is the communication complexity of the distributed planning algorithms which matters more than time complexity. Consequently, employment of multi-agent plain repair techniques can provide a tangible benefit over replanning for a team of robots whose multi-agent plan fails.\nThe motivation for our research is the intuition that multi-agent plan repair, even though not always the fastest approach, should under specific conditions generate lower communication overheads in comparison to replanning. The conditions correspond to the level of required coordination and the types of failures the environment generates. While the hypothesis is rather intuitive, our approach is significant in that we give it a rigorous treatment. Besides our preliminary approach in [7], this aspect of multi-agent planning and plan repair, while obviously important with arrival of truly distributed algorithms for multi-agent planning, such as the one by Nissim et al. [11] did not witness considerable attention of the community yet.\nThe contribution of the the presented paper is threefold. Firstly, after introducing the general problem of multi-agent planning stemming from the formulation due to Brafman and Domshlak [3] in Section 2, in subsequent Section 3 we formally introduce the multi-agent plan repair problem and formally state\nthe core hypothesis of the presented research. Secondly, still in Section 3, we propose three algorithms for multi-agent plan repair reducing the problem to specialized instances of the multi-agent planning problem. Finally, in Section 4 we present experimental validation confirming the core hypothesis of the paper. Section 5 concludes the paper by some final remarks regarding the shortcomings of our approach and future outlooks in the here described line of research."
    }, {
      "heading" : "2 Multi-agent planning",
      "text" : "Classical, single-agent planning problem is characterized by a set of states with a unique initial state, a final state (a set of final states) and a set of actions representing the transitions between these states that the system undertakes upon performing the actions. We define multi-agent planning problem as an extension of the classical single-agent planning. We consider a number of cooperative and coordinated actors featuring possibly distinct sets of capabilities (actions), which concurrently plan and subsequently execute their local plans so that they achieve a joint goal.\nAn instance of a multi-agent planning problem is defined by: i) an environment characterized by a state space, ii) a finite set of agents, each characterized by a set of primitive actions (or capabilities) it can execute in the environment, iii) an initial state the agents start their activities in and iv) a characterization of the desired goal states. Before treating the problem of multi-agent planning and summarizing a state-of-the-art algorithm for solving it, we first formally introduce the underlying concepts."
    }, {
      "heading" : "2.1 Preliminaries",
      "text" : "Consider a set of atoms {p1, . . . , pn}. A state is a set of terms from a language L = {p1, . . . , pn,¬p1, . . . ,¬pn} where ¬p denotes a negation of p. We also assume the standard tautology p ≡ ¬¬p for every p ∈ L. Furthermore, we require all states to be consistent, i.e., for a state s ⊆ L we have that p ∈ s if and only if ¬p /∈ s. Note, in general, the states do not have to be complete, i.e., it might be that there is a p ∈ L, such that {p,¬p} 6⊆ s. S denotes the set of all states and we assume there is a distinguished state χ ∈ S denoting an undefined state in which the overall system can be in. To simplify the notation, we also extend the negation to states as follows ¬s = {p|¬p ∈ s}. The set of atoms corresponding to a set of terms φ ⊆ L is denoted φ = {p | p ∈ φ or ¬p ∈ φ}.\nA primitive action, or simply an action, is a tuple φaψ, where a is a unique action label and φ, ψ ⊆ L respectively denote the sets of preconditions and effects of a. The preconditions and effects are assumed to be consistent sets of terms. Whenever the context is clear, we simply write a instead of φaψ. Act denotes the set of all actions and we furthermore assume there is a distinguished empty action ∅ ∅ ∈ Act with no preconditions and no effects.\nWe say that an action φaψ is applicable in a state s iff φ ⊆ s. An application of φaψ is defined by the state transformation operator ⊕ : S ×Act → S defined\nas follows:\ns⊕ a = {\n(s ∪ ψ) \\ ¬ψ iff φ is applicable in s, χ otherwise.\nNote that given a consistent state s and an action φaψ, the application of a to s results in either the undefined state χ, or, in the case a was applicable in s, a consistent state s′ again. The application of φaψ to s first enriches s with all the effects of a, however, in the case there exists some p ∈ s, s.t., ¬p ∈ ψ, the simple unification would make the resulting state inconsistent. The subsequent set subtraction of terms which were the source of such inconsistencies makes the resulting state consistent again, while at the same time preserving the effects of a. Furthermore, ⊕ is associative, hence we can write s⊕ a1 ⊕ · · · ⊕ an.\nAn agent α = {φa1a1ψa1 , . . . , φananψan} is characterized precisely by its capabilities, a finite repertoire of actions it can preform in the environment. From now on, we assume that there exists a language L giving rise to a state space S."
    }, {
      "heading" : "2.2 The Problem of Multi-agent Planning",
      "text" : "Definition 1 (multi-agent planning). A multi-agent planning problem is a tuple Π = (A, s0, Sg), whereA is a set of agents α1, . . . , αn, featuring mutually disjoint sets of actions, an initial state s0 ∈ S and a set of goal states Sg ⊆ S.\nBefore formally defining the notion of a solution to a multi-agent planning problem, we first introduce a sequel of auxiliary notions.\nGiven an agent α, a single-agent plan P is a sequence of actions a1, . . . , ak, s.t., ai ∈ α for every i. P [i] denotes the i-th action in P , or P [i] = in the case i is larger than the length of P , which in turn will be denoted |P |.\nA team of agents A = α1, . . . , αn can act in the environment concurrently. A joint action φaaψa of the team is specified by a = (φa1a1ψa1 , . . . , φananψan) a tuple of actions corresponding to the individual agents, i.e., ai ∈ αi for each i, its preconditions φa = ⋃n i=1 φai and its effects ψa = ⋃n i=1 ψai . a[k] denotes the k-th action of a. Similarly to actions of individual agents, φa and ψa are assumed to be consistent sets of terms. The notion of action applicability in a state s, as well as application of a to s straightforwardly extend from the definitions for primitive actions.\nDefinition 2 (multi-agent plan). Let Π = (A, s0, Sg) be a multi-agent planning problem with A = α1, . . . , αn. A synchronous multi-agent plan P = {P1, . . . , Pn}, consisting of single agent plans P1, . . . , Pn respectively constructed from actions of the agents α1, . . . , αn is a solution to Π if the plan P satisfies the following:\n1. P is well-formed, i.e., |Pi| = |Pj | for all i, j ≤ n. |P| = |Pk|, for some k ≤ n, denotes the length of the multi-agent plan P;\n2. P is feasible, i.e., there exists a sequel of states s1, . . . , sm, s.t. m = |P| and si+1 = si ⊕ ai with ai = (P1[i], . . . , Pn[i]) for all i < m; and finally\n3. P reaches the goal Sg, i.e., there exists sg ∈ Sg, s.t. sm ⊆ sg.\nWe also say that P solves the problem Π. Finally, Plans(Π) denotes the set of plans which are solutions to a given multi-agent planning problem Π.\nAdditionally, P[k] denotes the joint action of the team in the step k and P[k, i] denotes the primitive action of the agent i in the step k. This notation allows us to introduce the following plan-matrix notation for a multi-agent plan P, which provides a more visual understanding of multi-agent plans:\nP =  a11 a21 · · · am1 a12 a22 am2 ... . . . ...\na1n a2n · · · amn  where aij = P[i, j]. Indices i (i ≤ m = |P|) and j (j ≤ n) denote the step of the plan P and the agent which performs the primitive action, respectively.\nWe say that two multi-agent plans P1, P2 are equal (P1 = P2) iff they have the same length (|P1| = |P2|) and for all i and j we have P1[i, j] = P2[i, j].\nA concatenation of two multi-agent plans P1 and P2 over the same agents α1, . . . , αn is defined as a plan P = P1 · P2, where for each i and j we have P[i, j] = P1[i, j] if i ≤ |P1| and P[i, j] = P2[i − |P2|, j] for i > |P1|. In the plan-matrix notation, the concatenation would correspond to simple columnsappending operation. Note, concatenation of multi-agent plans is an associative operation.\nGiven a multi-agent plan P, P[i..j] denotes a fragment of P from the step i to the step j. More precisely, P[i..j] is a fragment of P iff there exist multi-agent plans Pprefix and Psuffix , such that Pprefix · P[i..j] · Psuffix = P. Finally, P[i..∞] denotes the i-th suffix of the plan P, i.e., P[i..∞] = P[i..|P|]. P1 · P2 is said to be a decomposition of a multi-agent plan P iff P = P1 · P2.\nGiven two multi-agent plans P1 and P2 we can define how different they are. diff (P1,P2) denotes the difference between P1 and P2, that is the overall number of primitive actions in P1, which do not correlate with the corresponding primitive actions in P2 and vice versa. Formally, diff (P1,P2) = |{(i, j) | P1[i, j] 6= P2[i, j]}|. In the case |P1| ≤ |P2|, diff (P1,P2) = diff (P1 · P ,P2), where P is a plan padding of P1 to the overall length |P1 · P | = |P2| and filled with empty actions, i.e, for each i, j, we have P [i, j] = . Note that the measure diff is position agnostic, i.e., we define diff (P1,P2) = diff (P2,P1) in the case |P1| > |P2|."
    }, {
      "heading" : "2.3 Planning Algorithm",
      "text" : "The above formulation of the multi-agent planning problem is well in line with the original formulation of MA-Strips planning due to Brafman and Domshlak [3]. The authors there additionally distinguish between the public and private actions of the individual agents. An action is public whenever its preconditions or effects involve atoms occurring in preconditions or effects of an action\nbelonging to another agent of the team. Formally, given a multi-agent team A = α1, . . . , αn, the set of public actions is defined as Actpub = {a | ∃i, j : i 6= j, a ∈ αi, a′ ∈ αj , and (φa ∪ψa)∩ (φa′ ∪ψa′) 6= ∅}. Recall, φ denotes the set of non-negated atoms occurring in φ. Actpriv = Act \\Actpub, where Act = ⋃n i=1 αi is the set of all actions the team A can perform. The distinction of actions to private and public turns out to be an important one. Since private actions do not depend, nor are dependencies of other actions performable by the team, planning of sequences of private actions can be implemented strictly locally by the agent the actions belongs to. In effect, the public actions become points of coordination among the multi-agent team members and a truly decentralized multi-agent planning algorithm for a planning problem Π can be implemented in two interleaving stages until a suitable multi-agent plan is found: i) a plan consisting exclusively of public actions of the agent team is calculated, and subsequently ii) the sequences of private actions between the public actions of each individual agent are computed to fill in the gaps.\nThe main contribution of the Brafman and Domshlak’s paper lies in pointing out that the algorithms can be implemented by reduction of the first stage of the planning process to a constraint satisfaction problem (CSP) corresponding to the multi-agent planning problem with public actions only. The second stage can be subsequently solved by any classical single-agent planning algorithm. In result, solving a given multi-agent planning problem can be loosely formulated as a CSP with the following two types of constraints:\ncoordination constraint: a sequence of joint actions P (candidate multiagent plan) satisfies the coordination constraint iff for every action φaaψa = P[k, i] performed by the agent αi in the step k we have, that if a is a public action, then\n• for every p ∈ φa, there must exist φapapψap = P[kp, ip], such that p ∈ ψap and 0 < kp < k (there is some previous action which causes p to hold), and\n• for no k′, s.t., kp ≤ k′ < k there exists φ′a′ψ′ = P[k′, i′], such that ¬p ∈ ψ′ (p won’t be invalidated between causing it in the step kp and execution of a in the step k).\ninternal planning constraint: a sequence of joint actions P satisfies the internal planning constraint iff for every agent, the corresponding singleagent planning problem with landmarks {a | a = P[k, i] ∈ Actpub} is solvable. I.e., a single-agent planning algorithm is able to fill in the gaps between the public actions in the candidate multi-agent plan.\nAlgorithm 1 lists the original multi-agent planning algorithm MA-Plan by Brafman and Domshlak in [3]. The algorithm iterates through CSP formulations of the planning problem according to δ, informally the number of coordination points between the agents in the multi-agent team. I.e., δ determines the number\nAlgorithm 1 MA-Plan(Π): Input: A multi-agent planning problem Π = (A, s0, Sg). Output: A multi-agent plan P solving Π, if such exists.\nδ = 1 loop\nConstruct CSPΠ;A if solve-csp(CSPΠ;δ) then\nReconstruct a plan P from a solution for CSPΠ;δ. return P\nelse δ = δ + 1\nend if end loop\nof joint actions in a candidate multi-agent plan consisting of only public actions. Filling the gaps between the individual single-agent public actions, if possible, then gives rise to the overall multi-agent plan. In the case such a plan completion does not exist, the process continues by testing longer candidate plans.\nThe original multi-agent planning algorithm assumes a centralized planning architecture. I.e., it is a centralized planning algorithm computing multi-agent plans for a team of agents which are supposed to be subsequently executed in a decentralized fashion. Our motivation is however a decentralized planning/plan repair algorithm followed by a decentralized plan execution.\nNissim et al. in [11] adapted the original blueprint algorithm described above to a distributed setting. The adaptation rests on formulating the multiagent planning problem as a distributed constraint satisfaction problem instance (DisCSP) and subsequently utilizing a a state-of-the-art DisCSP solver for solving it, plus managing the overhead involved in the resulting distributed algorithm. The resulting algorithm, however, closely follows the scheme of the original algorithm as listed in Algorithm 1. From now on, whenever we speak about the implementation of the multi-agent planning algorithm, we have in mind its decentralized version due to Nissim et al. [11]."
    }, {
      "heading" : "3 Multi-agent plan repair",
      "text" : "Consider a multi-agent planning problem Π = (A, s0, Sg) and a plan P solving Π. Furthermore, consider an environment in which, apart from the actions performed by the agents of the team A, no other exogenous events occur. We say that such an environment is ideal, or non-dynamic. The execution of P in such an environment is uniquely determined by the set of states s0, . . . , sm, such that si+1 = si ⊕ P[i] (cf. also Definition 2).\nIn dynamic environments, however, it can occur that in the course of execution of P, the environment interferes and the execution of some action P[i]\nfrom the plan P does not result in precisely the state si+1 as defined above. We could say that at step i an unexpected event occurred in the environment. For simplicity, we consider only unexpected events happening exclusively in the course of execution of some action (as if it took a non-zero time), not such which could occur while the agent is deliberating the execution (i.e., as if the deliberation was instantaneous).\nNote that not all unexpected events in a dynamic environment necessarily lead to problems with execution of the plan P. However, there are at least two cases of such events, which can be considered a plan execution failure.\nA weak failure of execution of the plan P at step i w.r.t. the multi-agent planning problem Π is such, when the state sf resulting from an attempt to perform an action φaaψa = P[i] for some i does not satisfy some of the postconditions of a, i.e., ψa 6⊆ sf .\nA strong failure of execution of the plan P at step i w.r.t. the planning problem Π occurs whenever the i-th action of P cannot be executed due to its inapplicability. I.e., the execution of the plan up to the step i resulted in states s0, s′1 . . . , s′i, possibly with some weak failures occurring in the course of execution of the plan fragment and P[i] is not applicable in s′i.\nThe weak and the strong plan execution failures are, however, just two examples of a plan failure. There certainly are application domains in which weak failures can be tolerated as far as the goal state is reached after execution of the multi-agent plan. Alternatively, there might be domains in which other types of plan execution failures can occur, e.g., any change of the state not caused by the involved agents can be considered a failure as well. To account for the range of various types failures, from now on, we only require that a plan execution monitoring process determines some plan execution failure at a step i which results in some failed state sf .\nDefinition 3 (multi-agent plan repair). Let Π = (A, s0, Sg) be a multi-agent planning problem. A multi-agent plan repair problem is a tuple Σ = (Π,P, sf , k), where P is a multi-agent plan solving the planning problem Π, k is the step of P in which its execution failed and sf ∈ S is the corresponding failed state.\nA solution to the plan repair problem Σ is a multi-agent plan P ′, such that P ′ is a solution to the planning problem Π′ = (A, sf , Sg). We say that P ′ repairs P in sf . In the case Plans(Π′) = ∅, we say that the plan is irreparable given the failure occurring at the state sf .\nGiven two multi-agent plans P1 and P2 both repairing a multi-agent plan P for a problem Π in a state sf , we say that P1 is preserving P more than P2 iff diff (P1,P) ≤ diff (P2,P) and denote the relation by P1 P2. The minimal repair of the multi-agent plan P is such a plan Pmin ∈ Plans(Π′), which is minimal w.r.t. the mutual differences between the plans solving Π′. I.e., Pmin ∈ arg min\nP′∈Plans(Π′) diff (P,P ′).\nNote, there might be several distinct minimal repairs of a given multi-agent plan.\nIn general, the multi-agent plan repair problem can be reduced to solving a modified multi-agent planning problem and thus gives rise to a straightforward plan repair algorithm based on replanning in two steps: 1) construct the multiagent replanning problem Π′ as prescribed in Definition 3, and subsequently 2) utilize the MA-Plan algorithm (Algorithm 1 ) to solve the problem Π′.\nThe original motivation underlying this paper was the hypothesis that attempts to repair failed multi-agent plans lead to lower communication overhead than replanning. Clearly, not all planning problems could benefit from such a mechanism. Since we focus on multi-agent planning problems, which in a sense enforce coordination among the members of a multi-agent team, we firstly introduce the concept of k-coordinated multi-agent planning problems.\nDefinition 4 (k-coordination). We say that a multi-agent plan P is k-coordinated iff each fragment P of length k contains at least one joint action containing a public action. Formally, for every P ′, s.t. P = Pprefix ·P ′ ·Psuffix with |P ′| = k, there exist i and j so that P ′[i, j] ∈ Actpub .\nWe say that a multi-agent problem Π is k-coordinated iff all the plans P ∈ Plans(Π) solving Π, which cannot be compressed are k-coordinated. A plan P ∈ Plans(Π) can be compressed iff it contains a fragment P ′, s.t. P = Pprefix · P ′ · Psuffix and Pprefix · Psuffix ∈ Plans(Π).\nWe informally say that multi-agent planning problems leading to plans containing coordination points (public actions) placed relatively frequently throughout the plans are tightly coordinated. More formally, a multi-agent planning problem Π is tightly coordinated if it is k-coordinated and k is relatively low in comparison to the lengths of plans from Plans(Π). In the case k is relatively high w.r.t. the plan lengths, we say that the problem is loosely coordinated and finally, if the plans do not involve public actions, i.e., coordination is not needed at all, we say the problem is uncoordinated.\nThe core hypothesis of the paper can be then formulated as follows:\nHypothesis 1. Multi-agent plan repair approaches producing more preserving repairs than replanning tend to generate lower communication overhead for tightly coordinated multi-agent problems.\nA crisper, though perhaps a more challenging version of the hypothesis would express the communication overhead in terms of the average communication complexity:\nHypothesis 2. When applied to tightly coordinated planning problems, multiagent plan repair algorithms producing more preserving repairs than replanning feature a lower average communication complexity than replanning.\nIn the remainder of this paper, we approach resolution of Hypothesis 1. Treatment of Hypothesis 2 is beyond the scope of this paper and is left for future work.\nAlgorithm 2 Back-on-Track-Repair(Σ) Input: A multi-agent plan repair problem Σ = (Π,P, sf , k), with Π =\n(A, s0, Sg) and a sequence of states s0, . . . , sm execution of P generates in the ideal environment.\nOutput: A multi-agent plan P ′ solving Σ if a solution exists.\nConstruct Πback = (A, sf , {s0, . . . , sm}) if MA-Plan(Πback) returns a solution Pback then\nRetrieve the state sj of P to which Pback returns return P ′ = Pback · P[j . . .∞]\nend if"
    }, {
      "heading" : "3.1 Back-on-track Repair",
      "text" : "Unexpected event occurring in an environment can cause a failure in execution of a plan performed by some multi-agent team in that environment. The result is that the overall state of the system is not the one expected by the undisturbed plan execution at the particular time step. A straightforward idea to fix the problem is to utilize a multi-agent planner to produce a plan from the failed state to the originally expected state and subsequently follow the rest of the original multi-agent plan from the step in which the failure occurred. The following multi-agent plan repair approach, coined back-on-track (BoT) repair, is inspired by this idea, in fact a slight generalization of it.\nDefinition 5 (back-on-track repair). Let Σ = (Π,P, sf , k) be a multi-agent plan repair problem and Π′ = (∆, sf , Sg) being the corresponding modified multi-agent replanning problem.\nWe say that a plan P ′ ∈ Plans(Π′) is a back-on-track repair of P iff there is a decomposition of P ′, such that P ′ = Pback · P[i..∞] for some i ≤ |P|. P ′ = Pback ·P[i..∞] is said to be a proper back-on-track repair iff |P[i..∞]| > 0. I.e., P ′ preserves some non-empty suffix of P.\nInformally, the back-on-track approach tries to preserve a suffix of the original plan and prefix it with a newly computed plan Pback starting in sf and leading to some state along the execution of P in the ideal environment. Note, that all plans from Plans(Π′) are back-on-track repairs of the original plan. The length of the preserved suffix of the original plan provides a handle on the repair quality ordering of the plans. The longer the preserved suffix, the more preserving the plan is. On the other hand, even when the plan repair problem Σ is indeed solvable, there might not be any valid proper back-on-track repair of the original planning problem.\nAlgorithm 2 realizes a multi-agent plan repair procedure according to the back-on-track plan repair principle. Since the MA-Plan algorithm searches for the shortest plan from the initial state to a goal state, the Back-on-Track-Repair computes plans which return back to the original one in the shortest possible way.\nAlgorithm 3 Lazy-Repair(Σ) Input: A multi-agent plan repair problem Σ = (Π,P, sf , k), with Π =\n(A, s0, Sg) and |A| = n. Output: A multi-agent plan P ′ solving the problem Σ according to the lazy\napproach, if a solution exists.\nConstruct P[k..∞], the executable remainder of P from the step k and state sf Construct Πlazy = (A, s|P|, Sg) let P lazy be a solution to MA-Plan(Πlazy) if such exists. return P[k..∞] · P lazy"
    }, {
      "heading" : "3.2 Simple Lazy Repair",
      "text" : "The back-on-track multi-agent plan repair approach seeks to compute a new prefix to some suffix of the original plan and repair the failure by their concatenation. An alternative approach, coined lazy, attempts to preserve the remainder of the original multi-agent plan and close the gap between the state resulting from the failed plan execution and a goal state of the original planning problem.\nLet sf be the state resulting from a failure in execution of a multi-agent plan P in a step k. We say that a sequence of joint actions P ′ is an executable remainder of P from the step k and the state sf iff there exists a sequence of states sk, . . . , s|P|, such that sk = sf , si+1 = si⊕P ′[i−k+ 1] and for every step i and every agent j, we have that P ′[i− k + 1, j] = P[i, j] in the case P[i, j] is applicable in the state si and P ′[i− k + 1, j] = otherwise.\nThe following definition provides a formal definition of the lazy approach.\nDefinition 6 (simple lazy repair). Let Σ = (Π,P, sf , k) be a multi-agent plan repair problem and Π′ = (A, sf , Sg) being the corresponding modified multiagent replanning problem.\nWe say that a plan P ′ ∈ Plans(Π′) is a lazy repair of P iff there is a decomposition of P ′, such that P ′ = P[k..∞] · P lazy, where P[k..∞] is the executable remainder of P from the step k, execution of which results in the state slazy when starting in sf , and P lazy is a solution to the multi-agent planning problem Πlazy = (A, slazy, Sg).\nAlgorithm 3 realizes multi-agent plan repair based on the lazy repair approach described above.\nThe back-on-track approach always succeeds to compute some multi-agent plan repairing the original plan from the failed state in the case the replanning form scratch would compute such a plan from that state as well. The lazy approach is in this sense incomplete, as it might happen that the execution of the executable remainder of the original plan diverges to a state from which no plan to some goal state exists. Given that dynamic environment in general could generate irreparable failures, this incompleteness cannot be considered a\nshortcoming of the lazy approach in general. Of course in domains in which no irreparable unexpected event might occur, while at the same time the agents are allowed to perform actions potentially having irreversible and potentially harmful effects, the lazy approach has to be employed with caution."
    }, {
      "heading" : "3.3 Repeated Lazy Repair",
      "text" : "In dynamic environment plan failures occur repeatedly, i.e., even after a repair of a failed plan, it is possible for the repaired plan to fail again. In this situation both the back-on-track, as well as the lazy multi-agent plan repair algorithms lead to prolonging the really executed plan. In the case of the back-on-track approach, this is inevitable, since upon the repair, the subsequent plan execution process immediately processes the newly added plan fragment. In the case of the lazy repair, however, upon occurrence of another failure during execution of the repaired plan, it is not always necessary to prolong the overall multi-agent plan.\nThe intuition behind the repeated lazy plan repair approach is that a failure during execution of an already repaired plan makes the previous repair attempt irrelevant and its result can be discarded, unless the failure occurred already in the plan fragment appended by the previous repair. The following definition formally introduces the extension of the lazy multi-agent plan repair approach. For clarity, from now on, we refer to the lazy multi-agent plan repair approach introduced in Definition 6 as simple lazy repair.\nDefinition 7 (Repeated lazy repair). Let Π = (A, s0, Sg) be a multi-agent planning problem with a solution P and Σ1 = (Π,P, sf1 , k1) be a multi-agent plan repair problem with a lazy repair solution PΣ1 = P[k1..∞] ·Psuffix and Σ2 = (Π,PΣ1 , sf2 , k2) be a multi-agent plan repair problem the system is currently facing.\nWe say that P ′ is a repeated lazy repair of P1 iff\n1. P ′ is a simple lazy repair solution to Σ′ = (Π,P, sf2 , k2) in the case k2 ≤ |P[k1..∞]|; and\n2. P ′ is a simple lazy repair solution to Σ2 otherwise.\nThe repeated lazy repair leads to a straightforward extension of the lazy plan repair algorithm listed in Algorithm 3. Note, that the repeated lazy repair algorithms enables a plan execution model which preserves significantly longer fragments of the original plan. That is, upon a failure, instead of trying to repair the failed plan right away, as both the back-on-track and simple lazy plan repair algorithms do, the system can simply proceed with execution of the remainder of the original plan and only after its complete execution the lazy plan repair is triggered. The approach simply ignores the plan failures during execution and postpones the repair the very end of the process, hence the “lazy” label for the two algorithms.\nAlgorithm 4 Repeated-Lazy-Repair(Σ1,PΣ1 ,Σ2) Input: A multi-agent planning problem Π = (A, s0, Sg), a multi-agent plan P = P[k..∞] · Psuffix solving Π, and two multi-agent plan repair problems Σ1 = (Π,P, sf1 , k1) with a solution PΣ1 and Σ2 = (Π,PΣ1 , sf2 , k2). Output: A multi-agent plan solving Σ2 if a solution exists.\nif k2 ≤ |P[k..∞]| then Construct Σ′ = (Π,P, sf2 , k2) else Σ′ = Σ2 end if return a solution to Lazy-Repair(Σ′) if such exists"
    }, {
      "heading" : "4 Experimental validation",
      "text" : "To verify the Hypothesis 1, we conducted a series of experiments with implementations of the multi-agent plan repair algorithms described in Section 3. Below, we firstly describe the experimental setup used for the experiments and subsequently interpret the data collected and revisit Hypothesis 1."
    }, {
      "heading" : "4.1 Experimental Setup",
      "text" : "The experiments were based on a two-stage algorithm. In the first phase, for a given domain a multi-agent plan was computed using the MA-Plan algorithm based on a distributed constraint satisfaction solver for computing the candidate coordination plans and implementation of a best-first-search action planning algorithm as part of FF [6] for computing the local, single-agent plans. We used the implementation of the distributed multi-agent planner authored by Nissim et al. also used for the experiments conducted in their paper [11]. In the second phase, we executed the multi-agent plan. In the course of the plan execution, we simulated the environment dynamics by producing various plan failures according to a variable failure probability. The plan execution was monitored and upon a failure detection a plan repair algorithm was invoked. Algorithm 5 lists the pseudo-code of the process.\nBefore execution of each plan step, the joint action is checked for applicability in the current state. In the case it is not applicable, a plan repair algorithm is invoked and the execution continues on the repaired plan. Otherwise, the state is updated with the joint action.\nThe execute-fail function in the algorithm either updates the current state by the joint action provided as a parameter as if in the ideal environment, or to generate an unexpected event occurring in the simulated dynamic environment.\nWe distinguish two types of plan failures: action failures and state perturbations. Both failure types are parametrized by a uniformly distributed probability P , which determines whether a simulation step fails, or not. Both failure types are weak failures. That is, they are not handled immediately, but can preclude\nAlgorithm 5 Plan execution and monitoring algorithm. Input: An initial multi-agent planning problem Π = (A, s0, Sg).\nP = MA-Plan(Π) s = s0; step = 1 repeat\nif φP[step] 6⊆ s then P = Repair(Π,P, s, step) step = 1 end if s = execute-fail(s,P[step]) step = step + 1\nuntil step > |P|\nthe plan execution and later result in a strong failure. Upon detection, a strong failure is handled by one of the plan repairing algorithms.\nAn action failure is simulated by not-executing some of the individual agent actions from the actual plan step. The individual action is chosen according to a uniformly distributed probability. The individual action is removed from the joint action and the current state is updated by the modified joint action.\nThe other simulated failure type, state perturbation, is parametrized by a positive non-zero integer c, which determines the number of state terms, which are removed from the current state, as well as the number of terms which are added to it. The terms to be added or removed are selected also randomly from the domain language according to a uniform distribution.\nThe experimental setup was implemented as a centralized simulator of the environment integrating a decentralized multi-agent domain-independent planner MA-Plan. The individual agents are initialized by a planning domain, together with a particular planning problem instance. Each agent runs in its own thread and they deliberate asynchronously. The agents send peer-to-peer messages among themselves. Message passing is mediated by the centralized simulator as well. The messages are sent in the DisCSP phase by the integrated solver, which is a part of the MA-Plan planner.\nThe experiments were performed on Phenom Quad Core 9950 processor at 2.6GHz with Java Virtual Machine limited to 2.5GB of RAM. The individual measurements were parametrized by the plan failure probability P and each problem instance was executed 6–10 with various value samples. The resulting data are, in the figures, presented with natural distribution. The candlestick charts depict the difference between the minimal and the maximal measurements, together with the standard deviation."
    }, {
      "heading" : "4.2 Test Problems, Algorithms and Metrics",
      "text" : "The experiments were conducted on three planning domains. The domains originate in the standard benchmark single-agent ICP planning domains pub-\nlished at [1]. Similarly to [11], we chose domains, which are straightforwardly modifiable to multi-agent planning problems: logistics (3 agents), rovers (3 agents), and satellites (2–5 agents).\nThe logistics domain is a tightly coordinated in that it requires relatively frequent coordination among the involved agents: airplanes and trucks need to wait for each other to load or unload the transported packages. The rovers domain is loosely coordinated in that it requires coordination only at the end of plans: there is a single shared communication channel between one of the rovers and the receiving station. Finally, the satellites domain is uncoordinated in that it does not need any coordination between the satellites acquiring images individually.\nTo evaluate validity of Hypothesis 1, the multi-agent planning problems were tested on the experimental setup against a plan repair algorithm implementing replanning from scratch and two of the repair algorithms Back-on-Track-Repair (Algorithm 2) and Repeated-Lazy-Repair (Algorithm 4) introduced in Section 3.\nEfficiency problems of the MA-Plan implementation limited the experiments to plans with maximally two landmarks (coordination points). The measurements of Back-on-Track-Repair algorithm runs were negatively influenced by sensitivity of the planner implementation to the number of terms in the goal state. Additionally, the Back-on-Track-Repair algorithm could not leverage disjunctive goal form (cf. Definition 5) and this was emulated by iterative process testing all term conjunctions in a sequence and thus resulting in multiple runs of the DisCSP solver instead of a single run with disjunctive goal.\nWe used three metrics to evaluate the measurements:\nexecution length is the overall number of joint actions the experimental setup executed.\nplanning time is the measured cumulative time consumed by the underlying MA-Plan planner used for generating initial and repairing plans; and finally\ncommunication is measuring the number of messages passed between the agents during the planning or plan repair process. That is messages generated by the DisCSP solver in the MA-Plan planner."
    }, {
      "heading" : "4.3 Results and Discussion",
      "text" : "The first batch of experiments directly targets validation of Hypothesis 1: multiagent plan repair is expected to generate lower communication overhead in tightly coordinated domains. We used logistics as a tightly coordinated domain and dynamics of the simulated environment modeled as action failures. Figure 1 depicts the results of the experiment. The communication overhead generated by the Back-on-Track-Repair algorithm is on average only 59% (36% at best) of that generated by the replanning approach. Furthermore, the Repeated-LazyRepair algorithm performed even better and on average produced only 43% (11% at best) of the communication overhead generated by the replanning algorithm. In result, the experiments strongly support our hypothesis.\nAdditionally, the overall time spent in the planning phase (used by the MAPlan algorithm) by the plan repair algorithms was 54% (34% at best) and 51% (12% at best) for Back-on-Track-Repair and Repeated-Lazy-Repair respectively. The execution length was lower in comparison the replanning approach as well being in average 96% (72% at best, 130% at worst) by Back-on-Track-Repair and lower being 81% (34% at best, 132% at worst) for Repeated-Lazy-Repair.\nThe second batch of experiments focused on boundaries of validity of the positive result presented above. In particular, we validated the condition on the coordination tightness and feasibility of failures. The auxiliary hypothesis we validated states: with decreasing coordination tightness of the domain, the communication efficiency gains of repairing techniques should decrease. For loosely coordinated domains the communication efficiency of plan repair should be on-par with that of the replanning approach.\nTo validate the auxiliary hypothesis we ran experiments with the rovers as a loosely coordinated domain. Figure 3 (top) presents the results supporting the hypothesis.\nThe third batch of experiments targeted the perturbation magnitude of the plan failures. The second auxiliary hypothesis we validated states: communication efficiency gain of plan repairing in contrast to replanning should decrease as the difference between a nominal and related failed state increases. The underlying intuition is that, in the case the dynamic environment generates only relatively small state perturbations and the failed states are “not far” from the actual state, the plan repair should perform relatively well. On the other hand, if the state essentially “teleports” the agents to completely different states, replanning tends to generate more efficient solutions than plan repair.\nTo answer this hypothesis, we have prepared another logistics experiment\nemploying state perturbations as the model of the environment dynamics. Figure 3 (bottom) depicts results of the experiment for c = 1. The perturbed state for c = 1 is produced by removing one term from the actual state and adding another one. As the chart shows, under random perturbations the plan repairing technique lost its improvement against replanning. For stronger perturbations with c = 2, 3, 4, the ratio between plan repairing and replanning remained on average the same. The trend of the absolute numbers of messages, planning time and execution length was slightly decreasing, as the probability of opportunistic effects increased.\nFinally, we conducted a series of experiments with a non-coordinated sattelites domain. The results depicted in Figure 3 show the anticipated lower plan repair communication efficiency in contrast to replanning."
    }, {
      "heading" : "5 Final remarks",
      "text" : "In the presented paper, we formally introduced the problem of multi-agent plan repair, proposed three algorithms for solving it and experimentally validated the hypothesis stating that under certain conditions, multi-agent plan repair approach tends to be more efficient in terms of the communication overhead it generates in comparison to the replanning approach. Our results well support the core hypothesis of the paper and we additionally performed a series of experiments validating its boundary conditions.\nThe line of research underlying this paper well correlates with recent works on classical single-agent planning sub-domains, such as partial ordered plan monitoring and repairing [9], conformant and contingency planning, plan reuse and plan adaptation. Environment dynamics is also handled by approaches based on Markov decision processes. The main difference to our approach is that the state perturbations utilized in our experiments have a priori unknown probabilities. Our own recent approach to the problem of multi-agent plan repair in [7] can be seen only as a precursor to the formal and rigorous treatment of the problem in this paper. Therein, we described the first steps towards formal treatment of the problem, as well as proposed two specific incomplete algorithms for solving the problem, very distinct from the ones presented here.\nThere are several open challenges resulting from the presented work. Firstly, the multi-agent planning framework (MA-Strips) is not expressive enough to describe certain aspects of concurrent actions and should be extended to this end, what, we suspect, will also influence the multi-agent planning complexity analysis. In particular, there is no way to account for joint actions which have effects strictly different than the unity of the individual actions involved. Another issue is that there is no way to enforce or forbid concurrent execution of certain individual actions. Secondly, the framework is not able to describe concurrent resource consumption, which is not an issue in single-agent Strips [4] planning, but in the multi-agent extension two individual concurrently executed actions might “consume” the same precondition, even though it is undesirable in the domain. Thirdly, there is a need for more efficient implementations of\nmulti-agent planners with more features as the gap between the state-of-theart classical planners and multi-agent planners is enormous. Fourthly, there is a lack of standardized planning benchmarks for multi-agent planning, especially considering tightly coordinated planning problems. Such are needed to further evaluate the hypotheses presented in this paper. Finally, we leave out the work towards resolving the validity of Hypothesis 2 aiming at investigation of complexity issues of multi-agent plan repair to future work."
    } ],
    "references" : [ {
      "title" : "On the complexity of plan adaptation by derivational analogy in a universal classical planning framework",
      "author" : [ "T.C. Au", "H. Munoz-Avila" ],
      "venue" : "Advances in Case-Based Reasoning,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2002
    }, {
      "title" : "From one to many: Planning for loosely coupled multi-agent systems",
      "author" : [ "Ronen I. Brafman", "Carmel Domshlak" ],
      "venue" : "In Proc. of ICAPS,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2008
    }, {
      "title" : "STRIPS: A new approach to the application of theorem proving to problem solving",
      "author" : [ "R. Fikes", "N. Nilsson" ],
      "venue" : "In Proc. of the 2nd International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1971
    }, {
      "title" : "Plan stability: Replanning versus plan repair",
      "author" : [ "Maria Fox", "Alfonso Gerevini", "Derek Long", "Ivan Serina" ],
      "venue" : "In Proc. of ICAPS,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2006
    }, {
      "title" : "The FF planning system: Fast plan generation through heuristic search",
      "author" : [ "Jörg Hoffmann", "Bernhard Nebel" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2001
    }, {
      "title" : "Multi-agent plan repairing",
      "author" : [ "Antońın Komenda", "Peter Novák" ],
      "venue" : "Proceedings of Decision Making in Partially Observable, Uncertain Worlds: Exploring Insights from Multiple Communities,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2011
    }, {
      "title" : "Self-interested planning agents using plan repair",
      "author" : [ "Roman van der Krogt", "Mathijs de Weerdt" ],
      "venue" : "In Proceedings of the ICAPS 2005 Workshop on Multiagent Planning and Scheduling,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2005
    }, {
      "title" : "Monitoring the execution of partial-order plans via regression",
      "author" : [ "Christian Muise", "Sheila A McIlraith", "J Christopher Beck" ],
      "venue" : "In Proc. of 22nd International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2011
    }, {
      "title" : "Plan reuse versus plan generation: a theoretical and empirical analysis",
      "author" : [ "B. Nebel", "Koehler J" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1995
    }, {
      "title" : "A general, fully distributed multi-agent planning algorithm",
      "author" : [ "Raz Nissim", "Ronen I. Brafman", "Carmel Domshlak" ],
      "venue" : "In Proc. of AAMAS,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : ", [8, 2, 5]).",
      "startOffset" : 2,
      "endOffset" : 11
    }, {
      "referenceID" : 0,
      "context" : ", [8, 2, 5]).",
      "startOffset" : 2,
      "endOffset" : 11
    }, {
      "referenceID" : 3,
      "context" : ", [8, 2, 5]).",
      "startOffset" : 2,
      "endOffset" : 11
    }, {
      "referenceID" : 8,
      "context" : "However, in [10], Nebel and Koehler theoretically analyzed plan re-use (plan repair), and conclude that in general it does not bring any benefit over replanning in terms of computational time complexity.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 5,
      "context" : "Besides our preliminary approach in [7], this aspect of multi-agent planning and plan repair, while obviously important with arrival of truly distributed algorithms for multi-agent planning, such as the one by Nissim et al.",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 9,
      "context" : "[11] did not witness considerable attention of the community yet.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 1,
      "context" : "Firstly, after introducing the general problem of multi-agent planning stemming from the formulation due to Brafman and Domshlak [3] in Section 2, in subsequent Section 3 we formally introduce the multi-agent plan repair problem and formally state",
      "startOffset" : 129,
      "endOffset" : 132
    }, {
      "referenceID" : 1,
      "context" : "The above formulation of the multi-agent planning problem is well in line with the original formulation of MA-Strips planning due to Brafman and Domshlak [3].",
      "startOffset" : 154,
      "endOffset" : 157
    }, {
      "referenceID" : 1,
      "context" : "Algorithm 1 lists the original multi-agent planning algorithm MA-Plan by Brafman and Domshlak in [3].",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 9,
      "context" : "in [11] adapted the original blueprint algorithm described above to a distributed setting.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 9,
      "context" : "[11].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 4,
      "context" : "In the first phase, for a given domain a multi-agent plan was computed using the MA-Plan algorithm based on a distributed constraint satisfaction solver for computing the candidate coordination plans and implementation of a best-first-search action planning algorithm as part of FF [6] for computing the local, single-agent plans.",
      "startOffset" : 282,
      "endOffset" : 285
    }, {
      "referenceID" : 9,
      "context" : "also used for the experiments conducted in their paper [11].",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 9,
      "context" : "Similarly to [11], we chose domains, which are straightforwardly modifiable to multi-agent planning problems: logistics (3 agents), rovers (3 agents), and satellites (2–5 agents).",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 7,
      "context" : "The line of research underlying this paper well correlates with recent works on classical single-agent planning sub-domains, such as partial ordered plan monitoring and repairing [9], conformant and contingency planning, plan reuse and plan adaptation.",
      "startOffset" : 179,
      "endOffset" : 182
    }, {
      "referenceID" : 5,
      "context" : "Our own recent approach to the problem of multi-agent plan repair in [7] can be seen only as a precursor to the formal and rigorous treatment of the problem in this paper.",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 2,
      "context" : "Secondly, the framework is not able to describe concurrent resource consumption, which is not an issue in single-agent Strips [4] planning, but in the multi-agent extension two individual concurrently executed actions might “consume” the same precondition, even though it is undesirable in the domain.",
      "startOffset" : 126,
      "endOffset" : 129
    } ],
    "year" : 2013,
    "abstractText" : "Achieving joint objectives by teams of cooperative planning agents requires significant coordination and communication efforts. For a singleagent system facing a plan failure in a dynamic environment, arguably, attempts to repair the failed plan in general do not straightforwardly bring any benefit in terms of time complexity. However, in multi-agent settings the communication complexity might be of a much higher importance, possibly a high communication overhead might be even prohibitive in certain domains. We hypothesize that in decentralized systems, where coordination is enforced to achieve joint objectives, attempts to repair failed multi-agent plans should lead to lower communication overhead than replanning from scratch. The contribution of the presented paper is threefold. Firstly, we formally introduce the multi-agent plan repair problem and formally present the core hypothesis underlying our work. Secondly, we propose three algorithms for multi-agent plan repair reducing the problem to specialized instances of the multi-agent planning problem. Finally, we present results of experimental validation confirming the core hypothesis of the paper.",
    "creator" : "LaTeX with hyperref package"
  }
}