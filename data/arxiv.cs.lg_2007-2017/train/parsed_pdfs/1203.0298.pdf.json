{
  "name" : "1203.0298.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Application of Gist SVM in Cancer Detection",
    "authors" : [ "Mrs S. Aruna", "S. P. Rajagopalan" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "SVM in disease prediction (detection of cancer). Pattern classification problems can be effectively solved by Support vector machines. Here we propose a classifier which can differentiate patients having benign and malignant cancer cells. To improve the accuracy of classification, we propose to determine the optimal size of the training set and perform feature selection. To find the optimal size of the training set, different sizes of training sets are experimented and the one with highest classification rate is selected. The optimal features are selected through their F-Scores. KEYWORDS: Pattern classification, Gist SVM, F-Score\nIntroduction\nPattern classification problems can be effectively solved by Support Vector Machines (SVMs) [CV95; Vap98]. SVMs find applications in data mining, bioinformatics, computer vision, and pattern recognition. There is a need to accelerate SVM training as the size of the training data sets is becoming larger.\nIn this paper, we study the application of SVM in the prediction of\ncancer using the GIST SVM. SVM is a class of learning methods that can be used for classification. The problem of the optimal feature selection plays a vital role in SVM which ensures high performance classification. Nonoptimal features ensure robustness since SVMs are trained for maximized margins.\nThis paper is structured as follows. Section 1 gives a brief\nintroduction to SVM classifier and the f-score method used for feature selection, In Section 2, we discuss the results obtained and concluding remarks are given in Section 3 to address further research issues."
    }, {
      "heading" : "1. Materials and Methods",
      "text" : ""
    }, {
      "heading" : "1.1. Support Vector Machines",
      "text" : "The use of the maximum-margin hyper plane is motivated by\nVapnik Chervonenkis theory, which provides a probabilistic test error bound that is minimized when the margin is maximized.\nThe parameters of the maximum-margin hyper plane are derived by\nsolving a quadratic programming (QP) optimization problem. There exist several specialized algorithms for quickly solving the QP problem that arises from SVMs.\nIn training SVMs, the decision boundaries are determined directly\nfrom the training data so that the generalization ability is maximized. Therefore, the generalization ability of the SVM is quite different from those of other classifiers, especially when the number of training data is small. Here we propose to use Gist (http://svm.sdsc.edu) to perform all the training and testing. Gist is an implementation of the SVM algorithm.\nTo apply SVM in training and testing the practical data sets, one has\nto determine the size of the training set and the optimal features. We attempt to find the optimum size for the training set and the test set because the time complexity of training an SVM is in the order of n 2 , where n is the number of training samples [Pla98; Joa98].\nWe used Wisconsin breast cancer (WBC) dataset which has 699\ninstances (Benign: 458 Malignant: 241) of which 16 instances has missing attribute values removing that we have 683 instances of which 444 benign and 239 are malignant. This breast cancer database was obtained from the University of Wisconsin Hospitals, Madison from Dr. William H.Wolberg available in UCI Machine learning depository http://archive.ics.uci.edu/ml).\nFeatures are computed from a digitized image of a Fine Needle\nAspiration (FNA). The feature extraction process is performed as follows: An FNA is taken from the breast mass. This material is then mounted on a microscope slide and stained to highlight the cellular nuclei. A portion of the slide in which the cells are well differentiated is then scanned using a digital camera and a frame-grabber board and identified nine visually assessed characteristics of an FNA sample, which he considered relevant to diagnosis. The resulting data set is well-known as the Wisconsin Breast Cancer Data.\nMost breast cancers are detected by the patient as a lump in the\nbreast. The majority of breast lumps are benign so it is the physician’s responsibility to diagnose breast cancer, that is, to distinguish benign lumps from malignant ones. There are three available methods for diagnosing breast cancer: Mammography, FNA (with visual interpretation) and surgical biopsy. The reported sensitivity (i.e., ability to correctly diagnose cancer when the disease is present) of Mammography varies from 68% to 79%, of FNA with visual interpretation from 65% to 98%, and of surgical biopsy close to 100%. Therefore, mammography lacks sensitivity, FNA sensitivity varies widely, and surgical biopsy, although accurate, is invasive, time consuming, and costly. The goal of the diagnostic aspect of this research is to develop a relatively objective system."
    }, {
      "heading" : "1.2. The F-score Method",
      "text" : "The F-score is used to measure the discrimination of two sets of real numbers [Ak09]. Given training vectors xk; k = 1; 2; _ _ _; l, and the number of positive and negative instances are n + and n - respectively, then the F-score of the ith feature is defined as:\n(1)\nwhere , and are the average of the i th feature of the whole, positive and negative datasets respectively, and is the i th feature of the k th positive instance and i th feature of the k th negative instance.\nThe numerator of the above equation indicates the discrimination between the positive and negative sets and denominator the one within each subset. The larger the F-score is, the more likely this feature is discriminative. This score is hence used as a feature selection criterion.\nThe procedure is summarized as follows:"
    }, {
      "heading" : "1. Initialize the destination subset empty and source subset with all n features",
      "text" : ""
    }, {
      "heading" : "2. Calculate the f-score for each feature as described in equation 1.",
      "text" : ""
    }, {
      "heading" : "3. Add the feature to the destination subset.",
      "text" : ""
    }, {
      "heading" : "4. Go to step 2 until all features in the source subset have been processed.",
      "text" : ""
    }, {
      "heading" : "5. Sort the destination subset features in descending order of the f-score.",
      "text" : ""
    }, {
      "heading" : "6. Train the sample set with SVM leaving the features with least score one",
      "text" : "by one. 7. Go to step 6 until the sample set contains top 5 features."
    }, {
      "heading" : "2. Results",
      "text" : "Using gist SVM, one can get the results as in Table 2 and Table 3. From the results, we see that the gist SVM is able to train a classifier with weight and discriminant. Let D be the discriminant value. Then, according to the discriminant value D, we can classify the new data point into the positive class if D > 0, and classify it into the negative class when D < 0.\nTable 2. Results for Training set\nExample class weight train_classification train_discriminant\n1165926 1 0 1 4.226 1112209 1 0 1 3.922 1116116 1 0 1 3.724 1116998 1 0.003103 1 0.9802\n1113483 1 0.01813 1 0.8974 1102573 1 0.02076 1 0.8893 1133041 -1 -0.08992 -1 -0.5062 1121919 -1 -0.08375 -1 -0.5315 1143978 -1 -0.04916 -1 -0.7296 1105524 -1 -0 -1 -1.2\n1116192 -1 -0 -1 -1.2 1147044 -1 -0 -1 -1.228 1137156 -1 -0 -1 -1.274\nTable 3. Results for Test set\nExample classification discriminant\n1106829 1 1.81912 1112209 1 1.66274\n1033078 1 1.66228 1105257 1 1.47566 1054590 1 0.167441 1041801 1 0.00841774 1048672 -1 -0.110634 1072179 -1 -0.154081\n1049815 -1 -0.164895 1050718 -1 -0.204936"
    }, {
      "heading" : "3. Explanation for the results",
      "text" : "• Example: The name provided for the sample\n• Class (training results only): The class membership provided for the sample.\n• Weight (training results only): The 'importance' of this example in setting the location of the decision boundary (which is the maximum\nmargin hyper plane). Examples with non-zero weights are support vectors.\n• Train_classification (training results only) or classification: The predicted class of this example, or, for training, the location of this\nexample with respect to the decision boundary. In training, if it differs from the Class, a training error is counted.\n• Train_discriminant (training results only) or discriminant (test results): How far this example is from the decision boundary. Larger\nvalues correspond to greater 'certainty' that the sample belongs in the predicted class.\nTo have a better classification rate, we need to consider the size of\nthe training set and also the selection of optimal features. In other words, we need to find the optimal size of the training set and the features. In training a SVM, the decision boundaries are determined directly from the training data so that the generalization ability is maximized. Moreover, one needs to solve a quadratic optimization problem with the number of variables equal to the number of training data. We note that determining the optimal size of the training set is important in improving the classification rate. To find the optimal size of the training set, we will try different sizes of training sets and see which one can achieve the highest classification rate. Table 4 gives the classification rate when the number of training-test partitions is 50-50, 60-40, 70-30, 80-20 and 40-60 respectively. From Table 4, one can see that the best classification rate is reached when we choose 50-50 training -test partitions.\nTable 4. Results for the various training-test set partitions\nTraining-Test partitions Training set accuracy (%) Test set accuracy (%)\n50-50 100 96 60-40 98 92.5 70-30 89 87.6\n80-20 86 89.9 40-60 98 95\n75\n80\n85\n90\n95\n100\n105\nTraining set\naccuracy (%)\nTest set accuracy\n(%)\n50-50\n60-40\n70-30\n80-20\n40-60\nFigure 2. Bar chart for Table 4\nTo investigate potential predictive value of these candidates, we next\nrank these 9 features based on their scores generated using F-score. Then we train the SVM. Here we used 4 sample sets to train SVM (Set 1 contains datasets with features 1, 2,3,4,5,6,7,9, Set 2 contains datasets with features 1,2,3,4,6,7,9, Set 3 contains datasets with features 1, 2, 3,6,7,9 and Set 4 contains datasets with features 1, 3, 6, 7, 9). The results obtained are presented in Table 5. From the table, we see that the best classification rate is reached when we choose the top five features. 2,4,5,8 considered to be weak features according to the f-score. After removing those features one by one did not make much difference in the accuracy of classification of the samples.\nTable 5. Classification rate for various datasets\nSample set Training set accuracy (%) Testing set accuracy (%)\nSet 1 98.1 95.9 Set 2 98.6 96.1 Set 3 97.3 96.2 Set 4 96.2 96.4\nConclusion\nIn this study we used SVM based diagnostic model for the diagnosis of breast cancer using Wisconsin dataset. Here we trained the datasets in Gist. In this study we tried to find out the optimum size of the training test partitions and optimum number of features for the dataset using SVM and F-score combination. In future work we proposed to use various kernel functions with SVM for better accuracy and compare the studies with various machine learning tools."
    } ],
    "references" : [ {
      "title" : "Akay - Support vector machines combined with feature selection for breast cancer diagnosis",
      "author" : [ "F M." ],
      "venue" : "Expert Systems with Applications,",
      "citeRegEx" : "Aka09",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Text categorization with support vector machine",
      "author" : [ "T. Joachims" ],
      "venue" : "Proceedings of European Conference on Machine Learning (ECML),",
      "citeRegEx" : "Joachims,? \\Q1998\\E",
      "shortCiteRegEx" : "Joachims",
      "year" : 1998
    }, {
      "title" : "Platt - Fast training of support vector machines using Sequential minimal optimization",
      "author" : [ "C J." ],
      "venue" : "Advances in Kernel Methods: Support Vector Machines.Cambridge MA: MIT Press, December",
      "citeRegEx" : "Pla98",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Wiley",
      "author" : [ "V. Vapnik - Statistical Learning Theory" ],
      "venue" : "NewYork,",
      "citeRegEx" : "Vap98",
      "shortCiteRegEx" : null,
      "year" : 1998
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Pattern classification problems can be effectively solved by Support Vector Machines (SVMs) [CV95; Vap98].",
      "startOffset" : 92,
      "endOffset" : 105
    }, {
      "referenceID" : 2,
      "context" : "We attempt to find the optimum size for the training set and the test set because the time complexity of training an SVM is in the order of n 2 , where n is the number of training samples [Pla98; Joa98].",
      "startOffset" : 188,
      "endOffset" : 202
    } ],
    "year" : 2011,
    "abstractText" : "In this paper, we study the application of GIST SVM in disease prediction (detection of cancer). Pattern classification problems can be effectively solved by Support vector machines. Here we propose a classifier which can differentiate patients having benign and malignant cancer cells. To improve the accuracy of classification, we propose to determine the optimal size of the training set and perform feature selection. To find the optimal size of the training set, different sizes of training sets are experimented and the one with highest classification rate is selected. The optimal features are selected through their F-Scores.",
    "creator" : "PScript5.dll Version 5.2"
  }
}