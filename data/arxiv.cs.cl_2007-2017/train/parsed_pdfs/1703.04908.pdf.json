{
  "name" : "1703.04908.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Emergence of Grounded Compositional Language in Multi-Agent Populations",
    "authors" : [ "Igor Mordatch", "Pieter Abbeel" ],
    "emails" : [ "<mordatch@opeanai.com>." ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Development of agents that are capable of communication and flexible language use is one of the long-standing challenges facing the field of artificial intelligence. Agents need to develop communication if they are to successfully coordinate as a collective. Furthermore, agents will need some language capacity if they are to interact and productively collaborate with humans or make decisions that are interpretable by humans. If such a capacity were to arise artificially, it could also offer important insights into questions surrounding development of human language and cognition.\nBut if we wish to arrive at formation of communication from first principles, it must form out of necessity. The approaches that learn to plausibly imitate language from examples of human language, while tremendously useful, do not learn why language exists. Such supervised approaches\n1OpenAI 2UC Berkeley. Correspondence to: Igor Mordatch <mordatch@opeanai.com>.\ncan capture structural and statistical relationships in language, but they do not capture its functional aspects, or that language happens for purposes of successful coordination between humans. Evaluating success of such imitationbased approaches on the basis of linguistic plausibility also presents challenges of ambiguity and requirement of human involvement.\nRecently there has been a surge of renewed interest in the pragmatic aspects of language use and it is also the focus of our work. We adopt a view of (Gauthier & Mordatch, 2016) that an agent possesses an understanding of language when it can use language (along with other tools such as non-verbal communication or physical acts) to accomplish goals in its environment. This leads to evaluation criteria that can be measured precisely and without human involvement.\nIn this paper, we propose a physically-situated multi-agent learning environment and learning methods that bring about emergence of a basic compositional language. This language is represented as streams of abstract discrete symbols uttered by agents over time, but nonetheless has a coherent structure that possesses a defined vocabulary and syntax. The agents utter communication symbols alongside performing actions in the physical environment to cooperatively accomplish goals defined by a joint reward function shared between all agents. There are no pre-designed meanings associated with the uttered symbols - the agents form concepts relevant to the task and environment and assign arbitrary symbols to communicate them.\nThere are similarly no explicit language usage goals, such as making correct utterances, and no explicit roles agents are assigned, such as speaker or listener, or explicit turntaking dialogue structure as in traditional language games. There may be an arbitrary number of agents in a population communicating at the same time and part of the difficulty is learning to refer specific agents. A population of agents is situated as moving particles in a continuous two-dimensional environment, possessing properties such as color and shape. The goals of the population are based on non-linguistic objectives, such as moving to a location and language arises from the need to coordinate on those goals. We do not rely on any supervision such as human demonstrations or text corpora.\nar X\niv :1\n70 3.\n04 90\n8v 1\n[ cs\n.A I]\n1 5\nM ar\n2 01\n7\nSimilar to recent work,we formulate the discovery the action and communication protocols for our agents jointly as a reinforcement learning problem. Agents perform physical actions and communication utterances according to an identical policy that is instantiated for all agents and fully determines the action and communication protocols. The policies are based on neural network models with an architecture composed of dynamically-instantiated recurrent modules. This allows decentralized execution with a variable number of agents and communication streams. The joint dynamics of all agents and environment, including discrete communication streams are fully-differentiable, the agents’ policy is trained end-to-end with backpropagation through time.\nThe languages we observe forming exhibit interpretable compositional structure that in general assigns symbols to separately refer to environment landmarks, action verbs, and agents. However, environment variation leads to a number of specialized languages, omitting words that are clear from context. For example, when there is only one type of action to take or one landmark to go to, words for those concepts do not form in the language. Considerations of the physical environment also have an impact on language structure. For example, a symbol denoting go action is typically uttered first because the listener can start moving before even hearing the destination. This effect only arises when linguistic and physical behaviors are treated jointly and not in isolation.\nThe presence of a physical environment also allows for alternative strategies aside from language use to accomplish goals. A visual sensory modality provides an alternative medium for communication and we observe emergence of non-verbal communication such as pointing and guiding when language communication is unavailable. When even non-verbal communication is unavailable, strategies such as direct pushing may be employed to succeed at the task. It is important to us to build an environment with a diverse set of capabilities which language use develops alongside with.\nOur work offers insights into why compositional structure emerges in our formed languages in the first place. In part, we find composition to emerge when we explicitly encourage active vocabulary sizes to be small through a soft penalty. This is consistent with analysis in evolutionary linguistics (Nowak et al., 2000) that finds composition to emerge only when number of concepts to be expressed becomes greater than a factor of agent’s symbol vocabulary capacity. Another important component leading to composition is training on a variety of tasks and environment configurations simultaneously. Training on cases where most information is clear from context (such as when there is only one landmark) leads to formation of atomic concepts\nthat are reused compositionally in more complicated cases. Further investigation is required to determine whether this is an artifact specific to our training setup, or a more fundamental requirement for compositional syntax formation."
    }, {
      "heading" : "2. Related Work",
      "text" : "Recent years have seen substantial progress in practical natural language applications such as machine translation (Sutskever et al., 2014; Bahdanau et al., 2014), sentiment analysis (Socher et al., 2013), document summarization (Durrett et al., 2016), and domain-specific dialogue (Dhingra et al., 2016). Much of this success is a result of intelligently designed statistical models trained on large static datasets. However, such approaches do not produce an understanding of language that can lead to productive cooperation with humans.\nAn interest in pragmatic view of language understanding has been longstanding (Austin, 1962; Grice, 1975) and has recently argued for in (Gauthier & Mordatch, 2016; Lake et al., 2016; Lazaridou et al., 2016b). Pragmatic language use has been proposed in the context of two-player reference games (Golland et al., 2010; Vogel et al., 2014; Andreas & Klein, 2016) focusing on the task of identifying object references through a learned language. (Winograd, 1973; Wang et al., 2016) ground language in a physical environment and focusing on language interaction with humans for completion of tasks in the physical environment. In such a pragmatic setting, language use for communication of spatial concepts has received particular attention in (Steels, 1995; Ullman et al., 2016).\nAside from producing agents that can interact with humans through language, research in pragmatic language understanding can be informative to the fields of linguistics and cognitive science. Of particular interest in these fields has been the question of how syntax and compositional structure in language emerged, and why it is largely unique to human languages (Kirby, 1999; Nowak et al., 2000; Steels, 2005). Models such as Rational Speech Acts (Frank & Goodman, 2012) and Iterated Learning (Kirby et al., 2014) have been popular in cognitive science and evolutionary linguistics, but such approaches tend to rely on pre-specified procedures or models that limit their generality.\nThe recent work that is most similar to ours is the application of reinforcement learning approaches towards the purposes of learning a communication protocol, as exemplified by (Foerster et al., 2016; Sukhbaatar et al., 2016; Lazaridou et al., 2016a)."
    }, {
      "heading" : "3. Problem Formulation",
      "text" : "The setting we are considering is a cooperative partially observable Markov game (Littman, 1994), which is a multiagent extension of a Markov decision process. A Markov game for N agents is defined by set of states S describing the possible configurations of all agents, a set of actions A1, ...,AN and a set of observations O1, ...,ON for each agent. Initial states are determined by a distribution ρ : S 7→ [0, 1]. State transitions are determined by a function T : S ×A1× ...×AN 7→ S. For each agent i, rewards are given by function ri : S ×Ai 7→ R, observations are given by function oi : S 7→ Oi. To choose actions, each agent i uses a stochastic policy πi : Oi ×Ai 7→ [0, 1].\nIn this work, we assume all agents have identical action and observation spaces, and all agents act according to the same policy π. We consider a finite horizon setting, with episode length T . In a cooperative setting, the problem is to find a policy that maximizes the expected shared return for all agents:\nmax π R(π), where (1)\nR(π) = E [ T∑ t=0 N∑ i=0 r(sti,a t i) ] (2)\nThe cooperative setting allows us to pose the problem as a joint minimization across all agents, as opposed to minimization-maximization problems resulting from competitive settings."
    }, {
      "heading" : "4. Grounded Communication Environment",
      "text" : "As argued in the introduction, grounding multi-agent communication in a physical environment is crucial for interesting communication behaviors to emerge. In this work, we consider a physically-simulated two-dimensional environment in continuous space and discrete time. This environment consists of N agents and M landmarks. Both agent and landmark entities inhabit a physical location in space p and posses descriptive physical characteristics, such as color and shape type. In addition, agents can direct their gaze to a location v. See Figure 1 for an example of environments we consider. Agents can act to move in the environment and direct their gaze, but may also be affected by physical interactions with other agents. We denote the physical state of an entity (including descriptive characteristics) by x and describe its precise details and transition dynamics in the Appendix.\nIn addition to performing physical actions, agents utter verbal communication symbols c at every timestep. These utterances are discrete elements of an abstract symbol vocabulary C of size K. We do not assign any significance or meaning to these symbols. They are treated as abstract\n..\n..\nπ\nπ\n.. ..\n.. .. .. .. .. θ\ncategorical variables that are emitted by each agent and observed by all other agents. It is up to agents at training time to assign meaning to these symbols. As shown in Section 7, these symbols become assigned to interpretable concepts. Agents may also choose not to utter anything at a given timestep, and there is a cost to making an utterance, loosely representing the metabolic effort of vocalization. We denote a vector representing one-hot encoding of symbol c with boldface c.\nEach agent has internal goals specified by vector g that are private and not observed by other agents. These goals are grounded in the physical environment and include tasks such as moving to or gazing at a location. These goals may involve other agents (requiring the other agent to move to a location, for example) but are not observed by them and thus necessitate coordination and communication between agents. Verbal utterances are one tool which the agents can use to cooperatively accomplish all goals, but we also observe emergent use of non-verbal signals and altogether non-communicative strategies.\nTo aid in accomplishing goals, each agent has internal recurrent memory bank m that is also private and not observed by other agents. This memory bank has no predesigned behavior and it is up to the agents to learn to utilize it appropriately.\nThe full state of the environment is given by s = [ x1,...,(N+M) c1,...,N m1,...,N g1,...,N ] ∈ S\nEach agent observes physical states of all entities in the environment, verbal utterances of all agents, and its own private memory and goal vector. The observation for agent i is\noi(s) = [ ix1,...,(N+M) c1,...,N mi gi ]\nWhere ixj is the observation of entity j’s physical state in agent i’s reference frame. More intricate observation models are possible, such as physical observations solely from pixels or verbal observations from a single input channel. These models would require agents learning to perform visual processing and source separation, which are orthogonal to this work. Despite the dimensionality of observations varying with the number of physical entities and communication streams, our policy architecture as described in Section 5.2 allows a single parameterization across these variations."
    }, {
      "heading" : "5. Policy Learning with Backpropagation",
      "text" : "Each agent acts by sampling actions from a stochastic policy π, which is identical for all agents and defined by parameters θ. There are several common options for finding optimal policy parameters. The model-free framework of Q-learning can be used to find the optimal state-action value function, and employ a policy that acts greedily to according to the value function. Unfortunately, Q function dimensionality scales quadratically with communication vocabulary size, which can quickly become intractably large. Alternatively it is possible to directly learn a policy function using model-free policy gradient methods, which use sampling to estimate the gradient of policy return dRdθ . The gradient estimates from these methods can exhibit very high variance and credit assignment becomes an especially difficult problem in the presence of sequential communication actions.\nInstead of using model-free reinforcement learning methods, we build an end-to-end differentiable model of all agent and environment state dynamics over time and calculate dRdθ with backpropagation. At every optimization iteration, we sample a new batch of 1024 random environment instantiations and backpropagate their dynamics through time to calculate the total return gradient. Figure 2 shows the dependency chain between two timesteps. A similar approach was employed by (Foerster et al., 2016; Sukhbaatar et al., 2016) to compute gradients for communication actions, although the latter still employed modelfree methods for physical action computation.\nThe physical state dynamics, including discontinuous contact events can be made differentiable with smoothing. However, communication actions require emission of discrete symbols, which present difficulties for backpropagation."
    }, {
      "heading" : "5.1. Discrete Communication and Gumbel-Softmax Estimator",
      "text" : "In order to use categorical communication emissions c in our setting, it must be possible to differentiate through\nthem. There has been a wealth of work in machine learning on differentiable models with discrete variables, but we found recent approach in (Jang et al., 2016; Maddison et al., 2016) to be particularly effective in our setting. The approach proposes a Gumbel-Softmax distribution, which is a continuous relaxation of a discrete categorical distribution. Given K-categorical distribution parameters p, a differentiable K-dimensional one-hot encoding sample G from the Gumbel-Softmax distribution can be calculated as:\nG(logp)k = exp((logpk + ε)/τ)∑K j=0 exp((logpj + ε)/τ)\nWhere ε are i.i.d. samples from Gumbel(0, 1) distribution,\nε = −log(−log(u)), u ∼ U [0, 1]\nAnd where τ is a softmax temperature parameter. We did not find it necessary to anneal the temperature and set it to 1 in all our experiments for training and sample directly from the categorical distribution at test time. To emit a communication symbol, our policy is trained to directly output logp ∈ RK , which is transformed to a symbol emission sample c ∼ G(logp). The resulting gradient can be estimated as dcdθ = dG dp dp dθ ."
    }, {
      "heading" : "5.2. Policy Architecture",
      "text" : "The policy class we consider in this work are stochastic neural networks. The policy outputs samples of an agent’s physical actions u, communication symbol utterance c, and internal memory updates ∆m. The policy must consolidate multiple incoming communication symbol streams emitted by other agents, as well as incoming observations of physical entities. Importantly, the number of agents (and thus the\nEmergence of Grounded Compositional Language in Multi-Agent Populations\nnumber of communication streams) and number of physical entities can vary between environment instantiations. To support this, the policy instantiates a collection of identical processing modules for each communication stream and each observed physical entity. Each processing module is a fully-connected multi-layer perceptron. The weights between all communication processing modules are shared (and similarly for all physical observation modules). The outputs of individual processing modules are pooled with a softmax operation into feature vectors φc and φx for communication and physical observation streams, respectively. Such weight sharing and pooling makes it possible to apply the same policy parameters to any number of communication and physical observations. We found that feeding φc as an additional input to each physical observation processing module can help training, but is not critical and is omitted for simplicity.\nThe pooled features and agent’s private goal vector are passed to the final processing module that outputs distribution parameters [ ψu ψc ] from which action samples are generated as u = ψu + ε and c ∼ G(ψc), where ε is a zero-mean Gaussian noise.\nUnlike communication games where agents only emit a single utterance, our agents continually emit a stream of symbols over time. Thus processing modules that read and write communication utterance streams benefit greatly from recurrent memory that can capture meaning of a stream over time. To this end, we augment each communication processing and output module with an independent internal memory state m, and each module outputs memory state updates ∆m. In this work we use simple additive memory updates for simplicity and interpretability, but other memory architectures such LSTMs can be used.\nmt = tanh(mt−1 + ∆mt−1 + ε)\nWe build all fully-connected modules with 256 hidden units and 2 layers each in all our experiments, using exponentiallinear units and dropout with a rate of 0.1 between all hidden layers. Size is feature vectors φ is 256 and size of each memory module is 32. The overall policy architecture is shown in Figure 3."
    }, {
      "heading" : "5.3. Auxiliary Prediction Reward",
      "text" : "To help policy training avoid local minima, we found it helpful to include auxiliary goal prediction tasks, similar to recent work in reinforcement learning (Dosovitskiy & Koltun, 2016; Silver et al., 2016). In agent i’s policy, each communication processing module j additionally outputs a prediction ĝi,j of agent j’s goals. At the end of the episode, we add a reward for predicting other agent’s goals, which in turn encourages communication utterances that convey the agent’s goals clearly to other agents. Across all agents\n.. ..\n.. ..\nm 1\nm N\nx\nc\nx\na\nC\na\ng\npool\npool\nFC C\nFC C\nFC X\nFC X\nFC a\nm 0\nN+M\nx\nc\nc\nN\n1\n1\no\nc\nu\na\nFigure 3. Overview of our policy architecture, mapping observations to actions at every point time time. FC indicates a fullyconnected processing module that shares weights with all others of its label. pool indicates a softmax pooling layer.\nthis reward has the form rg = − ∑\n{i,j|i6=j}\n‖ĝTi,j − gTj ‖2\nNote that we do not use ĝ as an input in calculating actions. It is only used for the purposes of auxiliary prediction task."
    }, {
      "heading" : "6. Compositionality and Vocabulary Size",
      "text" : "What leads to compositional syntax formation? One known constructive hypothesis requires modeling the process of language transmission and acquisition from one generation of agents to the next iteratively as in (Kirby et al., 2014). It such iterated learning setting, compositionality emerges due to poverty of stimulus - one generation will only observe a limited number of symbol utterances from the previous generation and must infer meaning of unseen symbols. This approach requires modeling language acquisition between agents, but when implemented with predesigned rules was shown over multiple iterations between generations to lead to formation of a compositional vocabulary.\nAlternatively, (Nowak et al., 2000) observed that emergence of compositionality requires the number of concepts describable by a language to be above a factor of vocabulary size. In our preliminary environments the number of concepts to communicate is still fairly small and is within the capacity of a non-compositional language. We use a maximum vocabulary size K = 20 in all our experiments. We tested a smaller maximum vocabulary size, but found that policy optimization became stuck in a poor local minima where concepts became conflated. Instead, we propose to use a large vocabulary size limit but use a soft penalty\nfunction to prevent the formation of unnecessarily large vocabularies. This allows the intermediate stages of policy optimization to explore large vocabularies, but then converge on an appropriate active vocabulary size. As shown in Figure 6, this is indeed what happens.\nHow do we penalize large vocabulary sizes? (Nowak et al., 2000) proposed a word population dynamics model that defines reproductive ratios of words to be proportional to their frequency, making already popular words more likely to survive. Inspired by these rich-get-richer dynamics, we model the communication symbols as being generated from a Dirichlet Process (Teh, 2011). Each communication symbol has a probability of being symbol ck as\np(ck) = nk\nα+ n− 1 Where nk is the number of times symbol ck has been uttered and n is the total number of symbols uttered. These counts are accumulated over agents, timesteps, and batch entries. α is a Dirichlet Process hyperparameter corresponding to the probability of observing an out-ofvocabulary word. The resulting reward across all agents is the log-likelihood of all communication utterances to independently have been generated by a Dirichlet Process:\nrc = ∑ i,t,k 1[cti = ck]logp(ck)\nMaximizing this reward leads to consolidation of symbols and the formation of compositionality. This approach is similar to encouraging code population sparsity in autoencoders (Ng, 2011), which was shown to give rise to compositional representations for images.\n7. Experiments1\nWe experimentally investigate how variation in goals, environment configuration, and agents physical capabilities lead to different communication strategies.\nIn this work, we consider three types of actions an agent needs to perform, go to location, look at location, and do nothing. Goal for agent i consists of an action to perform, a location to perform it on r̄, and an agent r that should perform that action. These goal properties are accumulated into goal description vector g. These goals are private to each agent, but may involve other agents. For example, agent i may want agent r to go to location r̄. This goal is not observed by agent r, and requires communication between agents i and r. The goals are assigned to agents such that no agent receives conflicting goals. We do however show generalization in the presence of conflicting goals in Section 7.3.\n1 Videos of our experimental results can be viewed at https://sites.google.com/site/multiagentlanguage/\nAgents have different reference frames and can only communicate in discrete symbols, and so cannot directly send goal position vector. Even if they could, agents observe the environment in different reference frames and have no shared global positioning reference. What makes the task possible is that we place goal locations r̄ on landmark locations of which are observed by all agents (in their independent reference frames). The strategy then is for agent i to unambiguously communicate landmark reference to agent r. Importantly, we do not provide explicit association between goal positions and landmark reference. It is up to the agents to learn to associate a position vector with a set of landmark properties and communicate them with discrete symbols.\nIn the results that follow, agents do not observe other agents. This disallows capacity for non-verbal communication, necessitating the use of language. In section 7.4 we report what happens when agents are able to observe each other and capacity for non-verbal communication is available.\nDespite training with continuous relaxation of the categorical distribution, we observe very similar reward performance at test time. No communication is provided as a baseline (again, non-verbal communication is not possible). The no-communication strategy is for all agents go towards the centroid of all landmarks."
    }, {
      "heading" : "7.1. Syntactic Structure",
      "text" : "We observe a compositional syntactic structure emerging in the stream of symbol uttered by agents. When trained on environments with only two agents, but multiple landmarks and actions, we observe symbols forming for each of the landmark colors and each of the action types. A typical conversation and physical agent configuration is shown in first row of Figure 4 and is as follows:\nGreen Agent: GOTO, GREEN, ... Blue Agent: GOTO, BLUE, ...\nThe labels for abstract symbols are chosen by us purely for interpretability and visualization and carry no meaning for training.\nPhysical environment considerations play a part in the syntactic structure. The action type verb GOTO is uttered first because actions take time to accomplish in the grounded environment. When the agent receives GOTO symbol it starts moving toward the centroid of all the landmarks (to\nbe equidistant from all of them) and then moves towards the specific landmark when it receives its color identity.\nWhen the environment configuration can contain more than three agents, agents need to form symbols for referring to each other. Three new symbols form to refer to agent colors that are separate in meaning from landmark colors. The typical conversations are shown in second and third rows of Figure 4.\nRed Agent: GOTO, RED, BLUE-AGENT, ... Green Agent: ..., ..., ..., ... Blue Agent: RED-AGENT, GREEN, LOOKAT, ...\nAgents may not omit any utterances when they are the subject of their private goal, in which case they have access to that information and have no need to announce it. In this language, there is no set ordering to word utterances. Each symbol contributes to sentence meaning independently, similar to case marking grammatical strategies used in many human languages (Beuls & Steels, 2013).\nThe agents largely settle on using a consistent set of symbols for each meaning, due to vocabulary size penalties and that discourage synonyms. We show the aggregate streams of communication utterances in Figure 5.\nIn simplified environment configurations when there is only one landmark or one type of action to take, no sym-\nbols are formed to refer to those concepts because they are clear from context."
    }, {
      "heading" : "7.2. Symbol Vocabulary Usage",
      "text" : "We find word activation counts to settle on the appropriate compositional word counts. That early during training large vocabulary sizes are being taken advantage of to explore the space of communication possibilities before settling on the appropriate effective vocabulary sizes as shown in Figure 6. In this figure, 1x1x3 case refers to environment with two agents and a single action, which requires only communicating one of three landmark identities. 1x2x3 contains two types of actions, and 3x3x3 case contains three agents that require explicit referencing."
    }, {
      "heading" : "7.3. Generalization to Unseen Configurations",
      "text" : "One of the advantages of decentralised execution policies is that trained agents can be placed into arbitrarily-sized groups and still function reasonably. When there are additional agents in the environment with the same color identity, all agents of the same color will perform the same task if they are being referred to. Additionally, when agents of a particular color are asked to perform two conflicting tasks (such as being asked go to two different landmarks by two\ndifferent agents), they will perform the average of the conflicting goals assigned to them. Such cases occur despite never having been seen during training.\nDue to the modularized observation architecture, the number of landmarks in the environment can also vary between training and execution. The agents perform sensible behaviors with different numbers of landmarks, despite not being trained in such environments. For example, when there are distractor landmarks of novel colors, the agents never go towards them. When there are multiple landmarks of the same color, the agent communicating the goal still utters landmark color (because the goal is the position of one of the landmarks). However, the agents receiving the landmark color utterance go towards the centroid of all landmark of the same color, showing a very sensible generalization strategy. An example of such case is shown in fourth row of Figure 4."
    }, {
      "heading" : "7.4. Non-verbal Communication and Other Strategies",
      "text" : "The presence of a physical environment also allows for alternative strategies aside from language use to accomplish goals. In this set of experiments we enable agents to observe other agents’ position and gaze location, and in turn disable communication capability via symbol utterances. When agents can observe each other’s gaze, a pointing strategy forms where the agent can communicate a landmark location by gazing in its direction, which the recipient correctly interprets and moves towards. When gazes of other agents cannot be observed, we see behavior of goal sender agent moving towards the location assigned to goal recipient agent (despite receiving no explicit reward for doing so), in order to guide the goal recipient to that location. Lastly, when neither visual not verbal observation is available on part of the goal recipient, we observe the behavior of goal sender directly pushing the recipient to the target location. Examples of such strategies are shown in Figure 7. It is important to us to build an environment with a diverse set of capabilities which language use develops alongside with."
    }, {
      "heading" : "8. Conclusion",
      "text" : "We have presented a multi-agent environment and learning methods that brings about emergence of an abstract compositional language from grounded experience. This abstract language is formed without any exposure to human language use. We investigated how variation in environment configuration and physical capabilities of agents affect the communication strategies that arise.\nIn the future, we would like experiment with larger number of actions that necessitate more complex syntax and larger vocabularies. We would also like integrate exposure to human language to form communication strategies that are compatible with human use."
    }, {
      "heading" : "8.1. Physical State and Dynamics",
      "text" : "The physical state of the agent is specified by x =[ p ṗ v d ] where ṗ is the velocity of p. d ∈ R3 is the color associted with the agent. Landmarks have similar state, but without gaze and velocity components. The physical state transition dynamics for a single agent are given by:\nxti = pṗ v t i =  p + ṗ∆tγṗ + (up + f(x1, ...,xN ))∆t uv t−1 i\nWhere f(x1, ...,xN ) are the physical interaction forces (such as collision) between all agents in the environment and any obstacles, ∆t is the simulation timestep (we use 0.1), and (1− γ) is a damping coefficient (we use 0.5)."
    }, {
      "heading" : "8.2. Goal Specification and Reward",
      "text" : "Goal for agent i consists of an action to perform, a location to perform it on r̄, and an agent r that should perform that action. Action type ∈ {go-to, look-at, do-nothing} is encoded as a one-hot vector gtype ∈ R3. These goal properties are accumulated into goal description vector g. The physical reward associated with goal gi for agent i at time t is:\nrti = −( ‖ptr − r̄‖2‖vtr − r̄‖2 0 ᵀ gtype + ‖uti‖2 + ‖cti‖2) The total return for the episode is\nR = rc + rg + ∑ t ∑ i rti"
    } ],
    "references" : [ {
      "title" : "Reasoning about pragmatics with neural listeners and speakers",
      "author" : [ "Andreas", "Jacob", "Klein", "Dan" ],
      "venue" : "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Andreas et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Andreas et al\\.",
      "year" : 2016
    }, {
      "title" : "How to Do Things with Words",
      "author" : [ "J.L. Austin" ],
      "venue" : null,
      "citeRegEx" : "Austin,? \\Q1962\\E",
      "shortCiteRegEx" : "Austin",
      "year" : 1962
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua" ],
      "venue" : "arXiv preprint arXiv:1409.0473,",
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Agent-based models of strategies for the emergence and evolution of grammatical agreement",
      "author" : [ "Beuls", "Katrien", "Steels", "Luc" ],
      "venue" : "PloS one,",
      "citeRegEx" : "Beuls et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Beuls et al\\.",
      "year" : 2013
    }, {
      "title" : "Endto-End Reinforcement Learning of Dialogue Agents for Information Access",
      "author" : [ "Dhingra", "Bhuwan", "Li", "Lihong", "Xiujun", "Gao", "Jianfeng", "Chen", "Yun-Nung", "Ahmed", "Faisal", "Deng" ],
      "venue" : "[cs],",
      "citeRegEx" : "Dhingra et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Dhingra et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning to act by predicting the future",
      "author" : [ "Dosovitskiy", "Alexey", "Koltun", "Vladlen" ],
      "venue" : "arXiv preprint arXiv:1611.01779,",
      "citeRegEx" : "Dosovitskiy et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Dosovitskiy et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning-based single-document summarization with compression and anaphoricity constraints",
      "author" : [ "Durrett", "Greg", "Berg-Kirkpatrick", "Taylor", "Klein", "Dan" ],
      "venue" : "arXiv preprint arXiv:1603.08887,",
      "citeRegEx" : "Durrett et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Durrett et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning to Communicate with Deep Multi-Agent Reinforcement Learning",
      "author" : [ "Foerster", "Jakob N", "Assael", "Yannis M", "de Freitas", "Nando", "Whiteson", "Shimon" ],
      "venue" : null,
      "citeRegEx" : "Foerster et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Foerster et al\\.",
      "year" : 2016
    }, {
      "title" : "Predicting Pragmatic Reasoning in Language",
      "author" : [ "Frank", "Michael C", "Goodman", "Noah D" ],
      "venue" : "Games. Science,",
      "citeRegEx" : "Frank et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Frank et al\\.",
      "year" : 2012
    }, {
      "title" : "A paradigm for situated and goal-driven language learning",
      "author" : [ "Gauthier", "Jon", "Mordatch", "Igor" ],
      "venue" : null,
      "citeRegEx" : "Gauthier et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Gauthier et al\\.",
      "year" : 2016
    }, {
      "title" : "A gametheoretic approach to generating spatial descriptions",
      "author" : [ "Golland", "Dave", "Liang", "Percy", "Klein", "Dan" ],
      "venue" : "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Golland et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Golland et al\\.",
      "year" : 2010
    }, {
      "title" : "Logic and conversation",
      "author" : [ "H.P. Grice" ],
      "venue" : "Syntax and Semantics:",
      "citeRegEx" : "Grice,? \\Q1975\\E",
      "shortCiteRegEx" : "Grice",
      "year" : 1975
    }, {
      "title" : "Categorical Reparameterization with Gumbel-Softmax",
      "author" : [ "E. Jang", "S. Gu", "B. Poole" ],
      "venue" : "ArXiv e-prints,",
      "citeRegEx" : "Jang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Jang et al\\.",
      "year" : 2016
    }, {
      "title" : "Syntax out of Learning: the cultural evolution of structured communication in a population of induction algorithms",
      "author" : [ "Kirby", "Simon" ],
      "venue" : null,
      "citeRegEx" : "Kirby and Simon.,? \\Q1999\\E",
      "shortCiteRegEx" : "Kirby and Simon.",
      "year" : 1999
    }, {
      "title" : "Iterated learning and the evolution of language",
      "author" : [ "Kirby", "Simon", "Griffiths", "Tom", "Smith", "Kenny" ],
      "venue" : "Current opinion in neurobiology,",
      "citeRegEx" : "Kirby et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kirby et al\\.",
      "year" : 2014
    }, {
      "title" : "Building machines that learn and think like people",
      "author" : [ "Lake", "Brenden M", "Ullman", "Tomer D", "Tenenbaum", "Joshua B", "Gershman", "Samuel J" ],
      "venue" : null,
      "citeRegEx" : "Lake et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lake et al\\.",
      "year" : 2016
    }, {
      "title" : "Multi-agent cooperation and the emergence of (natural) language",
      "author" : [ "Lazaridou", "Angeliki", "Peysakhovich", "Alexander", "Baroni", "Marco" ],
      "venue" : "arXiv preprint arXiv:1612.07182,",
      "citeRegEx" : "Lazaridou et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lazaridou et al\\.",
      "year" : 2016
    }, {
      "title" : "Towards Multi-Agent Communication-Based Language Learning",
      "author" : [ "Lazaridou", "Angeliki", "Pham", "Nghia The", "Baroni", "Marco" ],
      "venue" : "May 2016b",
      "citeRegEx" : "Lazaridou et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lazaridou et al\\.",
      "year" : 2016
    }, {
      "title" : "Markov games as a framework for multi-agent reinforcement learning",
      "author" : [ "Littman", "Michael L" ],
      "venue" : "In Proceedings of the eleventh international conference on machine learning,",
      "citeRegEx" : "Littman and L.,? \\Q1994\\E",
      "shortCiteRegEx" : "Littman and L.",
      "year" : 1994
    }, {
      "title" : "The concrete distribution: A continuous relaxation of discrete random variables",
      "author" : [ "Maddison", "Chris J", "Mnih", "Andriy", "Teh", "Yee Whye" ],
      "venue" : null,
      "citeRegEx" : "Maddison et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Maddison et al\\.",
      "year" : 2016
    }, {
      "title" : "Sparse autoencoder",
      "author" : [ "Ng", "Andrew" ],
      "venue" : "CS294A Lecture notes,",
      "citeRegEx" : "Ng and Andrew.,? \\Q2011\\E",
      "shortCiteRegEx" : "Ng and Andrew.",
      "year" : 2011
    }, {
      "title" : "The predictron: End-to-end learning and planning",
      "author" : [ "Silver", "David", "van Hasselt", "Hado", "Hessel", "Matteo", "Schaul", "Tom", "Guez", "Arthur", "Harley", "Tim", "Dulac-Arnold", "Gabriel", "Reichert", "Rabinowitz", "Neil", "Barreto", "Andre" ],
      "venue" : "arXiv preprint arXiv:1612.08810,",
      "citeRegEx" : "Silver et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Silver et al\\.",
      "year" : 2016
    }, {
      "title" : "A self-organizing spatial vocabulary",
      "author" : [ "Steels", "Luc" ],
      "venue" : "Artif. Life,",
      "citeRegEx" : "Steels and Luc.,? \\Q1995\\E",
      "shortCiteRegEx" : "Steels and Luc.",
      "year" : 1995
    }, {
      "title" : "What triggers the emergence of grammar? In AISB’05",
      "author" : [ "Steels", "Luc" ],
      "venue" : "Proceedings of the Second International Symposium on the Emergence and Evolution of Linguistic Communication",
      "citeRegEx" : "Steels and Luc.,? \\Q2005\\E",
      "shortCiteRegEx" : "Steels and Luc.",
      "year" : 2005
    }, {
      "title" : "Learning multiagent communication with backpropagation",
      "author" : [ "Sukhbaatar", "Sainbayar", "Szlam", "Arthur", "Fergus", "Rob" ],
      "venue" : "In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems",
      "citeRegEx" : "Sukhbaatar et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Sukhbaatar et al\\.",
      "year" : 2016
    }, {
      "title" : "Dirichlet process",
      "author" : [ "Teh", "Yee Whye" ],
      "venue" : "In Encyclopedia of machine learning,",
      "citeRegEx" : "Teh and Whye.,? \\Q2011\\E",
      "shortCiteRegEx" : "Teh and Whye.",
      "year" : 2011
    }, {
      "title" : "The pragmatics of spatial language",
      "author" : [ "Ullman", "Tomer", "Xu", "Yang", "Goodman", "Noah" ],
      "venue" : "In Proceedings of the Cognitive Science Society,",
      "citeRegEx" : "Ullman et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ullman et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning language games through interaction",
      "author" : [ "S.I. Wang", "P. Liang", "C. Manning" ],
      "venue" : "In Association for Computational Linguistics (ACL),",
      "citeRegEx" : "Wang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "A procedural model of language understanding",
      "author" : [ "Winograd", "Terry" ],
      "venue" : null,
      "citeRegEx" : "Winograd and Terry.,? \\Q1973\\E",
      "shortCiteRegEx" : "Winograd and Terry.",
      "year" : 1973
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "Recent years have seen substantial progress in practical natural language applications such as machine translation (Sutskever et al., 2014; Bahdanau et al., 2014), sentiment analysis (Socher et al.",
      "startOffset" : 115,
      "endOffset" : 162
    }, {
      "referenceID" : 6,
      "context" : ", 2013), document summarization (Durrett et al., 2016), and domain-specific dialogue (Dhingra et al.",
      "startOffset" : 32,
      "endOffset" : 54
    }, {
      "referenceID" : 4,
      "context" : ", 2016), and domain-specific dialogue (Dhingra et al., 2016).",
      "startOffset" : 38,
      "endOffset" : 60
    }, {
      "referenceID" : 1,
      "context" : "An interest in pragmatic view of language understanding has been longstanding (Austin, 1962; Grice, 1975) and has recently argued for in (Gauthier & Mordatch, 2016; Lake et al.",
      "startOffset" : 78,
      "endOffset" : 105
    }, {
      "referenceID" : 11,
      "context" : "An interest in pragmatic view of language understanding has been longstanding (Austin, 1962; Grice, 1975) and has recently argued for in (Gauthier & Mordatch, 2016; Lake et al.",
      "startOffset" : 78,
      "endOffset" : 105
    }, {
      "referenceID" : 15,
      "context" : "An interest in pragmatic view of language understanding has been longstanding (Austin, 1962; Grice, 1975) and has recently argued for in (Gauthier & Mordatch, 2016; Lake et al., 2016; Lazaridou et al., 2016b).",
      "startOffset" : 137,
      "endOffset" : 208
    }, {
      "referenceID" : 10,
      "context" : "Pragmatic language use has been proposed in the context of two-player reference games (Golland et al., 2010; Vogel et al., 2014; Andreas & Klein, 2016) focusing on the task of identifying object references through a learned language.",
      "startOffset" : 86,
      "endOffset" : 151
    }, {
      "referenceID" : 27,
      "context" : "(Winograd, 1973; Wang et al., 2016) ground language in a physical environment and focusing on language interaction with humans for completion of tasks in the physical environment.",
      "startOffset" : 0,
      "endOffset" : 35
    }, {
      "referenceID" : 26,
      "context" : "In such a pragmatic setting, language use for communication of spatial concepts has received particular attention in (Steels, 1995; Ullman et al., 2016).",
      "startOffset" : 117,
      "endOffset" : 152
    }, {
      "referenceID" : 14,
      "context" : "Models such as Rational Speech Acts (Frank & Goodman, 2012) and Iterated Learning (Kirby et al., 2014) have been popular in cognitive science and evolutionary linguistics, but such approaches tend to rely on pre-specified procedures or models that limit their generality.",
      "startOffset" : 82,
      "endOffset" : 102
    }, {
      "referenceID" : 7,
      "context" : "The recent work that is most similar to ours is the application of reinforcement learning approaches towards the purposes of learning a communication protocol, as exemplified by (Foerster et al., 2016; Sukhbaatar et al., 2016; Lazaridou et al., 2016a).",
      "startOffset" : 178,
      "endOffset" : 251
    }, {
      "referenceID" : 24,
      "context" : "The recent work that is most similar to ours is the application of reinforcement learning approaches towards the purposes of learning a communication protocol, as exemplified by (Foerster et al., 2016; Sukhbaatar et al., 2016; Lazaridou et al., 2016a).",
      "startOffset" : 178,
      "endOffset" : 251
    }, {
      "referenceID" : 7,
      "context" : "A similar approach was employed by (Foerster et al., 2016; Sukhbaatar et al., 2016) to compute gradients for communication actions, although the latter still employed modelfree methods for physical action computation.",
      "startOffset" : 35,
      "endOffset" : 83
    }, {
      "referenceID" : 24,
      "context" : "A similar approach was employed by (Foerster et al., 2016; Sukhbaatar et al., 2016) to compute gradients for communication actions, although the latter still employed modelfree methods for physical action computation.",
      "startOffset" : 35,
      "endOffset" : 83
    }, {
      "referenceID" : 12,
      "context" : "There has been a wealth of work in machine learning on differentiable models with discrete variables, but we found recent approach in (Jang et al., 2016; Maddison et al., 2016) to be particularly effective in our setting.",
      "startOffset" : 134,
      "endOffset" : 176
    }, {
      "referenceID" : 19,
      "context" : "There has been a wealth of work in machine learning on differentiable models with discrete variables, but we found recent approach in (Jang et al., 2016; Maddison et al., 2016) to be particularly effective in our setting.",
      "startOffset" : 134,
      "endOffset" : 176
    }, {
      "referenceID" : 21,
      "context" : "To help policy training avoid local minima, we found it helpful to include auxiliary goal prediction tasks, similar to recent work in reinforcement learning (Dosovitskiy & Koltun, 2016; Silver et al., 2016).",
      "startOffset" : 157,
      "endOffset" : 206
    }, {
      "referenceID" : 14,
      "context" : "What leads to compositional syntax formation? One known constructive hypothesis requires modeling the process of language transmission and acquisition from one generation of agents to the next iteratively as in (Kirby et al., 2014).",
      "startOffset" : 211,
      "endOffset" : 231
    } ],
    "year" : 2017,
    "abstractText" : "By capturing statistical patterns in large corpora, machine learning has enabled significant advances in natural language processing, including in machine translation, question answering, and sentiment analysis. However, for agents to intelligently interact with humans, simply capturing the statistical patterns is insufficient. In this paper we investigate if, and how, grounded compositional language can emerge as a means to achieve goals in multi-agent populations. Towards this end, we propose a multi-agent learning environment and learning methods that bring about emergence of a basic compositional language. This language is represented as streams of abstract discrete symbols uttered by agents over time, but nonetheless has a coherent structure that possesses a defined vocabulary and syntax. We also observe emergence of non-verbal communication such as pointing and guiding when language communication is unavailable.",
    "creator" : "LaTeX with hyperref package"
  }
}