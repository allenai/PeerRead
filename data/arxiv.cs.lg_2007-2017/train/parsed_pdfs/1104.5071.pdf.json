{
  "name" : "1104.5071.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Attacking and Defending Covert Channels and Behavioral Models",
    "authors" : [ "Valentino Crespi", "George Cybenko", "Annarita Giani" ],
    "emails" : [ "vcrespi@calstatela.edu.", "gvc@dartmouth.edu.", "agiani@eecs.berkeley.edu." ],
    "sections" : [ {
      "heading" : null,
      "text" : "In this paper we present methods for attacking and defending k-gram statistical analysis techniques that are used, for example, in network traffic analysis and covert channel detection. The main new result is our demonstration of how to use a behavior’s or process’ k-order statistics to build a stochastic process that has those same k-order stationary statistics but possesses different, deliberately designed, (k + 1)- order statistics if desired. Such a model realizes a “complexification” of the process or behavior which a defender can use to monitor whether an attacker is shaping the behavior. By deliberately introducing designed (k + 1)-order behaviors, the defender can check to see if those behaviors are present in the data. We also develop constructs for source codes that respect the k-order statistics of a process while encoding covert information. One fundamental consequence of these results is that certain types of behavior analyses techniques come down to an arms race in the sense that the advantage goes to the party that has more computing resources applied to the problem.\nPoints of view in this document are those of the authors and do not necessarily represent the official position of the sponsoring\nagencies or the U.S. Government.\nV. Crespi is with the Department of Computer Science, California State University at Los Angeles, Los Angeles CA, 90032 USA. email: vcrespi@calstatela.edu. Crespi’s work was partially supported by AFOSR Grant FA9550-07-1-0421 and by NSF Grant HRD-0932421.\nG. Cybenko is with the Thayer School of Engineering, Dartmouth College, Hanover NH 03755. email: gvc@dartmouth.edu. Cybenko’s work was partially supported by Air Force Research Laboratory contracts FA8750-10-1-0045, FA8750-09-1-0174, AFOSR contract FA9550-07-1-0421, U.S. Department of Homeland Security Grant 2006-CS-001-000001 and DARPA Contract HR001-06-1-0033\nA. Giani is with the Department of EECS, University of California at Berkeley, Berkeley CA 94720. email: agiani@eecs.berkeley.edu. Giani’s’s work was partially supported by U.S. Department of Homeland Security Grant 2006-CS001-000001 and DARPA Contract HR001-06-1-0033 when she was a Ph.D. student at Dartmouth\nApril 28, 2011 DRAFT\nar X\niv :1\n10 4.\n50 71\nv1 [\ncs .L\nG ]\n2 7\nA pr\n2 01\nIndex Terms\nCovert Channels, Exfiltration, Probabilistic Automata, Cognitive Attack, Anomaly Detection.\nI. INTRODUCTION\nComputer security researchers have been investigating statistical behavioral modeling techniques as a means for determining whether a machine, a network or data packet contents are behaving “normally” or not. These are so-called behavior analysis techniques and implicitly model stochastic processes at some level of fidelity.\nConsider for example, the problem of detecting covert channels. Some existing approaches assume that an adversary has installed an exfiltrating agent, or Trojan, which operates by encoding data in a way that introduces detectable regularities in some network traffic statistics. For example, Giani et al. [1] and Cabuk et al. [2] estimate certain first order statistics of packet inter-arrival delays in order to determine whether a time covert channel is being used. Dainotti et al. [3] learn a Hidden Markov Process [4], [5], [6], [7], [8] using both packet inter-arrival delays and packet sizes to detect traffic anomalies. Other techniques are based on various analyses of n-gram statistics [9]. In fact, some have called techniques that match n-gram statistics “mimicry attacks” and while techniques have been developed for detecting certain simple types of mimicry, techniques for building mimicry attacks as described in the present paper appear to be novel [9].\nGeneral discussions of covert channels and their taxonomies, existence and modeling have been published [10], [11], [12], [13], [14]. The design, implementation and experimental evaluation of several specific covert channel attacks in real systems is of specific interest [14]. That work presents threat models, achievable bit rates, noise properties and channel capacities for covert channels.\nThe existence and successful use of a covert channel is based on the assumption that the covert channel code does not perturb the measured statistical properties of behavior so that, over time, a covert transmission does not introduce discernible patterns which are different than expected, at least with respect to what is measured. In this paper we assume the ability to learn a k-gram type model of “normal behavior.” This is simply done by counting the occurrences of k-grams and then normalizing to produce frequencies or probabilities. It is important to note that researchers often talk about entropy as a channel statistic [10], [15] but entropy is typically\nApril 28, 2011 DRAFT\ncalculated from k-order statistics so that our methods for preserving k-order statistics preserves all lower order statistics and will also preserve the entropy.\nWe present a technique for encoding messages that respects these k-order statistics. Both attacker and defender can use this coding technique. The attacker could exfiltrate coded information while the defender could embed an encoded reference message or carrier to detect manipulations of the channel by an adversary attempting covert communications. That is, for any order k, an attacker or a defender can encode covert messages while otherwise respecting the k-order statistics of the traffic.\nAlso, we show how a defender can create a process of order k + 1 which has the same k-order statistics but specifically designed (k + 1)-order statistics that the defender can easily monitor to see if the (k+1)-order statistics have been changed. Researchers have recently started to develop systematic taxonomies and examples of attacks against statistical machine learning techniques [16]. In that spirit, the present work develops specific techniques to both attack and defend using certain statistical approaches.\nWe discuss these methods in the context of behaviors that have a finite set of observable symbols (the alphabet). Interpacket arrival times, packet sizes, header fields, packet contents and so on are examples of such observables if quantized into a finite number of bins. Our approach models the observables as a stationary stochastic process X [17]. After estimating the k-order statistics, we build a Probabilistic Deterministic Finite Automata model (PDFA) [18], [19], [20], [21] that realizes the k-order statistics.\nUsing that PDFA, we show that:\n1) an adversary can encode messages covertly while respecting the k-order statistics; 2) the defender can encode reference messages or a carrier while respecting the k-order\nstatistics and;\n3) the defender can build a more complex process which has the same k-order statistics but\npossesses deliberately designed (k + 1)-order statistics.\nExamples of such covert channels in network traffic include, but are not restricted to:\n• Timing Channels: The observable symbols are the inter-packet time delays, appropriately\nquantized;\n• Size Channels: The observable symbols are quantized sizes of the packets; • Header Channels: The observable symbols are various header fields in TCP/IP packets\nApril 28, 2011 DRAFT\nwhich can be manipulated by the transmitting entity without violating protocol semantics. Several such fields are known to exist [22].\nIt is important to clarify right away what we mean by a k-order statistic and a k-gram. Suppose we have an alphabet consisting of {α, β, γ} and we observe a sequence comprised of that alphabet, say\nαααβαβγγ.\nThe first order statistics are [1/2 1/4 1/4] indicating that 1/2 of the symbols are α’s, 1/4 are β’s and 1/4 are γ’s. The 1-grams are merely {α, β, γ}.\nThe 2-grams observed in this sequence are αα, αα, αβ, βα, αβ, βγ, γγ and the 2-order\nstatistics for the 9 possible 2-grams\nαα, αβ, αγ, βα, ββ, βγ, γα, γβ, γγ\nare respectively\n[2/7 2/7 0 1/7 0 1/7 0 0 1/7].\nThat is, our k-grams are obtained by moving a sliding window of width k across the data one symbol at a time. This is not to be confused with moving that window across the data sequence k symbols at a time.\nThe following discussion shows how a timing covert channel can be constructed based on a beacon and argues that a naive encoding of covert messages based on packet inter-arrival times produces a clearly detectable distortion of the 1-order statistics of those time intervals in network traffic [23], [1].\nFigure 1 describes the setup. Machine A sends a regularly timed beacon to machine D. (Such a beacon can be a time server request or a stay alive beacon for instance.) The inter-packet delays seen at machine B are not regular due to internal routing delays in the LAN. (These statistics were actually measured from a regularly timed beacon traveling several hops.) An intruder was able to compromise and control machine B which is inside the local network and a relay for the traffic between A and D. (B could be a proxy server, border router or other device for example.) Assume we set up a machine C outside the internal network perimeter to check for timing covert channels. C has seen a certain distribution of inter-packet delays coming from A going to D.\nApril 28, 2011 DRAFT\nIn this paper, we show how machine B can encode covert messages in the inter-packet delays in such a way that the first order statistics as seen by C remain unchanged from the original distribution. Conversely, we can deliberately defend against such channels by encoding messages so that any manipulation of the delays will be detectable on the outside at machine C, because the covert message will not be received at C.\nFigure 2 shows the number of packets received with a given delay in two scenarios. The horizontal axis reports the inter-arrival time in seconds, and the vertical axis the number of packets received with those delays. In the left graph of Figure 2 are the observed inter-packet delays resulting from a regularly timed beacon traversing multiple hops in a LAN. On the right hand graph, we depict a naive covert timing channel using two time intervals to encode a message. It is evident from the data that the naive covert communication in the right graph can be easily detected if the 1-order statistics of normal traffic have been measured and are those on\nApril 28, 2011 DRAFT\nthe left. However, the 1-order distribution on the left can be generated either by normal traffic, as it was obtained, or by a covert channel, as we will show.\nIn this contribution, we develop a more sophisticated approach than the naive approach insofar as we consider also statistics of arbitrarily higher order, i.e. k > 1, and our results effectively show that, for any k, defenders and attackers both have technical approaches for, respectively, attacking or defending a k-order behavior with respect to covert communications. Consequently, the situation is an arms race in the sense that whichever side has the ability to learn the highest order statistics wins."
    }, {
      "heading" : "A. Outline of the Paper",
      "text" : "In Section II we present an illustrative example. In Section III we describe our method and show how to manipulate a behavior’s statistics with Probabilistic Automata. In Section IV we provide a numerical example. In Section V we show how to use Probabilistic Automata to build a channel code that respects the statistics of traffic up to some predecided order. Finally, Section VI contains some conclusions and future work references.\nApril 28, 2011 DRAFT"
    }, {
      "heading" : "II. A SIMPLE ILLUSTRATIVE EXAMPLE",
      "text" : "To illustrate these concepts, consider a simple binary observable with values, 0 and 1. It is assumed that these observables are irrelevant to the normal operation of the underlying system and its semantics. For example, the observables could be quantized inter-arrival times or unused packet header fields.\nAssume that the 1-order statistics of these observables are r0 > 0 and r1 > 0 with r0 +r1 = 1. This means that the relative frequency of 0’s and 1’s as observed in the behavior are r0 and r1 respectively. Now suppose an attacker has estimated these probabilities and seeks to exfiltrate messages while respecting these probabilities. This is possible and, later in this paper, we review standard source coding ideas that allow the attacker to create such codes efficiently.\nIn fact, if the messages to be sent are binary and Bernoulli with p = 0.5 (such as for encrypted and/or compressed messages), then there are codes that use 1/H(r0) = 1/H(r1) bits in the covert channel per original message symbol where H(x) = −(x log2 x + (1 − x) log2(1 − x)) is the entropy function. We show how to construct such codes to respect k-order statistics as well.\nBy the same token, the defender can encode a reference signal, also respecting the first order\nstatistics as above, which can be decoded and verified at the receiving end.\nNote that no specific second order statistics r00, r01, r10 and r11 have been modeled so far, but if the process is modeled by a Bernoulli process with p = r1 then the second order statistics would be ri · rj = rij = Prob(ij) = Prob(ji) by independence.\nHowever, the defender can construct a second order process with second order statistics r00, r01, r10 and r11 for which rij 6= ri · rj while satisfying the required first order statistics, namely r0 and r1. If the attacker exploits the channel through a purely first order process, the constructed second order statistics rij will likely not be observed by the defender who could then conclude that the traffic is being shaped by an adversary.\nTo illustrate this 2-order construction, consider an automaton with two states, Q = {0, 1}, corresponding to the two 1-grams of observables. Let X be the matrix of the transition probabilities\nX =  p00 p01 p10 p11  . We seek PDFAs that have the stationary distribution π = [r0 1− r0] = [1− r1 r1] = [1− r r].\nApril 28, 2011 DRAFT\nSpecifically, we seek X that satisfies\n[1− r r] ·X = πX = π = [1− r r]\nwith X being a stochastic matrix (non-negative with row sums equal to 1). The class of PDFAs that are 1-order equivalent to the given process is therefore determined by a set of linear equality and inequality constraints as follows:\n(1− r) · p00 + r · p01 = 1− r\n(1− r) · p10 + r · p11 = r\np00 + p01 = 1\np10 + p11 = 1\npij ≥ 0.\nThe four equations are linearly dependent and we can reduce them to the three equations and\nconstraints\nr · p11 − (1− r) · p00 = 2 · r − 1\np00 + p01 = 1\np10 + p11 = 1\n0 ≤ p00 , p11 ≤ 1.\nThere are an infinite number of solutions according to\np11 = 1− r r p00 + 2r − 1 r , 0 ≤ p00 , p11 ≤ 1. (1)\nFor example, if r = 0.3, 1− r = 0.7 then the constraints become:\np11 = 0.7p00 − 0.4\n0.3\n0 ≤ p00 , p11 ≤ 1\nso letting p00 = 0.8 we get p11 = 0.160.3 = 0.53̄ and therefore p01 = 0.2 and p10 = 0.46̄. This\nApril 28, 2011 DRAFT\nyields 2-order statistics of:\nr00 = p00π1 = 0.8 · 0.7 = 0.56\nr01 = p01π1 = 0.2 · 0.7 = 0.14\nr10 = p10π2 = 0.46̄ · 0.3 = 0.14\nr11 = p11π2 = 0.53̄ · 0.3 = 0.16.\nNotice that r01 = r10 = 0.14, r01 + r00 = r0 = π1 = 0.3 and r01 + r11 = r1 = π2 = 0.7 as required.\nAnother, equivalent way to derive these relations is to note that there are two trivial solutions for X , namely X1 = I2 (the 2 by 2 identity matrix) and X2 = 1 · π where 1 = [1 1]T is the column vector whose entries are all 1’s. These two solutions are always different. Moreover, we can see that any convex combination ρX1 + (1 − ρ)X2 for 0 ≤ ρ ≤ 1 is also a solution to all the constraints and in fact yields the same class of solutions as above.\nThe point of this example is that we can shape the second order statistics of the observables without changing the first order statistics. In particular, multiple choices for p00 (and so for r00) are possible, all of which lead to the same 1-order statistics. A defender can shape the second order statistics so that if an attacker only obeys the first order statistics, the defender can detect that the expected second order statistics are wrong.\nNote that the second order process in this example satisfies an additional constraint - namely, the marginal distributions must agree with the first order process, namely r01 + r11 = r1 and so on. Moreover, r01 = r10 must be true as well (this is a symmetry which arises from considering the 0 to 1 and 1 to 0 transitions in the observed sequence which must be equal). For higher order processes, the construction involves identifying and dealing with additional constraints and finding realizations which satisfy them. These generalizations to higher orders are one of the main contributions of this paper.\nTo apply this construction to the empirical data shown in Figure 2, normalize the counts into frequencies or probabilities by dividing by the total packet count. This yields a vector of probabilities:\nR = [ 0.0029 0.0144 0.0734 0.1453 0.3094 0.1295 0.1151 0.1079 0.1007 0 0 0 0.0014 ] (2)\nApril 28, 2011 DRAFT\nwhere the coordinates 1 through 13 correspond to delays of 0.01 through 0.13 in increments of 0.01.\nWe seek to construct a Markov Chain whose states correspond to observable inter-packet delays and whose transition probabilities, P , describe the probability that one delay follows another. As explained above, P must satisfy two matrix equations (capturing the facts that R is a stationary vector for P and that P is row stochastic)"
    }, {
      "heading" : "R ∗ P = R and P ∗ 1 = 1",
      "text" : "where 1 is the column vector of all ones. Moreover, the entries of P are all non-negative.\nIn this simple case, there are two solutions which are simple to identify, namely\nPB = 1 ∗R and PD = I (3)\nwhere I is the 13 by 13 identity matrix. The reader can easily check that both these matrices satisfy the two required matrix equations. This construct is simple for 1-grams but becomes more complex for general k-grams as shown below.\nMoreover, for any 0 ≤ α ≤ 1, Pα = αPB + (1−α)PD is also a solution. Whereas PB defines a Bernoulli process and PD describes a completely disconnected Markov Chain with an infinite number of fixed distributions, Pα defines a Markov Chain that is irreducible, aperiodic and not a Bernoulli process for any 0 < α < 1 . Therefore, Pα can be used by a defender to create specific second order statistics which an attacker would have to first model and then respect."
    }, {
      "heading" : "III. CONSTRUCTING THE AUTOMATA",
      "text" : "In this section, we show how to construct automata that can reproduce observed statistics\ncomputed from data.\nLet Σ = {a, b, c, ...} be the finite observable alphabet and σ = |Σ| < ∞ be the number of observables. We are assuming that we have sequences of observables from which we compute the relative frequencies of k-grams (k ≥ 1):\n0 ≤ R(x) ≤ 1, ∑ x∈Σk R(x) = 1.\nHere Σk is the set of k-grams; that is, the set of all possible sequences of length k drawn from the alphabet Σ.\nApril 28, 2011 DRAFT\nRoughly speaking, if s0s1...sn−1 = S0:n−1 is an observed data sequence of length n > k, R(x) is approximated by the number of occurrences of the substring x in S0:n−1 divided by the total number of substrings of length k in S0:n−1, namely n − k + 1. The set of R(x)’s is precisely what we mean by the k-order statistics of the observations.\nThese statistics must satisfy certain regularity conditions required by the proposed construction\nso some care must be taken in their computation. Specifically, the identity∑ a∈Σ R(ay) = ∑ b∈Σ R(yb) = R(y) should hold for every y ∈ Σk−1. This can be accomplished by appending S0:n−1 with s0s1...sk−2 as a suffix, creating a periodic string effectively, and counting occurrences in the periodic string.\nMoreover, this can be repeated for every 1 < j < k by using a circular buffer appending\ns0s1...sj−2. All marginal distributions∑ w∈Σk−j R(wy) = ∑ w∈Σk−j R(yw) = R(y) will hold for all y ∈ Σj then. (Details are left to the reader.)\nWe will now construct a special type of Markov Chain in which Σk are the states and the semantics of the k-grams are preserved so that if x = ay ∈ Σk is an observed k-gram, then P (ay, yb) is the probability of transitioning to state yb where both a, b ∈ Σ. Such transitions are the only ones possible in the Markov Chain k-gram model. Such models are called kth-order Markov Models, k Markov Chains or k-gram models by different authors [19], [24].\nLet π be the vector of measured k-gram statistics, R(x), and let P be the desired Markov\nChain transition probabilities:\nP = (P (x, x′))\nwhere the entries of both π and P are indexed by x, x′ ∈ Σk.\nThe stationary probabilities of the desired Markov Chain are precisely π when the equation πP = π is satisfied. This matrix equation consists of σk equations and the stochasticity requirement on P is another σk equations resulting in the following 2σk equations overall:\n∑ x∈Σk P (x, x′)R(x) = R(x′), ∀x′ ∈ Σk , (stationary probability conditions) (4)\n∑ x′∈Σk P (x, x′) = 1, ∀x ∈ Σk (probability requirements) (5)\nApril 28, 2011 DRAFT\nwhere P (x, x′) ≥ 0 as well.\nBecause of the relationship between k-grams and the Markov Chain that we are seeking to construct, we can only have P (x, x′) 6= 0 when x = ay and x′ = yb for some a, b ∈ Σ and y ∈ Σk−1. That is, y is the suffix of the state x = ay and we can only transition to states x′ = yb which have y as a prefix and some suffix b ∈ Σ. Accordingly, for every y ∈ Σk−1, we have the 2σ equations ∑ a∈Σ\nP (ay, yb)R(ay) = R(yb), ∀b ∈ Σ , (6) ∑ b∈Σ P (ay, yb) = 1, ∀a ∈ Σ (7)\nP (ay, yb) ≥ 0 (8)\nwhich are completely decoupled from the equations corresponding to (k − 1)-grams other than y. Accordingly, we can solve each system independently.\nNoting that the k-grams statistics, R(x), satisfy the marginalization relations∑ a∈Σ R(ay) = ∑ b∈Σ R(yb) = R(y), ∀y ∈ Σk−1,\nsumming over b in the equations (6), we get∑ b∈Σ ∑ a∈Σ P (ay, yb)R(ay) = ∑ a∈Σ ∑ b∈Σ P (ay, yb)R(ay) = ∑ a∈Σ R(ay) = ∑ b∈Σ R(yb) = R(y) which is an identity not involving the unknown P (ay, yb).\nAccordingly, there are no more than 2σ − 1 linearly independent equations in (6). In fact, if we define pre(y) to be the number of nonzero R(ay) and post(y) be the number of nonzero R(yb), there are in fact no more than pre(y) · post(y) unknown probabilities, P (ay, yb), and no more than pre(y) + post(y)− 1 independent equations altogether."
    }, {
      "heading" : "A. The Standard Solution",
      "text" : "One solution to the equations, which we call the Standard Solution, is P̄ (ay, yb) = R(yb)/R(y)\nbecause then∑ a∈Σ P̄ (ay, yb)R(ay) = ∑ a∈Σ R(ay)R(yb)/R(y) = R(yb)/R(y) ∑ a∈Σ R(ay) = R(yb) and ∑ b∈Σ P̄ (ay, yb) = ∑ b∈Σ R(yb)/R(y) = R(y)/R(y) = 1.\nApril 28, 2011 DRAFT\nThis specific solution has pre(y) · post(y) nonzero probabilities, P (ay, yb), for the substring y ∈ Σk−1 by construction.\nBy construction, this Markov Chain is irreducible because we have constructed the transition probabilities from a circular buffer so that there is a nonzero probability of going from any state with nonzero probability, namely R(x), to any other state with nonzero probability. If additionally the constructed Standard Solution Markov Chain is aperiodic, its unique stationary distribution is precisely R(x) and its entropy rate is\nH(P̄ ) = HP (Xk+1|X k 1 ) = − ∑ a∈Σ ∑ y∈Σk−1 ∑ b∈Σ R(ay)P̄ (ay, yb) log(P̄ (ay, yb)). (9)"
    }, {
      "heading" : "B. Extended Solutions",
      "text" : "If pre(y) and post(y) are both strictly greater than 1, then pre(y) · post(y) > pre(y) + post(y) − 1. From the theory of linear programming, there are feasible solutions to the linear program defined by (6), (7) and (8) which have no more than pre(y) + post(y) − 1 nonzero coordinates, namely the Basic Feasible Solutions [25].\nLet such a Basic Feasible Solution be P̂ (ay, yb). As derived above, there are solutions with exactly pre(y) · post(y) nonzero coordinates, namely the Standard Solutions, P̄ (ay, yb). Note that strict convex combinations of P̂ with P̄ , Pu = uP̂ + (1 − u)P̄ with 0 < u < 1, define a continuum of solutions to (6), (7) and (8), with each solution corresponding to an irreducible Markov Chain. This is the case because every state is reachable from every other state with nonzero probability due to the construction of the Standard Solution.\nMoreover, when pre(y) and post(y) are both strictly greater than 1, P̂ and P̄ are different. As an aside, we have observed that Basic Feasible Solutions typically result in reducible chains because those solutions involve a minimal number of nonzero transition probabilities."
    }, {
      "heading" : "IV. NUMERICAL EXAMPLES",
      "text" : "In this section we demonstrate the constructions described above.\n1) We consider data generated by the automata depicted in Figure 3 which is a Hidden Markov\nModel (HMM), M = {A(0), A(1)}, defined by the two transition matrices\nA(0) =  0.5 0.5 0 0.5  , A(1) =  0 0 0.5 0  . April 28, 2011 DRAFT\n!\" #\"\n$\"%\"$&'\"\n$\"%\"$&'\"\n!\"%\"$&'\"\n$\"%\"$&'\"\nand  R(000) = 0.338 R(001) = 0.174 R(010) = 0.244 R(011) = 0.000 R(100) = 0.174 R(101) = 0.070 R(110) = 0.000\nR(111) = 0.000\n .\nObserve that R(01) = R(10) which is a necessary regularity that follows from the marginalization property: ∑ a R(ay) = ∑ b R(yb) = R(y) . In order to be sure that the estimates verify those consistency conditions we have treated the data stream as a circular buffer as described previously. 3) We built the Standard Solution, P , where P (ay, yb) = R(yb)/R(y), and then we computed\na different numerical solution, P̂ , of the linear program (6), (7) and (8).1 The two solutions are summarized below: P (00, 00) = 0.678 P (00, 01) = 0.322 P (10, 00) = 0.678 P (10, 01) = 0.322 P (01, 10) = 1.000 P (01, 11) = 0.000 P (11, 10) = 1.000\nP (11, 11) = 0.000\n ,  P̂ (00, 00) = 1.000 P̂ (00, 01) = 0.000 P̂ (10, 00) = 0.000 P̂ (10, 01) = 1.000 P̂ (01, 10) = 1.000 P̂ (01, 11) = 0.000 P̂ (11, 10) = 0.000\nP̂ (11, 11) = 1.000\n .\nNote that the Basic Feasible Solution, P̂ , has a maximal number of zeros and results in a reducible chain with three communicating classes, namely 00, {01, 10}, 11. By convexity\n1P̂ is a Basic Feasible Solution obtained by employing the Matlab linprog function.\nApril 28, 2011 DRAFT\nPu = u · P + (1 − u) · P̂ is also a solution, for any 0 < u < 1, so that for u = 0.5 and u = 0.2 we obtain respectively the following two different 2-grams: P0.5(00, 00) = 0.839 P0.5(00, 01) = 0.161 P0.5(10, 00) = 0.339 P0.5(10, 01) = 0.661 P0.5(01, 10) = 1.000 P0.5(01, 11) = 0.000 P0.5(11, 10) = 0.500\nP0.5(11, 11) = 0.500\n ,  P0.2(00, 00) = 0.936 P0.2(00, 01) = 0.064 P0.2(10, 00) = 0.136 P0.2(10, 01) = 0.864 P0.2(01, 10) = 1.000 P0.2(01, 11) = 0.000 P0.2(11, 10) = 0.200\nP0.2(11, 11) = 0.800\n .\n4) Now compare the original 2-order statistics specified by M with the statistics specified by\nthe two new models, namely P0.5 and P0.2 as above: R(00) = 0.513 R(01) = 0.244 R(10) = 0.244\nR(11) = 0.000\n ,  R0.5(00) = 0.513 R0.5(01) = 0.244 R0.5(10) = 0.244\nR0.5(11) = 0.000\n ,  R0.2(00) = 0.513 R0.2(01) = 0.244 R0.2(10) = 0.244\nR0.2(11) = 0.000\n .\nThey are numerically identical as expected. Finally we verify that the 3-order statistics are all different from each other and from the 3-order statistics of the original data, R, previously listed. R(000) = 0.348 R(001) = 0.165 R(010) = 0.244 R(011) = 0.000 R(100) = 0.165 R(101) = 0.079 R(110) = 0.000\nR(111) = 0.000\n ,  R̂(000) = 0.513 R̂(001) = 0.000 R̂(010) = 0.244 R̂(011) = 0.000 R̂(100) = 0.000 R̂(101) = 0.244 R̂(110) = 0.000\nR̂(111) = 0.000\n ,  R0.5(000) = 0.430 R0.5(001) = 0.083 R0.5(010) = 0.244 R0.5(011) = 0.000 R0.5(100) = 0.083 R0.5(101) = 0.161 R0.5(110) = 0.000\nR0.5(111) = 0.000\n ,  R0.2(000) = 0.480 R0.2(001) = 0.033 R0.2(010) = 0.244 R0.2(011) = 0.000 R0.2(100) = 0.033 R0.2(101) = 0.211 R0.2(110) = 0.000\nR0.2(111) = 0.000\n .\nThese 3-order statistics are calculated using the relationships\nR̃(ayb) = R(ay) · P̃ (ay, yb)\nfor the various R̃, a, y, b. Moreover, the Ru are the same convex combinations as the the various Pu’s. This example illustrates the various constructions we have described in complete generality in the previous section.\nApril 28, 2011 DRAFT"
    }, {
      "heading" : "V. A COVERT CHANNEL CODING TECHNIQUE",
      "text" : "In the previous section, we showed that given observed string frequencies, R(z), z ∈ Σk we can construct multiple Markov Chains, M , whose states are the k-grams (z ∈ Σk), transition probabilities are P (ay, yb), a ∈ Σ, y ∈ Σk−1 and whose stationary distributions are precisely the observed R.\nWe now show how to use such a Markov Chain to encode messages while preserving the statistics, R, of the channel. This means that someone monitoring the channel will observe the same k-gram statistics in spite of the fact that covert messages can be communicated within that channel. As noted before, this can be exploited by either attacker or defender.\nConceptually, the coding concept is the opposite of the classical Shannon Source Coding Theorem [17] in the sense that traditionally we start with a stochastic source with entropy rate H that we seek to compress into binary strings whereas in this case we start with a collection of 2r messages which we wish to efficiently encode using the dynamics and statistics of the given stochastic process. Because we have to respect the statistics of the channel, the encoding will typically not be be compressing but expanding the number of bits needed. Nonetheless, we still seek efficiency with respect to observing the channel’s k-gram statistics.\nIn this work, we will assume, for simplicity, that the communication covert channel is noiseless noting that the results can be extended to noisy channels in the traditional way. A more thorough analysis is deferred to a future study in which the Shannon capacity of noisy channels will be considered.\nThis construction involves several steps:\n1) Compute the entropy of the irreducible Markov Chain M , HM , specified by transition\nprobabilities, PM , and stationary distribution, RM :\nHM = − ∑ ay∈Σk RM(ay) ∑ b∈Σ PM(ay, yb) log2(PM(ay, yb)).\nNote that we construct the Markov Chains to have a given stationary distribution, R, so only PM is different for the different models. For the examples developed in the previous section, we have computed:\nHP = 0.6863 , HP̂ = 0 , HP0.2 = 0.3165 , HP0.5 = 0.5520.\nNote that P̂ is entirely deterministic and so has zero entropy.\nApril 28, 2011 DRAFT\nSince we constructed these Markov Chains so that different transitions from a state correspond to different observables (that is, be DPFA’s), knowledge of the initial state of the Markov Chain results in a one-to-one correspondence between state sequences and observation sequences. Hence the entropy rates of both the Markov Chain state sequences and resulting observation sequences are the same. Let Ys represent the stochastic process of observations produced by the constructed Markov Chain, M , starting in state s ∈ M . All states in M are recurrent by construction so the entropy rate of each process Ys is the same and equal to HM . 2) Apply the Shannon-MacMillan-Breiman Asymptotic Equipartition Property (AEP) The-\norem [17] to each Ys showing that for large n there are approximately 2nHM typical sequences of length n of Ys and each occurs with probability approximately (1/2)nHM . Consequently, in order to encode 2r covert messages, say Ci with 1 ≤ i ≤ 2r we must have r ≤ nHM or equivalently n ≥ r/HM so n is selected to encode 2r different covert message sequences accordingly. 3) Construct length n typical sequences of Ys by starting in state s and then performing a\nrandom walk of length n in M according to the probabilities PM . Such random walks define observation sequences of length n in Σn. Produce 2r ≤ 2nHM unique sequences for each state s, labeling them as Ys(i) where 1 ≤ i ≤ 2r ≤ 2nHM . (If a random walk produces a sequence already generated, simply repeat until a novel random walk is produced.) 4) Note that the k-gram frequencies of each z ∈ Σk within the Ys(i) approach the original\nR(z) as n → ∞ because R is the stationary distribution of the Markov Chain and Ys(i) is produced by taking a random walk in the chain. 5) For each state, s, assign the covert message Ci to Ys(i). Pick a random initial state s(0)\nand assign a sequences of covert messages Ci1Ci2 ...Cim to\nYs(0)(i1)Ys(1)(i2)...Ys(m−1)(im)\nwhere s(j) is recursively defined as the state in which Ys(j−1)(ij) ended.\nBecause each random walk in the sequence thus constructed starts in the state in which the previous random walk ended, the concatenated sequence of random walks is also a legal random walk in the Markov Chain, obeying all the transition probabilities. Moreover, the k-gram statistics in the overall concatenated sequence of mn observations is approximately R and approaches R\nApril 28, 2011 DRAFT\nas n→∞. The encoded sequence is uniquely decodable by the receiver as well.\nTo illustrate this construction, consider the example presented in the left of Figure 2 where we use the 1-order statistics as in equation (2). We take the convex combination (see Section II)\nP = 0.75 · PB + 0.25 · PD,\nwhich results in an entropy of HP = 0.004 as computed from (9). We build a (2r, n) codebook as described above with r = 8 and n = dr/HP e = 1995. That is, this encodes binary sequences of length 8 into inter-packet delays of length 1995. We encoded 16 blocks of 8 random source bits each into 16 ·1995 = 31920 symbols from the alphabet Σ = {1, 2, . . . , 13} which correspond to the delays in the left graph of Figure 2.\nThe obtained 1-order statistics of the resulting 31920 long concatenated codeword are\nR′ = [0.0029 0.0144 0.0734 0.1453 0.3094 0.1295 0.1151 0.1079 0.1007 0 0 0 0.0014];\nand are depicted in Figure 4. Note the empirical frequencies and graphs are identical to the displayed precision.\nThis illustrates empirically the effectiveness of the construction described in this paper. Matlab\ncode for reproducing these results is available upon request.\nApril 28, 2011 DRAFT"
    }, {
      "heading" : "VI. CONCLUSIONS AND FUTURE WORK",
      "text" : "This paper has demonstrated that covert channels can exist even when arbitrarily high order statistics about a channel are estimated and monitored. The resulting covert channels can be used to either exploit or defend the channel and the advantage goes to the party that has the ability to estimate the highest order statistics.\nThe adversarial nature of this situation falls within the scope of cognitive attacks [26], [27]. It can be described in abstract as follows: the environment (for example, inter-packet delays) is modeled as a stochastic process X (such as a Hidden Markov Model, Markov Chain or other formalism). Both the attacker, A, and the defender, D, monitor the environment through functions fA ∈ F and fD ∈ F respectively (for example, fD(X ) could be the probability distribution of k-grams produced by X ).\nThe attacker guesses fD and manipulates X in order to produce a new process, A(X ), so that covert communications can be performed while respecting the behavior that the defender expects; namely, fD(X ) = fD(A(X )).\nOn the other hand, the defender, by anticipating the attacker’s guess of fD, picks a different\nf̃D and manipulates X to produce a new process D(X ) so that:\n1) fD(A(D(X ))) = fD(D(X )) = fD(X ) = fD(A(X )): the defensive shaping action is\nimperceptible to the defender who uses fD;\n2) f̃D(A(D(X ))) 6= f̃D(D(X )): the attacker’s action (that is, creation of a covert channel) is\ndetectable by the defender.\nThe game consists of attacker and defender guessing and then exploiting each other’s monitoring strategy and manipulating the environment accordingly. The common objective of the players is to alter the environment in a manner that would be imperceptible to the opponent in order to perform a secret task (covert communication or covert channel detection).\nThis work raises some questions which are deferred to future work. In particular, the following\ndirections are worthy of future investigation:\n• Inter-packet delays involve real-world time so the question of stability when shaping the\nchannel must be considered. That is, packets can be delayed by certain times only if there are packets in the queue to be delayed. Discussions of such queuing aspects of timing channels and the possibility of jamming them have been studied [28]. Relating this work\nApril 28, 2011 DRAFT\nto timing channel jamming will be investigated.\n• We used a circular buffer in Section IV to numerically estimate k-gram statistics so that the\nstatistics have the required marginalization properties. A single pass, online algorithm for implementing this circular buffer only requires storing the first and last k symbols of the data. In the absence of such a buffer, the empirical statistics will not in general obey the marginalization identities and so some additional processing would be required. The use of singular value decompositions, non-negative matrix factorizations or other decomposition methods for imposing the regularity might be worth exploring further as alternatives to the circular buffer approach. • In principle, one can attempt to build automata smaller than the Markov Chains we construct.\nIn particular, Probabilistic Finite Automata (PFA) [29], [30], [31] could implement Markov Chains based on k-grams but using fewer states. Unlike k-gram based Markov Chains, kPSAs have states that are labeled with input sequences of length at most k. So they can be seen as “variable length” k-gram Markov Chains. They can be learned efficiently in the KL-PAC sense [32], [33], [34] and are generally smaller than k-gram based Markov chains (by having fewer states). • Within the space of possible Markov Chains that realize given k-gram statistics, it would\nbe good to select the “best” chain from the point of maximizing entropy so that the covert channel coding is as efficient as possible. Our experiments suggest that the so-called Standard Solutions presented in Section III-A have the largest entropy although we have not been able to prove that analytically. • It is reasonable to ask how our results relate to the use of Hidden Markov Models for\nmodeling traffic, as for example in [3]. It is known that a Hidden Markov Model with n states is completely determined by the 2n-grams produced by the model so that reproducing 2n-gram statistics will result in the same n state Hidden Markov Model [8]."
    } ],
    "references" : [ {
      "title" : "Detection of covert channel encoding in network packet delays",
      "author" : [ "V. Berk", "A. Giani", "G. Cybenko" ],
      "venue" : "Proc. of FloCon 2005 Pittsburgh, PA, 2005.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "IP covert timing channels: design and detection",
      "author" : [ "S. Cabuk", "C. Brodley", "C. Shields" ],
      "venue" : "Proceedings of the 11th ACM Conference on Computer and Communications Security, 2004. April 28, 2011  DRAFT  CRESPI, CYBENKO, GIANI  22",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Internet traffic modeling by means of Hidden Markov Models",
      "author" : [ "A. Dainotti", "A. Pescapé", "P.S. Rossi", "F. Palmieri", "G. Ventre" ],
      "venue" : "Computer Networks, vol. 52, pp. 2645–2662, 2008.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Hidden Markov Processes",
      "author" : [ "Y. Ephraim" ],
      "venue" : "IEEE Transactions on Information Theory, vol. 48, no. 6, pp. 1518–1569, 2002.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "A tutorial on Hidden Markov Models and selected applications in speech recognition",
      "author" : [ "L.R. Rabiner" ],
      "venue" : "Proceeding of the IEEE, vol. 77, no. 2, pp. 257–286, 1989.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Approximate Nonnegative Matrix Factorization via alternating minimization",
      "author" : [ "L. Finesso", "P. Spreij" ],
      "venue" : "Proceedings of Mathematical Theory of Networks and Systems, Leuven, Belgium, 2004.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Nonnegative Matrix Factorixation and I-divergence alternating minimization",
      "author" : [ "——" ],
      "venue" : "Linear Algebra and its Applications, vol. 416, pp. 270–287, 2006.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Learning Hidden Markov Models using Nonnegative Matrix Factorization",
      "author" : [ "G. Cybenko", "V. Crespi" ],
      "venue" : "IEEE Transactions on Information Theory, 2011, to appear. [Online]. Available: http://arxiv.org/abs/0809.4086",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Anagram: a content anomaly detector resistant to mimicry attack",
      "author" : [ "K. Wang", "J.J. Parekh", "S.J. Stolfo" ],
      "venue" : "Springer Lecture Notes in Computer Science, Recent Advances in Intrusion Detection, vol. 4219, 2006.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "On the limits of steganography",
      "author" : [ "R. Anderson", "F. Petitcolas" ],
      "venue" : "Selected Areas in Communications, IEEE Journal on, vol. 16, no. 4, pp. 474 –481, May 1998.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Shared Resource Matrix Methodology: an approach to identifying storage and timing channel",
      "author" : [ "R. Kemmerer" ],
      "venue" : "ACM Transactions on Computer Systems, vol. 1, no. 3, pp. 256–277, August 1983.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1983
    }, {
      "title" : "New constructive approach to covert channel modeling and channel capacity estimation",
      "author" : [ "Z. Wang", "R.B. Lee" ],
      "venue" : "Information Security, ser. Lecture Notes in Computer Science, J. Zhou, J. Lopez, R. H. Deng, and F. Bao, Eds. Springer Berlin / Heidelberg, 2005, vol. 3650, pp. 498–505.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Problems of modeling in the analysis of covert channels",
      "author" : [ "A. Grusho", "N. Grusho", "E. Timonina" ],
      "venue" : "Computer Network Security, ser. Lecture Notes in Computer Science, I. Kotenko and V. Skormin, Eds. Springer Berlin / Heidelberg, 2010, vol. 6258, pp. 118–124.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Design, implementation and evaluation of covert channel attacks",
      "author" : [ "H. Okhravi", "S. Bak", "S.T. King" ],
      "venue" : "HST ’10: Proceedings of IEEE Conference on Technologies for Homeland Security, Oct 2010.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Covert channels–here to stay?",
      "author" : [ "I. Moskowitz", "M. Kang" ],
      "venue" : "Computer Assurance,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1994
    }, {
      "title" : "Open problems in the security of learning",
      "author" : [ "M. Barreno", "P.L. Bartlett", "F.J. Chi", "A.D. Joseph", "B. Nelson", "B.I.P. Rubinstein", "U. Saini", "J.D. Tygar" ],
      "venue" : "Conference on Computer and Communications Security, Proceedings of the 1st ACM Workshop on Workshop on AISec. Alexandria, Virginia, USA: Assocation for Computing Machinery, 2008, pp. 19–26.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Probabilistic Finite State Machines – Part I",
      "author" : [ "E. Vidal", "F. Thollard", "C. de la Higuera", "F. Casacuberta", "R. Carrasco" ],
      "venue" : "PAMI, vol. 27, no. 7, pp. 1013–1025, July 2005.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Probabilistic Finite State Machines – Part II",
      "author" : [ "——" ],
      "venue" : "PAMI, vol. 27, no. 7, pp. 1026–1039, July 2005.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Learning deterministic linear languages",
      "author" : [ "C. De La Higuera", "J. Oncina" ],
      "venue" : "Lecture Notes in Computer Science - Lecture Notes in Artificial Intelligence, vol. 2375, pp. 185–200, 2002.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Learning stochastic finite automata",
      "author" : [ "C. de la Higuera", "J. Oncina" ],
      "venue" : "0302-9743 - Lecture Notes in Computer Science - Lecture Notes in Artificial Intelligence, vol. 3264, no. 3264, pp. 175–186, 2004. April 28, 2011  DRAFT  CRESPI, CYBENKO, GIANI  23",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Embedding covert channels into TCP/IP",
      "author" : [ "S.J. Murdoch", "S. Lewis" ],
      "venue" : "7th Information Hiding Workshop, Barcelona, Catalonia (Spain), June 2005.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Data exfiltration and covert channels",
      "author" : [ "A. Giani", "V.H. Berk", "G. Cybenko" ],
      "venue" : "Proceedings of the SPIE Vol. 6201, Sensors, and Command, Control, Communications, and Intelligence (C3I) Technologies for Homeland Security and Homeland Defense IV Orlando, Florida, April 2006.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Statistical methods for speech recognition",
      "author" : [ "F. Jelinek" ],
      "venue" : null,
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 1997
    }, {
      "title" : "Combinatorial optimization : algorithms and complexity",
      "author" : [ "C.H. Papadimitriou", "K. Steiglitz" ],
      "venue" : null,
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 1998
    }, {
      "title" : "Cognitive Hacking",
      "author" : [ "G. Cybenko", "A. Giani", "P. Thompson" ],
      "venue" : "Advances in Computers, vol. 60, pp. 36–75, 2004.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Cognitive Hacking: a battle for the mind",
      "author" : [ "——" ],
      "venue" : "IEEE Computer, vol. 35, no. 8, pp. 50–56, 2002.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "An information–theoretic and game–theoretic study of timing channels",
      "author" : [ "J. Giles", "B. Hajek" ],
      "venue" : "Information Theory, IEEE Transactions on, vol. 48, no. 9, pp. 2455–2477, sep 2002.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "The power of amnesia",
      "author" : [ "D. Ron", "Y. Singer", "N. Tishby" ],
      "venue" : "Advances in Neural Information Processing Systems, vol. 6, 1993.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Learning probabilistic automata with variable memory length",
      "author" : [ "——" ],
      "venue" : "Proceedings of the Workshop on Computational Learning Theory, 1994.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "PAC-learnability of probabilistic deterministic finite state automata in terms of variation distance",
      "author" : [ "N. Palmer", "P.W. Goldberg" ],
      "venue" : "Theor. Comput. Sci., vol. 387, no. 1, pp. 18–31, 2007.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "PAC-learnability of probabilistic deterministic finite state automata",
      "author" : [ "A. Clark", "F. Thollard" ],
      "venue" : "Journal of Machine Learning Research, vol. 5, pp. 437–497, 2004.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "On the computational complexity of approximating distributions by probabilistic automata",
      "author" : [ "N. Abe", "M. Warmuth" ],
      "venue" : "Machine Learning, vol. 9, pp. 205–260, 1992.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 1992
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "[1] and Cabuk et al.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "[2] estimate certain first order statistics of packet inter-arrival delays in order to determine whether a time covert channel is being used.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[3] learn a Hidden Markov Process [4], [5], [6], [7], [8] using both packet inter-arrival delays and packet sizes to detect traffic anomalies.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[3] learn a Hidden Markov Process [4], [5], [6], [7], [8] using both packet inter-arrival delays and packet sizes to detect traffic anomalies.",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 4,
      "context" : "[3] learn a Hidden Markov Process [4], [5], [6], [7], [8] using both packet inter-arrival delays and packet sizes to detect traffic anomalies.",
      "startOffset" : 39,
      "endOffset" : 42
    }, {
      "referenceID" : 5,
      "context" : "[3] learn a Hidden Markov Process [4], [5], [6], [7], [8] using both packet inter-arrival delays and packet sizes to detect traffic anomalies.",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 6,
      "context" : "[3] learn a Hidden Markov Process [4], [5], [6], [7], [8] using both packet inter-arrival delays and packet sizes to detect traffic anomalies.",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 7,
      "context" : "[3] learn a Hidden Markov Process [4], [5], [6], [7], [8] using both packet inter-arrival delays and packet sizes to detect traffic anomalies.",
      "startOffset" : 54,
      "endOffset" : 57
    }, {
      "referenceID" : 8,
      "context" : "Other techniques are based on various analyses of n-gram statistics [9].",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 8,
      "context" : "In fact, some have called techniques that match n-gram statistics “mimicry attacks” and while techniques have been developed for detecting certain simple types of mimicry, techniques for building mimicry attacks as described in the present paper appear to be novel [9].",
      "startOffset" : 265,
      "endOffset" : 268
    }, {
      "referenceID" : 9,
      "context" : "General discussions of covert channels and their taxonomies, existence and modeling have been published [10], [11], [12], [13], [14].",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 10,
      "context" : "General discussions of covert channels and their taxonomies, existence and modeling have been published [10], [11], [12], [13], [14].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 11,
      "context" : "General discussions of covert channels and their taxonomies, existence and modeling have been published [10], [11], [12], [13], [14].",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 12,
      "context" : "General discussions of covert channels and their taxonomies, existence and modeling have been published [10], [11], [12], [13], [14].",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 13,
      "context" : "General discussions of covert channels and their taxonomies, existence and modeling have been published [10], [11], [12], [13], [14].",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 13,
      "context" : "The design, implementation and experimental evaluation of several specific covert channel attacks in real systems is of specific interest [14].",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 9,
      "context" : "It is important to note that researchers often talk about entropy as a channel statistic [10], [15] but entropy is typically",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 14,
      "context" : "It is important to note that researchers often talk about entropy as a channel statistic [10], [15] but entropy is typically",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 15,
      "context" : "Researchers have recently started to develop systematic taxonomies and examples of attacks against statistical machine learning techniques [16].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 16,
      "context" : "After estimating the k-order statistics, we build a Probabilistic Deterministic Finite Automata model (PDFA) [18], [19], [20], [21] that realizes the k-order statistics.",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 17,
      "context" : "After estimating the k-order statistics, we build a Probabilistic Deterministic Finite Automata model (PDFA) [18], [19], [20], [21] that realizes the k-order statistics.",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 18,
      "context" : "After estimating the k-order statistics, we build a Probabilistic Deterministic Finite Automata model (PDFA) [18], [19], [20], [21] that realizes the k-order statistics.",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 19,
      "context" : "After estimating the k-order statistics, we build a Probabilistic Deterministic Finite Automata model (PDFA) [18], [19], [20], [21] that realizes the k-order statistics.",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 20,
      "context" : "Several such fields are known to exist [22].",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 21,
      "context" : "The following discussion shows how a timing covert channel can be constructed based on a beacon and argues that a naive encoding of covert messages based on packet inter-arrival times produces a clearly detectable distortion of the 1-order statistics of those time intervals in network traffic [23], [1].",
      "startOffset" : 294,
      "endOffset" : 298
    }, {
      "referenceID" : 0,
      "context" : "The following discussion shows how a timing covert channel can be constructed based on a beacon and argues that a naive encoding of covert messages based on packet inter-arrival times produces a clearly detectable distortion of the 1-order statistics of those time intervals in network traffic [23], [1].",
      "startOffset" : 300,
      "endOffset" : 303
    }, {
      "referenceID" : 0,
      "context" : "Another, equivalent way to derive these relations is to note that there are two trivial solutions for X , namely X1 = I2 (the 2 by 2 identity matrix) and X2 = 1 · π where 1 = [1 1] is the column vector whose entries are all 1’s.",
      "startOffset" : 175,
      "endOffset" : 180
    }, {
      "referenceID" : 0,
      "context" : "Another, equivalent way to derive these relations is to note that there are two trivial solutions for X , namely X1 = I2 (the 2 by 2 identity matrix) and X2 = 1 · π where 1 = [1 1] is the column vector whose entries are all 1’s.",
      "startOffset" : 175,
      "endOffset" : 180
    }, {
      "referenceID" : 17,
      "context" : "Such models are called k-order Markov Models, k Markov Chains or k-gram models by different authors [19], [24].",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 22,
      "context" : "Such models are called k-order Markov Models, k Markov Chains or k-gram models by different authors [19], [24].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 23,
      "context" : "From the theory of linear programming, there are feasible solutions to the linear program defined by (6), (7) and (8) which have no more than pre(y) + post(y) − 1 nonzero coordinates, namely the Basic Feasible Solutions [25].",
      "startOffset" : 220,
      "endOffset" : 224
    }, {
      "referenceID" : 24,
      "context" : "The adversarial nature of this situation falls within the scope of cognitive attacks [26], [27].",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 25,
      "context" : "The adversarial nature of this situation falls within the scope of cognitive attacks [26], [27].",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 26,
      "context" : "Discussions of such queuing aspects of timing channels and the possibility of jamming them have been studied [28].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 27,
      "context" : "In particular, Probabilistic Finite Automata (PFA) [29], [30], [31] could implement Markov Chains based on k-grams but using fewer states.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 28,
      "context" : "In particular, Probabilistic Finite Automata (PFA) [29], [30], [31] could implement Markov Chains based on k-grams but using fewer states.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 29,
      "context" : "In particular, Probabilistic Finite Automata (PFA) [29], [30], [31] could implement Markov Chains based on k-grams but using fewer states.",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 30,
      "context" : "They can be learned efficiently in the KL-PAC sense [32], [33], [34] and are generally smaller than k-gram based Markov chains (by having fewer states).",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 31,
      "context" : "They can be learned efficiently in the KL-PAC sense [32], [33], [34] and are generally smaller than k-gram based Markov chains (by having fewer states).",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 2,
      "context" : "• It is reasonable to ask how our results relate to the use of Hidden Markov Models for modeling traffic, as for example in [3].",
      "startOffset" : 124,
      "endOffset" : 127
    }, {
      "referenceID" : 7,
      "context" : "It is known that a Hidden Markov Model with n states is completely determined by the 2n-grams produced by the model so that reproducing 2n-gram statistics will result in the same n state Hidden Markov Model [8].",
      "startOffset" : 207,
      "endOffset" : 210
    } ],
    "year" : 2011,
    "abstractText" : "In this paper we present methods for attacking and defending k-gram statistical analysis techniques that are used, for example, in network traffic analysis and covert channel detection. The main new result is our demonstration of how to use a behavior’s or process’ k-order statistics to build a stochastic process that has those same k-order stationary statistics but possesses different, deliberately designed, (k + 1)order statistics if desired. Such a model realizes a “complexification” of the process or behavior which a defender can use to monitor whether an attacker is shaping the behavior. By deliberately introducing designed (k + 1)-order behaviors, the defender can check to see if those behaviors are present in the data. We also develop constructs for source codes that respect the k-order statistics of a process while encoding covert information. One fundamental consequence of these results is that certain types of behavior analyses techniques come down to an arms race in the sense that the advantage goes to the party that has more computing resources applied to the problem. Points of view in this document are those of the authors and do not necessarily represent the official position of the sponsoring agencies or the U.S. Government. V. Crespi is with the Department of Computer Science, California State University at Los Angeles, Los Angeles CA, 90032 USA. email: vcrespi@calstatela.edu. Crespi’s work was partially supported by AFOSR Grant FA9550-07-1-0421 and by NSF Grant HRD-0932421. G. Cybenko is with the Thayer School of Engineering, Dartmouth College, Hanover NH 03755. email: gvc@dartmouth.edu. Cybenko’s work was partially supported by Air Force Research Laboratory contracts FA8750-10-1-0045, FA8750-09-1-0174, AFOSR contract FA9550-07-1-0421, U.S. Department of Homeland Security Grant 2006-CS-001-000001 and DARPA Contract HR001-06-1-0033 A. Giani is with the Department of EECS, University of California at Berkeley, Berkeley CA 94720. email: agiani@eecs.berkeley.edu. Giani’s’s work was partially supported by U.S. Department of Homeland Security Grant 2006-CS001-000001 and DARPA Contract HR001-06-1-0033 when she was a Ph.D. student at Dartmouth April 28, 2011 DRAFT ar X iv :1 10 4. 50 71 v1 [ cs .L G ] 2 7 A pr 2 01 1 CRESPI, CYBENKO, GIANI 2",
    "creator" : "LaTeX with hyperref package"
  }
}