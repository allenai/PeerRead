{
  "name" : "1508.04145.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Paul F. Christiano" ],
    "emails" : [ "benja@intelligence.org", "jessica@intelligence.org", "paulfchristiano@eecs.berkeley.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 8.\n04 14\n5v 1\n[ cs\n.A I]\n1 7\nA ug\n2 01\nIn this paper, we introduce a “reflective” type of oracle, which is able to answer questions about the outputs of oracle machines with access to the same oracle. These oracles avoid diagonalization by answering some queries randomly. We show that machines with access to a reflective oracle can be used to define rational agents using causal decision theory. These agents model their environment as a probabilistic oracle machine, which may contain other agents as a non-distinguished part.\nWe show that if such agents interact, they will play a Nash equilibrium, with the randomization in mixed strategies coming from the randomization in the oracle’s answers. This can be seen as providing a foundation for classical game theory in which players aren’t special."
    }, {
      "heading" : "1 Introduction",
      "text" : "Classical decision theory and game theory are founded on the notion of a perfect Bayesian reasoner [2]. Such an agent may be uncertain which of several possible worlds describes the state of its environment, but given any particular possible world, it is able to deduce exactly what outcome each of its available actions will produce [3]. This assumption is, of course, unrealistic [4, 5]: Agents in the real world must necessarily\nResearch supported by the Machine Intelligence Research Institute (intelligence.org). This is an extended version of [1].\nbe boundedly rational reasoners, which make decisions with finite computational resources. Nevertheless, the notion of a perfect Bayesian reasoner provides an analytically tractable first approximation to the behavior of real-world agents, and underlies an enormous body of work in statistics [6], economics [7], computer science [8], and other fields.\nOn closer examination, however, the assumption that agents can compute what outcome each of their actions leads to in every possible world is troublesome even if we assume that agents have unbounded computing power. For example, consider the game of Matching Pennies, in which two players each choose between two actions (“heads” and “tails”); if the players choose the same action, the first player wins a dollar, if they choose differently, the second player wins. Suppose further that both players’ decision-making processes are Turing machines with unlimited computing power. Finally, suppose that both players know the exact state of the universe at the time they begin deliberating about the actions they are going to choose, including the source code of their opponent’s decision-making algorithm.1\nIn this set-up, by assumption, both agents know exactly which possible world they are in. Suppose that they are able to use this information to accurately predict their opponent’s behavior. Since both players’ decision-making processes are deterministic Turing machines, their behavior is deterministic given the initial state of the world; each player either definitely plays “heads” or definitely plays “tails”. But neither of these possibilities is consistent: For example, if the first player chooses heads and the second player can predict this, the second player will choose tails, but if the first player can predict this in turn, it will choose tails, contradicting the assumption that it chooses heads.\nThe problem is caused by the assumption that given its opponent’s source code, a player can figure out what action the opponent will choose. One might think that it could simply run its opponent’s source code, but if the opponent does the same, both programs will go into an infinite loop. Binmore [10], discussing the philosophical\n1The technique of quining (Kleene’s second recursion theorem [9]) shows that it is possible to write two programs that have access to each other’s source code.\njustification for game-theoretic concepts such as Nash equilibrium, puts this problem as follows:\nIn any case, if Turing machines are used to model the players, it is possible to suppose that the play of a game is prefixed by an exchange of the players’ Gödel numbers. . .Within this framework, a perfectly rational machine ought presumably to be able to predict the behavior of the opposing machines perfectly, since it will be familiar with every detail of their design. And a universal Turing machine can do this. What it cannot do is predict its opponents’ behavior perfectly and simultaneously participate in the action of the game. It is in this sense that the claim that perfect rationality is an unattainable ideal is to be understood.\nEven giving the players access to a halting oracle does not help, because even though a machine with access to a halting oracle can predict the behavior of an ordinary Turing machine, it cannot in general predict the behavior of another oracle machine.\nClassical game theory resolves this problem by allowing players to choose mixed strategies (probability distributions over actions); for example, the unique Nash equilibrium of Matching Pennies is for each player to assign “heads” and “tails” probability 0.5 each. However, instead of treating players’ decision-making algorithms as computable processes which are an ordinary part of a world with computable laws of physics, classical game theory treats players as special objects. For example, to describe a problem in game-theoretic terms, we must provide an explicit list of all relevant players, even though in the real world, “players” are ordinary physical objects, not fundamentally distinct from objects such as rocks or clouds.\nIn this paper, we show that it is possible to define a certain kind of probabilistic oracle—that is, an oracle which answers some queries non-deterministically— such that a Turing machine with access to this oracle can perform perfect Bayesian reasoning about environments that can themselves be described as oracle machines with access to the same oracle. This makes it possible for players to treat opponents simply as an ordinary part of this environment.\nWhen an environment contains multiple agents playing a game against each other, the probabilistic behavior of the oracle may cause the players’ behavior to be probabilistic as well. We show that in this case, the players will always play a Nash equilibrium, and for every particular Nash equilibrium there is an oracle that causes the players to behave according to this equilibrium. In this sense, our work can be seen as providing a foundation for classical game theory, demonstrating that the special treatment of players in the classical theory is not fundamental.\nThe oracles we consider are not halting oracles; instead, roughly speaking, they allow oracle machines\nwith access to such an oracle to determine the probability distribution of outputs of other machines with access to the same oracle. Because of their ability to deal with self-reference, we refer to these oracles as reflective oracles."
    }, {
      "heading" : "2 Reflective Oracles",
      "text" : "In many situations, programs would like to predict the output of other programs. They could simulate the other program in order to do this. However, this method fails when there are cycles (e.g. program A is concerned with the output of program B which is concerned with the output of program A). Furthermore, if a procedure to determine the output of another program existed, then it would be possible to construct a liar’s paradox of the form “if I return 1, then return 0, otherwise return 1”.\nThese paradoxes can be resolved by using probabilities. Let M be the set of probabilistic oracle machines, defined here as Turing machines which can execute special instructions to (i) flip a coin that has an arbitrary rational probability of coming up heads, and to (ii) call an oracle O, whose behavior might itself be probabilistic.\nRoughly speaking, the oracle answers questions of the form: “Is the probability that machine M returns 1 greater than p?” Thus, O takes two inputs, a machine M ∈ M and a rational probability p ∈ [0, 1] ∩ Q, and returns either 0 or 1. If M is guaranteed to halt and to output either 0 or 1 itself, we want O(M,p) = 1 to mean that the probability that M returns 1 (when run with O) is at least p, and O(M,p) = 0 to mean that it is at most p; if it is equal to p, both conditions are true, and the oracle may answer randomly. In summary,\nP(MO() = 1) > p =⇒ P(O(M,p) = 1) = 1\nP(MO() = 1) < p =⇒ P(O(M,p) = 0) = 1\nwhere we write P(MO() = 1) for the probability thatM returns 1 when run with oracle O, and P(O(M,p) = 1) for the probability that the oracle returns 1 on input (M,p). We assume that different calls to the oracle are stochastically independent events (even if they are about the same pair (M,p)); hence, the behavior of an oracle O is fully specified by the probabilities P(O(M,p) = 1).\nDefinition A query (with respect to a particular oracle O) is a pair (M,p), where p ∈ [0, 1]∩Q and MO() is a probabilistic oracle machine which almost surely halts and returns an element of {0, 1}.\nDefinition An oracle is called reflective on R, where R is a set of queries, if it satisfies the two conditions displayed above for every (M,p) ∈ R. It is called reflective if it is reflective on the set of all queries.\nTheorem 2.1. (i) There is a reflective oracle. (ii) For any oracle O and every set of queries R, there is an oracle O′ which is reflective on R and satisfies P(O′(M,p) = 1) = P(O(M,p) = 1) for all (M,p) /∈ R.\nProof. For the proof of (ii), see Appendix B; see also Theorem 5.1, which gives a more elementary proof of a special case. Part (i) follows from part (ii) by choosing R to be the set of all queries and letting O be arbitrary.\nAs an example, consider the machine given by MO() = 1 − O(M, 0.5), which implements a version of the liar paradox by asking the oracle what it will return and then returning the opposite. By the existence theorem, there is an oracle which is reflective on R = {(M, 0.5)}. This is no contradiction: We can set P(O(M, 0.5) = 1) = P(O(M, 0.5) = 0) = 0.5, leading the program to output 1 half the time and 0 the other half of the time."
    }, {
      "heading" : "3 From Reflective Oracles to Causal Decision Theory",
      "text" : "We now show how reflective oracles can be used to implement a perfect Bayesian reasoner. We assume that each possible environment that this agent might find itself in can likewise be modeled as an oracle machine; that is, we assume that the laws of physics are computable by a probabilistic Turing machine with access to the same reflective oracle as the agent. For example, we might imagine our agent as being embedded in a Turing-complete probabilistic cellular automaton, whose laws are specified in terms of the oracle.\nWe assume that each of the agent’s hypotheses about which environments it finds itself in can be modeled by a (possibly probabilistic) “world program” HO(), which simulates this environment and returns a description of what happened. We can then define a machine WO() which samples a hypothesis H according to the agent’s probability distribution and runs HO(). In the sequel, we will talk about WO() as if it refers to a particular environment, but this machine is assumed to incorporate subjective uncertainty about the laws of physics and the initial state of the world.\nWe further assume that the agent’s decision-making process, AO(), can be modeled as a probabilistic oracle machine embedded in this environment. As a simple example, consider the world program\nWO() =\n{\n$20 if AO() = 0\n$15 otherwise\nIn this world, the outcome is $20 (which in this case means the agent receives $20) if the agent chooses action 0 and $15 if the agent chooses action 1.\nOur task is to find an appropriate implementation of AO(). Here, we consider agents implementing causal\ndecision theory (CDT) [11], which evaluates actions according to the consequences they cause: For example, if the agent is a robot embedded in a cellular automaton, it might evaluate the expected utility of taking action 0 or 1 by simulating what would happen in the environment if the output signal of its decision-making component were replaced by either 0 or 1.\nWe will assume that the agent’s model of the counterfactual consequences of taking different actions a is described by a machine WOA (a), satisfying W\nO() = WOA (A\nO()) since in the real world, the agent takes action a = AO(). In our example,\nWOA (a) =\n{\n$20 if a = 0\n$15 otherwise\nWe assume that the agent has a utility function over outcomes, u(·), implemented as a lookup table, which takes rational values in [0, 1].2 Furthermore, we assume that both WOA (0) and W O A (1) halt almost surely and return a value in the domain of u(·). Causal decision theory then prescribes choosing the action that maximizes expected utility; in other words, we want to find an AO() such that\nAO() = argmax a E [ u ( WOA (a) )]\nIn the case of ties, any action maximizing utility is allowed, and it is acceptable for AO() to randomize.\nWe cannot compute this expectation by simply running u(WOA (a)) many times to obtain samples, since the environment might contain other agents of the same type, potentially leading to infinite loops. However, we can find an optimal action by making use of a reflective oracle. This is easiest when the agent has only two actions (0 and 1), but similar analysis extends to any number of actions. Define a machine\nEO() := flip\n(\nu(WOA (1))− u(W O A (0)) + 1\n2\n)\nwhere flip(p) is a probabilistic function that returns 1 with probability p and 0 with probability 1− p.\nTheorem 3.1. O is reflective on {(E, 1/2)} if and only if AO() := O(E, 1/2) returns a utility-maximizing action.\nProof. The demand that AO() return a utilitymaxmizing action is equivalent to\nE[u(WOA (1))] > E[u(W O A (0))] =⇒ A O() = 1 E[u(WOA (1))] < E[u(W O A (0))] =⇒ A O() = 0\nWe have\nP(EO() = 1) = E\n[\nu(WOA (1))− u(W O A (0)) + 1\n2\n]\n2Since the meaning of utility functions is invariant under affine transformations, the choice of the particular interval [0, 1] is no restriction.\nIt is not difficult to check that E[u(WOA (1))] ≷ E[u(WOa (0))] iff P(E\nO() = 1) ≷ 1/2. Together with the definition of AO(), we can use this to rewrite the above conditions as\nP(EO() = 1) > 1/2 =⇒ O(E, 1/2) = 1\nP(EO() = 1) < 1/2 =⇒ O(E, 1/2) = 0\nBut this is precisely the definition of “O is reflective on {(E, 1/2)}”.\nIn order to handle agents which can choose between more than two actions, we can compare action 0 to action 1, then compare action 2 to the best of actions 0 and 1, then compare action 3 to the best of the first three actions, and so on. Adding more actions in this fashion does not substantially change the analysis."
    }, {
      "heading" : "4 From Causal Decision Theory to Nash Equilibria",
      "text" : "Since we have taken care to define our agents’ world models WOA (a) in such a way that they can embed other agents,3 we need not do anything special to pass from single-agent to multi-agent settings. As in the single-agent case, we model the environment by a program WO() that contains embedded agent programs AO1 , . . . , A O n and returns an outcome. We can make the dependency on the agent program explicit by writing WO() = FO(AO1 (), . . . , A O n ()) for some oracle machine FO(· · · ). This allows us to define machines WOi (ai) := F O(ai, A O −i()) := F (AO1 (), . . . , A O i−1(), ai, A O i+1(), . . . , A O n ()), representing the causal effects of player i taking action ai. We assume that each agent has a utility function ui(·) of the same type as in the previous subsection. Hence, we can define the agent programs AOi () just as before:\nAOi () = O(Ei, 1/2)\nEOi () = flip\n(\nui(W O i (1))− ui(W O i (0)) + 1\n2\n)\nHere, each EOi () calls W O i (), which calls A O j () for each j 6= i, which refers to the source code of EOj (), but again, Kleene’s second recursion theorem shows that this kind of self-reference poses no theoretical problem [9].\nThis setup very much resembles the setting of normal-form games. In fact:\n3More precisely, we have only required that WOA (a) always halt and produce a value in the domain of the utility function u(·). Since all our agents do is to perform a single oracle call, they always halt, making them safe to call from WOA (a).\nTheorem 4.1. Given an oracle O, consider the n-player normal-form game in which the payoff of player i, given the pure strategy profile (a1, . . . , an), is E[ui(F\nO(a1, . . . , an))]. The mixed strategy profile given by si := P(A O i () = 1) is a Nash equilibrium of this game if and only if O is reflective on {(E1, 1/2), . . . , (En, 1/2)}.\nProof. For (s1, . . . , sn) to be a Nash equilibrium is equivalent to every player’s mixed strategy being a best response; i.e., a pure strategy ai can only be assigned positive probability if it maximizes\nE[ui(F O(ai, A O −i()))] = E[ui(W O i (ai))]\nBy an application of Theorem 3.1, this is equivalent to O being reflective on {(Ei, 1/2)}.\nNote that, in particular, any normal-form game with rational-valued payoffs can be represented in this way by simply choosing FO to be the identity function. In this case, the theorem shows that every reflective oracle (which exists by Theorem 2.1) gives rise to a Nash equilibrium. In the other direction, Theorem 4.1 together with Theorem 2.1(ii) show that for any Nash equilibrium (s1, . . . , sn) of the normal-form game, there is a reflective oracle such that P(AOi () = 1) = si."
    }, {
      "heading" : "5 From Nash Equilibria to Reflective Oracles",
      "text" : "In the previous section, we showed that a reflective oracle can be used to find Nash equilibria in arbitrary normal-form games. It is interesting to note that we can also go in the other direction: For finite sets R satisfying certain conditions, we can construct normal-form gamesGR such that the existence of oracles reflective on R follows from the existence of Nash equilibria in GR. This existence theorem is a special case of Theorem 2.1, but it not only provides a more elementary proof, but also provides a constructive way of finding such oracles (by applying any algorithm for finding Nash equilibria to GR).\nDefinition A set R of queries is closed if for every (M,p) ∈ R and every oracle O, MO() is guaranteed to only invoke the oracle on pairs (N, q) ∈ R. It is bounded if there is some bound BR ∈ N such that for every (M,p) ∈ R and every oracle O, MO() is guaranteed to invoke the oracle at most BR times.\nDefinition Given a finite set R = {(M1, p1), . . . , (Mn, pn)} and a vector ~x ∈ [0, 1]n, define O~x to be the oracle satisfying P(O~x(Mi, pi) = 1) = xi for i = 1, . . . , n, and P(O~x(M,p) = 1) = 0 for (M,p) /∈ R.\nTheorem 5.1. For any finite, closed, bounded set R = {(M1, p1), . . . , (Mn, pn)}, there is a normal form game\nGR with m := n · (2BR + 1) players, each of which has two pure strategies, such that for any Nash equilibrium strategy profile (s1, . . . , sm), the oracle O~x with ~x := (s1, . . . , sn) is reflective on R.\nProof. We divide the n · (2BR +1) players in our game into three sets: the main players i = 1, . . . , n, the copy players g(i, j) := j · n + i, and the auxiliary players h(i, j) := (BR+j)·n+i, for i = 1, . . . , n, j = 1, . . . , BR.\nThe mixed strategy si of a main player i will determine the probability that O~x(Mi, pi) = 1. We will force sg(i,j) = si, i.e., we will force the mixed strategy of each copy player to equal that of the corresponding main player; thus, the copy players will provide us with independent samples from the Bernoulli(si) distribution, allowing us to simulate up to BR independent calls to O(Mi, pi). Finally, the auxiliary players are used to enforce the constraint sg(i,j) = si, by having the copy player g(i, j) play a variant of Matching Pennies against the auxiliary player h(i, j).\nIn order to define the game’s payoff function, note first that by writing out each possible way that the at most BR oracle calls of M O~x i () might come out, we can write the probability that this machine returns 1 as a polynomial,\nP(MO~xi () = 1) = K ∑\nk=1\nci,k\nn ∏\ni′=1\nx di,k,i′\ni′\nwhere di,k,i′ ≤ BR. We want to force the main player i to choose pure strategy 1 if this probability is strictly greater than pi, pure strategy 0 if it is strictly smaller.\nTo do so, we set player i’s payoff function ui(~a) to\nui(~a) =\n{\n∑K k=1 fi,k(~a), if ai = 1, pi, otherwise\nwhere\nfi,k(~a) =\n{\nci,k if ag(i′,j) = 1 ∀1 ≤ i ′ ≤ n, 1 ≤ j ≤ di,k,i′ 0 otherwise\nThen, assuming we can guarantee sg(i,j) = si, the expected payoff of strategy 1 to player i is exactly P(MO~xi () = 1), while the payoff of strategy 0 is always pi; hence, as desired, the Nash equilibrium conditions force i to choose 1 if the probability is greater than pi, 0 if it is smaller.\nIt remains to choose the payoffs (ug(i,j)(~a), uh(i,j)(~a)) of the copy and auxiliary players. In order to force sg(i,j) = si, we set these payoffs as follows:\nai = 0 ah(i,j) = 0 ah(i,j) = 1\nag(i,j) = 0 (1, 0) (0, 0) ag(i,j) = 1 (0, 1) (1, 0)\nai = 1 ah(i,j) = 0 ah(i,j) = 1\nag(i,j) = 0 (1, 0) (0, 1) ag(i,j) = 1 (0, 0) (1, 0)\nWe show in Appendix A that at Nash equilibrium, these payoffs force sg(i,j) = si.\nTheorem 5.1 is a special case of Theorem 2.1(i). The proof can be adapted to also show an analog of Theorem 2.1(ii), but we omit the details here."
    }, {
      "heading" : "6 Related Work",
      "text" : "Joyce and Gibbard [12] describe one justification for mixed Nash equilibria in terms of causal decision theory. Specifically, they discuss a self-ratification condition that extends CDT to cases when one’s action is evidence of different underlying conditions that might change which actions are rational. An action self-ratifies if and only if it causally maximizes expected utility in a world model that has been updated on the evidence that this action is taken.\nFor example, consider the setting of a matching pennies game where players can predict each other accurately. The fact that player A plays “heads” is evidence that player B will predict that player A will play “heads” and play “tails” in response, so player A would then have preferred to play “tails”, and so the “heads” action would fail to self-ratify. However, the mixed strategy of flipping the coin would self-ratify. Our reflection principle encodes some global constraints on players’ mixed strategies that are similar to selfratification.\nThe question of how to model agents as an ordinary part of the environment is of interest in the speculative study of human-level and smarter-than-human artificial intelligence [13, 14]. Although such systems are still firmly in the domain of futurism, there has been a recent wave of interest in foundational research aimed at understanding their behavior, in order to ensure that they will behave as intended if and when they are developed [15, 16, 14].\nTheoretical models of smarter-than-human intelligence such as Hutter’s universally intelligent agent AIXI [17] typically treat the agent as separate from the environment, communicating only through well-defined input and output channels. In the real world, agents run on hardware that is part of the environment, and Orseau and Ring [13] have proposed formalisms for studying space-time embedded intelligence running on hardware that is embedded in its environment. Our formalism might be useful for studying idealized models of agents embedded in their environment: While real agents must be boundedly rational, the ability to study perfectly Bayesian space-time embedded intelligence might help to clarify which aspects of realistic systems are due to bounded rationality, and which are\ndue to the fact that real agents aren’t cleanly separated from their environment."
    }, {
      "heading" : "7 Conclusions and Future Work",
      "text" : "In this paper, we have introduced reflective oracles, a type of probabilistic oracle which is able to answer questions about the behavior of oracle machines with access to the same oracle. We’ve shown that such oracle machines can implement a version of causal decision theory, and used this to establish a close relationship between reflective oracles and Nash equilibria.\nWe have focused on answering queries about oracle machines that halt with probability 1, but the reflection principle presented in Section 2 can be modified to apply to machines that do not necessarily halt. To do so, we replace the condition\nP(MO() = 1) < p =⇒ P(O(M,p) = 0) = 1\nby the condition\nP(MO() 6= 0) < p =⇒ P(O(M,p) = 0) = 1\nThis is identical to the former principle if MO() is guaranteed to halt, but provides sensible information even if there is a chance that MO() loops. Appendix B proves the existence of reflective oracles satisfying this stronger reflection principle.\nThe ability to deal with non-halting machines opens up the possibility of applying reflective oracles to simplicity priors such as Solomonoff induction [18], which defines a probability distribution over infinite bit sequences by, roughly, choosing a random program and running it. Solomonoff induction deals with computable hypotheses, but is itself uncomputable (albeit computably approximable) because it must deal with the possibility that a randomly chosen program may go into an infinite loop after writing only a finite number of bits on its output tape. A reflective oracle version of Solomonoff induction would be able to deal with a hypothesis space consisting of arbitrary oracle machines, while itself being implementable as an oracle machine; this would make it possible to model a predictor which predicts an environment it is itself embedded in. We leave details to future work."
    }, {
      "heading" : "A Nash Equilibria in a Variant of Matching Pennies",
      "text" : "Lemma A.1. Consider an n-player game with three distinguished players, each of which has two pure strategies: Player Row has strategies Up and Down, player Column has strategies Left and Right, and player Matrix has strategies Front and Back. Suppose that the payoffs of (Row, Column) depend only on the strategies of these three players, as follows:\n(1, 0) (0, 0) (0, 1) (1, 0)\n(1, 0) (0, 1) (0, 0) (1, 0)\nwhere the first matrix indicates the payoffs when Matrix plays Front, and the second matrix indicates the payoffs when Matrix plays Back.\nWrite p for the probability that Row plays Down, and q for the probability that Matrix plays Back. At Nash equilibrium, we have p = q.\nProof. • Case 1: 0 < q < 1.\nSuppose that there is a Nash equilibrium where Column plays Left. Then Row would play Up, but then Column would strictly prefer Right, which is a contradiction.\nSuppose that there is a Nash equilibrium where Column plays Right. Then Row would play Down, but then Column would strictly prefer Left, which is a contradiction.\nThus, at every Nash equilibrium, Column must mix between strategies. Hence, at equilibrium, Column must be indifferent between Left and Right. This is equivalent to p(1 − q) = (1 − p)q. This implies p > 0, since otherwise we’d have 0(1 − q) = (1 − 0)q, i.e. 0 = q, but we assumed 0 < q < 1. Thus, we can divide the equation by pq, yielding:\n(1 − q)/q = (1− p)/p\n⇔ 1/x− 1 = 1/p− 1\n⇔ 1/q = 1/p\n⇔ q = p\n• Case 2: q = 0.\nThis gives us the following payoff matrix:\n(1, 0) (0, 0) (0, 1) (1, 0)\nSuppose that there is a Nash equilibrium with p > 0. Then at this equilibrium, Column must play Left; but if Column plays Left, then Row strictly prefers Up, which contradicts p > 0. Hence, we must have p = 0 = q.\n• Case 3: q = 1.\nThis gives us the following payoff matrix:\n(1, 0) (0, 1) (0, 0) (1, 0)\nSuppose that there is a Nash equilibrium with p < 1. Then at this equilibrium, Column must play Right; but if Column plays Right, then Row strictly prefers Down, which contradicts p < 1. Hence, we must have p = 1 = q."
    }, {
      "heading" : "B Proof of the Existence Theorem",
      "text" : "In this appendix, we prove Theorem 2.1(ii). Thus, suppose that R is a set of queries and O is some oracle; we want to show the existence of an oracle O′ which is reflective on R and satisfies P(O′(M,p) = 1) = P(O(M,p) = 1) for all (M,p) /∈ R.\nWe will describe the behavior of O′ by a pair of functions, query : M × ([0, 1] ∩ Q) → [0, 1] and eval : M → [0, 1]. The first of these gives the distribution of O′, i.e., query(M,p) = P(O′(M,p) = 1). The second gives the distribution of a machine’s behavior under O′: IfM almost surely returns either 0 or 1, then eval(M) = P(MO ′\n() = 1). Function pairs (query, eval) can be seen as elements of A := [0, 1]M×([0,1]∩Q)×[0, 1]M, which is a convex and compact subset of the locally convex topological vector space RM×([0,1]∩Q) ×RM (with the product topology). We now define a correspondence f : A → Pow(A), such that fixed points (query, eval) ∈ f(query, eval) yield oracles O′ of the desired form.\nWe define f by giving a set of necessary and sufficient conditions for (query′, eval′) ∈ f(query, eval). We place three conditions on query′(M,p): If (M,p) ∈ R and eval(M) > p, then query′(M,p) = 1; if (M,p) ∈ R and eval(M) < p, then query′(M,p) = 0; and if (M,p) /∈ R, then query′(M,p) = P(O(M,p) = 1).\nTo describe the conditions on eval′(M), we will consider the definition of “probabilistic oracle machine” to include the initial state of the machine’s working tapes, so that we can view the state of a machine MO() after one step of computation as a new machine NO(). Then, any machine M can be classified as performing one of the following operations as its first step of computation: (i) a deterministic computation step, yielding a new state N , in which case eval′(M) = eval(N); (ii) a coin flip, yielding a state N with a rational probability p and another state N ′ with probability 1 − p, in which case eval′(M) = p · eval(N) + (1 − p) · eval(N ′);\n(iii) halting, with the output tape containing 0 (in which case eval′(M) = 0) or 1 (in which case eval′(M) = 1) or some other output (in which case eval′(M) is arbitrary); or (iv) an invocation of the oracle on a pair (M ′, p), yielding a new state N if the oracle returns 0 and a different new state N ′ if it returns 1. In the last case, writing q := query(M ′, p), the condition is eval′(M) = (1− q) · eval(N) + q · eval(N ′).\nGiven a fixed point (query, eval) ∈ f(query, eval), define O′ by P(O′(M,p) = 1) = query(M,p). Then, it can be shown by induction that for every T ∈ N and everyM ∈ M, eval(M) is ≥ the probability that MO ′\n() returns 1 after at most T timesteps, and ≤ the probability that it returns something other than 0 within this time bound; in the limit, we obtain\nP(MO ′ () = 1) ≤ eval(M) ≤ P(MO ′ () 6= 0)\nTogether with the conditions on query(M,p), this shows that\nP(MO ′ () = 1) > p =⇒ P(O′(M,p) = 1) = 1\nP(MO ′ () = 0) > (1− p) =⇒ P(O′(M,p) = 0) = 1\nwhich is a strengthening of the conditions of Section 2: it is equivalent in the case whereMO ′ () halts with probability 1, but provides information even if MO ′\n() may fail to halt.\nIt remains to be shown that f(·) has a fixed point. To do so, we employ the infinite-dimensional generalization of Kakutani’s fixed-point theorem [19].\nIt is clear from the definition that f(query, eval) is non-empty, closed and convex for all (query, eval) ∈ A. Hence, to show that f has a fixed point, it is sufficient to show that it has closed graph.\nThus, assume that we have sequences (queryn, evaln) → (query, eval) and (query′n, eval ′ n) → (query ′, eval′), such that (query′n, eval ′\nn) ∈ f(queryn, evaln) for every n; we need to show that then, (query′, eval′) ∈ f(query, eval).\nFor the conditions on eval′, we can simply take the limit n → ∞ on both sides of each equation. The condition on query′(M,p) for (M,p) /∈ R is clearly fulfilled, since query′n(M,p) is constant in this case. The two remaining conditions on query′(M,p) are entirely symmetrical; without loss of generality, consider the case eval(M) > p, (M,p) ∈ R.\nIn this case, since (queryn, evaln) → (query, eval) and convergence is pointwise, there must be an n0 such that evaln(M) > p for all n ≥ n0. Since (query′n, eval ′\nn) ∈ f(queryn, evaln), it follows that query′n(M,p) = 1 for all n ≥ n0, whence query′(M,p) = 1 as desired. This completes the proof."
    } ],
    "references" : [ {
      "title" : "Reflective Oracles: A Foundation for Game Theory in Artificial Intelligence",
      "author" : [ "Benja Fallenstein", "Jessica Taylor", "Paul F. Christiano" ],
      "venue" : "Fifth International Workshop,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2015
    }, {
      "title" : "The Foundations of Statistics",
      "author" : [ "Leonard J. Savage" ],
      "venue" : "Dover Books on Mathematics. Dover Publications,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1972
    }, {
      "title" : "Bounded Rationality",
      "author" : [ "Gerd Gigerenzer", "Reinhard Selten", "eds" ],
      "venue" : "The Adaptive Toolbox. Dahlem Workshop Reports",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2001
    }, {
      "title" : "Reasoning with Limited Resources and Assigning Probabilities to Arithmetical Statements",
      "author" : [ "Haim Gaifman" ],
      "venue" : "Synthese",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2004
    }, {
      "title" : "Bayesian Inference in Statistical Analysis",
      "author" : [ "George E.P. Box", "George C. Tiao" ],
      "venue" : "Wiley Classics Library. Wiley,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2011
    }, {
      "title" : "A Course in Microeconomic Theory",
      "author" : [ "David M. Kreps" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1990
    }, {
      "title" : "Bayesian Artificial Intelligence",
      "author" : [ "Kevin B. Korb", "Ann E. Nicholson" ],
      "venue" : "Chapman & Hall/CRC Computer Science & Data Analysis. Taylor & Francis,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2003
    }, {
      "title" : "Theory of Recursive Functions and Effective Computability",
      "author" : [ "Hartley Rogers" ],
      "venue" : "New York: McGraw-Hill,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1967
    }, {
      "title" : "Modeling Rational Players: Part I",
      "author" : [ "Ken Binmore" ],
      "venue" : "Economics and Philosophy",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1987
    }, {
      "title" : "Causal Decision Theory",
      "author" : [ "Paul Weirich" ],
      "venue" : "The Stanford Encyclopedia of Philosophy",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2012
    }, {
      "title" : "Causal Decision Theory",
      "author" : [ "James Joyce", "Allan Gibbard" ],
      "venue" : "Handbook of Utility Theory,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1998
    }, {
      "title" : "Space-Time Embedded Intelligence",
      "author" : [ "Laurent Orseau", "Mark Ring" ],
      "venue" : "Artificial General Intelligence. 5th International Conference, AGI",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2012
    }, {
      "title" : "Aligning Superintelligence with Human Interests",
      "author" : [ "Nate Soares", "Benja Fallenstein" ],
      "venue" : "A Technical Research Agenda. Tech. rep. 2014–8. Berkeley, CA: Machine Intelligence Research Institute,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2014
    }, {
      "title" : "Superintelligence. Paths, Dangers, Strategies",
      "author" : [ "Nick Bostrom" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2014
    }, {
      "title" : "Universal Artificial Intelligence. Sequential Decisions Based On Algorithmic Probability",
      "author" : [ "Marcus Hutter" ],
      "venue" : "Texts in Theoretical Computer Science. Berlin: Springer,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2005
    }, {
      "title" : "A Formal Theory of Inductive Inference. Part I",
      "author" : [ "Ray J. Solomonoff" ],
      "venue" : "Information and Control",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1964
    }, {
      "title" : "Fixed-Point and Minimax Theorems in Locally Convex Topological Linear Spaces",
      "author" : [ "Ky Fan" ],
      "venue" : "Proceedings of the National Academy of Sciences of the United States of America",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1952
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Such an agent may be uncertain which of several possible worlds describes the state of its environment, but given any particular possible world, it is able to deduce exactly what outcome each of its available actions will produce [3].",
      "startOffset" : 230,
      "endOffset" : 233
    }, {
      "referenceID" : 2,
      "context" : "This assumption is, of course, unrealistic [4, 5]: Agents in the real world must necessarily",
      "startOffset" : 43,
      "endOffset" : 49
    }, {
      "referenceID" : 3,
      "context" : "This assumption is, of course, unrealistic [4, 5]: Agents in the real world must necessarily",
      "startOffset" : 43,
      "endOffset" : 49
    }, {
      "referenceID" : 0,
      "context" : "This is an extended version of [1].",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 4,
      "context" : "Nevertheless, the notion of a perfect Bayesian reasoner provides an analytically tractable first approximation to the behavior of real-world agents, and underlies an enormous body of work in statistics [6], economics [7], computer science [8], and other fields.",
      "startOffset" : 202,
      "endOffset" : 205
    }, {
      "referenceID" : 5,
      "context" : "Nevertheless, the notion of a perfect Bayesian reasoner provides an analytically tractable first approximation to the behavior of real-world agents, and underlies an enormous body of work in statistics [6], economics [7], computer science [8], and other fields.",
      "startOffset" : 217,
      "endOffset" : 220
    }, {
      "referenceID" : 6,
      "context" : "Nevertheless, the notion of a perfect Bayesian reasoner provides an analytically tractable first approximation to the behavior of real-world agents, and underlies an enormous body of work in statistics [6], economics [7], computer science [8], and other fields.",
      "startOffset" : 239,
      "endOffset" : 242
    }, {
      "referenceID" : 8,
      "context" : "Binmore [10], discussing the philosophical",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 7,
      "context" : "The technique of quining (Kleene’s second recursion theorem [9]) shows that it is possible to write two programs that have access to each other’s source code.",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 0,
      "context" : "Roughly speaking, the oracle answers questions of the form: “Is the probability that machine M returns 1 greater than p?” Thus, O takes two inputs, a machine M ∈ M and a rational probability p ∈ [0, 1] ∩ Q, and returns either 0 or 1.",
      "startOffset" : 195,
      "endOffset" : 201
    }, {
      "referenceID" : 0,
      "context" : "Definition A query (with respect to a particular oracle O) is a pair (M,p), where p ∈ [0, 1]∩Q and M() is a probabilistic oracle machine which almost surely halts and returns an element of {0, 1}.",
      "startOffset" : 86,
      "endOffset" : 92
    }, {
      "referenceID" : 9,
      "context" : "Here, we consider agents implementing causal decision theory (CDT) [11], which evaluates actions according to the consequences they cause: For example, if the agent is a robot embedded in a cellular automaton, it might evaluate the expected utility of taking action 0 or 1 by simulating what would happen in the environment if the output signal of its decision-making component were replaced by either 0 or 1.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 0,
      "context" : "We assume that the agent has a utility function over outcomes, u(·), implemented as a lookup table, which takes rational values in [0, 1].",
      "startOffset" : 131,
      "endOffset" : 137
    }, {
      "referenceID" : 0,
      "context" : "Since the meaning of utility functions is invariant under affine transformations, the choice of the particular interval [0, 1] is no restriction.",
      "startOffset" : 120,
      "endOffset" : 126
    }, {
      "referenceID" : 7,
      "context" : "Here, each E i () calls W O i (), which calls A O j () for each j 6= i, which refers to the source code of E j (), but again, Kleene’s second recursion theorem shows that this kind of self-reference poses no theoretical problem [9].",
      "startOffset" : 228,
      "endOffset" : 231
    }, {
      "referenceID" : 0,
      "context" : ", (Mn, pn)} and a vector ~x ∈ [0, 1], define O~x to be the oracle satisfying P(O~x(Mi, pi) = 1) = xi for i = 1, .",
      "startOffset" : 30,
      "endOffset" : 36
    }, {
      "referenceID" : 10,
      "context" : "Joyce and Gibbard [12] describe one justification for mixed Nash equilibria in terms of causal decision theory.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 11,
      "context" : "The question of how to model agents as an ordinary part of the environment is of interest in the speculative study of human-level and smarter-than-human artificial intelligence [13, 14].",
      "startOffset" : 177,
      "endOffset" : 185
    }, {
      "referenceID" : 12,
      "context" : "The question of how to model agents as an ordinary part of the environment is of interest in the speculative study of human-level and smarter-than-human artificial intelligence [13, 14].",
      "startOffset" : 177,
      "endOffset" : 185
    }, {
      "referenceID" : 13,
      "context" : "Although such systems are still firmly in the domain of futurism, there has been a recent wave of interest in foundational research aimed at understanding their behavior, in order to ensure that they will behave as intended if and when they are developed [15, 16, 14].",
      "startOffset" : 255,
      "endOffset" : 267
    }, {
      "referenceID" : 12,
      "context" : "Although such systems are still firmly in the domain of futurism, there has been a recent wave of interest in foundational research aimed at understanding their behavior, in order to ensure that they will behave as intended if and when they are developed [15, 16, 14].",
      "startOffset" : 255,
      "endOffset" : 267
    }, {
      "referenceID" : 14,
      "context" : "Theoretical models of smarter-than-human intelligence such as Hutter’s universally intelligent agent AIXI [17] typically treat the agent as separate from the environment, communicating only through well-defined input and output channels.",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 11,
      "context" : "In the real world, agents run on hardware that is part of the environment, and Orseau and Ring [13] have proposed formalisms for studying space-time embedded intelligence running on hardware that is embedded in its environment.",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 15,
      "context" : "The ability to deal with non-halting machines opens up the possibility of applying reflective oracles to simplicity priors such as Solomonoff induction [18], which defines a probability distribution over infinite bit sequences by, roughly, choosing a random program and running it.",
      "startOffset" : 152,
      "endOffset" : 156
    } ],
    "year" : 2015,
    "abstractText" : "Classical game theory treats players as special—a description of a game contains a full, explicit enumeration of all players—even though in the real world, “players” are no more fundamentally special than rocks or clouds. It isn’t trivial to find a decision-theoretic foundation for game theory in which an agent’s coplayers are a non-distinguished part of the agent’s environment. Attempts to model both players and the environment as Turing machines, for example, fail for standard diagonalization reasons. In this paper, we introduce a “reflective” type of oracle, which is able to answer questions about the outputs of oracle machines with access to the same oracle. These oracles avoid diagonalization by answering some queries randomly. We show that machines with access to a reflective oracle can be used to define rational agents using causal decision theory. These agents model their environment as a probabilistic oracle machine, which may contain other agents as a non-distinguished part. We show that if such agents interact, they will play a Nash equilibrium, with the randomization in mixed strategies coming from the randomization in the oracle’s answers. This can be seen as providing a foundation for classical game theory in which players aren’t special.",
    "creator" : "LaTeX with hyperref package"
  }
}