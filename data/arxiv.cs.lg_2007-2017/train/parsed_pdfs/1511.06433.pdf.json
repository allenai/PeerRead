{
  "name" : "1511.06433.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "COMPRESSING LSTMS", "INTO CNNS", "Krzysztof J. Geras", "Abdel-rahman Mohamed", "Rich Caruana", "Gregor Urban", "Shengjie Wang", "Özlem Aslan", "Matthai Philipose", "Matthew Richardson", "Charles Sutton" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "There is evidence that neural networks trained with the current training algorithms use their large capacity inefficiently (Le Cun et al., 1990; Denil et al., 2013; Dauphin & Bengio, 2013; Ba & Caruana, 2014; Hinton et al., 2015). Although this excess capacity may be necessary for accurate learning and generalization at training time, the function once learned often can be represented much more compactly. As deep neural net models become larger and more complex, their accuracy often increases, but the difficulty of fielding them also rises. Methods such as model compression sometimes allow the accurate function learned by large, complex models to be compressed into smaller models that are computationally more efficient at runtime.\nThere are a number of different kinds of deep neural network models such as deep (fully connected) neural networks (DNNs), convolutional neural networks (CNNs), recurrent neural networks (RNNs), and LSTMs (a specific type of RNN), and different domains typically benefit from deep models of different types. For example, CNNs usually yield highest accuracy in domains such as image recognition where the input forms a regular 1, 2, or 3-D image plane with structure that is partially invariant to shifts in position or scale. On the other hand, recurrent networks models such as LSTMs appear to be better suited to applications such as speech recognition or language modeling where inputs form sequences of varying lengths with possibly long-range interactions.\nThe differences between these deep learning architectures raises interesting questions about what is learnable by different kinds of deep models, and when it is possible for a deep model of one kind to represent and learn the function learned by a different kind of deep model. This issue is made all the more interesting because different deep models represent different computational challenges at run time. CNNs usually are computationally more expensive at prediction time than DNNs that contain a similar number of parameters. Similarly, LSTMs usually are more expensive at runtime than CNNs. If LSTMs are capable of learning functions that cannot be learned by CNNs, then there would be little hope of compressing an expensive LSTM into a less expensive CNN. The only hope would be to compress a large expensive LSTM into a smaller, faster LSTM. On the other hand, if CNNs can learn functions similar to those learned by LSTMs, then it might be possible to compress a large, expensive, but accurate LSTM into a smaller, faster CNN (or possibly even a DNN) with little loss in accuracy but large improvements in computational cost at runtime.\nIn this paper we demonstrate that, for a speech recognition task, it is possible to train accurate CNN models if they are given the right architecture inspired by recent developments in computer vision. Moreover, the experiments suggest that LSTMs and CNNs learn different functions when trained on the same data. This difference creates an opportunity: by merging the functions learned by CNNs\nar X\niv :1\n51 1.\n06 43\n3v 1\n[ cs\n.L G\n] 1\n9 N\nov 2\n01 5\nand LSTMs into a single model we can obtain better accuracy than either model class was capable of achieving independently. One approach to perform this merge is to use model compression. This yields computationally efficient CNN that has high accuracy and no extra cost at runtime. We explain the success of this approach by resemblance to a weighted ensemble of the two functions. Although just simple ensembling also yields high accuracy, it results in a model that is computationally more expensive at runtime than either the CNN or LSTM."
    }, {
      "heading" : "2 BACKGROUND",
      "text" : "Model compression (Bucila et al., 2006) is a machine learning technique which aims at training one model (a student) by mimicing another model (a teacher). Typically, the student model is a smaller, yet flexible, model and the teacher is a large, powerful model, which performs well in terms of accuracy though its execution at the test time is too computationally expensive.\nFor a classification task, this mimicry can be performed in two ways. One way is to train the student model to match logits (i.e. the values zi in the output layer of the network, before applying the softmax to compute pi = ezi/ ∑ j e\nzj ) predicted by the teacher on the training data, penalising the difference between logits of the two models with a squared loss. Alternatively, it can be done by training the student model to match class probabilities predicted by the teacher, by penalising cross-entropy between predictions of the teacher p (we will refer to them as soft labels) and the student q, i.e. by minimising - ∑ i pi(x) log qi(x) averaged over training examples. In the context of deep neural networks, this approach to model compression is also known as knowledge distillation (Hinton et al., 2015). Additionally, the student can also be shown the original labels of the training data (we will refer to them as hard labels).\nThe main advantage of training the student model using the model compression technique is the fact that the student trained with knowledge provided by the teacher gets a richer supervision signal than just the hard labels from the training data set, i.e. for each training example, it gets the information not only about the correct class but also about how similar the current training example is to other classes. Model compression can also be viewed as way to transfer inductive biases between models. For example, in the case of compressing deep models into shallow ones (Ba & Caruana, 2014), the student model is benefiting from hierarchical representation in the deep model, although it is not able to learn it on its own from hard labels.\nWhile model compression can be applied to arbitrary classifiers producing probabilistic prediction over target classes, with the recent success of deep neural networks fueled by new training techniques and dramatic increase in the computational power of modern computers, work on model compression focused on compressing large deep neural networks or ensembles thereof into smaller ones. That is, with less layers, less hidden units or less parameters. In the first paper pursuing that direction, Ba & Caruana (2014) showed that an ensemble of deep neural networks without convolutional layers can be compressed into a single layer network as accurate as a deep one. In a complementary work, Hinton et al. (2015) focused on compressing ensembles of deep networks into deep networks of the same architecture. They also experimented with softening predictions of the teacher by dividing the logits by a constant T greater than one called temperature. Using the techniques developed in prior work and adding an extra mimic layer in the middle of the student network, Romero et al. (2014) demonstrated that a moderately deep and wide convolutional network can be compressed into a deeper and narrower convolutional network with much fewer parameters than the teacher network."
    }, {
      "heading" : "2.1 BIDIRECTIONAL LSTM",
      "text" : "A great example of a very powerful neural network architecture yielding state-of-the-art performance on a range of tasks, yet expensive to run at the test time are the long-short memory network (LSTM) (Hochreiter & Schmidhuber, 1997) and the bidirectional LSTM (Graves & Schmidhuber, 2005; Graves et al., 2013), whose compression is the focus of this work.\nLSTM is a member of a broad family of recurrent neural networks (RNNs). Given a sequence of input vectors x = (x1, . . . , xT ), a standard RNN computes the hidden vector sequence h = (h1, . . . , hT ) and output vector sequence y = (y1, . . . , yT ) by iterating the following equations\nfrom t = 1 to T :\nht = H (Wxhxt +Whhht−1 + bh) , yt =Whyht + by,\nwhere theW terms denote weight matrices (e.g.Wxh is the input-hidden weight matrix), the b terms denote bias vectors (e.g. bh is hidden bias vector) andH is the hidden layer function. H is usually an element-wise application of a sigmoid function. Prior work (Graves, 2012; Graves et al., 2013; Sak et al., 2014) has shown however that the LSTM architecture, which uses purposebuilt memory cells to store information, is better at finding and exploiting long range context. Figure 1 illustrates a single LSTM memory cell. For the version of LSTM used in this paper (Gers et al., 2003)H is implemented by the following composite function:\nit = σ (Wxixt +Whiht−1 +Wcict−1 + bi) ,\nft = σ (Wxfxt +Whfht−1 +Wcfct−1 + bf ) ,\nct = ftct−1 + it tanh (Wxcxt +Whcht−1 + bc) ,\not = σ (Wxoxt +Whoht−1 +Wcoct + bo) ,\nht = ot tanh(ct),\nwhere σ is the logistic sigmoid function, and i, f , o and c are respectively the input gate, forget gate, output gate and cell activation vectors, all of which are the same size as the hidden vector h. The weight matrices from the cell to gate vectors (e.g. Wsi) are diagonal, so element m in each gate vector only receives input from element m of the cell vector.\nOne shortcoming of conventional RNNs is that they are only able to make use of previous context. In speech recognition, where whole utterances are transcribed at once, there is no reason not to exploit future context as well. Bidirectional RNNs (BRNNs) (Schuster & Paliwal, 1997) do this by processing the data in both directions with two separate hidden layers, which are then fed forwards to the same output layer. As illustrated in Figure 2, a BRNN computes the forward hidden sequence −→ h = ( −→ h 1, . . . , −→ h T ) the backward hidden sequence ←− h = ( ←− h 1, . . . , ←− h T ) and the output sequence y by iterating the backward layer from t = T to 1, the forward layer from t = 1 to T and then updating the output layer:\n−→ h t = H ( W\nx −→ h xt +W−→h−→h −→ h t−1 + b−→h\n) ,\n←− h t = H ( W\nx ←− h xt +W←−h←−h ←− h t+1 + b←−h\n) ,\nyt =W−→h y −→ h t +W←−h y ←− h t + by.\nCombing BRNNs with LSTM gives bidirectional LSTM, which can access the context in both input directions.\nFinally, deep RNNs can be created by stacking multiple RNN hidden layers on top of each other, with the output sequence of one layer forming the input sequence for the next. Assuming the same hidden layer function is used for all N layers in the stack, the hidden vector sequences hn are iteratively computed from n = 1 to N and t = 1 to T :\nhnt = H ( Whn−1hnh n−1 t +Whnhnh n t−1 + b n h ) ,\nwhere we define h0 = x. The network outputs yt are\nyt =WhNyh N t + by.\nDeep bidirectional RNNs can be implemented by replacing each hidden sequence hn with the forward and backward sequences −→ h n and ←− h n , and ensuring that every hidden layer receives input from both the forward and backward layers at the level below. If LSTM is used for the hidden layers we get deep bidirectional LSTM, the architecture we use as a teacher network in this paper."
    }, {
      "heading" : "3 VISION-STYLE CNNS FOR SPEECH RECOGNITION",
      "text" : "Convolutional neural networks (LeCun et al., 1998) were considered for speech for a long time (LeCun & Bengio, 1998; Lee et al., 2009), though only recently they have become very successful (Abdel-Hamid et al., 2012; Sainath et al., 2013; Abdel-Hamid et al., 2014; Sainath et al., 2015). The most successful of previous works on CNNs for speech used only two or three convolutional layers followed by fully connected layers. They also only did convolution and pooling over one dimension, either time or frequency. However, when looking at a spectrogram in Figure 5, it is obvious that similar patterns re-occur both across different points in time and across different frequencies. Using convolution across only one of these dimensions seems wasteful. We hypothesise that classification of windows of speech with CNNs is not that different from object recognition with CNNs.\nLooking at this problem through the lens of computer vision, we use a convolutional network architecture inspired by the work of Simonyan & Zisserman (2014). We only use small convolutional filters of size 3×3, non-overlapping 2×2 pooling regions and our network also has more layers than networks previously considered for the purpose of speech recognition. We use the same architecture for both baseline and student networks (described in detail in Table 1)."
    }, {
      "heading" : "4 COMPRESSING BIDIRECTIONAL LSTMS INTO VISION-STYLE CNNS",
      "text" : "We optimise the following training objective,\nL(λ) = λ −∑ j ∑ i p(i|xj) log q(i|xj) + (1− λ) −∑ j log q(yj |xj)  , (1) where p(i|xj) is the probability of class i for training example xj estimated by the teacher, q(i|xj) is the probability of class i assigned to training example xj by the student and yj is the correct class for training example xj . The coefficient λ ∈ [0, 1] controls the weight of the errors on soft and hard labels in the objective. λ = 0 means the network is only learning using the hard labels, ignoring the teacher, while λ = 1 means that the networks is only learning from the soft labels provided by the teacher, ignoring the hard labels. When λ /∈ {0, 1} optimising the objective in Equation 3 is a form of creating an implicit ensemble of the teacher model and what would be the model of the baseline architecture if it was trained separately.\nWe motivate the choice of working directly with probabilities instead of logits in two ways. Firstly, it is more direct to interpret retaining the subset of outputs looking at the fraction of the probability mass they cover, which will be necessary in our work (see section 5). Secondly, when using both soft and hard targets, it is easier to find an appropriate λ and learning rate, when the two objectives we are weighting together are of similar magnitudes and optimisation landscapes."
    }, {
      "heading" : "5 EXPERIMENTS",
      "text" : "In the experiments we use the Switchboard data set (Godfrey et al., 1992). The Switchboard data set consists of 309 hours of transcribed speech. We used 308 hours as training set and kept remaining one hour as a validation set. The data set is segmented into 248k utterances, i.e. continuous pieces of speech beginning and ending with a clear pause. Each utterance consists of a number of frames, i.e. 25 ms intervals of speech, with a constant shift of 10 ms. For every frame, the extracted features are 31-channel Mel-filterbank parameters passed through a 10-th root nonlinearity. Features for one utterance are visualised in Figure 5. To form our training and validation sets we extract windows of 41 frames, that is, the frame whose label we want to predict, 20 frames before it and 20 frames after it. As shown in Figure 3, distribution of the lengths of utterances is highly non-uniform, therefore to keep the sampling unbiased, we sample training examples by first sampling an utterance proportionally to its length and then sampling a window within that utterance uniformly. To form the validation set we simply extract all possible windows. In both cases, we pad each utterance with zeros at the beginning and at the end so that every frame in each utterance can be drawn as a middle frame. Every frame in the training and validation set has a label. The 9000 output classes represent tied tri-phone states that are generated by an HMM/GMM system (Young et al., 1994). Forced alignment is used to map each input frame to one output state using a baseline DNN model. The distribution of classes in the training data is visualised in Figure 4. We call the frame classification error on the validation set frame error rate (FER). The test set is a part of the standard Switchboard benchmark (the Hub5’00 SW data set). It was sampled from the same distribution as the training set and consists of 1831 utterances.\nThere are no frame-level labels in the test set and the final evaluation is based on the ability to predict words in the test utterances. To obtain the words predicted by the model, frame label posteriors generated from the neural network are first divided by their prior probabilities then passed to a finite state transducer-based decoder to be combined with 3-gram language model probabilities to generate the most probable word sequences. Hypothesized word sequences are aligned to the human reference transcription to count the number of word insertions (I), deletions (D), and substitutions (S). Word Error Rate (WER) is defined as WER = S+D+IN , where N is the total number of words in the reference transcription.\nSince the teacher model is very slow at prediction time, it would be impractical to do all the experiments running it every time when training a student. Unfortunately, running the teacher once and saving all its predictions to hard disk is also problematic. Because the output space is very large, storing the soft labels for all classes would require a vast amount of space (≈ 3.6 TB). Moreover,\n0 1500 3000 4500 6000 7500 9000 number of classes\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nfr a ct\nio n o\nf cl\na ss\np ro\nb a b ili\nty c\no v e re\nd\nFigure 4: Fraction of class probability covered as a function of the number of most likely classes included. Dashed line indicates what the curve would look like if the labels had a uniform distribution.\nto sample each minibatch in an unbiased manner, we would need constant random access to the hard drive, which would make training very slow. To deal with that problem we decided to save predictions only for a subset of classes. To diagnose whether it is a plausible solution, we checked what percentage of probability mass, averaged over the examples in the training set, is covered by the C most likely classes according to the teacher model. We denote that quantity TOPC(x). That is, we compute\nM(C) = 1 |{xi}| ∑ xi ∑ y∈TOPC(xi) p(y|xi), (2)\nwhere p(y|x) denotes the posterior probability of class y given a feature vector x. This relationship is shown in Figure 6. We found that, with very few exceptions, posteriors over classes are concentrated on very few values. Therefore, we decided to continue our experiments retaining top classes covering not more than 90 classes for each training example, cutting off after covering 99% probability mass. Using this approach, we retain, on average, 32.64 most probable classes and cover, on average, 98.69% probability mass. This allows us to store soft labels for the entire data set in the RAM, making unbiased sampling of training data efficient."
    }, {
      "heading" : "5.1 BASELINE NETWORKS",
      "text" : "We used Lasagne1, which is based on Theano (Bergstra et al., 2010; Bastien et al., 2012), for implementing CNNs. We used the architecture described in Table 1.\nWe used a batch-size of 256 randomly drawn examples. Each epoch consisted of 2000 mini-batches. Hyper-parameters of the baseline networks were: initial learning rate (1.7 × 10−2), momentum coefficient (0.9, we used Nesterov’s momentum) and learning rate decay coefficient (0.7). Because the data set we used is of an enormous size, the only form of regularisation we used was early\n1https://github.com/Lasagne/\nstopping in the following form. After every epoch we measured the loss (negative log-likelihood of correct class) on the validation set. If the loss did not improve for five epochs, we multiplied the learning rate by a constant learning rate decay coefficient. We stopped the training after the learning rate was smaller than 5× 10−5. It took 202 epochs to finish the training. The bidirectional LSTM teacher network we used is very similar to the one in the work of Mohamed et al. (2015). It has four hidden layers, each with 512 hidden units for each direction. The training starts with the learning rate equal to 0.05. We used the standard momentum with the momentum coefficient of 0.9. After three epochs of no improvement of frame error rate on the validation set, learning rate was multiplied by 23 and training process was rolled back to the last epoch after which validation error improved. Training stopped when learning rate was smaller than 10−5. It took 76 epochs (2000 minibatches of 256 samples) to finish the training.\nInterestingly, although our LSTM teacher achieves 34.71% in FER, clearly outperforming the baseline CNN which achieves 35.57%, the results in WER are the opposite. WER of the LSTM is 14.8%, while CNN’s result is 14.3%. That indicates while the LSTM is more suited to learning the labels on the frame level due to its sequential nature, inductive bias of the CNN architecture is better-matched to the WER. Having this result in mind, the question for further investigation is the following. Can we embed LSTM’s ability of frame-level prediction into the structure of a CNN? We tackle this question with model compression."
    }, {
      "heading" : "5.2 NETWORKS TRAINED WITH MODEL COMPRESSION",
      "text" : "We started our experiments with compressing the LSTMs into CNNs by optimising the objective in Equation 3 with λ = 1.0, i.e. only using the soft labels from the LSTM. Doing just this yields FER of 35.43% and WER of 14.7%. Although, using only the soft labels is not enough to match the FER of the teacher LSTM, remarkably, a model trained this way is better than the LSTM teacher in terms of the WER.\nOur next step was using both soft labels provided by the LSTM and hard labels from the training data. This time, our best student mixing soft and hard labels in equal proportions in the training objective, performs better than both the baseline CNN and the teacher LSTM in both FER and WER, achieving 34.19% and 14.0% respectively. The results for other values of λ are shown in Figure 7 and Figure 8.\nWe hypothesize that this surprising result is due to an ensembling effect. If the two models are sufficently different, they are making errors on different subsets of examples. When averaged together this gives a boost in performance. In our work, we train a convolutional network which has its own\ninductive bias, though it also cannot deviate too much from the predictions made by the LSTM, hence, the final network is a form of an implicit ensemble.\nTo test this hypothesis we form an explicit ensemble of the LSTM teacher and the CNN baseline. We mix posterior predictions of the two models in the following manner.\np(y|xi) = γpLSTM(y|xi) + (1− γ)pCNN(y|xi), (3)\nwhere γ ∈ [0, 1]. pLSTM(y|xi) and pCNN(y|xi) denote probabilities of class y given a feature vector xi, respectively for the LSTM and the baseline CNN. The results (shown in Figure 9 and Figure 10) are consistent with our hypothesis. The ensemble achieves best results than any other network in this paper in both FER (33.18%) and WER (13.8%). It is worth noting that the shapes of the curves for FER, in Figure 7 and Figure 9 are very similar. The same is the case for the curves for the WER in Figure 8 and Figure 10. All our results are summarised in Table 2."
    }, {
      "heading" : "6 RELATED WORK",
      "text" : "A few papers applied model compression in similar speech recognition settings. The most similar to our work is the work of Chan et al. (2015) who were compressing LSTMs into small networks without convolutional layers. Using the soft labels from an LSTM, they were able to show an improvement in WER over the baseline trained with hard labels. The main difference between this work and ours is that their student networks are non-convolutional and very tiny. While this allows for a decent improvement over the baseline, the student network is still much worse than the teacher. Hence, this work does not answer the question we address, i.e. whether a network without recurrent structure can perform as well or better as an LSTM when using soft labels provided by the LSTM. Model compression was also successfully applied to speech recognition by Li et al. (2014) who used deep neural networks without convolutional layers both as a teacher and a student. The architecture of the two networks was the same except that the student had less hidden units in each layer. Finally, the work in the opposite direction was done by Wang et al. (2015) and Tang et al. (2015). They demonstrated that, when using a small data set for which an LSTM is overfitting, a deep non-convolutional network can provide useful guidance for the LSTM. It can come either in the form of pre-training the LSTM with soft labels from a DNN or training the LSTM optimising a loss mixing the hard labels and the soft labels from a DNN."
    }, {
      "heading" : "7 DISCUSSION AND FUTURE WORK",
      "text" : "The main contribution of this paper is introducing using the technique of model compression in a previously unexplored setting, when both the teacher and the student architectures are powerful ones, yet designed with different inductive biases in mind. In fact, rather than model compression, it is more appropriate to call it model blending.\nWe showed that the LSTM and the CNN are learning different kinds of knowledge from the data. We demostrated how this observation can be leveraged through simple ensembling or model blending via model compression. Success of the combination of the two models, provides additional motivation for the study of neural network architectures which use the concepts from both recurrent networks and convolutional networks.\nFinally, we provided experimental evidence that CNNs of appropriate vision-style architecture have the necessary capacity to handle large speech data sets and gave a simple, practical recipe for improving the performance of CNN-based speech recognition models even further at no cost during test time.\nWe are currently running several experiments extending this work including: learning from an even bigger LSTM, compressing a CNN into an LSTM, checking the influence of how many top classes from the posteriors are retained and testing the effect of using temperatures different than one."
    } ],
    "references" : [ {
      "title" : "Applying convolutional neural networks concepts to hybrid NN-HMM model for speech recognition",
      "author" : [ "Abdel-Hamid", "Ossama", "Mohamed", "Abdel-rahman", "Jiang", "Hui", "Penn", "Gerald" ],
      "venue" : "In ICASSP,",
      "citeRegEx" : "Abdel.Hamid et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Abdel.Hamid et al\\.",
      "year" : 2012
    }, {
      "title" : "Convolutional neural networks for speech",
      "author" : [ "Abdel-Hamid", "Ossama", "Mohamed", "Abdel-rahman", "Jiang", "Hui", "Deng", "Li", "Penn", "Gerald", "Yu", "Dong" ],
      "venue" : "recognition. TASLP,",
      "citeRegEx" : "Abdel.Hamid et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Abdel.Hamid et al\\.",
      "year" : 2014
    }, {
      "title" : "Do deep nets really need to be deep",
      "author" : [ "Ba", "Jimmy", "Caruana", "Rich" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Ba et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Ba et al\\.",
      "year" : 2014
    }, {
      "title" : "Theano: new features and speed improvements",
      "author" : [ "Bastien", "Frédéric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Bergstra", "James", "Goodfellow", "Ian J", "Bergeron", "Arnaud", "Bouchard", "Nicolas", "Bengio", "Yoshua" ],
      "venue" : "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,",
      "citeRegEx" : "Bastien et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bastien et al\\.",
      "year" : 2012
    }, {
      "title" : "Theano: a CPU and GPU math expression compiler",
      "author" : [ "Bergstra", "James", "Breuleux", "Olivier", "Bastien", "Frédéric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Desjardins", "Guillaume", "Turian", "Joseph", "Warde-Farley", "David", "Bengio", "Yoshua" ],
      "venue" : "In SciPy,",
      "citeRegEx" : "Bergstra et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Bergstra et al\\.",
      "year" : 2010
    }, {
      "title" : "Transferring knowledge from a RNN to a DNN",
      "author" : [ "Chan", "William", "Ke", "Nan Rosemary", "Laner", "Ian" ],
      "venue" : null,
      "citeRegEx" : "Chan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Chan et al\\.",
      "year" : 2015
    }, {
      "title" : "Big neural networks waste capacity",
      "author" : [ "Dauphin", "Yann", "Bengio", "Yoshua" ],
      "venue" : null,
      "citeRegEx" : "Dauphin et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Dauphin et al\\.",
      "year" : 2013
    }, {
      "title" : "Predicting parameters in deep learning",
      "author" : [ "Denil", "Misha", "Shakibi", "Babak", "Dinh", "Laurent", "de Freitas", "Nando" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Denil et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Denil et al\\.",
      "year" : 2013
    }, {
      "title" : "Learning precise timing with lstm recurrent networks",
      "author" : [ "Gers", "Felix A", "Schraudolph", "Nicol N", "Schmidhuber", "Jürgen" ],
      "venue" : null,
      "citeRegEx" : "Gers et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Gers et al\\.",
      "year" : 2003
    }, {
      "title" : "Switchboard: telephone speech corpus for research and development",
      "author" : [ "J.J. Godfrey", "E.C. Holliman", "J. McDaniel" ],
      "venue" : "In Acoustics, Speech, and Signal Processing,",
      "citeRegEx" : "Godfrey et al\\.,? \\Q1992\\E",
      "shortCiteRegEx" : "Godfrey et al\\.",
      "year" : 1992
    }, {
      "title" : "Supervised sequence labelling with recurrent neural networks",
      "author" : [ "Graves", "Alex" ],
      "venue" : null,
      "citeRegEx" : "Graves and Alex.,? \\Q2012\\E",
      "shortCiteRegEx" : "Graves and Alex.",
      "year" : 2012
    }, {
      "title" : "Framewise phoneme classification with bidirectional LSTM and other neural network architectures",
      "author" : [ "Graves", "Alex", "Schmidhuber", "Jürgen" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "Graves et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2005
    }, {
      "title" : "Speech recognition with deep recurrent neural networks",
      "author" : [ "Graves", "Alex", "Mohamed", "Abdel-rahman", "Hinton", "Geoffrey E" ],
      "venue" : "In ICASSP,",
      "citeRegEx" : "Graves et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2013
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "Hinton", "Geoffrey", "Vinyals", "Oriol", "Dean", "Jeff" ],
      "venue" : null,
      "citeRegEx" : "Hinton et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2015
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Hochreiter", "Sepp", "Schmidhuber", "Jürgen" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Hochreiter et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter et al\\.",
      "year" : 1997
    }, {
      "title" : "Optimal brain damage",
      "author" : [ "Le Cun", "Yann", "Denker", "John S", "Solla", "Sara A" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Cun et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "Cun et al\\.",
      "year" : 1990
    }, {
      "title" : "Convolutional networks for images, speech, and time series",
      "author" : [ "LeCun", "Yann", "Bengio", "Yoshua" ],
      "venue" : "In The Handbook of Brain Theory and Neural Networks",
      "citeRegEx" : "LeCun et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1998
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "LeCun", "Yann", "Bottou", "Léon", "Bengio", "Yoshua", "Haffner", "Patrick" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "LeCun et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1998
    }, {
      "title" : "Unsupervised feature learning for audio classification using convolutional deep belief",
      "author" : [ "Lee", "Honglak", "Pham", "Peter", "Largman", "Yan", "Ng", "Andrew Y" ],
      "venue" : null,
      "citeRegEx" : "Lee et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2009
    }, {
      "title" : "Learning small-size dnn with outputdistribution-based criteria",
      "author" : [ "Li", "Jinyu", "Zhao", "Rui", "Huang", "Jui-Ting", "Gong", "Yifan" ],
      "venue" : "In Interspeech,",
      "citeRegEx" : "Li et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep bidirectional recurrent networks over spectral windows",
      "author" : [ "Mohamed", "Abdel-rahman", "Seide", "Frank", "Yu", "Dong", "Droppo", "Jasha", "Stolcke", "Andreas", "Zweig", "Geoffrey", "Penn", "Gerald" ],
      "venue" : "In ASRU,",
      "citeRegEx" : "Mohamed et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mohamed et al\\.",
      "year" : 2015
    }, {
      "title" : "Fitnets: Hints for thin deep nets",
      "author" : [ "Romero", "Adriana", "Ballas", "Nicolas", "Kahou", "Samira Ebrahimi", "Chassang", "Antoine", "Gatta", "Carlo", "Bengio", "Yoshua" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Romero et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Romero et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep convolutional neural networks for lvcsr",
      "author" : [ "Sainath", "Tara", "Mohamed", "Abdel-rahman", "Kingsbury", "Brian", "Ramabhadran", "Bhuvana" ],
      "venue" : "In ICASSP,",
      "citeRegEx" : "Sainath et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Sainath et al\\.",
      "year" : 2013
    }, {
      "title" : "Deep convolutional neural networks for large-scale speech tasks",
      "author" : [ "Sainath", "Tara", "Kingsbury", "Brian", "Saon", "George", "Soltau", "Hagen", "Mohamed", "Abdel-rahman", "Dahl", "Ramabhadran", "Bhuvana" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "Sainath et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sainath et al\\.",
      "year" : 2015
    }, {
      "title" : "Long short-term memory based recurrent neural network architectures for large vocabulary speech recognition",
      "author" : [ "Sak", "Hasim", "Senior", "Andrew W", "Beaufays", "Françoise" ],
      "venue" : null,
      "citeRegEx" : "Sak et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sak et al\\.",
      "year" : 2014
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "Simonyan", "Karen", "Zisserman", "Andrew" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Simonyan et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Simonyan et al\\.",
      "year" : 2014
    }, {
      "title" : "Knowledge transfer pre-training",
      "author" : [ "Tang", "Zhiyuan", "Wang", "Dong", "Pan", "Yiqiao", "Zhang", "Zhiyong" ],
      "venue" : null,
      "citeRegEx" : "Tang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2015
    }, {
      "title" : "Recurrent neural network training with dark knowledge transfer",
      "author" : [ "Wang", "Dong", "Liu", "Chao", "Tang", "Zhiyuan", "Zhang", "Zhiyong", "Zhao", "Mengyuan" ],
      "venue" : null,
      "citeRegEx" : "Wang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2015
    }, {
      "title" : "Tree-based state tying for high accuracy acoustic modelling",
      "author" : [ "S.J. Young", "J.J. Odell", "P.C. Woodland" ],
      "venue" : "In HLT,",
      "citeRegEx" : "Young et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Young et al\\.",
      "year" : 1994
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "There is evidence that neural networks trained with the current training algorithms use their large capacity inefficiently (Le Cun et al., 1990; Denil et al., 2013; Dauphin & Bengio, 2013; Ba & Caruana, 2014; Hinton et al., 2015).",
      "startOffset" : 123,
      "endOffset" : 229
    }, {
      "referenceID" : 13,
      "context" : "There is evidence that neural networks trained with the current training algorithms use their large capacity inefficiently (Le Cun et al., 1990; Denil et al., 2013; Dauphin & Bengio, 2013; Ba & Caruana, 2014; Hinton et al., 2015).",
      "startOffset" : 123,
      "endOffset" : 229
    }, {
      "referenceID" : 13,
      "context" : "In the context of deep neural networks, this approach to model compression is also known as knowledge distillation (Hinton et al., 2015).",
      "startOffset" : 115,
      "endOffset" : 136
    }, {
      "referenceID" : 13,
      "context" : "In the context of deep neural networks, this approach to model compression is also known as knowledge distillation (Hinton et al., 2015). Additionally, the student can also be shown the original labels of the training data (we will refer to them as hard labels). The main advantage of training the student model using the model compression technique is the fact that the student trained with knowledge provided by the teacher gets a richer supervision signal than just the hard labels from the training data set, i.e. for each training example, it gets the information not only about the correct class but also about how similar the current training example is to other classes. Model compression can also be viewed as way to transfer inductive biases between models. For example, in the case of compressing deep models into shallow ones (Ba & Caruana, 2014), the student model is benefiting from hierarchical representation in the deep model, although it is not able to learn it on its own from hard labels. While model compression can be applied to arbitrary classifiers producing probabilistic prediction over target classes, with the recent success of deep neural networks fueled by new training techniques and dramatic increase in the computational power of modern computers, work on model compression focused on compressing large deep neural networks or ensembles thereof into smaller ones. That is, with less layers, less hidden units or less parameters. In the first paper pursuing that direction, Ba & Caruana (2014) showed that an ensemble of deep neural networks without convolutional layers can be compressed into a single layer network as accurate as a deep one.",
      "startOffset" : 116,
      "endOffset" : 1526
    }, {
      "referenceID" : 13,
      "context" : "In the context of deep neural networks, this approach to model compression is also known as knowledge distillation (Hinton et al., 2015). Additionally, the student can also be shown the original labels of the training data (we will refer to them as hard labels). The main advantage of training the student model using the model compression technique is the fact that the student trained with knowledge provided by the teacher gets a richer supervision signal than just the hard labels from the training data set, i.e. for each training example, it gets the information not only about the correct class but also about how similar the current training example is to other classes. Model compression can also be viewed as way to transfer inductive biases between models. For example, in the case of compressing deep models into shallow ones (Ba & Caruana, 2014), the student model is benefiting from hierarchical representation in the deep model, although it is not able to learn it on its own from hard labels. While model compression can be applied to arbitrary classifiers producing probabilistic prediction over target classes, with the recent success of deep neural networks fueled by new training techniques and dramatic increase in the computational power of modern computers, work on model compression focused on compressing large deep neural networks or ensembles thereof into smaller ones. That is, with less layers, less hidden units or less parameters. In the first paper pursuing that direction, Ba & Caruana (2014) showed that an ensemble of deep neural networks without convolutional layers can be compressed into a single layer network as accurate as a deep one. In a complementary work, Hinton et al. (2015) focused on compressing ensembles of deep networks into deep networks of the same architecture.",
      "startOffset" : 116,
      "endOffset" : 1722
    }, {
      "referenceID" : 13,
      "context" : "In the context of deep neural networks, this approach to model compression is also known as knowledge distillation (Hinton et al., 2015). Additionally, the student can also be shown the original labels of the training data (we will refer to them as hard labels). The main advantage of training the student model using the model compression technique is the fact that the student trained with knowledge provided by the teacher gets a richer supervision signal than just the hard labels from the training data set, i.e. for each training example, it gets the information not only about the correct class but also about how similar the current training example is to other classes. Model compression can also be viewed as way to transfer inductive biases between models. For example, in the case of compressing deep models into shallow ones (Ba & Caruana, 2014), the student model is benefiting from hierarchical representation in the deep model, although it is not able to learn it on its own from hard labels. While model compression can be applied to arbitrary classifiers producing probabilistic prediction over target classes, with the recent success of deep neural networks fueled by new training techniques and dramatic increase in the computational power of modern computers, work on model compression focused on compressing large deep neural networks or ensembles thereof into smaller ones. That is, with less layers, less hidden units or less parameters. In the first paper pursuing that direction, Ba & Caruana (2014) showed that an ensemble of deep neural networks without convolutional layers can be compressed into a single layer network as accurate as a deep one. In a complementary work, Hinton et al. (2015) focused on compressing ensembles of deep networks into deep networks of the same architecture. They also experimented with softening predictions of the teacher by dividing the logits by a constant T greater than one called temperature. Using the techniques developed in prior work and adding an extra mimic layer in the middle of the student network, Romero et al. (2014) demonstrated that a moderately deep and wide convolutional network can be compressed into a deeper and narrower convolutional network with much fewer parameters than the teacher network.",
      "startOffset" : 116,
      "endOffset" : 2094
    }, {
      "referenceID" : 12,
      "context" : "A great example of a very powerful neural network architecture yielding state-of-the-art performance on a range of tasks, yet expensive to run at the test time are the long-short memory network (LSTM) (Hochreiter & Schmidhuber, 1997) and the bidirectional LSTM (Graves & Schmidhuber, 2005; Graves et al., 2013), whose compression is the focus of this work.",
      "startOffset" : 261,
      "endOffset" : 310
    }, {
      "referenceID" : 12,
      "context" : "Prior work (Graves, 2012; Graves et al., 2013; Sak et al., 2014) has shown however that the LSTM architecture, which uses purposebuilt memory cells to store information, is better at finding and exploiting long range context.",
      "startOffset" : 11,
      "endOffset" : 64
    }, {
      "referenceID" : 24,
      "context" : "Prior work (Graves, 2012; Graves et al., 2013; Sak et al., 2014) has shown however that the LSTM architecture, which uses purposebuilt memory cells to store information, is better at finding and exploiting long range context.",
      "startOffset" : 11,
      "endOffset" : 64
    }, {
      "referenceID" : 8,
      "context" : "For the version of LSTM used in this paper (Gers et al., 2003)H is implemented by the following composite function:",
      "startOffset" : 43,
      "endOffset" : 62
    }, {
      "referenceID" : 11,
      "context" : "Figure from Graves et al. (2013). Figure 2: Bidirectional RNN.",
      "startOffset" : 12,
      "endOffset" : 33
    }, {
      "referenceID" : 11,
      "context" : "Figure from Graves et al. (2013). Figure 2: Bidirectional RNN. Figure from Graves et al. (2013).",
      "startOffset" : 12,
      "endOffset" : 96
    }, {
      "referenceID" : 16,
      "context" : "Convolutional neural networks (LeCun et al., 1998) were considered for speech for a long time (LeCun & Bengio, 1998; Lee et al.",
      "startOffset" : 30,
      "endOffset" : 50
    }, {
      "referenceID" : 18,
      "context" : ", 1998) were considered for speech for a long time (LeCun & Bengio, 1998; Lee et al., 2009), though only recently they have become very successful (Abdel-Hamid et al.",
      "startOffset" : 51,
      "endOffset" : 91
    }, {
      "referenceID" : 0,
      "context" : ", 2009), though only recently they have become very successful (Abdel-Hamid et al., 2012; Sainath et al., 2013; Abdel-Hamid et al., 2014; Sainath et al., 2015).",
      "startOffset" : 63,
      "endOffset" : 159
    }, {
      "referenceID" : 22,
      "context" : ", 2009), though only recently they have become very successful (Abdel-Hamid et al., 2012; Sainath et al., 2013; Abdel-Hamid et al., 2014; Sainath et al., 2015).",
      "startOffset" : 63,
      "endOffset" : 159
    }, {
      "referenceID" : 1,
      "context" : ", 2009), though only recently they have become very successful (Abdel-Hamid et al., 2012; Sainath et al., 2013; Abdel-Hamid et al., 2014; Sainath et al., 2015).",
      "startOffset" : 63,
      "endOffset" : 159
    }, {
      "referenceID" : 23,
      "context" : ", 2009), though only recently they have become very successful (Abdel-Hamid et al., 2012; Sainath et al., 2013; Abdel-Hamid et al., 2014; Sainath et al., 2015).",
      "startOffset" : 63,
      "endOffset" : 159
    }, {
      "referenceID" : 0,
      "context" : ", 2009), though only recently they have become very successful (Abdel-Hamid et al., 2012; Sainath et al., 2013; Abdel-Hamid et al., 2014; Sainath et al., 2015). The most successful of previous works on CNNs for speech used only two or three convolutional layers followed by fully connected layers. They also only did convolution and pooling over one dimension, either time or frequency. However, when looking at a spectrogram in Figure 5, it is obvious that similar patterns re-occur both across different points in time and across different frequencies. Using convolution across only one of these dimensions seems wasteful. We hypothesise that classification of windows of speech with CNNs is not that different from object recognition with CNNs. Looking at this problem through the lens of computer vision, we use a convolutional network architecture inspired by the work of Simonyan & Zisserman (2014). We only use small convolutional filters of size 3×3, non-overlapping 2×2 pooling regions and our network also has more layers than networks previously considered for the purpose of speech recognition.",
      "startOffset" : 64,
      "endOffset" : 905
    }, {
      "referenceID" : 9,
      "context" : "In the experiments we use the Switchboard data set (Godfrey et al., 1992).",
      "startOffset" : 51,
      "endOffset" : 73
    }, {
      "referenceID" : 28,
      "context" : "The 9000 output classes represent tied tri-phone states that are generated by an HMM/GMM system (Young et al., 1994).",
      "startOffset" : 96,
      "endOffset" : 116
    }, {
      "referenceID" : 4,
      "context" : "1 BASELINE NETWORKS We used Lasagne1, which is based on Theano (Bergstra et al., 2010; Bastien et al., 2012), for implementing CNNs.",
      "startOffset" : 63,
      "endOffset" : 108
    }, {
      "referenceID" : 3,
      "context" : "1 BASELINE NETWORKS We used Lasagne1, which is based on Theano (Bergstra et al., 2010; Bastien et al., 2012), for implementing CNNs.",
      "startOffset" : 63,
      "endOffset" : 108
    }, {
      "referenceID" : 20,
      "context" : "The bidirectional LSTM teacher network we used is very similar to the one in the work of Mohamed et al. (2015). It has four hidden layers, each with 512 hidden units for each direction.",
      "startOffset" : 89,
      "endOffset" : 111
    }, {
      "referenceID" : 5,
      "context" : "The most similar to our work is the work of Chan et al. (2015) who were compressing LSTMs into small networks without convolutional layers.",
      "startOffset" : 44,
      "endOffset" : 63
    }, {
      "referenceID" : 5,
      "context" : "The most similar to our work is the work of Chan et al. (2015) who were compressing LSTMs into small networks without convolutional layers. Using the soft labels from an LSTM, they were able to show an improvement in WER over the baseline trained with hard labels. The main difference between this work and ours is that their student networks are non-convolutional and very tiny. While this allows for a decent improvement over the baseline, the student network is still much worse than the teacher. Hence, this work does not answer the question we address, i.e. whether a network without recurrent structure can perform as well or better as an LSTM when using soft labels provided by the LSTM. Model compression was also successfully applied to speech recognition by Li et al. (2014) who used deep neural networks without convolutional layers both as a teacher and a student.",
      "startOffset" : 44,
      "endOffset" : 785
    }, {
      "referenceID" : 5,
      "context" : "The most similar to our work is the work of Chan et al. (2015) who were compressing LSTMs into small networks without convolutional layers. Using the soft labels from an LSTM, they were able to show an improvement in WER over the baseline trained with hard labels. The main difference between this work and ours is that their student networks are non-convolutional and very tiny. While this allows for a decent improvement over the baseline, the student network is still much worse than the teacher. Hence, this work does not answer the question we address, i.e. whether a network without recurrent structure can perform as well or better as an LSTM when using soft labels provided by the LSTM. Model compression was also successfully applied to speech recognition by Li et al. (2014) who used deep neural networks without convolutional layers both as a teacher and a student. The architecture of the two networks was the same except that the student had less hidden units in each layer. Finally, the work in the opposite direction was done by Wang et al. (2015) and Tang et al.",
      "startOffset" : 44,
      "endOffset" : 1063
    }, {
      "referenceID" : 5,
      "context" : "The most similar to our work is the work of Chan et al. (2015) who were compressing LSTMs into small networks without convolutional layers. Using the soft labels from an LSTM, they were able to show an improvement in WER over the baseline trained with hard labels. The main difference between this work and ours is that their student networks are non-convolutional and very tiny. While this allows for a decent improvement over the baseline, the student network is still much worse than the teacher. Hence, this work does not answer the question we address, i.e. whether a network without recurrent structure can perform as well or better as an LSTM when using soft labels provided by the LSTM. Model compression was also successfully applied to speech recognition by Li et al. (2014) who used deep neural networks without convolutional layers both as a teacher and a student. The architecture of the two networks was the same except that the student had less hidden units in each layer. Finally, the work in the opposite direction was done by Wang et al. (2015) and Tang et al. (2015). They demonstrated that, when using a small data set for which an LSTM is overfitting, a deep non-convolutional network can provide useful guidance for the LSTM.",
      "startOffset" : 44,
      "endOffset" : 1086
    } ],
    "year" : 2017,
    "abstractText" : "We show that a deep convolutional network with an architecture inspired by the models used in image recognition can yield accuracy similar to a long-short term memory (LSTM) network, which achieves the state-of-the-art performance on the standard Switchboard automatic speech recognition task. Moreover, we demonstrate that merging the knowledge in the CNN and LSTM models via model compression further improves the accuracy of the convolutional model.",
    "creator" : "LaTeX with hyperref package"
  }
}