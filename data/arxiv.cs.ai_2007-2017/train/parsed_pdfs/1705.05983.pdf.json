{
  "name" : "1705.05983.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "At the dawn of Computing, Alan Turing proposed that instead of comprising many different specific machines, the computing machinery for AI should be a universal digital computer, modeled after human computers, which carry out calculations with pencil on paper. Based on the belief that a digital computer would be significantly faster, more diligent and patient than a human, he anticipated that AI would be advanced as software. In modern terminology, a universal computer would be designed to understand a language known as an Instruction Set Architecture (ISA), and software would be translated into the ISA. Since then, universal computers have become exponentially faster and more energy efficient through Moore’s Law, while software has grown more sophisticated. Even though software has not yet made a machine think, it has been changing how we live fundamentally. The computing revolution started when the software was decoupled from the computing machinery.\nSince the slowdown of Moore’s Law in 2005, the universal computer is no longer improving exponentially in terms of speed and energy efficiency. It has to carry ISA legacy, and cannot be aggressively modified to save energy. Turing’s proposition of AI-as-software is challenged, and the temptation of making many domain-specific AI machines emerges. Thanks to Deep Learning, software can stay decoupled from the computing machinery in the language of linear algebra, which it has in common with Supercomputing. A new universal computer for AI understands such language natively to then become a Native Supercomputer.\nAI has been and will still be the inspiration for Computing. The quest to make machines think continues amid the slowdown of Moore’s Law. AI might not only maximize the remaining benefits of Moore’s Law, but also revive Moore’s Law beyond current technology.\nKeywords AI, Deep Learning, Moore’s Law, History of Computing, Parallel Computing, Supercomputing, High Performance Computing, Dataflow"
    }, {
      "heading" : "AI and the Universal Computer",
      "text" : "What kind of computing machinery do we need to advance AI to human level? At the dawn of Computing, one of the founding fathers, Alan Turing, believed that AI could be approached as software running on a universal computer. This was a revolutionary idea given that during his time, the term “computer” was generally referred to as a human hired to do calculations with pencil on paper. Turing referred to a machine as a “digital computer” to distinguish it from the human one. In the context of AI, Alan Turing is remembered for his Imitation Game, or later referred to as Turing Test, in which a machine strives to exhibit intelligence to make itself indistinguishable from a human in the eyes of an interrogator. In his landmark paper, “Computing Machinery and Intelligence” (Turing, 1950), he tried to address the ultimate AI question, “Can machines think?” He reframed the question more precisely and unambiguously by asking how well a machine does in the imitation game. Turing hypothesized that human intelligence is “computable,” which has a precise\nmathematical meaning famously established by himself (Turing, 1936), as a bag of discrete state machines, and reframed the ultimate AI question as\nAre there discrete machines that would do well (in the imitation game)? (Turing, 1950)\nBut what exactly are the discrete state machines to win the imitation game? Apparently, he did not know during his time. But witnessing the extreme difficulty of building a non-human, electronic computer himself (Turing, 1950b), he envisioned only one machine, the Universal Digital Computer that could mimic any discrete state machine. Each discrete state machine can be encoded as numbers to be processed by a universal computer. The numbers that encode a discrete state machine become software, and the computing machinery became the “stored program computer” envisioned by John von Neumann in his incomplete report (Neumann, 1945). Thus, Turing concluded:\nConsiderations of speed apart, it is unnecessary to design various new machines to do various computing processes. They can all be done with one digital computer, suitably programmed for each case. (Turing, 1950)\nThereafter, the history of computing has been mainly the race to build faster Universal Computers to answer the following challenge:\nAre there imaginable digital computers that would do well (in the imitation game)? (Turing, 1950)\nAI researchers and thinkers have been advancing AI without worrying about the underlying computing machinery. People might argue that this applies only to traditional rule-based AI. However, even connectionists have to translate their connectionist systems into algorithms in software to prove and demonstrate their ideas. We have been seeing advances and innovations in Deep Learning completely decoupled from the underlying computing machinery. Today, we use terms like “machines”, “networks”, “neurons”, and “synapses”, without a second thought about the fact that those entities do not have to exist physically. People ponder about a grand unified theory of Deep Learning using ideas like “emergent behaviors”, “intuitions”, “nonlinear dynamics”, believing that those concepts could be adequately represented or approximated by software. According to Turing, any fixed-function Deep Learning accelerator can be simulated in software, and there is always a fallback path to software in applications using such an accelerator. AI has been and will be advanced as software."
    }, {
      "heading" : "The Perfect Marriage between The Universal Computer and Moore’s Law",
      "text" : "Turing’s Universal Computer inspired von Neumann to come up with a powerful computing paradigm, in which complex functions were expressed in a simple yet complete language, the Instruction Set Architecture (ISA), that computing machinery could understand and execute. It brought us computers, as well as the software industry. The prevailing computing machinery in the era of von Neumann paradigm is the microprocessor, now a synonym of the Central Processing Unit (CPU), designed to run instructions in stored programs sequentially. The CPU, the Graphics\nProcessing Unit (GPU), and the various kinds of Digital Signal Processor (DSP) and programmable alternatives are all modern incarnations of such a Universal Digital Computer. But how would such a computer, emulating non-intelligent and non-thinking behaviors of a human, demonstrate human-level intelligence? Turing’s answer was this:\nProvided it could be carried out sufficiently quickly the digital computer could mimic the behaviour of any discrete state machine. (Turing, 1950)\nAs far as AI is concerned, Turing’s idea was that AI can fundamentally be approached through software running on a Universal Digital Computer. It would be the responsibility of the architects of the computing machinery to make it sufficiently fast. But how would we make it faster and at what rate? Moore’s Law, coined in Gordon Moore’s seminal paper (Moore, 1965), has been followed by the semiconductor industry as a consensus and commitment to double the number of transistors per area every two years. Based on the technology scaling rule called Dennard Scaling, transistors have not only become smaller, but also faster and more energy efficient such that a chip now offers at least twice the performance at roughly the same dollar and power budgets. The performance growth mainly came from Moore’s Law driving the clock speed exponentially faster. From 1982 to 2005, typical CPU clock speed grew by 500 times from 6 MHz to 3 GHz. Computing machinery vendors strived to build more capable CPUs, through faster clock speeds and capacity to do more than one thing at a time while maintaining the sequential semantics of a universal computer. Software vendors endeavored to explore new application scenarios and solve the problems algorithmically. The decoupling of software from the computing machinery and the scaling power of Moore’s Law triggered the Computing revolution that has made today’s smart phones more powerful than supercomputers two decades ago. However, faster computers have not helped AI pass the Turing Test yet. AI started out as a discipline to model intelligence behaviors with algorithmic programs following the von Neumann paradigm. It had been struggling to solve real world problems and waiting for even faster computers. Unfortunately, the exponential performance growth of a universal computer has ground to a halt."
    }, {
      "heading" : "The Slowdown of Moore’s Law",
      "text" : "The turning point happened in 2005, when the transistors, while continuing to double in numbers, were neither faster nor more energy efficient at the same rates as before due to the breakdown of Dennard Scaling. Intel wasted little time to bury the race for faster clock speed, and introduced multicore to keep up performance by running multiple “cores” in parallel. A universal computer became a CPU “core”. Multi-core has been a synonym of Parallel Computing in the CPU community. It was expected that there would be a smooth transition from von Neumann paradigm to its parallel heir, and the race for faster clock speed would be replaced with one for higher core count starting from dual and quad cores, to eventually a sea of cores. Around the same time, programmers were asked to take on the challenge of writing and managing a sea of programs, or “threads” (Sutter, 2005).\nSuch a race to double core count has not happened. Intel and the CPU industry have been struggling to add cores aggressively due to the issue of lagging improvements in transistor energy efficiency, manifested as the Dark Silicon phenomenon. It implies that while being able to accommodate four times more cores on a die through two generations of transistor shrinking, we could power up only half of the cores. If this does not look serious enough, only one quarter of the cores can be powered up at the third generation of transistor shrinking. Unless we reduce the core aggressively to compensate for the lagging improvement in energy efficiency, there might be no incentive to go with the fourth generation of transistor shrinking as there will be negligible performance improvement (see Figure 1). To make the situation even worse, the gap between the speed of memory and that of logic has been widening exponentially.\nSuch a limit applies to any computing machinery with an ISA legacy to carry, including the GPU. Although the GPU does not need to support ISA compatibility to every bit, it still needs to support higher level standards such as OpenGL and DirectX shading languages and OpenCL, and intermediate level standards such as SPIR-V. NVIDIA needs to maintain the legacy in their propriety CUDA. For software, managing the threads explicitly for a sea of cores has turned out to be untenable unless we restrict the communications among the threads to some patterns. Such massive and unwieldy parallelism is not for the computing machinery and software to tackle. Some prominent research on Dark Silicon, such as “Dark Silicon and the End of Multicore Scaling” by Hadi Esmaeilzadeh (2011), confused the physical limitation in semiconductor with that from Amdahl’s law, and prematurely declared the death of parallelism along with the slowdown of Moore’s Law. There is abundant parallelism in AI with Deep Learning as we will see later. Once AI with Deep Learning replaces von Neumann architecture as the dominant computing paradigm, abundant parallelism will be the norm."
    }, {
      "heading" : "AI and Moore’s Law",
      "text" : "Turing was not specific about the performance and energy efficiency of a universal computer. He assumed that computers would always be sufficiently fast, and would not be a gating factor for the\nquest for human-level AI. But if passing the Turing Test is the ultimate criteria for machine intelligence, he would have suggested that the computers must achieve a certain level of performance and efficiency to exhibit intelligence; otherwise, the interrogator would be suspicious if it takes too long for a computer to respond to questions or consumes too many resources in the effort. Turing envisioned his digital computer as one that models the slow thinking process of a human doing calculations with a pencil on a piece of paper. The universal digital computer was named to imply that it was designed to model after a human “computer.” According to Turing:\nThe human computer is supposed to be following fixed rules; he has no authority to deviate from them in any detail. (Turing, 1950)\nIn other words, such a universal digital computer does not think, but follows the instructions provided by software. It is the software that makes it think. Following fixed rules strictly requires intensive concentration and is an energy-consuming and slow process for a human brain. Try to multiply 123 by 456 in your head while you are running. It will slow you down. Interestingly, what’s energy consuming for human is also for computers. To accomplish a task by executing one instruction at a time takes relatively more energy than doing it natively without the intermediate ISA. Approaching AI as software in the von Neumann paradigm is like mimicking fast and effortless human mental functions, such as intuition, with a machine that is based on the slow mental process of a human.\nTuring did not foresee that a universal computer would run out of steam. If we are to stay with the von Neumann computing paradigm, we need to put an army of universal computers in a machine to continue the quest. These universal computers would have to communicate data and coordinate tasks among them. However, the slowdown of Moore’s Law and the legacy of the von Neumann paradigm suggest that we will not able to supply sufficient energy to keep such an army growing in size. There needs to be a paradigm shift for AI and Computing.\nTuring did foresee that it would be difficult for human programmers to write all the software to achieve human-level AI. He suggested that we build a learning machine. He said:\nInstead of trying to produce a programme to simulate the adult mind, why not rather try to produce one which simulates the child's? If this were then subjected to an appropriate course of education one would obtain the adult brain. (Turing, 1950)\nThis idea points to Deep Learning. Although Turing did not predict the emergence of Deep Learning, he was aware of the approach with Neural Networks:\nIt is generally thought that there was always an antagonism between programming and the ‘connectionist’ approach of neural networks. But Turing never expressed such a dichotomy, writing that both approaches should be tried. (Hodges, 2013)\nIf Turing were alive today and witnessed the emergence of Deep Learning, he would have revised his proposition on the computing machinery for AI. Since we only need to simulate the child’s mind, and educate it, the computing machinery can model a child’s ability to learn and an adult’s capability to leverage the learned knowledge. Such a machine would be different from the universal computer he envisioned."
    }, {
      "heading" : "Deep Learning and the New AI Machine",
      "text" : "Deep Learning has been transforming and consolidating AI since it came to the center stage of Computing in 2012. With Deep Learning, the intelligence is not coded directly by programmers but acquired indirectly by mining training data sets, and then encoded in the various forms of Neural Networks. The acquisition and manifestation of the intelligence can be formulated as computations dominated by a compact set of linear algebra primitives analogous to those defined in BLAS (Basic Linear Algebra Subprograms), the fundamental application programming interface used in Supercomputing and High Performance Computing (HPC). AI with Deep Learning and Supercomputing effectively speak the same language with dialectical variances in numerical precisions, and minor differences in domain-specific requirements. As mentioned earlier, the massive and unwieldy parallelism under the von Neumann paradigm is not for the computing machinery and software to tackle. On the other hand, the patterns of parallelism in Supercomputing can be summarized as Collective Communications (see Figure 2) as described in Frank Capello’s “Communication Determinism in Parallel HPC Applications” (2010). Collective Communication has been proven to be scalable and manageable in large-scale distributed supercomputing systems. The question is how to leverage the collective patterns on a chip. Through Deep Learning, AI can potentially be liberated from the von Neumann architecture and talk to a native linear algebra machine with massive hardware parallelism, if there is one."
    }, {
      "heading" : "Why Linear Algebra?",
      "text" : "The fundamental primitives in Deep Learning are tensors, high-dimensional data arrays used to represent layers of Deep Neural Networks. A Deep Learning task can be described as a Tensor Computation Graph (Figure 3):\nA tensor computation graph is effectively a piece of AI software. Tensors can be unfolded into 2- dimensional matrices, and matrix computations are handled thru matrix kernels (see Figure 4). Matrix kernels refer to CPU or GPU programs implementing different types of matrix computations comprising many MAC (multiply accumulate) operations. Such a matrix-centric approach is described in Sharan Chetlur (2014). The MAC operations for matrix multiplication are the most time-consuming part of Deep Learning. One might ask, if computations in Deep Learning are predominantly MACs in matrix computations, why don’t we simplify a core all the way to a MAC unit that does nothing but a MAC operation? In fact, why does a MAC unit need to keep the legacy of being a core at all?"
    }, {
      "heading" : "The TPU and Systolic Arrays",
      "text" : "In the highly-anticipated paper, “In-Datacenter Performance Analysis of a Tensor Processing Unit” (Jouppi, 2017), Google disclosed the technical details and performance metrics of the Tensor Processing Unit (TPU). The TPU was built around a matrix multiply unit based on systolic arrays. What’s eye-catching is the choice by the TPU design team to use a systolic array. A systolic array is a specific spatial dataflow machine. A Processing Element (PE) in a systolic array works in lock step with its neighbors. Each PE in a systolic array is basically a MAC unit with some glue logic to store and forward data. In comparison, a computing unit equivalent to a PE in a mesh-connected parallel processor is a full-featured processor core with its own frontend and necessary peripherals, whereas a PE equivalent in a GPU is a simplified processor core sharing a common frontend and peripherals with other cores in the same compute cluster. Among the three solutions, the density of MAC units is the highest in a systolic array. These differences are shown in Figure 5:\nA systolic array claims several advantages: simple and regular design, concurrency and communication, and balancing computation with I/O. However, until now, there has been no commercially successful processor based on a systolic array. The TPU is the first, and it is impressive, arguably the largest systolic array implemented or even conceived. Their design is reminiscent of an idea introduced by H. T. Kung (Kung, 1982). However, due to the curse of the square shape, it suffers from scalability issues as elaborated in the LinkedIn article, “Should We All Embrace Systolic Arrays” (Lu, 2017)."
    }, {
      "heading" : "Spatial Dataflow Architecture",
      "text" : "Like a systolic array, the building block of a generic spatial dataflow machine is often referred to as the PE, which is typically a MAC unit with some glue logic. Mesh topology is a strikingly popular\nway to organize PEs, for examples, Google’s TPU (Jouppi, 2017), the DianNao family (Chen, 2014), MIT’s Eyeriss (Sze, 2017). See Figure 6.\nIt seems logical to use a mesh topology to organize the PEs on a 2-dimensional chip when there are lots of PEs and regularity is desirable. Such an arrangement leads to the following two mesh-centric assumptions:\n1. The distance for a piece of data to travel across the mesh in one clock period is fixed as that between 2 neighboring PEs, even though it could be much further; 2. A PE depends on the upstream neighboring PEs to compute even though such a dependency mainly comes more from the spatial order, rather than from true data dependency.\nThe first assumption is a legacy inherited from distributed parallel processors comprising many compute nodes. Each compute node has to communicate among themselves through intermediate nodes. It is analogous to the situation when a high-speed train stops at every single station on the way to the destination, as shown in Figure 7. Within one clock period, a piece of data could travel over a distance equal to hundreds of the width of a MAC unit without having to hop over every single MAC\nunit in between. Restricting dataflows to PE hopping in a mesh topology causes an increase in latency by several orders of magnitude.\nThe second assumption is another legacy inherited from distributed parallel processors. Each compute node not only handles computations but also plays a part in the distributed storage of the data. The nodes need to exchange data among them to make forward progress. For an on-chip processing mesh, however, the data comes from the side interfacing with the memory. The data flow through the mesh and the results are collected on the other side as shown in Figure 8. Due to the local topology, an internal PE has to get the data through the PEs sitting between it and the memory. Likewise, it has to contribute its partial result through the intermediate PEs before reaching the memory. The resulting dataflows are due to the spatial order of the PE in the mesh, not as a result of true data dependency.\nGiven the two mesh-centric assumptions, no matter how many PEs and how much bandwidth you have, the performance to solve a problem on a d-dimensional mesh is limited by the dimensionality d of the mesh, not the number of the PEs, nor the IO bandwidth. Suppose a problem requires I inputs, K outputs, and T computations, then the asymptotic running time to solve the problem on a ddimensional mesh is given by Fisher’s bound (Fisher, 1988):\nt = Ω(max I 2 , K2 , T245 ).\nFisher’s bound implies there are upper bounds on the number of PEs and bandwidth beyond which no further running time improvement is achievable.\nApplying Fisher’s bound to the inner product, the running time to do an inner product is Ω(n) on a 1- dimensional mesh. If you can afford to have a 2-dimensional mesh, the running time is Ω( \uD835\uDC5B). Can we do better? Instead of using 1 or 2-dimensional mesh, we can feed the input to n PEs and add the\nproducts in pairs recursively. A Ω(log(n)) running time can be achieved. However, it is not possible to achieve such a performance on an either 1 or 2-dimensional mesh unless we organize the PEs in the way shown in Figure 9.\nThe reasons for such a super-optimal result compared to the theoretical limits on a mesh is that there is no PE hopping, and it uses links of different lengths assuming that it takes the same time for a piece of data to travel over links with different lengths. If the distance is too long for a piece of data to travel in one clock period, we can add flops in the middle. It should be an implementation issue, not an architectural one."
    }, {
      "heading" : "Matrix Multiplication According to Supercomputing",
      "text" : "Let’s look at the most time-consuming part of Deep Learning: Matrix Multiplication, which has always been at the heart of Supercomputing. State-of-the-art parallel matrix multiplication performance on modern supercomputers is achieved with the following two major advancements:\n1. Scalable matrix multiplication algorithms 2. Efficient collective communications with logarithmic overhead\nScalable matrix multiplication algorithms See Figure 10 for the demonstration of matrix multiplication in outer products. The computations are 2-dimensional, but both the data and the communications among them are 1-dimensional.\nThe width of a block column and a block row can be a constant and is independent of the number of nodes. On a systolic array, the computations are also broken down into outer products. However, the width of the block column/row must match the side length of the systolic array to achieve optimal performance. Otherwise, the array is poorly occupied for problems with low inner dimension.\nOuter product-based matrix multiplication algorithms, such as Scalable Universal Matrix Multiplication Algorithm (SUMMA) (Robert A. van de Geijn, 1995), have been proven to be very scalable both in theory and in practice in distributed systems.\nEfficient collective communications with logarithmic overhead The communication patterns in SUMMA or similar algorithms are based on collective communications defined for parallel computing on distributed systems. Advances in collective communication for HPC with recursive algorithms (Rajeev Thakur) reduce the communication overheads to be proportional to a logarithmic of the number of nodes and have been instrumental in the continuing performance growth in supercomputing."
    }, {
      "heading" : "Native Supercomputing",
      "text" : "It is interesting to compare how matrix multiplication is achieved with a systolic array and a supercomputer, even though they are at completely different scales: one is on-chip and each node is a PE; the other is at the scale of a data center and each node is a compute cluster (Figure 11).\nBroadcasts are implemented as forwarding data rightward, and reductions (a synonym of “accumulate” in the terminology of collective communications) are implemented as passing partial sums downward in a systolic array and accumulate along the way.\nIn comparison with an algorithm like SUMMA, broadcasts on a supercomputer happen in two dimensions among the nodes, while reductions are achieved in place at each node. There is no dependency, thus no dataflow but collective communication among the participating nodes. Since the reduction is in place, the number of nodes in either dimension is independent of the inner dimension of the matrices. As a matter of fact, the nodes don’t even have to be arranged physically in a 2- dimensional topology as long as collection communication can be supported efficiently.\nToday’s distributed supercomputers are descendants of “Killer Micro” (Brooks, 1989), which were considered aliens invading the land of supercomputing in the early 90s. As a matter of fact, early supercomputers were purposely built to do matrix computations. Imagine that we build a supercomputer-on-chip by\n1. Shrinking a compute cluster to a PE with only densely packed MAC units 2. Building on-chip data delivery fabric to support Collective Streaming, reminiscent of Collective\nCommunication in Supercomputing\nJust as efficient Collective Communication can be achieved recursively, efficient Collect Streaming can be accomplished recursively through the building block, Collective Streaming Element (CE). The CEs are inserted between the PEs and the memory to broadcast or scatter the data hierarchically to the PEs, and to reduce or gather the results recursively from the PEs. The 4 operations are analogous to the counterparts in collective communication in Supercomputing for the compute nodes to exchange data among themselves as shown in Figure 12. Compared to systolic arrays, the PEs do not have to be interlocked in a 2-dimensional grid and the latency can be within a constant factor of a logarithm of numbers of PEs. Building a supercomputer-on-chip can be considered as an effort to return to the matrix-centric root of supercomputing. It is effectively a Native Supercomputer (see Figure 13)."
    }, {
      "heading" : "Why Collective Streaming?",
      "text" : "In many conventional parallel processors, including the GPU, a core, as a universal computer, not only has to support many functions other than MAC, but also needs to retrieve data from the memory, expecting the data to be shared through memory hierarchy. As a result, it requires a significant investment in area and energy for generic functions, multiple levels of caches, scratch memory, and\nregister files. Collective Streaming allows the computing units to comprise only MAC units without a memory hierarchy. In a spatial dataflow machine, such as a systolic array, a PE still keep the legacy of a core having to communicate with other PEs. This causes latency and makes it difficult to scale. Collective Streaming allows orders of magnitude more MAC units without sacrificing latency. A programmable dataflow machine is expected to resolve the dependencies among fine-grain data items. Given that dependencies among data items are collective, the efficiency of a programmable dataflow machine to handle generic data dependencies will be worse than a spatial dataflow machine."
    }, {
      "heading" : "Conclusion",
      "text" : "As mentioned earlier, Turing envisioned a universal AI machine modeling a human computer hired to do calculations with a pencil on paper. According to Turing, it is the software that tells it step by step what to do to make it thinks. With Deep Learning, the software will be like a CEO provisioning the resources and planning for the workflow to educate the machine. It involves formulating the timeconsuming tasks through a software stack, running on legacy universal computers, into the most efficient computing resources. The new AI machine will be built on top of the achievements of the previous Computing revolution. However, the workhorse will be the computing resources that perform linear algebraic tasks natively. AI was the inspiration behind the previous Computing revolution. It shaped Computing as we know it today. The history of Computing now comes full circle. AI is coming back again to inspire Computing. The quest to make machines think continues amid the slowdown of Moore’s Law. AI might not only maximize the remaining benefits of Moore’s Law, but also revive Moore’s Law beyond the current technology.\nBibliography\nBLAS (Basic Linear Algebra Subprograms). (n.d.). Retrieved from netlib.org: http://www.netlib.org/blas/\nBrooks, E. (1989). The Attack of Killer Micros. Supercomputing 1989. Reno, NV.\nChen, Y. (2014). DaDianNao: A Machine-Learning Supercomputer. Microarchitecture (MICRO), 2014 47th Annual IEEE/ACM International Symposium on. Cambridge, UK: IEEE.\nChristensen, C. (1997). The Innovator's Dilemma. United States: Havard Business Review Press.\nFisher, D. C. (1988, February). Your Favorite Parallel Algirithms Might Not Be as Fast as You Think. IEEE Transactions on Computers, 37(2).\nFranck Cappello, A. G. (2010). On communication determinism in parallel HPC applications. International Conference on Computer Communication Networks. Zurich, Switzerland: ICCCN.\nHadi Esmaeilzadeh, E. B. (2011). Dark Silicon and the End of Multicore Scaling. The 38th International Symposium on Computer Architecture (ISCA).\nHodges, A. (2013, Sep 30). Alan Turing. (E. N. Zalta, Ed.) Retrieved from The Stanford Encyclopedia of Philosophy: https://plato.stanford.edu/entries/turing/#Unc\nJouppi, N. P. (2017). In-Datacenter Performance Analysis of a Tensor Processing Unit. Retrieved from https://drive.google.com/file/d/0Bx4hafXDDq2EMzRNcy1vSUxtcEk/view\nKung, H. T. (1982, Jan.). Why Systolic Architecture? IEEE Computer, 15(1), 37-46.\nLu, C.-P. (2017, April 28). Should We All Embrace Systolic Arrays? Retrieved from LinkedIn Publishing: https://www.linkedin.com/pulse/should-we-all-embrace-systolic-arrays-chienping-lu\nMoore, G. (1965, April 19). Cramming more components onto integrated circuits . Electronics, 8.\nNeumann, J. v. (1945). First Draft of a Report on the EDVAC. University of Pennsylvania , Moore School of Electrical Engineering .\nRajeev Thakur, R. R. (n.d.). Optimization of Collective Communication Operations in MPICH. Retrieved from http://www.mcs.anl.gov/~thakur/papers/ijhpca-coll.pdf\nRobert A. van de Geijn, J. W. (1995). SUMMA: Scalable Universal Matrix Multiplication Algorithm. Technical Report, University of Texas at Austin, Department of Computer Science.\nSharan Chetlur, C. W. (2014, Dec 18). cuDNN: Efficient Primitives for Deep Learning. Retrieved from arXiv.org: https://arxiv.org/abs/1410.0759\nSutter, H. (2005). The Free Lunch Is Over: A Fundamental Turn Toward Concurrency in Software. Retrieved from gotw.ca: http://www.gotw.ca/publications/concurrency-ddj.htm\nSze, V. (2017, Mar 27). Efficient Processing of Deep Neural Networks: A Tutorial and Survey. Retrieved from arXiv.org: https://arxiv.org/abs/1703.09039\nTuring, A. (1936). On computable numbers, with an application to the Entscheidungsproblem. 42, 230- 265.\nTuring, A. (1950). Computing Machinery and Intelligence. Mind, 50, 433-460.\nTuring, A. (1950b). Programmers' Handbook for the Manchester Electronic Computer. Manchester University, Computing Laboratory. Manchester University."
    } ],
    "references" : [ {
      "title" : "The Attack of Killer Micros",
      "author" : [ ],
      "venue" : "Supercomputing",
      "citeRegEx" : "Brooks,? \\Q1989\\E",
      "shortCiteRegEx" : "Brooks",
      "year" : 1989
    }, {
      "title" : "February). Your Favorite Parallel Algirithms Might Not Be as Fast as You Think",
      "author" : [ "D.C. Fisher" ],
      "venue" : null,
      "citeRegEx" : "Fisher,? \\Q1988\\E",
      "shortCiteRegEx" : "Fisher",
      "year" : 1988
    }, {
      "title" : "On communication determinism in parallel HPC applications",
      "author" : [ "A.G. Franck Cappello" ],
      "venue" : "IEEE Transactions on Computers,",
      "citeRegEx" : "Cappello,? \\Q2010\\E",
      "shortCiteRegEx" : "Cappello",
      "year" : 2010
    }, {
      "title" : "Dark Silicon and the End of Multicore Scaling",
      "author" : [ "E.B. ICCCN. Hadi Esmaeilzadeh" ],
      "venue" : null,
      "citeRegEx" : "Esmaeilzadeh,? \\Q2011\\E",
      "shortCiteRegEx" : "Esmaeilzadeh",
      "year" : 2011
    }, {
      "title" : "In-Datacenter Performance Analysis of a Tensor Processing Unit",
      "author" : [ ],
      "venue" : null,
      "citeRegEx" : "Jouppi,? \\Q2017\\E",
      "shortCiteRegEx" : "Jouppi",
      "year" : 2017
    }, {
      "title" : "SUMMA: Scalable Universal Matrix Multiplication Algorithm",
      "author" : [ ],
      "venue" : null,
      "citeRegEx" : "Geijn,? \\Q1995\\E",
      "shortCiteRegEx" : "Geijn",
      "year" : 1995
    }, {
      "title" : "The Free Lunch Is Over: A Fundamental Turn Toward Concurrency in Software",
      "author" : [ "H. Sutter" ],
      "venue" : null,
      "citeRegEx" : "Sutter,? \\Q2005\\E",
      "shortCiteRegEx" : "Sutter",
      "year" : 2005
    }, {
      "title" : "Efficient Processing of Deep Neural Networks: A Tutorial and Survey",
      "author" : [ "Sze", "Mar" ],
      "venue" : null,
      "citeRegEx" : "Sze et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Sze et al\\.",
      "year" : 2017
    }, {
      "title" : "Programmers' Handbook for the Manchester Electronic Computer",
      "author" : [ "A. Turing" ],
      "venue" : "Computing Machinery and Intelligence. Mind,",
      "citeRegEx" : "265",
      "shortCiteRegEx" : "265",
      "year" : 1950
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "Around the same time, programmers were asked to take on the challenge of writing and managing a sea of programs, or “threads” (Sutter, 2005).",
      "startOffset" : 126,
      "endOffset" : 140
    }, {
      "referenceID" : 3,
      "context" : "Some prominent research on Dark Silicon, such as “Dark Silicon and the End of Multicore Scaling” by Hadi Esmaeilzadeh (2011), confused the physical limitation in semiconductor with that from Amdahl’s law, and prematurely declared the death of parallelism along with the slowdown of Moore’s Law.",
      "startOffset" : 105,
      "endOffset" : 125
    }, {
      "referenceID" : 4,
      "context" : "8 The TPU and Systolic Arrays In the highly-anticipated paper, “In-Datacenter Performance Analysis of a Tensor Processing Unit” (Jouppi, 2017), Google disclosed the technical details and performance metrics of the Tensor Processing Unit (TPU).",
      "startOffset" : 128,
      "endOffset" : 142
    }, {
      "referenceID" : 4,
      "context" : "9 way to organize PEs, for examples, Google’s TPU (Jouppi, 2017), the DianNao family (Chen, 2014), MIT’s Eyeriss (Sze, 2017).",
      "startOffset" : 50,
      "endOffset" : 64
    }, {
      "referenceID" : 1,
      "context" : "Suppose a problem requires I inputs, K outputs, and T computations, then the asymptotic running time to solve the problem on a ddimensional mesh is given by Fisher’s bound (Fisher, 1988):",
      "startOffset" : 172,
      "endOffset" : 186
    }, {
      "referenceID" : 0,
      "context" : "Today’s distributed supercomputers are descendants of “Killer Micro” (Brooks, 1989), which were considered aliens invading the land of supercomputing in the early 90s.",
      "startOffset" : 69,
      "endOffset" : 83
    } ],
    "year" : 2017,
    "abstractText" : "Artificial Intelligence (AI) was the inspiration that shaped Computing as we know it today. In this article, I explore why and how AI would continue to inspire Computing and reinvent it when Moore’s Law is running out of steam. At the dawn of Computing, Alan Turing proposed that instead of comprising many different specific machines, the computing machinery for AI should be a universal digital computer, modeled after human computers, which carry out calculations with pencil on paper. Based on the belief that a digital computer would be significantly faster, more diligent and patient than a human, he anticipated that AI would be advanced as software. In modern terminology, a universal computer would be designed to understand a language known as an Instruction Set Architecture (ISA), and software would be translated into the ISA. Since then, universal computers have become exponentially faster and more energy efficient through Moore’s Law, while software has grown more sophisticated. Even though software has not yet made a machine think, it has been changing how we live fundamentally. The computing revolution started when the software was decoupled from the computing machinery. Since the slowdown of Moore’s Law in 2005, the universal computer is no longer improving exponentially in terms of speed and energy efficiency. It has to carry ISA legacy, and cannot be aggressively modified to save energy. Turing’s proposition of AI-as-software is challenged, and the temptation of making many domain-specific AI machines emerges. Thanks to Deep Learning, software can stay decoupled from the computing machinery in the language of linear algebra, which it has in common with Supercomputing. A new universal computer for AI understands such language natively to then become a Native Supercomputer. AI has been and will still be the inspiration for Computing. The quest to make machines think continues amid the slowdown of Moore’s Law. AI might not only maximize the remaining benefits of Moore’s Law, but also revive Moore’s Law beyond current technology.",
    "creator" : "Word"
  }
}