{
  "name" : "1706.08502.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Natural Language Does Not Emerge ‘Naturally’ in Multi-Agent Dialog",
    "authors" : [ "Satwik Kottur", "Stefan Lee", "Dhruv Batra" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "A number of recent works have proposed techniques for end-to-end learning of communication protocols among cooperative multi-agent populations, and have simultaneously found the emergence of grounded human-interpretable language in the protocols developed by the agents, all learned without any human supervision!\nIn this paper, using a Task and Tell reference game between two agents as a testbed, we present a sequence of ‘negative’ results culminating in a ‘positive’ one – showing that while most agent-invented languages are effective (i.e. achieve nearperfect task rewards), they are decidedly not interpretable or compositional. In essence, we find that natural language does not emerge ‘naturally’, despite the semblance of ease of natural-languageemergence that one may gather from recent literature. We discuss how it is possible to coax the invented languages to become more and more human-like and compositional by increasing restrictions on how two agents may communicate."
    }, {
      "heading" : "1 Introduction",
      "text" : "One fundamental goal of artificial intelligence (AI) is the development of goal-driven dialog agents – specifically, agents that can perceive their environment (through vision, audition, or other sensors), and communicate with humans or other agents in natural language towards some goal.\nWhile historically such agents have been based on slot filling (Lemon et al., 2006), the domi-\nnant paradigm today is neural dialog models (Bordes and Weston, 2016; Weston, 2016; Serban et al., 2016a,b) trained on large quantities of data. Perhaps somewhat counterintuitively, this current paradigm treats dialog as a static supervised learning problem, rather than as the interactive agent learning problem that it naturally is. Specifically, a typical pipeline is to collect a large dataset of human-human dialog (Lowe et al., 2015; Das et al., 2017a; de Vries et al., 2017; Mostafazadeh et al., 2017), inject a machine in the middle of a dialog from the dataset, and supervise it to mimic the human response. While this teaches the agent correlations between symbols, it does not convey the functional meaning of language, grounding (mapping words to physical concepts), compositionality (combining knowledge of simpler concepts to describe richer concepts), or aspects of planning (understanding the goal of the conversation).\nAn alternative paradigm that has a long history (Winograd, 1971) and is witnessing a recent resurgence (Wang et al., 2016; Foerster et al., 2016; Sukhbaatar et al., 2016; Jorge et al., 2016; Lazaridou et al., 2017; Havrylov and Titov, 2017; Mordatch and Abbeel, 2017; Das et al., 2017b) – is situated language learning. A number of recent works have proposed reinforcement learning techniques for learning the communication protocols of agents situated in virtual environments in a completely end-to-end manner – from perceptual input (e.g. pixels) to communication (discrete symbols without any pre-specified meanings) to action (e.g. signaling in reference games or navigating in an environment) – and have simultaneously found the emergence of grounded humaninterpretable (often compositional) language in the communication protocols developed by the agents, without any human supervision or pretraining, simply to succeed at the task. ar X iv :1 70 6.\n08 50\n2v 1\n[ cs\n.C L\n] 2\n6 Ju\nn 20\n17\nIn this paper, we study the following question – what are the conditions that lead to the emergence of human-interpretable or compositional grounded language? Our key finding is that natural language does not emerge ‘naturally’ in multi-agent dialog, despite the semblance of ease of natural-languageemergence in multi-agent games that one may gather from recent literature.\nSpecifically, in a sequence of ‘negative’ results culminating in a ‘positive’ one, we find that while agents always successfully invent communication protocols and languages to achieve their goals with near-perfect accuracies, the invented languages are decidedly not compositional, interpretable, or ‘natural’; and that it is possible to coax the invented languages to become more and more human-like and compositional by increasing restrictions on how two agents may communicate.\nRelated work and novelty. The starting point for our investigation is the recent work of Das et al. (2017b), who proposed a cooperative reference game between two agents, where communication is necessary to accomplish the goal due to an information asymmetry. Our key contribution over Das et al. (2017b) is an exhaustive study of the conditions that must be present before compositional grounded language emerges, and subtle but important differences in execution – tabular QLearning (which does not scale) vs. REINFORCE (which does), and generalization to novel environments (not studied in prior work). We hope our findings shed more light into the interpretability of languages invented in cooperative multi-agent settings, place recent work in appropriate context, and inform fruitful directions for future work.\n2 The Task & Talk Game\nOur testbed is a cooperative reference game (Task & Talk) between two agents, Q-BOT and A-BOT. The game is grounded in a synthetic world of objects comprised of three attributes – color, style, and shape – each with four possible values for a total of 4× 4× 4 = 64 objects. Fig. 1a shows all the possible attribute values.\nTask & Talk plays out over multiple rounds of dialog. At the start, A-BOT is given an object unseen by Q-BOT, e.g. (green, dotted, square). On the other side, Q-BOT is assigned a task G (unknown to A-BOT) consisting of two attributes, e.g. (color, style) and the goal is for Q-BOT to discover\nthese two attributes of the hidden object, through dialog with A-BOT. Specifically, Q-BOT and ABOT exchange utterances from finite vocabularies VQ and VA over two rounds, with Q-BOT speaking first. The game culminates in Q-BOT guessing a pair of attribute values, e.g. (green, dotted), and both agents are rewarded identically based on the accuracy of Q-BOT ‘s prediction.\nNote that the Task & Talk game setting involves an informational asymmetry between the agents – ABOT sees the object while Q-BOT does not; similarly Q-BOT knows the task while A-BOT does not. Thus, a two-way communication is necessary for success. Without this asymmetry, A-BOT could simply convey the target attributes from the task without Q-BOT having to speak. Such a setting has been widely studies in economics and game theory as the classic Lewis Signaling (LS) game (Lewis, 2008). By necessitating dialog between agents, we are able ground both VA and VQ in our final setting (Sec. 4.3)."
    }, {
      "heading" : "3 Modeling Q-BOT and A-BOT",
      "text" : "We formalize Q-BOT and A-BOT as agents operating in a partially observable world and optimize their policies using deep reinforcement learning.\nStates and Actions. Each agent observes its own input (task G for Q-BOT and object instance I for A-BOT) and the output of the other agent as a stochastic environment. At the beginning of round t, Q-BOT observes state stQ=[G, q1, a1, . . . , qt−1, at−1] and acts by uttering some token qt from its vocabulary VQ. Simi-\nlarly, A-BOT observes the history and this new utterance as state stA=[I, q1, a1, . . . , qt−1, at−1, qt] and emits a response at from VA. At the last round, Q-BOT takes a final action by predicting a pair of attribute values ŵG = (ŵG1 , ŵ G 2 ) to solve the task. Cooperative Reward. Both Q-BOT and A-BOT are rewarded identically based on the accuracy of Q-BOT’s prediction ŵG, receiving a positive reward of R=1 if the prediction matches ground truth wG and a negative reward of R=−10 otherwise. We arrive at these values empirically based on the speed of convergence in our experiments.\nPolicy Networks. We model Q-BOT and A-BOT as operating under stochastic policies πQ(qt|sQt ; θQ) and πA(at|sAt ; θA) respectively, which we instantiate as LSTM-based models. We use lower case characters (e.g. sQt ) to denote the strings (e.g. Q-BOT’s token at round t), and upper case SQt to denote the corresponding vector as encoded by the model.\nAs show in Fig. 2, Q-BOT is modeled with three modules – speaking, listening, and prediction. The task G is received as a 6-dimensional one-hot encoding over the space of possible tasks and embedded via the listener LSTM. At each round t, the speaker network models the probability of output utterances qt ∈ VQ based on the state SQt−1. This is modeled as a fully-connected layer followed by a softmax that transforms SQt−1 to a distribution over VQ. After receiving the reply at from A-BOT, the listener LSTM updates the state by processing both tokens of the dialog exchange. In the final round, the prediction LSTM is unrolled twice to produce Q-BOT’s prediction based on the final state SQT and the task G. As before, task G is fed in as a onehot encoding to the prediction LSTM for two time\nsteps, resulting in a pair of outputs used as the prediction ŵG.\nAnalogously, A-BOT is modeled as a combination of a speaker network, a listener LSTM, and an instance encoder. Like in Q-BOT, the speaker network models the probability of utterances at ∈ VA given the state SAt and the listener LSTM updates the state SAt based on dialog exchanges. The instance encoder embeds each one-hot attribute vector via a linear layer and concatenates all three encodings to obtain a unified instance representation.\nLearning Policies with REINFORCE. To train these agents, we update policy parameters θQ and θA using the popular REINFORCE (Williams, 1992) policy gradient algorithm. Note that while the game is fully-cooperative, we do not assume full observability of one agent by another, opting instead to treat each agent as part of the stochastic environment when updating the other. We will now derive the parameter gradients for our setup.\nRecall that our agents take actions – utterances (qt and at) and attribute prediction (ŵG) – and our objective is to maximize the expected reward under the agents’ policies:\nmax θA,θQ J(θA, θQ) where, (1a)\nJ(θA, θQ) = E πQ,πA\n[ R ( ŵG, wG )] (1b)\nThough the agents receive the reward at the end of gameplay, all intermediate actions are assigned the same reward R. Following the REINFORCE algorithm, we write the gradient of this expectation as an expectation of policy gradients. For θQ, we derive this explicitly at a time step t:\n∇θQJ = ∇θQ [\nE πQ,πA\n[ R ( ŵG, wG )]] = ∇θQ [∑ qt,at πQ ( qt|sQt−1 ) πA ( at|sAt ) R(.)\n] =\n∑ qt,at πQ ( qt|sQt−1 ) ∇θQ log πQ ( qt|sQt−1 ) πA ( at|sAt ) R(.)\n= E πQ,πA\n[ R(.)∇θQ log πQ ( qt|sQt−1 )] (2)\nSimilarly, gradient w.r.t. θA, i.e., ∇θAJ will be:\n∇θAJ = E πQ,πA\n[ R(.)∇θA log πA ( at|sAt )] (3)\nAs is standard practice, we estimate these expectations with sample averages – sampling an environment (object instance and task), sampling a dialog between Q-BOT and A-BOT, culminating in a prediction from Q-BOT and the received reward. The REINFORCE update rule above has an intuitive interpretation – an informative dialog (qt, at) that leads to positive reward will be made more probable (positive gradient), while a poor exchange leading to negative reward will be pushed down (negative gradient).\nImplementation Details. All our models are implemented using the Pytorch1 deep learning framework. To represent instances, we learn a 20 dimensional embedding for every possible attribute values and concatenate the three instance attributes to obtain a final instance representation of size 60. Tokens from VQ and VA are encoded as one-hot vectors and then embedded into 20 dimension vectors. Both A-BOT and Q-BOT learn their own token embeddings without sharing. The listener networks in both agents are implemented as LSTMs with a hidden layer size of 50 dimensions. All modules within an agent are initialized using the Xavier method (Glorot and Bengio, 2010).\nWe use 1000 episodes of two-round dialogs to compute policy gradients, and perform updates according to Adam optimizer (Kingma and Ba, 2015), with a learning rate of 0.01. Furthermore, gradients are clipped at [−5.0, 5.0]. For faster convergence, 80% of train episodes for the next iteration are from instances misclassified by the current network, while randomly sampling the remaining from all instances. Our code is publicly available2."
    }, {
      "heading" : "4 The Road to Compositionality",
      "text" : "This section details our key observation – that while the agents always successfully invent a lan-\n1github.com/pytorch/pytorch 2github.com/batra-mlp-lab/lang-emerge\nguage to solve the game with near-perfect accuracies, the invented languages are decidedly not compositional, interpretable, or ‘natural’ (e.g. ABOT ignoring Q-BOT’s utterances and simply encoding every object with a unique symbol if the vocabulary is sufficiently large).\nThrough this section, we present a series of settings that get progressively more restrictive to coax the agents towards adopting a compositional language, providing analysis of the learned languages and ‘cheating’ strategies developed along the way. Tab. 2 summarizes results for all settings. In all experiments, optimal policies (achieving near-perfect training rewards) were found. For each setting, we provide qualitative analysis of the learned languages and report their ability to generalize to unseen instances. We use 80% of the object-instances for training and the remaining 20% for evaluation."
    }, {
      "heading" : "4.1 Overcomplete Vocabularies",
      "text" : "We begin with the simplest setting where both ABOT and Q-BOT are given arbitrarily large vocabularies. We find that when |VA| is greater than the number of instances (64), the learned policy mostly ignores what Q-BOT asks and instead has A-BOT convey the instance using pairs of symbols across rounds unique to an instance, e.g., both token pairs (a1, a2)=(14, 31), (40, 1) convey (red, triangle, filled), as shown in Fig. 3. Notice, this means no ‘dialog’ is necessary and amounts to each agent having a codebook that maps symbols to object instances. In essence, this setting has collapsed to an analog of Lewis Signaling (LS) game with A-BOT signaling its complete world state and Q-BOT simply reporting the target attributes. More examples to illustrate this behavior for this setting are shown in Fig. 3.\nPerhaps as expected, the generalization of this language to unseen instances is quite poor (success rate: 25.6%). The adopted strategy of mapping instances to token pairs fails for test instances containing novel combinations of attributes for which the agents lack an agreed-upon code from training.\nIt seems clear that like in human communication (Nowak et al., 2000), a limited vocabulary that cannot possibly encode the richness of the world seems to be necessary for non-trivial dialog to emerge. We explore such a setting next."
    }, {
      "heading" : "4.2 Attribute-Value Vocabulary",
      "text" : "Since our world has 3 attributes (shape/color/ style) and 4+4+4 = 12 possible settings of their states, one may believe that the intuitive choice of |VQ| = 3 and |VA| = 12 will be enough to circumvent the ‘cheating’ enumeration strategy from the previous experiment. Surprisingly, we find that the new language learned in this setting is not only decidedly non-compositional but also very difficult to interpret! Some observations follow from Fig. 4 that shows sample dialogs for this setting.\nWe observe that Q-BOT uses only the first round to convey the task to A-BOT by encoding tasks in an order-agnostic fashion, as: (style, shape),(shape, style) → X, (color, shape),(shape, color) → Y, and (color, style),(style, color) → Z. Thus, multiple rounds of utterance from Q-BOT are rendered unnecessary and we find the second round is inconsistent across instances even for the same task. For example, symmetric tasks (color, shape) and (shape, color) from first row of Fig. 4 induce q1=Y as the first token from Q-BOT.\nGiven the task from Q-BOT in the first round, ABOT only needs to identify one of the 4×4=16 attribute pairs for a given task. Rather than ground its symbols into individual states, A-BOT follows a ‘set partitioning’ strategy, i.e. A-BOT identifies\na pair of attributes with a unique combinations of round 1 and 2 utterances (i.e. the round 2 utterance has no meaning independent from round 1). Thus, symbols are reused across tasks to describe different attributes (i.e. symbols do not have individual consistent groundings). This ‘set partitioning’ strategy is consistent with known results from game theory on Nash equilibria in ‘cheap talk’ games (Crawford and Sobel, 1982).\nThis strategy has improved generalization to unseen instances because it is able to communicate the task; however, it fails on unseen attribute value combinations because it is not compositional."
    }, {
      "heading" : "4.3 Memoryless A-BOT, Minimal Vocabulary",
      "text" : "The key problem with the previous setting is that A-BOT’s utterances mean different things based on the round of dialog (a1 = 1 is different from a2 = 1). Essentially, the communication protocol is over-parameterized and we must limit it further. First, we limit A-BOT’s vocabulary to |VA|=4 to reduce the number of ‘synonyms’ the agents learn. Second, we eliminate A-BOT’s capability to identify different rounds of interaction by removing ABOT’s memory. In other words, we reset the state vector SA at each time step so that A-BOT can no longer distinguish rounds from one another. By doing so, we hypothesize that Q-BOT must now ground its own and A-BOT’s tokens consistently\nacross rounds to be able to communicate with a memoryless A-BOT.\nThese restrictions result in a learned language that grounds individual symbols into attributes and their states. For example, Q-BOT learns that Y → shape, X → color, and Z → style. Q-BOT does not however learn to always utter these symbols in the same order as the task, e.g. asking for shape first for both (color, shape) and (shape, color). Notice that this is perfectly valid as Q-BOT can later re-arrange the attributes in the task desired order. Similarly, A-BOT learns mappings to attribute values for each attribute query that remain consistent regardless of round (i.e. when asked for color, 1 always means blue).\nThis is similar to learned languages reported in recent works and is most closely related to Das et al. (2017b), who solve this problem by taking away Q-BOT’s state rather than A-BOT’s memory. Their approach of taking away task G from Q-BOT’s state can be interpreted as Q-BOT ‘forgetting’ the task after interacting with A-BOT. However, this behavior of Q-BOT to remember the task only during dialog but not while predicting is somewhat unnatural compared to our setting.\nTab. 1 enumerates the learnt groundings for both the agents. Given this mapping, we can predict a plausible dialog between the agents for any unseen instance and task combination. Notice that this is\npossible only due to the compositionality in the emergent language between the two agents. For example, consider solving the task (shape, color) for an instance (red, square, filled) from Fig. 7(b). Q-BOT queries Y (shape) and X (color) across two rounds, and receives 2 (square) and 4 (red) as answers. More examples along with grounded meaning of each tokens are shown in Fig. 5.\nIntuitively, this consistently grounded and compositional language has the greatest ability to generalize among the settings we have explored, correctly answering 74.4% of the held out instances. We note that errors in this setting seem to largely be due to A-BOT giving an incorrect answers despite Q-BOT asking the correct questions to accomplish the task. A plausible reason could be the model approximation error stemming from the instance encoder as test instances are unseen and have novel attribute combinations."
    }, {
      "heading" : "5 Evolution of Language",
      "text" : "As demonstrated by the previous sections, even though compositional language is one of the optimal policies, the agents tend to learn other equally useful forms of communication. Thus, compositional language does not naturally emerge without an explicit need for it. Even in situations where compositionality does emerge, perhaps it is more interesting to analyze the process of emergence than the learnt language itself. Therefore, we present such a study that explicitly identifies when each symbol has been grounded by the agents in the training timeline, along with implications thereof on the performance on Task & Talk game."
    }, {
      "heading" : "5.1 Dialog Trees",
      "text" : "When two agents–Q-BOT and A-BOT–converse with each other, they can be seen as traversing through a dialog tree, a subtree of which is depicted in Fig. 6. Simply put, a dialog tree is an enumeration of all possible dialogs represented in the form of tree, with levels of the tree corresponding to the round of interaction. To elaborate, consider a partial dialog tree for (shape, color) task shown in Fig. 6 for the setting in Sec. 4.3. For QBOT’s first token q1 = Y , A-BOT has |VA| = 4 plausible replies shown as a 4-way branch off. In general, the dialog tree for Task & Talk contains a total of |VQ|2|VA|2 leaves and is 4 levels deep. We use the dialog between the agents to descend and land in one of these leaves.\nDialog trees offer an interesting alternate view of our learning problem. The goal of learning communication between the two agents can be equivalently seen as mapping (instance, task) pairs to one of the dialog tree leaves. Each leaf is labeled with an attribute pair used to accomplish the prediction task. For example, if solving (shape, color) for (blue, triangle, solid) results in the dialog Y→1→X→1, we descend the dialog tree along the corresponding path and assign the tuple (blue, triangle, solid, shape, color) to the resulting leaf. In case of a compositional, grounded dialog, all tuples of the form (blue, triangle, ∗, shape, color) would get mapped to the same leaf, which can then be labeled as (triangle, blue) to successfully solve the task. Note the wildcard style attribute in the tuple above, as it is irrelevant for this\nparticular task.\nIn the following section, we use dialog trees to explore the evolution of language as learnt by the two agents in the memoryless A-bot, minimal vocabulary setting in Sec. 4.3."
    }, {
      "heading" : "5.2 Evolution Timeline",
      "text" : "To gain further insight into the languages learned, we create a language evolution plot shown in Fig. 7. Specifically, at regular intervals during policy learning, we construct dialog trees. At some point in the learning, the nodes in the tree become and stay ‘pure’ (all (instance, task) at the\nnode are identical), at which point we can say that the agents have learned this dialog subsequence. Fig. 7 depicts a timeline of concepts learned at various nodes of the trees during training. We next describe the procedure to identify when a particular ‘concept’ has been grounded by the agents in their language.\nConstruction. After constructing dialog trees at regular intervals, we identify ‘concepts’ at each node/leaf using the dialog tree of the completely trained model, which achieves a perfect accuracy on train set. A concept is simply the common trend among all the (instance, task) tuples either assigned to a leaf or contained within the subtree with a node as root. To illustrate, the concept of the top right leaf in Fig. 6 is (blue, triangle, ∗, shape, color), i.e., all instances assigned to that leaf for (shape, color) task are blue triangles. Next, given a resultant concept for each of the node/leaf, we backtrack in time and check for the first occurrence when only tuples which satisfy the corresponding concept are assigned to that particular node/leaf. In other words, we compute the earliest time when a node/leaf is ‘pure’ with respect to its final learned concept. Finally, we plot these leaves/nodes and the associated concept with their backtracked time to get Fig. 7.\nObservations. We highlight the key observations\nfrom Fig. 7 below:\n(a) The agents ground most of the tasks initially at around epoch 20. Specifically, Q-BOT assigns Y to both (shape, style), (style, shape), (shape,color) and (color, shape), while (color, style) is mapped to Z. Hence, Q-BOT learns its first token very early into the training procedure at around 20 epochs. (b) The only other task (style, color) is grounded towards the end (around epoch 170) using X, leading to an immediate convergence. (c) We see a strong correlation between improvement in performance and when agents learn a language grounding. In particular, there is an improvement from 40% to 80% within a span of 25 epochs where most of the grounding is achieved, as seen from Fig. 7."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In conclusion, we presented a sequence of ‘negative’ results culminating in a ‘positive’ one – showing that while most invented languages are effective (i.e. achieve near-perfect rewards), they are decidedly not interpretable or compositional. Our goal is simply to improve understanding and interpretability of invented languages in multiagent dialog, place recent work in context, and inform fruitful directions for future work."
    } ],
    "references" : [ {
      "title" : "Learning End-to-End Goal-Oriented Dialog",
      "author" : [ "Antoine Bordes", "Jason Weston." ],
      "venue" : "arXiv preprint arXiv:1605.07683 .",
      "citeRegEx" : "Bordes and Weston.,? 2016",
      "shortCiteRegEx" : "Bordes and Weston.",
      "year" : 2016
    }, {
      "title" : "Strategic information transmission",
      "author" : [ "Vincent Crawford", "Joel Sobel." ],
      "venue" : "Econometrica 50(6):1431–51.",
      "citeRegEx" : "Crawford and Sobel.,? 1982",
      "shortCiteRegEx" : "Crawford and Sobel.",
      "year" : 1982
    }, {
      "title" : "Visual Dialog",
      "author" : [ "Abhishek Das", "Satwik Kottur", "Khushi Gupta", "Avi Singh", "Deshraj Yadav", "José M.F. Moura", "Devi Parikh", "Dhruv Batra." ],
      "venue" : "CVPR.",
      "citeRegEx" : "Das et al\\.,? 2017a",
      "shortCiteRegEx" : "Das et al\\.",
      "year" : 2017
    }, {
      "title" : "Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning",
      "author" : [ "Abhishek Das", "Satwik Kottur", "José M.F. Moura", "Stefan Lee", "Dhruv Batra." ],
      "venue" : "arXiv preprint arXiv:1703.06585 .",
      "citeRegEx" : "Das et al\\.,? 2017b",
      "shortCiteRegEx" : "Das et al\\.",
      "year" : 2017
    }, {
      "title" : "Guesswhat?! visual object discovery through multi-modal dialogue",
      "author" : [ "Harm de Vries", "Florian Strub", "Sarath Chandar", "Olivier Pietquin", "Hugo Larochelle", "Aaron C. Courville." ],
      "venue" : "CVPR.",
      "citeRegEx" : "Vries et al\\.,? 2017",
      "shortCiteRegEx" : "Vries et al\\.",
      "year" : 2017
    }, {
      "title" : "Learning to communicate with deep multi-agent reinforcement learning",
      "author" : [ "Jakob Foerster", "Yannis M Assael", "Nando de Freitas", "Shimon Whiteson." ],
      "venue" : "NIPS.",
      "citeRegEx" : "Foerster et al\\.,? 2016",
      "shortCiteRegEx" : "Foerster et al\\.",
      "year" : 2016
    }, {
      "title" : "Understanding the difficulty of training deep feedforward neural networks",
      "author" : [ "Xavier Glorot", "Yoshua Bengio." ],
      "venue" : "AISTATS.",
      "citeRegEx" : "Glorot and Bengio.,? 2010",
      "shortCiteRegEx" : "Glorot and Bengio.",
      "year" : 2010
    }, {
      "title" : "Emergence of language with multi-agent games: Learning to communicate with sequences of symbols",
      "author" : [ "Serhii Havrylov", "Ivan Titov." ],
      "venue" : "ICLR Workshop.",
      "citeRegEx" : "Havrylov and Titov.,? 2017",
      "shortCiteRegEx" : "Havrylov and Titov.",
      "year" : 2017
    }, {
      "title" : "Learning to play guess who? and inventing a grounded language as a consequence",
      "author" : [ "Emilio Jorge", "Mikael Kågebäck", "Emil Gustavsson." ],
      "venue" : "NIPS workshop on deep reinforcement learning.",
      "citeRegEx" : "Jorge et al\\.,? 2016",
      "shortCiteRegEx" : "Jorge et al\\.",
      "year" : 2016
    }, {
      "title" : "Adam: A Method for Stochastic Optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Multi-agent cooperation and the emergence of (natural) language",
      "author" : [ "Angeliki Lazaridou", "Alexander Peysakhovich", "Marco Baroni." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Lazaridou et al\\.,? 2017",
      "shortCiteRegEx" : "Lazaridou et al\\.",
      "year" : 2017
    }, {
      "title" : "An ISU dialogue system exhibiting reinforcement learning of dialogue policies: generic slot-filling in the TALK in-car system",
      "author" : [ "Oliver Lemon", "Kallirroi Georgila", "James Henderson", "Matthew Stuttle." ],
      "venue" : "EACL.",
      "citeRegEx" : "Lemon et al\\.,? 2006",
      "shortCiteRegEx" : "Lemon et al\\.",
      "year" : 2006
    }, {
      "title" : "Convention: A philosophical study",
      "author" : [ "David Lewis." ],
      "venue" : "John Wiley & Sons.",
      "citeRegEx" : "Lewis.,? 2008",
      "shortCiteRegEx" : "Lewis.",
      "year" : 2008
    }, {
      "title" : "The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems",
      "author" : [ "Ryan Lowe", "Nissan Pow", "Iulian Serban", "Joelle Pineau." ],
      "venue" : "SIGDIAL.",
      "citeRegEx" : "Lowe et al\\.,? 2015",
      "shortCiteRegEx" : "Lowe et al\\.",
      "year" : 2015
    }, {
      "title" : "Emergence of grounded compositional language in multi-agent populations",
      "author" : [ "Igor Mordatch", "Pieter Abbeel." ],
      "venue" : "arXiv preprint arXiv:1703.04908 . Nasrin Mostafazadeh, Chris Brockett, Bill Dolan, Michel Galley, Jianfeng Gao, Georgios P. Spithourakis,",
      "citeRegEx" : "Mordatch and Abbeel.,? 2017",
      "shortCiteRegEx" : "Mordatch and Abbeel.",
      "year" : 2017
    }, {
      "title" : "Image-Grounded Conversations: Multimodal Context for Natural Question and Response Generation",
      "author" : [ "Lucy Vanderwende" ],
      "venue" : null,
      "citeRegEx" : "Vanderwende.,? \\Q2017\\E",
      "shortCiteRegEx" : "Vanderwende.",
      "year" : 2017
    }, {
      "title" : "The evolution of syntactic communication",
      "author" : [ "Martin A. Nowak", "Joshua B. Plotkin", "Vincent A.A. Jansen." ],
      "venue" : "Nature 404(6777):495–498.",
      "citeRegEx" : "Nowak et al\\.,? 2000",
      "shortCiteRegEx" : "Nowak et al\\.",
      "year" : 2000
    }, {
      "title" : "Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models",
      "author" : [ "Iulian V. Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Serban et al\\.,? 2016a",
      "shortCiteRegEx" : "Serban et al\\.",
      "year" : 2016
    }, {
      "title" : "A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues",
      "author" : [ "Iulian Vlad Serban", "Alessandro Sordoni", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1605.06069 .",
      "citeRegEx" : "Serban et al\\.,? 2016b",
      "shortCiteRegEx" : "Serban et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning multiagent communication with backpropagation",
      "author" : [ "Sainbayar Sukhbaatar", "Rob Fergus" ],
      "venue" : "In NIPS",
      "citeRegEx" : "Sukhbaatar and Fergus,? \\Q2016\\E",
      "shortCiteRegEx" : "Sukhbaatar and Fergus",
      "year" : 2016
    }, {
      "title" : "Learning language games through interaction",
      "author" : [ "Sida I Wang", "Percy Liang", "Christopher D Manning." ],
      "venue" : "Association for Computational Linguistics (ACL) .",
      "citeRegEx" : "Wang et al\\.,? 2016",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "Dialog-based language learning",
      "author" : [ "Jason Weston." ],
      "venue" : "arXiv preprint arXiv:1604.06045 .",
      "citeRegEx" : "Weston.,? 2016",
      "shortCiteRegEx" : "Weston.",
      "year" : 2016
    }, {
      "title" : "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning",
      "author" : [ "Ronald J Williams." ],
      "venue" : "Machine learning 8(3-4):229–256.",
      "citeRegEx" : "Williams.,? 1992",
      "shortCiteRegEx" : "Williams.",
      "year" : 1992
    }, {
      "title" : "Procedures as a representation for data in a computer program for understanding natural language",
      "author" : [ "Terry Winograd." ],
      "venue" : "Technical report, DTIC Document.",
      "citeRegEx" : "Winograd.,? 1971",
      "shortCiteRegEx" : "Winograd.",
      "year" : 1971
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "While historically such agents have been based on slot filling (Lemon et al., 2006), the dominant paradigm today is neural dialog models (Bordes and Weston, 2016; Weston, 2016; Serban et al.",
      "startOffset" : 63,
      "endOffset" : 83
    }, {
      "referenceID" : 13,
      "context" : "Specifically, a typical pipeline is to collect a large dataset of human-human dialog (Lowe et al., 2015; Das et al., 2017a; de Vries et al., 2017; Mostafazadeh et al., 2017), inject a machine in the middle of a di-",
      "startOffset" : 85,
      "endOffset" : 173
    }, {
      "referenceID" : 2,
      "context" : "Specifically, a typical pipeline is to collect a large dataset of human-human dialog (Lowe et al., 2015; Das et al., 2017a; de Vries et al., 2017; Mostafazadeh et al., 2017), inject a machine in the middle of a di-",
      "startOffset" : 85,
      "endOffset" : 173
    }, {
      "referenceID" : 23,
      "context" : "An alternative paradigm that has a long history (Winograd, 1971) and is witnessing a recent resurgence (Wang et al.",
      "startOffset" : 48,
      "endOffset" : 64
    }, {
      "referenceID" : 2,
      "context" : "for our investigation is the recent work of Das et al. (2017b), who proposed a cooperative reference game between two agents, where communication is necessary to accomplish the goal due to an information asymmetry.",
      "startOffset" : 44,
      "endOffset" : 63
    }, {
      "referenceID" : 2,
      "context" : "over Das et al. (2017b) is an exhaustive study of the conditions that must be present before compositional grounded language emerges, and subtle but important differences in execution – tabular QLearning (which does not scale) vs.",
      "startOffset" : 5,
      "endOffset" : 24
    }, {
      "referenceID" : 12,
      "context" : "Such a setting has been widely studies in economics and game theory as the classic Lewis Signaling (LS) game (Lewis, 2008).",
      "startOffset" : 109,
      "endOffset" : 122
    }, {
      "referenceID" : 22,
      "context" : "To train these agents, we update policy parameters θQ and θA using the popular REINFORCE (Williams, 1992) policy gradient algorithm.",
      "startOffset" : 89,
      "endOffset" : 105
    }, {
      "referenceID" : 6,
      "context" : "All modules within an agent are initialized using the Xavier method (Glorot and Bengio, 2010).",
      "startOffset" : 68,
      "endOffset" : 93
    }, {
      "referenceID" : 9,
      "context" : "We use 1000 episodes of two-round dialogs to compute policy gradients, and perform updates according to Adam optimizer (Kingma and Ba, 2015), with a learning rate of 0.",
      "startOffset" : 119,
      "endOffset" : 140
    }, {
      "referenceID" : 16,
      "context" : "It seems clear that like in human communication (Nowak et al., 2000), a limited vocabulary that",
      "startOffset" : 48,
      "endOffset" : 68
    }, {
      "referenceID" : 1,
      "context" : "talk’ games (Crawford and Sobel, 1982).",
      "startOffset" : 12,
      "endOffset" : 38
    }, {
      "referenceID" : 2,
      "context" : "This is similar to learned languages reported in recent works and is most closely related to Das et al. (2017b), who solve this problem by taking away Q-BOT’s state rather than A-BOT’s memory.",
      "startOffset" : 93,
      "endOffset" : 112
    } ],
    "year" : 2017,
    "abstractText" : "A number of recent works have proposed techniques for end-to-end learning of communication protocols among cooperative multi-agent populations, and have simultaneously found the emergence of grounded human-interpretable language in the protocols developed by the agents, all learned without any human supervision! In this paper, using a Task and Tell reference game between two agents as a testbed, we present a sequence of ‘negative’ results culminating in a ‘positive’ one – showing that while most agent-invented languages are effective (i.e. achieve nearperfect task rewards), they are decidedly not interpretable or compositional. In essence, we find that natural language does not emerge ‘naturally’, despite the semblance of ease of natural-languageemergence that one may gather from recent literature. We discuss how it is possible to coax the invented languages to become more and more human-like and compositional by increasing restrictions on how two agents may communicate.",
    "creator" : "LaTeX with hyperref package"
  }
}