{
  "name" : "1412.1114.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Easy Hyperparameter Search Using Optunity",
    "authors" : [ "Marc Claesen", "Jaak Simm", "Dusan Popovic", "Yves Moreau", "Bart De Moor" ],
    "emails" : [ "marc.claesen@esat.kuleuven.be", "jaak.simm@esat.kuleuven.be", "dusan.popovic@esat.kuleuven.be", "yves.moreau@esat.kuleuven.be", "bart.demoor@esat.kuleuven.be" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: hyperparameter search, black-box optimization, algorithm tuning, Python"
    }, {
      "heading" : "1. Introduction",
      "text" : "Many machine learning tasks aim to train a model M which minimizes some loss function L(M |X(te)) on given test data X(te). A model is obtained via a learning algorithm A which uses a training set X(tr) and solves some optimization problem. The learning algorithm A may itself be parameterized by a set of hyperparameters λ, e.g. M = A(X(tr) | λ). Hyperparameter search – also known as tuning – aims to find a set of hyperparameters λ∗, such that the learning algorithm yields an optimal modelM∗ that minimizes L(M | X(te)):\nλ∗ = arg min λ\nL ( A(X(tr) | λ) | X(te) ) = arg min\nλ F(λ | A, X(tr),X(te), L) (1)\nIn the context of tuning, F is the objective function and λ is a tuple of hyperparameters (optimization variables). The learning algorithm A and data sets X(tr) and X(te) are known. Depending on the learning task, X(tr) and X(te) may be labeled and/or equal to each other. The objective function often has a constrained domain (for example regularization terms must be positive) and is assumed to be expensive to evaluate, black-box and non-smooth.\nTuning hyperparameters is a recurrent task in many machine learning approaches. Some common hyperparameters that must be tuned are related to kernels, regularization, learning rates and network architecture. Tuning can be necessary in both supervised and unsupervised settings and may significantly impact the resulting model’s performance.\nc© Marc Claesen, Jaak Simm, Dusan Popovic, Yves Moreau and Bart De Moor.\nar X\niv :1\n41 2.\n11 14\nv1 [\ncs .L\nG ]\nGeneral machine learning packages typically provide only basic tuning methods like grid search. The most common tuning approaches are grid search and manual tuning (Hsu et al., 2003; Hinton, 2012). Grid search suffers from the curse of dimensionality when the number of hyperparameters grows large while manual tuning requires considerable expertise which leads to poor reproducibility, particularly when many hyperparameters are involved."
    }, {
      "heading" : "2. Optunity",
      "text" : "Our software is a Swiss army knife for hyperparameter search. Optunity offers a series of configurable optimization methods and utility functions that enable efficient hyperparameter optimization. Only a handful of lines of code are necessary to perform tuning. Optunity should be used in tandem with existing machine learning packages that implement learning algorithms. The package uses a BSD license and is simple to deploy in any environment. Optunity has been tested in Python, R and MATLAB on Linux, OSX and Windows."
    }, {
      "heading" : "2.1 Functional overview",
      "text" : "Optunity provides both simple routines for lay users and expert routines that enable finegrained control of various aspects of the solving process. Basic tuning can be performed with minimal configuration, requiring only an objective function, an upper limit on the number of evaluations and box constraints on the hyperparameters to be optimized.\nThe objective function must be defined by the user. It takes a hyperparameter tuple λ and typically involves three steps: (i) training a model M with λ, (ii) use M to predict a test set (iii) compute some score or loss based on the predictions. In unsupervised tasks, the separation between (i) and (ii) need not exist, for example in clustering a data set.\nTuning involves a series of function evaluations until convergence or until a predefined maximum number of evaluations is reached. Optunity is capable of vectorizing evaluations in the working environment to speed up the process at the end user’s volition.\nOptunity additionally provides k-fold cross-validation to estimate the generalization performance of supervised modeling approaches. The cross-validation implementation can account for strata and clusters.1 Finally, a variety of common quality metrics is available.\nThe code example below illustrates tuning an SVM with scikit-learn and Optunity.2\n1 @optunity.cross_validated(x=data, y=labels, num_folds=10, num_iter=2) 2 def svm auc(x_train, y_train, x_test, y_test, C, gamma): 3 model = sklearn.svm.SVC(C=C, gamma=gamma).fit(x_train, y_train) 4 decision_values = model.decision_function(x_test) 5 return optunity.metrics.roc_auc(y_test, decision_values) 6 7 optimal_pars, _, _ = optunity.maximize(svm auc, num_evals=100, C=[0, 10], gamma=[0, 1]) 8 optimal_model = sklearn.svm.SVC(**optimal_pars).fit(data, labels)\nThe objective function as per Equation (1) is defined on lines 1 to 5, where λ = (C, γ), A is the SVM training algorithm and L is area under the ROC curve. We use 2× iterated 10-fold cross-validation to estimate area under the ROC curve. Up to 100 hyperparameter tuples are tested within the box constraints 0 < C < 10 and 0 < γ < 1 on line 7.\n1. Instances in a stratum should be spread across folds. Clustered instances must remain in a single fold. 2. We assume the correct imports are made and data and labels contain appropriate content."
    }, {
      "heading" : "2.2 Available solvers",
      "text" : "Optunity provides a wide variety of solvers, ranging from basic, undirected methods like grid search and random search (Bergstra and Bengio, 2012) to evolutionary methods such as particle swarm optimization (Kennedy, 2010) and the covariance matrix adaptation evolutionary strategy (CMA-ES) (Hansen and Ostermeier, 2001). Finally, we provide the Nelder-Mead simplex (Nelder and Mead, 1965), which is useful for local search after a good region has been determined. Optunity’s current default solver is particle swarm optimization, as our experiments have shown it to perform well for a large variety of tuning tasks involving various learning algorithms. Additional solvers will be incorporated in the future."
    }, {
      "heading" : "2.3 Software design and implementation",
      "text" : "The design philosophy of Optunity prioritizes code clarity over performance. This is justified by the fact that objective function evaluations constitute the real performance bottleneck.\nIn contrast to typical Python packages, we avoid dependencies on big packages like NumPy/SciPy and scikit-learn to facilitate users working in non-Python environments (sometimes at the cost of performance). To prevent issues for users that are unfamiliar with Python, care is taken to ensure all code in Optunity works out of the box on any Python version above 2.7, without requiring tools like 2to3 to make explicit conversions. Optunity has a single dependency on DEAP (Fortin et al., 2012) for the CMA-ES solver.\nA key aspect of Optunity’s design is interoperability with external environments. This requires bidirectional communication between Optunity’s Python back-end (O) and the external environment (E) and roughly involves three steps: (i) E → O solver configuration, (ii) O ↔ E objective function evaluations and (iii) O → E solution and solver summary. To this end, Optunity can do straightforward communication with any environment via sockets using JSON messages as shown in Figure 1. Only some information must be communicated, big objects like data sets are never exchanged. To port Optunity to a new environment, a thin wrapper must be implemented to handle communication."
    }, {
      "heading" : "2.4 Documentation",
      "text" : "Code is documented using Sphinx and contains many doctests that can serve as both unit tests and examples of the associated functions. Our website contains API documentation, user documentation and a wide range of examples to illustrate all aspects of the software. The examples involve various packages, including scikit-learn (Pedregosa et al., 2011), OpenCV (Bradski, 2000) and Spark’s MLlib (Zaharia et al., 2010)."
    }, {
      "heading" : "2.5 Collaborative and future development",
      "text" : "Collaborative development is organized via GitHub.3 The project’s master branch is kept stable and is subjected to continuous integration tests using Travis CI. We recommend prospective users to clone the master branch for the most up-to-date stable version of the software. Bug reports and feature requests can be filed via issues on GitHub.\nFuture development efforts will focus on wrappers for Java, Julia and C/C++. This will make Optunity readily available in all main environments related to machine learning. We additionally plan to incorporate Bayesian optimization strategies (Jones et al., 1998)."
    }, {
      "heading" : "3. Related work",
      "text" : "A number of software solutions exist for hyperparameter search. HyperOpt offers random search and sequential model-based optimization (Bergstra et al., 2013). Some packages dedicated to Bayesian approaches include Spearmint (Snoek et al., 2012), DiceKriging (Roustant et al., 2012) and BayesOpt (Martinez-Cantin, 2014). Finally, ParamILS is a command-lineonly tuning framework providing iterated local search (Hutter et al., 2009).\nOptunity distinguishes itself from existing packages by exposing a variety of fundamentally different solvers. This matters because the no free lunch theorem suggests that no single approach is best in all settings (Wolpert and Macready, 1997). Additionally, Optunity is easy to integrate in various environments and features a very simple API."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This research was funded via the following channels:\n• Research Council KU Leuven: GOA/10/09 MaNet, CoE PFV/10/016 SymBioSys;\n• Flemish Government: FWO: projects: G.0871.12N (Neural circuits); IWT: TBMLogic Insulin(100793), TBM Rectal Cancer(100783), TBM IETA(130256), O&O ExaScience Life Pharma, ChemBioBridge, PhD grants (specifically 111065); Industrial Research fund (IOF): IOF/HB/13/027 Logic Insulin; iMinds Medical Information Technologies SBO 2014; VLK Stichting E. van der Schueren: rectal cancer\n• Federal Government: FOD: Cancer Plan 2012-2015 KPC-29-023 (prostate)\n• COST: Action: BM1104: Mass Spectrometry Imaging"
    } ],
    "references" : [ {
      "title" : "Random search for hyper-parameter optimization",
      "author" : [ "James Bergstra", "Yoshua Bengio" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Bergstra and Bengio.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bergstra and Bengio.",
      "year" : 2012
    }, {
      "title" : "Hyperopt: A python library for optimizing the hyperparameters of machine learning algorithms",
      "author" : [ "James Bergstra", "Dan Yamins", "David D Cox" ],
      "venue" : "In Proceedings of the 12th Python in Science Conference,",
      "citeRegEx" : "Bergstra et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bergstra et al\\.",
      "year" : 2013
    }, {
      "title" : "The OpenCV library",
      "author" : [ "G. Bradski" ],
      "venue" : "Dr. Dobb’s Journal of Software Tools,",
      "citeRegEx" : "Bradski.,? \\Q2000\\E",
      "shortCiteRegEx" : "Bradski.",
      "year" : 2000
    }, {
      "title" : "DEAP: Evolutionary algorithms made easy",
      "author" : [ "Félix-Antoine Fortin", "De Rainville", "Marc-André Gardner Gardner", "Marc Parizeau", "Christian Gagné" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Fortin et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Fortin et al\\.",
      "year" : 2012
    }, {
      "title" : "Completely derandomized self-adaptation in evolution strategies",
      "author" : [ "Nikolaus Hansen", "Andreas Ostermeier" ],
      "venue" : "Evolutionary computation,",
      "citeRegEx" : "Hansen and Ostermeier.,? \\Q2001\\E",
      "shortCiteRegEx" : "Hansen and Ostermeier.",
      "year" : 2001
    }, {
      "title" : "A practical guide to training restricted boltzmann machines",
      "author" : [ "Geoffrey E Hinton" ],
      "venue" : "In Neural Networks: Tricks of the Trade,",
      "citeRegEx" : "Hinton.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hinton.",
      "year" : 2012
    }, {
      "title" : "A practical guide to support vector classification",
      "author" : [ "Chih-Wei Hsu", "Chih-Chung Chang", "Chih-Jen Lin" ],
      "venue" : null,
      "citeRegEx" : "Hsu et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Hsu et al\\.",
      "year" : 2003
    }, {
      "title" : "ParamILS: an automatic algorithm configuration framework",
      "author" : [ "Frank Hutter", "Holger H Hoos", "Kevin Leyton-Brown", "Thomas Stützle" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Hutter et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Hutter et al\\.",
      "year" : 2009
    }, {
      "title" : "Efficient global optimization of expensive black-box functions",
      "author" : [ "Donald R Jones", "Matthias Schonlau", "William J Welch" ],
      "venue" : "Journal of Global optimization,",
      "citeRegEx" : "Jones et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Jones et al\\.",
      "year" : 1998
    }, {
      "title" : "Particle swarm optimization",
      "author" : [ "James Kennedy" ],
      "venue" : "In Encyclopedia of Machine Learning,",
      "citeRegEx" : "Kennedy.,? \\Q2010\\E",
      "shortCiteRegEx" : "Kennedy.",
      "year" : 2010
    }, {
      "title" : "BayesOpt: A Bayesian optimization library for nonlinear optimization, experimental design and bandits",
      "author" : [ "Ruben Martinez-Cantin" ],
      "venue" : "arXiv preprint arXiv:1405.7430,",
      "citeRegEx" : "Martinez.Cantin.,? \\Q2014\\E",
      "shortCiteRegEx" : "Martinez.Cantin.",
      "year" : 2014
    }, {
      "title" : "A simplex method for function minimization",
      "author" : [ "John A Nelder", "Roger Mead" ],
      "venue" : "The computer journal,",
      "citeRegEx" : "Nelder and Mead.,? \\Q1965\\E",
      "shortCiteRegEx" : "Nelder and Mead.",
      "year" : 1965
    }, {
      "title" : "Scikit-learn: Machine learning in Python",
      "author" : [ "Fabian Pedregosa", "Gaël Varoquaux", "Alexandre Gramfort", "Vincent Michel", "Bertrand Thirion", "Olivier Grisel", "Mathieu Blondel", "Peter Prettenhofer", "Ron Weiss", "Vincent Dubourg" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Pedregosa et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Pedregosa et al\\.",
      "year" : 2011
    }, {
      "title" : "DiceKriging, DiceOptim: Two R packages for the analysis of computer experiments by kriging-based metamodeling and optimization",
      "author" : [ "Olivier Roustant", "David Ginsbourger", "Yves Deville" ],
      "venue" : null,
      "citeRegEx" : "Roustant et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Roustant et al\\.",
      "year" : 2012
    }, {
      "title" : "Practical Bayesian optimization of machine learning algorithms",
      "author" : [ "Jasper Snoek", "Hugo Larochelle", "Ryan P Adams" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Snoek et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Snoek et al\\.",
      "year" : 2012
    }, {
      "title" : "No free lunch theorems for optimization",
      "author" : [ "David H Wolpert", "William G Macready" ],
      "venue" : "Evolutionary Computation, IEEE Transactions on,",
      "citeRegEx" : "Wolpert and Macready.,? \\Q1997\\E",
      "shortCiteRegEx" : "Wolpert and Macready.",
      "year" : 1997
    }, {
      "title" : "Spark: cluster computing with working sets",
      "author" : [ "Matei Zaharia", "Mosharaf Chowdhury", "Michael J Franklin", "Scott Shenker", "Ion Stoica" ],
      "venue" : "In Proceedings of the 2nd USENIX conference on Hot topics in cloud computing,",
      "citeRegEx" : "Zaharia et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Zaharia et al\\.",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "The most common tuning approaches are grid search and manual tuning (Hsu et al., 2003; Hinton, 2012).",
      "startOffset" : 68,
      "endOffset" : 100
    }, {
      "referenceID" : 5,
      "context" : "The most common tuning approaches are grid search and manual tuning (Hsu et al., 2003; Hinton, 2012).",
      "startOffset" : 68,
      "endOffset" : 100
    }, {
      "referenceID" : 0,
      "context" : "2 Available solvers Optunity provides a wide variety of solvers, ranging from basic, undirected methods like grid search and random search (Bergstra and Bengio, 2012) to evolutionary methods such as particle swarm optimization (Kennedy, 2010) and the covariance matrix adaptation evolutionary strategy (CMA-ES) (Hansen and Ostermeier, 2001).",
      "startOffset" : 139,
      "endOffset" : 166
    }, {
      "referenceID" : 9,
      "context" : "2 Available solvers Optunity provides a wide variety of solvers, ranging from basic, undirected methods like grid search and random search (Bergstra and Bengio, 2012) to evolutionary methods such as particle swarm optimization (Kennedy, 2010) and the covariance matrix adaptation evolutionary strategy (CMA-ES) (Hansen and Ostermeier, 2001).",
      "startOffset" : 227,
      "endOffset" : 242
    }, {
      "referenceID" : 4,
      "context" : "2 Available solvers Optunity provides a wide variety of solvers, ranging from basic, undirected methods like grid search and random search (Bergstra and Bengio, 2012) to evolutionary methods such as particle swarm optimization (Kennedy, 2010) and the covariance matrix adaptation evolutionary strategy (CMA-ES) (Hansen and Ostermeier, 2001).",
      "startOffset" : 311,
      "endOffset" : 340
    }, {
      "referenceID" : 11,
      "context" : "Finally, we provide the Nelder-Mead simplex (Nelder and Mead, 1965), which is useful for local search after a good region has been determined.",
      "startOffset" : 44,
      "endOffset" : 67
    }, {
      "referenceID" : 3,
      "context" : "Optunity has a single dependency on DEAP (Fortin et al., 2012) for the CMA-ES solver.",
      "startOffset" : 41,
      "endOffset" : 62
    }, {
      "referenceID" : 12,
      "context" : "The examples involve various packages, including scikit-learn (Pedregosa et al., 2011), OpenCV (Bradski, 2000) and Spark’s MLlib (Zaharia et al.",
      "startOffset" : 62,
      "endOffset" : 86
    }, {
      "referenceID" : 2,
      "context" : ", 2011), OpenCV (Bradski, 2000) and Spark’s MLlib (Zaharia et al.",
      "startOffset" : 16,
      "endOffset" : 31
    }, {
      "referenceID" : 16,
      "context" : ", 2011), OpenCV (Bradski, 2000) and Spark’s MLlib (Zaharia et al., 2010).",
      "startOffset" : 50,
      "endOffset" : 72
    }, {
      "referenceID" : 8,
      "context" : "We additionally plan to incorporate Bayesian optimization strategies (Jones et al., 1998).",
      "startOffset" : 69,
      "endOffset" : 89
    }, {
      "referenceID" : 1,
      "context" : "HyperOpt offers random search and sequential model-based optimization (Bergstra et al., 2013).",
      "startOffset" : 70,
      "endOffset" : 93
    }, {
      "referenceID" : 14,
      "context" : "Some packages dedicated to Bayesian approaches include Spearmint (Snoek et al., 2012), DiceKriging (Roustant et al.",
      "startOffset" : 65,
      "endOffset" : 85
    }, {
      "referenceID" : 13,
      "context" : ", 2012), DiceKriging (Roustant et al., 2012) and BayesOpt (Martinez-Cantin, 2014).",
      "startOffset" : 21,
      "endOffset" : 44
    }, {
      "referenceID" : 10,
      "context" : ", 2012) and BayesOpt (Martinez-Cantin, 2014).",
      "startOffset" : 21,
      "endOffset" : 44
    }, {
      "referenceID" : 7,
      "context" : "Finally, ParamILS is a command-lineonly tuning framework providing iterated local search (Hutter et al., 2009).",
      "startOffset" : 89,
      "endOffset" : 110
    }, {
      "referenceID" : 15,
      "context" : "This matters because the no free lunch theorem suggests that no single approach is best in all settings (Wolpert and Macready, 1997).",
      "startOffset" : 104,
      "endOffset" : 132
    } ],
    "year" : 2014,
    "abstractText" : "Optunity is a free software package dedicated to hyperparameter optimization. It contains various types of solvers, ranging from undirected methods to direct search, particle swarm and evolutionary optimization. The design focuses on ease of use, flexibility, code clarity and interoperability with existing software in all machine learning environments. Optunity is written in Python and contains interfaces to environments such as R and MATLAB. Optunity uses a BSD license and is freely available online at http://www.optunity.net.",
    "creator" : "LaTeX with hyperref package"
  }
}