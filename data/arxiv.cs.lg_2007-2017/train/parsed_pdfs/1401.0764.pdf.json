{
  "name" : "1401.0764.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Context-Aware Hypergraph Construction for Robust Spectral Clustering",
    "authors" : [ "Xi Li", "Weiming Hu", "Chunhua Shen", "Anthony Dick", "Zhongfei Zhang" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms—Hypergraph construction, spectral clustering, graph partitioning, similarity measure.\nF"
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Spectral clustering is an effective means of clustering data with complex topological structure [8]–[13], [15], [16], [31], [34]–[36]. It plays an important role in unsupervised learning from data, and therefore has a wide range of applications, including circuit layout [1], [2], load balancing [3], image segmentation [4]–[7], [14], motion segmentation [25], video retrieval [26], etc. Typically, the affinity relationships between data samples are modeled by a graph, and therefore spectral clustering aims to optimize a graph partitioning criterion for data clustering based on local vertex similarities. However, there are still several unsolved issues for traditional spectral clustering methods: i) how to automatically discover the number of clusters; ii) how to correctly choose the scaling parameter for graph construction; iii) how to counteract the adverse effect of noise or outliers; and iv) how to incorporate different types of information to enhance the clustering performance.\nIn the literature, Zelnik-Manor and Perona [11] attempt to address issues i) and ii) by designing a local scaling mechanism, which adaptively calculates the affinity matrix and explores the intrinsic structural information on the energy eigenvalue spectrum of the normalized graph Laplacian to discover the number of clusters. However, this local scaling mechanism is susceptible to noise or outliers. Following [11], Li et al. [15] propose a noise robust spectral clustering (NRSC) algorithm to resolve issues i) and iii). The proposed NRSC algorithm can automatically estimate the number of clusters via computing the largest eigenvalue gap of the normalized graph Laplacian. In addition, the proposed NRSC algorithm maps the original data samples (vertices) into a new space, in which the clusters have a higher intra-cluster compactness and inter-cluster separability. If the noisy data samples are weakly interconnected with each other, this mapping relocates the noisy data samples around the origin of the new space. This usually results in a compact noise cluster. However, the real-world data samples (e.g., images and videos) within\n• X. Li and W. Hu are with National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, China. E-mail: {lixi, wmhu}@nlpr.ia.ac.cn • X. Li, C. Shen, and A. Dick are with School of Computer Science, The University of Adelaide, Australia. • Z. Zhang is with State University of New York, Binghamton, NY 13902, USA.\na cluster are often not densely interconnected due to problems with the visual feature description. Therefore, the mapping may lead to topological information loss for the clusters. As a result, the weakly interconnected samples in the ordinary clusters are also relocated around the origin of the new space. This may result in low separability between clusters.\nMore recently, hypergraph analysis [17], [18] has emerged as a popular tool for addressing issues iii) and iv). The fundamental idea of hypergraph analysis is to explore the underlying affinity relationships among vertices by constructing a hypergraph with a variety of hyperedges that capture affinity. This has been applied to many domains such as image matching [19], multi-label classification [20], video object segmentation [21], and image retrieval [22]. For example, Sun et al. [20] carry out hypergraph construction by sequentially introducing new vertices into existing hyperedges using clique expansion or star expansion. Huang et al. [22] propose a probabilistic hypergraph model that softly assigns a vertex to a hyperedge according to the similarity between the vertex and the centroid of the hyperedge.\nMotivation and contribution In general, most existing spectral clustering algorithms only focus on the pairwise interactions between vertices. In other words, the pairwise similarity between two vertices is only based on the individual vertices themselves. If a vertex is corrupted, this pairwise similarity can change significantly. Consequently, their true affinity may not be stably represented. Thus, designing a robust similarity measure is one of the key problems in data clustering.\nHere, we show that the high-order contextual information on vertices can help alleviate this problem. Contexts are groups of vertices that share some common properties. Once contexts have been computed, the vertex similarity measure depends on not only two individual vertices but also their corresponding contexts. The similarity measure that includes contextual information is much more stable because it takes into account local grouping and neighborhood information of each vertex. When a single vertex is corrupted, the high-order contextual similarity can still provide complementary information to counteract the impact of the corruption.\nMotivated by this observation, we propose a robust spectral clustering algorithm based on a context-aware hypergraph similarity measure. We use three different types of hypergraphs: pairwise hypergraph, k-nearest-neighbor (kNN) hypergraph, and high-order over-clustering hypergraph. The pairwise hypergraph is capable of\nar X\niv :1\n40 1.\n07 64\nv1 [\ncs .C\nV ]\n4 J\nan 2\n01 4\nencoding pairwise affinity information on vertices. In contrast, the kNN hypergraph and the over-clustering hypergraph capture the underlying manifold structure on vertices by modeling their highorder neighborhood and contextual grouping properties, respectively. By combining these hypergraphs, we obtain the context-aware hypergraph similarity measure that characterizes the intrinsic connectivity relationships among vertices, resulting in the clustering robustness in the case of noise or outlier corruption.\nThe main contributions of this work are therefore three-fold. • We introduce a high-order context into the spectral clustering\nprocess. The high-order context of a vertex is defined as a set of vertices with similar properties to the vertex. Each vertex in a context is influenced by other vertices in the same context. • The problem of building the high-order context is converted to that of hypergraph construction, which encodes the local affinity information using different types of hypergraphs. To this end, we design three types of hypergraphs to encode the pairwise, neighboring, and local grouping information on vertices. Based on these hypergraphs, we further propose a context-aware hypergraph similarity measure (CAHSM) to capture the intrinsic topological information on vertices. In essence, CAHSM is a generalization of traditional similarity measures, and aims to utilize the hypergraph context to explore the underlying affinity relationships between vertices. • We propose a discriminative hypergraph partitioning criterion (DHPC) to characterize the intra-cluster compactness and the inter-cluster separability of vertices. By maximizing the DHPC, we effectively capture the discriminative information on vertices. The optimization of DHPC can be relaxed into a trace-ratio maximization problem. Using both CAHSM and DHPC, we develop a pairwise+kNN+over-clustering hypergraph spectral clustering algorithm (referred to as PKO+HSC) for data clustering."
    }, {
      "heading" : "2 CONTEXT-AWARE HYPERGRAPH CONSTRUCTION",
      "text" : "In what follows, we first discuss how to construct a robust hypergraph similarity measure using three different types of hypergraphs, and then describe a discriminative hypergraph partitioning criterion for data clustering."
    }, {
      "heading" : "2.1 Context-aware hypergraph similarity measure",
      "text" : "In order to effectively explore the high-order affinity relationships among vertices, we propose a hypergraph construction mechanism\nbased on three types of hypergraphs, which are the pairwise hypergraph, the k-nearest-neighbor (kNN) hypergraph, and the overclustering hypergraph. The pairwise hypergraph reflects the pairwise relationships between vertices; the kNN hypergraph characterizes the neighboring information on vertices, and the over-clustering hypergraph captures the local grouping relationships among vertices. By combining these three types of hypergraphs, the proposed hypergraph construction mechanism is capable of exploring the underlying highorder affinity relationships among vertices.\n1) Pairwise hypergraph. For easy exposition, let Z = {zi}Ni=1 denote a sample set. Based on Z = {zi}Ni=1, we create a pairwise graph Gp with N vertices. Mathematically, the graph Gp can be denoted as Gp = (V, Ep,Wp), where V = {vi}Ni=1 is the vertex set corresponding to {zi}Ni=1, Ep ⊆ V×V is the edge set containing all possible pairwise edges, and Wp is the edge-weight function returning the affinity value between two vertices. In practice, the graph Gp is formulated as a weighted similarity matrix A = (aij)N×N :\naij =\n{ Wp(vi, vj) if (vi, vj) ∈ Ep,\n0 otherwise, (1)\nwhere Wp(vi, vj) = G(zi, zj) is a kernel function used for measuring the similarity between zi and zj . Note that the above procedure of graph creation is independent of the choice of kernel functions. In other words, it is easy to incorporate various kernel functions into the above graph creation process.\nAccording to the hypergraph theory, the pairwise graph Gp is merely a special hypergraph whose hyperedge cardinality equals 2. Therefore, we reformulate Gp using the hypergraph terminologies. As a generalization of a traditional pairwise graph, a hypergraph is composed of many hyperedges, and each hyperedge corresponds to a set of vertices which have some common properties. Mathematically, these hyperedges are generally associated with a hypergraph incidence matrix Hp = (hp(vi, ep` ))|V|×|Ep|:\nhp(vi, e p ` ) = { 1, if vi ∈ ep` , 0, otherwise,\n(2)\nwhere ep` = (vm, vn) is the `-th hyperedge of Ep. In order to measure the degree of the within-hyperedge vertices belonging to the same cluster, we introduce the notion of the pairwise hyperedge weight, which is defined as the pairwise similarity of the vertices in each hyperedge. So we can define the pairwise hypergraph similarity as:\nuij = ∑ e p ` ∈Ep ηp(e p ` )hp(vi, e p ` )hp(vj , e p ` ) = aij , (3)\nwhere ηp(ep` ) is the corresponding hyperedge weight of e p ` = (vm, vn) such that ηp(ep` ) = amn. As a result, we have a weighted hypergraph similarity matrix U = (uij)N×N that characterizes the pairwise affinity relationships between vertices. For simplicity, the resulting pairwise hypergraph similarity matrix is represented as its corresponding matrix form:\nU = HpΣpH T p = A, (4)\nwhere Σp is a diagonal matrix whose diagonal elements are denoted as (ηp(ep` ))ep`∈Ep .\n2) k-nearest-neighbor (kNN) hypergraph. Based on the neighboring information on vertices, we further define a kNN hypergraph Gn, as shown in Fig. 1. For each vertex v`, we search its corresponding k nearest neighbors {vq|vq ∈ N kv`} (as shown in Fig. 2(a)), and then use these nearest neighbors to form a kNN hyperedge en` . By concatenating all the kNN hyperedges, a kNN hyperedge set is generated as En = {en` }N`=1, as illustrated in Fig. 3. To characterize the vertex-to-hyperedge membership, we define an indicator function as:\nI(vi, en` ) = {\n1, if vi ∈ en` , 0, otherwise,\n(5)\nBased on this indicator function, we design a hypergraph model for softly assigning a vertex to each hyperedge:\nhn(vi, e n ` ) =\na`i √\nI(vi, en` )√∑N t=1 δtI(vi, ent )a2ti , (6)\nwhere δt is the hyperedge weight associated with the t-th hyperedge ent such that δt = 1|ent | ∑ j∈{r|vr∈ent }\natj , and a`i represents the vertex-to-hyperedge similarity between vi and the `-th kNN hyperedge en` . Specifically, e n ` is composed of a centroid vertex v` and its corresponding k nearest vertices. Based on Eq. (1), the vertexto-hyperedge similarity a`i is computed as the pairwise similarity between vi and v`. As a result, we obtain a kNN hypergraph incidence matrix Hn = (hn(vi, en` ))|V|×|En| for capturing the vertex-tohyperedge relationships. Based on Hn, a kNN hypergraph similarity bij between vi and vj is derived as:\nbij = ∑\nen ` ∈En\nδ` hn(vi, e n ` )hn(vj , e n ` )\n= ∑N `=1(a`i √ I(vi,en` )δ`)(a`j √\nI(vj ,en` )δ`)√∑N t=1 a 2 ti( √ I(vi,ent )δt)2 √∑N t=1 a 2 tj( √ I(vj ,ent )δt)2\n= 〈xi,xj〉 ‖xi‖‖xj‖ ,\n(7)\nwhere xm = ( a`m √ I(vm, en` )δ` )N `=1\n, < ·, ·> is the inner product operator, and ‖·‖ is the 2-norm. Indeed, xm is a vertex-to-hyperedge feature vector that characterizes the correlation between vm and the kNN hyperedges. For example, the `-th element of xm contains two terms: a`m and √ I(vm, en` )δ`. The first term a`m is the pairwise similarity between vm and the vertex v` of the `-th kNN hyperedge, and the second term √ I(vm, en` )δ` measures the cohesiveness of the `-th kNN hyperedge by computing the average connection similarity between the vertex v` and the other vertices in the `-th kNN hyperedge. Thus, the kNN hypergraph similarity bij can be interpreted as the cosine similarity between two vertex-to-hyperedge feature vectors\nxi and xj . As a result, we obtain a weighted similarity matrix B = (bij)N×N associated with Gn. Essentially, B = (bij)N×N aims to explore the local neighboring relationships between vertices. For simplicity, the kNN hypergraph similarity matrix B is denoted as its corresponding matrix form: B = HnΣnHTn where Σn is a diagonal matrix with the `-th diagonal element being δ`. Fig. 1 gives an illustration of the process of the kNN hypergraph construction.\n3) High-order over-clustering hypergraph. In practice, the vertices are often distributed in different cohesive vertex communities, and each community contains a set of mutually correlated vertices with some common properties. In order to effectively discover such cohesive vertex communities, we propose a high-order over-clustering hypergraph Go based on over-clustering (or over-segmentation) using different clustering methods. Specifically, an over-clustering mechanism is employed to generate a set of vertex groups, each of which corresponds to a cohesive vertex community (as shown in Fig. 2(a)). In this case, the vertices belonging to the same vertex community are mutually influenced, and work as the high-order contexts of the other vertices in the same vertex community, as illustrated in Fig. 4. Without loss of generality, we assume that there are L vertex communities in total. For convenience, let Eo = {eo`}L`=1 denote these vertex communities, each of which corresponds to a hyperedge eo` . Based on these hyperedges, we define the high-order over-clustering hypergraph incidence matrix Ho = (ho(vi, eo`))|V|×L as:\nho(vi, e o `) =\n√ I(vi, eo`)(1 + 1\n|N`i | ∑ m∈N`i\nami)√ ∑ eo ` ∈Eo µ`I(vi, eo`)(1 + 1 |N`i | ∑ m∈N`i ami) , (8)\nwhere I(·, ·) is the indicator function in Eq. (5), N`i is the corresponding vertex index set of the nearest neighbors of vi in the hyperedge eo` (s.t. |N`i | = 3 in the experiments), and µ` is the associated hyperedge weight of eo` such that:\nµ` = 1\n2 1 + 1|eo` | ∑\ni∈{q|vq∈eo`} ∑ m∈N`i ami |N`i |  . (9) Here, ami is the vertex-to-hyperedge similarity between vi and the `-th over-clustering hyperedge eo` . To ensure the robustness of similarity evaluation, we only take into account the affinity relationships between vi and its corresponding nearest neighbors (indexed by N`i ) in eo` . Therefore, ami is the similarity between vi and the m-th vertex of N`i . With the definition of Ho, the high-order over-clustering\nhypergraph similarity cij between vi and vj is formulated as: cij = ∑\neo ` ∈Eo\nµ` ho(vi, e o `)ho(vj , e o `)\n=\n∑ eo ` ∈Eo\n√√√√√µ`I(vi,eo` ) 1+ ∑ m∈N` i ami\n|N` i |\n √√√√√µ`I(vj ,eo` ) 1+ ∑ m∈N` j amj\n|N` j |  √√√√√ ∑\neo ` ∈Eo\nµ`I(vi,eo` )\n1+ ∑ m∈N` i ami\n|N` i |\n √√√√√ ∑\neo ` ∈Eo\nµ`I(vj ,eo` )\n1+ ∑ m∈N` j amj\n|N` j |  = 〈yi,yj〉 ‖yi‖‖yj‖ ,\n(10) where yq is an L-dimensional vector with the `-th element being√ µ`I(vq, eo`)(1 +\n1 |N`q| ∑ m∈N`q\namq). Actually, yq is a vertex-tohyperedge feature vector, and its `-th element consists of two components: µ` and I(vq, eo`)(1 + 1|N`q| ∑ m∈N`q\namq). As defined in Eq. (9), the left term measures the average cross-link degree of the within-community-` vertices with respect to the other vertices in the same community, while the right term reflects the average affinity relationships between vq and the vertices in the `-th community. Therefore, the high-order over-clustering hypergraph similarity cij can be viewed as the cosine similarity between two vertexto-hyperedge feature vectors yi and yj . As a result, a weighted similarity matrix C = (cij)N×N is obtained to capture the local grouping information on vertices. For simplicity, the high-order overclustering hypergraph similarity matrix C can be expressed as its corresponding matrix form: C = HoΣoHTo , where Σo is a diagonal matrix with the `-th diagonal element being µ`.\nFig. 5 gives an example of showing the different hypergraph similarity matrices on the ORL face dataset (referred to in Sec. 3). By linearly combining the above three types of hypergraphs, a contextaware hypergraph similarity matrix S = (sij)N×N is obtained as follows:\nS = H  αΣp 0 00 βΣn 0 0 0 (1− α− β)Σo HT , (11) where H = (HpHnHo) and (α, β) are the nonnegative weighting factors such that (α+β) ≤ 1. Encoding the local neighboring information, the kNN hypergraph plays a role in locally smoothing the clustering results (obtained by only using the pairwise hypergraph), as shown in the bottom-left part of Fig. 5. By constructing the highorder over-clustering hypergraph, we are capable of capturing the manifold structure information among data samples at larger scales, which leads to more accurate clustering results (shown in the bottomright part of Fig. 5). Therefore, the final context-aware hypergraph similarity matrix keeps a balance among the three types of hypergraph information. In practice, it is easy to emphasize one particular type of hypergraph information by enlarging its associated weight. Fig. 2 (b) gives an example of illustrating the hypergraph clustering results\nbased on the above hypergraph similarities."
    }, {
      "heading" : "2.2 Discriminative hypergraph partitioning for spectral clustering",
      "text" : "Having defined a context aware vertex similarity measure, we now propose a discriminative hypergraph partitioning criterion based on this measure, with its corresponding optimization procedure.\n1) Preliminaries of hypergraph partitioning. Hypergraph partitioning seeks an optimal hypergraph cut solution for effective data clustering. K-way normalized cut [12] is a well-known hypergraph partitioning criterion, which aims to optimally partition the vertex set V into K disjoint subsets (i.e., V = ⋃K l=1 Vl s.t. Vm ⋂ Vn = ∅, ∀m 6= n) by solving the following optimization problem:\nmax f(X) = 1K ∑K n=1 XTn SXn XTnDXn ,\ns.t. X ∈ {0, 1}N×K, X1K = 1N , (12)\nwhere X is an N ×K partition matrix such that XTX is a diagonal matrix, 1d denotes a d × 1 vector with each element being 1, D is an N ×N diagonal matrix with the m-th diagonal element being the sum of the elements belonging to the m-th row of S for 1 ≤ m ≤ N , and Xn is the n-th column of X for 1 ≤ n ≤ K. As pointed out in [12], the optimization problem (12) is typically relaxed to:\nmax h(Z) = 1K tr(Z TSZ),\ns.t. ZTDZ = IK, (13)\nwhere IK is a K × K identity matrix, tr(·) denotes the trace of a matrix, and Z = X(XTDX)− 1 2 . Eq. (13) is a trace maximization problem and can be solved by generalized eigenvalue decomposition. To simultaneously capture both intra-cluster compactness and the inter-cluster separability among the vertices in a unified clustering framework, we propose a discriminative hypergraph partitioning criterion which can be formulated as a trace-ratio optimization problem.\n2) Discriminative hypergraph partitioning criterion (DHPC). The proposed DHPC considers both the inter-cluster separability and the intra-cluster compactness, and thus aims to solve the following\nInput: A dataset Z = {zi}Ni=1 and the number of clusters K 1) Obtain the hypergraph similarity matrix S = (sij)N×N .\n• Compute the pairwise hypergraph similarity upij in Eq. (3). • Obtain the kNN hypergraph similarity unij in Eq. (7). • Compute the high-order over-clustering hypergraph\nsimilarity ucij in Eq. (10). • Combine the above three similarities to generate sij by Eq. (11).\n2) Perform spectral graph partitioning. • Compute the graph Laplacian matrix Q = D − S. • Solve the optimization problem (19) by the Newton-Lanczos algorithm. • Calculate a candidate graph partitioning solution X̃ by:\nX̃ = Diag(diag− 1 2 (PPT ))P .\n• Iteratively refine X̃ to find an optimal discrete solution X . Output: The optimal graph partitioning solution X .\nAlgorithm 1: The proposed pairwise+kNN+over-clustering hypergraph spectral clustering algorithm (PKO+HSC).\noptimization problem: max g(X) = 1K ∑K n=1 XTn SXn XTnQXn\n= 1K ∑K n=1 [Xn(X T nXn) − 1 2 ]T S[Xn(X T nXn) − 1 2 ]\n[Xn(XTnXn) − 1 2 ]TQ[Xn(XTnXn) − 1\n2 ] ,\ns.t. X ∈ {0, 1}N×K, X1K = 1N ,\n(14)\nwhere Q = D − S. In the proposed DHPC, the intra-cluster compactness and the inter-cluster separability are respectively captured by XTn SXn and XTnQXn, which are formulated as:\nXTn SXn = ∑ i∈Vn ∑ j∈Vn sij , X T nQXn = ∑ i∈Vn ∑ j /∈Vn sij , (15)\nwhere Vn denotes the vertex set belonging to the n-th cluster. The larger the value of XTn SXn, the more compact the intra-cluster samples. The smaller the value of XTnQXn, the more separable the inter-cluster samples. As a result, an optimal hypergraph partitioning solution is obtained by maximizing g(X) in Eq. (14). For simplicity, let Pn denote the vertex-to-cluster membership vector associated with the n-th cluster such that Pn = Xn(XTnXn)− 1 2 , and P denote the vertex-to-cluster membership matrix that is a concatenation of all the vertex-to-cluster membership vectors such that P = (P1 P2 . . . PK) = X(XTX)− 1 2 . It can be shown that P is an orthogonal matrix:\nPTP = [X(XTX)− 1 2 ]T [X(XTX)− 1 2 ]\n= (XTX)− 1 2 (XTX)(XTX)− 1 2 = IK,\n(16)\nwhere XTX is a diagonal matrix. According to the conclusion in [12], we obtain X̃ = Diag(diag− 1 2 (PPT ))P that is the corresponding inverse transform of P = X(XTX)− 1 2 . Here, Diag(·) denotes a diagonal matrix formed from its vector argument, and diag(·) represents a column vector formed from the diagonal elements of its matrix argument. Consequently, the optimization problem in Eq. (14) can be rewritten as:\nmax g(X) = 1K ∑K n=1 PTn SPn PTn QPn = 1K ∑K n=1 tr(PTn SPn) tr(PTn QPn) ,\ns.t. PTP = IK. (17)\nThis is a trace-ratio-sum optimization problem, which is non-convex and difficult to solve [28]. Thus, we approximate the original optimization problem (17) using the following sum-trace-ratio optimization problem:\nmax f(P ) = 1K ∑K n=1 tr(P T n SPn)∑K\nn=1 tr(P T n QPn)\n= 1K tr(\n∑K n=1 P T n SPn)\ntr( ∑K\nn=1 P T n QPn)\n,\ns.t. PTP = IK. (18)\nDue to tr( ∑K n=1 P T n SPn) = tr(P TSP ) and tr( ∑K n=1 P T n QPn) = tr(PTQP ), the above optimization problem (18) can be reformulated\nas: max f(P ) = 1K tr(PT SP ) tr(PTQP ) ,\ns.t. PTP = IK. (19)\nThe trace-ratio optimization problem (19) has been investigated in [29], [30], [33], [38]. In order to obtain an effective solution to Eq. (19), we therefore utilize the Newton-Lanczos algorithm [33] for trace-ratio maximization. The Newton-Lanczos algorithm includes the following three iterative steps:\n• Compute the trace ratio ρ = tr(P T SP )\ntr(PTQP ) ;\n• Run the Lanczos algorithm [37] to compute the K largest eigenvalues of S − ρQ as well as their associated eigenvectors (P1 P2 . . . PK) ≡ P ;\n• Repeat the above two steps until convergence. In practice, the initial solution P is chosen as the K principal eigenvectors (i.e., corresponding to the K largest eigenvalues) of the matrix Q−1S. If Q is a singular matrix, Q−1S is replaced with the matrix (Q + IN )−1S, where IN is an N ×N identity matrix and is a small positive constant ( = 10−6 in the experiments).\nAfter solving the trace-ratio optimization problem (19), we obtain a candidate solution X̃ to Eq. (14) as follows:\nX̃ = Diag(diag− 1 2 (PPT ))P. (20)\nHowever, the candidate solution X̃ is a real-valued hypergraph partitioning solution, and thus does not satisfy the discrete-solution requirements for data clustering. As a result, an iterative refining procedure [12] may be used to find the optimal discrete hypergraph partitioning solution X to Eq. (14) (more details can be found in Steps four to eight of the algorithm in [12]). After combining the constructed pairwise+kNN+over-clustering hypergraphs (referred to in Sec. 2.1), we have a DHPC-based spectral clustering algorithm called PKO+HSC (pairwise+kNN+over-clustering hypergraph spectral clustering), as listed in Algorithm 1."
    }, {
      "heading" : "3 EXPERIMENTS",
      "text" : ""
    }, {
      "heading" : "3.1 Data description and implementation details",
      "text" : "In the experiments, we evaluate the proposed PKO+HSC on seven datasets, which have the ground truth labels for classification and\nclustering tasks. The detailed configurations of these datasets are given as follows.\nThe first dataset is the ORL face dataset1. It comprises 400 face images of 40 persons, and each person has 10 images. The second dataset is a subset of the YaleB face dataset2, and contains 2432 near frontal face images from 38 individuals under different illuminations. For computational convenience, all the face images from the two datasets are resized to 32×32 pixels. The third dataset is a subset of the US Postal Service (USPS) handwritten digit dataset3, and consists of 9298 16 × 16 handwritten digit images from ten clusters. The fourth dataset is a subset of the MNIST handwritten digit dataset4, and constitutes 2000 28 × 28 digit images from ten clusters. As shown in Fig. 6, the fifth dataset [27] is a trajectory dataset containing 2500 trajectories from 50 clusters, and each cluster comprises 50 trajectories with complex shapes. The sixth dataset is the Iris dataset from the UCI repository5, and contains 150 samples from 3 clusters. The seventh dataset is the Corel image dataset6 that is composed of 1000 images from ten clusters, as shown in Fig. 7.\nFor graph construction, the features used in the two face datasets and the two digit datasets are directly flattened into grayscale intensity column vectors. As a result, the feature dimensions for these four datasets are 1024 (ORL), 1024 (YaleB), 256 (USPS), and 784 (MNIST), respectively. In addition, the corresponding image features for the Corel dataset are the 960-dimensional GIST descriptors (as in [23]) that are widely used in computer vision and pattern recognition. The corresponding features for the trajectory dataset are 18-dimensional discrete Fourier transform (DFT) coefficient features (as in [24]). The feature dimension for the Iris dataset is 4, as shown in the UCI repository. Moreover, the kernel function G(zi, zj) (as in Eq. (3)) is selected as follows: G(zi, zj) = exp ( −‖zi − zj‖2/2σ2\n) where σ is a scaling factor. In practice, σ is tuned from the set {y|y = 0.2ρ + (λ − 1)0.2ρ} where λ is a positive integer such that λ ∈ {1, 2, . . . , 15} and ρ is the average of the distances\n1. http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html 2. http://vision.ucsd.edu/ leekc/ExtYaleDatabase/ExtYaleB.html 3. http://www.csie.ntu.edu.tw/∼cjlin/libsvmtools/datasets/multiclass.html#usps 4. http://yann.lecun.com/exdb/mnist/ 5. http://archive.ics.uci.edu/ml/datasets/Iris 6. Corel Gallery Magic 65000 (1999), www.corel.com\nfrom each sample zi to the other samples. The parameter k in the kNN hypergraph is set to 3. The weighting factors (α, β) in Eq. (11) are chosen from the set {(0.2, 0.2), (0.2, 0.4), (0.2, 0.6), (0.4, 0.2), (0.4, 0.4), (0.6, 0.2), (0.6, 0.4), (0.2, 0.0), (0.0, 0.2)}. The task of constructing the high-order over-clustering hypergraph can be accomplished by using a set of existing clustering methods. In our case, we take advantage of classic spectral clustering [10] and multi-class spectral clustering [12] to generate a set of vertex communities. For each over-clustering method, the number of the vertex communities is chosen as 2K with K being the desired number of clusters (referred to in Algorithm 1). Since we focus on the issues of iii) and iv) referred to in Sec. 1, K is directly set as the ground truth number of clusters for each dataset. The above experimental configurations remain the same for all the experiments below.\nComputational complexity analysis Given N data samples, our pairwise hypergraph construction requires N2 kernel computation operations (referred to in Eq. (1)). Accordingly, the kNN hypergraph construction needs to calculate N2 cosine similarities (defined in Eq. (7)) with respect to the N data samples. Similarly, the overclustering hypergraph construction involves N2 cosine similarity computation operations (mentioned in Eq. (10)). The main computational cost of graph partitioning lies in the eigenvalue decomposition of S − ρQ while solving the optimization problem (19) using the Newton-Lanczos algorithm. According to [37], the time complexity of the Lanczos iterations in our graph partitioning is O(N2). Therefore, the overall time complexity of our method is O(N2), which is the same as standard spectral clustering methods. For example, the average running time of our method on the USPS dataset (s.t. N = 9298) is 15.21 seconds. The spatial complexity of our method lies in the four aspects: 1) the N ×N pairwise hypergraph incidence matrix Hp; 2) the N ×N kNN hypergraph incidence matrix Hn; 3) the N ×N over-clustering hypergraph incidence matrix Ho; and 4)\nthe final N ×N similarity matrix S. It therefore has O(N2) overall spatial complexity."
    }, {
      "heading" : "3.2 Competing algorithms",
      "text" : "We compare the proposed PKO+HSC with several representative spectral clustering algorithms. These spectral clustering algorithms are recently proposed, and have significant impacts on the data clustering community. For descriptive convenience, they are respectively referred to as CSC (classic spectral clustering [10]), STSC (selftuning spectral clustering [11]), and NRSC (noise-robust spectral clustering [15]).\nIn order to verify the effect of different hypergraph components, we compare the proposed PKO+HSC with PK+HSC (pairwise+kNN hypergraph spectral clustering) and PO+HSC (pairwise+over-clustering hypergraph spectral clustering). Actually, PK+HSC and PO+HSC are special cases of the proposed PKO+HSC with different configurations of (α, β). In order to evaluate the performance of different graph partitioning criteria, we perform a comparison experiment against PKO+NC (our context-aware hypergraph similarity measure together with the normalized cut criterion [12]). Furthermore, in order to demonstrate the effectiveness of our context-aware hypergraph similarity measure (defined in Eq. (11)), we make a quantitative comparison with another similarity measure called RWDSM (Random Walk Diffusion Similarity Measure [32]). We put both RWDSM and our similarity measure into the same discriminative hypergraph partitioning criterion (DHPC) for data clustering."
    }, {
      "heading" : "3.3 Evaluation criteria",
      "text" : "For a quantitative comparison, we introduce two evaluation criteria— NMI (normalized mutual information) and clustering accuracy. Specifically, the NMI criterion is defined as: NMI(X,Y) =\nI(X,Y)/ √ H(X)H(Y) where X and Y are two random variables, H(X) and H(Y) are the corresponding entropies of X and Y, and I(X,Y) is the mutual information on X and Y. In principle, NMI(X,Y) has the range of [0, 1], and is equal to 1 when X = Y. As far as data clustering is concerned, the NMI criterion is explicitly formulated as:\nNMI(S′,S) =\n∑C i=1 ∑K j=1 qij m log\n( qij m\nmi m\nm′ j m ) √\n( ∑C i=1 mi m log mi m )( ∑K j=1 m′j m log m′j m ) (21)\nwhere S = {Si}Ci=1 is the ground truth clustering configuration of a dataset, S′ = {S′j}Kj=1 is the clustering configuration obtained by a clustering algorithm, C is the ground truth cluster number, K is the obtained cluster number, qij is the cardinality of the intersection of Si and S′j , mi is the cardinality of Si, m ′ j is the cardinality of S ′ j , and m is the cardinality of the whole dataset. The larger the NMI, the better the clustering performance.\nOn the other hand, the clustering accuracy is defined as: Accuracy = 1\nm ∑K j=1 nj where nj is the number of the samples\nwhose ground truth cluster labels have the highest proportion in the j-th cluster S′j . The larger the clustering accuracy, the better the clustering results."
    }, {
      "heading" : "3.4 Clustering results",
      "text" : "In the experiments, we aim to evaluate the clustering performances of different clustering algorithms in the following three aspects: i) evaluating the clustering performances (i.e., NMI and clustering accuracy) on the original datasets; ii) quantitative comparisons on the datasets after noise-based feature perturbations; and iii) performance evaluations on the datasets with outlier corruptions. The purposes of the above-mentioned three aspects are to verify the clustering effectiveness and the clustering robustness.\nFor i), Tab. 1 reports the corresponding NMIs and accuracies of the eight clustering algorithms. It is seen from Tab. 1 that the proposed PKO+HSC obtains higher NMIs and accuracies than the other clustering algorithms. More specifically, the average NMI gains of PKO+HSC regarding the seven datasets are (5.53%, 3.65%, 2.26%, 8.07%, 20.78%, 11.16%, 19.67%) over those of (PK+HSC, PO+HSC, PKO+NC, CSC, NRSC, RWDSM, STSC), respectively; and the average accuracy gains are (2.83%, 2.42%, 2.80%, 9.02%, 25.55%, 10.72%, 21.58%), respectively. Furthermore, Tab. 2 reports the NMIs and accuracies of the proposed PKO+HSC with different\nconfigurations of the weighting factors (α, β). From Tab. 2, we see that the clustering performances of the proposed PKO+HSC are not very sensitive to the configurations of the weighting factors.\nFor ii), noise-based feature perturbations are performed by using additive random noises. Figs. 8 and 9 show the NMI and accuracy curves with error bars in eleven different noise levels (i.e., L0 corresponds to i), and L1 → L10 are associated with ten ascending noise levels whose magnitudes are chosen from {0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0}). Clearly, the proposed PKO+HSC achieves the highest NMIs and accuracies in most noise levels. Furthermore, Tab. 3 reports the average NMIs and accuracies of the eight clustering algorithms regarding different noise levels. The average NMI and accuracy gains of the proposed PKO+HSC are (6.03%, 5.19%, 2.80%, 23.54%, 34.54%, 26.92%, 20.43%) and (7.01%, 6.71%, 3.14%, 24.78%, 35.19%, 28.81%, 24.32%) over those of (PK+HSC, PO+HSC, PKO+NC, CSC, NRSC, RWDSM, STSC), respectively.\nFor iii), outlier corruptions are performed by randomly setting the feature elements to zeros. Fig. 10 displays the clustering performances of the eight clustering algorithms regarding four outlier corruption levels (i.e., L0 corresponds to i), and L1 → L3 are associated with three ascending corruption levels whose corruption ratios are chosen from {0.2, 0.4, 0.6}). From Fig. 10, we see that the proposed PKO+HSC consistently achieves higher NMIs and accuracies than the other clustering algorithms. Moreover, Tab. 4 shows the average NMIs and accuracies of the eight clustering algorithms regarding different outlier corruption levels. The average NMI and accuracy gains of the proposed PKO+HSC are (7.87%, 2.96%, 2.99%, 11.02%, 30.55%, 14.83%, 30.58%) and (7.39%, 3.80%, 2.91%, 10.11%, 34.62%, 14.04%, 33.66%) over those of (PK+HSC, PO+HSC, PKO+NC, CSC, NRSC, RWDSM, STSC), respectively.\nBesides, we report the clustering results of the proposed PKO+HSC with different configurations of the nearest-neighbor number k for kNN hypergraph construction in Fig. 11. From Fig. 11, we see that the proposed PKO+HSC is not very sensitive to the settings of k. Moreover, Fig. 12 shows the clustering performances of the proposed PKO+HSC using different numbers of vertex communities for highorder over-clustering hypergraph construction in NMI and clustering accuracy on the two datasets. It is clearly seen from Fig. 12 that the\nproposed PKO+HSC is not very sensitive to the choice of the vertex community number.\nOverall, the proposed PKO+HSC outperforms the other clustering algorithms. Considering three types of hypergraph information (i.e., pairwise, kNN, and over-clustering), the proposed PKO+HSC is capable of effectively exploring the intrinsic topological information among vertices. By optimizing the discriminative hypergraph partitioning criterion (DHPC), the proposed PKO+HSC considers both intra-cluster compactness and inter-cluster separability, resulting in the overall clustering robustness."
    }, {
      "heading" : "4 CONCLUSION AND FUTURE WORK",
      "text" : "In this work, we have proposed a context-aware hypergraph similarity measure (CAHSM), which is based on three types of hypergraphs— pairwise hypergraph, k-nearest-neighbor (kNN) hypergraph, and high-order over-clustering hypergraph. These hypergraphs capture the pairwise, neighborhood, and local grouping information on vertices. By effectively combining these types of affinity information, CAHSM is capable of effectively exploring the intrinsic structural information on vertices, resulting in the robust clustering performance. In order to fully capture the intra-cluster compactness and the inter-cluster separability of vertices, we have also designed a discriminative\nhypergraph partitioning criterion (DHPC) that is solved by traceratio maximization. Based on both CAHSM and DHPC, a robust spectral clustering algorithm (referred to as PKO+HSC) is developed for data clustering. Experimental results on various datasets, with and without noisy perturbation and outlier corruption, demonstrate that the proposed PKO+HSC has higher clustering robustness and effectiveness than competing algorithms in most cases.\nOn the other hand, this work is likely to have two limitations: 1) it is incapable of adaptively combining the aforementioned three types of hypergraphs; and 2) the number of clusters in spectral clustering is required to be provided in advance. Therefore, our future work is to figure out an adaptive weighting mechanism for hypergraph combination and an effective scheme for automatically estimating the cluster number prior to spectral clustering."
    } ],
    "references" : [ {
      "title" : "Multiway Partitioning via Geometric Embeddings, Orderings and Dynamic Programming",
      "author" : [ "C.J. Alpert", "A.B. Kahng" ],
      "venue" : "IEEE Trans. Computer-aided Design of Integrated Circuits and Systems, Vol.14, Iss.11, pp.1342-1358, 1995.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Spectral K-Way Ratio- Cut Partitioning and Clustering",
      "author" : [ "P.K. Chan", "M.D.F. Schlag", "J.Y. Zien" ],
      "venue" : "IEEE Trans. Computer-aided Design of Integrated Circuits and Systems, Vol.13, Iss.9, pp.1088-1096, 1994.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "An Improved Spectral Graph Partitioning Algorithm for Mapping Parallel Computations",
      "author" : [ "B. Hendrickson", "R. Leland" ],
      "venue" : "SIAM J. Sci. Comput., Vol.16, Iss.2, pp.452-459, 1995.  APPEARING IN IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, 2013  10",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Normalized Cuts and Image Segmentation",
      "author" : [ "J. Shi", "J. Malik" ],
      "venue" : "IEEE Trans. Pattern Aanalysis Mach. Intelli., Vol.22, Iss.8, pp.888-905, 2000.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Contour and Texture Analysis for Image Segmentation",
      "author" : [ "J. Malik", "S. Belongie", "T. Leung", "J. Shi" ],
      "venue" : "Int. J. Computer Vision, 2001.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Segmentation Using Eigenvectors: A Unifying View",
      "author" : [ "Y. Weiss" ],
      "venue" : "Proc. Int. Conf. Computer Vision, pp.975-982, 1999.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Learning Segmentation by Random Walks",
      "author" : [ "M. Meila", "J. Shi" ],
      "venue" : "Proc. Advances in Neural Information Processing Systems, pp.873-879, 2000.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Self-Organization in Vision: Stochastic Clustering for Image Segmentation, Perceptual Grouping, and Image Database Organization",
      "author" : [ "Y. Gdalyahu", "D. Weinshall", "M. Werman" ],
      "venue" : "IEEE Trans. Pattern Analysis Mach. Intelli., Vol. 23, Iss. 10, pp.1053-1074, Oct. 2001.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "A Min-Max Cut Algorithm for Graph Partitioning and Data Clustering",
      "author" : [ "C.H.Q. Ding", "X. He", "H. Zha", "M. Gu", "H.D. Simon" ],
      "venue" : "Proc. Int. Conf. Data Mining, pp.107-114, 2001.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "On Spectral Clustering: Analysis and An Algorithm",
      "author" : [ "A.Y. Ng", "M.I. Jordan", "Y. Weiss" ],
      "venue" : "Proc. Advances in Neural Information Processing Systems, MIT Press, 2001.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Self-Tuning Spectral Clustering",
      "author" : [ "L. Zelnik-Manor", "P. Perona" ],
      "venue" : "Proc. Advances in Neural Information Processing Systems, pp.1601-1608, 2005.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Multiclass Spectral Clustering",
      "author" : [ "S.X. Yu", "J. Shi" ],
      "venue" : "Proc. Int. Conf. Computer Vision, Vol.1, pp.313-319, 2003.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Diffusion Maps, Spectral Clustering and Eigenfunctions of Fokkerplanck Operators",
      "author" : [ "B. Nadler", "S. Lafon", "R. Coifman", "I. Kevrekidis" ],
      "venue" : "Proc. Advances in Neural Information Processing Systems, pp.955-962, 2006.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Robust Path-Based Spectral Clustering with Application to Image Segmentation",
      "author" : [ "H. Chang", "D.Y. Yeung" ],
      "venue" : "Proc. Int. Conf. Computer Vision, 2005.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Noise Robust Spectral Clustering",
      "author" : [ "Z. Li", "J. Liu", "S. Chen", "X. Tang" ],
      "venue" : "Proc. Int. Conf. Computer Vision, 2007.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "A Tutorial on Spectral Clustering",
      "author" : [ "U. Von Luxburg" ],
      "venue" : "Statistics and Computing, Vol. 17, Iss. 4, pp. 395-416, 2007.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Learning With Hypergraphs: Clustering, Classification, and Embedding",
      "author" : [ "D. Zhou", "J. Huang", "B. Schökopf" ],
      "venue" : "Proc. Advances in Neural Information Processing Systems, 2006.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Beyond Pairwise Clustering",
      "author" : [ "S. Agarwal", "J. Lim", "L. Zelnik Manor", "P. Perona", "D. Kriegman", "S. Belongie" ],
      "venue" : "Proc. IEEE Conf. Computer Vision Pattern Recognition, 2005.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Probabilistic Graph and Hypergraph Matching",
      "author" : [ "R. Zass", "A. Shashua" ],
      "venue" : "Proc. IEEE Conf. Computer Vision Pattern Recognition, 2008.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Hypergraph Spectral Learning for Multi-Label Classification",
      "author" : [ "L. Sun", "S. Ji", "J. Ye" ],
      "venue" : "Proc. ACM SIG KDD, 2008.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Video Object Segmentation by Hypergraph Cut",
      "author" : [ "Y. Huang", "Q. Liu", "D. Metaxas" ],
      "venue" : "Proc. IEEE Conf. Computer Vision Pattern Recognition, 2009.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Image Retrieval via Probabilistic Hypergraph Ranking",
      "author" : [ "Y. Huang", "Q. Liu", "S. Zhang", "D.N. Metaxas" ],
      "venue" : "Proc. IEEE Conf. Computer Vision Pattern Recognition, 2010.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Modeling The Shape of The Scene: A Holistic Representation of The Spatial Envelope",
      "author" : [ "A. Oliva", "A. Torralba" ],
      "venue" : "Int. J. Computer Vision, 42(3):145-175, 2001.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Motion Trajectory Learning in the DFT- Coefficient Feature Space",
      "author" : [ "A. Naftel", "S. Khalid" ],
      "venue" : "Proc. IEEE Int. Conf. Computer Vision Systems, 2006.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Semantic-Based Surveillance Video Retrieval",
      "author" : [ "W. Hu", "D. Xie", "Z. Fu", "W. Zeng", "S. Maybank" ],
      "venue" : "IEEE Trans. on Image Processing, Vol. 16, Iss. 4, pp. 1168-1181, 2007.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Motion-Based Video Retrieval by Trajectory Matching",
      "author" : [ "J. Hsieh", "S. Yu", "Y. Chen" ],
      "venue" : "IEEE Trans. on Circuit System for Video Technology, Vol. 16, Iss. 3, pp. 396-409, 2006.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "A Variant of the Trace Quotient Formulation for Dimensionality Reduction",
      "author" : [ "P. Wang", "C. Shen", "H. Zheng", "Z. Ren" ],
      "venue" : "pp. 277-286, in Proc. Asian Conf. Computer Vision, 2009.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Trace Ratio vs. Ratio Trace for Dimensionality Reduction",
      "author" : [ "H. Wang", "S. Yan", "D. Xu", "X. Tang", "T. Huang" ],
      "venue" : "in Proc. IEEE Conf. Computer Vision Pattern Recognition,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2007
    }, {
      "title" : "Trace Ratio Problem Revisited",
      "author" : [ "Y. Jia", "F. Nie", "C. Zhang" ],
      "venue" : "IEEE. Trans. on Neural Networks,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2009
    }, {
      "title" : "Constrained Spectral Clustering through Affinity Propagation",
      "author" : [ "Z. Lu", "M.A. Carreira-Perpiñán" ],
      "venue" : "Proc. IEEE Conf. Computer Vision Pattern Recognition, 2008.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Spectral Graph Partitioning Based on a Random Walk Diffusion Similarity Measure",
      "author" : [ "X. Li", "W. Hu", "Z. Zhang", "Y. Liu" ],
      "venue" : "Proc. Asian Conf. Computer Vision, 2009.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "The Trace Ratio Optimization Problem",
      "author" : [ "T.T. Ngo", "M. Bellalij", "Y. Saad" ],
      "venue" : "SIAM Review, Vol. 54, Iss. 3, pp. 545-569, 2012.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Discriminative Nonnegative Spectral Clustering with Out-of-Sample Extension",
      "author" : [ "Y. Yang", "Y. Yang", "Y. Zhang", "X. Du", "H.T. Shen", "X. Zhou" ],
      "venue" : "IEEE Trans. Knowledge and Data Engineering, 2012.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Clustering with Local and Global Regularization",
      "author" : [ "F. Wang", "C. Zhang", "T. Li" ],
      "venue" : "IEEE Trans. Knowledge and Data Engineering, Vol. 21, Iss. 12, pp. 1665-1678, 2009.",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Spectral Regression for Efficient Regularized Subspace Learning",
      "author" : [ "D. Cai", "X. He", "J. Han" ],
      "venue" : "Proc. Int. Conf. Computer Vision, 2007.",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Supervised Dimensionality Reduction via Sequential Semidefinite Programming",
      "author" : [ "C. Shen", "H. Li", "B.J. Michael" ],
      "venue" : "Pattern Recognition, Vol. 41, Iss. 12, pp. 3644-3652, 2008.",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "F 1 INTRODUCTION Spectral clustering is an effective means of clustering data with complex topological structure [8]–[13], [15], [16], [31], [34]–[36].",
      "startOffset" : 113,
      "endOffset" : 116
    }, {
      "referenceID" : 12,
      "context" : "F 1 INTRODUCTION Spectral clustering is an effective means of clustering data with complex topological structure [8]–[13], [15], [16], [31], [34]–[36].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 14,
      "context" : "F 1 INTRODUCTION Spectral clustering is an effective means of clustering data with complex topological structure [8]–[13], [15], [16], [31], [34]–[36].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 15,
      "context" : "F 1 INTRODUCTION Spectral clustering is an effective means of clustering data with complex topological structure [8]–[13], [15], [16], [31], [34]–[36].",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 29,
      "context" : "F 1 INTRODUCTION Spectral clustering is an effective means of clustering data with complex topological structure [8]–[13], [15], [16], [31], [34]–[36].",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 32,
      "context" : "F 1 INTRODUCTION Spectral clustering is an effective means of clustering data with complex topological structure [8]–[13], [15], [16], [31], [34]–[36].",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 34,
      "context" : "F 1 INTRODUCTION Spectral clustering is an effective means of clustering data with complex topological structure [8]–[13], [15], [16], [31], [34]–[36].",
      "startOffset" : 146,
      "endOffset" : 150
    }, {
      "referenceID" : 0,
      "context" : "It plays an important role in unsupervised learning from data, and therefore has a wide range of applications, including circuit layout [1], [2], load balancing [3], image segmentation [4]–[7], [14], motion segmentation [25], video retrieval [26], etc.",
      "startOffset" : 136,
      "endOffset" : 139
    }, {
      "referenceID" : 1,
      "context" : "It plays an important role in unsupervised learning from data, and therefore has a wide range of applications, including circuit layout [1], [2], load balancing [3], image segmentation [4]–[7], [14], motion segmentation [25], video retrieval [26], etc.",
      "startOffset" : 141,
      "endOffset" : 144
    }, {
      "referenceID" : 2,
      "context" : "It plays an important role in unsupervised learning from data, and therefore has a wide range of applications, including circuit layout [1], [2], load balancing [3], image segmentation [4]–[7], [14], motion segmentation [25], video retrieval [26], etc.",
      "startOffset" : 161,
      "endOffset" : 164
    }, {
      "referenceID" : 3,
      "context" : "It plays an important role in unsupervised learning from data, and therefore has a wide range of applications, including circuit layout [1], [2], load balancing [3], image segmentation [4]–[7], [14], motion segmentation [25], video retrieval [26], etc.",
      "startOffset" : 185,
      "endOffset" : 188
    }, {
      "referenceID" : 6,
      "context" : "It plays an important role in unsupervised learning from data, and therefore has a wide range of applications, including circuit layout [1], [2], load balancing [3], image segmentation [4]–[7], [14], motion segmentation [25], video retrieval [26], etc.",
      "startOffset" : 189,
      "endOffset" : 192
    }, {
      "referenceID" : 13,
      "context" : "It plays an important role in unsupervised learning from data, and therefore has a wide range of applications, including circuit layout [1], [2], load balancing [3], image segmentation [4]–[7], [14], motion segmentation [25], video retrieval [26], etc.",
      "startOffset" : 194,
      "endOffset" : 198
    }, {
      "referenceID" : 24,
      "context" : "It plays an important role in unsupervised learning from data, and therefore has a wide range of applications, including circuit layout [1], [2], load balancing [3], image segmentation [4]–[7], [14], motion segmentation [25], video retrieval [26], etc.",
      "startOffset" : 242,
      "endOffset" : 246
    }, {
      "referenceID" : 10,
      "context" : "In the literature, Zelnik-Manor and Perona [11] attempt to address issues i) and ii) by designing a local scaling mechanism, which adaptively calculates the affinity matrix and explores the intrinsic structural information on the energy eigenvalue spectrum of the normalized graph Laplacian to discover the number of clusters.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 10,
      "context" : "Following [11], Li et al.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 14,
      "context" : "[15] propose a noise robust spectral clustering (NRSC) algorithm to resolve issues i) and iii).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "More recently, hypergraph analysis [17], [18] has emerged as a popular tool for addressing issues iii) and iv).",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 17,
      "context" : "More recently, hypergraph analysis [17], [18] has emerged as a popular tool for addressing issues iii) and iv).",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 18,
      "context" : "This has been applied to many domains such as image matching [19], multi-label classification [20], video object segmentation [21], and image retrieval [22].",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 19,
      "context" : "This has been applied to many domains such as image matching [19], multi-label classification [20], video object segmentation [21], and image retrieval [22].",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 20,
      "context" : "This has been applied to many domains such as image matching [19], multi-label classification [20], video object segmentation [21], and image retrieval [22].",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 21,
      "context" : "This has been applied to many domains such as image matching [19], multi-label classification [20], video object segmentation [21], and image retrieval [22].",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 19,
      "context" : "[20] carry out hypergraph construction by sequentially introducing new vertices into existing hyperedges using clique expansion or star expansion.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "[22] propose a probabilistic hypergraph model that softly assigns a vertex to a hyperedge according to the similarity between the vertex and the centroid of the hyperedge.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "K-way normalized cut [12] is a well-known hypergraph partitioning criterion, which aims to optimally partition the vertex set V into K disjoint subsets (i.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 11,
      "context" : "As pointed out in [12], the optimization problem (12) is typically relaxed to: max h(Z) = 1 K tr(Z SZ), s.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 11,
      "context" : "According to the conclusion in [12], we obtain X̃ = Diag(diag− 1 2 (PP ))P that is the corresponding inverse transform of P = X(XTX)− 1 2 .",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 26,
      "context" : "(17) This is a trace-ratio-sum optimization problem, which is non-convex and difficult to solve [28].",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 27,
      "context" : "(19) The trace-ratio optimization problem (19) has been investigated in [29], [30], [33], [38].",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 28,
      "context" : "(19) The trace-ratio optimization problem (19) has been investigated in [29], [30], [33], [38].",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 31,
      "context" : "(19) The trace-ratio optimization problem (19) has been investigated in [29], [30], [33], [38].",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 35,
      "context" : "(19) The trace-ratio optimization problem (19) has been investigated in [29], [30], [33], [38].",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 31,
      "context" : "(19), we therefore utilize the Newton-Lanczos algorithm [33] for trace-ratio maximization.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 11,
      "context" : "As a result, an iterative refining procedure [12] may be used to find the optimal discrete hypergraph partitioning solution X to Eq.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 11,
      "context" : "(14) (more details can be found in Steps four to eight of the algorithm in [12]).",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 25,
      "context" : "6, the fifth dataset [27] is a trajectory dataset containing 2500 trajectories from 50 clusters, and each cluster comprises 50 trajectories with complex shapes.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 22,
      "context" : "In addition, the corresponding image features for the Corel dataset are the 960-dimensional GIST descriptors (as in [23]) that are widely used in computer vision and pattern recognition.",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 23,
      "context" : "The corresponding features for the trajectory dataset are 18-dimensional discrete Fourier transform (DFT) coefficient features (as in [24]).",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 9,
      "context" : "In our case, we take advantage of classic spectral clustering [10] and multi-class spectral clustering [12] to generate a set of vertex communities.",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 11,
      "context" : "In our case, we take advantage of classic spectral clustering [10] and multi-class spectral clustering [12] to generate a set of vertex communities.",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 9,
      "context" : "For descriptive convenience, they are respectively referred to as CSC (classic spectral clustering [10]), STSC (selftuning spectral clustering [11]), and NRSC (noise-robust spectral clustering [15]).",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 10,
      "context" : "For descriptive convenience, they are respectively referred to as CSC (classic spectral clustering [10]), STSC (selftuning spectral clustering [11]), and NRSC (noise-robust spectral clustering [15]).",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 14,
      "context" : "For descriptive convenience, they are respectively referred to as CSC (classic spectral clustering [10]), STSC (selftuning spectral clustering [11]), and NRSC (noise-robust spectral clustering [15]).",
      "startOffset" : 193,
      "endOffset" : 197
    }, {
      "referenceID" : 11,
      "context" : "In order to evaluate the performance of different graph partitioning criteria, we perform a comparison experiment against PKO+NC (our context-aware hypergraph similarity measure together with the normalized cut criterion [12]).",
      "startOffset" : 221,
      "endOffset" : 225
    }, {
      "referenceID" : 30,
      "context" : "(11)), we make a quantitative comparison with another similarity measure called RWDSM (Random Walk Diffusion Similarity Measure [32]).",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 0,
      "context" : "In principle, NMI(X,Y) has the range of [0, 1], and is equal to 1 when X = Y.",
      "startOffset" : 40,
      "endOffset" : 46
    } ],
    "year" : 2014,
    "abstractText" : "Abstract—Spectral clustering is a powerful tool for unsupervised data analysis. In this paper, we propose a contextaware hypergraph similarity measure (CAHSM), which leads to robust spectral clustering in the case of noisy data. We construct three types of hypergraph—the pairwise hypergraph, the k-nearest-neighbor (kNN) hypergraph, and the high-order over-clustering hypergraph. The pairwise hypergraph captures the pairwise similarity of data points; the kNN hypergraph captures the neighborhood of each point; and the clustering hypergraph encodes high-order contexts within the dataset. By combining the affinity information from these three hypergraphs, the CAHSM algorithm is able to explore the intrinsic topological information of the dataset. Therefore, data clustering using CAHSM tends to be more robust. Considering the intracluster compactness and the inter-cluster separability of vertices, we further design a discriminative hypergraph partitioning criterion (DHPC). Using both CAHSM and DHPC, a robust spectral clustering algorithm is developed. Theoretical analysis and experimental evaluation demonstrate the effectiveness and robustness of the proposed algorithm.",
    "creator" : "TeX"
  }
}