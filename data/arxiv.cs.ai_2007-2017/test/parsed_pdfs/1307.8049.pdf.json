{
  "name" : "1307.8049.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Optimistic Concurrency Control for Distributed Unsupervised Learning",
    "authors" : [ "Xinghao Pan", "Joseph E. Gonzalez", "Stefanie Jegelka" ],
    "emails" : [ "xinghao@eecs.berkeley.edu", "jegonzal@eecs.berkeley.edu", "stefje@eecs.berkeley.edu", "tab@stat.berkeley.edu", "jordan@cs.berkeley.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "The desire to apply machine learning to increasingly larger datasets has pushed the machine learning community to address the challenges of distributed algorithm design: partitioning and coordinating computation across the processing resources. In many cases, when computing statistics of iid data or transforming features, the computation factors according to the data and coordination is only required during aggregation. For these embarrassingly parallel tasks, the machine learning community has embraced the map-reduce paradigm, which provides a template for constructing distributed algorithms that are fault tolerant, scalable, and easy to study.\nHowever, in pursuit of richer models, we often introduce statistical dependencies that require more sophisticated algorithms (e.g., collapsed Gibbs sampling or coordinate ascent) which were developed and studied in the serial setting. Because these algorithms iteratively transform a global state, parallelization can be challenging and often requires frequent and complex coordination.\nRecent efforts to distribute these algorithms can be divided into two primary approaches. The mutual exclusion approach, adopted by [12] and [16],\nar X\niv :1\n30 7.\n80 49\nv1 [\ncs .L\nG ]\nguarantees a serializable execution preserving the theoretical properties of the serial algorithm but at the expense of parallelism and costly locking overhead. Alternatively, in the coordination-free approach, proposed by [21] and [1], processors communicate frequently without coordination minimizing the cost of contention but leading to stochasticity, data-corruption, and requiring potentially complex analysis to prove algorithm correctness.\nIn this paper we explore a third approach, optimistic concurrency control (OCC) [14] which offers the performance gains of the coordination-free approach while at the same time ensuring a serializable execution and preserving the theoretical properties of the serial algorithm. Like the coordination-free approach, OCC exploits the infrequency of data-corrupting operations. However, instead of allowing occasional data-corruption, OCC detects data-corrupting operations and applies correcting computation. As a consequence, OCC automatically ensures correctness, and the analysis is only necessary to guarantee optimal scaling performance.\nWe apply OCC to distributed nonparametric unsupervised learning— including but not limited to clustering—and implement distributed versions of the DP-Means [13], BP-Means [4], and online facility location (OFL) algorithms. We demonstrate how to analyze OCC in the context of the DP-Means algorithm and evaluate the empirical scalability of the OCC approach on all three of the proposed algorithms. The primary contributions of this paper are:\n1. Concurrency control approach to distributing unsupervised learning algorithms.\n2. Reinterpretation of online nonparametric clustering in the form of facility location with approximation guarantees.\n3. Analysis of optimistic concurrency control for unsupervised learning.\n4. Application to feature modeling and clustering."
    }, {
      "heading" : "1 Optimistic Concurrency Control",
      "text" : "Many machine learning algorithms iteratively transform some global state (e.g., model parameters or variable assignment) giving the illusion of serial dependencies between each operation. However, due to sparsity, exchangeability, and other symmetries, it is often the case that many, but not all, of the state-transforming operations can be computed concurrently while still preserving serializability: the equivalence to some serial execution where individual operations have been reordered.\nThis opportunity for serializable concurrency forms the foundation of distributed database systems. For example, two customers may concurrently make purchases exhausting the inventory of unrelated products, but if they try to purchase the same product then we may need to serialize their purchases to ensure sufficient inventory. One solution (mutual exclusion) associates locks with each product type and forces each purchase of the same product to be processed serially. This might work for an unpopular, rare product but if we are\ninterested in selling a popular product for which we have a large inventory the serialization overhead could lead to unnecessarily slow response times. To address this problem, the database community has adopted optimistic concurrency control (OCC) [14] in which the system tries to satisfy the customers requests without locking and corrects transactions that could lead to negative inventory (e.g., by forcing the customer to checkout again).\nOptimistic concurrency control exploits situations where most operations can execute concurrently without conflicting or violating serial invariants in our program. For example, given sufficient inventory the order in which customers are satisfied is immaterial and concurrent operations can be executed serially to yield the same final result. However, in the rare event that inventory is nearly depleted two concurrent purchases may not be serializable since the inventory can never be negative. By shifting the cost of concurrency control to rare events we can admit more costly concurrency control mechanisms (e.g., re-computation) in exchange for an efficient, simple, coordination-free execution for the majority of the events.\nFormally, to apply OCC we must define a set of transactions (i.e., operations or collections of operations), a mechanism to detect when a transaction violates serialization invariants (i.e., cannot be executed concurrently), and a method to correct (e.g., rollback) transactions that violate the serialization invariants. Optimistic concurrency control is most effective when the cost of validating concurrent transactions is small and conflicts occur infrequently.\nMachine learning algorithms are ideal for optimistic concurrency control. The conditional independence structure and sparsity in our models and data often leads to sparse parameter updates substantially reducing the chance of conflicts. Similarly, symmetry in our models often provides the flexibility to reorder serial operations while preserving algorithm invariants. Because the dependency structure is encoded in the model we can easily detect when an operation violates serial invariants and correct by rejecting the change and rerunning the computation. Alternatively, we can exploit the semantics of the operations to resolve the conflict by accepting a modified update. As a consequence OCC allows us to easily construct provably correct and efficient distributed algorithms without the need to develop new theoretical tools to analyze chaotic convergence or non-deterministic distributed behavior."
    }, {
      "heading" : "1.1 The OCC Pattern for Machine Learning",
      "text" : "Optimistic concurrency control can be distilled to a simple pattern (metaalgorithm) for the design and implementation of distributed machine learning systems. We begin by evenly partitioning N data points (and the corresponding computation) across the P available processors. Each processor maintains a replicated view of the global state and serially applies the learning algorithm as a sequence of operations on its assigned data and the global state. If an operation mutates the global state in a way that preserves the serialization invariants then the operation is accepted locally and its effect on the global state, if any, is eventually replicated to other processors.\nHowever, if an operation could potentially conflict with operations on other processors then it is sent to a unique serializing processor where it is rejected or corrected and the resulting global state change is eventually replicated to the rest of the processors. Meanwhile the originating processor either tentatively accepts the state change (if a rollback operator is defined) or proceeds as though the operation has been deferred to some point in the future.\nWhile it is possible to execute this pattern asynchronously with minimal coordination, for simplicity we adopt the bulk-synchronous model of [23] and divide the computation into epochs. Within an epoch t, b data points B(p, t) are evenly assigned to each of the P processors. Any state changes or serialization operations are transmitted at the end of the epoch and processed before the next epoch. While potentially slower than an asynchronous execution, the bulksynchronous execution is deterministic and can be easily expressed using existing systems like Hadoop or Spark [25]."
    }, {
      "heading" : "2 OCC for Unsupervised Learning",
      "text" : "Much of the existing literature on distributed machine learning algorithms has focused on classification and regression problems, where the underlying model is continuous. In this paper we apply the OCC pattern to machine learning problems that have a more discrete, combinatorial flavor—in particular unsupervised clustering and latent feature learning problems. These problems exhibit symmetry via their invariance to both data permutation and cluster or feature permutation. Together with the sparsity of interacting operations in their existing serial algorithms, these problems offer a unique opportunity to develop OCC algorithms.\nThe K-means algorithm provides a paradigm example; here the inferential goal is to partition the data. Rather than focusing solely on K-means, however, we have been inspired by recent work in which a general family of K-means-like algorithms have been obtained by taking Bayesian nonparametric (BNP) models based on combinatorial stochastic processes such as the Dirichlet process, the beta process, and hierarchical versions of these processes, and subjecting them to small-variance asymptotics where the posterior probability under the BNP model is transformed into a cost function that can be optimized [4]. The algorithms considered to date in this literature have been developed and analyzed in the serial setting; our goal is to explore distributed algorithms for optimizing these cost functions that preserve the structure and analysis of their serial counterparts."
    }, {
      "heading" : "2.1 OCC DP-Means",
      "text" : "We first consider the DP-means algorithm (Alg. 1) introduced by [13]. Like the K-means algorithm, DP-Means alternates between updating the cluster assignment zi for each point xi and recomputing the centroids C = {µk}Kk=1 associated with each clusters. However, DP-Means differs in that the number of clusters is not fixed a priori. Instead, if the distance from a given data point to\nall existing cluster centroids is greater than a parameter λ, then a new cluster is created. While the second phase is trivially parallel, the process of introducing clusters in the first phase is inherently serial. However, clusters tend to be introduced infrequently, and thus DP-Means provides an opportunity for OCC.\nIn Alg. 3 we present an OCC parallelization of the DP-Means algorithm in which each iteration of the serial DP-Means algorithm is divided into N/(Pb) bulk-synchronous epochs. The data is evenly partitioned {xi}i∈B(p,t) across processor-epochs into blocks of size b = |B(p, t)|. During each epoch t, each processor p evaluates the cluster membership of its assigned data {xi}i∈B(p,t) using the cluster centers C from the previous epoch and optimistically proposes a new set of cluster centers Ĉ. At the end of each epoch the proposed cluster centers, Ĉ, are serially validated using Alg. 2. The validation process accepts cluster centers that are not covered by (i.e., not within λ of) already accepted cluster centers. When a cluster center is rejected we update its reference to point to the already accepted center, thereby correcting the original point assignment."
    }, {
      "heading" : "2.2 OCC Facility Location",
      "text" : "The DP-Means objective turns out to be equivalent to the classic Facility Location (FL) objective: J(C) = ∑ x∈X min µ∈C ‖x− µ‖2 + λ2|C|, which selects the set of cluster centers (facilities) µ ∈ C that minimizes the shortest distance ‖x− µ‖ to each point (customer) x as well as the penalized cost of the clusters λ2 |C|. However, while DP-Means allows the clusters to be arbitrary points (e.g., C ∈ RD), FL constrains the clusters to be points C ⊆ F in a set of candidate locations F . Hence, we obtain a link between combinatorial Bayesian models and FL allowing us to apply algorithms with known approximation bounds to Bayesian inspired nonparametric models. As we will see in Section 3, our OCC algorithm provides constant-factor approximations for both FL and DP-means.\nFacility location has been studied intensely. We build on the online facility location (OFL) algorithm described by Meyerson [17]. The OFL algorithm processes each data point x serially in a single pass by either adding x to the set of clusters with probability min(1,minµ∈C ‖x− µ‖2 /λ2) or assigning x to the nearest existing cluster. Using OCC we are able to construct a distributed OFL algorithm (Alg. 4) which is nearly identical to the OCC DP-Means algorithm (Alg. 3) but which provides strong approximation bounds. The OCC OFL algorithm differs only in that clusters are introduced and validated stochastically— the validation process ensures that the new clusters are accepted with probability equal to the serial algorithm."
    }, {
      "heading" : "2.3 OCC BP-Means",
      "text" : "BP-means is an algorithm for learning collections of latent binary features, providing a way to define groupings of data points that need not be mutually\nexclusive or exhaustive like clusters. As with serial DP-means, there are two phases in serial BP-means (Alg. 7). In the first phase, each data point xi is labeled with binary assignments from a collection of features (zik = 0 if xi doesn’t belong to feature k; otherwise zik = 1) to construct a representation xi ≈ ∑ k zikfk. In the second phase, parameter values (the feature means fk ∈ Ĉ) are updated based on the assignments. The first step also includes the possibility of introducing an additional feature. While the second phase is trivially parallel, the inherently serial nature of the first phase combined with the infrequent introduction of new features points to the usefulness of OCC in this domain.\nThe OCC parallelization for BP-means follows the same basic structure as OCC DP-means. Each transaction operates on a data point xi in two phases. In the first, analysis phase, the optimal representation ∑ k zikfk is found. If xi is\nnot well represented (i.e., ‖xi − ∑ k zikfk‖ > λ), the difference is proposed as a new feature in the second validation phase. At the end of epoch t, the proposed features {fnewi } are serially validated to obtain a set of accepted features C̃. For each proposed feature fnewi , the validation process first finds the optimal representation fnewi ≈ ∑ fk∈C̃ zikfk using newly accepted features. If f new i is not\nwell represented, the difference fnewi − ∑ fk∈C̃ zikfk is added to C̃ and accepted as a new feature. Finally, to update the feature means, let F be the K-row matrix of feature means. The feature means update F ← (ZTZ)−1ZTX can be evaluated as a single transaction by computing the sums ZTZ = ∑ i ziz T i (where zi is a K × 1 column vector so ziz T i is a K ×K matrix) and ZTX = ∑ i zix T i in parallel.\nWe present the pseudocode for the OCC parallelization of BP-means in Appendix A."
    }, {
      "heading" : "3 Analysis of Correctness and Scalability",
      "text" : "We now establish the correctness and scalability of the proposed OCC algorithms. In contrast to the coordination-free pattern in which scalability is trivial and correctness often requires strong assumptions or holds only in expectation, the OCC pattern leads to simple proofs of correctness and challenging scalability analysis. However, in many cases it is preferable to have algorithms that are correct and probably fast rather than fast and possibly correct. We first establish serializability:\nTheorem 3.1 (Serializability). The distributed DP-means, OFL, and BP-means algorithms are serially equivalent to DP-means, OFL and BP-means, respectively.\nThe proof (Appendix B) of Theorem 3.1 is relatively straightforward and is obtained by constructing a permutation function that describes an equivalent serial execution for each distributed execution. The proof can easily be extended to many other machine learning algorithms.\nSerializability allows us to easily extend important theoretical properties of the serial algorithm to the distributed setting. For example, by invoking serializability, we can establish the following result for the OCC version of the online facility location (OFL) algorithm:\nLemma 3.2. If the data is randomly ordered, then the OCC OFL algorithm provides a constant-factor approximation for the DP-means objective. If the data is adversarially ordered, then OCC OFL provides a log-factor approximation to the DP-means objective.\nThe proof (Appendix B) of Lemma 3.2 is first derived in the serial setting then extended to the distributed setting through serializability. In contrast to divide-and-conquer schemes, whose approximation bounds commonly depend multiplicatively on the number of levels [18], Lemma 3.2 is unaffected by distributed processing and has no communication or coarsening tradeoffs. Furthermore, to retain the same factors as a batch algorithm on the full data, divide-and-conquer schemes need a large number of preliminary centers at lower levels [18, 2]. In that case, the communication cost can be high, since all proposed clusters are sent at the same time, as opposed to the OCC approach. We address the communication overhead (the number of rejections) for our scheme next.\nScalability The scalability of the OCC algorithms depends on the number of transactions that are rejected during validation (i.e., the rejection rate). While a general scalability analysis can be challenging, it is often possible to gain some insight into the asymptotic dependencies by making simplifying assumptions. In contrast to the coordination-free approach, we can still safely apply OCC algorithms in the absence of a scalability analysis or when simplifying assumptions do not hold.\nTo illustrate the techniques employed in OCC scalability analysis we study the DP-Means algorithm. The scalability limiting factor of the DP-Means algorithm\nis determined by the number of points that must be serially validated. In the following theorem we show that the communication cost only depends on the number of clusters and processing resources and does not directly depend on the number of data points. The proof is in App. C.\nTheorem 3.3 (DP-Means Scalability). Assume N data points are generated iid to form a random number (KN) of well-spaced clusters of diameter λ: λ is an upper bound on the distances within clusters and a lower bound on the distance between clusters. Then the expected number of serially validated points is bounded above by Pb+ E [KN ] for P processors and b points per epoch.\nUnder the separation assumptions of the theorem, the number of clusters present in N data points, KN , is exactly equal to the number of clusters found by DP-Means in N data points; call this latter quantity kN . The experimental results in Figure 3 suggest that the bound of Pb+ kN may hold more generally beyond the assumptions above. Since the master must process at least kN points, the overhead caused by rejections is Pb and independent of N .\nTo analyze the total running time, we note that after each of the N/(Pb) epochs the master and workers must communicate. Each worker must process N/P data points, and the master sees at most kN + Pb points. Thus, the total expected running time is O(N/(Pb) +N/P + Pb)."
    }, {
      "heading" : "4 Evaluation",
      "text" : "For our experiments, we generated synthetic data for clustering (DP-means and OFL) and feature modeling (BP-means). The cluster and feature proportions were generated nonparametrically as described below. All data points were generated in R16 space. The threshold parameter λ was fixed at 1.\nClustering: The cluster proportions and indicators were generated simultaneously using the stick-breaking procedure for Dirichlet processes—‘sticks’ are ‘broken’ on-the-fly to generate new clusters as and when necessary.1 For our experiments, we used a fixed concentration parameter θ = 1. Cluster means were sampled µk ∼ N(0, I16), and data points were generated at xi ∼ N(µzi , 14I16).\nFeature modeling: We use the stick-breaking procedure of [20] to generate feature weights. Unlike with Dirichlet processes, we are unable to perform stickbreaking on-the-fly with Beta processes. Instead, we generate enough features so that with high probability (> 0.9999) the remaining non-generated features will have negligible weights (< 0.0001). The concentration parameter was also fixed at θ = 1. We generated feature means fk ∼ N(0, I16) and data points xi ∼ N( ∑ k zikfk, 1 4I16)."
    }, {
      "heading" : "4.1 Simulated experiments",
      "text" : "To test the efficiency of our algorithms, we simulated the first iteration (one complete pass over all the data, where most clusters / features are created and thus greatest coordination is needed) of each algorithm in MATLAB. The number of data points, N , was varied from 256 to 2560 in intervals of 256. We also varied Pb, the number of data points processed in one epoch, from 16 to 256 in powers of 2. For each value of N and Pb, we empirically measured kN , the number of accepted clusters / features, and MN , the number of proposed clusters / features. This was repeated 400 times to obtain the empirical average Ê[MN − kN ], the number of rejections. For OCC DP-means, we observe Ê[MN − kN ] is bounded above by Pb (Fig. 3a), and that this bound is independent of the data set size, even when the assumptions of Thm 3.3 are violated. (We also verified that similar empirical results are obtained when the assumptions are not violated; see Appendix C.) As shown in Fig. 3b and Fig. 3c the same behavior is observed for the OCC OFL and OCC BP-means algorithms."
    }, {
      "heading" : "4.2 Distributed implementation and experiments",
      "text" : "We also implemented the distributed algorithms in Spark [25], an open-source cluster computing system. The DP-means and BP-means algorithms were bootstrapped by pre-processing a small number of data points (1/16 of the first Pb points)—this reduces the number of data points sent to the master on the first epoch, while still preserving serializability of the algorithms. Our Spark implementations were tested on Amazon EC2 by processing a fixed data set on 1, 2, 4, 8 m2.4xlarge (memory-optimized, quadruple extra large, with 8 virtual cores and 64.8GiB memory) instances.\n1We chose to use stick-breaking procedures because the Chinese restaurant and Indian buffet processes are inherently sequential. Stick-breaking procedures can be distributed by either truncation, or using OCC!\nIdeally, to process the same amount of data, an algorithm and implementation with perfect scaling would take half the runtime on 8 machines as it would on 4, and so on. The plots in Figure 4 shows this comparison by dividing all runtimes by the runtime on one machine.\nDP-means: We ran the distributed DP-means algorithm on 227 ≈ 134M data points, using λ = 2. The block size b was chosen to keep Pb = 223 ≈ 8M constant. The algorithm was run for 5 iterations (complete pass over all data in 16 epochs). We were able to get perfect scaling (Figure 4a) in all but the first iteration, when the master has to perform the most synchronization of proposed centers.\nOFL: The distributed OFL algorithm was run on 220 ≈ 1M data points, using λ = 2. Unlike DP-means and BP-means, we did not perform bootstrapping. Also, OFL is a single pass (one iteration) algorithm. The block size b was chosen such that Pb = 216 ≈ 66K data points are processed each epoch, which gives us 16 epochs. Figure 4b shows that we get no scaling in the first epoch, where all the work is performed by the master processing all Pb data points. In later epochs, the master’s workload decreases as fewer data points are proposed, but the workers’ workload increases as the total number of centers increases. Thus, scaling improves in the later epochs.\nBP-means: Distributed BP-means was run on 223 ≈ 8M data points, with λ = 1; block size was chosen such that Pb = 219 ≈ 0.5M is constant. Five iterations were run, with 16 epochs per iteration. As with DP-means, we were able to achieve nearly perfect scaling; see Figure 4c."
    }, {
      "heading" : "5 Related work",
      "text" : "Others have proposed alternatives to mutual exclusion and coordination-free parallelism for machine learning algorithm design. Newman [19] proposed trans-\nforming the underlying model to expose additional parallelism while preserving the marginal posterior. However, such constructions can be challenging or infeasible and many hinder mixing or convergence. Likewise, Lovell [15] proposed a reparameterization of the underlying model to expose additional parallelism through conditional independence.\nAdditional work similar in spirit to ours using OCC-like techniques includes Doshi-Velez et al. [9] who proposed an approximate parallel sampling algorithm for the IBP which is made exact by introducing an additional MetropolisHastings step, and Xu and Ihler [24] who proposed a look-ahead strategy in which future samples are computed optimistically based on the likely outcomes of current samples.\nA great amount of work addresses scalable clustering algorithms [8, 7, 10]. Many algorithms with provable approximation factors are streaming algorithms [18, 22, 6] and inherently use hierarchies, or related divide-and-conquer approaches [2]. The approximation factors in such algorithms multiply across levels [18], and demand a careful tradeoff between communication and approximation quality that is obviated in our framework. Other approaches use core sets [5, 11]. A lot of methods [2, 3, 22] first collect a set of centers and then re-cluster them, and therefore need to communicate all intermediate centers. Our approach avoids that, since a center causes no rejections in the epochs after it is established: the rejection rate does not grow with K. Still, as our examples demonstrate, our OCC framework can easily integrate and exploit many of the ideas in the cited works."
    }, {
      "heading" : "6 Discussion",
      "text" : "In this paper we have shown how optimistic concurrency control can be usefully employed in the design of distributed machine learning algorithms. As opposed to previous approaches, this preserves correctness, in most cases at a small cost. We established the equivalence of our distributed OCC DP-means, OFL and BP-means algorithms to their serial counterparts, thus preserving their theoretical properties. In particular, the strong approximation guarantees of serial OFL translate immediately to the distributed algorithm. Our theoretical analysis ensures OCC DP-means achieves high parallelism without sacrificing correctness. We implemented and evaluated all three OCC algorithms on a distributed computing platform and demonstrate strong scalability in practice.\nWe believe that there is much more to do in this vein. Indeed, machine learning algorithms have many properties that distinguish them from classical database operations and may allow going beyond the classic formulation of optimistic concurrency control. In particular we may be able to partially or probabilistically accept non-serializable operations in a way that preserves underlying algorithm invariants. Laws of large numbers and concentration theorems may provide tools for designing such operations. Moreover, the conflict detection mechanism can be treated as a control knob, allowing us to softly switch between stable, theoretically sound algorithms and potentially faster coordination-free\nalgorithms."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This research is supported in part by NSF CISE Expeditions award CCF-1139158 and DARPA XData Award FA8750-12-2-0331, and gifts from Amazon Web Services, Google, SAP, Blue Goji, Cisco, Clearstory Data, Cloudera, Ericsson, Facebook, General Electric, Hortonworks, Intel, Microsoft, NetApp, Oracle, Samsung, Splunk, VMware and Yahoo!. This material is also based upon work supported in part by the Office of Naval Research under contract/grant number N00014-11-1-0688. X. Pan’s work is also supported in part by a DSO National Laboratories Postgraduate Scholarship. T. Broderick’s work is supported by a Berkeley Fellowship."
    }, {
      "heading" : "A Pseudocode for OCC BP-means",
      "text" : "Here we show the Serial BP-Means algorithm (Alg. 7) and a parallel implementation of BP-means using the OCC pattern (Alg. 6 and Alg. 8), similar to OCC DP-means. Instead of proposing new clusters centered at the data point xi, in OCC BP-means we propose features f new i that allow us to obtain perfect representations of the data point. The validation process continues to improve on the representation xi ≈ ∑ k zikfk by using the most recently accepted features fk′ ∈ Ĉ, and only accepts a proposed feature if the data point is still not well-represented.\nAlgorithm 6: Parallel BP-means\nInput: data {xi}Ni=1, threshold λ Input: Epoch size b and P processors Input: Partitioning B(p, t) of data {xi}i∈B(p,t) to processor-epochs where b = |B(p, t)| C ← ∅ while not converged do\nfor epoch t = 1 to N/(Pb) do\nĈ ← ∅ // New candidate features for p ∈ {1, . . . , P} do in parallel\n// Process local data for i ∈ B(p, t) do // Optimistic Transaction\nfor fk ∈ C do Set zik to minimize ‖xi − ∑ j zijfj‖ 2 2\nif ‖xi − ∑ j zijfj‖ 2 2 > λ 2 then\nfnewi ← xi − ∑ j zijfj zi ← zi⊕ Ref(fnewi ) Ĉ ← Ĉ ∪ fnewi\n// Serially validate features\nC ← C ∪ DPValidate(Ĉ) Compute ZTZ = ∑ i ziz T i and Z TX = ∑ i zix T i in parallel\nRe-estimate features F ← (ZTZ)−1ZTX Output: Accepted feature centers C"
    }, {
      "heading" : "B Proof of serializability of distributed algo-",
      "text" : "rithms\nB.1 Proof of Theorem 3.1 for DP-means\nWe note that both distributed DP-means and BP-means iterate over z-updates and cluster / feature means re-estimation until convergence. In each iteration,\nAlgorithm 7: Serial BP-means\nInput: data {xi}Ni=1, threshold λ Initialize zi1 = 1, f1 = N −1 ∑ i xi, K = 1 while not converged do for i = 1 to N do\nfor k = 1 to K do Set zik to minimize ‖xi − ∑K j=1 zijfj‖ 2 2\nif ‖xi − ∑K j=1 zijfi,j‖ 2 2 > λ 2 then\nSet K ← K + 1 Create feature fK ← xi − ∑K k=1 zikfj Assign ziK ← 1 (and ziK ← 0 for j 6= i)\nF ← (ZTZ)−1ZTX\nAlgorithm 8: BPValidate\nInput: Set of proposed feature centers Ĉ C ← ∅ for fnew ∈ Ĉ do\nfor fk′ ∈ C do Set zik′ to minimize ‖fnew − ∑ fj∈C zijfj‖ 2 2\nif ‖fnew − ∑ fj∈C zijfj‖ 2 2 > λ 2 then\nC ← C ∪ { fnew − ∑ fj∈C zijfj } Ref(fnew) ← {zij}fj∈C\nOutput: Accepted feature centers C\ndistributed DP-means and BP-means perform the same set of updates as their serial counterparts. Thus, it suffices to show that each iteration of the distributed algorithm is serially equivalent to an iteration of the serial algorithm.\nConsider the following ordering on transactions:\n• Transactions on individual data points are ordered before transactions that re-estimate cluster / feature means are ordered.\n• A transaction on data point xi is ordered before a transaction on data point xj if\n1. xi is processed in epoch t, xj is processed in epoch t ′, and t < t′\n2. xi and xj are processed in the same epoch, xi and xj are not sent to the master for validation, and i < j\n3. xi and xj are processed in the same epoch, xi is not sent to the master for validation but xj is\n4. xi and xj are processed in the same epoch, xi and xj are sent to the master for validation, and the master serially validates xi before xj\nWe show below that the distributed algorithms are equivalent to the serial algorithms under the above ordering, by inductively demonstrating that the outputs of each transaction is the same in both the distributed and serial algorithms.\nDenote the set of clusters after the t epoch as Ct. The first transaction on xj in the serial ordering has C0 as its input. By definition of our ordering, this transaction belongs the first epoch, and is either (1) not sent to the master for validation, or (2) the first data point validated at the master. Thus in both the serial and distributed algorithms, the first transaction either (1) assigns xj to the closest cluster in C0 if minµk∈C0 ‖xj − µk‖ < λ, or (2) creates a new cluster with center at xj otherwise.\nNow consider any other transaction on xj in epoch t.\nCase 1: xj is not sent to the master for validation.\nIn the distributed algorithm, the input to the transaction is Ct−1. Since the transaction is not sent to the master for validation, we can infer that there exists µk ∈ Ct−1 such that ‖xj − µk‖ < λ. In the serial algorithm, xj is ordered after any xi if (1) xi was processed in an earlier epoch, or (2) xi was processed in the same epoch but not sent to the master (i.e. does not create any new cluster) and i < j. Thus, the input to this transaction is the set of clusters obtained at the end of the previous epoch, Ct−1, and the serial algorithm assigns xj to the closest cluster in Ct−1 (which is less than λ away).\nCase 2: xj is sent to the master for validation.\nIn the distributed algorithm, xj is not within λ of any cluster center in Ct−1. Let Ĉt be the new clusters created at the master in epoch t before validating xj . The distributed algorithm either (1) assigns xj to µk∗ = argminµk∈Ĉt ‖xj − µk‖ if ‖xj − µk‖ ≤ λ, or (2) creates a new cluster with center at xj otherwise.\nIn the serial algorithm, xj is ordered after any xi if (1) xi was processed in an earlier epoch, or (2) xi was processed in the same epoch t, but xi was not sent to the master (i.e. does not create any new cluster), or (3) xi was processed in the same epoch t, xi was sent to the master, and serially validated at the master before xj . Thus, the input to the transaction is Ct−1 ∪ Ĉt. We know that xj is not within λ of any cluster center in Ct−1, so the outcome of the transaction is either (1) assign xj to µk∗ = argminµk∈Ĉt ‖xj − µk‖ if ‖xj − µk‖ ≤ λ, or (2) create a new cluster with center at xj otherwise. This is exactly the same as the distributed algorithm.\nB.2 Proof of Theorem 3.1 for BP-means\nThe serial ordering for BP-means is exactly the same as that in DP-means. The proof for the serializability of BP-means follows the same argument as in the\nDP-means case, except that we perform feature assignments instead of cluster assignments.\nB.3 Proof of Theorem 3.1 for OFL\nHere we prove Theorem 3.1 that the distributed OFL algorithm is equivalent to a serial algorithm.\n(Theorem 3.1, OFL). We show that with respect to the returned centers (facilities), the distributed OFL algorithm is equivalent to running the serial OFL algorithm on a particular permutation of the input data. We assume that the input data is randomly permuted and the indices i of the points xi refer to this permutation. We assign the data points to processors by assigning the first b points to processor p1, the next b points to processor p2, and so on, cycling through the processors and assigning them batches of b points, as illustrated in Figure 5. In this respect, our ordering is generic, and can be adapted to any assignments of points to processors. We assume that each processor visits its points in the order induced by the indices, and likewise the master processes the points of an epoch in that order.\nFor the serial algorithm, we will use the following ordering of the data: Point xi precedes point xj if\n1. xi is processed in epoch t and xj is processed in epoch t ′, and t < t′, or\n2. xi and xj are processed in the same epoch and i < j.\nIf the data is assigned to processors as outlined above, then the serial algorithm will process the points exactly in the order induced by the indices. That means the set of points processed in any given epoch t is the same for the serial and distributed algorithm. We denote by Ct the global set of validated centers collected by OCC OFL up to (including) epoch t, and by C̃i the set of centers collected by the serial algorithm up to (including) point xi.\nWe will prove the equivalence inductively.\nEpoch t = 1. In the first epoch, all points are sent to the master. These are the first Pb points. Since the master processes them in the same order as the serial algorithm, the distributed and serial algorithms are equivalent.\nEpoch t > 1. Assume that the algorithms are equivalent up to point xi−1 in the serial order, and point xi is processed in epoch t. By assumption, the set Ct−1 of global facilities for the distributed algorithm is the same as the set C̃(t−1)Pb collected by the serial algorithm up to point x(t−1)Pb. For notational convenience, let D(xi, Ct) = minµ∈Ct D(xi, µ) be the distance of xi to the closest global facility.\nThe essential issue to prove is the following claim:\nClaim 1. If the algorithms are equivalent up to point xi−1, then the probability of xi becoming a new facility is the same for the distributed and serial algorithm.\nThe serial algorithm accepts xi as a new facility with probability min{1, D(xi, C̃i−1)/λ2}. The distributed algorithm sends xi to the master with probability min{1, D(xi, Ct−1)}. The probability of ultimate acceptance (validation) of xi as a global facility is the probability of being sent to the master and being accepted by the master. In epoch t, the master receives a set of candidate facilities with indices between (t − 1)Pb + 1 and tPb. It processes them in the order of their indices, i.e., all candidates xj with j < i are processed before i. Hence, the assumed equivalence of the algorithms up to point xi−1 implies that, when the master processes xi, the set Ct−1 ∪ Ĉ equals the set of facilities C̃i−1 of the serial algorithm. The master consolidates xi as a global facility with probability 1 if D(xi, C̃i−1 ∪ Ĉ) > λ2 and with probability D(xi, C̃i−1 ∪ Ĉ)/D(xi, Ct−1) otherwise.\nWe now distinguish two cases. If the serial algorithm accepts xi because D(xi, C̃i−1) ≥ λ2, then for the distributed algorithm, it holds that\nD(xi, Ct−1) ≥ D(xi, Ct−1 ∪ Ĉ) = D(xi, C̃i−1) ≥ λ2 (1)\nand therefore the distributed algorithm also always accepts xi. Otherwise, if D(xi, C̃i−1) < λ2, then the serial algorithm accepts with probability D(xi, C̃i−1)/λ2. The distributed algorithm accepts with probability\nP(xi accepted) = P(xi sent to master ) · P(xi accepted at master) (2)\n= D(xi, Ct−1) λ2 · D(xi, C̃ i−1 ∪ Ĉ) D(xi, Ct−1)\n(3)\n= D(xi, C̃i−1)\nλ2 . (4)\nThis proves the claim. The claim implies that if the algorithms are equivalent up to point xi−1, then they are also equivalent up to point xi. This proves the theorem.\nB.4 Proof of Lemma 3.2 (Approximation bound)\nWe begin by relating the results of facility location algorithms and DP-means. Recall that the objective of DP-means and FL is\nJ(C) = ∑ x∈X min µ∈C ‖x− µ‖2 + λ2|C|. (5)\nIn FL, the facilities may only be chosen from a pre-fixed set of centers (e.g., the set of all data points), whereas DP-means allows the centers to be arbitrary, and therefore be the empirical mean of the points in a given cluster. However, choosing centers from among the data points still gives a factor-2 approximation. Once we have established the corresponding clusters, shifting the means to the empirical cluster centers never hurts the objective. The following proposition has been a useful tool in analyzing clustering algorithms:\nProposition B.1. Let C∗ be an optimal solution to the DP-means problem (5), and let CFL be an optimal solution to the corresponding FL problem, where the centers are chosen from the data points. Then\nJ(CFL) ≤ 2J(C∗).\nProof. (Proposition B.1) It is folklore that Proposition B.1) holds for the Kmeans objective, i.e.,\nmin C⊆X,|C|=k n∑ i=1 min µ∈C ‖xi − µ‖2 ≤ 2 min C⊆X n∑ i=1 min µ∈C ‖xi − µ‖2. (6)\nIn particular, this holds for the optimal number K∗ = |C∗|. Hence, it holds that\nJ(CFL) ≤ min C⊆X,|C|=K∗ n∑ i=1 min µ∈C ‖xi − µ‖2 + λ2K∗ ≤ 2J(C∗). (7)\nWith this proposition at hand, all that remains is to prove an approximation factor for the FL problem.\nProof. (Lemma 3.2) First, we observe that the proof of Theorem 3.1 implies that, for any random order of the data, the OCC and serial algorithm process the data in exactly the same way, performing ultimately exactly the same operations. Therefore, any approximation factor that holds for the serial algorithm straightforwardly holds for the OCC algorithm too.\nHence, it remains to prove the approximation factor of the serial algorithm. Let CFL1 , . . . , C FL k be the clusters in an optimal solution to the FL problem, with centers µFL1 , . . . µ FL k . We analyze each optimal cluster individually. The proof follows along the lines of the proofs of Theorems 2.1 and 4.2 in [17], adapting it to non-metric squared distances. We show the proof for the constant factor, the logarithmic factor follows analogously by using the ring-splitting as in [17].\nFirst, we see that the expected total cost of any point x is bounded by the distance to the closest open facility y that is present when x arrives. If we always count in the distance of ‖x− y‖2 into the cost of x, then the expected cost is γ(x) = λ2‖x− y‖2/λ2 + ‖x− y‖2 = 2‖x− y‖2.\nWe consider an arbitrary cluster C∗i and divide it into |C∗|/2 good points and |C∗|/2 bad points. Let Di = 1|CFL| ∑ x∈C∗i ‖x − µi‖ be the average service\ncost of the cluster, and let dg and db be the service cost of the good and bad points, respectively (i.e., Di = (dg + db)/|CFLi |). The good points satisfy ‖x− µFLi ‖ ≤ 2Di. Suppose the algorithm has chosen a center, say y, from the points CFLi . Then any other point x ∈ CFLi can be served at cost at most\n‖x− y‖2 ≤ ( ‖x− µFLi ‖+ ‖y − µFLi ‖ )2 ≤ 2‖x− µFLi ‖2 + 4Di. (8)\nThat means once the algorithm has established a good center within CFLi , all other good points together may be serviced within a constant factor of the total optimal service cost of CFL, i.e., at 2dg + 4(dg + db). The assignment cost of all the good points in CFLi that are passed before opening a good facility is, by construction of the algorithm and expected waiting times, in expectation λ2. Hence, in expectation, the cost of the good points in CFLi will be bounded by∑ xgood γ(x) ≤ 2(2dg + 4dg + 4db + λ2). Next, we bound the expected cost of the bad points. We may assume that the bad points are injected randomly in between the good points, and bound the servicing cost of a bad point xb ∈ CFLi in terms of the closest good point xg ∈ CFLi preceding it in our data sequence. Let y be the closest open facility to µFLi when y arrives. Then\n‖xb − y‖2 ≤ 2‖y − µFLi ‖2 + 2‖xb − µFL‖2. (9)\nNow assume that xg was assigned to y ′. Then\n‖y − µFLi ‖2 ≤ ‖y′ − µFLi ‖2 ≤ 2‖y′ − xg‖2 + 2‖xg − µFL‖2. (10)\nFrom (9) and (8), it then follows that\n‖xb − y‖2 ≤ 4‖y′ − xg‖2 + 4‖xg − µFL‖2 + 2‖xb − µFL‖2 (11) = 2γ(xg) + 4‖xg − µFL‖2 + 2‖xb − µFL‖2. (12)\nSince the data is randomly permuted, xg could be, with equal probability, any good point, and in expectation we will average over all good points.\nFinally, with probability 2/|CFLi | there is no good point before xg. In that case, we will count in xb as the most costly case of opening a new facility, incurring cost λ2. In summary, we can bound the expected total cost of CFL by∑\nx good γ(x) + ∑ x bad γ(x)\n≤ 12dg + 8db + λ2 + 2CFL\n2CFL λ2 + 2(2 2|CFLi | 2|CFL| (12dg + 8db + λ 2) + 4dg + 2db)\n(13)\n≤ 68dg + 42db + 4λ2 ≤ 68J(CFL). (14)\nThis result together with Proposition B.1 proves the lemma."
    }, {
      "heading" : "C Proof of master processing bound for DP-",
      "text" : "means (Theorem 3.3)\nProof. As in the theorem statement, we assume P processors, b points assigned to each processor per epoch, and N total data points. We further assume a generative model for the cluster memberships: namely, that they are generated iid from an arbitrary distribution (πj) ∞ j=1. That is, we have ∑∞ j=1 πj = 1 and, for each j, πj ∈ [0, 1]. We see that there are perhaps infinitely many latent clusters. Nonetheless, in any data set of finite size N , there will of course be only finitely many clusters to which any data point in the set belongs. Call the number of such clusters KN .\nConsider any particular cluster indexed by j. At the end of the first epoch in which a worker sees j, that worker (and perhaps other workers) will send some data point from j to the master. By construction, some data point from j will belong to the collection of cluster centers at the master by the end of the processing done at the master and therefore by the beginning of the next epoch. It follows from our assumption (all data points within a single cluster are within a λ diameter) that no other data point from cluster j will be sent to the master in future epochs. It follows from our assumption about the separation of clusters that no points in other clusters will be covered by any data point from cluster j.\nLet Sj represent the (random) number of points from cluster j sent to the master. Since there are Pb points processed by workers in a single epoch, Nj is constrained to take values between 0 and Pb. Further, note that there are a total of N/(Pb) epochs.\nLet Aj,s,t be the event that the master is sent s data points from cluster j in epoch t. All of the events {Aj,s,t} with s = 1, . . . , P b and t = 1, . . . , N/(Pb) are disjoint. Define A′j,0 to be the event that, for all epochs t = 1, . . . , N/(Pb), zero data points are sent to the master; i.e., A′j,0 := ⋃ tAj,0,t. Then A ′ j,0 is also disjoint from the events {Aj,s,t} with s = 1, . . . , P b and t = 1, . . . , N/(Pb). Finally,\nA′j,0 ∪ Pb⋃ s=1 N/(Pb)⋃ t=1 Aj,s,t\ncovers all possible data configurations. It follows that\nE[Sj ] = 0 ∗ P[A′j,0] + Pb∑ s=1 N/(Pb)∑ t=1 sP[Aj,s,t] = Pb∑ s=1 N/(Pb)∑ t=1 sP[Aj,s,t]\nNote that, for s points from cluster j to be sent to the master at epoch t, it must be the case that no points from cluster j were seen by workers during epochs 1, . . . , t − 1, and then s points were seen in epoch t. That is, P[Aj,s,t] = (1− πj)Pb(t−1) · ( Pb s ) πsj (1− πj)Pb−s.\nThen\nE[Sj ] = ( Pb∑ s=1 s ( Pb s ) πsj (1− πj)Pb−s ) · N/(Pb)∑ t=1 (1− πj)Pb(t−1) \n= πjPb · 1− (1− πj)Pb·N/(Pb)\n1− (1− πj)Pb ,\nwhere the last line uses the known, respective forms of the expectation of a binomial random variable and of the sum of a geometric series.\nTo proceed, we make use of a lemma.\nLemma C.1. Let m be a positive integer and π ∈ (0, 1]. Then 1\n1− (1− π)m ≤ 1 mπ + 1.\nProof. A particular subcase of Bernoulli’s inequality tells us that, for integer l ≤ 0 and real x ≥ −1, we have (1 + x)l ≥ 1 + lx. Choose l = −m and x = −π. Then\n(1− π)m ≤ 1 1 +mπ\n⇔ 1− (1− π)m ≥ 1− 1 1 +mπ = mπ 1 +mπ\n⇔ 1 1− (1− π)m ≤ mπ + 1 mπ = 1 mπ + 1.\nWe can use the lemma to find the expected total number of data points sent to the master:\nE ∞∑ j=1 Sj = ∞∑ j=1 ESj = ∞∑ j=1 πjPb · 1− (1− πj)N 1− (1− πj)Pb\n≤ ∞∑ j=1 πjPb · ( 1 + 1 πjPb ) · ( 1− (1− πj)N ) = Pb\n∞∑ j=1 πj ( 1− (1− πj)N ) + ∞∑ j=1 ( 1− (1− πj)N ) ≤ Pb+\n∞∑ j=1 P(cluster j occurs in the first N points)\n= Pb+ E[KN ].\nConversely,\nE ∞∑ j=1 Sj ≥ ∞∑ j=1 πjPb = Pb.\nC.1 Experiment\nTo demonstrate the bound on the expected number of data points proposed but not accepted as new centers, we generated synthetic data with separable clusters. Cluster proportions are generated using the stick-breaking procedure for the Dirichlet process, with concentration parameter θ = 1. Cluster means are set at µk = (2k, 0, 0, . . . , 0), and generated data uniformly in a ball of radius 1/2 around each center. Thus, all data points from the same cluster are at most distance 1 from one another, and more than distance of 1 from any data point from a different cluster.\nWe follow the same experimental framework in Section 4.1.\nIn the case where we have separable clusters (Figure 6), Ê[MN − kN ] is bounded from above by Pb, which is in line with the above Theorem 3.3."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2013,
    "abstractText" : "Research on distributed machine learning algorithms has focused pri-<lb>marily on one of two extremes—algorithms that obey strict concurrency<lb>constraints or algorithms that obey few or no such constraints. We consider<lb>an intermediate alternative in which algorithms optimistically assume that<lb>conflicts are unlikely and if conflicts do arise a conflict-resolution protocol<lb>is invoked. We view this “optimistic concurrency control” paradigm as<lb>particularly appropriate for large-scale machine learning algorithms, partic-<lb>ularly in the unsupervised setting. We demonstrate our approach in three<lb>problem areas: clustering, feature learning and online facility location. We<lb>evaluate our methods via large-scale experiments in a cluster computing<lb>environment. The desire to apply machine learning to increasingly larger datasets has<lb>pushed the machine learning community to address the challenges of distributed<lb>algorithm design: partitioning and coordinating computation across the pro-<lb>cessing resources. In many cases, when computing statistics of iid data or<lb>transforming features, the computation factors according to the data and coor-<lb>dination is only required during aggregation. For these embarrassingly parallel<lb>tasks, the machine learning community has embraced the map-reduce paradigm,<lb>which provides a template for constructing distributed algorithms that are fault<lb>tolerant, scalable, and easy to study.<lb>However, in pursuit of richer models, we often introduce statistical dependen-<lb>cies that require more sophisticated algorithms (e.g., collapsed Gibbs sampling<lb>or coordinate ascent) which were developed and studied in the serial setting.<lb>Because these algorithms iteratively transform a global state, parallelization can<lb>be challenging and often requires frequent and complex coordination.<lb>Recent efforts to distribute these algorithms can be divided into two pri-<lb>mary approaches. The mutual exclusion approach, adopted by [12] and [16], 1<lb>ar<lb>X<lb>iv<lb>:1<lb>30<lb>7.<lb>80<lb>49<lb>v1<lb>[<lb>cs<lb>.L<lb>G<lb>]<lb>3<lb>0<lb>Ju<lb>l<lb>2<lb>01<lb>3 guarantees a serializable execution preserving the theoretical properties of the<lb>serial algorithm but at the expense of parallelism and costly locking overhead.<lb>Alternatively, in the coordination-free approach, proposed by [21] and [1],<lb>processors communicate frequently without coordination minimizing the cost of<lb>contention but leading to stochasticity, data-corruption, and requiring potentially<lb>complex analysis to prove algorithm correctness.<lb>In this paper we explore a third approach, optimistic concurrency control<lb>(OCC) [14] which offers the performance gains of the coordination-free approach<lb>while at the same time ensuring a serializable execution and preserving the<lb>theoretical properties of the serial algorithm. Like the coordination-free approach,<lb>OCC exploits the infrequency of data-corrupting operations. However, instead<lb>of allowing occasional data-corruption, OCC detects data-corrupting operations<lb>and applies correcting computation. As a consequence, OCC automatically<lb>ensures correctness, and the analysis is only necessary to guarantee optimal<lb>scaling performance.<lb>We apply OCC to distributed nonparametric unsupervised learning—<lb>including but not limited to clustering—and implement distributed versions of<lb>the DP-Means [13], BP-Means [4], and online facility location (OFL) algorithms.<lb>We demonstrate how to analyze OCC in the context of the DP-Means algorithm<lb>and evaluate the empirical scalability of the OCC approach on all three of the<lb>proposed algorithms. The primary contributions of this paper are: 1. Concurrency control approach to distributing unsupervised learning algo-<lb>rithms. 2. Reinterpretation of online nonparametric clustering in the form of facility<lb>location with approximation guarantees. 3. Analysis of optimistic concurrency control for unsupervised learning. 4. Application to feature modeling and clustering. 1 Optimistic Concurrency Control Many machine learning algorithms iteratively transform some global state (e.g.,<lb>model parameters or variable assignment) giving the illusion of serial dependencies<lb>between each operation. However, due to sparsity, exchangeability, and other<lb>symmetries, it is often the case that many, but not all, of the state-transforming<lb>operations can be computed concurrently while still preserving serializability:<lb>the equivalence to some serial execution where individual operations have been<lb>reordered.<lb>This opportunity for serializable concurrency forms the foundation of dis-<lb>tributed database systems. For example, two customers may concurrently make<lb>purchases exhausting the inventory of unrelated products, but if they try to<lb>purchase the same product then we may need to serialize their purchases to<lb>ensure sufficient inventory. One solution (mutual exclusion) associates locks<lb>with each product type and forces each purchase of the same product to be<lb>processed serially. This might work for an unpopular, rare product but if we are",
    "creator" : "LaTeX with hyperref package"
  }
}