{
  "name" : "1611.00625.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "TorchCraft: a Library for Machine Learning Research on Real-Time Strategy Games",
    "authors" : [ "Gabriel Synnaeve", "Nantas Nardelli", "Alex Auvolat", "Soumith Chintala", "Timothée Lacroix", "Zeming Lin", "Florian Richoux", "Nicolas Usunier" ],
    "emails" : [ "gab@fb.com,", "nantas@robots.ox.ac.uk" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Deep Learning techniques [13] have recently enabled researchers to successfully tackle low-level perception problems in a supervised learning fashion. In the field of Reinforcement Learning this has transferred into the ability to develop agents able to learn to act in high-dimensional input spaces. In particular, deep neural networks have been used to help reinforcement learning scale to environments with visual inputs, allowing them to learn policies in testbeds that previously were completely intractable. For instance, algorithms such as Deep Q-Network (DQN) [14] have been shown to reach human-level performances on most of the classic ATARI 2600 games by learning a controller directly from raw pixels, and without any additional supervision beside the score. Most of the work spawned in this new area has however tackled environments where the state is fully observable, the reward function has no or low delay, and the action set is relatively small. To solve the great majority of real life problems agents must instead be able to handle partial observability, structured and complex dynamics, and noisy and high-dimensional control interfaces.\nTo provide the community with useful research environments, work was done towards building platforms based on videogames such as Torcs [27], Mario AI [20], Unreal’s BotPrize [10], the Atari Learning Environment [3], VizDoom [12], and Minecraft [11], all of which have allowed researchers to train deep learning models with imitation learning, reinforcement learning and various decision making algorithms on increasingly difficult problems. Recently there have also been efforts to unite those and many other such environments in one platform to provide a standard interface for interacting with them [4]. We propose a bridge between StarCraft: Brood War, an RTS game with an active AI research community and annual AI competitions [16, 6, 1], and Lua, with examples in Torch [9] (a machine learning library).\nar X\niv :1\n61 1.\n00 62\n5v 1\n[ cs\n.L G\n] 1\nN ov"
    }, {
      "heading" : "2 Real-Time Strategy for Games AI",
      "text" : "Real-time strategy (RTS) games have historically been a domain of interest of the planning and decision making research communities [5, 2, 6, 16, 17]. This type of games aims to simulate the control of multiple units in a military setting at different scales and level of complexity, usually in a fixed-size 2D map, in duel or in small teams. The goal of the player is to collect resources which can be used to expand their control on the map, create buildings and units to fight off enemy deployments, and ultimately destroy the opponents. These games exhibit durative moves (with complex game dynamics) with simultaneous actions (all players can give commands to any of their units at any time), and very often partial observability (a “fog of war”: opponent units not in the vicinity of a player’s units are not shown).\nRTS gameplay: Components RTS game play are economy and battles (“macro” and “micro” respectively): players need to gather resources to build military units and defeat their opponents. To that end, they often have worker units (or extraction structures) that can gather resources needed to build workers, buildings, military units and research upgrades. Workers are often also builders (as in StarCraft), and are weak in fights compared to military units. Resources may be of varying degrees of abundance and importance. For instance, in StarCraft minerals are used for everything, whereas gas is only required for advanced buildings or military units, and technology upgrades. Buildings and research define technology trees (directed acyclic graphs) and each state of a “tech tree” allow for the production of different unit types and the training of new unit abilities. Each unit and building has a range of sight that provides the player with a view of the map. Parts of the map not in the sight range of the player’s units are under fog of war and the player cannot observe what happens there. A considerable part of the strategy and the tactics lies in which armies to deploy and where.\nMilitary units in RTS games have multiple properties which differ between unit types, such as: attack range (including melee), damage types, armor, speed, area of effects, invisibility, flight, and special abilities. Units can have attacks and defenses that counter each others in a rock-paper-scissors fashion, making planning armies a extremely challenging and strategically rich process. An “opening” denotes the same thing as in Chess: an early game plan for which the player has to make choices. That is the case in Chess because one can move only one piece at a time (each turn), and in RTS games because, during the development phase, one is economically limited and has to choose which tech paths to pursue. Available resources constrain the technology advancement and the number of units one can produce. As producing buildings and units also take time, the arbitrage between investing in the economy, in technological advancement, and in units production is the crux of the strategy during the whole game.\nRelated work: Classical AI approaches normally involving planning and search [2, 15, 24, 7] are extremely challenged by the combinatorial action space and the complex dynamics of RTS games, making simulation (and thus Monte Carlo tree search) difficult [8, 22]. Other characteristics such as partial observability, the non-obvious quantification of the value of the state, and the problem of featurizing a dynamic and structured state contribute to making them an interesting problem, which altogether ultimately also make them an excellent benchmark for AI. As the scope of this paper is not to give a review of RTS AI research, we refer the reader to these surveys about existing research on RTS and StarCraft AI [16, 17].\nIt is currently tedious to do machine learning research in this domain. Most previous reinforcement learning research involve simple models or limited experimental settings [26, 23]. Other models are trained on offline datasets of highly skilled players [25, 18, 19, 21]. Contrary to most Atari games [3], RTS games have much higher action spaces and much more structured states. Thus, we advocate here to have not only the pixels as input and keyboard/mouse for commands, as in [3, 4, 12], but also a structured representation of the game state, as in\n[11]. This makes it easier to try a broad variety of models, and may be useful in shaping loss functions for pixel-based models.\nFinally, StarCraft: Brood War is a highly popular game (more than 9.5 million copies sold) with professional players, which provides interesting datasets, human feedback, and a good benchmark of what is possible to achieve within the game. There also exists an active academic community that organizes AI competitions."
    }, {
      "heading" : "3 Design",
      "text" : "The simplistic design of TorchCraft is applicable to any video game and any machine learning library or framework. Our current implementation connects Torch to a low level interface [1] to StarCraft: Brood War. TorchCraft’s approach is to dynamically inject a piece of code in the game engine that will be a server. This server sends the state of the game to a client (our machine learning code), and receives commands to send to the game. This is illustrated in Figure 1. The two modules are entirely synchronous, but the we provide two modalities of execution based on how we interact with the game:\nGame-controlled - we inject a DLL that provides the game interface to the bots, and one that includes all the instructions to communicate with the machine learning client, interpreted by the game as a player (or bot AI). In this mode, the server starts at the beginning of the match and shuts down when that ends. In-between matches it is therefore necessary to re-establish the connection with the client, however this allows for the setting of multiple learning instances extremely easily.\nGame-attached - we inject a DLL that provides the game interface to the bots, and we interact with it by attaching to the game process and communicating via pipes. In this mode there is no need to re-establish the connection with the game every time, and the control of the game is completely automatized out of the box, however it’s currently impossible to create multiple learning instances on the same guest OS.\nWhatever mode one chooses to use, TorchCraft is seen by the AI programmer as a library that provides: connect(), receive() (to get the state), send(commands), and some helper functions about specifics of StarCraft’s rules and state representation. TorchCraft also provides an efficient way to store game frames data from past (played or observed) games so that existing state (“replays”, “traces”) can be re-examined."
    }, {
      "heading" : "4 Conclusion",
      "text" : "We presented several work that established RTS games as a source of interesting and relevant problems for the AI research community to work on. We believe that an efficient bridge between low level existing APIs and machine learning frameworks/libraries would enable and foster research on such games. We presented TorchCraft: a library that enables state-of-the-art machine learning research on real game data by interfacing Torch with StarCraft: BroodWar. TorchCraft has already been used in reinforcement learning experiments on StarCraft, which led to the results in [23] (soon to be open-sourced too and included within TorchCraft)."
    }, {
      "heading" : "5 Acknowledgements",
      "text" : "We would like to thank Yann LeCun, Léon Bottou, Pushmeet Kohli, Subramanian Ramamoorthy, and Phil Torr for the continuous feedback and help with various aspects of this work. Many thanks to David Churchill for proofreading early versions of this paper."
    }, {
      "heading" : "A Frame data",
      "text" : "In addition to the visual data, the TorchCraft server extracts certain information for the game state and sends it over to the connected clients in a structured “frame”. The frame is formatted in a table in roughly the following structure:\n1 Rece i v ed update : { 2 // Number o f f rames i n the c u r r e n t game 3 // NB: a ’ game ’ can be composed o f s e v e r a l b a t t l e s 4 frame_from_bwapi : i n t 5 un i t s_myse l f : 6 { 7 // Un i t ID 8 i n t : 9 {\n10 // Un i t ID 11 t a r g e t : i n t 12 t a r g e t p o s : 13 { 14 // Abso lu t e x 15 1 : i n t 16 // Abso lu t e y 17 2 : i n t 18 } 19 // Type o f a i r weapon 20 awtype : i n t 21 // Type o f ground weapon 22 gwtype : i n t 23 // Number o f f rames b e f o r e nex t a i r weapon p o s s i b l e a t t a c k 24 awcd : i n t 25 // Number o f h i t p o i n t s 26 hp : i n t 27 // Number o f ene rgy / mana po i n t s , i f any 28 ene rgy : i n t 29 // Un i t type 30 t ype : i n t 31 p o s i t i o n : 32 { 33 // Abso lu t e x 34 1 : i n t 35 // Abso lu t e y 36 2 : i n t 37 } 38 // Number o f armor p o i n t s 39 armor : i n t 40 // Number o f f rames b e f o r e nex t ground weapon p o s s i b l e a t t a c k 41 gwcd : i n t 42 // Ground weapon a t t a c k damage 43 gwattack : i n t 44 // Pro to s s s h i e l d p o i n t s ( l i k e HP, but w i th s p e c i a l p r o p e r t i e s ) 45 s h i e l d : i n t 46 // A i r weapon a t t a c k damage 47 awattack : i n t ( a i r weapon a t t a c k damage ) 48 // S i z e o f the u n i t 49 s i z e : i n t 50 // Whether u n i t i s an enemy or not 51 enemy : boo l 52 // Whether u n i t i s i d l e , i . e . not f o l l o w i n g any o r d e r s c u r r e n t l y 53 i d l e : boo l 54 // Ground weapon max range 55 gwrange : i n t 56 // A i r weapon max range 57 awrange : i n t 58 } 59 } 60 // Same format as \" un i t s_myse l f \" 61 units_enemy : . . . 62 }"
    } ],
    "references" : [ {
      "title" : "Learning to win: Case-based plan selection in a real-time strategy game",
      "author" : [ "D.W. Aha", "M. Molineaux", "M. Ponsen" ],
      "venue" : "In International Conference on Case-Based Reasoning",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2005
    }, {
      "title" : "The arcade learning environment: An evaluation platform for general agents",
      "author" : [ "M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling" ],
      "venue" : "Journal of Artificial Intelligence Research",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2012
    }, {
      "title" : "Rts games and real-time ai research",
      "author" : [ "M. Buro", "T. Furtak" ],
      "venue" : "In Proceedings of the Behavior Representation in Modeling and Simulation Conference (BRIMS)",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2004
    }, {
      "title" : "Starcraft ai competition",
      "author" : [ "D. Churchill" ],
      "venue" : "http://www.cs.mun.ca/~dchurchill/ starcraftaicomp/,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2011
    }, {
      "title" : "Heuristic Search Techniques for Real-Time Strategy Games",
      "author" : [ "D. Churchill" ],
      "venue" : "PhD thesis, University of Alberta,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2016
    }, {
      "title" : "Fast heuristic search for rts game combat scenarios",
      "author" : [ "D. Churchill", "A. Saffidine", "M. Buro" ],
      "venue" : "AIIDE",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2012
    }, {
      "title" : "Torch7: A matlab-like environment for machine learning",
      "author" : [ "R. Collobert", "K. Kavukcuoglu", "C. Farabet" ],
      "venue" : "In BigLearn, NIPS Workshop",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2011
    }, {
      "title" : "A turing test for computer game bots",
      "author" : [ "P. Hingston" ],
      "venue" : "IEEE Transactions on Computational Intelligence and AI in Games 1,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2009
    }, {
      "title" : "The malmo platform for artificial intelligence experimentation",
      "author" : [ "M. Johnson", "K. Hofmann", "T. Hutton", "D. Bignell" ],
      "venue" : "In International joint conference on artificial intelligence (IJCAI)",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2016
    }, {
      "title" : "Vizdoom: A doombased ai research platform for visual reinforcement learning",
      "author" : [ "M. Kempka", "M. Wydmuch", "G. Runc", "J. Toczek", "W. Jaśkowski" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2016
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G Ostrovski" ],
      "venue" : "Nature 518,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2015
    }, {
      "title" : "Case-based planning and execution for real-time strategy games",
      "author" : [ "S. Ontañón", "K. Mishra", "N. Sugandh", "A. Ram" ],
      "venue" : "In International Conference on Case-Based Reasoning",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2007
    }, {
      "title" : "A survey of real-time strategy game ai research and competition in starcraft",
      "author" : [ "S. Ontanón", "G. Synnaeve", "A. Uriarte", "F. Richoux", "D. Churchill", "M. Preuss" ],
      "venue" : "Computational Intelligence and AI in Games, IEEE Transactions on 5,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2013
    }, {
      "title" : "A review of real-time strategy game ai",
      "author" : [ "G. Robertson", "I. Watson" ],
      "venue" : "AI Magazine 35,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2014
    }, {
      "title" : "Bayesian programming and learning for multi-player video games: application to RTS AI",
      "author" : [ "G. Synnaeve" ],
      "venue" : "PhD thesis, PhD thesis, Institut National Polytechnique de Grenoble—INPG,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2012
    }, {
      "title" : "A dataset for starcraft ai & an example of armies clustering",
      "author" : [ "G. Synnaeve", "P. Bessiere" ],
      "venue" : "arXiv preprint arXiv:1211.4552",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2012
    }, {
      "title" : "The 2009 mario ai competition",
      "author" : [ "J. Togelius", "S. Karakovskiy", "R. Baumgarten" ],
      "venue" : "In IEEE Congress on Evolutionary Computation",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2010
    }, {
      "title" : "Starcraft brood war data mining",
      "author" : [ "A. Uriarte" ],
      "venue" : "http://nova.wolfwork.com/dataMining.html,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2015
    }, {
      "title" : "Game-tree search over high-level game states in rts games",
      "author" : [ "A. Uriarte", "S. Ontañón" ],
      "venue" : "In Tenth Artificial Intelligence and Interactive Digital Entertainment Conference",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2014
    }, {
      "title" : "Episodic exploration for deep deterministic policies: An application to starcraft micromanagement",
      "author" : [ "N. Usunier", "G. Synnaeve", "Z. Lin", "S. Chintala" ],
      "venue" : null,
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2016
    }, {
      "title" : "Reactive planning for micromanagement in rts games",
      "author" : [ "B. Weber" ],
      "venue" : "Department of Computer Science,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2014
    }, {
      "title" : "A data mining approach to strategy prediction",
      "author" : [ "B.G. Weber", "M. Mateas" ],
      "venue" : "In 2009 IEEE Symposium on Computational Intelligence and Games (2009),",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2009
    }, {
      "title" : "Applying reinforcement learning to small scale combat in the real-time strategy game starcraft: broodwar",
      "author" : [ "S. Wender", "I. Watson" ],
      "venue" : "In Computational Intelligence and Games (CIG),",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "We present TorchCraft, an open-source library that enables deep learning research on Real-Time Strategy (RTS) games such as StarCraft: Brood War, by making it easier to control these games from a machine learning framework, here Torch [9].",
      "startOffset" : 235,
      "endOffset" : 238
    }, {
      "referenceID" : 10,
      "context" : "For instance, algorithms such as Deep Q-Network (DQN) [14] have been shown to reach human-level performances on most of the classic ATARI 2600 games by learning a controller directly from raw pixels, and without any additional supervision beside the score.",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 16,
      "context" : "To provide the community with useful research environments, work was done towards building platforms based on videogames such as Torcs [27], Mario AI [20], Unreal’s BotPrize [10], the Atari Learning Environment [3], VizDoom [12], and Minecraft [11], all of which have allowed researchers to train deep learning models with imitation learning, reinforcement learning and various decision making algorithms on increasingly difficult problems.",
      "startOffset" : 150,
      "endOffset" : 154
    }, {
      "referenceID" : 7,
      "context" : "To provide the community with useful research environments, work was done towards building platforms based on videogames such as Torcs [27], Mario AI [20], Unreal’s BotPrize [10], the Atari Learning Environment [3], VizDoom [12], and Minecraft [11], all of which have allowed researchers to train deep learning models with imitation learning, reinforcement learning and various decision making algorithms on increasingly difficult problems.",
      "startOffset" : 174,
      "endOffset" : 178
    }, {
      "referenceID" : 1,
      "context" : "To provide the community with useful research environments, work was done towards building platforms based on videogames such as Torcs [27], Mario AI [20], Unreal’s BotPrize [10], the Atari Learning Environment [3], VizDoom [12], and Minecraft [11], all of which have allowed researchers to train deep learning models with imitation learning, reinforcement learning and various decision making algorithms on increasingly difficult problems.",
      "startOffset" : 211,
      "endOffset" : 214
    }, {
      "referenceID" : 9,
      "context" : "To provide the community with useful research environments, work was done towards building platforms based on videogames such as Torcs [27], Mario AI [20], Unreal’s BotPrize [10], the Atari Learning Environment [3], VizDoom [12], and Minecraft [11], all of which have allowed researchers to train deep learning models with imitation learning, reinforcement learning and various decision making algorithms on increasingly difficult problems.",
      "startOffset" : 224,
      "endOffset" : 228
    }, {
      "referenceID" : 8,
      "context" : "To provide the community with useful research environments, work was done towards building platforms based on videogames such as Torcs [27], Mario AI [20], Unreal’s BotPrize [10], the Atari Learning Environment [3], VizDoom [12], and Minecraft [11], all of which have allowed researchers to train deep learning models with imitation learning, reinforcement learning and various decision making algorithms on increasingly difficult problems.",
      "startOffset" : 244,
      "endOffset" : 248
    }, {
      "referenceID" : 12,
      "context" : "We propose a bridge between StarCraft: Brood War, an RTS game with an active AI research community and annual AI competitions [16, 6, 1], and Lua, with examples in Torch [9] (a machine learning library).",
      "startOffset" : 126,
      "endOffset" : 136
    }, {
      "referenceID" : 3,
      "context" : "We propose a bridge between StarCraft: Brood War, an RTS game with an active AI research community and annual AI competitions [16, 6, 1], and Lua, with examples in Torch [9] (a machine learning library).",
      "startOffset" : 126,
      "endOffset" : 136
    }, {
      "referenceID" : 6,
      "context" : "We propose a bridge between StarCraft: Brood War, an RTS game with an active AI research community and annual AI competitions [16, 6, 1], and Lua, with examples in Torch [9] (a machine learning library).",
      "startOffset" : 170,
      "endOffset" : 173
    }, {
      "referenceID" : 2,
      "context" : "Real-time strategy (RTS) games have historically been a domain of interest of the planning and decision making research communities [5, 2, 6, 16, 17].",
      "startOffset" : 132,
      "endOffset" : 149
    }, {
      "referenceID" : 0,
      "context" : "Real-time strategy (RTS) games have historically been a domain of interest of the planning and decision making research communities [5, 2, 6, 16, 17].",
      "startOffset" : 132,
      "endOffset" : 149
    }, {
      "referenceID" : 3,
      "context" : "Real-time strategy (RTS) games have historically been a domain of interest of the planning and decision making research communities [5, 2, 6, 16, 17].",
      "startOffset" : 132,
      "endOffset" : 149
    }, {
      "referenceID" : 12,
      "context" : "Real-time strategy (RTS) games have historically been a domain of interest of the planning and decision making research communities [5, 2, 6, 16, 17].",
      "startOffset" : 132,
      "endOffset" : 149
    }, {
      "referenceID" : 13,
      "context" : "Real-time strategy (RTS) games have historically been a domain of interest of the planning and decision making research communities [5, 2, 6, 16, 17].",
      "startOffset" : 132,
      "endOffset" : 149
    }, {
      "referenceID" : 0,
      "context" : "Related work: Classical AI approaches normally involving planning and search [2, 15, 24, 7] are extremely challenged by the combinatorial action space and the complex dynamics of RTS games, making simulation (and thus Monte Carlo tree search) difficult [8, 22].",
      "startOffset" : 77,
      "endOffset" : 91
    }, {
      "referenceID" : 11,
      "context" : "Related work: Classical AI approaches normally involving planning and search [2, 15, 24, 7] are extremely challenged by the combinatorial action space and the complex dynamics of RTS games, making simulation (and thus Monte Carlo tree search) difficult [8, 22].",
      "startOffset" : 77,
      "endOffset" : 91
    }, {
      "referenceID" : 20,
      "context" : "Related work: Classical AI approaches normally involving planning and search [2, 15, 24, 7] are extremely challenged by the combinatorial action space and the complex dynamics of RTS games, making simulation (and thus Monte Carlo tree search) difficult [8, 22].",
      "startOffset" : 77,
      "endOffset" : 91
    }, {
      "referenceID" : 4,
      "context" : "Related work: Classical AI approaches normally involving planning and search [2, 15, 24, 7] are extremely challenged by the combinatorial action space and the complex dynamics of RTS games, making simulation (and thus Monte Carlo tree search) difficult [8, 22].",
      "startOffset" : 77,
      "endOffset" : 91
    }, {
      "referenceID" : 5,
      "context" : "Related work: Classical AI approaches normally involving planning and search [2, 15, 24, 7] are extremely challenged by the combinatorial action space and the complex dynamics of RTS games, making simulation (and thus Monte Carlo tree search) difficult [8, 22].",
      "startOffset" : 253,
      "endOffset" : 260
    }, {
      "referenceID" : 18,
      "context" : "Related work: Classical AI approaches normally involving planning and search [2, 15, 24, 7] are extremely challenged by the combinatorial action space and the complex dynamics of RTS games, making simulation (and thus Monte Carlo tree search) difficult [8, 22].",
      "startOffset" : 253,
      "endOffset" : 260
    }, {
      "referenceID" : 12,
      "context" : "As the scope of this paper is not to give a review of RTS AI research, we refer the reader to these surveys about existing research on RTS and StarCraft AI [16, 17].",
      "startOffset" : 156,
      "endOffset" : 164
    }, {
      "referenceID" : 13,
      "context" : "As the scope of this paper is not to give a review of RTS AI research, we refer the reader to these surveys about existing research on RTS and StarCraft AI [16, 17].",
      "startOffset" : 156,
      "endOffset" : 164
    }, {
      "referenceID" : 22,
      "context" : "Most previous reinforcement learning research involve simple models or limited experimental settings [26, 23].",
      "startOffset" : 101,
      "endOffset" : 109
    }, {
      "referenceID" : 19,
      "context" : "Most previous reinforcement learning research involve simple models or limited experimental settings [26, 23].",
      "startOffset" : 101,
      "endOffset" : 109
    }, {
      "referenceID" : 21,
      "context" : "Other models are trained on offline datasets of highly skilled players [25, 18, 19, 21].",
      "startOffset" : 71,
      "endOffset" : 87
    }, {
      "referenceID" : 14,
      "context" : "Other models are trained on offline datasets of highly skilled players [25, 18, 19, 21].",
      "startOffset" : 71,
      "endOffset" : 87
    }, {
      "referenceID" : 15,
      "context" : "Other models are trained on offline datasets of highly skilled players [25, 18, 19, 21].",
      "startOffset" : 71,
      "endOffset" : 87
    }, {
      "referenceID" : 17,
      "context" : "Other models are trained on offline datasets of highly skilled players [25, 18, 19, 21].",
      "startOffset" : 71,
      "endOffset" : 87
    }, {
      "referenceID" : 1,
      "context" : "Contrary to most Atari games [3], RTS games have much higher action spaces and much more structured states.",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 1,
      "context" : "Thus, we advocate here to have not only the pixels as input and keyboard/mouse for commands, as in [3, 4, 12], but also a structured representation of the game state, as in",
      "startOffset" : 99,
      "endOffset" : 109
    }, {
      "referenceID" : 9,
      "context" : "Thus, we advocate here to have not only the pixels as input and keyboard/mouse for commands, as in [3, 4, 12], but also a structured representation of the game state, as in",
      "startOffset" : 99,
      "endOffset" : 109
    }, {
      "referenceID" : 8,
      "context" : "[11].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "TorchCraft has already been used in reinforcement learning experiments on StarCraft, which led to the results in [23] (soon to be open-sourced too and included within TorchCraft).",
      "startOffset" : 113,
      "endOffset" : 117
    } ],
    "year" : 2016,
    "abstractText" : "We present TorchCraft, an open-source library that enables deep learning research on Real-Time Strategy (RTS) games such as StarCraft: Brood War, by making it easier to control these games from a machine learning framework, here Torch [9]. This white paper argues for using RTS games as a benchmark for AI research, and describes the design and components of TorchCraft.",
    "creator" : "LaTeX with hyperref package"
  }
}