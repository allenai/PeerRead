{
  "name" : "1610.01520.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Comparative study of LSA vs Word2vec embeddings in small corpora: a case study in dreams database",
    "authors" : [ "Edgar Altszyler", "Mariano Sigman", "Diego Fernández Slezak" ],
    "emails" : [ "ealtszyler@dc.uba.ar" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Corpus-based semantic representations (i.e. embeddings) exploits statistical properties of textual structure to embed words in a vectorial space. In this space, terms with similar meanings tend to be located close to each other. These methods rely in the idea that words with similar meanings tend to occur in similar contexts [1]. This proposition is called distributional hypothesis and provides a practical framework to understand and compute semantic relationship between words.\nWord embeddings has been used in a wide variety of applications such as sentiment analysis [2], psychiatry [3], psychology [4, 5], philology [6], cognitive science [7] and social science [8, 9].\nLatent Semantic Analysis (LSA) [10, 11, 12], is one of the most used methods for word meaning representation. LSA takes as input a training corpus, i.e. a collection of documents. A word by document co-occurrence matrix is constructed. Typically, normalization is applied to reduce the weight of uninformative high-frequency words in the words-documents matrix [13]. Finally, a dimensionality reduction is implemented by a truncated Singular Value Decomposition, SVD, which projects every word in a subspace\n∗Corresponding author: ealtszyler@dc.uba.ar\nar X\niv :1\n61 0.\n01 52\n0v 1\n[ cs\n.C L\n] 5\nO ct\n2 01\nof a predefined number of dimensions. Once the vectorial representation of words is obtained, the semantic similarity between two terms is typically computed by the cosine of the angle between them.\nMore recently, neural-network language embeddings have received an increasing attention [14, 15], leaving aside classical word representation methods such as LSA. In particular, Word2vec models [15, 16] have become especially popular in embeddings generation.\nWord2vec consists of two neural network language models, Continuous Bag of Words (CBOW) and Skip-gram. In both models, a window of predefined length is moved along the corpus, and in each step the network is trained with the words inside the window. Whereas the CBOW model is trained to predict the word in the center of the window based on the surrounding words, the Skip-gram model is trained to predict the contexts based on the central word. Once the neural network has been trained, the learned linear transformation in the hidden layer is taken as the word representation. In the present paper, we use Skip-gram model, which shows better performance in [16] semantic task.\nAn intrinsic difference between LSA and Word2vec is that while LSA is a counterbased model, Word2vec is a prediction-based model. Although prediction-based models have strongly increased in popularity, it is not clear whether they outperform classical counter-based models [17, 18, 19].\nIn particular, Word2vec methods have a distinct advantage in handling large datasets, since they do not consume as much memory as some classic methods like LSA and, as part of the Big Data revolution, Word2vec has been trained with large datasets of about billions of tokens. However, often in several problems of natural and social sciences one has to form semantic embeddings based on scarce data. For example, when analyzing the semantic map of a psychiatric patient or tracking the semantic network growth in children’s writing. Moreover, this kind of approach is also relevant in sociological and linguistic research, in which linguistic patterns in word meaning networks are tracked along the time line, and small time chunks are needed [9, 20].\nWhich is the best method when only small amounts of data are available? In the present paper we investigate this, on the working hypothesis that Word2vec will produce very low quality embeddings when trained with small corpus, as it is a prediction-based model and it would need lot of training data in order to fit its high number or parameters. Here we examine this hypothesis, investigating the optimality of different methods to achieve reliable semantic mappings when only medium to small corpora are available for training. In these conditions, we compare Word2vec performances with LSA in a semantic categorization test, in which the capabilities of the model to represent semantic categories (such as, drinks, countries, tools and clothes) is measured.\nThen we examine the performance of these models in a real-life and challenging problem based on relatively short texts: analyzing and disambiguating the content of dreams. Dream content show gender and cultural differences, consistency over time of the dreams content, and concordance of dreaming features (such as activity and emotions) with waking-life experiences [21, 22, 23]. Also, there is evidence of change in dreams contents after drug treatment [24] and shifts in content patterns in people with psychiatric disorders [25].\nMost of the newest dreams content analysis methods are based on frequency wordcounting of predefined categories in dreams reports [23]. A well known limitation of this\napproach is the impossibility of identifying the meaning of the counted words, which is determined by the context in which they appears. For example, the occurrence of the word fall in a dream report may be used in different contexts, such as, falling from a cliff, teeth falling out or falling sick. In this context, we will test the capabilities of LSA and Word2vec on identify patterns in the usage of words among subjects. In particular, we set to analyze the semantic neighborhood of the word run present in the dreams reports of the different subjects. We have chosen this word because its frequency in dreams and the variety of contexts where it can be used. For example, run may be associated to sports activities or with chase/escape situations, which is reported to be one of the most typical dreams [26, 27].\nHere we specifically analyze the capabilities of both models to identify word associations in dreams reports. We believe that word embeddings can bring new insights in dreams content analysis. On the other hand, we claim that LSA would be more appropriate in small-size corpus and should outperform Word2Vec performance in this context."
    }, {
      "heading" : "2 Methods",
      "text" : ""
    }, {
      "heading" : "2.1 Semantic representations",
      "text" : "Both, LSA and Word2vec semantic representations were generated with the Gensim Python library [28]. In LSA implementation, a tf-idf transformation was applied before the truncated Singular Value Decomposition. LSA’s representation dimensionality were tuned in order to maximize its performance in each case. In Word2vec (Skip-gram) implementations no minimum frequency threshold were used, and the window size, the number of negative samples and the representation dimensionality were tuned to maximize the performance. All other Skip-gram parameters were set to default Gensim values.\nGiven a vectorial representation, the semantic similarity (S) of two words was calculated using the cosine similarity measure between their respective vectorial representation (v1,v2),\nS(v1,v2) = cos(v1,v2) = v1.v2\n‖v1‖.‖v2‖ (1)\nThe semantic distances between two words d(v1,v2) was calculated as 1 minus the semantic similarity ( d(v1,v2) = 1− S(v1,v2))."
    }, {
      "heading" : "2.2 Semantic tests",
      "text" : "To compare LSA and Skip-gram semantic representation quality, we perform two tests in two different corpora (TASA and UkWaC): (1) a semantic categorization test and (2) a word-pairs similarity test. For each test, we studied how the performance of LSA and Skip-gram embeddings depend on the corpus size. To do this, we take 6 nested sub-samples of the training corpora, in which documents where progressively eliminated, following [29, 30]. In both cases, the minimum sub-corpus size contains only 600 documents. When any of the test words did not appear at least once in a sub-corpus, a random document was replaced with one of the discarded ones."
    }, {
      "heading" : "2.2.1 Semantic categorization test",
      "text" : "In this test we measured the capabilities of the model to represent the semantic categories [31, 29] (such as, drinks, countries, tools and clothes). The test is composed by 53 categories with 10 words each. In order to measure how well the word i is grouped visà-vis the other words in its semantic category we used the Silhouette Coefficients, s(i) [32],\ns(i) = b(i)− a(i)\nmax{a(i), b(i)} , (2)\nwhere a(i) is the mean distance of word i with all other words within the same category, and b(i) is the minimummean distance of word i to any words within another category (i.e. the mean distance to the neighboring category). In other words, Silhouette Coefficients measure how close a word is to other words within the same category compared to words of the closest category. The Silhouette Score is computed as the mean value of all Silhouette Coefficients. The score takes values between -1 and 1, higher values reporting localized categories with larger distances between categories, representing better clustering."
    }, {
      "heading" : "2.2.2 Word-pairs similarity test",
      "text" : "This test measures the capabilities of the model to capture semantic similarity between concepts. We used the well established WordSim353 test collection [33], which consist of 353 word-pairs (such as Maradona-football or physics-chemistry) associated with a mean human-assigned similarity score. Each word-pair is rated on a scale ranging from 0 (highly dissimilar words) to 10 (highly similar words). The evaluation score is computed as the Spearman correlation between the human scores and the model semantic similarities."
    }, {
      "heading" : "2.3 Case study: Semantic association in dreams reports",
      "text" : "In this case study, we analyze the capabilities of the models to capture semantic word associations, testing whether the models embeddings can capture the semantic neighborhood of a target word in single subject’s dream series ( a collection of dream reports written by the same person). In particular, we selected the word run as the target word, and we focused on the detection of its distance to escape/chase contexts. The rank distance of a given word “w” with respect to run was measured as the rank of “w” among the cosine similarity between run and all other words in the vocabulary. For example, if a word has a rank of 20, it means that, among all words in the vocabulary, it is the 20th closest word using a cosine similarity metric. Finally, we will define the rank distance of escape/chase concepts as the minimum value within the ranks of the words escape* and chase* 1.\nFor each dream series, two independent annotators read all the dreams in which the word run appears, and labeled whether they refer to an escape/chase situation or not. Escape/chase situations were defined as those in which (1) someone is being chased or is under the impression of being chased or (2) someone is escaping from a real or imaginary threat. Also, for a run frame to be counted as an escape/chase context, it must be\n1The asterisk (*) refers to a word in all its forms, i.e. escape* stands for escape, escapes, escaping and escaped.\nassociated to a negative emotional valence, thus discarding, for instance, escape/chase situations related to games or sports. With these criteria, for each dream series the annotators calculate the fraction of times the word run appears in an escape/chase context, obtaining a Pearson correlation coefficient of 0.98. We use the average of the annotators measurement as the ground truth, and we will refer to this values as the escape/chase fraction.\nWe used the escape/chase fraction as a ground truth to test the embeddings quality. Good representations should produce low rank distance in series with high escape/chase fraction and high rank distance in series with low escape/chase fraction. Thus, not only do we expect negative correlations between the escape/chase rank distance and the ground truth, but we also expect the differences in rank distances to be large when the models are trained with low and high escape/chase fraction.\nIn order to quantify these differences, we computed the linear regression of log10(rank distance) vs the escape/chase fraction, and we used the log-linear slope as a measurement of performance. Thus, the more negative the slope is, the better the performance. It should be noted that in this analysis the series in which the word run appears less that 5 times were excluded."
    }, {
      "heading" : "2.4 Corpora",
      "text" : "In both test, we use as training corpora the TASA corpus [34] and a random subsample of ukWaC corpus [35]. TASA corpus is a commonly used linguistic corpus consisting of 37k educational texts with a corpus size of 5M words in its cleaned form. UkWaC consists of web pages material from .uk domain. The random subsample has 140k documents with a corpus size of 57M words in its cleaned form.\nFor the case study we use the Dreambank reports corpus [36, 23]. The DreamBank corpus consists of 19k dreams reports from 59 subjects, containing about 1.3M words in its cleaned form.\nTo clean the corpora, we performed a word tokenization, discarding punctuation marks and symbols. Then, we transformed each word to lowercase and eliminated stopwords, using the stoplist in NLTK Python package [37]. Also, all numbers were replaced with the string “NUM”."
    }, {
      "heading" : "3 Results",
      "text" : ""
    }, {
      "heading" : "3.1 Corpus size analysis in the clustering test",
      "text" : "As a first step for all analyses, we carried out the Skip-gram parameter optimization for both tests (Table 1). The best scores where selected to perform the corpus size analysis. In the semantic categorization test, in the case of TASA corpus, neg = 15 was chosen given that it showed slightly better performance.\nTo compare LSA and Skip-gram embeddings quality in small size corpora, we tested both methods in random nested subsamples of TASA and ukWaC corpus (see Figure 1). Given that the appropriate embeddings dimensions depends on the corpus size [38], for each sub-corpus, we ran the models with a wide range of dimension values\n(7,15,25,50,100,200,400), using in each case the dimension that produces the best performance.\nFigure 1 shows that Skip-gram word-knowledge acquisition rate tends to be larger than LSA’s. While Skip-gram tends to produce better embeddings than LSA when they are trained with large corpora, under training with small corpora Skip-gram performance is considerable lower than LSA’s. We believe that this behavior is grounded in the fact that Skip-grams is a prediction-based model, so it requires substantial training data in order to fit its high number or parameters."
    }, {
      "heading" : "3.2 Case study: semantic association in dreams report",
      "text" : "In order to check the expected differences between the associations of the word run in dreams and waking life, we built LSA and Skip-gram embeddings trained each corpora, and we extracted the 25 words most similar to run in each case. Infrequent words which appear less than 15 times were excluded. We found that word embeddings are capable of identifying differences in usage patterns of word between dreams and waking life. In TASA and ukWaC corpora, run is linked with words associated with a big variety of contexts, such as sports, means of transport and programming, while in dreams, run is directly related with words associated with chase/escape situations. For example with LSA trained in dreams we obtained words such as: chase, scream, chasing, escape, chases, grab, screaming, nazi, hide, chased, yells, safety, devil, evil, attacking, killing, slam and yell. In the same line, with Skip-gram model we found words such as: escape, catch, chase, chasing, follow, dangerous, guards, robbers, hide, hiding, escaped, safely, safe, protect and tornado.\nThen, we tested the ability of both models to extract semantic tendencies in single dreams series following the method described in subsection 2.3. A parameter selection was made, obtaining the best performance for LSA in 200 dimensions and for Skip-gram in win=15 and neg=10 (Table 2 and Table 3).\nThe sensitivity of the method to detect both situation of the term run rely on the slope steepness. We expect negative correlations with steep slopes between the escape/chase rank distance and the escape/chase fraction (see Methods section for details). In figure 2 we plot the calculated distance vs the ground truth for each individual series in the selected parameters.\nWhile both models present a downward trend, the LSA outperforms Skip-gram with a negative log-linear slope of -1.99 and -1.12, respectively. We ran this test for 10 iterations\nand slopes showed significant difference between methods (Kolmogorov-Smirnoff test, p < 3× 10−4).\nIn order to illustrate to what extent we can use these methods to explore the usage pattern of a target word in individual dream series, we show in figure 3 the 25 closest words of run in 3 different dreams series, using the same parameter set as in figure 2. Series 1 and 2, are the two series with the highest escape/chase fraction, while series 3 has no escape/chase situations in dreams that contain the word run. In the first two series, we observe that run neighborhood in LSA embedding contains words highly related with escape/chase situations, such as chased and hide in series 1 and chasing, chases or trapped in series 2. Conversely, Skip-gram embeddings do not succeed in identifying escape/chase contexts in these series. As a control case, it can be seen that series 3 do not show escape/chase related words."
    }, {
      "heading" : "3.3 Conclusion",
      "text" : "In the present paper, we compare the capabilities of Skip-gram and LSA to learn accurate word embeddings in small text corpora. In order to do that, we first tested the models capability to represent semantic categories (such as drinks, countries, tools or clothes) in nested subsamples of a medium size corpus. We found that Word2vec embeddings outperform LSA’s when the models are trained with medium size datasets (∼ 10 millions of words). However, when the corpus size is reduced, Word2vec performance has a severe decrease, thus LSA becoming the more suitable tool. This finding gives a new insight into the prediction-based vs counter-based models discussion [17, 18, 19]. We believe that\nWord2vec performance decrease in small corpora is grounded in the fact that predictionbased models need a lot of training data in order to fit their high number of parameters.\nAs a case study, we have studied LSA and Skip-gram capabilities to extract relevant semantic words associations in dreams reports. We found that LSA can accurately capture semantic words relations even in cases of series with low number of dreams and low frequency of target words. This is a step foward to the application of word embeddings to the analysis of dreams content. This research field addresses questions such as “what do we dream about?” and “how do gender, cultural background and waking life experiences shape the dreams content?” [21, 25, 22, 23]. We propose that LSA can be used to explore words associations in dreams reports, which could bring new insight into this old research area of psychology."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We want to thank the teams behind the TASA [34], WaCky [35] and Dreambank [23] projects for providing us the corpora and Eduardo Schmidt for helpful discussions.\nConflict of Interest Statement The authors declare that there is no conflict of interest regarding the publication of this paper."
    } ],
    "references" : [ {
      "title" : "Word Distributional structure",
      "author" : [ "Z. Harris" ],
      "venue" : "23(10):146–162",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1954
    }, {
      "title" : "Semantic Compositionality through Recursive Matrix-Vector Spaces",
      "author" : [ "Richard Socher", "Brody Huval", "Christopher D Manning", "Andrew Y Ng" ],
      "venue" : "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2012
    }, {
      "title" : "Automated analysis of free speech predicts psychosis onset in high-risk youths",
      "author" : [ "Gillinder Bedi", "Facundo Carrillo", "Guillermo A. Cecchi", "Diego Fernández Slezak", "Mariano Sigman", "Natália B. Mota", "Sidarta Ribeiro", "Daniel C. Javitt", "Mauro Copelli", "Cheryl M. Corcoran" ],
      "venue" : "npj Schizophrenia,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2015
    }, {
      "title" : "Identifying Issue Frames in Text",
      "author" : [ "Eyal Sagi", "Daniel Diermeier", "Stefan Kaufmann" ],
      "venue" : "PLoS ONE,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2013
    }, {
      "title" : "Scale-invariant transition probabilities in free word association trajectories",
      "author" : [ "Martin Elias Costa", "Flavia Bonomo", "Mariano Sigman" ],
      "venue" : "Frontiers in integrative neuroscience,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2009
    }, {
      "title" : "A quantitative philology of introspection",
      "author" : [ "Carlos G. Diuk", "D. Fernandez Slezak", "I. Raskovsky", "M. Sigman", "G. a. Cecchi" ],
      "venue" : "Frontiers in Integrative Neuroscience,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2012
    }, {
      "title" : "Lsa as a theory of meaning",
      "author" : [ "Thomas K Landauer" ],
      "venue" : "Handbook of latent semantic analysis,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2007
    }, {
      "title" : "Statistically significant detection of linguistic change",
      "author" : [ "Vivek Kulkarni", "Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena" ],
      "venue" : "Proceedings of the 24th international conference on World Wide Web (WWW",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2015
    }, {
      "title" : "Indexing by latent semantic analysis",
      "author" : [ "Scott Deerwester", "Susan T Dumais", "Thomas Landauer", "George Furnas", "Richard. Harshman" ],
      "venue" : "JAsIs,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1990
    }, {
      "title" : "A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge",
      "author" : [ "Thomas K. Landauer", "Susan T. Dumais" ],
      "venue" : "Psychological Review,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1997
    }, {
      "title" : "a Graesser",
      "author" : [ "X Hu", "Z Cai", "P Wiemer-Hastings" ],
      "venue" : "and D McNamara. Strengths, limitations, and extensions of LSA. Handbook of Latent Semantic Analysis, pages 401–426",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Improving the retrieval of information from external sources",
      "author" : [ "Susan Dumais" ],
      "venue" : "Behavior Research Methods, Instruments, & Computers,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1991
    }, {
      "title" : "A Unified Architecture for Natural Language Processing : Deep Neural Networks with Multitask Learning",
      "author" : [ "Ronan Collobert", "Jason Weston" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2008
    }, {
      "title" : "Distributed Representations of Words and Phrases and their Compositionality",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean" ],
      "venue" : "Nips, pages",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2013
    }, {
      "title" : "Efficient Estimation of Word Representations in Vector Space",
      "author" : [ "Tomas Mikolov", "Greg Corrado", "Kai Chen", "Jeffrey Dean" ],
      "venue" : "Proceedings of the International Conference on Learning Representations (ICLR",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2013
    }, {
      "title" : "Don’t count , predict ! A systematic comparison of context-counting vs . context-predicting semantic vectors",
      "author" : [ "Marco Baroni", "Georgiana Dinu", "German Kruszewski" ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics.,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2014
    }, {
      "title" : "Neural Word Embedding as Implicit Matrix Factorization",
      "author" : [ "Omer Levy", "Yoav Goldberg" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2014
    }, {
      "title" : "Improving Distributional Similarity with Lessons Learned from Word Embeddings",
      "author" : [ "Omer Levy", "Yoav Goldberg", "Ido Dagan" ],
      "venue" : "Transactions of the Association for Computational Linguistics,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2015
    }, {
      "title" : "Semantically aware time evolution tracking of communities in co-authorship networks",
      "author" : [ "Dionisios N Sotiropoulos", "Demitrios E. Pournarakis", "George M Giaglis" ],
      "venue" : "Proceedings of the 19th Panhellenic Conference on Informatics - PCI",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2015
    }, {
      "title" : "The personality of a child molester: An analysis of dreams",
      "author" : [ "Alan Paul Bell", "Calvin Springer Hall" ],
      "venue" : "Transaction Publishers,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2011
    }, {
      "title" : "Using content analysis to study dreams: applications and implications for the humanities",
      "author" : [ "G.W. Domhoff" ],
      "venue" : "Bulkeley (Ed.), New York: Palgrave.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Studying dream content using the archive and search engine on DreamBank.net",
      "author" : [ "G. William Domhoff", "Adam Schneider" ],
      "venue" : "Consciousness and Cognition,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2008
    }, {
      "title" : "Medication and dreams: Changes in dream content after drug treatment. Dreaming",
      "author" : [ "Nili T Kirschner" ],
      "venue" : null,
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 1999
    }, {
      "title" : "Methods and measures for the study of dream content",
      "author" : [ "G William Domhoff" ],
      "venue" : "Principles and practices of sleep medicine, 3:463–471",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "The typical dreams",
      "author" : [ "Tore A. Nielsen", "Antonio L. Zadra", "Valerie Simard", "Sebastien Saucier", "Philippe Stenstrom", "Carlyle Smith", "Don Kuiken" ],
      "venue" : "Canadian University students. Dreaming,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2003
    }, {
      "title" : "The universality of typical dreams: Japanese vs. Americans",
      "author" : [ "Rm Griffith", "O Miyagi", "A Tago" ],
      "venue" : "American Anthropologist,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 1958
    }, {
      "title" : "Software Framework for Topic Modelling with Large Corpora",
      "author" : [ "Radim Rehuek", "Petr Sojka" ],
      "venue" : "In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2010
    }, {
      "title" : "Extracting semantic representations from word co-occurrence statistics: a computational study",
      "author" : [ "John A Bullinaria", "Joseph P Levy" ],
      "venue" : "Behavior research methods,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2007
    }, {
      "title" : "Extracting semantic representations from word co-occurrence statistics: stop-lists, stemming, and svd",
      "author" : [ "John A Bullinaria", "Joseph P Levy" ],
      "venue" : "Behavior research methods,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2012
    }, {
      "title" : "Extracting semantic representations from large text corpora",
      "author" : [ "Malti Patel", "John A. Bullinaria", "Joseph P Levy" ],
      "venue" : "Proceedings of the 4th Neural Computation and Psychology Workshop,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 1997
    }, {
      "title" : "Silhouettes: A graphical aid to the interpretation and validation of cluster analysis",
      "author" : [ "Peter J. Rousseeuw" ],
      "venue" : "Journal of Computational and Applied Mathematics,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 1987
    }, {
      "title" : "Placing search in context: The concept revisited",
      "author" : [ "Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin" ],
      "venue" : "In Proceedings of the 10th international conference on World Wide Web,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2001
    }, {
      "title" : "and R",
      "author" : [ "S. Zeno", "S. Ivens" ],
      "venue" : "Millard, R.and Duvvuri. The educator’s word frequency guide. Brewster",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "The wacky wide web: a collection of very large linguistically processed web-crawled corpora",
      "author" : [ "Marco Baroni", "Silvia Bernardini", "Adriano Ferraresi", "Eros Zanchetta" ],
      "venue" : "Language resources and evaluation,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2009
    }, {
      "title" : "Dreambank",
      "author" : [ "A. Schneider", "G. William Domhoff" ],
      "venue" : "http://www.dreambank.net/, last accessed: Sep. 12, 2016",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Natural language processing with Python",
      "author" : [ "Steven Bird", "Ewan Klein", "Edward Loper" ],
      "venue" : null,
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2009
    }, {
      "title" : "Automatic estimation of the lsa dimension",
      "author" : [ "Jorge Fernandes", "Andreia Artífice", "Manuel J Fonseca" ],
      "venue" : "In KDIR,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "These methods rely in the idea that words with similar meanings tend to occur in similar contexts [1].",
      "startOffset" : 98,
      "endOffset" : 101
    }, {
      "referenceID" : 1,
      "context" : "Word embeddings has been used in a wide variety of applications such as sentiment analysis [2], psychiatry [3], psychology [4, 5], philology [6], cognitive science [7] and social science [8, 9].",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 2,
      "context" : "Word embeddings has been used in a wide variety of applications such as sentiment analysis [2], psychiatry [3], psychology [4, 5], philology [6], cognitive science [7] and social science [8, 9].",
      "startOffset" : 107,
      "endOffset" : 110
    }, {
      "referenceID" : 3,
      "context" : "Word embeddings has been used in a wide variety of applications such as sentiment analysis [2], psychiatry [3], psychology [4, 5], philology [6], cognitive science [7] and social science [8, 9].",
      "startOffset" : 123,
      "endOffset" : 129
    }, {
      "referenceID" : 4,
      "context" : "Word embeddings has been used in a wide variety of applications such as sentiment analysis [2], psychiatry [3], psychology [4, 5], philology [6], cognitive science [7] and social science [8, 9].",
      "startOffset" : 123,
      "endOffset" : 129
    }, {
      "referenceID" : 5,
      "context" : "Word embeddings has been used in a wide variety of applications such as sentiment analysis [2], psychiatry [3], psychology [4, 5], philology [6], cognitive science [7] and social science [8, 9].",
      "startOffset" : 141,
      "endOffset" : 144
    }, {
      "referenceID" : 6,
      "context" : "Word embeddings has been used in a wide variety of applications such as sentiment analysis [2], psychiatry [3], psychology [4, 5], philology [6], cognitive science [7] and social science [8, 9].",
      "startOffset" : 164,
      "endOffset" : 167
    }, {
      "referenceID" : 7,
      "context" : "Word embeddings has been used in a wide variety of applications such as sentiment analysis [2], psychiatry [3], psychology [4, 5], philology [6], cognitive science [7] and social science [8, 9].",
      "startOffset" : 187,
      "endOffset" : 193
    }, {
      "referenceID" : 8,
      "context" : "Latent Semantic Analysis (LSA) [10, 11, 12], is one of the most used methods for word meaning representation.",
      "startOffset" : 31,
      "endOffset" : 43
    }, {
      "referenceID" : 9,
      "context" : "Latent Semantic Analysis (LSA) [10, 11, 12], is one of the most used methods for word meaning representation.",
      "startOffset" : 31,
      "endOffset" : 43
    }, {
      "referenceID" : 10,
      "context" : "Latent Semantic Analysis (LSA) [10, 11, 12], is one of the most used methods for word meaning representation.",
      "startOffset" : 31,
      "endOffset" : 43
    }, {
      "referenceID" : 11,
      "context" : "Typically, normalization is applied to reduce the weight of uninformative high-frequency words in the words-documents matrix [13].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 12,
      "context" : "More recently, neural-network language embeddings have received an increasing attention [14, 15], leaving aside classical word representation methods such as LSA.",
      "startOffset" : 88,
      "endOffset" : 96
    }, {
      "referenceID" : 13,
      "context" : "More recently, neural-network language embeddings have received an increasing attention [14, 15], leaving aside classical word representation methods such as LSA.",
      "startOffset" : 88,
      "endOffset" : 96
    }, {
      "referenceID" : 13,
      "context" : "In particular, Word2vec models [15, 16] have become especially popular in embeddings generation.",
      "startOffset" : 31,
      "endOffset" : 39
    }, {
      "referenceID" : 14,
      "context" : "In particular, Word2vec models [15, 16] have become especially popular in embeddings generation.",
      "startOffset" : 31,
      "endOffset" : 39
    }, {
      "referenceID" : 14,
      "context" : "In the present paper, we use Skip-gram model, which shows better performance in [16] semantic task.",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 15,
      "context" : "Although prediction-based models have strongly increased in popularity, it is not clear whether they outperform classical counter-based models [17, 18, 19].",
      "startOffset" : 143,
      "endOffset" : 155
    }, {
      "referenceID" : 16,
      "context" : "Although prediction-based models have strongly increased in popularity, it is not clear whether they outperform classical counter-based models [17, 18, 19].",
      "startOffset" : 143,
      "endOffset" : 155
    }, {
      "referenceID" : 17,
      "context" : "Although prediction-based models have strongly increased in popularity, it is not clear whether they outperform classical counter-based models [17, 18, 19].",
      "startOffset" : 143,
      "endOffset" : 155
    }, {
      "referenceID" : 7,
      "context" : "Moreover, this kind of approach is also relevant in sociological and linguistic research, in which linguistic patterns in word meaning networks are tracked along the time line, and small time chunks are needed [9, 20].",
      "startOffset" : 210,
      "endOffset" : 217
    }, {
      "referenceID" : 18,
      "context" : "Moreover, this kind of approach is also relevant in sociological and linguistic research, in which linguistic patterns in word meaning networks are tracked along the time line, and small time chunks are needed [9, 20].",
      "startOffset" : 210,
      "endOffset" : 217
    }, {
      "referenceID" : 19,
      "context" : "Dream content show gender and cultural differences, consistency over time of the dreams content, and concordance of dreaming features (such as activity and emotions) with waking-life experiences [21, 22, 23].",
      "startOffset" : 195,
      "endOffset" : 207
    }, {
      "referenceID" : 20,
      "context" : "Dream content show gender and cultural differences, consistency over time of the dreams content, and concordance of dreaming features (such as activity and emotions) with waking-life experiences [21, 22, 23].",
      "startOffset" : 195,
      "endOffset" : 207
    }, {
      "referenceID" : 21,
      "context" : "Dream content show gender and cultural differences, consistency over time of the dreams content, and concordance of dreaming features (such as activity and emotions) with waking-life experiences [21, 22, 23].",
      "startOffset" : 195,
      "endOffset" : 207
    }, {
      "referenceID" : 22,
      "context" : "Also, there is evidence of change in dreams contents after drug treatment [24] and shifts in content patterns in people with psychiatric disorders [25].",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 23,
      "context" : "Also, there is evidence of change in dreams contents after drug treatment [24] and shifts in content patterns in people with psychiatric disorders [25].",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 21,
      "context" : "Most of the newest dreams content analysis methods are based on frequency wordcounting of predefined categories in dreams reports [23].",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 24,
      "context" : "For example, run may be associated to sports activities or with chase/escape situations, which is reported to be one of the most typical dreams [26, 27].",
      "startOffset" : 144,
      "endOffset" : 152
    }, {
      "referenceID" : 25,
      "context" : "For example, run may be associated to sports activities or with chase/escape situations, which is reported to be one of the most typical dreams [26, 27].",
      "startOffset" : 144,
      "endOffset" : 152
    }, {
      "referenceID" : 26,
      "context" : "1 Semantic representations Both, LSA and Word2vec semantic representations were generated with the Gensim Python library [28].",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 27,
      "context" : "To do this, we take 6 nested sub-samples of the training corpora, in which documents where progressively eliminated, following [29, 30].",
      "startOffset" : 127,
      "endOffset" : 135
    }, {
      "referenceID" : 28,
      "context" : "To do this, we take 6 nested sub-samples of the training corpora, in which documents where progressively eliminated, following [29, 30].",
      "startOffset" : 127,
      "endOffset" : 135
    }, {
      "referenceID" : 29,
      "context" : "In this test we measured the capabilities of the model to represent the semantic categories [31, 29] (such as, drinks, countries, tools and clothes).",
      "startOffset" : 92,
      "endOffset" : 100
    }, {
      "referenceID" : 27,
      "context" : "In this test we measured the capabilities of the model to represent the semantic categories [31, 29] (such as, drinks, countries, tools and clothes).",
      "startOffset" : 92,
      "endOffset" : 100
    }, {
      "referenceID" : 30,
      "context" : "In order to measure how well the word i is grouped visà-vis the other words in its semantic category we used the Silhouette Coefficients, s(i) [32],",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 31,
      "context" : "We used the well established WordSim353 test collection [33], which consist of 353 word-pairs (such as Maradona-football or physics-chemistry) associated with a mean human-assigned similarity score.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 32,
      "context" : "4 Corpora In both test, we use as training corpora the TASA corpus [34] and a random subsample of ukWaC corpus [35].",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 33,
      "context" : "4 Corpora In both test, we use as training corpora the TASA corpus [34] and a random subsample of ukWaC corpus [35].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 34,
      "context" : "For the case study we use the Dreambank reports corpus [36, 23].",
      "startOffset" : 55,
      "endOffset" : 63
    }, {
      "referenceID" : 21,
      "context" : "For the case study we use the Dreambank reports corpus [36, 23].",
      "startOffset" : 55,
      "endOffset" : 63
    }, {
      "referenceID" : 35,
      "context" : "Then, we transformed each word to lowercase and eliminated stopwords, using the stoplist in NLTK Python package [37].",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 36,
      "context" : "Given that the appropriate embeddings dimensions depends on the corpus size [38], for each sub-corpus, we ran the models with a wide range of dimension values",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 15,
      "context" : "This finding gives a new insight into the prediction-based vs counter-based models discussion [17, 18, 19].",
      "startOffset" : 94,
      "endOffset" : 106
    }, {
      "referenceID" : 16,
      "context" : "This finding gives a new insight into the prediction-based vs counter-based models discussion [17, 18, 19].",
      "startOffset" : 94,
      "endOffset" : 106
    }, {
      "referenceID" : 17,
      "context" : "This finding gives a new insight into the prediction-based vs counter-based models discussion [17, 18, 19].",
      "startOffset" : 94,
      "endOffset" : 106
    }, {
      "referenceID" : 19,
      "context" : "This research field addresses questions such as “what do we dream about?” and “how do gender, cultural background and waking life experiences shape the dreams content?” [21, 25, 22, 23].",
      "startOffset" : 169,
      "endOffset" : 185
    }, {
      "referenceID" : 23,
      "context" : "This research field addresses questions such as “what do we dream about?” and “how do gender, cultural background and waking life experiences shape the dreams content?” [21, 25, 22, 23].",
      "startOffset" : 169,
      "endOffset" : 185
    }, {
      "referenceID" : 20,
      "context" : "This research field addresses questions such as “what do we dream about?” and “how do gender, cultural background and waking life experiences shape the dreams content?” [21, 25, 22, 23].",
      "startOffset" : 169,
      "endOffset" : 185
    }, {
      "referenceID" : 21,
      "context" : "This research field addresses questions such as “what do we dream about?” and “how do gender, cultural background and waking life experiences shape the dreams content?” [21, 25, 22, 23].",
      "startOffset" : 169,
      "endOffset" : 185
    }, {
      "referenceID" : 32,
      "context" : "We want to thank the teams behind the TASA [34], WaCky [35] and Dreambank [23] projects for providing us the corpora and Eduardo Schmidt for helpful discussions.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 33,
      "context" : "We want to thank the teams behind the TASA [34], WaCky [35] and Dreambank [23] projects for providing us the corpora and Eduardo Schmidt for helpful discussions.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 21,
      "context" : "We want to thank the teams behind the TASA [34], WaCky [35] and Dreambank [23] projects for providing us the corpora and Eduardo Schmidt for helpful discussions.",
      "startOffset" : 74,
      "endOffset" : 78
    } ],
    "year" : 2016,
    "abstractText" : "Word embeddings have been extensively studied in large text datasets. However, only a few studies analyze semantic representations of small corpora, particularly relevant in single-person text production studies. In the present paper, we compare Skip-gram and LSA capabilities in this scenario, and we test both techniques to extract relevant semantic patterns in single-series dreams reports. LSA showed better performance than Skip-gram in small size training corpus in two semantic tests. As a study case, we show that LSA can capture relevant words associations in dream reports series, even in cases of small number of dreams or low-frequency words. We propose that LSA can be used to explore words associations in dreams reports, which could bring new insight into this classic research area of psychology",
    "creator" : "LaTeX with hyperref package"
  }
}