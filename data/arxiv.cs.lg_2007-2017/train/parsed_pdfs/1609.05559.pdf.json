{
  "name" : "1609.05559.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Opponent Modeling in Deep Reinforcement Learning",
    "authors" : [ "He He", "Kevin Kwok" ],
    "emails" : [ "HHE@UMIACS.UMD.EDU", "JORDAN.BOYD.GRABER@COLORADO.EDU", "KKWOK@MIT.EDU", "HAL@UMIACS.UMD.EDU" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "An intelligent agent working in strategic settings (e.g., collaborative or competitive tasks) must predict the action of other agents and reason about their intentions. This is important because all active agents affect the state of the world. For example, a multi-player game AI can exploit suboptimal players if it can predict their bad moves; a negotiating\nProceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s).\nagent can reach an agreement faster if it knows the other party’s bottom line; a self-driving car must avoid accidents by predicting where cars and pedestrians are going. Two critical questions in opponent modeling are what variable(s) to model and how to use the predicted information. However, the answers depend much on the specific application, and most previous work (Billings et al., 1998a; Southey et al., 2005; Ganzfried & Sandholm, 2011) focuses exclusively on poker games which require substantial domain knowledge.\nWe aim to build a general opponent modeling framework in the reinforcement learning setting, which enables the agent to exploit idiosyncrasies of various opponents. First, to account for changing behavior, we model uncertainty in the opponent’s strategy instead of classifying it into a set of stereotypes. Second, domain knowledge is often required when prediction of the opponents are separated from learning the dynamics of the world. Therefore, we jointly learn a policy and model the opponent probabilistically.\nWe develop a new model, DRON (Deep Reinforcement Opponent Network), based on the recent deep Q-Network of Mnih et al. (2015, DQN) in Section 3. DRON has a policy learning module that predicts Q-values and an opponent learning module that infers opponent strategy.1 Instead of explicitly predicting opponent properties, DRON learns hidden representation of the opponents based on past observations and uses it (in addition to the state information) to compute an adaptive response. More specifically, we propose two architectures, one using simple concatenation to combine the two modules and one based on the Mixtureof-Experts network. While we model opponents implicitly, additional supervision (e.g., the action or strategy taken) can\n1Code and data: https://github.com/hhexiy/ opponent\nar X\niv :1\n60 9.\n05 55\n9v 1\n[ cs\n.L G\n] 1\n8 Se\nbe added through multitasking.\nCompared to previous models that are specialized in particular applications, DRON is designed with a general purpose and does not require knowledge of possible (parameterized) game strategies.\nA second contribution is DQN agents that learn in multiagent settings. Deep reinforcement learning has shown competitive performance in various tasks: arcade games (Mnih et al., 2015), object recognition (Mnih et al., 2014), and robot navigation (Zhang et al., 2015). However, it has been mostly applied to the single-agent decision-theoretic settings with stationary environments. One exception is Tampuu et al. (2015), where two agents controlled by independent DQNs interact under collaborative and competitive rewards. While their focus is the collective behavior of a multi-agent system with known controllers, we study from the view point of a single agent that must learn a reactive policy in a stochastic environment filled with unknown opponents.\nWe evaluate our method on two tasks in Section 4: a simulated two-player soccer game in a grid world, and a real question-answering game, quiz bowl, against users playing online. Both games have opponents with a mixture of strategies that require different counter-strategies. Our model consistently achieves better results than the DQN baseline. In addition, we show our method is more robust to non-stationary strategies; it successfully identifies the opponent’s strategy and responds correspondingly."
    }, {
      "heading" : "2. Deep Q-Learning",
      "text" : "Reinforcement learning is commonly used for solving Markov-decision processes (MDP), where an agent interacts with the world and collects rewards. Formally, the agent takes an action a in state s, goes to the next state s′ according to the transition probability T (s, a, s′) = Pr(s′|s, a) and receives reward r. States and actions are defined by the state space S and the action space A. Rewards r are assigned by a real-valued reward functionR(s, a, s′). The agent’s behavior is defined by a policy π such that π(a|s) is the probability of taking action a in state s. The goal of reinforcement learning is to find an optimal policy π∗ that maximizes the expected discounted cumulative reward R = E [∑T t=0 γ trt ] , where γ ∈ [0, 1] is the discount factor and T is the time step when the episode ends.\nOne approach to solve MDPs is to compute its Q-function: the expected reward starting from state s, taking action a and following policy π: Qπ(s, a) ≡ E [∑t γtrt|s0 = s, a0 = a, π]. Q-values of an optimal policy solve the Bellman Equation (Sutton & Barto,\n1998):\nQ∗(s, a) = ∑\ns′\nT (s, a, s′) [ r + γmax\na′ Q∗(s′, a′)\n] .\nOptimal policies always select the action with the highest Qvalue for a given state. Q-learning (Watkins & Dayan, 1992; Sutton & Barto, 1998) finds the optimal Q-values without knowledge of T . Given observed transitions (s, a, s′, r), Q-values are updated recursively: Q(s, a)← Q(s, a) + α [ r + γmax\na′ Q(s′, a′)−Q(s, a)\n] .\nFor complex problems with continuous states, the Qfunction cannot be expressed as a lookup table, requiring a continuous approximation. Deep reinforcement learning such as DQN (Mnih et al., 2015)—a deep Q-learning method with experience replay—approximates the Q-function using a neural network. It draws samples (s, a, s′, r) from a replay memory M , and the neural network predicts Q∗ by minimizing squared loss at iteration i:\nLi(θi) = E(s,a,s′,r)∼U(M) [( r + γmax\na′ Q(s′, a′; θi−1)\n−Q(s, a; θi) )2] ,\nwhere U(M) is a uniform distribution over replay memory."
    }, {
      "heading" : "3. Deep Reinforcement Opponent Network",
      "text" : "In a multi-agent setting, the environment is affected by the joint action of all agents. From the perspective of one agent, the outcome of an action in a given state is no longer stable, but is dependent on actions of other agents. In this section, we first analyze the effect of multiple agents on the Q-learning framework; then we present DRON and its multitasking variation."
    }, {
      "heading" : "3.1. Q-Learning with Opponents",
      "text" : "In MDP terms, the joint action space is defined by AM = A1 × A2 × . . . × An where n is the total number of agents. We use a to denote the action of the agent we control (the primary agent) and o to denote the joint action of all other agents (secondary agents), such that (a, o) ∈ AM . Similarly, the transition probability becomes T M (s, a, o, s′) = Pr(s′|s, a, o), and the new reward function isRM (s, a, o, s′). Our goal is to learn an optimal policy for the primary agent given interactions with the joint policy πo of the secondary agents.2\nIf πo is stationary, then the multi-agent MDP reduces to a single-agent MDP: the opponents can be considered part of the world. Thus, they redefine the transitions and reward:\nT (s, a, s′) = ∑\no\nπo(o|s)T M (s, a, o, s′),\nR(s, a, s′) = ∑\no\nπo(o|s)RM (s, a, o, s′).\nTherefore, an agent can ignore other agents, and standard Q-learning suffices.\nNevertheless, it is often unrealistic to assume opponents use fixed policies. Other agents may also be learning or adapting to maximize rewards. For example, in strategy games, players may disguise their true strategies at the beginning to fool the opponents; winning players protect their lead by playing defensively; and losing players play more aggressively. In these situations, we face opponents with an unknown policy πot that changes over time.\nConsidering the effects of other agents, the definition of an optimal policy in Section 2 no longer applies—the effectiveness policies now depends on policies of secondary agents. We therefore define the optimal Q-function relative to the\n2While a joint policy defines the distribution of joint actions, the opponents may be controlled by independent policies.\njoint policy of opponents: Q∗|π o = maxπ Q π|πo(s, a) ∀s ∈ S and ∀a ∈ A. The recurrent relation between Q-values holds:\nQπ|π o (st, at) = ∑\not\nπot (ot|st) ∑\nst+1 T (st, at, ot, st+1) [ R(st, at, ot, st+1) + γEat+1 [ Qπ|π o (st+1, at+1) ]] . (1)"
    }, {
      "heading" : "3.2. DQN with Opponent Modeling",
      "text" : "Given Equation 1, we can continue applying Q-learning and estimate both the transition function and the opponents’ policy by stochastic updates. However, treating opponents as part of the world can slow responses to adaptive opponents (Uther & Veloso, 2003), because the change in behavior is masked by the dynamics of the world.\nTo encode opponent behavior explicitly, we propose the Deep Reinforcement Opponent Network (DRON) that models Q·|π o\nand πo jointly. DRON is a Q-Network (NQ) that evaluates actions for a state and an opponent network (No) that learns representation of πo. The remaining questions are how to combine the two networks and what supervision signal to use. To answer the first question, we investigate two network architectures: DRON-concat that concatenates NQ and No, and DRON-MOE that applies a Mixture-ofExperts model.\nTo answer the second question, we consider two settings: (a) predicting Q-values only, as our goal is the best reward instead of accurately simulating opponents; and (b) also predicting extra information about the opponent when it is available, e.g., the type of their strategy.\nDRON-concat We extract features from the state (φs) and the opponent (φo ) and then use linear layers with rectification or convolutional neural networks—NQ and No—to embed them in separate hidden spaces (hs and ho). To incorporate knowledge of πo into the Q-Network, we concatenate representations of the state and the opponent (Figure 1a). The concatenation then jointly predicts the Q-value. Therefore, the last layer(s) of the neural network is responsible for understanding the interaction between opponents and Q-values. Since there is only one Q-Network, the model requires a more discriminative representation of the opponents to learn an adaptive policy. To alleviate this, our second model encodes a stronger prior of the relation between opponents’ actions and Q-values based on Equation 1.\nDRON-MOE The right part of Equation 1 can be written as ∑ ot πot (ot|st)Qπ(st, at, ot), an expectation over different opponent behavior. We use a Mixture-of-Experts network (Jacobs et al., 1991) to explicitly model the opponent action as a hidden variable and marginalize over it (Figure 1b). The expected Q-value is obtained by combining\npredictions from multiple expert networks:\nQ(st, at; θ) =\nK∑\ni=1\nwiQi(h s, at)\nQi(h s, ·) = f(W si hs + bsi ).\nEach expert network predicts a possible reward in the current state. A gating network based on the opponent representation computes combination weights (distribution over experts):\nw = softmax (f(W oho + bo)) .\nHere f(·) is a nonlinear activation function (ReLU for all experiments),W represents the linear transformation matrix, and b is the bias term.\nUnlike DRON-concat, which ignores the interaction between the world and opponent behavior, DRON-MOE knows that Q-values have different distributions depending on φo; each expert network captures one type of opponent strategy.\nMultitasking with DRON The previous two models predict Q-values only, thus the opponent representation is learned indirectly through feedback from the Q-value. Extra information about the opponent can provide direct supervision for No. Many games reveal additional information besides the final reward at the end of a game. At the very least the agent has observed actions taken by the opponents in past states; sometimes their private information such as the hidden cards in poker. More high-level information includes abstracted plans or strategies. Such information reflects characteristics of opponents and can aid policy learning.\nUnlike previous work that learns a separate model to predict these information about the opponent (Davidson, 1999; Ganzfried & Sandholm, 2011; Schadd et al., 2007), we apply multitask learning and use the observation as extra supervision to learn a shared opponent representation ho. Figure 2 shows the architecture of multitask DRON, where supervision is yo. The advantage of multitasking over explicit opponent modeling is that it uses high-level knowledge of the game and the opponent, while remaining robust to insufficient opponent data and modeling error from Q-values. In Section 4, we evaluate multitasking DRON with two types of supervision signals: future action and overall strategy of the opponent."
    }, {
      "heading" : "4. Experiments",
      "text" : "In this section, we evaluate our models on two tasks, the soccer game and quiz bowl. Both tasks have two players against each other and the opponent presents varying behavior. We compare DRON models with DQN and analyze their response against different types of opponents.\nAll systems are trained under the same Q-learning framework. Unless stated otherwise, the experiments have the following configuration: discount factor γ is 0.9, parameters are optimized by AdaGrad (Duchi et al., 2011) with a learning rate of 0.0005, and the mini-batch size is 64. We use -greedy exploration during training, starting with an exploration rate of 0.3 that linearly decays to 0.1 within 500,000 steps. We train all models for fifty epochs. Cross Entropy is used as the loss in multitasking learning."
    }, {
      "heading" : "4.1. Soccer",
      "text" : "Our first testbed is a soccer variant following previous work on multi-player games (Littman, 1994; Collins, 2007; Uther & Veloso, 2003). The game is played on a 6 × 9 grid (Figure 3) by two players, A and B.3 The game starts with A and B in a randomly squares in the left and right half (except the goals), and the ball goes to one of them. Players choose from five actions: move N, S, W, E or stand still (Figure 3(1)). An action is invalid if it takes the player to a shaded square or outside of the border. If two players move to the same square, the player who possesses the ball before the move loses it to the opponent (Figure 3(2)), and the move does not take place.\nA player scores one point if they take the ball to the opponent’s goal (Figure 3(3), (4)) and the game ends. If neither player gets a goal within one hundred steps, the game ends with a zero–zero tie.\nImplementation We design a two-mode rule-based agent as the opponent Figure 3(right). In the offensive mode, the agent always prioritize attacking over defending. In 5000 games against a random agent, it wins 99.86% of the time and the average episode length is 10.46. In defensive mode, the agent only focuses on defending its own goal. As a result, it wins 31.80% of the games and ties 58.40% of them; the average episode length is 81.70. It is easy to find a strategy to defeat the opponent in either mode, however, the strategy does not work well for both modes, as we will show in Table 2. Therefore, the agent randomly chooses between\n3Although the game is played in a grid world, we do not represent the Q-function in tabular form as in previous work. Therefore it can be generalized to more complex pixel-based settings.\nthe two modes in each game to create a varying strategy.\nThe input state is a 1×15 vector representing coordinates of the agent, the opponent, the axis limits of the field, positions of the goal areas and ball possession. We define a player’s move by five cases: approaching the agent, avoiding the agent, approaching the agent’s goal, approaching self goal and standing still. Opponent features include frequencies of observed opponent moves, its most recent move and action, and the frequency of losing the ball to the opponent.\nThe baseline DQN has two hidden layers, both with 50 hidden units. We call this model DQN-world: opponents are modeled as part of the world. The hidden layer of the opponent network in DRON also has 50 hidden units. For multitasking, we experiment with two supervision signals, opponent action in the current state (+action) and the opponent mode (+type).\nResults In Table 1, we compare rewards of DRON models, their multitasking variations, and DQN-world. After each epoch, we evaluate the policy with 5000 randomly generated games (the test set) and compute the average reward. We report the mean test reward after the model stabilizes and the maximum test reward ever achieved. The DRON models outperform the DQN baseline. Our model also has much smaller variance (Figure 4).\nAdding additional supervision signals improves DRONconcat but not DRON-MOE (multitask column). DRONconcat does not explicitly learn different strategies for different types of opponents, therefore more discriminative opponent representation helps model the relation between opponent behavior and Q-values. However, for DRON-MOE, while better opponent representation is still desirable, the supervision signal may not be aligned with “classification”\nof the opponents learned from the Q-values.\nTo investigate how the learned policies adapt to different opponents, we test the agents against a defensive opponent and an offensive opponent separately. Furthermore, we train two DQN agents targeting at each type of opponent respectively. Their performance is best an agent can do when facing a single type of opponent (in our setting), as the strategies are learned to defeat this particular opponent. Table 2 shows the average rewards of each model and the DQN upper bounds (in bold). DQN-world is confused by the defensive behavior and significantly sacrifices its performance against the offensive opponent; DRON achieves a much better trade-off, retaining rewards close to both upper bounds against the varying opponent.\nFinally, we examine how the number of experts in DRONMOE affects the result. From Figure 5, we see no significant difference in varying the number of experts, and DRONMOE consistently performs better than DQN across all K. Multitasking does not help here."
    }, {
      "heading" : "4.2. Quiz Bowl",
      "text" : "Quiz bowl is a trivia game widely played in Englishspeaking countries between schools, with tournaments held most weekends. It is usually played between two teams. The questions are read to players and they score points by “buzzing in” first (often before the question is finished) and answering the question correctly. One example question\nwith buzzes is shown in Figure 7. A successful quiz bowl player needs two things: a content model to predict answers given (partial) questions and a buzzing model to decide when to buzz.\nContent Model We model the question answering part as an incremental text-classification problem. Our content model is a recurrent neural network with gated recurrent units (GRU). It reads in the question sequentially and outputs a distribution over answers at each word given past information encoded in the hidden states.\nBuzzing Model To test depth of knowledge, questions start with obscure information and reveals more and more obvious clues towards the end (e.g., Figure 7). Therefore, the buzzing model faces a speed-accuracy tradeoff: while buzzing later increases one’s chance of answering correctly, it also increases the risk of losing the chance to answer. A safe strategy is to always buzz as soon as the content model is confident enough. A smarter strategy, however, is to adapt to different opponents: if the opponent often buzzes late, wait for more clues; otherwise, buzz more aggressively.\nTo model interaction with other players, we take a reinforcement learning approach to learn a buzzing policy. The state includes words revealed and predictions from the content model, and the actions are buzz and wait. Upon buzzing, the content model outputs the most likely answer at the current position. An episode terminates when one player buzzes and answers the question correctly. Correct answers are worth 10 points and wrong answers are −5.\nImplementation We collect question/answer pairs and log user buzzes from Protobowl, an online multi-player quizbowl application.4 Additionally, we include data from Boyd-Graber et al. (2012). Most buzzes are from strong tournament players. After removing answers with fewer\n4http://protobowl.com\nthan five questions and users who played fewer than twenty questions, we end up with 1045 answers, 37.7k questions and 3610 users. We divide all questions into two nonoverlapping sets: one for training the content model and one for training the buzzing policy. The two sets are further divided into train/dev and train/dev/test sets randomly. There are clearly two clusters of players (Figure 6(a)): aggressive players who buzz early with varying accuracies and cautious players who buzz late but maintain higher accuracy. Our GRU content model (Figure 6(b)) is more accurate with more input words—a behavior similar to human players.\nOur input state must represent information from the content model and the opponents. Information from the content model takes the form of a belief vector: a vector (1× 1045) with the current estimate (as a log probability) of each possible guess being the correct answer given our input so far. We concatinate the belief vector from the previous time step to capture sudden shifts in certainty, which are often good opportunities to buzz. In addition, we include the number of words seen and whether a wrong buzz has happened.\nThe opponent features include the number of questions the opponent has answered, the average buzz position, and the error rate. The basic DQN has two hidden layers, both with 128 hidden units. The hidden layer for the opponent has ten hidden units. Similar to soccer, we experiment with two settings for multitasking: (a) predicting how opponent buzzes; (b) predicting the opponent type. We approximate the ground truth for (a) by min(1, t/buzz position) and use the mean square error as the loss function. The ground truth for (b) is based on dividing players into four groups according to their buzz positions—the percentage of question revealed.\nResults In addition to DQN-world, we also compare with DQN-self, a baseline without interaction with opponents at all. DQN-self is ignorant of the opponents and plays the safe\nstrategy: answer as soon as the content model is confident. During training, when the answer prediction is correct, it receives reward 10 for buzz and -10 for wait. When the answer prediction is incorrect, it receives reward -15 for buzz and 15 for wait. Since all rewards are immediate, we set γ to 0 for DQN-self.5 With data of the opponents’ responses, DRON and DQN-world use the game payoff (from the perspective of the computer) as the reward.\nFirst we compare the average rewards on test set of our models—DRON-concat and DRON-MOE (with 3 experts)— and the baseline models: DQN-self and DQN-world. From the first column in Table 3, our models achieve statistically significant improvements over the DQN baselines and DRONMOE outperforms DRON-concat. In addition, the DRON models have much less variance compared to DQN-world as the learning curves show in Figure 9.\nTo investigate strategies learned by these models, we show their performance against different types of players (as de-\n5This is equivalent to cost-sensitive classification.\nfined at the end of “Implementation”) in Table 3, right column. We compare three measures of performance, the average reward (R), percentage of early and incorrect buzzes (rush), and percentage of missing the chance to buzz correctly before the opponent (miss).\nAll models beat Type 2 players, mainly because they are the majority in our dataset. As expected, DQN-self learns a safe strategy that tends to buzz early. It performs the best against Type 1 players who answer early. However, it has very high rush rate against cautious players, resulting in much lower rewards against Type 3 and Type 4 players. Without opponent modeling, DQN-world is biased towards the majority player, thus having the same problem as DQNself when playing against players who buzz late. Both DRON models exploit cautious players while holding their own against aggressive players. Furthermore, DRON-MOE matches DQN-self against Type 1 players, thus it discovers different buzzing strategies.\nFigure 7 shows an example question with buzz positions labeled. The DRON agents demonstrate dynamic behavior against different players; DRON-MOE almost always buzzes right before the opponent in this example. In addition, when the player buzzes wrong and the game continues, DRONMOE learns to wait longer since the opponent is gone, while the other agents are still in a rush.\nAs with the Soccer task, adding extra supervision does not yield better results over DRON-MOE (Table 3) but significantly improves DRON-concat.\nFigure 8 varies the number of experts in DRON-MOE (K) from two to four. Using a mixture model for the opponents consistently improves over the DQN baseline, and using three experts gives better performance on this task. For multitasking, adding the action supervision does not help at all. However, the more high-level type supervision yields competent results, especially with four experts, mostly because the number of experts matches the number of types."
    }, {
      "heading" : "5. Related Work and Discussion",
      "text" : "Implicit vs. Explicit opponent modeling Opponent modeling has been studied extensively in games. Most existing approaches fall into the category of explicit modeling, where a model (e.g., decision trees, neural networks, Bayesian models) is built to directly predict parameters of the opponent, e.g., actions (Uther & Veloso, 2003; Ganzfried & Sandholm, 2011), private information (Billings et al., 1998b; Richards & Amir, 2007), or domain-specific strategies (Schadd et al., 2007; Southey et al., 2005). One difficulty is that the model may need a prohibitive number of examples before producing anything useful. Another is that as the opponent behavior is modeled separately from the world, it is not always clear how to incorporate these predictions robustly into policy learning. The results on multitasking DRON also suggest that improvement from explicit modeling is limited. However, it is better suited to games of incomplete information, where it is clear what information needs to be predicted to achieve higher reward.\nOur work is closely related to implicit opponent modeling. Since the agent aims to maximize its own expected reward without having to identify the opponent’s strategy, this approach does not have the difficulty of incorporating prediction of the opponent’s parameters. Rubin & Watson\n(2011) and Bard et al. (2013) construct a a portfolio of strategies offline based on domain knowledge or past experience for heads-up limit Texas hold’em; they then select strategies online using multi-arm bandit algorithms. Our approach does not have a clear online/offline distinction. We learn strategies and their selector in a joint, probabilistic way. However, the offline construction can be mimicked in our models by initializing expert networks with DQN pre-trained against different opponents.\nNeural network opponent models Davidson (1999) applies neural networks to opponent modeling, where a simple multi-layer perceptron is trained as a classifier to predict opponent actions given game logs. Lockett et al. (2007) propose an architecture similar to DRON-concat that aims to identify the type of an opponent. However, instead of learning a hidden representation, they learn a mixture weights over a pre-specified set of cardinal opponents; and they use the neural network as a standalone solver without the reinforcement learning setting, which may not be suitable for more complex problems. Foerster et al. (2016) use modern neural networks to learn a group of parameter-sharing agents that solve a coordination task, where each agent is controlled by a deep recurrent Q-Network (Hausknecht & Stone, 2015). Our setting is different in that we control only one agent and the policy space of other agents is unknown. Opponent modeling with neural networks remains understudied with ample room for improvement."
    }, {
      "heading" : "6. Conclusion and Future Work",
      "text" : "Our general opponent modeling approach in the reinforcement learning setting incorporates (implicit) prediction of opponents’ behavior into policy learning without domain knowledge. We use recent deep Q-learning advances to learn a representation of opponents that better maximizes available rewards. The proposed network architectures are novel models that capture the interaction between opponent behavior and Q-values. Our model is also flexible enough to include supervision for parameters of the opponents, much as in explicit modeling.\nThese gains can further benefit from advances in deep learning. For example, Eigen et al. (2014) extends the Mixtureof-Experts network to a stacked model—deep Mixture-ofExperts—which can be combined with hierarchical reinforcement learning to learn a hierarchy of opponent strategies in large, complex domains such as online strategy games. In addition, instead of hand-crafting opponent features, we can feed in raw opponent actions and use a recurrent neural network to learn the opponent representation. Another important direction is to design online algorithms that can adapt to fast changing behavior and balance exploitation and exploration of opponents."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We thank Hua He, Xiujun Li, and Mohit Iyyer for helpful discussions about deep Q-learning and our model. We also thank the anonymous reviewers for their insightful comments. This work was supported by NSF grant IIS-1320538. Boyd-Graber is also partially supported by NSF grants CCF1409287 and NCSE-1422492. Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the view of the sponsor."
    } ],
    "references" : [ {
      "title" : "Online implicit agent modelling",
      "author" : [ "Bard", "Nolan", "Johanson", "Michael", "Burch", "Neil", "Bowling" ],
      "venue" : "In Proceedings of International Conference on Autonomous Agents and Multiagent Systems,",
      "citeRegEx" : "Bard et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bard et al\\.",
      "year" : 2013
    }, {
      "title" : "Opponent modeling in poker",
      "author" : [ "Billings", "Darse", "Papp", "Denis", "Schaeffer", "Jonathan", "Szafron", "Duane" ],
      "venue" : "In Association for the Advancement of Artificial Intelligence,",
      "citeRegEx" : "Billings et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Billings et al\\.",
      "year" : 1998
    }, {
      "title" : "Opponent modeling in poker",
      "author" : [ "Billings", "Darse", "Papp", "Denis", "Schaeffer", "Jonathan", "Szafron", "Duane" ],
      "venue" : "In Association for the Advancement of Artificial Intelligence,",
      "citeRegEx" : "Billings et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Billings et al\\.",
      "year" : 1998
    }, {
      "title" : "Besting the quiz master: Crowdsourcing incremental classification games",
      "author" : [ "Boyd-Graber", "Jordan", "Satinoff", "Brianna", "He", "Daumé III", "Hal" ],
      "venue" : "In Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Boyd.Graber et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Boyd.Graber et al\\.",
      "year" : 2012
    }, {
      "title" : "Combining opponent modeling and modelbased reinforcement learning in a two-player competitive game",
      "author" : [ "Collins", "Brian" ],
      "venue" : "Master’s thesis, School of Informatics, University of Edinburgh,",
      "citeRegEx" : "Collins and Brian.,? \\Q2007\\E",
      "shortCiteRegEx" : "Collins and Brian.",
      "year" : 2007
    }, {
      "title" : "Using artifical neural networks to model opponents in texas hold’em",
      "author" : [ "Davidson", "Aaron" ],
      "venue" : "CMPUT 499 - Research Project Review,",
      "citeRegEx" : "Davidson and Aaron.,? \\Q1999\\E",
      "shortCiteRegEx" : "Davidson and Aaron.",
      "year" : 1999
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "Duchi", "John", "Hazan", "Elad", "Singer", "Yoram" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Duchi et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2011
    }, {
      "title" : "Learning factored representations in a deep mixture of experts",
      "author" : [ "Eigen", "David", "Ranzato", "Marc’Aurelio", "Sutskever", "Ilya" ],
      "venue" : "In ICLR Workshop,",
      "citeRegEx" : "Eigen et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Eigen et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning to communicate to solve riddles with deep distributed recurrent q-networks",
      "author" : [ "Foerster", "Jakob N", "Assael", "Yannis M", "de Freitas", "Nando", "Whiteson", "Shimon" ],
      "venue" : null,
      "citeRegEx" : "Foerster et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Foerster et al\\.",
      "year" : 2016
    }, {
      "title" : "Game theorybased opponent modeling in large imperfect-information games",
      "author" : [ "Ganzfried", "Sam", "Sandholm", "Tuomas" ],
      "venue" : "In Proceedings of International Conference on Autonomous Agents and Multiagent Systems,",
      "citeRegEx" : "Ganzfried et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Ganzfried et al\\.",
      "year" : 2011
    }, {
      "title" : "Deep recurrent q-learning for partially observable MDPs",
      "author" : [ "Hausknecht", "Matthew", "Stone", "Peter" ],
      "venue" : null,
      "citeRegEx" : "Hausknecht et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hausknecht et al\\.",
      "year" : 2015
    }, {
      "title" : "Adaptive mixtures of local experts",
      "author" : [ "Jacobs", "Robert A", "Jordan", "Michael I", "Nowlan", "Steven J", "Hinton", "Geoffrey E" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Jacobs et al\\.,? \\Q1991\\E",
      "shortCiteRegEx" : "Jacobs et al\\.",
      "year" : 1991
    }, {
      "title" : "Markov games as a framework for multi-agent reinforcement learning",
      "author" : [ "Littman", "Michael L" ],
      "venue" : "In Proceedings of the International Conference of Machine Learning,",
      "citeRegEx" : "Littman and L.,? \\Q1994\\E",
      "shortCiteRegEx" : "Littman and L.",
      "year" : 1994
    }, {
      "title" : "Evolving explicit opponent models in game playing",
      "author" : [ "Lockett", "Alan J", "Chen", "Charles L", "Miikkulainen", "Risto" ],
      "venue" : "In Proceeedings of the Genetic and Evolutionary Computation Conference,",
      "citeRegEx" : "Lockett et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Lockett et al\\.",
      "year" : 2007
    }, {
      "title" : "Recurrent models of visual attention",
      "author" : [ "Mnih", "Volodymyr", "Heess", "Nicolas", "Graves", "Alex", "Kavukcuoglu", "Koray" ],
      "venue" : "In Proceedings of Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Mnih et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2014
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "shan", "Wierstra", "Daan", "Legg", "Shane", "Hassabis", "Demis" ],
      "venue" : "Nature, 518(7540):529–533,",
      "citeRegEx" : "shan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "shan et al\\.",
      "year" : 2015
    }, {
      "title" : "Opponent modeling in Scrabble",
      "author" : [ "Richards", "Mark", "Amir", "Eyal" ],
      "venue" : "In International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "Richards et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Richards et al\\.",
      "year" : 2007
    }, {
      "title" : "On combining decisions from multiple expert imitators for performance",
      "author" : [ "Rubin", "Jonathan", "Watson", "Ian" ],
      "venue" : "In International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "Rubin et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Rubin et al\\.",
      "year" : 2011
    }, {
      "title" : "Opponent modeling in real-time strategy games",
      "author" : [ "Schadd", "Frederik", "Bakkes", "Er", "Spronck", "Pieter" ],
      "venue" : "In Proceedings of the GAME-ON",
      "citeRegEx" : "Schadd et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Schadd et al\\.",
      "year" : 2007
    }, {
      "title" : "Bayes’ bluff: Opponent modelling in poker",
      "author" : [ "Southey", "Finnegan", "Bowling", "Michael", "Larson", "Bryce", "Piccione", "Carmelo", "Burch", "Neil", "Billings", "Darse", "Rayner", "Chris" ],
      "venue" : "In Proceedings of Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Southey et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Southey et al\\.",
      "year" : 2005
    }, {
      "title" : "Reinforcement learning: An introduction, volume 1",
      "author" : [ "Sutton", "Richard S", "Barto", "Andrew G" ],
      "venue" : null,
      "citeRegEx" : "Sutton et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 1998
    }, {
      "title" : "Multiagent cooperation and competition with deep reinforcement learning",
      "author" : [ "Tampuu", "Ardi", "Matiisen", "Tambet", "Kodelja", "Dorian", "Kuzovkin", "Ilya", "Korjus", "Kristjan", "Aru", "Juhan", "Jaan", "Vicente", "Raul" ],
      "venue" : null,
      "citeRegEx" : "Tampuu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Tampuu et al\\.",
      "year" : 2015
    }, {
      "title" : "Adversarial reinforcement learning",
      "author" : [ "Uther", "William", "Veloso", "Manuela" ],
      "venue" : "Technical Report CMU-CS-03-107,",
      "citeRegEx" : "Uther et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Uther et al\\.",
      "year" : 2003
    }, {
      "title" : "Learning deep neural network policies with continuous memory states",
      "author" : [ "Zhang", "Marvin", "McCarthy", "Zoe", "Finn", "Chelsea", "Levine", "Sergey", "Abbeel", "Pieter" ],
      "venue" : null,
      "citeRegEx" : "Zhang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 19,
      "context" : "However, the answers depend much on the specific application, and most previous work (Billings et al., 1998a; Southey et al., 2005; Ganzfried & Sandholm, 2011) focuses exclusively on poker games which require substantial domain knowledge.",
      "startOffset" : 85,
      "endOffset" : 159
    }, {
      "referenceID" : 14,
      "context" : ", 2015), object recognition (Mnih et al., 2014), and robot navigation (Zhang et al.",
      "startOffset" : 28,
      "endOffset" : 47
    }, {
      "referenceID" : 23,
      "context" : ", 2014), and robot navigation (Zhang et al., 2015).",
      "startOffset" : 30,
      "endOffset" : 50
    }, {
      "referenceID" : 14,
      "context" : "Deep reinforcement learning has shown competitive performance in various tasks: arcade games (Mnih et al., 2015), object recognition (Mnih et al., 2014), and robot navigation (Zhang et al., 2015). However, it has been mostly applied to the single-agent decision-theoretic settings with stationary environments. One exception is Tampuu et al. (2015), where two agents controlled by independent DQNs interact under collaborative and competitive rewards.",
      "startOffset" : 94,
      "endOffset" : 349
    }, {
      "referenceID" : 11,
      "context" : "We use a Mixture-of-Experts network (Jacobs et al., 1991) to explicitly model the opponent action as a hidden variable and marginalize over it (Figure 1b).",
      "startOffset" : 36,
      "endOffset" : 57
    }, {
      "referenceID" : 18,
      "context" : "Unlike previous work that learns a separate model to predict these information about the opponent (Davidson, 1999; Ganzfried & Sandholm, 2011; Schadd et al., 2007), we apply multitask learning and use the observation as extra supervision to learn a shared opponent representation h.",
      "startOffset" : 98,
      "endOffset" : 163
    }, {
      "referenceID" : 6,
      "context" : "9, parameters are optimized by AdaGrad (Duchi et al., 2011) with a learning rate of 0.",
      "startOffset" : 39,
      "endOffset" : 59
    }, {
      "referenceID" : 3,
      "context" : "4 Additionally, we include data from Boyd-Graber et al. (2012). Most buzzes are from strong tournament players.",
      "startOffset" : 37,
      "endOffset" : 63
    }, {
      "referenceID" : 18,
      "context" : ", 1998b; Richards & Amir, 2007), or domain-specific strategies (Schadd et al., 2007; Southey et al., 2005).",
      "startOffset" : 63,
      "endOffset" : 106
    }, {
      "referenceID" : 19,
      "context" : ", 1998b; Richards & Amir, 2007), or domain-specific strategies (Schadd et al., 2007; Southey et al., 2005).",
      "startOffset" : 63,
      "endOffset" : 106
    }, {
      "referenceID" : 0,
      "context" : "Rubin & Watson (2011) and Bard et al. (2013) construct a a portfolio of strategies offline based on domain knowledge or past experience for heads-up limit Texas hold’em; they then select strategies online using multi-arm bandit algorithms.",
      "startOffset" : 26,
      "endOffset" : 45
    }, {
      "referenceID" : 12,
      "context" : "Lockett et al. (2007) propose an architecture similar to DRON-concat that aims to identify the type of an opponent.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 8,
      "context" : "Foerster et al. (2016) use modern neural networks to learn a group of parameter-sharing agents that solve a coordination task, where each agent is controlled by a deep recurrent Q-Network (Hausknecht & Stone, 2015).",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 7,
      "context" : "For example, Eigen et al. (2014) extends the Mixtureof-Experts network to a stacked model—deep Mixture-ofExperts—which can be combined with hierarchical reinforcement learning to learn a hierarchy of opponent strategies in large, complex domains such as online strategy games.",
      "startOffset" : 13,
      "endOffset" : 33
    } ],
    "year" : 2016,
    "abstractText" : "Opponent modeling is necessary in multi-agent settings where secondary agents with competing goals also adapt their strategies, yet it remains challenging because strategies interact with each other and change. Most previous work focuses on developing probabilistic models or parameterized strategies for specific applications. Inspired by the recent success of deep reinforcement learning, we present neural-based models that jointly learn a policy and the behavior of opponents. Instead of explicitly predicting the opponent’s action, we encode observation of the opponents into a deep Q-Network (DQN); however, we retain explicit modeling (if desired) using multitasking. By using a Mixture-of-Experts architecture, our model automatically discovers different strategy patterns of opponents without extra supervision. We evaluate our models on a simulated soccer game and a popular trivia game, showing superior performance over DQN and its variants.",
    "creator" : "LaTeX with hyperref package"
  }
}