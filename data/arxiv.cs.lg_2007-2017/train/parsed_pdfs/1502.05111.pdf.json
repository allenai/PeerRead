{
  "name" : "1502.05111.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "CSAL: Self-adaptive Labeling based Clustering Integrating Supervised Learning on Unlabeled Data",
    "authors" : [ "Fangfang Li" ],
    "emails" : [ "Fangfang.Li@student.uts.edu.au", "Guandong.Xu@uts.edu.au", "Longbing.Cao@uts.edu.au" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 2.\n05 11\n1v 1\n[ cs\n.L G\n] 1\n8 Fe\nb 20\nKeywords Clustering · Classification · Supervised Learning · ExpectationMaximization\nFangfang Li University of Technology Sydney E-mail: Fangfang.Li@student.uts.edu.au\nGuandong Xu University of Technology Sydney E-mail: Guandong.Xu@uts.edu.au\nLongbing Cao University of Technology Sydney E-mail: Longbing.Cao@uts.edu.au"
    }, {
      "heading" : "1 Introduction",
      "text" : "Clustering is an important task in unsupervised learning for dividing objects into different groups. However, the performance of the existing clustering methods is largely limited due to the absence of labels. Differently, classification is advantageous for categorizing the new observations by a supervised classifier learned from a training data set containing labeled observations. The effectiveness of classification, to a great extent, depends on the quality of labeled training data. Actually, lacking labeled data is the core challenging problem in clustering and classification. In real applications, however, it is often very costly and sometimes even impossible to obtain sufficient correctly labeled data for training models. In order to solve this challenge, semi-supervised learning [5] is proposed, which partitions unlabeled data based on the partially available labeled training data, then the original labeled and self-labeled data form a whole training data set for supervised learning. The typical application scenario of semi-supervised learning is when the training data contains a small amount of labeled data but with a large amount of unlabeled data. Semi-supervised learning does not completely solve this problem because it still needs the partially labeled data as a start, which leaves an open question for supervised learning.\nAs an alternative, recently a new kind of approaches integrating unsupervised and supervised learning on unlabeled data has been proposed, which has shown to be effective in improving the learning performance. Through the new integrated methods, the high quality labeled data points are first produced by purely clustering the unlabeled data and the cluster assignments are used to label training data, and finally the obtained labeled data is involved for better classification. For example, Wang proposed a FCM-SLNMN [20] combined algorithm by selecting training data from Fuzzy c-means (FCM) [6] clustering results. Then the selected training data is applied to build a classifier with a supervised normal mixture model. Similarly, Maulik et al. proposed a modified differential evolution (DE)-based fuzzy c-medoids (FCMdd) [15][19] algorithm. FCMdd selects 50% of the data points nearest to the cluster center (distance) for training a SVM classifier.\nAmong these methods, the key step is the distance-based labeling process for the unlabeled data (i.e., the correctly assigned data points by clustering algorithms are taken as training data to train a classifier), which essentially impacts the final classification results. Although these methods are more effective than traditional clustering algorithms, they also have some drawbacks. For example, distance-based training data selection method may wrongly label some points on the boundary. Specifically, due to the intrinsic unsupervised limitations of the extant clustering algorithms, the labeling outcome is not perfect: some points are distinctively assigned to one cluster, while others are on the cluster boundary, where the points have the same distance from several cluster centers, are wrongly placed. In order to choose the best quality labeled data, one simple criteria is to select the points which are close to the cluster centers. However, due to the existence of cluster overlapping fact in real ap-\nplications, there may exist some points which are close to the cluster centers but are located around the cluster boundaries. Fig. 1 gives an example of such observations from clustering results on the Iris data set [7] by K-means [9][11] algorithm. Here although point A is close to the cluster center, it should be excluded from the labeling. It indicates that simply relying on distance-based methods may select wrong data points into the training data, and eventually mislead the final partition performance.\nDistance-based labeling algorithm chooses the data points which are close to cluster center point. The algorithm works reliably when most points are closely gathered in different groups. However, this approach fails to handle the clusters which possess some intersections, i.e., the cluster boundaries are overlapping largely. In another word, some points close to cluster centers but around the boundaries should not be labeled. In order to solve this problem, entropy-based algorithms [12] are proposed which estimate the entropies of data points and select the points with lower entropy values, indicating the points very likely belonging to one cluster but weakly associated with other clusters. This approach can easily identify the points around cluster boundaries than the distance-based method. In short, the entropy-based selection approach handles the points around the boundaries well, while the distancebased labeling approach finds the good points close to the clustering center well. However, due to the diversity of cluster characteristics which undoubtedly influence the labeling method performance, only one labeling approach is not able to manipulate the various cluster characteristics. In this paper, we thus propose a self-adaptive labeling approach for selecting training data in line with the cluster characteristics. The underlying idea is that if a cluster is\ncompact and separated well from others, distance-based labeling will be applied in the clusters. Otherwise, if one cluster is intersected and not separated well with other clusters, the entropy-based labeling will be employed. In particular, we adopt Silhouette coefficient [10] to measure the compactness and separation of the clusters and determine which method should be applied.\nAlthough the labeling approach can find possible labeled training data, the classification is not always satisfied as a result of the quality of labeled training data, derived from the clustering. For instance, data point xi which belongs to cluster Cm is wrongly partitioned into cluster Cn (m 6= n). This scenario will result in the wrong labels for the training data. Thus the classification performance will be accordingly limited. This problem motivates us to further devise a solution following a labeling-classifying-relabeling-reclassifying mechanism to refine the classifier training.\nTherefore, we propose an iterative clustering framework by combining the above two strategies, i.e., self-adaptive labeling and classifier refinement. The iterative process aims at refining the classifiers from labeled training data via an expectation-maximization algorithm. In each iteration, we use the trained classifiers to classify the source data and all the labels of the source data are updated accordingly, then the improved labels of training data are fed back to train the classifiers iteratively until the convergence of classifications. Unlike the semi-supervised learning, the self-adaptive labeling based clustering (CSAL) method does not need any labeled data in advance, instead it only involves clustering for labeling training data and building classifiers.\nThe contributions of the paper are as follows:\n– We propose the CSAL method which integrates clustering and classification together for unlabeled data. – We compare the self-adaptive labeling method with distance-based and entropy-based labeling approaches. – We conduct substantial experiments to verify our labeling algorithms and integrated framework.\nThe rest of this paper is organized as follows. Section 2 presents the related work. In Section 3 we first introduce the proposed clustering framework, then detail training data labeling algorithms. Section 4 introduces several clustering algorithms integrating with supervised classification together such as CEM and CSAL. Experiments are then conducted and analyzed in Section 5. The paper is concluded in the last section.\n2 Related Work\n2.1 Semi-Supervised Learning\nThe underlying idea of semi-supervised learning is taking the existing rare labeled data as a guidance to partition unlabeled data for reducing the painful manually labeling process. Blum and Mitchell proposed a co-training approach\n[2] which uses two independent classifiers and labeled examples to classify the unlabeled data and pick up the most confident positive and negative examples to enlarge the training set of the other. Nigam and Ghani [16] proposed co-EM which combines the co-training and Expectation Maximization (EM) for low errors. Nigam et al. [17] also introduced an algorithm for learning from labeled and unlabeled documents based on the combination of EM and a Naive Bayes classifier. Some other good outcomes such as graph-based semi-supervised learning [4] [14], semi-supervised support vector [1] [13] and multi-manifold semi-supervised learning [8] also assumed that a small amount of labeled data is involved. Generally, semi-supervised learning can be typically used when the data has small pieces of labeled data with a large amount of unlabeled data. This indicates that a few data points needed to be labeled first when applying semi-supervised learning approaches on the completely unlabeled data. In real applications, however, the labeling is often time-consuming. So it is beneficial to integrate clustering with classification for directly handling unlabeled data.\n2.2 Integrating Clustering for Classification\nIn order to solve the challenging problems, the combination of clustering and classification becomes an active research area. Unlike semi-supervised learning combining labeled data with unlabeled data together, this method integrates clustering and classification for directly partitioning on unlabeled data. For example, Celeux and Govaert [3] described a classification EM algorithm (CEM) which is a classification version of the EM algorithm, it incorporates a classification step between the E-step and M-step of the EM algorithm by a maximum a posteriori principle. Wang also proposed a FCM-SLNMN clustering algorithm [20] by a distance-based training data labeling method from FCM clustering results. But the performance of the extant integrated classification algorithms is limited by distance-based training data labeling approach. Different from FCM-SLNMN, the proposed CSAL relies on a self-adaptive labeling process considering information entropy and distance to select training data, leading to much more reliable labeled data for further classification."
    }, {
      "heading" : "3 CSAL Framework",
      "text" : "This section introduces the effective clustering framework (CSAL) which is described in Fig 2.\n3.1 Notations\nFor better illustration of the CSAL framework, we first list the notations used in this paper in Table 1.\n3.2 The CSAL Framework\nThe core components of the CSAL framework include: clustering, training data labeling, training classifier and iteration. Among the components, training data labeling plays a vital role by connecting clustering and supervised classifier together. Since the selected training data may have wrong assigned points, the reselecting and iteration steps aim to refine the selected training data after considering the classification results. The process of CSAL framework is detailed below:\n1. Clustering: Clustering algorithms are employed to divide the unlabeled data into different groups; 2. Calculating information entropy and distance: Information entropy and distance are computed for every point in terms of its probability of being associated with each cluster; 3. Labeling training data: Different strategies (distance-based, entropy-based and self-adaptive methods) are applied to select training data; 4. Training a classifier: Classifiers are trained based on the training data set;\n5. Classifying data: The trained classifier is used to classify the source data; 6. Recalculating information entropy and distance: Information entropy and\ndistance are recalculated regarding the classification outcomes; 7. Reselecting training data: Training data is refined based on the results of\nstep 6); 8. Iteration: Repeating the steps 2) - 7); and 9. Stopping: The algorithm stops until the partition is converged.\n3.3 Training Data Labeling Algorithms\nThe goal of clustering is to groupN data points intoK clusters. To improve the clustering results, the selected training data should consist of a proper quantity of points which are correctly assigned to clusters. If the size of selected training data is too small or too big, the performance of further classification will be affected. Besides this, the quality of selected training data determines the effectiveness of the further classification. In this section, we introduce distancebased, entropy-based training data selection approaches, and propose a selfadaptive labeling method involving distance and entropy to select training data from the clustering results."
    }, {
      "heading" : "3.3.1 Distance-based Training Data Labeling Algorithm",
      "text" : "The distance-based training data selection is depicted in Algorithm 1.\nInput: A source data set {x1, x2, ..., xN} Output: Selected training data E\n1. Initiate E to be the empty set for selected training data; 2. Apply clustering algorithm on the given data to get clusters {C1, C2, ...,CN}; 3. Calculate the distance of xi to the center of the cluster it belongs to; 4. Select the points which are close to the cluster centers with percentage A% into E.\nAlgorithm 1: Distance-based Training Data Labeling"
    }, {
      "heading" : "3.3.2 Entropy-based Training Data Labeling Algorithm",
      "text" : "First let us introduce the definition of entropy.\nDefinition 1 The information entropy H(xi) of point xi is defined as:\nH(xi) = −Σ K l=1P (Cl|xi)log2P (Cl|xi) (1)\nwhereΣKl=1P (Cl|xi) = 1, P (Cl|xi) ≥ 0, l = 1, ...,K and P (Cl|xi)log2P (Cl|xi) = 0 when P (Clxi) = 0.\nThe information entropy specified in Definition 1 makes clustering get a better data partition. The underlying idea of entropy-based training data selection algorithm is below: if the probability of assigning a point to a cluster is much higher than to other clusters, then its information entropy to the cluster is much lower than to any other clusters. If the probability of assigning a point to several clusters are similar, then it indicates that the point is likely on the boundaries of the clusters.\nOn top of the above idea, the entropy-based training data selection is given in Algorithm 2.\nInput: A source data set {x1, x2, ..., xN} Output: Selected training data E\n1. Initiate E to be the empty set for selected training data; 2. Apply clustering algorithm on the given data to get clusters {C1, C2, ...,CN}; 3. Calculate the information entropy of point xi by (1) ; 4. Select points which have the lowest information entropy with percentage A% into E.\nAlgorithm 2: Entropy-based Training Data Labeling"
    }, {
      "heading" : "3.3.3 Self-adaptive Training Data Labeling Algorithm",
      "text" : "The two training data selection algorithms described above (distance-based and entropy-based selection algorithms) have corresponding advantages and disadvantages. The distance-based labeling algorithm is effective for the points which are close to the cluster center. By contrast, the entropy-based selection algorithm has a higher accuracy for the points which are on the boundaries of clusters. We propose a novel and self-adaptive selection algorithm integrating the distance-based and the entropy-based selection algorithms by data characteristics.\nDefinition 2 The Silhouette coefficient s(xi) of point xi is defined as:\ns(xi) = b(xi)− a(xi)\nmax(a(xi), b(xi)) (2)\nwhere, a(xi) is the average dissimilarity between point xi and all other points in the same cluster, and b(xi) is the average dissimilarity between point xi and all other points in the next nearest cluster.\nSilhouette coefficient is a good method to evaluate how objects in a cluster are closely related, and how distinct or well-separated a cluster is from other clusters. A higher Silhouette coefficient score relates to a model with better defined clusters. For a specific cluster, we use the mean silhouette coefficient which is defined in (3) to evaluate the cluster.\nMS(Cl) = 1\nn Σni=1s(xi) (3)\nHigh MS(Cl) denotes that the cluster Cl is separated from other clusters well and the related objects in this cluster are close with each other. We know that the distance-based training data labeling method is effective for the points which are close to the center of the cluster, the entropy-based training data labeling method outperforms the former on the boundaries of the clusters. The mean silhouette coefficient can be applied to determine which method should be chosen for labeling training data. The integrated self-adaptive training data labeling algorithm is described as Algorithm 3.\nInput: A source data set {x1, x2, ..., xN} Output: Selected training data E\n1. Initiate E to be the empty set for selected training data; 2. Apply clustering algorithm on the given data to get clusters {C1, C2, ...,CN}; 3. Compute the mean silhouette coefficient MS(Cl) of cluster Cl; 4. if MS(Cl) > threshold then\nSelect A% of points in Cl into E by distance-based labeling algorithm; else Select A% of points in Cl into E by entropy-based labeling algorithm. end\nAlgorithm 3: Self-Adaptive Labeling"
    }, {
      "heading" : "4 Integrated Algorithms",
      "text" : "Different supervised classification methods can be applied in the CSAL framework. In this section, we firstly introduce a classification expectation maximization (CEM) algorithm [3], then detail our proposed innovative classification algorithm (CSAL) integrating clustering and supervised normal mixture model, followed by three derived algorithms.\n4.1 CEM Algorithm\nThe CEM algorithm calculates the parameters, determines the clusters by a classification approach and starts from an initial partition.\nStart: Given an initial partition {x1, x2, ..., xN}, E-step: For cluster l = 1, ...,K, and data point i = 1, ..., N , calculate the\ncurrent posterior probabilities of xi belonging to Cl:\nωil k =\nα̂kl f(xi, µ̂ k l , Σ̂ k t )\nΣKt=1α̂ k t f(xi, µ̂ k l , Σ̂ k t )\n(4)\nwhere f(xi, µ̂kl , Σ̂ k t ) denotes the d-dimensional normal density in terms of mean µ̂kl and covariance matrix Σ̂ k t .\nC-step: Assign xi to cluster Cl with the maximum posterior probability ωkil, and generate the partition results Pk.\nM-step: For l = 1, ...,K, calculate the estimates of maximum likelihood α̂k+1l ,µ̂ k+1 l ,Σ̂ k+1 t by (5).\n\n   \n   \nα̂k+1l = #Pk l N µ̂k+1l = Σ xi∈P k l xi\n#Pk l\nΣ̂k+1t = ΣK i=1Σxi∈P k l ‖xi−µ̂ k+1 l ‖\nNd\n(5)\nwhere #P kl is the number of points assigned to cluster Cl.\n4.2 CSAL Algorithm\nThe CSAL algorithm is described as follows. First, a clustering algorithm is applied to cluster the data set, and each point is given a class label. Second, A% data points in each cluster are selected as training data, then a supervised normal mixture model for classification is trained on the selected training data. θ̂0l = 1 N ΣNi=1yil are calculated by (6):\n\n  \n  \nα̂0l = 1 N ΣNi=1yil µ̂0l = ΣN i=1yilxi\nΣN i=1 yil\nΣ̂0l = ΣN i=1yil(xi−µ̂ 0 l )(xi−µ̂ 0 l )T\nΣN i=1 yil\n(6)\nFinally, we repeat the process of training data labeling and classification of the whole data set until the convergence of the algorithm. Below, we show the main process of the CSAL algorithm:\nStart: Given an initial partition {x1, x2, ..., xN}, E-step: For l = 1, ...,K, and i = 1, ..., N , compute the posterior probabili-\nties of xi belonging to Cl:\nω̂kil = α̂kl |Σ̂ k l | − 1 2 exp− 12 (xi − µ̂ k l )Σ̂ k l (xi − µ̂ k l ) T\nΣKt=1α̂ k t |Σ̂ k l | − 1 2 exp− 12 (xi − µ̂ k t )Σ̂ k t (xi − µ̂ k t )\nT (7)\nC-step: Assign xi to cluster Cl with the maximum posterior probability ωkil, and set y k il = 1 and y k it = 0, t 6= l.\nS-step: Select A% data points from each cluster by training data selection algorithm. If point xi is selected as training data, si = 1; otherwise si = 0. Let\nλ =\n{\n1, yil = 1, and si = 1; 0, otherwise.\n(8)\nM-step: l = 1, ...,K, calculate the estimates of maximum likelihood α̂k+1l ,\nµ̂k+1l , Σ̂ k+1 l by (9).\n\n     \n      \nα̂k+1l =\n∑\nN i=1 λk il\n∑\nK t=1\n∑\nN i=1 λk il\nµ̂k+1l =\n∑\nN i=1 λk il xi\n∑\nN i=1 λk il\nΣ̂k+1l =\n∑\nN i=1 yil(xi−µ̂ k+1 l )(xi−µ̂ k+1 l ) T\n∑\nN i=1 λk il\n(9)\nThe CSAL algorithm extends the CEM algorithm: the S-step is added after the C-step to select training data, and in the M-step the parameters are calculated on the selected training data. Different clustering algorithms such as K-means, FCM and Gaussian Mixture Model (GMM)[18] can be integrated with the CSAL algorithm, and respectively generate the derived algorithms as Kmeans-CSAL, FCM-CSAL and GMM-CSAL.\n4.3 CSAL Convergence and Complexity Analysis\nThe convergence of the CEM algorithm has been proved [3]. Similarly, we prove the convergence of the CSAL algorithm as follows.\nTheorem 1 The log likelihood of CSAL is log p ( x|λ̂k, α̂k, µ̂k, Σ̂k ) , and\nthe estimates of the parameters θ̂k+1 = {α̂k+1l , µ̂ k+1 l , Σ̂ k+1 l } K l=1 of the CSAL algorithm converges to fixed values. Proof The log likelihood is\nlog (p (x|θ)) =\nN ∑\ni=1\nK ∑\nl=1\nλil logαlp(xi|µl, Σl) (10)\nθ̂k+1 = {α̂k+1l , µ̂ k+1 l , Σ̂ k+1 l } K l=1 is the maximum log likelihood of log ( p ( x|λ̂k, α̂k, µ̂k, Σ̂k )) , we have:\nlog ( p ( x|λ̂k, α̂k+1, µ̂k+1, Σ̂k+1 )) ≥ log ( p ( x|λ̂k, α̂k, µ̂k, Σ̂k ))\n(11)\nand when ykil = 1, ŵ k+1 il ≥ ŵ k+1 it holds for all t 6= l, which implies\nα̂k+1l p(xi|µ̂ k+1, Σk+1) ≥ α̂kl p(xi|µ̂ k, Σk) (12)\nIn addition, the proportion of selected data is the same in each iteration, and the selected data has the highest ŵk+1il , so\nlog p ( x|λ̂k+1, α̂k+1, µ̂k+1, Σ̂k+1 ) ≥ log p ( x|λ̂k, α̂k, µ̂k, Σ̂k )\n(13)\nSince the number of instances into each cluster is finite, log p ( x|λ̂k, α̂k, µ̂k, Σ̂k )\nconverges to a fixed value. If k is large enough, then \n\n\nα̂k = α̂k+1 µ̂k = µ̂k+1\nΣ̂k = Σ̂k+1 (14)\nIn each iteration, the complexity of the CSAL algorithm is O(dKN)."
    }, {
      "heading" : "5 Experiments and Evaluation",
      "text" : "In this section, we firstly introduce the experiment settings and evaluation method. Then we detail the experimental results.\n5.1 Data Sets\nThe data sets mainly involve two synthetic data sets and four real data sets from the UC Irvine Machine Learning Repository [7].\n1) Synthetic Gaussian data: The Gaussian data sets are synthesized in two ways. Firstly, 200 instances (we name the data set GData1) falling into two classes of bivariate Gaussian density with the following parameter settings:\n\n     \n     \nµ1 = (1, 1) µ2 = (2, 0)\nΣ1 =\n[\n1 0 0 0.25\n]\nΣ2 =\n[\n0.8 0 0 1\n]\n(15)\nSecondly, 300 instances (we call it GData2) falling into three classes of bivariate Gaussian density with the following parameter settings:\n\n           \n           \nµ1 = (0, 0) µ2 = (6, 6)\nµ3 = (−10,−10)\nΣ1 =\n[\n1 0 0 1\n]\nΣ2 =\n[\n3 0 0 3\n]\nΣ3 =\n[\n100 0 0 100\n]\n(16)\n2) Real Data Sets: Four real data sets from UCI repository, which are Iris, Heart Diseases, New Thyroid and Wine are exploited in this paper.\n5.2 Experimental Settings\nClassification accuracy is used to evaluate the performance of the CSAL algorithms.\nClassificationAccuracy =\n∑K\nl=1 Dl\nN (17)\nwhere Dl is the number of samples correctly partitioned in the genuine class Cl.\nTo evaluate the performance of our proposed CSAL algorithms, experiments are conducted as follows. First, we compare different training data labeling strategies which connect clustering algorithms with supervised classifiers together. Second, we compare the derived CSAL algorithms with three different clustering algorithms (K-means, FCM and GMM). Next, we evaluate the derived CSAL algorithms comparing with clustering algorithms integrating different classification methods (Naive bayes and SVM). Then they are compared with the CEM algorithms. Last, the execution time of all the algorithms are compared.\n5.3 Experimental Results"
    }, {
      "heading" : "5.3.1 Training Data Labeling",
      "text" : "In this paper, different training data labeling strategies (distance-based, entropybased and self-adaptive methods) are compared, where threshold is empirically set to 0.35. Fig. 3-8 show that the entropy-based method performs better than the distance-based method for selecting training data, and self-adaptive method outperforms the distance-based and entropy-based methods. Because entropy-based and self-adaptive methods have the same performance on Iris and New Thyroid data sets, their curves are overlapped in Figs. 5 and 6."
    }, {
      "heading" : "5.3.2 Comparison of the CSAL with Traditional Clustering Algorithms",
      "text" : "In order to evaluate our CSAL framework, we compare it with traditional clustering algorithms (K-means, FCM and GMM). The comparison result is described in Fig. 9 which clearly indicates that the CSAL is more effective than traditional clustering algorithms. The derived Kmeans-CSAL, FCM-CSAL and GMM-CSAL algorithms performs much better than the corresponding clustering algorithms."
    }, {
      "heading" : "5.3.3 Comparison of the CSAL Algorithms with Traditional Clustering algorithms Integrating Different Supervised Classification Methods",
      "text" : "The comparison results are shown in Figs. 10 and 11. For all data sets, Fig. 10 indicates that the derived CSAL algorithms are more effective than the corresponding clustering algorithms which integrate Naive Bayes classifier without the training data labeling process. For most of the data sets, Fig. 11 shows that the derived CSAL algorithms perform better than the corresponding clustering algorithms which integrate SVM classifier without a training data labeling process."
    }, {
      "heading" : "5.3.4 Comparison with the CEM algorithm",
      "text" : "The CSAL is also compared with the CEM algorithm, which is shown in Fig. 12. It illustrates that the derived CSAL algorithms are more effective than the corresponding CEM algorithms."
    }, {
      "heading" : "5.3.5 Comparison of Runtime",
      "text" : "The following Table 2 indicates the comparison of execution time of different algorithms. The algorithms are coded in Matlab and executed in a machine with 3.10GHz CPU and 4.00GB RAM. Table 2 shows that the execution time of the CSAL process is longer than that of the Naive Bayes classifier and the CEM algorithms, but much shorter than that of the SVM algorithm. The runtime of GMM is much longer than the CSAL process, consequently GMMCSAL takes slightly longer time than GMM. The runtime of the KmeansCSAL algorithm is in the same size of that of K-means. The runtime of the FCM-CSAL algorithm is much longer than that of FCM, but they are still in the same order of magnitude."
    }, {
      "heading" : "6 Conclusion",
      "text" : "The challenging problem in clustering and classification is the absence of labeled data, however, it is costly and time-consuming to obtain labeled data in real applications. In this paper, we proposed a novel clustering framework CSAL to solve the challenge. In this framework, we also proposed a new selfadaptive labeling approach for training data selection and compared it with\ndistance-based and entropy-based labeling methods. The experiments on publicly data sets showed that the self-adaptive labeling method outperforms the distance-based and entropy-based methods. Additionally, the experiments also demonstrated that the CSAL is more effective than the corresponding comparison partners. From the experiments we know that the CSAL can handle unlabeled data well, but it still does not consider the data characteristics such as gaussian or gamma distribution of the data. We believe that the performance of the CSAL framework can be further enhanced if we involve the data distribution in the future."
    } ],
    "references" : [ {
      "title" : "Semi-supervised support vector machines",
      "author" : [ "Kristin P. Bennett", "Ayhan Demiriz" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1998
    }, {
      "title" : "Combining labeled and unlabeled sata with cotraining",
      "author" : [ "Avrim Blum", "Tom M. Mitchell" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1998
    }, {
      "title" : "A classification EM algorithm for clustering and two stochastic versions",
      "author" : [ "C. Celeux", "G. Govaert" ],
      "venue" : "Computational Statistics and Data Analysis, 14(3):315–332,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Graph-based semisupervised learning",
      "author" : [ "Mark Culp", "George Michailidis" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell.,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2008
    }, {
      "title" : "Semi-supervised clustering using genetic algorithms ayhan demiriz",
      "author" : [ "Ayhan Demiriz", "Kristin Bennett", "K.P. Bennett", "M.J. Embrechts" ],
      "venue" : "Artificial Neural Networks in Engineering",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1999
    }, {
      "title" : "Some recent investigations of a new fuzzy partitioning algorithm and its application to pattern classification problems",
      "author" : [ "J.C. Dunn" ],
      "venue" : "Journal of Cybernetics, 4(2):1–15,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1974
    }, {
      "title" : "and A",
      "author" : [ "A. Fran" ],
      "venue" : "Asuncion. UCI machine learning repository,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Multimanifold semi-supervised learning",
      "author" : [ "Andrew B. Goldberg", "Xiaojin Zhu", "Aarti Singh", "Zhiting Xu", "Robert Nowak" ],
      "venue" : "Journal of Machine Learning Research - Proceedings Track,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2009
    }, {
      "title" : "A K-means clustering algorithm",
      "author" : [ "J.A. Hartigan", "M.A. Wong" ],
      "venue" : "Applied Statistics, 28:100–108,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1979
    }, {
      "title" : "Data clustering: 50 years beyond k-means",
      "author" : [ "Anil K. Jain" ],
      "venue" : "Pattern Recognition Letters,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2010
    }, {
      "title" : "Senior Member, and Senior Member. An efficient k-means clustering algorithm: Analysis and implementation",
      "author" : [ "Tapas Kanungo", "David M. Mount", "Nathan S. Netanyahu", "Christine D. Piatko", "Ruth Silverman", "Angela Y. Wu" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2002
    }, {
      "title" : "Minimum entropy clustering and applications to gene expression analysis. In CSB, pages 142–151",
      "author" : [ "Haifeng Li", "Keshu Zhang", "Tao Jiang" ],
      "venue" : "IEEE Computer Society,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2004
    }, {
      "title" : "Improving semi-supervised support vector machines through unlabeled instances selection",
      "author" : [ "Yu-Feng Li", "Zhi-Hua Zhou" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2011
    }, {
      "title" : "Robust and scalable graph-based semisupervised learning",
      "author" : [ "Wei Liu", "Jun Wang", "Shih-Fu Chang" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2012
    }, {
      "title" : "Integrating clustering and supervised learning for categorical data analysis",
      "author" : [ "Ujjwal Maulik", "Sanghamitra Bandyopadhyay", "Indrajit Saha" ],
      "venue" : "IEEE Transactions on Systems, Man, and Cybernetics, Part A,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2010
    }, {
      "title" : "Semi-Supervised Text Classification",
      "author" : [ "Kamal Nigam", "Andrew Mccallum", "Tom M. Mitchell" ],
      "venue" : "Using EM,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2006
    }, {
      "title" : "The infinite gaussian mixture model",
      "author" : [ "Carl Edward Rasmussen" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2000
    }, {
      "title" : "Improvement of new automatic differential fuzzy clustering using svm classifier for microarray analysis",
      "author" : [ "Indrajit Saha", "Ujjwal Maulik", "Sanghamitra Bandyopadhyay", "Dariusz Plewczynski" ],
      "venue" : "Expert Syst. Appl.,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2011
    }, {
      "title" : "A clustering algorithm combine the fcm algorithm with supervised learning normal mixture model",
      "author" : [ "Wei Wang", "Chunheng Wang", "Xia Cui", "Ai Wang" ],
      "venue" : "In ICPR,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "In order to solve this challenge, semi-supervised learning [5] is proposed, which partitions unlabeled data based on the partially available labeled training data, then the original labeled and self-labeled data form a whole training data set for supervised learning.",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 18,
      "context" : "For example, Wang proposed a FCM-SLNMN [20] combined algorithm by selecting training data from Fuzzy c-means (FCM) [6] clustering results.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 5,
      "context" : "For example, Wang proposed a FCM-SLNMN [20] combined algorithm by selecting training data from Fuzzy c-means (FCM) [6] clustering results.",
      "startOffset" : 115,
      "endOffset" : 118
    }, {
      "referenceID" : 14,
      "context" : "proposed a modified differential evolution (DE)-based fuzzy c-medoids (FCMdd) [15][19] algorithm.",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 17,
      "context" : "proposed a modified differential evolution (DE)-based fuzzy c-medoids (FCMdd) [15][19] algorithm.",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 6,
      "context" : "1 gives an example of such observations from clustering results on the Iris data set [7] by K-means [9][11] algorithm.",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 8,
      "context" : "1 gives an example of such observations from clustering results on the Iris data set [7] by K-means [9][11] algorithm.",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 10,
      "context" : "1 gives an example of such observations from clustering results on the Iris data set [7] by K-means [9][11] algorithm.",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 11,
      "context" : "In order to solve this problem, entropy-based algorithms [12] are proposed which estimate the entropies of data points and select the points with lower entropy values, indicating the points very likely belonging to one cluster but weakly associated with other clusters.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 9,
      "context" : "In particular, we adopt Silhouette coefficient [10] to measure the compactness and separation of the clusters and determine which method should be applied.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 1,
      "context" : "CSAL 5 [2] which uses two independent classifiers and labeled examples to classify the unlabeled data and pick up the most confident positive and negative examples to enlarge the training set of the other.",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 15,
      "context" : "[17] also introduced an algorithm for learning from labeled and unlabeled documents based on the combination of EM and a Naive Bayes classifier.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 3,
      "context" : "Some other good outcomes such as graph-based semi-supervised learning [4] [14], semi-supervised support vector [1] [13] and multi-manifold semi-supervised learning [8] also assumed that a small amount of labeled data is involved.",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 13,
      "context" : "Some other good outcomes such as graph-based semi-supervised learning [4] [14], semi-supervised support vector [1] [13] and multi-manifold semi-supervised learning [8] also assumed that a small amount of labeled data is involved.",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 0,
      "context" : "Some other good outcomes such as graph-based semi-supervised learning [4] [14], semi-supervised support vector [1] [13] and multi-manifold semi-supervised learning [8] also assumed that a small amount of labeled data is involved.",
      "startOffset" : 111,
      "endOffset" : 114
    }, {
      "referenceID" : 12,
      "context" : "Some other good outcomes such as graph-based semi-supervised learning [4] [14], semi-supervised support vector [1] [13] and multi-manifold semi-supervised learning [8] also assumed that a small amount of labeled data is involved.",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 7,
      "context" : "Some other good outcomes such as graph-based semi-supervised learning [4] [14], semi-supervised support vector [1] [13] and multi-manifold semi-supervised learning [8] also assumed that a small amount of labeled data is involved.",
      "startOffset" : 164,
      "endOffset" : 167
    }, {
      "referenceID" : 2,
      "context" : "For example, Celeux and Govaert [3] described a classification EM algorithm (CEM) which is a classification version of the EM algorithm, it incorporates a classification step between the E-step and M-step of the EM algorithm by a maximum a posteriori principle.",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 18,
      "context" : "Wang also proposed a FCM-SLNMN clustering algorithm [20] by a distance-based training data labeling method from FCM clustering results.",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 2,
      "context" : "In this section, we firstly introduce a classification expectation maximization (CEM) algorithm [3], then detail our proposed innovative classification algorithm (CSAL) integrating clustering and supervised normal mixture model, followed by three derived algorithms.",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 16,
      "context" : "Different clustering algorithms such as K-means, FCM and Gaussian Mixture Model (GMM)[18] can be integrated with the CSAL algorithm, and respectively generate the derived algorithms as Kmeans-CSAL, FCM-CSAL and GMM-CSAL.",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 2,
      "context" : "3 CSAL Convergence and Complexity Analysis The convergence of the CEM algorithm has been proved [3].",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 6,
      "context" : "1 Data Sets The data sets mainly involve two synthetic data sets and four real data sets from the UC Irvine Machine Learning Repository [7].",
      "startOffset" : 136,
      "endOffset" : 139
    } ],
    "year" : 2015,
    "abstractText" : "Supervised classification approaches can predict labels for unknown data because of the supervised training process. The success of classification is heavily dependent on the labeled training data. Differently, clustering is effective in revealing the aggregation property of unlabeled data, but the performance of most clustering methods is limited by the absence of labeled data. In real applications, however, it is time-consuming and sometimes impossible to obtain labeled data. The combination of clustering and classification is a promising and active approach which can largely improve the performance. In this paper, we propose an innovative and effective clustering framework based on self-adaptive labeling (CSAL) which integrates clustering and classification on unlabeled data. Clustering is first employed to partition data and a certain proportion of clustered data are selected by our proposed labeling approach for training classifiers. In order to refine the trained classifiers, an iterative process of Expectation-Maximization algorithm is devised into the proposed clustering framework CSAL. Experiments are conducted on publicly data sets to test different combinations of clustering algorithms and classification models as well as various training data labeling methods. The experimental results show that our approach along with the self-adaptive method outperforms other methods.",
    "creator" : "LaTeX with hyperref package"
  }
}