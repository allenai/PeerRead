{
  "name" : "1702.00956.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "KU-ISPL SPEAKER RECOGNITION SYSTEMS",
    "authors" : [ "Suwon Shon", "Hanseok Ko" ],
    "emails" : [ "swshon@ispl.korea.ac.kr,", "hsko@korea.ac.kr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ISPL) developed speaker recognition system for SRE16 fixed training condition. Data for evaluation trials are collected from outside North America, spoken in Tagalog and Cantonese while training data only is spoken English. Thus, main issue for SRE16 is compensating the discrepancy between different languages. As development dataset which is spoken in Cebuano and Mandarin, we could prepare the evaluation trials through preliminary experiments to compensate the language mismatched condition. Our team developed 4 different approaches to extract i-vectors and applied state-of-the-art techniques as backend. To compensate language mismatch, we investigated and endeavored unique method such as unsupervised language clustering, inter language variability compensation and gender/language dependent score normalization.\nIndex Terms— SRE16, i-vector, language mismatch"
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "This document is description of the Korea University –\nIntelligent Signal Processing Laboratory (KU-ISPL) speaker recognition system for NIST 2016 speaker recognition evaluation (SRE16).\nUnder i-vector framework, new approaches are introduced using Bottleneck Feature (BNF) and Deep Neural Network (DNN) which were validated successfully its performance improvement on ASR. In this study, we developed the state-of-the-art i-vector systems for validating the performances on language mismatch condition using SRE16 dataset. Based on the prior studies about domain adaptation and compensation, Inter Dataset Variability Compensation (IDVC) and unsupervised domain adaptation using interpolated PLDA are also applied.\nAfter studying about prior works, we proposed additional techniques for compensating the language mismatch condition to obtain robust performance on SRE 16 dataset. For official evaluation, we submitted total 3 systems including 1 primary system and 2 contrastive systems in fixed training data condition. We carefully followed the\nSRE16 rules and requirements during training and test processes.\nIn the following, we introduce a dataset of SRE 16 at section 2. At Section 3 and 4, system components for development of state-of-the-art i-vector extraction are described."
    }, {
      "heading" : "2. DATASET PREPARATION FOR FIXED TRAINING CONDITION",
      "text" : "For fixed training condition, we use Fisher English, SRE 04~10 and SWB-2 (phase1~3, cellular 1~2) dataset for training set. Language of all dataset in training set is English. The dataset for SRE 16 evaluation trials are collected from speakers who located outside North America and spoke Tagalog and Cantonese (referred as major language). Before evaluation dataset is available, development dataset which mirrors the evaluation conditions to prepare the language mismatch condition on evaluation set. The development dataset is collected from speaker who located outside North America and spoke Cebuano and Mandarin (referred as minor language). Additionally, unlabeled minor and major language dataset is also given to participants for development set. The development set are free to use for any purpose and detailed statistics about evaluation and development dataset are shown in table 1."
    }, {
      "heading" : "3. SYSTEM COMPONENT DESCRIPTION",
      "text" : ""
    }, {
      "heading" : "3.1. Acoustic features",
      "text" : "For training speaker recognition system on this paper, Mel-Frequency Cepstral Coefficients (MFCC) is used to generate 60 dimensional acoustic features. It is consist of 20 cepstral coefficients including log-energy C0, then, it is appended with its delta and acceleration. For training DNN based acoustic model that is inspired by Automatic Speech Recognition (ASR) area, different configuration was adopt to generate 40 ceptral coefficient without energy component for high resolution of acoustic features (ASR-MFCC). For feature normalization, Cepstral Mean Normalization is applied with 3 seconds-length sliding window.\nAfter extracting acoustic features, Voice Activity algorithm was adopted to remove silence and low energy segments on the speech dataset. Simple energy based VAD was used with log-mean scaled threshold. Using log-energy (C0) component of MFCC, the mean log-energy of each segment can be calculated and it is scale to half value and then plus by 6. That is the final scaled threshold for VAD. We do not apply VAD algorithm when we training DNN acoustic model."
    }, {
      "heading" : "3.2. I-vector extraction",
      "text" : "For performance comparison of SRE 16 trials, four\ndifferent approaches to extract i-vectors are developed."
    }, {
      "heading" : "3.2.1. GMM-UBM (GU)",
      "text" : "For General i-vector extraction approach [1] by modifying Kaldi’s recipe (sre08/v1). For training GMM-UBM and total variability matrix, SRE(04~10, part of 12) and switchboard dataset (p2 1~3, cellular 1~2) were used."
    }, {
      "heading" : "3.2.2. DNN-UBM (DU)",
      "text" : "Based on Kaldi’s recipe (sre10/v2), Fisher English was used for training Time Delay Neural Network with ASRMFCC feature. After training TDNN, the DNN-UBM is estimated on DNN-MFCC feature which is high resolution version of MFCC. SRE (04~10, part of 12) and Switchboard Dataset were used for training DNN-UBM and total variability matrix [2]."
    }, {
      "heading" : "3.2.3. Supervised GMM-UBM (SU)",
      "text" : "Based on Kaldi’s recipe (sre10/v2), Supervised GMMUBM[2] was trained using posterior of TDNN network. Same dataset was used as GMM-UBM system for training Supervised GMM-UBM and total variability matrix"
    }, {
      "heading" : "3.2.4. Bottleneck Feature based GMM-UBM (BU)",
      "text" : "BNF features were extracted using DNN which containing bottleneck layer [3]. DNN layer structure was set to 1500- 1500-80-1500 with total 4 layer and MFCC feature of all\ndataset was converted to BNF feature (80 dim). After extracting BNF feature, it follows general GMM-UBM based i-vector extraction approaches such as GMM-UBM system at Sec. 3.2.1 and same dataset was used for GMMUBM total variability matrix."
    }, {
      "heading" : "3.3. Backend procedures",
      "text" : "3.3.1. Inter Dataset Variability Compensation (IDVC)[4]\nSRE and Switchboard (SWB) Dataset sub-corpora label and gender label are used for obtaining the average i-vectors of each dataset by gender. SRE can be divide in to 5 subcorpora (SRE-04, 05, 06, 08, 10) and SWB can be divide in to 5 sub-corpora (switchboard-2 phase 1,2,3 and cellular part 1, 2). Finally, 600 dimensional i-vectors projected to 580 dimension by removing dataset dependent dimension.\n3.3.2. Whitening Transform and Length Normalization using unlabeled dataset (WTLN)[5]\nWhitening transformation and length normalization are simple and powerful techniques to improve performance of speaker recognition system by compensating the mismatch between enrollment and test i-vector length. It became a mandatory process of i-vector based speaker recognition system back-end and, moreover, recent study validated its effectiveness on domain adaptation by calculating whitening transform matrix using the in-domain dataset. We use use both unlabeled minor and major dataset for whitening and length normalization."
    }, {
      "heading" : "3.3.3. Interpolated PLDA (SRE04-08) + PLDA (speaker",
      "text" : "clustering using AHC) (IPLDA) [6]\nAgglomerative Hierarchical Clustering approach for unlabeled in-domain datasets to estimate the PLDA model was introduced by Garcia-Romero. Using the clustered speaker information, in-domain Within-speaker Covariance (WC) and Across-speaker Covariance (AC) of PLDA model are interpolated from out-of-domain WC and AC. We applied this approach on the unlabeled dataset of minor and major language. By experiments, 30 and 450 clusters were used for speaker clustering of unlabeled minor and major dataset. The 450 clusters (speaker) information of unlabeled major dataset could be used for calibration as Sec.4.5.\n3.3.4. S-norm [7]\nSymmetric normalization(S-norm) is adopted for score normalization. Basically, unlabeled major dataset was used as imposter utterances for both development and evaluation trials."
    }, {
      "heading" : "4. STUDIES FOR COMPENSATING LANGUAGE MISMATCH",
      "text" : ""
    }, {
      "heading" : "4.1. Gender Classification and unsupervised Language Classification of minor/major unlabeled dataset (GCLC)",
      "text" : "Gender classification could be done by comparing cosine similarity between gender i-vector and input i-vector which we want to classify the gender. Gender i-vector obtained by averaging the i-vectors of training set by gender.\nLanguage classification can be done by unsupervised clustering algorithm such as AHC or k-means clustering. Since k-means clustering performance is greatly depend on initial point as figure 1, AHC is frequently used on i-vector feature space.\nFor high accuracy and reliability of clustering algorithm, we proposed 2-step approach by running k-means algorithm twice on different i-vector representation space. If the kmeans algorithm secure good initial clusters that represent the mean of i-vectors from each language, we can get a high language classification performance while reducing the risk of misclassification by random initial cluster.\nWe check this on the minor enrollment and test dataset in development set which have language labels, so that they allowed us to investigate the performance of language classification. We use 2-step approach for language clustering as below schemes.\n1. Initializing on IDVC subspace (alike PCA) 2. K-means\nOr\n1. Initialing using AHC 2. K-means\nBoth approaches shows same result on minor enrollment and test dataset and it showed 100% language clustering accuracy on minor enrollment dataset and 99.8% on minor test dataset. Using this 2-step approach, we classify the unlabeled minor and major dataset to discover the language label. These valuable information were used very effectively\nfor language mismatch compensation and score normalization in next sections."
    }, {
      "heading" : "4.2. Inter Language Variability Compensation for gender and minor/major language (ILVC)",
      "text" : "If the system has gender and language information, inter language variability factor can be removed by the same scheme of IDVC. By the high performance of GCLC approach that we proposed in section 4.1, we could have valuable gender and language labels of minor/major unlabeled dataset. Finally, the i-vector subspace removal can be done by the mean i-vector of 10 sub-category according to language (English, Cebuano, Mandarin, Tagalog and Cantonese) and gender."
    }, {
      "heading" : "4.3. Simplified Autoencoder based Domain adaptation (SADA)",
      "text" : "The Autoencoder based Domain adaptation (AEDA) was proposed recently and its paper is in peer-reviewing process for publication. On this study, we simplify the AEDA to more simple Autoencoder structure and proposed a Simplified Autoencoder based Domain Adaptation (SADA), but it still performs almost same as AEDA. More details about SADA can be found in next studies."
    }, {
      "heading" : "4.4. Gender and Language dependent score normalization (GL-Norm)",
      "text" : "We have gender and language information of unlabeled dataset by GCLC approach in section 4.1. So we divide Snorm parameter into 4 sub-categories by gender and language. Gender and language of input i-vector are also classified by GCLC approach and use appropriate parameters to get gender and language specific score normalization."
    }, {
      "heading" : "4.5. Calibration and fusion",
      "text" : "For calibration and fusion, simple linear calibration and linear fusion were done by Bosaris toolkit [8]. For calibration, speaker clustering information of unlabeled major dataset was used (see Sec. 3.3.3) to obtain target and non-target score distribution of evaluation experiments. The mean of speaker cluster represent speaker i-vector and they can be scored with unlabeled major i-vectors. As we have already speaker label from speaker clustering, we can obtain target score and non-target score distribution and they could be used for score calibration on evaluation trials. Calibration was done on both before and after score normalization."
    }, {
      "heading" : "5. SUBSYSTEMS FOR MINOR LANUGUAGES",
      "text" : "By applying the components described in section 3 and 4, we evaluate the development trials in terms of EER, minimum Cprimary, and actual Cprimary. From the experiments\non development trials, we confirm that the propose ILVC and GL-norm approach works better than prior works in language mismatch conditions."
    }, {
      "heading" : "6. SUBSYSTEMS AND ITS FUSION FOR MAJOR LANGUAGE",
      "text" : "We try to use development dataset as much as possible because the development set is mirror the evaluation, so it would contain more valuable information than training dataset which language is English.\nEach subsystems has been applied most competitive techniques from the studies in previous sections for best result. Contrary to estimating the gender and language labels of unlabeled dataset for ILVC at Sec.5 and table 2, we use enrollment and test dataset of minor language and its labels\nfor ILVC. According this method, the performance of subsystems improved dramatically as table 3. Main reason is that we did not process each trials independently and use development enrollment and test dataset information on development trials. We expect that the performance of evaluation trials would not be improved dramatically like development trials, however, still convince that it would influence beneficial effects on systems for major languages. Submitted system has little difference on score normalization and usage of minor language labels.\nThe primary system is fusion of 5 subsystems of top 5 rows in table 3. We used entire minor dataset (enrollment, test and unlabeled) and unlabeled major dataset for GLnorm on both development and evaluation experiments.\nFor Contrastive 1 system, only unlabeled major dataset is used for GL-norm on both development and evaluation\nexperiment. Ground truth label was not used for ILVC, so the result is same with the last row table. 2 at GL-norm tab.\nContrastive 2 system is same with primary system. The difference is that only unlabeled major dataset is used for GL-norm on both development and evaluation experiment."
    }, {
      "heading" : "7. CPU EXECUTION TIME",
      "text" : "All tasks were performed on 64bit linux with 64G RAM and Intel i7 6700 3.4GHz and GTX1080 for GPU. All CPU times are counted based on one core CPU."
    }, {
      "heading" : "8. REFERENCES",
      "text" : "[1] N. Dehak, P. J. Kenny, R. Dehak, P. Dumouchel, and P.\nOuellet, “Front-End Factor Analysis for Speaker Verification,” IEEE Trans. Audio, Speech, Lang. Process., vol. 19, no. 4, pp. 788–798, May 2011.\n[2] D. Snyder, D. Garcia-Romero, and D. Povey, “Time delay\ndeep neural network-based universal background models for speaker recognition,” 2015 IEEE Work. Autom. Speech Recognit. Understanding, ASRU 2015 - Proc., no. 1232825, pp. 92–97, 2016.\n[3] F. Richardson, S. Member, D. Reynolds, and N. Dehak,\n“Deep Neural Network Approaches to Speaker and Language Recognition,” IEEE Signal Process. Lett., vol. 22, no. 10, pp. 1671–1675, 2015.\n[4] H. Aronowitz, “Inter dataset variability compensation for\nspeaker recognition,” in IEEE ICASSP, 2014, pp. 4002–4006.\n[5] D. Garcia-Romero and C. Y. Espy-Wilson, “Analysis of i-\nvector Length Normalization in Speaker Recognition Systems.,” in Interspeech, 2011, pp. 249–252.\n[6] D. Garcia-Romero, A. McCree, S. Shum, N. Brummer, and C.\nVaquero, “Unsupervised Domain Adaptation for I-Vector Speaker Recognition,” Proc. Odyssey 2014 - Speak. Lang. Recognit. Work., no. June, pp. 260–264, 2014.\n[7] S. Shum, N. Dehak, R. Dehak, and J. R. Glass,\n“Unsupervised Speaker Adaptation based on the Cosine Similarity for Text-Independent Speaker Verification,” Proc. Odyssey, 2010.\n[8] N. Brümmer and E. de Villiers, “The BOSARIS Toolkit:\nTheory, Algorithms and Code for Surviving the New DCF,” in NIST SRE’11 Analysis Workshop, 2011."
    } ],
    "references" : [ {
      "title" : "Front-End Factor Analysis for Speaker Verification",
      "author" : [ "N. Dehak", "P.J. Kenny", "R. Dehak", "P. Dumouchel", "P. Ouellet" ],
      "venue" : "IEEE Trans. Audio, Speech, Lang. Process., vol. 19, no. 4, pp. 788–798, May 2011.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Time delay deep neural network-based universal background models for speaker recognition",
      "author" : [ "D. Snyder", "D. Garcia-Romero", "D. Povey" ],
      "venue" : "2015 IEEE Work. Autom. Speech Recognit. Understanding, ASRU 2015 - Proc., no. 1232825, pp. 92–97, 2016.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Deep Neural Network Approaches to Speaker and Language Recognition",
      "author" : [ "F. Richardson", "S. Member", "D. Reynolds", "N. Dehak" ],
      "venue" : "IEEE Signal Process. Lett., vol. 22, no. 10, pp. 1671–1675, 2015.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Inter dataset variability compensation for speaker recognition",
      "author" : [ "H. Aronowitz" ],
      "venue" : "IEEE ICASSP, 2014, pp. 4002–4006.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Analysis of ivector Length Normalization in Speaker Recognition Systems",
      "author" : [ "D. Garcia-Romero", "C.Y. Espy-Wilson" ],
      "venue" : "Interspeech, 2011, pp. 249–252.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Unsupervised Domain Adaptation for I-Vector Speaker Recognition",
      "author" : [ "D. Garcia-Romero", "A. McCree", "S. Shum", "N. Brummer", "C. Vaquero" ],
      "venue" : "Proc. Odyssey 2014 - Speak. Lang. Recognit. Work., no. June, pp. 260–264, 2014.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Unsupervised Speaker Adaptation based on the Cosine Similarity for Text-Independent Speaker Verification",
      "author" : [ "S. Shum", "N. Dehak", "R. Dehak", "J.R. Glass" ],
      "venue" : "Proc. Odyssey, 2010.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "The BOSARIS Toolkit: Theory, Algorithms and Code for Surviving the New DCF",
      "author" : [ "N. Brümmer", "E. de Villiers" ],
      "venue" : "NIST SRE’11 Analysis Workshop, 2011.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "GMM-UBM (GU) For General i-vector extraction approach [1] by modifying Kaldi’s recipe (sre08/v1).",
      "startOffset" : 54,
      "endOffset" : 57
    }, {
      "referenceID" : 1,
      "context" : "SRE (04~10, part of 12) and Switchboard Dataset were used for training DNN-UBM and total variability matrix [2].",
      "startOffset" : 108,
      "endOffset" : 111
    }, {
      "referenceID" : 1,
      "context" : "Based on Kaldi’s recipe (sre10/v2), Supervised GMMUBM[2] was trained using posterior of TDNN network.",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 2,
      "context" : "BNF features were extracted using DNN which containing bottleneck layer [3].",
      "startOffset" : 72,
      "endOffset" : 75
    }, {
      "referenceID" : 3,
      "context" : "Inter Dataset Variability Compensation (IDVC)[4]",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 4,
      "context" : "Whitening Transform and Length Normalization using unlabeled dataset (WTLN)[5]",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 5,
      "context" : "Interpolated PLDA (SRE04-08) + PLDA (speaker clustering using AHC) (IPLDA) [6]",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 6,
      "context" : "S-norm [7]",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 7,
      "context" : "For calibration and fusion, simple linear calibration and linear fusion were done by Bosaris toolkit [8].",
      "startOffset" : 101,
      "endOffset" : 104
    } ],
    "year" : 2017,
    "abstractText" : "Korea University – Intelligent Signal Processing Lab. (KUISPL) developed speaker recognition system for SRE16 fixed training condition. Data for evaluation trials are collected from outside North America, spoken in Tagalog and Cantonese while training data only is spoken English. Thus, main issue for SRE16 is compensating the discrepancy between different languages. As development dataset which is spoken in Cebuano and Mandarin, we could prepare the evaluation trials through preliminary experiments to compensate the language mismatched condition. Our team developed 4 different approaches to extract i-vectors and applied state-of-the-art techniques as backend. To compensate language mismatch, we investigated and endeavored unique method such as unsupervised language clustering, inter language variability compensation and gender/language dependent score normalization.",
    "creator" : "Microsoft® Word 2013"
  }
}