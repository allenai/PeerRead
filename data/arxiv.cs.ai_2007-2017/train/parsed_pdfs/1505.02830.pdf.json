{
  "name" : "1505.02830.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Adapting Improved Upper Confidence Bounds for Monte-Carlo Tree Search",
    "authors" : [ "Yun-Ching Liu", "Yoshimasa Tsuruoka" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 5.\n02 83\n0v 1\n[ cs\n.A I]\n1 1\nM ay\n2 01\n5"
    }, {
      "heading" : "1 Introduction",
      "text" : "The development of Monte-Carlo Tree Search (MCTS) has made significant impact on various fields of computer game play, especially the field of computer Go [6]. The UCT algorithm [3] is an MCTS algorithm that combines the UCB algorithm [4] and MCTS, by treating each node as a single instance of the multiarmed bandit problem. The UCT algorithm is one of the most prominent variants of the Monte-Carlo Tree Search [6].\nRecently, various investigations have been carried out on exploring the possibility of applying other bandit algorithms to MCTS. The application of simple regret minizing bandit algorithms has shown the potential to overcome some weaknesses of the UCT algorithm [7]. The sequential halving on trees (SHOT) [8] applies the sequential halving algorithm [11] to MCTS. The SHOT algorithm has various advantages over the UCT algorithm, and has demonstrated better performance on the game of NoGo. The H-MCTS algorithm [9] performs selection by the SHOT algorithm for nodes that are near to the root and the UCT algorithm for deeper nodes. H-MCTS has also shown superiority over the UCT in games such as 8 × 8 Amazons and 8 × 8 AtariGo. Applications of the KLUCB [12] and Thompson sampling [13] to MCTS have also been investigated and produced some interesting results[10].\nThe improved UCB algorithm [2] is a modification of the UCB algorithm, and it has been shown that the improved UCB algorithm has a tighter regret upper bound than the UCB algorithm. In this research, we will explore the possibility of applying the improved UCB algorithm to MCTS. However, some characteristics of the improved UCB algorithm may not be desirable for a direct application to MCTS. Therefore, we have made some modifications to the improved UCB algorithm, making it more suitable for the task of game tree search. We will demonstrate the impact and implications of the modifications we have made on the improved UCB algorithm in an empirical study under the conventional multi-armed bandit problem setting. We will introduce the Mi-UCT algorithm, which is the application of the modified improved UCB algorithm to MCTS. We will demonstrate the performance of the Mi-UCB algorithm on the game of 9×9 Go and 9× 9 NoGo, which has shown to outperform the plain UCT when given a small number of playouts, and roughly on the same level when more playouts are given.\nAlgorithm 1 The Improved UCB Algorithm [2]\nInput: A set of arms A, total number of trials T Initialization: Expected regret ∆0 ← 1, a set of candidates arms B0 ← A for rounds m = 0, 1, · · · , ⌊ 1\n2 log2\nT e ⌋ do\n(1) Arm Selection: for all arms ai ∈ Bm do\nfor nm = ⌈ 2 log(T∆2 m )\n∆2 ⌉ times do\nsample the arm ai and update its average reward wi end for\nend for\n(2) Arm Elimination: amax ← MaximumRewardArm(Bm) for all arms ai ∈ Bm do\nif (wi + √\nlog(T∆2) 2nm\n) < (wmax − √\nlog(T∆2) 2nm ) then\nremove ai from Bm end if\nend for\n(3) Update ∆m ∆m+1 = ∆m\n2\nend for"
    }, {
      "heading" : "2 Applying Modified Improved UCB Algorithm to Trees",
      "text" : "In this section we will first introduce the improved UCB algorithm. We will then proceed to make some modifications to the improved UCB algorithm, and finally show how to apply the modified algorithm to Monte-Carlo Tree Search."
    }, {
      "heading" : "2.1 Improved UCB Algorithm",
      "text" : "In the multi-armed bandit problem (MAB), a player is faced with a K-armed bandit, and the player can decide to pull one of the arms at each play. The bandit will produce a reward r ∈ [0, 1] according to the arm that has been pulled. The distribution of the reward of each arm is unknown to the player. The objective of the player is to maximize the total amount of reward over T plays. Bandit algorithms are policies that the player can follow to achieve this goal. Equivalent to maximizing the total expected reward, bandit algorithms aim to minimize the cumulative regret, which is defined as\nRt = ∑T t=1 r ∗ − rIt ,\nwhere r∗ is the expected mean reward of the optimal arm, and rIt is the received reward when the player chooses to play arm It ∈ K at play t ∈ T . If a bandit algorithm can restrict the cumulative regret to the order of O(log T ), it is said to be optimal [1]. The UCB algorithm [4], which is used in the UCT algorithm [3], is an optimal algorithm which restricts the cumulative regret to O(K log(T ) ∆\n), where ∆ is the difference of expected reward between a suboptimal arm and the optimal arm. The improved UCB algorithm [2] is a modification of the UCB algorithm, and it can further restrict the growth of the cumulative regret to the order of O(K log(T∆ 2)\n∆ ).\nThe improved UCB algorithm, shown in Algorithm 1, essentially maintains a candidate set Bm of potential optimal arms, and then proceeds to systematically eliminate arms which are estimated to be suboptimal from that set. A predetermined number of total plays T is given to the algorithm, and the plays are further divided into ⌊ 12 log2(Te )⌋ rounds. Each round consists of three major steps. In the first step, the algorithm samples each arm that is in the candidate set nm = ⌈ 2 log(T∆ 2 m )\n∆2 m ⌉ times. Next, the algorithm proceeds to remove the arms whose upper bounds of estimated expected reward are less than the lower bound of the current best arm. The estimated difference ∆m is then halved in the final step. After each round, the expected reward of the arm ai is effectively estimated as\nwi ± √ log(T∆2 m )\n2nm = wi ±\n√\nlog(T∆2 m )·∆2 m\n4 log(T∆2 m ) = wi ± ∆m 2 ,\nwhere wi is the current average reward received from arm ai. In the case when the total number of plays T is not predetermined, the improved UCB algorithm can be run in an episodic manner; a total of T0 = 2 plays is given to algorithm in the initial episode, and the number of plays of subsequent episodes is given by Tℓ+1 = T 2 ℓ .\nAlgorithm 2 Modified Improved UCB Algorithm\nInput: A set of arms A, total number of trials T Initialization: Expected regret ∆0 ← 1, arm count Nm ← |A|, plays till ∆k update T∆0 ← n0 · Nm, where n0 ← ⌈ 2 log(T∆20)\n∆2 0\n⌉, number of times arm ai ∈ A has been\nsampled ti ← 0.\nfor rounds m = 0, 1, · · ·T do\n(1)Sample Best Arm:\namax ← arg max i∈|A| (wi +\n√\nlog(T∆2 k )·ri\n2nk ), where ri =\nT ti\nwmax ← UpdateMaxWinRate(A) ti ← ti + 1\n(2) Arm Count Update: for all arms ai do\nif (wi + √ log(T∆2 k )\n2nk ) < (wmax −\n√\nlog(T∆2 k )\n2nk ) then\nNm ← Nm − 1 end if\nend for\n(3) Update ∆k when Deadline T∆k is Reached if m ≥ T∆k then\n∆k+1 = ∆k\n2\nnk+1 ← ⌈ 2 log(T∆2 k+1)\n∆2 k+1\n⌉\nT∆k+1 ← m+ (nk+1 ·Nm) k ← k + 1\nend if end for"
    }, {
      "heading" : "2.2 Modification of the Improved UCB Algorithm",
      "text" : "Various characteristics of the improved UCB algorithm might be problematic for its application to MCTS:\n– Early explorations. The improved UCB algorithm tries to find the optimal arm by the process of elimination. Therefore, in order to eliminate suboptimal arms as early as possible, it has the tendency to devote more plays to suboptimal arms in the early stages. This might not be ideal when it comes to MCTS, especially in situations when time and resources are rather restricted, because it may end up spending most of the time exploring irrelevant parts of the game tree, rather than searching deeper into more promising subtrees. – Not an anytime algorithm. The improved UCB algorithm requires the total number of plays to be specified beforehand, and its major properties or theoretical guarantees may not hold if it is stopped prematurely. Since we are considering each node as a single instance of the MAB problem in MCTS, internal nodes which are deeper in the tree are most likely the instances that are prematurely stopped. The “temporal” solutions provided by these nodes might be erroneous, and the effect of these errors may be magnified as they propagate upward to the root node. On the other hand, it would be rather expensive to ensure the required conditions are met for the improved UCB algorithms on each node, because the necessary amount of playouts will grow exponentially as the number of expanded node increases.\nTherefore, we have made some adjustments to the improved UCB algorithm before applying it to MCTS.\nThe modified improved UCB bandit algorithm is shown in Algorithm 2. The modifications try to retain the major characteristics of the improved UCB algorithm, especially the way the confidence bounds are updated and maintained. Nonetheless, we should note that these modifications will change the algorithm’s behaviour, and the theoretical guarantees of the original algorithmmay no longer be applicable.\nAlgorithmic Modifications We have made two major adjustments to the algorithmic aspect of the improved UCB algorithm:\n1. Greedy optimistic sampling. We only sample the arm that currently has the highest upper bound, rather than sampling every possible arm nm times. 2. Maintain candidate arm count. We will only maintain the count of potential optimal arms, instead of maintaining a candidate set.\nSince we are only sampling the current best arm, we are effectively performing a more aggressive arm elimination; arms that are perceived to be suboptimal are not being sampled. Therefore, there is no longer a need for maintaining a candidate set.\nAlgorithm 3 Modified Improved UCB Algorithm applied to Trees (Mi-UCT)\nfunction Mi-UCT(Node N) bestucb ← −∞ for all child nodes ni of N do\nif ni.t = 0 then ni.ucb ← ∞ else ri ← N.episodeUpdate/ni.t\nni.ucb ← n.w + √ log(N.T×N.∆2)×ri 2N.k\nend if if bestucb ≤ ni.ucb then\nbestucb ← ni.ucb nbest ← ni\nend if end for\nif nbest.times = 0 then result ←RandomSimulation((nbest)) else if nbest is not yet expanded then NodeExpansion((nbest)) result ← Mi-UCT((nbest)) end if\nN.w ← (N.w ×N.t + result)/(N.t+ 1) N.t ← N.t+ 1\nif N.t ≥ N.T then N.∆ ← 1 N.T ← N.t+N.T ×N.T N.armCount ← Total number of child nodes N.k ← ⌈ 2 log(N.T×N.∆ 2)\nN.∆2 ⌉\nN.deltaUpdate ← N.t+N.k ×N.armCount end if\nif N.t ≥ N.deltaUpdate then for all child nodes ni of N do\nif (ni.w + √ log(N.T×N.∆2) 2n.k ) < (N.w − √ log(N.T×N.∆2) 2n.k\n) then N.armCount ← N.armCount − 1\nend if end for\nN.∆ ← N.∆ 2 N.k ← ⌈ 2 log(N.T×N.∆ 2)\nN.∆2 ⌉\nN.deltaUpdate ← N.t+N.k ×N.armCount end if return result\nend function\nfunction NodeExpansion(Node N) N.∆ ← 1 N.T ← 2 N.armCount ← Total number of child nodes N.k ← ⌈ 2 log(N.t×N.∆ 2)\nN.∆2 ⌉\nN.deltaUpdate ← N.k ×N.armCount end function\nHowever, the confidence bound in the improved UCB algorithm for arm ai\nis defined as wi ± √ log(T∆2 m )\n2nm , and the updates of ∆m and nm are both dictated\nby the number of plays in each round, which is determined by (|Bm| · nm), i.e., the total number of plays that is needed to sample each arm in the candidate set Bm for nm times. Therefore, in order to update the confidence bound we will need to maintain the count of potential optimal arms.\nThe implication of sampling the current best arm is that the guarantee for the estimated bound wi ± ∆m to hold will be higher than the improved UCB algorithm, because the current best will likely be sampled more or equal to nm times. This is desirable in game tree search, since it would be more efficient to verify a variation is indeed the principal variation, than trying to identify and verify others are suboptimal.\nConfidence Bound Modification Since we have modified the algorithm to sample only the current best arm, the confidence bound for the current best arm should be tighter than other arms. Hence, an adjustment to the confidence bound is also needed.\nIn order to reflect the fact that the current best arm is sampled more than other arms, we have modified the definition of the confidence bound for arm ai to\nwi ± √ log(T∆2 m )·ri\n2nm ,\nwhere the factor ri = T ti , and ti is the number of times that the arm has been sampled. The more arm ai is sampled, the smaller ri will be, and hence the tighter is the confidence bound. Therefore, the expected reward of arm ai will be estimated as\nwi ± √ log(T∆2 m )·ri\n2nm = wi ±\n√\nlog(T∆2 m )·∆2 m ·ri\n4 log(T∆2 m ) = wi ± ∆m 2 √ ri = wi ± ∆m2 √ T ti .\nSince it would be more desirable that the total number of plays is not required beforehand, we will run the modified improved UCB algorithm in an episodic fashion when we apply it to MCTS, i.e., assigning a total of T0 = 2 plays to the algorithm in the initial episode, and Tℓ+1 = T 2 ℓ plays in the subsequent episodes. After each episode, all the relevant terms in the confidence bound, such as ∆m and nm, will be re-initialized, and hence information from previous episodes will be lost. Therefore, in order to “share” information across episodes, we will not re-initialize ri after each episode."
    }, {
      "heading" : "2.3 Modified Improved UCB applied to Trees (Mi-UCT)",
      "text" : "We will now introduce the application of the modified improved UCB algorithm to Monte-Carlo Tree Search, or the Mi-UCT algorithm. The details of the MiUCT algorithm are shown in Algorithm 3.\nThe Mi-UCT algorithm adopts the same game tree expansion paradigm as the UCT algorithm, that is, the game tree is expanded over a number of iterations, and each iteration consists of four steps: selection, expansion, simulation, and backpropagation [3]. The difference is that the tree policy is replaced by the modified improved UCB algorithm. The modified improved UCB on each node is run in an episodic manner; a total of T0 = 2 plays to the algorithm in the initial episode, and Tℓ+1 = T 2 ℓ plays in the subsequent episodes.\nThe Mi-UCT algorithm keeps track of when N.∆ should be updated and the starting point of a new episode by using the variables N.deltaUpdate and N.T , respectively. When the number of playouts N.t of the node N reaches the updating deadline N.deltaUpdate, the algorithm halves the current estimated regret N.∆ and calculates the next deadline for halving N.∆. The variable N.T marks the starting point of a new episode. Hence, when N.t reaches N.T , the related variables N.∆ and N.armCount are re-initialized, and the starting point N.T of the next episode, along with the new N.deltaUpdate are calculated."
    }, {
      "heading" : "3 Experimental Results",
      "text" : "We will first examine how the various modifications we have made to the improved UCB algorithm affect its performance on the multi-armed bandit problem. Next, we will demonstrate the performance of the Mi-UCT algorithm against the plain UCT algorithm on the game of 9× 9 Go and 9× 9 NoGo."
    }, {
      "heading" : "3.1 Performance on Multi-armed Bandits Problem",
      "text" : "The experimental settings follow the multi-armed bandit testbed that is specified in [5]. The results are averaged over 2000 randomly generated K-armed bandit tasks. We have set K = 60 to simulate more closely the conditions in which bandit algorithms will face when they are applied in MCTS for games that have a middle-high branching factor. The reward distribution of each bandit is a normal (Gaussian) distribution with the mean wi, i ∈ K, and variance 1. The mean wi of each bandit of every generated K-armed bandit task was randomly selected according to a normal distribution with mean 0 and variance 1.\nThe cumulative regret and optimal action percentage are shown in Figure 1 and Figure 2, respectively. The various results correspond to different algorithms as follows:\n– UCB: the UCB algorithm. – I-UCB: the improved UCB algorithm. – I-UCB (episodic): the improved UCB algorithm ran episodically. – Modified I-UCB (no r): only algorithmic modifications on the improved\nUCB algorithm. – Modified I-UCB (no r, episodic): only algorithmic modifications on the\nimproved UCB algorithm ran episodically. – Modified I-UCB: both algorithmic and confidence bound modifications on\nthe improved UCB algorithm.\n– Modified I-UCB (episodic): both algorithmic and confidence bound modifications on the improved UCB algorithm ran episodically.\nContrary to theoretical analysis, we are surprised to observe the original improved UCB, both I-UCB and I-UCB(episodic), produced the worst cumulative regret. However, their optimal action percentages are increasing at a very rapid rate, and are likely to overtake the UCB algorithm if more plays are given. This suggests that the improved UCB algorithm does indeed devote more plays to exploration in the early stages.\nThe “slack” in the curves of the algorithms that were run episodically are the points when a new episode begins. Since the confidence bounds are essentially re-initialized after every episode, effectively extra explorations are performed. Therefore, there were extra penalties on the performance, and it can be clearly observed in the cumulative regret.\nWe can further see that by making only the algorithmic modification, to give Modified I-UCB (no r) and Modified I-UCB(no r, episodic), the optimal action percentage increases very rapidly, but it eventually plateaued and stuck to suboptimal arms. Their cumulative regret also increased linearly instead of logarithmically.\nHowever, by adding the factor ri to the confidence bound, the optimal action percentage increases rapidly and might even overtake the UCB algorithm if more plays are given. Although the optimal action percentage of the modified improved UCB, both Modified I-UCB and Modified I-UCB (episodic), are rapidly catching up with that of the UCB algorithm; there is still a significant gap between their cumulative regret."
    }, {
      "heading" : "3.2 Performance of Mi-UCT against Plain UCT on 9 × 9 Go",
      "text" : "We will demonstrate the performance of the Mi-UCT algorithm against the plain UCT algorithm on the game of Go played on a 9× 9 board.\nFor an effective comparison of the two algorithms, no performance enhancing heuristics were applied. The simulations are all pure random simulations without any patterns or simulation policies. A total of 1000 games were played for each constant C setting of the UCT algorithm, each taking turns to play Black. The total number of playouts was fixed to 1000, 3000, and 5000 for both algorithms.\nThe results are shown in Table 1. It can be observed that the performance of the Mi-UCT algorithm is quite stable against various constant C settings of the plain UCT algorithm, and is roughly on the same level. The Mi-UCT algorithm seems to have better performance when only 1000 playouts are given, but slightly deteriorates when more playouts are available."
    }, {
      "heading" : "3.3 Performance of Mi-UCT against Plain UCT on 9 × 9 NoGo",
      "text" : "We will demonstrate the performance of the the Mi-UCT algorithm against the plain UCT algorithm on the game of NoGo played on a 9 × 9 board. NoGo is\na misere version of the game of Go, in which the first player that has no legal moves other than capturing the opponent’s stone loses.\nAll the simulations are all pure random simulations, and no extra heuristics or simulation policies were applied. A total of 1000 games were played for each constant C setting of the UCT algorithm, each taking turns to play Black. The total number of playouts was fixed to 1000, 3000, and 5000 for both algorithms.\nThe results are shown in Table 2. We can observe that the Mi-UCT algorithm significantly dominates the plain UCT algorithm when only 1000 playouts were given, and the performance deteriorates rapidly when more playouts are available, although it is still roughly on the same level as the plain UCT algorithm.\nThe results on both 9× 9 Go and 9× 9 NoGo suggest that the performance of the Mi-UCT algorithm is comparable to that of the plain UCT algorithm, but scalability seems poorer. Since the proposed modified improved UCB algorithm essentially estimates the expected reward of each bandit by wi + ∆m 2 √ ri, where ri = √\nT ti , the exploration term converges slower than the of the UCB algorithm,\nand hence more exploration might be needed for the modified improved UCB confidence bounds to converge to a “good-enough” estimate value; this might be the reason why Mi-UCT algorithm has poor scalability. Therefore, we might able to overcome this problem by trying other definitions for ri."
    }, {
      "heading" : "4 Conclusion",
      "text" : "The improved UCB algorithm is a modification of the UCB algorithm, and has a better regret upper bound than the UCB algorithm. Various characteristics of the improved UCB algorithm, such as early exploration and not being an anytime algorithm, are not ideal for a direct application to MCTS. Therefore, we have made some modifications to the improved UCB algorithm, making it more suitable for the task of game tree search. We have investigated the impact and implications of each modification through an empirical study under the conventional multi-armed bandit problem setting.\nThe Mi-UCT algorithm is the application of the modified improved UCB algorithm applied to Monte-Carlo Tree Search. We have demonstrated that it outperforms the plain UCT algorithm on both games of 9 × 9 Go and 9 × 9 NoGo when only a small number of playouts are given, and on comparable level with increased playouts. One possible way of improving the scalability would be trying other definition of ri in the modified improved UCB confidence bounds.\nIt would also be interesting to investigate the possibility of enhancing the performance of the Mi-UCT algorithm by combining it with commonly used heuristics [6] or develop new heuristics that are unique to the Mi-UCT algorithm. Finally, since the modifications we have made essentially changed the behaviour of the original algorithm, investigation into the theoretical properties of our modified improved UCB algorithm may provide further insight into the relation between bandit algorithms and Monte-Carlo Tree Search."
    } ],
    "references" : [ {
      "title" : "Asymptotically efficient adaptive allocation rules",
      "author" : [ "T.L. Lai", "H. Robbins" ],
      "venue" : "Advances in applied mathematics 6 (1): 4",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1985
    }, {
      "title" : "UCB revisited: Improved regret bounds for the stochastic multi-armed bandit problem",
      "author" : [ "P. Auer", "R. Ortner" ],
      "venue" : "Periodica Mathematica Hungarica 61, pp. 1-2",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Bandit Based Monte-carlo Planning",
      "author" : [ "L. Kocsis", "C. Szepesvári" ],
      "venue" : "Proceedings of the 17th European Conference on Machine Learning (ECML’06), pp. 282-293",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Finite-time Analysis of the Multiarmed Bandit Problem.Machine Learning",
      "author" : [ "P. Auer", "N. Cesa-Bianchi", "P. Fischer" ],
      "venue" : "Issue",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2002
    }, {
      "title" : "Reinforcement Learning: An Introduction",
      "author" : [ "R.S. Sutton", "A.G. Barto" ],
      "venue" : "MIT Press, Cambridge, MA,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "A Survey of Monte Carlo Tree Search Methods",
      "author" : [ "C.B. Browne", "E. Powley", "D. Whitehouse", "S.M. Lucas", "P.I. Cowling", "P. Rohlfshagen", "S. Tavener", "D. Perez", "S. Samothrakis", "S. Colton" ],
      "venue" : "IEEE Trans. Comp. Intell. AI Games 4(1), pp. 1-43",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "MCTS Based on Simple Regret",
      "author" : [ "D. Tolpin", "S.E. Shimony" ],
      "venue" : "Proceedings of the 26th AAAI Conference on Artificial Intelligence, pp. 570-576",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Sequential Halving applied to Trees",
      "author" : [ "T. Cazenave" ],
      "venue" : "IEEE Trans. Comp. Intell. AI Games volPP, no.99, pp.1-1",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Minimizing Simple and Cumulative Regret in Monte-Carlo Tree Search",
      "author" : [ "T. Pepels", "T. Cazenave", "M.H.M. Winands", "M. Lanctot" ],
      "venue" : "Proceedings of Computer Games Workshop at the 21st European Conference on Artificial Intelligence",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Applying Multi Armed Bandit Algorithms to MCTS and Those Analysis",
      "author" : [ "T. Imagawa", "T. Kaneko" ],
      "venue" : "Proceedings of the 19th Game Programming Workshop (GPW-14), pp.145-150",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Almost Optimal Exploration in Multi-Armed Bandits",
      "author" : [ "Z. Karnin", "T. Koren", "S. Oren" ],
      "venue" : "Proceedings of the 30th International Conference on Machine Learning (ICML’13), pp.1238-1246",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Cappe, A.:The KL-UCB algorithm for bounded stochastic bandits and beyond",
      "author" : [ "A. Garivier" ],
      "venue" : "Proceedings of 24th Annual Conference on Learning Theory (COLT ’11),",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2011
    }, {
      "title" : "Thompson Sampling: An Asymptotically Optimal Finite-Time Analysis",
      "author" : [ "E. Kaufmann", "N. Korda", "R. Munos" ],
      "venue" : "Proceedings of 23rd Algorithmic Learning Theory (ALT’12), pp.199-213",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "[2], with MCTS.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "The development of Monte-Carlo Tree Search (MCTS) has made significant impact on various fields of computer game play, especially the field of computer Go [6].",
      "startOffset" : 155,
      "endOffset" : 158
    }, {
      "referenceID" : 2,
      "context" : "The UCT algorithm [3] is an MCTS algorithm that combines the UCB algorithm [4] and MCTS, by treating each node as a single instance of the multiarmed bandit problem.",
      "startOffset" : 18,
      "endOffset" : 21
    }, {
      "referenceID" : 3,
      "context" : "The UCT algorithm [3] is an MCTS algorithm that combines the UCB algorithm [4] and MCTS, by treating each node as a single instance of the multiarmed bandit problem.",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 5,
      "context" : "The UCT algorithm is one of the most prominent variants of the Monte-Carlo Tree Search [6].",
      "startOffset" : 87,
      "endOffset" : 90
    }, {
      "referenceID" : 6,
      "context" : "The application of simple regret minizing bandit algorithms has shown the potential to overcome some weaknesses of the UCT algorithm [7].",
      "startOffset" : 133,
      "endOffset" : 136
    }, {
      "referenceID" : 7,
      "context" : "The sequential halving on trees (SHOT) [8] applies the sequential halving algorithm [11] to MCTS.",
      "startOffset" : 39,
      "endOffset" : 42
    }, {
      "referenceID" : 10,
      "context" : "The sequential halving on trees (SHOT) [8] applies the sequential halving algorithm [11] to MCTS.",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 8,
      "context" : "The H-MCTS algorithm [9] performs selection by the SHOT algorithm for nodes that are near to the root and the UCT algorithm for deeper nodes.",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 11,
      "context" : "Applications of the KLUCB [12] and Thompson sampling [13] to MCTS have also been investigated and produced some interesting results[10].",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 12,
      "context" : "Applications of the KLUCB [12] and Thompson sampling [13] to MCTS have also been investigated and produced some interesting results[10].",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 9,
      "context" : "Applications of the KLUCB [12] and Thompson sampling [13] to MCTS have also been investigated and produced some interesting results[10].",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 1,
      "context" : "The improved UCB algorithm [2] is a modification of the UCB algorithm, and it has been shown that the improved UCB algorithm has a tighter regret upper bound than the UCB algorithm.",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 1,
      "context" : "Algorithm 1 The Improved UCB Algorithm [2]",
      "startOffset" : 39,
      "endOffset" : 42
    }, {
      "referenceID" : 0,
      "context" : "The bandit will produce a reward r ∈ [0, 1] according to the arm that has been pulled.",
      "startOffset" : 37,
      "endOffset" : 43
    }, {
      "referenceID" : 0,
      "context" : "If a bandit algorithm can restrict the cumulative regret to the order of O(log T ), it is said to be optimal [1].",
      "startOffset" : 109,
      "endOffset" : 112
    }, {
      "referenceID" : 3,
      "context" : "The UCB algorithm [4], which is used in the UCT algorithm [3], is an optimal algorithm which restricts the cumulative regret to O( log(T ) ∆ ), where ∆ is the difference of expected reward between a suboptimal arm and the optimal arm.",
      "startOffset" : 18,
      "endOffset" : 21
    }, {
      "referenceID" : 2,
      "context" : "The UCB algorithm [4], which is used in the UCT algorithm [3], is an optimal algorithm which restricts the cumulative regret to O( log(T ) ∆ ), where ∆ is the difference of expected reward between a suboptimal arm and the optimal arm.",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 1,
      "context" : "The improved UCB algorithm [2] is a modification of the UCB algorithm, and it can further restrict the growth of the cumulative regret to the order of O( log(T∆ )",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 2,
      "context" : "The Mi-UCT algorithm adopts the same game tree expansion paradigm as the UCT algorithm, that is, the game tree is expanded over a number of iterations, and each iteration consists of four steps: selection, expansion, simulation, and backpropagation [3].",
      "startOffset" : 249,
      "endOffset" : 252
    }, {
      "referenceID" : 4,
      "context" : "The experimental settings follow the multi-armed bandit testbed that is specified in [5].",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 5,
      "context" : "It would also be interesting to investigate the possibility of enhancing the performance of the Mi-UCT algorithm by combining it with commonly used heuristics [6] or develop new heuristics that are unique to the Mi-UCT algorithm.",
      "startOffset" : 159,
      "endOffset" : 162
    } ],
    "year" : 2015,
    "abstractText" : "The UCT algorithm, which combines the UCB algorithm and Monte-Carlo Tree Search (MCTS), is currently the most widely used variant of MCTS. Recently, a number of investigations into applying other bandit algorithms to MCTS have produced interesting results. In this research, we will investigate the possibility of combining the improved UCB algorithm, proposed by Auer et al. [2], with MCTS. However, various characteristics and properties of the improved UCB algorithm may not be ideal for a direct application to MCTS. Therefore, some modifications were made to the improved UCB algorithm, making it more suitable for the task of game tree search. The Mi-UCT algorithm is the application of the modified UCB algorithm applied to trees. The performance of Mi-UCT is demonstrated on the games of 9× 9 Go and 9× 9 NoGo, and has shown to outperform the plain UCT algorithm when only a small number of playouts are given, and rougly on the same level when more playouts are available.",
    "creator" : "gnuplot 4.4 patchlevel 3"
  }
}