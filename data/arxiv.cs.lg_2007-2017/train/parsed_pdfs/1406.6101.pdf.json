{
  "name" : "1406.6101.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Improved Frame Level Features and SVM Supervectors Approach for the Recogniton of Emotional States from Speech: Application to categorical and dimensional states",
    "authors" : [ "Imen Trabelsi", "Dorra Ben Ayed", "Noureddine Ellouze" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "system is to classify speaker's utterances into different\nemotional states such as disgust, boredom, sadness,\nneutral and happiness.\nSpeech features that are commonly used in speech emotion recognition (SER) rely on global utterance level\nprosodic features. In our work, we evaluate the impact\nof frame-level feature extraction. The speech samples\nare from Berlin emotional database and the features\nextracted from these utterances are energy, different variant of mel frequency cepstrum coefficients (MFCC),\nvelocity and acceleration features. The idea is to explore\nthe successful approach in the literature of speaker\nrecognition GMM-UBM to handle with emotion\nidentification tasks. In addition, we propose a\nclassification scheme for the labeling of emotions on a continuous dimensional-based approach.\nIndex Terms—speech emotion recognition, valence,\narousal, MFCC, GMM Supervector, SVM\nI. INTRODUCTION\nSpeech emotion recognition (SER) is an extremely challenging task in the domain of human-robot interfaces and affective computing and has various applications in call centers [1] , intelligent tutoring systems [2], spoken language research [3] and other research areas. The primary channels for robots to recognize human’s emotion include facial expressions, gesture and body posture. Among these indicators, the speech is considered as a rapid transfer of complex information. This signal provides a strong interface for communication with computers. Many kind of acoustic features have been explored to build the emotion models [4].\nVarious classification methods have been verified for emotional pattern classification such as hidden markov models [5], gaussian mixture [6], artificial neural network [7] and support vector machines [8]. In our paper, we investigate the relationship between generative method based GMM and discriminative method based SVM [9, 10]. In addition, we present two\napproaches, a categorical-based approach, modeling emotions in terms of distinct and discrete categories and a dimensional-based approach, modeling emotions on a continuous space, in which an emotion is mapped within a bipolar dimension: valence and arousal. The valence dimension refers to how positive or negative emotion is. The arousal dimension refers to how excited or not excited emotion is (see fig.3). This concept has gained much attention in recent years.\nThe rest of paper is organized as follows: First, the description of the proposed speech emotion recognition system. Second, the experimental results of the system. Conclusion is drawn in the final section.\nII. EMOTION RECOGNITION SYSTEM\nThe proposed speech emotion recognition system contains three main modules (see fig.1) namely (1) extraction of feature, (2) learning the models using machine learning techniques and (3) evaluation of models. First, suitable data sets for training and testing are collected. Second, relevant features are extracted. Third, the extracted features are modeled. Fourth, a set of machine learning techniques could be used to learn the training models. Finally, testing unknown emotional samples are used to evaluate the performances of the models."
    }, {
      "heading" : "A. Feature extraction",
      "text" : "The first problem that occur when trying to build a recognition framework is the discrimination of the features to be used. Common acoustic features used to build the emotion model include pitch, intensity, voice quality features and formants [9]. Others include cepstral analysis [4]. These features can be divided into two categories: utterance-level features [10] and framelevel features [11].\nIn this paper, our feature extractor is based on: Mel Frequency Cepstral Coefficients (MFCCs), MFCC-low, energy, velocity and acceleration coefficients. They are extracted on the frame level.\n MFCCs have been the most popular low-level features. They demonstrate good performance in speech and speaker recognition. We use the advantage of this representation for our emotion identification task.\n MFCC-Low are a variant of MFCC. Mel filter banks are placed in [20-300] Hz. Our reason for introducing MFCC-low was to represent pitch variation.\n Energy is an important prosodic feature of speech. It is, often referred to as the volume or intensity of the speech, is also known to contain valuable information [13]. Studies have shown that short term energy has been one of the most important features which provide information that can be used to distinguish between different sets of emotions.\n Velocity (delta) and acceleration (delta-delta) parameters have been shown to play an important role in capturing the temporal characteristics between the different frames that can contribute to a better discrimination [14]. The time derivative is approximated by differentiating between frames after and before the current. It has become common to combine both dynamic features and static features."
    }, {
      "heading" : "B. The acoustic emotion gaussians model",
      "text" : "GMMs have been successfully employed in emotion recognition [15]. The probability density function of the feature space for each emotion is modeled with a weighted mixture of simple gaussian components.\n,1 ( ) ( ; ).\nN\ni i ii P x W N x     (1)\nwhere N(; ,) is the gaussian density function, wi, µi and i are the weight, mean and covariance matrix of the i-th gaussian component, respectively.\nThis module is assured by the construction of a universal background model (UBM), which is trained over all emotional classes. There are a number of different parameters involved in the UBM training process, which are the mean vector, covariance matrix and the weight.\nThese parameters are estimated using the iterative expectation-maximization (EM) algorithm [17]. Each\nemotional utterance is then modeled separately by adapting only the mean vectors of UBM using Maximum A Posteriori (MAP) criterion [18], while the weights and covariance matrix were set to the corresponding parameters of the UBM. To use a whole utterance as a feature vector, we transform the acoustic vector sequence to a single vector of fixed dimension. This vector is called supervector and it takes the form as:\n(2)\nThis transformation allows the production of features with a fixed dimension for all the utterances. Therefore, we can use the GMM supervectors as input for SVM classifier."
    }, {
      "heading" : "C. SVM Classification Algorithm",
      "text" : "The support vector machines (SVM) [19] are supervised learning machines that find the maximum margin hyperplane separating two classes of data. SVM solve non-linear problems by projecting the input features vectors into a higher dimensional space by means of a mercer kernel.\nThis powerful tool is explored for discriminating the emotions using GMM mean supervectors. The reason for choosing the SVM classifier for this task is that, it will provide better discrimination even with a high dimension feature space. In our research, we give each training supervector sample with the corresponding emotion class label. After that, we input them to the SVM classifier and gain a SVM emotional model. The output of each model is given to the decision logic. The model having the best score determines the emotion statue. The output of the matching step is a posteriori probability.\nIn this work, we investigate two SVM kernels in the proposed GMM supervector based SVM: linear and gaussian RBF kernels. The two kernels, take the form as equations (3) and (4) respectively.\n( , ) . .i ik x v x v (3)\n21( , ) exp ( ) .\n2 i ik x v x v \n     \n  (4)\nwhere x is the input data, vi are the support vectors and is the width of the radial basis function.\nWe select in each experiment the best of the two kernels. One against one strategy is used for multi-class\nclassification.\nOur experiments are implemented using the LibSvm [20]. The whole speech emotion recognition is shown in Fig. 2.\nIII. EXPERIMENTS AND RESULTS"
    }, {
      "heading" : "A. Emotional speech database",
      "text" : "The database used in this paper is the Berlin database of emotional speech (EMO-DB) which is recorded by speech workgroup leaded in the anechoic chamber of the Technical University in Berlin. It is a simulated open source speech database. This database contains about 500 speech samples proven from ten professional native German actors (5 actors and 5 actresses), to simulate 7 different emotions.\nThe length of the speech samples varies from 2 seconds to 8 seconds. Table 1 summarizes the different emotions."
    }, {
      "heading" : "B. System Description",
      "text" : "The data were recorded at a sample rate of 16 KHZ and a resolution of 16 bits. First, the signal is segmented into speech and silence. Then, silence segments are thrown away and the speech segments are pre-emphasized with a coefficient of 0.95. From preemphasized speech, each feature vector is extracted from at 8 ms shift using a 16 ms analysis window. A hamming window is applied to each signal frame to reduce signal discontinuity.\nOur baseline system is built using 128 UBM gaussian component from the acoustic data of different emotional sentences. Individual emotion models are MAP-adapted. Only the mean vectors are adapted with a relevance factor of 16."
    }, {
      "heading" : "C. Results and discussion",
      "text" : "1) Categorical emotion results\nIn these experiments, we diverse emotions labeling to discrete states."
    }, {
      "heading" : "TABLE.2 RECOGNITION RATE FROM DIFFERENT VARIANT OF MFCC",
      "text" : "Combination of MFCC and MFCC-low led to an accuracy of 81.35%. MFCC-low features perform well in comparison with the small scale of filter banks used, it may be due to its ability to capture voice source\nquality variations. For the rest of the paper, we choose the combined MFCC (CMFCC).\nThe table below (table 3) shows the full feature set used for evaluation."
    }, {
      "heading" : "TABLE3. DIFFERENT SPEECH FEATURE VECTORS",
      "text" : "We can observe that negative emotion (sadness, boredom, disgust, fear) got the highest classification rate; this could be attributed to the exaggerated expression of emotion by the actors. The lowest rate was for the neutral synthesized speech at 50%, this cloud be explained by the fact that neutral speech doesn’t contain specific emotional information.\nThe addition of energy is beneficial for emotions like anger (from 89.47% to 97.37%), disgust (from 92.31% to 100%) and sadness (from 81.47% to 84.21%). The addition of derivatives significantly improves the recognition rate at the happy emotion (from 71.43 to 80.95). We also conclude that GMM SVM achieves higher recognition rate even when the training data size is small (45 utterances for sadness).\nMore detailed results in the confusion matrix (table 5), are further shown to analyze the confusion between different emotions associated with MFCC features. The columns show the emotions that the system tried to induce, and the rows are the output recognized emotions.\nFrom these results, we can see that happiness and anger are the most frequently confused emotions. The confusion is also noted between neutral and boredom. This matrix reveals that there are similarities between different categories of emotions that we will try to understand in the rest of the paper.\nTABLE. 5 MISCLASSIFICATION BETWEEN 7 DIFFERENT EMOTIONAL\nSTATES"
    }, {
      "heading" : "Recognized As Ang Disg Fear Happ Neut Sad Bor",
      "text" : "Ang 34 1 0 2 1 0 0\nDisg 0 12 0 0 0 1 0\nFear 0 0 20 1 0 0 0\nHapp 5 0 1 15 0 0 0\nNeut 0 1 0 0 5 0 8\nSad 0 0 0 0 0 17 2\nBor 0 0 0 0 0 1 23\n2) Dimensional emotion results\nIn these experiments, we diverse emotions labeling to binary arousal and valence. The confusion matrix, which can be seen in table 6, illustrates the classifications of the two arousal classes individually (high vs low). The recognition rate is 98.24% for low arousal and 97.84 % for high arousal. As can be seen, high and low emotions are easy classified.\nTABLE. 6 CONFUSION MATRIX OF AROUSAL CLASSIFICATION\nRecognized as High Low\nHigh 91 2 Low 1 56\nAccuracy(%) 97,85 98,24\nIn valence, there are 3 classes which are positive, neutral and negative. We will classify the affective states into these classes. The obtained recognition rate is 100% for negative, 21.42% for neutral and 57.14% for positive on the valence dimension separately. The worst performance is observed in classifying the neutral state. The table highlights that positive and neutral emotions were confused with negative emotions with the same arousal characteristics."
    }, {
      "heading" : "Recognized as Negative Neutral Positive",
      "text" : ""
    }, {
      "heading" : "TABLE.8 CONFUSION MATRIX :NEGATIVE VS POSITIVE",
      "text" : "Finally, we make a distinction between emotional and neutral speech. As can be seen in table 9, the phrases belonging to neutral state are totally misclassified."
    }, {
      "heading" : "TABLE.9 CONFUSION MATRIX :EMOTIONAL VS NEUTRAL",
      "text" : "Recognized as Emotional Neutral\nEmotional 108 7\nNeutral 14 0\nAccuracy(%) 93,91 0\nIn contrast to arousal, recognition of valence seems to be very challenging, resulting in no more than 60%. Thus it appears that some emotional states share similar acoustic characteristics which make it difficult to discriminate between these emotions.\nPositive emotions are poorly recognized, this is due to the fact that happiness which is a positive expression is generally confused with anger which is a negative expression. Given that these two emotions have exactly the same highest rating on the dimension of arousal that suggests that arousal plays an important role in the recognition of emotions. This is one of the reasons why acoustic discriminability on the valence dimension is\nstill problematic: there are no strong discriminative speech features available to discriminate between positive speech and negative speech.\nOn the other hand, acoustic features are more discriminative between aroused speech (e.g., anger) and not aroused speech (e.g., sadness).\nIV. CONCLUSION\nEmotional speech recognition is gaining interest due to the widespread applications into various fields.\nIn our work, this task has been evaluated using frame level features, modeled by GMM-SVM and tested on EMO-DB. Results showed that MFCC, with filter banks placed in [0-3400] extracted at the frame level outperform the traditional MFCC.\nResults in emotion recognition experiments are hard to compare, because different database designs are used. Some use elicited speech, whereas others collect spontaneous emotions, some are multi-speaker and others are not. Different basic emotions sets are considered and different data sets are used. Possibly the work presented in [21] is the closest to this one, as acted speech with the same list of emotions. In this paper, Schwenker describe the use of EMO-DB and utilize RASTA-PLP features. Recognition accuracy obtained is 79%.\nIn addition, we investigated separability on the valence dimension and on the arousal dimension. We found that the arousal dimension seems to be better modeled than the valence dimension.\nRecognizing emotions by computer with high recognition accuracy still remains a challenge due to the lack of a full understanding of emotion in human minds. The problem is extremely complicated and thus, the researchers usually deal with acted emotions, just like in our paper. However, in real situations, different individuals show their emotions in a diverse degree and manner. In our future work, we will try to study the performance of the proposed system in a spontaneous emotional database. We will explore the possibilities of integrating other modalities such as manual gestures and facial expression and combine with the result of some other machine learning methods such as KNN, HMM or Random Forest."
    }, {
      "heading" : "Emotion Negative Positive",
      "text" : "Negative 94 0\nPositive 13 8\nAccuracy(%) 100 38,09\n[4] D. Ververidis and C. Kotropoulos, Emotional speech recognition: Resources, features, and methods. Speech Communication, vol. 48, no. 9, pp. 1162–1181, 2006.\n[5] T.New, S. Foo and L. D. Silva, Speech emotion recognition using hidden Markov models. Speech Communication, vol. 41, pp. 603–623, 2003.\n[6] D. Neiberg, K. Elenius, and K. Laskowski, Emotion recognition in spontaneous speech using GMMs, in Proc. INTERSPEECH, 2006.\n[7] T.-L. Pao, Y.-T. Chen, and J.-H. Yeh, Mandarin emotional speech recognition based on SVM and NN. In Proc of the 18th International Conference on Pattern Recognition (ICPR'06), vol. 1, pp. 1096- 1100, 2006.\n[8] Y. Lin, and G. Wei, Speech emotion recognition based on HMM and SVM. In Proc. of 2005 International Conference on Machine Learning and Cybernetics, vol. 8, pp. 4898-4901.\n[9] I. Trabelsi and D. BenAyed, Evaluation d’une approche hybride GMM-SVM pour l’identification de locuteurs. La revue e-STA, 8(1), 61-65, 2011. http://www.see.asso.fr/esta/?page=8&id_document =458&id_article=231.\n[10] I. Trabelsi and D. Ben Ayed, On the use of different feature extraction methods for linear and non linear kernels, in Proc. Of Sciences of Electronics, Technologies of Information and Telecommunications (SETIT 2012), pp. 797 – 802, 2012\n[11] J. S. Park, J. H. Kim and Y. H. Oh, J. Feature Vector Classification based Speech Emotion Recognition for Service Robots, IEEE Trans. on Consumer Electronics. 55, 1590-1596, 2009.\n[12] D. Bitouk, R. Verma, A. Nenkova, Class-level spectral features for emotion recognition，Speech Communication. 52, pp. 613-625, 2010.\n[13] R. Fernandez and R.W. Picard, Classical and Novel Discriminant Features for Affect Recognition from Speech. In Proc. Of InterSpeech , pp. 1-4, Lisbon, Portugal, 2005.\n[14] S.N. Wrigley, G. J. Brown, V. Wanand and S. Renals, Speech and crosstalk detection in multichannel audio. In IEEE Transactions on Speech and Audio Processing, 2005.\n[15] J. Bouvrie, J. Ezzat, and T. Poggio, Localized Spectro-Temporal Cepstral Analysis of Speech. In Proc. ICASSP 2008, pp. 4733-4736, 2008.\n[16] B. Vlasenko, B.Schuller, A. Wendemuth and G. Rigoll, Frame vs. turn-level: emotion recognition from speech considering static and dynamic processing. In Proc. of Affective Computing and Intelligent Interaction, pages 139–147, Lisbon, Portugal, 2007\n[17] A. P. Dempster, N. M. Laid, and D. Durbin, Maximum Likelihood from incomplete data via the EM algorithm. J. Royal Statistical Soc, vol. 39, pp. 1-38, 1977.\n[18] D. Reynolds, T. Quatieri, and R. Dunn, Speaker verification using adapted gaussian mixture models. DSP, Vol. 10, No. 3, pp. 19–41, 2000.\n[19] V. Vapnik, The nature of statistical learning theory. Spring-verlag, New York, 2005.\n[20] C. C. Chang and C. J . Lin (2001). LIBSVM : a library for support vector machines. Available: http://www.csie.ntu.edu.tw/~cjlin/libsvm.\n[21] F. Schwenker, S. Scherer, Y.M. Magdi, G. Palm, The GMM-SVM supervector approach for the recognition of the emotional status from speech. In: Alippi, C.,Polycarpou, M., Panayiotou, C., Ellinas, G. (eds.) ICANN 2009, Part I. LNCS, vol. 5768, pp. 894–903. Springer, Heidelberg, 2009"
    }, {
      "heading" : "Imen Trabelsi",
      "text" : "I. Trabelsi received a university diploma in computer\nscience in 2009 from the High Institute of Management\nof Tunis (ISG-Tunisia), the MS degree signal processing\nin 2011 from the Institute of Computer Science of Tunis\n(ISI-Tunisia). She is currently working towards the Ph.D. degree in electrical engineering (signal\nprocessing) at the National School of Engineer of Tunis\n(ENIT). Her areas of interests are speech processing,\npattern recognition, emotion recognition and speaker\nrecognition.\nE-mail: trabelsi.imen1@gmail.com"
    }, {
      "heading" : "Dorra Ben Ayed Mezghani",
      "text" : "D. Ayed Mezghani received computer science\nengineering degree in 1995 from the National School Computer Science (ENSI-Tunisia), the MS degree in\nelectrical engineering (signal processing) in 1997 from\nthe National School of Engineer of Tunis (ENITTunisia),\nthe Ph. D. degree in electrical engineering (signal\nprocessing) in 2003 from (ENIT-Tunisia).\nShe is currently an associate professor in the computer science department at the High Institute of Computer\nScience of Tunis (ISI-Tunisia). Her research interests\ninclude fuzzy logic, support vector machines, artificial\nintelligence, pattern recognition, speech recognition and\nspeaker identification.\nE-mail:Dorra.mezghani@isi.rnu.tn, DorraInsat@yahoo.fr"
    }, {
      "heading" : "Noureddine Ellouze",
      "text" : "N. Ellouze received a Ph.D. degree in 1977 from l’Institut National Polytechnique at Paul Sabatier\nUniversity (Toulouse-France), and Electronic Engineer\nDiploma from ENSEEIHT in 1968 at the same\nUniversity. In 1978, Dr. Ellouze joined the Department\nof Electrical Engineering at the National School of\nEngineer of Tunis (ENIT–Tunisia), as assistant professor in statistic, electronic, signal processing and computer\narchitecture. In 1990, he became Professor in signal\nprocessing digital signal processing and stochastic\nprocess. He has also served as director of electrical\ndepartment at ENIT from 1978 to 1983, general\nmanager and president of the Research Institute on Informatics and Telecommunication IRSIT from 1987-\n1990, and president of the Institute in 1990-1994. He is\nnow Director of Signal Processing Research Laboratory\nLSTS at ENIT, and is in charge of Control and Signal\nProcessing Master degree at ENIT. Pr Ellouze is IEEE\nfellow since 1987; he directed multiple Masters and Thesis and published over 200 scientific papers both in\njournals and proceedings. He is chief editor of the\nscientific journal Annales Maghrébines de l’Ingénieur.\nHis research interest include neural networks and fuzzy\nclassification, pattern recognition, signal processing and\nimage processing applied in biomedical, multimedia,\nand man machine communication. E-mail:N.Ellouze@enit.rnu.tn"
    } ],
    "references" : [ {
      "title" : "Ensemble methods for spoken emotion recognition in callcentres",
      "author" : [ "M. Donn", "W. Ruili", "C. Liyanage" ],
      "venue" : "Speech communication, vol. 49",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "A Robust Multi-Modal Emotion Recognition Framework for Intelligent Tutorig Systems",
      "author" : [ "L. Xiao", "J. Yadegar", "N. Kamat" ],
      "venue" : "IEEE International Conference on Advanced Learning Technologies (ICALT)",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Using bigrams to identify relationships between student certainness states and tutor responses in a spoken dialogue corpus",
      "author" : [ "K. Forbes", "D. Litman" ],
      "venue" : "Proc. Of 6th SIGdial Workshop on Discourse and Dialogue, Lisbon, Portugal",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Emotional speech recognition: Resources",
      "author" : [ "D. Ververidis", "C. Kotropoulos" ],
      "venue" : "features, and methods. Speech Communication, vol. 48, no. 9, pp. 1162–1181",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Speech emotion recognition using hidden Markov models",
      "author" : [ "T.New", "S. Foo", "L.D. Silva" ],
      "venue" : "Speech Communication,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2003
    }, {
      "title" : "Emotion recognition in spontaneous speech using GMMs",
      "author" : [ "D. Neiberg", "K. Elenius", "K. Laskowski" ],
      "venue" : "Proc. INTERSPEECH",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Mandarin emotional speech recognition based on SVM and NN",
      "author" : [ "T.-L. Pao", "Y.-T. Chen", "J.-H. Yeh" ],
      "venue" : "Proc of the 18th International Conference on Pattern Recognition (ICPR'06), vol. 1, pp. 1096- 1100",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Speech emotion recognition based on HMM and SVM",
      "author" : [ "Y. Lin", "G. Wei" ],
      "venue" : "In Proc. of 2005 International Conference on Machine Learning and Cybernetics,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2005
    }, {
      "title" : "Evaluation d’une approche hybride GMM-SVM pour l’identification de locuteurs",
      "author" : [ "I. Trabelsi", "D. BenAyed" ],
      "venue" : "La revue e-STA, 8(1), 61-65",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "On the use of different feature extraction methods for linear and non linear kernels",
      "author" : [ "I. Trabelsi", "D. Ben Ayed" ],
      "venue" : "Proc. Of Sciences of Electronics, Technologies of Information and Telecommunications ",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "J",
      "author" : [ "J.S. Park", "J.H. Kim", "Y.H. Oh" ],
      "venue" : "Feature Vector Classification based Speech Emotion Recognition for Service Robots, IEEE Trans. on Consumer Electronics. 55, 1590-1596",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A",
      "author" : [ "D. Bitouk", "R. Verma" ],
      "venue" : "Nenkova, Class-level spectral features for emotion recognition,Speech Communication. 52, pp. 613-625",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Classical and Novel Discriminant Features for Affect Recognition from Speech",
      "author" : [ "R. Fernandez", "R.W. Picard" ],
      "venue" : "Proc. Of InterSpeech , pp. 1-4, Lisbon, Portugal",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Speech and crosstalk detection in multichannel audio",
      "author" : [ "S.N. Wrigley", "G.J. Brown", "V. Wanand", "S. Renals" ],
      "venue" : "IEEE Transactions on Speech and Audio Processing",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Localized Spectro-Temporal Cepstral Analysis of Speech",
      "author" : [ "J. Bouvrie", "J. Ezzat", "T. Poggio" ],
      "venue" : "Proc. ICASSP 2008, pp. 4733-4736",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Frame vs. turn-level: emotion recognition from speech considering static and dynamic processing",
      "author" : [ "B. Vlasenko", "B.Schuller", "A. Wendemuth", "G. Rigoll" ],
      "venue" : "In Proc. of Affective Computing and Intelligent Interaction,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2007
    }, {
      "title" : "Maximum Likelihood from incomplete data via the EM algorithm",
      "author" : [ "A.P. Dempster", "N.M. Laid", "D. Durbin" ],
      "venue" : "J. Royal Statistical Soc, vol. 39, pp. 1-38",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1977
    }, {
      "title" : "Speaker verification using adapted gaussian mixture models",
      "author" : [ "D. Reynolds", "T. Quatieri", "R. Dunn" ],
      "venue" : "DSP, Vol. 10, No. 3, pp. 19–41",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "The nature of statistical learning theory",
      "author" : [ "V. Vapnik" ],
      "venue" : "Spring-verlag, New York",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "LIBSVM : a library for support vector machines",
      "author" : [ "C.C. Chang", "C. J . Lin" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2001
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Speech emotion recognition (SER) is an extremely challenging task in the domain of human-robot interfaces and affective computing and has various applications in call centers [1] , intelligent tutoring systems [2], spoken language research [3] and other research areas.",
      "startOffset" : 175,
      "endOffset" : 178
    }, {
      "referenceID" : 1,
      "context" : "Speech emotion recognition (SER) is an extremely challenging task in the domain of human-robot interfaces and affective computing and has various applications in call centers [1] , intelligent tutoring systems [2], spoken language research [3] and other research areas.",
      "startOffset" : 210,
      "endOffset" : 213
    }, {
      "referenceID" : 2,
      "context" : "Speech emotion recognition (SER) is an extremely challenging task in the domain of human-robot interfaces and affective computing and has various applications in call centers [1] , intelligent tutoring systems [2], spoken language research [3] and other research areas.",
      "startOffset" : 240,
      "endOffset" : 243
    }, {
      "referenceID" : 3,
      "context" : "Many kind of acoustic features have been explored to build the emotion models [4].",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 4,
      "context" : "Various classification methods have been verified for emotional pattern classification such as hidden markov models [5], gaussian mixture [6], artificial neural network [7] and support vector machines [8].",
      "startOffset" : 116,
      "endOffset" : 119
    }, {
      "referenceID" : 5,
      "context" : "Various classification methods have been verified for emotional pattern classification such as hidden markov models [5], gaussian mixture [6], artificial neural network [7] and support vector machines [8].",
      "startOffset" : 138,
      "endOffset" : 141
    }, {
      "referenceID" : 6,
      "context" : "Various classification methods have been verified for emotional pattern classification such as hidden markov models [5], gaussian mixture [6], artificial neural network [7] and support vector machines [8].",
      "startOffset" : 169,
      "endOffset" : 172
    }, {
      "referenceID" : 7,
      "context" : "Various classification methods have been verified for emotional pattern classification such as hidden markov models [5], gaussian mixture [6], artificial neural network [7] and support vector machines [8].",
      "startOffset" : 201,
      "endOffset" : 204
    }, {
      "referenceID" : 8,
      "context" : "In our paper, we investigate the relationship between generative method based GMM and discriminative method based SVM [9, 10].",
      "startOffset" : 118,
      "endOffset" : 125
    }, {
      "referenceID" : 9,
      "context" : "In our paper, we investigate the relationship between generative method based GMM and discriminative method based SVM [9, 10].",
      "startOffset" : 118,
      "endOffset" : 125
    }, {
      "referenceID" : 8,
      "context" : "Common acoustic features used to build the emotion model include pitch, intensity, voice quality features and formants [9].",
      "startOffset" : 119,
      "endOffset" : 122
    }, {
      "referenceID" : 3,
      "context" : "Others include cepstral analysis [4].",
      "startOffset" : 33,
      "endOffset" : 36
    }, {
      "referenceID" : 9,
      "context" : "These features can be divided into two categories: utterance-level features [10] and framelevel features [11].",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 10,
      "context" : "These features can be divided into two categories: utterance-level features [10] and framelevel features [11].",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 19,
      "context" : "Mel filter banks are placed in [20-300] Hz.",
      "startOffset" : 31,
      "endOffset" : 39
    }, {
      "referenceID" : 12,
      "context" : "It is, often referred to as the volume or intensity of the speech, is also known to contain valuable information [13].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 13,
      "context" : " Velocity (delta) and acceleration (delta-delta) parameters have been shown to play an important role in capturing the temporal characteristics between the different frames that can contribute to a better discrimination [14].",
      "startOffset" : 221,
      "endOffset" : 225
    }, {
      "referenceID" : 14,
      "context" : "The acoustic emotion gaussians model GMMs have been successfully employed in emotion recognition [15].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 16,
      "context" : "These parameters are estimated using the iterative expectation-maximization (EM) algorithm [17].",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 17,
      "context" : "Each emotional utterance is then modeled separately by adapting only the mean vectors of UBM using Maximum A Posteriori (MAP) criterion [18], while the weights and covariance matrix were set to the corresponding parameters of the UBM.",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 18,
      "context" : "SVM Classification Algorithm The support vector machines (SVM) [19] are supervised learning machines that find the maximum margin hyperplane separating two classes of data.",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 19,
      "context" : "Our experiments are implemented using the LibSvm [20].",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 0,
      "context" : "Results showed that MFCC, with filter banks placed in [0-3400] extracted at the frame level outperform the traditional MFCC.",
      "startOffset" : 54,
      "endOffset" : 62
    }, {
      "referenceID" : 1,
      "context" : "Results showed that MFCC, with filter banks placed in [0-3400] extracted at the frame level outperform the traditional MFCC.",
      "startOffset" : 54,
      "endOffset" : 62
    }, {
      "referenceID" : 2,
      "context" : "Results showed that MFCC, with filter banks placed in [0-3400] extracted at the frame level outperform the traditional MFCC.",
      "startOffset" : 54,
      "endOffset" : 62
    }, {
      "referenceID" : 3,
      "context" : "Results showed that MFCC, with filter banks placed in [0-3400] extracted at the frame level outperform the traditional MFCC.",
      "startOffset" : 54,
      "endOffset" : 62
    }, {
      "referenceID" : 4,
      "context" : "Results showed that MFCC, with filter banks placed in [0-3400] extracted at the frame level outperform the traditional MFCC.",
      "startOffset" : 54,
      "endOffset" : 62
    }, {
      "referenceID" : 5,
      "context" : "Results showed that MFCC, with filter banks placed in [0-3400] extracted at the frame level outperform the traditional MFCC.",
      "startOffset" : 54,
      "endOffset" : 62
    }, {
      "referenceID" : 6,
      "context" : "Results showed that MFCC, with filter banks placed in [0-3400] extracted at the frame level outperform the traditional MFCC.",
      "startOffset" : 54,
      "endOffset" : 62
    }, {
      "referenceID" : 7,
      "context" : "Results showed that MFCC, with filter banks placed in [0-3400] extracted at the frame level outperform the traditional MFCC.",
      "startOffset" : 54,
      "endOffset" : 62
    }, {
      "referenceID" : 8,
      "context" : "Results showed that MFCC, with filter banks placed in [0-3400] extracted at the frame level outperform the traditional MFCC.",
      "startOffset" : 54,
      "endOffset" : 62
    }, {
      "referenceID" : 9,
      "context" : "Results showed that MFCC, with filter banks placed in [0-3400] extracted at the frame level outperform the traditional MFCC.",
      "startOffset" : 54,
      "endOffset" : 62
    }, {
      "referenceID" : 10,
      "context" : "Results showed that MFCC, with filter banks placed in [0-3400] extracted at the frame level outperform the traditional MFCC.",
      "startOffset" : 54,
      "endOffset" : 62
    }, {
      "referenceID" : 11,
      "context" : "Results showed that MFCC, with filter banks placed in [0-3400] extracted at the frame level outperform the traditional MFCC.",
      "startOffset" : 54,
      "endOffset" : 62
    }, {
      "referenceID" : 12,
      "context" : "Results showed that MFCC, with filter banks placed in [0-3400] extracted at the frame level outperform the traditional MFCC.",
      "startOffset" : 54,
      "endOffset" : 62
    }, {
      "referenceID" : 13,
      "context" : "Results showed that MFCC, with filter banks placed in [0-3400] extracted at the frame level outperform the traditional MFCC.",
      "startOffset" : 54,
      "endOffset" : 62
    }, {
      "referenceID" : 14,
      "context" : "Results showed that MFCC, with filter banks placed in [0-3400] extracted at the frame level outperform the traditional MFCC.",
      "startOffset" : 54,
      "endOffset" : 62
    }, {
      "referenceID" : 15,
      "context" : "Results showed that MFCC, with filter banks placed in [0-3400] extracted at the frame level outperform the traditional MFCC.",
      "startOffset" : 54,
      "endOffset" : 62
    }, {
      "referenceID" : 16,
      "context" : "Results showed that MFCC, with filter banks placed in [0-3400] extracted at the frame level outperform the traditional MFCC.",
      "startOffset" : 54,
      "endOffset" : 62
    }, {
      "referenceID" : 17,
      "context" : "Results showed that MFCC, with filter banks placed in [0-3400] extracted at the frame level outperform the traditional MFCC.",
      "startOffset" : 54,
      "endOffset" : 62
    }, {
      "referenceID" : 18,
      "context" : "Results showed that MFCC, with filter banks placed in [0-3400] extracted at the frame level outperform the traditional MFCC.",
      "startOffset" : 54,
      "endOffset" : 62
    }, {
      "referenceID" : 19,
      "context" : "Results showed that MFCC, with filter banks placed in [0-3400] extracted at the frame level outperform the traditional MFCC.",
      "startOffset" : 54,
      "endOffset" : 62
    } ],
    "year" : 2014,
    "abstractText" : "The purpose of speech emotion recognition system is to classify speaker's utterances into different emotional states such as disgust, boredom, sadness, neutral and happiness. Speech features that are commonly used in speech emotion recognition (SER) rely on global utterance level prosodic features. In our work, we evaluate the impact of frame-level feature extraction. The speech samples are from Berlin emotional database and the features extracted from these utterances are energy, different variant of mel frequency cepstrum coefficients (MFCC), velocity and acceleration features. The idea is to explore the successful approach in the literature of speaker recognition GMM-UBM to handle with emotion identification tasks. In addition, we propose a classification scheme for the labeling of emotions on a continuous dimensional-based approach.",
    "creator" : "Microsoft® Office Word 2007"
  }
}