{
  "name" : "1502.05090.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "REAL TIME CLUSTERING OF TIME SERIES USING TRIANGULAR POTENTIALS",
    "authors" : [ "Aldo Pacchiano", "Oliver J. Williams" ],
    "emails" : [ "aldopacchiano@gmail.com", "oliver.williams@markhamrae.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "KEYWORDS Clustering, Expected Utility, Graphical Models, k-Clique Problem"
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "A common problem in finance is the question of how best to construct a diversified portfolio of investments. This problem is ubiquitous in fund management, banking and insurance and has led to an extensive evolving literature, both theoretical and empirical. From an informal mathematical perspective the central challenge is to devise a method for determining weightings for a set of random variables such that ex post realisations of the weighted sum optimise some objective function on average. The objective function most typically used in financial economics is a concave utility function, hence from an ex ante pespective the portfolio construction problem is a matter of optimising so-called expected utility. Koller and Friedman provide a detailed discussion of utility functions and decision theory in the general machine learning context [1]. The theoretical literature analyses many alternative weighting strategies which can be distinguished based on such criteria as: (a) the investor’s time horizon (e.g. does utility depend on realisations on a single time horizon in a ‘one-shot’ scenario or does uncertainty resolve over multiple time periods, affording the investor opportunities to alter portfolio composition dynamically?), (b) the nature of the information available to investors regarding the distribution of future returns (this may be extremely limited or highly-structured for mathematical expediency), and (c) the investor’s particular utility function (where, for instance, it can be shown that curvature can be interpreted as representing the investor’s risk-preferences [2]).\nOne of the most prominent theoretical results is the concept of mean-variance efficiency which has its roots in the work of Markowitz [3]: the idea is that in a one period model (under certain restrictive assumptions) if investors seek to maximize return and minimise portfolio variance, the optimal ex ante weighting vector is given by\n(1)\nwhere is the covariance matrix of future returns, is the mean vector of expected returns, is a risk-aversion parameter and is the risk-free rate of return [4]. A key aspect of this formula is the dependency on the inverse of the covariance matrix which is never known with certainty and will in practice be a forecast in its own right (and the same will be true for\nand quite possibly ). When deploying this formula in real-world investment, practitioners are divided over how to account for parameter uncertainty, with a number of alternative approaches in common usage (including ignoring uncertainty entirely). Unfortunately it is widely recognised that the exact weightings in (1) have a sensitivity to covariance assumptions which is unacceptably high; in other words small changes in covariance assumptions can lead to large changes in prescribed weightings. Further significant concerns are raised by the fact that long time series are required to generate acceptable estimates for a large covariance matrix but financial returns series are notoriously non-stationary – it is therefore easy for an analyst to fall into the trap of thinking that they are applying prudent statistical methods when in reality their input data may be stale or entirely inappropriate. The forecasting of expected returns is also regarded as an exceptionally difficult task. In these circumstances one strand of literature considers simpler weighting schemes which are predicated on relatively few assumptions; one prominent example, popular with practitioners, is\nthe self-explanatory equally-weighted (or ) approach [5]. This method requires no explicit\nforecasts of correlation or returns and it can be shown that this is equivalent to mean-variance methods if the correlation between all possible pairs of investments is equal, along with all means and variances. Although this may be far from the truth it may be more innocuous to assume this than to suffer potentially negative effects of erroneous statistical forecasts and there is a body of empirical literature which demonstrates the efficiency of the approach [6]. Refinements to the basic method can include weighting each asset by the inverse of the forecast standard deviation of its returns (known as volatility) which allows some heterogeneity to be incorporated. Nevertheless it is intuitively obvious that such a simple method presents potential dangers of its own, and is particularly inappropriate when the universe of alternative investments contains subgroups of two or more investments which are highly correlated with each other. Suppose, for instance, a portfolio of investments in world stock market indices which includes several alternative indices for the United States (e.g. Dow Jones, S&P 500, Russell 2000) but only single indices for other markets (e.g. the CAC-40 for France, FTSE-100 for UK, etc.). In this\nsetting the approach may (arguably) significantly overweight US equities in comparison to\neach foreign market and in general regional weightings will be more dependent on the cardinality of available indices than any economic properties of the markets. In a systematic investment process it is clearly impractical to have analysts manually sift through investments to ensure an appropriate ‘balance’ (which defeats the object of a weighting algorithm) and indeed potential diversification benefits argue in favour of including a broad range of investments anyway. The contribution of this paper is to explore potential weighting methods based on clustering,\nsuch that highly ‘similar’ investments can be identified, grouped together and treated (for weighting purposes) as if they are a single ‘composite’ investment. By contrast, investments which exhibit relatively little similarity to each other are treated individually in their own right. Our focus here is on a process for identifying clusters rather than evaluation of ex post investment performance, which we leave for a separate analysis, and in fact we draw attention to the applicability of our methods to fields beyond finance where clustering may be required, e.g. well-known problems in biology, medicine and computer science. We also present an intriguing theoretical result arising from our work, which emphasises limitations of certain clustering techniques and may help to guide other researchers in their search for suitable methods. The paper is organised as follows: in Section 2 we formally specify the problem at hand, in Section 3 we demonstrate spectral clustering as a preliminary benchmark approach and in Section 4 we explore an alternative method based on a graphical model where we propose a specific estimation technique involving triangular potentials and provide illustrative examples. Section 5 briefly considers extension to a more dynamic setting (via a Hidden Markov Model) and Section 6 concludes."
    }, {
      "heading" : "2. PROBLEM SPECIFICATION",
      "text" : "Definition 1 Let\n€\nn∈N, define the set of natural numbers from to .\nLet be time series, where for .\nDefinition 2 Clustering.\nA clustering of is an equivalence relation\n€\n~ over such that: 1. Reflexivity: If\n€\ni ~ j then\n€\nj ~ i. 2. Transitivity: If\n€\ni ~ j and\n€\nj ~ k then\n€\nk ~ i . Definition 3 Time dependent clustering.\nWe say\n€\ni~ k j if and are clustered at time .\nOur aim is to find a sequence\n€\n{~ k }k=1 m , i.e. we allow the nature of the clustering relation to\nevolve over time.\nWe denote the distance between series at time as for all and the similarity\nat time defined as . The functions are specified by the user of the algorithm and may be chosen based on prior domain-specific knowledge, or perhaps by a more systematic process of searching across alternative specifications guided by out-of-sample performance. Definition 4 Distance Matrix.\nGiven a family of time-dependent distance functions , we define a family of\ndistance matrices as .\nDefinition 5 Similarity Matrix.\nGiven a family of time-dependent similarity functions , we define a family of\nsimilarity matrices as .\nDefinition 6 Similarization function.\nWe say is a similarization function if for any distance function\n, is a valid similarity function.\nIn what follows we restrict our attention to reflexive and non-negative distance and similarity functions and thus to symmetric similarity and distance matrices. We will also use the variable\nto represent the number of data points observed at each time step when the clustering algorithm will be applied."
    }, {
      "heading" : "3. SPECTRAL CLUSTERING",
      "text" : "Here we introduce the Spectral Clustering algorithm, which is suitable for data where the cluster structure does not change over time. Later in the paper we will compare the performance of our proposed approach with this benchmark method. Definition 7 The Laplacian matrix of a similarity matrix is defined as follows:\nwhere .\nThe most basic spectral clustering algorithm for bipartition of data is the Shi Malik bipartition algorithm which we describe below."
    }, {
      "heading" : "3.1. Shi Malik algorithm",
      "text" : "Given items and a similarity matrix , the Shi Malik algorithm bipartitions the data into two sets with and based on the eigenvector corresponding to the second smallest eigenvalue of the normalized Laplacian matrix of . Algorithm 1 The Shi Malik bipartition algorithm: 1. Compute the Laplacian from a similarity matrix. 2. Compute the second smallest eigenvalue and its corresponding eigenvector . 3. Compute the median of its corresponding eigenvector.\n4. All points whose component in is greater than are allocated to , the remaining points are allocated to .\nUnfortunately the Shi Malik algorithm is not a dynamic procedure, i.e. it is not intended to identify an underlying cluster structure which is time-varying. However various clustering approaches are available which specifically seek to address this and we outline one such approach next."
    }, {
      "heading" : "3.2. A generalized spectral clustering approach",
      "text" : "The following algorithm is an extension of the Shi Malik algorithm that can handle two or more clusters. It can be found at [7]. Given items and a similarity matrix the goal of Dynamic Spectral Clustering is to find a clustering\n€\n~ of .\nAlgorithm 2 Dynamic Spectral Clustering 1. Compute the Laplacian of the similarity matrix. 2. Compute the Laplacian’s eigenvalues and eigenvectors 3. Let be a desired number of clusters. 4. Find the eigenvectors of the corresponding eigenvalues found on the previous step. Let the corresponding matrix be called .\n5. Rotate , by multiplying it with an appropriate rotation matrix so each of the corresponding rows of have (ideally) only one nonzero entry. In reality the resulting matrix we will use the largest (in absolute value) entry of the matrix. is a rotation matrix in\n.\n6. The cluster to which point is assigned is .\nIn order to find an appropriate rotation matrix , there is a theorem that guarantees that any\nrotation matrix can be written as a product where\n€\nk = c(c −1) 2 and\neach equals a Givens rotation matrix.\nGivens rotation matrices are parameterized as follows:\nHence for each there is an associated angle and we represent these angles by the\nvector . In order to find the optimal for a given number of clusters , we use gradient descent on the following objective function:\nsubject to the constraint\nFollowing [7] we set .\nAs suggested by [7], the optimal number of clusters can be obtained by choosing the value of that maximizes a scoring function given by"
    }, {
      "heading" : "3.2.1. A dynamic clustering algorithm",
      "text" : "Given a family of time dependent similarity functions defining a family of\nsimilarity matrices , an optimal time-varying clustering structure\n€\n~ k can be\nestimated by applying Algorithm 2 at time using input similarity matrix . Hence for time series data we propose the following algorithm:\nAlgorithm 3 Let be time series. Where for . Let\nbe a window parameter, be a distance function and be a similarization function.\n1. Let be the distance matrix having\nfor every pair .\n2. Let be the similarity matrix having for every pair .\n3. Let\n€\n~ m\nbe the clustering resulting from running Algorithm 2 with input similarity matrix .\n4. Output clustering\n€\n~ m\n. Extensions of this approach include considering a geometric decay factor in the distance computation, alternative distance functions and different similarization functions. We tried various combinations as shown in Table 1 but found no significant improvement on the stability of the resulting clusters.\nWe did not consider a scenario where the distance or similarization functions change through time although there may be certain applications where this might be appropriate."
    }, {
      "heading" : "3.3. Overview",
      "text" : "We present the performance of this algorithm in Figure 1. Some of the observed characteristics of this method are the following: • The resulting clustering values are notably sensitive to the similarity function used in the model. • The clustering structure estimated by this method tends to be relatively unstable over time. Although in some applications this may be plausible, in the context of financial time series we have a strong prior belief that clusters typically arise due to common factors relating to economic fundamentals (e.g. similar commodities, currency pairs belonging to close trading partners, etc.) which would tend to change very slowly relative to the frequency of market data.\nwith : at each time step we generate random standard normal variates which are common\nto each cluster, then for each of the 3 returns we add independent Gaussian noise with a relatively small variance. The members of each cluster therefore have a large portion of\nrandomness in common, but each observation also includes its own independent noise. The cluster structure is randomly changed over time and represented by coloured bars in each row,\ni.e. all columns with the same colour belong in the same cluster."
    }, {
      "heading" : "4. GRAPHICAL MODEL APPROACH",
      "text" : "Instead of representing clusterings as a binary matrix such that if cluster as the authors of [8] do, we approach the problem in a different way. Consider a symmetric ( ) family of Bernoulli random variables, such that:\n€\nCi, j =1 if i, j are in the same cluster, or 0 otherwise.\nWe wish to learn a distribution over the ensemble . The model we will use in this paper is the following:\nwhere is a similarity matrix; in other words, we consider that the observed similarity between a pair of points will come from one of two distributions, depending on whether or not\nthe two points belong to the same cluster.\nIn what follows it will be useful to think of the matrix as an adjacency matrix. The\nresulting graph where and , has an edge between\nevery two nodes that are in the same cluster. Learning a distribution over can be\nthought of as learning a distribution over the set of undirected graphs with .\nThe goal of this section is to compute the following posterior:\nThe algorithms we present here output , the MAP estimator for the posterior. A short algebraic manipulation (Bayes Theorem) yields:\nSince is fixed:\nIn the following two sections we present different models for inference on the ensemble , their performance and their relationship to clusterings. The training data will be:\n1. A set of similarity matrices .\n2. The set of corresponding clusterings\n€\n{~ k } produced via a clustering algorithm such as\nthe ones described earlier in Section 3."
    }, {
      "heading" : "4.1. Exponential model",
      "text" : "As a starting point we propose the following model for the ensemble , in which we impose conditional independence assumptions between observed similarities. We therefore assume the following factorization:\nIn this model we assume , and . This is\nequivalent to assuming full pairwise independence of the variables and the conditionals\n.\nFor implementational purposes we assume are exponentially distributed and the\nare Bernoulli random variables.\n4.1.1. Training\n€\n{~ k } can be translated into a training sequence of ensemble values via the\ntransformation if\n€\ni~ k j . Because of the independence assumptions underlying this\nmodel, the ML estimate for the posterior distribution of the ensemble can be computed by obtaining the ML estimate for each of the distributions and . The ML\nestimate for the rate parameter of equals the inverse of the sample mean, and the\nML estimate for the mean of equals the sample frequency of . More formally:\nObservation 1 Define . And let .\nThe ML estimator of the parameters for the posterior distribution\nhas\n€\nP(Si, j |Ci, j ) ~ exp(λi, j ) and ."
    }, {
      "heading" : "4.1.2. Prediction",
      "text" : "Prediction under this model is performed by finding the MAP assignment for the ensemble and turning it into a clustering. is obtained by maximizing each likelihood\nindependently:\nFor the ensemble assignment we output a clustering composed of a cluster for each connected component of the graph corresponding to . Results are presented in Figure 2.\nThe prediction algorithm is linear."
    }, {
      "heading" : "4.1.3. Limitations",
      "text" : "Consider the following joint posterior distribution over clusterings of .\n€\np(⋅) = 0.1 if ⋅ = (1,2,3) 0.41 if ⋅ = (1,2),(3) 0.41 if ⋅ = (1,3),(2) 0 if ⋅ = (2,3),(1) 0.17 if ⋅ = (1),(2),(3) ⎧ ⎨ ⎪ ⎪ ⎪\n⎩\n⎪ ⎪ ⎪\nThe marginals\n€\np((1,2)), p((2,3)) > 0.5 . The current algorithm will output ."
    }, {
      "heading" : "4.2. Triangular Potentials",
      "text" : "The main limitation of the approach described in the previous section is that there is potential for spurious large clusters to emerge solely from the independent optimization of the potentials. If the marginal probability is large, it is likely that the MAP of the ensemble will\nhave regardless of the values of any of the other similarities or clustering\nassignments . It is also possible for the algorithm to suggest cluster shapes which are intuitively implausible (and do not conform to prior notions of cluster structure which may be appropriate to a particular domain); we illustrate this in Figure 3.\nWe therefore proceed to address these issues by a modification to the basic model as described by the following observations: Observation 2 is a valid clustering\n€\n~ if, for all triplets of distinct numbers , .\nObservation 3 is a valid clustering if the graph whose adjacency matrix equals is\ncomposed of a disjoint union of cliques. In this section we assume the following factorization:\n(2)\nwhere\n€\nΨi, j ,k 3 (Ci, j ,Ci,k,C j,k ) = 0 if Ci, j =Ci,k =1,C j ,k = 0 0 if Ci, j =C j,k =1,Ci,k = 0 0 if Ci,k =C j,k =1,Ci, j = 0 1 otherwise ⎧ ⎨ ⎪ ⎪ ⎩ ⎪ ⎪\nThis has the effect of turning into a potential function such that all the assignments of the joint distribution of the ensemble with a nonzero probability are valid clusterings."
    }, {
      "heading" : "4.2.1. Training algorithm",
      "text" : "We use the same construction for the univariate and bivariate potentials as the one used in the previous section. The distribution over clusterings will vary because the triangular potentials restrict the mass of the distribution to the space of valid clusterings. It is of course also possible to add other potentials relating different sets of clustering variables although we leave that direction for future research."
    }, {
      "heading" : "4.2.2. Prediction algorithms",
      "text" : "This model can be thought of as an undirected graphical model with variables for\nand and edges , , and for all . If the\nvariable is identified with the point , then there is an edge between every two variables on the same vertical line and between every two variables on the same horizontal line. We tackle the problem of obtaining the MAP assignment over clusterings under this model using either the Elimination Algorithm or MCMC. To obtain an estimate for the MAP assignment using MCMC we sample from the posterior and output the clustering arrangement which appears most often. The MCMC chain construction is described in the next section. By construction there is a clique of size along the horizontal line for As a consequence, the elimination algorithm has an exponential running time over this graphical model. Similarly, there are no easy theoretical guarantees for the performance of the MCMC\nmethod. In particular, it is possible for the probability mass over the optimal assignment to be so small that there are no concentration inequalities to guarantee that the proposed algorithm will output the MAP with high probability in polynomial time. In the following section we show this behavior is not only a result of the graphical model formulation or our proposed algorithm but an intrinsic limitation of the model itself."
    }, {
      "heading" : "4.3. Results and Limitations",
      "text" : "We next apply the classic sumproduct algorithm or the MAP elimination algorithm to find the best clustering, with results shown in Figure 4, however the drawbacks are that this solution becomes intractable as the number of products becomes large. The elimination algorithm could\nbe worst case which becomes intractable quite fast."
    }, {
      "heading" : "4.3.1. Theoretical limitations",
      "text" : "Let be an ensemble of probabilities with such that and\n. Define a distribution over simple graphs via\nLet\n€\nˆ P (G) = P(G | G is a disjoint union of cliques).\nIt is easy to see that finding the MAP assignment for the distribution defined via Equation (2) is equivalent to finding the MAP assignment for with:\nSince it is conceivable that any arrangement of the values can result from the training data, the two problems are equivalent.\nIn what follows we talk interchangeably of the MAP assignment of\n€\nqi, j * ∈{ ˆ p i, j ,1− ˆ p i, j}( ) and the graph defined by and\n. The complement of contains all those pairs for which\n.\nTheorem 1 If there is a polynomial time algorithm for finding the MAP assignment over then P = NP.\nProof. Let be an algorithm for finding the MAP over the distribution as\ndefined by . We show can be used to construct an algorithm for solving the\nclique problem. The clique problem is the problem of deciding whether a graph has a clique of size where both and are inputs to be specified. If was polynomial, the algorithm we propose for clique would run in polynomial time. Because clique is NP complete we conclude the existence of would imply P = NP. The following algorithm solves clique:\nAlgorithm 4 Inputs: .\nLet and .\n1. Construct with and\n. The edges of equal\nall edges in , plus all possible edges between and and all possible edges among\nelements of .\n2. For all pairs define:\n3. Let be the output edges in the MAP assignment from .\n4. If output , else output . This step runs in polynomial\ntime because every connected component of is a clique graph. The probability of the MAP assignment equals\nwhich can be written as a product of the product of the chosen probabilities of\npairs belonging to , a cross component of probabilities from and a component of probabilities from . By construction, the edges in but not in are not chosen. The MAP restricted to and is a disjoint union of cliques. Because\nwe can conclude:\n1. The MAP assignment restricted to must be a complete graph: Suppose the MAP\nrestricted to had more than one component, say , with\ntheir (possibly empty) corresponding clique intersections in . It can be shown\nvia the rearrangement inequality that the MAP must have . Let MAP1 be the assignment obtained via joining into (the complete graph on ) and\nreconnecting all to . If , a simple counting argument shows that\n. The latter, and imply that\n, a contradiction.\n2. The edges must connect with one of the largest cliques of .\nThe correctness of the algorithm follows. The algorithm above runs in polynomial time, provided is in P."
    }, {
      "heading" : "5. EXTENSIONS",
      "text" : ""
    }, {
      "heading" : "5.1. HMM",
      "text" : "Because the training procedure we propose is done over fully annotated data, more sophisticated and time-dependent models can be explored. We propose a generalization of the previous models via an HMM. In this model, each hidden state is a clustering and the transition probabilities are obtained from the sampled frequencies of the transitions in the training phase. When the hidden states of the training data are known, the ML estimate of the transition probabilities of an HMM equals the transitions sample frequencies. The results of applying this method are shown in Figure 5, where it is apparent that relatively good performance is achieved.\n.\nThe version implemented here is hard-coded for only series and therefore only possible\nclustering states. The length of the chain can be adjusted as desired."
    }, {
      "heading" : "5.2. Coagulation Fragmentation",
      "text" : "The underlying chain for the MCMC sampler uses a fragmentation coagulation process to walk over clusterings. At each step, the chain either selects a random cluster, and divides it into two, or selects two random clusters and joins them together. The acceptance/rejection probabilities can be computed with respect to any coagulation fragmentation process. In our implementation, we pick either a uniform random cluster and a random bipartition of it (fragmentation), or a uniform random pair of clusters (coagulation). We believe the mixing time of this process should be fast as it is related to a coagulation fragmentation process known as the random transposition walk. Diaconis and Shahshahani provided a polynomial upper bound for this walk’s mixing time [9]."
    }, {
      "heading" : "5.2.1. Alternative model",
      "text" : "We believe a worthwhile alternative to the ideas described above is to represent the clustering evolution as an HMM on fragmentation-coagulation parameters: the simplest model having only two parameters , one controlling the probability of fragmentation and the other controlling the probability of coagulation. If the number of fragmentation-coagulation parameters is small, inference could be tractable."
    }, {
      "heading" : "6. CONCLUSIONS",
      "text" : "Our intention in this paper has been to show how various clustering methods can be applied to datasets which arise in financial markets. We have documented the process by which we analysed the problem and considered a method for determining clusters using triangular potentials. This latter method can be computationally intensive and we have provided some preliminary theoretical results concerning its limitations. However, notwithstanding these considerations, we have found promising empirical results from applying the method to simulated datasets and we look forward to extending this to real-world data in due course. In future work we aim to extend the idea to a setting where we place a non-uniform prior on clusterings, e.g. if expert knowledge suggests that a group of investments are likely to share similar return characteristics then we can configure potentials such that appropriate weighted links are established among these products. There is also considerable scope to investigate efficiency improvements to the MCMC estimation process, based on the particular structure of potentials in this context."
    } ],
    "references" : [ {
      "title" : "Probabilistic Graphical Models",
      "author" : [ "D. Koller", "N. Friedman" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2009
    }, {
      "title" : "Portfolio Selection",
      "author" : [ "H. Markowitz" ],
      "venue" : "Journal of Finance,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1952
    }, {
      "title" : "Theory of Financial Decision Making, Rowman and Littlefield",
      "author" : [ "J.E. Ingersoll Jr." ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1987
    }, {
      "title" : "Naive diversification strategies in defined contribution saving plans",
      "author" : [ "S. Benartzi", "R.H. Thaler" ],
      "venue" : "American Economic Review,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2001
    }, {
      "title" : "Optimal versus naive diversification: How inefficient is the portfolio strategy?",
      "author" : [ "V. De Miguel", "L. Garlappi", "R. Uppal" ],
      "venue" : "Review of Financial Studies,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2009
    }, {
      "title" : "Dynamic Spectral Clustering",
      "author" : [ "A. LaViers", "A. Rahmani", "M. Egerstedt" ],
      "venue" : "Proceedings of the 19th International Symposium on Mathematical Theory of Networks and Systems – MTNS 2010,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2010
    }, {
      "title" : "Evolutionary Spectral Clustering by Incorporating Temporal Smoothness",
      "author" : [ "Y. Chi", "X. Song", "D. Zhou", "K. Hino", "B.L. Tseng" ],
      "venue" : "Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2007
    }, {
      "title" : "Generating a Random Permutation with Random Transpositions",
      "author" : [ "P. Diaconis", "M. Shahshahani" ],
      "venue" : "Zeitschrift für Wahrscheinlichkeitstheorie und Verwandte Gebiete,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1981
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Koller and Friedman provide a detailed discussion of utility functions and decision theory in the general machine learning context [1].",
      "startOffset" : 131,
      "endOffset" : 134
    }, {
      "referenceID" : 1,
      "context" : "One of the most prominent theoretical results is the concept of mean-variance efficiency which has its roots in the work of Markowitz [3]: the idea is that in a one period model (under certain restrictive assumptions) if investors seek to maximize return and minimise portfolio variance, the optimal ex ante weighting vector is given by",
      "startOffset" : 134,
      "endOffset" : 137
    }, {
      "referenceID" : 2,
      "context" : "where is the covariance matrix of future returns, is the mean vector of expected returns, is a risk-aversion parameter and is the risk-free rate of return [4].",
      "startOffset" : 155,
      "endOffset" : 158
    }, {
      "referenceID" : 3,
      "context" : "In these circumstances one strand of literature considers simpler weighting schemes which are predicated on relatively few assumptions; one prominent example, popular with practitioners, is the self-explanatory equally-weighted (or ) approach [5].",
      "startOffset" : 243,
      "endOffset" : 246
    }, {
      "referenceID" : 4,
      "context" : "Although this may be far from the truth it may be more innocuous to assume this than to suffer potentially negative effects of erroneous statistical forecasts and there is a body of empirical literature which demonstrates the efficiency of the approach [6].",
      "startOffset" : 253,
      "endOffset" : 256
    }, {
      "referenceID" : 5,
      "context" : "It can be found at [7].",
      "startOffset" : 19,
      "endOffset" : 22
    }, {
      "referenceID" : 5,
      "context" : "Following [7] we set .",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 5,
      "context" : "As suggested by [7], the optimal number of clusters can be obtained by choosing the value of that maximizes a scoring function given by",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 6,
      "context" : "GRAPHICAL MODEL APPROACH Instead of representing clusterings as a binary matrix such that if cluster as the authors of [8] do, we approach the problem in a different way.",
      "startOffset" : 119,
      "endOffset" : 122
    }, {
      "referenceID" : 7,
      "context" : "Diaconis and Shahshahani provided a polynomial upper bound for this walk’s mixing time [9].",
      "startOffset" : 87,
      "endOffset" : 90
    } ],
    "year" : 2015,
    "abstractText" : "Motivated by the problem of computing investment portfolio weightings we investigate various methods of clustering as alternatives to traditional mean-variance approaches. Such methods can have significant benefits from a practical point of view since they remove the need to invert a sample covariance matrix, which can suffer from estimation error and will almost certainly be non-stationary. The general idea is to find groups of assets which share similar return characteristics over time and treat each group as a single composite asset. We then apply inverse volatility weightings to these new composite assets. In the course of our investigation we devise a method of clustering based on triangular potentials and we present associated theoretical results as well as various examples based on synthetic data.",
    "creator" : "Word"
  }
}