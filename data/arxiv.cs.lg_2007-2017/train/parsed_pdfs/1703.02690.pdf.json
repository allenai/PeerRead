{
  "name" : "1703.02690.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Leveraging Sparsity for Efficient Submodular Data Summarization",
    "authors" : [ "Erik M. Lindgren", "Shanshan Wu", "Alexandros G. Dimakis" ],
    "emails" : [ "erikml@utexas.edu,", "shanshan@utexas.edu,", "dimakis@austin.utexas.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 3.\n02 69\n0v 1\n[ st\nat .M\nL ]\n8 M"
    }, {
      "heading" : "1 Introduction",
      "text" : "In this paper we study the facility location problem: we are given sets V of size n, I of size m and a benefit matrix of nonnegative numbers C ∈ RI×V , where Civ describes the benefit that element i receives from element v. Our goal is to select a small set A of k columns in this matrix. Once we have chosen A, element i will get a benefit equal to the best choice out of the available columns, maxv∈A Civ. The total reward is the sum of the row rewards, so the optimal choice of columns is the solution of:\nargmax {A⊆V :|A|≤k}\n∑\ni∈I\nmax v∈A Civ. (1)\nA natural application of this problem is in finding a small set of representative images in a big dataset, where Civ represents the similarity between images i and v. The problem is to select k images that provide a good coverage of the full dataset, since each one has a close representative in the chosen set.\nThroughout this paper we follow the nomenclature common to the submodular optimization for machine learning literature. This problem is also known as the maximization version of the k-medians problem. A number of recent works have used this problem for selecting subsets of documents or images from a larger corpus [27, 39], to identify locations to monitor in order to quickly identify important events in sensor or blog networks [24, 26], as well as clustering applications [23, 34].\nWe can naturally interpret Problem 1 as a maximization of a set function F (A) which takes as an input the selected set of columns and returns the total reward of that set. Formally, let F (∅) = 0 and for all other sets A ⊆ V define\nF (A) = ∑\ni∈I\nmax v∈A Civ. (2)\nThe set function F is submodular, since for all j ∈ V and sets A ⊆ B ⊆ V \\ {j}, we have F (A ∪ {j}) − F (A) ≥ F (B ∪ {j}) − F (B), that is, the gain of an element is diminishes as we add elements. Since the entries of C are nonnegative, F is monotone, since for all A ⊆ B ⊆ V , we have F (A) ≤ F (B). We also have F normalized, since F (∅) = 0.\nThe facility location problem is NP-Hard, so we consider approximation algorithms. Like all monotone and normalized submodular functions, the greedy algorithm guarantees a (1− 1/e)-factor approximation to the optimal solution [35]. The greedy algorithm starts with the empty set, then for k iterations adds the element with the largest reward. This approximation is the best possible—the maximum coverage problem is an instance of the facility location problem, which was shown to be NP-Hard to optimize within a factor of 1− 1/e+ ε for all ε > 0 [13].\nThe problem is that the greedy algorithm has super-quadratic running time Θ(nmk) and in many datasets n and m can be in the millions. For this reason, several recent papers have focused on accelerating the greedy algorithm. In [26], the authors point out that if the benefit matrix is sparse, this can dramatically speed up the computation time. Unfortunately, in many problems of interest, data similarities or rewards are not sparse. Wei et al. [40] proposed to first sparsify the benefit matrix and then run the greedy algorithm on this new sparse matrix. In particular, [40] considers t-nearest neighbor sparsification, i.e., keeping for each row the t largest entries and zeroing out the rest. Using this technique they demonstrated an impressive 80-fold speedup over the greedy algorithm with little loss in solution quality. One limitation of their theoretical analysis was the limited setting under which provable approximation guarantees were established.\nOur Contributions: Inspired by the work of Wei et al. [40] we improve the theoretical analysis of the approximation error induced by sparsification. Specifically, the previous analysis assumes that the input came from a probability distribution where the preferences of each element of i ∈ I are independently chosen uniformly at random. For this distribution, when k = Ω(n), they establish that the sparsity can be taken to be O(logn) and running the greedy algorithm on the sparsified problem will guarantee a constant factor approximation with high probability. We improve the analysis in the following ways:\n• We prove guarantees for all values of k and our guarantees do not require any assumptions on the input besides nonnegativity of the benefit matrix.\n• In the case where k = Ω(n), we show that it is possible to take the sparsity of each row as low as O(1) while guaranteeing a constant factor approximation.\n• Unlike previous work, our analysis does not require the use of any particular algorithm and can be integrated to many algorithms for solving facility location problems.\n• We establish a lower bound which shows that our approximation guarantees are tight up to log factors, for all desired approximation factors.\nIn addition to the above results we propose a novel algorithm that uses a threshold based sparsification where we keep matrix elements that are above a set value threshold. This type of sparsification is easier to efficiently implement using nearest neighbor methods. For this method of sparsification, we obtain worst case guarantees and a lower bound that matches up to constant factors. We also obtain a data dependent guarantee which helps explain why our algorithm empirically performs better than the worst case.\nFurther, we propose the use of Locality Sensitive Hashing (LSH) and random walk methods to accelerate approximate nearest neighbor computations. Specifically, we use two types of similarity metrics: inner products and personalized PageRank (PPR). We propose the use of fast approximations for these metrics and empirically show that they dramatically improve running times. LSH functions are well-known but, to the best of our knowledge, this is the first time they have been used to accelerate facility location problems. Furthermore, we utilize personalized PageRank as the similarity between vertices on a graph. Random walks can quickly approximate this similarity and we demonstrate that it yields highly interpretable results for real datasets."
    }, {
      "heading" : "2 Related Work",
      "text" : "The use of a sparsified proxy function was shown by Wei et al. to also be useful for finding a subset for training nearest neighbor classifiers [41]. Further, they also show a connection of nearest neighbor classifiers to the facility location function. The facility location function was also used by Mirzasoleiman et al. as part of a summarization objective function in [32], where they present a summarization algorithm that is able to handle a variety of constraints.\nThe stochastic greedy algorithm was shown to get a 1 − 1/e − ε approximation with runtime O(nm log 1\nε ), which has no dependance on k [33]. It works by choosing a sample set\nfrom V of size n k log 1 ε each iteration and adding to the current set the element of the sample set with the largest gain. Also, there are several related algorithms for the streaming setting [5] and distributed setting [6, 25, 31, 34]. Since the objective function is defined over the entire dataset, optimizing the facility location function becomes more complicated in these memory limited settings. Often the function is estimated by considering a randomly chosen subset from the set I."
    }, {
      "heading" : "2.1 Benefits Functions and Nearest Neighbor Methods",
      "text" : "For many problems, the elements V and I are vectors in some feature space where the benefit matrix is defined by some similarity function sim. For example, in Rd we may use\nthe RBF kernel sim(x, y) = e−γ‖x−y‖ 2 2 , dot product sim(x, y) = xT y, or cosine similarity sim(x, y) = x T y\n‖x‖‖y‖ .\nThere has been decades of research on nearest neighbor search in geometric spaces. If the vectors are low dimensional, then classical techniques such as kd-trees [7] work well and are exact. However it has been observed that as the dimensions grow that the runtime of all known exact methods does little better than a linear scan over the dataset.\nAs a compromise, researchers have started to work on approximate nearest neighbor methods, one of the most successful approaches being locality sensitive hashing [15, 20]. LSH uses a hash function that hashes together items that are close. Locality sensitive hash functions exist for a variety of metrics and similarities such as Euclidean [11], cosine similarity [3, 9], and dot product [36, 38]. Nearest neighbor methods other than LSH that have been shown to work for machine learning problems include [8, 10]. Additionally, see [14] for efficient and exact GPU methods.\nAn alternative to vector functions is to use similarities and benefits defined from graph structures. For instance, we can use the personalized PageRank of vertices in a graph to define the benefit matrix [37]. The personalized PageRank is similar to the classic PageRank, except the random jumps, rather than going to anywhere in the graph, go back to the users “home” vertex. This can be used as a value of “reputation” or “influence” between vertices in a graph [17].\nThere are a variety of algorithms for finding the vertices with a large PageRank personalized to some vertex. One popular one is the random walk method. If πi is the personalized PageRank vector to some vertex i, then πi(v) is the same as the probability that a random walk of geometric length starting from i ends on a vertex v (where the parameter of the geometric distribution is defined by the probability of jumping back to i) [4]. Using this approach, we can quickly estimate all elements in the benefit matrix greater than some value τ .\n3 Guarantees for t-Nearest Neighbor Sparsification\nWe associate a bipartite support graph G = (V, I, E) by having an edge between v ∈ V and i ∈ I whenever Cij > 0. If the support graph is sparse, we can use the graph to calculate the gain of an element much more efficiently, since we only need to consider the neighbors of the element versus the entire set I. If the average degree of a vertex i ∈ I is t, (and we use a cache for the current best value of an element i) then we can execute greedy in time O(mtk). See Algorithm 1 in the Appendix for pseudocode. If the sparsity t is much smaller than the size of V , the runtime is greatly improved.\nHowever, the instance we wish to optimize may not be sparse. One idea is to sparsify the original matrix by only keeping the values in the benefit matrix C that are t-nearest neighbors, which was considered in [40]. That is, for every element i in I, we only keep the top t elements of Ci1, Ci2, . . . , Cin and set the rest equal to zero. This leads to a matrix with mt nonzero elements. We then want the solution from optimizing the sparse problem to be close to the value of the optimal solution in the original objective function F .\nOur main theorem is that we can set the sparsity parameter t to be O( n αk log m αk )—which is a significant improvement for large enough k—while still having the solution to the sparsified\nproblem be at most a factor of 1 1+α from the value of the optimal solution.\nTheorem 1. Let Ot be the optimal solution to an instance of the facility location problem with a benefit matrix that was sparsified with t-nearest neighbor. For any t ≥ t∗(α) = O( n\nαk log n αk ),\nwe have F (Ot) ≥ 1\n1+α OPT.\nProof Sketch. For the value of t chosen, there exists a set Γ of size αk such that every element of I has a neighbor in the t-nearest neighbor graph; this is proven using the probabilistic method. By appending Γ to the optimal solution and using the monotonicity of F , we can move to the sparsified function, since no element of I would prefer an element that was zeroed out in the sparsified matrix as one of their top t most beneficial elements is present in the set Γ. The optimal solution appended with Γ is a set of size (1 +α)k. We then bound the amount that the optimal value of a submodular function can increase by when adding αk elements. See the appendix for the complete proof.\nNote that Theorem 1 is agnostic to the algorithm used to optimize the sparsified function, and so if we use a ρ-approximation algorithm, then we are at most a factor of ρ\n1+α from the\noptimal solution. Later this section we will utilize this to design a subquadratic algorithm for optimizing facility location problems as long as we can quickly compute t-nearest neighbors and k is large enough.\nIf m = O(n) and k = Ω(n), we can achieve a constant factor approximation even when taking the sparsity parameter as low as t = O(1), which means that the benefit matrix C has only O(n) nonzero entries. Also note that the only assumption we need is that the benefits between elements are nonnegative. When k = Ω(n), previous work was only able to take t = O(logn) and required the benefit matrix to come from a probability distribution [40].\nOur guarantee has two regimes depending on the value of α. If we want the optimal solution to the sparsified function to be a 1 − ε factor from the optimal solution to the original function, we have that t∗(ε) = O( n\nεk log m εk ) suffices. Conversely, if we want to take\nthe sparsity t to be much smaller than n k log m k , then this is equivalent to taking α very large and we have some guarantee of optimality. In the proof of Theorem 1, the only time we utilize the value of t is to show that there exists a small set Γ that covers the entire set I in the t-nearest neighbor graph. Real datasets often contain a covering set of size αk for t much smaller than O( n\nαk log m αk ). This observation\nyields the following corollary.\nCorollary 2. If after sparsifying a problem instance there exists a covering set of size αk in the t-nearest neighbor graph, then the optimal solution Ot of the sparsified problem satisfies F (Ot) ≥ 1 1+α OPT.\nIn the datasets we consider in our experiments of roughly 7000 items, we have covering sets with only 25 elements for t = 75, and a covering set of size 10 for t = 150. The size of covering set was upper bounded by using the greedy set cover algorithm. In Figure 2 in the appendix, we see how the size of the covering set changes with the choice of the number of neighbors chosen t.\nIt would be desirable to take the sparsity parameter t lower than the value dictated by t∗(α). As demonstrated by the following lower bound, is not possible to take the sparsity significantly lower than 1\nα n k and still have a 1 1+α approximation in the worst case.\nProposition 3. Suppose we take\nt = max\n{\n1\n2α ,\n1\n1 + α\n}\nn− 1\nk .\nThere exists a family of inputs such that we have F (Ot) ≤ 1\n1+α−2/k OPT.\nThe example we create to show this has the property that in the t-nearest neighbor graph, the set Γ needs αk elements to cover every element of I. We plant a much smaller covering set that is very close in value to Γ but is hidden after sparsification. We then embed a modular function within the facility location objective. With knowledge of the small covering set, an optimal solver can take advantage of this modular function, while the sparsified solution would prefer to first choose the set Γ before considering the modular function. See the appendix for full details.\nSparsification integrates well with the stochastic greedy algorithm [33]. By taking t ≥ t∗(ε/2) and running stochastic greedy with sample sets of size n\nk ln 2 ε , we get a 1 − 1/e − ε\napproximation in expectation that runs in expected time O(nm εk log 1 ε log m εk ). If we can quickly sparsify the problem and k is large enough, for example n1/3, this is subquadratic. The following proposition shows a high probability guarantee on the runtime of this algorithm and is proven in the appendix.\nProposition 4. When m = O(n), the stochastic greedy algorithm [33] with set sizes of size n\nk log 2 ε , combined with sparsification with sparsity parameter t, will terminate in time\nO(n log 1 ε max{t, logn}) with high probability. When t ≥ t∗(ε/2) = O( n εk log m εk ), this algorithm has a 1− 1/e− ε approximation in expectation."
    }, {
      "heading" : "4 Guarantees for Threshold-Based Sparsification",
      "text" : "Rather than t-nearest neighbor sparsification, we now consider using τ -threshold sparsification, where we zero-out all entries that have value below a threshold τ . Recall the definition of a locality sensitive hash.\nDefinition. H is a (τ,Kτ, p, q)-locality sensitive hash family if for x, y satisfying sim(x, y) ≥ τ we have Ph∈H(h(x) = h(y)) ≥ p and if x, y satisfy sim(x, y) ≤ Kτ we have Ph∈H(h(x) = h(y)) ≤ q.\nWe see that τ -threshold sparsification is a better model than t-nearest neighbors for LSH, as for K = 1 it is a noisy τ -sparsification and for non-adversarial datasets it is a reasonable approximation of a τ -sparsification method. Note that due to the approximation constant K, we do not have an a priori guarantee on the runtime of arbitrary datasets. However we would expect in practice that we would only see a few elements with threshold above the value τ . See [2] for a discussion on this.\nOne issue is that we do not know how to choose the threshold τ . We can sample elements of the benefit matrix C to estimate how sparse the threshold graph will be for a given\nthreshold τ . Assuming the values of C are in general position1, by using the DvoretzkyKiefer-Wolfowitz-Massart Inequality [12, 28] we can bound the number of samples needed to choose a threshold that achieves a desired sparsification level.\nWe establish the following data-dependent bound on the difference in the optimal solutions of the τ -threshold sparsified function and the original function. We denote the set of vertices adjacent to S in the τ -threshold graph with N(S).\nTheorem 5. Let Oτ be the optimal solution to an instance of the facility location problem with a benefit matrix that was sparsified using a τ -threshold. Assume there exists a set S of size k such that in the τ -threshold graph we have the neighborhood of S satisfying |N(S)| ≥ µn. Then we have\nF (Oτ) ≥\n(\n1 + 1\nµ\n)−1\nOPT.\nFor the datasets we consider in our experiments, we see that we can keep just a 0.01−0.001 fraction of the elements of C while still having a small set S with a neighborhood N(S) that satisfied |N(S)| ≥ 0.3n. In Figure 3 in the appendix, we plot the relationship between the number of edges in the τ -threshold graph and the number of coverable element by a a set of small size, as estimated by the greedy algorithm for max-cover.\nAdditionally, we have worst case dependency on the number of edges in the τ -threshold graph and the approximation factor. The guarantees follow from applying Theorem 5 with the following Lemma.\nLemma 6. For k ≥ c 1−2c2 1 δ , any graph with 1 2 δ2n2 edges has a set S of size k such that the neighborhood N(S) satisfies |N(S)| ≥ cδn.\nTo get a matching lower bound, consider the case where the graph has two disjoint cliques, one of size δn and one of size (1− δ)n. Details are in the appendix."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Summarizing Movies and Music from Ratings Data",
      "text" : "We consider the problem of summarizing a large collection of movies. We first need to create a feature vector for each movie. Movies can be categorized by the people who like them, and so we create our feature vectors from the MovieLens ratings data [16]. The MovieLens database has 20 million ratings for 27,000 movies from 138,000 users. To do this, we perform low-rank matrix completion and factorization on the ratings matrix [21, 22] to get a matrix X = UV T , where X is the completed ratings matrix, U is a matrix of feature vectors for each user and V is a matrix of feature vectors for each movie. For movies i and j with vectors vi and vj , we set the benefit function Cij = v T i vj. We do not use the normalized dot product (cosine similarity) because we want our summary movies to be movies that were\n1By this we mean that the values of C are all unique, or at least only a few elements take any particular value. We need this to hold since otherwise a threshold based sparsification may exclusively return an empty graph or the complete graph.\nhighly rated, and not normalizing makes highly rated movies increase the objective function more.\nWe complete the ratings matrix using the MLlib library in Apache Spark [29] after removing all but the top seven thousand most rated movies to remove noise from the data. We use locality sensitive hashing to perform sparsification; in particular we use the LSH in the FALCONN library for cosine similarity [3] and the reduction from a cosine simiarlity hash to a dot product hash [36]. As a baseline we consider sparsification using a scan over the entire dataset, the stochastic greedy algorithm with lazy evaluations [33], and the greedy algorithm with lazy evaluations [30]. The number of elements chosen was set to 40 and for the LSH method and stochastic greedy we average over five trials.\nWe then do a scan over the sparsity parameter t for the sparsification methods and a scan over the number of samples drawn each iteration for the stochastic greedy algorithm. The sparsified methods use the (non-stochastic) lazy greedy algorithm as the base optimization algorithm, which we found worked best for this particular problem2. In Figure 1(a) we see that the LSH method very quickly approaches the greedy solution—it is almost identical in value just after a few seconds even though the value of t is much less than t∗(ε). The stochastic greedy method requires much more time to get the same function value. Lazy greedy is not plotted, since it took over 500 seconds to finish.\nA performance metric that can be better than the objective value is the fraction of elements returned that are common with the greedy algorithm. We treat this as a proxy for the interpretability of the results. We believe this metric is reasonable since we found the subset returned by the greedy algorithm to be quite interpretable. We plot this metric\n2When experimenting on very larger datasets, we found that runtime constraints can make it necessary to use stochastic greedy as the base optimization algorithm\nagainst runtime in Figure 1(b). We see that the LSH method quickly gets to 90% of the elements in the greedy set while stochastic greedy takes much longer to get to just 70% of the elements. The exact sparsification method is able to completely match the greedy solution at this point. One interesting feature is that the LSH method does not go much higher than 90%. This may be due to the increased inaccuracy when looking at elements with smaller dot products. We plot this metric against the number of exact and approximate nearest neighbors t in Figure 4 in the appendix.\nWe include a subset of the summarization and for each representative a few elements who are represented by this representative with the largest dot product in Table 1 to show the interpretability of our results."
    }, {
      "heading" : "5.2 Finding Influential Actors and Actresses",
      "text" : "For our second experiment, we consider how to find a diverse subset of actors and actresses in a collaboration network. We have an edge between an actor or actress if they collaborated in a movie together, weighted by the number of collaborations. Data was obtained from [19] and an actor or actress was only included if he or she was one of the top six in the cast billing. As a measure of influence, we use personalized PageRank [37]. To quickly calculate the people with the largest influence relative to someone, we used the random walk method[4].\nWe first consider a small instance where we can see how well the sparsified approach works. We build a graph based on the cast in the top thousand most rated movies. This graph has roughly 6000 vertices and 60,000 edges. We then calculate the entire PPR matrix using the power method. Note that this is infeasible on larger graphs in terms of time and memory. Even on this moderate sized graph it took six hours and takes 2GB of space. We then compare the value of the greedy algorithm using the entire PPR matrix with the sparsified algorithm using the matrix approximated by Monte Carlo sampling using the two\nmetrics mentioned in the previous section. We omit exact nearest neighbor and stochastic greedy because it is not clear how it would work without having to compute the entire PPR matrix. Instead we compare to an approach where we choose a sample from I and calculate the PPR only on these elements using the power method. As mentioned in Section 2, several algorithms utilize random sampling from I. We take k to be 50 for this instance. In Figure 5 in the appendix we see that sparsification performs drastically better in both function value and percent of the greedy set contained for a given runtime.\nWe now scale up to a larger graph by taking the actors and actresses billed in the top six for the twenty thousand most rated movies. This graph has 57,000 vertices and 400,000 edges. We would not be able to compute the entire PPR matrix for this graph in a reasonable amount of time or space. However we can run the sparsified algorithm in three hours using only 2 GB of memory, which could be improved further by parallelizing the Monte Carlo approximation.\nWe run the greedy algorithm separately on the actors and actresses. For each we take the top twenty-five and compare to the actors and actresses with the largest (non-personalized) PageRank. In Figure 2 of the appendix, we see that the PageRank output fails to capture the diversity in nationality of the dataset, while the facility location optimization returns actors and actresses from many of the worlds film industries."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No. DGE-1110007 as well as NSF Grants CCF 1344179, 1344364, 1407278, 1422549 and ARO YIP W911NF-14-1-0258."
    }, {
      "heading" : "6 Appendix: Additional Figures",
      "text" : "Algorithm 1 Greedy algorithm with sparsity graph\nInput: benefit matrix C, sparsity graph G = (V, I, E) define N(v): return the neighbors of v in G for all i ∈ I:\n# cache of the current benefit given to i βi ← 0\nA ← ∅ for k iterations:\nfor all v ∈ V : # calculate the gain of element v gv ← 0 for all i ∈ N(v):\n# add the gain of element v from i gv ← gv +max(Civ − βi, 0)\nv∗ ← argmaxV gv A ← A ∪ {v∗} for all i ∈ N(v∗)\n# update the cache of the current benefit for i βi ← max(βi, Civ∗)\nreturn A"
    }, {
      "heading" : "7 Appendix: Full Proofs",
      "text" : ""
    }, {
      "heading" : "7.1 Proof of Theorem 1",
      "text" : "We will use the following two lemmas in the proof of Theorem 1, which are proven later in this section. The first lemma bounds the size of the smallest set of left vertices covering every right vertex in a t-regular bipartite graph.\nLemma 7. For any bipartite graph G = (V, I, E) such that |V | = n, |I| = m, every vertex i ∈ I has degree at least t, and n ≤ mt, there exists a set of vertices Γ ⊆ V such that every vertex in I has a neighbor in Γ and\n|Γ| ≤ n\nt\n(\n1 + ln mt\nn\n)\n. (3)\nThe second lemma bounds the rate that the optimal solution grows as a function of k.\nLemma 8. Let f be any normalized submodular function and let O2 and O1 be optimal solutions for their respective sizes, with |O2| ≥ |O1|. We have\nf(O2) ≤ |O2|\n|O1| f(O1).\nWe now prove Theorem 1.\nProof. We will take t∗(α) to be the smallest value of t such that |Γ| ≥ αk in Equation (3). It can be verified that t∗(α) ≤ ⌈4 n\nαk max{1, ln n αk }⌉.\nLet Γ ⊆ V be a set such that all elements of I has a t-nearest neighbor in Γ. By Lemma 7, one is guaranteed to exists of size at most αk for t ≥ t∗(α). Let O be the optimal set of size k and let F (t) be the objective function of the sparsified function. Let Okt and O (1+α)k t be the optimal solutions to F (t) of size k and (1 + α)k. We have\nF (O) ≤ F (O ∪ Γ)\n= F (t)(O ∪ Γ)\n≤ F (t)(O (1+α)k t ). (4)\nThe first inequality is due to the monotonicity of F . The second is because every element of I would prefer to choose one of their t nearest neighbors and because of Γ they can. The third inequality is because |O ∪ Γ| ≤ (1 + α)k and O\n(1+α)k t is the optimal solution for this\nsize. Now by Lemma 8, we can bound the approximation for shrinking from O\n(1+α)k t to O k t .\nApplying Lemma 8 and continuing from Equation (4) implies\nF (O) ≤ (1 + α)F (t)(Okt ).\nObserve that F (t)(A) ≤ F (A) for any set A to obtain the final bound."
    }, {
      "heading" : "7.2 Proof of Proposition 3",
      "text" : "Define Πn(t) to be the n× (n+1) matrix where for i = 1, . . . , n we have column i equal to 1 for positions i to i+ t−1, potentially cycling the position back to the beginning if necessary, and then 0 otherwise. For column n+ 1 make all values 1− 1/2n. For example,\nΠ6(3) =\n\n       1 0 0 0 1 1 11/12 1 1 0 0 0 1 11/12 1 1 1 0 0 0 11/12 0 1 1 1 0 0 11/12 0 0 1 1 1 0 11/12 0 0 0 1 1 1 11/12\n\n       .\nWe will show the lower bound in two parts, when α < 1 and when α ≥ 1.\nProof of case α ≥ 1. Let F be the facility location function defined on the benefit matrix C = Πn(δ n k ). For t = δ n k , the sparsified matrix C(t) has all of its elements except the n+ 1st row. With k elements, the optimal solution to F (t) is to choose the k elements that let us cover δn of the elements of I, giving a value of δn. However if we chose the n + 1th element, we would have gotten a value of n − 1/2, giving an approximation of δ\n1−1/(2n) .\nSetting δ = 1/(1 + α) and using α ≤ n/k implies\nF (Ot) ≤ 1\n1 + α− 1/k OPT\nwhen we take t = 1 1+α |V |−1 k (note that for this problem |V | = n+ 1).\nProof of case α < 1. Let F be the facility location function defined on the benefit matrix\nC =\n(\nΠn( 1 α n k ) 0\n0 ( 1 α n k − 1 2n ) In×n\n)\nFor t = 1 α n k , the optimal solution to F (t) is to use αk elements to cover all the elements of Πn, then use the remaining (1−α)k elements in the identity section of the matrix. This has a value of less than 1\nα n. For F , the optimal solution is to choose the n + 1st element of Πn,\nthen use the remaining k − 1 elements in the identity section of the identity section of the matrix. This has a value of more than n(1 + 1\nα − 1 kα − 1 n ), and therefore an approximation of\n1 1+α−1/k−1/n . Note that in this case |V | = 2n+ 1 and so we have\nF (Ot) ≤ 1\n1 + α− 1/k − 1/n OPT\nwhen we take t = 1 2α |V |−1/2 k ."
    }, {
      "heading" : "7.3 Proof of Proposition 4",
      "text" : "Proof. The stochastic greedy algorithm works by choosing a set of elements Sj each iteration of size n\nk log 1 ε . We will assume m = n and ε = 1/e to simplify notation. We want to show\nthat k\n∑\nj=1\n∑\nv∈Sj\ndv = O(nt)\nwith high probability, where dv is the degree of element v in the sparsity graph. We will show this using Bernstein’s Inequality: given n i.i.d. random variables X1, . . . , Xn such that E(Xℓ) = 0, Var(Xℓ) = σ 2, and |Xℓ| ≤ c with probability 1, we have\nP\n(\nn ∑\nℓ=1\nXℓ ≥ λn\n)\n≤ exp\n(\n− nλ2\n2σ2 + 2 3 cλ\n)\n.\nWe will take Xℓ to be the degree of the ℓth element of V chosen uniformly at random, shifted by the mean of t. Although in the stochastic greedy algorithm the elements are not chosen i.i.d. but instead iterations in k iterations of sampling without replacement, treating them as i.i.d. random variables for purposes of Bernstein’s Inequality is justified by Theorem 4 of [18].\nWe have |Xℓ| ≤ n, and Var(Xℓ) ≤ tn, where the variance bound is because variance for a given mean t on support [0, m] is maximized by putting mass t\nn on n and 1− t n on 0.\nIf t ≥ lnn, then take λ = 8 3 t. If t < lnn, take λ = 8 3 lnn. This yields\nP\n\n\nk ∑\nj=1\n∑\nv∈Sj\ndv ≥ nt+ 8\n3\n√\nm n max{nt, lnn}\n\n ≤ 1\nn ."
    }, {
      "heading" : "7.4 Proof of Lemma 7",
      "text" : "We now prove Lemma 7, which is a modification of Theorem 1.2.2 of [1].\nProof. Choose a set X by picking each element of V with probability p, where p is to be decided later. For every element of I without a neighbor in X , add one arbitrarily. Call this set Y . We have E(|X ∪ Y |) ≤ np + m(1 − p)t ≤ np + me−pt. Optimizing for p yields p = 1\nt ln mt n . This is a valid probability when mt n ≥ 1, which we assumed, and when m n ≤ e\nt t\n(we do not need to worry about the latter case because if it does not hold then it implies an inequality weaker than the trivial one |Γ| ≤ n)."
    }, {
      "heading" : "7.5 Proof of Lemma 8",
      "text" : "Before we prove Lemma 8, we need the following Lemma.\nLemma 9. Let f be any normalized submodular function and let O be an optimal solution for its respective size. Let A be any set. We have\nf(O ∪ A) ≤\n(\n1 + |A|\n|O|\n)\nf(O).\nWe now prove Lemma 8.\nProof. Let A∗ = argmax\n{A⊆O2:|A|≤|O1|}\nf(A)\nand let A′ = O2 \\ A ∗. Since A∗ is optimal for the function when restricted to a ground set O2, by Lemma 9 and the optimality of O1 for sets of size |O1|, we have\nf(O2) = f(A ∗ ∪A′)\n≤\n(\n1 + |A′|\n|A∗|\n)\nf(A∗)\n= |O2|\n|O1| f(A∗)\n≤ |O2|\n|O1| f(O1).\nWe now prove Lemma 9.\nProof. Define f(v | A) = f({v} ∪ A) − f(A). Let O = {o1, . . . ok}, where the ordering is arbitrary except that\nf(ok | O \\ {ok}) = argmin i=1,...,k f(oi | O \\ {oi}).\nLet A = {a1, . . . , aℓ}, where the ordering is arbitrary except that\nf(a1 | O) = argmax i=1,...,ℓ f(ai | O).\nWe will first show that f(a1 | O) ≤ f(ok | O \\ {ok}). (5)\nBy submodularity, we have\nf(a1 | O) ≤ f(a1 | O \\ {ok}).\nIf it was true that f(a1 | O \\ {ok}) > f(ok | O \\ {ok}),\nthen we would have\nf((O \\ {ok}) ∪ {a1}) = f(a1 | O \\ {ok}) + k−1 ∑\ni=1\nf(oi | {o1, . . . , oi−1})\n≥ k ∑\ni=1\nf(oi | {o1, . . . , oi−1})\n= f(O),\ncontradicting the optimality of O, thus showing that Inequality 5 holds. Now since for all i ∈ {1, 2, . . . , k}\nf(a1 | O) ≤ f(ok | O \\ {ok})\n≤ f(oi | O \\ {oi})\n≤ f(oi | {o1, . . . , oi−1}),\nit is worse than the average of f(oi | {o1, . . . , oi−1}), which is 1 k ∑k i=1 f(oi | {o1, . . . , oi−1}), and showing that\nf(a1 | O) ≤ 1\nk f(O). (6)\nFinally, we have\nf(O ∪A) = f(O) + ℓ ∑\ni=1\nf(ai | O ∪ {a1, . . . , ai−1})\n≤ f(O) + ℓ ∑\ni=1\nf(ai | O)\n≤ f(O) + ℓf(a1 | O)\n≤\n(\n1 + ℓ\nk\n)\nf(O),\nwhich is what we wanted to show."
    }, {
      "heading" : "7.6 Proof of Theorem 5",
      "text" : "Proof. Let O be the optimal solution to the original problem. Let Fτ andF τ be the functions defined restricting to the matrix elements with benefit at least τ and all remaining elements, respectively. If there exists a set S of size k such that µn elements have a neighbor in S, then we have\nF (O) ≤ Fτ (O) +F τ (O)\n≤ Fτ (O) + nτ ≤ Fτ (O) + 1\nµ Fτ (S)\n≤\n(\n1 + 1\nµ\n)\nFτ (Oτ )\nwhere the last inequality follows from Oτ being optimal for Fτ ."
    }, {
      "heading" : "7.7 Proof of Lemma 6",
      "text" : "Proof. Consider the following algorithm:\nB ← ∅ S ← ∅\nwhile |B| ≤ cδn v∗ ← argmax |N(v)| add v∗ to S add N(v∗) to B remove N(v∗) ∪ {v∗} from G\nWe will show that after T = c (1−2c2)δ iterations this algorithm will terminate. When it does, S will satisfy |N(S)| ≥ cδn since every element of B has a neighbor in S. If there exists a vertex of degree cδn, then we will terminate after the first iteration. Otherwise all vertices have degree less than cδn. Assuming all vertices have degree less than cδn, until we terminate the number of edges incident to B is at most |B|cδn ≤ c2δ2n2. At each iteration, the number of edges in the graph is at least (1\n2 −c2)δ2n2, thus in each iteration\nwe can find a v∗ with degree at least (1− 2c2)δ2n. Therefore, after T iterations, we will have terminated with the size of S is at most T and |N(S)| ≥ cδn.\nWe see that this is tight up to constant factors by the following proposition.\nProposition 10. There exists an example where for ∆ = δ2n, the optimal solution to the sparsified function is a factor of O(δ) from the optimal solution to the original function.\nProof. Consider the following benefit matrix.\nC =\n(\n1δn×δn + (1 + 1\nk−1 )I 0\n0 (1− 1 (1−δ)n )1(1−δn)×(1−δn)\n)\nThe sparsified optimal would only choose elements in the top left clique and would get a value of roughly δn, while the true optimal solution would cover both cliques and get a value of roughly n."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "The facility location problem is widely used for summarizing large datasets and<lb>has additional applications in sensor placement, image retrieval, and clustering. One<lb>difficulty of this problem is that submodular optimization algorithms require the cal-<lb>culation of pairwise benefits for all items in the dataset. This is infeasible for large<lb>problems, so recent work proposed to only calculate nearest neighbor benefits. One<lb>limitation is that several strong assumptions were invoked to obtain provable approx-<lb>imation guarantees. In this paper we establish that these extra assumptions are not<lb>necessary—solving the sparsified problem will be almost optimal under the standard<lb>assumptions of the problem. We then analyze a different method of sparsification that<lb>is a better model for methods such as Locality Sensitive Hashing to accelerate the<lb>nearest neighbor computations and extend the use of the problem to a broader family<lb>of similarities. We validate our approach by demonstrating that it rapidly generates<lb>interpretable summaries.",
    "creator" : "LaTeX with hyperref package"
  }
}