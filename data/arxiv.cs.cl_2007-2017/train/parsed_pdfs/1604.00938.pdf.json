{
  "name" : "1604.00938.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Multi-Field Structural Decomposition for Question Answering",
    "authors" : [ "Tomasz Jurczyk", "Jinho D. Choi" ],
    "emails" : [ "tomasz.jurczyk@emory.edu", "jinho.choi@emory.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Towards machine reading, question answering has recently gained lots of interest among researchers from both natural language processing (Moschitti and Quarteroni, 2011; Yih et al., 2013; Hixon et al., 2015) and information retrieval (Schiffman et al., 2007; Kolomiyets and Moens, 2011). People from these two research fields, NLP and IR, have shown tremendous progress on question answering, yet only few efforts have been made to adapt technologies from both sides. The NLP side often tackles the task by analyzing linguistic aspects, whereas the IR side tackles it by searching likely patterns.\nWhile these two approaches perform well individually, more sophisticated solutions are needed to handle a wide range of questions. By considering linguistic structures such as syntactic and semantic trees, QA systems can infer deeper meaning of the context and handle more complex questions. However, extracting answers from these structures through either graph matching or predicate logic is\nnot necessarily scalable when the size of the context is large. On the other hand, searching patterns is scalable for large data, especially when coupled with indexing, although it does not always concern with the actual meaning of the context.\nWe present a multi-field weighted indexing approach for question answering that combines good aspects of both NLP and IR. We begin by describing how linguistic structures are decomposed into multiple fields (Section 3.3), and explain how the decomposed fields are used to rank documents containing answers through statistical learning (Sections 3.4 and 3.5). We evaluate our approach to 8 types of questions; our final model shows significant improvement over the baseline model using simple search (Section 4)."
    }, {
      "heading" : "2 Related Work",
      "text" : "Shen and Lapata (2007) assessed the contribution of semantic roles to factoid question answering and showed promising results. Pizzato and Mollá (2008) proposed a question prediction language model providing rich information and achieved improved speed and accuracy. Although related, our work is distinguished from theirs because we consider multiple fields whereas the others consider only one field representing semantic roles. Ferrucci et al. (2010) presented IBM Watson taking a hybrid approach between NLP and IR, and advanced the question answering task to another level.\nFader et al. (2013) proposed a paraphrase-driven perceptron learning approach using a seed lexicon. Our learning process is similar; however, it is distinguished in a way that we learn weights for individual fields instead of lexicons. Yih et al. (2014) introduced a semantic parsing framework for open domain question answering, which used convolutional neural networks for measuring similarities between decomposed entities. Weston et al. (2015) presented the Memory Networks models designed to memorize information about known objects and\nar X\niv :1\n60 4.\n00 93\n8v 1\n[ cs\n.C L\n] 4\nA pr\n2 01\n6\nactors. Our work is related to the this work; however, memory networks are designed to store and manipulate information about specific types of objects while our framework is generalizable to any type of objects induced from the context."
    }, {
      "heading" : "3 Approach",
      "text" : ""
    }, {
      "heading" : "3.1 Overall framework",
      "text" : "Figure 1 shows the overall framework. Our system is designed in a modular architectural way, so any further extension of fields can be easily integrated. The system takes input documents, generates linguistic structures using NLP tools, decomposes them into multiple fields, and indexes those fields. Questions are processed in the same way. To answer a question, the system queries the index for each field extracted from the question and measures the relevance score. All documents are ranked with respect to the relevance scores and their weights associated with the fields, and the document with the highest score is selected as the answer."
    }, {
      "heading" : "3.2 Modules",
      "text" : "Our system consists of several modules closely connected together providing a fully working solution for the question answering selection task."
    }, {
      "heading" : "3.2.1 Documents and questions",
      "text" : "Documents provide the context where the questions find their answers from. Each document can contain one or more sentences, in which answers for coming questions are annotated for training. Documents may simply be Wikipedia articles, news articles, fictional stories, etc. Questions are treated as regular documents containing only one sentence."
    }, {
      "heading" : "3.2.2 NLP tools",
      "text" : "For the generation of syntactic and semantic structures, we used the part-of-speech tagger (Choi and Palmer, 2012), the dependency parser (Choi and McCallum, 2013), the semantic role labeler (Choi and Palmer, 2011), and the coreference resolution tool in ClearNLP1. Ensuring good and robust accuracy for these NLP tools is important because all the following modules depend on their output."
    }, {
      "heading" : "3.2.3 Field extractor",
      "text" : "The field extractor takes the linguistic structures from the NLP tools and decomposes them into multiple fields (Section 3.3). All fields extracted from the documents are passed to the index engine, whereas fields extracted from the questions are sent directly to the answer ranker module."
    }, {
      "heading" : "3.2.4 Index engine",
      "text" : "The index engine is a search server that receives a list of fields decomposed by the field extractor, indexes terms in the fields, and responses to the queries generated from questions with their relevance scores. We used Elastic Search2, as it provides a distributed, multi-tenancy-capable search."
    }, {
      "heading" : "3.2.5 Answer ranker",
      "text" : "The answer ranker takes the decomposed fields extracted from a question, converts them into queries, and builds a matrix of documents with their relevance scores across all fields through the index engine (Section 3.4). It also uses different weights for individual fields trained by statistical modeling (Section 3.5).\n1http://www.clearnlp.com 2https://www.elastic.co"
    }, {
      "heading" : "3.3 Structural decomposition",
      "text" : "Each sentence is represented by the index engine as a document with multiple fields grouped into categories. Figure 2 shows an example of how the sentence is decomposed into multiple fields consisting of syntactic and semantic structures. Due to the extensible nature of our field extractor, additional groups and fields can be easily integrated. Currently, our system supports 24 fields grouped into the following three categories:\n• Lexical fields (e.g., word-forms, lemmas).\n• Syntactic fields (e.g., dependency labels).\n• Semantic fields (e.g., semantic roles, distances between predicates)."
    }, {
      "heading" : "3.4 Answer ranking",
      "text" : "When a question q is asked, it is decomposed into the n-number of fields. Each field is transformed into a query where certain words are replaced with wildcards (e.g., {where a1, is pred, she a2} → {* a1 is pred she a2}). Then, the relevance score r is measured between each field in the question and the same field in each document dt ∈ D by the index engine.3 The product of the relevance scores\n3We set the elastic search results limit to 20.\nand individual weights for all fields are summed, and the document d̂ with the highest score f is taken as the answer. Note that in our dataset, each document contains only one sentence so that retrieving a document is equivalent to retrieving a sentence. The following equations describe how the document d̂ is selected by measuring the overall score f(q, dt) using the relevance scores r(qi, dti) and the weights λi.\nd̂ = argmax dt∈D\nf(q, dt)\nf(q, dt) = n∑\ni=1\nλi · r(qi, dti)\nr(qi, d t i) = ∑ v∈qi∩dti tfti(v) · idfi(v) · normti(v)"
    }, {
      "heading" : "3.5 Training weights for individual fields",
      "text" : "Algorithm 1 shows how the weights for all fields are learned during training. We adapt the averaged perceptron algorithm, which has been widely used for many NLP tasks. All the weights ~λ are initialized to 1. For each question q ∈ Q, it predicts the document d̂ that most likely contains the answer. If d̂ is incorrect, then it compares the relevance score r between (q, d̂) and (q, d) for each field, and updates the weight accordingly, where d is the true document from the oracle. This procedure is repeated multiple times through iterations. Finally, the algorithm returns the averaged weights, where each dimension represents the weight for each field.\nAlgorithm 1 Averaged perceptron training.\nInput: D: document set, Q: question set. M : max-number of iterations, α: learning rate.\nOutput: The averaged weight vector.\n1: ~λ← 1; ~λ′ ← 0 2: for iter ∈ [1,M ] do 3: foreach q ∈ Q do 4: d̂ = argmaxdt∈D f(q, d t) 5: if d̂ 6= d then # d is the oracle 6: foreach i ∈ [1, n] do # for each field 7: δ ← α · sign[r(qi, di)− r(qi, d̂i)] 8: λi ← λi + δ 9: ~λ′ ← ~λ′ + ~λ\n10: return ~λ′ · 1 M∗|Q|\nAll hyper-parameters were optimized on the development sets and evaluated on the test sets. For our experiments, we used the following hyperparameters: M = 40, α = 0.002."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Data and evaluation metrics",
      "text" : "Our approach is evaluated on a subset of the bAbI tasks (Weston et al., 2015). The original data contains 20 tasks, where each task represents a different kind of question answering challenge. We select 8 tasks, in which answer for a single question is located within a single sentence. For consistency and replicability, we follow the same training, development, and evaluation set splits as provided, where every set contains 1,000 questions.\nFor the evaluation metrics, we use mean average precision (MAP) and mean reciprocal rank (MRR) of the top-3 predictions. The mean average precision is measured by counting the number of questions, for which sentences containing the answers are correctly selected as the best predictions. The reciprocal rank of a query response is the multiplicative inverse of the rank of the first correct answer. Mean reciprocal rank is the average of the reciprocal ranks of all question queries."
    }, {
      "heading" : "4.2 Evaluation",
      "text" : "Table 1 shows the results from our system on different types of questions. The MAP and MRR show clear correlation with respect to the number of active fields. For the majority of tasks, using only the lexical fields does not perform well. The fictional stories included in this data often contain multiple occurrences of the same lexicons, and the lexical fields alone are not able to select the correct answer. Significantly lower accuracy for the last task is due to a fact that besides an answer is located within a single sentence, multiple passages for the single question are required to correctly locate the sentence with the answers. Lexical fields coupled with only syntactic fields do not perform much better. It may be due to a fact that the syntactic fields con-\ntaining ordinary dependency labels do not provide sufficient context-wise information so that they do not generate enough features for statistical learning to capture specific characteristic of the context. The significant improvement, however, is reached when the semantics fields are added as they provide deeper understanding of the context.\nNot that this data set has also been used for evaluating the Memory Networks approach to question answering (Weston et al., 2015). The authors achieved high accuracy, reaching 100% in several tasks; however, our work still finds its own value because our approach is completely data-driven such that it can be easily adapted or extended to other types of questions. As a matter of fact, we are using the same system for all tasks with different trained models, yet still able to achieve high accuracy for most tasks we evaluate on."
    }, {
      "heading" : "5 Conclusion",
      "text" : "This paper presents a multi-field weighted indexing approach for question answering. Our system decomposes linguistic structures into multiple fields, indexes terms of individual fields, and retrieves the documents containing the answers with respect to the relevance scores weighted differently. We observe significant improvement as we add more semantic fields and apply averaged perceptron learning to statistically designate weights for the fields.\nIn the future, we plan to extend our work by integrating additional layers of fields (e.g., Freebase, WordNet). Furthermore, we plan to improve our NLP tools to enable even deeper understanding of the context for more complex question answering."
    } ],
    "references" : [ {
      "title" : "Transition-based Dependency Parsing with Selec",
      "author" : [ "Jinho D. Choi", "Andrew McCallum" ],
      "venue" : null,
      "citeRegEx" : "Choi and McCallum.,? \\Q2013\\E",
      "shortCiteRegEx" : "Choi and McCallum.",
      "year" : 2013
    }, {
      "title" : "Transitionbased Semantic Role Labeling Using Predicate Argument Clustering",
      "author" : [ "Jinho D. Choi", "Martha Palmer." ],
      "venue" : "Proceedings of ACL workshop on Relational Models of Semantics, RELMS’11, pages 37–45.",
      "citeRegEx" : "Choi and Palmer.,? 2011",
      "shortCiteRegEx" : "Choi and Palmer.",
      "year" : 2011
    }, {
      "title" : "Fast and Robust Part-of-Speech Tagging Using Dynamic Model Selection",
      "author" : [ "Jinho D. Choi", "Martha Palmer." ],
      "venue" : "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), ACL’12, pages 363–367,",
      "citeRegEx" : "Choi and Palmer.,? 2012",
      "shortCiteRegEx" : "Choi and Palmer.",
      "year" : 2012
    }, {
      "title" : "Paraphrase-Driven Learning for Open Question Answering",
      "author" : [ "Anthony Fader", "Luke Zettlemoyer", "Oren Etzioni." ],
      "venue" : "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, ACL’13, pages 1608–1618.",
      "citeRegEx" : "Fader et al\\.,? 2013",
      "shortCiteRegEx" : "Fader et al\\.",
      "year" : 2013
    }, {
      "title" : "Building Watson: An Overview",
      "author" : [ "David A. Ferrucci", "Eric W. Brown", "Jennifer ChuCarroll", "James Fan", "David Gondek", "Aditya Kalyanpur", "Adam Lally", "J. William Murdock", "Eric Nyberg", "John M. Prager", "Nico Schlaefer", "Christopher A. Welty" ],
      "venue" : null,
      "citeRegEx" : "Ferrucci et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Ferrucci et al\\.",
      "year" : 2010
    }, {
      "title" : "Learning Knowledge Graphs for Question Answering through Conversational Dialog",
      "author" : [ "Ben Hixon", "Peter Clark", "Hannaneh Hajishirzi." ],
      "venue" : "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Hixon et al\\.,? 2015",
      "shortCiteRegEx" : "Hixon et al\\.",
      "year" : 2015
    }, {
      "title" : "A Survey on Question Answering Technology from an Information Retrieval Perspective",
      "author" : [ "Oleksandr Kolomiyets", "Marie-Francine Moens." ],
      "venue" : "Information Sciences, 181(24):5412–5434.",
      "citeRegEx" : "Kolomiyets and Moens.,? 2011",
      "shortCiteRegEx" : "Kolomiyets and Moens.",
      "year" : 2011
    }, {
      "title" : "Linguistic kernels for answer re-ranking in question answering systems",
      "author" : [ "Alessandro Moschitti", "Silvia Quarteroni." ],
      "venue" : "Information and Processing Management, 47(6):825–842.",
      "citeRegEx" : "Moschitti and Quarteroni.,? 2011",
      "shortCiteRegEx" : "Moschitti and Quarteroni.",
      "year" : 2011
    }, {
      "title" : "Indexing on Semantic Roles for Question Answering",
      "author" : [ "Luiz Augusto Pizzato", "Diego Mollá." ],
      "venue" : "Proceedings of the 2nd workshop on Information Retrieval for Question Answering, IR4QA’08, pages 74–81.",
      "citeRegEx" : "Pizzato and Mollá.,? 2008",
      "shortCiteRegEx" : "Pizzato and Mollá.",
      "year" : 2008
    }, {
      "title" : "Question Answering Using Integrated Information Retrieval and Information Extraction",
      "author" : [ "Barry Schiffman", "Kathleen McKeown", "Ralph Grishman", "James Allan." ],
      "venue" : "The Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Schiffman et al\\.,? 2007",
      "shortCiteRegEx" : "Schiffman et al\\.",
      "year" : 2007
    }, {
      "title" : "Using Semantic Roles to Improve Question Answering",
      "author" : [ "Dan Shen", "Mirella Lapata." ],
      "venue" : "Proceedings of the 2007 Joint Conference on Empirical",
      "citeRegEx" : "Shen and Lapata.,? 2007",
      "shortCiteRegEx" : "Shen and Lapata.",
      "year" : 2007
    }, {
      "title" : "Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks",
      "author" : [ "Jason Weston", "Antoine Bordes", "Sumit Chopra", "Tomas Mikolov." ],
      "venue" : "arXiv:1502.05698.",
      "citeRegEx" : "Weston et al\\.,? 2015",
      "shortCiteRegEx" : "Weston et al\\.",
      "year" : 2015
    }, {
      "title" : "Question Answering Using Enhanced Lexical Semantic Models",
      "author" : [ "Wen-tau Yih", "Ming-Wei Chang", "Christopher Meek", "Andrzej Pastusiak." ],
      "venue" : "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, ACL’13, pages",
      "citeRegEx" : "Yih et al\\.,? 2013",
      "shortCiteRegEx" : "Yih et al\\.",
      "year" : 2013
    }, {
      "title" : "Semantic Parsing for Single-Relation Question Answering",
      "author" : [ "Wen-tau Yih", "Xiaodong He", "Christopher Meek." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL’14, pages 643–648.",
      "citeRegEx" : "Yih et al\\.,? 2014",
      "shortCiteRegEx" : "Yih et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "Towards machine reading, question answering has recently gained lots of interest among researchers from both natural language processing (Moschitti and Quarteroni, 2011; Yih et al., 2013; Hixon et al., 2015) and information retrieval (Schiffman et al.",
      "startOffset" : 137,
      "endOffset" : 207
    }, {
      "referenceID" : 12,
      "context" : "Towards machine reading, question answering has recently gained lots of interest among researchers from both natural language processing (Moschitti and Quarteroni, 2011; Yih et al., 2013; Hixon et al., 2015) and information retrieval (Schiffman et al.",
      "startOffset" : 137,
      "endOffset" : 207
    }, {
      "referenceID" : 5,
      "context" : "Towards machine reading, question answering has recently gained lots of interest among researchers from both natural language processing (Moschitti and Quarteroni, 2011; Yih et al., 2013; Hixon et al., 2015) and information retrieval (Schiffman et al.",
      "startOffset" : 137,
      "endOffset" : 207
    }, {
      "referenceID" : 9,
      "context" : ", 2015) and information retrieval (Schiffman et al., 2007; Kolomiyets and Moens, 2011).",
      "startOffset" : 34,
      "endOffset" : 86
    }, {
      "referenceID" : 6,
      "context" : ", 2015) and information retrieval (Schiffman et al., 2007; Kolomiyets and Moens, 2011).",
      "startOffset" : 34,
      "endOffset" : 86
    }, {
      "referenceID" : 6,
      "context" : "Pizzato and Mollá (2008) proposed a question prediction language model providing rich information and achieved improved speed and accuracy.",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 3,
      "context" : "Ferrucci et al. (2010) presented IBM Watson taking a hybrid approach between NLP and IR, and advanced the question answering task to another level.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 3,
      "context" : "Fader et al. (2013) proposed a paraphrase-driven perceptron learning approach using a seed lexicon.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 3,
      "context" : "Fader et al. (2013) proposed a paraphrase-driven perceptron learning approach using a seed lexicon. Our learning process is similar; however, it is distinguished in a way that we learn weights for individual fields instead of lexicons. Yih et al. (2014) introduced a semantic parsing framework for open domain question answering, which used convolutional neural networks for measuring similarities between decomposed entities.",
      "startOffset" : 0,
      "endOffset" : 254
    }, {
      "referenceID" : 3,
      "context" : "Fader et al. (2013) proposed a paraphrase-driven perceptron learning approach using a seed lexicon. Our learning process is similar; however, it is distinguished in a way that we learn weights for individual fields instead of lexicons. Yih et al. (2014) introduced a semantic parsing framework for open domain question answering, which used convolutional neural networks for measuring similarities between decomposed entities. Weston et al. (2015) presented the Memory Networks models designed to memorize information about known objects and ar X iv :1 60 4.",
      "startOffset" : 0,
      "endOffset" : 448
    }, {
      "referenceID" : 2,
      "context" : "For the generation of syntactic and semantic structures, we used the part-of-speech tagger (Choi and Palmer, 2012), the dependency parser (Choi and McCallum, 2013), the semantic role labeler (Choi and Palmer, 2011), and the coreference resolution tool in ClearNLP1.",
      "startOffset" : 91,
      "endOffset" : 114
    }, {
      "referenceID" : 0,
      "context" : "For the generation of syntactic and semantic structures, we used the part-of-speech tagger (Choi and Palmer, 2012), the dependency parser (Choi and McCallum, 2013), the semantic role labeler (Choi and Palmer, 2011), and the coreference resolution tool in ClearNLP1.",
      "startOffset" : 138,
      "endOffset" : 163
    }, {
      "referenceID" : 1,
      "context" : "For the generation of syntactic and semantic structures, we used the part-of-speech tagger (Choi and Palmer, 2012), the dependency parser (Choi and McCallum, 2013), the semantic role labeler (Choi and Palmer, 2011), and the coreference resolution tool in ClearNLP1.",
      "startOffset" : 191,
      "endOffset" : 214
    }, {
      "referenceID" : 11,
      "context" : "Our approach is evaluated on a subset of the bAbI tasks (Weston et al., 2015).",
      "startOffset" : 56,
      "endOffset" : 77
    }, {
      "referenceID" : 11,
      "context" : "Not that this data set has also been used for evaluating the Memory Networks approach to question answering (Weston et al., 2015).",
      "startOffset" : 108,
      "endOffset" : 129
    } ],
    "year" : 2016,
    "abstractText" : "This paper presents a precursory yet novel approach to the question answering task using structural decomposition. Our system first generates linguistic structures such as syntactic and semantic trees from text, decomposes them into multiple fields, then indexes the terms in each field. For each question, it decomposes the question into multiple fields, measures the relevance score of each field to the indexed ones, then ranks all documents by their relevance scores and weights associated with the fields, where the weights are learned through statistical modeling. Our final model gives an absolute improvement of over 40% to the baseline approach using simple search for detecting documents containing answers.",
    "creator" : "TeX"
  }
}