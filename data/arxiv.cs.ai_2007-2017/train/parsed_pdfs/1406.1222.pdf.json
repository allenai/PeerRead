{
  "name" : "1406.1222.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Discovering Structure in High-Dimensional Data Through Correlation Explanation",
    "authors" : [ "Greg Ver Steeg", "Aram Galstyan" ],
    "emails" : [ "gregv@isi.edu", "galstyan@isi.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Without any prior knowledge, what can be automatically learned from high-dimensional data? If the variables are uncorrelated then the system is not really high-dimensional but should be viewed as a collection of unrelated univariate systems. If correlations exist, however, then some common cause or causes must be responsible for generating them. Without assuming any particular model for these hidden common causes, is it still possible to reconstruct them? We propose an informationtheoretic principle, which we refer to as “correlation explanation”, that codifies this problem in a model-free, mathematically principled way. Essentially, we are searching for latent factors so that, conditioned on these factors, the correlations in the data are minimized (as measured by multivariate mutual information). In other words, we look for the simplest explanation that accounts for the most correlations in the data. As a bonus, building on this information-based foundation leads naturally to an innovative paradigm for learning hierarchical representations that is more tractable than Bayesian structure learning and provides richer insights than neural network inspired approaches [1].\nAfter introducing the principle of “Correlation Explanation” (CorEx) in Sec. 2, we show that it can be efficiently implemented in Sec. 3. To demonstrate the power of this approach, we begin Sec. 4 with a simple synthetic example and show that standard learning techniques all fail to detect highdimensional structure while CorEx succeeds. In Sec. 4.2.1, we show that CorEx perfectly reverse engineers the “big five” personality types from survey data while other approaches fail to do so. In Sec. 4.2.2, CorEx automatically discovers in DNA nearly perfect predictors of independent signals relating to gender, geography, and ethnicity. In Sec. 4.2.3, we apply CorEx to text and recover both stylistic features and hierarchical topic representations. After briefly considering intriguing theoretical connections in Sec. 5, we conclude with future directions in Sec. 6."
    }, {
      "heading" : "2 Correlation Explanation",
      "text" : "Using standard notation [2], capital X denotes a discrete random variable whose instances are written in lowercase. A probability distribution over a random variable X , pX(X = x), is shortened\nar X\niv :1\n40 6.\n12 22\nv2 [\ncs .L\nG ]\n3 1\nO ct\nto p(x) unless ambiguity arises. The cardinality of the set of values that a random variable can take will always be finite and denoted by |X|. If we have n random variables, then G is a subset of indices G ⊆ Nn = {1, . . . , n} and XG is the corresponding subset of the random variables (XNn is shortened to X). Entropy is defined in the usual way as H(X) ≡ EX [− log p(x)]. Higherorder entropies can be constructed in various ways from this standard definition. For instance, the mutual information between two random variables, X1 and X2 can be written I(X1 : X2) = H(X1) +H(X2)−H(X1, X2). The following measure of mutual information among many variables was first introduced as “total correlation” [3] and is also called multi-information [4] or multivariate mutual information [5].\nTC(XG) = ∑ i∈G H(Xi)−H(XG) (1)\nFor G = {i1, i2}, this corresponds to the mutual information, I(Xi1 : Xi2). TC(XG) is nonnegative and zero if and only if the probability distribution factorizes. In fact, total correlation can also be written as a KL divergence, TC(XG) = DKL(p(xG)|| ∏ i∈G p(xi)).\nThe total correlation among a group of variables, X , after conditioning on some other variable, Y , is simply TC(X|Y ) = ∑ iH(Xi|Y ) −H(X|Y ). We can measure the extent to which Y explains the correlations in X by looking at how much the total correlation is reduced.\nTC(X;Y ) ≡ TC(X)− TC(X|Y ) = ∑ i∈Nn I(Xi : Y )− I(X : Y ) (2)\nWe use semicolons as a reminder that TC(X;Y ) is not symmetric in the arguments, unlike mutual information. TC(X|Y ) is zero (and TC(X;Y ) maximized) if and only if the distribution of X’s conditioned on Y factorizes. This would be the case if Y were the common cause of all the Xi’s in which case Y explains all the correlation in X . TC(XG|Y ) = 0 can also be seen as encoding local Markov properties among a group of variables and, therefore, specifying a DAG [6]. This quantity has appeared as a measure of the redundant information that the Xi’s carry about Y [7]. More connections are discussed in Sec. 5.\nOptimizing over Eq. 2 can now be seen as a search for a latent factor, Y , that explains the correlations in X . We can make this concrete by letting Y be a discrete random variable that can take one of k possible values and searching over all probabilistic functions of X , p(y|x).\nmax p(y|x) TC(X;Y ) s.t. |Y | = k, (3)\nThe solution to this optimization is given as a special case in Sec. A. Total correlation is a functional over the joint distribution, p(x, y) = p(y|x)p(x), so the optimization implicitly depends on the data through p(x). Typically, we have only a small number of samples drawn from p(x) (compared to the size of the state space). To make matters worse, if x ∈ {0, 1}n then optimizing over all p(y|x) involves at least 2n variables. Surprisingly, despite these difficulties we show in the next section that this optimization can be carried out efficiently. The maximum achievable value of this objective occurs for some finite k when TC(X|Y ) = 0. This implies that the data are perfectly described by a naive Bayes model with Y as the parent and Xi as the children.\nGenerally, we expect that correlations in data may result from several different factors. Therefore, we extend the optimization above to include m different factors, Y1, . . . , Ym.1\nmax Gj ,p(yj |xGj ) m∑ j=1 TC(XGj ;Yj) s.t. |Yj | = k,Gj ∩Gj′ 6=j = ∅ (4)\nHere we simultaneously search subsets of variables Gj and over variables Yj that explain the correlations in each group. While it is not necessary to make the optimization tractable, we impose an additional condition on Gj so that each variable Xi is in a single group, Gj , associated with a single “parent”, Yj . The reason for this restriction is that it has been shown that the value of the objective can then be interpreted as a lower bound on TC(X) [8]. Note that this objective is valid\n1Note that in principle we could have just replaced Y in Eq. 3 with (Y1, . . . , Ym), but the state space would have been exponential in m, leading to an intractable optimization.\nand meaningful regardless of details about the data-generating process. We only assume that we are given p(x) or iid samples from it.\nThe output of this procedure gives us Yj’s, which are probabilistic functions of X . If we iteratively apply this optimization to the resulting probability distribution over Y by searching for some Z1, . . . , Zm̃ that explain the correlations in the Y ’s, we will end up with a hierarchy of variables that forms a tree. We now show that the optimization in Eq. 4 can be carried out efficiently even for high-dimensional spaces and small numbers of samples."
    }, {
      "heading" : "3 CorEx: Efficient Implementation of Correlation Explanation",
      "text" : "We begin by re-writing the optimization in Eq. 4 in terms of mutual informations using Eq. 2.\nmax G,p(yj |x) m∑ j=1 ∑ i∈Gj I(Yj : Xi)− m∑ j=1 I(Yj : XGj ) (5)\nNext, we replace G with a set indicator variable, αi,j = I[Xi ∈ Gj ] ∈ {0, 1}.\nmax α,p(yj |x) m∑ j=1 n∑ i=1 αi,jI(Yj : Xi)− m∑ j=1 I(Yj : X) (6)\nThe non-overlapping group constraint is enforced by demanding that ∑ j̄ αi,j̄ = 1. Note also that we dropped the subscript Gj in the second term of Eq. 6 but this has no effect because solutions must satisfy I(Yj : X) = I(Yj : XGj ), as we now show.\nFor fixed α, it is straightforward to find the solution of the Lagrangian optimization problem as the solution to a set of self-consistent equations. Details of the derivation can be found in Sec. A.\np(yj |x) = 1\nZj(x) p(yj) n∏ i=1 ( p(yj |xi) p(yj) )αi,j (7)\np(yj |xi) = ∑ x̄ p(yj |x̄)p(x̄)δx̄i,xi/p(xi) and p(yj) = ∑ x̄ p(yj |x̄)p(x̄) (8)\nNote that δ is the Kronecker delta and that Yj depends only on the Xi for which αi,j is non-zero. Remarkably, Yj’s dependence onX can be written in terms of a linear (in n, the number of variables) number of parameters which are just the marginals, p(yj), p(yj |xi). We approximate p(x) with the empirical distribution, p̂(x̄) = ∑N l=1 δx̄,x(l)/N . This approximation allows us to estimate marginals with fixed accuracy using only a constant number of iid samples from the true distribution. In Sec. A we show that Eq. 7, which defines the soft labeling of any x, can be seen as a linear function followed by a non-linear threshold, reminiscent of neural networks. Also note that the normalization constant for any x, Zj(x), can be calculated easily by summing over just |Yj | = k values. For fixed values of the parameters p(yj |xi), we have an integer linear program for α made easy by the constraint ∑ j̄ αi,j̄ = 1. The solution is α ∗ i,j = I[j = arg maxj̄ I(Xi : Yj̄)]. However, this leads to a rough optimization space. The solution in Eq. 7 is valid (and meaningful, see Sec. 5 and [8]) for arbitrary values of α so we relax our optimization accordingly. At step t = 0 in the optimization, we pick αt=0i,j ∼ U(1/2, 1) uniformly at random (violating the constraints). At step t+ 1, we make a small update on α in the direction of the solution.\nαt+1i,j = (1− λ)α t i,j + λα ∗∗ i,j (9) The second term, α∗∗i,j = exp ( γ(I(Xi : Yj)−maxj̄ I(Xi : Yj̄)) ) , implements a soft-max which converges to the true solution for α∗ in the limit γ → ∞. This leads to a smooth optimization and good choices for λ, γ can be set through intuitive arguments described in Sec. B.\nNow that we have rules to update both α and p(yj |xi) to increase the value of the objective, we simply iterate between them until we achieve convergence. While there is no guarantee to find the global optimum, the objective is upper bounded by TC(X) (or equivalently, TC(X|Y ) is lower bounded by 0). Pseudo-code for this approach is described in Algorithm 1 with additional details provided in Sec. B and source code available online2. The overall complexity is linear in the number\n2Open source code is available at http://github.com/gregversteeg/CorEx.\ninput : A matrix of size ns × n representing ns samples of n discrete random variables set : Set m, the number of latent variables, Yj , and k, so that |Yj | = k output: Parameters αi,j , p(yj |xi), p(yj), p(y|x(l)) for i ∈ Nn, j ∈ Nm, l ∈ Nns , y ∈ Nk, xi ∈ Xi Randomly initialize αi,j , p(y|x(l)); repeat\nEstimate marginals, p(yj), p(yj |xi) using Eq. 8; Calculate I(Xi : Yj) from marginals; Update α using Eq. 9; Calculate p(y|x(l)), l = 1, . . . , ns using Eq. 7;\nuntil convergence; Algorithm 1: Pseudo-code implementing Correlation Explanation (CorEx)\nof variables. To bound the complexity in terms of the number of samples, we can always use minibatches of fixed size to estimate the marginals in Eq. 8.\nA common problem in representation learning is how to pick m, the number of latent variables to describe the data. Consider the limit in which we set m = n. To use all Y1, . . . , Ym in our representation, we would need exactly one variable, Xi, in each group, Gj . Then ∀j, TC(XGj ) = 0 and, therefore, the whole objective will be 0. This suggests that the maximum value of the objective must be achieved for some value of m < n. In practice, this means that if we set m too high, only some subset of latent variables will be used in the solution, as we will demonstrate in Fig. 2. In other words, if m is set high enough, the optimization will result in some number of clusters m′ < m that is optimal with respect to the objective. Representations with different numbers of layers, different m, and different k can be compared according to how tight of a lower bound they provide on TC(X) [8]."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Synthetic data",
      "text" : "To test CorEx’s ability to recover latent structure from data we begin by generating synthetic data according to the latent tree model depicted in Fig. 1 in which all the variables are hidden except for the leaf nodes. The most difficult part of reconstructing this tree is clustering of the leaf nodes. If a clustering method can do that then the latent variables can be reconstructed for each cluster easily using EM. We consider many different clustering methods, typically with several variations\nof each technique, details of which are described in Sec. C. We use the adjusted Rand index (ARI) to measure the accuracy with which inferred clusters recover the ground truth. 3\nWe generated samples from the model in Fig. 1 with b = 8 and varied c, the number of leaves per branch. The Xi’s depend on Yj’s through a binary erasure channel (BEC) with erasure probability δ. The capacity of the BEC is 1− δ so we let δ = 1− 2/c to reflect the intuition that the signal from each parent node is weakly distributed across all its children (but cannot be inferred from a single child). We generated max(200, 2n) samples. In this example, all the Yj’s are weakly correlated with the root node, Z, through a binary symmetric channel with flip probability of 1/3.\nFig. 1 shows that for a small to medium number of variables, all the techniques recover the structure fairly well, but as the dimensionality increases only CorEx continues to do so. ICA and hierarchical clustering compete for second place. CorEx also perfectly recovers the values of the latent factors in this example. For latent tree models, recovery of the latent factors gives a global optimum of the objective in Eq. 4. Even though CorEx is only guaranteed to find local optima, in this example it correctly converges to the global optimum over a range of problem sizes.\nNote that a growing literature on latent tree learning attempts to reconstruct latent trees with theoretical guarantees [9, 10]. In principle, we should compare to these techniques, but they scale as O(n2) − O(n5) (see [31], Table 1) while our method is O(n). In a recent survey on latent tree learning methods, only one out of 15 techniques was able to run on the largest dataset considered (see [31], Table 3), while most of the datasets in this paper are orders of magnitude larger than that one.\n0 1\nI(Yj : Xi)\nt = 0 t = 10\nUncorrelated variables\ni = 1, . . . , nv j = 1 ... m ↵i,j t = 50\nFigure 2: (Color online) A visualization of structure learning in CorEx, see text for details.\nFig. 2 visualizes the structure learning process.4 This example is similar to that above but includes some uncorrelated random variables to show how they are treated by CorEx. We set b = 5 clusters of variables but we used m = 10 hidden variables. At each iteration, t, we show which hidden variables, Yj , are connected to input variables, Xi, through the connectivity matrix, α (shown on top). The mutual information is shown on the bottom. At the beginning, we started with full connectivity, but with nothing learned we have I(Yj : Xi) = 0. Over time, the hidden units “compete” to find a group of Xi’s for which they can explain all the correlations. After only ten iterations the overall structure appears and by 50 iterations it is exactly described. At the end, the uncorrelated random variables (Xi’s) and the hidden variables (Yj’s) which have not explained any correlations can be easily distinguished and discarded (visually and mathematically, see Sec. B)."
    }, {
      "heading" : "4.2 Discovering Structure in Diverse Real-World Datasets",
      "text" : ""
    }, {
      "heading" : "4.2.1 Personality Surveys and the “Big Five” Personality Traits",
      "text" : "One psychological theory suggests that there are five traits that largely reflect the differences in personality types [11]: extraversion, neuroticism, agreeableness, conscientiousness and openness to experience. Psychologists have designed various instruments intended to measure whether individuals exhibit these traits. We consider a survey in which subjects rate fifty statements, such as, “I am the life of the party”, on a five point scale: (1) disagree, (2) slightly disagree, (3) neutral, (4) slightly agree, and (5) agree.5 The data consist of answers to these questions from about ten\n3Rand index counts the percentage of pairs whose relative classification matches in both clusterings. ARI adds a correction so that a random clustering will give a score of zero, while an ARI of 1 corresponds to a perfect match.\n4A video is available online at http://isi.edu/˜gregv/corex_structure.mpg. 5Data and full list of questions are available at http://personality-testing.info/\n_rawdata/.\nthousand test-takers. The test was designed with the intention that each question should belong to a cluster according to which personality trait the question gauges. Is it true that there are five factors that strongly predict the answers to these questions?\nCorEx learned a two-level hierarchical representation when applied to this data (full model shown in Fig. C.2). On the first level, CorEx automatically determined that the questions should cluster into five groups. Surprisingly, the five clusters exactly correspond to the big five personality traits as labeled by the test designers. It is unusual to recover the ground truth with perfect accuracy on an unsupervised learning problem so we tried a number of other standard clustering methods to see if they could reproduce this result. We display the results using confusion matrices in Fig. 3. The details of the techniques used are described in Sec. C but all of them had an advantage over CorEx since they required that we specify the correct number of clusters. None of the other techniques are able to recover the five personality types exactly.\nInterestingly, Independent Component Analysis (ICA) [12] is the only other method that comes close. The intuition behind ICA is that it find a linear transformation on the input that minimizes the multi-information among the outputs (Yj). In contrast, CorEx searches for Yj’s so that multiinformation among the Xi’s is minimized after conditioning on Y . ICA assumes that the signals that give rise to the data are independent while CorEx does not. In this case, personality traits like “extraversion” and “agreeableness” are correlated, violating the independence assumption."
    }, {
      "heading" : "4.2.2 DNA from the Human Genome Diversity Project",
      "text" : "Next, we consider DNA data taken from 952 individuals of diverse geographic and ethnic backgrounds [13]. The data consist of 4170 variables describing different SNPs (single nucleotide polymorphisms).6 We use CorEx to learn a hierarchical representation which is depicted in Fig. 3. To evaluate the quality of the representation, we use the adjusted Rand index (ARI) to compare clusters induced by each latent variable in the hierarchical representation to different demographic variables in the data. Latent variables which substantially match demographic variables are labeled in Fig. 3.\nThe representation learned (unsupervised) on the first layer contains a perfect match for Oceania (the Pacific Islands) and nearly perfect matches for America (Native Americans), Subsaharan Africa, and gender. The second layer has three variables which correspond very closely to broad geographic regions: Subsaharan Africa, the “East” (including China, Japan, Oceania, America), and EurAsia."
    }, {
      "heading" : "4.2.3 Text from the Twenty Newsgroups Dataset",
      "text" : "The twenty newsgroups dataset consists of documents taken from twenty different topical message boards with about a thousand posts each [14]. For analyzing unstructured text, typical feature en-\n6Data, descriptions of SNPs, and detailed demographics of subjects is available at ftp://ftp.cephb. fr/hgdp_v3/.\ngineering approaches heuristically separate signals like style, sentiment, or topics. In principle, all three of these signals manifest themselves in terms of subtle correlations in word usage. Recent attempts at learning large-scale unsupervised hierarchical representations of text have produced interesting results [15], though validation is difficult because quantitative measures of representation quality often do not correlate well with human judgment [16].\nTo focus on linguistic signals, we removed meta-data like headers, footers, and replies even though these give strong signals for supervised newsgroup classification. We considered the top ten thousand most frequent tokens and constructed a bag of words representation. Then we used CorEx to learn a five level representation of the data with 326 latent variables in the first layer. Details are described in Sec. C.1. Portions of the first three levels of the tree keeping only nodes with the highest normalized mutual information with their parents are shown in Fig. 4 and in Fig. C.1.7\nTo provide a more quantitative benchmark of the results, we again test to what extent learned representations are related to known structure in the data. Each post can be labeled by the newsgroup it belongs to, according to broad categories (e.g. groups that include “comp”), or by author. Most learned binary variables were active in around 1% of the posts, so we report the fraction of activations that coincide with a known label (precision) in Fig. 4. Most variables clearly represent sub-topics of the newsgroup topics, so we do not expect high recall. The small portion of the tree shown in Fig. 4 reflects intuitive relationships that contain hierarchies of related sub-topics as well as clusters of function words (e.g. pronouns like “he/his/him” or tense with “have/be”).\nOnce again, several learned variables perfectly captured known structure in the data. Some users sent images in text using an encoded format. One feature matched all the image posts (with perfect precision and recall) due to the correlated presence of unusual short tokens. There were also perfect matches for three frequent authors: G. Banks, D. Medin, and B. Beauchaine. Note that the learned variables did not trigger if just their names appeared in the text, but only for posts they authored. These authors had elaborate signatures with long, identifiable quotes that evaded preprocessing but created a strongly correlated signal. Another variable with perfect precision for the “forsale” newsgroup labeled comic book sales (but did not activate for discussion of comics in other newsgroups). Other nearly perfect predictors described extensive discussions of Armenia/Turkey in talk.politics.mideast (a fifth of all discussion in that group), specialized unix jargon, and a match for sci.crypt which had 90% precision and 55% recall. When we ranked all the latent factors according to a normalized version of Eq. 2, these examples all showed up in the top 20."
    }, {
      "heading" : "5 Connections and Related Work",
      "text" : "While the basic measures used in Eq. 1 and Eq. 2 have appeared in several contexts [7, 17, 4, 3, 18], the interpretation of these quantities is an active area of research [19, 20]. The optimizations we\n7An interactive tool for exploring the full hierarchy is available at http://bit.ly/corexvis.\ndefine have some interesting but less obvious connections. For instance, the optimization in Eq. 3 is similar to one recently introduced as a measure of “common information” [21]. The objective in Eq. 6 (for a single Yj) appears exactly as a bound on “ancestral” information [22]. For instance, if all the αi = 1/β then Steudel and Ay [22] show that the objective is positive only if at least 1 + β variables share a common ancestor in any DAG describing them. This provides extra rationale for relaxing our original optimization to include non-binary values of αi,j .\nThe most similar learning approach to the one presented here is the information bottleneck [23] and its extension the multivariate information bottleneck [24, 25]. The motivation behind information bottleneck is to compress the data (X) into a smaller representation (Y ) so that information about some relevance term (typically labels in a supervised learning setting) is maintained. The second term in Eq. 6 is analogous to the compression term. Instead of maximizing a relevance term, we are maximizing information about all the individual sub-systems of X , the Xi. The most redundant information in the data is preferentially stored while uncorrelated random variables are completely ignored.\nThe broad problem of transforming complex data into simpler, more meaningful forms goes under the rubric of representation learning [26] which shares many goals with dimensionality reduction and subspace clustering. Insofar as our approach learns a hierarchy of representations it superficially resembles “deep” approaches like neural nets and autoencoders [27, 28, 29, 30]. While those approaches are scalable, a common critique is that they involve many heuristics discovered through trial-and-error that are difficult to justify. On the other hand, a rich literature on learning latent tree models [31, 32, 9, 10] have excellent theoretical properties but do not scale well. By basing our method on an information-theoretic optimization that can nevertheless be performed quite efficiently, we hope to preserve the best of both worlds."
    }, {
      "heading" : "6 Conclusion",
      "text" : "The most challenging open problems today involve high-dimensional data from diverse sources including human behavior, language, and biology.8 The complexity of the underlying systems makes modeling difficult. We have demonstrated a model-free approach to learn successfully more coarsegrained representations of complex data by efficiently optimizing an information-theoretic objective. The principle of explaining as much correlation in the data as possible provides an intuitive and fully data-driven way to discover previously inaccessible structure in high-dimensional systems.\nIt may seem surprising that CorEx should perfectly recover structure in diverse domains without using labeled data or prior knowledge. On the other hand, the patterns discovered are “low-hanging fruit” from the right point of view. Intelligent systems should be able to learn robust and general patterns in the face of rich inputs even in the absence of labels to define what is important. Information that is very redundant in high-dimensional data provides a good starting point.\nSeveral fruitful directions stand out. First, the promising preliminary results invite in-depth investigations on these and related problems. From a computational point of view, the main work of the algorithm involves a matrix multiplication followed by an element-wise non-linear transform. The same is true for neural networks and they have been scaled to very large data using, e.g., GPUs. On the theoretical side, generalizing this approach to allow non-tree representations appears both feasible and desirable [8]."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank Virgil Griffith, Shuyang Gao, Hsuan-Yi Chu, Shirley Pepke, Bilal Shaw, Jose-Luis Ambite, and Nathan Hodas for helpful conversations. This research was supported in part by AFOSR grant FA9550-12-1-0417 and DARPA grant W911NF-12-1-0034."
    }, {
      "heading" : "A Derivation of Eqs. 7 and 8",
      "text" : "We want to optimize the following objective.\nmax α,p(yj |x) m∑ j=1 n∑ i=1 αi,jI(Yj : Xi)− m∑ j=1 I(Yj : X)\ns.t. ∑ yj p(yj |x) = 1 (10)\nIn principle, we would also like ∀i, j, αi,j ∈ {0, 1}, ∑ j̄ αi,j̄ = 1, but we begin by solving the optimization for fixed α.\nWe proceed using Lagrangian optimization. We introduce a Lagrange multiplier λj(x) for each value of x and each j to enforce the normalization constraint and then reduce the constrained optimization problem to the unconstrained optimization of the objective Ltot = ∑ j Lj . We show the solution for a single Lj , but drop the j index to avoid clutter. (For fixed α, the optimization for different j totally decouple.)\nL = ∑ x,y p(x)p(y|x) (∑ i αi(log p(y|xi)− log(p(y)))− (log p(y|x)− log(p(y))) ) + ∑ x λ(x)( ∑ y p(y|x)− 1)\nNote that we are optimizing over p(y|x) and so the marginals p(y|xi), p(y) are actually linear functions of p(y|x). Next we take the functional derivatives with respect to p(y|x) and set them equal to 0. Note that this can be done symbolically and proceeds in similar fashion to the detailed calculations of information bottleneck [25].\nThis leads to the following condition.\np(yj |x) = 1\nZ(x) p(yj) n∏ i=1 ( p(yj |xi) p(yj) )αi,j But this is only a formal solution since the marginals themselves are defined in terms of p(y|x).\np(y) = ∑ x p(x)p(y|x), p(y|xi) = ∑ xj 6=i p(y|x)p(x)/p(xi)\nThe partition constant, Z(x) can be easily calculated by summing over just |Yj | terms.\nImagine we are given l = 1, . . . , N samples, x(l), drawn from unknown distribution p(x). If x is very high dimensional, we do not want to enumerate over all possible values of x. Instead, we consider the quantity in Eq. 7 and Eq. 8 only for observed samples.\np(y|x(l)) = 1 Z(x(l)) p(y) n∏ i=1 ( p(y|x(l)i ) p(y) )αi,j In log-space, this has an even simpler form.\nlog p(y|x(l)) = (1− ∑ i αi) log p(y) + n∑ i=1 αi log p(y|x(l)i )− logZ(x (l))\nThat is, the probabilistic label, y, for any sample, x, is a linear combination of weighted terms for each xi. We recover p(y|x) by doing a nonlinear transformation consisting of exponentiation and normalization.\nThe consistency requirements which are sums over the state space of x can be replaced with sample expectations.\np(y) = ∑ x p(x)p(y|x) ≈ 1 N N∑ j=1 p(y|x(l)),\nwith similar estimates for the marginals p(y|xi). In practice, to limit the complexity in terms of the number of samples, we can choose a random subset of samples at each iteration and estimate the probabilistic labels and marginals only for them. The details of the optimization over α are described in the next section.\nSpecial case for Eq. 3 Note that the optimization in Eq. 3 corresponds to j = 1, . . . ,m with m = 1 and ∀i, αi = 1.\nConvergence The updates for the iterative procedure described here are guaranteed not to decrease the objective at each step and are guaranteed to converge to a local optimum. Theoretical details are described elsewhere [8].\nB Implementation Details for CorEx\nAs pointed out in Sec. 5, the objective in Eq. 6 (for a single Yj) appears exactly as a bound on “ancestral” information [22]. We use this fact to motivate our choice for parameters in Eq. 9. Consider the soft-max function we use to define α∗.\nα∗i,j = exp ( γ(I(Xi : Yj)−max\nj̄ I(Xi : Yj̄)) ) First of all, we allow γi,j to take different values at different i, j. We start by enforcing the form γi,j = Cj/H(Xi). That way, the value of the exponent depends on normalized mutual information (NMI) instead of mutual information. The minimum value that can occur is exp(−Cj). We set Cj = 1. If the difference of NMI’s take the minimum value of −1, we get α∗i,j ∼ 1/3. According to the Steudel and Ay bound, Xi can still contribute to a non-negative value for the part of objective Eq. 6 that involves Yj as long as Xi shares a common ancestor with at least 1/α+ 1 other variables. At the beginning of the learning, this is desirable as it allows all Yj’s to learn significant structures even starting from small values of αi,j . However, as the computation progresses, we would like to force the soft-max function to get closer to the true hard max solution. To that end, we set γi,j = (1 +Dj)/H(Xi), where Dj = 500 · |EX(− logZj(x))|. The Dj term represents the amount of correlation learned by Yj [8]. For instance, if all p(yj |xi) = p(yj), logZj(x) = 0 and Yj has not learned anything. As the computation progresses and Yj learns more structure, we smoothly transition to a hard-max constraint.\nIn all the experiments shown here, we set |Yj | = k = 2. For convergence of Algorithm 1, we check when the magnitude of changes of EX log(− ∑ j Zj(x)) consistently falls below a threshold of 10−5 or when we reach 1000 iterations, whichever occurs first. We set λ = 0.3 based on several tests with synthetic data.\nWe construct higher order representation from the bottom up. After applying Algorithm 1, we take the most likely value of Yj for each sample in the dataset. Then we apply CorEx again using these labels as the input. In principle, this sample of Y ’s does not accurately reflect p(y) = ∑ x p(y|x)p(x) and a more nuanced approximation like contrastive divergence could be used. However, in practice it seems that CorEx typically learns nearly deterministic functions of x, so that the maximum likelihood labels well reflect the true distribution.\nIn Fig. 2, we suggested that uncorrelated random variables could be easily detected. In practice we used a threshold that this was the case if MI(Xi : Yparent(Xi))/min(H(Xi), H(Y )) < 0.05. At higher layers of representation, this helps us identify root nodes. For the DNA example in Fig. 3, “gender” was a root node, but for visual simplicity all root nodes were connected at the top level. Following similar reasoning as above, we can also check which Yj’s have learned significant structure by looking at the value of EX(− logZj(x)).\nC Implementation Details for Comparisons\nWe represented the data from the binary erasure channel either as integers (0[Xi = 0], 1[Xi = e], 2[Xi = 1]) for methods that deal with categorical data, or as floating numbers on the unit interval for methods that require data of that form, (0[Xi = 0], 0.5[Xi = e], 1[Xi = 1]). In principle, we could also have treated “erased” information as missing. But we treated erasure as another outcome in all cases, including for CorEx.\nCorEx naturally handles missing information (you can see that Eq. 7 can be easily marginalized to find labels even if some variables are missing). We had to use this fact for the DNA dataset which did have some SNPs missing for some samples. In fact, because CorEx is, in a sense, looking for the most redundant information, it is quite robust to missing information.\nWe will now briefly describe the settings for various learning algorithms learned. We used implementations of standard learning techniques in the scikit library for comparisons [34] (v. 0.14). We only used the standard, default implementation for k-means, PCA, ICA, and “hierarchical clustering” using the Ward method. For spectral clustering we used a Gaussian kernel for the affinity matrix and a nearest neighbors affinity matrix using 3 or 10 neighbors. For spectral bi-clustering we tried clustering either the data matrix or its transpose. We set the number of clusters to be m in the direction of variables and either 10 or 32 clusters for the variables. Note that the true number of clusterings in the sample space was 28. For NMF we tried Projected Gradient NMF and NMF with the two types of implemented sparseness constraints. For the restricted Boltzmann machine, we used a single layer network with m units and learning rates 0.01, 0.05, 0.1. To cluster the input variables, we looked for the neuron with the maximum magnitude weight. For dimensionality reduction techniques like LLE and Isomap, we used either 3 or 10 nearest neighbors and looked for a m component representation. Then we clustered variables by looking at which variables contributed most to each component of the representation.\nC.1 Twenty newsgroups\nFor the twenty newsgroups dataset, scikit has built-in function for retrieving and processing the dataset. We used the command below, resulting in a dataset with 18, 846 posts. (Several different versions of this dataset are in circulation.) sklearn.datasets.fetch_20newsgroups(subset=’all’,\nremove=(’headers’,’footers’,’quotes’)).\nBecause we are doing unsupervised learning, we combined the parts of the data normally split into training and testing sets. The attempt to strip footers turned out to be particularly relevant. The heuristic to do so looks for a single line at the end of the file, set apart from the others by a blank line or some number of dashes. Obviously, many signature lines fail to conform to this format and this resulted in strongly correlated signals. This led to features at layer 1 that were perfect predictors of authors, like Gordon Banks, who always included a quote: “Skepticism is the chastity of the intellect, and it is shameful to surrender it too soon.”\nWe considered any collection of upper or lower-case letters as a “word”. All characters were lowercased. Apostrophes were removed (so that “I’ve” becomes “ive”). We considered the top ten thousand most frequent words. For the thousand most frequent words, for each document we recorded a 0 if the word was not present, 1 if it was present but occurred with less than the average frequency, or a 2 if it occurred with more than average frequency. For the remaining words we just used a 0/1 representation to reflect if a word was present.\nCorEx details For the twenty newsgroups data, we trained CorEx in a top-down-bottom-up way. We started with a “low resolution” model with m = 100 hidden units and k = 2. We used the result of this optimization to construct 100 large groups of words. Then, for each (now much smaller) group of words, we applied CorEx again to get a more fine-grained representation (and then we discard the representation that we used to find the original clustering). The result was a representation at layer 1 with 326 variables. At the next layer we fixed m = 50, all units were used. At the next two layers we fixed m = 10, 1, respectively.\n�� ��\n��� ��\n��\n����\n��� ��� �� ��� � �� ��� �� ���\n�� �� ���\n��� ��\n�� ��\n��� ��� ���\n�� ��\n��� ��\n��� ���\n��\n�� ��\n�\n�� ���\n���\n�� ��\n�\n����� �\n��� ��� �� �� �\n�� �� ���\n�\n��� ���\n�� ��\n�� ���\n��� �\n��� ��\n��� �� ���\n��� �� ���\n�\n�� ��\n��� ��\n�� ��\n��� ��\n�� ��\n�� �\n�� ��� �\n�� ��\n�\n��� ���\n��� �� �\n�� ��\n��� ���\n�\n�� ��\n�� ���\n��� ��\n��� ���\n��\n��� ���\n�� �� ���\n��� ��\n��� ���\n��\n�� ��\n��� ��\n�� ��\n�� �\n��� ����\n��� �\n�� ��\n�\n�� ��\n�� �� ��� � �� ���\n�� ����\n��\n��� ���\n�� ��\n�\n��� ���\n�� ���\n��� ��\n��� ���\n��� ��\n��\n�� ��� � �� ��� ���� ��� �\n��� ��� �� �� �� ���\n��� ��\n��� ���\n�� ��\n��� ��\n��� ���\n�� ��\n��� ��\n��� ���\n�\n�� �� �� �\n��� ���\n��� ���\n��� ��\n��� ��\n�� ��\n���� �\n�� ���\n�� ��\n��� �\n��� �\n��� � �� ���\n��� �\n�� �\n�� �� �� �\n�� ����\n��� �� �\n�� ���\n��\n�� ���\n�� ���\n�� �� �\n��� �� �\n��� ���\n���\n�� ��\n���\n��� ��\n�\n�� �� ���\n���\n�� ���\n��� �\n��� �� �� �� ��\n�� ��\n�� ����\n� ��\n�� ���\n���\n�� ��\n��� ���\n�� ��� ��� �� �\n�� ���\n��� ��� �� ��� ��� ��� �� �� �� �� ��� �� ��\n�� ��\n��� ��\n�� ���\n��� �� �� �� ���\n�� ���\n��� ���\n�� � ��\n�� �\n�� ��\n��� ��\n��� �\n�� ��\n�� ��� �� ���\n�\n�� �� �\n�� �� ��\n�� ���\n��� �\n��� ���\n��\n�� ��\n�� ��\n�� ���\n�\n���\n��� ��\n�� ���\n��� �\n�� ��\n�� �� �� �\n�� ���\n���\n��� � ��� �� �� �� �� �\n�� ���\n�\n��� ���\n�� ��� �� ��� �� ��� � �� ���\n�� ��\n�� �\n�� ��\n��� ��\n�� ���\n�� ���\n�� ���\n��� �\n�� ����\n�\n�� ���\n��� �\n�� ���\n��\n�� ��\n��� �\n���\n��� ���\n��\n�� ���\n��� ��\n��� �� ���\n�� �\n�� �� ��\n�� ��\n��� ��\n��� ��\n��� ���\n��� ��\n�� ��\n�\n��� ���\n��\n��\n��� ���\n��\n�� ��\n��� ��\n�� ���\n�� ���\n��� �\n���\n�� ���\n��\n�� ��� ��� ���\n�\n��� ��� �� �� ���\n�� �� ���\n�� ��\n��� ���\n���\n�� �� ���\n�� �\n��� ���\n���\n��� ���\n��� ���\n��\n�� ��\n��� �\n��� �� �\n�� ����� � �� ���\n��� ��� �� �� ��� ��� �� ��\n��� ��\n�� ��\n��� ���\n��\n��� ��� �� �� ���\n�� ���\n�� �\n��� ���\n��� ���\n��\n�� ��\n�� �� ���\n��� �� � ��� ��� ��� �� �� �� ��� � ��� ��� ��� ��� �� ��\n�� ���\n�\n�� ��\n��� ��\n��\n����\n��� ���\n�\n��� ��\n�� ��\n�� �� �\n�� ���\n��� �� ���\n��� ��\n��� ���\n�� �� ���\n��� ���\n���\n��� �� ���\n�� ��\n��� ���\n��\n�� �� ��\n�� ���\n��\n��� �� ���\n�� ���\n��\n�� ���\n��\n��� ���\n�� ��\n��� ��\n�� ��\n��� �\n��� ���\n�� �\n�� ��\n���� �\n��� ���\n��\n�� ��\n�� ���\n�� ��\n�� ��\n��� ��\n��� �� ���\n��� �\n�� ��\n��� ���\n�� �\n�� ���\n�� ��\n��\n��� ��\n��� �� �\n�� �� �� �\n�� �� �� ���\n��� ���\n�� ��\n�\n��� �� �� �� �� ���\n�\n�� ��\n��� ��\n��� ���\n����� ���\n�� ���\n�� �� �� ���\n��� �\n�� ��\n��� �\n��� �\n�� ��� ��\n��� ��� ���\n�\n��� ���\n��\n��� ��� � ��� ��� � � �� ��� ��\n�� ���\n� ���\n��� ��\n��\n��� ���\n�� �\n��� ��\n��\n��� ���\n�\n�� ��\n��� ��\n�� �� ��\n��� ���\n�� �� �� ��\n�� �� ��� ���\n��� ��� ��� ��� ��� �\n��� ��\n�\n�� ���\n�\n��� ���\n�� ��\n��\n�� ���\n�\n��� �\n�� ��\n��� ��\n�� ���\n��� �\n�� ���\n�\n�� ���\n���� ���\n�� �\n�� ���\n��� �� �\n��� ���\n�� ���\n��\n�� ���\n�� ���\n�� �� �\n��� ��� � ��� ��� ���\n��� ���\n�� �� �\n�� ��\n�� ��\n�� �� ���\n�� �� � ��� ���\n��� ���\n��\n��� ���\n��\n�� ��\n��� ��\n�� ��� ��� ��� ��� �\n�� ���\n��\n�� ��\n��� � ��\n�� ���\n�� ���\n��\n�� ���\n��� �\n��� ���\n��� �� ���\n�� ��� �� ��� ��� �� �� �� �� ��� ��� ��\n�� ��\n��� �� ��� �� ��\n��� ��\n�� ��\n��� �� �\n��� ��\n��� �� �\n���\n��� ���\n���\n��� �� ���\n��� ��\n���\n��� ��� �� �� �� �� ��\n�� ��\n��� �� ���\n���\n��� ��� �� ��� �� ��� � ��� ��� � �� ��� ��\n��� ��� �� ��\n��\n��� ���\n��� ���\n��� ��� ��� �\n�� ��\n�� ���\n��\n��� ���\n���\n�� �� � �� ��\n��� ��\n�� ��\n�� ��\n��� ��\n�� ��\n��� ��\n�� ��\n��� �� �� ��\n��� ��\n�� ��\n��� ��\n�� ��\n��� ��\n�� ��\n��� ��\n�� ��\n��� ��\n��� �� ��� �� �� �� ���� ��� �\n�� ���\n��� ��\n��� ��\n�� �� �\n�� ����\n� �� �� ��� �� �� �� ���\n��� �� �\n�� �� ��\n�� ���\n��� ���\n�� ���\n��� �\n��� ��� �� �\n��� �� �\n��� �� ��\n��� ��\n��� �\n�� ���\n��� �\n�� ���\n��� ���\n�\n��� �\n��� � ���\n�� ���\n��\n�� ��\n��� ��\n�� ���\n��� ���\n�\n��� ��\n��� ���\n��\n�� ���� ���\n�\n��� ���\n� �� ��\n��� ��\n�� ��\n��� �� �� ��\n��� ��\n��� ��\n�� �� �� � ��\n�� ���\n��� �� ��\n���\n�� ���\n�\n��� ��\n�� ��\n��� �\n�� ��\n���\n�� ��� �� ��\n��\n�� ��\n��� ��\n�� ��\n��� ��\n�� ��\n��� ��\n�� ��\n��� ��\n�� ���\n����� ���\n���\n�� �� ���\n���\n�� ��\n�� �� �\n�� � �\n��\n�� ���\n��� ��� ��� �� �� ���\n�\n��\n����\n�� �� ��\n�� �� ��\n��\n�� ��\n��� ��� ��� ��� ��� �� ��� ��\n��� �\n�� ���\n�� ��\n�� ���\n���\n�� ��\n��\n�� ��\n��� ��\n���\n��� �\n���\n��� �� � �� ��\n�� ��� �� �\n��� ���\n��� �� ���\n��� �\n�� ���\n�� ��\n�� ���\n��\n��� ���\n��� �� ���\n��� �� ���\n�� ���\n��� �\n�� ��\n�� ��\n�� ���\n��� �\n�� ���\n��� �\n��� �\n��� ����\n��� �\n��� ���\n��\n�� �� ���\n���\n�� ��\n�� �� ��\n�� ���\n��\n��� ��\n���\n��� �\n�� ��\n�� ���\n��� ��\n�� ��\n��� ���\n��\n��� �� �\n��� ���\n�� �� ���\n��\n�� ���\n�� ��� ���\n�� ��� ��� ��� �� �� � �� ���� � �� �� �� �� ��� �� ��\n�� �� ���\n��� ���\n�� ��\n��� ��\n�\n�� ���\n�� ��\n�� ���\n��� ����\n�\n��� �� �� ��� ���\n�� ���\n� �� ���\n�� ���\n��� ���\n�\n��� ��� �� ���\n�� ��� �� ���\n��� ��\n�\n�� ��\n�� �� ��\n��� ���\n�� �� �� ���\n���\n�� ��\n��� ���\n��� �� ��\n���\n�� ��\n��� ��\n�� �� ���\n��\n�� ���\n��� ��\n��� ���\n�� ��\n�� ��\n��� ��\n��� ���\n�\n�� ���\n�\n��� ���\n��\n�� ��\n��� ���\n�\n��� ���\n��\n��� ���\n��\n�� ��\n��� ��\n�\n�� �� ��\n���\n��� ���\n��� ��\n��� �\n��� ��\n��� ���\n��� �� ��\n��\n��� ��\n��\n�� �� �\n�� ��\n��� �� ��� ����\n�\n�� ��\n�� ��\n�� ���\n�\n��� ���\n�� ���\n��� ��\n�\n��� ��\n��� ��\n�� ���\n��� ���\n��� �� �� ���\n�\n�� ����\n�\n�� ��\n�\n�� ���\n��� �\n��� �\n�� ���\n��� ���\n��� �� ���\n��\n��� �� ��\n�� ��\n� ���\n��� �\n��� ��� � �� �� ��� �� ��\n�� ��\n��� ��\n�� ��\n��\n��� ��\n�� ���� ��\n�� ��\n��� ��\n��� �� �\n��� ��� ��\n���\n�� ��\n��� ���\n��� ��\n��� ��\n��� ��\n�\n�� ���\n�\n�� ���\n��\n�� ���\n�\n�� ���\n���\n��� ���\n�\n�� ��� �� �� ��� ��\n��� ���\n�� ���\n�\n�� �� ��� ��\n��� �\n��� ���\n��\n�� ��\n��� ��\n��� �� �� ���\n�� ���\n�\n��� ���\n��� ��� � �� ��� � �� ��� ���\n�� ��\n��� ��\n�� ����\n��� �� ���\n��\n��� ���\n�� ���\n��� ���\n��� ���\n���\n��� ���\n��\n��� ���\n���\n��� ���\n���\n��� ��\n��� ��\n�� ��\n�� ���\n��� �\n��� ���\n�� � �� �� ���\n�\n�� �� ���� ��� �� ���\n�� ���\n�� �� ���\n�� �\n�� �� ��\n��\n��� ���\n�� �� �\n��� ��\n��� �� �\n�� ��\n�� ��\n��\n�� �� �\n�� ���\n��� �\n�� ���\n��� ��\n�� ��\n��� ��\n��� �\n��� �\n��� ��\n��� ���\n��� ��� ��� �� ��� �\n�� ���\n�� �\n�� �� ���\n��� �\n��� �\n��� ���\n�� ���\n�\n��� ��� ��� ��� �\n��� �\n��� ���\n��� �\n�� ��\n�� ��\n��� ��\n��� ���\n�� ���\n��� �� � �� ���\n�\n��� ���\n��\n�� ���\n�\n��� ���\n��\n�� ���\n��� ��\n��� ���\n���\n�� ���\n�\n�� ��\n��� �\n�� ���\n��� ��\n�� ��\n�\n�� ���\n��� ��\n��� ��\n�� ��\n��� ��\n�� ���\n���\n�� ��\n��� ��\n�� ��\n��� ��\n�� ��� ��� �� �� ��� ��\n�� �� ��\n��� �\n�� ��\n�� ��\n��� ����\n��� ���\n��\n��� ���\n�� ��� �� ��\n��� �\n�� ��\n���\n��� ���\n�\n��� ����\n��\n�� ���\n�� ���\n�� �� ���\n�\n�� ��\n�� ��� � ��� � �� ��� �\n�� ��\n��� ��\n�� �� �� �� ��� ��� �\n��� �\n��� ���\n��� ���\n�� ���\n�� ���\n���\n��� ���\n�� ���\n�� ���\n���\n�� ���\n��\n�� ��\n�\n�� ���\n��� ���� ���\n� ��\n���� �\n�� ���\n��\n��� ���\n�\n��� ���\n��\n��� ���\n�� �� � ��� �� �� �� ��� ��� �� �� � �� �\n��� �\n�� ��\n�� �\n�� �� �� �� ��� ��\n��� ���\n��\n�� ���\n��� ��\n�� ���\n��\n��� ���\n�� ���\n�� ���\n�\n�� ��� �� � ��� ��\n��� ���\n��\n��� �\n��� ��\n�� ���\n�\n��� ���\n�\n��� ��\n��� ��\n��� ���\n��\n�� ���\n��� ��\n�� �\n�� �\n��� ���\n��\n��� ���\n��\n��� ���\n��\n��� ���\n��\n�� ���\n��� ��\n�� �\n�� ���\n��� �\n�� ���\n�� ��\n�\n�� ���\n��� ��\n��\n�� ���\n�\n��� ���\n��\n�� ���\n� ��� ��� ��\n��� ���\n�\n�� �� ��� ��� �\n��� ���\n�� ��\n��\n�� �� � ���\n��� ��\n��� �\n�� �\n��� �\n�� �� ���\n�� �\n�� ���\n��\n�� �� �\n��� �\n��� �� �� �\n��� ��� ��� �� �� �� ���\n�� �� ���\n��\n��� �� ��� �\n�� �\n��� ��\n�� ��\n���\n�� ���\n�� ��� � �� �\n�� ���\n�\n�� �\n��� ��\n��� ���\n�� �\n�� �� ���� �� �� �� �� �\n��� ���\n��� ���\n�� ��\n�� ���\n�\n��� ���\n�� ��\n��� ��\n��� �\n��� ��� �� �� �� �\n��� ���\n��\n��� ���\n�� ��\n��� �� �� �� ���\n��� ���\n� ��\n��� ��\n��� ��\n��� ��� �� ��� �� �� ��� �� �\n��� ��� �� �� �� �\n��� ���\n� �� ���\n���\n��� ��� �� ��� ��� �� �� ���\n�� �\n�� �� ���\n�� �� ��\n��\n�� ��� �� �\n�� ��\n��� ��\n�� ��\n��� ��\n�� ���\n�� �\n��� �\n��� ���\n�� �� ��\n�� ��\n�� ���\n�� ���\n��\n��� ���\n���\n��� ���\n�� ���\n�\n��� �\n��� ���\n��� ���\n��� ��� ��� ���\n���\n��� ��� �� ��� �� ��� �� �� � ��� �� �� ��� �� �� ��� �� �� ��� � �� ��� ��\n�� ���\n�\n�� ��� �� ��� ��\n��� ��\n���\n��� ��� � ���\n�� ��\n��� ���\n�\n��� ���\n��\n��� ���\n��� ��\n�� ��\n�� ���\n�\n��� ���\n�\n�� ���\n��� ��� �� �� �� ��� ��� � �� ��� ��� �� �� ��� �� ��� ��\n��� ���\n�� ���\n�\n��� ���\n��� ��\n���\n�� �� ���\n��\n��� �� ��\n�� ��\n��� ��\n�� � ��\n�� ��\n��� ��� ��\n��� �\n�� ��\n��� ��\n��� ���\n�\n�� ��\n�� ��\n�\n���\n��� ���\n�� �� ��\n�� ���\n�\n��� �� �\n�� ��� � ��� ��� �� ��� �\n��� ���\n�\n��� ��� � ��� ��� � �� �� �� �� ��� �� �\n�� ���\n���� ���\n�\n�� ���\n�� ���\n�� ���\n�� �� � ��\n��� ���\n�� �� � ���\n�� �\n�� ��\n�\n��� �� �\n��� ��� �� ��� ��� ��� ��� �� ��� �� �� ���� �\n�� ��\n�� �\n� �� ���\n��� �\n��� �\n�� ��� ��� �� ��\n�� ��\n�� ��\n�� �� �� ��\n�� �� �� �� �� ��\n�� �� �\n���\n��� �� ��� � �� ���\n�� ���\n��� � �� � ��� �� �\n��� �\n��� ���\n� ���\n��� ��\n��� ���\n��\n�� ��� ��� �� �� �� ��� � ��� ���\n��� �\n�� ���\n�\n��� ���\n� ���\n��� �\n�� ���\n�\n�� ��\n��� ���\n���\n��� ���\n�� ��� �� ��� ��� �\n��� ���\n��� ���\n���\n�� ���\n���\n��� �\n��� ���\n���\n�� ��\n��� �� �� ��\n��\n�� ��\n��� ��\n��� �� �� �\n��� �� ���\n�� �� ���\n��� ��\n�\n�� ��\n��� ��\n��� ��\n��� ���\n��� ���\n�� � �� ��� �� ���\n��\n�� ��� ���\n��� �� ��� �� �\n�� ���\n���\n��� ��\n�� ���\n��� ���\n��\n��� ��\n� ��� ��\n��� ��\n��� ���\n���\n�� ���\n��� �\n�� �� ���\n�� ���\n�� ��\n�� ��\n�� �\n�� ���\n�\n���\n�� ���\n�\n��� ���\n�� ���\n��� �\n�� ���\n�\n�� ��\n��� �� �\n�� ���\n��� �\n�� �� �� ��� ��� � ��� �� ��\n�� ��� �� ��\n��� �\n�� �� ���\n� ��� ���\n��� �� ���\n��� ���\n��� ���\n��\n��� ���\n�\n�� ���\n��� ��\n��� � ��� ��\n�� ���\n��\n�� ���\n��� �\n��� ��� �� ��� �� �\n�� ��\n��� ��\n�� ��\n��� ��\n��\n����\n�� ���\n���\n��� �\n�� �� ��\n�� ��\n�\n�� ���\n��� ���\n��\n�� ���\n��� ���\n�� �� ���\n�� ���\n�� �\n�� ���\n�� ���\n�\n��� ���\n��� �� ���\n��� ��\n�� ���\n�\n��� ���\n�\n�� ��\n��� �� �� ��� �� ���\n�� �� ��� ��\n��� ���\n��� ���\n�� �� ��� ��\n� ��\n��� ���\n��� �� ���\n���\n�� ���\n�\n��� �\n�� ��\n���\n��� �\n��� ���\n�\n��� ��� ��� ��� ��� ��\n�� ��\n�� �� �� ��� ��� �� ���\n��� ���\n�� ��� �� �\n��� ���\n�� �\n��� ���\n��\n�� ��\n��� ��\n��� ��\n�� ���\n��� �\n�� ��\n�� ���\n�� ���\n��� ��\n�� ��\n��� ��\n�� ���\n�� �� �\n�� �� ��\n�� ���\n� �� �� ��� �\n��� ��� �� �� �� ��� ���\n�� ���\n�� �\n��� ���\n��� ���\n��� ��\n�� ���\n�\n�� ��\n�� ��\n�� ��\n��� ����\n��� �� �� �� �� ���\n�\n�� ���\n�� �� �� �� �� �� �� �� �� ��� � �� �\n��� ��\n��� ��\n�� ���\n��� ��\n�� ��\n��� � �� ���\n�� ��\n�� ��� � �� �� ��� �\n��� ���\n��\n��� ���\n�� �\n�� ��\n��� ����\n�\n�� ���\n�� ��� �� ��� ��� ��� �\n��� ���\n��� ��\n��� ���\n�� �\n�� ��\n�� ���\n�� ���\n�� ��\n�� ���\n��\n�� ���\n���\n�� ��\n��� �� ��� ���� � ��� ��\n��� ��\n��� ��\n�� ���\n��� �\n��� ���\n�� ���\n�� �\n��� ���\n���\n��� ��\n�� ���\n��� �\n��� ���\n�\n�� ��� ���\n�� ���\n�� ���\n�� �\n�� ��\n��� ��\n�� ���\n��\n�� ��\n��\n���\n�� ��\n��� ��\n�� ���\n�\n��� ���\n��\n�� � �� ���\n��� �\n��� �� �\n�� ��\n�\n�� �� �\n�� ���\n�\n��� ��\n�� �� ���\n�\n��� �� �\n�� ���\n���\n�� �� ��\n��� ���\n��� �� ���\n�� �\n�� ���\n�\n��\n��� ���\n��� ���\n�� �\n�� ��\n���� �\n��� ��\n��� ���\n��� ���\n��� �\n��� �� ���\n��� ��\n��� �\n�� ��\n�� ��\n�� ���\n�\n�� �� ���\n�� ��\n��� ��\n�� ���\n�� �� �� �� ��� �� ���\n� �\n��� ���\n�� �� �� ��\n�� ���\n��� �� �� ��� ���\n�� ���\n��� �\n���\n�� ���\n�� �\n�� ���\n�� ��\n��� ��\n�� ��\n��� ��\n�� ���\n�\n�� ��\n��\n�\n��� �� �\n��� ��\n��� ���\n��� �� �\n��� ���\n��\n��� ���\n��� ���\n��� �� �\n�� �� �� �� ��\n��� ���\n�� ��� � ��� ��\n��� �� ��\n��� �\n�� �� ���\n��� ���\n��\nFi gu\nre C\n.1 :T\nhe bo\ntto m\nth re\ne la\nye rs\nof th\ne hi\ner ar\nch ic\nal re\npr es\nen ta\ntio n\nle ar\nne d\nfo rt\nhe tw\nen ty\nne w\nsg ro\nup s\nda ta\nse t,\nke ep\nin g\non ly\nth e\nth re\ne le\naf no\nde s\nw ith\nth e\nhi gh\nes t\nno rm\nal iz\ned m\nut ua\nli nf\nor m\nat io\nn w\nith th\nei rp\nar en\nts an\nd up\nto ei\ngh tb\nra nc\nhe s\npe rn\nod e\nat la\nye r2\n.F or\nla te\nnt va\nri ab\nle s,\nw e\nlis ta\nn ab\nbr ev\nia tio\nn of\nth e\nne w\nsg ro\nup it\nbe st\nco rr\nes po\nnd s\nto al\non g\nw ith\nth e\npr ec\nis io\nn. Fo\nra zo\nom ab\nle ve\nrs io\nn on\nlin e\ngo to h t t p : / / b i t . l y / c o r e x v i s\n.\n(N ) C\nha ng\ne m\ny m\noo d\na lo\nt\n(N ) W\nor ry\na bo\nut th\nin gs\n(N ) S\nel do\nm fe\nel b\nlu e\n(N ) H\nav e\nfr eq\nue nt\nm oo\nd sw\nin gs\n(N ) G\net ir\nrit at\ned e\nas ily\n(N ) O\nfte n\nfe el\nb lu e (N\n) G et\ns tr\nes se d ou t e as ily\n(N ) A\nm re\nla xe d m os t o f t he ti m\ne\n(N ) A\nm e\nas ily\nd is\ntu rb\ned\n(N ) G\net u\nps et\ne as\nily\n(C ) G\net c\nho re s do ne ri gh t a w ay (C ) O fte n\nfo rg et to p ut th in gs b ac k in th ei r p ro pe r p la ce\n(C ) L\nik e\nor de\nr\n(C ) A\nm e\nxa ct\nin g\nin m\ny w\nor k\n(C ) L\nea ve\nm y\nbe lo\nng in\ngs a\nro un\nd (C ) P\nay a\ntte nt\nio n\nto d\net ai ls (C ) M ak e a m es s of th in gs (C ) S hi rk m y du tie s (C ) F ol lo\nw a\ns ch\ned ul\ne\n(C ) A\nm a\nlw ay\ns pr\nep ar\ned\n(E ) A\nm th e lif e of th e\npa rt\ny\n(E ) K\nee p in th e ba ck gr\nou nd\n(E ) S\nta rt\nco nv\ner sa\ntio ns\n(E ) H\nav e\nlit tle\nto s\nay\n(E ) A\nm q\nui et\nar ou\nnd s\ntr an\nge rs\n(E ) D\non 't\nta lk\na lo\nt\n(E ) F\nee l c\nom fo\nrt ab le ar ou nd p eo pl e\n(E ) T\nal k\nto a\nlo t o\nf d iff\ner en t p eo pl e at p ar\ntie s\n(E ) D\non 't\nlik e to dr aw a tte nt io n to m ys el f (E ) D on 't m in d\nbe in\ng th\ne ce\nnt er\nof a\ntte nt\nio n\n(O ) H\nav e\na vi\nvi d\nim ag\nin at\nio n\n(O ) A\nm n\not in\nte re\nst ed\nin a\nbs tr\nac t i\nde as\n(O ) H\nav e\nex ce\nlle nt\nid ea\ns\n(O ) U\nse d\niffi cu\nlt w\nor ds\n(O ) S\npe nd\nti m e re fle ct in g on th\nin gs\n(O ) A\nm fu ll of id ea s\n(O ) H\nav e\na ric\nh vo\nca bu\nla ry\n(O ) H\nav e\ndi ffi\ncu lty\nun de\nrs ta\nnd in g ab st ra ct id ea s\n(O ) D\no no\nt h av\ne a\ngo od\nim ag\nin at\nio n\n(O ) A\nm q\nui ck\nto u\nnd er\nst an d th in gs\n(A ) F\nee l l\nitt le\nco nc\ner n\nfo r o\nth er\ns\n(A ) A\nm n\not in\nte re\nst ed\nin o\nth er\np eo\npl e' s pr ob le m s\n(A ) H\nav e\na so\nft he\nar t\n(A ) M\nak e\npe op\nle fe el at e as e\n(A ) A\nm in\nte re\nst ed\nin p\neo pl\ne\n(A ) I\nns ul\nt p eo\npl e\n(A ) S\nym pa\nth iz\ne w ith ot he rs ' f ee lin gs (A ) A m n ot re\nal ly\nin te\nre st\ned in\no th\ner s\n(A ) T\nak e\ntim e\nou t f\nor o\nth er\ns\n(A ) F\nee l o\nth er s' em ot io ns\nFi gu\nre C\n.2 :C\nor E\nx le\nar ns\na hi\ner ar\nch ic\nal re\npr es\nen ta\ntio n\nfr om\npe rs\non al\nity su\nrv ey\ns w\nith 50\nqu es\ntio ns\n.T he\nnu m\nbe ro\nfl at\nen tn\nod es\nin th\ne tr\nee an\nd nu\nm be\nro fl\nev el\ns ar e au to m at ic al ly de te rm in ed .T he qu es tio n gr ou pi ng s at th e fir st le ve le xa ct ly co rr es po nd to th e “b ig fiv e” pe rs on al ity tr ai ts .T he pr efi x of ea ch qu es tio n in di ca te s th e tr ai tt es td es ig ne rs in te nd ed it to m ea su re .T he th ic kn es s of ea ch ed ge re pr es en ts m ut ua li nf or m at io n be tw ee n fe at ur es an d th e si ze of ea ch no de re pr es en ts th e to ta l co rr el at io n th at th e no de ca pt ur es ab ou ti ts ch ild re n."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2014,
    "abstractText" : "<lb>We introduce a method to learn a hierarchy of successively more abstract repre-<lb>sentations of complex data based on optimizing an information-theoretic objec-<lb>tive. Intuitively, the optimization searches for a set of latent factors that best ex-<lb>plain the correlations in the data as measured by multivariate mutual information.<lb>The method is unsupervised, requires no model assumptions, and scales linearly<lb>with the number of variables which makes it an attractive approach for very high<lb>dimensional systems. We demonstrate that Correlation Explanation (CorEx) auto-<lb>matically discovers meaningful structure for data from diverse sources including<lb>personality tests, DNA, and human language.<lb>",
    "creator" : "LaTeX with hyperref package"
  }
}