{
  "name" : "1706.06529.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "A Divergence Bound for Hybrids of MCMC and Variational Inference and an Application to Langevin Dynamics and SGVI",
    "authors" : [ "Justin Domke" ],
    "emails" : [ "<domke@cs.umass.edu>." ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "One of the simplest Markov chain Monte Carlo (MCMC) methods is Langevin dynamics (Grenander & Miller, 1994; Robert & Casella, 2004, Sec. 7.8.5) where one repeats gradient steps with injected Gaussian noise. Namely, one iterates\nz z + ✏ 2 rz log p(z) +\np ✏⌘, (1)\nwhere ⌘ is sampled from a standard Gaussian distribution and ✏ is a step-size that may decay over time. To get exact samples, a Metropolis-Hastings rejection step should be used. However, as the step-size ✏ becomes smaller, the acceptance probability goes to one, and so this can be disregarded. In the Bayesian inference setting, z denotes unknown parameters and p(z) represents a posterior over\n1College of Computing and Information Sciences, University of Massachusetts, Amherst, USA. Correspondence to: Justin Domke <domke@cs.umass.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nthem. Computing p(z) thus requires a full pass over the dataset. The idea of Stochastic Gradient Langevin Dynamics (Welling & Teh, 2011) is to get an unbiased estimate of the gradient of log p(z) by approximating the posterior using a minibatch. Since the gradient term is multiplied by ✏, the variance of its error has order ✏2, while the variance of p ✏⌘ has order ✏. Thus, with a small step ✏, the injected noise dominates, meaning one can might expect these to have a stationary distribution close to the target p(z).\nThe idea of variational inference is to posit a simple family of distributions qw(z) and then optimize w to minimize the KL-divergence from qw(z) to the target p(z). (In a Bayesian setting, this is equivalent to maximizing the evidence lower bound.) If one were to use plain gradient ascent to perform this optimization, this leads to the iterate of\nw w + ✏ 2 rwEqw(Z)[log p(Z) log qw(Z)], (2)\nwhere the gradient step ✏ is divided by two for convenience, and may again decay over time. In practice, doing gradient ascent like this has two difficulties. First, in the Bayesian inference setting, computing log p(z) requires a full pass over the data. This can be approximated without bias using a minibatch. The second difficulty stems from the expectation of log p(z) with respect to qw(z) in Eq. 2. The gradient of this expectation can be approximated in different ways using samples from qw(Z) (Section 3.2). Since this gives unbiased estimates of the gradient, stochastic optimization methods, such as stochastic gradient descent, will converge to a local optima when the step-size becomes small.\nIntuitively, both MCMC and variational methods can be thought of as trying to find a high-probability region of log p(z), with different strategies to encourage entropy to get coverage of the distribution. In MCMC, entropy is created by randomness in the Markov chain, while in variational methods the KL-divergence directly measures the entropy of the variational distribution. It is natural to think that hybrid methods might employ fractions of these two strategies. This paper does so by defining a distribution over the parameters w of the approximating family q(z|w). This distribution q(w) is designed to minimize a bound on the KL-divergence of the resulting marginal distribution\nar X\niv :1\n70 6.\n06 52\n9v 1\n[ cs\n.L G\n] 2\n0 Ju\nn 20\n17\nq(z) to the target p(z). Intuitively, when defining q(w), there are two ways to encourage entropy, namely to give higher density to vectors w where q(Z|w) has high entropy or to encourage entropy over w itself.\nGiven a distribution q(w, z) and a target distribution p(z), this paper uses two bounds on the marginal divergence Dtrue = KL (q(Z)kp(Z)) . First, one can upper-bound by the conditional divergence D0 = KL (q(Z|W )kp(Z)). Second, one can “adjoin” some distribution p(w|z) to p(z), and bound Dtrue by the joint divergence D1 = KL (q(W, Z)kp(W, Z)). This paper takes a convex combination of these two bounds, i.e. uses the family of bounds D = (1 )D0 + D1 . For any between 0 and 1, the distribution q(w) is derived that minimizes this bound for fixed p and a fixed approximating family q(z|w) (Theorem 3). Then, one can sample from this distribution over w by (stochastic) Langevin dynamics. This turns out to lead to the iterate\nw w + ✏ 2 rw\n⇣ Eqw [log p(Z) + ( 1) log qw(Z)]\n+ log r(w) ⌘ + p ✏ ⌘, (3)\nwhere r(w) is related to the adjoining distribution p(w|z) and may be thought of as a “base measure” 1 over the space of w. Of course, exactly computing this gradient is intractable in general, but the same strategies used for variational inference can be used to efficiently compute an un-\n1To see why r is necessary, consider a reparameterization of w. In order for the marginal distribution q(z) to stay the same, the density of the base measure must be transformed corresponding to the reparameterization. Put another way, without r(w), the results of inference would depend on the parameterization of w, even with an arbitrarily small step-size.\nbiased estimate.\nIt can be shown that as ! 0, the solution concentrates in the optima of the variational optimization, and thus the sampling algorithm reduces to gradient ascent on the variational objective. Meanwhile, as will be discussed in Sections 2.3 and 5 it is best to make r a function of such that w with high entropy have less density when approaches 1. When this is true, then as ! 1, only w indexing highly concentrated distributions over z retain density, and so sampling from q(w) is essentially just a reparameterization of sampling from p(z), and the algorithm reduces to Langevin dynamics.\nThus, for the extreme cases of = 0 and = 1, this algorithm reduces to variational inference and Langevin dynamics, respectively. Informally, at these extremes, the algorithm is “fast but approximate” and “slow but accurate”. In the intermediate range, the algorithm exhibits new behavior with a fine-grained trade-off between speed and accuracy. Thus, this approach gives a smooth continuum of algorithms with different priorities of speed and accuracy."
    }, {
      "heading" : "1.1. Notation",
      "text" : "This paper uses three notational conventions that are not universal. First, the conditional probability q(z|w) will alternatively be written as qw(z) when convenient. Second, A ' B indicates that A and B have the same expected value or (if A is a constant) that B is an unbiased estimator of A. Finally, upper and lower-cases indicate what terms are random variables in a KL-divergence. So, KL (q(Z|w)kp(Z|w)) = Eq(Z|w) log (q(Z|w)/p(Z|w)) is the divergence between q(z|w) and p(z|w) for a fixed w, while KL (q(Z|W )kp(Z|W )) =\nEq(W,Z) log (q(Z|W )/p(Z|W )) is a standard conditional divergence."
    }, {
      "heading" : "2. Information Theoretic Results",
      "text" : "This section derives a few results from an information theoretic viewpoint, without any particular regard for form of the target distribution, or how one might sample from it. Proofs are given in the appendix."
    }, {
      "heading" : "2.1. Preliminaries",
      "text" : "Let p(z) be the target distribution. For simplicity, z is treated here as a continuous variable, although the results in this section remain true if it is discrete. There is some fixed set of conditional distributions q(z|w), which may also be written as qw(z). In principle, one might like to know how to set q(w) such that the resulting marginal distribution over z, is close to p(z), as measured by the KLdivergence,\nDtrue = KL (q(Z)kp(Z)) = Eq(Z) log q(Z)\np(Z) . (4)\nThis quantity is difficult to control directly, since the marginal q(z) typically cannot be evaluated in closed form. One bound comes from the conditional KL-divergence, as stated in the following Lemma.\nLemma 1. The divergence from q(z) to p(z) is\nKL (q(Z)kp(Z)) = KL (q(Z|W )kp(Z))| {z } D0 Iq[W, Z],\n(5) where D0 = Eq(W,Z) log (q(Z|W )/p(Z)) is the conditional divergence and Iq = KL (q(Z, W )kq(Z)q(W )) denotes mutual information under q.\nA second bound comes from adjoining some distribution p(w|z) to p(z). Then, the following lemma is essentially just a re-statement of the chain rule for the KL-divergence (Cover & Thomas, 2006, Thm. 2.5.3).\nLemma 2. The KL-divergence between q(z) and p(z) can be written as\nKL(q(Z)kp(Z))) = KL (q(W, Z)kp(W, Z))| {z } D1\nKL (q(W |Z)kp(W |Z)) . (6)\nSince the mutual information in Eq. 5 and the conditional divergence on the right of Eq. 6 are both non-negative, both D0 and D1 are upper-bounds on Dtrue. Note that if p(w|z) = q(w), then D1 = D0, and so Lemma 1 follows from Lemma 2."
    }, {
      "heading" : "2.2. Divergence Bound and its Minimizer",
      "text" : "This paper is based on convex combination of D0 and D1 , parameterized by some 2 [0, 1], namely\nD = (1 )D0 + D1. (7)\nSince D0 and D1 are upper-bounds, so is D . The following theorem gives the distribution q(w) to minimize D .\nTheorem 3. For fixed values of and p(w|z), the distribution q(w) that minimizes D is\nq⇤(w) = exp s(w) A) (8)\nA = log\nZ\nw\nexp s(w)\ns(w) = log p(w) KL (q(Z|w)kp(Z|w))\n1 1 KL (q(Z|w)kp(Z)) ."
    }, {
      "heading" : "Moreover, at q⇤, the objective value is D⇤ = A.",
      "text" : "Since A is an upper-bound on the KLdivergence, A must be non-positive. This can also be seen directly, since it can be written as A = log R w\np(w) exp( KL (q(Z|w)kp(Z|w)) 1 1 KL (q(Z|w)kp(Z))), and the inner divergences as well as 1 1 are non-negative. To understand the connection of this bound to variational inference and MCMC, one can look at behavior where = 0 or where = 1. For the former case, one obtains a result closely related to the solution to the variational inference problem. Remark 4. In the limit where ! 0 the divergence bound at the optimal q⇤ becomes\nlim !0 D⇤ = inf w KL (q(Z|w)kp(Z)) . (9)\nSimilarly, when ! 0, q⇤(w) concentrates at the minimizer(s) of this divergence. (Formally, with multiple global optima with the same divergence, q(w) will concentrate equally around all minimizers.)\nFor the case where = 1 , it is easy to see that\nD⇤1 = log Z\nw p(w) exp ( KL (q(Z|w)kp(Z|w))) , (10)\nwhich will be zero if p(w) is only supported on w where the divergence between q(z|w) and p(z|w) is zero. This may initially seem like a strange condition, given that p(w) results from both the target distribution p(z) and the adjoined distribution p(w|z). However, the next section gives more specifics.\n2.3. Specific Form for p(w|z) The results in the previous section were written without without considering the specific form of p(w|z). This paper takes the approach of defining\np(w|z) = r(w)q(z|w) rz , (11)\nwhere r(w) is a possibly improper distribution over w and rz = R w\nr(w)q(z|w) is a normalizer. Note that p(w|z) is still well-defined as long as the integral defining rz exists.\nHere, we assume for convenience that rz is a constant, not depending on z. Enforcing rz to be constant essentially means choosing r(w) in such a way that it doesn’t “favor” any z over any other since if q(w) / r(w) then q(z) is uniform over z.\nLemma 5. If p(w|z) = r(w)q(z|w)/rz and rz is a constant, then the solution in Thm. 3 holds with\ns(w) = log r(w) log rz + Eqw(Z)[ 1 log p(Z) + (1 1) log qw(Z)]. (12)\nThis gives a distribution over q(w) that can be written in various equivalent ways, such as\nq⇤(w) / r(w) exp 1E(w) + ( 1 1)H(w) (13)\n=r(w) exp H(w) 1KL (qw(Z)kp(Z))\n=r(w) exp1/ E qw(Z) [log p(Z) + ( 1) log qw(Z)] ,\nwhere H(w) = Eqw(Z)[log qw(Z)] is the entropy of qw and E(w) = Eqw(z)[log p(Z)].\nHere, r(w) plays a role similar to a base density in an exponential family. If one simply used r(w) = 1, then p(w|z) may not be well-defined. Further, without a base density, the marginal q(z) would depend on the particular parameterization of w. To see this, take single parameter w, and imagine re-parameterizing so the negative reals are “squashed” by a factor of two. The base density must increase by a factor of two for the negative reals to compensate in order to leave the marginal q(z) unchanged under the reparameterization."
    }, {
      "heading" : "2.4. Latent variables in variational inference",
      "text" : "Adjoining p(w|z) to p(z) to define D1 above is strongly related to auxiliary random variables (Agakov & Barber, 2004) explored in variational inference to increase the representative power of q, e.g. by including latent stochastic transition operators (Salimans et al., 2015), random Gaussian process mappings (Tran et al., 2016), or hierarchical variables (Ranganath et al., 2016). Imagine doing variational inference with q✓(z, w), where now w is auxiliary\nand the goal is to set ✓ so that q✓(z) ⇡ p(z). Since w must be integrated out, the entropy of z under q is typically intractable, but can be bounded by augmenting p with some distribution p(w|z) and then optimizing the joint KLdivergence over z and w, similarly to Lemma 2. Note two differences. First, here the distribution over w is mathematically derived to minimize the bound, while previous work numerically optimizes ✓ at run-time. Second, previous work also optimizes parameters of the distribution p(w|z) to further improve the bound, while here it is left fixed (for each ) for simplicity (Sec. 5). Future work might explore optimizing p(w|z) at run-time in the current paper’s framework."
    }, {
      "heading" : "3. Bayesian Inference",
      "text" : "This section considers algorithms to sample from the distribution defined by Eq. 13. Probabilistic inference can be used in various settings, but for concreteness the rest of this paper will focus on Bayesian inference. To fix notation, suppose a set of N inputs xi with corresponding outputs yi. (In a generative setting, xi would be empty.) Then, if z is a vector of parameters, the posterior distribution over z is, up to a constant\nlog p(z) = log p0(z) + NX\ni=1\nlog p(yi|xi, z) + C, (14)\nwhere p0(z) is the prior, and p(yi|xi, z) is the likelihood for the i-th datum. The goal of probabilistic inference is to be able to evaluate expectations with respect to p(z)."
    }, {
      "heading" : "3.1. Langevin Dynamics",
      "text" : "The goal of MCMC methods is to obtain samples from the target distribution p(z). Langevin dynamics sample by an extremely simple process of repeating gradient steps of log(z) with injected Gaussian noise. Specifically, the iterate is\nz z + ✏ 2 r log p(z) +p✏⌘, (15)\nwhere ⌘ is sampled from a standard Multivariate Gaussian distribution and ✏ is a step-size that may decay over time. If Langevin dynamics are used as a proposal for a MetropolisHastings sampler, it can be shown that correct acceptance ratio is\nexp s(z0) s(z) + ✏ 8 krs(z)k2 ✏ 8 krs(z0)k2\n+ 1 2 (z z0) · (rs(z) +rs(z0)) , (16)\nwhere s(z) = log p(z) up to a constant factor, and z0 is the proposed point from Eq. 15. It is easy to see that as ✏ becomes small, this acceptance ratio will go to one, and so one can disregard the acceptance step with some bias in the results determined by the step size.\nLangevin dynamics explore the space of z through a random walk, and thus are likely to be slower than alternatives such as Hamiltonian Monte Carlo when used as an exact method (Neal, 2010, Section 5.2). The main practical advantage of Langevin dynamics comes from the case where the number of data N is large. In that case, one can use a minibatch of M elements and approximate r log p(z) as\nr log p(z) ' r log p0(z)+ N\nM\nX\ni2minibatch r log p(yi|xi, z).\n(17)\nThus, Stochastic Gradient Langevin Dynamics, avoid a full pass through the dataset in each iteration, and so can scale to large datasets. Though Eq. 17 is an unbiased estimate of the gradient of log p(z) this still adds a bias to the stationary distribution of the the chain. (See (Mandt et al., 2016) for an analysis.) In the limit of a small step-size, the variance of the noise due to stochastic estimation of the gradient of log p(z) will be of order ✏2 while the variance of the injected noise is of order ✏, meaning the latter dominates (Welling & Teh, 2011; Teh et al., 2016)."
    }, {
      "heading" : "3.2. Stochastic Gradient Variational Inference",
      "text" : "The goal of variational inference is to maximize\nL(w) = Eqw(Z)[log p(Z) log qw(Z)], (18) equivalent to minimizing the KL divergence between qw(z) and p(z). If it were possible to exactly compute L, this could be maximized (to a local optima) by a simple gradient iteration like\nw w + ✏ 2 rwL(w). (19)\nWhile in some cases with specific p and q, one can derive exact updates of L (Ghahramani & Beal, 2000) in general one cannot exactly evaluate the expectation over Z in L. One line of approach to this (Ranganath et al., 2014; Salimans & Knowles, 2014) is to write the gradient as rL = Eqw(Z)[(log qw(Z) log p(Z))r log qw(Z)], and estimate this by drawing samples from qw(z). This experimentally seems to result in gradients with large variance, but this can be reduced by two strategies. Firstly, one can use control variates, based on either Taylor expansion (Paisley et al., 2012) or the fact that the expected value of rw log qw(Z) is zero. Secondly, since log p(z) is often a sum of terms defined on subsets of variables, one can RaoBlackwellize by integrating out other variables. This approach only needs to be able to compute log p(z)– no other access, even to the gradient of p, is needed."
    }, {
      "heading" : "3.2.1. REPARAMETERIZATION TRICK",
      "text" : "Another line of approach to variational inference is based on the “reparameterization trick” (Kingma & Welling,\n2014; Rezende et al., 2014; Titsias & Lázaro-Gredilla, 2014), known in the stochastic approximation literature as a “pathwise” derivative (Kushner & Yin, 2003, Sec. 2.5.1). Suppose that there is a deterministic mapping zr,w from parameters w and random numbers r (from some fixed standard distribution) such that zr,w ⇠ qw(z). Then, we could equivalently write\nL(w) = ER[log p(zR,w)] + H(w). (20)\nwhere H(w) = Eqw(Z) log qw(Z) denotes the entropy. The advantage of Eq. 20 this is that the expectation is not a function of w, and so if computing the gradient of L, the gradient moves inside the expectation. Then, provided zr,w is differentiable with respect to w, an unbiased estimate of the gradient of L is available as\nrwL(w) ' rw log p(zr,w) +rwH(w). (21)\nIf H(w) cannot be integrated in closed-form an alternative estimator based on\nL(w) = ER[log p(zR,w) log qw(zR,w)] (22)\nis possible, and in some circumstances this can even have lower variance (Roeder et al., 2016).\nComputing the gradient of log p(zr,w) with respect to w amounts to computing the derivative of log p(z) with respect to z multiplied by the Jacobian dzTr,w/dw. For simple distributions like Gaussians zr,w this is easy to do analytically (Section 5). Alternatively, this can all be done efficiently by automatic differentiation, (Kucukelbir et al., 2017) the approach used here.\nAs with Langevin dynamics, with a large dataset, log p(z) can be approximated by taking a sum over a minibatch. The most obious approach to this would be to choose a single vector r and use it throughout the minibatch. However, variance can often be reduced by the “local reparameterization trick” (Kingma et al., 2015) in which a different random vector ri is drawn for each datum in the minibatch. Intuitively, the motivation is that if i and j are two data in the minibatch then log p(yi|xi, zr,w) should be less correlated with log p(yj |xj , zr0,w) when a different random vector r0 is used for the second datum (consider the limiting case where all data are identical). Then, one can write the approximation as\nrwL(w) ' 1\nM\nX\ni2minibatch\n⇣ rw log p0(zri,w)\n+ Nrw log p(yi|xi, zri,w) rw log qw(zri,w) ⌘ . (23)\nNote that the approximation on the right-hand side depends both on the choice of the minibatch and on the random vectors ri, one chosen per element of the minibatch."
    }, {
      "heading" : "4. Hybrid Dynamics",
      "text" : "An algorithm that interpolates between the methods in the previous section results from applying Langevin dynamics to the distribution over w defined by Thm. 3 If the adjoining distribution p(w|z) is chosen as in in Sec. 2.3 with some r (w) that depends on , then define\nL(w) = log r (w) + Eqw(Z)[log p(z) + ( 1) log q(z|w)], (24)\nwhich is times the form of s(w) derived in 5, with the constant term of log rz dropped. Applying Langevin Dynamics2 to this results in the iteration\nw w + ✏ 2 rL(w) +\np ✏ ⌘, (25)\nwhere again ✏ is a step-size that may decrease over time and ⌘ is sampled from a standard Gaussian distribution.\nThis paper uses a closed-form for the entropy H(w) = Eqw(Z) log qw(Z), and so uses the gradient estimator\nrL(w) ' r log r (w) + (1 )rH(w) + Eqw(Z)[log p(z) + ( 1) log q(z|w)]. (26)"
    }, {
      "heading" : "5. Specifics For Gaussians",
      "text" : "The following experiments use the simplest common variational distribution for qw(Z), namely a fully-factorized Gaussian distribution. To make w unconstrained, let w = (µ, ⌫) where µ is a vector of mean components and the standard deviation of the i-th dimension is i = 10⌫i . To sample from this distribution given a vector r, one can simply take zr,w = µ + r where is element-wise product. The entropy of qw(Z) is, up to constant factors H(w) = P i ⌫i ln 10."
    }, {
      "heading" : "It remains to set the base density. These experiments used",
      "text" : "an improper prior r (w) / Q i N (⌫i|u , 1) that is uniform over µ with a Gaussian prior on ⌫ with a fixed variance of 1 and a mean u . Since this is uniform over µ, rz is a constant. The value u was calculated to numerically optimize the divergence bound D⇤ for a one-dimensional standard Gaussian p(z). Since D⇤ = A at the solution (Theorem 3), for any given and u , D⇤ can be evaluated by symbolically integrating out µ and then numerically integrating over ⌫. The values used in these experiments are below, with linear interpolation used for any other .\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 u -.33 -.472 -.631 -.792 -.953 -1.11 -1.29 -1.49 -1.74 -2.10 -10\n2For some step-size ✏0, the Langevin dynamics would immediately be w w + (✏0/2)rs(w) + p ✏0⌘. If we define ✏ = ✏0, then this results in the given form since ✏0rs(w) = (✏/ )rs(w) = ✏rL(w).\nTheoretically, it’s easy to show that as ! 1, D⇤ is minimized (with a minimum of zero) by u ! 1, meaning that after one iteration, ⌫ = 1, and henceforth, zr,w = µ. Thus, Eq. 25 is equivalent to Langevin dynamics as in Eq. 15. However, using u = 10, r(w) (a prior centered on a standard deviation of = 10 10) is practically equivalent and is more useful to guide linear interpolation. Meanwhile, when ! 0, the algorithm is independent of r(w), though the optimal value of u ⇡ .33 is correct for small .\nThe above choice of r (w) is quite arbitrary. It is possible that other choices could be better, though performance was surprisingly insensitive in practice. With large datasets log p(z) dominates log r(w) just as likelihoods dominate Bayesian priors. Calling on inspiration from recent work in variational inference, it might also be useful to optimize p(w|z) at runtime (Section 2.4)."
    }, {
      "heading" : "6. Experiments",
      "text" : "There does not appear to be a single standard performance measure to evaluate approximate inference algorithms. For Bayesian inference, test-set accuracy or likelihood are sometimes used, but these measures have high variance due to the dataset, and conflate evaluation of the inference method with evaluation of the model it is running on. (An inference method with poor coverage of p(z) can have higher accuracy than a perfect sampler due to model mis-specification or dataset peculiarities.)\nFor a more fine-grained measure of inference performance, this paper uses the Maximum Mean Discrepancy (MMD) measure (Gretton et al., 2006; 2012). MMD is essentially the empirical difference of the means of two samples in some feature space. We first draw a sample from p(z) by exhaustively running a traditional MCMC algorithm (Stan (Team, 2016), based on a variant of Hamil-\ntonian Monte Carlo (Hoffman & Gelman, 2014)) for a large number of iterations, and compare this to the results of approximate inference. Since we have large samples, and want to compute the online performance at all iterations, a finite-dimensional feature space must be used to avoid the quadratic complexity of computing MMD using an arbitrary kernel. For two-dimensional problems, it was beneficial to use random Fourier features (Rahimi & Recht, 2007) to approximate the radial-basis function kernel k(x, y) = exp( kx yk2). For higher dimensions, the original feature space was sufficiently discriminative.\nApproximate inference gives a sequence of vectors wt. For each time-step, a set of 100 samples At are drawn from q(z|wt). Then, the MMD between A1[A2[ · · · At and the sample from exhaustive MCMC can be computed for all t in linear time. A disadvantage of this approach is the need for exhaustive MCMC for comparison, which by definition eludes the large-scale cases that motivate Langevin dynamics and stochastic variational inference. However this gives a more fine-grained view of performance."
    }, {
      "heading" : "6.1. Toy Distributions",
      "text" : "For a first demonstration, the algorithm was applied to a set of toy 2-dimensional distributions, with various values of . A few results are shown in Fig. 1, with more in\nthe appendix. While the algorithm displays the expected trade-offs between speed and accuracy for different (Fig. 2), the results fairly uninteresting as all curves cross near the same time/MMD point where = 0 (variational inference) and = 1 (MCMC) meet. So, for any given amount of time, either variational inference or MCMC are nearoptimal, and so intermediate do not much expand the “Pareto frontier” on these problems, although one might prefer an intermediate since the crossover horizon is not known in advance."
    }, {
      "heading" : "6.2. Bayesian Logistic Regression",
      "text" : "Next, the algorithm was applied to binary logistic regression with several standard datasets, using a minibatch size of 25, a different random vector r for each element in the minibatch, and a standard multivariate Laplace distribution as a prior. Simply picking a single value or schedule for the step ✏ is unsatisfactory, as the best schedule for a given problem, length of time, and value of varies. To give a fair comparison, inference was run for with a range of constant ✏ varying by factors of two from 23/N to 2 2/N . This spans the range from large enough to often diverge to below optimal even after 106 iterations. Then, for each time, the best value of ✏ was retrospectively chosen (with performance averaged over repetitions before this choice). It is likely that decaying step-size schedules would perform\nbetter, but this was not pursued since it would reduce transparency of the results.\nThe mean errors after 100 repetitions are shown in Fig. 4, while samples are compared to the ground truth for ionosphere in Fig. 3. This confirms the basic ideas of the algorithm, namely that it smoothly interpolates between the two previous algorithms without introducing any new behavior. The main experimental finding is that there is usually a large range of time horizons for which an intermediate setting of performs better than either of the previous existing algorithms. This suggests that, say, someone who has a time budget slightly larger than that needed for variational inference could benefit from running the algorithm with a small value of for a slightly larger time. This is in contrast to the toy two-dimensional examples where for almost all time horizons either = 0 or = 1 was optimal."
    }, {
      "heading" : "7. Discussion",
      "text" : "This paper has two contributions. First, a distribution over variational parameters is derived to minimize a bound on the marginal divergence to a target distribution. Secondly, this is used to derive an algorithm that interpolates between Langevin dynamics and stochastic variational inference. The Bayesian logistic regression experiments show that the intermediate values of this algorithm are useful in the sense that there are several orders of magnitude of time horizons where an intermediate algorithm performs better than either Langevin dynamics or variational inference.\nMany improvements to stochastic Langevin dynamics and variational inference have been proposed beyond the simple algorithms used here. For Langevin dynamics, Welling & Teh (2011) and Li et al. (2016) suggest adding a preconditioner, Patterson & Teh (2013) propose Riemannian Langevin Dynamics, and Dubey et al. (2016) propose to make use of the finite sum structure in a Bayesian posterior p(z) by incorporating techniques for incremental optimization. For stochastic variational inference, Hoffman et al.\n(2013) propose to use the natural gradient, Mandt & Blei (2014) propose using smoothed gradients, and Sakaya & Klami (2016) propose a strategy of re-using previous gradient computations. It would be interesting to consider using these ideas with the hybrid algorithm, which could give insight into the relationship between the two different methods.\nThere is other work based on combining the advantages of variational inference and MCMC. Stochastic Gradient Fisher Scoring (Ahn et al., 2012) interpolates between Langevin dynamics and a Gaussian approximation of the target. However, this approximation is based on the Bayesian central limit theorem and so will not be the same in general as the optimal variational distribution, even when a Gaussian is used as the variational family. Another line of research is that of interpreting the path of a MCMC algorithm as a variational distribution, and then fitting parameters to tighten a variational bound (Salimans et al., 2015; Rezende & Mohammed, 2015). While motivationally quite similar, the contribution of this paper is orthogonal, in that one could conceivably use these same ideas to define the variational family qw(z) used in this paper. This would result in a somewhat unusual method that runs MCMC over the parameters of variational family of distributions that are themselves defined in terms of a (different) MCMC algorithm."
    }, {
      "heading" : "Ghahramani, Zoubin and Beal, Matthew J. Propagation",
      "text" : "algorithms for variational bayesian learning. In NIPS, 2000. 3.2"
    }, {
      "heading" : "Grenander, Ulf and Miller, Michael. Representations of",
      "text" : "knowledge in complex systems. J. R. Statist. Soc., 56 (4):549–603, 1994. 1"
    }, {
      "heading" : "Gretton, Arthur, Borgwardt, Karsten M., Rasch, Malte J.,",
      "text" : "Schölkopf, Bernhard, and Smola, Alexander J. A kernel method for the two-sample-problem. In NIPS, 2006. 6"
    }, {
      "heading" : "Gretton, Arthur, Borgwardt, Karsten M., Rasch, Malte J.,",
      "text" : "Schölkopf, Bernhard, and Smola, Alexander J. A kernel two-sample test. Journal of Machine Learning Research, 13:723–773, 2012. 6"
    }, {
      "heading" : "Hoffman, Matthew D. and Gelman, Andrew. The no-u-turn",
      "text" : "sampler: adaptively setting path lengths in hamiltonian monte carlo. Journal of Machine Learning Research, 15 (1):1593–1623, 2014. 6"
    }, {
      "heading" : "Hoffman, Matthew D., Blei, David M., Wang, Chong,",
      "text" : "and Paisley, John William. Stochastic variational inference. Journal of Machine Learning Research, 14(1): 1303–1347, 2013. 7"
    }, {
      "heading" : "Kingma, Diederik P and Welling, Max. Stochastic gradient",
      "text" : "vb and the variational auto-encoder. In ICLR, 2014. 3.2.1"
    }, {
      "heading" : "Kingma, Diederik P., Salimans, Tim, and Welling, Max.",
      "text" : "Variational dropout and the local reparameterization trick. In NIPS, 2015. 3.2.1\nKucukelbir, Alp, Tran, Dustin, Ranganath, Rajesh, Gelman, Andrew, and Blei, David M. Automatic differentiation variational inference. Journal of Machine Learning Research, 18(14):1–45, 2017. 3.2.1\nKushner, Harold and Yin, George. Stochastic Approximation and Recursive Algorithms and Applications. Springer-Verlag, 2003. 3.2.1"
    }, {
      "heading" : "Li, Chunyuan, Chen, Changyou, Carlson, David E., and",
      "text" : "Carin, Lawrence. Preconditioned stochastic gradient langevin dynamics for deep neural networks. In AAAI, 2016. 7"
    }, {
      "heading" : "Mandt, Stephan and Blei, David M. Smoothed gradients",
      "text" : "for stochastic variational inference. In NIPS, 2014. 7"
    }, {
      "heading" : "Mandt, Stephan, Hoffman, Matthew D., and Blei, David M.",
      "text" : "A variational analysis of stochastic gradient algorithms. In ICML, 2016. 3.1\nNeal, Radford. MCMC using Hamiltonian dynamics, volume Handbook of Markov Chain Monte Carlo, pp. 113– 162. Chapman & Hall / CRC Press, 2010. 3.1"
    }, {
      "heading" : "Paisley, John William, Blei, David M., and Jordan,",
      "text" : "Michael I. Variational bayesian inference with stochastic search. In ICML, 2012. 3.2\nPatterson, Sam and Teh, Yee Whye. Stochastic gradient riemannian langevin dynamics on the probability simplex. In NIPS, 2013. 7"
    }, {
      "heading" : "Rahimi, Ali and Recht, Benjamin. Random features for",
      "text" : "large-scale kernel machines. In NIPS, 2007. 6"
    }, {
      "heading" : "Ranganath, Rajesh, Gerrish, Sean, and Blei, David M.",
      "text" : "Black box variational inference. In AISTATS, 2014. 3.2\nRanganath, Rajesh, Tran, Dustin, and Blei, David M. Hierarchical variational models. In ICML, 2016. 2.4\nRezende, Danilo Jimenez and Mohammed, Shakir. Variational inference with normalizing flows. In ICML, 2015. 7"
    }, {
      "heading" : "Rezende, Danilo Jimenez, Mohamed, Shakir, and Wierstra,",
      "text" : "Daan. Stochastic backpropagation and variational inference in deep latent gaussian models. In International Conference on Machine Learning, 2014. 3.2.1\nRobert, Christian and Casella, George. Monte Carlo Statistical Methods. Springer-Verlag, 2 edition, 2004. 1"
    }, {
      "heading" : "Roeder, Geoffrey, Wu, Yuhuai, and Duvenaud, David.",
      "text" : "Sticking the landing: A simple reduced-variance gradient for advi. NIPS workshop in Advances in Approximate Bayesian Inference, 2016. 3.2.1\nSakaya, Joseph and Klami, Arto. Re-using gradient computations in automatic variational inference. Technical report, 2016. NIPS workshop on approximate Bayesian Inference. 7\nSalimans, Tim and Knowles, David A. On using control variates with stochastic approximation for variational bayes and its connection to stochastic linear regression. In ICLR, 2014. 3.2"
    }, {
      "heading" : "Salimans, Tim, Kingma, Diedrik, and Welling, Max.",
      "text" : "Markov chain monte carlo and variational inference: Bridging the gap. In ICML, 2015. 2.4, 7\nTeam, Stan Development. Stan Modeling Language Users Guide and Reference Manual, 2.14.0. edition, 2016. 6\nTeh, Yee Whye, Thiery, Alexandre H., and Vollmer, Sebastian J. Consistency and fluctuations for stochastic gradient langevin dynamics. J. Mach. Learn. Res., 17(1): 193–225, 2016. 3.1"
    }, {
      "heading" : "Titsias, Michalis K. and Lázaro-Gredilla, Miguel. Doubly",
      "text" : "stochastic variational bayes for non-conjugate inference. In ICML, 2014. 3.2.1"
    }, {
      "heading" : "Tran, Dustin, Ranganath, Rajesh, and Blei, David M. The",
      "text" : "variational gaussian process. In ICLR, 2016. 2.4"
    }, {
      "heading" : "Welling, Max and Teh, Yee Whye. Bayesian learning via",
      "text" : "stochastic gradient langevin dynamics. In ICML, 2011. 1, 3.1, 7"
    }, {
      "heading" : "8. Appendix",
      "text" : "This appendix contains additional plots and proofs of the results from Section 2.\nLemma 6. The divergence from q(z) to p(z) is\nKL (q(Z)kp(Z)) = KL (q(Z|W )kp(Z))| {z } D0 Iq[W, Z],\n(27) where D0 = Eq(W,Z) log (q(Z|W )/p(Z)) is conditional divergence and Iq denotes mutual information under q.\nProof. Define the joint distribution p(w, z) = q(w)p(z). Then, the chain-rule of KL-divergence (Cover & Thomas, 2006, Thm. 2.5.3) states that\nKL (q(Z, W )kp(Z, W )) = KL (q(W |Z)kp(W |Z)) + KL (q(Z)kp(Z)) . (28)\nThe left-hand side simplifies into D0, and the first term on the right-hand side simplifies into Iq[W, Z].\nTheorem 7. For fixed values of and p(w|z), the distribution q(w) that minimizes D is\nq⇤(w) = exp s(w) A)\nA = log\nZ\nw\nexp s(w)\ns(w) = log p(w) KL (q(Z|w)kp(Z|w)) 1 1 KL (q(Z|w)kp(Z)) ."
    }, {
      "heading" : "Moreover, at q⇤, the objective value is D⇤ = A.",
      "text" : "Proof. First, consider derivatives of D0 and D1 with respect to q(w). The first can immediately be seen to be\ndD0 dq(w) = KL (q(Z|w)kp(Z)) .\nFor the second, we can derive\ndD1 dq(w) = d dq(w)\nZ\nw,z\nq(w, z) log q(z|w) p(w, z)\n+ d\ndq(w)\nZ\nw,z\nq(w) log q(w)\n=\nZ\nz\nq(z|w) log q(z|w) p(w, z) + log q(w) + 1\n= KL (q(Z|w)kp(Z|w)) log p(w) + log q(w) + 1.\nIf we create a Lagrangian for D with a Lagrange multiplier to enforce normalization of q(w), we know that at\nthe optimal q(w) its gradient will be zero. Using the above derivatives, we therefore have that\n0 =(1 )KL (q(Z|w)kp(Z)) + KL (q(Z|w)kp(Z|w)) log p(w) + log q(w) + ,\nWhich solved for q(w), this gives\nq(w) / exp (1 1)KL (q(Z|w)kp(Z))\nKL (q(Z|w)kp(Z|w)) + log p(w) ,\nwhich establishes the given form for s(w) and A.\nNow, to establish the value of D at the solution, expand the negative entropy of q(w) to get\nZ\nw\nq(w) log q(w)\n=\nZ\nw\nq(w) ⇣ (1 1)KL (q(Z|w)kp(Z))\nKL (q(Z|w)kp(Z|w)) + log p(w) ⌘ A. (29)\nNow, taking the left-hand side and terms in the bottom line, we can recognize that\nZ\nw\nq(w) ✓ log p(w)\nq(w) KL (q(Z|w)kp(Z|w))\n◆ = D1.\nFurther, if we take the terms from the middle line, we have that\nZ\nw\nq(w)(1 1)KL (q(Z|w)kp(Z)) = ( 1)D0.\nThus, we can re-write Eq. 29 as A = (1 )D0+ D1, establishing the value of D⇤ .\nRemark 8. In the limit where ! 0 the divergence bound becomes\nlim !0 D⇤ = inf w KL (q(Z|w)kp(Z)) .\nProof. Use the representation that lim !0 D⇤ = lim !0 A is equal to\nlim !0\nlog Z\nw\nexp ⇣ log p(w) KL (q(Z|w)kp(Z|w))\n1 1 KL (q(Z|w)kp(Z))\n⌘\n= lim !0\nlog Z\nw\nexp ⇣ 1KL (q(Z|w)kp(Z)) ⌘ .\nThe form for D⇤ follows from the fact that lim !0 log R w exp( 1f(w)) = supw f(w).\nLemma 9. If p(w|z) = r(w)q(z|w)/rz and rz is a constant, then the solution in Thm. 3 holds with\ns(w) = log r(w) log rz + Eqw(Z)[ 1 log p(z) + (1 1) log q(z|w)].\nProof. First, without using the particular form for p(w|z), we can write s(w) as\nlog p(w) Z\nz\nq(z|w) log q(z|w) p(z|w)\n1 1\nZ\nz\nq(z|w) log q(z|w) p(z)\nCancelling terms involving q(z|w) in the numerators, this is\nlog p(w) Z\nz\nq(z|w) log p(z) p(z|w)\n1 Z\nz\nq(z|w) log q(z|w) p(z)\nThe log p(w) can be absorbed into the first term to give, after some cancellation that\ns(w) =\nZ\nz\nq(z|w) log p(w|z) 1KL (q(Z|w)kp(Z)) .\nNow, using the assumed form for p(w|z), we can immediately write that s(w) is Z\nz\nq(z|w) log r(w)q(z|w) rz\n1 Z\nz\nq(z|w) log q(z|w) p(z) ,\nequivalent to the form stated."
    } ],
    "references" : [ {
      "title" : "An auxiliary variational method",
      "author" : [ "Agakov", "Felix V", "Barber", "David" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Agakov et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Agakov et al\\.",
      "year" : 2004
    }, {
      "title" : "Bayesian posterior sampling via stochastic gradient fisher scoring",
      "author" : [ "Ahn", "Sungjin", "Balan", "Anoop Korattikara", "Welling", "Max" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Ahn et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Ahn et al\\.",
      "year" : 2012
    }, {
      "title" : "Variance reduction in stochastic gradient langevin dynamics",
      "author" : [ "Sinead A", "Póczos", "Barnabás", "Smola", "Alexander J", "Xing", "Eric P" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "A. et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "A. et al\\.",
      "year" : 2016
    }, {
      "title" : "Propagation algorithms for variational bayesian learning",
      "author" : [ "Ghahramani", "Zoubin", "Beal", "Matthew J" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Ghahramani et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Ghahramani et al\\.",
      "year" : 2000
    }, {
      "title" : "Representations of knowledge in complex systems",
      "author" : [ "Grenander", "Ulf", "Miller", "Michael" ],
      "venue" : "J. R. Statist. Soc.,",
      "citeRegEx" : "Grenander et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Grenander et al\\.",
      "year" : 1994
    }, {
      "title" : "A kernel method for the two-sample-problem",
      "author" : [ "Gretton", "Arthur", "Borgwardt", "Karsten M", "Rasch", "Malte J", "Schölkopf", "Bernhard", "Smola", "Alexander J" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Gretton et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Gretton et al\\.",
      "year" : 2006
    }, {
      "title" : "A kernel two-sample test",
      "author" : [ "Gretton", "Arthur", "Borgwardt", "Karsten M", "Rasch", "Malte J", "Schölkopf", "Bernhard", "Smola", "Alexander J" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Gretton et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Gretton et al\\.",
      "year" : 2012
    }, {
      "title" : "The no-u-turn sampler: adaptively setting path lengths in hamiltonian monte carlo",
      "author" : [ "Hoffman", "Matthew D", "Gelman", "Andrew" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Hoffman et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hoffman et al\\.",
      "year" : 2014
    }, {
      "title" : "Stochastic variational inference",
      "author" : [ "Hoffman", "Matthew D", "Blei", "David M", "Wang", "Chong", "Paisley", "John William" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Hoffman et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Hoffman et al\\.",
      "year" : 2013
    }, {
      "title" : "Stochastic gradient vb and the variational auto-encoder",
      "author" : [ "Kingma", "Diederik P", "Welling", "Max" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Kingma et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2014
    }, {
      "title" : "Variational dropout and the local reparameterization trick",
      "author" : [ "Kingma", "Diederik P", "Salimans", "Tim", "Welling", "Max" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Kingma et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2015
    }, {
      "title" : "Automatic differentiation variational inference",
      "author" : [ "Kucukelbir", "Alp", "Tran", "Dustin", "Ranganath", "Rajesh", "Gelman", "Andrew", "Blei", "David M" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Kucukelbir et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Kucukelbir et al\\.",
      "year" : 2017
    }, {
      "title" : "Stochastic Approximation and Recursive Algorithms and Applications",
      "author" : [ "Kushner", "Harold", "Yin", "George" ],
      "venue" : null,
      "citeRegEx" : "Kushner et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Kushner et al\\.",
      "year" : 2003
    }, {
      "title" : "Preconditioned stochastic gradient langevin dynamics for deep neural networks",
      "author" : [ "Li", "Chunyuan", "Chen", "Changyou", "Carlson", "David E", "Carin", "Lawrence" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "Li et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "Smoothed gradients for stochastic variational inference",
      "author" : [ "Mandt", "Stephan", "Blei", "David M" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Mandt et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mandt et al\\.",
      "year" : 2014
    }, {
      "title" : "A variational analysis of stochastic gradient algorithms",
      "author" : [ "Mandt", "Stephan", "Hoffman", "Matthew D", "Blei", "David M" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Mandt et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Mandt et al\\.",
      "year" : 2016
    }, {
      "title" : "MCMC using Hamiltonian dynamics, volume Handbook of Markov Chain Monte Carlo",
      "author" : [ "Neal", "Radford" ],
      "venue" : null,
      "citeRegEx" : "Neal and Radford.,? \\Q2010\\E",
      "shortCiteRegEx" : "Neal and Radford.",
      "year" : 2010
    }, {
      "title" : "Variational bayesian inference with stochastic search",
      "author" : [ "Paisley", "John William", "Blei", "David M", "Jordan", "Michael I" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Paisley et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Paisley et al\\.",
      "year" : 2012
    }, {
      "title" : "Stochastic gradient riemannian langevin dynamics on the probability simplex",
      "author" : [ "Patterson", "Sam", "Teh", "Yee Whye" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Patterson et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Patterson et al\\.",
      "year" : 2013
    }, {
      "title" : "Random features for large-scale kernel machines",
      "author" : [ "Rahimi", "Ali", "Recht", "Benjamin" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Rahimi et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Rahimi et al\\.",
      "year" : 2007
    }, {
      "title" : "Black box variational inference",
      "author" : [ "Ranganath", "Rajesh", "Gerrish", "Sean", "Blei", "David M" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Ranganath et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Ranganath et al\\.",
      "year" : 2014
    }, {
      "title" : "Hierarchical variational models",
      "author" : [ "Ranganath", "Rajesh", "Tran", "Dustin", "Blei", "David M" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Ranganath et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ranganath et al\\.",
      "year" : 2016
    }, {
      "title" : "Variational inference with normalizing flows",
      "author" : [ "Rezende", "Danilo Jimenez", "Mohammed", "Shakir" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Rezende et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Rezende et al\\.",
      "year" : 2015
    }, {
      "title" : "Stochastic backpropagation and variational inference in deep latent gaussian models",
      "author" : [ "Rezende", "Danilo Jimenez", "Mohamed", "Shakir", "Wierstra", "Daan" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Rezende et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Rezende et al\\.",
      "year" : 2014
    }, {
      "title" : "Sticking the landing: A simple reduced-variance gradient for advi",
      "author" : [ "Roeder", "Geoffrey", "Wu", "Yuhuai", "Duvenaud", "David" ],
      "venue" : "NIPS workshop in Advances in Approximate Bayesian Inference,",
      "citeRegEx" : "Roeder et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Roeder et al\\.",
      "year" : 2016
    }, {
      "title" : "Re-using gradient computations in automatic variational inference",
      "author" : [ "Sakaya", "Joseph", "Klami", "Arto" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Sakaya et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Sakaya et al\\.",
      "year" : 2016
    }, {
      "title" : "On using control variates with stochastic approximation for variational bayes and its connection to stochastic linear regression",
      "author" : [ "Salimans", "Tim", "Knowles", "David A" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Salimans et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Salimans et al\\.",
      "year" : 2014
    }, {
      "title" : "Markov chain monte carlo and variational inference: Bridging the gap",
      "author" : [ "Salimans", "Tim", "Kingma", "Diedrik", "Welling", "Max" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Salimans et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Salimans et al\\.",
      "year" : 2015
    }, {
      "title" : "Consistency and fluctuations for stochastic gradient langevin dynamics",
      "author" : [ "Teh", "Yee Whye", "Thiery", "Alexandre H", "Vollmer", "Sebastian J" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Teh et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Teh et al\\.",
      "year" : 2016
    }, {
      "title" : "Doubly stochastic variational bayes for non-conjugate inference",
      "author" : [ "Titsias", "Michalis K", "Lázaro-Gredilla", "Miguel" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Titsias et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Titsias et al\\.",
      "year" : 2014
    }, {
      "title" : "The variational gaussian process",
      "author" : [ "Tran", "Dustin", "Ranganath", "Rajesh", "Blei", "David M" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Tran et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Tran et al\\.",
      "year" : 2016
    }, {
      "title" : "Bayesian learning via stochastic gradient langevin dynamics",
      "author" : [ "Welling", "Max", "Teh", "Yee Whye" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Welling et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Welling et al\\.",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 27,
      "context" : "by including latent stochastic transition operators (Salimans et al., 2015), random Gaussian process mappings (Tran et al.",
      "startOffset" : 52,
      "endOffset" : 75
    }, {
      "referenceID" : 30,
      "context" : ", 2015), random Gaussian process mappings (Tran et al., 2016), or hierarchical variables (Ranganath et al.",
      "startOffset" : 42,
      "endOffset" : 61
    }, {
      "referenceID" : 21,
      "context" : ", 2016), or hierarchical variables (Ranganath et al., 2016).",
      "startOffset" : 35,
      "endOffset" : 59
    }, {
      "referenceID" : 15,
      "context" : "(See (Mandt et al., 2016) for an analysis.",
      "startOffset" : 5,
      "endOffset" : 25
    }, {
      "referenceID" : 28,
      "context" : ") In the limit of a small step-size, the variance of the noise due to stochastic estimation of the gradient of log p(z) will be of order ✏ while the variance of the injected noise is of order ✏, meaning the latter dominates (Welling & Teh, 2011; Teh et al., 2016).",
      "startOffset" : 224,
      "endOffset" : 263
    }, {
      "referenceID" : 20,
      "context" : "One line of approach to this (Ranganath et al., 2014; Salimans & Knowles, 2014) is to write the gradient as rL = Eqw(Z)[(log qw(Z) log p(Z))r log qw(Z)], and estimate this by drawing samples from qw(z).",
      "startOffset" : 29,
      "endOffset" : 79
    }, {
      "referenceID" : 17,
      "context" : "Firstly, one can use control variates, based on either Taylor expansion (Paisley et al., 2012) or the fact that the expected value of rw log qw(Z) is zero.",
      "startOffset" : 72,
      "endOffset" : 94
    }, {
      "referenceID" : 23,
      "context" : "REPARAMETERIZATION TRICK Another line of approach to variational inference is based on the “reparameterization trick” (Kingma & Welling, 2014; Rezende et al., 2014; Titsias & Lázaro-Gredilla, 2014), known in the stochastic approximation literature as a “pathwise” derivative (Kushner & Yin, 2003, Sec.",
      "startOffset" : 118,
      "endOffset" : 197
    }, {
      "referenceID" : 24,
      "context" : "If H(w) cannot be integrated in closed-form an alternative estimator based on L(w) = ER[log p(zR,w) log qw(zR,w)] (22) is possible, and in some circumstances this can even have lower variance (Roeder et al., 2016).",
      "startOffset" : 192,
      "endOffset" : 213
    }, {
      "referenceID" : 11,
      "context" : "Alternatively, this can all be done efficiently by automatic differentiation, (Kucukelbir et al., 2017) the approach used here.",
      "startOffset" : 78,
      "endOffset" : 103
    }, {
      "referenceID" : 10,
      "context" : "However, variance can often be reduced by the “local reparameterization trick” (Kingma et al., 2015) in which a different random vector ri is drawn for each datum in the minibatch.",
      "startOffset" : 79,
      "endOffset" : 100
    }, {
      "referenceID" : 5,
      "context" : ") For a more fine-grained measure of inference performance, this paper uses the Maximum Mean Discrepancy (MMD) measure (Gretton et al., 2006; 2012).",
      "startOffset" : 119,
      "endOffset" : 147
    } ],
    "year" : 2017,
    "abstractText" : "Two popular classes of methods for approximate inference are Markov chain Monte Carlo (MCMC) and variational inference. MCMC tends to be accurate if run for a long enough time, while variational inference tends to give better approximations at shorter time horizons. However, the amount of time needed for MCMC to exceed the performance of variational methods can be quite high, motivating more fine-grained tradeoffs. This paper derives a distribution over variational parameters, designed to minimize a bound on the divergence between the resulting marginal distribution and the target, and gives an example of how to sample from this distribution in a way that interpolates between the behavior of existing methods based on Langevin dynamics and stochastic gradient variational inference (SGVI).",
    "creator" : "LaTeX with hyperref package"
  }
}