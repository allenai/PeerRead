{
  "name" : "1603.01840.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Hierarchical Decision Making In Electricity Grid Management",
    "authors" : [ "Gal Dalal", "Elad Gilboa", "Shie Mannor" ],
    "emails" : [ "GALD@TX.TECHNION.AC.IL", "EGILBOA@TX.TECHNION.AC.IL", "SHIE@EE.TECHNION.AC.IL" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "The power grid is a complex and vital system that requires high level of reliability. Reliability is of utmost importance, as the consequences of outages can be catastrophic. System operators (SOs) achieve reliability by means of sophisticated control operations and planning, which often require solving sequential stochastic decision problems. Sequential decision making under uncertainty in energy systems is studied in different communities such as control theory, dynamic programming, stochastic programming and robust optimization (Powell & Meisel, 2015; Bertsimas et al., 2013; Bienstock, 2011; Koutsopoulos & Tassiulas, 2012;\nProceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s).\nBienstock et al., 2014).\nReliability assessment and control are highly complicated tasks in complex real-world systems such as the power grid. Complications in the power grid arise because of strict physical restrictions, such as generation must meet consumption continuously and transmission lines can not exceed their limited thermal capacity. Further complications stem from the structure of decision making in different time-horizons. For example, long-term system expansion and development such as building a new wind farm or a high-voltage line take years, mid-term asset management decisions such as performing maintenance are decided upon months in advance, short-term generation schedules are planned daily, and real-time operational control decisions are made on the scale of minutes. In these interdependent hierarchical decision making processes decisions are taken by multiple stakeholders. Furthermore, over the last decade, wind and solar energy sources become increasingly preeminent with further significant expansion being envisaged (Talbot, 2009). These generators introduce high uncertainty to the system, making the control task significantly more difficult. The complex dependence between multiple time-horizon with growing uncertainty, the curse of dimensionality when dealing with large systems, and the non-linear dependence of reliability measures to the multiple time-horizon decisions, make this problem extremely hard to tackle.\nTo stress the dimensionality complexity, consider the IEEE RTS-96 power network used in our experiments (Wong et al., 1999). This network is an example for a power grid of a medium sized European country or a state in the USA. Its state-space is O(10300), and its action space is O(10100); see Sec. 5. Assessment of each control choice carries a computational burden as it requires solving a set of non-linear trigonometric equations named alternating current power flow (ACPF); see Sec. 2.1.\nar X\niv :1\n60 3.\n01 84\n0v 1\n[ cs\n.A I]\n6 M\nar 2\nNowadays, the common practice in industry is solving large mixed integer programs (MIP), often with a linear relaxation, in an attempt to reach a valid solution (Grainger & Stevenson, 1994; Allan et al., 2013). Although this model is extensive, its computational burden makes it hard even for deterministic predictions (taking an order of a day in real-world systems), and inappropriate in the stochastic case. This limits SOs to sample snapshots of future grid states or analyze a few sequential trajectories. The narrow view of possible outcomes is likely to miss important benefits and increase the costs of decisions, thereby offering little in terms of dealing with uncertainty.\nTo handle uncertainty, work has been done in stochastic optimization and control theory. These often use restrictive simplifications such as independence between the decision processes in the different time-scales or consider myopic decisions only (Abiri-Jahromi et al., 2009; Wu et al., 2010; Abiri-Jahromi et al., 2013).\nAnother approach is to use approximate dynamic programming (Powell, 2007; Si, 2004). However, the natural hierarchical structure of the problem, where several stakeholders operating in different time-scales and exposed to different information are making decisions with mutual influence, does not naturally fit the standard Markov Decision Process (MDP) structure. Furthermore, the problem is heavily constrained, since physical electrical restrictions must be met at all times.\nMaking this problem tractable requires a level of abstraction in the form of fast proxy methods to approximate the impact of real-time decisions on longer-term reliability and costs. To our knowledge, few attempts have been made to construct such proxies using tools from machine learning. An example for such, is the work conducted in a recent European project, iTesla (iTe). This work focuses on analyzing snapshots of system states at different time points using data-mining methods. Then, classification and clustering algorithms are used for constructing security rules for predicting reliability level, given a failure and an electrical network state (Anil, 2013). Such approaches can aid SOs in real-time control, but lack the dynamic perspective of state-action evolution needed to evaluate consequences of policies in a sequential decision making scenario.\nIn this work we suggest a novel approach to mitigate the intractability of the hierarchical decision making problem of the day-ahead (DA) and real-time (RT) reliability of the power grid. The contributions of our work are:\n• We introduce an interleaved MDPs hierarchical structure with separate state space, action space, and reward metric.\n• We devise an algorithm that alternates between highlevel policy improvement and lower-level value ap-\nproximation, i.e., the policy improvement in the first MDP is based on the second MDP’s value function.\n• We show the efficacy of our method on a mediumsized power grid problem.\n• We introduce a new real-world application to the RL community and provide a simulation environment.\nThe rest of paper is organized as follows. In Sec. 2 we present background on power system engineering. In Sec. 3, we formulate the two-layer MDPs. In Sec. 4, we introduce our interleaved approximate policy improvement (IAPI) algorithm and present results on the IEEE RTS-96 network. We conclude our work in Sec. 6."
    }, {
      "heading" : "2. Background",
      "text" : "In this section we present a brief introduction to the field of power systems engineering. This is vast a field with extensive background and theory. For more information please refer to (Grainger & Stevenson, 1994; Allan et al., 2013)."
    }, {
      "heading" : "2.1. Decision Processes and Power Flow in Power Grids",
      "text" : "To better explain the multiple time-horizon decision processes we use a toy 6-bus power grid example (Wood & Wollenberg, 1996), shown in Fig. 1. The 6-bus system is composed of 6 electrical nodes referred to as “buses”. Each bus can have loads and generators attached to it. Loads (shown in blue) are consumers (e.g., large neighborhoods or cities and factories), and generators (shown in red) are power producers such as nuclear plants, coal plants, wind turbines, and solar panels. Load values change continuously throughout the day and closely follow daily, weekly, and yearly profiles. Controllable generators are operated such that the overall power generation meets the overall load at all times (up to transmission losses). The edges connecting the buses represent transmission lines which, due to thermal restrictions, can only transfer a limited amount of power before risking tripping.\nGiven a snapshot of loads and generation values, and the power grid topology (buses and transmission lines), it is possible to solve the complete alternating current power flow (ACPF) equations. The ACPF is a set of non-convex trigonometric equations that model the physical electrical characteristics of the power grid, i.e., voltage magnitude and angles of each node (Cain et al., 2012). The ACPF solution includes the amount of power passing through each transmission line (shown in green in Fig. 1).\nIn general, reliability of a power system is measured based on the avoidance of full or partial blackouts (both planned and unplanned) and their negative effect on social welfare. A blackout is an event where demand cannot be met. This\ncan occur predominantly because of contingencies (i.e., asset malfunctions) which lead to unsafe operation and may require the SOs to disconnect loads in order to avoid catastrophes. Contingencies can stem from multiple causes, such as a tree falling, lighting strike, poor maintenance or exceeding the thermal limits of a transmission line. To maintain a high reliability level at all times, the current practice of SOs is to immunize the system against a predetermined contingency list. A common choice for this list is all single asset contingencies, resulting in the so-called N − 1 reliability criterion.\nHowever, contingency probabilities are difficult to obtain and their impact is hard to assess. Furthermore, the high penetration of stochastic and often uncontrollable renewable generators, makes the planing tasks significantly harder for several reasons. First, generation must equal demand at all times. Second, multiple decision making processes are taking place simultaneously on multiple timescales. Third, each decision process involves high dimensional decision variables, and complex non-linear (Powell & Meisel, 2015), often intractable mathematical formulations.\nFor example, in the 6-bus system in Fig. 1, a system developer might plan to expand the system by building a new transmission line between buses 3 and 4. Expanding the grid is a long term process and a decision must be taken years in advance. However, this decision affects the future maintenance decisions, which will affect future daily planning that in turn affects the future real-time control room operations. Ideally, the system developer should consider all possible future realizations of the environment, grid, and the decision processes in all other time horizons."
    }, {
      "heading" : "2.2. Related work",
      "text" : "Several works in the literature of power systems, operational research and more recently machine learning offer approaches for solving sequential stochastic problems\nusing dynamic programming. Of which, the majority of these works focus on energy storage (Lai et al., 2010; Xi et al., 2014; Jiang et al., 2014; Scott & Powell, 2012), unit commitment (Padhy, 2004; Dalal & Mannor, 2015; Ernst et al., 2007), and energy market bidding strategies (Song & Wang, 2003; Urieli & Stone, 2014; Jiang & Powell, 2015). To our knowledge, no work has been done to use MDPs for assessing the reliability in power grids.\nFor our proxy abstraction devise a hierarchical model. Hierarchical models, offer several benefits over flat models when appropriate. They can improve exploration, enable learning from fewer trials, and allow faster learning for new problems by reusing subtasks learned on previous problems (Dietterich, 1998). Standard approaches for hierarchical models include: planning with options (often referred to as skills) (Sutton et al., 1999), task hierarchy (Barto & Mahadevan, 2003) and hierarchy of abstract machines (Parr & Russell, 1998). These models include levels of decision making that share the same state-space and a termination condition to switch between controllers. This structure does not fit our problem well where two separate decision makers run on different state-spaces and temporal resolutions."
    }, {
      "heading" : "3. Problem Formulation",
      "text" : "Here we present a formulation for the two sequential decision processes occurring in the day ahead (DA) and realtime (RT) in terms of a hierarchal two MDP model. DA decisions are taken in order to maximize the system’s next day reliability. However, the next day reliability can only be assessed in RT, and is dependent on the system operator decision taken in RT. This results in a complex dependence between DA and RT actions and system reliability. We therefore formulate the problem using two layers of interleaved MDPs: a RT-MDP, describing the state of the system, reliability, and decisions on an hourly basis, and a DA-MDP describing the DA action of choosing a daily subset of active generators based on the upcoming day predictions. In our terminology, the former serves as a proxy for assessing decisions taken in the latter, see Fig. 2."
    }, {
      "heading" : "3.1. Day-Ahead MDP",
      "text" : "The DA-MDP is a tuple (SDA,ADA,PDA,RDA). Time index is td, denoting days. Day-ahead state sDAtd ∈ S\nDA consists of a day ahead prediction of hourly demand on each bus, and wind generation of each wind generator.1 Therefore, SDA = RTD·(nb+ng), where TD is the number of intraday time steps (24 in our case), and nb, ng are the number of buses and wind generators. For the day ahead ac-\n1In this work we consider only wind generation as a renewable source for simplicity.\ntion aDAtd ∈ A DA we use a simplified model which considers a binary vector indicating which generators participate in the next day’s generation process. The sets of generators contained in ADA represent common settings an SO can choose from. This set can be constructed by experts or inferred from data. An action aDAtd is chosen according to a policy aDAtd = π\nDA(sDAtd ). The next day state is chosen according to PDA, and is purely exogenous, i.e., P (sDAtd+1|s DA td , aDAtd ) = P (s DA td+1 |sDAtd ). The reward function RDA is a complicated function of the reliability in RT. Since we cannot obtain the day ahead reward directly, we revert to use the RT reward as a surrogate for comparing DA policies. Notice that we cannot directly use the sum of RT rewards between consecutive days as a replacement for the DA reward since the model will no longer be Markovian."
    }, {
      "heading" : "3.2. Real-Time MDP",
      "text" : "The RT-MDP is a tuple (SRT,ART,PRT,RRT). It represents the real time reliability control process. Time index is t, denoting intra-day time steps (e.g., hours). In RT power network operation, an operator may choose preventive actions at each time step, trying to immunize the system against potential malfunctions by attempting to avoid unreliable states. We model this decision making process using post-states (Powell, 2007), where at the beginning of each time interval, the agent observes the current state sRTt ∈ SRT, i.e., the realized demand and wind values for this interval and chooses an action aRTt ∈ ART. Following the agent’s action, the system is now in a post-decision state sRT,at , which is the new state, after performing action aRTt from state s RT t . Next, exogenous random information Wt is obtained, informing whether equipment malfunction (contingency) occurred during time interval t. Given sRT,at and Wt, the real time reward rRTt (s RT,a t ,Wt) which represents the system’s reliability, can be calculated, and a transition to sRTt+1 occurs, governed by P (s RT t+1|s RT,a t ,Wt). The history of this RT process can be written as hRTt = (sRT0 , a RT 0 , s RT,a 0 ,W0, r RT 0 , s RT 1 , . . . , s RT t )."
    }, {
      "heading" : "3.2.1. REAL-TIME STATE SPACE",
      "text" : "We define a RT state sRTt to be the tuple (d,w, g, e), where:\nd is a vector of stochastic nodal demand.\nw is a vector of stochastic nodal wind generation. g is a vector of controllable generation values. The DA\naction aDAtd determines which generators will have positive values, and which will be set to 0 throughout the day. Each generator has minimal and maximal generation limits while in operation. e is the topology of the grid. Includes information of current state of each edge (transmission line). e ∈ {0, 1, . . . , E}, where 0 is operational and the rest is a countdown process till the line is fixed."
    }, {
      "heading" : "3.2.2. REAL-TIME ACTION SPACE",
      "text" : "A RT action aRTt is a preventive action, that attempts to achieve better reliability of the system by immunizing against potential contingencies. The action involves redispatch ∆g, i.e., change the generation values of the working controllable generators (chosen in DA):\nsRTt aRTt−−→ sRT,at = (d,w, g + ∆g, e).\nAny action is allowed as long as it is within the minimal and maximal generator limits. Notice that ∆gi 6= 0 for working generators only (aDAtd,i = 1)."
    }, {
      "heading" : "3.2.3. REAL-TIME TRANSITION KERNEL",
      "text" : "The RT transition kernel can be factorized to exogenous transitions of demand, wind generation, and contingencies. It is conditioned on the last RT state and action (encoded in the RT post-state), and on the corresponding last DA decision taken to determine participating generators:\nsRTt+1 = f(s RT,a t ,Wt, a DA td ).\nThe dependence between RT and DA states is expressed using two sets of equations. The first is RT demand process, based on DA demand prediction:\ndRTt = d DA t + δt (1)\nδt+1 = δt + t, (2)\nwhere dRTt is the RT demand vector at time t, and d DA t is the DA prediction vector for time t of the day. The dynamics in Eqs. (1)-(2) also hold for the wind generation process. For this work we chose this autoregressive random bias process for simplicity, however more complicated methods, such as in (Box et al., 2015; Papavasiliou & Oren, 2013; Taylor & Buizza, 2002), can be considered. The second equation coupling DA and RT determines the generators participating in current day generation process:\ngt+1,i = { gt,i + ∆gt,i if i ∈ I(aDAtd,i) 0 otherwise,\n(3)\nwhere I(aDAtd,i) is the index set of generators chosen by DA action aDAtd,i.\nLastly, random exogenous information Wt specifies whether a contingency happened in the system, causing transmission line i to fail, changing the network topology to et+1. The probability of line i to fail at each time-step is pi if at the last time-step ei was 0, and 0 otherwise."
    }, {
      "heading" : "3.2.4. REAL-TIME REWARD",
      "text" : "We choose the RT reward to be the reliability level of the power system at the current time. To assess the level of reliability, we employ the common criterion used in the industry, termed N − 1, which assesses the system ability to withstand any contingency of a single asset.\nTo calculate the reliability of the system, it is examined using a sequence of tests (contingency list), where each test is an attempt to take out a single line (contingency) and check if the system retains safe operation. Hence, the reward rRTt (s RT,a t ,Wt) is a number in [0, 1], expressing the portion of tests passed out of the predetermined contingency list, which includes all single contingencies c ∈ N−1. The reliability is calculated for a given state of the grid, and is dependent of current topology (et) and the changes to the topology due to possible new contingencies (Wt) . In practice, preserving the system in safe operation means being able to obtain a feasible solution to the power flow equations (see Sec. 2) of the network circuit. We define I[PF(c,sRT,at ,Wt)] to be 1 if a power flow solution exists, and 0 otherwise. As a result, the RT reward is:\nrRTt (s RT,a t ,Wt) =\n1 |N−1| ∑ c∈N−1 I[PF(c,sRT,at ,Wt)]."
    }, {
      "heading" : "4. Interleaved Approximate Policy Improvement",
      "text" : "In this section we present our algorithm, called Interleaved Approximate Policy Improvement (IAPI), presented in Alg. 1, for jointly learning the RT reliability value function while searching for an optimal DA policy. We use the term interleaved since the policy improvement in one MDP is based on the second MDP’s value function. We use simulation based value learning to assess the RT reliability of the system and the cross entropy method (De Boer et al., 2005; Szita & Lörincz, 2006) for improving the DA policy. Our method scales to large systems since it uses simple models with carefully engineered features and design to run on distributed computing. Since the algorithm is massively parallelizable, the more cores available, the faster the convergence will be.\nOur goal is to find an optimal DA policy πDA, under the assumption that the RT policy πRT is known. Henceforth,\nwe will use π to symbolize πDA. As explained in Sec. 3, reliability is not explicitly defined on the DA level and we instead use the RT value function vπ as a surrogate for comparing between different DA policies. Differently than the common notation, vπ denotes the RT value function, under the fixed RT policy πRT, and a DA policy π.\nAlgorithm 1 IAPI Algorithm\nInput: initial distribution P (0)ψ for DA policy parameters Output: optimal DA policy π(ψ∗)\n1: initialize SRTtest = ∅ 2: repeat 3: for i ≤ N do 4: draw ψi ∼ P (k)ψ 5: sample Nepisodes trajectories using πi = π(ψi) 6: approximate vπi using TD(0) 7: add TD(0) trajectories to SRTtest 8: end for 9: set empirical mean v̂(k)i = ∑ s∈SRTtest\nvπi(s), ∀i ∈ [N ] 10: rank policies πi according to v̂i 11: use ψi of the top percentile πi to update P (k) ψ 12: k = k + 1 13: until convergence\nOur method includes the following components:\nDay Ahead Policy Approximation We define a parametric DA policy as π(sDA;ψ) = arg maxaDA∈ADA ψ\n>Φ(sDA, aDA), where aDA is the day ahead action dictating which generators will be active during the day, Φ(sDA, aDA) are features of DA state sDA and action aDA.\nA plausible choice for mapping DA state sDA to an action aDA is using multi-class classifiers. However, for large number of classes (20 in our experiments) these methods require a significant number of simulations for training (Bishop, 2006). Furthermore, approaches for classification-based policy learning often require obtaining multiple rollouts for all the actions from a state during the training procedure (Gabillon et al., 2011), which in our case will result in a full value evaluation per each action and might prove overly encumbering. To mitigate these complexities, our policy chooses the action that maximizes the inner product with ψ.\nReal Time Value Function Approximation For a fixed DA policy π we approximate the RT value function using the TD(0) algorithm (Sutton & Barto, 1998); see Fig 3. The RT value function is parametrized as vπ(sRT; θπ) = θ>π φ(s\nRT), with the parameter vector θπ depends on π, and φ(sRT) being the features of RT state sRT.\nDay Ahead Policy Comparison A comparison between different DA policies πi is done by calculating the empirical mean of RT value function Evπi ≈ ∑ s∈SRTtest\nvπi(s), using a set of representative RT initial states SRTtest. This set is composed of the full history of all RT states visited during the current IAPI iteration, enabling expected value estimation using many probable states with only linear computational complexity in |SRTtest|.\nDay Ahead Policy Improvement using Cross Entropy DA policy improvement is achieved using the cross entropy method (De Boer et al., 2005; Szita & Lörincz, 2006). In this method, initial policies are sampled from a distribution P (0)ψ . Following which, in each iteration k policy parameters ψ are drawn from P (k)ψ , and their top percentile, according to the RT value, is used to update P (k+1)ψ (De Boer et al., 2005; Szita & Lörincz, 2006). In our experiments we set P (0)ψ such that it includes ψ that equally separate ψ>Φ(sDA0 , a DA i ), making this inner product equal, for all the different actions aDAi . The distribution P (k) ψ is a Gaussian mixture with means set to ψ(k−1) that belong to the top percentile. The convergence criterion we use in our experiments with the difference between the top-percentile values average of two consecutive iterations 1 Ntop ∑Ntop i=1 ( v̂ (k) i − v̂ (k−1) i )2 < . By using the cross entropy method, we avoid using gradient-based optimization which may be difficult to compute in our case due to the discrete, non-linear nature of ACPF solutions and their dependence of generation (Cain et al., 2012), which dictate the level of reliability.\nThe criterion for comparing policies is a parametric RT value function, vπ(sRT; θπ), as oppose to using rollouts for policy evaluation (Gabillon et al., 2011). The reason for this choice is three-fold. First, since a rollout only explores a small part of the space, assuming a structure allows us to better generalize to unvisited states. This assumption is supported by our experiments; see Fig. 5. Second, this functional representation allows us to fairly compare different DA policies using a common set of representative RT initial states SRTtest. Third, our end-goal is to use the value\nfunction learned by this algorithm as a proxy for system reliability in RT."
    }, {
      "heading" : "5. Experiments",
      "text" : "In this section we show results of IAPI algorithm on the IEEE RTS-96 test system, that is considered a standard testcase in the power systems literature (Wong et al., 1999); see Fig. 4. This test-case is an example for a power grid of a medium-sized country, containing 73 buses, 99 generators, and 120 transmission lines. We updated the test-case to include 9 additional wind generators to better represent current power grids. We use daily demand and wind profiles based on real historical records as published in (Pandzic et al., 2015). As stated in Sec. 1, this is a complicated, high dimensional system, which cannot be solved using bruteforce methods. The state space of this system can have O(2120) line configurations, with O(D73 · G9r ) demand values (D) and wind generation values (Gr) at each time, which are of a stochastic nature. This is without accounting for the day-ahead prediction, which will be the power of 24 of this number (for each hour of the day). Controlling which controllable generators are on/off makesO(299) integer decisions, and O(G99·24c ) generation levels for Gc possible values per each generator.\nTo compose the DA action set ADA, we define 20 subsets of active generators chosen at random, and fix it for the rest of the simulation. These subsets contain varying numbers of generators with different capacities, to enable meeting demand for the different possible daily profiles. For the DA we use a K + 4 feature vector\nΦ(sDA, aDA) = (1, Uv, Lv, P, I[aDA=a1], . . . , I[aDA=aK ]),\nwhere\nK is the number of actions (K = 20 in our experiments).\nUv indicates if generation can meet maximal predicted daily demand.\nLv indicates if generation can meet minimal predicted daily demand.\nP is a barrier penalty function that penalizes if the average demand is close to the upper or lower generation bounds achieved by aDA.\nI[aDA=ai] is an indicator function over the selected DA action.\nFor the RT policy πRT we employ a simple heuristic, of shifting the hourly generation values to meet the realized effective demand. We consider effective demand to be demand values minus wind generation values. This is a natural approach as wind generation is not under the decision maker’s control and therefore is not considered a part of regular controllable generation. The RT feature vector φ(sRT) contains polynomial features of (D, ed, eg), where\nD is the total RT effective demand, ed is the demand entropy across the different buses, and eg is the generation entropy across the different buses,\nresulting in a 10 dimensional vector. We use the entropy feature since it compactly maps the spread of generation and demand across the network. The spread is important as the concentrations of generation and demand are directly linked to reliability issues, see Fig. 5.\nFor parameters for the dynamics described in Eqs. (1)-(2) we use (Lu et al., 2013) and choose δw0 ∼ N (0, 0.05 ·wRT0 ) for the wind forecast error, and δd0 ∼ N (0, 0.01 · d RT 0 ) for the demand forecast error. The real time variation is chosen to be t ∼ N (0, 0.05 · δ0). Line failure probability pi is set to 5 · 10−4 for each line, and its time-fill-fix E = 5.\nIn our simulation we use Nepisodes = 50 episodes, each with a 3 day horizon. Each episode starts from a random DA state sDA0 , drawn from several representing demand and wind profiles, to which we add normally distributed noise. The next day transition corresponds to adding a normally distributed bias to the previous day profile. In each crossentropy iteration we evaluate 200 DA policies (N = 200) and choose the top 20-th percentile for updating Pψ . The DA policies are evaluated in parallel, on a 200 cores cluster. For the TD(0) algorithm we use discounting with γ = 0.95.\nIn Fig. 5 we show the learned RT value vπ(sRTt ; θπ), as a function of the deviation of the overall effective demand (demand minus wind) from the DA prediction, and generation entropy across the different buses. The RT value shown is marginalized over the rest of the features, time, and daily profiles. As shown in the figure, as the realtime demand deviates from the predicted demand, reliability suffers in a quadratic dependence. This is because the generators chosen in the DA will reach their upper or\nlower thresholds, causing generation to not meet the demand. The monotonic dependence in generation entropy implies that the higher the entropy the more reliable the system. This can be understood since high entropy corresponds to a more distributed generation throughout the network, mitigating the consequence of line outages. The reason this mitigation occurs is that when less emphasis is put on specific areas of the network, the system has more flexibility to find alternative routes from generation to demand. This, however, incurs a price in real life since generation cannot be concentrated only on cheap generators.\nNext, in Fig. 6 we show the top 20-th percentile convergence of the IAPI algorithm. As can be seen, the average value is increasing and converging after 8 iterations, while the variance of the top percentile solutions is decreasing. In Fig. 7, we visualize the convergence of the IAPI algorithm by projecting on the top two principal components (PC) of the DA policy parameters ψ. We use the same PCs for all the plots. The figure shows the scattering of the drawn ψi in each iteration. As described in Alg. 1, each ψi defines a policy π(ψi) for which we calculate the estimated expected\nvalue v̂i. The dark ’+’ mark the ψs corresponding to the top percentile of v̂i. As can be seen, the IAPI algorithm explores the policy space until converging to local minima.\nIn Fig. 8 we present different daily effective demand profiles, colored according to the DA action chosen by the DA policy π(sDA;ψ), that was learned by the IAPI algorithm. A clear clustering can be observed between different daily demand profiles and the resulting action taken by the DA policy. The policy distinguishes between different consumption patterns and maps them to a corresponding set of active generators for reliable operation of the day to come.\nTo test our algorithm we compare the learned DA policy to three common heuristics. Taking the daily state as an input, these heuristics choose an eligible generator subset\nthat can satisfy the maximal effective demand according to that day’s DA prediction. The difference between them is the way they choose among the eligible subsets of each day. ’Random’ chooses one at random, ’Cost’ chooses the cheapest combination of generators, and ’Elastic’ chooses the subset with the most flexible generators, having the largest ratio between upper and lower generation limits. We evaluate the performance of the different policies using rollouts of 2000 episodes per each policy. Fig. 9 presents the box-plots of the results. As can be seen, the value varies greatly between the different methods. In the ’Random’ policy, there is an almost flat spread, demonstrating a lack of preference for a single subset when encountering a new day. The ’Cost’ and ’Elastic’ policies produce a more concentrated spread, corresponding to their preference of subset choices. The policy learned using IAPI obtains higher reward than the heuristics. This result shows the IAPI algorithm’s ability to learn a diverse DA policy."
    }, {
      "heading" : "6. Discussion",
      "text" : "In this work we present an interleaved two-MDP model, inspired by the hierarchical decision making problem of managing power grid reliability. The IAPI algorithm presented alternates between improving the DA policy, and learning the RT reliability value. The IEEE RTS-96 network in our experiments is a large enough network to capture computational complexities that arise in real-world networks.\nIn this work we focus on the power grid, however our model can be adapted to other important applications with an hierarchical decision making structure in different timescales where high level of reliability and sustainability is required. Examples for such applications are sewer systems, smart cities and traffic control.\nThe coarse model presented in this work was crafted jointly as an initial step with several SOs. This work is the tip of the iceberg and many enhancements can be considered. For example, an important aspect that is not covered by it is budget consideration. Following the practice in the power system industry, reliability and money are often treated as different “currencies”. Considering a budget will impose limitations on action selection and will complicate this problem even more. Another addition that can be made to extend the IAPI algorithm to interleave in reverse, i.e., alternating the DA improvement with improving the RT policy. Suspected drawbacks in this case are convergence problems, and the need for even more intense simulation.\nManaging high reliability in stochastic complex systems, with interleaved decision making in different time horizons, is inherently difficult and results in intractable formulations. To mitigate this, there is a growing interest in the power system community to utilize proxies that will\nenable quick assessment of reliability for different states of the grid. In this work we introduce new models and formulations, along with a simulation environment. Our hope is that this will provide a platform for other researchers in the community to develop and explore their own innovative methods, and will help to bring these two fields closer. The code for the simulation environment is available at hidden to preserve anonymity."
    } ],
    "references" : [ {
      "title" : "An efficient mixed-integer linear formulation for long-term overhead lines maintenance scheduling in power distribution systems. Power Delivery",
      "author" : [ "A Abiri-Jahromi", "M Fotuhi-Firuzabad", "E. Abbasi" ],
      "venue" : "IEEE Transactions on,",
      "citeRegEx" : "Abiri.Jahromi et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Abiri.Jahromi et al\\.",
      "year" : 2009
    }, {
      "title" : "A two-stage framework for power transformer asset maintenance management – Part I: Models and formulations",
      "author" : [ "Abiri-Jahromi", "Amir", "Parvania", "Masood", "Bouffard", "Francois", "Fotuhi-Firuzabad", "Mahmud" ],
      "venue" : "Power Systems, IEEE Transactions on,",
      "citeRegEx" : "Abiri.Jahromi et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Abiri.Jahromi et al\\.",
      "year" : 2013
    }, {
      "title" : "Reliability evaluation of power systems",
      "author" : [ "RN Allan" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "Allan,? \\Q2013\\E",
      "shortCiteRegEx" : "Allan",
      "year" : 2013
    }, {
      "title" : "Benchmarking of data mining techniques as applied to power system analysis",
      "author" : [ "Anil", "Can" ],
      "venue" : null,
      "citeRegEx" : "Anil and Can.,? \\Q2013\\E",
      "shortCiteRegEx" : "Anil and Can.",
      "year" : 2013
    }, {
      "title" : "Recent advances in hierarchical reinforcement learning",
      "author" : [ "Barto", "Andrew G", "Mahadevan", "Sridhar" ],
      "venue" : "Discrete Event Dynamic Systems,",
      "citeRegEx" : "Barto et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Barto et al\\.",
      "year" : 2003
    }, {
      "title" : "Adaptive robust optimization for the security constrained unit commitment problem",
      "author" : [ "Bertsimas", "Dimitris", "Litvinov", "Eugene", "Sun", "Xu Andy", "Zhao", "Jinye", "Zheng", "Tongxin" ],
      "venue" : "Power Systems, IEEE Transactions on,",
      "citeRegEx" : "Bertsimas et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bertsimas et al\\.",
      "year" : 2013
    }, {
      "title" : "Chance-constrained optimal power flow: Risk-aware network control under uncertainty",
      "author" : [ "Bienstock", "Daniel", "Chertkov", "Michael", "Harnett", "Sean" ],
      "venue" : "SIAM Review,",
      "citeRegEx" : "Bienstock et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bienstock et al\\.",
      "year" : 2014
    }, {
      "title" : "Time series analysis: forecasting and control",
      "author" : [ "Box", "George EP", "Jenkins", "Gwilym M", "Reinsel", "Gregory C", "Ljung", "Greta M" ],
      "venue" : null,
      "citeRegEx" : "Box et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Box et al\\.",
      "year" : 2015
    }, {
      "title" : "History of optimal power flow and formulations",
      "author" : [ "Cain", "Mary B", "Oneill", "Richard P", "Castillo", "Anya" ],
      "venue" : "Federal Energy Regulatory Commission,",
      "citeRegEx" : "Cain et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Cain et al\\.",
      "year" : 2012
    }, {
      "title" : "Reinforcement learning for the unit commitment problem",
      "author" : [ "Dalal", "Gal", "Mannor", "Shie" ],
      "venue" : "In PowerTech,",
      "citeRegEx" : "Dalal et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Dalal et al\\.",
      "year" : 2015
    }, {
      "title" : "A tutorial on the cross-entropy method",
      "author" : [ "De Boer", "Pieter-Tjerk", "Kroese", "Dirk P", "Mannor", "Shie", "Rubinstein", "Reuven Y" ],
      "venue" : "Annals of operations research,",
      "citeRegEx" : "Boer et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Boer et al\\.",
      "year" : 2005
    }, {
      "title" : "The MAXQ method for hierarchical reinforcement learning",
      "author" : [ "Dietterich", "Thomas G" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Dietterich and G.,? \\Q1998\\E",
      "shortCiteRegEx" : "Dietterich and G.",
      "year" : 1998
    }, {
      "title" : "The cross-entropy method for power system combinatorial optimization problems",
      "author" : [ "Ernst", "Damien", "Glavic", "Mevludin", "Stan", "Guy-Bart", "Mannor", "Shie", "Wehenkel", "Louis" ],
      "venue" : "Power Tech,",
      "citeRegEx" : "Ernst et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Ernst et al\\.",
      "year" : 2007
    }, {
      "title" : "Classification-based policy iteration with a critic",
      "author" : [ "Gabillon", "Victor", "Lazaric", "Alessandro", "Ghavamzadeh", "Mohammad", "Scherrer", "Bruno" ],
      "venue" : null,
      "citeRegEx" : "Gabillon et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Gabillon et al\\.",
      "year" : 2011
    }, {
      "title" : "Optimal hour-ahead bidding in the real-time electricity market with battery storage using approximate dynamic programming",
      "author" : [ "Jiang", "Daniel R", "Powell", "Warren B" ],
      "venue" : "INFORMS Journal on Computing,",
      "citeRegEx" : "Jiang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2015
    }, {
      "title" : "Optimal control policies for power demand scheduling in the smart grid",
      "author" : [ "Koutsopoulos", "Iordanis", "Tassiulas", "Leandros" ],
      "venue" : "Selected Areas in Communications, IEEE Journal on,",
      "citeRegEx" : "Koutsopoulos et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Koutsopoulos et al\\.",
      "year" : 2012
    }, {
      "title" : "An approximate dynamic programming approach to benchmark practice-based heuristics for natural gas storage valuation",
      "author" : [ "Lai", "Guoming", "Margot", "François", "Secomandi", "Nicola" ],
      "venue" : "Operations research,",
      "citeRegEx" : "Lai et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Lai et al\\.",
      "year" : 2010
    }, {
      "title" : "A comparison of forecast error generators for modeling wind and load uncertainty",
      "author" : [ "Lu", "Ning", "Diao", "Ruisheng", "Hafen", "Ryan P", "Samaan", "Nancy", "Makarov", "Yuri V" ],
      "venue" : "In Power and Energy Society General Meeting (PES),",
      "citeRegEx" : "Lu et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2013
    }, {
      "title" : "Unit commitment-a bibliographical survey",
      "author" : [ "Padhy", "Narayana Prasad" ],
      "venue" : "Power Systems, IEEE Transactions on,",
      "citeRegEx" : "Padhy and Prasad.,? \\Q2004\\E",
      "shortCiteRegEx" : "Padhy and Prasad.",
      "year" : 2004
    }, {
      "title" : "Near-optimal method for siting and sizing of distributed storage in a transmission",
      "author" : [ "Pandzic", "Hrvoje", "Wang", "Yannan", "Qiu", "Ting", "Dvorkin", "Yury", "Kirschen", "Daniel S" ],
      "venue" : null,
      "citeRegEx" : "Pandzic et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Pandzic et al\\.",
      "year" : 2015
    }, {
      "title" : "Multiarea stochastic unit commitment for high wind penetration in a transmission constrained network",
      "author" : [ "Papavasiliou", "Anthony", "Oren", "Shmuel S" ],
      "venue" : "Operations Research,",
      "citeRegEx" : "Papavasiliou et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Papavasiliou et al\\.",
      "year" : 2013
    }, {
      "title" : "Reinforcement learning with hierarchies of machines",
      "author" : [ "Parr", "Ronald", "Russell", "Stuart" ],
      "venue" : "Advances in neural information processing systems,",
      "citeRegEx" : "Parr et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Parr et al\\.",
      "year" : 1998
    }, {
      "title" : "Approximate Dynamic Programming: Solving the curses of dimensionality, volume 703",
      "author" : [ "Powell", "Warren B" ],
      "venue" : null,
      "citeRegEx" : "Powell and B.,? \\Q2007\\E",
      "shortCiteRegEx" : "Powell and B.",
      "year" : 2007
    }, {
      "title" : "Tutorial on stochastic optimization in energy – Part I: Modeling and Policies",
      "author" : [ "Powell", "Warren B", "Meisel", "Stephan" ],
      "venue" : null,
      "citeRegEx" : "Powell et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Powell et al\\.",
      "year" : 2015
    }, {
      "title" : "Approximate dynamic programming for energy storage with new results on instrumental variables and projected bellman errors",
      "author" : [ "W Scott", "Powell", "Warren B" ],
      "venue" : "Submitted to Operations Research (Under Review),",
      "citeRegEx" : "Scott et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Scott et al\\.",
      "year" : 2012
    }, {
      "title" : "Handbook of learning and approximate dynamic programming, volume 2",
      "author" : [ "Si", "Jennie" ],
      "venue" : null,
      "citeRegEx" : "Si and Jennie.,? \\Q2004\\E",
      "shortCiteRegEx" : "Si and Jennie.",
      "year" : 2004
    }, {
      "title" : "Operation of marketoriented power systems",
      "author" : [ "Song", "Yong-Hua", "Wang", "Xi-Fan" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "Song et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2003
    }, {
      "title" : "Reinforcement learning: An introduction",
      "author" : [ "Sutton", "Richard S", "Barto", "Andrew G" ],
      "venue" : "MIT press,",
      "citeRegEx" : "Sutton et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 1998
    }, {
      "title" : "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning",
      "author" : [ "Sutton", "Richard S", "Precup", "Doina", "Singh", "Satinder" ],
      "venue" : "Artificial intelligence,",
      "citeRegEx" : "Sutton et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 1999
    }, {
      "title" : "Learning Tetris using the noisy cross-entropy method",
      "author" : [ "Szita", "István", "Lörincz", "András" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Szita et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Szita et al\\.",
      "year" : 2006
    }, {
      "title" : "Lifeline for renewable power",
      "author" : [ "Talbot", "David" ],
      "venue" : "Technol Rev,",
      "citeRegEx" : "Talbot and David.,? \\Q2009\\E",
      "shortCiteRegEx" : "Talbot and David.",
      "year" : 2009
    }, {
      "title" : "Neural network load forecasting with weather ensemble predictions",
      "author" : [ "Taylor", "James W", "Buizza", "Roberto" ],
      "venue" : "Power Systems, IEEE Transactions on,",
      "citeRegEx" : "Taylor et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Taylor et al\\.",
      "year" : 2002
    }, {
      "title" : "Tactex’13: a champion adaptive power trading agent",
      "author" : [ "Urieli", "Daniel", "Stone", "Peter" ],
      "venue" : "In Proceedings of the 2014 international conference on Autonomous agents and multi-agent systems,",
      "citeRegEx" : "Urieli et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Urieli et al\\.",
      "year" : 2014
    }, {
      "title" : "The IEEE reliability test system1996. a report prepared by the reliability test system task force of the application of probability methods subcom",
      "author" : [ "Wong", "Paul", "P Albrecht", "R Allan", "Billinton", "Roy", "Chen", "Qian", "C Fong", "Haddad", "Sandro", "Li", "Wenyuan", "R Mukerji", "Patton", "Diane" ],
      "venue" : null,
      "citeRegEx" : "Wong et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Wong et al\\.",
      "year" : 1996
    }, {
      "title" : "Power generation operation and control2nd edition",
      "author" : [ "Wood", "Allen J", "B. Wollenberg" ],
      "venue" : "In Fuel and Energy Abstracts,",
      "citeRegEx" : "Wood et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Wood et al\\.",
      "year" : 1996
    }, {
      "title" : "Security-constrained generation and transmission outage scheduling with uncertainties",
      "author" : [ "Wu", "Lei", "Shahidehpour", "Mohammad", "Fu", "Yong" ],
      "venue" : "Power Systems, IEEE Transactions on,",
      "citeRegEx" : "Wu et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2010
    }, {
      "title" : "A stochastic dynamic programming model for co-optimization of distributed energy storage",
      "author" : [ "Xi", "Xiaomin", "Sioshansi", "Ramteen", "Marano", "Vincenzo" ],
      "venue" : "Energy Systems,",
      "citeRegEx" : "Xi et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Xi et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "These often use restrictive simplifications such as independence between the decision processes in the different time-scales or consider myopic decisions only (Abiri-Jahromi et al., 2009; Wu et al., 2010; Abiri-Jahromi et al., 2013).",
      "startOffset" : 159,
      "endOffset" : 232
    }, {
      "referenceID" : 35,
      "context" : "These often use restrictive simplifications such as independence between the decision processes in the different time-scales or consider myopic decisions only (Abiri-Jahromi et al., 2009; Wu et al., 2010; Abiri-Jahromi et al., 2013).",
      "startOffset" : 159,
      "endOffset" : 232
    }, {
      "referenceID" : 1,
      "context" : "These often use restrictive simplifications such as independence between the decision processes in the different time-scales or consider myopic decisions only (Abiri-Jahromi et al., 2009; Wu et al., 2010; Abiri-Jahromi et al., 2013).",
      "startOffset" : 159,
      "endOffset" : 232
    }, {
      "referenceID" : 8,
      "context" : ", voltage magnitude and angles of each node (Cain et al., 2012).",
      "startOffset" : 44,
      "endOffset" : 63
    }, {
      "referenceID" : 16,
      "context" : "Of which, the majority of these works focus on energy storage (Lai et al., 2010; Xi et al., 2014; Jiang et al., 2014; Scott & Powell, 2012), unit commitment (Padhy, 2004; Dalal & Mannor, 2015; Ernst et al.",
      "startOffset" : 62,
      "endOffset" : 139
    }, {
      "referenceID" : 36,
      "context" : "Of which, the majority of these works focus on energy storage (Lai et al., 2010; Xi et al., 2014; Jiang et al., 2014; Scott & Powell, 2012), unit commitment (Padhy, 2004; Dalal & Mannor, 2015; Ernst et al.",
      "startOffset" : 62,
      "endOffset" : 139
    }, {
      "referenceID" : 12,
      "context" : ", 2014; Scott & Powell, 2012), unit commitment (Padhy, 2004; Dalal & Mannor, 2015; Ernst et al., 2007), and energy market bidding strategies (Song & Wang, 2003; Urieli & Stone, 2014; Jiang & Powell, 2015).",
      "startOffset" : 47,
      "endOffset" : 102
    }, {
      "referenceID" : 28,
      "context" : "Standard approaches for hierarchical models include: planning with options (often referred to as skills) (Sutton et al., 1999), task hierarchy (Barto & Mahadevan, 2003) and hierarchy of abstract machines (Parr & Russell, 1998).",
      "startOffset" : 105,
      "endOffset" : 126
    }, {
      "referenceID" : 7,
      "context" : "For this work we chose this autoregressive random bias process for simplicity, however more complicated methods, such as in (Box et al., 2015; Papavasiliou & Oren, 2013; Taylor & Buizza, 2002), can be considered.",
      "startOffset" : 124,
      "endOffset" : 192
    }, {
      "referenceID" : 13,
      "context" : "Furthermore, approaches for classification-based policy learning often require obtaining multiple rollouts for all the actions from a state during the training procedure (Gabillon et al., 2011), which in our case will result in a full value evaluation per each action and might prove overly encumbering.",
      "startOffset" : 170,
      "endOffset" : 193
    }, {
      "referenceID" : 8,
      "context" : "tropy method, we avoid using gradient-based optimization which may be difficult to compute in our case due to the discrete, non-linear nature of ACPF solutions and their dependence of generation (Cain et al., 2012), which dictate the level of reliability.",
      "startOffset" : 195,
      "endOffset" : 214
    }, {
      "referenceID" : 13,
      "context" : "The criterion for comparing policies is a parametric RT value function, vπ(sRT; θπ), as oppose to using rollouts for policy evaluation (Gabillon et al., 2011).",
      "startOffset" : 135,
      "endOffset" : 158
    }, {
      "referenceID" : 19,
      "context" : "We use daily demand and wind profiles based on real historical records as published in (Pandzic et al., 2015).",
      "startOffset" : 87,
      "endOffset" : 109
    }, {
      "referenceID" : 17,
      "context" : "(1)-(2) we use (Lu et al., 2013) and choose δ 0 ∼ N (0, 0.",
      "startOffset" : 15,
      "endOffset" : 32
    } ],
    "year" : 2016,
    "abstractText" : "The power grid is a complex and vital system that necessitates careful reliability management. Managing the grid is a difficult problem with multiple time scales of decision making and stochastic behavior due to renewable energy generations, variable demand and unplanned outages. Solving this problem in the face of uncertainty requires a new methodology with tractable algorithms. In this work, we introduce a new model for hierarchical decision making in complex systems. We apply reinforcement learning (RL) methods to learn a proxy, i.e., a level of abstraction, for real-time power grid reliability. We devise an algorithm that alternates between slow time-scale policy improvement, and fast timescale value function approximation. We compare our results to prevailing heuristics, and show the strength of our method.",
    "creator" : "LaTeX with hyperref package"
  }
}