{
  "name" : "1107.0019.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Sear hing for Bayesian Network Stru tures in the Spa e of"
    }, {
      "heading" : "Restri ted A y li Partially Dire ted Graphs",
      "text" : "Silvia A id a id de sai.ugr.es Luis M. de Campos l i de sai.ugr.es Departamento de Cien ias de la Computa i on e I.A. E.T.S.I. Inform ati a, Universidad de Granada 18071 - Granada, SPAIN"
    }, {
      "heading" : "Abstra t",
      "text" : "Although many algorithms have been designed to onstru t Bayesian network stru - tures using di erent approa hes and prin iples, they all employ only two methods: those based on independen e riteria, and those based on a s oring fun tion and a sear h pro-\nedure (although some methods ombine the two). Within the s ore+sear h paradigm, the dominant approa h uses lo al sear h methods in the spa e of dire ted a y li graphs (DAGs), where the usual hoi es for de ning the elementary modi ations (lo al hanges) that an be applied are ar addition, ar deletion, and ar reversal. In this paper, we propose a new lo al sear h method that uses a di erent sear h spa e, and whi h takes a ount of the on ept of equivalen e between network stru tures: restri ted a y li partially dire ted graphs (RPDAGs). In this way, the number of di erent on gurations of the sear h spa e is redu ed, thus improving eÆ ien y. Moreover, although the nal result must ne essarily be a lo al optimum given the nature of the sear h method, the topology of the new sear h spa e, whi h avoids making early de isions about the dire tions of the ar s, may help to nd better lo al optima than those obtained by sear hing in the DAG spa e. Detailed results of the evaluation of the proposed sear h method on several test problems, in luding the well-known Alarm Monitoring System, are also presented."
    }, {
      "heading" : "1. Introdu tion",
      "text" : "Nowadays, the usefulness of Bayesian networks (Pearl, 1988) in representing knowledge with un ertainty and eÆ ient reasoning is widely a epted. A Bayesian network onsists of a qualitative part, a dire ted a y li graph (DAG), and a quantitative one, a olle tion of numeri al parameters, usually onditional probability tables. The knowledge represented in the graphi al omponent is expressed in terms of dependen e and independen e relationships between variables. These relationships are en oded using the presen e or absen e of links between nodes in the graph. The knowledge represented in the numeri al part quanti es the dependen es en oded in the graph, and allows us to introdu e un ertainty into the model. All in all, Bayesian networks provide a very intuitive graphi al tool for representing available knowledge.\nAnother attra tion of Bayesian networks is their ability to eÆ iently perform reasoning tasks (Jensen, 1996; Pearl, 1988). The independen es represented in the DAG are the key to this ability, redu ing hanges in the knowledge state to lo al omputations. In addition, important savings in storage requirements are possible sin e independen es allow a fa torization of the global numeri al representation (the joint probability distribution).\n2003 AI A ess Foundation and Morgan Kaufmann Publishers. All rights reserved.\nThere has been a lot of work in re ent years on the automati learning of Bayesian networks from data. Consequently, there are a great many learning algorithms whi h may be subdivided into two general approa hes: methods based on onditional independen e tests (also alled onstraint-based), and methods based on a s oring fun tion 1 and a sear h pro edure.\nThe algorithms based on independen e tests perform a qualitative study of the dependen e and independen e relationships among the variables in the domain, and attempt to\nnd a network that represents these relationships as far as possible. They therefore take a list of onditional independen e relationships (obtained from the data by means of onditional independen e tests) as the input, and generate a network that represents most of these relationships. The omputational ost of these algorithms is mainly due to the number and\nomplexity of su h tests, whi h an also ause unreliable results. Some of the algorithms based on this approa h obtain simpli ed models (de Campos, 1998; de Campos & Huete, 1997; Geiger, Paz & Pearl, 1990, 1993; Huete & de Campos, 1993), whereas other are designed for general DAGs (de Campos & Huete, 2000a; Cheng, Bell & Liu, 1997; Meek, 1995; Spirtes, Glymour & S heines, 1993; Verma & Pearl, 1990; Wermuth & Lauritzen, 1983).\nThe algorithms based on a s oring fun tion attempt to nd a graph that maximizes the sele ted s ore; the s oring fun tion is usually de ned as a measure of t between the graph and the data. All of them use a s oring fun tion in ombination with a sear h method in order to measure the goodness of ea h explored stru ture from the spa e of feasible solutions. During the exploration pro ess, the s oring fun tion is applied in order to evaluate the tness of ea h andidate stru ture to the data. Ea h algorithm is hara terized by the spe i s oring fun tion and sear h pro edure used. The s oring fun tions are based on di erent prin iples, su h as entropy (Herskovits & Cooper, 1990; Chow & Liu, 1968; de Campos, 1998; Rebane & Pearl, 1987), Bayesian approa hes (Buntine, 1994, 1996; Cooper & Herskovits, 1992; Friedman & Koller, 2000; Friedman, Na hman & Pe er, 1999; Geiger & He kerman, 1995; He kerman, 1996; He kerman, Geiger & Chi kering, 1995; Madigan & Raftery, 1994; Ramoni & Sebastiani, 1997; Ste k, 2000), or the Minimum Des ription Length (Bou kaert, 1993; Friedman & Goldszmidt, 1996; Lam & Ba hus, 1994; Suzuki, 1993, 1996; Tian, 2000).\nThere are also hybrid algorithms that use a ombination of onstraint-based and s oringbased methods: In several works (Singh & Valtorta, 1993, 1995; Spirtes & Meek, 1995; Dash & Druzdzel, 1999; de Campos, Fern andez-Luna & Puerta, 2003) the independen e-based and s oring-based algorithms are maintained as separate pro esses, whi h are ombined in some way, whereas the hybridization proposed by A id and de Campos (2000, 2001) is based on the development of a s oring fun tion that quanti es the dis repan ies between the independen es displayed by the andidate network and the database, and the sear h pro ess is limited by the results of some independen e tests.\nIn this paper, we fo us on the s oring+sear h approa h. Although algorithms in this ategory have ommonly used lo al sear h methods (Buntine, 1991; Cooper & Herskovits, 1992; Chi kering, Geiger & He kerman, 1995; de Campos et al., 2003; He kerman et al., 1995), due to the exponentially large size of the sear h spa e, there is a growing interest in other heuristi sear h methods, i.e. simulated annealing (Chi kering et al., 1995),\n1. Some authors also use the term s oring metri .\ntabu sear h (Bou kaert, 1995; Muntenau & Cau, 2000), bran h and bound (Tian, 2000), geneti algorithms and evolutionary programming (Larra~naga, Poza, Yurramendi, Murga & Kuijpers, 1996; Myers, Laskey & Levitt, 1999; Wong, Lam & Leung, 1999), Markov\nhain Monte Carlo (Ko ka & Castelo, 2001; Myers et al., 1999), variable neighborhood sear h (de Campos & Puerta, 2001a; Puerta, 2001), ant olony optimization (de Campos, Fern andez-Luna, G amez & Puerta, 2002; Puerta, 2001), greedy randomized adaptive sear h pro edures (de Campos, Fern andez-Luna & Puerta, 2002), and estimation of distribution algorithms (Blan o, Inza & Larra~naga, 2003).\nAll of these employ di erent sear h methods but the same sear h spa e: the spa e of DAGs. A possible alternative is the spa e of the orderings of the variables (de Campos, G amez & Puerta, 2002; de Campos & Huete, 2000b; de Campos & Puerta, 2001b; Friedman & Koller, 2000; Larra~naga, Kuijpers & Murga, 1996). In this paper, however, we are more interested in the spa e of equivalen e lasses of DAGs (Pearl & Verma, 1990), i.e. lasses of DAGs with ea h representing a di erent set of probability distributions. There is also a number of learning algorithms that arry out the sear h in this spa e (Andersson, Madigan & Perlman, 1997; Chi kering, 1996; Dash & Druzdzel, 1999; Madigan, Anderson, Perlman & Volinsky, 1996; Spirtes & Meek, 1995). This feature redu es the size of the sear h spa e, although re ent results (Gillispie & Perlman, 2001) on rm that this redu tion is not as important in terms of the DAG spa e as previously hoped (the ratio of the number of DAGs to the number of equivalen e lasses is lower than four). The pri e we have to pay for this redu tion is that the evaluation of the andidate stru tures does not take advantage of an important property of many s oring fun tions, namely de omposability, and therefore the\norresponding algorithms are less eÆ ient.\nIn this paper we propose a new sear h spa e whi h is losely related to the spa e of equivalen e lasses of DAGs, and whi h we have alled the spa e of restri ted a y li partially dire ted graphs (RPDAGs). We de ne a lo al sear h algorithm in this spa e, and show that by using a de omposable s oring fun tion, we an evaluate lo ally the s ore of the stru tures in the neighborhood of the urrent RPDAG, thus obtaining an eÆ ient algorithm while retaining many of the advantages of using equivalen e lasses of DAGs. After the original submission of this paper, Chi kering (2002) proposed another learning algorithm that sear hes in the spa e of equivalen e lasses of DAGs and whi h an also s ore the\nandidate stru tures lo ally, using a anoni al representation s heme for equivalen e lasses, alled ompleted a y li partially dire ted graphs (CPDAGs).\nThe rest of the paper is organized as follows: se tion 2 dis usses some preliminaries and the advantages and disadvantages of arrying out the sear h pro ess in the spa es of DAGs and equivalen e lasses of DAGs. Se tion 3 des ribes the graphi al obje ts, RPDAGs, that will be in luded in the proposed sear h spa e. In Se tion 4, a detailed des ription of the lo al sear h method used to explore this spa e is provided. Se tion 5 shows how we an evaluate RPDAGs eÆ iently using a de omposable s oring fun tion. Se tion 6 ontains the experimental results of the evaluation of the proposed algorithm on the Alarm (Beinli h, Suermondt, Chavez & Cooper, 1989), Insuran e (Binder, Koller, Russell & Kanazawa, 1997) and Hail nder (Abramson, Brown, Murphy & Winkler, 1996) networks, as well as on databases from the UCI Ma hine Learning Repository. We also in lude an empiri al\nomparison with other state-of-the-art learning algorithms. Finally, Se tion 7 ontains the on luding remarks and some proposals for future work."
    }, {
      "heading" : "2. DAGs and Equivalen e Classes of DAGs",
      "text" : "The sear h pro edures used within Bayesian network learning algorithms usually operate on the spa e of DAGs. In this ontext, the problem an be formally expressed as: Given a omplete training set (i.e. we do not onsider missing values or latent variables) D = fu 1 ; : : : ;u m g of instan es of a nite set of n variables, U , nd the DAG H su h that\nH = arg max\nH2H\nn\ng(H : D) (1)\nwhere g(H : D) is a s oring fun tion measuring the tness of any andidate DAG H to the dataset D, and H\nn\nis the family of all the DAGs with n nodes\n2\n.\nMany of the sear h pro edures, in luding the ommonly used lo al sear h methods, rely on a neighborhood stru ture that de nes the lo al rules (operators) used to move within the sear h spa e. The standard neighborhood in the spa e of DAGs uses the operators of ar addition, ar deletion and ar reversal, thereby avoiding (in the rst and the third ase) the in lusion of dire ted y les in the graph.\nThe algorithms that sear h in the spa e of DAGs using lo al methods are eÆ ient mainly be ause of the de omposability property that many s oring fun tions exhibit. A s oring fun tion g is said to be de omposable if the s ore of any Bayesian network stru ture may be expressed as a produ t (or a sum in the log-spa e) of lo al s ores involving only one node and its parents:\ng(H : D) =\nX\ny2U\ng\nD\n(y; Pa\nH\n(y)) (2)\ng\nD\n(y; Pa\nH\n(y)) = g(y; Pa\nH\n(y) : N\ny;Pa\nH\n(y)\n) (3)\nwhere N\ny;Pa\nH\n(y)\nare the statisti s of the variables y and Pa\nH\n(y) in D, i.e. the number of\ninstan es in D that mat h ea h possible instantiation of y and Pa\nH\n(y). Pa\nH\n(y) will denote\nthe parent set of y in the DAG H, i.e. Pa\nH\n(y) = ft 2 U j t! y 2 Hg.\nA pro edure that hanges one ar at ea h move an eÆ iently evaluate the improvement obtained by this hange. Su h a pro edure an reuse the omputations arried out at previous stages, and only the statisti s orresponding to the variables whose parent sets have been modi ed need to be re omputed. The addition or deletion of an ar x! y in a DAG H an therefore be evaluated by omputing only one new lo al s ore, g\nD\n(y; Pa\nH[fxg\n(y)) or\ng\nD\n(y; Pa\nHnfxg\n(y)), respe tively. The evaluation of the reversal of an ar x! y requires the\nomputation of two new lo al s ores, g\nD\n(y; Pa\nHnfxg\n(y)) and g\nD\n(x; Pa\nH[fyg\n(x)).\nIt should be noted that ea h stru ture in the DAG spa e is not always di erent from the others in terms of its representation apability: if we interpret the ar s in a DAG as\nausal intera tions between variables, then ea h DAG represents a di erent model; however, if we see a DAG as a set of dependen e/independen e relationships between variables (that permits us to fa torize a joint probability distribution), then di erent DAGs may represent the same model. Even in the ase of using a ausal interpretation, if we use observationonly data (as opposed to experimental data where some variables may be manipulated), it\n2. For reasons of simpli ity, the set of nodes whi h are in one-to-one orresponden e with the variables in\nU will also be denoted by U .\nis quite ommon for two Bayesian networks to be empiri ally indistinguishable. When two DAGs H and H 0 an represent the same set of onditional independen e assertions, we say that these DAGs are equivalent 3 (Pearl & Verma, 1990), and we denote this as H ' H 0 .\nWhen learning Bayesian networks from data using s oring fun tions, two di erent (but equivalent) DAGs may be indistinguishable, due to the existen e of invariant properties on equivalent DAGs, yielding equal s ores. We ould take advantage of this in order to get a more redu ed spa e of stru tures to be explored.\nThe following theorem provides a graphi al riterion for determining the equivalen e of\ntwo DAGs:\nTheorem 1 (Pearl & Verma, 1990) Two DAGs are equivalent if and only if they have the same skeleton and the same v-stru tures.\nThe skeleton of a DAG is the undire ted graph that results from ignoring the dire tionality of every edge. A v-stru ture in a DAG H is an ordered triplet of nodes, (x; y; z), su h that (1) H ontains the ar s x! y and y z, and (2) the nodes x and z are not adja ent in H. A head-to-head pattern (shortened h-h) in a DAG H is an ordered triplet of nodes, (x; y; z), su h that H ontains the ar s x ! y and y z. Note that in an h-h pattern (x; y; z) the nodes x and z an be adja ent.\nAnother hara terization of equivalent DAGs was presented by Chi kering (1995), together with proof that several s oring fun tions used for learning Bayesian networks from data give the same s ore to equivalent stru tures (su h fun tions are alled s ore equivalent fun tions).\nThe on ept of equivalen e of DAGs partitions the spa e of DAGs into a set of equivalen e lasses. Whenever a s ore equivalent fun tion is used, it seems natural to sear h for the best on guration in this new spa e of equivalen e lasses of DAGs. This hange in the sear h spa e may bring several advantages:\nThe spa e of equivalent lasses is more redu ed than the spa e of DAGs (although it is still enormous). We ould therefore expe t to obtain better results (with the same sear h e ort).\nAs we do not spend time generating (using the operators de ned to move between neighboring on gurations in the sear h spa e) and evaluating (using the s oring fun tion) equivalent DAGs, we ould obtain more eÆ ient algorithms. However, as the ratio of the number of equivalen e lasses to the number of DAGs seems (empiri ally) to asymptote to 0.267 (Gillispie & Perlman, 2001), the eÆ ien y improvement may be small.\nThe sear h in the spa e of DAGs may be easily trapped in a lo al optimum, and the situation worsens as the operators de ned for this spa e an move between on gurations orresponding to equivalent DAGs (whi h will be evaluated with the same s ore). This diÆ ulty an be partially avoided if we sear h in the spa e of equivalen e\nlasses.\n3. Several authors also use the term independen e equivalent, and reserve the term distribution equivalent\n(wrt some family of distributions F) for the more restri ted ase where the two DAGs an represent the same probability distributions. In the ommon situation where all the variables are dis rete and F is the family of unrestri ted multinomial distributions, these two on epts of equivalen e oin ide.\nThe disadvantages are that, in this spa e of equivalen e lasses, it is more expensive to generate neighboring on gurations, be ause we may be for ed to perform some kind of\nonsisten y he k, in order to ensure that these on gurations represent equivalen e lasses\n4\n;\nin addition, the evaluation of the neighboring on gurations may also be more expensive if we are not able to take advantage of the de omposability property of the s oring fun tion. Finally, the new sear h spa e might introdu e new lo al maxima that are not present in DAG spa e.\nIn order to design an exploring pro ess for the spa e of equivalen e lasses we ould use two distin t approa hes: the rst onsists in onsidering that an equivalen e lass is represented by any of its omponents (in this ase, it is ne essary to avoid evaluating more than one omponent per lass); and the se ond onsists in using a anoni al representation s heme for the lasses.\nThe graphi al obje ts ommonly used to represent equivalen e lasses of DAGs are a y li partially dire ted graphs (Pearl & Verma, 1990) (known as PDAGs). These graphs\nontain both dire ted (ar s) and undire ted (links) edges, but no dire ted y les. Given a PDAG G de ned on a nite set of nodes U and a node y 2 U , the following subsets of nodes are de ned:\nPa\nG\n(y) = ft 2 U j t! y 2 Gg, the set of parents of y.\nCh\nG\n(y) = ft 2 U j y ! t 2 Gg, the set of hildren of y.\nNe\nG\n(y) = ft 2 U j y|t 2 Gg, the set of neighbors of y.\nAd\nG\n(y) = ft 2 U j t ! y 2 G; or y ! t 2 G or y|t 2 Gg, the set of adja ents to y.\nObviously Ad\nG\n(y) = Pa\nG\n(y) [ Ch\nG\n(y) [Ne\nG\n(y).\nAn ar x ! y in a DAG H is ompelled if it appears in every DAG belonging to the same equivalen e lass as H. An ar x ! y in H is said to be reversible if it is not ompelled, i.e. there is a DAG H 0 equivalent to H that ontains the ar x y. As every DAG in a parti ular equivalen e lass has the same set of ompelled and reversible ar s, a anoni al representation of an equivalen e lass is the PDAG onsisting of an ar for every ompelled ar in the equivalen e lass, and a link for every reversible ar . This kind of representation has been given several names: pattern (Spirtes & Meek, 1995), ompleted PDAG (CPDAG) (Chi kering, 1996), essential graph (Andersson et al., 1997; Dash & Druzdzel, 1999). As a onsequen e of theorem 1, a ompleted PDAG possesses an ar x ! y if and only if a triplet of nodes (x; y; z) forms a v-stru ture or the ar x! y is required to be dire ted due to other v-stru tures (to avoid forming a new v-stru ture or reating a dire ted y le) (see Figure 1).\nNote that an arbitrary PDAG does not ne essarily represent some equivalen e lass of DAGs, although there is a one-to-one orresponden e between ompleted PDAGs and equivalen e lasses of DAGs. Nevertheless, ompleted PDAGs are onsiderably more ompli ated than general PDAGs. A hara terization of the spe i properties that a PDAG must verify in order to be a ompleted PDAG was obtained by Andersson et al. (1997):\n4. Note that the operators of addition and reversal of an ar in the DAG spa e also need a onsisten y\nhe k, but in this ase we simply test the absen e of dire ted y les.\nTheorem 2 (Andersson et al., 1997) A PDAG G is a ompleted PDAG if and only if it satis es the following onditions:\n1 G is a hain graph, i.e. it ontains no (partially) dire ted y les.\n2 The subgraph indu ed by every hain omponent\n5\nof G is hordal (i.e. on every undi-\nre ted y le of length greater than or equal to 4 there are two non- onse utive nodes\nonne ted by a link).\n3 The on guration x! y|z does not o ur as an indu ed subgraph of G.\n4 Every ar x ! y 2 G o urs in at least one of the four on gurations displayed in\nFigure 2 as an indu ed subgraph of G.\nLet us illustrate the advantages of sear hing in the spa e of equivalen e lasses of DAGs rather than the spa e of DAGs with a simple example. Figure 3 displays the set of possible DAGs involving three nodes fx; y; zg, with ar s between z and x, and between y and x. The rst three DAGs are equivalent. In terms of independen e information, they lead to the same independen e statement I(y; zjx) (y and z are onditionally independent given x), whereas the statement I(y; zj;) (y and z are marginally independent) orresponds to the fourth one. The four DAGs may be summarized by only two di erent ompleted PDAGs, shown in Figure 4.\nAs we an see, the sear h spa e may be redu ed by using PDAGs to represent the lasses: in our example, to two lasses instead of four on gurations; it an be seen (Andersson et al., 1997) that the ratio of the number of DAGs to the number of lasses is 25 = 11 for three nodes, 543 = 185 for four nodes and 29281 = 8792 for ve nodes; in more general terms, the results obtained by Gillispie and Perlman (2001) indi ate that this ratio approa hes a value of less than four as the number of nodes in reases. The use of equivalen e lasses therefore entails onvenient savings in exploration and evaluation e ort, although the gain is not spe ta ular.\nOn the other hand, the use of a anoni al representation s heme allows us to explore the spa e progressively and systemati ally, without losing any unexplored on guration unne essarily. Returning to our example, let us suppose that the true model is the DAG displayed in Figure 4.b and we start the sear h with an empty graph (with no ar s). Let us also assume that the sear h algorithm identi es that an edge between x and y produ es the greatest improvement in the s ore. At this moment, the two alternatives, x ! y and x y ( ase 1 and ase 2 in Figure 5, respe tively), are equivalent. Let us now suppose that we de ide to onne t the nodes x and z; again we have two options: x! z or x z. Nevertheless, depending on the previous sele ted on guration, we obtain di erent out omes that are no longer equivalent (see Figure 5).\nIf we had hosen ase 1 (thus obtaining either ase 1.1 or ase 1.2), we would have eliminated the possibility of exploring the DAG z ! x y, and therefore the exploring pro ess would have been trimmed. As the true model is pre isely this DAG ( ase 2.1 in Figure 5), then the sear h pro ess would have to in lude another ar onne ting y and z\n5. A hain omponent of G is any onne ted omponent of the undire ted graph obtained from G by\nremoving all the ar s.\n( ases 1.1.1, 1.2.1 or 1.2.2), be ause y and z are onditionally dependent given x. At this moment, any lo al sear h pro ess would stop (in a lo al optimum), be ause every lo al\nConsequently, our purpose onsists in adding or removing edges (either links or ar s) to the stru ture without pruning the sear h spa e unne essarily. We ould therefore introdu e links instead of ar s (when there is not enough information to distinguish between di erent patterns of ar s), whi h would serve as templates or dynami linkers to equivalen e patterns. They represent any valid ombination of ar s whi h results in a DAG belonging to the same equivalen e lass.\nLooking again at the previous example, we would pro eed as follows: assuming that in our sear h spa e the operators of link addition and reation of h-h patterns are available, we would rst in lude the link x|y; se ondly, when onsidering the in lusion of a onne tion between x and z, we would have two options, shown in Figure 4: the h-h pattern z ! x y and the pattern z|x|y. In this ase the s oring fun tion would assign the greatest s ore to the h-h pattern z ! x y, thus obtaining the orre t DAG."
    }, {
      "heading" : "3. Restri ted A y li Partially Dire ted Graphs",
      "text" : "The s heme of representation that we will use is slightly di erent from the formalism of\nompleted PDAGs. It is not ne essary for ea h on guration of our sear h spa e (whi h we all restri ted PDAG or RPDAG) to orrespond to a di erent equivalen e lass; two di erent RPDAGs may orrespond to the same equivalen e lass. The main reason for this is eÆ ien y: by allowing an equivalen e lass to be represented (only in some ases) by di erent RPDAGs, we will gain in eÆ ien y to explore the spa e. Before explaining this in greater detail, let us de ne the on ept of RPDAG:\nDe nition 1 (restri ted PDAG) A PDAG G is a restri ted PDAG (RPDAG) if and only if it satis es the following onditions:\n1 8y 2 U , Pa\nG\n(y) 6= ; ) Ne\nG\n(y) = ;.\n2 G does not ontain any dire ted y le.\n3 G does not ontain any ompletely undire ted y le, i.e. a y le ontaining only links.\n4 If x! y exists in G then either jPa\nG\n(y)j 2 or Pa\nG\n(x) 6= ;.\nThis ondition states that an ar x! y exists in G only if it is either part of an h-h pattern or there is another ar (originated by an h-h pattern) going to x.\nAs an RPDAG is a PDAG, it ould be onsidered to be a representation of a set of (equivalent) DAGs. We therefore must de ne whi h the set of DAGs is represented by a given RPDAG G, i.e. how dire tion may be given to the links in G in order to extend it to a DAG. The following de nition formalizes this idea.\nDe nition 2 (Extension of a PDAG) Given a PDAG G, we say that a DAG H is an extension of G if and only if:\n1 G and H have the same skeleton.\n2 If x! y is an ar in G then x! y is also an ar in H (no ar is redire ted).\n3 G and H have the same h-h patterns (i.e. the pro ess of dire ting the links in G in\norder to produ e H does not generate new h-h patterns).\nWe will use Ext(G) to denote the set of DAGs that are extensions of a given PDAG G.\nProposition 1 Let G be an RPDAG. Then:\n(a) Ext(G) 6= ; (G an be extended to obtain a DAG, i.e. the extension of an RPDAG is\nwell-de ned).\n(b) 8H;H\n0\n2 Ext(G) H ' H\n0\n(i.e. all the di erent DAGs that an be obtained from G\nby extending it are equivalent)."
    }, {
      "heading" : "Proof:",
      "text" : "(a) As G has no dire ted y le ( ondition 2 in De nition 1), then either G is already a DAG or it has some links. Let us onsider an arbitrary link x|y. Using ondition 1 in De nition 1, neither x nor y an have a parent. We an then dire t the link x|y in either dire tion without reating an h-h pattern. If we dire t the link x|y as x! y and y is part of another link y|z, then we dire t it as y ! z (in order to avoid a new h-h pattern). We\nan ontinue dire ting the links in a hain in this way, and this pro ess annot generate a dire ted y le be ause, a ording to ondition 3 in De nition 1, G has no ompletely undire ted y le. (b) The extension pro ess of G does not modify the skeleton and does not reate new h-h patterns. Therefore, all the extensions of G have the same skeleton and the same vstru tures (a v-stru ture is a parti ular ase of h-h pattern), hen e they are equivalent.\nIt should be noted that ondition 4 in De nition 1 is not ne essary to prove the results in Proposition 1. This ondition is in luded to ensure that the type of PDAG used to represent subsets of equivalent DAGs is as general as possible. In other words, ondition 4\nguarantees that an RPDAG is a representation of the greatest number of equivalent DAGs, subje t to the restri tions imposed by onditions 1-3 in De nition 1. As we will see in the next proposition, this is a hieved by dire ting the minimum number of edges. For example, z|x! y ! u would not be a valid RPDAG. The RPDAG that we would use in this ase is z|x|y|u.\nProposition 2 Let G be a PDAG verifying the onditions 1-3 in De nition 1. There is then a single RPDAG R su h that Ext(G) Ext(R)."
    }, {
      "heading" : "Proof:",
      "text" : "The proof is onstru tive. We shall build the RPDAG R as follows: the skeleton and the h-h patterns of R are the same as those in G. An ar x! y in G shall now be onsidered su h that Pa\nG\n(x) = ; and Pa\nG\n(y) = fxg (if su h an ar does not exist, then G itself would\nbe an RPDAG): we onvert the ar x! y into the link x|y. This pro ess is then repeated. Obviously, the PDAG R obtained in this way has no dire ted y le and veri es ondition 4 in De nition 1. Moreover, we annot obtain a on guration z ! x|y as a subgraph of R be ause Pa\nG\n(x) = ; (we only remove the dire tion of ar s whose initial nodes have no\nparent). In addition, R annot have any ompletely undire ted y le be ause either the ar x! y is not part of any y le in G or it is part of a y le in G that must ontain at least one h-h pattern (and the dire tions of the ar s in this pattern will never be removed). R is therefore an RPDAG.\nLet us now prove that Ext(G) Ext(R): if H 2 Ext(G) then H and G have the same skeleton and h-h patterns, hen e H and R also have the same skeleton and h-h patterns. Moreover, as all the ar s in R are also ar s in G, if x ! y 2 R then x ! y 2 G, whi h in turn implies that x! y 2 H. Therefore, a ording to De nition 2, H 2 Ext(G).\nFinally, let us prove the uniqueness of R: we already know that any other RPDAG R\n0\nverifying that Ext(G) Ext(R\n0\n) has the same skeleton and h-h patterns as R. A ording\nto ondition 1 in De nition 1, the edges that are not part of any of these h-h patterns but are in ident to the middle node y in any h-h pattern x ! y z must be dire ted away from y (in order to avoid new h-h patterns). The remaining edges that are not part of any h-h pattern must be undire ted, in order to satisfy ondition 4 in De nition 1. There is therefore only one RPDAG that mat hes a given skeleton and a set of h-h patterns, so R is the only RPDAG verifying that Ext(G) Ext(R). Figure 6 shows an example of the\nonstru tion pro ess.\nThe following proposition ensures that the on ept of RPDAG allows us to de ne a\npartition in the spa e of DAGs.\nProposition 3 Let G and G\n0\nbe two di erent RPDAGs. Then Ext(G) \\Ext(G\n0\n) = ;."
    }, {
      "heading" : "Proof:",
      "text" : "Let H be any DAG. Then H itself is a PDAG and obviously H = Ext(H). By applying the result in Proposition 2, we an assert that there is a single RPDAG G su h thatH Ext(G).\nIn the proposition below, we show the properties whi h are ommon to all the DAGs\nbelonging to the same extension of an RPDAG.\nProposition 4 Two DAGs belong to the extension of the same RPDAG if and only if they have the same skeleton and the same h-h patterns."
    }, {
      "heading" : "Proof:",
      "text" : "The ne essary ondition is obvious. Let us prove the suÆ ient ondition: let H and H 0 be two DAGs with ommon skeleton and h-h patterns. We shall onstru t a PDAG G as follows: the skeleton and the h-h patterns of G are the same as those in H and H 0 ; the edges that have the same orientation in H and H 0 are dire ted in G in the same way; the other edges in G remain undire ted. From De nition 2, it is lear that H;H 0 2 Ext(G).\nG has no dire ted y les be ause H and H\n0\nare DAGs. G has no ompletely undire ted\ny les, sin e all the y les in H and H\n0\nshare at least the h-h patterns. In addition,\nx! y|z annot be a subgraph ofG be ause this would imply the existen e of the subgraphs x! y z and x! y ! z in H and H 0 , respe tively, and therefore these two DAGs would not have the same h-h patterns.\nTherefore, the PDAG G satis es onditions 1-3 in De nition 1. By applying Proposition 2, we an then build a single RPDAG R su h that Ext(G) Ext(R), hen e H;H 0 2 Ext(R).\nA hara terization of the extension of an RPDAG that will be useful later is:\nProposition 5 Given an RPDAG G and a DAG H, then H is an extension of G if and only if the following onditions hold:\n1 G and H have the same skeleton.\n2 8y 2 U , if Pa\nG\n(y) 6= ; then Pa\nH\n(y) = Pa\nG\n(y).\n3 8y 2 U , if Pa\nG\n(y) = ; and Pa\nH\n(y) 6= ; then jPa\nH\n(y)j = 1."
    }, {
      "heading" : "Proof:",
      "text" : "Ne essary ondition:\n{ Pa\nG\n(y) 6= ;: Let x 2 Pa\nG\n(y), i.e., x ! y 2 G. Then, from ondition 2 in De nition 2,\nx! y 2 H, i.e., x 2 Pa\nH\n(y). Moreover, z 2 Pa\nG\n(y), x! y z is an h-h pattern in G.\nFrom ondition 3 in De nition 2, this o urs if and only if x! y z is an h-h pattern in H, whi h is equivalent to z 2 Pa\nH\n(y). Therefore, Pa\nH\n(y) = Pa\nG\n(y).\n{ Pa\nG\n(y) = ; and Pa\nH\n(y) 6= ;: Let x 2 Pa\nH\n(y). If there is another node z 2 Pa\nH\n(y),\nthen x! y z is an h-h pattern in H and therefore it is also an h-h pattern in G, whi h\nontradi ts the fa t that Pa\nG\n(y) = ;. So, y annot have more than one parent in H, hen e\njPa\nH\n(y)j = 1.\nSuÆ ient ondition:\n{ If x ! y 2 G then Pa\nG\n(y) 6= ;. From ondition 2 we have Pa\nH\n(y) = Pa\nG\n(y), hen e\nx! y 2 H. { If x! y z is an h-h pattern in G, on e again from ondition 2, Pa\nH\n(y) = Pa\nG\n(y) and\ntherefore x! y z is an h-h pattern in H. { If x ! y z is an h-h pattern in H, then jPa\nH\n(y)j 6= 1 and Pa\nH\n(y) 6= ;. So, from\nondition 3, we obtain Pa\nG\n(y) 6= ; and, from ondition 2, Pa\nG\n(y) = Pa\nH\n(y). Therefore\nx! y z is an h-h pattern in G."
    }, {
      "heading" : "3.1 Restri ted PDAGs and Completed PDAGs",
      "text" : "Let us now examine the main di eren es between the di erent representations: a representation based on PDAGs ensures that every equivalen e lass has a unique representation, but there are PDAGs that do not orrespond to any equivalen e lass (in other words, the mapping from equivalen e lasses to PDAGs is inje tive). On the other hand, our representation based on RPDAGs guarantees that every RPDAG orresponds to an equivalen e\nlass (proposition 1) but does not ensure that every equivalen e lass has a single representation (the mapping from equivalen e lasses to RPDAGs is onto). However, the mapping from equivalen e lasses to CPDAGs is bije tive. Figures 7.a, 7.b and 7. show the three RPDAGs orresponding to the same equivalen e lass; the asso iated ompleted PDAG is shown in Figure 7.d. In this example, the number of DAGs in the equivalen e lass is 12.\nAs we an see, the di eren e appears when there are triangular stru tures. If we ompare the de nition of RPDAG (De nition 1) with the hara terization of CPDAGs (Theorem 2),\nwe may observe that the essential di eren e is that a CPDAG may have ompletely undire ted y les, but these y les must be hordal. In RPDAGs, undire ted y les are therefore forbidden, whereas in CPDAGs undire ted non- hordal y les are forbidden.\nIt should be noted that we ould also de ne RPDAGs by repla ing ondition 3 in Definition 1 for its equivalent: The subgraph indu ed by every hain omponent of G is a tree (whi h is a spe i type of hordal graph). In this way, the similarities and di eren es between CPDAGs and RPDAGs are even learer. Any of the RPDAGs in the same equivalen e lass is obtained from the orresponding CPDAG by removing some of the links ( onverting them into ar s) in order to obtain a tree stru ture.\nExamining the problem from another perspe tive, from Theorem 1 and Proposition 4 we an see that the role played by the v-stru tures in CPDAGs is the same as that played by the h-h patterns in RPDAGs.\nIt is also interesting to note that the number of DAGs whi h are extensions of a given RPDAG, G, an be al ulated very easily: the subgraph indu ed by ea h hain omponent of G is a tree, and this tree an be dire ted in di erent ways by sele ting any of the nodes as the root node. Moreover, we an pro eed independently within ea h hain omponent. The number of DAGs in Ext(G) is therefore equal to the produ t of the number of nodes within ea h hain omponent of G. Regarding the number of RPDAGs that represent the same equivalen e lass, this number grows exponentially with the size of the undire ted liques in the CPDAG. For example, if the subgraph indu ed by a hain omponent in a CPDAG\nonsists of a omplete subgraph of m nodes, then the number of RPDAG representations is\nm!\n2\n. This obviously does not mean that a sear h method based on RPDAGs must explore\nall these equivalent representations.\nOur reason for using RPDAGs is almost ex lusively pra ti al. In fa t, RPDAGs do not have a lear semanti s (they are a somewhat hybrid reature, between DAGs and ompleted PDAGs). We an only say that RPDAGs would orrespond to sets of equivalent DAGs whi h share all the ausal patterns where an e e t node has at least two auses (and only the ausal patterns where a single ause provokes an e e t are not determined). This is not problemati when we are performing model sele tion but it be omes riti al if we are doing model averaging: without a semanti understanding of the lass of RPDAGs, it will be quite diÆ ult to assign a prior to them.\nOur intention is to trade the uniqueness of the representation of equivalen e lasses of DAGs (CPDAGs) for a more manageable one (RPDAGs): testing whether a given PDAG G is an RPDAG is easier than testing whether G is a ompleted PDAG. In the rst ase, the onsisten y he k involves testing for the absen e of dire ted and ompletely undire ted\ny les (the omplexity of these tests and those ne essary to verify whether a dire ted graph is a DAG is exa tly the same), whereas in the se ond ase, in addition to testing for the absen e of dire ted and partially dire ted y les, we also need to perform hordality tests and he k that ea h ar is part of one of the indu ed subgraphs displayed in Figure 2. The pri e we have to pay for using RPDAGs is that we may o asionally need to evaluate an equivalen e lass more than on e. In the next se tion, we will examine how a lo al sear h method whi h uses RPDAGs an also take advantage of the de omposability property of a s oring fun tion in order to eÆ iently evaluate neighboring stru tures."
    }, {
      "heading" : "4. The Sear h Method",
      "text" : "We will use a lo al method to explore the sear h spa e of RPDAGs. The starting point of the sear h pro ess will be an empty RPDAG ( orresponding to an empty DAG). Nevertheless, we ould start from another on guration if we have some prior knowledge about the presen e or absen e of some edges or v-stru tures. We must then de ne the operators to move from one on guration to another neighboring on guration."
    }, {
      "heading" : "4.1 Overview",
      "text" : "Our basi operators are the in lusion of an edge between a pair of non-adja ent nodes and the removal of an existing edge between a pair of adja ent nodes in the urrent on guration. These edges may be either dire ted or undire ted.\nThe in lusion of an isolated link x|y will serve as a template for the ar s x ! y and x y; however, the link x|y together with another link x|z represent any ombination of ar s ex ept those that reate new h-h patterns (the DAGs (a), (b) and ( ) in Figure 3). In the\nase of adding an ar , we may obtain several di erent neighboring on gurations, depending on the topology of the urrent RPDAG and the dire tion of the ar being in luded. As we will see, if we are testing the in lusion of an edge between two nodes x and y, this may involve testing some of the di erent valid on gurations obtained by the in lusion of the link x|y, the ar x ! y, the ar x y, the h-h pattern x ! y z or the h-h pattern z ! x y (where z in the last two ases would be any node su h that either the link y|z or the link z|x exists in the urrent on guration). However, the removal of an edge will always result in only one neighboring on guration. Other operators, su h as ar reversal (Chi kering, 1996), will not be used by our sear h method. The set of neighboring\non gurations of a given RPDAG G will therefore be the set of all the di erent RPDAGs\nobtained from G by adding or deleting a single edge (either dire ted or undire ted).\nBefore explaining the details of the sear h method, let us illustrate the main ideas by means of the following example: onsider the RPDAG in Figure 8, whi h represents the\nIn this situation, we annot introdu e the link x|y be ause we would violate one of the onditions de ning RPDAGs ( ondition 1). We may introdu e the ar x ! y and in this ase, again in order to preserve ondition 1, the two neighbors of y must be onverted\ninto hildren. We an also in lude the ar x y. Finally, we may in lude two di erent h-h patterns x! y z, where z is a neighbor of y (the other neighbor must be onverted into a hild, on e again in order to preserve ondition 1). These four di erent on gurations are displayed in Figure 9."
    }, {
      "heading" : "4.2 Neighboring Con gurations",
      "text" : "In order to design a systemati way to determine whi h neighboring RPDAGs arise from the in lusion or the removal of an edge in an RPDAG, it is suÆ ient to onsider some lo al parameters of the two nodes to be onne ted. First, some additional notation is introdu ed. If j j represents the ardinality of a set, given a node x in a PDAG G, we de ne:\np\nG\n(x) = jPa\nG\n(x)j,\nG\n(x) = jCh\nG\n(x)j\nn\nG\n(x) = jNe\nG\n(x)j, a\nG\n(x) = jAd\nG\n(x)j\nObserve that for any RPDAG G, the following two properties hold:\np\nG\n(x) +\nG\n(x) + n\nG\n(x) = a\nG\n(x)\nif p\nG\n(x) 6= 0) n\nG\n(x) = 0 (hen e p\nG\n(x) +\nG\n(x) = a\nG\n(x))"
    }, {
      "heading" : "4.2.1 Adding Edges",
      "text" : "The number and type of neighboring on gurations that an be obtained from the in lusion in the urrent RPDAG of an edge between x and y an be determined from the parameters above. The resultant asuistry may be redu ed to seven states, whi h we have labeled\nfrom A to G. In order to fa ilitate its des ription, we shall use the de ision tree shown in Figure 10 6 .\n6. This tree ould be organized di erently in order to improve the eÆ ien y in the lo ation of the urrent\nstate. However, this parti ular tree was sele ted so as to larify the presentation.\nIn this tree, the lower box of ea h non-terminal vertex ontains a test (about the number of parents or the number of neighbors of nodes x and y). The lower box of ea h terminal vertex ontains the label of the state resulting from following the path from the root to that terminal vertex. The des ription of ea h state (i.e. the di erent neighboring on gurations that an be obtained in this ase) an be found in Table 1. The upper boxes of all the verti es in the tree show the restri tions imposed on ea h intermediate or terminal state. For example, state B orresponds to a situation where both nodes x and y do not have neighbors and at least one of them has some parent. Although the tree has seven di erent states, there are only ve truly di erent states, sin e states D and E, and states F and G are symmetri al.\nIn Table 1, ea h row orresponds to a state: the rst olumn ontains the labels of the states; the se ond olumn displays the total number of neighboring on gurations that an be obtained for ea h state; the third olumn shows the di erent types of edges that, for ea h state, an be added to the urrent on guration; olumns four, ve and six will be dis ussed later.\nUsing the example in Figure 8, we shall explain the use of the de ision tree as well as the instantiation of the information in Table 1. Following the de ision tree, at level 1 (the root vertex), the test is false sin e y has two neighbors. At level 2, the test is also false as x has one parent. At level 3 the test is true, sin e x has no neighbor. At level 4 we rea h\na terminal vertex. Our urrent on guration therefore orresponds to state F. Then, by examining state F in Table 1, we an on rm that we rea h four di erent on gurations (n\nG\n(y) = 2): G[fx yg, G[fx! yg without new h-h patterns, and two G[fx! y zg whi h produ e new h-h patterns. So, these are the only stru tures that our algorithm must evaluate when onsidering the in lusion of the andidate edge x|y. In Figure 14, we show an example for ea h of the ve non-symmetri al states (on e again, these examples only display the part of the RPDAGs orresponding to the neighborhood of the nodes x and y).\nWe therefore have a systemati way to explore all the neighboring on gurations whi h result from adding an edge. However, it will sometimes be ne essary to perform some additional steps sin e the on gurations obtained must be RPDAGs:\nFirst, we must maintain the ondition 1 (p\nG\n(y) 6= 0 ) n\nG\n(y) = 0). It is therefore\nne essary to omplete the on guration for some of the des ribed states, i.e. some of the links must be onverted into ar s. The ompleting pro ess onsists in ring an orientation in as ade, starting from the links y|t su h that the ar just introdu ed is x ! y. Let us onsider the situation in Figure 11, where we want to onne t the nodes x and y, a ase orresponding to state D. Among the three possible neighboring\non gurations, let us suppose that we are testing the one whi h introdu es the h-h pattern x ! y z. The RPDAG obtained from the ompleting pro ess is also displayed in Figure 11. The sixth olumn in Table 1 shows whi h states and neighboring\non gurations may require the ompleting pro ess.\nSe ondly, it is possible that some of the neighboring on gurations must be reje ted, as they give rise to dire ted or ompletely undire ted y les ( onditions 2 and 3 de ning RPDAGs). For example, let us onsider the situation displayed in Figure 12, whi h\norresponds to state F. In this ase, the on guration obtained after in luding the ar x ! y and ompleting would generate a dire ted y le. This on guration must therefore be reje ted. The olumns four and ve in Table 1 show whi h states and\non gurations may require a dete tion of dire ted or ompletely undire ted y les,\nrespe tively."
    }, {
      "heading" : "4.2.2 Deleting Edges",
      "text" : "The other basi operator, the removal of an edge (either link or ar ) is mu h simpler than the addition of an edge, sin e only one neighboring on guration is obtained when we delete an edge. Moreover, it is not ne essary to perform any test for dete ting dire ted or undire ted y les. However, in this ase we need to preserve ondition 4 in the de nition of RPDAG (if x ! y exists in G ) jPa\nG\n(y)j 2 or Pa\nG\n(x) 6= ;), that ould be violated\nafter an ar is deleted. This situation may appear only when we are going to remove an ar x ! y and either Pa\nG\n(y) = fxg or Pa\nG\n(y) = fx; ug: in the rst ase, all the hildren\nof y that do not have other parents than y must be onverted into neighbors of y, and this pro ess is repeated starting from ea h of these hildren; in the se ond ase, if Pa\nG\n(u) = ;\nthen in addition to the previous pro ess, the ar u ! y must be onverted into the link u|y. Figure 13 illustrates these situations. This pro edure of transforming ar s into links is exa tly the same as the one des ribed in the proof of Proposition 2."
    }, {
      "heading" : "4.3 The Operators",
      "text" : "Although the previous des ription of the operators that de ne the neighborhood of an RPDAG is quite onvenient from a pra ti al (implementation) point of view, for the sake of larity, we shall des ribe them in another way. In fa t, we shall use ve operators:\nA ar (x; y), addition of an ar x! y.\nx y x yA\nA link(x; y), addition of a link x|y.\nD ar (x; y), deletion of an ar x! y.\nD link(x; y), deletion of a link x|y.\nA hh(x; y; z), addition of an ar x! y and reation of the h-h pattern x! y z by transforming the link y|z into the ar y z.\nThe onditions that the urrent RPDAG G must verify so that ea h of these operators an be applied in order to obtain a valid neighboring RPDAG are shown in Table 2. These onditions an be easily derived from the information in Figure 10 and Table 1. In Table 2, UC(x|y) represents a test for dete ting ompletely undire ted y les after inserting the link x|y in the urrent RPDAG. Note that we an perform this test very easily without a tually inserting the link: it is only ne essary to he k the existen e of a path between x and y ex lusively formed by the links. Similarly, DC(x! y) and DC(x! y z) represent tests for dete ting dire ted y les after inserting the ar x! y and the h-h pattern x! y z in the urrent RPDAG, respe tively (and perhaps ompleting). It should also be noted that we an perform these tests without inserting the ar or the h-h pattern: in this ase we only need to he k the existen e of a path from y to x ontaining only either links or ar s dire ted away from y (a partially dire ted path from y to x). Table 2 also shows whi h operators may require a post-pro essing step in order to ensure that the orresponding neighboring on guration of G is an RPDAG. In Table 2, Complete(y) and Undo(y) refer to the pro edures that preserve onditions 1 and 4 in De nition 1, respe tively. Note that both UC(x|y) and Complete(y) take time O(l\ny\n) in the worst ase, where l\ny\nis the number\nof links in the subgraph indu ed by the hain omponent of G that ontains y; Undo(y) takes time O(d\ny\n) in the worst ase, where d\ny\nis the number of ar s in the subgraph indu ed by the\nset of des endants of y in G that only have one parent; DC(x ! y) and DC(x ! y z) both take time O(dl\ny\n) in the worst ase, where dl\ny\nis the number of edges (either ar s or\nlinks) in the subgraph indu ed by the nodes in the hain omponent of G that ontains y together with their des endants."
    }, {
      "heading" : "5. The Exploring Pro ess and the Evaluation of Candidate Stru tures",
      "text" : "The sear h method we have des ribed may be applied in ombination with any s ore equivalent fun tion g (for example the AIC, BIC, MDL and BDe s oring fun tions are s ore equivalent). An easy (but ineÆ ient) way to integrate our sear h method with a s ore equivalent fun tion would be as follows: given an RPDAG G to be evaluated, sele t any extension H of G and ompute g(H : D). We ould also use other (non-equivalent) s oring fun tions, although the s ore of G would depend on the sele ted extension.\nHowever, let us onsider the ase of a de omposable s oring fun tion g: the DAG obtained by adding or removing an ar from the urrent DAG H an be evaluated by modifying only one lo al s ore:\ng(H [ fx! yg : D) = g(H : D) g\nD\n(y; Pa\nH\n(y)) + g\nD\n(y; Pa\nH\n(y) [ fxg)\ng(H n fx! yg : D) = g(H : D) g\nD\n(y; Pa\nH\n(y)) + g\nD\n(y; Pa\nH\n(y) n fxg)\nUsing de omposable s oring fun tions, the pro ess of sele ting, given an RPDAG, a representative DAG and then evaluating it may be quite ineÆ ient, sin e we would have to re ompute the lo al s ores for all the nodes instead of only one lo al s ore. This fa t\nan make a learning algorithm that sear hes in the spa e of equivalen e lasses of DAGs onsiderably slower than an algorithm that sear hes in the spa e of DAGs (this is the ase\nof the algorithm proposed by Chi kering, 1996).\nOur sear h method an be used for de omposable s oring fun tions so that: (1) it is not ne essary to transform the RPDAG into a DAG, the RPDAG an be evaluated dire tly, and (2) the s ore of any neighboring RPDAG an be obtained by omputing at most two lo al s ores. All the advantages of the sear h methods on the spa e of DAGs are therefore retained, but a more redu ed and robust sear h spa e is used.\nBefore these assertions are proved, let us examine an example. Consider the RPDAG G in Figure 15 and the three neighboring on gurations produ ed by the in lusion of an edge between x and y, G\n1\n, G\n2\nand G\n3\n(also displayed in Figure 15).\nThe s ore of ea h of these RPDAGs is equal to the s ore of any of their extensions.\nFigure 16 displays one extension for ea h neighboring on guration.\na\nb\nc\ny\nd\nx\na\nb\nc\ny\nd\nx\na\nb\nc\ny\nd\nx\nH H H1 2 3\n1\nD\nD\ng(G\n2\n: D) = g(G : D) g\nD\n(y; d) + g\nD\n(y; fxdg)\ng(G\n3\n: D) = g(G : D) g\nD\n(y; ) + g\nD\n(y; fx g)\nTherefore, the s ore of any neighboring on guration may be obtained from the s ore of G by omputing only two lo al s ores. Note that some of these lo al s ores may have already been omputed at previous iterations of the sear h pro ess: for example, g\nD\n(y; ;)\nhad to be used to s ore the initial empty RPDAG, and either g\nD\n(y; d) or g\nD\n(y; ) ould\nhave been omputed when the link y|d or y| was inserted into the stru ture.\nProposition 6 Let G be an RPDAG and G\n0\nbe any RPDAG obtained by applying one\nof the operators des ribed in Table 2 to G. Let g be a s ore equivalent and de omposable fun tion.\n(a) If the operator is A link(x; y) then\ng(G\n0\n: D) = g(G : D) g\nD\n(y; ;) + g\nD\n(y; fxg)\n(b) If the operator is A ar (x; y) then\ng(G\n0\n: D) = g(G : D) g\nD\n(y; Pa\nG\n(y)) + g\nD\n(y; Pa\nG\n(y) [ fxg)\n( ) If the operator is A hh(x; y; z) then\ng(G\n0\n: D) = g(G : D) g\nD\n(y; fzg) + g\nD\n(y; fx; zg)\n(d) If the operator is D link(x; y) then\ng(G\n0\n: D) = g(G : D) g\nD\n(y; fxg) + g\nD\n(y; ;)\n(e) If the operator is D ar (x; y) then\ng(G\n0\n: D) = g(G : D) g\nD\n(y; Pa\nG\n(y)) + g\nD\n(y; Pa\nG\n(y) n fxg)"
    }, {
      "heading" : "Proof:",
      "text" : "(1) First, we shall prove that we an onstru t an extension H\n0\nof G\n0\nand another extension\nH of G, su h that H and H\n0\ndi er in only one ar (this ar being x! y).\nConsider the ases (a), (b), and ( ), whi h orrespond to the addition of an edge between\nx and y: in ase (a), G\n0\n= G[fx|yg and let H\n0\nbe an extension of G\n0\nthat ontains the ar\nx! y; in ase (b), where G\n0\n= G[fx! yg, and in ase ( ), where G\n0\n= (Gnfy|zg)[fx!\ny zg, let H\n0\nbe any extension of G\n0\n(whi h will ontain the ar x! y). In all three ases,\nlet H = H\n0\nn fx! yg. We shall prove that H is an extension of G:\n{ First, it is obvious that G and H have the same skeleton. { Se ondly, if u! v 2 G (in either ase u! v 6= x! y), then u! v 2 G 0 . As H 0 is an\nextension of G\n0\n, then u! v 2 H\n0\n, and this implies that u! v 2 H. Therefore, all the ar s\nin G are also ar s in H. This result also ensures that every h-h pattern in G is also an h-h pattern in H.\n{ Thirdly, if u ! v w is an h-h pattern in H (in either ase u ! v w 6= x !\ny w), then u ! v w 2 H\n0\n. On e again, as H\n0\nis an extension of G\n0\n, we an see that\nu! v w 2 G\n0\n, and then u! v w 2 G. So, G and H have the same h-h patterns.\nH is therefore an extension of G, a ording to De nition 2. Note that 8u 6= y Pa\nH\n(u) =\nPa\nH\n0\n(u) and Pa\nH\n(y) = Pa\nH\n0\n(y) n fxg.\nLet us now onsider ases (d) and (e), whi h orrespond to the deletion of an edge between x and y (either a link or an ar , respe tively): in ase (d), let H be an extension of G ontaining the ar x! y; in ase (e), let H be any extension of G. In both ases, let H 0 = H n fx! yg. We will prove that H 0 is an extension of G 0 :\n{ First, it is lear that G\n0\nand H\n0\nhave the same skeleton.\n{ Se ondly, if u ! v 2 G\n0\n(note that u ! v 6= x ! y), then u ! v 2 G. As H is an\nextension of G, then u! v 2 H, and therefore u! v 2 H\n0\n. So, all the ar s in G\n0\nare also\nar s in H\n0\n. Moreover, every h-h pattern in G\n0\nis also an h-h pattern in H\n0\n.\n{ Thirdly, if u ! v w is an h-h pattern in H\n0\n(and we know that u ! v w 6=\nx ! y w), then u ! v w 2 H. As H is an extension of G, then u ! v w 2 G. Therefore, u! v w 2 G 0 (the removal of the ar x! y annot destroy any h-h pattern where x! y is not involved). So, G 0 and H 0 have the same h-h patterns.\nIn this way, H\n0\nis an extension of G\n0\n. Moreover, we an see that 8u 6= y Pa\nH\n0\n(u) =\nPa\nH\n(u) and Pa\nH\n0\n(y) = Pa\nH\n(y) n fxg.\n(2) The s ores of G and G\n0\nare the same as the s ores of H and H\n0\nrespe tively, sin e g is\ns ore equivalent. Moreover, as g is de omposable, we an write\ng(G\n0\n: D) = g(H\n0\n: D) =\nP\nu\ng\nD\n(u; Pa\nH\n0\n(u)) =\nP\nu6=y\ng\nD\n(u; Pa\nH\n0\n(u)) + g\nD\n(y; Pa\nH\n0\n(y)) =\nP\nu6=y\ng\nD\n(u; Pa\nH\n(u)) + g\nD\n(y; Pa\nH\n(y)) g\nD\n(y; Pa\nH\n(y)) + g\nD\n(y; Pa\nH\n0\n(y)) =\nP\nu\ng\nD\n(u; Pa\nH\n(u)) g\nD\n(y; Pa\nH\n(y)) + g\nD\n(y; Pa\nH\n0\n(y)) =\ng(H : D) g\nD\n(y; Pa\nH\n(y)) + g\nD\n(y; Pa\nH\n0\n(y)) =\ng(G : D) g\nD\n(y; Pa\nH\n(y)) + g\nD\n(y; Pa\nH\n0\n(y))\n(4)\nLet us now onsider the ve di erent ases:\n(a) In this ase, we know from Table 2 that Pa\nG\n(y) = ;. Moreover, Pa\nG\n0\n(y) = ; (be ause\nwe are inserting a link) and Pa\nH\n0\n(y) 6= ; (be ause H\n0\nis an extension of G\n0\nthat ontains\nthe ar x ! y). Then, from Proposition 5 we obtain jPa\nH\n0\n(y)j = 1, i.e. Pa\nH\n0\n(y) = fxg.\nMoreover, Pa\nH\n(y) = Pa\nH\n0\n(y) n fxg = ;. So, Eq. (4) be omes\ng(G\n0\n: D) = g(G : D) g\nD\n(y; ;) + g\nD\n(y; fxg)\n(b) From Table 2 we get Pa\nG\n(y) 6= ; or Pa\nG\n(x) 6= ;.\nIf Pa\nG\n(y) 6= ;, from Proposition 5 we obtain Pa\nH\n(y) = Pa\nG\n(y). Moreover, Pa\nH\n0\n(y) =\nPa\nH\n(y) [ fxg = Pa\nG\n(y) [ fxg.\nIf Pa\nG\n(y) = ; then Pa\nG\n0\n(y) = fxg (be ause we are adding the ar x ! y). From\nProposition 5 we obtain Pa\nH\n0\n(y) = Pa\nG\n0\n(y) = fxg = Pa\nG\n(y) [ fxg. Moreover, Pa\nH\n(y) =\nPa\nH\n0\n(y) n fxg = ; = Pa\nG\n(y).\nIn either ase, Eq. (4) be omes\ng(G\n0\n: D) = g(G : D) g\nD\n(y; Pa\nG\n(y)) + g\nD\n(y; Pa\nG\n(y) [ fxg)\n( ) In this ase, Pa\nG\n(y) = ; and Pa\nG\n0\n(y) = fx; zg. From Proposition 5 we obtain Pa\nH\n0\n(y) =\nfx; zg. Moreover, Pa\nH\n(y) = Pa\nH\n0\n(y) n fxg = fzg. Then, Eq. (4) be omes\ng(G\n0\n: D) = g(G : D) g\nD\n(y; fzg) + g\nD\n(y; fx; zg)\n(d) As Pa\nG\n(y) = ; and H is an extension of G ontaining the ar x! y, from Proposition 5\nwe get Pa\nH\n(y) = fxg. Moreover, Pa\nH\n0\n(y) = Pa\nH\n(y)nfxg = ;. In this ase Eq. (4) be omes\ng(G\n0\n: D) = g(G : D) g\nD\n(y; fxg) + g\nD\n(y; ;)\n(e) In this ase, as Pa\nG\n(y) 6= ;, Proposition 5 asserts that Pa\nH\n(y) = Pa\nG\n(y). Moreover,\nPa\nH\n0\n(y) = Pa\nH\n(y) n fxg = Pa\nG\n(y) n fxg. Therefore, Eq. (4) be omes\ng(G\n0\n: D) = g(G : D) g\nD\n(y; Pa\nG\n(y)) + g\nD\n(y; Pa\nG\n(y) n fxg)"
    }, {
      "heading" : "5.1 Comparison with Other Approa hes",
      "text" : "As we have already mentioned, there are several works devoted to learning Bayesian networks, within the s ore+sear h approa h, whi h use the spa e of ompleted PDAGs to arry out the sear h pro ess. There is a slight di eren e between the operators onsidered in the di erent works: the addition and deletion of edges is onsidered by Madigan et al. (1996), within a Markov Chain Monte Carlo pro ess, whi h also performs Monte Carlo sampling from the spa e of the orderings of the variables ompatible with the urrent CPDAG. Edge addition and deletion is also used by Spirtes and Meek (1995), but within a greedy pro ess that rst grows the stru ture by adding edges and then thins it by deleting edges. Additional operators are onsidered by Chi kering (1996), in luding ar reversal and reation of v-stru tures.\nAll these methods move through the spa e of ompleted PDAGs in the following way: given the urrent CPDAG G, after sele ting an operator, applying it to G and obtaining a neighboring PDAG G 0 , they generate a onsistent extension H 0 of G 0 (a DAG belonging to the equivalen e lass represented by the PDAG), if one exists. If this is the ase (otherwise G 0 is not a valid on guration), thenG 0 is evaluated by omputing the s ore ofH 0 , g(H 0 : D). The ompleted PDAG representation of G 0 is then re overed from its onsistent extension H 0 .\nThe pro ess of he king the existen e of a onsistent extension and generating it is arried out with a pro edure alled PDAG-to-DAG (Dor & Tarsi, 1992), whi h runs in time O(n e) in the worst ase, where e denotes the number of edges in the PDAG. Another pro edure,\nalled DAG-to-PDAG, is invoked in order to obtain the ompleted PDAG representation of the new valid on guration. There are di erent implementations of DAG-to-PDAG (Andersson et al., 1997; Chi kering, 1995; Meek, 1995; Pearl & Verma, 1990). For example, the time omplexity of the algorithm proposed by Chi kering (1995) is O(e) on the average and O(n e) in the worst ase.\nOur sear h method does not need to use any of these two pro edures: in order to he k the validity of a neighboring on guration of an RPDAG G, it is only ne essary, in some\nases, to perform a test to dete t either an undire ted path or a partially dire ted path between two nodes in G (implemented by pro edures UC() and DC() in Se tion 4.3). On the other hand, on e the sear h pro ess has explored the neighborhood of G and determined the best neighboring on guration G 0 , G 0 is not always an RPDAG, and we must generate its RPDAG representation. This generation pro edure is also very simple: it onsists in\nring, starting from a single node y, a as aded pro ess that either dire ts links away from y or undire ts ar s (implemented by pro edures Complete() and Undo() in Se tion 4.3). Note that all these pro edures used by our sear h method are less time- onsuming than PDAG-to-DAG and DAG-to-PDAG.\nMore importantly, our sear h method an take advantage of the de omposability of many s oring fun tions, and ea h RPDAG (ex ept the initial one) an be evaluated by\nomputing only two lo al s ores. However, the methods based on ompleted PDAGs need to re ompute all the lo al s ores, although the algorithm proposed by Muntenau and Cau (2000), whi h operates on ompleted PDAGs and uses three insertion operators (for ar s, links and v-stru tures) is also able to s ore any neighboring on guration using two lo al s ores; however, the validity onditions of some of these operators are not orre t.\nFinally, Chi kering (2002)\n7\ndes ribes an algorithm that sear hes in the spa e of om-\npleted PDAGs and is also able to evaluate on gurations by omputing only (up to four) lo al s ores. It uses six operators, link and ar addition, link and ar deletion, reation of v-stru tures by dire ting two already existing links, and reversal of ar s. All the operators\nan be evaluated using two lo al s ores, ex ept reversal and reation of v-stru tures, that require four lo al s ores. The validity onditions of the operators are established essentially in terms of two onditions: (1) the absen e of semi-dire ted or undire ted paths between two nodes that do not pass through ertain set of nodes, S, and (2) the fa t that a ertain set of nodes forms a lique. Link insertion and reation of v-stru tures need the rst type of\nondition, link and ar deletion need the se ond one, whereas ar insertion and ar reversal require both onditions. The \\path\" validity onditions take time O(jSj + e) in the worst\nase, and the \\ lique\" onditions take time O(jSj\n2\n), also in the worst ase. This algorithm\nalso requires the PDAG-to-DAG and DAG-to-PDAG pro edures to be used.\nSo, although the validity onditions of the operators in Chi kering's algorithm and their postpro essing are somewhat more omplex than ours, the advantage is that this algorithm does not have any dupli ate representations of the equivalen e lasses. Whether the omputational ost of moves in the CPDAG spa e an ompensate for the larger number of RPDAGs (and the larger number of lo al s ores to be omputed) is a matter of empiri al evaluation, that will possibly depend on the \\sparseness\" of the spe i domain problem\nonsidered."
    }, {
      "heading" : "6. Experimental Results",
      "text" : "In this se tion we shall des ribe the experiments arried out with our algorithm, the obtained results, and a omparative study with other algorithms for learning Bayesian networks. We have sele ted nine di erent problems to test our algorithm, all of whi h only ontain dis rete variables: Alarm (Figure 18), Insuran e (Figure 19), Hail nder (Figure 20), Breast-Can er,\nrx, Flare2, House-Votes, Mushroom, and Nursery.\n7. This work appeared after the original submission of this paper.\nThe Alarm network displays the relevant variables and relationships for the Alarm Monitoring System (Beinli h et al., 1989), a diagnosti appli ation for patient monitoring. This network, whi h ontains 37 variables and 46 ar s, has been onsidered as a ben hmark for evaluating Bayesian network learning algorithms. The input data ommonly used are subsets of the Alarm database built by Herskovits (1991), whi h ontains 20000 ases that were sto hasti ally generated using the Alarm network. In our experiments, we have used three databases of di erent sizes (the rst k ases in the Alarm database, for k = 3000; 5000 and 10000).\nInsuran e (Binder et al., 1997) is a network for evaluating ar insuran e risks. The Insuran e network ontains 27 variables and 52 ar s. In our experiments, we have used ve databases ontaining 10000 ases, generated from the Insuran e Bayesian network.\nHail nder (Abramson et al., 1996) is a normative system that fore asts severe summer hail in northeastern Colorado. The Hail nder network ontains 56 variables and 66 ar s. In this ase, we have also used ve databases with 10000 ases generated from the Hail nder network.\nBreast-Can er, rx, Flare2, House-Votes, Mushroom, and Nursery are databases available from the UCI Ma hine Learning Repository. Breast-Can er ontains 10 variables (9 attributes, two of whi h have missing values, and a binary lass variable) and 286 instan es. The rx database on erns redit ard appli ations. It has 490 ases and 16 variables (15 attributes and a lass variable), and seven variables have missing values. Moreover, six of the variables in the rx database are ontinuous and were dis retized using the MLC++ system (Kohavi, John, Long, Manley & P eger, 1994). Flare2 uses 13 variables (10 attributes and 3 lass variables, one for the number of times a ertain type of solar are o ured in a 24-hour period) and ontains 1066 instan es, without missing values. House-Votes stores the votes for ea h of the U.S. House of Representatives Congressmen on 16 key votes; it has 17 variables and 435 re ords and all the variables ex ept two have missing values. Mushroom ontains 8124 ases orresponding to spe ies of gilled mushrooms in the Agari us and Lepiota Family; there are 23 variables (a lass variable, stating whether the mushroom is edible or poisonous, and 22 attribute variables) and only one variable has missing values."
    }, {
      "heading" : "SocioEcon",
      "text" : "Nursery ontains data relative to the evaluation of appli ations for nursery s hools, and has 9 variables and 12960 ases, without missing values. In all of the ases, missing values are not dis arded but treated as a distin t state.\nIn the rst series of experiments, we aim to ompare the behavior of our RPDAG-based lo al sear h method (rpdag) with the lassi al lo al sear h in the spa e of DAGs (dag). The s oring fun tion sele ted is BDeu (He kerman et al., 1995) (whi h is s ore equivalent and de omposable), with the parameter representing the equivalent sample size set to 1 and a uniform stru ture prior. The starting point of the sear h is the empty graph in both\nases.\nWe have olle ted the following information about the experiments:\nBDeu.- The BDeu s ore (log version) of the learned network.\nEdg.- The number of edges in luded in the learned network.\nH.- The Hamming distan e, H=A+D+I, i.e. the number of di erent edges, added (A),\ndeleted (D), or wrongly oriented (without taking into a ount the di eren es between equivalent stru tures) (I), in the learned network with respe t to the gold standard network (the original model). This measure is only omputed for the three test domains where a gold standard exists.\nIter.- The number of iterations arried out by the algorithm to rea h the best network, i.e.\nthe number of operators used to transform the initial graph into a lo al optimum.\nInd.- The number of individuals (either DAGs or RPDAGs) evaluated by the algorithm.\nEstEv.- The number of di erent statisti s evaluated during the exe ution of the algorithm.\nThis is a useful value to measure the eÆ ien y of the algorithms, be ause most of\nthe running time of a s oring-based learning algorithm is spent in the evaluation of statisti s from the database.\nTEst.- The total number of statisti s used by the algorithm. Note that this number an\nbe onsiderably greater than EstEv. By using hashing te hniques we an store and eÆ iently retrieve any previously al ulated statisti s. It is not therefore ne essary to re ompute them by a essing the database, thus gaining in eÆ ien y.\nNVars.- The average number of variables that intervene in the di erent statisti s (i.e. the\nvalues N\ny;Pa\nH\n(y)\nin Eq. (3)) omputed. This value is also important be ause the time\nrequired to ompute a statisti in reases exponentially with the number of variables involved.\nTime.- The time, measured in se onds, employed by the algorithm to learn the network.\nOur implementation is written in the JAVA programming language and runs under Linux. This value is only a rough measure of the eÆ ien y of the algorithms, be-\nause there are many ir umstan es that may in uen e the running time (external loading in a networked omputer, a hing or any other aspe t of the omputer ar-\nhite ture, memory paging, use of virtual memory, threading, di erent ode, et .). Nevertheless, we have tried to ensure that the two algorithms run under the same\nonditions as far as possible, and the two implementations share most of the ode. In fa t, the two algorithms have been integrated into the Elvira pa kage (available at http://www.leo.ugr.es/~elvira).\nFor the Insuran e and Hail nder domains, the reported results are the average values a ross the ve databases onsidered. The results of our experiments for syntheti data, i.e. Alarm, Insuran e and Hail nder, are displayed in Tables 3, 4 and 5, respe tively, where we also show the BDeu values for the true (T\nD\n) and the empty (;\nD\n) networks (with parameters\nre-trained from the orresponding database D), whi h may serve as a kind of s ale. The results obtained for real data are displayed in Table 6.\nAs we onsider there to be a lear di eren e between the results obtained for the syn-\ntheti and the UCI domains, we shall dis uss them separately.\nFor the syntheti domains (Tables 3, 4 and 5):\n{ Our RPDAG-based algorithm outperforms the DAG-based one with respe t to\nthe value of the s oring fun tion used to guide the sear h: we always obtain better results on the ve databases onsidered. Note that we are using a logarithmi version of the s oring fun tion, so that the di eren es are mu h greater in a non-logarithmi s ale. These results support the idea that RPDAGs are able to\nnd new and better lo al maxima within the s ore+sear h approa h for learning\nBayesian networks in this type of highly stru tured domains.\n{ Our sear h method is also preferable from the point of view of the Hamming\ndistan es, whi h are always onsiderably lower than the ones obtained by using the DAG spa e.\n{ Moreover, our sear h method is generally more eÆ ient: it arries out fewer iter-\nations (on the ve ases), evaluates fewer individuals (on four ases), omputes"
    }, {
      "heading" : "BDeu Edg H A D I Iter Ind EstEv TEst NVar Time",
      "text" : "fewer di erent statisti s from the databases (on three ases), uses fewer statisti s (on the ve ases), and runs faster (on four ases). On the ontrary, the average number of variables involved in the statisti s is slightly greater (on four ases).\nFor the UCI domains (Table 6):\n{ The results in this ase are not as on lusive about the advantages of the RPDAG-\nbased method with respe t to the DAG-based one in terms of e e tiveness: both"
    }, {
      "heading" : "BDeu Edg Iter Ind EstEv TEst NVar Time",
      "text" : ""
    }, {
      "heading" : "Breast-Can er",
      "text" : "algorithms rea h the same solution on two ases from six, rpdag is better than dag on three ases, and dag is better on one ase.\n{ With respe t to the eÆ ien y of the two algorithms, the situation is similar:\nneither algorithm learly outperforms the other with respe t to any of the ve eÆ ien y measures onsidered.\nIn a se ond series of experiments, we aim to test the behavior of the sear h in the RPDAG spa e when used in ombination with a sear h heuristi whi h is more powerful than a simple greedy sear h. The heuristi sele ted is Tabu Sear h (Glover, 1989; Bou kaert, 1995), whi h tries to es ape from a lo al maximum by sele ting a solution that minimally de reases the value of the s oring fun tion; immediate re-sele tion of the lo al maximum just visited is prevented by maintaining a list of solutions that are forbidden, the so- alled tabu list (although for pra ti al reasons the tabu list stores forbidden operators not solutions, and\nonsequently, solutions whi h have not been visited previously may also be ome forbidden).\nWe have implemented two simple versions of tabu sear h: ts-rpdag and ts-dag, whi h explore the RPDAG and DAG spa es, respe tively, using the same operators as their respe tive greedy versions. The parameters used by these algorithms are the length tll of the tabu list and the number tsit of iterations required to stop the sear h pro ess. In our experiments, these values have been xed as follows: tll = n and tsit = n(n 1), n being the number of variables in the domain. The s oring fun tion and the initial graph are the same as in previous experiments, as well as the olle ted performan e measures, with one ex eption: as the number of iterations is now xed (Iter=tsit), we ompute the iteration where\nthe best graph was found (BIter) instead. The results of these experiments are displayed in Tables 7, 8, 9 and 10.\nFor the syntheti domains, in all the ases, ex ept in one of the insuran e databases, the results obtained by ts-rpdag and rpdag are the same. This phenomenon also appears for the UCI databases, where only in two databases does ts-rpdag improve the results of rpdag. Therefore, the Tabu Sear h does not ontribute signi antly to improving the greedy sear h in the RPDAG spa e (at least using the sele ted values for the parameters tll and tsit). This is in ontrast with the situation in the DAG spa e, where ts-dag improves the results obtained by dag, with the ex eption of two UCI databases (equal results) and Alarm-3000 (where dag performs better than ts-dag).\nWith respe t to the omparison between ts-rpdag and ts-dag, we still onsider tsrpdag to be preferable to ts-dag on the syntheti domains, although in this ase ts-dag performs better on the insuran e domain. For the UCI databases, the two algorithms perform similarly: ea h algorithm is better than the other on two domains, and both algorithms"
    }, {
      "heading" : "Breast-Can er",
      "text" : "perform equally on the remaining two domains. ts-rpdag is somewhat more eÆ ient than ts-dag with respe t to running time.\nWe have arried out a third series of experiments to ompare our learning algorithm based on RPDAGs with other algorithms for learning Bayesian networks. In this ase, the\nomparison is only intended to measure the quality of the learned network. In addition to the DAG-based lo al and tabu sear h previously onsidered, we have also used the following algorithms:\np (Spirtes et al., 1993), an algorithm based on independen e tests. We used an independen e test based on the measure of onditional mutual information (Kullba k, 1968), with a xed on den e level equal to 0.99.\nThe K2 sear h method (Cooper & Herskovits, 1992), in ombination with the BDeu s oring fun tion (k2). Note that k2 needs an ordering of the variables as the input. We used an ordering onsistent with the topology of the orresponding networks.\nAnother algorithm, BN Power Constru tor (bnp ), that uses independen e tests (Cheng et al., 1997; Cheng, Bell & Liu, 1998).\nThe two independen e-based algorithms, p and bnp , operate on the spa e of equivalen e\nlasses, whereas k2 explores the spa e of DAGs whi h are ompatible with a given ordering. We have in luded the algorithm k2 in the omparison, using a orre t ordering and the same s oring fun tion as the RPDAG and DAG-based sear h methods, in order to test whether our method an outperform the results obtained with a more informed algorithm. The\nresults for the algorithms p and k2 have been obtained using our own implementations (whi h are also in luded in the Elvira software). For bnp , we used the software pa kage available at http://www. s.ualberta. a/~j heng/bnsoft.htm.\nThe test domains in luded in these experiments are Alarm, Insuran e, and Hail nder. In addition to the BDeu values, the number of edges in the learned networks and the Hamming distan es, we have olle ted two additional performan e measures:\nBIC.- The value of the BIC (Bayesian Information Criterion) s oring fun tion (S hwarz,\n1978) for the learned network. This value measures the quality of the network using maximum likelihood and a penalty term. Note that BIC is also s ore-equivalent and de omposable.\nKL.- The Kullba k-Leibler distan e ( ross-entropy) (Kullba k, 1968) between the probabil-\nity distribution, P , asso iated to the database (the empiri al frequen y distribution) and the probability distribution asso iated to the learned network, P\nG\n. Noti e that\nthis measure is a tually the same as the log probability of the data. We have in fa t al ulated a de reasing monotoni linear transformation of the Kullba k-Leibler distan e, be ause this one has exponential omplexity and the transformation an be\nomputed very eÆ iently: If P\nG\nis the joint probability distribution asso iated to a\nnetwork G, then the KL distan e an be written in the following way (de Campos, 1998; Lam & Ba hus, 1994):\nKL(P; P\nG\n) = H\nP\n(U) +\nX\nx2U\nH\nP\n(x)\nX\nx2U ;Pa\nG\n(x)6=;\nMI\nP\n(x; Pa\nG\n(x)) (5)\nwhere H\nP\n(Z) denotes Shannon entropy with respe t to the distribution P for the sub-\nset of variables Z andMI\nP\n(x; Pa\nG\n(x)) is the measure of mutual information between\nthe two sets of variables fxg and Pa\nG\n(x). As the rst two terms of the expression\nabove do not depend on the graph G, our transformation onsists in al ulating only the third term in equation (5). So, the interpretation of our transformation of the Kullba k-Leibler distan e is: the higher this value is, the better the network ts the data. However, this measure should be handled with aution, sin e a high KL value may also indi ate over tting (a network with many edges will probably have a high KL value).\nAlthough for those algorithms whose goal is to optimize the Bayesian s ore, BDeu is really the metri that should be used to evaluate them, we have also omputed BIC and KL be ause two of the algorithms onsidered use independen e tests instead of a s oring fun tion.\nThe results of these experiments are displayed in Table 11. The best value for ea h performan e measure and ea h database is written in bold, and the se ond best value in itali s. These results indi ate that our sear h method in the RPDAG spa e, in ombination with the BDeu s oring fun tion, is ompetitive with respe t to other algorithms: only the ts-dag algorithm, whi h uses a more powerful (and more omputationally intensive) sear h heuristi in the DAG spa e, and, to a lesser extent, the more informed k2 algorithm, perform better than rpdag in some ases. Observe that both ts-dag and k2 perform better than rpdag in terms of KL on four ases from ve."
    }, {
      "heading" : "BDeu BIC KL Edg H A D I",
      "text" : "The fourth series of experiments attempts to evaluate the behavior of the same algorithms on a dataset whi h is di erent from the training set used to learn the network. In order to do so, we have omputed the BDeu, BIC, and KL values of the network stru - ture learned using a database, with respe t to a di erent database: for the Alarm domain, the training set is the Alarm-3000 database used previously, and the test set is formed by the 3000 next ases in the Alarm database; for both Insuran e and Hail nder, we sele ted one of the ve databases that we have been using as the training set and another of these databases as the test set. The results are shown in Table 12. We an observe that they\nare analogous to the results obtained in Table 11, where the same databases were used for training and testing. However, in this ase rpdag also performs better than ts-dag and k2 in terms of KL. Therefore, the good behavior of our algorithm annot be attributed to over tting.\nFinally, we have arried out another series of experiments, whi h aim to ompare our rpdag algorithm with the algorithm proposed by Chi kering (2002), that sear hes in the CPDAG spa e. In this ase, we have sele ted the House-Votes and Mushroom domains (whi h were two of the datasets used by Chi kering). In order to approximate our experimental onditions to those des ribed in Chi kering's work, we used the BDeu s oring fun tion with a prior equivalent sample size of ten, and a stru ture prior of 0:001 f , where f is the number of free parameters in the DAG; moreover, we used ve random subsets of the original databases, ea h ontaining approximately 70% of the total data (304 ases for House-Votes and 5686 for Mushroom). Table 13 displays the average values a ross the\nve datasets of the relative improvement of the per- ase s ore obtained by rpdag to the per- ase s ore of dag, as well as the ratio of the time spent by dag to the time spent by rpdag. We also show in Table 13 the orresponding values obtained by Chi kering (using only one dataset) for the omparison between his pdag algorithm and dag.\nWe may observe that the behavior of rpdag and pdag is somewhat di erent: although both algorithms are more eÆ ient than dag, it seems to us that pdag runs faster than rpdag. With respe t to e e tiveness, both rpdag and pdag obtain exa tly the same\nsolution as dag in the House-Votes domain (no relative improvement); however, in the other domain, rpdag outperforms dag (on the ve datasets onsidered) whereas pdag performs worse than dag. In any ase, the di eren es are small (they ould be a result of di eren es in the experimental setup) and a mu h more systemati experimentation with these algorithms would be ne essary in order to establish general on lusions about their\nomparative behavior."
    }, {
      "heading" : "7. Con luding Remarks",
      "text" : "We have developed a new lo al sear h algorithm, within the s ore+sear h approa h for learning Bayesian network stru tures from databases. The main feature of our algorithm is that it does not sear h in the spa e of DAGs, but uses a new form of representation, restri ted PDAGs, that allows us to sear h eÆ iently in a spa e similar to the spa e of equivalen e lasses of DAGs. For the ommon situation in whi h a de omposable s oring fun tion is used, the set of operators that de ne the neighborhood stru ture of our sear h spa e an be s ored lo ally (as it happens in the spa e of DAGs), i.e. we an evaluate any neighboring restri ted PDAG by omputing at most two lo al s ores. In this way, we maintain the omputational eÆ ien y that the spa e of DAGs o ers and, at the same time, we explore a more redu ed sear h spa e, with a smoother lands ape, whi h avoids some early de isions on edge dire tions. These hara teristi s may help to dire t the sear h pro ess towards better network stru tures.\nThe experimental results show that our sear h method based on restri ted PDAGs an eÆ iently and a urately re over omplex Bayesian network stru tures from data, and an\nompete with several state of the art Bayesian network learning algorithms, although it does not signi antly improve them. Our experiments in Se tion 6, as well as those ondu ted by Chi kering (2002), seem to point out that sear h algorithms based on PDAGs an obtain slightly better results, with respe t to both e e tiveness and eÆ ien y, than sear h methods based on DAGs, espe ially for highly stru tured models (i.e., models that an be (almost) perfe tly represented by a DAG). We believe that PDAGs an also be useful in domains whi h are omplex ( ontain many variables and ompli ated dependen e patterns) and sparse (represent many independen e relationships).\nFor future resear h, we are planning to integrate the te hniques developed in this paper within more powerful sear h methods, su h as the ones onsidered by Blan o et al. (2003), de Campos et al. (2002) or de Campos and Puerta (2001a). Additionally, in the light of the results obtained by our method in ombination with Tabu Sear h, it may be interesting to in orporate another operator, whi h ould either be a lassi al ar reversal or some kind of\nspe i operator to destroy h-h patterns. We also intend to work on the adaptation and appli ation of our algorithm to real problems in the eld of lassi ation."
    }, {
      "heading" : "A knowledgments",
      "text" : "This work has been supported by the Spanish Ministerio de Cien ia y Te nolog a (MCYT) and the Junta de Comunidades de Castilla-La Man ha under Proje ts TIC2001-2973-CO501 and PBC-02-002, respe tively. We are grateful to our olleague Jos e M. Puerta for his invaluable help with the implementation of several algorithms. We are also grateful to the three anonymous reviewers for useful omments and suggestions."
    }, {
      "heading" : "Referen es",
      "text" : "Abramson, B., Brown, J., Murphy, A., & Winkler, R. L. (1996). Hail nder: A Bayesian\nsystem for fore asting severe weather. International Journal of Fore asting, 12, 57{71.\nA id, S., & de Campos, L. M. (2000). Learning right sized belief networks by means of a\nhybrid methodology. Le ture Notes in Arti ial Intelligen e, 1910, 309{315.\nA id, S., & de Campos, L. M. (2001). A hybrid methodology for learning belief networks:\nBenedi t. International Journal of Approximate Reasoning, 27, 235{262.\nAndersson, S., Madigan, D., & Perlman, M. (1997). A Chara terization of Markov equiva-\nlen e lasses for a y li digraphs. Annals of Statisti s, 25, 505{541.\nBeinli h, I. A., Suermondt, H. J., Chavez, R. M., & Cooper, G. F. (1989). The alarm\nmonitoring system: A ase study with two probabilisti inferen e te hniques for belief networks. In Pro eedings of the European Conferen e on Arti ial Intelligen e in Medi ine, 247{256.\nBinder, J., Koller, D., Russell, S., & Kanazawa, K. (1997). Adaptive probabilisti networks\nwith hidden variables. Ma hine Learning, 29, 213{244.\nBlan o, R., Inza, I., & Larra~naga, P. (2003). Learning Bayesian networks in the spa e of\nstru tures by estimation of distribution algorithms. International Journal of Intelligent Systems, 18, 205{220.\nBou kaert, R. R. (1993). Belief networks onstru tion using the minimum des ription length\nprin iple. Le ture Notes in Computer S ien e, 747, 41{48.\nBou kaert, R. R. (1995). Bayesian belief networks: from onstru tion to inferen e. Ph.D.\nthesis, University of Utre ht.\nBuntine, W. (1991). Theory re nement of Bayesian networks. In Pro eedings of the Seventh\nConferen e on Un ertainty in Arti ial Intelligen e, 52{60.\nBuntine, W. (1994). Operations for learning with graphi al models. Journal of Arti ial\nIntelligen e Resear h, 2, 159{225.\nBuntine, W. (1996). A guide to the literature on learning probabilisti networks from data.\nIEEE Transa tions on Knowledge and Data Engineering, 8, 195{210.\nCheng, J., Bell, D. A., & Liu, W. (1997). An algorithm for Bayesian belief network on-\nstru tion from data. In Pro eedings of AI and STAT'97, 83{90.\nCheng, J., Bell, D. A., & Liu, W. (1998). Learning Bayesian networks from data: An\neÆ ient approa h based on information theory. Te h. rep., University of Alberta.\nChi kering, D. M. (1995). A transformational hara terization of equivalent Bayesian net-\nwork stru tures. In Pro eedings of the Eleventh Conferen e on Un ertainty in Arti ial Intelligen e, 87{98.\nChi kering, D. M. (1996). Learning equivalen e lasses of Bayesian network stru tures.\nIn Pro eedings of the Twelfth Conferen e on Un ertainty in Arti ial Intelligen e, 150{157.\nChi kering, D. M. (2002). Learning equivalen e lasses of Bayesian network stru tures.\nJournal of Ma hine Learning Resear h, 2, 445{498.\nChi kering, D. M., Geiger, D., & He kerman, D. (1995). Learning Bayesian networks: Sear h\nmethods and experimental results. In Preliminary Papers of the Fifth International Workshop on Arti ial Intelligen e and Statisti s, 112{128.\nChow, C., & Liu, C. (1968). Approximating dis rete probability distributions with depen-\nden e trees. IEEE transa tions on Information Theory, 14, 462{467.\nCooper, G. F., & Herskovits, E. (1992). A Bayesian method for the indu tion of probabilisti\nnetworks from data. Ma hine Learning, 9, 309{348.\nDash, D., & Druzdzel, M. (1999). A hybrid anytime algorithm for the onstru tion of ausal\nmodels from sparse data. In Pro eedings of the Fifteenth Conferen e on Un ertainty in Arti ial Intelligen e, 142{149.\nde Campos, L. M. (1998). Independen y relationships and learning algorithms for singly\nonne ted networks. Journal of Experimental and Theoreti al Arti ial Intelligen e,\n10, 511{549.\nde Campos, L. M., Fern andez-Luna, J. M., G amez, J. A., & Puerta, J. M. (2002). Ant olony\noptimization for learning Bayesian networks. International Journal of Approximate Reasoning, 31, 291{311.\nde Campos, L. M., Fern andez-Luna, J. M., & Puerta, J. M. (2002). Lo al sear h methods\nfor learning Bayesian networks using a modi ed neighborhood in the spa e of dags. Le ture Notes in Computer S ien e, 2527, 182{192.\nde Campos, L. M., Fern andez-Luna, J. M., & Puerta, J. M. (2003). An iterated lo al\nsear h algorithm for learning Bayesian networks with restarts based on onditional independen e tests. International Journal of Intelligent Systems, 18, 221{235.\nde Campos, L. M., G amez, J. A., & Puerta, J. M. (in press). Learning Bayesian networks\nby ant olony optimisation: Sear hing in two di erent spa es. Mathware and Soft Computing.\nde Campos, L. M., & Huete, J. F. (1997). On the use of independen e relationships for\nlearning simpli ed belief networks. International Journal of Intelligent Systems, 12, 495{522.\nde Campos, L. M., & Huete, J. F. (2000). A new approa h for learning belief networks using\nindependen e riteria. International Journal of Approximate Reasoning, 24, 11{37.\nde Campos, L. M., & Huete, J. F. (2000). Approximating ausal orderings for Bayesian\nnetworks using geneti algorithms and simulated annealing. In Pro eedings of the Eighth IPMU Conferen e, 333{340.\nde Campos, L. M., & Puerta, J. M. (2001). Sto hasti lo al and distributed sear h algo-\nrithms for learning belief networks. In Pro eedings of the III International Symposium on Adaptive Systems: Evolutionary Computation and Probabilisti Graphi al Model, 109{115.\nde Campos, L. M., & Puerta, J. M. (2001). Sto hasti lo al sear h algorithms for learn-\ning belief networks: Sear hing in the spa e of orderings. Le ture Notes in Arti ial Intelligen e, 2143, 228{239.\nDor, D., & Tarsi, M. (1992). A simple algorithm to onstru t a onsistent extension of a\npartially oriented graph. Te h. rep. R-185, Cognitive Systems Laboratory, Department of Computer S ien e, UCLA.\nFriedman, N., & Goldszmidt, M. (1996). Learning Bayesian networks with lo al stru ture.\nIn Pro eedings of the Twelfth Conferen e on Un ertainty in Arti ial Intelligen e, 252{262.\nFriedman, N., & Koller, D. (2000). Being Bayesian about network stru ture. In Pro eedings\nof the Sixteenth Conferen e on Un ertainty in Arti ial Intelligen e, 201{210.\nFriedman, N., Na hman, I., & Pe er, D. (1999). Learning Bayesian network stru ture from\nmassive datasets: The \"sparse andidate\" algorithm. In Pro eedings of the Fifteenth Conferen e on Un ertainty in Arti ial Intelligen e, 206{215.\nGeiger, D., & He kerman, D. (1995). A hara terisation of the Diri hlet distribution with\nappli ation to learning Bayesian networks. In Pro eedings of the Eleventh Conferen e on Un ertainty in Arti ial Intelligen e, 196{207.\nGeiger, D., Paz, A., & Pearl, J. (1990). Learning ausal trees from dependen e information.\nIn Pro eedings of AAAI-90, 770{776.\nGeiger, D., Paz, A., & Pearl, J. (1993). Learning simple ausal stru tures. International\nJournal of Intelligent Systems, 8, 231{247.\nGillispie, S. B., & Perlman, M. D. (2001). Enumerating Markov equivalen e lasses of\na y li digraphs models. In Pro eedings of the Seventeenth Conferen e on Un ertainty in Arti ial Intelligen e, 171{177.\nGlover, F. (1989). Tabu sear h, Part I. ORSA Journal of Computing, 1, 190{206.\nHe kerman, D. (1996). Bayesian networks for knowledge dis overy. In U.M. Fayyad, G.\nPiatetsky-Shapiro, P. Smyth, R. Uthurusamy (Eds.), Advan es in Knowledge Dis overy and Data Mining. Cambridge: MIT Press, 273{305.\nHe kerman, D., Geiger, D., & Chi kering, D. M. (1995). Learning Bayesian networks: The\nombination of knowledge and statisti al data. Ma hine Learning, 20, 197{243.\nHerskovits, E. (1991). Computer-based probabilisti networks onstru tion. Ph.D thesis,\nMedi al Information S ien es, Stanford University.\nHerskovits, E., & Cooper, G. F. (1990). Kutat o: An entropy-driven system for the on-\nstru tion of probabilisti expert systems from databases. In Pro eedings of the Sixth Conferen e on Un ertainty in Arti ial Intelligen e, 54{62.\nHuete, J. F., & de Campos, L. M. (1993). Learning ausal polytrees. Le ture Notes in\nComputer S ien e, 747, 180{185.\nJensen, F. V. (1996). An Introdu tion to Bayesian Networks. UCL Press.\nKo ka, T., & Castelo, R. (2001). Improved learning of Bayesian networks. In Pro eedings\nof the Seventeenth Conferen e on Un ertainty in Arti ial Intelligen e, 269{276.\nKohavi, R., John, G., Long, R., Manley, D., & P eger, K. (1994). MLC++: A ma hine\nlearning library in C++. In Pro eedings of the Sixth International Conferen e on Tools with Arti ial Intelligen e, 740{743.\nKullba k, S. (1968). Information Theory and Statisti s. Dover Publi ation.\nLam, W., & Ba hus, F. (1994). Learning Bayesian belief networks. An approa h based on\nthe MDL prin iple. Computational Intelligen e, 10, 269{293.\nLarra~naga, P., Poza, M., Yurramendi, Y., Murga, R., & Kuijpers, C. (1996). Stru ture\nlearning of Bayesian networks by geneti algorithms: A performan e analysis of ontrol parameters. IEEE Transa tions on Pattern Analysis and Ma hine Intelligen e, 18, 912{926.\nLarra~naga, P., Kuijpers, C., & Murga, R. (1996). Learning Bayesian network stru tures\nby sear hing for the best ordering with geneti algorithms. IEEE Transa tions on System, Man and Cyberneti s, 26, 487{493.\nMadigan, D., Anderson, S. A., Perlman, M. D., & Volinsky, C. T. (1996). Bayesian model\naveraging and model sele tion for Markov equivalen e lasses of a y li digraphs. Communi ations in Statisti s { Theory and Methods, 25, 2493{2520.\nMadigan, D., & Raftery, A. (1994). Model sele tion and a ounting for model un ertainty\nin graphi al models using O am's window. Journal of the Ameri an Statisti s Asso-\niation, 89, 1535{1546.\nMeek, C. (1995). Causal inferen e and ausal explanation with ba kground knowledge.\nIn Pro eedings of the Eleventh Conferen e on Un ertainty in Arti ial Intelligen e, 403{410.\nMuntenau, P., & Cau, D. (2000). EÆ ient s ore-based learning of equivalen e lasses of\nBayesian networks. Le ture Notes in Arti ial Intelligen e, 1910, 96{105.\nMyers, J. W., Laskey, K. B., & Levitt, T. (1999). Learning Bayesian networks from in om-\nplete data with sto hasti sear h algorithms. In Pro eedings of the Fifteenth Conferen e on Un ertainty in Arti ial Intelligen e, 476{485.\nPearl, J. (1988). Probabilisti Reasoning in Intelligent Systems: Networks of Plausible\nInferen e. San Mateo: Morgan Kaufmann.\nPearl, J., & Verma, T. S. (1990). Equivalen e and synthesis of ausal models. In Pro eedings\nof the Sixth Conferen e on Un ertainty in Arti ial Intelligen e, 220{227.\nPuerta, J. M. (2001). M etodos lo ales y distribuidos para la onstru i on de redes de reen ia\nest ati as y din ami as (in Spanish). Ph.D. thesis, Department of Computer S ien e and Arti ial Intelligen e, University of Granada.\nRamoni, M., & Sebastiani, P. (1997). Learning Bayesian networks from in omplete\ndatabases. In Pro eedings of the Thirteenth Conferen e on Un ertainty in Arti ial Intelligen e, 401{408.\nRebane, G., & Pearl, J. (1987). The re overy of ausal poly-trees from statisti al data. In\nL.N. Kanal, T.S. Levitt, J.F. Lemmer (Eds.), Un ertainty in Arti ial Intelligen e 3, Amsterdam: North-Holland, 222{228.\nS hwarz, G. (1978). Estimating the dimension of a model. Annals of Statisti s, 6, 461{464.\nSingh, M., & Valtorta, M. (1993). An algorithm for the onstru tion of Bayesian net-\nwork stru tures from data. In Pro eedings of the Ninth Conferen e on Un ertainty in Arti ial Intelligen e, 259{265.\nSingh, M., & Valtorta, M. (1995). Constru tion of Bayesian network stru tures from data:\nA brief survey and an eÆ ient algorithm. International Journal of Approximate Reasoning, 12, 111{131.\nSpirtes, P., Glymour, C., & S heines, R. (1993). Causation, Predi tion and Sear h. Le ture\nNotes in Statisti s 81, New York: Springer Verlag.\nSpirtes, P., & Meek, C. (1995). Learning Bayesian networks with dis rete variables from\ndata. In Pro eedings of the First International Conferen e on Knowledge Dis overy and Data Mining, 294{299.\nSte k, H. (2000). On the use of skeletons when learning in Bayesian networks. In Pro eedings\nof the Sixteenth Conferen e on Un ertainty in Arti ial Intelligen e, 558{565.\nSuzuki, J. (1993). A onstru tion of Bayesian networks from databases based on the MDL\nprin iple. In Pro eedings of the Ninth Conferen e on Un ertainty in Arti ial Intelligen e, 266{273.\nSuzuki, J. (1996). Learning Bayesian belief networks based on the minimum des ription\nlength prin iple: An eÆ ient algorithm using the B&B te hnique. In Pro eedings of the Thirteenth International Conferen e on Ma hine Learning, 462{470.\nTian, J. (2000). A bran h-and-bound algorithm for MDL learning Bayesian neworks. In\nPro eedings of the Sixteenth Conferen e on Un ertainty in Arti ial Intelligen e, 580{ 587.\nVerma, T., & Pearl, J. (1990). Causal networks: Semanti s and expressiveness. In R.D.\nSha hter, T.S. Lewitt, L.N. Kanal, J.F. Lemmer (Eds.), Un ertainty in Arti ial Intelligen e, 4, Amsterdam: North-Holland, 69{76.\nWermuth, N., & Lauritzen, S. (1983). Graphi al and re ursive models for ontingen e\ntables. Biometrika, 72, 537{552.\nWong, M. L., Lam, W., & Leung, K. S. (1999). Using evolutionay omputation and min-\nimum des ription length prin iple for data mining of probabilisti knowledge. IEEE Transa tions on Pattern Analysis and Ma hine Intelligen e, 21, 174{178."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2013,
    "abstractText" : "Although many algorithms have been designed to onstru t Bayesian network stru tures using di erent approa hes and prin iples, they all employ only two methods: those based on independen e riteria, and those based on a s oring fun tion and a sear h proedure (although some methods ombine the two). Within the s ore+sear h paradigm, the dominant approa h uses lo al sear h methods in the spa e of dire ted a y li graphs (DAGs), where the usual hoi es for de ning the elementary modi ations (lo al hanges) that an be applied are ar addition, ar deletion, and ar reversal. In this paper, we propose a new lo al sear h method that uses a di erent sear h spa e, and whi h takes a ount of the on ept of equivalen e between network stru tures: restri ted a y li partially dire ted graphs (RPDAGs). In this way, the number of di erent on gurations of the sear h spa e is redu ed, thus improving eÆ ien y. Moreover, although the nal result must ne essarily be a lo al optimum given the nature of the sear h method, the topology of the new sear h spa e, whi h avoids making early de isions about the dire tions of the ar s, may help to nd better lo al optima than those obtained by sear hing in the DAG spa e. Detailed results of the evaluation of the proposed sear h method on several test problems, in luding the well-known Alarm Monitoring System, are also presented.",
    "creator" : "dvips(k) 5.86 Copyright 1999 Radical Eye Software"
  }
}