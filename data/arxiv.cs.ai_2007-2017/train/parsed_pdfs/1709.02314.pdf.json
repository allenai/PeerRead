{
  "name" : "1709.02314.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Representation Learning for Visual-Relational Knowledge Graphs",
    "authors" : [ "Daniel Oñoro-Rubio", "Mathias Niepert", "Roberto González-Sánchez" ],
    "emails" : [ "daniel.onoro@neclab.eu", "mathias.niepert@neclab.eu", "alberto.duran@neclab.eu", "roberto.gonzalez@neclab.eu", "robertoj.lopez@uah.es" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Knowledge graphs (KGs) are an important component of numerous AI systems. KGs facilitate the integration, organization, and retrieval of structured data and support various forms of reasoning. The history of AI has to a large extend been influenced by the development of methods for representing, querying, and learning from KGs. Moreover, in recent years KGs have been playing an increasingly crucial role in fields such as question answering [4], language modeling [1], and text generation [30]. Even though there is a large body of work on learning and reasoning in KGs, the setting of visualrelational KGs, where entities are associated with visual data, has not received much attention. A visual-relational KG represents entities, relations between these entities, and a large number of images associated with the entities (see Figure 1 for an example). While IMAGENET [7] and the VISUALGENOME [19] datasets are based on KGs such as WordNet they are predominantly used as either a classification data set as in the case of IMAGENET or to facilitate scene understanding in a single image. With IMAGEGRAPH, on the other hand, we propose to reason about visual concepts across a large set of images organized in a knowledge graph.\nWe are currently working on the release of IMAGEGRAPH.\nar X\niv :1\n70 9.\n02 31\n4v 1\n[ cs\n.L G\nThe idea is to use images as first-class citizens both in the KG and in relational queries. The query type most of the existing deep learning methods address is that of determining the depicted object given an image. However, given the relational structure of a KG, numerous more complex queries are possible. The main objective of our work is to understand to what extent visual data associated with entities of a KG can be used in conjunction with deep learning methods to answer visual-relational queries. Allowing images to be arguments of queries facilitates numerous novel query types. In Figure 1 we list some of the query types we address in this paper. In order to answer these queries, we built both on KG embedding methods as well as deep representation learning approaches for visual data. There has been a flurry of machine learning approaches tailored to specific problems such as link prediction in knowledge graphs. Examples are knowledge base factorization and embedding approaches [5, 25, 15, 23] and random-walk based ML models [20, 11]. The former learn latent features for the entities and relations in the knowledge graph and use those to perform link prediction. The latter explore specific relational features such as path types between two entities and train a machine learning model for link prediction. We will built on these ideas and combine them with deep representation learning approaches to build models for visual-relational query answering.\nWe make the following contributions. First, we publicly release IMAGEGRAPH, a visual-relational KG with 1330 relations where 829,931 images are associated with 14,870 different entities. Second, we introduce a new set of visual-relational query types. Third, we propose a set of deep neural networks that we use for answering these novel query types. Fourth, we show that the proposed class of deep neural networks are also successful for zero-shot learning, that is, creating relations between entirely unseen entities and the KG using only visual data at query time."
    }, {
      "heading" : "2 Related Work",
      "text" : "KGs and Visual Concepts. Answering queries in a visual-relational knowledge graph is the main objective of this paper. Previous work on combining relational and visual data has focused on object detection [10, 12, 28] and scene recognition [8, 26, 29, 33] which are required for more complex visual-relational reasoning. Recent years have witnessed a surge in reasoning about human-object, object-object, and object-attribute relationships [14, 9, 22, 35, 10, 6, 16, 37]. The VisualGenome project [19] is a knowledge base that integrates language and vision modalities. The project provides a knowledge graph, based on WORDNET, which provides annotations of objects, attributes, and relationships for each image. Recent work has used the VisualGenome dataset to focus on understanding the relationships of the visual entities within scenes depicted in one image. For instance, Lu et al. [21] propose a model to detect relationships between objects depicted in one image. Their system is able to infer structured sentences such as man riding bicycle. Johnson et al. [18] propose to use the VisualGenome dataset to recover images from simple text queries such as man riding a bike near a mountain. IMAGEGRAPH is different from all of these data sets in that the relationships hold between different images and image annotated entities. This defines a different class of problems where the graph can be used to answer complex queries such as How are the given images related to each other? We address different problems ranging from predicting the relation given a pair of visual concepts to completing a query with a missing image.\nZero-shot learning. In this paper, we also focus on exploring how our KG can be used to find relationships between unseen images, which belong to novel entities that are not part of the KG, and known KG entities. This is a form of zero-shot learning (ZSL) where the objective is to generalize to novel visual concepts without seeing any training examples. Generally, ZSL methods (e.g. [27, 36]) rely on an underlying embedding space, such as attributes, in order to recognize the unseen categories. However, in this paper, we do not assume the availability of such a common embedding space but we assume the existence of an external corpus of knowledge in the form of a visual-relational KG. Similar to our approach, when this explicit knowledge is not encoded in the underlying embedding space, other works rely on finding the similarities through the linguistic space (e.g. [2, 21]), leveraging distributional word representations so as to capture a notion of taxonomy and similarity. But these works can operate only at the image level, i.e. these models are able to detect the visual relationships about a scene in a single given image. On the contrary, our models are able to find relationships across different images and entities. We approach ZSL as the problem of establishing relations with visual concepts in an existing KG.\nFigure 1: A small part of a visual-relational knowledge graph and a set of query types."
    }, {
      "heading" : "3 IMAGEGRAPH: A Visual-Relational Knowledge Graph",
      "text" : "IMAGEGRAPH is a visual-relational KG whose relational structure is based on that of FREEBASE [3]. More specifically, it is based on FB15K, a subset of FREEBASE, which has been used as a benchmark data set in previous work on KG completion [23]. FB15K does not include visual data and we perform the following steps to enrich the KG entities with image data. In order to obtain images related to each of the entities of FB15K, we implemented a web crawler that is able to parse query results for the image search engines Google Images, Bing Images, and Yahoo Image Search. To minimize the amount of noise due to polysemous entity labels (for example, there are more than 100 FREEBASE entities with the text label “Springfield\"), we extracted for each entity in FB15K all of its Wikipedia keys from the 1.9 billion triple FREEBASE RDF dump. For instance, for Springfield, Massachusetts, we obtained such keys as Springfield_(Massachusetts,_United_States) and Springfield_(MA). We used the crawler to download more than 2.4M images (more than 462Gb of data). We further processed the data set to remove low quality and duplicate images. This resulted in 829,931 images associated with 14,870 different entities (55.8 images per entity). After filtering out those FB15K triples where either the head or tail entity could not be associated with an image, the visual KG consists of 564,010 triples expressing 1,330 different relations between 14,870 entities.\nWe provide three sets of triples for training, validation, and testing. Table 1 lists the statistics of the resulting visual KG. The data will be made available to the public.\nThe major differences between IMAGEGRAPH and IMAGENET are the following. IMAGENET is based on WORDNET a lexical database where synonymous words from the same lexical category are grouped into synsets. There are 18 relations expressing connections between synsets. The relation part_of is used to expresses that, for instance, a leg is a part of an animal. In FREEBASE, on the other hand, there are two order of magnitudes more relations. In FB15K there are 1345 relations expressing location of places, positions of basketball players, and gender of entities. Moreover, entities in IMAGENET exclusively represent categories such as cats and cars whereas entities in FREEBASE may represent categories as well as individual entities such as Albert Einstein and Paris. This makes the computer vision problems much more fine-grained and challenging than that of existing data sets. Moreover, previous work has focused on image to category classification problems. With IMAGEGRAPH, the focus is on learning relational machine learning models that incorporate visual data both during learning and at query time."
    }, {
      "heading" : "4 Deep Representation Learning for Visual-Relational Graphs",
      "text" : "A knowledge graph (KG) K is given by a set of triples T, that is, statements of the form (h, r, t), where h, t ∈ E are the head and tail entities, respectively, and r ∈ R is a relation type. The entities can be associated with additional data such as images, text, and numerical features. Figure 1 depicts a small fragment of a KG with relations between entities and images associated with the entities. Prior work has not included image data and has, therefore, focused on the following two types of queries. First, the query type (h, ?r, t) asks for the relations between the head and tail entities. Second, the query types (h, r, ?t) and (?h, r, t), asks for entities correctly completing the triple. The latter query type is often referred to as knowledge base completion."
    }, {
      "heading" : "4.1 Visual-Relational Query Answering",
      "text" : "When entities are associated with image data, several completely novel query types are possible. Figure 1 lists the query types we focus on in this paper. We refer to images used during training as seen and all other images as unseen.\n(1) Given a pair of unseen images for which we do not know their KG entities, determine the relations between these underlying entities.\n(2) Given an unseen image, for which we do not know the underlying KG entity, and a relation, determine the seen images that complete the query.\n(3) Given an unseen image of an entirely new entity that is not part of the KG, and an unseen image for which we do not know the underlying KG entity, determine the relations between the two underlying entities.\n(4) Given an unseen image of an entirely new entity that is not part of the KG, and a known KG entity, determine the relations between the two entities.\nFor each of these query types, the sought-after relation between the underlying entities has never been observed during training. Query types (3) and (4) are a form of zero-shot learning since neither the new entity’s relationships with other entities nor its images have been observed during training. These considerations illustrate the novel nature of the visual query types. The machine learning models have to be able to learn the relational semantics of the KG and not simply a classifier that assigns images to entities. These query types are also motivated by the fact that for typical KGs the number of entities is orders of magnitude greater than the number of relations."
    }, {
      "heading" : "4.2 Deep Representation Learning for Query Answering",
      "text" : "We first describe the state of the art in traditional KG completion and translate the concepts to query answering in visual-relational KGs. Let rawi be the raw feature representation for entity i ∈ R and let f and g be differentiable functions. Most KG completion methods learn an embedding of the entities in a vector space via some scoring function that is trained to assign high scores to correct triples and low scores to incorrect triples. Scoring functions have often the form fr(eh, et) where r is a relation, eh and et are d-dimensional vectors (the embeddings of the head and tail entities, respectively), and where ei = g(rawi) is an embedding function that maps the raw input representation of entities to the embedding space. In the case of KGs without additional visual data, the raw representation of an entity is simply its one-hot encoding.\nExisting KG completion methods use the embedding function g(rawh) = raw ᵀ iW where W is a |E| × d matrix, and differ only in their scoring function, that is, in the way that the embedding vectors of the head and tail entities are combined with the parameter vector φr:\n• Difference (TRANSE[5]): fr(eh, et) = −||eh + φr − et||2 where φr is a d-dimensional vector; • Multiplication (DISTMULT[34]): fr(eh, et) = (eh ∗ et) · φr where ∗ is the element-wise\nproduct and φr a d-dimensional vector; • Circular correlation (HOLE[24]): fr(eh, et) = (eh ? et) · φr where [a ? b]k =∑d−1\ni=0 aib(i+k) mod d and φr a d-dimensional vector; and\n• Concatenation: fr(eh, et) = (eh et) ·φr where is the concatenation operator and φr a 2d-dimensional vector.\nFor each of these instances, the matrix W (storing the entity embeddings) and the vectors φr are learned during training. In general, the parameters are trained such that fr(eh, et) is high for true triples and low for triples assumed not to hold in the KG. The training objective is often based on the logistic loss, which has been shown to be superior for most of the composition functions [32],\nmin Θ ∑ (h,r,t)∈Tpos log(1 + exp(−fr(eh, et)) + ∑ (h,r,t)∈Tneg log(1 + exp(fr(eh, et))) + λ||Θ||22, (1)\nwhere Tpos and Tneg are the set of positive and negative training triples, respectively, Θ are the parameters trained during learning and λ is a regularization hyperparameter. For the above objective, a process for creating corrupted triples Tneg is required. This often involves sampling a random entity for either the head or tail entity. To answer queries of the types (h, r, ?t) and (?h, r, t) after training, we form all possible completions of the queries and compute a ranking based on the scores assigned by the trained model to these completions.\nFor the queries of type (h, ?r, t) one typically uses the softmax activation in conjunction with the categorical cross-entropy loss, which does not require negative triples\nmin Θ ∑ (h,r,t)∈Tpos − log ( exp(fr(eh, et))∑ r∈R exp(fr(eh, et)) ) + λ||Θ||22, (2)\nwhere Θ are the parameters trained during learning and λ a regularization hyperparameter.\nFor visual-relational KGs, the input consists of raw image data instead of the one-hot encodings of entities. The aim of this work is to design deep neural networks so as to be able to answer the novel query types efficiently and with high accuracy. The approach we propose builds on the ideas and methods developed for KG completion. Instead of having a simple embedding function g that multiplies the input with a weight matrix, however, we use deep convolutional neural networks so as to extract meaningful visual features from the input images. For the composition function f we evaluate the four operations that were used in the KG completion literature: difference, multiplication, concatenation, and circular correlation. Figure 2 depicts the basic architecture we trained for query answering. The weights of the parts of the neural network responsible for embedding the raw image input, denoted by g, are tied. We also experimented with additional hidden layers after the composition operation, indicated by the dashed dense layer. The composition operation op is either difference, multiplication, concatenation, or circular correlation."
    }, {
      "heading" : "5 Experiments",
      "text" : "We conduct a series of experiments to evaluate our purposed network architectures for visual-relational query answering as defined before 4. First, we describe the experimental set-up that applies to all experiments. Second, we explain and report on results that we have achieved for the different types of queries introduced in the previous sections."
    }, {
      "heading" : "5.1 General Set-up",
      "text" : "We used CAFFE, a deep learning framework [17] for designing, training, and evaluating the proposed deep representation learning models. The embedding function g is based on the VGG16 model introduced in [31]. In summary, we pre-trained the VGG16 on the ILSVRC2012 challenge data set derived from IMAGENET [7] and removed the softmax layer of the original VGG16. We added a 256-dimensional layer after the last dense layer of the VGG16. The output of this layer serves as the embedding of the input images. The reason for reducing the embedding dimensionality from 4096 to 256 is motivated by the objective to obtain an efficient and compact latent representation that is feasible for KGs with billion of entities. For the composition function f, we performed either of the four operations difference, multiplication, concatenation, and circular correlation. We also experimented with an additional hidden layer with ReLu activation. Figure 2 depicts the generic network architecture. The output layer of the architecture has a softmax or sigmoid activation with cross-entropy loss. We initialized the weights of the newly added layers with the Xavier method [13].\nWe used a batch size of 45 which was the largest that fits into our GPU memory. To create the training batches, we sample a random triple uniformly at random from the training triples. For the given triple, we randomly sample one image for the head and one for the tail from the set of training images. We applied stochastic gradient descent with a learning rate of 10−5 for the parameters of the VGG16 and a learning rate of 10−3 for the remaining parameters. It was important to use two different learning rates since the large gradients in the newly added layers would lead to unreasonable changes in the pretrained part of the network. We set the weight decay to 5× 10−4. We reduced the learning rate by a factor of 0.1 every 40,000 iterations. Each of the models was trained for 100,000 iterations.\nSince the answers to all query types are either rankings of images or rankings of relations, we utilize metrics measuring the quality of rankings. In particular, we report results for hits@1 (hits@10, hits@100) measuring the number of times the correct relation was ranked highest (ranked in the top 10, top 100). We also compute the median of the ranks of the correct entities or relations and the Mean Reciprocal Rank (MRR) for entity and relation rankings, respectively, defined as follows:\nMRR = 1 2|T| ∑\n(h,r,t)∈T\n( 1\nrankimg(h) +\n1\nrankimg(t)\n) (3) MRR = 1 |T| ∑\n(h,r,t)∈T\n1\nrankr , (4)\nwhere T is the set of all test triples, rankr is the rank of the correct relation, and rankimg(h) is the rank of the highest ranked image of entity h. For each query, we remove all triples that are also correct answers to the query from the ranking. As in previous work in KG completion, we refer to this as the filtered setting. All experiments were run on commodity hardware with 128GB RAM, a single 2.8 GHz CPU, and a NVIDIA 1080 Ti.\n100 101 102 103 Relations\n102 104\nC ou\nnt\n100 101 102 103 104 Entities\n102 104\nC ou\nnt\nFigure 3: The distribution of relations and entities in the data set."
    }, {
      "heading" : "5.2 Visual Relation Prediction",
      "text" : "Given a pair of unseen images for which we do not know their entities, we want to determine the relations between their underlying entities. This can be expressed with (imgh, ?r, imgt). Figure 1(a) illustrates this query type which we refer to as visual relation prediction. We train the deep architectures using the training and validation triples and images, respectively. For each triple (h, r, t) in the training data set, we sample one training image uniformly at random for both the head and the tail entity. We use the architecture depicted in Figure 2 with the softmax activation and the categorical cross-entropy loss. For each test triple, we sample one image uniformly at random from the test images of the head and tail entity, respectively. We then use the pair of images to query the trained deep neural networks. To get a more robust statistical estimate of the evaluation measures, we repeat the above process three times per test triple. Again, note that none of the test triples and images are seen during training nor are any of the training images used during testing. Computing the answer to one query takes the model 20 ms.\nWe compare the results with a baseline that computes the probability of each relation by using the set of training and validation triples. The baseline ranks relations based on these probabilities. This baseline is competitive since the distribution of relations in the training, validation, and test triples is highly skewed, that is, a smaller number of relations occurs in a larger fraction of triples. Figure 3 plots the counts of relations and entities. Table 2 lists the results of the experiments. DIFF, MULT, and CAT stand for the different possible composition operations. We omitted the composition operation circular correlation since we were not able to make the corresponding model converge, despite trying several different optimizers and hyperparameter settings. The post-fix 1HL stands for architectures where we added an additional hidden layer with ReLu activation before the softmax. The concatenation operation clearly outperforms the multiplication and difference operations. This is contrary to findings in the KG completion literature where MULT and DIFF outperformed the concatenation operation. The models with the additional hidden layer did not perform better than their shallower counterparts with the exception of the DIFF model. We hypothesize that this is due to difference being the only linear composition operation, benefiting from an additional non-linearity."
    }, {
      "heading" : "5.3 Image Completion",
      "text" : "Given an unseen image, for which we do not know the underlying entity in the KG, and a relation, determine the seen images that complete the query. If the head is given, we will return a ranking of images for the tail entity; if the tail entity is given we will return a ranking of images for the head entity. For this problem we performed experiments with each of the three composition functions f and for two different activation/loss functions. First, we used the models trained with the softmax activation and the categorical cross-entropy loss to rank images. Second, we took the models trained with the softmax activation and substituted the softmax activation with a sigmoid activation and the corresponding binary cross-entropy loss. For each training triple (h, r, t) we then created two negative triples by sampling once the head and once the tail entity from the set of entities. The negative triples are then used in conjunction with the binary cross-entropy loss of equation 1 to refine the pretrained weights of the models. Directly training a model with the binary cross-entropy loss was not possible since the model did not converge properly. Hence, the pretraining with the softmax activation and categorical cross-entropy loss is crucial to make the learning with a binary loss work.\nDuring testing, we used the test triples and ranked the images based on the probabilities returned by the respective models. For instance, given the query (imgSenso-ji, locatedIn, ?imgt), we substituted ?imgt with all training and validation images, one at a time, and ranked the images according to the probabilities returned by the models. We use the rank of the highest ranked image belonging to the true entity (here: Japan) to compute the values for the evaluation measures. We repeat the same experiment three times (each time randomly sampling the images) and report average values. Again, we compare the results for the different architectures with a probabilistic baseline. For the baseline, however, we compute a distribution of head and tail entities for each of the relations. For example, for the relation locatedIn we compute two distribution, one for head and one for tail entities. We used the same evaluation measures as in the previous experiment to evaluate the returned image rankings.\nTable 3 lists the results of the experiments. As for relation prediction, the best performing models are based on the concatenation operation, followed by the difference and multiplication operations. The architectures with an additional hidden layer do not improve the performance. We also provide the results for the concatenation-based model with softmax activation where we refined the weights using a sigmoid activation and negative sampling as described before. This model is the best performing model. All neural network models are significantly better than the baseline with respect to the median and hits@100. However, the baseline has slightly superior results for the MRR. This is due to the skewed distribution of entities and relations in the KG (see Figure 3). This shows once more that the baseline is highly competitive for the given KG. Figure 4 visualizes the answers the CAT-SIG model provided for a set of four example queries. For the two queries on the left, the model performed well and ranked the correct entity in the top 3 (green frame). The examples on the right side illustrate example queries for which the model returned a highly inaccurate ranking. To perform query answering in a highly efficient manner, we precomputed and stored all image embeddings once, and only compute the scoring function (involving the composition operation and a dot product with φr) at query time. Answering one image completion query (which would otherwise require 613,138 individual queries to a deep neural networks, one per possible image) took only 90 ms."
    }, {
      "heading" : "5.4 Zero-Shot Visual Relation Prediction",
      "text" : "The last set of experiments addresses the problem of zero-shot learning via visual relation prediction. For both query types, we are given an unseen image of an entirely new entity that is not part of the KG. The first query type asks for relations between the given image and an unseen image for which we do not know the underlying KG entity. The second query type asks for the relations between the given image and an existing KG entity. We believe that this is a reasonable approach to zero-shot learning since an unseen entity or category is integrated into an existing KG. The relations to existing visual concepts provides a characterization of the unseen entity. Note that this problem cannot be addressed with KG embedding methods since entities need to be part of the KG during training to obtain an embedding.\nFor the zero-shot experiments, we generated a new set of training, validation, and test triples. We randomly sampled 500 entities that occur as head (tail) in the set of test triples. We then removed all training and validation triples whose head or tail is one of these 1000 entities. Finally, we only kept those test triples with one of the 1000 entities either as head or tail but not both. For query type (4) where we know the target entity, we sample 10 of its images and use the models 10 times\nhasCrewJob hasGenre\nhasProfession\nhasNutrient filmHasLocation peopleBornHere...\nTaxonomyHasEntry Card gameLib of C.\nBack To Future Special Effects Superv.\nFigure 5: Example results for zeroshot learning.\nto compute a probability. We use the average probabilities to rank the relations. For query type (3) we only use one image sampled randomly. As with previous experiments, we repeated procedure three times and averaged the results. For the baseline, we compute the probabilities of relation in the training and validation set (for query type (3)) and the probabilities of relations conditioned on the target entity (for query type (4)). Again, these are very competitive baselines due to the skewed distribution of relations and entities. Table 4 lists the results of the experiments. The model based on the concatenation operation (CAT) outperforms the baseline and performs surprisingly well. The deep models are able to generalize to unseen images since their performance is comparable to the performance in the relation prediction task (query (1)) where the entity was part of the KG during training (see Table 2). Figure 5 depicts example queries for the zero-shot query (3). For the first query example, the CAT model ranked the correct relation type first (indicated by the green bounding box). The second example is more challenging and the correct relation was not ranked in the top 10."
    }, {
      "heading" : "6 Conclusion",
      "text" : "Knowledge graphs are at the core of numerous AI applications. In the past, research has focused either on KG completion methods working only on the relational structure or on retrieving relations between objects within the same image. We present a novel visual-relational knowledge graph where the entities are enriched with visual data. We proposed several novel query types and show introduce novel deep neural networks suitable for query answering. We evaluate several composition functions that have been used in the KG completion literature. We also propose a novel angle on zero-shot learning where an unseen image of an entirely new entity is grounded in an existing KG. Future work will focus on more sophisticated deep architectures as well as extensions and improvements of the data and problems associated with visual-relational graphs."
    } ],
    "references" : [ {
      "title" : "A neural knowledge language model",
      "author" : [ "S. Ahn", "H. Choi", "T. Parnamaa", "Y. Bengio" ],
      "venue" : "arXiv preprint arXiv:1608.00318,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2016
    }, {
      "title" : "Predicting deep zero-shot convolutional neural networks using textual descriptions",
      "author" : [ "J. Ba", "K. Swersky", "S. Fidler", "R. Salakhutdinov" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2015
    }, {
      "title" : "Freebase: A collaboratively created graph database for structuring human knowledge",
      "author" : [ "K. Bollacker", "C. Evans", "P. Paritosh", "T. Sturge", "J. Taylor" ],
      "venue" : "In SIGMOD,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2008
    }, {
      "title" : "Large-scale simple question answering with memory networks",
      "author" : [ "A. Bordes", "N. Usunier", "S. Chopra", "J. Weston" ],
      "venue" : "arXiv preprint arXiv:1506.02075,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2015
    }, {
      "title" : "Translating embeddings for modeling multi-relational data",
      "author" : [ "A. Bordes", "N. Usunier", "A. Garcia-Duran", "J. Weston", "O. Yakhnenko" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2013
    }, {
      "title" : "Extracting visual knowledge from web data",
      "author" : [ "X. Chen", "A. Shrivastava", "A. Gupta. Neil" ],
      "venue" : "In Proceedings of the IEEE International Conference on Computer Vision,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2013
    }, {
      "title" : "ImageNet: A Large-Scale Hierarchical Image Database",
      "author" : [ "J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2009
    }, {
      "title" : "Mid-level visual element discovery as discriminative mode seeking",
      "author" : [ "C. Doersch", "A. Gupta", "A.A. Efros" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2013
    }, {
      "title" : "Describing objects by their attributes",
      "author" : [ "A. Farhadi", "I. Endres", "D. Hoiem", "D.A. Forsyth" ],
      "venue" : "In 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2009
    }, {
      "title" : "Object detection with discriminatively trained part-based models",
      "author" : [ "P.F. Felzenszwalb", "R.B. Girshick", "D. McAllester", "D. Ramanan" ],
      "venue" : "IEEE transactions on pattern analysis and machine intelligence,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2010
    }, {
      "title" : "Efficient and expressive knowledge base completion using subgraph feature extraction",
      "author" : [ "M. Gardner", "T.M. Mitchell" ],
      "venue" : "In EMNLP,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2015
    }, {
      "title" : "Rich feature hierarchies for accurate object detection and semantic segmentation",
      "author" : [ "R. Girshick", "J. Donahue", "T. Darrell", "J. Malik" ],
      "venue" : "In Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2014
    }, {
      "title" : "Understanding the difficulty of training deep feedforward neural networks",
      "author" : [ "X. Glorot", "Y. Bengio" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2010
    }, {
      "title" : "Observing human-object interactions: Using spatial and functional compatibility for recognition",
      "author" : [ "A. Gupta", "A. Kembhavi", "L.S. Davis" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2009
    }, {
      "title" : "Traversing knowledge graphs in vector space",
      "author" : [ "K. Guu", "J. Miller", "P. Liang" ],
      "venue" : "arXiv preprint arXiv:1506.01094,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2015
    }, {
      "title" : "Incorporating scene context and object layout into appearance modeling",
      "author" : [ "H. Izadinia", "F. Sadeghi", "A. Farhadi" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2014
    }, {
      "title" : "Caffe: Convolutional architecture for fast feature embedding",
      "author" : [ "Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell" ],
      "venue" : "arXiv preprint arXiv:1408.5093,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2014
    }, {
      "title" : "Image retrieval using scene graphs",
      "author" : [ "J. Johnson", "R. Krishna", "M. Stark", "L.-J. Li", "D.A. Shamma", "M. Bernstein", "L. Fei-Fei" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2015
    }, {
      "title" : "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
      "author" : [ "R. Krishna", "Y. Zhu", "O. Groth", "J. Johnson", "K. Hata", "J. Kravitz", "S. Chen", "Y. Kalantidis", "L.-J. Li", "D.A. Shamma", "M. Bernstein", "L. Fei-Fei" ],
      "venue" : "In arXiv preprint arXiv:1602.07332,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2016
    }, {
      "title" : "Random walk inference and learning in a large scale knowledge base",
      "author" : [ "N. Lao", "T. Mitchell", "W.W. Cohen" ],
      "venue" : "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2011
    }, {
      "title" : "Visual relationship detection with language priors",
      "author" : [ "C. Lu", "R. Krishna", "M. Bernstein", "L. Fei-Fei" ],
      "venue" : "In ECCV,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2016
    }, {
      "title" : "Beyond categories: The visual memex model for reasoning about object relationships",
      "author" : [ "T. Malisiewicz", "A.A. Efros" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2009
    }, {
      "title" : "A review of relational machine learning for knowledge graphs",
      "author" : [ "M. Nickel", "K. Murphy", "V. Tresp", "E. Gabrilovich" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2016
    }, {
      "title" : "Holographic embeddings of knowledge graphs",
      "author" : [ "M. Nickel", "L. Rosasco", "T.A. Poggio" ],
      "venue" : "In Proceedings of the Thirtieth Conference on Artificial Intelligence,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2016
    }, {
      "title" : "A three-way model for collective learning on multirelational data",
      "author" : [ "M. Nickel", "V. Tresp", "H.-P. Kriegel" ],
      "venue" : "In Proceedings of the 28th international conference on machine learning",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2011
    }, {
      "title" : "Scene recognition and weakly supervised object localization with deformable part-based models",
      "author" : [ "M. Pandey", "S. Lazebnik" ],
      "venue" : "In Computer Vision (ICCV),",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2011
    }, {
      "title" : "An embarrassingly simple approach to zero-shot learning",
      "author" : [ "B. Romera-Paredes", "P. Torr" ],
      "venue" : "In ICML,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2015
    }, {
      "title" : "Detecting avocados to zucchinis: what have we done, and where are we going",
      "author" : [ "O. Russakovsky", "J. Deng", "Z. Huang", "A.C. Berg", "L. Fei-Fei" ],
      "venue" : "In International Conference on Computer Vision (ICCV),",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2013
    }, {
      "title" : "Latent pyramidal regions for recognizing scenes",
      "author" : [ "F. Sadeghi", "M.F. Tappen" ],
      "venue" : "In Proceedings of the 12th European Conference on Computer Vision - Volume Part V,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2012
    }, {
      "title" : "Generating factoid questions with recurrent neural networks: The 30m factoid question-answer corpus",
      "author" : [ "I.V. Serban", "A. García-Durán", "C. Gulcehre", "S. Ahn", "S. Chandar", "A. Courville", "Y. Bengio" ],
      "venue" : "arXiv preprint arXiv:1603.06807,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2016
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "K. Simonyan", "A. Zisserman" ],
      "venue" : null,
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2014
    }, {
      "title" : "Complex embeddings for simple link prediction",
      "author" : [ "T. Trouillon", "J. Welbl", "S. Riedel", "É. Gaussier", "G. Bouchard" ],
      "venue" : "arXiv preprint arXiv:1606.06357,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2016
    }, {
      "title" : "SUN database: Large-scale scene recognition from abbey to zoo",
      "author" : [ "J. Xiao", "J. Hays", "K.A. Ehinger", "A. Oliva", "A. Torralba" ],
      "venue" : "In The Twenty-Third IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2010
    }, {
      "title" : "Learning multi-relational semantics using neural-embedding models",
      "author" : [ "B. Yang", "W.-t. Yih", "X. He", "J. Gao", "L. Deng" ],
      "venue" : "arXiv preprint arXiv:1411.4072,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2014
    }, {
      "title" : "Modeling mutual context of object and human pose in human-object interaction activities",
      "author" : [ "B. Yao", "L. Fei-Fei" ],
      "venue" : "In Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2010
    }, {
      "title" : "Zero-shot learning via semantic similarity embedding",
      "author" : [ "Z. Zhang", "V. Saligrama" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2015
    }, {
      "title" : "Reasoning about object affordances in a knowledge base representation",
      "author" : [ "Y. Zhu", "A. Fathi", "L. Fei-Fei" ],
      "venue" : "In European conference on computer vision,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Moreover, in recent years KGs have been playing an increasingly crucial role in fields such as question answering [4], language modeling [1], and text generation [30].",
      "startOffset" : 114,
      "endOffset" : 117
    }, {
      "referenceID" : 0,
      "context" : "Moreover, in recent years KGs have been playing an increasingly crucial role in fields such as question answering [4], language modeling [1], and text generation [30].",
      "startOffset" : 137,
      "endOffset" : 140
    }, {
      "referenceID" : 29,
      "context" : "Moreover, in recent years KGs have been playing an increasingly crucial role in fields such as question answering [4], language modeling [1], and text generation [30].",
      "startOffset" : 162,
      "endOffset" : 166
    }, {
      "referenceID" : 6,
      "context" : "While IMAGENET [7] and the VISUALGENOME [19] datasets are based on KGs such as WordNet they are predominantly used as either a classification data set as in the case of IMAGENET or to facilitate scene understanding in a single image.",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 18,
      "context" : "While IMAGENET [7] and the VISUALGENOME [19] datasets are based on KGs such as WordNet they are predominantly used as either a classification data set as in the case of IMAGENET or to facilitate scene understanding in a single image.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 4,
      "context" : "Examples are knowledge base factorization and embedding approaches [5, 25, 15, 23] and random-walk based ML models [20, 11].",
      "startOffset" : 67,
      "endOffset" : 82
    }, {
      "referenceID" : 24,
      "context" : "Examples are knowledge base factorization and embedding approaches [5, 25, 15, 23] and random-walk based ML models [20, 11].",
      "startOffset" : 67,
      "endOffset" : 82
    }, {
      "referenceID" : 14,
      "context" : "Examples are knowledge base factorization and embedding approaches [5, 25, 15, 23] and random-walk based ML models [20, 11].",
      "startOffset" : 67,
      "endOffset" : 82
    }, {
      "referenceID" : 22,
      "context" : "Examples are knowledge base factorization and embedding approaches [5, 25, 15, 23] and random-walk based ML models [20, 11].",
      "startOffset" : 67,
      "endOffset" : 82
    }, {
      "referenceID" : 19,
      "context" : "Examples are knowledge base factorization and embedding approaches [5, 25, 15, 23] and random-walk based ML models [20, 11].",
      "startOffset" : 115,
      "endOffset" : 123
    }, {
      "referenceID" : 10,
      "context" : "Examples are knowledge base factorization and embedding approaches [5, 25, 15, 23] and random-walk based ML models [20, 11].",
      "startOffset" : 115,
      "endOffset" : 123
    }, {
      "referenceID" : 9,
      "context" : "Previous work on combining relational and visual data has focused on object detection [10, 12, 28] and scene recognition [8, 26, 29, 33] which are required for more complex visual-relational reasoning.",
      "startOffset" : 86,
      "endOffset" : 98
    }, {
      "referenceID" : 11,
      "context" : "Previous work on combining relational and visual data has focused on object detection [10, 12, 28] and scene recognition [8, 26, 29, 33] which are required for more complex visual-relational reasoning.",
      "startOffset" : 86,
      "endOffset" : 98
    }, {
      "referenceID" : 27,
      "context" : "Previous work on combining relational and visual data has focused on object detection [10, 12, 28] and scene recognition [8, 26, 29, 33] which are required for more complex visual-relational reasoning.",
      "startOffset" : 86,
      "endOffset" : 98
    }, {
      "referenceID" : 7,
      "context" : "Previous work on combining relational and visual data has focused on object detection [10, 12, 28] and scene recognition [8, 26, 29, 33] which are required for more complex visual-relational reasoning.",
      "startOffset" : 121,
      "endOffset" : 136
    }, {
      "referenceID" : 25,
      "context" : "Previous work on combining relational and visual data has focused on object detection [10, 12, 28] and scene recognition [8, 26, 29, 33] which are required for more complex visual-relational reasoning.",
      "startOffset" : 121,
      "endOffset" : 136
    }, {
      "referenceID" : 28,
      "context" : "Previous work on combining relational and visual data has focused on object detection [10, 12, 28] and scene recognition [8, 26, 29, 33] which are required for more complex visual-relational reasoning.",
      "startOffset" : 121,
      "endOffset" : 136
    }, {
      "referenceID" : 32,
      "context" : "Previous work on combining relational and visual data has focused on object detection [10, 12, 28] and scene recognition [8, 26, 29, 33] which are required for more complex visual-relational reasoning.",
      "startOffset" : 121,
      "endOffset" : 136
    }, {
      "referenceID" : 13,
      "context" : "Recent years have witnessed a surge in reasoning about human-object, object-object, and object-attribute relationships [14, 9, 22, 35, 10, 6, 16, 37].",
      "startOffset" : 119,
      "endOffset" : 149
    }, {
      "referenceID" : 8,
      "context" : "Recent years have witnessed a surge in reasoning about human-object, object-object, and object-attribute relationships [14, 9, 22, 35, 10, 6, 16, 37].",
      "startOffset" : 119,
      "endOffset" : 149
    }, {
      "referenceID" : 21,
      "context" : "Recent years have witnessed a surge in reasoning about human-object, object-object, and object-attribute relationships [14, 9, 22, 35, 10, 6, 16, 37].",
      "startOffset" : 119,
      "endOffset" : 149
    }, {
      "referenceID" : 34,
      "context" : "Recent years have witnessed a surge in reasoning about human-object, object-object, and object-attribute relationships [14, 9, 22, 35, 10, 6, 16, 37].",
      "startOffset" : 119,
      "endOffset" : 149
    }, {
      "referenceID" : 9,
      "context" : "Recent years have witnessed a surge in reasoning about human-object, object-object, and object-attribute relationships [14, 9, 22, 35, 10, 6, 16, 37].",
      "startOffset" : 119,
      "endOffset" : 149
    }, {
      "referenceID" : 5,
      "context" : "Recent years have witnessed a surge in reasoning about human-object, object-object, and object-attribute relationships [14, 9, 22, 35, 10, 6, 16, 37].",
      "startOffset" : 119,
      "endOffset" : 149
    }, {
      "referenceID" : 15,
      "context" : "Recent years have witnessed a surge in reasoning about human-object, object-object, and object-attribute relationships [14, 9, 22, 35, 10, 6, 16, 37].",
      "startOffset" : 119,
      "endOffset" : 149
    }, {
      "referenceID" : 36,
      "context" : "Recent years have witnessed a surge in reasoning about human-object, object-object, and object-attribute relationships [14, 9, 22, 35, 10, 6, 16, 37].",
      "startOffset" : 119,
      "endOffset" : 149
    }, {
      "referenceID" : 18,
      "context" : "The VisualGenome project [19] is a knowledge base that integrates language and vision modalities.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 20,
      "context" : "[21] propose a model to detect relationships between objects depicted in one image.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "[18] propose to use the VisualGenome dataset to recover images from simple text queries such as man riding a bike near a mountain.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 26,
      "context" : "[27, 36]) rely on an underlying embedding space, such as attributes, in order to recognize the unseen categories.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 35,
      "context" : "[27, 36]) rely on an underlying embedding space, such as attributes, in order to recognize the unseen categories.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 1,
      "context" : "[2, 21]), leveraging distributional word representations so as to capture a notion of taxonomy and similarity.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 20,
      "context" : "[2, 21]), leveraging distributional word representations so as to capture a notion of taxonomy and similarity.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 2,
      "context" : "IMAGEGRAPH is a visual-relational KG whose relational structure is based on that of FREEBASE [3].",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 22,
      "context" : "More specifically, it is based on FB15K, a subset of FREEBASE, which has been used as a benchmark data set in previous work on KG completion [23].",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 4,
      "context" : "Entities Relations Triples Images |E| |R| Train Valid Test Train Valid Test FB15k [5] 14,951 1,345 483,142 50,000 59,071 0 0 0 IMAGEGRAPH 14,870 1,330 460,406 47,533 56,071 411,306 201,832 216,793",
      "startOffset" : 82,
      "endOffset" : 85
    }, {
      "referenceID" : 4,
      "context" : "Existing KG completion methods use the embedding function g(rawh) = raw T iW where W is a |E| × d matrix, and differ only in their scoring function, that is, in the way that the embedding vectors of the head and tail entities are combined with the parameter vector φr: • Difference (TRANSE[5]): fr(eh, et) = −||eh + φr − et||2 where φr is a d-dimensional vector; • Multiplication (DISTMULT[34]): fr(eh, et) = (eh ∗ et) · φr where ∗ is the element-wise product and φr a d-dimensional vector; • Circular correlation (HOLE[24]): fr(eh, et) = (eh ? et) · φr where [a ? b]k = ∑d−1 i=0 aib(i+k) mod d and φr a d-dimensional vector; and • Concatenation: fr(eh, et) = (eh et) ·φr where is the concatenation operator and φr a 2d-dimensional vector.",
      "startOffset" : 289,
      "endOffset" : 292
    }, {
      "referenceID" : 33,
      "context" : "Existing KG completion methods use the embedding function g(rawh) = raw T iW where W is a |E| × d matrix, and differ only in their scoring function, that is, in the way that the embedding vectors of the head and tail entities are combined with the parameter vector φr: • Difference (TRANSE[5]): fr(eh, et) = −||eh + φr − et||2 where φr is a d-dimensional vector; • Multiplication (DISTMULT[34]): fr(eh, et) = (eh ∗ et) · φr where ∗ is the element-wise product and φr a d-dimensional vector; • Circular correlation (HOLE[24]): fr(eh, et) = (eh ? et) · φr where [a ? b]k = ∑d−1 i=0 aib(i+k) mod d and φr a d-dimensional vector; and • Concatenation: fr(eh, et) = (eh et) ·φr where is the concatenation operator and φr a 2d-dimensional vector.",
      "startOffset" : 389,
      "endOffset" : 393
    }, {
      "referenceID" : 23,
      "context" : "Existing KG completion methods use the embedding function g(rawh) = raw T iW where W is a |E| × d matrix, and differ only in their scoring function, that is, in the way that the embedding vectors of the head and tail entities are combined with the parameter vector φr: • Difference (TRANSE[5]): fr(eh, et) = −||eh + φr − et||2 where φr is a d-dimensional vector; • Multiplication (DISTMULT[34]): fr(eh, et) = (eh ∗ et) · φr where ∗ is the element-wise product and φr a d-dimensional vector; • Circular correlation (HOLE[24]): fr(eh, et) = (eh ? et) · φr where [a ? b]k = ∑d−1 i=0 aib(i+k) mod d and φr a d-dimensional vector; and • Concatenation: fr(eh, et) = (eh et) ·φr where is the concatenation operator and φr a 2d-dimensional vector.",
      "startOffset" : 519,
      "endOffset" : 523
    }, {
      "referenceID" : 31,
      "context" : "The training objective is often based on the logistic loss, which has been shown to be superior for most of the composition functions [32],",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 16,
      "context" : "We used CAFFE, a deep learning framework [17] for designing, training, and evaluating the proposed deep representation learning models.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 30,
      "context" : "The embedding function g is based on the VGG16 model introduced in [31].",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 6,
      "context" : "In summary, we pre-trained the VGG16 on the ILSVRC2012 challenge data set derived from IMAGENET [7] and removed the softmax layer of the original VGG16.",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 12,
      "context" : "We initialized the weights of the newly added layers with the Xavier method [13].",
      "startOffset" : 76,
      "endOffset" : 80
    } ],
    "year" : 2017,
    "abstractText" : "Much progress has been made towards the goal of developing ML systems that are able to recognize and interpret visual scenes. With this paper, we propose query answering in visual-relational knowledge graphs (KGs) as a novel and important reasoning problem. A visual-relational KG is a KG whose entities are associated with image data. We introduce IMAGEGRAPH, a publicly available KG with 1330 relation types, 14,870 entities, and 829,931 images. Visual-relational KGs naturally lead to several novel query types treating images as first-class citizens. We approach the query answering problems by combining ideas from the areas of KG embedding learning and deep learning for computer vision. The resulting ML models can answer queries such as “How are these two unseen images related to each other?\" We also explore a novel zero-shot learning scenario where an image of an entirely new entity is linked to entities of an existing visual-relational KG. An extensive set of experiments shows that the proposed deep neural networks are able to answer the visual-relational queries efficiently and accurately.",
    "creator" : "LaTeX with hyperref package"
  }
}