{
  "name" : "1610.07149.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Two are Better than One: An Ensemble of Retrieval- and Generation-Based Dialog Systems",
    "authors" : [ "Yiping Song", "Rui Yan", "Xiang Li", "Dongyan Zhao", "Ming Zhang" ],
    "emails" : [ "}@pku.edu.cn" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Automatic dialog/conversation systems have served humans for a long time in various fields, ranging from train routing [2] to museum guiding [4]. In the above scenarios, the dialogs are domain-specific, and a typical approach to such in-domain systems is by human engineering, for example, using manually constructed ontologies [23], natural language templates [20], and even predefined dialog states [25].\nRecently, researchers have paid increasing attention to open-domain, chatbot-style human-computer conversation, because of its important commercial applications, and because it tackles the real challenges of natural language understanding and generation [6, 16, 18]. For open-domain dialogs, rules and temples would probably fail as we can hardly handle the great diversity of dialog topics and natural language sentences. With the increasing number of human-human conversation utterances available on the Internet, previous studies have developed data-oriented approaches in the open domain, which can be roughly categorized into two groups: retrieval systems and generative systems.\nWhen a user issues an utterance (called a query), retrieval systems search for a most similar query in a massive database (which consists of large numbers of query-reply pairs), and respond to the user with the corresponding reply [6,7]. Through information retrieval, however, we cannot obtain new utterances, that is, all replies have to appear in the database. Also, the ranking of candidate replies is usually judged by surface forms (e.g., word overlaps, tf·idf features) and hardly addresses the real semantics of natural languages.\nGenerative dialog systems, on the other hand, can synthesize a new sentence as the reply by language models [16, 18, 19]. Typically, a recurrent neural network (RNN) captures the query’s semantics with one or a few distributed, real-valued vectors (also known as embeddings); another RNN decodes the query embeddings to a reply. Deep neural networks allow complicated interaction by multiple non-linear transformations; RNNs are further suitable for modeling time-series data (e.g., a sequence of words) especially when enhanced with long\nar X\niv :1\n61 0.\n07 14\n9v 1\n[ cs\n.C L\n] 2\n3 O\nRetrievalbased  dialog system Postreranking Generative dialog system\nshort term memory (LSTM) or gated recurrent units (GRUs). Despite these, RNN also has its own weakness when applied to dialog systems: the generated sentence tends to be short, universal, and meaningless, for example, “I don’t know” [8] or “something” [16]. This is probably because chatbot-like dialogs are highly diversified and a query may not convey sufficient information for the reply. Even though such universal utterances may be suited in certain dialog context, they make users feel boring and lose interest, and thus are not desirable in real applications.\nIn this paper, we are curious if we can combine the above two streams of approaches for open-domain conversation. To this end, we propose an ensemble of retrieval and generative dialog systems. Given a userissued query, we first obtain a candidate reply by information retrieval from a large database. The query, along with the candidate reply, is then fed to an utterance generator based on the “bi-sequence to sequence” (biseq2seq) model [30]. Such sequence generator takes into consideration the information contained in not only the query but also the retrieved reply; hence, it alleviates the low-substance problem and can synthesize replies that are more meaningful. After that we use the scorer in the retrieval system again for post-reranking. This step can filter out less relevant retrieved replies or meaningless generated ones. The higher ranked candidate (either retrieved or generated) is returned to the user as the reply.\nFrom the above process, we see that the retrieval and generative systems are integrated by two mechanisms: (1) The retrieved candidate is fed to the sequence generator to mitigate the “low-substance” problem; (2) The post-reranker can make better use of both the retrieved candidate and the generated utterance. In this sense, we call our overall approach an ensemble in this paper. To the best of our knowledge, we are the first to combine retrieval and generative models for open-domain conversation.\nExperimental results show that our ensemble model consistently outperforms each single component in terms of several subjective and objective metrics, and that both retrieval and generative methods contribute an important portion to the overall approach. This also verifies the rationale for building model ensembles for dialog systems."
    }, {
      "heading" : "2 The Proposed Model Ensemble",
      "text" : ""
    }, {
      "heading" : "2.1 Overview",
      "text" : "Figure 1 depicts the overall framework of our proposed ensemble of retrieval and generative dialog systems. It mainly consists of the following components.\n• When a user sends a query utterance q, our approach utilizes a state-of-the-practice information retrieval system to search for a query-reply pair 〈q∗, r∗〉 that best matches the user-issued query q. The corresponding r∗ is retrieved as a candidate reply.\n• Then a biseq2seq model takes the original query q and the retrieved candidate reply r∗ as input,\neach sequence being transformed to a fixed-size vector. These two vectors are concatenated and linearly transformed as the initial state of the decoder, which generates a new utterance r+ as another candidate reply.\n• Finally, we use a reranker (which is a part of the retrieval system) to select either r∗ or r+ as the ultimate response to the original query q.\nIn the rest of this section, we describe each component in detail."
    }, {
      "heading" : "2.2 Retrieval-Based Dialog System",
      "text" : "Information retrieval is among prevailing techniques for open-domain, chatbot-style human-computer conversation [6, 7].\nWe utilize a state-of-the-practice retrieval system with extensive manual engineering and on a basis of tens of millions of existing human-human utterance pairs. Basically, it works in a two-step retrieval-and-ranking strategy, similar to the Lucene1 and Solr2 systems.\nFirst, a user-issued utterance is treated as bag-of-words features with stop-words being removed. After querying it in a knowledge base, we obtain a list containing up to 1000 query-reply pairs 〈q∗, r∗〉, whose queries share most words as the input query q. This step retrieves coarse-grained candidates efficiently, which is accomplished by an inversed index.\nThen, we measure the relatedness between the query q and each 〈q∗, r∗〉 pair in a fine-grained fashion. In our system, both q-q∗ and q-r∗ relevance scores are considered. A classifier judges whether q matches q∗ and r∗; its confidence degree is used as the scorer. We have tens of features, and several important ones include word overlap ratio, the cosine measure of a pretrained topic model coefficients, and the cosine measures of word embedding vectors. (Details are beyond the scope of this paper; any well-designed retrieval system might fit into our framework.)\nIn this way, we obtain a query-reply pair 〈q∗, r∗〉 that best matches the original query q; the corresponding utterance r∗ is considered as a candidate reply retrieved from the database.\n2.3 The biseq2seq Utterance Generator Using neural networks to build end-to-end trainable dialog systems has become a new research trend in the past year. A generative dialog system can synthesize new utterances, which is complementary to retrieval-based methods.\nTypically, an encoder-decoder architecture is applied to encode a query as vectors and to decode the vectors to a reply utterance. With recurrent neural networks (RNNs) as the encoder and decoder, such architecture is also known as a seq2seq model, which has wide applications in neural machine translation [21], abstractive summarization [15], etc. That being said, previous studies indicate seq2seq has its own shortcoming for dialog systems. [12] suggests that, in open-domain conversation systems, the query does not carry sufficient information for the reply; that the seq2seq model thus tends to generate short and meaningless sentences with little substance.\nTo address this problem, we adopt a biseq2seq model, which is proposed in [30] for multi-source machine translation. The biseq2seq model takes into consideration the retrieved reply as a reference in addition to query information (Figure 2). Hence, the generated reply can be not only fluent and logical with respect to the query, but also meaningful as it is enhanced by a retrieved candidate.\nSpecifically, we use an RNN with gated recurrent units (GRUs) for sequence modeling. Let xt be the word\n1http://lucene.apache.org 2http://lucene.apache.org/solr\nembeddings of the time step t and ht−1 be the previous hidden state of RNN. We have\nrt = σ(Wrxt + Urht−1 + br) (1) zt = σ(Wzxt + Urht−1 + bz) (2)\nh̃t = tanh ( Wxxt + Ux(rt ◦ ht−1) ) (3)\nht = (1− zt) ◦ ht−1 + zt ◦ h̃t (4)\nwhere rt and zt are known as gates, W ’s and b’s are parameters, and “◦” refers to element-wise product. After two RNNs go through q and r∗, respectively, we obtain two vectors capturing their meanings. We denote them as bold letters q and r∗, which are concatenated as [q; r∗] and linearly transformed before being fed to the decoder as the initial state.\nDuring reply generation, we also use GRU-RNN, given by Equations 1–4. But at each time step, a softmax layer outputs the probability that a word would occur in the next step, i.e.,\np(wi|ht) = exp\n{ W>i ht } + b∑\nj exp { W>j ht + b } (5) where Wi is the i-th row of the output weight matrix (corresponding to wi) and b is a bias term.\nNotice that we assign different sets of parameters—indicated by three colors in Figure 2—for the two encoders (q and r∗) and the decoder (r+). This treatment is because the RNNs’ semantics differ significantly from one another (even between the two encoders)."
    }, {
      "heading" : "2.4 Post-Reranking",
      "text" : "Now that we have a retrieved candidate reply r∗ as well as a generated one r+, we select one as the final reply by the q-r scorer in the retrieval-based dialog system (described in previous sections and not repeated here).\nUsing manually engineered features, this step can eliminate either meaningless short replies that are unfortunately generated by biseq2seq or less relevant replies given by the retrieval system. We call this a post-reranker in our model ensemble."
    }, {
      "heading" : "2.5 Training",
      "text" : "We train each component separately because the retrieval part is not end-to-end learnable.\nIn the retrieval system, we use the classifier’s confidence as the relevance score. The training set consists of 10k samples, which are either in the original human-human utterance pairs or generated by negative sampling. We made efforts to collect binary labels from a crowd-sourcing platform, indicating whether a query is relevant to another query and whether it is relevant to a particular reply. We find using crowd-sourced labels results in better performance than original negative sampling.\nFor biseq2seq, we use human-human utterance pairs 〈q, r〉 as data samples. A retrieved candidate r∗ is also provided as the input when we train the neural network. Standard cross-entropy loss of all words in the reply is applied as the training objective. For a particular training sample whose reply is of length T , the cost is\nJ = − T∑\ni=1 V∑ j=1 t (i) j log y (i) j (6)\nwhere t(i) is the one-hot vector of the next target word, serving as the groundtruth, y is the output probability by softmax, and V is the vocabulary size. We adopt mini-batched AdaDelta [29] for optimization."
    }, {
      "heading" : "3 Evaluation",
      "text" : "In this section, we evaluate our model ensemble on Chinese (language) human-computer conversation. We first describe the datasets and settings. Then we compare our approach with strong baselines."
    }, {
      "heading" : "3.1 Experimental Setup",
      "text" : "Typically, a very large database of query-reply pairs is a premise for a successful retrieval-based conversation system, because the reply must appear in the database. For RNN-based sequence generators, however, it is time-consuming to train with such a large dataset; RNN’s performance may also saturate when we have several million samples.\nTo construct a database for information retrieval, we collected human-human utterances from massive online forums, microblogs, and question-answering communities, such as Sina Weibo,3 Baidu Zhidao,4 and Baidu Tieba.5 We filtered out short and meaningless replies like “. . . ” and “Errr.” In total, the database contains 7 million query-reply pairs for retrieval.\nFor the generation part, we constructed another dataset from various resources in public websites comprising 1,606,741 query-reply pairs. For each query q, we searched for a candidate reply r∗ by the retrieval component and obtained a tuple 〈q, r∗, r〉. As a friendly reminder, q and r∗ are the input of biseq2seq, whose output should approximate r. We randomly selected 100k triples for validation and another 6,741 for testing. The train-val-test split remains the same for all competing models.\nThe biseq2seq then degrades to an utterance autoencoder [9]. Also, the validation and test sets are disjoint with the training set and the database for retrieval, which complies with the convention of machine learning.\n3http://weibo.com 4http://zhidao.baidu.com 5http://tieba.baidu.com\nTo train our neural models, we followed [18] for hyperparameter settings. All embeddings were set to 620-dimensional and hidden states 1000d. We applied AdaDelta with a mini-batch size of 80 and other default hyperparameters for optimization. Chinese word segmentation was performed on all utterances. We kept a same set of 100k words (Chinese terms) for two encoders, but 30k for the decoder due to efficiency concerns. The three neural networks do not share parameters (neither connection weights nor embeddings).\nWe did not tune the above hyperparameters, which were set empirically. The validation set was used for early stop based on the perplexity measure."
    }, {
      "heading" : "3.2 Competing Methods",
      "text" : "We compare our model ensemble with each individual component and provide a thorough ablation test. Listed below are the competing methods in our experiments.\n• Retrieval. A state-of-the-practice dialog system, which is a component of our model ensemble; it is also a strong baseline because of extensive human engineering.\n• seq2seq. An encoder-encoder framework [21], first introduced in [18] for dialog systems.\n• biseq2seq. Another component in our approach, adapted from [30], which is essentially a seq2seq model extended with a retrieved reply.\n• Rerank(Retrieval,seq2seq). Post-reranking between a retrieved candidate and one generated by seq2seq.\n• Rerank(Retrieval,biseq2seq). This is the full proposed model ensemble.\nAll baselines were trained and tuned in a same way as our full model, when applicable, so that the comparison is fair."
    }, {
      "heading" : "3.3 Overall Performance",
      "text" : "We evaluated our approach in terms of both subjective and objective evaluation.\n• Human evaluation, albeit time- and labor-consuming, conforms to the ultimate goal of open-domain conversation systems. We asked three educated volunteers to annotate the results using a common protocol known as pointwise annotation [10, 12, 18]. In other words, annotators were asked to label either “0” (bad), “1” (borderline), or “2” (good) to a query-reply pair. The subjective evaluation was performed in a strict random and blind fashion to rule out human bias.\n• We adopted BLEU-1, BLEU-2, BLEU-3 and BLEU-4 as automatic evaluation. While [11] further aggressively argues that no existing automatic metric is appropriate for open-domain dialogs, they show a slight positive correlation between BLEU-2 and human evaluation in non-technical Twitter domain, which is similar to our scenario. We nonetheless include BLEU scores as expedient objective evaluation, serving as supporting evidence. BLEUs are also used in [8] for model comparison and in [12] for model selection.\nNotice that, automatic metrics were computed on the entire test set, whereas subjective evaluation was based on 79 randomly chosen test samples due to the limitation of human resources available.\nWe present our main results in Table 2. As shown, the retrieval system, which our model ensemble is based on, achieves better performance than RNN-based sequence generation. The result is not consistent with [18], where their RNNs are slightly better than retrieval-based methods. After closely examining their paper, we find that their database is multiple times smaller than ours, which may, along with different features and retrieval methods, explain the phenomenon. This also verifies that the retrieval-based dialog system in our experiment is a strong baseline to compare with.\nCombining the retrieval system and the RNN generator by bi-sequence input and post-reranking, we achieve the highest performance in terms of both human evaluation and BLEU scores. Concretely, our model ensemble outperforms the state-of-the-practice retrieval system by +13.6% averaged human scores, which we believe is a large margin."
    }, {
      "heading" : "3.4 Analysis and Discussion",
      "text" : "Having verified that our model ensemble achieves better performance than all baselines, we are further curious how each gadget contributes to our final system. Specially, we focus on the following research questions.\nRQ1: What is the performance of biseq2seq (the 1© step in Figure 1) in comparison with traditional seq2seq?\nFrom the BLEU scores in Table 2, we see biseq2seq significantly outperforms conventional seq2seq, showing that, if enriched with a retrieved human utterance as a candidate, the encoder-decoder framework can generate much more human-like utterances.\nWe then introduce in Table 3 another measure, the entropy of a sentence, defined as\n− 1 |R| ∑ w∈R log p(w)\nwhereR refers to all replies. Entropy is used in [17] and [12] to measure the serendipity of generated utterances.6 The results in Table 3 confirm that biseq2seq indeed integrates information from the retrieved candidate, so that it alleviates the “low-substance” problem of RNNs and can generate utterances more meaningful than traditional seq2seq. And the statistic result also displays that biseq2seq generates longer sentences than seq2seq approach.\nRQ2: How do the retrieval- and generation-based systems contribute to post-reranking (the 2© step in Figure 1)?\nWe plot in Figure 3 the percentage by which the post-raranker chooses a retrieved candidate or a generated one. In the retrieval-and-seq2seq ensemble (Figure 3a), 54.65% retrieved results and 45.35% generated ones are selected. In retrieval-and-biseq2seq ensemble, the percentage becomes 44.77% vs. 55.23%. The trend further indicates that biseq2seq is better than seq2seq (at least) from the reranker’s point of view. More\n6Notice that, the entropy of retrieved replies is not a fair metric to compare in Table 3, because the retrieval system has filtered out short, meaningless utterances in advance by surface statistics (e.g., length). We nevertheless report the result here out of curiosity: its entropy is 9.507, which is even higher than groundtruth.\n(a)\n(b)\nimportantly, as the percentages are close to 50%, both the retrieval system and the generation system contribute a significant portion to our final ensemble.\nRQ3: Do we obtain further gain by combining the two gadgets 1© and 2© in Figure 1? We would also like to verify if the combination of biseq2seq and post-reranking mechanisms will yield further gain in our ensemble. To test this, we compare the full model Rerank(Retrieval,biseq2seq) with an ensemble that uses traditional seq2seq, i.e., Rerank(Retrieval,seq2seq). As indicated in Table 2, even with the post-reranking mechanism, the ensemble with underlying biseq2seq still outperforms the one with seq2seq. Likewise, Rerank(Retrieval,biseq2seq) outperforms both Retrieval and biseq2seq. These results are consistent in terms of all metrics except a BLEU-4 score.\nThrough the above ablation tests, we conclude that both gadgets (biseq2seq and post-reranking) play a role in our ensemble when we combine the retrieval and the generative systems."
    }, {
      "heading" : "3.5 Case Study",
      "text" : "Table 4 presents two examples of our ensemble and its “base” models. We see that biseq2seq is indeed influenced by the retrieved candidates. As opposed to traditional seq2seq, several content words in the retrieved replies (e.g., crush) also appear in biseq2seq’s output, making the utterances more meaningful. The\npost-reranker also chooses a more appropriate candidate as the reply."
    }, {
      "heading" : "4 Related Work",
      "text" : "In early years, researchers mainly focus on domain-specific dialog systems, e.g., train routing [2], movie information [1], and human tutoring [5]. Typically, a pre-constructed ontology defines a finite set of slots and values, for example, cuisine, location, and price range in a food service dialog system; during human-computer interaction, a state tracker fills plausible values to each slot from user input, and recommend the restaurant that best meets the user’s requirement [13, 24, 26].\nIn the open domain, however, such slot-filling approaches would probably fail because of the diversity of topics and natural language utterances. [6] applies information retrieval techniques to search for related queries and replies. [7] and [28] use both shallow hand-crafted features and deep neural networks for matching. [10] proposes a random walk-style algorithm to rank candidate replies. In addition, their model can introduce additional content (related entities in the dialog context) by searching a knowledge base when a stalemate occurs during human-computer conversation.\nGenerative dialog systems have recently attracted increasing attention in the NLP community. [14] formulates query-reply transformation as a phrase-based machine translation. Since the last year, the renewed prosperity of neural networks witnesses an emerging trend in using RNN for dialog systems [16, 18, 19, 22]. However, a known issue with RNN is that it prefers to generate short, meaningless utterances. [8] proposes a mutual information objective in contrast to the conventional maximum likelihood criterion. [12] and [27] introduce additional content (either the most mutually informative word or topic information) to the reply generator. [17] applies a variational encoder to capture query information as a distribution, from which a random vector is sampled for reply generation.\nTo the best of our knowledge, we are the first to combine retrieval-based and generation-based dialog systems. The use of biseq2seq and post-reranking is also a new insight of this paper."
    }, {
      "heading" : "5 Conclusion and Future Work",
      "text" : "In this paper, we propose a novel ensemble of retrieval-based and generation-based open-domain dialog systems. The retrieval part searches a best-match candidate reply, which is, along with the original query, fed to an RNN-based biseq2seq reply generator. The generated utterance is fed back as a new candidate to the retrieval system for post-reranking. Experimental results show that our ensemble outperforms its underlying retrieval system and generation system by a large margin. In addition, the ablation test demonstrates both the biseq2seq and post-reranking mechanisms play an important role in the ensemble.\nOur research also points out several promising directions for future work, for example, developing new mechanisms of combining retrieval and generative dialog systems, as well as incorporating other data-driven approaches to human-computer conversation."
    } ],
    "references" : [ {
      "title" : "MIMIC: An adaptive mixed initiative spoken dialogue system for information queries",
      "author" : [ "Jennifer Chu-Carroll" ],
      "venue" : "In Proc. Conf. Applied Natural Language Processing,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2000
    }, {
      "title" : "Trains-95: Towards a mixed-initiative planning assistant",
      "author" : [ "G. Ferguson", "J. Allen", "B. Miller" ],
      "venue" : "In AIPS,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1996
    }, {
      "title" : "Measuring nominal scale agreement among many raters",
      "author" : [ "Joseph L Fleiss" ],
      "venue" : "Psychological Bulletin,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1971
    }, {
      "title" : "Engagement driven topic selection for an informationgiving agent",
      "author" : [ "Nadine Glas", "Ken Prepin", "Catherine Pelachaud" ],
      "venue" : "In Proc. Workshop on the Semantics and Pragmatics of Dialogue,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2015
    }, {
      "title" : "AutoTutor: An intelligent tutoring system with mixedinitiative dialogue",
      "author" : [ "A. Graesser", "P. Chipman", "B. Haynes", "A. Olney" ],
      "venue" : "IEEE Trans. Education,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2005
    }, {
      "title" : "Cobot in LambdaMOO: A social statistics agent",
      "author" : [ "Charles Lee Isbell", "Michael Kearns", "Dave Kormann", "Satinder Singh", "Peter Stone" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2000
    }, {
      "title" : "An information retrieval approach to short text conversation",
      "author" : [ "Zongcheng Ji", "Zhengdong Lu", "Hang Li" ],
      "venue" : "arXiv preprint arXiv:1408.6988,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2014
    }, {
      "title" : "A diversity-promoting objective function for neural conversation models",
      "author" : [ "Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan" ],
      "venue" : "In NAACL-HLT,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2016
    }, {
      "title" : "A hierarchical neural autoencoder for paragraphs and documents",
      "author" : [ "Jiwei Li", "Thang Luong", "Dan Jurafsky" ],
      "venue" : "In ACL-IJCNLP,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2015
    }, {
      "title" : "StalemateBreaker: A proactive content-introducing approach to automatic human-computer conversation",
      "author" : [ "Xiang Li", "Lili Mou", "Rui Yan", "Ming Zhang" ],
      "venue" : "In IJCAI,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2016
    }, {
      "title" : "How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation",
      "author" : [ "Chia-Wei Liu", "Ryan Lowe", "Iulian V Serban", "Michael Noseworthy", "Laurent Charlin", "Joelle Pineau" ],
      "venue" : "In EMNLP (to appear),",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2016
    }, {
      "title" : "Sequence to backward and forward sequences: A content-introducing approach to generative short-text conversation",
      "author" : [ "Lili Mou", "Yiping Song", "Rui Yan", "Ge Li", "Lu Zhang", "Zhi Jin" ],
      "venue" : "arXiv preprint arXiv:1607.00970,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2016
    }, {
      "title" : "Multi-domain dialog state tracking using recurrent neural networks",
      "author" : [ "Nikola Mrkšić", "Diarmuid Ó Séaghdha", "Blaise Thomson", "Milica Gasic", "Pei-Hao Su", "David Vandyke", "Tsung-Hsien Wen", "Steve Young" ],
      "venue" : "In ACL-IJCNLP,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2015
    }, {
      "title" : "Data-driven response generation in social media",
      "author" : [ "Alan Ritter", "Colin Cherry", "William B Dolan" ],
      "venue" : "In EMNLP,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2011
    }, {
      "title" : "A neural attention model for abstractive sentence summarization",
      "author" : [ "Alexander M. Rush", "Sumit Chopra", "Jason Weston" ],
      "venue" : "In EMNLP,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2015
    }, {
      "title" : "Building endto-end dialogue systems using generative hierarchical neural network models",
      "author" : [ "Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2016
    }, {
      "title" : "A hierarchical latent variable encoder-decoder model for generating dialogues",
      "author" : [ "Iulian Vlad Serban", "Alessandro Sordoni", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1605.06069,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2016
    }, {
      "title" : "Neural responding machine for short-text conversation",
      "author" : [ "Lifeng Shang", "Zhengdong Lu", "Hang Li" ],
      "venue" : "In ACL-IJCNLP,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2015
    }, {
      "title" : "A neural network approach to context-sensitive generation of conversational responses",
      "author" : [ "Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian- Yun Nie", "Jianfeng Gao", "Bill Dolan" ],
      "venue" : "In NAACL-HLT,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2015
    }, {
      "title" : "Continuously learning neural dialogue management",
      "author" : [ "Pei-Hao Su", "Milica Gasic", "Nikola Mrksic", "Lina Rojas-Barahona", "Stefan Ultes", "David Vandyke", "Tsung- Hsien Wen", "Steve Young" ],
      "venue" : "arXiv preprint arXiv:1606.02689,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2016
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le" ],
      "venue" : "In NIPS, pages 3104–3112,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2014
    }, {
      "title" : "A neural conversational model",
      "author" : [ "Oriol Vinyals", "Quoc Le" ],
      "venue" : "arXiv preprint arXiv:1506.05869,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2015
    }, {
      "title" : "Stochastic language generation in dialogue using recurrent neural networks with convolutional sentence reranking",
      "author" : [ "Tsung-Hsien Wen", "Milica Gasic", "Dongho Kim", "Nikola Mrksic", "Pei-Hao Su", "David Vandyke", "Steve Young" ],
      "venue" : "In SIGDIAL,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2015
    }, {
      "title" : "A network-based end-to-end trainable task-oriented dialogue system",
      "author" : [ "Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrksic", "Lina M Rojas-Barahona", "Pei-Hao Su", "Stefan Ultes", "David Vandyke", "Steve Young" ],
      "venue" : "arXiv preprint arXiv:1604.04562,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2016
    }, {
      "title" : "The dialog state tracking challenge",
      "author" : [ "Jason Williams", "Antoine Raux", "Deepak Ramachandran", "Alan Black" ],
      "venue" : "In SIGDIAL,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2013
    }, {
      "title" : "Web-style ranking and SLU combination for dialog state tracking",
      "author" : [ "Jason D Williams" ],
      "venue" : "In SIGDIAL,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2014
    }, {
      "title" : "Topic augmented neural response generation with a joint attention mechanism",
      "author" : [ "Chen Xing", "Wei Wu", "Yu Wu", "Jie Liu", "Yalou Huang", "Ming Zhou", "Wei-Ying Ma" ],
      "venue" : "arXiv preprint arXiv:1606.08340,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2016
    }, {
      "title" : "Learning to respond with deep neural networks for retrieval-based human-computer conversation system",
      "author" : [ "Rui Yan", "Yiping Song", "Hua Wu" ],
      "venue" : "In SIGIR,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2016
    }, {
      "title" : "AdaDelta: An adaptive learning rate method",
      "author" : [ "Matthew D Zeiler" ],
      "venue" : "arXiv preprint arXiv:1212.5701,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2012
    }, {
      "title" : "Multi-source neural translation",
      "author" : [ "Barret Zoph", "Kevin Knight" ],
      "venue" : "In NAACL-ACL,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Automatic dialog/conversation systems have served humans for a long time in various fields, ranging from train routing [2] to museum guiding [4].",
      "startOffset" : 119,
      "endOffset" : 122
    }, {
      "referenceID" : 3,
      "context" : "Automatic dialog/conversation systems have served humans for a long time in various fields, ranging from train routing [2] to museum guiding [4].",
      "startOffset" : 141,
      "endOffset" : 144
    }, {
      "referenceID" : 22,
      "context" : "In the above scenarios, the dialogs are domain-specific, and a typical approach to such in-domain systems is by human engineering, for example, using manually constructed ontologies [23], natural language templates [20], and even predefined dialog states [25].",
      "startOffset" : 182,
      "endOffset" : 186
    }, {
      "referenceID" : 19,
      "context" : "In the above scenarios, the dialogs are domain-specific, and a typical approach to such in-domain systems is by human engineering, for example, using manually constructed ontologies [23], natural language templates [20], and even predefined dialog states [25].",
      "startOffset" : 215,
      "endOffset" : 219
    }, {
      "referenceID" : 24,
      "context" : "In the above scenarios, the dialogs are domain-specific, and a typical approach to such in-domain systems is by human engineering, for example, using manually constructed ontologies [23], natural language templates [20], and even predefined dialog states [25].",
      "startOffset" : 255,
      "endOffset" : 259
    }, {
      "referenceID" : 5,
      "context" : "Recently, researchers have paid increasing attention to open-domain, chatbot-style human-computer conversation, because of its important commercial applications, and because it tackles the real challenges of natural language understanding and generation [6, 16, 18].",
      "startOffset" : 254,
      "endOffset" : 265
    }, {
      "referenceID" : 15,
      "context" : "Recently, researchers have paid increasing attention to open-domain, chatbot-style human-computer conversation, because of its important commercial applications, and because it tackles the real challenges of natural language understanding and generation [6, 16, 18].",
      "startOffset" : 254,
      "endOffset" : 265
    }, {
      "referenceID" : 17,
      "context" : "Recently, researchers have paid increasing attention to open-domain, chatbot-style human-computer conversation, because of its important commercial applications, and because it tackles the real challenges of natural language understanding and generation [6, 16, 18].",
      "startOffset" : 254,
      "endOffset" : 265
    }, {
      "referenceID" : 5,
      "context" : "When a user issues an utterance (called a query), retrieval systems search for a most similar query in a massive database (which consists of large numbers of query-reply pairs), and respond to the user with the corresponding reply [6,7].",
      "startOffset" : 231,
      "endOffset" : 236
    }, {
      "referenceID" : 6,
      "context" : "When a user issues an utterance (called a query), retrieval systems search for a most similar query in a massive database (which consists of large numbers of query-reply pairs), and respond to the user with the corresponding reply [6,7].",
      "startOffset" : 231,
      "endOffset" : 236
    }, {
      "referenceID" : 15,
      "context" : "Generative dialog systems, on the other hand, can synthesize a new sentence as the reply by language models [16, 18, 19].",
      "startOffset" : 108,
      "endOffset" : 120
    }, {
      "referenceID" : 17,
      "context" : "Generative dialog systems, on the other hand, can synthesize a new sentence as the reply by language models [16, 18, 19].",
      "startOffset" : 108,
      "endOffset" : 120
    }, {
      "referenceID" : 18,
      "context" : "Generative dialog systems, on the other hand, can synthesize a new sentence as the reply by language models [16, 18, 19].",
      "startOffset" : 108,
      "endOffset" : 120
    }, {
      "referenceID" : 7,
      "context" : "Despite these, RNN also has its own weakness when applied to dialog systems: the generated sentence tends to be short, universal, and meaningless, for example, “I don’t know” [8] or “something” [16].",
      "startOffset" : 175,
      "endOffset" : 178
    }, {
      "referenceID" : 15,
      "context" : "Despite these, RNN also has its own weakness when applied to dialog systems: the generated sentence tends to be short, universal, and meaningless, for example, “I don’t know” [8] or “something” [16].",
      "startOffset" : 194,
      "endOffset" : 198
    }, {
      "referenceID" : 29,
      "context" : "The query, along with the candidate reply, is then fed to an utterance generator based on the “bi-sequence to sequence” (biseq2seq) model [30].",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 5,
      "context" : "2 Retrieval-Based Dialog System Information retrieval is among prevailing techniques for open-domain, chatbot-style human-computer conversation [6, 7].",
      "startOffset" : 144,
      "endOffset" : 150
    }, {
      "referenceID" : 6,
      "context" : "2 Retrieval-Based Dialog System Information retrieval is among prevailing techniques for open-domain, chatbot-style human-computer conversation [6, 7].",
      "startOffset" : 144,
      "endOffset" : 150
    }, {
      "referenceID" : 20,
      "context" : "With recurrent neural networks (RNNs) as the encoder and decoder, such architecture is also known as a seq2seq model, which has wide applications in neural machine translation [21], abstractive summarization [15], etc.",
      "startOffset" : 176,
      "endOffset" : 180
    }, {
      "referenceID" : 14,
      "context" : "With recurrent neural networks (RNNs) as the encoder and decoder, such architecture is also known as a seq2seq model, which has wide applications in neural machine translation [21], abstractive summarization [15], etc.",
      "startOffset" : 208,
      "endOffset" : 212
    }, {
      "referenceID" : 11,
      "context" : "[12] suggests that, in open-domain conversation systems, the query does not carry sufficient information for the reply; that the seq2seq model thus tends to generate short and meaningless sentences with little substance.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 29,
      "context" : "To address this problem, we adopt a biseq2seq model, which is proposed in [30] for multi-source machine translation.",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 28,
      "context" : "We adopt mini-batched AdaDelta [29] for optimization.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 8,
      "context" : "The biseq2seq then degrades to an utterance autoencoder [9].",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 2,
      "context" : "2824 [3], std = 0.",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 17,
      "context" : "To train our neural models, we followed [18] for hyperparameter settings.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 20,
      "context" : "An encoder-encoder framework [21], first introduced in [18] for dialog systems.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 17,
      "context" : "An encoder-encoder framework [21], first introduced in [18] for dialog systems.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 29,
      "context" : "Another component in our approach, adapted from [30], which is essentially a seq2seq model extended with a retrieved reply.",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 9,
      "context" : "We asked three educated volunteers to annotate the results using a common protocol known as pointwise annotation [10, 12, 18].",
      "startOffset" : 113,
      "endOffset" : 125
    }, {
      "referenceID" : 11,
      "context" : "We asked three educated volunteers to annotate the results using a common protocol known as pointwise annotation [10, 12, 18].",
      "startOffset" : 113,
      "endOffset" : 125
    }, {
      "referenceID" : 17,
      "context" : "We asked three educated volunteers to annotate the results using a common protocol known as pointwise annotation [10, 12, 18].",
      "startOffset" : 113,
      "endOffset" : 125
    }, {
      "referenceID" : 10,
      "context" : "While [11] further aggressively argues that no existing automatic metric is appropriate for open-domain dialogs, they show a slight positive correlation between BLEU-2 and human evaluation in non-technical Twitter domain, which is similar to our scenario.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 7,
      "context" : "BLEUs are also used in [8] for model comparison and in [12] for model selection.",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 11,
      "context" : "BLEUs are also used in [8] for model comparison and in [12] for model selection.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 17,
      "context" : "The result is not consistent with [18], where their RNNs are slightly better than retrieval-based methods.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 16,
      "context" : "Entropy is used in [17] and [12] to measure the serendipity of generated utterances.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 11,
      "context" : "Entropy is used in [17] and [12] to measure the serendipity of generated utterances.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 1,
      "context" : ", train routing [2], movie information [1], and human tutoring [5].",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 0,
      "context" : ", train routing [2], movie information [1], and human tutoring [5].",
      "startOffset" : 39,
      "endOffset" : 42
    }, {
      "referenceID" : 4,
      "context" : ", train routing [2], movie information [1], and human tutoring [5].",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 12,
      "context" : "Typically, a pre-constructed ontology defines a finite set of slots and values, for example, cuisine, location, and price range in a food service dialog system; during human-computer interaction, a state tracker fills plausible values to each slot from user input, and recommend the restaurant that best meets the user’s requirement [13, 24, 26].",
      "startOffset" : 333,
      "endOffset" : 345
    }, {
      "referenceID" : 23,
      "context" : "Typically, a pre-constructed ontology defines a finite set of slots and values, for example, cuisine, location, and price range in a food service dialog system; during human-computer interaction, a state tracker fills plausible values to each slot from user input, and recommend the restaurant that best meets the user’s requirement [13, 24, 26].",
      "startOffset" : 333,
      "endOffset" : 345
    }, {
      "referenceID" : 25,
      "context" : "Typically, a pre-constructed ontology defines a finite set of slots and values, for example, cuisine, location, and price range in a food service dialog system; during human-computer interaction, a state tracker fills plausible values to each slot from user input, and recommend the restaurant that best meets the user’s requirement [13, 24, 26].",
      "startOffset" : 333,
      "endOffset" : 345
    }, {
      "referenceID" : 5,
      "context" : "[6] applies information retrieval techniques to search for related queries and replies.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "[7] and [28] use both shallow hand-crafted features and deep neural networks for matching.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 27,
      "context" : "[7] and [28] use both shallow hand-crafted features and deep neural networks for matching.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 9,
      "context" : "[10] proposes a random walk-style algorithm to rank candidate replies.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "[14] formulates query-reply transformation as a phrase-based machine translation.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "Since the last year, the renewed prosperity of neural networks witnesses an emerging trend in using RNN for dialog systems [16, 18, 19, 22].",
      "startOffset" : 123,
      "endOffset" : 139
    }, {
      "referenceID" : 17,
      "context" : "Since the last year, the renewed prosperity of neural networks witnesses an emerging trend in using RNN for dialog systems [16, 18, 19, 22].",
      "startOffset" : 123,
      "endOffset" : 139
    }, {
      "referenceID" : 18,
      "context" : "Since the last year, the renewed prosperity of neural networks witnesses an emerging trend in using RNN for dialog systems [16, 18, 19, 22].",
      "startOffset" : 123,
      "endOffset" : 139
    }, {
      "referenceID" : 21,
      "context" : "Since the last year, the renewed prosperity of neural networks witnesses an emerging trend in using RNN for dialog systems [16, 18, 19, 22].",
      "startOffset" : 123,
      "endOffset" : 139
    }, {
      "referenceID" : 7,
      "context" : "[8] proposes a mutual information objective in contrast to the conventional maximum likelihood criterion.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 11,
      "context" : "[12] and [27] introduce additional content (either the most mutually informative word or topic information) to the reply generator.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 26,
      "context" : "[12] and [27] introduce additional content (either the most mutually informative word or topic information) to the reply generator.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 16,
      "context" : "[17] applies a variational encoder to capture query information as a distribution, from which a random vector is sampled for reply generation.",
      "startOffset" : 0,
      "endOffset" : 4
    } ],
    "year" : 2016,
    "abstractText" : "Open-domain human-computer conversation has attracted much attention in the field of NLP. Contrary to ruleor template-based domain-specific dialog systems, open-domain conversation usually requires datadriven approaches, which can be roughly divided into two categories: retrieval-based and generation-based systems. Retrieval systems search a user-issued utterance (called a query) in a large database, and return a reply that best matches the query. Generative approaches, typically based on recurrent neural networks (RNNs), can synthesize new replies, but they suffer from the problem of generating short, meaningless utterances. In this paper, we propose a novel ensemble of retrieval-based and generation-based dialog systems in the open domain. In our approach, the retrieved candidate, in addition to the original query, is fed to an RNN-based reply generator, so that the neural model is aware of more information. The generated reply is then fed back as a new candidate for post-reranking. Experimental results show that such ensemble outperforms each single part of it by a large margin.",
    "creator" : "LaTeX with hyperref package"
  }
}