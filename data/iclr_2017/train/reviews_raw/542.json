{"conference": "ICLR 2017 conference submission", "title": "Modular Multitask Reinforcement Learning with Policy Sketches", "abstract": "We describe a framework for multitask deep reinforcement learning guided by policy sketches. Sketches annotate each task with a sequence of named subtasks, providing high-level structural relationships among tasks, but not providing the detailed guidance required by previous work on learning policy abstractions for RL (e.g. intermediate rewards, subtask completion signals, or intrinsic motivations). Our approach associates every subtask with its own modular subpolicy, and jointly optimizes over full task-specific policies by tying parameters across shared subpolicies. This optimization is accomplished via a simple decoupled actor\u2013critic training objective that facilitates learning common behaviors from dissimilar reward functions. We evaluate the effectiveness of our approach on a maze navigation game and a 2-D Minecraft-inspired crafting game. Both games feature extremely sparse rewards that can be obtained only after completing a number of high-level subgoals (e.g. escaping from a sequence of locked rooms or collecting and combining various ingredients in the proper order). Experiments illustrate two main advantages of our approach. First, we outperform standard baselines that learn task-specific or shared monolithic policies. Second, our method naturally induces a library of primitive behaviors that can be recombined to rapidly acquire policies for new tasks.", "reviews": [{"is_meta_review": true, "comments": "The paper presents an approach to learning shared neural representations of temporal abstractions in hierarchical RL, based on actor-critic methods. The approach is illustrated in two tasks: gridworld with objects and a simplified Minecraft problem).  The idea of providing symbolic descriptions of tasks and learning corresponding \"implementations\" is potentially interesting and the empirical results are promising.  However, there are two main drawbacks of the current incarnation of this work.  First, the ideas presented in the paper have all been explored in other work (symbolic specifications, actor-critic, shared representations).  While related work is discussed, it is not really clear what is new here, and what is the main contribution of this work besides providing a new implementation of existing ideas in the context of deep learning. The main contribution if the work needs to be clearly spelled out.  Secondly, the approach presented relies crucially on curriculum learning (this is quite clear from the experiments).  While the authors argue that specifying tasks in simplified language is easy, designing a curriculum may in fact be pretty complicated, depending on the task at hand.  The examples provided are fairly small, and there is no hint of how curriculum can be designed for larger problems. Because the approach is sensitive to the curriculum, this limits the potential utility of the work. It is also unclear if there is a way to provide supervision automatically, instead of doing it based on prior domain knowledge.\nMore minor comments:\n- The experiments are not described in enough detail in the paper. It's great to provide github code, but one needs to explain in the paper why certain choices were made in the task setup (were these optimized? What's this the first thing that worked?) Even with the code, the experiments as described are not reproducible\n- The description of the approach is pretty tangled with the specific algorithmic choices. Can the authors step back and think more generally of how this approach can be formalized?  I think this would help relate it to the prior work more clearly as well.", "IS_META_REVIEW": true}, {"TITLE": "ICLR committee final decision", "OTHER_KEYS": "ICLR 2017 pcs", "comments": "As per all the reviews, the work is clearly promising, but is seen to need additional discussion / formalization / experimental comparison with related work, and stronger demonstrations of the application of this technique.\n Further back-and-forth with the reviewers would have been useful, but there should be enough to go on in terms of directions. This work would benefit from being part of the workshop track.", "IS_META_REVIEW": false, "DATE": "06 Feb 2017", "is_meta_review": false}, {"TITLE": "No questions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "IS_META_REVIEW": false, "DATE": "27 Dec 2016", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Potentially interesting approach but novelty and utility are not clear at this a point", "is_meta_review": false, "comments": "The paper presents an approach to learning shared neural representations of temporal abstractions in hierarchical RL, based on actor-critic methods. The approach is illustrated in two tasks: gridworld with objects and a simplified Minecraft problem).  The idea of providing symbolic descriptions of tasks and learning corresponding \"implementations\" is potentially interesting and the empirical results are promising.  However, there are two main drawbacks of the current incarnation of this work.  First, the ideas presented in the paper have all been explored in other work (symbolic specifications, actor-critic, shared representations).  While related work is discussed, it is not really clear what is new here, and what is the main contribution of this work besides providing a new implementation of existing ideas in the context of deep learning. The main contribution if the work needs to be clearly spelled out.  Secondly, the approach presented relies crucially on curriculum learning (this is quite clear from the experiments).  While the authors argue that specifying tasks in simplified language is easy, designing a curriculum may in fact be pretty complicated, depending on the task at hand.  The examples provided are fairly small, and there is no hint of how curriculum can be designed for larger problems. Because the approach is sensitive to the curriculum, this limits the potential utility of the work. It is also unclear if there is a way to provide supervision automatically, instead of doing it based on prior domain knowledge.\nMore minor comments:\n- The experiments are not described in enough detail in the paper. It's great to provide github code, but one needs to explain in the paper why certain choices were made in the task setup (were these optimized? What's this the first thing that worked?) Even with the code, the experiments as described are not reproducible\n- The description of the approach is pretty tangled with the specific algorithmic choices. Can the authors step back and think more generally of how this approach can be formalized?  I think this would help relate it to the prior work more clearly as well.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "27 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "framework for multiagent hierarchical RL using policy sketches ", "is_meta_review": false, "comments": "This paper studies the problem of abstract hierarchical multiagent RL with policy sketches, high level descriptions of abstract actions. The work is related to much previous work in hierarchical RL, and adds some new elements by using neural implementations of prior work on hierarchical learning and skill representations. \n\nSketches are sequences of high level symbolic labels drawn from some fixed vocabulary, which initially are devoid of any meaning. Eventually the sketches get mapped into real policies and enable policy transfer and temporal abstraction. Learning occurs through a variant of the standard actor critic architecture. \n\nExperiments are provided through a standard game like domain (maze, minecraft etc.). \n\nThe paper as written suffers from two problems. One, the idea of policy sketches is nice, but not sufficiently fleshed out to have any real impact. It would have been useful to see this spelled out in the context of abstract SMDP models to see what they bring to the table. What one gets here is some specialized invocation of this idea in the context of the specific approach proposed here. Second, the experiments are not thorough enough in terms of comparing with all the related work. For example, Ghavamzadeh et al. explored the use of MAXQ like abstractions in the context of mulitagent RL. It would be great to get a more detailed comparison to MAXQ based multiagent RL approaches, where the value function is explicitly decomposed. ", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Review", "is_meta_review": false, "comments": "The paper proposes a new RL architecture that aims at learning policies from sketches i.e sequence of high-level operations to execute for solving a particular task. The model relies on a hierarchical structure where the sub-policy is chosen depending on the current operation to execute in the sketch . The learning algorithm is based on an extension of the actor-critic model for that particular case, and also involves curriculum learning techniques when the task to solve is hard. Experimental results are provided on different learning problems and compared to baseline methods. \n\nThe paper is well-written and very easy to follow. I am not really convinced by the impact of such a paper since the problem solved here can be seen as an option-learning problem with a richer supervision (i.e the sequence of option is given). It thus corresponds to an easier problem with a limited impact. Moreover, I do not really understand to which concrete application this setting corresponds. For example, learning from natural langage instructions is clearly more relevant. So since the model proposed in this article is not a major contribution and shares many common ideas with existing hierarchical reinforcement learning methods,  the paper lacks a strong motivation and/or concrete application. So, the paper only has a marginal interest for the RL community\n\n@pros: \n* Original problem with well design experiments\n* Simple adaptation of the actor-critic method to the problem of learning sub policies\n\n\n@cons:\n* Very simple task that can be seen as a simplification of more complex problems like options discovery, hierarchical RL or learning from instructions\n* No strong underlying applications that could help to 'reinforce' the interest of the approach\n", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "pre-review question", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "is_meta_review": false}, {"TITLE": "Relation to work on programmable HAMs", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "is_meta_review": false}], "SCORE": 4, "authors": "Jacob Andreas, Dan Klein, Sergey Levine", "KEYWORDS": "Learning multitask deep hierarchical policies with guidance from symbolic policy sketches", "accepted": false, "id": ""}
