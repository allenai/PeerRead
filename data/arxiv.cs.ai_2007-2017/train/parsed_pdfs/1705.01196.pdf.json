{
  "name" : "1705.01196.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Navigating Intersections with Autonomous Vehicles using Deep Reinforcement Learning",
    "authors" : [ "David Isele", "Akansel Cosgun", "Kaushik Subramanian", "Kikuo Fujimura" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "I. INTRODUCTION\nOne of the most challenging problems for autonomous vehicles is to handle unsignaled intersections in urban environments. In order to successfully navigate through an intersection, it is necessary to understand vehicle dynamics, interpret the intent of other drivers, and behave predictably so that other drivers can appropriately respond. Learning this behavior requires optimizing multiple conflicting objectives including safety, efficiency, and minimizing the disruption of traffic. Balancing these trade-offs can be challenging even for human drivers: 20% of all accidents occur at intersections [1]. Acquiring the ability to perform optimally at traffic junctions can both extend the abilities of autonomous agents and increase safety through driver assistance when a human driver is in control.\nA number of rule-based strategies have already been applied to intersection handling, including cooperative [2] and heuristic [3] approaches. Cooperative approaches require vehicle-to-vehicle communication and thus not scalable to general intersection handling. The current state of the art is a rule-based method based on time-to-collision (TTC) [4], [5], which is a widely used heuristic as a safety indicator in the automotive industry [6]. Variants of the TTC approach has been used for autonomous driving [7] and the DARPA urban challenge, where hand engineered hierarchical state machines were a popular approach to handle intersections [8], [9]. TTC is currently the method we employ on our autonomous vehicle [10].\nWhile TTC has many benefits - it is relatively reliable, generates behavior that is easy to interpret, and can be tuned to reach a high level of safety - it also has limitations. First, the TTC models assume constant velocity, which ignores nearly all information concerning driver intent. This\n1David Isele is with The University of Pennsylvania 2Akansel Cosgun and Kikuo Fujimura are with Honda Research Institute 3Kaushik Subramanian is with the Georgia Institute of Technology\nis problematic: in the DARPA Urban Challenge, one reason behind a collision between two autonomous cars was “failure to anticipate vehicle intent” [11]. Second, in public roads the often unpredictable behavior of human drivers complicates the use of rule-based algorithms. Third, in many cases an agent using TTC may be overly cautious, creating unnecessary delays. These reasons motivate our investigation of machine-learning based approaches for intersection handling in autonomous vehicles.\nA number of machine learning based approaches have been used for the intersection handling, such as imitation learning, online planning and offline learning. In imitation learning, the policy is learned from a human drivers [12], however this policy does not offer a solution if the agent finds itself in a state that is not part of the training data. Online planners compute the best action to take by simulating the future states from the current time step. Online planners based on partially observable Monte Carlo Planning (POMCP) have been shown to handle intersections [13], but rely on the existence of an accurate generative model. Offline learning tackles the intersection problem, often by using Markov Decision Processes (MDP) in the back-end [14], [15].\nIn this paper, we use an offline learning method based on Deep Reinforcement Learning (Deep RL), which to our knowledge hasn’t previously been studied for the intersection problem. Given the recent success of Deep Learning on a variety of control problems, [16], [17], [18] we are interested in evaluating the effectiveness of Deep RL in the domain of intersection handling. As seen in Figure 6, the autonomous vehicle is at an unsignaled intersection where the traffic is flowing in both directions. The autonomous vehicle starts from an initially stopped position. The path is assumed to be given by a higher level process, and the proposed approach is tasked to determine the acceleration profile along the path.\nar X\niv :1\n70 5.\n01 19\n6v 1\n[ cs\n.A I]\n2 M\nay 2\n01 7\nThe contributions of this paper are three-fold. The first contribution is the novel way we combine several recent deep learning techniques in order to boost learning speed and improve performance. We use dynamic frame skipping [19] to expedite the system learning repeated actions, and prioritized reply [20] to ensure the network balances learning both positive and negative cases. Additionally, we take advantage of the off-policy nature imposed by experience replay learning to calculate and train on the full n-step return [21] which we found greatly reduces the learning time of DQNs.\nThe second contribution is our analysis of how well the DQN performs as compared to TTC in five different intersection simulation scenarios, considering safety metrics such as collision rate, and efficiency metrics such as success rate, total time to complete the intersection, and disruption of traffic. We further offer insight on the types of scenarios where DQN could be preferable over TTC. This analysis is of interest beyond the specific choice of DQNs as a base learner as it identifies scenarios where TTC can be improved and suggests methods that could improve it.\nThe third contribution is the analysis on how well the trained DQN policies transfer to different scenarios. To ensure safety and reliability, any learning system must generalize well to out-of-sample data. To fully appreciate the strengths and failings of our learned networks we run each network on intersection scenarios it did not train on. This analysis allows us to identify when we can and cannot expect a network to succeed in novel situations.\nThe rest of the paper is organized as follows: In Chapter II we describe the DQN formulation. In Chapter III, we describe the experimental setup and simulation environment. In Chapter IV we discuss the TTC and DQN results, and investigate how DQN policies transfer to different scenarios. In Chapter V, we draw our conclusions pertaining this work."
    }, {
      "heading" : "II. APPROACH",
      "text" : "We view intersection handling as a reinforcement learning problem, and use a Deep Q Network (DQN) to learn the state action value Q-function."
    }, {
      "heading" : "A. Reinforcement Learning",
      "text" : "In the reinforcement learning framework, at time t an agent in state st takes an action at according to the policy π . The agent transitions to the state st+1, and receives a reward rt . The sequence of states, actions, and rewards is given as a trajectory τ = {(s1,a1,r1), . . . ,(st ,at ,rt)} over a horizon T .\nA reinforcement learning task is typically formulated as a Markov Decision Process (MDP) 〈S,A,P,R,γ〉, where S is the set of states, and A is the set of actions that the agent may execute. MDPs follow the Markov assumption that the probability of transitioning to a new state given the current state and action is independent of all previous states and actions p(st+1|st ,at , . . . ,s0,a0) = p(st+1|st ,at). The state transition probability P : S×S→ [0,1] describes the systems dynamics, the reward function R : S×A×S→ R gives the real valued reward for a given time step, and γ ∈ (0,1] is a\ndiscount factor that adds preference for earlier rewards and provides stability in the case of infinite time horizons.\nThe goal of reinforcement learning is to maximize the expected return R = ∑Tt=0 γ trt over a sequence of actions. The expected return for a specific state can similarly be represented Rt = ∑Tk=0 γ trt+k. We use Q-learning to perform this optimization."
    }, {
      "heading" : "B. Q-learning",
      "text" : "In Q-learning [22], the action value function Qπ(s,a) is the expected return E[Rt |st = s,a] for a state-action pair following a policy π . Given an optimal value function Q∗(s,a) the optimal policy can be inferred by selecting the action with maximum value maxa Q∗(s,a) at every time step.\nIn Deep Q-learning [16], the optimal value function is approximated with a neural network Q∗(s,a) ≈ Q(s,a;θ) with parameters θ . The action value function is learned by iteratively minimizing the error between the expected return and the state-action value predicted by the network.\nL(θ) = ( E[Rt |st = s,a]−Q(s,a;θ) )2 (1)\nSince in practice the true return is not known, it is often approximated by the one step return\nE[Rt |st = s,a]≈ rt + γ max at+1 Q(st+1,at+1;θ) (2)\nUsing the one step return can make learning slow since many updates are required to propagate the reward to the appropriate preceding states and actions. One way to make learning more efficient is to use n-step return[21] E[Rt |st = s,a] ≈ rt + γrt+1 + · · ·+ γn−1rt+n−1 + γn maxat+n Q(st+n,at+n;θ).\nDuring learning, an ε-greedy policy is followed by selecting a random action with probability ε to promote exploration and otherwise greedily selecting the best action maxaQ(s,a;θ) according to the current network. In order to improve the effectiveness of the random exploration we make use of dynamic frame skipping."
    }, {
      "heading" : "C. Dynamic Frame Skipping",
      "text" : "Frequently the same repeated actions is required over several time steps. It was recently shown that allowing an agent to select actions over extended time periods improves the learning time of an agent [19]. For example, rather than having to explore through trial and error and build up over a series of learning steps that eight time steps is the appropriate amount of time an agent should wait for a car to pass, the agent need only discover that a ”wait eight steps” action is appropriate. Dynamic frame skipping can viewed as a simplified version of options [23] which is recently starting to be explored by the Deep RL community. [24], [25], [26]."
    }, {
      "heading" : "D. Prioritized Experience Replay",
      "text" : "In order to break correlations between sequential steps of the agent, experience replay is used [27]. An experience replay buffer stores previous trajectories which can be sampled during learning. A benefit of using experience replay is that important sequences which happen less frequently\ncan be preferentially sampled [20]. We use the simplified approach proposed by Jaderberg et al. [24] which avoids the computation of a rank list and instead samples to balancing reward across trajectories."
    }, {
      "heading" : "E. State-Action Representations",
      "text" : "Autonomous vehicles use a suite of sensors, and allow for planning at multiple levels of abstraction. This allows for a wide variety of state and action representations. An exploratory search of representations showed the selection of the representation had a significant impact on the agent’s ability to learn. In this paper we present the two representations we found to work well. Sequential Actions In sequential action representation the desired path is provided to the agent and the agent determines to accelerate, decelerate, or maintain constant velocity at every point in time along the desired path.\nA birds eye view of space is discretized into a grid in Cartesian coordinates relative to the car’s reference frame. This is a representation that could easily be constructed from a car’s LiDAR scans. Every car in the space is represented by it’s heading angle, velocity, and calculated time to collision. The heading angle, velocity, and calculated time to collision are all represented as real values. Time-to-Go In the Time-to-Go representation the desired path is provided to the agent and the agent determines the timing of departure through a sequences of actions to wait or go. Every wait action is followed by another wait or go decision, meaning every trajectory is a series of wait decisions terminating in a go decision, and the agent is not allowed to wait after the go action has been selected.\nA birds eye view of space is discretized into a grid in Cartesian coordinates. Every car in the space is represented by it’s heading angle, velocity, and bias term.\nThe sequential scenario allows for more complex behaviors: the agent could potential slow down half way through the intersection and wait for on coming traffic to pass. The Time-to-Go representation more closely compares to TTC, placing importance on the time of departure. By analyzing the sequential action representation we are able to observe if their is a benefit to allowing more complex behaviors. The Time-to-Go representation focuses on the departure time, allowing us to specifically probe how changes in departure time can affect performance."
    }, {
      "heading" : "III. EXPERIMENTS",
      "text" : "We train two different DQNs (Sequential Actions and Time-to-Go) on a variety of intersection scenarios and com-\npare the performance against the heuristic Time-to-Collision (TTC) algorithm."
    }, {
      "heading" : "A. Time-To-Collision (TTC) Policy",
      "text" : "The TTC policy serves as a baseline in our analysis. It uses a single threshold to decide when to cross. Below is an explanation of the algorithm. Consider an imaginary line emanating from the front of the ego vehicle, aligned with the longitudinal axis. We calculate the TTC with another vehicle as the time it takes for the vehicle to reach this imaginary line, assuming it will travel with constant speed. Among all the vehicles in the scene, we consider the one with the minimum TTC value. If this value exceeds the TTC threshold, then the ego vehicle starts the crossing phase and follows the Intelligent Driver Model (IDM) [28] until the goal is reached. If it doesn’t exceed the threshold, the ego car continues to wait."
    }, {
      "heading" : "B. Experimental setup",
      "text" : "Experiments were run using the Sumo simulator [29], which is an open source traffic simulation package. This package allows users to model road networks, road signs, traffic lights, a variety of vehicles (including public transportation), pedestrians to simulate traffic conditions in different types of scenarios. Importantly for the purpose of testing and evaluation of autonomous vehicle systems, Sumo provides tools that facilitate online interaction and vehicle control. For any traffic scenario, users can have control over a vehicle’s position, velocity, acceleration, steering direction and can simulate motion using basic kinematics models. Traffic scenarios like multi-lane intersections can be setup by defining the road network (lanes and intersections) along with specifications that control traffic conditions. To simulate traffic, users have control over the types of vehicles, road paths, vehicle density, departure times and so on. Traffic cars follow IDM to control their motion. In Sumo, randomness is simulated by varying the speed distribution of the vehicles and by using parameters that control driver imperfection (based on the Krauss stochastic driving model [30]). The simulator runs based on a predefined time interval which controls the length of every step.\nWe ran experiments using five different intersection scenarios: Right, Left, Left2, Forward and a Challenge. Each of these scenarios is depicted in Figure 2. The Right scenario involves making a right turn, the Forward scenario involves crossing the intersection, the Left scenario involves making a left turn, the Left2 scenario involves making a left turn across\ntwo lanes, and the Challenge scenario involves crossing a six lane intersection with increased traffic density.\nEach lane has a 45 miles per hour (20 m/s) max speed. The car begins from a stopped position. Each time step is equal to 0.2 seconds. The max number of step per trial is capped 100 steps which is equivalent to 20 seconds. The traffic density is set by the probability that a vehicle will be emitted randomly per second. We use 0.2 for all scenarios except the challenge scenario where it is set to 0.7.\nWe evaluate each method according to four metrics and run 10,000 trials of each scenario in order to collect our statistics. The metrics are as follows: • Percentage of successes: the percentage of the runs the\ncar successfully reached the goal. This metric takes into both collisions and time-outs. • Percentage of collisions: a measure of the safety of the method. • Average time: how long it takes a successful trial to run to completion. • Average braking time: the amount of time other cars in the simulator are braking, this can be seen as a measure of how disruptive the autonomous car is to traffic.\nFor TTC and the Time-to-Go DQN, after the algorithm has decided the path is clear it follows the Intelligent Driver Model. All state representations ignores occlusion, assuming all cars are always visible.\nFor the sequential action DQN, space is represented as a 5× 11 grid discretizing 0 to 20 meters in front of the car and ±90 meters to the left and right of the car. Each spatial pixel, if occupied, contains the normalized real valued heading angles, velocity, and calculated time to collision. The 5×11×3 representation results in a 165 dimensional space. Exploratory studies found that this spatial representation outperformed higher dimensional representations with finer granularity. It was also found that real representations of the heading angle, velocity, and time to collision outperformed discretized versions. We hypothesize that this is because the real values allow for greater generalization.\nFor the Time-to-Go DQN, space is represented as a 18× 26 grid in global coordinates. Unlike the sequential action DQN, this representation does not use the calculated time to collision for each car.\nThe sequential action network is a fully connected networks with leaky ReLU [31] activation functions. The network consists of 3 hidden layers each of 100 nodes each and a final linear layer with 12 outputs. The 12 outputs correspond to three actions (accelerate, decelerate, maintain velocity) at four time scales (1, 2, 4, and 8 time steps).\nThe Time-to-Go DQN network uses a convolutional neural network with two convolution layers, and one fully connected layer. The first convolutional layer has 32 6×6 filters with stride two, the second convolution layer has 64 3×3 filters with stride 2. The fully connected layer has 100 nodes. All layers use leaky ReLU activation functions. The final linear output layer has five outputs: a single go action, and a wait action at four time scales (1, 2, 4, and 8 time steps). Both networks are optimized using the RMSProp algorithm [32].\nOur experience replay buffers store 100,000 time steps. We have two buffers, one for collisions and one for both successes and timeouts. At each learning iteration we samples 25 steps from each buffer for a total batch size of 50.\nSince the experience replay buffer imposes off-policy learning, we are able to calculate the return for each stateaction pair in the trajectory prior to adding each step into the replay buffer. This allows us to train directly on the nstep return and forgo the added complexity of using target networks [33].\nEach sequential action scenario was trained on one million simulations. Each Time-to-go scenario was trained on 250 thousand simulations. This difference was in order to roughly balance the runtime of the experiments.\nThe epsilon governing random exploration was decayed from 1.0 to 0.05 linearly over half the number of iterations.\nFor the reward we used +1 for successfully navigating the intersection, −10 for a collision, and −0.01 step cost."
    }, {
      "heading" : "IV. RESULTS",
      "text" : "Table I shows the results from our experiments. To yield the best results for TTC, TTC threshold for each scenario is chosen as the lowest that achieve zero collisions.\nTTC method didn’t have a collision in any of the scenarios, given that a large enough threshold is chosen. All other methods had non-zero collision rates for all scenarios, except DQN-Sequential for the Left scenario, which also had zero collisions. Among DQN methods, DQN Time-to-Go had substantially lower collision rate than DQN-sequential. For scenarios except Challenge, DQN Time-to-Go had only 7 collisions in a total of 40,000 simulations. We think it is\npossible that the collision rate would converge to zero for these scenarios if the network is trained for long enough.\nWe see that both DQN methods are substantially more efficient reaching the goal than TTC. DQN Time-to-Go has the best task completion time in all scenarios, except Forward, where DQN-Sequential is faster. On average, DQN Time-toGo was %28 faster in reaching to goal than TTC, whereas DQN Sequential was %19 faster than TTC. Therefore, the DQN methods have potential to reduce traffic jams due to their efficiency navigating intersection.\nBraking time, a measure of how disruptive the car is to other traffic, is comparable for all methods. Because of this, we didn’t find conclusive evidence of DQN methods being more aggressive than TTC, despite DQN methods posting non-zero collision rates.\nDQN Time-to-Go has the highest success rates in all experiments, except one, where its success rate is only marginally lower than TTC. Both DQN methods and TTC were able to produce sound policies, evidenced by the ego car reaching the goal at least %99.5 of the time for all scenarios except Challenge. Success rates are shown in Figure 3.\nAn interesting result was that TTC did not reach the goal the majority of the time in the Challenge scenario, reaching the goal only %39.2 of the time, and posting a success rate only slightly more than Random (%29.9). For the same scenario, DQN Time-to-Go reached the goal %98.46 of the time, significantly outperforming other methods. We offer more insight on these results in Section IV-B.\nWhile the DQNs are substantially more efficient, they are seldom able to minimize the number of collisions as successfully as TTC. This is due to the fact that TTC has a tunable parameter that adjusts the safety margin and we tune TTC to the lowest threshold that gives zero collisions.\nComparing the DQN’s performance against the TTC curve as we trade off speed vs. safety (Figure 4), we see that in every instance the DQN’s performance dominates the performance of TTC. This suggests that it is possible to design an algorithm that has zero collision rate, but with better performance metrics than TTC."
    }, {
      "heading" : "A. Generalization and Transfer",
      "text" : "While the DQN does outperform TTC, being able to achieve a zero percent collision rate is very important.\nWe believe that the few collisions that occur are a symptom of the system’s difficulty generalizing. Rather than spending increased effort shaping the reward to drive down the number of collisions, we feel it is more in the service of building a robust system to understand how the system generalizes. In order to do this, we run the network trained from one scenario on every other scenario.\nWe suspect that training on multiple scenarios will improve the performance of each individual task. This is one of the core principles of multi-task learning [34], it has recently been demonstrated specifically on robots learning CNN representations from physical actions [35], and it is the intended direction of our future research. But with this initial study the focus was to get an understanding of how well a deep net system can generalize to out-of-sample data.\nFigure 5 shows the transfer performance for both the sequential and Time-to-Go DQNs. We see that the networks trained on the Left scenario tend to transfer well to the other single lane scenarios Right and Forward. The networks trained on the Challenge scenario transfer well to the other multi-lane setting Left2. Generally speaking, the more challenging scenarios (based on the performance of the Random baseline) transfer well to easier domains, but changing the number of lanes creates interference. The Timeto-Go network trained on the Left2 scenario appears to be an example of over fitting, where the system refuses to move in any of the single lane scenarios. Notably, no method transfers well to all tasks."
    }, {
      "heading" : "B. Qualitative Analysis",
      "text" : "Comparing trials of the learned DQN networks and TTC, the DQN strategies take into account predictive behavior of the traffic. The DQNs can accurately predict that traffic in distant lanes will have passed by the time the ego car arrives\nat the lane. Also the DQN driver is able to anticipate whether on coming traffic will have sufficient time to brake or not. The few collisions seem to relate to discretization effects, where the car nearly misses the on coming traffic.\nIn contrast, TTC does not leave until all cars have cleared its path. In addition, TTC leaves a sufficient safety margin for on coming cars in distant lanes, since the same safety margin is used the gap gets exaggerated in close lanes. As a result TTC often waits until the road is completely clear, missing many opportunities to cross.\nWe see that selecting the departure time offers sufficient opportunity to improve over TTC without the need to incorporate the added complexity of allowing for dynamic acceleration and deceleration behavior. This holds even for the Challenge scenario. The Time-to-Go DQN often chooses departures when other cars are either in or approaching the intersection, correctly predicting that they will be clear by the time the car reaches that position."
    }, {
      "heading" : "V. CONCLUSIONS",
      "text" : "Unsignaled intersection handling remains a hard task for autonomous vehicles, mainly because of unpredictable agent behavior. Rule-based intersection handling methods offer reliable and easy-to-interpret solutions, however result in sub-optimal behavior and task performance.\nWe showed a first system that uses Deep Q-Networks for the specific problem of intersection handling. By making use of the latest Deep RL techniques, we were able to build networks that, in some metrics, outperform a commonly used rule-based algorithm based on the Time-to-Collision (TTC) heuristic. While TTC achieved zero collision rate for all cases, DQN performed better on task efficiency and success rate. Although rarely, DQN methods caused collisions and thus is unsuitable for real-world implementation in its current form. Therefore, more investigation on DQN’s is necessary to reduce the collision rate to zero.\nWe saw that determining the time to go is the most im-\nportant part of the task, even in more complex environments when the ability to change speeds through the intersection might be beneficial. This can greatly reduce the complexity of the decision task. We also identified specific instances when TTC has difficulty.\nWhile the networks demonstrate some ability to generalize to novel domains and out-of-sample data, more research is required to increase the robustness. Training one network for one scenario took almost a day on a modern multi-GPU computer, which made it impractical for network parameter tuning. As future work, we will investigate how the policy learned from one type of intersection scenario could be used as an initial policy for another scenario, building up a system that performs well in wide variety of domains."
    } ],
    "references" : [ {
      "title" : "Cooperative collision avoidance at intersections: Algorithms and experiments",
      "author" : [ "M.R. Hafner", "D. Cunningham", "L. Caminiti", "D. Del Vecchio" ],
      "venue" : "IEEE Transactions on Intelligent Transportation Systems, vol. 14, no. 3, pp. 1162–1175, 2013.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Autonomous vehicle control systems for safe crossroads",
      "author" : [ "J. Alonso", "V. Milanés", "J. Pérez", "E. Onieva", "C. González", "T. De Pedro" ],
      "venue" : "Transportation research part C: emerging technologies, vol. 19, no. 6, pp. 1095–1110, 2011.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Extended time-to-collision measures for road traffic safety assessment",
      "author" : [ "M.M. Minderhoud", "P.H. Bovy" ],
      "venue" : "Accident Analysis & Prevention, vol. 33, no. 1, pp. 89–97, 2001.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Time-to-collision and collision avoidance systems",
      "author" : [ "R. van der Horst", "J. Hogema" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1993
    }, {
      "title" : "A comparison of headway and time to collision as safety indicators",
      "author" : [ "K. Vogel" ],
      "venue" : "Accident analysis & prevention, vol. 35, no. 3, pp. 427– 433, 2003.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "A reasoning framework for autonomous urban driving",
      "author" : [ "D. Ferguson", "C. Baker", "M. Likhachev", "J. Dolan" ],
      "venue" : "Intelligent Vehicles Symposium, 2008 IEEE. IEEE, 2008, pp. 775–780.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Team AnnieWays autonomous system for the DARPA urban challenge 2007",
      "author" : [ "S. Kammel", "J. Ziegler", "B. Pitzer", "M. Werling", "T. Gindele", "D. Jagzent", "J. Schöder", "M. Thuy", "M. Goebl", "F. von Hundelshausen" ],
      "venue" : "The DARPA Urban Challenge. Springer, 2009, pp. 359–391.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Autonomous driving in urban environments: Boss and the urban challenge",
      "author" : [ "C. Urmson", "J. Anhalt", "D. Bagnell", "C. Baker", "R. Bittner", "M. Clark", "J. Dolan", "D. Duggins", "T. Galatali", "C. Geyer" ],
      "venue" : "Journal of Field Robotics, vol. 25, no. 8, pp. 425–466, 2008.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Towards full automated drive in urban environments: A demonstration in gomentum station, california",
      "author" : [ "A. Cosgun", "L. Ma", "J. Chiu", "J. Huang", "M. Demir", "A.M. Anon", "T. Lian", "H. Tafish", "S. Al-Stouhi" ],
      "venue" : "IEEE Intelligent Vehicles Symposium (IV), 2017.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "The MIT– Cornell collision and why it happened",
      "author" : [ "L. Fletcher", "S. Teller", "E. Olson", "D. Moore", "Y. Kuwata", "J. How", "J. Leonard", "I. Miller", "M. Campbell", "D. Huttenlocher" ],
      "venue" : "Journal of Field Robotics, vol. 25, no. 10, pp. 775–807, 2008.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "End to end learning for self-driving cars",
      "author" : [ "M. Bojarski", "D. Del Testa", "D. Dworakowski", "B. Firner", "B. Flepp", "P. Goyal", "L.D. Jackel", "M. Monfort", "U. Muller", "J. Zhang" ],
      "venue" : "arXiv preprint arXiv:1604.07316, 2016.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Belief state planning for navigating urban intersections",
      "author" : [ "M. Bouton", "A. Cosgun", "M.J. Kochenderfer" ],
      "venue" : "IEEE Intelligent Vehicles Symposium (IV), 2017.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Intention-aware autonomous driving decision-making in an uncontrolled intersection",
      "author" : [ "W. Song", "G. Xiong", "H. Chen" ],
      "venue" : "Mathematical Problems in Engineering, vol. 2016, 2016.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Probabilistic decisionmaking under uncertainty for autonomous driving using continuous pomdps",
      "author" : [ "S. Brechtel", "T. Gindele", "R. Dillmann" ],
      "venue" : "Intelligent Transportation Systems (ITSC), 2014 IEEE 17th International Conference on. IEEE, 2014, pp. 392–399.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Playing atari with deep reinforcement learning",
      "author" : [ "V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller" ],
      "venue" : "arXiv preprint arXiv:1312.5602, 2013.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Learning handeye coordination for robotic grasping with deep learning and largescale data collection",
      "author" : [ "S. Levine", "P. Pastor", "A. Krizhevsky", "D. Quillen" ],
      "venue" : "arXiv preprint arXiv:1603.02199, 2016.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "End-to-end training of deep visuomotor policies",
      "author" : [ "S. Levine", "C. Finn", "T. Darrell", "P. Abbeel" ],
      "venue" : "Journal of Machine Learning Research, vol. 17, no. 39, pp. 1–40, 2016.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Dynamic action repetition for deep reinforcement learning",
      "author" : [ "A. Srinivas", "S. Sharma", "B. Ravindran" ],
      "venue" : "AAAI Conference on Artificial Intelligence (AAAI), 2017.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Prioritized experience replay",
      "author" : [ "T. Schaul", "J. Quan", "I. Antonoglou", "D. Silver" ],
      "venue" : "arXiv preprint arXiv:1511.05952, 2015.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Incremental multi-step q-learning",
      "author" : [ "J. Peng", "R.J. Williams" ],
      "venue" : "Machine learning, vol. 22, no. 1-3, pp. 283–290, 1996.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Q-learning",
      "author" : [ "C.J. Watkins", "P. Dayan" ],
      "venue" : "Machine learning, vol. 8, no. 3-4, pp. 279–292, 1992.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Reinforcement learning: An introduction",
      "author" : [ "R.S. Sutton", "A.G. Barto" ],
      "venue" : "MIT press Cambridge,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1998
    }, {
      "title" : "Reinforcement learning with unsupervised auxiliary tasks",
      "author" : [ "M. Jaderberg", "V. Mnih", "W.M. Czarnecki", "T. Schaul", "J.Z. Leibo", "D. Silver", "K. Kavukcuoglu" ],
      "venue" : "arXiv preprint arXiv:1611.05397, 2016.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "A deep hierarchical approach to lifelong learning in minecraft",
      "author" : [ "C. Tessler", "S. Givony", "T. Zahavy", "D.J. Mankowitz", "S. Mannor" ],
      "venue" : "arXiv preprint arXiv:1604.07255, 2016.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation",
      "author" : [ "T.D. Kulkarni", "K. Narasimhan", "A. Saeedi", "J. Tenenbaum" ],
      "venue" : "Advances in Neural Information Processing Systems, 2016, pp. 3675–3683.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Self-improving reactive agents based on reinforcement learning, planning and teaching",
      "author" : [ "L.-J. Lin" ],
      "venue" : "Machine learning, vol. 8, no. 3-4, pp. 293–321, 1992.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Congested traffic states in empirical observations and microscopic simulations",
      "author" : [ "M. Treiber", "A. Hennecke", "D. Helbing" ],
      "venue" : "Physical Review E, vol. 62, no. 2, p. 1805, 2000.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 1805
    }, {
      "title" : "Recent development and applications of SUMO–simulation of urban mobility",
      "author" : [ "D. Krajzewicz", "J. Erdmann", "M. Behrisch", "L. Bieker" ],
      "venue" : "International Journal on Advances in Systems and Measurements (IARIA), vol. 5, no. 3–4, 2012.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Microscopic modeling of traffic flow: Investigation of collision free vehicle dynamics",
      "author" : [ "S. Krauss" ],
      "venue" : "Ph.D. dissertation, Deutsches Zentrum fuer Luft-und Raumfahrt, 1998.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Rectifier nonlinearities improve neural network acoustic models",
      "author" : [ "A.L. Maas", "A.Y. Hannun", "A.Y. Ng" ],
      "venue" : "Proc. ICML, vol. 30, no. 1, 2013.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Lecture 6.5-rmsprop, coursera: Neural networks for machine learning",
      "author" : [ "T. Tieleman", "G. Hinton" ],
      "venue" : "University of Toronto, Tech. Rep, 2012.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski" ],
      "venue" : "Nature, vol. 518, no. 7540, pp. 529–533, 2015.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Multitask Learning",
      "author" : [ "R. Caruana" ],
      "venue" : "Machine Learning, vol. 28, pp. 41– 75, 1997.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Learning to push by grasping: Using multiple tasks for effective learning",
      "author" : [ "L. Pinto", "A. Gupta" ],
      "venue" : "arXiv preprint arXiv:1609.09025, 2016.",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "A number of rule-based strategies have already been applied to intersection handling, including cooperative [2] and heuristic [3] approaches.",
      "startOffset" : 108,
      "endOffset" : 111
    }, {
      "referenceID" : 1,
      "context" : "A number of rule-based strategies have already been applied to intersection handling, including cooperative [2] and heuristic [3] approaches.",
      "startOffset" : 126,
      "endOffset" : 129
    }, {
      "referenceID" : 2,
      "context" : "The current state of the art is a rule-based method based on time-to-collision (TTC) [4], [5], which is a widely used heuristic as a safety indicator in the automotive industry [6].",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 3,
      "context" : "The current state of the art is a rule-based method based on time-to-collision (TTC) [4], [5], which is a widely used heuristic as a safety indicator in the automotive industry [6].",
      "startOffset" : 90,
      "endOffset" : 93
    }, {
      "referenceID" : 4,
      "context" : "The current state of the art is a rule-based method based on time-to-collision (TTC) [4], [5], which is a widely used heuristic as a safety indicator in the automotive industry [6].",
      "startOffset" : 177,
      "endOffset" : 180
    }, {
      "referenceID" : 5,
      "context" : "Variants of the TTC approach has been used for autonomous driving [7] and the DARPA urban challenge, where hand engineered hierarchical state machines were a popular approach to handle intersections [8], [9].",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 6,
      "context" : "Variants of the TTC approach has been used for autonomous driving [7] and the DARPA urban challenge, where hand engineered hierarchical state machines were a popular approach to handle intersections [8], [9].",
      "startOffset" : 199,
      "endOffset" : 202
    }, {
      "referenceID" : 7,
      "context" : "Variants of the TTC approach has been used for autonomous driving [7] and the DARPA urban challenge, where hand engineered hierarchical state machines were a popular approach to handle intersections [8], [9].",
      "startOffset" : 204,
      "endOffset" : 207
    }, {
      "referenceID" : 8,
      "context" : "TTC is currently the method we employ on our autonomous vehicle [10].",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 9,
      "context" : "is problematic: in the DARPA Urban Challenge, one reason behind a collision between two autonomous cars was “failure to anticipate vehicle intent” [11].",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 10,
      "context" : "In imitation learning, the policy is learned from a human drivers [12], however this policy does not offer a solution if the agent finds itself in a state that is not part of the training data.",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 11,
      "context" : "(POMCP) have been shown to handle intersections [13], but rely on the existence of an accurate generative model.",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 12,
      "context" : "Offline learning tackles the intersection problem, often by using Markov Decision Processes (MDP) in the back-end [14], [15].",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 13,
      "context" : "Offline learning tackles the intersection problem, often by using Markov Decision Processes (MDP) in the back-end [14], [15].",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 14,
      "context" : "Given the recent success of Deep Learning on a variety of control problems, [16], [17], [18] we are interested in evaluating the effectiveness of Deep RL in the domain of intersection handling.",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 15,
      "context" : "Given the recent success of Deep Learning on a variety of control problems, [16], [17], [18] we are interested in evaluating the effectiveness of Deep RL in the domain of intersection handling.",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 16,
      "context" : "Given the recent success of Deep Learning on a variety of control problems, [16], [17], [18] we are interested in evaluating the effectiveness of Deep RL in the domain of intersection handling.",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 17,
      "context" : "We use dynamic frame skipping [19] to expedite the system learning repeated actions, and prioritized reply [20] to ensure the network balances learning both positive and negative cases.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 18,
      "context" : "We use dynamic frame skipping [19] to expedite the system learning repeated actions, and prioritized reply [20] to ensure the network balances learning both positive and negative cases.",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 19,
      "context" : "advantage of the off-policy nature imposed by experience replay learning to calculate and train on the full n-step return [21] which we found greatly reduces the learning time of DQNs.",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 20,
      "context" : "In Q-learning [22], the action value function Qπ(s,a) is the expected return E[Rt |st = s,a] for a state-action pair following a policy π .",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 14,
      "context" : "In Deep Q-learning [16], the optimal value function is approximated with a neural network Q∗(s,a) ≈ Q(s,a;θ) with parameters θ .",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 19,
      "context" : "One way to make learning more efficient is to use n-step return[21] E[Rt |st = s,a] ≈ rt + γrt+1 + · · ·+ γrt+n−1 + γn maxat+n Q(st+n,at+n;θ).",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 17,
      "context" : "It was recently shown that allowing an agent to select actions over extended time periods improves the learning time of an agent [19].",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 21,
      "context" : "Dynamic frame skipping can viewed as a simplified version of options [23] which is recently starting to be explored by the Deep RL community.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 22,
      "context" : "[24], [25], [26].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "[24], [25], [26].",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 24,
      "context" : "[24], [25], [26].",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 25,
      "context" : "In order to break correlations between sequential steps of the agent, experience replay is used [27].",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 18,
      "context" : "can be preferentially sampled [20].",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 22,
      "context" : "[24] which avoids the computation of a rank list and instead samples to balancing reward across trajectories.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 26,
      "context" : "If this value exceeds the TTC threshold, then the ego vehicle starts the crossing phase and follows the Intelligent Driver Model (IDM) [28] until the goal is reached.",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 27,
      "context" : "Experiments were run using the Sumo simulator [29], which is an open source traffic simulation package.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 28,
      "context" : "and by using parameters that control driver imperfection (based on the Krauss stochastic driving model [30]).",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 29,
      "context" : "The sequential action network is a fully connected networks with leaky ReLU [31] activation functions.",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 30,
      "context" : "Both networks are optimized using the RMSProp algorithm [32].",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 31,
      "context" : "This allows us to train directly on the nstep return and forgo the added complexity of using target networks [33].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 32,
      "context" : "This is one of the core principles of multi-task learning [34], it has recently been demonstrated specifically on robots learning CNN representations from physical actions [35], and it is the intended direction of our future research.",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 33,
      "context" : "This is one of the core principles of multi-task learning [34], it has recently been demonstrated specifically on robots learning CNN representations from physical actions [35], and it is the intended direction of our future research.",
      "startOffset" : 172,
      "endOffset" : 176
    } ],
    "year" : 2017,
    "abstractText" : "Providing an efficient strategy to navigate safely through unsignaled intersections is a difficult task that requires determining the intent of other drivers. We explore the effectiveness of using Deep Reinforcement Learning to handle intersection problems. Combining several recent advances in Deep RL, were we able to learn policies that surpass the performance of a commonly-used heuristic approach in several metrics including task completion time and goal success rate. Our analysis, and the solutions learned by the network point out several short comings of current rule-based methods. The fact that Deep RL policies resulted in collisions, although rarely, combined with the limitations of the policy to generalize well to out-of-sample scenarios suggest a need for further research.",
    "creator" : "LaTeX with hyperref package"
  }
}