{
  "name" : "1511.08277.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Deep Architecture for Semantic Matching with Multiple Positional Sentence Representations",
    "authors" : [ "Shengxian Wan", "Yanyan Lan", "Jiafeng Guo", "Jun Xu", "Liang Pang", "Xueqi Cheng" ],
    "emails" : [ "pangliang}@software.ict.ac.cn,", "cxq}@ict.ac.cn" ],
    "sections" : [ {
      "heading" : "Introduction",
      "text" : "Semantic matching is a critical task for many applications in natural language processing (NLP), such as information retrieval (Li and Xu 2013), question answering (Berger et al. 2000) and paraphrase identification (Dolan, Quirk, and Brockett 2004). Taking question answering as an example, given a pair of question and answer, a matching function is required to determine the matching degree between these two sentences.\nRecently, deep neural network based models have been applied in this area and achieved some important progresses. A lot of deep models follow the paradigm to first represent the whole sentence to a single distributed representation, and then compute similarities between the two vectors to output the matching score. Examples include DSSM (Huang et al. 2013), CDSMM (Shen et al. 2014), ARC-I (Hu et al. 2014),\nCopyright c© 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nCNTN (Qiu and Huang 2015) and LSTM-RNN (Palangi et al. 2015). In general, this paradigm is quite straightforward and easy to implement, however, the main disadvantage lies in that important local information is lost when compressing such a complicated sentence into a single vector. Taking a question and two answers as an example:\nQ: “ Which teams won top three in the World Cup?” A1: “ Germany is the champion of the World Cup.” A2: “The top three of the European Cup are Spain, Netherlands and Germany. ” We can see that the keywords such as “top three” and “World Cup” are very important to determine which answer (between A1 and A2) is better for Q. When attending to “top three”, obviously A2 is better than A1; while if attending to “World Cup”, we can get an opposite conclusion. However, single sentence representation methods cannot well capture such important local information, by directly representing a complicated sentence as a single compact vector (Bahdanau, Cho, and Bengio 2014).\nSome other works focus on taking multiple granularity, e.g. word, phrase, and sentence level representations, into consideration for the matching process. Examples include ARC-II (Hu et al. 2014), RAE (Socher et al. 2011), DeepMatch (Lu and Li 2013), Bi-CNN-MI (Yin and Schütze 2015a) and MultiGranCNN (Yin and Schütze 2015b). They can alleviate the above problem, but are still far from completely solving the matching problem. That is because they are limited to well capture the contextualized local information, by directly involving word and phrase level representations. Taking the following answer as an example:\nA3: “The top three attendees of the European Cup are from Germany, France and Spain.”\nObviously, A2 is better than A3 with respect to Q, although both of them have the important keywords “top three”. This is because the two terms of “top three” have different meanings from the whole sentence perspective. “top three” in A2 focuses on talking about top three football teams, while that in A3 is indicating the top three attendees from different countries. However, existing multiple granularity deep models cannot well distinguish the two “top three”s. This is mainly because the word/phrase level representations are local (usually depend on contexts in a fixed window size), thus limited to reflect the true meanings of these words/phrases (e.g. top three) from the perspective of\nar X\niv :1\n51 1.\n08 27\n7v 1\n[ cs\n.A I]\n2 6\nN ov\n2 01\n5\nthe whole sentence. From the above analysis, we can see that the matching degree between two sentences requires sentence representations from contextualized local perspectives. This key observation motivates us to conduct matching from multiple views of a sentence. That is to say, we can use multiple sentence representations in the matching process, with each sentence representation focusing on different local information.\nIn this paper, we propose a new deep neural network architecture for semantic matching with multiple positional sentence representations, namely MV-LSTM. Firstly, each positional sentence representation is defined as a sentence representation at one position. We adopt a bidirectional long short term memory (Bi-LSTM) to generate such positional sentence representations in this paper. Specifically for each position, Bi-LSTM can obtain two hidden vectors to reflect the meaning of the whole sentence from two directions when attending to this position. The positional sentence representation can be generated by concatenating them directly. The second step is to model the interactions between those positional sentence representations. In this paper, three different operations are adopted to model the interactions: cosine, bilinear, and tensor layer. Finally, we adopt a k-Max pooling strategy to automatically select the top k strongest interaction signals, and aggregate them to produce the final matching score by a multi-layer perceptron (MLP). Our model is end to end, and all the parameters are learned automatically from the training data, by BackPropagation and Stochastic Gradient Descent.\nWe can see that our model can well capture contextualized local information in the matching process. Compared with single sentence representation methods, MV-LSTM can well capture important local information by introducing multiple positional sentence representations. While compared with multiple granularity deep models, MV-LSTM has leveraged rich context to determine the importance of the local information by using Bi-LSTM to generate each positional sentence representation. Finally, we conduct extensive experiments on two tasks, i.e. question answering and sentence completion, to validate these arguments. Our experimental results show that MV-LSTM can outperform several existing baselines on both tasks, including ARC-I , ARC-II, CNTN, DeepMatch, RAE, MultiGranCNN and LSTM-RNN.\nThe contribution of this work lies in three folds: • the proposal of matching with multiple positional sen-\ntence representations, to capture important contextualized local information;\n• a new deep architecture to aggregate the interactions of those positional sentence representations for semantic matching, with each positional sentence representation generated by a Bi-LSTM;\n• experiments on two tasks (i.e. question answering and sentence completion) to show the benefits of our model."
    }, {
      "heading" : "Our Approach",
      "text" : "In this section, we present our new deep architecture for matching two sentences with multiple positional sentence representations, namely MV-LSTM. As illustrated in Figure\n1, MV-LSTM consists of three parts: Firstly, each positional sentence representation is a sentence representation at one position, generated by a bidirectional long short term memory (Bi-LSTM); Secondly, the interactions between different positional sentence representations form a similarity matrix/tensor by different similarity functions; Lastly, the final matching score is produced by aggregating such interactions through k-Max pooling and a multilayer perceptron.\nStep 1: Positional Sentence Representation Each positional sentence representation requires to reflect the representation of the whole sentence when attending to this position. Therefore, it is natural to use a Bi-LSTM to generate such representation because LSTM can both capture long and short term dependencies in the sentences. Besides, it has a nice property to emphasize nearby words in the representation process (Bahdanau, Cho, and Bengio 2014).\nFirstly, we give an introduction to LSTM and Bi-LSTM. Long short term memory (LSTM) is an advanced type of Recurrent Neural Network by further using memory cells and gates to learn long term dependencies within a sequence (Hochreiter and Schmidhuber 1997). LSTM has several variants (Greff et al. 2015), and we adopt one common implementation used in (Graves, Mohamed, and Hinton 2013), but without peephole connections, as did in (Palangi et al. 2015). Given an input sentence S = (x0, x1, · · · , xT ), where xt is the word embedding at position t. LSTM outputs a representation ht for position t as follows.\nit = σ(Wxixt +Whiht−1 + bi),\nft = σ(Wxfxt +Whfht−1 + bf ),\nct = ftct−1 + it tanh(Wxcxt +Whcht−1 + bc),\not = σ(Wxoxt +Whoht−1 + bo),\nht = ot tanh(ct)\nwhere i, f, o denote the input, forget and output gates respectively. c is the information stored in memory cells and h is the representation. Compared with single directional LSTM, bidirectional LSTM utilizes both the previous and future context, by processing the data from two directions with two separate LSTMs (Schuster and Paliwal 1997). One LSTM processes the input sequence in the forward direction while the other processes the input in the reverse direction. Therefore, we can obtain two vectors −→ ht and ←− ht for each position. Intuitively, −→ ht and ←− ht reflect the meaning of the whole sentence from two directions when attending to this position, therefore it is reasonable to define the positional sentence representation as the combination of them. Specifically, for each position t, the t-th positional sentence representation pt is generated by concatenating −→ ht and ←− ht , i.e. pt = [ −→ ht T , ←− ht\nT ]T , where (·)T stands for the transposition operation which will also be used later.\nStep 2: Interactions Between Two Sentences On the basis of positional sentence representations, we can model the interactions between a pair of sentences from different positions. Many kinds of similarity functions can be\nused for modeling the interactions between pXi and pY j , where pXi and pY j stand for the i and j-th positional sentence representations for two sentences SX and SY , respectively. In this paper, we use three similarity functions, including cosine, bilinear and tensor layer. Given two vectors u and v, the three functions will output the similarity score s(u, v) as follows.\nCosine is a common function to model interactions. The similarity score is viewed as the angle of two vectors:\ns(u, v) = uT v\n||u|| · ||v|| ,\nwhere || · || stands for the L2 norm. Bilinear further considers interactions between different dimensions, thus can capture more complicated interactions as compared with cosine. Specifically, the similarity score is computed as follows:\ns(u, v) = uTMv + b,\nwhere M is the matrix to reweight the interactions between different dimensions, and b is the bias. When applying bilinear to compute the interaction between two corresponding positional sentence representations pXi and pY j for sentence SX and SY , obviously bilinear can well capture the interleaving interactions between −−→ hXi and ←−− hY j , while cosine cannot. Therefore bilinear can capture more meaningful interactions between two positional sentence representations, compared with cosine.\nTensor Layer is more powerful than the above two functions, which can roll back to other similarity metrics such as bilinear and dot product. It has also shown great superiority in modeling interactions between two vectors (Socher et al. 2013b; Socher et al. 2013a; Qiu and Huang 2015). That’s\nwhy we choose it as an interaction function in this paper. Other than outputing a scalar value as bilinear and cosine do, tensor layer outputs a vector, as described as follows.\ns(u, v) = f(uTM [1:c]v +Wuv [ u v ] + b),\nwhereM i, i ∈ [1, ..., c] is one slice of the tensor parameters, Wuv and b are parameters of the linear part. f is a non-linear function, and we use rectifier f(z) = max(0, z) (Glorot, Bordes, and Bengio 2011) in this paper, since it always outputs a positive value which is compatible as a similarity.\nWe can see that the outputs of the former two similarity functions (i.e. cosine and bilinear) are both interaction matrices, while the tensor layer will output an interaction tensor, as illustrated in Figure 1.\nStep 3: Interaction Aggregation Now we introduce the third step of our architecture, i.e. how to integrate such interactions between different positional sentence representations to output a matching score for two sentences.\nk-Max Pooling The matching between two sentences is usually determined by some strong interaction signals. Therefore, we use k-Max pooling to automatically extract top k strongest interactions in the matrix/tensor, similar to (Kalchbrenner, Grefenstette, and Blunsom 2014). Specifically for the interaction matrix, we scan the whole matrix and the top k values are directly returned to form a vector q according to the descending order. While for the interaction tensor, the top k values of each slice of the tensor are returned to form a vector. Finally, these vectors are further concatenated to a single vector q. k-Max pooling is meaningful: suppose we use cosine similarity, when k = 1, it directly outputs the largest interaction, which means that only the “best matching position” is considered in our model; while k is larger than 1 means that we utilize the top k matching positions to conduct semantic matching. Therefore, it is easy to detect where the best matching position lies, and whether we need to aggregate multiple interactions from different positions for matching. Our experiments show that the best matching position is usually not the first or last one, and better results can be obtained by leveraging matchings on multiple positions.\nMultiLayer Perception Finally, we use a MLP to output the matching score by aggregating such strong interaction signals filtered by k-Max pooling. Specifically, the feature vector q obtained by k-Max pooling is first feed into a full connection hidden layer to obtain a higher level representation r. Then the matching score s is obtained by a linear transformation:\nr = f(Wrq + br), s =Wsr + bs,\nwhere Wr and Ws stands for the parameter matrices, and br and bs are corresponding biases."
    }, {
      "heading" : "Model Training",
      "text" : "For different tasks, we need to utilize different loss functions to train our model. For example, if the task is formalized\nas a ranking problem, we can utilize pairwise ranking loss such as hinge loss for training. Given a triple (SX , S+Y , S − Y ), where S+Y is ranked higher than S − Y when matching with SX , the loss function is defined as:\nL(SX , S+Y , S − Y ) = max(0, 1− s(SX , S + Y ) + s(SX , S − Y ))\nwhere s(SX , S+Y ) and s(SX , S − Y ) are the corresponding matching scores. All parameters of the model, including the parameters of word embedding, Bi-LSTM, interaction function and MLP, are trained jointly by BackPropagation and Stochastic Gradient Descent. Specifically, we use Adagrad (Duchi, Hazan, and Singer 2011) on all parameters in training."
    }, {
      "heading" : "Discussions",
      "text" : "MV-LSTM can cover LSTM-RNN (Palangi et al. 2015) as a special case. Specifically, if we only consider the last positional sentence representation of each sentence, generated by a single directional LSTM, MV-LSTM directly reduces to LSTM-RNN. Therefore, MV-LSTM is more general and has the ability to leverages more positional sentence representations for matching, as compared with LSTM-RNN.\nMV-LSTM has implicitly taken multiple granularity into consideration. By using Bi-LSTM, which has the ability to involve both long and short term dependencies in representing a sentence, MV-LSTM has the potential to capture important n-gram matching patterns. Furthermore, MV-LSTM is flexible to involve important granularity adaptively, compared with CNN based models using fixed window sizes."
    }, {
      "heading" : "Experiments",
      "text" : "In this section, we demonstrate our experiments on two different matching tasks, question answering (QA) and sentence completion (SC)."
    }, {
      "heading" : "Experimental Settings",
      "text" : "Firstly, we introduce our experimental settings, including baselines, parameter settings, and evaluation metrics.\nBaselines The experiments on the two tasks use the same baselines listed as follows.\n• Random Guess: outputs a random ranking list for testing. • BM25: is a popular and strong baseline for information\nretrieval (Robertson et al. 1995).\n• ARC-I: uses CNNs to construct sentence representations and relies on a MLP to produce the final matching score (Hu et al. 2014).\n• ARC-II: firstly generates local matching patterns, and then composites them by multiple convolution layers to produce the matching score (Hu et al. 2014).\n• CNTN: is based on the structure of ARC-I, but further uses a tensor layer to compute the matching score, instead of a MLP (Qiu and Huang 2015).\n• LSTM-RNN: adopts a LSTM to construct sentence representations and uses cosine similarity to output the matching score (Palangi et al. 2015).\n• RAE: relies on a recursive autoencoder to learn multiple levels’ representations (Socher et al. 2011).\n• DeepMatch: considers multiple granularity from the perspective of topics, obtained via LDA(Lu and Li 2013).\n• MultiGranCNN: first uses CNNs to obtain word, phrase and sentence level representations, and then computes the matching score based on the interactions among all these representations (Yin and Schütze 2015b). We can see that ARC-I, CNTN and LSTM-RNN are all single sentence representation models, while ARC-II, DeepMatch, RAE and MultiGranCNN represent a sentence with multiple granularity.\nParameter settings Word embeddings required in our model and some other baseline deep models are all initialized by SkipGram of word2vec (Mikolov et al. 2013). For SC, word embedding are trained on Wiki corpus1 for directly comparing with previous works. For QA, word embeddings are trained on the whole QA dataset. The dimensions are all set to 50. Besides, the hidden representation dimensions of LSTMs are also set to 50. The batchsize of SGD is set to 128 for both tasks. All other trainable parameters are initialized randomly by uniform distribution with the same scale, which is selected according to the performance on validation set ((-0.1,0.1) for both tasks). The initial learning rates of AdaGrad are also selected by validation (0.03 for QA and 0.3 for SC).\nEvaluation Metrics Both tasks are formalized as a ranking problem. Specifically, the output is a ranking list of sentences according to the descending order of matching scores. The goal is to rank the positive one higher than the negative ones. Therefore, we use Precision at 1 (denoted as P@1) and Mean Reciprocal Rank (MRR) as evaluation metrics. Since there is only one positive example in a list, P@1 and MRR can be formalized as follows,\nP@1 = 1\nN N∑ i=1 δ(r(S +(i) Y ) = 1),\nMRR = 1\nN N∑ i=1\n1\nr(S +(i) Y )\n,\nwhere N is the number of testing ranking lists, S+(i)Y is the positive sentence in the i − th ranking list, r(·) denotes the rank of a sentence in the ranking list, and δ is the indicator function."
    }, {
      "heading" : "Question Answering",
      "text" : "Question answering (QA) is a typical task for semantic matching. In this paper, we use the dataset2 collected from Yahoo! Answers which is a community question answering system where some users propose questions to the system and other users will submit their answers. The user who\n1http://nlp.stanford.edu/data/WestburyLab. wikicorp.201004.txt.bz2\n2http://webscope.sandbox.yahoo.com/ catalog.php?datatype=l&did=10\nproposes the question will decide which one is the best answer. The whole dataset contains 142,627 (question, answer) pairs, where each question is accompanied by its best answer. We select the pairs in which questions and their best answers both have a length between 5 and 50. After that, we have 60,564 (questions, answer) pairs which form the positive pairs. Negative sampling is adopted to construct the negative pairs. Specifically for each question, we first use its best answer as a query to retrieval the top 1,000 results from the whole answer set, with Lucene3. Then we randomly select 4 answers from them to construct the negative pairs. At last, we separate the whole dataset to the training, validation and testing data with proportion 8:1:1. Table 1 gives an example of the data.\n(1) Analysis of Different Pooling Parameters As introduced in our approach, pooling parameter k is meaningful for our model. If we set k to 1, we can obtain the best matching position for two sentences. While if k is larger than 1, we are leveraging multiple matching positions to determine the final score. Therefore, we conduct experiments to demonstrate the influences of different pooling parameters. Here, the interaction function is fixed to cosine in order to directly compare with the baseline model LSTM-RNN, similar results can be obtained for other interaction functions such as bilinear and tensor layer.\nIn this experiment, we report different results when k is set to 1, 3, 5 and 10. As shown in Table 2, the performance is better when larger k is used in k-Max pooling for our MVLSTM. It means that multiple sentence representations do help matching. We also observe that when k is larger than 5, the improvement is quite limited. Therefore, we set k to 5 in the following experiments.\nWe further compare our model with LSTM-RNN and BiLSTM-RNN, where they use LSTM from one and two directions to generate sentence representations, respectively. That is to say, LSTM-RNN views the matching problems as matching at the last position, while Bi-LSTM-RNN both\n3http://lucene.apache.org\nleverage matchings at the first and last position. From the results in Table 2, we can see that all our MV-LSTMs can beat them consistently. The results indicate that the best matching position is not always in the first or last one. Therefore, the consideration of multiple positional sentence representations is necessary.\nWe further conduct a case study to show some detailed analysis. Considering the positive pair (SX , S+Y ) in Table 1, if k is set to 1, the interaction our model pools out is happened at position 6 and 9 in SX and S+Y , respectively. The corresponding words at the positions are “memory” and “memory”. It means that the matching of these two sentences is best modeled when attending to these two words. Clearly the best matching position in this case is not the last one, as implicitly assumed in LSTM-RNN. If k = 5, the matching positions4 are (“memory”, “memory”, 0.84), (“error”, “error”, 0.81), (“stick”, “stick”, 0.76), (“stick”, “memory”, 0.65), (“memory”, “stick”, 0.63), with the number stands for the interaction produced by the similarity function. We can see that our model focuses on the keyword correctly and the matching is largely influenced by the positional representations on these keywords. In addition, we also observe that the interactions between “stick” and “memory” play an important role for the final matching. Therefore, our model can capture important n-gram matching patterns by involving rich context to represent local information.\n(2) Performance Comparison We compare our model with all other baselines on the task of QA. Since there are three different interaction functions, our model has three versions, denoted as MV-LSTM-Cosine, MV-LSTM-Bilinear and MV-LSTM-Tensor, respectively. The experimental results are listed in Table 3. From the results, we have several experimental findings. Firstly, all end to end deep models (i.e., all baselines except for RAE and DeepMatch) outperform BM25. This is mainly because deep models can learn better representations and deal with the mismatch problem effectively. Secondly, comparing our model with single sentence representation deep models, such as ARC-I,\n4Here, we use the corresponding word at the position to indicate a position.\nCNTN, and LSTM-RNN, we can see that all our three models are better than them. Specifically, MV-LSTM-Tensor obtains 11.1% relative improvement over LSTM-RNN on P@1. This is mainly because our multiple positional sentence representations can capture more detailed local information than them. Thirdly, comparing our model with multiple granularity models such as RAE, DeepMatch, ARCII, and MultiGranCNN, our model also outperforms them. Specifically, MV-LSTM-Tensor obtains 5.6% relative improvement over MultiGranCNN on P@1. The reason lies in that our local information are obtained by representing the whole sentence, therefore, rich context information can be leveraged to determine the importance of different local information. Finally, among our three models, MV-LSTMTensor performs best. This is mainly because tensor layer can capture more complicated interactions, which is consistent with the observation that CNTN outperforms ARC-I significantly.\nWe give an example in the data, as illustrated in Table 4, to further show why our model outperforms the best model which considers multiple granularity, i.e. MultiGranCNN. Our experiments show that MultiGranCNN is largely influenced by the word level matching “free” to “free”, and thus get a wrong answer. This is because the two “free”s are of different meanings, the first one is focusing on free language resources, while the second one is talking about free games. Therefore, the word/phrase level matching requires to consider the whole context. Our model can tackle this problem by considering multiple positional sentence representations. Specifically, the positional interactions “(free, free)” is large in matching SX and S+Y , while it is small in matching SX and S−Y , , which is consistent with our intuitive understanding for matching."
    }, {
      "heading" : "Sentence Completion",
      "text" : "In this section, we show our experimental results on sentence completion, which tries to match the first and the second clauses in the same sentence. We are using exactly the same dataset constructed in (Hu et al. 2014) from Reuters (Lewis et al. 2004). Specifically, the sentences which have two “balanced” clauses (with 8-28 words) divided by a comma are extracted from the original Reuters dataset and the two clauses form a positive matching pair. For negative examples, the first clause are kept and the second clauses are sampled from other clauses which are similar with it by cosine similarity. For each positive example, 4 negative examples\nare constructed, and thus we will also get 20% for P@1 by random guess.\nThe experimental results are listed in Table 5. Considering we are using the same data, some baseline results are directly cited from (Hu et al. 2014), such as ACR-I, ARCII, RAE and DeepMatch. Since Hu et al. only used P@1 for evaluation in their paper, the results of MRR for these baselines are missing in Table 5. From the results, we can see that deep models gain larger improvements over BM25, compared with that on QA. This is because the mismatch problem is more serious on this dataset, and usually the first clause has few same keyword with the second one. The other results are mainly consistent with those on QA, and MVLSTM still performs better than all baseline methods significantly, with 11.4% relative improvement on P@1 over the strongest baseline."
    }, {
      "heading" : "Conclusions",
      "text" : "In this paper, we propose a novel deep architecture for matching two sentences with multiple positional sentence representations, namely MV-LSTM. One advantage of our model lies in that it can both capture the local information and leverage rich context information to determine the importance of local keywords from the whole sentence view. Our experimental results and case studies show some valuable insights: (1) Under the assumption that the final matching is solely determined by one interaction (i.e. pooling parameter is fixed to 1), MV-LSTM can achieve better results than all single sentence representation methods including LSTM-RNN. This means that the best matching position does not always lie in the last one, therefore, the consideration of multiple positional sentence representations is necessary. (2) If we allow the aggregation of multiple interactions (i.e. pooling parameter is fixed to larger than 1), MV-LSTM can achieve even better results. This means that the matching degree is usually determined by the combination of matchings at different positions. Therefore, it is much more effective by considering multiple sentence representations. (3) Our model is also better than multiple granularity methods, such as DeepMatch, RAE and MultiGranCNN. This means that the consideration of multi-granularity need to rely on rich context of the whole sentence."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was funded by 973 Program of China under Grants No. 2014CB340401 and 2012CB316303, 863 Program of China under Grant No. 2014AA015204, the National Natural Science Foundation of China (NSFC) under Grants No. 61472401, 61433014, 61425016, 61203298, and 61425016, Key Research Program of the Chinese Academy of Sciences under Grant No. KGZD-EW-T03-2, and Youth Innovation Promotion Association CAS under Grant No. 20144310. We also would like to thank Prof. Chengxiang Zhai for the constructive comments."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Cho Bahdanau", "D. Bengio 2014] Bahdanau", "K. Cho", "Y. Bengio" ],
      "venue" : "CoRR abs/1409.0473",
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Bridging the Lexical Chasm: Statistical Approaches to Answer-finding",
      "author" : [ "Berger" ],
      "venue" : "In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,",
      "citeRegEx" : "Berger,? \\Q2000\\E",
      "shortCiteRegEx" : "Berger",
      "year" : 2000
    }, {
      "title" : "Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources",
      "author" : [ "Quirk Dolan", "B. Brockett 2004] Dolan", "C. Quirk", "C. Brockett" ],
      "venue" : "In Proceedings of the 20th International Conference on Computational Linguistics (Coling),",
      "citeRegEx" : "Dolan et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Dolan et al\\.",
      "year" : 2004
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "Hazan Duchi", "J. Singer 2011] Duchi", "E. Hazan", "Y. Singer" ],
      "venue" : "The Journal of Machine Learning Research 12:2121–2159",
      "citeRegEx" : "Duchi et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2011
    }, {
      "title" : "Deep sparse rectifier networks",
      "author" : [ "Bordes Glorot", "X. Bengio 2011] Glorot", "A. Bordes", "Y. Bengio" ],
      "venue" : "In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS),",
      "citeRegEx" : "Glorot et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Glorot et al\\.",
      "year" : 2011
    }, {
      "title" : "Speech recognition with deep recurrent neural networks",
      "author" : [ "Mohamed Graves", "A. Hinton 2013] Graves", "A.-r. Mohamed", "G. Hinton" ],
      "venue" : "In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
      "citeRegEx" : "Graves et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2013
    }, {
      "title" : "R",
      "author" : [ "Greff, K.", "Srivastava" ],
      "venue" : "K.; Koutnı́k, J.; Steunebrink, B. R.; and Schmidhuber, J.",
      "citeRegEx" : "Greff et al. 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "and Schmidhuber",
      "author" : [ "S. Hochreiter" ],
      "venue" : "J.",
      "citeRegEx" : "Hochreiter and Schmidhuber 1997",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Convolutional neural network architectures for matching natural language sentences",
      "author" : [ "Hu" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Hu,? \\Q2014\\E",
      "shortCiteRegEx" : "Hu",
      "year" : 2014
    }, {
      "title" : "Learning deep structured semantic models for web search using clickthrough data",
      "author" : [ "Huang" ],
      "venue" : "In Proceedings of the 22nd ACM International Conference on Information & Knowledge Management (CIKM),",
      "citeRegEx" : "Huang,? \\Q2013\\E",
      "shortCiteRegEx" : "Huang",
      "year" : 2013
    }, {
      "title" : "A convolutional neural network for modelling sentences",
      "author" : [ "Grefenstette Kalchbrenner", "N. Blunsom 2014] Kalchbrenner", "E. Grefenstette", "P. Blunsom" ],
      "venue" : "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL),",
      "citeRegEx" : "Kalchbrenner et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kalchbrenner et al\\.",
      "year" : 2014
    }, {
      "title" : "T",
      "author" : [ "D.D. Lewis", "Y. Yang", "Rose" ],
      "venue" : "G.; and Li, F.",
      "citeRegEx" : "Lewis et al. 2004",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "and Xu",
      "author" : [ "H. Li" ],
      "venue" : "J.",
      "citeRegEx" : "Li and Xu 2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "and Li",
      "author" : [ "Z. Lu" ],
      "venue" : "H.",
      "citeRegEx" : "Lu and Li 2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "G",
      "author" : [ "T. Mikolov", "I. Sutskever", "K. Chen", "Corrado" ],
      "venue" : "S.; and Dean, J.",
      "citeRegEx" : "Mikolov et al. 2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "R",
      "author" : [ "H. Palangi", "L. Deng", "Y. Shen", "J. Gao", "X. He", "J. Chen", "X. Song", "Ward" ],
      "venue" : "K.",
      "citeRegEx" : "Palangi et al. 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "and Huang",
      "author" : [ "X. Qiu" ],
      "venue" : "X.",
      "citeRegEx" : "Qiu and Huang 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "M",
      "author" : [ "S.E. Robertson", "S. Walker", "S. Jones", "Hancock-Beaulieu" ],
      "venue" : "M.; Gatford, M.; and Others.",
      "citeRegEx" : "Robertson et al. 1995",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "K",
      "author" : [ "M. Schuster", "Paliwal" ],
      "venue" : "K.",
      "citeRegEx" : "Schuster and Paliwal 1997",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "A Latent Semantic Model with Convolutional-Pooling Structure for Information Retrieval",
      "author" : [ "Shen" ],
      "venue" : "In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management",
      "citeRegEx" : "Shen,? \\Q2014\\E",
      "shortCiteRegEx" : "Shen",
      "year" : 2014
    }, {
      "title" : "C",
      "author" : [ "R. Socher", "E.H. Huang", "J. Pennington", "A.Y. Ng", "Manning" ],
      "venue" : "D.",
      "citeRegEx" : "Socher et al. 2011",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Reasoning with neural tensor networks for knowledge base completion",
      "author" : [ "Socher" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Socher,? \\Q2013\\E",
      "shortCiteRegEx" : "Socher",
      "year" : 2013
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Socher" ],
      "venue" : "Proceedings of the Conference",
      "citeRegEx" : "Socher,? \\Q2013\\E",
      "shortCiteRegEx" : "Socher",
      "year" : 2013
    }, {
      "title" : "Convolutional Neural Network for Paraphrase Identification",
      "author" : [ "Yin", "W. Schütze 2015a] Yin", "H. Schütze" ],
      "venue" : "In The 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL),",
      "citeRegEx" : "Yin et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Yin et al\\.",
      "year" : 2015
    }, {
      "title" : "MultiGranCNN: An Architecture for General Matching of Text Chunks on Multiple Levels of Granularity",
      "author" : [ "Yin", "W. Schütze 2015b] Yin", "H. Schütze" ],
      "venue" : "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL),",
      "citeRegEx" : "Yin et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Yin et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ ],
    "year" : 2015,
    "abstractText" : "Matching natural language sentences is central for many applications such as information retrieval and question answering. Existing deep models rely on a single sentence representation or multiple granularity representations for matching. However, such methods cannot well capture the contextualized local information in the matching process. To tackle this problem, we present a new deep architecture to match two sentences with multiple positional sentence representations. Specifically, each positional sentence representation is a sentence representation at this position, generated by a bidirectional long short term memory (Bi-LSTM). The matching score is finally produced by aggregating interactions between these different positional sentence representations, through k-Max pooling and a multi-layer perceptron. Our model has several advantages: (1) By using Bi-LSTM, rich context of the whole sentence is leveraged to capture the contextualized local information in each positional sentence representation; (2) By matching with multiple positional sentence representations, it is flexible to aggregate different important contextualized local information in a sentence to support the matching; (3) Experiments on different tasks such as question answering and sentence completion demonstrate the superiority of our model.",
    "creator" : "LaTeX with hyperref package"
  }
}