{
  "name" : "1602.08045.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "PCA/LDA Approach for Text-Independent Speaker Recognition",
    "authors" : [ "Zhenhao Ge", "Sudhendu R. Sharma", "Mark J.T. Smith" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: Speaker Recognition, PCA, LDA, GMM, MFCCs"
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "Speaker recognition is a growing research area with applications in access control, transaction authentication, law enforcement, speech data management, to mention a few.1 Speaker recognition typically encompasses speaker classification and speaker verification.2 The former one classifies the testing speaker into one of the pre-modelled classes, or identify the testing speaker as a new speaker (in the open-set case∗), while the latter one makes a binary decision on whether the input speaker is the the speaker he/she claims to be. Speaker verification can be considered to be a special case of speaker classification in an open-set case. Speaker recognition is often sub-divided into text-independent and text-dependent cases, based on whether or not the speech used is known for each speaker. This paper mainly focuses on text-independent speaker classification in a close-set case, which means the input speaker must be pre-modelled and included in the classifier.\nMany statistical model approaches have been considered in this area, such as these based on Gaussian Mixture Models (GMMs),3 Hidden Markov Models (HMMs),4,5 Support Vector Machines (SVMs),6 Artificial Neural Networks (ANN),7 and so on. These algorithms generally achieve high accuracy but usually require a significant amount of time to train and test. For some cases like meetings and conferences, there may be a need for very fast speaker recognition response times in which case a more efficient and low cost approach may be desirable.\nThere have been extensive studies that demonstrate that both Principle Component Analysis (PCA) and Linear Discriminant Analysis (LDA) are helpful in improving the efficiency of speaker recognition system while maintaining high accuracy. Zhang et al.8 investigated a PCA-based classifier involving both a Principle Component Space (PCS) and a Truncation Error Space (TES), and showed that this mixed classifier can outperform either of the two individual PCS or TES classifier. Jin et al.9 reported that the GMM classifier with LDA feature reduction can achieve higher performance with respect to accuracy and efficiency in some circumstances. Other researchers have integrated PCA with GMM10 and genetic algorithms,11 and have applied PCA and LDA in conjunction with K-Nearest Neighbors (KNN) algorithms12 for speaker identification.\nFurther author information: Zhenhao Ge: zge@purdue.edu, Sudhendu R. Sharma: sharmasr@purdue.edu ∗open-set case is to decide whether or not the unknown testing speaker belong to a set of S known speakers, while the\nclose-set case is to classify the input speaker to one of the speakers pre-modelled in the classifier.\nar X\niv :1\n60 2.\n08 04\n5v 1\n[ cs\n.S D\n] 2\n5 Fe\nb 20\n16\nThis paper extends some of this previous work in new directions. We first explore text-independent speaker classification using PCA or LDA individually with optimized parameter settings. Then, we will combine these two classifiers into one and demonstrate that the composite achieves comparable results to conventional GMM approaches while significantly reducing computation time. The general structure of the speaker classification system used in this paper is shown in Fig. 1. We used 80% of the data from each speaker to train the PCA-based, LDA-based and combined classifiers, 60% for enrollment and 20% for validation. The remaining 20% is used to test the performance of the final composite classifier."
    }, {
      "heading" : "2. PCA METHOD FOR TEXT-INDEPENDENT SPEAKER RECOGNITION",
      "text" : "In this section, we introduce a PCA classification method and demonstrate how to compute PCA eigenspaces and transform data to the eigenspace with reduced dimension. Then, a PCA classifier based on both Principle Component Space (PCS) and Truncation Error Space (TES) is explored and evaluated."
    }, {
      "heading" : "2.1 Introduction of PCA",
      "text" : "Principle Component Analysis (PCA) is a standard data analysis technique that transforms the original data set D{Γ1,Γ2, ...ΓN} to an K-dimensional uncorrelated and orthogonal space spanned by UK using eigenvalue decomposition. The dimension of the transformed data set D′{ω1, ω2, ..., ωN} is normally reduced and sorted by the corresponding variance, i.e., the eigenvalue of the data covariance matrix C, on that dimension. A new eigenspace Uk with reduced dimension k is formed by the k normalized eigenvector of C associated with the k largest eigenvalues.\nThis process of data transformation or projection from the original space to the eigenspace can be implemented as follows:\n1. Assuming each original sample Γi, i ∈ [1, N ] is M -dimensional, find the sample mean Ψ = 1M ∑M i=1 Γi; 2. Shift Γi to zero mean and form a new data matrix A = [Φi,Φ2, ...,ΦN ] (M ×N), where Φi = Γi −Ψ; 3. Find the covariance matrix C = 1M ∑M i=1 ΦiΦ T i ∝ AAT (ignore the constant factor 1M ); 4. Use an eigen-decomposition method, such as Singular Value Decomposition (SVD), to find the eigenvalues Λ = [λ1, λ2, ..., λK ], K = min(M,N), and the corresponding eigenvectors UK = [u1,u2, ...,uK ] (M ×K) of C in descending order w.r.t Λ;\n5. Choose the first k(k ≤ K) eigenvectors to form the eigenspace Uk = [u1,u2, ...,uk] (M × k); 6. Project the mean-shifted data set A (M × N) to the eigenspace Uk (M × k) and represent the data by\nΩ = UTk A (k ×N) or ωi = UTk Φi,i ∈ [1, N ].\nData components on the dimensions with larger eigenvalues contain significant information about the data. Removing components on the dimensions with smaller eigenvalues maintains principle information while reducing dimension. With the assumption that the principle information also helps to distinguish classes, one can:\n1. Compute an overall eigenspace with data from all classes and classify the transformed data based on the distance from eigenspace (edfes) and the distance within eigenspace (edies), using some pattern recognition algorithm, such as K-Nearest Neighbors (KNN), or Gaussian Mixture Models (GMMs);\n2. Compute an individual eigenspace for each classes s and form a PCA classifier g(s)(X) to perform classification directly by computing Ŝ = arg max g(s)(X), in which s ∈ [1, S], S is the total number of classes, and X is the input feature set.\nThe first approach is usually applied when the data set of all classes share similar characteristics that can be used to distinguish them from random data using edfes, e.g., face images are easily distinguished from random image in face recognition. In addition, data from the same class are normally clustered in the eigenspace computed by all classes. For example, an input face image can be determined by measuring its distance edies to each centroid of the face class in the eigenspace.\nHowever, for data such as Mel-Frequency Cepstral Coefficients (MFCCs) used in text-independent speaker recognition, the major variation of data is the content of speech rather than the characteristics of the speakers, and the projected data samples from different classes are significantly overlapped in the overall eigenspace. Thus, we can only choose the 2nd approach to decorrelate and orthogonalize data using PCA, explore which dimensions contribute more than the others, and finally form a PCA-based classifier to reduce the dimensionality of data and maintain good speaker recognition performance at the same time."
    }, {
      "heading" : "2.2 PCA-based Classifier",
      "text" : "As mentioned in Section 2.1, the PCA-based classifier requires the computation of eigenspace UK(s) for each class s, s ∈ [1, S], i.e., for each speaker. Twelve seconds of text-independent speech for each speaker s is converted to an M ×N MFCCs feature matrix with delta and double delta using a 10 msec frame rate, where M and N are the dimension and length (number of frames) of the cepstral features. After mean-shifting, covariance matrix computation, and eigenvalue decomposition, the full eigenspace U (s) for each speaker s can be found and a input sample Γi can be classified by measuring its “distance” of mean-shifted features Φi to each eigenspace U (s).\nOnce the full eigenspace UK is reduced to the Principle Component Space (PCS) with dimension kp, the rest of the K − kp eigenvectors form another eigenspace called the Truncation Error Space (TES), which is orthogonal to the PCS. After the samples Γ from one input speaker being converted to X, two intuitive classification approaches are considered: maximize the projection of X onto PCS; and minimize its projection onto the TES. The relationships among the PCS, the TCS and the full eigenspace, and the projections onto these spaces are illustrated in Fig. 2.\nPCS‐TES 1\nThe PCS and TES classifers may be described as:\nPCS Classifier: Ŝ = arg max s∈[1,S]\ng(s) PCS (X), where g(s) PCS (X) = T∑ t=1 ‖U (s)kp Φ (s) t ‖ = T∑ t=1 ‖U (s)kp (xt −Ψ (s))‖, (1)\nTES Classifier: Ŝ = arg min s∈[1,S]\ng(s) TES (X), where g(s) TES (X) = T∑ t=1 ‖U (s)K−kpΦ (s) t ‖ = T∑ t=1 ‖U (s)K−kp(xt −Ψ (s))‖. (2)\nIn Eq. (1) and Eq. (2), U (s) kp and U (s) K−kp denote the PCS and TES of speaker s, xt and Φt denote the tth feature vector of X (M × T ) before and after mean-shift based on the speaker s. Zhang, et al.8 have shown combining these two criteria can reach higher classification performance than either of the two individual classifiers based on PCS or TES. Here we further investigate how each dimension component in PCS and TES contributes to classification accuracy. We propose a new mixed classifier defined by the equation\nPCS&TES Classifier: Ŝ = arg max s∈[1,S] g(s)(X|λ) = arg max s∈[1,S] p g(s) PCS (X|kp)− (1−p) g(s)TES(X|kt), where λ = {kp, kt, p}. (3) In Eq. (3), parameter set λ contains kp, kt and p, which denote the dimensions of PCS and TES and the weight of these two individual classifiers. The relation between kp and kt and the projection of Φt onto both eigenspaces spanned by Ukp and Ukt is illustrated in Fig. 3. Next, we intend to find λ\n∗ which optimizes the mixed classifier and renders best performance, through exhausted search in the 3-dimensional space of (kp, kt, p).\nIn order to find appropriate search ranges for kp and kt, the performance in terms of classification rate (r) of PCS and TES individually along with different dimensions is investigated is illustrated in Fig. 4. Three curves of classification rates are presented, including PCS (blue dash-dot line), TES (green dashed line) and the combination of PCS and TES with p = 0.5 (red solid line). It indicates that the combined classifier may outperform the single classifier at certain dimensions, and also shows both PCS and TES classifiers reach peak performance within dimension range (1,bM/2c), where M = 39 is the feature dimension used in this project. Thus, it is sufficient to implement exhausted search in kp, kt ∈ (1, bM/2c) and p ∈ [0, 1] with interval 0.01."
    }, {
      "heading" : "2.3 Performance Evaluation of the PCA-based classifier",
      "text" : "Classification performances using PCS and TES individually, and the mixed model w.r.t parameter set λ = {kp, kt, p} are compared and listed below in Table 1. Based on the results, the mixed classifier is in general better than the other two, while the TES classifier is better than the PCS, and thus contributes more in the mixed classifier. The dimension kt in TES is relatively consistent compared with the dimension in PCS as the population size increases. For the mixed classifier at each population level, there are multiple points of (kp, kt, p) that reach the top performance, w.r.t. both accuracy in terms of classification rate and efficiency in terms of number of total dimension kp + kt. A more detailed list of all points of (kp, kt, p) is included in Appendix A."
    }, {
      "heading" : "3. LDA METHOD FOR TEXT-INDEPENDENT SPEAKER RECOGNITION",
      "text" : "While PCA seeks a dimension-reduced orthogonal eigenspace with largest data variance in each direction; the goal of Linear Discriminant Analysis (LDA) is to find another K-dimensional eigenspace (K ≤ M , the data dimension), with the first K directions that maximally discriminate among different classes. In this section, we first introduce LDA, then project MFCCs to a new eigenspace based on LDA with fewer dimensions. After that, we use a Gaussian Mixture Model (GMM)-based classifier with dimension reduced features to perform speaker classification."
    }, {
      "heading" : "3.1 Introduction of LDA",
      "text" : "For this discussion of Linear Discriminant Analysis (LDA), assume there is M × Ts data X for class s ∈ [1, S], where M is the sample dimension and Ts is the number of samples in this class s. Φ and Φs are the global mean over all classes and the local mean for each class s respectively. Then, we define between-class scatter SB and within-class scatter SW by\nSB = 1\nS S∑ s=1 (Φs − Φ)(Φs − Φ)T , (4)\nSW = 1\nS S∑ s=1 1 Ts Ts∑ t=1 (Xt − Φs)(Xt − Φs)T . (5)\nIf we choose w from the underlying space W , then wTSBw and w TSWw are the projections of SB and SW onto the direction w. Searching the directions w for the best class discrimination is equivalent to maximizing the ratio of (wTSBw)/(w TSWw) subject to w TSWw = 1. The latter is called the Fisher Discriminant Function and can be converted to SBw = λSWw, then S −1 W SBw = λw (6) by Lagrange multipliers and solved by eigen-decomposition of S−1W SB .\nUsing this process on the M -dimensional MFCCs feature set of speaker data, we find the eigenspace WK (K ≤ M) and reduce the feature dimension from M to K by projecting them to the eigenspace. Then, we use Gaussian Mixture Models (GMMs), which have been successfully used to classify speakers based on MFCCs3 to perform speaker recognition. Since the construction of eigenspaces in LDA requires information from all classes, we cannot construct LDA eigenspaces for each speaker and form a LDA-based classifier using the method described for the PCA-based classifier. Instead, we use LDA for dimension reduction prior to GMM-based classification."
    }, {
      "heading" : "3.2 GMM for speaker recognition",
      "text" : "Gaussian mixture density models the feature distribution of each speaker as a weighted sum of multiple Gaussian distributions. For each feature vector x in the M × T feature set X, the probability of x can be formulated by the equation\np(x|λ) = N∑ i=1 pibi(x), bi(x) = 1 (2π)M/2|Σi|1/2 exp{−1 2 (x− µi)TΣ−1i (x− µi)}, (7)\nwhere M is the dimension of the feature vector x, N is the number of mixture components, bi(x), i = 1, ..., N , are the component densities, pi, i = 1, ..., N , are the mixture weights and λ = {pi, µi,Σi}, i = 1, 2, ..., N is the collective representation of the parameters.\nGMMs are attractive for modelling speakers because they may reveal the underlying vocal tract configurations, which help to distinguish speakers; and they are capable of representing a large class of sample distributions.3\nGiven MFCCs feature X (M × T ) from speaker s, the Maximum Likelihood Estimation (MLE) is used to maximize the GMM likelihood, which can be written as\nλ∗ = arg max λ p(X|λ) = arg max λ T∏ t=1 p(xt|λ). (8)\nSince this expression is non-linear and direct maximization is difficult, the parameter set λ = {p, µ,Σ} is iteratively estimated using a special case of the Expectation-Maximization (EM) algorithm13 and is summarized below:\np̄i = 1\nT T∑ t=1 p(i|xt, λ); µ̄i = ∑T t=1 p(i|xt, λ)xt∑T t=1 p(i|xt), λ) ; σ̄2i = ∑T t=1 p(i|xt, λ)x2t∑T t=1 p(i|xt, λ) − µ̄2i , (9)\nwhere p̄i, µ̄i, σ̄ 2 i , i = 1, ..., N are the mixture weights, means, and variances for the ith component. p(i|xt, λ) is a posteriori probability for the i-th component given by\np(i|xt, λ) = pibi(xt)∑M k=1 pkbk(xt) . (10)\nThese estimates are based on the assumption of independence among feature dimension, so for each speaker class s, the non-zero values of the covariance matrix are only on the diagonals.\nThis algorithm guarantee a monotonic increase of the model’s likelihood on each EM iteration. Detailed implementation with parameter initialization is discussed in Sec. 4.\nAfter obtaining the GMM parameter set λs for speaker class s ∈ [1, S], the GMM-based classifier, which maximize a posteriori probability for a feature sequence X, (M × T ) can be formulated as follows:\nGMM Classifier: Ŝ = arg max s∈[1,S] Pr(λs|X) = arg max s∈[1,S] p(X|λs)λs p(X) ∝ arg max s∈[1,S] p(X|λs) ∝ arg max s∈[1,S] T∑ t=1 logp(xt|λs).\n(11) The first equation is due to Bayes’ rule; the first proportion is assuming Pr(λs) = 1/S and p(X) is the same for all speaker models; the second proportion uses logarithm and independence between input samples xt, t ∈ [1, T ].\nThe LDA-GMM classifier is similar to the GMM classifier in Eq. (11). The only difference is the input features X are replaced by feature Y with fewer dimensions reduced by LDA.\nOne of the important issues when using a GMM as a classifier is determining the approximate number of mixtures N , i.e., the model order. Applying a GMM on the original feature data without LDA dimension reduction, and plotting performance vs. model order (N = 8, 16, ..., 64) curves in multiple population levels (Fig. 5), we found the range of N ∈ [15, 40] is appropriate for our database. Using LDA prior to the GMM yields similar results. Thus, we select the GMM with model order N = 15 and N = 30 to evaluate LDA-GMM classifier in the next section."
    }, {
      "heading" : "3.3 Performance Evaluation of the LDA-GMM classifier",
      "text" : "Jin et al. have shown that applying LDA can not only reduce the feature dimension and the computation cost, but also improve the classification rate.9 In this paper, we aim to quantitatively find the “optimal” reduced feature dimension k (k < M), that balances both classification accuracy and efficiency. Fig. 6 shows the classification rate r of the LDA-GMM classifier increases along with the LDA eigenspace dimension k. If the dimension is optimized by maximizing the accuracy, it may be close to the original full dimension M and the computational cost is similar to the GMM classifier without LDA. Thus, a joint performance/complexity optimized dimension k is developed by minimizing the product of the error rate (1− r) and dimension k and is formulated below:\nk∗ = arg min k∈[1,M ] (1− r)k. (12)\nThe “plus signs” in Fig. 6 show the jointly optimized dimension and the corresponding classification rate r, while the “stars” show the solutions with r maximized.\nIn Table 2, the performance of the LDA-GMM classifier with optimized LDA dimensions using both criteria is compared with the GMM classifier. When model order N = 30, the LDA-GMM classifier with maximized accuracy always performs better than or equal to the GMM classifier, while when M = 15, it only performs better than the GMM classifier at speaker level S = 200. This indicates that applying LDA is more useful when the model order or population size is large. In terms of efficiency, by comparing rGMM and rLDA−GMM(jointly opt.) at population size 100 and 200, reducing feature dimension when the model order is low will maintain competitive\naccuracy with good efficiency. The LDA-GMM classifier configuration highlighted digits will be used to implement the mixed PCA/LDA approach discussed in Sec. 4."
    }, {
      "heading" : "4. IMPLEMENTATION OF THE PCA/LDA COMBINED METHOD",
      "text" : "Though PCA and LDA are commonly used for feature dimension reduction, both of them have their own advantages and disadvantages and it has been a long debate over decades on which one is better.14,15 PCA is relatively easy to implement, since the matrix used in eigen-decomposition is always non-singular. This is not the case for LDA. Moreover, PCA requires less computation, especially when we compute PCA eigenspace for each class and form a PCA-based classifier without other pattern classification techniques, such as the PCA classifier designed in this paper. However, the discriminant information may not reside in the direction with large component variance. That is the weakness of PCA and where LDA shines. LDA suffers from the issue of singularity and the “peaking” problem,16 when the training database is small. Recent studies show PCA may outperform LDA when the training set is small and PCA is less sensitive to different training sets.17\nSince PCA and LDA are two complementary techniques, after discussing text-independent speaker recognition based on each of them in Sec. 2 and Sec. 3, we present the detailed experimental setting to implement PCA/LDA combined method with better results than single PCA and LDA classifiers."
    }, {
      "heading" : "4.1 Database and Feature Extraction",
      "text" : "A subset of the TIMIT corpus consisting of 200 Male speakers with 10 utterances each from region 1 to 4 is used for implementation. The 10 utterances from each speaker are then divided into 3 parts, one for enrollment, one for validation and one for testing.\n1. The first 12 seconds from the concatenation of No. 3 to No. 8 utterances is used for enrollment, such as computing PCA, LDA eigenspace, or training the GMM models for each speaker.\n2. The first 4 seconds from the concatenation of No.1 and No. 2 utterances is used for validation, such as computing the classification rate of the PCA and LDA-GMM classifiers and determining the settings of parameters λ\nPCA = {kp, kt, p} and λLDA−GMM = {N, k} and the weight p in the PCA/LDA combined\nclassifier.\n3. The first 4 seconds from the concatenation of No. 9 and No. 10 utterances is used for testing the performance of the PCA/LDA combined classifier, which is illustrated later in this section.\nThirty-nine dimensional HTK-style MFCCs with delta and double delta at 10 msec frame rate are used.18 The cepstral coefficients from order 2 to 13 (removing order 1 of the DC component) plus 1 energy feature comprise the original 13-dimensional MFCCs. 13-dimensional delta (or velocity) feature and another 13-dimensional double delta (or acceleration) feature are added in to measure the changes between frames in the corresponding cepstral/energy ferature.19 The Hamming window length is 25 msec. So for each speaker, the sizes of the feature sets for enrollment, validation and testing are 39× 1198, 39× 398 and 39× 398."
    }, {
      "heading" : "4.2 GMM Initialization and Training",
      "text" : "Given an M × T feature set X for each speaker, where M is the dimension of each feature vector and T is the number of vectors (samples), to initialize an N -dimensional GMM, N evenly spaced feature vectors xi, i ∈ [1, N ] from X are selected to be the initial mean µi, The initial variance matrix Σi is M ×M identity matrix, and the weight pi is 1/N .\nIn GMM training, a variance limiting constraint Var(x)/T 2 is used on each dimension of X to avoid singularity in the model’s likelihood function. The maximum number of iterations in the EM algorithm to find the MLE of the GMM is 100 with an early termination condition that the increment of log-likelihood between two consecutive iterations is less than 0.001."
    }, {
      "heading" : "4.3 Combined Classifier based on PCA/LDA",
      "text" : "To combine the PCA-based (PCS&TES) classifier and the LDA-GMM-based classifier, we first normalized them to uniform scale using the following equation:\ng (s) 1 =\ng(s) PCS&TES (X)∑S s=1(g (s) PCS&TES(X))2 , g (s) 2 =\ng(s) LDA−GMM (X)∑S s=1(g (s) LDA−GMM(X))2 , (13)\nwhere g1 and g2 are the normalized classifiers of both approaches. Then, we optimized the weight p on the combined classifier by\np∗ = arg max p∈[0,1] r(p) = arg max p∈[0,1]\n∑S s=1 Is=argmaxs∈[1,S] pg (s) 1 (X)+(1−p)g (s) 2 (X)\nS , (14)\nwhere r(p) is the classification rate of combined classifier based on p and I s=argmaxs∈[1,S] pg (s) 1 (X)+(1−p)g (s) 2 (X) is an indicator that speaker s is correctly classified using the combined classifier. Fig. 7 shows the ranges of p∗ with 100 and 200 population sizes are 4% to 31% and 30% to 50% respectively, given the parameter set {kp, kt, p|S} for g1 is {5, 8, 4%|S = 100} and {9, 8, 15%|S = 200}, and the parameter set {N, k|S} for g2 is {15, 8|S = 100} and {15, 10|S = 200}.\nFor population 100 and 200, we select p∗ = 17.5% and p∗ = 40%, which are the medians in the range of the optimized p, as the final p∗ in the combined classifier. The PCA/LDA combined classifier is given by:\nPCA/LDA Combined Classifier: Ŝ = arg max s∈[1,S]\np∗g (s) 1 (X) + (1− p∗)g (s) 2 (X). (15)\nWhen tested using the remaining two utterances for each speaker, the combined classifier achieves 100%, 96%, 95% classification rate for population size of 50, 100, 200, which is slightly better than GMM classifier (100%, 96%, 94%\ngiven model order N = 15, refer to Table 2), but with significantly less computation time, about 80% less than the GMM classifier in this implementation."
    }, {
      "heading" : "5. CONCLUSION AND FUTURE WORK",
      "text" : "This paper presents classifiers based on PCA and LDA with optimized parameter settings. In the PCA approach, a classifier based on both PCS and TES is discussed and evaluated with globally optimized settings in kp,kt the dimension of PCS and TES and the weight p, which balances the two individual PCS and TES classifiers. In the LDA approach, the feature dimension is reduced using LDA, before being passed on as an input to the GMM classifier. The GMMs are initialized with appropriate model order with consideration of accuracy and efficiency. Then, a LDA-GMM classifier with optimized settings is shown to achieve comparable accuracy in speaker classification with significantly reduced time. After the development of both PCA and LDA-GMM classifier, a classifier combining these two techniques achieve higher performance w.r.t. both accuracy and efficiency. Using only 12 seconds of text-independent utterances for training, and 4 seconds for testing, the combined classifier achieves 100%, 96% and 95% classification rates for population sizes of 50, 100 and 200, which is comparable to the performance of the conventional GMM classifier on the same data. However, the new combined system reduces the computation by up to 85% in training and 78% in testing, compared with GMM classifier.\nFor future work, there are at least three types of evaluation on the combined classifier worth exploring, which may eventually provide a more robust PCA/LDA-based speaker classifier for various databases:\n1. One can experiment with different lengths of speech in both training and testing and obtain an optimized balance between the accuracy and efficiency for specific databases;\n2. One can also investigate the classifier with different types of features, such as Linear Predictive Coding Coefficients (LPCCs) and Perceptual Linear Prediction (PLP), even using MFCCs, one can explore how the delta and double delta features can help to distinguish classes;\n3. In terms of robustness, the classifier should also be tested with multiple databases with different noise levels."
    }, {
      "heading" : "APPENDIX A. PARAMETER SETTING OF PCA-BASED CLASSIFIER WITH OPTIMIZED PERFORMANCE",
      "text" : "When implementing exhausted search to find the optimized parameter set λ = {kp, kt, p} that provides the best classification performance, multiple points in the parameter space were found. Some of them were clustered together, showing a stable region that reaches high performance, while the other were scattered. Here we provide a more detailed list of these points of parameter setting than shown in Table 1 in Sec. 2.3. These points have been sorted in ascending order of computational cost w.r.t. kp+kt. After considering both classification accuracy and efficiency, we have selected some points to form a combined classifier with LDA and the results are provided in Sec. 4."
    } ],
    "references" : [ {
      "title" : "Automatic speaker recognition: Current approaches and future trends",
      "author" : [ "D. Reynolds" ],
      "venue" : "Speaker Verification: From Research to Reality (2001).",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Speaker recognition: A tutorial",
      "author" : [ "J. Campbell Jr" ],
      "venue" : "Proceedings of the IEEE 85(9), 1437–1462 (1997).",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Robust text-independent speaker identification using gaussian mixture speaker models",
      "author" : [ "D. Reynolds", "R. Rose" ],
      "venue" : "Speech and Audio Processing, IEEE Transactions on 3(1), 72–83 (1995).",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Variable parameter speaker verification system based on hidden markov modeling",
      "author" : [ "M. Savic", "S. Gupta" ],
      "venue" : "[Acoustics, Speech, and Signal Processing, 1990. ICASSP-90., 1990 International Conference on ], 281–284, IEEE (1990).",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "User-customized password speaker verification using multiple reference and background models",
      "author" : [ "M.F. BenZeghiba", "H. Bourlard" ],
      "venue" : "Speech Communication 8 (0 2006). IDIAP-RR 04-41.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Support vector machines for speaker verification and identification",
      "author" : [ "V. Wan", "W. Campbell" ],
      "venue" : "[Neural Networks for Signal Processing X, 2000. Proceedings of the 2000 IEEE Signal Processing Society Workshop ], 2, 775–784, IEEE (2000).",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Speaker recognition using neural networks and conventional classifiers",
      "author" : [ "K. Farrell", "R. Mammone", "K. Assaleh" ],
      "venue" : "Speech and Audio Processing, IEEE Transactions on 2(1), 194–205 (1994).",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Exploiting pca classifiers to speaker recognition",
      "author" : [ "W. Zhang", "Y. Yang", "Z. Wu" ],
      "venue" : "[Neural Networks, 2003. Proceedings of the International Joint Conference on ], 1, 820–823, IEEE (2003).",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Application of lda to speaker recognition",
      "author" : [ "Q. Jin", "A. Waibel" ],
      "venue" : "[Sixth International Conference on Spoken Language Processing ], (2000).",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Gmm based on local pca for speaker identification",
      "author" : [ "C. Seo", "K. Lee", "J. Lee" ],
      "venue" : "Electronics Letters 37(24), 1486–1488 (2001).",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Noise robust speaker identification using pca based genetic algorithm",
      "author" : [ "R. Islam", "F. Rahman" ],
      "venue" : "International Journal of Computer Applications IJCA 4(12), 27–31 (2010).",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Speaker identification by k-nearest neighbors: Application of pca and lda prior to knn",
      "author" : [ "J. Kacur", "R. Vargic", "P. Mulinka" ],
      "venue" : "[Systems, Signals and Image Processing (IWSSIP), 2011 18th International Conference on ], 1–4, IEEE (2011).",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Maximum likelihood from incomplete data via the em algorithm",
      "author" : [ "A. Dempster", "N. Laird", "D. Rubin" ],
      "venue" : "Journal of the Royal Statistical Society. Series B (Methodological) , 1–38 (1977).",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1977
    }, {
      "title" : "Theoretical, statistical, and practical perspectives on pattern-based classification approaches to the analysis of functional neuroimaging data",
      "author" : [ "A. O’Toole", "F. Jiang", "H. Abdi", "N. Pénard", "J. Dunlop", "M. Parent" ],
      "venue" : "Journal of cognitive neuroscience 19(11), 1735–1752 (2007).",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Why can lda be performed in pca transformed space",
      "author" : [ "J. Yang", "J. Yang" ],
      "venue" : "Pattern recognition 36(2), 563–566 (2003).",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "The peaking phenomenon in the presence of feature-selection",
      "author" : [ "C. Sima", "E. Dougherty" ],
      "venue" : "Pattern Recognition Letters 29(11), 1667–1674 (2008).",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Pca versus lda",
      "author" : [ "A. Martinez", "A. Kak" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on 23(2), 228–233 (2001).",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Reproducing the feature outputs of common programs using matlab and melfcc.m",
      "author" : [ "D. Ellis" ],
      "venue" : "(May 2005). http://http://labrosa.ee.columbia.edu/matlab/rastamat/mfccs.html.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Speech and language processing: An introduction to speech recognition",
      "author" : [ "D. Jurafsky", "J. Martin" ],
      "venue" : "Computational Linguistics and Natural Language Processing. 2nd Edn., Prentice Hall, ISBN 10.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 0
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Thus, it is sufficient to implement exhausted search in kp, kt ∈ (1, bM/2c) and p ∈ [0, 1] with interval 0.",
      "startOffset" : 84,
      "endOffset" : 90
    }, {
      "referenceID" : 14,
      "context" : "5), we found the range of N ∈ [15, 40] is appropriate for our database.",
      "startOffset" : 30,
      "endOffset" : 38
    }, {
      "referenceID" : 7,
      "context" : ")[k] 100 [24] 100 [26] 95 [8] 96 [36] 96 [32] 93.",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 7,
      "context" : ")[k] 100 [24] 100 [26] 95 [8] 94 [8] 93.",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 7,
      "context" : ")[k] 100 [24] 100 [26] 95 [8] 94 [8] 93.",
      "startOffset" : 33,
      "endOffset" : 36
    }, {
      "referenceID" : 9,
      "context" : "5 [10] 87 [8]",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 7,
      "context" : "5 [10] 87 [8]",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 0,
      "context" : "p∗ = arg max p∈[0,1] r(p) = arg max p∈[0,1] ∑S s=1 s=argmaxs∈[1,S] pg (s) 1 (X)+(1−p)g (s) 2 (X) S , (14)",
      "startOffset" : 15,
      "endOffset" : 20
    }, {
      "referenceID" : 0,
      "context" : "p∗ = arg max p∈[0,1] r(p) = arg max p∈[0,1] ∑S s=1 s=argmaxs∈[1,S] pg (s) 1 (X)+(1−p)g (s) 2 (X) S , (14)",
      "startOffset" : 38,
      "endOffset" : 43
    } ],
    "year" : 2016,
    "abstractText" : "Various algorithms for text-independent speaker recognition have been developed through the decades, aiming to improve both accuracy and efficiency. This paper presents a novel PCA/LDA-based approach that is faster than traditional statistical model-based methods and achieves competitive results. First, the performance based on only PCA and only LDA is measured; then a mixed model, taking advantages of both methods, is introduced. A subset of the TIMIT corpus composed of 200 male speakers, is used for enrollment, validation and testing. The best results achieve 100%, 96% and 95% classification rate at population level 50, 100 and 200, using 39dimensional MFCC features with delta and double delta. These results are based on 12-second text-independent speech for training and 4-second data for test. These are comparable to the conventional MFCC-GMM methods, but require significantly less time to train and operate.",
    "creator" : "LaTeX with hyperref package"
  }
}