{
  "name" : "1204.0140.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "ROGET’S THESAURUS AS A LEXICAL RESOURCE FOR NATURAL LANGUAGE PROCESSING",
    "authors" : [ "Mario Jarmasz" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "WordNet proved that it is possible to construct a large-scale electronic lexical database on the\nprinciples of lexical semantics. It has been accepted and used extensively by computational\nlinguists ever since it was released. Some of its applications include information retrieval,\nlanguage generation, question answering, text categorization, text classification and word sense\ndisambiguation. Inspired by WordNet's success, we propose as an alternative a similar resource,\nbased on the 1987 Penguin edition of Roget’s Thesaurus of English Words and Phrases.\nPeter Mark Roget published his first Thesaurus over 150 years ago. Countless writers, orators\nand students of the English language have used it. Computational linguists have employed\nRoget’s for almost 50 years in Natural Language Processing. Some of the tasks they have used it\nfor include machine translation, computing lexical cohesion in texts and constructing databases\nthat can infer common sense knowledge. This dissertation presents Roget’s merits by explaining\nwhat it really is and how it has been used, while comparing its applications to those of WordNet.\nThe NLP community has hesitated in accepting Roget’s Thesaurus because a proper machine-\ntractable version was not available.\nThis dissertation presents an implementation of a machine-tractable version of the 1987 Penguin\nedition of Roget’s Thesaurus – the first implementation of its kind to use an entire current\nedition. It explains the steps necessary for taking a machine-readable file and transforming it into\na tractable system. This involves converting the lexical material into a format that can be more\neasily exploited, identifying data structures and designing classes to computerize the Thesaurus.\nRoget’s organization is studied in detail and contrasted with WordNet’s.\nWe show two applications of the computerized Thesaurus: computing semantic similarity\nbetween words and phrases, and building lexical chains in a text. The experiments are performed\nusing well-known benchmarks and the results are compared to those of other systems that use\nRoget’s, WordNet and statistical techniques. Roget’s has turned out to be an excellent resource\nfor measuring semantic similarity; lexical chains are easily built but more difficult to evaluate.\nWe also explain ways in which Roget’s Thesaurus and WordNet can be combined.\nTo my parents, who are my most valued treasure.\ni\nTABLE OF CONTENTS"
    }, {
      "heading" : "1 INTRODUCTION..............................................................................................................................................1",
      "text" : "1.1 LEXICAL RESOURCES FOR NATURAL LANGUAGE PROCESSING ...................................................................1 1.2 ELECTRONIC LEXICAL KNOWLEDGE BASES ................................................................................................2 1.3 AN INTRODUCTION TO ROGET’S THESAURUS ...............................................................................................2\n1.3.1 The Roget’s Electronic Lexical Knowledge Base ..................................................................................3 1.4 GOALS OF THIS THESIS ................................................................................................................................3 1.5 ORGANIZATION OF THE THESIS ...................................................................................................................4\n1.5.1 Paper Map .............................................................................................................................................6"
    }, {
      "heading" : "2 THE USE OF THESAURI IN NATURAL LANGUAGE PROCESSING ...................................................7",
      "text" : "2.1 THE ROLE OF THESAURI IN NLP .................................................................................................................7 2.2 AN OVERVIEW OF ROGET’S THESAURUS ......................................................................................................7\n2.2.1 The Many Versions of Roget’s ...............................................................................................................7 2.2.2 A Comparison of Potential Candidates for Building an ELKB .............................................................8 2.3 NLP APPLICATIONS OF ROGET’S THESAURUS AND WORDNET......................................................................9 2.3.1 Using Roget’s Thesaurus in NLP.........................................................................................................10 2.3.2 Using WordNet in NLP ........................................................................................................................12 2.3.3 Combining Roget’s Thesaurus and WordNet in NLP ..........................................................................13 2.4 ROGET’S THESAURUS AS A RESOURCE FOR NLP ........................................................................................13 2.4.1 Why Have People Used Roget’s for NLP?...........................................................................................13 2.4.2 Why Do People Not Use Roget’s More for NLP?................................................................................14 2.4.3 A Machine-tractable Version of Roget’s extended With WordNet Relations.......................................14 2.5 THE EVALUATION OF A THESAURUS DESIGNED FOR NLP.........................................................................17\n3 THE DESIGN AND IMPLEMENTATION OF THE ELKB .......................................................................18\n3.1 GENERAL ORGANIZATION OF ROGET’S AND WORDNET..............................................................................18 3.2 THE COUNTS OF WORDS AND PHRASES IN ROGET’S AND WORDNET .........................................................23 3.3 THE SEMANTIC RELATIONS OF ROGET’S AND WORDNET ............................................................................24 3.4 ACCESSING ROGET’S AND WORDNET.........................................................................................................27 3.5 THE PREPARATION OF THE LEXICAL MATERIAL ........................................................................................29\n3.5.1 Errors and Exceptions in the Source Files ..........................................................................................30 3.6 THE JAVA IMPLEMENTATION OF THE ELKB...............................................................................................30\n3.6.1 The ELKBRoget and Related Classes ..................................................................................................32 3.6.2 The Category and Related Classes ......................................................................................................32 3.6.3 The RogetText and Related Classes .....................................................................................................33 3.6.4 The Index and Related Classes ............................................................................................................33 3.6.5 Morphological Transformations..........................................................................................................34 3.6.6 Basic Operations of the ELKB.............................................................................................................35\n4 USING ROGET’S THESAURUS TO MEASURE SEMANTIC SIMILARITY.........................................37\n4.1 THE NOTIONS OF SYNONYMY AND SEMANTIC SIMILARITY.........................................................................37 4.2 EDGE COUNTING AS A METRIC FOR CALCULATING SYNONYMY .................................................................38 4.3 AN EVALUATION BASED ON HUMAN JUDGMENTS ....................................................................................42\n4.3.1 The Experiment ....................................................................................................................................42 4.3.2 The Results...........................................................................................................................................43 4.4 AN EVALUATION BASED ON SYNONYMY PROBLEMS ................................................................................47 4.4.1 The Experiment ....................................................................................................................................47 4.4.2 The Results...........................................................................................................................................48 4.4.3 The Impact of Nouns on Semantic Similarity Measures ......................................................................50 4.4.4 Analysis of results obtained by the ELKB for RDWP questions ..........................................................51 4.5 SUMMARY OF RESULTS..............................................................................................................................52\nii"
    }, {
      "heading" : "5 AUTOMATING THE CONSTRUCTION OF LEXICAL CHAINS USING ROGET’S ...........................54",
      "text" : "5.1 PREVIOUS WORK ON LEXICAL CHAINS .....................................................................................................54 5.2 LEXICAL CHAIN BUILDING ALGORITHMS..................................................................................................55\n5.2.1 Step 1: Choose a Set of Thesaural Relations .......................................................................................55 5.2.2 Step 2: Select a Set of Candidate Words..............................................................................................58 5.2.3 Step 3: Build all Proto-chains for Each Candidate Word ...................................................................58 5.2.4 Step 4: Select the Best Proto-chains for Each Candidate Word ..........................................................59 5.2.5 Step 5: Select the Lexical Chains.........................................................................................................60 5.3 STEP-BY-STEP EXAMPLE OF LEXICAL CHAIN CONSTRUCTION ..................................................................60 5.4 A COMPARISON TO THE ORIGINAL IMPLEMENTATION...............................................................................63 5.5 COMPLEXITY OF THE LEXICAL CHAIN BUILDING ALGORITHM..................................................................66 5.6 EVALUATING LEXICAL CHAINS.................................................................................................................67 5.7 ABOUT THE STRAIGHTFORWARDNESS OF IMPLEMENTING LEXICAL CHAINS .............................................68\n6 FINDING THE HIDDEN TREASURES IN THE THESAURUS ................................................................69\n6.1 A QUANTITATIVE COMPARISON OF ROGET’S AND WORDNET ....................................................................69 6.2 COMBINING ROGET’S AND WORDNET ........................................................................................................72 6.3 IMPORTING SEMANTIC RELATIONS FROM WORDNET INTO ROGET’S ..........................................................74 6.4 AUGMENTING WORDNET WITH INFORMATION CONTAINED IN ROGET’S.....................................................76 6.5 OTHER TECHNIQUES FOR IMPROVING THE ELKB ......................................................................................77"
    }, {
      "heading" : "7 SUMMARY, DISCUSSION, AND FUTURE WORK ..................................................................................78",
      "text" : "7.1 SUMMARY .................................................................................................................................................78 7.2 CONCLUSIONS ...........................................................................................................................................79\n7.2.1 Building an ELKB from an Existing Lexical Resource ........................................................................79 7.2.2 Comparison of the ELKB to WordNet .................................................................................................80 7.2.3 Using the ELKB for NLP Experiments ................................................................................................80 7.2.4 Known Errors in the ELKB..................................................................................................................80 7.2.5 Improvements to the ELKB ..................................................................................................................80 7.3 FUTURE WORK ..........................................................................................................................................82 7.3.1 More Complete Evaluation of the ELKB .............................................................................................82 7.3.2 Extending the Applications Presented in the Thesis ............................................................................82 7.3.3 Enhancing the ELKB ...........................................................................................................................83"
    }, {
      "heading" : "8 REFERENCES.................................................................................................................................................84",
      "text" : "APPENDICES\nAppendix A: The Basic Functions and Use Cases of the ELKB.......................................................... A-1 Appendix B: The ELKB Java Documentation ......................................................................................B-1 Appendix C: The ELKB Graphical and Command Line Interfaces......................................................C-1 Appendix D: The Programs Developed for this Thesis ....................................................................... D-1 Appendix E: Converting the Pearson Codes into HTML-like Tags .....................................................E-1 Appendix F: Some Errors in the Pearson Source Files......................................................................... F-1 Appendix G: The 646 American and British Spelling Variations ....................................................... G-1 Appendix H: The 980-element Stop List............................................................................................. H-1 Appendix I: The Rubenstein and Goodenough 65 Noun Pairs.............................................................. I-1 Appendix J: The WordSimilarity-353 Test Collection..........................................................................J-1 Appendix K: TOEFL, ESL and RDWPquestions ................................................................................ K-1 Appendix L: A Lexical Chain Building Example ................................................................................L-1 Appendix M: The First Two Levels of the WordNet 1.7.1 Noun Hierarchy .......................................M-1\niii\nTABLES\nTABLE 3.1: THE HIERARCHICAL STRUCTURE OF ABSTRACT RELATIONS IN ROGET’S THESAURUS. ...................................22 TABLE 3.2: THE HIERARCHICAL STRUCTURE OF ABSTRACTION IN WORDNET ................................................................23 TABLE 3.3: 1987 ROGET’S THESAURUS STATISTICS. ......................................................................................................24 TABLE 3.4: WORDNET 1.7.1 STATISTICS. COMMON REFERS TO STRINGS BOTH IN WORDNET AND ROGET’S...................24 TABLE 3.5: THE SEMANTIC RELATIONS IN WORDNET. ..................................................................................................26 TABLE 3.6: TRANSFORMATION RULES FOR THE VARIOUS PARTS-OF-SPEECH. ...............................................................35 TABLE 4.1: DISTANCE VALUES ATTRIBUTED TO THE VARIOUS PATH LENGTHS IN THE THESAURUS. .............................39 TABLE 4.2: COMPARISON OF SEMANTIC SIMILARITY MEASURES USING THE MILLER AND CHARLES DATA...................43 TABLE 4.3: COMPARISON OF SEMANTIC SIMILARITY MEASURES USING THE RUBENSTEIN AND GOODENOUGH DATA...44 TABLE 4.4: COMPARISON OF SEMANTIC SIMILARITY MEASURES USING THE FINKELSTEIN ET AL. DATA. ......................45 TABLE 4.5: FINKELSTEIN ET AL. WORD PAIRS NOT FOUND IN ROGET’S THESAURUS. .....................................................45 TABLE 4.6: COMPARISON OF CORRELATION VALUES FOR THE DIFFERENT MEASURES USING THE MILLER AND CHARLES DATA...................................................................................................................................................................46 TABLE 4.7: COMPARISON OF THE SIMILARITY MEASURES FOR ANSWERING THE 80 TOEFL QUESTIONS.......................49 TABLE 4.8: COMPARISON OF THE SIMILARITY MEASURES FOR ANSWERING THE 50 ESL QUESTIONS. ...........................49 TABLE 4.9: COMPARISON OF THE SIMILARITY MEASURES FOR ANSWERING THE 300 RDWP QUESTIONS. .....................49 TABLE 4.10: COMPARISON OF THE MEASURES FOR ANSWERING THE 18 TOEFL QUESTIONS THAT CONTAIN ONLY NOUNS. ...............................................................................................................................................................50 TABLE 4.11: COMPARISON OF THE MEASURES FOR ANSWERING THE 25 ESL QUESTIONS THAT CONTAIN ONLY NOUNS. ...........................................................................................................................................................................50 TABLE 4.12: COMPARISON OF THE MEASURES FOR ANSWERING THE 154 RDWP QUESTIONS THAT CONTAIN ONLY NOUNS. ...............................................................................................................................................................51 TABLE 4.13: SCORE OF THE ELKB FOR THE RDWP QUESTIONS PER CATEGORY...........................................................52 TABLE 4.14: SUMMARY OF RESULTS – RANKING OF SIMILARITY MEASURES FOR THE EXPERIMENTS. ...........................52 TABLE 5.1: SCORES ATTRIBUTED TO THESAURAL RELATIONS IN THE META-CHAINS. ...................................................60 TABLE 6.1: DISTRIBUTION OF WORDS AND PHRASES WITHIN ROGET'S THESAURUS ORDERED BY CLASS NUMBER. ........70 TABLE 6.2: DISTRIBUTION OF WORDS AND PHRASES WITHIN ROGET'S THESAURUS ORDERED BY PERCENTAGE OF\nCOMMON STRINGS. ..............................................................................................................................................71\nFIGURES FIGURE 2.1: THE ROGET’S THESAURUS PARAGRAPH SOLDIER 722 N.........................................................................16 FIGURE 2.2: THE KINDS OF SOLDIER IN WORDNET 1.7.1 ...............................................................................................16 FIGURE 3.1: THE ROGET’S THESAURUS HEAD 864 WONDER..........................................................................................20 FIGURE 3.2: THE WORDNET 1.7.1 UNIQUE BEGINNERS. ................................................................................................20 FIGURE 3.3: OVERVIEW OF DAILY IN WORDNET 1.7.1. ..................................................................................................29 FIGURE 3.4: CLASS DIAGRAM OF THE ELKB. ...............................................................................................................31 FIGURE 4.1: ALL THE PATHS BETWEEN FELINE AND LYNX IN ROGET’S THESAURUS. .....................................................40 FIGURE 4.2: SOLUTION TO A RDWP QUESTION USING THE ELKB................................................................................47 FIGURE 5.2: THE EINSTEIN QUOTATION FOR WHICH LEXICAL CHAINS ARE BUILT. ........................................................61 FIGURE 5.3: THE FIRST SECTION OF THE OUTLAND ARTICLE. .......................................................................................65 FIGURE 5.4: ALGORITHM FOR BUILDING ALL PROTO-CHAINS. ......................................................................................66 FIGURE 6.1: THE ROGET'S THESAURUS HEAD 567 PERSPICUITY. ...............................................................................70 FIGURE 6.2: THE WORDNET SYNSETS FOR PERSPICUITY AND PERSPICUOUS. ................................................................71 FIGURE 6.3: THE FIRST PARAGRAPH OF THE ROGET’S THESAURUS HEAD 276 AIRCRAFT..............................................74 FIGURE 6.4: THE ROGET’S THESAURUS NOUN PARAGRAPH OF HEAD 42 DECREMENT. ...................................................75 FIGURE 6.5: THE WORDNET MINI-NET FOR THE NOUN DECREMENT. ...............................................................................75 FIGURE 6.6: THE ROGET’S NOUN PARAGRAPH OF HEAD 42 DECREMENT LABELLED WITH WORDNET RELATIONS..........76 FIGURE 6.7: THE ROGET’S THESAURUS BALL GAME 837 N. PARAGRAPH. ...................................................................76\niv\nACKNOWLEDGEMENTS\nI would like to acknowledge the help that I have received in preparing this thesis.\nGrateful thanks to:\n• Dr. Stan Szpakowicz, for having been my guide and mentor on this quest for the holy grail of computational lexicography.\n• Steve Crowdy, Denise McKeough and Martin Toseland from Pearson Education who helped us obtain the electronic copy of Roget’s Thesaurus and answered our many questions.\n• Dr. Peter Turney for having given me the idea to evaluate semantic similarity using synonymy questions and for his great insights on my research.\n• Vivi Nastase for always being available to discuss my work, and her contribution to combining Roget’s Thesaurus and WordNet.\n• Terry Copeck for his kind encouragement and having prepared the stop list used in building lexical chains.\n• Ted Pedersen and Siddharth Patwardhan for promptly developing and edge counting module for WordNet when requested.\n• Tad Stach for collecting 200 Reader’s Digest Word Power questions and helping to develop software to answer them using Roget’s Thesaurus.\n• Pierre Chrétien and Gilles Roy for implementing the graphical user interface to the electronic Roget’s Thesaurus.\n• Dr. Caroline Barrière, Dr. Ken Barker, Dr. Sylvain Delisle and Dr. Nathalie Japkowicz for their comments and feedback at the various stages of my thesis.\nI would also like to thank the members of my committee:\n• Dr. Jean-Pierre Corriveau, Carleton University\n• Dr. Stan Matwin, University of Ottawa\n• Dr. Stan Szpakowicz (thesis supervisor), University of Ottawa\nv\nPeter Mark Roget and his Thesaurus 1779 – 1869\nWhen Peter Mark Roget published his first Thesaurus\nover 150 years ago in 1852, he could not imagine that\nhis work would be used to further research in human\nlanguage technologies. He was born in London in\n1779, the son of Jean Roget, a Genevan protestant\npastor, and Catherine, the granddaughter of a French\nHuguenot who had fled to London after the revocation\nof the Edict of Nantes. Peter Mark’s whole life\nprepared him to become the author of the Treasury of\nWords. This physician’s preferences were anatomy\nand physiology, subjects that involve dissection and\nclassification. As expressed by Kent (Chapman, 1992, p. vii): “A lifetime of secretaryships for\nseveral learned societies had thoroughly familiarized him with the need for clarity and\nforcefulness of expression. […] It was Roget’s meticulous, precise way of looking at order, at\nplan and interdependence in animal economy that would eventually find expression in his unique\nand practical lexicographic experiment.” As a professor, Roget prepared a notebook of lists of\nrelated words and phrases in various orders to help him express himself in the best possible way.\nKirkpatrick (Kirkpatrick, 1998, p. xi) describes the manner in which Roget prepared the\nThesaurus once he retired from professional life: “Now, in his seventies, he was able to draw on\na lifetime’s experience of lecturing, writing and editing to make these lists into a coherent system\navailable for others to use. It took him four years, longer than he had thought, and required all his\norganizational skills and the meticulous attention to detail that had characterized his editing\nwork. Not only did the Thesaurus utilize all Roget’s competences, it also fulfilled a need for him:\nthe need, in a society changing with frightening speed, where the old moral and religious order\nwas increasingly in question, to reaffirm order, stability and unity, and through them the purpose\nof a universal, supernatural authority.” Roget supervised about twenty–five editions and printings of the Thesaurus until he died at the age of ninety.1\n1 Image of Peter Mark Roget: http://www.toadshow.com.au/anne/images/roget_peter.gif\nChapter 1. Introduction\n1"
    }, {
      "heading" : "1 Introduction",
      "text" : ""
    }, {
      "heading" : "1.1 Lexical Resources for Natural Language Processing",
      "text" : "Natural Language Processing (NLP) applications need access to vast numbers of words and\nphrases. There are now many ways of obtaining large-scale lexicons: query words on the Internet\n(Turney, 2001), use large corpora such as the Wall Street Journal or the British National Corpus,\nas well as extract information from machine-readable dictionaries (Wilks et al., 1996) or use\nelectronic lexical databases such as WordNet (Fellbaum, 1998). Which of these methods is best?\nOur intuition presented in this dissertation is that computational linguists should extend and\ncomputerize the work of lexicographers, professionals who take concrete decisions about words,\ntheir senses and how they should be arranged. Computer programmers have been reckless in\nimagining that they can do without linguists. To investigate this conjecture I have implemented a\nlarge-scale electronic lexical knowledge base designed on the 1987 Penguin edition of Roget’s\nThesaurus (Kirkpatrick, 1987).\nThe first use of Roget’s Thesaurus in NLP dates back to 1957 (Masterman); WordNet, a kind of\nthesaurus, has been available since 1991. These lexical resources have been used, among others,\nfor the following applications:\n• Machine Translation\n• proposing synonyms to improve word for word translations\n• Information Retrieval\n• performing word sense disambiguation\n• expanding query terms by proposing synonyms\n• detecting the expected answer types of questions\n• Information Extraction and Text Summarization\n• measuring semantic similarity between words\n• building lexical chains\nComputational linguists have given presentations on the following subjects in two workshops\nabout WordNet organized by the Association for Computational Linguistics (Harabagiu, 1998;\nMoldovan and Peters, 2001): Information Retrieval, Language Generation, Question Answering,\nChapter 1. Introduction\n2\nText Categorization, Text Classification and Word Sense Disambiguation. Researchers displayed\ntechniques for combining WordNet and Roget’s at both workshops. I discuss some of these\napplications further in Chapter 2."
    }, {
      "heading" : "1.2 Electronic Lexical Knowledge Bases",
      "text" : "Let’s begin by defining the term Electronic Lexical Knowledge Base (ELKB). It is a model for a\nlexical resource, implemented in software, for classifying, indexing, storing and retrieving words\nwith their senses and the connections that exist between them. It relies on a rich data repository\nto do so. This model defines explicit semantic relationships between words and word groups. It\nmaps out an automatic process for building an electronic lexicon. It is electronic not only\nbecause it is encoded in a digital format, but rather because it is computer-usable, or tractable.\nThe process for creating an ELKB presents all the steps involved in this task: from the\npreparation and acquisition of the lexical material, to defining the allowable operations on the\nvarious words and phrases. The use of a defined systematic approach to building an ELKB\nshould reduce the irregularities usually contained in handcrafted lexicons. My thesis is a way of\nverifying this intuition.\nMy ELKB has been created from the machine readable text files with the contents of the 1987\nPenguin’s Roget’s Thesaurus. It must maintain the information available in the printed\nThesaurus while it is put in a tractable format. Going from readable to tractable involves\ncleaning up and re-formatting the original files, deciding what services the ELKB should offer\nand implementing those services.\n1.3 An Introduction to Roget’s Thesaurus\nRoget’s Thesaurus is sometimes described as a dictionary in reverse (Wilks et al., 1996, p. 65).\nAccording to Roget, it is “… a collection of the words it [the English language] contains and of\nthe idiomatic combinations peculiar to it, arranged, not in alphabetical order as they are in a\nDictionary, but according to the ideas which they express” (Roget, 1852). The Thesaurus is a\ncatalogue of semantically similar words and phrases, divided into nouns, verbs, adjectives,\nadverbs and interjections. A phrase in Roget’s is not one in the grammatical sense, but rather a\ncollocation or an idiom, for example: fatal gift, poisoned apple, or have kissed the\nBlarney Stone. The reader perceives implicit semantic relations between groups of similar\nwords. This resembles how Miller describes WordNet as lexical information organized by word\nmeanings, rather than word forms (Miller, 1990). In WordNet, English nouns, verbs, adjectives\nand adverbs are organized into sets of near synonyms, called synsets, each representing a\nlexicalized concept. Semantic relations serve as links between the synsets.\nChapter 1. Introduction\n3\nA major strength of Roget’s Thesaurus is its unique system of classification “of the ideas which\nare expressible by language” (Roget, 1852). The original system was organized in six classes:\nAbstract relations, Space, Material World, Intellect, Volition, Sentient and Moral Powers. Roget\ndevised this system in the following way: “I have accordingly adopted such principles of\narrangement as appeared to me to be the simplest and most natural, and which would not require,\neither for their comprehension or application, any disciplined acumen, or depth of metaphysical\nor antiquarian lore” (ibid.). Within these six classes are sections, and under the sections are\nalmost 1,000 heads. This system of classification has withstood the test of time remarkably well,\nhardly changing in 150 years, a tribute to the robustness of Roget’s design.\nRoget’s Thesaurus is the creative wordsmith’s instrument, helping clarify and give shape to\none’s thoughts. “The assistance it gives is that of furnishing on every topic a copious store of\nwords and phrases, adapted to express all the recognizable shades and modifications of the\ngeneral idea under which those words and phrases are arranged” (ibid.). Roget’s is indeed, as its\nname’s Greek etymology indicates, a vast treasure house of English words and phrases.\n1.3.1 The Roget’s Electronic Lexical Knowledge Base\nThe objective of this thesis is to produce a machine-tractable version of the 1987 Penguin edition\nof Roget’s Thesaurus (Kirkpatrick, 1987) — the first implementation of an ELKB that uses an\nentire current edition. The FACTOTUM semantic network (Cassidy, 1996, 2000) is the first\nimplementation of a knowledge base derived from a version of Roget’s Thesaurus, using the\n1911 edition that is publicly available from the Project Gutenberg Web Site (Hart, 1991). A good\nELKB should have a large, modern vocabulary, a simple way of identifying the different word\nsenses, a clear classification system, and usage frequencies, several ways of grouping words and\nphrases to represent concepts, explicit links between the various units of meaning and an index\nof all words and phrases in the resource. It should also contain a lexicon of idiomatic expressions\nand proper nouns. Definitions of the words and phrases, as well as subcategorization\ninformation, akin to what can be found in a learner’s dictionary, would also be beneficial. The\nELKB constructed in the course of this research is not ideal, but it is sufficient to demonstrate\nthat Roget’s is a useful and interesting resource."
    }, {
      "heading" : "1.4 Goals of this Thesis",
      "text" : "The goal of this thesis is to investigate the usefulness of Roget’s Thesaurus for NLP. I expect\nthat it will be an effective alternative to WordNet. To achieve this goal, Roget’s must be first\ncomputerized, evaluated and applied to some interesting tasks. The quantitative and qualitative\nChapter 1. Introduction\n4\nevaluation constantly uses WordNet 1.7.1 as a benchmark. Other more cursory comparisons are\nmade with other lexical resources.\nThis treasure of the English language is exploited to create a new resource for computational\nlinguists. The goal is to computerize the Thesaurus: create a machine-tractable lexical\nknowledge base, and represent in it the explicit, and some of the implicit, relationships between\nwords. I have performed experiments using the system for measuring the semantic similarity\nbetween words and building lexical chains. This dissertation also presents steps for combining\nRoget’s and WordNet."
    }, {
      "heading" : "1.5 Organization of the Thesis",
      "text" : "Chapter 2 of the thesis gives an overview of the role of thesauri in NLP, discussing how such\nresources benefit research. The various versions of Roget’s Thesaurus and WordNet demand and\nreceive special attention.\nChapter 3 presents the details of the design and implementation of the ELKB. It explains all of\nthe necessary steps for transforming the Roget’s text files into a machine tractable format. These\nsteps discuss the necessary functionality of such a system. The construction of the ELKB fulfills\nthe first goal of this thesis.\nChapter 4 presents a measure of semantic similarity between words and phrases using the\nRoget’s ELKB. It presents a semantic distance measure and evaluates it using a few typical tests.\nI perform a comparison to WordNet based measures and other statistical techniques.\nChapter 5 presents an implementation of lexical chain construction using the Roget’s ELKB. It\ndiscusses in detail every design decision and includes a comparison to lexical chains built by\nhand using WordNet.\nChapter 6 discusses how to combine Roget’s Thesaurus and WordNet. It presents steps on how to\nlink the senses of the words and phrases included in both resources as well as how to add explicit\nsemantic relations to the Thesaurus.\nChapter 7 gives a summary of the thesis, discusses problems, and presents future work to be\ndone in improving the Roget ELKB and avenues for further applications.\nThe appendices contain detailed information regarding the following topics:\n• The design and implementation of the ELKB. Appendices A through C present the basic functions and use cases of the ELKB, the documentation as well as the graphical and\ncommand line interfaces to the system.\nChapter 1. Introduction\n5\n• The programs developed for this thesis and the preparation of the lexical material. Appendices D through F list the programs developed for this thesis, state the manner in\nwhich they must be used to prepare the lexical material for the ELKB, give a detailed\naccount of the conversion of the Pearson source files into the format used by the system,\nand present some errors found in the source files.\n• The word lists used by the ELKB. Appendices G and H show the 646 American and British spelling variations and the 980-element stop list used by the ELKB.\n• Experiments performed in this thesis. Appendices I through L present results of the semantic similarity and lexical chain building experiments.\n• Appendix M shows the first two levels of the WordNet 1.7.1 noun hierarchy.\nChapter 1. Introduction\n6"
    }, {
      "heading" : "1.5.1 Paper Map",
      "text" : "Parts of the dissertation have already been subject of papers:\nTopic dissertation sections previously described in\nThe use of Thesauri in NLP 2.1-2.3.2, 2.4 Jarmasz and Szpakowicz (2001a),\nJarmasz and Szpakowicz (2001c)\nThe Design of the ELKB 2.5, 3.1-3.4 Jarmasz and Szpakowicz (2001a),\nJarmasz and Szpakowicz (2001b),\nJarmasz and Szpakowicz (2001c)\nThe Implementation of the\nELKB\n3.5, 7.2 Jarmasz and Szpakowicz (2001b),\nJarmasz and Szpakowicz (2001b)\nUsing Roget’s to Measure\nSemantic Similarity\n4 Jarmasz and Szpakowicz (2001c),\nJarmasz and Szpakowicz (2003b)\nLexical Chain Construction\nUsing Roget’s\n5 Jarmasz and Szpakowicz (2003a)\nCombining Roget’s and\nWordNet\n2.3.3, 6.1-6.4, 7.3 Jarmasz and Szpakowicz (2001a),\nJarmasz and Szpakowicz (2001b),\nJarmasz and Szpakowicz (2001c)\nChapter 2. The Use of Thesauri in Natural Language Processing\n7"
    }, {
      "heading" : "2 The Use of Thesauri in Natural Language Processing",
      "text" : "This chapter presents the way in which this research field has used the two most celebrated\nthesauri in NLP, Roget’s and WordNet. It illustrates the history of both lexical resources,\nexplains their conception and original purpose. I analyze some of the various versions of Roget’s\nand elucidate the decision to use Penguin’s Roget’s Thesaurus of English Words and Phrases as\nthe source for my ELKB. This chapter further shows the manner in which researchers have used\nthe Thesaurus and WordNet in NLP and discusses the trend towards merging lexical resources.\nFinally, I present the desideratum for an ELKB based on Roget’s and outline its evaluation\nprocedure."
    }, {
      "heading" : "2.1 The Role of Thesauri in NLP",
      "text" : "Computational linguists have used dictionaries and thesauri in NLP ever since they first\naddressed the problem of language understanding. Ide and Véronis (1998) explain that machine-\nreadable dictionaries (MRDs) became a popular source of knowledge for language processing\nduring the 1980s. Much research activity focused on automatic knowledge extraction from\nMRDs to construct large knowledge bases. Thesauri using controlled vocabularies, for example\nthe Medical Subject Headings thesaurus (Medical Subject Headings, 1983), the Educational\nResources Information Centre thesaurus (Houston, 1984) and the IEE Inspec thesaurus which\ncontains technical literature in domains related to engineering (Inspec Thesaurus, 1985), have\nproven effective in information retrieval (Lesk, 1995). George Miller and his team constructed\nmanually WordNet, the only broad coverage, freely available lexical resource of its kind. I\npropose an ELKB similar in scope and function to WordNet and construct it automatically from\nthe most celebrated thesaurus.\n2.2 An Overview of Roget’s Thesaurus\nRoget’s Thesaurus, a collection of words and phrases arranged according to the ideas they\nexpress, presents a solid framework for a lexical knowledge base. Its explicit ontology offers a\nclassification system for all concepts that can be expressed by English words; its rich semantic\ngroups are a large resource that this thesis shows to be beneficial for NLP experiments. Yet this\nresource must be studied carefully before an ELKB can be devised."
    }, {
      "heading" : "2.2.1 The Many Versions of Roget’s",
      "text" : "The name “Roget’s” has become synonymous with the Thesaurus, yet most thesauri are not\nbased on the original classification system. Roget published the first edition of his Thesaurus of\nChapter 2. The Use of Thesauri in Natural Language Processing\n8\nEnglish Words and Phrases in May 1852. Already by 1854, Reverend B. Sears had copied it in\nthe United States, removing all of the phrases and placing all words and expressions borrowed\nfrom a foreign language in an Appendix (Kirkpatrick, 1998). Today, a quick search for Roget’s\nThesaurus at Amazon.com reveals well over 100 results with such titles as Roget's 21st Century\nThesaurus, Roget International Thesaurus, Roget's II: The New Thesaurus, Roget's Children's\nThesaurus, Bartlett's Roget's Thesaurus, Roget's Super Thesaurus and Roget's Thesaurus of the\nBible. Thesauri are now commonplace in written reference libraries or electronic formats, found\non the Internet (Lexico, 2001), in word processors or prepared for NLP research like the ELKB.\nWhich thesaurus is the best? A study of the publishers’ descriptions of their works suggests that\nthere are many excellent thesauri. The introduction to Roget’s International Thesaurus describes\nit as “a more efficient word-finder because it has a structure especially designed to stimulate\nthought and help you organize your ideas” (Chapman, 1992). Roget’s II: The New Thesaurus\ngives itself a clear mandate as “a book devoted entirely to meaning” (Master, 1995). Penguin’s\n1998 edition of Roget’s Thesaurus of English Words and Phrases (Kirkpatrick) is the present day\nthesaurus most similar to the original: “The unique classification system which was devised by\nPeter Mark Roget and is described fully in the Introduction, has withstood the test of time\nremarkably well. … it is eminently capable of absorbing new concepts and vocabulary and of\nreflecting what is happening in the English language as time goes by.”\nThe various thesauri boast anywhere from 200,000 to 300,000 words but size alone is not what\nmatters. Roget’s International Thesaurus (Chapman, 1992) lists close to 180 different kinds of\ntrees from the acacia to the zebrawood. This enumeration is not nearly enough to be exhaustive,\nnor does it really help to describe all aspects of the concept 310 Plants. If the size of a\nthesaurus determined a “winner”, one could simply publish a list of plants, animals, and so on.\nThis thesis demonstrates that the classification system of the 1987 edition of Penguin’s Roget’s\nThesaurus is its great strength. It is interesting to note that Roget’s source of inspiration for the\nThesaurus indeed was plant taxonomy, so it is much more than a mere catalogue. Every word or\nphrase is carefully placed in the hierarchy. While a good thesaurus must contain many words, it\nshould above all classify them methodically, according to the ideas which they express.\n2.2.2 A Comparison of Potential Candidates for Building an ELKB\nIn this sea of thesauri, only four candidates remain as contenders for building an electronic\nlexical knowledge base: the 1911 Project Gutenberg (Hart, 1991) edition, Patrick Cassidy’s\nFACTOTUM semantic network (Cassidy, 1996, 2000), HarperCollins’ Roget’s International\nThesaurus and Penguin’s Roget’s Thesaurus of English Words and Phrases. The first two\nresources are electronic versions of an early Roget’s Thesaurus, the latter two printed versions.\nChapter 2. The Use of Thesauri in Natural Language Processing\n9\nThe 1911 Project Gutenberg (Hart, 1991) edition is a text file derived from the 1911 version of\nRoget's Thesaurus. MICRA, INC. prepared it in May 1991. This is a public domain version of\nthe Thesaurus. This version consists of six classes, 1035 major subject headings and roughly\n41,000 words, 1,000 of them added by MICRA Inc. The original classification system has been\npreserved. This 1911 version is the foundation for FACTOTUM, which is described as a\nsemantic network organized very similarly to Roget’s but with a more explicit hierarchy and 400\nsemantic relations that link the individual words to one of the 1035 heads (Cassidy, 2000)\nThe fifth edition of Roget’s International Thesaurus is “the most up-to-date and definitive\nthesaurus” according to its editor, Robert L. Chapman (1992). It consists of fifteen classes, 1073\nheadwords and more than 325,000 words. Previous printed editions of Roget’s International\nThesaurus have been successfully used in NLP for word sense disambiguation, information\nretrieval and computing lexical cohesion in texts. We present examples of such experiments in\nsection 2.3.1. As far as we know, no electronic versions of Roget’s International Thesaurus have\nbeen made available to the public at large.\nPenguin’s Roget’s Thesaurus of English Words and Phrases, edited by Betty Kirkpatrick (1998)\nis “a vast treasure-house according to ideas and meanings” as advertised by Penguin Books. It consists of six classes, 990 headwords and more than 250,000 words. It is has maintained a\nclassification system similar to that of the original edition, and the vocabulary has been updated to reflect the changes since the mid-19th century.\nTwo criteria for choosing the starting point for the electronic lexical knowledge base stand out:\nan extensive and up-to-date vocabulary and a classification system very similar to that of the original Thesaurus. The constraint on the classification system allows investigating how well it\nhas stood the test of time. Only the 1911 version and the Penguin edition have kept the original\nclassification system, but the latter is more complete. For this reasons I have chosen Roget’s\nThesaurus of English Words and Phrases as the foundation of my electronic lexical knowledge\nbase.\n2.3 NLP Applications of Roget’s Thesaurus and WordNet\nIt is commonly accepted that a lexical resource should not be prepared if there is no specific task\nfor it. I have developed what is often referred to as a “vanilla flavor” lexicon – a resource that\nhas a broad, general coverage of the English language (Wilks et al., 1996). The fear in creating\nsuch a lexical resource is that by trying to be suitable for all applications, it ends up being useful\nfor none. WordNet is also such an instrument and it has proven to be invaluable to the NLP\ncommunity.\nChapter 2. The Use of Thesauri in Natural Language Processing\n10\nBoth Roget’s Thesaurus and WordNet were not initially intended for NLP. The first was planned\nfor writers and orators, for those who are “painfully groping their way and struggling with the\ndifficulties of composition” (Kirkpatrick, 1998), the latter as a model for psycholinguists,\ndevised as “an on-line representation of a major part of the English lexicon that aims to be\npsychologically realistic” (Beckwith et al., 1991). Like penicillin, WordNet is now considered a\npanacea. Roget’s Thesaurus, whose potential I intend to demonstrate, should become equally\neffective. The following sections describe how both resources have been used in NLP.\n2.3.1 Using Roget’s Thesaurus in NLP\nRoget’s Thesaurus has been used sporadically in NLP since about 1950 when it was first put into\na machine-readable form (Masterman, 1957). The most notable applications include machine\ntranslation (Masterman, 1957, Sparck Jones, 1964), information retrieval (Driscoll, 1992,\nMandala et al., 1999), computing lexical cohesion in texts (Morris and Hirst, 1991) and word\nsense disambiguation (Yarowsky, 1992).\nMasterman (ibid.) used a version on punched cards to improve word-for-word machine\ntranslation. She demonstrated how the Thesaurus could improve an initially unsatisfactory\ntranslation. As an example, Masterman explains that the Italian phrase “tale problema si\npresenta particolarmente interressante”, translated word-for-word as “such problems\nself-present particularly interesting”, can be retranslated as “such problems strike\none as, [or prove] particularly interesting”. The essence of this “thesaurus\nprocedure” is to build, for every significant word of the initial translation, a list of Heads under\nwhich they appear, and to look for intersections between these lists. Replacements for\ninaccurately translated words or phrases are selected from the Head that contains the most words\nfrom the initial translation. Alternatives can be manually selected by choosing a better word from\nthis Head. Masterman notes that the sense of a word, as used in a sentence, can be uniquely\nidentified by knowing to which Head this sense belongs.\nSparck Jones (1964) realized that the Thesaurus in its present form had to be improved for it to\nbe effective for machine translation. She therefore set out to create the ideal machine translation\ndictionary that “… has to be a dictionary in the ordinary sense: it must give definitions or\ndescriptions of the meanings of words. It must also, however, give some indication of the kinds\nof contexts in which the words are used, that is, must be a ‘semantic classification’ as well as a\ndictionary” (ibid.). Sparck Jones believed that by classifying dictionary definitions using Roget’s\nHeads she could construct the resource that she required.\nChapter 2. The Use of Thesauri in Natural Language Processing\n11\nUnder Roget’s headwords there are groups of closely semantically related words, located in the\nsame paragraph and separated by semicolons. Sparck Jones (1964) built “rows” consisting of\nclose semi-synonyms, using the Oxford English Dictionary. She then attempted to classify the\nrows according to the common membership in headwords. Here are some rows that have been\nclassified as belonging to the Head activity:\nactivity animation activity liveliness animation activity animation movement activity action work activity energy vigour\nSparck Jones later used these techniques for information retrieval. Other people (Driscoll, 1992\nand Mandala et al., 1999) have also used the Thesaurus for this purpose, but this time to expand\nthe initial queries.\nHalliday and Hasan (1976) explain that a cohesive text is identified by the presence of strong\nsemantic relations between the words that it is made up of. Morris and Hirst (1991) calculated\nlexical cohesion, which they call “the result of chains of related words that contribute to the\ncontinuity of lexical meaning” within texts. The fourth edition of Roget’s International\nThesaurus (Chapman, 1977) was used to compute manually lexical chains, which are indicators\nof lexical cohesion. Stairmand (1994) automated this process using the 1911 edition of Roget’s\nThesaurus but did not obtain good results, as this version of the Thesaurus contains limited and\nantiquated vocabulary. Ellman (2000) once again used the 1911 edition to build lexical chains so\nas to construct a representation of a text’s meaning from its content. An implementation of a\nlexical chain building system that uses the ELKB is presented in Chapter 5.\nWord sense disambiguation must be the most popular use of Roget’s Thesaurus in NLP.\nYarowsky (1992) defines the sense of a word as “the categories listed for that word in Roget’s\nInternational Thesaurus” The 1000 headwords allow to partition the major senses of a word\nquite accurately. To perform sense disambiguation, one must determine under which headword\nthe given sense belongs. This can be determined by using the context of a polysemous word and\nthe words of a given class. Other people who have used Roget’s for word sense disambiguation\ninclude Bryan (1973, 1974), Patrick (1985), Sedelow and Mooney (1988), Kwong (2001b).\nRoget’s Thesaurus has also been used to measure semantic similarity with extremely good\ncorrelation with human judgments. It was first accomplished by McHale (1998) using the\ntaxonomy of Roget’s International Thesaurus, third edition (Berrey and Carruth, 1962). Another\nimplementation, using the ELKB, is discussed in Chapter 4.\nChapter 2. The Use of Thesauri in Natural Language Processing\n12\nScientists have also attempted to build databases that can infer common sense knowledge, for\nexample that “blind men cannot see”, using Roget’s. Some implementations include Cassidy’s\nFACTOTUM semantic network (Cassidy, 2000) and the work of Sedelow and Sedelow (1992).\n2.3.2 Using WordNet in NLP\nGeorge Miller first thought of WordNet in the mid-1960s. The WordNet project started in 1985\nand seven different versions have been released since version 1.0 in June of 1991 (Miller,\n1998a). Although it is an electronic lexical database based on psycholinguistic principles, it has\nbeen used almost exclusively in NLP. For numerous research groups around the world it is now a\ngeneric resource. A tribute to its success are the Coling-ACL ’98 workshop entitled Usage of\nWordNet in Natural Language Processing Systems (Harabagiu, 1998), the NAACL 2001 WordNet and Other Lexical Resources workshop (Moldovan and Peters, 2001), the 1st\nInternational WordNet Conference (Fellbaum, 2002) and the close to 300 references in the\nWordNet Bibliography (Mihalcea, 2003). Some of the issues discussed at the WordNet\nworkshops and conference include determining for which applications WordNet is a valuable\nresource, evaluating if WordNet can be used to develop high performance word sense\ndisambiguation algorithms and extending WordNet for specific tasks. The semantic relations in\nWordNet have often been studied, even exploited for a variety of applications, for example\nmeasuring semantic similarity (Budanitsky and Hirst, 2001), and encoding models for answers\ntypes in open-domain question answering systems (Pasca and Harabagiu, 2001). Using WordNet\nas a blueprint, various multilingual lexical databases have been implemented, the first one being\nEuroWordNet (Vossen, 1998), a multilingual electronic lexical database for Dutch, Italian,\nSpanish, German, French and Estonian.\nDue to the limitations of the printed version of Roget’s Thesaurus, many researchers have opted\nfor WordNet when attempting to extend their algorithms beyond toy problems. Systems that\nperform word sense disambiguation have been implemented using this electronic lexical\ndatabase (Sussna, 1993; Okumura and Honda, 1994; Li et al., 1995; Mihalcea and Moldovan,\n1998; Kwong, 2001b; Fellbaum et al.2001). WordNet’s taxonomy has been exploited to measure\nsemantic similarity – for a survey of these metrics see (Budanitsky and Hirst, 2001). Lexical\nchains (Morris and Hirst, 1991) were first built by hand using Roget’s International Thesaurus.\nHirst and St-Onge (1998) later implemented them using WordNet. Lexical chains built using\nWordNet have been applied in text summarization by Barzilay and Elhadad (1997), Brunn et al.\n(2001) as well as Silber and McCoy (2000, 2002). It is impossible to evaluate how many people\nwho have used WordNet would have used Roget’s Thesaurus were it in a machine-tractable form\nand free.\nChapter 2. The Use of Thesauri in Natural Language Processing\n13\n2.3.3 Combining Roget’s Thesaurus and WordNet in NLP\nA current trend in NLP is towards combining lexical resources to attempt to overcome their\nindividual weaknesses. Roget’s taxonomy has been used to compute the semantic similarity\nbetween words and the results have been compared to those of the same experiments using\nWordNet (Mc Hale, 1998). Although the two resources were not actually combined, and it is not\nclear which system produces better results, it is an interesting investigation, which reiterates the\nfact that both can be used for the same applications. This experiment is repeated in Chapter 4\nusing the ELKB. Others have attempted to enrich WordNet with Roget’s Thesaurus to\nsupplement the lack of relations between part of speech and proper nouns by those available in\nthe Thesaurus (Mandala et al., 1999). Kwong (1998, 2001a) presents an algorithm for aligning\nthe word senses of the nouns in the 1987 edition of Roget’s Thesaurus of English Words and\nPhrases an those in WordNet 1.6. A limited number of noun senses has been mapped. Nastase\nand Szpakowicz (2001) perform a similar experiment on a smaller set of words but use a larger\nset of parts-of-speech: nouns, adjectives and adverbs. An implementation of the mapping of\nword senses in the ELKB and WordNet 1.7 is discussed in Chapter 6. This is of importance, since\nas Kwong states (1998): “In general we cannot expect that a single resource will be sufficient for\nany NLP applications. WordNet is no exception, but we can nevertheless enhance its utility”. The\nanalogy can be drawn with Roget’s: alone it cannot serve all tasks; combined with WordNet the\nThesaurus will be enriched.\n2.4 Roget’s Thesaurus as a Resource for NLP"
    }, {
      "heading" : "2.4.1 Why Have People Used Roget’s for NLP?",
      "text" : "This chapter has presented examples of NLP applications that used Roget’s Thesaurus. What\nwere the incentives? The structure based on the hierarchy of categories is very simple to\ncomputerize and use, as was demonstrated by Masterman (1957) and Sparck Jones (1964). No\n“reverse engineering” is required to access this lattice of concepts, as it would have to be if one\nwere building it from a dictionary. Roget’s has a long established tradition and is believed to be\nthe best thesaurus. It is, however, not machine tractable in the way WordNet is. To quote McHale\n(1998): “Roget’s remains, though, an attractive lexical resource for those with access to it. Its\nwide, shallow hierarchy is densely populated with nearly 200,000 words and phrases. The\nrelationships among the words are also much richer than WordNet’s IS-A or HAS-PART links.\nThe price paid for this richness is a somewhat unwieldy tool with ambiguous links”. Indeed, the\nextreme difficulty of exploiting implicit semantic relations is one of the reasons why the\nThesaurus has been considered but discarded by many researchers.\nChapter 2. The Use of Thesauri in Natural Language Processing\n14"
    }, {
      "heading" : "2.4.2 Why Do People Not Use Roget’s More for NLP?",
      "text" : "It is difficult for a computer to use a resource prepared for humans. WordNet is simply easier to\nuse, as explained by Hirst and St-Onge (1995): “Morris and Hirst were never able to implement\ntheir algorithm for finding lexical chains with Roget’s because no on-line copy of the thesaurus\nwas available to them. However, the subsequent development of WordNet raises the possibility\nthat, with a suitable modification of the algorithm, WordNet could be used in place of Roget’s”.\nAn electronic version of the 1911 edition of Roget’s Thesaurus has been available since 1991.\nThis edition also proves inadequate for NLP, as Hirst and St-Onge (1995) describe: “Recent\neditions of Roget’s could not be licensed. The on-line version of the 1911 edition was available,\nbut it does not include the index that is crucial to the algorithm. Moreover, because of its age, it\nlacks much of the vocabulary necessary for processing many contemporary texts, especially\nnewspaper and magazine articles and technical papers.” Stairmand (1994) confirmed that it is not\npossible to implement a lexical chainer using the on-line 1911 version.\nThe literature shows that only Penguin’s Roget’s Thesaurus of English Words and Phrases,\nHarperCollins’ Roget’s International Thesaurus as well as the 1911 edition have been used for\nNLP research. Choosing the concept hierarchy of one or the other does not ensure a definitive\nadvantage, as Yarowsky (1992) states: “Note that this edition of Roget’s Thesaurus [Chapman,\n1977] is much more extensive than the 1911 version, though somewhat more difficult to obtain\nin electronic form. One could use other concept hierarchies, such as WordNet (Miller, 1990) or\nthe LDOCE subject codes (Slator, 1992). All that is necessary is a set of semantic categories and\na list of the words in each category.” Roget’s is more than a concept hierarchy, but the elements\nthat are most easily accessed using a printed version are the classification system and the index.\nFor this reason, computational linguists have limited their experiments to computerizing and\nmanipulating the index.\nThe availability of the lexical material of a current edition of Roget’s Thesaurus is the major\nhindrance for using this resource in NLP. The publishers of Roget’s do not make it easy to obtain\nan electronic copy.\n2.4.3 A Machine-tractable Version of Roget’s extended With WordNet Relations\nRoget’s Thesaurus has many undeniable advantages. It is based on a well-constructed concept\nclassification, and its entries were written by professional lexicographers. It contains around\n250,000 words compared to WordNet’s almost 200,000. Roget’s employs a rich set of semantic\nrelations, most of them implicit (Cassidy, 2000). These relationships are one of the most\ninteresting qualities. Morris and Hirst (1991) say: “A thesaurus simply groups related words\nChapter 2. The Use of Thesauri in Natural Language Processing\n15\nwithout attempting to explicitly name each relationship. In a traditional computer database, a\nsystematic semantic relationship can be represented by a slot value for a frame, or by a named\nlink in a semantic network. If it is hard to classify a relationship in a systematic semantic way, it\nwill be hard to represent the relationship in a traditional frame or semantic network formalism”.\nA machine-tractable thesaurus will possibly present a better way of organizing semantic\nrelations, although the challenge will be to label them explicitly.\nRoget’s Thesaurus does not have some of WordNet’s shortcomings, such as the lack of links\nbetween parts of speech and the absence of topical groupings. The clusters of closely related\nwords are obviously not the same in both resources. WordNet relies on a set of about 15 semantic\nrelations, which I present in Chapter 3. Search in this lexical database requires a word and a\nsemantic relation; for every word some (but never all) of 15 relations can be used in search. It is\nimpossible to express a relationship that involves more than one of the 15 relations: it cannot be\nstored in WordNet. The Thesaurus can link the noun bank, the business that provides financial\nservices, and the verb invest, to give money to a bank to get a profit, as used in the following\nsentences, by placing them in a common head 784 Lending.\n1. Mary went to the bank yesterday.\n2. She invested $5,000.00 in mutual funds.\nThis notion cannot be described using WordNet’s semantic relations. While an English speaker\ncan identify a relation not provided by WordNet, for example that one invests money in a bank,\nthis is not sufficient for a computer system. The main challenge is to label such relations\nexplicitly. I expect to be able to identify a good number of implicit semantic relations in Roget’s\nby combining it with WordNet .This process is described in Chapter 6.\nWordNet was built using different linguistic sources. They include the Brown Corpus (Francis\nand Kucera, 1982), the Basic Book of Synonyms and Antonyms (Urdang, 1978a), The Synonym Finder (Urdang, 1978b), the 4th edition of Roget’s International Thesaurus (Chapman, 1977) and\nRalph Grishman’s COMLEX (Macleod, et al., 1994). Many of the lexical files were written by\ngraduate students hired part-time. Penguin’s Roget’s Thesaurus of English Words and Phrases is\nprepared by professional lexicographers and validated using data from the Longman Corpus\nNetwork of many millions words. The carefully prepared Thesaurus is therefore more consistent\nthan WordNet as is shown by comparing the soldier paragraph in the Thesaurus Head 722\nCombattant. Army. Navy. Air Force and the kinds of soldier listed in WordNet 1.7.1.\nChapter 2. The Use of Thesauri in Natural Language Processing\n16\nThe Roget’s paragraph does not contain any proper nouns, which at first may seem as a\nweakness compared to WordNet, but is the most rational decision for such a lexical resource, as\nWordNet’s list contains but an infinitely small number of the world’s great soldiers. Although\nWordNet lists Allen, Bayard, Borgia, Higginson, Kosciusko, Lafayette, Lawrence, Lee,\nMohamed Ali, Morgan, Percy, Peron, Smuts and Tancred other arguably even greater, such as\nChapter 2. The Use of Thesauri in Natural Language Processing\n17\nAlexander, Caesar, Charlemagne, Timur, Genghis Khan, Napoleon, Nelson are absent. This\nexample is not sufficient to prove that the Thesaurus is a more carefully crafted resource than\nWordNet, but is enough to indicate that Roget’s is a very good foundation for an electronic\nlexical knowledge base and that WordNet is not perfect. Extending Roget’s with WordNet can\nonly make it better, as the combined information makes for a richer semantic network. Although\nWordNet has become the de facto standard electronic lexical knowledge base for NLP, there is\nno reason why it should be the only one. The ELKB is built from Roget’s Thesaurus and can be\ncombined with WordNet. This results in an interesting alternative for solving NLP problems."
    }, {
      "heading" : "2.5 The Evaluation of a Thesaurus Designed for NLP",
      "text" : "The evaluation of the ELKB must be functional, quantitative and qualitative. It is functional in\nthe sense that the ELKB must allow the same manipulations as the printed Thesaurus: word and\nphrase lookup, browsing via the hierarchy, random browsing and following links. Experiments\nthat have been previously done by hand, for example calculating the distance between words or\nphrases by counting their relative separation in Roget’s, must be automated. Chapter 3 presents\nthe various use scenarios and discusses how they are implemented in the ELKB. The evaluation\nis quantitative in the sense that the ELKB should have a comparable number of word senses as\nWordNet. This evaluation is performed in Chapter 3. It finally is qualitative in the sense that the\nwords and phrases contained in Roget’s should perform a wide variety of NLP applications. The\nThesaurus is put to the test by calculating semantic similarity between words and phrases,\nexplained in Chapter 4, and in the task of building lexical chains, described in Chapter 5. The\nexperiments involving mapping Roget’s senses onto WordNet senses in Chapter 6 expose the\ndifferences in lexical material between both resources.\nChapter 3. The Design and Implementation of the ELKB\n18\n3 The Design and Implementation of the ELKB\nThe preliminary step in evaluating the usefulness of Roget’s Thesaurus for NLP is the\nimplementation of the ELKB. This chapter describes the steps involved in computerizing the\nThesaurus, from the details of how this resource is organized, to a Java implementation of the\nELKB. I explain Roget’s structure, behavior and function as well as present the way in which the\nELKB works. This chapter shows the steps involved in transforming the source material into a\nformat that is adequate for further processing, and discusses the required data structures for the\nsystem. It finally illustrates some scenarios of how the ELKB is to be used. I perform a\ncomparison to WordNet, the de facto standard for electronic lexical databases, at the various\nstages of the design and implementation of this electronic resource.\n3.1 General organization of Roget’s and WordNet\nOntologies have been used in Artificial Intelligence since the 1950s. Researchers agree that they\nare extremely useful for a wide variety of applications but do not agree on their contents and\nstructure (Lehmann, 1995). Roget proposed a classification system that is essentially a taxonomy\nof ideas that can be expressed in the English language. His system has a very Victorian bias to it,\nbut this thesis demonstrates that it is useful to modern day researchers nonetheless. Let’s\nexamine its properties and compare it to the ontology of nouns implicitly present in WordNet.\nRoget’s ontology is headed by six Classes. The first three Classes cover the external world:\nAbstract Relations deals with such ideas as number, order and time; Space is concerned with\nmovement, shapes and sizes, while Matter covers the physical world and humankind’s\nperception of it by means of five senses. The remaining Classes deal with the internal world of\nhuman beings: the mind (Intellect), the will (Volition), the heart and soul (Emotion, Religion and\nMorality). There is a logical progression from abstract concepts, through the material universe, to\nmankind itself, culminating in what Roget saw as mankind’s highest achievements: morality and\nreligion (Kirkpatrick, 1998). Class Four, Intellect, is divided into Formation of ideas and\nCommunication of ideas, and Class Five, Volition, into Individual volition and Social volition. In\npractice, therefore, the Thesaurus is headed by eight Classes. This is the structure that has been\nadopted for the ELKB.\nA path in Roget’s ontology always begins with one of the Classes. It branches to one of the 39\nSections, then to one of the 79 Sub-Sections, then to one of the 596 Head Groups and finally to\none of the 990 Heads. Each Head is divided into paragraphs grouped by parts of speech: nouns,\nadjectives, verbs and adverbs. According to Kirkpatrick (1998) “Not all Heads have a full\nChapter 3. The Design and Implementation of the ELKB\n19\ncomplement of parts of speech, nor are the labels themselves applied too strictly, words and\nphrases being allocated to the part-of-speech which most closely describes their function”. Much\nis left to the lexicographers’ intuitions, which makes it hard to use Roget’s as it is for NLP\napplications. Finally a paragraph is divided into semicolon groups of semantically closely related\nwords. These paths create a graph in the Thesaurus since they are interconnected at various\npoints. An example of a Head in Roget’s is 864 Wonder. I show it here with the first paragraph\nfor every part-of-speech as well as its path in the ontology:\nClass six: Emotion, religion and morality\nSection two: Personal emotion\nSub-section: Contemplative\nHead Group: 864 Wonder – 865 Lack of wonder\nHead: 864 Wonder\nN. wonder, state of wonder, wonderment, raptness; admiration, hero worship, 887 love; awe, fascination; cry of wonder, gasp of admiration, whistle, wolf wolf, exclamation, exclamation mark; shocked silence, 399 silence; open mouth, popping eyes, eyes on stalks; shock, surprise, surprisal, 508 lack of expectation; astonishment, astoundment, amazement; stupor, stupefaction; bewilderment, bafflement, 474 uncertainty; consternation, 854 fear.\n…\nAdj. wondering, marvelling, admiring, etc. vb.; awed, awestruck, fascinated, spellbound, 818 impressed; surprised, 508 inexpectant; astonished, amazed, astounded; in wonderment, rapt, lost in wonder, lost in amazement, unable to believe one's eyes or senses; wide-eyed, round-eyed, pop-eyed, with one's eyes starting out of one's head, with eyes on stalks; open-mouthed, agape, gaping; dazzled, blinded; dumbfounded, dumb, struck dumb, inarticulate, speechless, breathless, wordless, left without words, silenced, 399 silent; bowled over, struck all of a heap, thunderstruck; transfixed, rooted to the spot; dazed, stupefied, bewildered, 517 puzzled; aghast, flabbergasted; shocked, scandalized, 924 disapproving.\n…\nVb. wonder, marvel, admire, whistle; hold one's breath, gasp, gasp with admiration; hero-worship, 887 love; stare, gaze and gaze, goggle at, gawk, open one's eyes wide, rub one's eyes, not believe one's eyes; gape, gawp, open one's mouth, stand in amazement, look aghast, 508 not expect; be awestruck, be overwhelmed, 854 fear; have no words to express, not know what to say, be reduced to silence, be struck dumb, 399 be silent.\nChapter 3. The Design and Implementation of the ELKB\n20\nMiller took a different approach to constructing an ontology for WordNet. Only nouns are clearly\norganized into a hierarchy. Adjectives, verbs and adverbs are organized individually into various\nwebs that are difficult to untangle. This decision has been based on pragmatic reasons more than\non theories of lexical semantics, as Miller (1998b) admits: “Partitioning the nouns has one\nimportant practical advantage: it reduces the size of the files that lexicographers must work with\nand makes it possible to assign the writing and editing of the different files to different people.”\nIndeed, organizing WordNet’s more than 100,000 nouns must have required a fair amount of\nplanning.\nIn WordNet version 1.7.1 noun hierarchies are organized around nine unique beginners. A\nunique beginner is a synset which is found at the top of the noun ontology. Most synsets are\naccompanied with a gloss which is a short definition of the synonym set. The following are the\nunique beginners:\nentity, physical thing (that which is perceived or known or inferred to have its own physical existence (living or nonliving)) psychological feature, (a feature of the mental life of a living organism) abstraction, (a general concept formed by extracting common features from specific examples) state, (the way something is with respect to its main attributes; \"the current state of knowledge\"; \"his state of health\"; \"in a weak financial state\") event, (something that happens at a given place and time) act, human action, human activity, (something that people do or cause to happen) group, grouping, (any number of entities (members) considered as a unit) possession, (anything owned or possessed) phenomenon, (any state or process known through the senses rather than by intuition or reasoning)\nFigure 3.2: The WordNet 1.7.1 unique beginners.\nChapter 3. The Design and Implementation of the ELKB\n21\nAll of the other nouns can eventually be traced back to these nine synsets. If these are considered\nanalogous to Roget’s Classes, the next level of nouns can be considered as the Sections. In all,\nthe unique beginners have 161 noun synsets directly linked to them (Appendix M). The number\nof nouns that are two levels away from the top of the noun hierarchies have not been identified,\nbut if even a quarter of the WordNet nouns can be found here, they would represent close to\n37,000 words.\nMiller (1998b) mentions that WordNet’s noun ontology is relatively shallow in the sense that it\nseems to have a limited number of levels of specialization. In theory, of course, there is no limit\nto the number of levels an inheritance system can have. Lexical inheritance systems, however,\nseldom go more than 10 or 12 levels deep, and the deepest examples usually contain technical\ndistinctions that are not part of the everyday vocabulary. For example, a Shetland pony is a pony,\na horse, an equid, an odd-toe ungulate, a placental mammal, a mammal, a vertebrate, a chordate,\nan animal, an organism, an object and an entity: 13 levels, half of them technical (ibid.).\nThe IS-A relations connect WordNet’s noun hierarchy in a vertical fashion, whereas the IS-\nPART, IS-SUBSTANCE, IS-MEMBER and the HAS-PART, HAS-SUBSTANCE, HAS-\nMEMBER relations allow for horizontal connections. This allows interconnecting various word\nnets, represented by the synsets, into a large web.\nA simple quantitative comparison of the two ontologies is difficult. Roget (1852) claims that\norganizing words hierarchically is very useful: “In constructing the following system of\nclassification of the ideas which are expressible by language, my chief aim has been to obtain the\ngreatest amount of practical utility.” Miller, on the other hand, feels that it is basically impossible\nto create a hierarchy for all words, since: “these abstract generic concepts [which make up the\ntop levels of the ontology] carry so little semantic information; it is doubtful that people could\nagree on appropriate words to express them.” (1998b) The Tabular synopsis of categories, which\nrepresents the concept hierarchy, is presented at the beginning of the Thesaurus. On the other\nhand, in WordNet only the unique beginners are listed, and only in the documentation. This\nshows that much more value was attributed to the ontology in Roget’s. Tables 3.1 and 3.2 show\nthe portions of Roget’s and WordNet’s ontologies that classify the various abstract relations.\nBoth tables use the Class, Section and Head notation from Roget’s Thesaurus. The Section Time\nhas been expanded to present the underlying Heads. The glosses that accompany each WordNet\nnoun synset are not included in the table so as to compare them to semicolon groups, for which\nthe Thesaurus does not give definitions.\nChapter 3. The Design and Implementation of the ELKB\n22\nClass One: Abstract Relations\nThe Heads in the printed Roget’s Thesaurus are placed in two distinct columns to express\nopposing ideas such as 128 Morning and 129 Evening. Sometimes there is an intermediate\nidea, for example:\n132 Young person 133 Old person 134 Adultness\nThe visual representation of the hypernym tree for WordNet’s Time Section has been chosen so\nas to facilitate the comparison with Roget’s. The order of the synsets in the table is the one given\nby WordNet. There does not seem to be as clear an underlying structure as the one presented by\nthe Thesaurus.\nChapter 3. The Design and Implementation of the ELKB\n23\nUnique Beginner Three: Abstraction\n3.2 The Counts of Words and Phrases in Roget’s and WordNet\nThe simplest way to compare Roget’s Thesaurus and WordNet is to count strings. Table 3.3\nshows the word and phrase counts for the 1987 Roget’s, divided among parts of speech. A sense\nis defined as the occurrence of a word or phrase within a unique semicolon group, for example\nthe slope in {rising ground, rise, bank, ben, brae, slope, climb, incline}. Table\n3.4 presents the different counts for WordNet 1.7.1 and the strings in common with Roget’s. Here\na sense is the occurrence of a string within a unique synset, for example slope in {slope,\nincline, side}.\nThe absolute sizes are similar. The surprisingly low 32% overlap may be due to the fact that\nWordNet’s vocabulary dates to 1990, while Roget’s contains a vocabulary that spans 150 years,\nsince many words have been added to the original 1852 edition, but few have been removed. It is\nalso rich in idioms: “The present Work is intended to supply, with respect to the English\nlanguage, a desideratum hitherto unsupplied in any language; namely a collection of words it\nChapter 3. The Design and Implementation of the ELKB\n24\ncontains and of the idiomatic combinations peculiar to it …” (Roget, 1852). Fellbaum (1998b)\nadmits that WordNet contains little figurative language. She explains that idioms must appear in\nan ELKB if it is to serve NLP applications that deal with real texts where idiomatic language is\npervasive.\nPOS Unique Strings Paragraphs Semicolon Groups\nSenses\nNoun 56307 2876 31133 114052\nVerb 24724 1497 13968 55647\nAdjective 21665 1500 12889 48712\nAdverb 4140 498 1822 5708\nInterjection 372 61 65 406\nTotals 107208 6432 59877 224525\n3.3 The semantic relations of Roget’s and WordNet\nCassidy (2000) has identified 400 kinds of semantic relations in the FACTOTUM semantic\nnetwork which is based on the 1911 edition of Roget’s Thesaurus. This suggests that the 1987\nPenguin edition of Roget’s has a rich set of implicit semantic relations. To build a useful\nelectronic lexical knowledge base from the Thesaurus, these relations must be made available\nexplicitly. Some semantic relations are present already within the Tabular synopsis of categories,\nas Kirkpatrick (1998) explains: “Most Heads are in pairs, representing the positive and negative\naspects of an idea, e.g. 852 Hope, 853 Hoplessness.” This antonymy relationship that is present\nfor Heads does not necessarily translate into a relation of opposition for the words contained\nunder each Heads. This is due to the fact that Heads and words that belong to it represent two\nChapter 3. The Design and Implementation of the ELKB\n25\ndifferent types of concepts. The Head represents a general concept, whereas the words and\nphrases represent all of the various aspects of this concept. Thus, under the Head 539 School\ncan be found such notions as ;college, lycée, gymnasium, senior secondary school; …\n;lecture room, lecture hall, auditorium, amphitheatre; and ;platform, stage,\npodium, estrade;.\nTwo types of explicit relationships are present at the word level: Cross-reference and See. Cross-\nreference is a link between Heads via the syntactic form of a word. For example, the Heads 373\nFemale and 169 Parentage are linked by the Cross-reference 169 maternity. The word\nmaternity is present within the group mother, grandmother 169 maternity in the Head 373\nFemale and is the first word of a paragraph in the head 169 Parentage. According to\nKirkpatrick (1998), the See relationship is used to refer the reader to another paragraph within\nthe same Head, where the idea under consideration is dealt with more thoroughly. An example of\nthis is when a general paragraph such as killing in Head 362 Killing: destruction of\nlife is followed by more specific paragraphs homicide and slaughter. The relationship\nappears in the following manner in the text: murder, assassination, bumping off (see\nhomicide).\nIt is a common misconception that the Thesaurus is simply a book of synonyms. Roget (1852)\nadmits in fact that “it is hardly possible to find two words having in all respect the same\nmeaning, and being therefore interchangeable; that is, admitting of being employed\nindiscriminately, the one or the other, in all applications”. According to Kirkpatrick (1998), the\nintention is to offer words that express every aspect of an idea, rather than to list synonyms. The\ngroups of words found under a Head follow one another in a logical sequence. Systematic\nsemantic relations, such as IS-A and PART-OF, are not required between the semicolon groups\nand the Head. For example, both restaurant and have brunch are found under the same Head\n301 Food: eating and drinking. Although the native English speaker can identify various\nrelations between food, restaurant and have brunch, it is not an easy thing to discover\nautomatically. This is a major challenge; some possible algorithms for automatic labeling of\nsemantic relations are presented in Chapter 6.\nWordNet is based on about fifteen semantic relations, the most important of which is synonymy.\nEvery part-of-speech in WordNet has a different set of semantic relations. It is important to note\nthat synonymy is the only relation between words. All others are between synsets. For example,\nthe synsets car, auto, automobile, machine, motorcar -- (4-wheeled motor vehicle; usually propelled by an internal combustion engine; “he needs a car to get to work”) and accelerator, accelerator pedal, gas pedal, gas, throttle,\nChapter 3. The Design and Implementation of the ELKB\n26\nChapter 3. The Design and Implementation of the ELKB\n27\ngun -- (a pedal that controls the throttle valve; “he stepped on the gas”) are\nlinked by the meronym (has part) relation, whereas the nouns car and auto are linked by\nsynonymy. Table 3.5 summarizes the semantic relations. All of the examples are taken from\nWordNet version 1.7.1. and {...} represents a synset.\nWordNet’s semantic relations are discussed in detail in the International Journal of\nLexicography 3(4) and WordNet: An Electronic Lexical Database (Fellbaum, 1998).\n3.4 Accessing Roget’s and WordNet\nIt is very important to have adequate methods of accessing an electronic lexical knowledge base.\nThese access methods should be designed in a computationally efficient manner, since this\nresource is to be machine-tractable, and faithfully reproduce how the printed version is used. For\nthe task of computerizing Roget’s Thesaurus, a study of its manual use can offer good\nsuggestions. WordNet can also be a source of more ideas. Roget’s provides an Index of the words\nand phrases in the Thesaurus. For every item a list of keywords, with their Head numbers and\npart-of-speech, indicates in what Paragraph a word can be found. The different Keywords give\nan indication of the various senses of a word. The combination keyword, head number, part-\nof-speech represents a unique key in the Thesaurus.\nThe following is an example of an Index entry:\ndaily\noften 139 adv.\nseasonal 141 adj.\nperidodically 141 adv.\njournal 528 n.\nthe press 528 n.\nusual 620 adj.\ncleaner 648 n.\nservant 742 n.\nThe Paragraph pointed to by the key journal 528 n. is the following:\n528 Publication\nN. ...\njournal, review magazine, glossy m., specialist m., women’s m., male-interest m., pulp m.; part-work, periodical, serial, daily, weekly, monthly, quarterly, annual; gazette, trade journal, house magazine, trade publication 589 reading matter.\nChapter 3. The Design and Implementation of the ELKB\n28\nA Roget’s Paragraph is made up of a Keyword and a sequence of Semicolon Groups. The\nKeyword, an italicized word at the beginning of a Paragraph, is not intended to be a synonym of\nthe words that follow it, but is rather a concept that generalizes the whole Paragraph. It also\nallows to identify the position of other words in the Index and to locate Cross-references\n(Kirkpatrick, 1998). A Semicolon Group is a list of closely related words and phrases, for\nexample: ; part-work, periodical, serial, daily, weekly, monthly, quarterly,\nannual; Such lists are separated by semicolons. This is the smallest unit above single words and\nphrases in Roget’s.\nMost people use the Index when looking up a word in the Thesaurus, but Roget (1852) intended\nhis classification system to also serve this purpose: “By the aid of this table the reader will, with\na little practice, readily discover the place which the peculiar topic he is in search of occupies in\nthe series; and on turning to the page in the body of the Work which contains it, he will find the\ngroup of expressions he requires, out of which he may cull those that are the most appropriate to\nhis purpose”. Searching the Thesaurus like this allows looking at all of the words found under a\nHead, regardless of the part-of-speech. In this manner all of the concepts that express every\naspect of a given idea can be found.\nFor the human user, the Index is the most practical means of looking up a word. For the\ncomputer, the classification system is extremely practical, as it has been shown, for example, by\nYarowsky’s (1992) word sense disambiguation experiment or the semantic similarity metric\npresented in Chapter 4. It is important to be able to locate a word within its semicolon group, and\nfrom there to look at the other words in the same Paragraph, the same Head, knowing at all times\nin which place the word is found in the classification system. Using the ELKB it must be possible\nto follow the different paths built from the parts of speech, semantic relations and the ontology.\nGraphical and command line interfaces exist for WordNet. Both work essentially in the same\nway. After selecting a word, all of its senses appear, listed within the synsets to which they\nbelong, ordered by frequency and part-of-speech. For example, the search for the word daily\nreturns the following:\nChapter 3. The Design and Implementation of the ELKB\n29\nAt this point, the user can decide to continue his search of the database by using one of the\nsemantic relations. The system does not show the exact location in the ontology where the search\nresults are originating from, nor can all the concepts describing an idea be easily extracted. The\nnumber of times the word senses occur in Semcor (Landes et al., 1998), a semantic concordance\nbased on the Brown Corpus (Francis and Kucera, 1982), is displayed by WordNet.\nAll the methods to access Roget’s that the printed version offers have been implemented in the\nELKB. WordNet provides an interface to its lexical material in a manner that is similar to only\nusing Roget’s index. The ELKB allows performing this type of search as well as using the\nclassification system. Appendix A presents the use cases and explains the basic functions of the\nELKB."
    }, {
      "heading" : "3.5 The preparation of the Lexical Material",
      "text" : "We have licensed the source of the 1987 Roget’s from Pearson Education. It is divided into files\nwith the text of the Thesaurus and files with its index. The Text file and Index file, both about 4\nChapter 3. The Design and Implementation of the ELKB\n30\nMB in size, are marked up using codes devised by the owners of the resource. Appendix E\npresents the steps for converting the codes into HTML-like tags. Appendix D lists the Perl scripts\nused for transforming the lexical material into a format that is suitable for the ELKB along with\ntheir accompanying documentation. The ELKB is created using only the Text file; the Index is\nconstructed using the words and phrases loaded in the knowledge base. This is a necessary step,\nas the supplied Index file does not contain entries for all of the words contained in the Thesaurus.\nCertain space-saving conventions are used in the source data. Where consecutive expressions use\nthe same word, repetitions may be avoided using “or”, as in “drop a brick or clanger”,\n“countryman or –woman”. The repeated word may also be indicated by its first letter, followed\nby a full stop: “weasel word, loan w., nonce w.,” (Kirkpatrick, 1998). All such\nabbreviations must be expanded before the lexical material is loaded into the ELKB. A Perl script\nwas written to do this as well as to replace the Pearson codes by HTML-like tags, easier to\nprocess automatically. Other programs validate the expansion errors mostly due to noise in the\noriginal material."
    }, {
      "heading" : "3.5.1 Errors and Exceptions in the Source Files",
      "text" : "The original text files supplied by Pearson Education contain some errors. There are 8\noccurrences of lines that include the string “Bad Character”, for example: Err{\\pbf\\ Bad\nCharacter: \\char`\\?\\char`\\ \\ }. There are some phrases where spaces are missing between\nthe words, for example “creativeaccounting” instead of “creative accounting”. Other\nwords are split which seem to be spelling mistakes at first but a closer look reveals that the\nmissing letters are separated from the words by a space, for example: “incommunicativene\nss”. The code #1$:#5 is frequently inserted in the file but does not mean anything. Appendix F\nshows 179 instances of errors where spaces are missing and 26 instances of errors that contain an\nextra space, as well as specifying the original file in which they can be found.\n3.6 The Java implementation of the ELKB The entire functionality of the printed version of Roget’s has been implemented in Java. The\nELKB, which is comprised of eighteen classes, is organized around four major ones: the\nRogetELKB class which is the main entry point into the system, the Category class which models\nthe taxonomy and has methods to traverse it, the RogetText class which represents the 990 Heads\nas well as the words and phrases stored under them and the Index class which contains the\nreferences to all of the words and phrases in the Thesaurus. These four classes, as well as their\nrelation to the other fourteen, are described in the following sections. The class diagram of the\nELKB is shown in Figure 3.4. Appendix B presents a detailed documentation of the system.\nChapter 3. The Design and Implementation of the ELKB\n31\nChapter 3. The Design and Implementation of the ELKB\n32\nCorrectly reproducing the printed Roget’s Thesaurus, creating an application programming\ninterface (API) that is both efficient an easy to use, performance, memory and the availability of\nthe ELKB software to the largest possible audience have been the main concerns of this\nimplementation. The library of Java API is used for all of the experiments included in this\ndissertation: evaluating the semantic similarity of words and phrases, the construction of lexical\nchains, and investigating algorithms for mapping the Thesaurus onto WordNet, presented in\nchapters 4, 5 and 6 respectively. The ELKB can be accessed by making direct call to the API, as\nwell as by using the command line and graphical user interface (GUI). A first version of the GUI\nwas created by Pierre Chrétien and Gilles Roy to fulfill the requirements of their fourth-year\nHonors Software Engineering project. Appendix C presents the GUI and the command line\ninterface.\n3.6.1 The ELKBRoget and Related Classes\nThe ELKBRoget class is the one that contains all others in the system. It has methods to perform\nmanipulations on the ELKB, from looking up a word or phrase, calculating the distance\nbetweentwo words or phrases, to identifying their relative position in the taxonomy. Instances of\nthe Index and Category classes are loaded into memory to allow for rapid access. The Heads,\ncontained in the RogetText object, are read from files when required. This configuration is\nThe best compromise between performance and memory usage. Two objects, Path which\ncalculates and stores the path between two references, and PathSet which contains all paths\nbetween a pair of words and phrases, are accessed from ELKBRoget. They allow calculating the\nshortest path between two words or phrases.\n3.6.2 The Category and Related Classes\nThe Category class models the Tabular synopsis of categories described in Section 3.1. It is\nmade up of two arrays; the first contains Category and the second HeadInfo objects. The\nCategory objects are comprised of an array of Section objects, which in turn contain an array of\nSubSection objects, which have an array of Group objects which are finally made up of an array\nof HeadInfo objects. The HeadInfo objects describe a Head entirely with respect to its location in\nRoget’s taxonomy. It is defined uniquely by the Class number, Section number, Sub-section\nname, Head Group, as well as the name and number of the Head. It requires little memory as it\ndoes not contain any of the words or phrases. The second array of the Category class contains the\n990 HeadInfo objects that describe the 990 Heads of the Thesaurus. Thus, the taxonomy can be\ntraversed depth-wise, starting with the Class objects, or breadth-wise by accessing the HeadInfo\nChapter 3. The Design and Implementation of the ELKB\n33\nobjects. It is often more interesting to access the array of Heads, as this can be done via random\naccess using a Head number.\n3.6.3 The RogetText and Related Classes\nThe RogetText class represents everything that is contained in the Text of the Thesaurus. It\ncontains the Head, Paragraph, SG and SemRel classes. The SG objects contain Semicolon\ngroups. The SemRel class is used to model Roget’s explicit semantic relations, Cross-references\nand See-references, discussed in Section 3.3. A Head object contains five arrays of Paragraphs,\none for each of nouns, adjectives, verbs, adverbs and interjections, which are in turn made up of\nan array of SG objects. The RogetText is stored as 990 files, one for each Head. They are loaded\nas required. A word or phrase is looked up using a reference, for example: contempt 922 int.\nwhen searching for the phrase in your face. The reference specifies the Head number, 922 in\nthis example, which allows the specified Head to be retrieved in constant time. The correct\nParagraph is retrieved by finding the index of the Paragraph that corresponds to the reference,\nand looking up this Paragraph. The search is done in linear time and the look up in constant\ntime. To find the word or phrase in the Semicolon group, its membership must be first identified\nin the array of SG objects, and then the Semicolon group is searched sequentially. The ELKB\ncontains methods to retrieve the Paragraphs and SG objects in constant time if the references are\ntranslated into absolute addresses, for example the reference contempt 922 int. could be\nchanged to 922.5.1.1.3., which would represent Head 922, part-of-speech label interjection, 1st Paragraph, 1st Semicolon group and third word. As the lexical material does\nnot generally change, these references are not required to be calculated often, but the\ntransformation procedure has not been implemented in this version of the ELKB. There are 990\nHead, 6,432 Paragraph, and 59,877 SG objects in this computerized Roget’s.\n3.6.4 The Index and Related Classes\nThe Index contains all of the words and phrases found in the 990 Heads. It consists of a hash\ntable of Index entries, which represent the words and phrases of the Thesaurus, with pointers to\ntheir corresponding references, as well as an array of all distinct references in the ELKB. The\nsame reference is used for all of the words or phrases in a Paragraph. It is more economical to\nstore this reference once and to have the entries maintain a pointer to it, rather than to store it\nevery time it is used. The size of the Index object is a major concern, as it must be stored in\nmemory to ensure rapid access. Finding an entry in the Index hash table and looking up its\ncorresponding references is performed in constant time. The references are stored in Reference\nobjects and their pointers are encoded as Strings, which represent their addresses in the\nChapter 3. The Design and Implementation of the ELKB\n34\nreferences array, separated by semicolons. For example, the Index words know-how, stealth,\ndiplomacy and intrigue all have the reference cunning 698 n. as do all the other words that\nare located in this Roget’s Paragraph. Instead of storing this reference with every entry that it\nbelongs to, only the location in an array of unique references is kept. Since an entry may have\nseveral references, all the addresses are kept, for example 345:456:2045:12374. The Morphy\nand Variant classes are used by the Index object to perform transformations that allow retrieving\nwords and phrases written in forms that are not contained in the Thesaurus. These classes and the\ntransformations are described in the following section. There are 104,333 Index entries and\n223,219 total References, and 6,432 unique Reference objects in the ELKB."
    }, {
      "heading" : "3.6.5 Morphological Transformations",
      "text" : "The ability to perform morphological transformations is essential for a good lexical knowledge\nbase. The lemma of a word or phrase written in British English is generally stored in the Index,\nas this is the form in which it also appears in the 1987 edition of Penguin’s Roget’s Thesaurus. If\nan inflected word is passed to the ELKB it must be transformed so as to retrieve the appropriate\nReferences. Three tools are available to execute the passage of an input string into a recognized\nform: a file containing pairs of strings in American and British spelling; rules for detaching\ninflectional endings to obtain base forms; and exception list files for nouns, verbs, adjectives and\nadverbs in which inflected forms can be searched and base forms found. Appendix G presents\nthe American and British spelling word list used by the ELKB. It contains 646 pairs of spelling\nvariations and has been compiled from various lists freely available on the web. The rules,\npresented in Table 3.6, and the exception lists were taken from WordNet 1.7.1. The exception\nlists are quite extensive: 5992 pairs in the noun.exc file, 5285 in the verb.exc, 1486 in adj.exc\nand 7 in the adv.exc file. These transformations are performed in the Index class using the\nVariant and Morphy classes described previously. There is no method for identifying the part-of-\nspeech of an input string in the ELKB. For this reason all the detachment rules and exception\nfiles are applied and searched. Although this is not the best implementation, it allows a good\nrecall of the References stored in the Index.\nRetrieving phrases from the Index is problematic for the ELKB. There are many in Roget’s, a lot\nof which are specific to British English, for example: man on the Clapham omnibus, drunk as\nDavid's sow or come from other languages, for example: faute de mieux, Alea jacta est.\nOnce again, the exact string must be entered for the corresponding references to be returned.\nVerbs will not be found if they are preceded by to or be for example: to offer, be\ndisorderly. This problem can be easily circumvented, but has not been implemented in the\nELKB as it is very difficult to conceive rules that deal with all possible verb phrases that contain\nChapter 3. The Design and Implementation of the ELKB\n35\nprepositions, for example: ask for it. There are several possible solutions to the problem of\nphrases. They could be indexed under every word in the phrase or a method that retrieves all the\nphrases in the Index that contain specific words could be implemented, but none has been so far.\nThis is detrimental to the ELKB, as Roget’s Thesaurus has a very rich collection of phrases, but\nhas not been a hindrance for obtaining good results in NLP applications, as is presented in\nChapter 4 and 5.\n3.6.6 Basic Operations of the ELKB\nThe basic operations of the ELKB are:\n• finding the references for a given word or phrase in the Index\n• looking up a given reference in the Text\nChapter 3. The Design and Implementation of the ELKB\n36\n• traversing the taxonomy to calculate the distance between words and phrases\n• identifying the type of relationship that exists between words and phrases as defined by their location in the hierarchy\nA host of NLP application can be implemented using these four basic operations. Three are\npresented in this dissertation: measuring semantic similarity, building lexical chains, and\nmapping Roget’s Thesaurus onto WordNet. The first uses the taxonomy to measure the distances,\nthe second relies on properties of the Thesaurus to identify specific relations, and the third\nexploits word and phrase look-up as well as Roget’s hierarchy. In the current implementation of\nthe ELKB, the slowest operation is the word and phrase look up. This is due to the fact that\nHeads are read from a file, and sequential searches performed on the Paragraph and SG arrays to\nfind the location of the symbolic references. Performance can be greatly increased by loading the\n990 Heads into memory and using absolute addresses for the references. Both of these are\nrealistic improvements.\nChapter 4. Using Roget’s Thesaurus to Measure Semantic Similarity\n37\n4 Using Roget’s Thesaurus to Measure Semantic Similarity\nMeasuring semantic similarity with the ELKB allows us to present a first application of the\nsystem as well as to perform a qualitative evaluation. In this chapter, we define the notions of\nsynonymy and semantic similarity and explain a metric for calculating similarity based on\nRoget’s taxonomy. We evaluate it using a few typical tests. The experiments in this chapter\ncompare the synonymy judgments of the system to gold standards established by Rubenstein and\nGoodenough (1965), Miller and Charles (1991) as well as Finkelstein et al. (2002; Gabrilovich\n2002) for assessing the similarity of pairs of words. We further evaluate the metric by using the\nsystem to answer Test of English as a Foreign Language [TOEFL] (Landauer and Dumais, 1997)\nand English as a Second Language tests [ESL] (Turney, 2001), as well as the Reader’s Digest\nWord Power Game [RDWP] (Lewis, 2000-2001) questions where a correct synonym must be\nchosen amongst four target words. We compare the results to six other WordNet-based metrics\nand two statistical methods."
    }, {
      "heading" : "4.1 The notions of synonymy and semantic similarity",
      "text" : "People identify synonyms — strictly speaking, near-synonyms (Edmonds and Hirst, 2002) —\nsuch as angel – cherub, without being able to define synonymy properly. The term tends to be\nused loosely, even in the crucially synonymy-oriented WordNet with the synset as the basic\nsemantic unit (Fellbaum, 1998, p. 23). Miller and Charles (1991) restate a formal, and linguis-\ntically quite inaccurate, definition of synonymy usually attributed to Leibniz: “two words are\nsaid to be synonyms if one can be used in a statement in place of the other without changing the\nmeaning of the statement”. With this strict definition there may be no perfect synonyms in\nnatural language (Edmonds and Hirst, ibid.). Computational linguists often find it more useful to\nestablish the degree of synonymy between two words, referred to as semantic similarity.\nMiller and Charles’ semantic similarity is a continuous variable that describes the degree of\nsynonymy between two words (ibid.). They argue that native speakers can order pairs of words\nby semantic similarity, for example ship – vessel, ship – watercraft, ship –\nriverboat, ship – sail, ship – house, ship – dog, ship – sun. The concept can be\nusefully extended to quantify relations between non-synonymous but closely related words, for\nexample airplane – wing.\nRubenstein and Goodenough (1965) investigated the validity of the assumption that “... pairs of\nwords which have many contexts in common are semantically closely related”. This led them to\nestablish synonymy judgments for 65 pairs of nouns with the help of human experts. Miller and\nChapter 4. Using Roget’s Thesaurus to Measure Semantic Similarity\n38\nCharles (ibid.) selected 30 of those pairs, and studied semantic similarity as a function of the\ncontexts in which words are used. Others have calculated similarity using semantic nets (Rada et\nal., 1989), in particular WordNet (Resnik, 1995; Jiang and Conrath, 1997; Lin, 1998; Hirst and\nSt-Onge, 1998; Leacock and Chodorow, 1998) and Roget’s Thesaurus (McHale, 1998), or\nstatistical methods (Landauer and Dumais, 1997; Turney, 2001). Terra and Clarke (2003) present\na survey of statistical methods. This leads naturally to combined approaches that rely on\nstatistical methods enhanced with information contained in WordNet (Finkelstein et al., 2002)\nand methods that merge the results of various statistical systems (Bigham et al., 2003).\nThe objective is to test the intuition that Roget’s Thesaurus, sometimes treated as a book of\nsynonyms, allows to measure semantic similarity effectively. We propose a measure of semantic\ndistance, the inverse of semantic similarity (Budanitsky and Hirst, 2001) based on Roget’s\ntaxonomy. We convert it into a semantic similarity measure, and empirically compare it to\nhuman judgments and to those of NLP systems. We evaluate the measure by performing the task\nof assigning a similarity value to pairs of nouns and choosing the correct synonym of a problem\nword given the choice of four target words. This chapter explains in detail the measures and the\nexperiments, and draws a few conclusions."
    }, {
      "heading" : "4.2 Edge counting as a metric for calculating synonymy",
      "text" : "Roget’s structure provides an easy mechanism for calculating the semantic distance using edge\ncounting. Given two words, the system looks up the corresponding references in the index, and\nthen calculates all paths between the references using the taxonomy. The distance value is equal\nto the number of edges in the shortest path as indicated in Table 4.1. For example, the distance\nbetween feline and lynx is 2. It can be calculated as follows:\nThe word feline has the following references in Roget’s:\n1) animal 365 ADJ. 2) cat 365 N. 3) cunning 698 ADJ.\nThe word lynx has the following references in Roget’s:\n1. cat 365 N. 2. eye 438 N.\nThese six paths are obtained:\nPath between feline (cat 365 N.) and lynx (cat 365 N.) [ length = 2 ] feline → cat ← lynx\nChapter 4. Using Roget’s Thesaurus to Measure Semantic Similarity\n39\nPath between feline (animal 365 ADJ.) and lynx (cat 365 N.) [ length = 6 ] feline → animal → ADJ. → 365. Animality. Animal ← N. ← cat ← lynx Path between feline (animal 365 ADJ.) and lynx (eye 438 N.) [ length = 12 ] feline → animal → ADJ. → 365. Animality. Animal → [365, 366] → Vitality → Section three : Organic matter ← Sensation ← [438, 439, 440] ← 438. Vision ← N. ← eye ← lynx\nChapter 4. Using Roget’s Thesaurus to Measure Semantic Similarity\n40\nPath between feline (cat 365 N.) and lynx (eye 438 N.) [ length = 12 ] feline → cat → N. → 365. Animality. Animal → [365, 366] → Vitality → Section three : Organic matter ← Sensation ← [438, 439, 440] ← 438. Vision ← N. ← eye ← lynx Path between feline (cunning 698 ADJ.) and lynx (cat 365 N.) [ length = 16 ] feline → cunning → ADJ. → 698. Cunning → [698, 699] → Complex → Section three : Voluntary action → Class six : Volition: individual volition → T ← Class three : Matter ← Section three : Organic matter ← Vitality ← [365,"
    }, {
      "heading" : "366] ← 365. Animality. Animal ← N. ← cat ← lynx",
      "text" : "Path between feline (cunning 698 ADJ.) and lynx (eye 438 N.) [ length = 16 ] feline → cunning → ADJ. → 698. Cunning → [698, 699] → Complex → Section three : Voluntary action → Class six : Volition: individual volition → T ← Class three : Matter ← Section three : Organic matter ← Sensation ← [438,"
    }, {
      "heading" : "439, 440] ← 438. Vision ← N. ← eye ← lynx",
      "text" : "Figure 4.1: All the paths between feline and lynx in Roget’s Thesaurus.\nMcHale (1998) has also used the Third Edition of Roget’s International Thesaurus (Berrey and\nCarruth, 1962) to measure semantic similarity. He calculated the semantic distance between\nnouns using four metrics: counting the number of edges, the absolute number of words and\nphrases between two target nouns, and by using measures first presented by Resnik (1995) as\nwell as Jiang and Conrath (1997) for WordNet-based systems. McHale finds that edge counting\nis the best of the implemented Roget’s-based measures and correlates extremely well with human\njudges. He calculates semantic similarity using the Miller and Charles (1991) set in which the\npairs cemetery – woodland and shore – woodland have been removed, as the noun woodland\nis not present in WordNet version 1.4, the resource to which the results are being compared.\nMcHale obtains a correlation with the gold standard of r=.88, which is quite close to r=.90\nobtained by Resnik (ibid.) who repeated the experiment using human judges on the 28 pairs of\nnouns. Although the publishers of Roget’s International Thesaurus have not made it publicly\navailable in a machine-tractable format, it does suggest that we can obtain equally good results\nusing the ELKB.\nRada et al. (1989) explain that a distance measure in a taxonomy should satisfy the properties of\na metric. A function f(x,y) is a metric if the following properties are satisfied:\nChapter 4. Using Roget’s Thesaurus to Measure Semantic Similarity\n41\n1) f(x, x) = 0, zero property,\n2) f(x,y) = f(y, x), symmetric property\n3) f(x, y) ≥ 0, positive property, and\n4) f(x, y) + f(y, z) ≥ f(x, z), triangular inequality.\nThe proposed semantic distance measure using Roget’s taxonomy is in fact a metric, as it\nsatisfies the four properies:\n1) zero property: the shortest distance between a word and itself is always zero as it belongs to a semicolon group.\n2) symmetric property: the shortest distance between two words is equal to the least number of edges between them. Order is not important, and therefore this property holds.\n3) positive property: the distance value between two words is an integer between 0 and 16.\n4) triangular inequality: if x and z belong to the same semicolon group, this property is true as f(x, z) = 0, and the sum of any other distance measure will be at least equal to 0. If x, y\nand z are all in different classes, then f(x,z) = 16 and f(x,y) + f(y,z) = 32. The shortest path\nbetween x and z going through y will always be greater or equal to the shortest path\nbetween x and z as the word y introduces the extra distance in the taxonomy towards the\nfirst common node.\nFor the purpose of comparing to other experiments, the semantic distance must be transformed\ninto a semantic similarity measure. The literature proposes two formulas to perform this\ntransformation. The first is to subtract the path length from the maximum possible path length\n(Resnik, 1995):\nsim1 (w1, w2) = 16 – [min distance(r1, r2)] (1)\nThe second is to take the inverse of the distance value plus one (Lin, 1998):\nsim2 (w1, w2) = 1 / (1 + [min distance(r1, r2)] ) (2)\nIn both formulas r1 and r2 are the sets of references for the words or phrases w1 and w2. As the\nmaximum distance in the Thesaurus is 16, the values for sim1 range from 0 to 16 and for sim2 from 0.059 to 1.000. In both formulas, the more related the two words or phrases are, the larger\nthe score. As the distances are quite small, the second formula can never reach a value close to 0.\nChapter 4. Using Roget’s Thesaurus to Measure Semantic Similarity\n42\nThe constant 1 which is added to the divider is also quite arbitrary. The first formula is best\nsuited to edge counting as it maintains the same distribution of values as the distance metric. We\nuse it for the experiments presented in this chapter."
    }, {
      "heading" : "4.3 An Evaluation Based on Human Judgments",
      "text" : ""
    }, {
      "heading" : "4.3.1 The Experiment",
      "text" : "Rubenstein and Goodenough (1965) established synonymy judgments for 65 pairs of nouns.\nThey invited 51 judges who assigned to every pair a score between 4.0 and 0.0 indicating\nsemantic similarity. They chose words from non-technical every day English. They felt that,\nsince the phenomenon under investigation was a general property of language, it was not\nnecessary to study technical vocabulary. Miller and Charles (1991) repeated the experiment\nrestricting themselves to 30 pairs of nouns selected from Rubentein and Goodenough’s list,\ndivided equally amongst words with high, intermediate and low similarity. More recently,\nFinkelstein et al. (2002) have prepared the WordSimilarity – 353 Test Collection (Gabrilovich,\n2002) which contains 353 English word pairs along with similarity judgments performed by\nhumans. The set also contains proper nouns and verbs. It is discussed in more detail in the next\nsection.\nThe three experiments have been repeated using the Roget’s Thesaurus system. The results are\ncompared to six other similarity measures that rely on WordNet. We use Pedersen’s Semantic\nDistance software package (2003) with WordNet 1.7.1 to obtain the results. The first WordNet\nmeasure used is edge counting. It serves as a baseline, as it is the simplest and most intuitive\nmeasure. The next measure, from Hirst and St-Onge (1998), relies on the path length as well as\nthe number of changes of direction in the path; they define these changes in function of WordNet\nsemantic relations. Jiang and Conrath (1997) propose a combined approach based on edge\ncounting enhanced by the node-based approach of the information content calculation proposed\nby Resnik (1995). Leacock and Chodorow (1998) count the path length in nodes rather than\nlinks, and adjust it to take into account the maximum depth of the taxonomy. Lin (1998)\ncalculates semantic similarity using a formula derived from information theory. Resnik (1995)\ncalculates the information content of the concepts that subsume them in the taxonomy. We\ncalculate the Pearson product-moment correlation coefficient for the human judgments with the\nvalues achieved by the systems. The correlation is significant to at the 0.01 level. These similarity\nmeasures appear in Tables 4.2, 4.3 and 4.4.\nChapter 4. Using Roget’s Thesaurus to Measure Semantic Similarity\n43"
    }, {
      "heading" : "4.3.2 The Results",
      "text" : "We begin the analysis with the results obtained by Roget’s. The Miller and Charles data in Table\n4.2 show that pairs of words with a semantic similarity value of 16 have high similarity, those\nwith a score of 12 to 14 have intermediate similarity, and those with a score below 10 are of low\nNoun Pair Miller Charles Penguin Roget WordNet Edges Hirst St.Onge Jiang Conrath Leacock Chodorow Lin Resnik\nsimilarity. This is intuitively correct, as words or phrases that are in the same Semicolon Group\nwill have a similarity score of 16, those that are in the same Paragraph, part-of-speech or Head\nChapter 4. Using Roget’s Thesaurus to Measure Semantic Similarity\n44\nwill have a score of 10 to 14, and words that cannot be found in the same Head, therefore do not\nbelong to the same concept, will have a score between 0 and 8. Roget’s results correlate very\nwell with human judgment for the Miller and Charles list (r=.878), almost attaining the upper\nbound (r=.885) set by human judges (Resnik, 1995) despite the outlier crane – implement, two\nwords that are not related in the Thesaurus.\nThe correlation between human judges and Roget’s for the Rubenstein and Goodenough data is\nalso very good (r=.818) as shown in Table 4.3. Appendix I presents the 65 pairs of nouns. The\noutliers merit discussion. Roget’s deems five pairs of low similarity words to be of intermediate\nsimilarity, all with the semantic distance value of 12. We therefore find these pairs of words all\nunder the same Head and belonging to noun groups. The Thesaurus makes correct associations\nbut not the most intuitive ones: glass - jewel is assigned a value of 1.78 by the human judges\nbut can be found under the Head 844 Ornamentation, car – journey is assigned 1.55 and is\nfound under the Head 267 Land travel, monk – oracle 0.91 found under Head 986 Clergy,\nboy – rooster 0.44 under Head 372 Male, and fruit – furnace 0.05 under Head 301 Food:\neating and drinking.\nWe have also performed the same experiment on the WordSimilarity – 353 Test Collection. The\ncorrelation of Roget’s measure with human judges is r=.539, which seems quite low, but is still\nbetter than the best WordNet based measure, r=.375, obtained using Resnik’s function and\ncomparable to Finkelstein et al.’s combined metric which obtains a score of r=.550. Table 4.4\nsummarizes these results and Appendix J presents the entire 353 word pair list. We cannot\nsimply attribute the low scores to the measures not scaling up to larger data sets. The Finkelstein\net al. list contains pairs that are associated but not similar in the semantic sense, for example:\nliquid – water. The list also contains many culturally biased pairs, for example: Arafat – terror\nand verbs. Table 4.5 presents all of the pairs for which at least one word is not present in Roget’s.\nThese can be placed in five categories: proper nouns, verbs, new words that were not in\nwidespread use in 1986, words for which the plural is present in Roget’s but not its singular\nform, and words that are simply not in the Thesaurus. The authors of the list describe it as\nrepresenting various degrees of similarity and write that they employed 16 subjects to rate\nChapter 4. Using Roget’s Thesaurus to Measure Semantic Similarity\n45\nChapter 4. Using Roget’s Thesaurus to Measure Semantic Similarity\n46\nthe semantic similarity on a scale from 0 to 10, 0 representing totally unrelated words and 10\nvery much related or identical words (Finkelstein et al., 2002). They do not explain the\nmethodology used for preparing this list. Human subjects find it more difficult to use a scale\nfrom 0 to 10 rather than 0 to 4. These issues cast a doubt on the validity of this list, and we\ntherefore do not consider it as a suitable benchmark for performing experiments on semantic\nsimilarity.\nResnik (1995) argues that edge counting using WordNet 1.4 is not a good measure of semantic\nsimilarity as it relies on the notion that links in the taxonomy represent uniform distances. Tables\n4.2 and 4.3 show that this measure performs well for WordNet 1.7.1 . It is most probable that\nGeorge Miller’s team has much improved the lexical databases’ taxonomy and that the distances\nbetween words are more uniform, but the goal of this dissertation is not to investigate the\nimprovements made to WordNet. Table 4.6 shows that it is difficult to replicate accurately\nexperiments using WordNet-based measures. Budanitsky and Hirst (2001) repeated the Miller\nand Charles experiment using the WordNet similarity measures of Hirst and St-Onge (1998),\nJiang and Conrath (1997), Leacock and Chodorow (1998), Lin (1998) and Resnik (1995). They\nclaim that the discrepancies in the results can be explained by minor differences in\nimplementation, different versions of WordNet, and differences in the corpora used to obtain the\nfrequency data used by the similarity measures. Pedersen’s software (2003) does not yield the\nexact results either. We concur with Budanitsky and Hirst, pointing out that the Resnik, Leacock\nand Chodorow as well as the Lin experiments were performed not using the entire Miller and\nCharles set, but a 28 noun-pair subset discussed previously.\nChapter 4. Using Roget’s Thesaurus to Measure Semantic Similarity\n47"
    }, {
      "heading" : "4.4 An Evaluation Based on Synonymy Problems",
      "text" : ""
    }, {
      "heading" : "4.4.1 The Experiment",
      "text" : "Another method of evaluating semantic similarity metrics is to see how well the different\nmeasures can score on a standardized synonymy test. Such tests have questions where the correct\nsynonym is one of four possible choices. TOEFL (Landauer and Dumais, 1997), ESL (Turney,\n2001), and RDWP (Lewis, 2000-2001) contain these kinds of questions. Although this evaluation\nmethod is not widespread in NLP, researchers have used it in Psychology (Landauer and\nDumais, ibid.) and Machine Learning (Turney, ibid.). The experimental question set consists of\n80 TOEFL questions provided by the Educational Testing Service via Thomas Landauer, 50 ESL\nquestions created by Donna Tatsuki for Japanese ESL students (Tatsuki, 1998), 100 RDWP\nquestions gathered by Peter Turney and 200 RDWP questions gathered from 2000 – 2001 issues\nof the Canadian edition of Reader’s Digest (Lewis, ibid.) by Tad Stach.\nA RDWP question is presented like this: “Check the word or phrase you believe is nearest in meaning. ode – A: heavy debt. B: poem. C: sweet smell. D: surprise.” (Lewis, 2001, n. 938). The ELKB calculates the semantic distance between the\nproblem word and each choice word or phrase. The choice word with the shortest semantic\ndistance becomes the solution. Choosing the word or phrase that has the most paths with the\nshortest distance breaks ties. Phrases that cannot be found in the Thesaurus present a special\nproblem. The distance between each word in the choice phrase and the problem word is\ncalculated; we ignore the conjunction and, the preposition to, and the verb be. The system\nconsiders the shortest distance between the individual words of the phrase and the problem word\nas the semantic distance for the phrase. This technique, although simplistic, can deal with\nphrases like rise and fall; to urge; and be joyous that may not be found in the Thesaurus.\nThe ELKB is not restricted to nouns when finding the shortest path – it considers nouns,\nadjectives, verbs and adverbs. Using the previous RDWP example, the system would output the\nfollowing:\n• ode N. to heavy debt N., length = 12, 42 path(s) of this length\n• ode N. to poem N., length = 2, 2 path(s) of this length\n• ode N. to sweet smell N., length = 16, 6 path(s) of this length\n• ode N. to surprise VB., length = 12, 18 path(s) of this length\nRoget thinks that ode means poem: CORRECT\nFigure 4.2: Solution to a RDWP question using the ELKB.\nChapter 4. Using Roget’s Thesaurus to Measure Semantic Similarity\n48\nWe put the WordNet semantic similarity measures to the same task of answering the synonymy\nquestions. The purpose of this experiment is not to improve the measures, but to use them as a\ncomparison for the ELKB. The answer is the choice word that has the largest semantic similarity\nvalue with the problem word, except for edge-counting where the system picks the smallest\nvalue, which represents the shortest distance. When ties occur, a partial score is given; .5 if two\nwords are tied for the highest similarity value, .33 if three, and .25 if four. The results appear in\nTables 4.7 to 4.9. Appendix K presents the output of the ELKB and the systems using WordNet-\nbased measures implemented using the Semantic Distance software package (Pedersen, 2003).\nWe have not tailored the WordNet measures to the task of answering these questions. All of\nthem, except Hirst and St-Onge, rely on the IS-A hierarchy to calculate the path between words.\nThis implies that these measures have been limited to finding similarities between nouns, as the\nWordNet hyponym tree only exists for nouns and verbs; there are hardly any links between parts\nof speech. We have not implemented special techniques to deal with phrases. It is therefore quite\nprobable that the WordNet-based similarity measures can be improved for the task of answering\nsynonymy questions.\nThis experiment also compares the results to those achieved by state-of-the-art statistical\ntechniques. Latent Semantic Analysis (LSA) is a general theory of acquired similarity and\nknowledge representation (Landauer and Dumais, 1997). It was used to answer the 80 TOEFL\nquestions. The algorithm, called PMI-IR (Turney, 2001), uses Pointwise Mutual Information\n(PMI) and Information Retrieval (IR) to measure the similarity of pairs of words. Turney has\nevaluated it using the TOEFL and ESL questions. Researchers have determined the best\nstatistical methods (Terra and Clarke, 2003; Bigham et al., 2003) and evaluated them using the\nsame 80 TOEFL problems."
    }, {
      "heading" : "4.4.2 The Results",
      "text" : "The ELKB answers 78.75% of the TOEFL questions (Table 4.7). The two next best systems are\nHirst St-Onge and PMI-IR, which answer 77.91% and 73.75% of the questions respectively. LSA\nis not too far behind, with 64.38%. Terra and Clarke (2003) obtained a score of 81.25% using a\nstatistical technique similar to Turney’s. The discrepancies in results are most probably due to\ndifferences in the corpora used to measure the probabilities. By combining the results of four\nstatistical methods, including LSA and PMI-IR, Bigham et al. (2003) obtain a score of 97.50%.\nThey further declare the problem of this TOEFL set to be “solved”. All the other WordNet-based\nmeasures perform poorly, with accuracy not surpassing 25.0%. According to Landauer and\nDumais (ibid.), a large sample of applicants to US colleges from non-English speaking countries\nChapter 4. Using Roget’s Thesaurus to Measure Semantic Similarity\n49\ntook the TOEFL tests containing these items. Those people averaged 64.5%, considered an\nadequate score for admission to many US universities.\nThe ESL experiment (Table 4.8) presents similar results. Once again, the Roget’s system is best,\nanswering 82% of the questions correctly. The two next best systems, PMI-IR and Hirst and St-\nOnge fall behind, with scores of 74% and 62% respectively. All other WordNet measures give\nvery poor results, not answering more than 36% of the questions. The Roget’s similarity measure\nis clearly superior to the WordNet ones for the RDWP questions (Table 4.9). Roget’s answers\nChapter 4. Using Roget’s Thesaurus to Measure Semantic Similarity\n50\n74.33% of the questions, which is almost equal to a “Good” vocabulary rating according to\nReader’s Digest (Lewis, 2000-2001), where the next best WordNet measure, Hirst and St-Onge,\nanswers only 45.65% correctly. All others do not surpass 25%."
    }, {
      "heading" : "4.4.3 The Impact of Nouns on Semantic Similarity Measures",
      "text" : "The TOEFL, ESL and RDWP experiments give a clear advantage to measures that can evaluate\nthe similarity between words of different parts-of-speech. This is the case for Roget’s, Hirst and\nSt-Onge, and the statistical measures. To be fair to the other WordNet-based systems, the\nexperiments have been repeated using subsets of the questions that contain only nouns. The\nresults are presented in Tables 4.10 to 4.12.\nThe WordNet measures perform much more uniformly and yield better results, but the Roget’s\nsystem is still best. The performance of the ELKB has increased for the TOEFL questions,\ndecreased for the ESL and remained about the same for RDWP. Although this is not an\nexhaustive manner of evaluating the efficiency of edge counting as a measure of semantic\nsimilarity for various parts-of-speech, it does show that it is effective for nouns, as well as\nChapter 4. Using Roget’s Thesaurus to Measure Semantic Similarity\n51\nadjectives, verbs and adverbs. Most of the nouns not found in WordNet are phrases. For example,\nthe RDWP problem “swatch – A: sample of cloth. B: quick blow. C: petty theft.\nD: repair of clothing.” cannot be answered using WordNet. The phrases sample of\ncloth; quick blow; petty theft and repair of clothing are simply not in the lexical\ndatabase. The ELKB finds the correct answer by using the technique presented in section 4.4.1. If\na tailored method were used to deal with phrases in WordNet, the scores of the systems using this\nresource would definitely improve but this research is beyond the scope of this dissertation as the\ngoal of this thesis is to investigate the usefulness of Roget’s Thesaurus for NLP.\n4.4.4 Analysis of results obtained by the ELKB for RDWP questions\nTwenty RDWP questions are presented in every issue of Reader’s Digest (Lewis, 2000-2001).\nThese questions generally belong to a specific topic, for example: nature, Canadian Forces\npeace keeping or Food preparation, serving and eating. The results per topic are\npresented in Table 4.13. Reader’s Digest gives the following Vocabulary Ratings for the human\nwho plays the game:\n• Fair: 10 – 14 (50% – 70%)\n• Good: 15 – 17 (75% – 85%)\n• Excellent: 18 – 20 (90% – 100%)\nThe issue per issue analysis allows identifying some of the topics that are well represented and\nsome that are not in the Roget’s. The ELKB performs extremely well, obtaining a rating of\nExcellent, for the questions pertaining to Greek rooted words and manners. This can be\nattributed to the fact that the first edition of the Thesaurus was prepared during the Victorian era\nby a doctor who was well accustomed to Greek words and good manners. Words are generally\nnot removed from subsequent editions of Roget’s, but the process of adding new words is a more\nChapter 4. Using Roget’s Thesaurus to Measure Semantic Similarity\n52\narduous one, in particular technical terms, as is demonstrated by the low rating of Fair obtained\nthe financial term set."
    }, {
      "heading" : "4.5 Summary of results",
      "text" : "This chapter has shown that the electronic version of the ELKB is as good as, if not better than,\nWordNet for measuring semantic similarity. The distance measure used, often called edge\ncounting, can be calculated quickly and performs extremely well on a series of standard\nsynonymy tests. Table 4.14 summarizes the results for the Roget’s and WordNet-based measures.\nOut of 8 experiments, the ELKB is first every time, except on the Rubenstein and Goodenough\nlist of 65 noun pairs. Combined statistical methods that use the Internet as a corpus perform\nbetter, but they access many more words than are contained in either lexical resource.\nChapter 4. Using Roget’s Thesaurus to Measure Semantic Similarity\n53\nMost of the WordNet-based systems perform poorly at the task of answering synonym questions.\nThis is due in part to the fact that the similarity measures can only by calculated between nouns,\nbecause they rely on the hierarchical structure that is almost only present for nouns in WordNet.\nThese systems also suffer from not being able to deal with many phrases. A system that is\ntailored to evaluate synonymy between pairs of words and phrases might perform much better\nthan what has been presented here.\nThe Roget’s Thesaurus similarity measures correlate well with human judges, and perform\nsimilarly to the WordNet-based measures at assigning synonymy judgments to pairs of nouns.\nRoget’s shines at answering standard synonym tests. This result was expected, but remains\nimpressive: the semantic distance measure is extremely simple and no context is taken into\naccount, and the system does not perform word sense disambiguation when answering the\nquestions. Standardized language tests appear quite helpful in evaluating NLP systems, as they\nfocus on specific linguistic phenomena and offer an inexpensive alternative to human evaluation.\nChapter 5. Automating the Construction of Lexical Chains using Roget’s\n54"
    }, {
      "heading" : "5 Automating the Construction of Lexical Chains using Roget’s",
      "text" : "Morris and Hirst (1991) present a method of linking significant words that are about the same\ntopic. The resulting lexical chains are a means of identifying cohesive regions in a text, with\napplications in many natural language processing tasks, including text summarization. Morris\nand Hirst constructed the first lexical chains manually using Roget’s International Thesaurus.\nThey wrote that automation would be straightforward given an electronic thesaurus. Most\napplications so far have used WordNet to produce lexical chains, perhaps because adequate\nelectronic versions of Roget’s were not available until recently. This chapter discusses the\nbuilding of lexical chains using the electronic version of Roget’s Thesaurus, the second\napplication of the ELKB. We implement a variant of the original algorithm. We explain the\nnecessary design decisions and include a comparison with other implementations. Computational\nlinguists have proposed several evaluation methods, in particular one where they construct\nlexical chains for a variety of documents and then compare them to gold standard summaries.\nThis chapter discusses related research on the topic of lexical chains."
    }, {
      "heading" : "5.1 Previous Work on Lexical Chains",
      "text" : "Lexical chains (Morris and Hirst, ibid.) are sequences of words in a text that represent the same\ntopic. The original implementation was inspired by the notion of cohesion in discourse (Halliday\nand Hasan, 1976). An electronic system requires a sufficiently rich and subtle lexical resource to\ndecide on the semantic proximity of words.\nComputational linguists have used lexical chains in a variety of tasks, from text segmentation\n(Morris and Hirst, 1991; Okumura and Honda, 1994), to summarization (Barzilay, 1997;\nBarzilay and Elhadad, 1997; Brunn, Chali and Pinchak, 2001; Silber and McCoy, 2000, 2002),\ndetection of malapropisms (Hirst and St-Onge, 1998), the building of hypertext links within and\nbetween texts (Green, 1999), analysis of the structure of texts to compute their similarity\n(Ellman, 2000), topic detection (Chali, 2001), and even a form of word sense disambiguation\n(Barzilay, 1997; Okumura and Honda, 1994). Most of the systems use WordNet to build lexical\nchains, perhaps in part because it is readily available. Building lexical chains is a natural task for\nRoget’s Thesaurus as they were conceived using this resource. Ellman (ibid.) has used the 1911\nedition of Roget’s and the 1987 edition of Longman’s Original Roget’s Thesaurus of English\nWords and Phrases. The lexical chain construction process is computationally expensive but the\nprice seems worth paying if lexical semantics can be incorporated in natural language systems.\nChapter 5. Automating the Construction of Lexical Chains using Roget’s\n55\nOur implementation builds the lexical chains using the ELKB. The original lexical chain\nalgorithm (Morris and Hirst, ibid.) exploits certain organizational properties of Roget’s\nThesaurus. WordNet-based implementations cannot take advantage of Roget's relations. They\nalso usually only link nouns, as relations between parts-of-speech are limited in WordNet. Morris\nand Hirst wrote: “Given a copy [of a machine readable thesaurus], implementation [of lexical\nchains] would clearly be straightforward”. The goal of this experiment is to test this statement in\npractice. This work is guided by the efforts of those who originally conceived lexical chains, as\nwell as Barzilay and Elhadad (1997), the first to use a WordNet-based implementation for text\nsummarization, and Silber and McCoy (2002), the authors of the most efficient WordNet-based\nimplementation."
    }, {
      "heading" : "5.2 Lexical Chain Building Algorithms",
      "text" : "Algorithms that build lexical chains consider one by one words for inclusion in the chains\nconstructed so far. Important parameters to consider are the lexical resource used, which\ndetermines the lexicon and the possible relations between the words, called thesaural relations\nby Morris and Hirst (1991), the thesaural relations themselves, the transitivity of word relations\nand the distance — measured in sentences — allowed between words in a chain (Morris and\nHirst, ibid.).\nOur lexical chain building process builds proto-chains, a set of words linked via thesaural\nrelations. Our implementation refines the proto-chains to obtain the final lexical chains. We\nsummarize the lexical chain building process with these five high levels steps:\n1. Choose a set of thesaural relations;\n2. Select a set of candidate words;\n3. Build all proto-chains for each candidate word;\n4. Select the best proto-chains for each candidate word;\n5. Select the lexical chains."
    }, {
      "heading" : "5.2.1 Step 1: Choose a Set of Thesaural Relations",
      "text" : "Halliday and Hasan (1976) have identified five basic classes of dependency relationships\nbetween words that allow classifying lexical cohesion. Identifying these relationships in a text is\nthe first step towards constructing lexical chains. These five classes are:\n1. Reiteration with identity of reference:\na. Mary bit into a peach.\nb. Unfortunately the peach wasn’t ripe.\nChapter 5. Automating the Construction of Lexical Chains using Roget’s\n56\n2. Reiteration without identity of reference:\na. Mary ate some peaches.\nb. She likes peaches very much.\n3. Reiteration by means of a superordinate:\na. Mary ate a peach.\nb. She likes fruit.\n4. Systematic semantic relation (systematically classifiable):\na. Mary likes green apples.\nb. She does not like red ones.\n5. Nonsystematic semantic relation (not systematically classifiable):\na. Mary spent three hours in the garden yesterday.\nb. She was digging potatoes.\nOf the five basic classes of dependency relationships, the first two are easy to identify, the next\ntwo are identifiable using a resource such as Roget’s or WordNet. Morris and Hirst identify five\ntypes of thesaural relations that suggest the inclusion of a candidate word in a chain (1991).\nAlthough the fourth edition of Roget’s International Thesaurus (Chapman, 1977) is used, the\nrelations can be described according to the structure of Penguin’s Roget’s Thesaurus, which has\nbeen presented in Chapter 3. The five thesaural relations used are:\n1. Inclusion in the same Head.\n2. Inclusion in two different Heads linked by a Cross-reference.\n3. Inclusion in References of the same Index Entry.\n4. Inclusion in the same Head Group.\n5. Inclusion in two different Heads linked to a common third Head by a Cross-reference.\nMorris and Hirst state that although these five relations are used “the first two are by far the most\nprevalent, constituting over 90% of the lexical relationships.”\nIn our implementation, the decision has been made to adopt only a refinement of the first\nthesaural relation, as it is the most frequent relation, can be computed rapidly and consists of a\nlarge set of closely related words. The use of the second relation is computationally expensive\nand not intuitive. A Cross-reference in Roget’s Thesaurus belongs to a Semicolon Group and\npoints to another Paragraph in a specific Head. For example, the Cross-reference 137 timely in\nthe Semicolon Group ;in loco, well-timed, auspicious, opportune, 137 timely;\npoints from this Semicolon Group in the adjective Paragraph with keyword advisable in the\nChapter 5. Automating the Construction of Lexical Chains using Roget’s\n57\nHead 642 Good Policy to the adjective paragraph with keyword timely in Head 137\nOccasion: timeliness. The Cross-reference is therefore a relation from a Semicolon Group to\na Paragraph and does not link all words and phrases in a Paragraph to those of the Paragraph to\nwhich it points. It is clearly not symmetric and does not link comparable concepts. There are\nabout 10 times more words and phrases in the Thesaurus than Cross-references, which suggests\nthat the first relation should be at least 10 times more frequent than the second one.\nIn conjunction with the first relation, simple term repetition is exploited. All other presented by\nMorris and Hirst are discarded. The two relations used for the implementation of lexical chains\nusing the ELKB are:\n1. Repetition of the same word, for example: Rome, Rome.\n2. Inclusion in the same Paragraph.\nChapter 3 discusses the manner in which words and phrases found under the same Paragraph are\nrelated. A large number of them are near-synonyms, or are related by the IS-A and PART-OF\nrelations, as is experimentally shown in Chapter 6.\nFor the sake of comparison, here are WordNet relations that Silber and McCoy (2002) use in\ntheir implementation of lexical chains:\n1. Two noun instances are identical, and are used in the same sense.\n2. Two noun instances are synonyms.\n3. The senses of two noun instances are linked by the hypernym / hyponym relation.\n4. The senses of two noun instances are siblings in the hypernym / hyponym tree.\nThe first three relations used by Silber and McCoy have counterparts in the Roget’s Thesaurus\nimplementation. A sense of a word or phrase can be uniquely identified in the ELKB by its\nlocation in the taxonomy. The fourth relationship is used to link all of the words and phrases that\nare hypernyms or hyponyms of a synset. In this manner, train and railroad train are related\nas they belong to the same synset, they are related to boat train as it is hyponym of train. Car\ntrain, freight train, rattler, hospital train, mail train, passenger train,\nstreamliner and subway train are in turn all related to boat train as they are hyponyms of\ntrain. A counterpart of this relation cannot be explicitly found in the Thesaurus as it is\ndependent on WordNet’s structure, although the synsets that are grouped by these four relations\nare comparable to the Semicolon Groups that make up a Paragraph, as is discussed in Chapter 6.\nMorphological processing must be automated to assess the relation between words. This is done\nChapter 5. Automating the Construction of Lexical Chains using Roget’s\n58\nboth by WordNet and the ELKB. A resource that contains proper names and world knowledge,\nsuch as the layout of streets in the city of Ottawa, or who is the Prime Minister of Canada, would\nbe a great asset for the construction of lexical chains. This information is not found in Roget’s or\nWordNet, but could be added in some simplified form, using gazetteers and other knowledge\nsources, like the World Gazetteer (World Gazetter, 2003) or the Central Intelligence Agency\nWorld Factbook (CIA Factbook, 2002). We have not incorporated this kind of information into\nthe ELKB."
    }, {
      "heading" : "5.2.2 Step 2: Select a Set of Candidate Words",
      "text" : "The building process does not consider repeated occurrences of closed-class words and high\nfrequency words (Morris and Hirst, 1991). Our system removes the words that should not appear\nin lexical chains using a 980-element stop list, union of five publicly-available lists: Oracle 8\nConText, SMART, Hyperwave, and lists from the University of Kansas and Ohio State\nUniversity. The stop list is presented in Appendix H. After eliminating these high frequency\nwords it would be beneficial to identify nominal compounds and proper nouns. Most of the\nknown WordNet-based implementations of lexical chains consider only nouns. This may be due\nto limitations in WordNet, in particular the fact that the IS-A hierarchy, essential to most\nsystems, is only developed extensively for nouns. Roget’s allows building lexical chains using\nnouns, adjectives, verb, adverbs and interjections. Our implementation considers the five parts-\nof-speech. Nominal compounds can be crucial in building correct lexical chains, as argued by\nBarzilay (1997); considering the words crystal and ball independently is not at all the same thing\nas considering the phrase crystal ball. Roget’s has a very large number of phrases, but this is not\nexploited. We have not developed a method for tagging phrases in a text in conjunction with the\nELKB. Roget’s contains around 100 000 unique words and phrases, but very few are technical or\nproper nouns. Any word or phrase that is not in the Thesaurus can only be included in a chain via\nsimple repetition."
    }, {
      "heading" : "5.2.3 Step 3: Build all Proto-chains for Each Candidate Word",
      "text" : "Inclusion in a proto-chain requires a relation between the candidate word and the chain. This is\nan essential step, open to interpretation. Should all word in the proto-chain be related via a\nthesaural relation, or is it enough to link adjacent words in the chain? An example of a chain is\n{cow, sheep, wool, scarf, boots, hat, snow} (Morris and Hirst, 1991). Should all of the\nwords in the chain be directly related to one another? This would mean that cow and snow should\nnot appear in the same chain. Should only specific senses of a word be included in a chain?\nShould a chain be built on an entire text, or only segments of it? Barzilay (1997) performs word\nChapter 5. Automating the Construction of Lexical Chains using Roget’s\n59\nsense disambiguation as well segmentation before building lexical chains. In theory, chains\nshould disambiguate individual senses of words and segment the text in which they are found; in\npractice this is difficult to achieve. What should be the distance between two words in a chain?\nThese issues are discussed by Morris and Hirst (ibid.) but not definitively answered by any\nimplementation. These are serious considerations, as it easy to generate spurious chains.\nSilber and McCoy (2002) build all possible proto-chains for the candidate words. All of the\nwords in a chain must be related to one another. The best intermediate chains are kept and\nbecome the output to the system. This implementation adopts a similar methodology. All\npossible proto-chains are built for the set of candidate words. All words in a chain must be\nrelated via the two proposed thesaural relations. For example, all the words in the chain {driving, exciting, hating, setting, set, setting, hated, drive, driving, driven, drove, drove, drove, cut} can be found in the Head 46 Disunion under the\nfollowing Paragraph once morphological transformations have been applied:\nForcing all words to be related allows building cohesive chains. Transitive relations that would\nallow two words to be related via a third one, for example sheep and scarf related through\nwool, are not allowed in this implementation. The proto-chains are scored using the procedure\ndescribed in the next section and the best ones are kept. The text is not segmented; rather the\ndistance in sentences between words in a proto-chain is taken into account by the scoring system."
    }, {
      "heading" : "5.2.4 Step 4: Select the Best Proto-chains for Each Candidate Word",
      "text" : "As a word or phrase may have several senses, it may also have several proto-chains, but the\nsystem must only keep one. Morris and Hirst (1991) identify three factors for evaluating strength\nof a lexical chain: reiteration, density, defined in terms of the types of thesaural relations that are\ncontained in the chain, and length. The more repetitious, denser and longer the chain, the\nstronger it is. This notion has been generally accepted by the other implementations of lexical\nchains, with the addition of taking into account the type of relations used in the chain when\nChapter 5. Automating the Construction of Lexical Chains using Roget’s\n60\nscoring its strength. The values in Table 5.1 are used to score the meta-chains. This is similar to\nSilber and McCoy’s (2002) scoring system.\nThe rationale for these scores is that the repetition of the same term anywhere in a text represents\na strong relation. Good writing style encourages the use of synonyms to convey the same idea.\nThese can be found in the same Roget’s Paragraph as the other words in the chain, but since a\nParagraph does not only contain synonyms, this relation is not as strong as reiteration. Since the\nfurther two words are in a text, the less chance they have of discussing the same topic, unless it is\na reference to previous idea, the relation based on inclusion in the Paragraph decreases in\nstrength as the distance increases. The scores attributed to each relation have been chosen on an\nad hoc basis. These values can be refined in conjunction with an accurate evaluation method."
    }, {
      "heading" : "5.2.5 Step 5: Select the Lexical Chains",
      "text" : "Our system selects the lexical chains from the best proto-chains. In Sibler and McCoy’s\nimplementation (2002) a word belongs to only one lexical chain. Most implementations have\nadopted this strategy. We have as well so as to compare our lexical chains to those of other\nsystems. A word belongs in the chain to which it contributes the most, which means the proto-\nchain with the highest score. The word is removed from all other proto-chains and their scores\nare adjusted accordingly. The lexical chain building procedure stops once the best proto-chain is\nselected for each word."
    }, {
      "heading" : "5.3 Step-by-Step Example of Lexical Chain Construction",
      "text" : "Ellman (2000) analyses the following quotation, attributed to Einstein, for the purpose of\nbuilding lexical chains. The words in bold are the candidate words retained by this\nimplementation that uses the ELKB after the stop list has been applied.\nWe suppose a very long train travelling along the rails with a constant velocity v and in the\ndirection indicated in Figure 1. People travelling in this train will with advantage use the train\nas a rigid reference-body; they regard all events in reference to the train. Then every event\nwhich takes place along the line also takes place at a particular point of the train. Also, the definition of simultaneity can be given relative to the train in exactly the same way as with\nChapter 5. Automating the Construction of Lexical Chains using Roget’s\n61\nrespect to the embankment.\nFigure 5.2: The Einstein quotation for which lexical chains are built.\nOur system builds all possible proto-chains, consisting of at least two words, for each candidate\nword, proceeding forward through the text. Since most words have multiple senses, they also\nhave multiple proto-chains. For this example, there are 9 proto-chains for the word suppose, 167\nfor train, 29 for travelling, 1 for rails, 2 for constant, 7 for direction, 3 for advantage,\n11 for regard, 15 for events, 131 for takes and 2 for line. These proto-chains are presented in\nAppendix L. The chain building procedure considers only the candidate words found between\nthe current location and the end of the file. The number of meta-chains is a function of the\nnumber of senses of a word and the number of remaining candidate words to be considered for\nthe chain. The best meta-chains retained for each word by the system ordered by their score with\nthe sense number (which corresponds to the Head in which the word can be found) and line\nnumbers of the first word are:\n1. train, rails, train, train, train, line, train, train, embankment [score: 9.0, sense: 624, line: 1]\n2. direction, regard, reference, respect [score: 4.0, sense: 9, line: 1]\n3. travelling, travelling, takes, takes [score: 4.0, sense: 981, line: 1]\n4. suppose, regard, takes, takes [score: 4.0, sense: 485, line: 1]\n5. regard, takes, takes [score: 3.0, sense: 438, line: 2]\n6. advantage, takes, takes [score: 3.0, sense: 916, line: 2]\n7. takes, takes, respect [score: 3.0, sense: 851, line: 3]\n8. constant, rigid [score: 2.0, sense: 494, line: 1]\n9. events, event [score: 2.0, sense: 725, line: 2]\n10.line, relative [score: 2.0, sense: 27, line: 3]\n11.rails, respect [score: 1.75, sense: 924, line: 1]\nOnce it is determined to which meta-chain a word contributes the most, the final lexical chains\ngenerated by the system are:\n1. train, rails, train, train, train, line, train, train, embankment [score: 9.0, sense: 624, line: 1]\n2. suppose, regard, takes, takes [score: 4.0, sense: 485, line: 1]\n3. direction, reference, respect [score: 3.0, sense: 9, line: 1]\n4. travelling, travelling [score: 2.0, sense: 981, line: 1]\n5. constant, rigid [score: 2.0, sense: 494, line: 1]\nChapter 5. Automating the Construction of Lexical Chains using Roget’s\n62\n6. events, event [score: 2.0, sense: 725, line: 2]\nAs a comparison, these eight lexical chains are obtained by Ellman (2002):\n1. train, rails, train, line, train, train, embankment\n2. direction, people, direction\n3. reference, regard, relative-to, respect\n4. travelling, velocity, travelling, rigid\n5. suppose, reference-to, place, place\n6. advantage, events, event\n7. long, constant\n8. figure, body\nThe Einstein quotation was first studied by St-Onge (1995) who obtained the following nine\nlexical chains using his WordNet-based system:\n1. train, velocity, direction, train, train, train, advantage, reference, reference-to, train, train, respect-to, simultaneity\n2. travelling, travelling\n3. rails, line\n4. constant, given\n5. figure, people, body\n6. regard, particular, point\n7. events, event, place, place\n8. definition\n9. embankment\nThe ELKB does not generate as many chains as Ellman or St-Onge, but the chains seem to\nadequately represent the paragraph. The best lexical chains generated by the ELKB {train,\nrails, train, train, train, line, train, train, embankment} and Ellman {train,\nrails, train, line, train, train, embankment} are almost identical. This is to be\nexpected, as they both use Roget’s Thesaurus. The only difference is the number of repetitions of\nthe nouns train, which is an indication that Ellman’s implementation is not as rigorous as it\nshould be. It is surprising that the remaining chains are so different, especially since certain\nChapter 5. Automating the Construction of Lexical Chains using Roget’s\n63\nwords are not even related in the ELKB, for example direction and people, or advantage and\nevent, as in the chains {direction, people, direction} and {advantage, events,\nevent}. This is a clear indication that the versions of Roget’s used by the systems are quite\ndifferent. Ellman’s second chain {direction, people, direction} is clearly erroneous since\nthe word direction only appears once in the paragraph. St-Onge generates chains that are hard\nto quantify as coherent compared the ones the two other systems build. In the chain {train, velocity, direction, train, train, train, advantage, reference, reference-to, train, train, respect-to, simultaneity} there is no intuitive relation between velocity\nand respect-to, although it is possible to consider a transitive relation using other words in the\nchain. It is odd that rails and line are not in the same chain as train, since these concepts are\nvery closely related. The singleton chains {definition} and {embankment} are also listed.\nLexical chain building systems generally do not consider these as they are too short to represent\na cohesive region in a text. This subjective comparison does not allow determining which system\nis best. An objective way is required for evaluating lexical chains, which we discuss in Section\n5.6."
    }, {
      "heading" : "5.4 A Comparison to the Original Implementation",
      "text" : "Morris’ and Hirst’s (1991) demonstrate their manual lexical chain procedure on the first section\nof an article in Toronto magazine, December 1987, by Jay Teitel, entitled “Outland”. This\nsection presents the text, where the candidate words are highlighted, and compares the lexical\nchains generated by the ELKB to those of the original algorithm.\nI spent the first 19 years of my life in the suburbs, the initial 14 or so relatively contented, the\nlast four or five wanting mainly to be elsewhere. The final two I remember vividly: I passed them\ndriving to and from the University of Toronto in a red 1962 Volkswagen 1500 afflicted with night blindness. The car's lights never worked - every dusk turned into a kind of medieval race\nagainst darkness, a panicky, mournful rush north, away from everything I knew was exciting,\ntoward everything I knew was deadly. I remember looking through the windows at the\ncommuters mired in traffic beside me and actively hating them for their passivity. I actually punched holes in the white vinyl ceiling of the Volks and then, by way of penance, wrote beside\nthem the names and phone numbers of the girls I would call when I had my own apartment in the\ncity. One thing I swore to myself: I would never live in the suburbs again.\nMy aversion was as much a matter of environment as it was traffic - one particular piece of the\nsuburban setting: \"the cruel sun.\" Growing up in the suburbs you can get used to a surprising number of things - the relentless \"residentialness\" of your surroundings, the weird certainty you\nhave that everything will stay vaguely new-looking and immune to historic soul no matter how\nmany years pass. You don't notice the eerie silence that descends each weekday when every\nsound is drained out of your neighbourhood along with all the people who've gone to work. I got used to pizza, and cars, and the fact that the cultural hub of my community was the collective TV\nChapter 5. Automating the Construction of Lexical Chains using Roget’s\n64\nset. But once a week I would step outside as dusk was about to fall and be absolutely bowled\nover by the setting sun, slanting huge and cold across the untreed front lawns, reminding me not just how barren and sterile, but how undefended life could be. As much as I hated the\nsuburban drive to school, I wanted to get away from the cruel suburban sun.\nWhen I was married a few years later, my attitude hadn't changed. My wife was a city girl\nherself, and although her reaction to the suburbs was less intense than mine, we lived in a series\nof apartments safely straddling Bloor Street. But four years ago, we had a second child, and simultaneously the school my wife taught at moved to Bathurst Street north of Finch Avenue.\nShe was now driving 45 minutes north to work every morning, along a route that was perversely\nidentical to the one I'd driven in college.\nWe started looking for a house. Our first limit was St. Clair - we would go no farther north. When we took a closer look at the price tags in the area though, we conceded that maybe we'd\nhave to go to Eglinton - but that was definitely it. But the streets whose names had once been\nmagical barriers, latitudes of tolerance, quickly changed to something else as the Sundays\npassed. Eglinton became Lawrence, which became Wilson, which became Sheppard. One wind-\nswept day in May I found myself sitting in a town-house development north of Steeles Avenue called Shakespeare Estates. It wasn't until we stepped outside, and the sun, blazing unopposed\nover a country club, smacked me in the eyes, that I came to. It was the cruel sun. We got into the\ncar and drove back to the Danforth and porches as fast as we could, grateful to have been\nreprieved.\nAnd then one Sunday in June I drove north alone. This time I drove up Bathurst past my wife's\nnew school, hit Steeles, and kept going, beyond Centre Street and past Highway 7 as well. I\npassed farms, a man selling lobsters out of his trunk on the shoulder of the road, a chronic care\nhospital, a country club and what looked like a mosque. I reached a light and turned right. I saw\na sign that said Houses and turned right again.\nIn front of me lay a virgin crescent cut out of pine bush. A dozen houses were going up, in\nvarious stages of construction, surrounded by hummocks of dry earth and stands of\nprecariously tall trees nude halfway up their trunks. They were the kind of trees you might see in\nthe mountains. A couple was walking hand-in-hand up the dusty dirt roadway, wearing matching blue track suits. On a \"front lawn\" beyond them, several little girls with hair exactly\nthe same colour of blond as my daughter's were whispering and laughing together. The air\nsmelled of sawdust and sun.\nIt was a suburb, but somehow different from any suburb I knew. It felt warm.\nIt was Casa Drive.\nIn 1976 there were 2,124,291 people in Metropolitan Toronto, an area bordered by Steeles\nAvenue to the north, Etobicoke Creek on the west, and the Rouge River to the east. In 1986, the\nsame area contained 2,192,721 people, an increase of 3 percent, all but negligible on an urban\nscale. In the same span of time the three outlying regions stretching across the top of Metro -\nPeel, Durham, and York - increased in population by 55 percent, from 814,000 to some 1,262,000. Half a million people had poured into the crescent north of Toronto in the space of a\ndecade, during which time the population of the City of Toronto actually declined as did the\npopulation of the \"old\" suburbs with the exception of Etobicoke and Scarborough. If the\nChapter 5. Automating the Construction of Lexical Chains using Roget’s\n65\nThe ELKB generates 110 lexical chains. The first 9 are presented here, the remaining are in\nAppendix L.\n1. suburbs, commuters, city, suburbs, suburbs, community, city, suburbs, closer, streets, road, crescent, houses, suburb, suburb, urban, crescent, suburbs, sprawling, city, city, suburbia [score: 20.0, sense: 192, line: 1]\n2. life, lights, rush, notice, weekday, week, fall, life, minutes, morning, day, time, light, span, time, stretching, decade, time, quarter [score: 17.0, sense: 110, line: 1]\n3. driving, exciting, hating, setting, set, setting, hated, drive, driving, driven, drove, drove, drove, cut [score: 12.75, sense: 46, line: 2]\n4. north, north, north, limit, north, north, north, north, top, north [score: 10.0, sense: 213, line: 3]\n5. girls, people, girl, virgin, girls, people, people, people, people, people [score: 8.75, sense: 132, line: 5]\n6. spent, passed, pass, moved, passed, fast, past, past, passed, wearing, past [score: 8.5, sense: 111, line: 1]\n7. final, night, call, house, called, hit, stages, construction, stands, whispering [score: 8.25, sense: 594, line: 2]\n8. sun, sun, sun, sun, sun, air, sun, space [score: 7.0, sense: 383, line: 7]\nMorris and Hirst identified the 9 following lexical chains:\n1. suburbs, driving, Volkswagen, car's, lights, commuters, traffic, Volks, apartment, city, suburbs, traffic, suburban, suburbs, residentialness, neighbourhood, community, suburban, drive, suburban, city, suburbs, apartments, Bloor St., Bathurst St., Finch St., driving, route, driven, house, St. Clair, Eglinton, streets, Eglinton, Lawrence, Wilson, Sheppard, town-house, Steeles, car, drove, Danforth, porches, drove, drove, Bathurst, Steeles, Centre St., Highway 7, trunk, road, light, turned, houses, turned, houses, roadway, lawn, suburb, suburb, people, Metropolitan Toronto, Steeles, people, urban, Metro, Peel, Durham, York, population, people, Toronto, population, city, Toronto, population, suburbs, Etobicoke, Scarborough, people, Toronto, city, suburbia, people.\n2. afflicted, darkness, panicky, mournful, exciting, deadly, hating, aversion, cruel, relentless, weird, eerie, cold, barren, sterile, hated, cruel, perversely, cruel\nChapter 5. Automating the Construction of Lexical Chains using Roget’s\n66\n3. married, wife, wife, wife\n4. conceded, tolerance\n5. virgin, pine, bush, trees, trunks, trees\n6. hand-in-hand, matching, whispering, laughing, warm\n7. first, initial, final\n8. night, dusk, darkness\n9. environment, setting, surrounding\nThe lexical chains produced by Morris and Hirst can be quite long. The first has 84 and the\nsecond 19 words. Specific knowledge of Toronto is used to build the first chain, something that\ncannot be reproduced by an automated implementation based only on Roget’s or WordNet. Both\nsets of chains identify suburbia as the main topic of the text. The ELKB hints that driving is a hated activity in the 3rd lexical chain. As with the Einstein example, a subjective comparison of\nlexical chains is not very conclusive."
    }, {
      "heading" : "5.5 Complexity of the Lexical Chain Building Algorithm",
      "text" : "The most computationally expensive part of the lexical chain building process is the construction\nof all possible meta-chains as described in Step 3: Build All Proto-chains for Each Candidate\nWord. The complexity of the other components of the implementation is negligible compared to\nthis one. Step 3 can be described by the following pseudo-code:\nGiven that there are n candidate words in a text, and each word has on average 2.14 senses in the\nELKB and that in the worst case there are as many unique candidate words as there are total candidate words in a text, the complexity of Step 3 is n * 2.14 * n which is O(n2). We use\nheuristics to improve performance, for example a sense, identified by the triple Head number,\nParagraph key and part-of-speech, is only considered once during the meta-chain building\nprocess, and the list of candidate words is reduced once all chains have been built for a given\nChapter 5. Automating the Construction of Lexical Chains using Roget’s\n67\nunique candidate word, but the computational complexity of the chain building procedure remains O(n2).\nSilber and McCoy (2002) propose a linear time algorithm for the implementation of lexical\nchains. Their system can process a 40,000 word document in 11 seconds using a Sparc Ultra 10\nCreator. As a manner of comparison, the ELKB implementation requires 5 seconds to process\nthe 89 word Einstein text and 51 seconds for the 964 word Outlands document using an Intel\nPentium 4, 2.40 GHz processor with 256 MB of RAM. The 4 seconds that it takes to load the\nELKB into memory is included in these times. This implementation is clearly much slower than\nSilber and McCoy’s although it has not been refined for this task. One of the goals of their\nWordNet-based implementation is to optimize the process so as to construct chains for extremely\nlarge documents. It is the fastest known lexical chain building system."
    }, {
      "heading" : "5.6 Evaluating Lexical Chains",
      "text" : "Morris and Hirst (1991) evaluate their lexical chains by comparing them to the heading structure\nof a text assigned by the author. This evaluation is adequate if the goal of lexical chains is to\nsegment a text into distinct regions according to their topic. It assumes that the author has\npresented the only possible correct partitioning of the text. Lexical chains are generally not used\nfor this purpose, Barzilay (1997) has even segmented the text before building the chains, and\nauthors do not always assign subject headings to identify the various ideas. For these reasons,\nthat evaluation procedure cannot be used to evaluate and objectively compare the lexical chains\ncreated by various systems.\nHirst and St-Onge (1998) propose the task of malapropism detection to evaluate lexical chains. A\nmalapropism is defined as “the confounding of an intended word with another word of similar\nsound or similar spelling that has a quite different and malapropos meaning, for example, an\ningenuous [for ingenious] machine for peeling oranges.” (Fellbaum, 1998, p. 304). This task is\nnot very common and the evaluation procedure requires a corpus of malapropisms, a resource\nthat is not readily available.\nSilber and McCoy (2002) evaluate their implementation by comparing their lexical chains to\nsummaries of a document collection. Their evaluation method is inspired by those used in text\nsummarization. Their corpus is made up of scientific documents with abstracts and chapters from\nUniversity textbooks that contain chapter summaries. Marcu (1999) has argued that abstracts of\narticles can be accepted as reasonable summaries. This procedure involves comparing the senses\nof the words in the lexical chains to the senses of the words in the abstract. This is necessary, as\nthe summaries many not contain the same words as the texts, and therefore the lexical chains.\nChapter 5. Automating the Construction of Lexical Chains using Roget’s\n68\nThis evaluation procedure is interesting, since large amounts of documents with their summaries\nare produced for the Document Understanding Conferences (DUC, 2001, 2002). For this\nevaluation procedure to work with Roget’s, it is necessary to tag the senses of the words and\nphrases in the texts and summaries using those found in the Thesaurus. We have not performed\nor implemented any manual or automatic procedure to do so. Although promising, we have not\nevaluated our implementation of lexical chains using this procedure.\nLexical chains can also be evaluated by assessing the quality of the summaries that are produced\nby them (Barzilay and Elhadad, 1997; Brunn, Chali and Pinchak, 2001) but the investigation of\nthis task is beyond the scope of this dissertation."
    }, {
      "heading" : "5.7 About the Straightforwardness of Implementing Lexical Chains",
      "text" : "The experiment shows that it is possible to create lexical chains using our electronic version of\nRoget’s Thesaurus, but that it is not as straightforward as it was originally claimed. Roget’s has a\nvery rich structure that can be exploited for lexical chain construction. Using the ELKB, many\nmore thesaural relations can be used than in this implementation, but they come with a\ncomputational cost. WordNet implementations have access to a different set of relations and\nlexical material. Although there is a consensus on the high-level algorithm, there are significant\ndifferences in implementations. The major criticism of lexical chains is that there is no adequate\nevaluation of their quality. Until it is established, it will be hard to compare implementations of\nlexical chain construction algorithms. This experiment demonstrates that the ELKB and WordNet\ncan be used effectively for the same task.\nChapter 6. Finding the Hidden Treasures in the Thesaurus\n69\n6 Finding the Hidden Treasures in the Thesaurus\nThe experiments presented in Chapters 4 and 5 show that Roget’s Thesaurus is a valuable\nresource for NLP, yet these applications exploit only a fraction of this abundant lexical\nknowledge base. A current trend in NLP is to combine lexical resources to overcome their\nindividual weaknesses. This chapter discusses the correspondence between Roget’s and\nWordNet. We show a method for disambiguating Roget’s paragraphs by mean of groups of\nsynsets. This procedure exposes WordNet’s semantic relations that are present in the Thesaurus.\nThe fact that the ELKB does not label semantic relations explicitly is a major hindrance for NLP\napplications: “Roget’s remains … an attractive lexical resource for those with access to it. Its\nwide, shallow hierarchy is densely populated with nearly 200,000 words and phrases. The\nrelationships among the words are also much richer than WordNet’s IS-A or HAS-PART links.\nThe price paid for this richness is a somewhat unwieldy tool with ambiguous links” (McHale,\n1998). Machine learning techniques can label these relations given sufficient training data. This\nchapter concludes with a study of avenues to improve the ELKB using Longman’s Dictionary of\nContemporary English (LDOCE) (Procter, 1978).\n6.1 A Quantitative Comparison of Roget’s and WordNet\nChapter 3 describes the similarities between Roget’s and WordNet. This section presents a\ndetailed examination of the portions that contain the most and least overlap in lexical content.\nRoget’s ontology can be divided into classes that describe the external world (Abstract\nRelations, Space, Matter) and ones that describe the internal world of the human. These\nsubjects almost evenly dived the lexical material, 446 headwords belong to the external world,\n544 to the internal world. WordNet seems to favor the external world, with only two of the nine\nunique beginners {psychological feature} and {act, human action, human activity}\ndescribing the internal world of the human. Intuition suggests that Roget’s and WordNet should\nhave a big overlap in lexical material pertaining to the material world. Experiments have\nidentified a list of over 45,000 strings that can be found in WordNet 1.7.1 and the 1987 Roget’s.\nTable 6.1 presents the distribution of words and phrases within the ELKB ordered by class\nnumber. % of c.h., % of c.k. and % of c.s. in Table 6.1 indicate the percentage of heads,\nkeywords and strings that can be found in this common word and phrase list.\nBoth lexical resources are similar in absolute size, containing about 200,000 word-sense pairs.\n53% of all words in Roget’s are nouns, 20% adjectives, 23% verbs, 4% adverbs and less than 1%\nare interjections. 74% of WordNet are nouns, 15% adjectives, 8% verbs and 3% adverbs. There\nare no interjections in WordNet. Intuition suggested a large overlap between both resources, but\nChapter 6. Finding the Hidden Treasures in the Thesaurus\n70\nthe 46,399 common words and phrases only represent about 43% of the unique words and\nphrases in the Thesaurus and 32% of WordNet. All of the occurrences of common strings make\nup 63% of Roget’s total lexical content. The equivalent calculation has not been performed for\nWordNet. The top-level of the ontologies hinted that the overlap would be concentrated in the\nfirst three Roget classes, but the results in Table 6.1 shows the common strings distributed pretty\nevenly across the whole resource. A head-per-head analysis shows that 78% of head names as\nwell as 75% of paragraph keywords can be found in WordNet. 86% of heads have at least 50% of\ntheir words in common with those of WordNet and 93% of heads have at least 50% of keywords\nin common. Table 6.2 shows the 10 heads with the highest and lowest percentage of common\nstrings. H in WN indicates that the head name can be found in WordNet.\nIdentifying the areas where WordNet and the Thesaurus overlap are of interest since this should\nbe a good indicator of where the two resources can be combined. The comparison of the Head\n567 Perspicuity has with the WordNet synsets that are synonymous to perspicuity and\nperspicuous shows that the semicolon groups and the synsets organized around these two\nwords can be quite similar. The content of the Head is the following:\nChapter 6. Finding the Hidden Treasures in the Thesaurus\n71\nThe synsets containing the synonyms of perspicuity and perspicuous are:\nThe synonym synsets do not contain all of the words and phrases in the Head, even though 94%\nof these are contained in WordNet. This is an indication that the semantic relations that link the\nsemicolon groups in a paragraph extend beyond synonymy. This is further discussed in section\n6.3.\nChapter 6. Finding the Hidden Treasures in the Thesaurus\n72\nBoth lexical resources are comparable in size, but WordNet’s 111,223 synsets are almost double\nthe 59,877 semicolon groups in the ELKB. Only 1,431 semicolon groups and synsets are\nidentical, 916 consist of one word or phrase, 459 of two, 51 of three, 4 of four and 1 of five\nwords and phrases. The common sets of four or more words are:\n• {compass, grasp, range, reach} • {ease, relaxation, repose, rest} • {escape, leak, leakage, outflow} • {fourfold, quadruple, quadruplex, quadruplicate} • {coronach, dirge, lament, requiem, threnody}\nSemicolon groups contain on average 3.75 words and phrases, synsets 1.76. This may indicate\nthat synsets represent a much more focused concept than semicolon groups even though both are\ndefined as sets of closely related words. The next section investigates a technique for matching\nsemicolon groups to synsets.\n6.2 Combining Roget’s and WordNet\nThe semicolon group and the synset represent the smallest independent unit of Roget's and\nWordNet. Although not identical, these groups can be compared and linked. Kwong (1998, 2001)\nproposes an algorithm for aligning WordNet noun synsets with their equivalent noun sense in the\n1987 edition of Penguin’s Roget’s Thesaurus. A sense in Roget’s is defined by a noun and its\nlocation within a specific semicolon group, paragraph and head. The following steps describe my\nvariant of Kwong’s algorithm:\nStep 0: Take an index item W from Roget’s index. For example, the word desk is an index item in:\ndesk\ncabinet 194 n. stand 218 n. classroom 539 n.\nStep 1: In Roget’s, find all paragraphs Pm such that W ∈ Pm.\nStep 2: In WordNet, build all mini-nets Mn for W. A mini-net consists of a synset Sn such that W\n∈ Sn with its corresponding hypernym synsets Hyp(Sn), and coordinate synsets Co(Sn). The coordinate synsets represent the immediate hypernyms of the synset as well as the hypernyms’\nimmediate hyponyms. This is done to compare similar structures in both resources and to ensure\nenough lexical material to calculate a significant overlap.\nChapter 6. Finding the Hidden Treasures in the Thesaurus\n73\nStep 3: Compute a similarity score matrix A for the WordNet mini-nets and the Roget’s paragraphs. A similarity score Ajk is computed for the j th WordNet mini-net and the kth Roget’s\nparagraph, according to the following formula:\nAjk = α1|Sj ∩ Pk| + α2|Hyp(Sj) ∩ Pk| + α3|Co(Sj) ∩ Pk|\nKwong sets α1 = α2 = α3 = 1. These weights are reasonable as it seems that no one relation is more important than another. The procedure uses lemmata when comparing words but does not\nlemmatize elements of phrases.\nStep 4: Find the global maximum max(Ajk) of the matrix A. The jth WordNet mini-net can be aligned with the kth Roget’s Thesaurus paragraph found in the maximum intersection.\nKwong (ibid.) takes a WordNet synset and assigns a Roget’s sense to it. The system maps 18,000\nnoun synsets onto 30,000 senses. She gives the following statement regarding accuracy:\n“Although it had been difficult and impractical to check the mappings … exhaustively given the\nhuge amount of data, extensive sampling of the results showed that over 70% of the mappings\nare expected to be accurate”. We have not verified this precision value.\nNastase and Szpakowicz (2001) have implemented a procedure that links WordNet semicolon\ngroups to Roget’s senses for all parts-of-speech. As the same complement of semantic relations\nis not available to all parts-of-speech in WordNet, different ones must be used for building mini-\nnets:\n• using nouns: synonyms, hyponyms, hypernyms, meronyms and holonyms;\n• using adjectives and adverbs: synonyms;\n• using words that are derived from another word w: the information pertaining to the word w, according to its part-of-speech.\nThe precision of this algorithm is 57% when applied to the various parts-of-speech,\ncomparatively to Kwong’s 70% for nouns. We have not implemented much of this algorithm\nusing the ELKB due to the large amount of data which makes it very time consuming to evaluate.\nThe task of aligning semicolon groups with synsets is more complicated as this mapping is\nmany-to-many. Daudé et al. (2001) map WordNet 1.5 synsets onto WordNet 1.6 synsets using\nrelaxation labeling. Their technique is very effective, but it relies on the structure imposed by the\nsemantic relations. This mapping cannot be simply translated to align Roget’s semicolon groups\nonto WordNet synsets as no explicit semantic relations are given in the Thesaurus. Without\nfurther experiments, it is difficult to assess the feasibility of this algorithm.\nChapter 6. Finding the Hidden Treasures in the Thesaurus\n74\n6.3 Importing Semantic Relations from WordNet into Roget’s\nRoget’s lacks explicitly labelled semantic relations. Chapter 4 has shown that its structure can be\nexploited to measure semantic similarity effectively, but having labeled semantic relations allows\nto further untangling the rich information contained in the paragraphs. This can be illustrated by\nexamining the first paragraph of the Head 276 Aircraft. The hyponym and meronym relations\nused by WordNet are clearly present, as well as the notions of “science of aircraft”,\n“testing of an aircraft”, as well as “places where an aircraft can land”:\nNastase and Szpakowicz (ibid.) have found empirically the hypernym relation to be prevalent\nbetween the keyword and the other phrases that make up a paragraph. Cassidy (2000) has\nidentified 400 semantic relations in the 1911 edition of Roget’s Thesaurus. Some of these\nrelations are: is-caused-by, is-performed-by, has-consequence, is-measured-by, is-job-\nof. Cassidy’s work is done manually. An alternative to this is to align paragraphs and mini-nets\nand label the relations automatically. The paragraph for the noun decrement illustrates the\nprocedure:\nChapter 6. Finding the Hidden Treasures in the Thesaurus\n75\nThe mini-net for the noun decrement can be built in the following way:\nOverview of noun decrement The noun decrement has 2 senses:\n1. decrease, decrement -- (the amount by which something decreases) 2. decrease, decrement -- (a process of becoming smaller)\nSynonyms/Hypernyms of noun decrement:\nSense 1 - decrease, decrement: {amount} Sense 2 - decrease, decrement: {process}\nHyponyms of noun decrement:\nSense 1 - decrease, decrement: {drop, fall}, {shrinkage} Sense 2 - decrease, decrement: {wastage}, {decay, decline}, {slippage}, {decline, diminution}, {desensitization, de sensitisation}, {narrowing}\nCoordinate Terms of noun decrement:\nBy matching semicolon groups and synsets where at least one word or phrase is in common, it is\npossible to rearrange the Roget’s paragraph in the following manner:\nN. decrement\nHyponym: deduction, depreciation, cut 37 diminution; refund, shortage, slippage, defect 307 shortfall, 636 insufficiency;\nChapter 6. Finding the Hidden Treasures in the Thesaurus\n76\nThis algorithm only allows for discovering the WordNet relations that are present in Roget’s.\nLearning the relations labeled by Cassidy and using machine learning techniques (O’Hara and\nWiebe, 2003) would expose more clearly the richness of the Thesaurus. This has yet to be\nattempted using the ELKB.\n6.4 Augmenting WordNet with Information contained in Roget’s\nSemicolon groups are organized around subjects in Roget’s whereas synsets are linked by the\nclosed set of semantic relations in WordNet. Fellbaum (1998, p.10) calls this particularity of\nWordNet the Tennis Problem and describes it in the following manner: “… WordNet does not\nlink racquet, ball, and net in a way that would show that these words, and the concepts behind\nthem, are part of another concept that can be expressed by court game.” George Miller has\npromised that this will be corrected in WordNet 2.0. Roget’s Thesaurus can help in this task. It\ncontains the paragraph ball game in the Head 837 Amusement:\nThis paragraph contains rackets, ball game and netball, similar words to racquet, ball and\nnet. A native English speaker can make the connection. This example illustrates that the\norganization of Roget’s lexical material is quite different than that of WordNet’s, that WordNet\nwould benefit from Roget’s topical clustering and that adding such links automatically is not a\ntrivial task. Stevenson (2001) considers that synsets can be linked using a new relationship when\nChapter 6. Finding the Hidden Treasures in the Thesaurus\n77\na Roget’s paragraph has a strong overlap with three or more synsets. He links 24,633 WordNet\nsynsets to 3,091 Roget’s International Thesaurus (Chapman, 1977) paragraphs. These figures\nrepresent almost 25% of WordNet synsets and 50% of ELKB paragraphs in terms of absolute\nnumbers, which suggests that augmenting WordNet in this manner is a promising avenue of\nresearch.\n6.5 Other Techniques for Improving the ELKB\nKwong (1998) has shown that it is possible to obtain a mapping between LDOCE and Roget’s\nThesaurus using WordNet. Although Kwong only performs this experiment on a small set of 36\nnouns, the idea of incorporating information contained in LDOCE into Roget’s is very attractive.\nLDOCE contains definition and frequency information that is very beneficial. Researchers have\nalso proven it to be a valuable resource for NLP. The ELKB that would contain definitions,\nfrequency information as well as a set of labeled semantic relations is very close to the holy grail\nof computational lexicography: “a neutral, machine-tractable, dictionary” (Wilks et al., 1996).\nChapter 7. Summary, Discussion, and Future Work\n78"
    }, {
      "heading" : "7 Summary, Discussion, and Future Work",
      "text" : "This chapter summarizes the contributions of the thesis and presents known flaws of the ELKB,\naspects that should be improved and ideas for future applications."
    }, {
      "heading" : "7.1 Summary",
      "text" : "The goal of this thesis was to establish if Roget’s Thesaurus can be a realistic alternative to\nWordNet. To achieve this, various sub-goals had to be met. The first is the design and\nimplementation of the ELKB; next I performed NLP experiments whose results are compared to\nthose of WordNet-based systems. The thesis also contains a quantitative comparison of both\nlexical knowledge bases.\nChapter 1 presents the context, goals and organization of this thesis.\nChapter 2 gives a brief history of how computational linguists have used thesauri in NLP. It\ndiscusses the various versions of Roget’s and explains the rationale for choosing the 1987 edition\nof Penguin’s Roget’s Thesaurus of English Words and Phrases as the source for the ELKB. This\nchapter also discusses several applications of Roget’s Thesaurus and WordNet in NLP.\nChapter 3 discusses the design of the ELKB as well as its implementation. It shows the necessary\nsteps to transform the computer-readable Pearson Education files into a tractable form. This\ninvolves converting the lexical material into a format that can be more easily exploited,\nidentifying data structures and classes to computerize the Thesaurus, indexing all of the words\nand phrases in the resource and ensuring that they can be retrieved even when the exact string is\nnot supplied. I explain in detail Roget’s organization and contrast it with WordNet’s. The\nimplementation verifies the accuracy of the design and ensures that Roget’s functionality is\nfaithfully reproduced by the ELKB.\nChapter 4 explains how Roget’s Thesaurus can be used to measure semantic distance. Using\nthree well known benchmarks for the evaluation of semantic similarity, I correlate the similarity\nvalues calculated by the ELKB and six WordNet-based measures with those assigned by human\njudges. Roget’s gets scores of over .80 for two of the three benchmarks, quite close to those\nobtained when the experiments are replicated using humans. The system outperforms the\nWordNet-based measures most of the time. The chapter presents a second class of experiments,\nwhere the correct synonym must be selected amongst a group of four words. These are taken\nfrom ESL, TOEFL and Readers’ Digest questions. The ELKB is compared to the same WordNet-\nChapter 7. Summary, Discussion, and Future Work\n79\nbased measures as well as to statistical methods. The ELKB outperforms all systems that do not\nrely on combined approaches, obtaining scores in the 80% range.\nChapter 5 explains how lexical chains can be built using the ELKB. It presents the necessary\ndesign decisions for automating the chain building procedure by walking through the algorithm\nthat has been used for all implementations. I compare the lexical chains the ELKB constructs to\nthose built manually by the inventors of lexical chains, automatically using a partially\ncomputerized Roget’s Thesaurus and a WordNet-based system. This chapter discusses several\nevaluation procedures, in particular one in which the lexical chains are compared to summaries\nof texts.\nChapter 6 describes steps for combining Roget’s and WordNet. It shows some of the rich\nimplicit semantic relations that are found in the Thesaurus. I explain how WordNet can enrich\nRoget’s and vice-versa, as well as present an algorithm for aligning both resources.\nChapter 7 presents a summary of the dissertation. It discusses known flaws as of the ELKB as\nwell as future extensions and applications."
    }, {
      "heading" : "7.2 Conclusions",
      "text" : "This dissertation has shown that it is possible to computerize Roget’s Thesaurus so that it\nmaintains all of the functionality of the printed version and allows for manipulations suitable for\nNLP applications. I have used the ELKB in a few experiments, but these are not enough to\ndetermine if it is a credible alternative to WordNet. I offer a few ideas for those who intend to use\nthe ELKB or want to build a similar knowledge base.\n7.2.1 Building an ELKB from an Existing Lexical Resource\nBuilding an ELKB from an existing lexical resource is a very attractive proposition. A\ncomputational linguist can save much time by exploiting the structure and lexical material\ncontained in existing dictionaries and thesauri. I have encountered two major problems when\nimplementing the ELKB. The lexicographer’s directives are not known and it is very tedious to\ncomprehend the organization of paragraphs and semicolon groups without having specific\nexplanations for the underlying decisions. Implementing the ELKB would have been much\nsimpler had the editor’s instructions for the preparation of the Roget’s Thesaurus been available.\nThe next problem is that the lexical material must be licensed from the publisher for a\nconsiderable price. This hinders the public acceptance of the ELKB as most research groups are\nunwilling to spend money on an unproven resource.\nChapter 7. Summary, Discussion, and Future Work\n80\n7.2.2 Comparison of the ELKB to WordNet\nThe ELKB is comparable to WordNet in many ways. It contains a similar number of words and\nphrases and this thesis has shown that they both can be used for the same tasks. Although they\nare similar, this dissertation demonstrates that their organization is quite different. The ELKB\ndraws on the 150 years that lexicographers have taken to prepare the Thesaurus. Pearson’s\npublishes a new edition roughly every ten years. The ELKB lacks the support of the NLP\ncommunity which WordNet has. WordNet is slightly more than ten years old, new versions are\nreleased about every two years. Version 2.0 promises to correct many flaws that are discussed in\nthis dissertation. Several research groups work independently from George Miller’s to enhance\nthis lexical resource. Its prevalence is not only due to its quality but largely also to the fact that it\nis free.\n7.2.3 Using the ELKB for NLP Experiments\nThis thesis has used the ELKB to measure semantic similarity between words and phrases and to\nbuild lexical chains. I have been able to perform these experiments with ease using the Java\nimplementations. These two applications can be integrated into larger systems, for example one\nthat performs Text Summarization or Question Answering tasks. The ELKB has also been used\nin two computer science honors projects. Gilles Roy and Pierre Chrétien created a graphical user\ninterface for the Thesaurus, Tad Stach wrote a program to play the Reader’s Digest Word Power\nGame. The ELKB must be used in more experiments to test the software thoroughly.\n7.2.4 Known Errors in the ELKB\nThe ELKB still contains errors, mostly in its lexical material. These are often due to the mistakes\ncontained in the original Pearson files. I estimate that about 2% - 3% of the words and phrases in\nthe ELKB are incorrect. Although this percentage is small, it is enough to be noticed and\nadversely affect future applications.\n7.2.5 Improvements to the ELKB\nThe computerized Roget’s Thesaurus that I have implemented is far from being the perfect\nlexical knowledge base. Many improvements can be made to the software."
    }, {
      "heading" : "7.2.5.1 Retrieval of Phrases",
      "text" : "The ELKB does not perform any morphological transformations when looking up a phrase. If a\nuser does not supply the exact string contained in the Index, no result will be returned. For\nexample, the phrase “sixty four thousand dollar question” will not be found because the\nChapter 7. Summary, Discussion, and Future Work\n81\nexact string in the ELKB is “the sixty-four-thousand-dollar question”. Giving access to\nall of Roget’s phrases is a difficult problem to solve but is one that merits attention. The\nThesaurus contains many phrases, some of them very peculiar, for example: “Cheshire cat\ngrin”, “Homeric laughter” or “wisest fool in Christendom”. It is possibly an area where\nthe ELKB is superior to WordNet. I have not investigated this. An ideal solution would be to\nintegrate in the Index a method that could extract all phrases that contain certain words. Also,\nmorphological transformations would have to be performed on all words in a phrase to find the\nform contained in the Thesaurus. An imperfect solution that I have adopted for the ELKB is to\nindex all two word phrases under each of the words. Although this improves the recall of\nphrases, it introduces many odd references for index entries. For example, the phrase fish food\nis now indexed under fish and food. The ELKB determines that the distance between food and\nrooster is 4, meaning that the words are quite similar, when the intuitive association is not that\nstrong. The system is finding the shortest path between all references of food and rooster,\nwhich happens to be between fish food and rooster, found in two different noun paragraphs\nof the head 365 Animality. Animal. When the phrase fish food is ignored, the distance\nbetween food and rooster is 10."
    }, {
      "heading" : "7.2.5.2 Displaying the Semicolon Group Which Contains a Variant of the Search Word",
      "text" : "As described in Chapter 3, when a word is looked up, morphological transformations are\nperformed to find all matching entries that are contained in the Index. For example, when a user\nenters the word tire, the ELKB finds the words tire and tyre in the Index. The reference for\ntyre is wheel 250 N. The system finds the Head 250 Circularity: simple circularity,\nand locates the wheel noun paragraph. The ELKB searches the paragraph sequentially until the\nword tire is found. Since tyre is the word contained in the paragraph, the correct semicolon\ngroup is not returned. This causes a slight problem when calculating semantic distance. For\nexample, the system determines the distance between the words hub and tire to be 2 instead of\n0 as the words do not appear in the semicolon group ;hub, felloe, felly, tyre;. This is not\ndifficult to correct, but awkward, so I left it as one of a number of future adjustments."
    }, {
      "heading" : "7.2.5.3 Original vs. New Index",
      "text" : "The ELKB uses an Index that is generated from all of the words and phrases that it contains.\nPearson Education supplies an Index that is about half the size of the automatically generated\none. The system stores the two in separate files that cannot be used at the same time. I would\nhave more faithfully reproduced Roget’s Thesaurus if the entries in the Index had been flagged\nas original and new.\nChapter 7. Summary, Discussion, and Future Work\n82"
    }, {
      "heading" : "7.2.5.4 Optimization of the ELKB",
      "text" : "This version of the ELKB serves as a proof of concept. Future releases must improve memory\nusage and speed if this resource is to be a viable alternative to WordNet. Performance can be\nimproved by loading the text of the 990 Heads into memory and storing absolute references to\nParagraphs and semicolon groups, as described in Chapter 3. The current implementation loads\nin 3 seconds on a Pentium 4, 2.40 GHz with 256 MB of RAM, and requires about 40 MB of\nRAM."
    }, {
      "heading" : "7.3 Future Work",
      "text" : "The ultimate goal of this research should be to make the ELKB available to any research group\nthat requests it. Beyond the fact that the lexical material must be licensed, future maintainers of\nthe system should thoroughly evaluate the ELKB and use it in a wide variety of applications so as\nto attract the interest of the NLP community. It should also be enhanced to make it more\ncompetitive with regards to WordNet.\n7.3.1 More Complete Evaluation of the ELKB\nThis dissertation has performed a partial evaluation of the ELKB by comparing it to WordNet-\nbased systems and statistical techniques. A comparison to other versions of Roget’s Thesaurus,\nnamely the 1911 edition, FACTOTUM and Roget’s International Thesaurus should be carried\nout. Until this is done, I cannot say how good this version of Roget’s is compared to all others.\nFuture research should perform further benchmark experiments with the ELKB, namely Word\nSense Disambiguation. This is a problem that has a long history in NLP and for which thesauri\nhave been used (Ide and Véronis, 1998)."
    }, {
      "heading" : "7.3.2 Extending the Applications Presented in the Thesis",
      "text" : "Turney (2002) has used his semantic similarity metric to classify automobile and movie reviews.\nBigham et al. (2003) use their similarity metric to answer analogy problems. In an analogy\nproblem, the correct pair of words must be chosen amongst four pairs, for example: cat:meow::\n(a) mouse:scamper, (b) bird:peck, (c) dog:bark, (d) horse:groom, (e) lion:scratch. To\ncorrectly answer dog:bark, a system must know that a meow is the sound that a cat makes and\na bark the sound that a dog makes. Both of these applications can be implemented with the\nELKB.\nAs discussed in Chapter 5, several researchers have used lexical chains for Text Summarization,\nmost notably Barzilay and Elhadad (1997) as well as Silber and McCoy (2000). Since I have\nChapter 7. Summary, Discussion, and Future Work\n83\nimplemented a system that can build lexical chains, it would be very interesting to put it to this\ntask.\n7.3.3 Enhancing the ELKB\nChapter 6 describes several enhancements to the ELKB. If I had combined Roget’s with\nWordNet, labeled the implicit semantic relations and included frequency information as well as\ndictionary definitions from LDOCE, the ELKB would be one of the premier lexical resources for\nNLP.\nChapter 8. References\n84"
    }, {
      "heading" : "8 References",
      "text" : "1. Barzilay, R. (1997). Lexical Chains for Summarization. Master’s thesis, Ben-Gurion\nUniversity.\n2. Barzilay, R., and Elhadad, M. (1987). Using lexical chains for text summarization. In: ACL/EACL-97 summarization workshop, 10 – 18.\n3. Beckwith, R., Fellbaum, C., Gross, D. and Miller, G. (1991). WordNet: A Lexical Database Organized on Psycholinguistic Principles. In Zernik, ed., Lexical Acquisition. Exploiting On-\nLine Resources to Build a Lexicon. Hillsdale, NJ: Lawrence Erlbaum Associates, 211 – 232.\n4. Berrey, L. and Carruth G. (eds.) (1962). Roget’s International Thesaurus, Third Edition. New York: Thomas Y. Crowell Co.\n5. Bigham, J., Littman, M., Shnayder, V. and Turney, P. (2003). Combining Indepented Modules to Solve Multiple-choice Synonym and Analogy Problems. Submitted to the 19th\nConference on Uncertainty in Artificial Intelligence, Acapulco, Mexico, August.\n6. Brunn, M., Chali, Y. and Pinchak, C. (2001). Text Summarization Using Lexical Chains. Workshop on Text Summarization. New Orleans, LA., September.\n7. Bryan, R. (1973). Abstract Thesauri and Graph Theory Applications to Thesaurus Research. In Sedelow, ed., Automated Language Analysis (1972 – 3). Lawrence: University of Kansas\nPress, 45 – 89.\n8. Bryan, R. (1974). Modelling in Thesaurus Research. In Sedelow, ed., Automated Language Analysis (1973 – 4). Lawrence: University of Kansas Press, 44 – 59.\n9. Budanitsky, A. and Hirst, G. (2001). Semantic distance in WordNet: An experimental, application-oriented evaluation of five measures. In Proceedings of NAACL 2001 WordNet\nand Other Lexical Resources Workshop, Pittsburgh, 29 – 34.\n10. Cassidy, P. (1996). Modified Roget Available, http://www.hit.uib.no/corpora/19962/0042.html, May 28.\n11. Cassidy, P. (2000). An investigation of the semantic relations in the Roget’s Thesaurus: Preliminary results. In Proceedings of CICLing ’00.\n12. Chali, Y. (2001). Topic Detection Using Lexical Chains. Proceedings of the Fourteenth International Conference on Industrial and Engineering Applications of Artificial\nIntelligence and Expert Systems, Budapest, June 2001, 552 – 558.\n13. Chapman, R. (1977). Roget’s International Thesaurus (Fourth Edition). New York: Harper\nChapter 8. References\n85\nand Row.\n14. Chapman, R. (1992). Roget’s International Thesaurus (Fifth Edition). New York: HarperCollins.\n15. CIA Factbook. (2002). The World Factbook 2002. Central Intelligence Agency. http://www.cia.gov/cia/publications/factbook/\n16. Daudé, J., Padró, L. and Rigau, G. (2001). A Complete WN 1.5 to WN 1.6 Mapping. Proceedings of the NAACL WordNet and Other Lexical Resources workshop. Pittsburgh,\nJune, 83 – 88.\n17. Driscoll, J., Lautenschlager, J. and Zhao, M. (1992). The QA System. Proceedings of the first Text Retrieval Conference (TREC-1). Gaithersburg, Maryland, 199 – 208.\n18. DUC. (2001, 2002). Document Understanding Conferences. http://duc.nist.gov/\n19. Edmonds, P. and Hirst, G. (2002). Near-Synonymy and Lexical Choice. Computational Linguistics, 28(2): 105 – 144.\n20. Ellman, J. and Tait, J. (1999). Roget's thesaurus: An additional Knowledge Source for Textual CBR? Proceedings of 19th SGES International Conference on Knowledge Based\nand Applied Artificial Intelligence. Springer-Verlag, London, 204 – 217.\n21. Ellman, J. (2000) Using Roget’s Thesaurus to Determine the Similarity of Texts. Ph.D. thesis, School of Computing Engineering and Technology, University of Sunderland,\nSunderland, England.\n22. Ellman, J. and Tait, J. (2000). On the Generality of Thesaurally derived Lexical Links. Proceedings of the 5th International Conference on the Statistical Analysis of Textual Data\n(JADT 2000). Lauzanne, Switzerland, May, 147 – 154.\n23. Fellbaum, C. (ed.) (1998). WordNet: An Electronic Lexical Database. Cambridge: The MIT Press.\n24. Fellbaum, C. (2001). Manual and Automatic Semantic Annotation with WordNet. Proceedings of the NAACL WordNet and Other Lexical Resources workshop. Pittsburgh,\nJune, 3 – 10.\n25. Fellbaum, C. (ed.) (2002). Proceedings of the first International WordNet Conference. Mysore, India, January.\n26. Finkelstein, L., Gabrilovich, E., Matias, Y., Rivlin, E., Solan, Z., Wolfman, G. and Ruppin, E. (2002). Placing Search in Context: The Concept Revisited. ACM Transactions on\nInformation Systems, January, 20(1):116 – 131.\nChapter 8. References\n86\n27. Francis, W. and Kucera, H. (1982). Frequency Analysis of English Usage. Houghton Mifflin, Boston.\n28. Gabrilovich, E. (2003). The WordSimilarity-353 Test Collection. http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/wordsim353.html\n29. Green, S. (1999). Lexical Semantics and Automatic Hypertext Construction. In ACM Computing Surveys, 31(4), December.\n30. Halliday, M. and Hasan, R. (1976). Cohesion in English. Longman.\n31. Harabagiu, S. (ed.) (1998). Proceedings of the COLING/ACL Workshop on Usage of WordNet in Natural Language Processing Systems. Montreal, Canada, August.\n32. Hart, M. (1991). Project Gutenberg Official Home Site. http://www.gutenberg.net/\n33. Hirst, G. and St-Onge, D. (1998). Lexical chains as representation of context for the detection and correction of malapropisms. In Fellbaum, ed., WordNet: An electronic lexical\ndatabase. Cambridge, MA: The MIT Press, 305 – 332.\n34. Houston, J. (1984). Thesaurus of ERIC Descriptors. Phoenix: Oryx Press.\n35. Ide, N. and Véronis, J. (1998). Introduction to the Special Issue on Word Sense Disambiguation: The State of the Art. Computational Linguistics. Special Issue on Word\nSense Disambiguation, 24(1), 1-40.\n36. Inspec Thesaurus (1985). London: IEE.\n37. Jarmasz, M. and Szpakowicz, S. (2001a). Roget’s Thesaurus as an Electronic Lexical Knowledge Base. In NIE BEZ ZNACZENIA. Prace ofiarowane Profesorowi Zygmuntowi\nSaloniemu z okazji 40-lecia pracy naukowej. W. Gruszczynski, D. Kopcinska, eds.,\nBialystok.\n38. Jarmasz, M. and Szpakowicz, S. (2001b). Roget's Thesaurus: a Lexical Resource to Treasure. Proceedings of the NAACL WordNet and Other Lexical Resources workshop.\nPittsburgh, June, 186 – 188.\n39. Jarmasz, M. and Szpakowicz, S. (2001c). The Design and Implementation of an Electronic Lexical Knowledge Base. Proceeding of the 14th Biennial Conference of the Canadian\nSociety for Computational Studies of Intelligence (AI 2001), Ottawa, Canada, June, 325–\n334.\n40. Jarmasz, M. and Szpakowicz, S. (2003a). Not As Easy As It Seems: Automating the Construction of Lexical Chains Using Roget’s Thesaurus. Proceedings of the 16th Canadian\nConference on Artificial Intelligence (AI 2003), Halifax, Canada, June, 544–549.\nChapter 8. References\n87\n41. Jarmasz, M. and Szpakowicz, S. (2003b). Roget’s Thesaurus and Semantic Similarity. Proceedings of Conference on Recent Advances in Natural Language Processing (RANLP\n2003), Borovets, Bulgaria, September, to appear.\n42. Jiang, J. and Conrath, D. (1997). Semantic similarity based on corpus statistics and lexical taxonomy. In Proceedings of International Conference on Research in Computational\nLinguistics, Taiwan.\n43. Kirkpatrick, B. (1998). Roget’s Thesaurus of English Words and Phrases. Harmondsworth, Middlesex, England: Penguin.\n44. Kwong, O. (1998). Aligning WordNet with Additional Lexical Resources. Proceedings of the COLING/ACL Workshop on Usage of WordNet in Natural Language Processing\nSystems. Montreal, Canada, August, 73 – 79.\n45. Kwong, O. (2001a). Forming and Integrated Lexical Resource for Word Sense Disambiguation. Proceedings of the 15th Pacific Asia Conference on Language, Information\nand Computation (PACLIC 15). Hong Kong, 109 – 119.\n46. Kwong, O. (2001b). Word Sense Disambiguation with an Integrated Lexical Resource. Proceedings of the NAACL WordNet and Other Lexical Resources workshop. Pittsburgh,\nJune, 11 – 16.\n47. Landauer, T. and Dumais, S. (1997). A Solution to Plato’s Problem: The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge. Psychological\nReview, 104 (1997), 211 – 240.\n48. Landes, S., Leacock, C. and Tengi, R. (1998). Building Semantic Concordances. In Fellbaum, ed., WordNet: An electronic lexical database. Cambridge, MA: The MIT Press,\n200 – 216.\n49. Leacock, C. and Chodorow, M. (1998). Combining local context and WordNet similarity for word sense identification. In Fellbaum, ed., WordNet: An electronic lexical database.\nCambridge, MA: The MIT Press, 265 – 283.\n50. Lehmann, F. (1995). Combining Ontologies, Thesauri, and Standards. Proceedings of the IJCAI Workshop on Basic Ontological Issues in Knowledge Sharing. Montreal, Canada,\nAugust.\n51. Lenat, D. (1984). Computer Software for Intelligent Systems. Scientific American, September, 204.\n52. Lesk, M. (1995). Why Use Words to Label Ideas: The Use of Dictionaries and Thesauri in Information Retrieval. In Walker et al., eds., Automating the Lexicon. Research and Practice\nChapter 8. References\n88\nin a Multilingual Environment. Oxford, England: Oxford University Press, 285 – 300.\n53. Lewis, M. (ed.) (2000 – 2001) Reader’s Digest, 158(932, 934, 935, 936, 937, 938, 939, 940), 159(944, 948). Reader’s Digest Magazines Canada Limited.\n54. Lexico, LLC. (2001) Thesaurus.com Web Site. http://www.thesaurus.com/\n55. Li, X., Szpakowicz, S. and Matwin, S. (1995). A WordNet Based Algorithm for Word Sense Disambiguation. Proceedings of the Eleventh International Joint Conference on Artificial\nIntelligence (IJCAI-95). Montreal, Canada, 1368 – 1374.\n56. Lin, D. (1998). An information-theoretic definition of similarity. In Proceedings of the 15th International Conference on Machine Learning, Madison, WI.\n57. Macleod, C., Grishman, R., and Meyers, A. (1994). The Comlex Syntax Project: The First Year. Proceedings of the ARPA Human Language Technology Workshop. San Francisco:\nMorgan Kaufmann, 8 – 12.\n58. Mandala, R., Tokunaga, T. and Tanaka, H. (1999). Complementing WordNet with Roget and Corpus-based Automatically Constructed Thesauri for Information Retrieval.\nProceedings of the Ninth Conference of the European Chapter of the Association for\nComputational Linguistics, Bergen.\n59. Marcu, D. (1999). The Automatic Creation of Large Scale Corpora for Summarization Research. Proceedings of the 22nd International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval, Berkley, CA.\n60. Master, R. (1995). Roget’s II: The New Thesaurus. Boston : Houghton Mifflin.\n61. Masterman, M. (1957). The Thesaurus in Syntax and Semantics. Mechanical Translation, 4(1 – 2), 35 – 43.\n62. McHale, M. (1998). A Comparison of WordNet and Roget's Taxonomy for Measuring Semantic Similarity. Proceedings of the COLING/ACL Workshop on Usage of WordNet in\nNatural Language Processing Systems. Montreal, Canada, August.\n63. Medical Subject Headings (1983), Bethesda, Md.: National Library of Medicine.\n64. Mihalcea, R. and Moldovan, D. (1998). Word Sense Disambiguation Based on Semantic Density. Proceedings of the COLING/ACL Workshop on Usage of WordNet in Natural\nLanguage Processing Systems. Montreal, Canada, August.\n65. Mihalcea, R. (2003). WordNet Bibliography. http://engr.smu.edu/~rada/wnb/\n66. Miller, G. (1990). WordNet: an On-line Lexical Database. International Journal of Lexicography, 3(4).\nChapter 8. References\n89\n67. Miller, G. and Charles, W. (1991). Contextual correlates of semantic similarity. Language and Cognitive Processes, 6(1): 1-28.\n68. Miller, G. (1998a). Foreword. In Fellbaum, ed., WordNet: An electronic lexical database. Cambridge, MA: The MIT Press, xv – xxii.\n69. Miller, G. (1998b). Nouns in WordNet. In Fellbaum, ed., WordNet: An electronic lexical database. Cambridge, MA: The MIT Press, 23 – 46.\n70. Moldovan, D., and Peters, W. (eds.) (2001). Proceedings of the NAACL 2001 Workshop on WordNet and Other Lexical Resources: Applications, Extensions and Customizations.\nPittsburgh, June.\n71. Morris, J. and Hirst, G. (1991). Lexical Cohesion Computed by Thesaural Relations as an Indicator of the Structure of Text. Computational linguistics, 17:1, 21 – 48.\n72. Nastase, V. and Szpakowicz, S. (2001). Word Sense Disambiguation with an Integrated Lexical Resource. Proceedings of the NAACL WordNet and Other Lexical Resources\nworkshop. Pittsburgh, June, 17 – 22.\n73. O’Hara, T. and Wiebe, J. (2003). Classifying Functional Relations in Factotum via WordNet Hypernym Associations. Proceedings of CICLing-2003, CIC, IPN, Mexico City.\n74. Okumura, M. and Honda, T. (1994). Word sense disambiguation and text segmentation based on lexical cohesion. Proceedings of the Fifteenth Conference on Computational\nLinguistics (COLING-94), volume 2, 755 – 761.\n75. Olsen, M. (1996). ARTFL Project: ROGET'S Thesaurus Search Form. http://humanities.uchicago.edu/forms_unrest/ROGET.html\n76. Pasca, M. and Harabagiu, S. (2001). The Informative Role of WordNet in Open-Domain Question Answering. Proceedings of the NAACL WordNet and Other Lexical Resources\nworkshop. Pittsburgh, June, 138 – 143.\n77. Patrick, A. (1985). An Exploration of Abstract Thesaurus Instantiation. M. Sc. Thesis, University of Kansas, Lawrence, KS.\n78. Pedersen, T. (2003). Semantic Distance in WordNet Package. http://www.d.umn.edu/~tpederse/tools.html\n79. Procter, P. (1978). Longman Dictionary of Contemporary English. Harlow, Essex, England: Longman Group Ltd.\n80. Reader’s Digest Word Power Game. (2003). http://www.rdnetwork.net/wp/can/\n81. Resnik, P. (1995). Using information content to evaluate semantic similarity. In Proceedings\nChapter 8. References\n90\nof the 14th International Joint Conference on Artificial Intelligence, pages 448 – 453,\nMontreal.\n82. Rada, R., Mili, H., Bicknell, E., and Bletner, M. (1989). Development and Application of a Metric on Semantic Nets. IEEE Transactions on Systems, Man and Cybernetics, Vol. 19,\nNo. 1, 17 – 30\n83. Roget, P. (1852). Roget’s Thesaurus of English Words and Phrases. Harlow, Essex, England: Longman Group Ltd.\n84. Rubenstein, H. and Goodenough, J. (1965). Contextual correlates of synonymy. Communications of the ACM, 8(10): 627 – 633.\n85. Sedelow, S. and Sedelow, W. (1992). Recent Model-based and Model-related Studies of a Large-scale Lexical Resource [Roget's Thesaurus]. Proceedings of the Fourteenth\nInternational Conference on Computational Linguistics. Nantes, France, August, 1223 –\n1227.\n86. Sedelow, S. and Mooney, D. (1988). Knowledge Retrieval from Domain-transcendent Expert Systems II. Research Results. Proceedings of the American Society for Information\nScience (ASIS) Annual Meeting. Knowledge Industry Publications, White Plains, New York,\n209 – 212.\n87. Silber, H. and McCoy K. (2000). Efficient Text Summarization Using Lexical Chains. Proceedings of the 13th International Conference on Intelligent User Interfaces, IUI2000.\nNew Orleans, LA, January, 252 – 255.\n88. Silber, H. and McCoy K. (2002). Efficiently Computed Lexical Chains As an Intermediate Representation for Automatic Text Summarization. Computational Linguistics. Special\nIssue on Summarization. 28(4), 487 – 496.\n89. Slator, B. (1992). Using Context for Sense Preference. In Jacobs, ed., Text-Based Intelligent Systems: Current Research in Text Analysis, Information Extraction and Retrieval.\nSchenectady, New York: GE Research and Development Center.\n90. Sparck Jones, K. (1964). Synonymy and Semantic Classification. Ph.D. thesis, University of Cambridge, Cambridge, England.\n91. Stairmand, M. (1994). Lexical Chains, WordNet and Information Retrieval. Unpublished manuscript, Centre for Computational Linguistics, UMIST, Manchester.\n92. Stevenson, M. (2001). Adding Thesaural Information to Noun Taxonomies. Proceeding of Conference on Recent Advances in Natural Language Processing (RANLP 2001), Bulgaria,\nSeptember, 297 –299.\nChapter 8. References\n91\n93. Sussna, M. (1993). Word Sense Disambiguation for Free-text Indexing Using a Massive Semantic Network. In Bhargava, B., Finin, T. and Yesha, Y., eds., Proceedings of the 2nd\nInternational Conference on Information and Knowledge Management, Arlington, 67 – 74.\n94. Terra, E. and Clarke, C. (2003). Frequency Estimates for Statistical Word Similiarity Measures. Proceedings of the Human Language Technology Conference (HLT-NAACL\n2003), Edmonton, Canada, May, to appear.\n95. Test of English as a Foreign Language (2003), Educational Testing Service, Princeton, New Jersey, http://www.ets.org/.\n96. Tatsuki, D. (1998) Basic 2000 Words – Synonym Match 1. In Interactive JavaScript Quizzes for ESL Students, http://www.aitech.ac.jp/~iteslj/quizzes/js/dt/mc-2000-01syn.html\n97. Turney, P. (2001). Mining the Web for Synonyms: PMI-IR vs LSA on TOEFL. Proceedings of the Twelfth European Conference on Machine Learning (ECML-2001), Freiburg,\nGermany, pp. 491 – 502.\n98. Urdang, L. (1978a). Basic Book of Synonyms and Antonyms. Harmondsworth, Middlesex, England: Penguin.\n99. Urdang, L. (1978b). The Synonym Finder. Emmaus, Pa: Rodale Press.\n100. Vossen, P. (1998). EuroWordNet: A Multilingual Database with Lexical Semantic Networks. Kluwer Academic Publishers, Dordrecht.\n101. Walker, D., Zampolli, A. and Calzolari N. (eds.) (1995). Automating the Lexicon. Research and Practice in a Multilingual Environment. Oxford, England: Oxford University Press.\n102. Wilks, Y., Slator, B. and Guthrie, L. (1996). Electric Words : Dictionaries, Computers, and Meanings. Cambridge : The MIT Press.\n103. World Gazetter. (2003). http://www.world-gazetteer.com/home.htm\n104. Yarowsky, D. (1992). Word-Sense Disambiguation Using Statistical Models of Roget's Categories Trained on Large Corpora. Proceedings of the 14th International Conference on\nComputational Linguistics (COLING-92). Nantes, France, August, 454 – 460.\n105. Zernik, U. (ed.) (1991). Lexical Acquisition. Exploiting On-Line Resources to Build a Lexicon. Hillsdale, NJ: Lawrence Erlbaum Associates."
    }, {
      "heading" : "Appendix A: The Basic Functions and Use Cases of the ELKB",
      "text" : "A - 1\nAppendix A: The Basic Functions and Use Cases of the ELKB\nThese are the basic functions of the ELKB:\n1. Look up a Word or Phrase. 2. Browse the Taxonomy. 3. Look up All Words and Phrases in a Head. 4. Calculate the Distance between Two Words or Phrases. 5. Identify the Thesaural Relation between Two Words or Phrases.\nThese functions can be described by their accompanying use cases."
    }, {
      "heading" : "1 Look up a Word or Phrase",
      "text" : "1. The user enters a word or phrase. 2. The system performs morphological transformations on the word or phrases. 3. The system searches the index for all entries that contain the transformed search term. 4. The system returns all references for the found index entries. 5. The user chooses a reference from the result list. 6. The system returns the paragraph that contains the reference. 7. The semicolon group that contains the reference is located. Alternative: The search term is not in the index. At step 3, the system fails to find the search term in the index. Allow the user to re-enter a word or phrase. Return to primary scenario at step 2. Alternative: The user cancels the look up. At step 1 or 5, the user cancels look up."
    }, {
      "heading" : "2 Browse the Taxonomy",
      "text" : "1. The system displays the names of the classes. 2. The user chooses a class to expand. 3. The system displays the sections that belong to the selected class. 4. The user chooses a section to expand. 5. The system displays the sub-sections that belong to the selected section. 6. The user chooses a sub-section to expand. 7. The system displays the head groups that belong to the selected sub-section. 8. The user chooses a head group to expand. 9. The system displays the heads that belong to the selected head group. 10. The user chooses a head to expand. 11. The system displays the text of the selected head."
    }, {
      "heading" : "Appendix A: The Basic Functions and Use Cases of the ELKB",
      "text" : "A - 2\nAlternative: The user selects another class, section, sub-section, head group or head. At steps 2, 4, 6, 8 or 10 the user can decide to expand another class, section, sub-section, head group or head. Return to primary scenario at step 3, 5, 7, 9 or 11 depending on what step has been performed. Alternative: The user collapses a class, section, sub-section, head group or head. At step 3, 5, 7, 9 or 11 the user can decide to collapse a class, section, sub-section, head group or head. The system hides any of the content displayed by the selected class, section, sub-section, head group or head. Return to primary scenario at step 2, 4, 6, 8 or 10 depending on what steps are possible. Alternative: The user specifies a head number. The user may know the exact head number he wants to look up. The system displays the entire path indicating the class, section, sub-section, head group and head. The system continues at step 11."
    }, {
      "heading" : "3 Look up all Words and Phrases in a Head",
      "text" : "1. The user selects or enters a head number. 2. The system displays the text of the selected head."
    }, {
      "heading" : "4 Calculate the Distance between Two Words or Phrases",
      "text" : "1. The user enters two words or phrases. 2. The system performs morphological transformations on each word or phrase. 3. The system looks up each transformed word or phrase in the index. 4. The system finds all paths between each reference of the words or phrases. 5. The system assigns a score to every path: 0 if the two references point to the same semicolon\ngroup, 2 if they point to the same paragraph, 4 if the point to the same part-of-speech of the same head, 6 if they point to the same head, 8 if they point to the same head group, 10 if they point to the same sub-section, 12 if they point to the same section, 14 if they point to the same class and 16 if the references are in two different classes of the ELKB.\n6. The distance is given by the smallest score."
    }, {
      "heading" : "5 Identify the Thesaural Relation between Two Words or Phrases",
      "text" : "1. The user enters two words or phrases 2. If the same lexicographical string was entered, the thesaural relation is “T0: reiteration”.\nTerminate the procedure. 3. Else, the system performs morphological transformations on each word or phrase."
    }, {
      "heading" : "Appendix A: The Basic Functions and Use Cases of the ELKB",
      "text" : "A - 3\n4. The system looks up each transformed word or phrase in the index. 5. The system compares pair wise the references of the index entries. 6. If two references point to the same paragraph, then the thesaural relation is “T1”. Terminate\nthe procedure. 7. Else, no thesaural relations exist between these two words or phrases."
    }, {
      "heading" : "Appendix B: The ELKB Java Documentation",
      "text" : "B - 1\nAppendix B: The ELKB Java Documentation\nThis appendix presents a summary of the Java documentation for all the classes of the ELKB.\nPackage ca.site.elkb\nClass Summary Category Represents the Roget's Thesaurus Tabular Synopsis of Categories.\nGroup Represents a Roget's Thesaurus Head group.\nHead Represents a Roget's Thesaurus Head.\nHeadInfo Object used to store the information that defines a Head but not its words and phrases.\nIndex Represents the computer index of the words and phrases of Roget's Thesaurus.\nMorphy Performs morphological transformations using the same rules as WordNet.\nParagraph Represents a Roget's Thesaurus Paragraph.\nPath Represents a path in Roget's Thesaurus between two words or phrases.\nPathSet A set that contains all of the paths between two words and phrases as well as the number of minimum length paths.\nReference Represents a symbolic pointer to a location where a specific word or phrase can be found in Roget's Thesaurus.\nRogetClass Represents the topmost element in Roget's Thesaurus Tabular Synopsis of Categories.\nRogetELKB Main class of the Roget's Thesaurus Electronic Lexical KnowledgeBase.\nRogetText Represents the Text of Roget's Thesaurus.\nSection Represents a Roget's Thesaurus Section.\nSemRel Represents a Roget's Thesaurus relation between a word or phrase.\nSG Represents a Roget's Thesaurus Semicolon Group.\nSubSection Represents a Roget's Thesaurus Sub-section.\nVariant Allows to obtain a variant of an English spelling."
    }, {
      "heading" : "Appendix B: The ELKB Java Documentation",
      "text" : "B - 2\nca.site.elkb Class Category\njava.lang.Object | +--ca.site.elkb.Category\npublic class Category extends java.lang.Object\nRepresents the Roget's Thesaurus Tabular Synopsis of Categories. The topmost level of this ontology divides the Thesaurus into eight Classes:\n1. Abstract Relations\n2. Space\n3. Matter\n4. Intellect: the exercise of the mind (Formation of ideas)\n5. Intellect: the exercise of the mind (Communication of ideas)\n6. Volition: the exercise of the will (Individual volition)\n7. Volition: the exercise of the will (Social volition)\n8. Emotion, religion and morality\nClasses are further divided into Sections, Sub-sections, Head groups, and Heads.\nConstructor Summary Category() Default constructor.\nCategory(java.lang.String filename) Constructor that builds the Category object using the information contained in a file.\nMethod Summary int getClassCount()\nReturns the number of Roget's Classes in this ontology.\njava.util.ArrayList getClassList() Returns the array of RogetClass objects.\nint getHeadCount() Returns the number of Heads in this ontology.\nint getHeadGroupCount() Returns the number of Head groups in this ontology.\njava.util.ArrayList getHeadList() Returns the array of HeadInfo objects.\nca.site.elkb.RogetClass getRogetClass(int index) Returns the Roget's Class at the specified position in the array of"
    }, {
      "heading" : "Appendix B: The ELKB Java Documentation",
      "text" : "B - 3\nClasses.\nint getSectionCount() Returns the number of Sections in this ontology.\nint getSubSectionCount() Returns the number of Sub-sections in this ontology.\nvoid printHeadInfo() Prints the array of HeadInfo objects to the standard output.\nvoid printRogetClass(int index) Prints the Roget's Class at the specified position in the array of Classes to the standard output.\njava.lang.String toString() Converts to a string representation the Category object."
    }, {
      "heading" : "Appendix B: The ELKB Java Documentation",
      "text" : "B - 4\nca.site.elkb Class Group\njava.lang.Object | +--ca.site.elkb.Group\npublic class Group extends java.lang.Object\nRepresents a Roget's Thesaurus Head group. For example:\n79 Generality 80 Speciality A Group can contain 1,2 or 3 HeadInfo objects.\nConstructor Summary Group() Default constructor.\nGroup(int start) Constructor that takes an integer to indicate first Head number of the Group.\nMethod Summary void addHead(ca.site.elkb.HeadInfo head)\nAdd a HeadInfo object to this Group.\nint getHeadCount() Returns the number of Heads in this Group.\njava.util.ArrayList getHeadList() Returns the array of HeadInfo objects.\nint getHeadStart() Returns the number of the first Head in this Group.\nvoid setHeadStart(int start) Sets the number of the first Head in this Group.\njava.lang.String toString() Converts to a string representation the Group object."
    }, {
      "heading" : "Appendix B: The ELKB Java Documentation",
      "text" : "B - 5\nca.site.elkb Class Head java.lang.Object | +--ca.site.elkb.Head\npublic class Head extends java.lang.Object\nRepresents a Roget's Thesaurus Head. A Head is defined by the following attributes:\n• Head number • Head name • Class number • Section num • list of paragraphs • number of paragraphs • number of semicolon groups • number of words and phrases • number of cross-references • number of see references\nThe relative postions of the noun, adjective verb, adverb and interjection paragraphs in the array of paragarphs is kept by the nStart, adjStart, vbStart, advStart, and intStart attributes.\nConstructor Summary Head() Default constructor.\nHead(int num, java.lang.String name, int clNum, int section) Constructor which sets the Head number and name, as well as the Class and Section number.\nHead(java.lang.String fname) Constructor that builds the Head object using the information contained in a file.\nMethod Summary int getAdjCount()\nReturns the number of adjective word and phrases of this Head.\nint getAdjCRefCount() Returns the number of adjective cross-references of this Head.\nint getAdjParaCount() Returns the number of adjective paragraphs of this Head.\nint getAdjSeeCount() Returns the number of adjective references of this Head."
    }, {
      "heading" : "Appendix B: The ELKB Java Documentation",
      "text" : "B - 6\nint getAdjSGCount() Returns the number of adjective semicolon groups of this Head.\nint getAdjStart() Returns the index of the first adjective paragraph in the array of Pragraph objects of this Head.\nint getAdvCount() Returns the number of adverb word and phrases of this Head.\nint getAdvCRefCount() Returns the number of adverb cross-references of this Head.\nint getAdvParaCount() Returns the number of adverb paragraphs of this Head.\nint getAdvSeeCount() Returns the number of adverb references of this Head.\nint getAdvSGCount() Returns the number of adverb groups of this Head.\nint getAdvStart() Returns the index of the first adverb paragraph in the array of Pragraph objects of this Head.\nint getClassNum() Returns the Class number of this Head.\nint getCRefCount() Returns the number of cross-references of this Head.\njava.lang.String getHeadName() Returns the name of this Head.\nint getHeadNum() Returns the number of this Head.\nint getIntCount() Returns the number of interjection word and phrases of this Head.\nint getIntCRefCount() Returns the number of interjection cross-references of this Head.\nint getIntParaCount() Returns the number of interjection paragraphs of this Head.\nint getIntSeeCount() Returns the number of interjection references of this Head.\nint getIntSGCount() Returns the number of interjection semicolon groups of this Head.\nint getIntStart() Returns the index of the first interjection paragraph in the array of Pragraph objects of this Head.\nint getNCount() Returns the number of noun word and phrases of this Head.\nint getNCRefCount() Returns the number of noun cross-references of this Head."
    }, {
      "heading" : "Appendix B: The ELKB Java Documentation",
      "text" : "B - 7\nint getNParaCount() Returns the number of noun paragraphs of this Head.\nint getNSeeCount() Returns the number of noun see references of this Head.\nint getNSGCount() Returns the number of noun semicolon groups of this Head.\nint getNStart() Returns the index of the first noun paragraph in the array of Pragraph objects of this Head.\nca.site.elkb.Paragraph getPara(int paraNum, java.lang.String pos) Returns the a Paragraph object specified by the paragraph number and part-of-speech.\nca.site.elkb.Paragraph getPara(java.lang.String paraKey, java.lang.String pos) Returns the a Paragraph object specified by the paragraph key and part-of-speech.\nint getParaCount() Returns the number of paragraphs of this Head.\nint getSectionNum() Returns the Section number of this Head.\nint getSeeCount() Returns the number of see references of this Head.\nint getSGCount() Returns the number of semicolon groups of this Head.\nint getVbCount() Returns the number of verb word and phrases of this Head.\nint getVbCRefCount() Returns the number of verb cross-references of this Head.\nint getVbParaCount() Returns the number of verb paragraphs of this Head.\nint getVbSeeCount() Returns the number of verb references of this Head.\nint getVbSGCount() Returns the number of verb groups of this Head.\nint getVbStart() Returns the index of the first verb paragraph in the array of Pragraph objects of this Head.\nint getWordCount() Returns the number of words of this Head.\nvoid print() Prints the contents of this Head to the standard output.\nvoid printAllSG() Prints all the semicolon groups of this Head separated on a separate line to the standard output."
    }, {
      "heading" : "Appendix B: The ELKB Java Documentation",
      "text" : "B - 8\nvoid printAllWords() Prints all the words and phrases of this Head separated on a separate line to the standard output.\nvoid setClassNum(int num) Sets the Class number of this Head.\nvoid setHeadName(java.lang.String name) Sets the name of this Head.\nvoid setHeadNum(int num) Sets the number of this Head.\nvoid setSectionNum(int num) Sets the Section number of this Head.\njava.lang.String toString() Converts to a string representation the Head object."
    }, {
      "heading" : "Appendix B: The ELKB Java Documentation",
      "text" : "B - 9\nca.site.elkb Class HeadInfo java.lang.Object | +--ca.site.elkb.HeadInfo\npublic class HeadInfo extends java.lang.Object\nObject used to store the information that defines a Head but not its words and phrases. It contains the following attributes:\n• Head number • Head name • Class number • Section number • Sub-section name • Head group, defined as a list of HeadInfo objects\nConstructor Summary HeadInfo() Default constructor.\nHeadInfo(int number, java.lang.String name, int cn, int sn, java.lang.String subName, java.util.ArrayList groupList) Constructor which sets the Head number and name, as well as the Class and Section number, Sub-section name and Head group list.\nHeadInfo(java.lang.String sInfo, int cn, int sn, java.lang.String subSectInfo, java.lang.String sGroupInfo) Constructor which sets the Head number and name, as well as the Class and Section number, Sub-section name and Head group list.\nMethod Summary int getClassNum()\nReturns the Class number of this Head.\njava.util.ArrayList getHeadGroup() Returns the array of HeadGroup objects of this Head.\njava.lang.String getHeadName() Returns the name of this Head.\nint getHeadNum() Returns the number of this Head.\nint getSectNum() Returns the Section number of this Head.\njava.lang.String getSubSectName()"
    }, {
      "heading" : "Appendix B: The ELKB Java Documentation",
      "text" : "B - 10\nReturns the Sub-section name of this Head.\nvoid setClassNum(int num) Sets the number of this Head.\nvoid setHeadGroup(java.util.ArrayList group) Sets the array of HeadGroup objects of this Head.\nvoid setHeadName(java.lang.String name) Sets the name of this Head.\nvoid setHeadNum(int num) Sets the number of this Head.\nvoid setSectNum(int num) Sets the Section number of this Head.\nvoid setSubSectName(java.lang.String name) Sets the Section name of this Head.\njava.lang.String toString() Converts to a string representation the HeadInfo object."
    }, {
      "heading" : "Appendix B: The ELKB Java Documentation",
      "text" : "B - 11\nca.site.elkb Class Index\njava.lang.Object | +--ca.site.elkb.Index\nAll Implemented Interfaces:\njava.io.Serializable\npublic class Index extends java.lang.Object implements java.io.Serializable\nRepresents the computer index of the words and phrases of Roget's Thesaurus. According to Kirkpatrick (1998) \"The index consists of a list of items, each of which is followed by one or more references to the text. These references consist of a Head number, a keyword in italics, and a part of speech label (n. for nouns, adj. for adjectives, vb. for verbs, adv. for adverbs, and int. for interjections). The keyword is given to identify the paragraph which contains the word you have looked up; it also gives and indication of the ideas contained in that paragraph, so it can be used as a clue where a word has several meanings and therefire several references.\" An example of an Index Entry is:\nstork obstetrics 167 n. bird 365 n.\nIn this example stork is an Index Item and obstetrics 167 n. is a Reference. This Index object consists of a hashtable of Index Entries, hashed on the String value of the Index Item. For every key (Index Item) the value is a list of Reference objects. The hashtable is implemented using a HashMap. See Also:\nSerialized Form\nConstructor Summary Index() Default constructor.\nIndex(java.lang.String filename) Constructor that builds the Index object using the information contained in a file.\nIndex(java.lang.String fileName, int size) Constructor that builds the Index object using the information contained in a file and sets the initial size of the index hashtable.\nMethod Summary boolean containsEntry(java.lang.String key)\nReturns true if the specified entry is contained in this index.\njava.util.TreeSet getEntry(java.lang.String key) Returns all references for a given word or phrase in the index."
    }, {
      "heading" : "Appendix B: The ELKB Java Documentation",
      "text" : "B - 12\njava.util.ArrayList getEntryList(java.lang.String key) Returns the list of references for a given word or phrase in the index.\njava.util.ArrayList getEntryList(java.lang.String key, int itemNo) Returns the list of references for a given word or phrase in the index preceded by a number to identify the reference.\njava.util.TreeSet getHeadNumbers(java.lang.String key) Returns a set of head numbers in which a word or phrase can be found.\nint getItemCount() Returns the number of entries in this index.\nint getItemsMapSize() Returns the number of items contained in the hash map of this index.\nint getRefCount() Returns the number of references in this index.\njava.util.ArrayList getRefObjList(java.lang.String key) Returns an array of Reference objects.\njava.lang.String getRefPOS(java.lang.String key) Returns a string containing the part-of-speech of the references for a given index entry.\njava.lang.String getStrRef(java.lang.String strIndex) Returns a reference in String format as printed in Roget's Thesaurus.\njava.util.ArrayList getStrRefList(java.lang.String key) Returns a list of references in string format instead of pointers.\nint getUniqRefCount() Returns the number of unique references in this index.\nvoid printEntry(java.lang.String key) Prints the index entry along with its references to the standard output.\nvoid printEntry(java.lang.String key, int itemNo) Prints the index entry along with its numbered references to the standard output."
    }, {
      "heading" : "Appendix B: The ELKB Java Documentation",
      "text" : "B - 13\nca.site.elkb Class Morphy\njava.lang.Object | +--ca.site.elkb.Morphy\nAll Implemented Interfaces:\njava.io.Serializable\npublic class Morphy extends java.lang.Object implements java.io.Serializable\nPerforms morphological transformations using the same rules as WordNet.\nThe following suffix substitutions are done for:\n• nouns: 1. \"s\" -> \"\"\n2. \"ses\" -> \"s\"\n3. \"xes\" -> \"x\"\n4. \"zes\" -> \"z\"\n5. \"ches\" -> \"ch\"\n6. \"shes\" -> \"sh\"\n7. \"men\" -> \"man\"\n• adjectives: 1. \"er\" -> \"\"\n2. \"est\" -> \"\"\n3. \"er\" -> \"e\"\n4. \"est\" -> \"e\"\n• verbs: 1. \"s\" -> \"\"\n2. \"ies\" -> \"y\"\n3. \"es\" -> \"e\"\n4. \"es\" -> \"\"\n5. \"ed\" -> \"e\"\n6. \"ed\" -> \"\"\n7. \"ing\" -> \"e\"\n8. \"ing\" -> \"\"\nThe noun.exc, adj.exc, verb.exc and adv.exc exception files, located in the $HOME/roget_elkb directory, are searched before applying the rules of detachment. See Also:\nSerialized Form"
    }, {
      "heading" : "Appendix B: The ELKB Java Documentation",
      "text" : "B - 14\nField Summary static java.lang.String ADJ_EXC\nLocation of the adj.exc file.\nstatic java.lang.String ADV_EXC Location of the adv.exc file.\nstatic java.lang.String ELKB_PATH Location of the ELKB data directory.\nstatic java.lang.String NOUN_EXC Location of the noun.exc file.\nstatic java.lang.String USER_HOME Location of user's Home directory.\nstatic java.lang.String VERB_EXC Location of the verb.exc file.\nConstructor Summary Morphy() Default constructor.\nMethod Summary java.util.HashSet getBaseForm(java.lang.String words)\nReruns all the base forms for a given word.\nstatic void main(java.lang.String[] args) Allows the Morphy class to be used via the command line."
    }, {
      "heading" : "Appendix B: The ELKB Java Documentation",
      "text" : "B - 15\nca.site.elkb Class Paragraph java.lang.Object | +--ca.site.elkb.Paragraph\npublic class Paragraph extends java.lang.Object\nRepresents a Roget's Thesaurus Paragraph. A Paragraph is defined by the following attributes:\n• Head number • Paragraph name • Paragraph keyword • Part-of-speech • list of Semicolon Groups • number of Semicolon Groups • number of words and phrases • number of Cross-references • number of See references\nConstructor Summary Paragraph() Default constructor.\nParagraph(int head, int para, java.lang.String p) Constructor which sets the Head number, Paragraph number and part-of-speech.\nParagraph(int head, int para, java.lang.String key, java.lang.String p) Constructor which sets the Head number, Paragraph number, keyword, and part-of-speech.\nMethod Summary void addSG(java.lang.String sg)\nAdds a Semicolon Group, repreented as a string, to the Paragraph.\nboolean equals(java.lang.Object anObject) Compares this paragraph to the specified object.\njava.lang.String format() Converts to a string representation, similar to the printed format, the Paragraph object.\njava.util.ArrayList getAllWordList() Returns all of the words and phrases in a paragraph.\nint getCRefCount() Returns the number of Cross-references in this Paragraph.\nint getHeadNum()"
    }, {
      "heading" : "Appendix B: The ELKB Java Documentation",
      "text" : "B - 16\nReturns the Head number of this Paragraph.\njava.lang.String getParaKey() Returns the keyword of this Paragraph.\nint getParaNum() Returns the number of this Paragraph.\njava.lang.String getPOS() Returns the part-of-speech of this Paragraph.\nint getSeeCount() Returns the number of See references in this Paragraph.\nca.site.elkb.SG getSG(int index) Returns the Semicolon Group at the specified position in the array of Semicolon Groups.\nca.site.elkb.SG getSG(java.lang.String word) Returns the the first Semicolon Group in this Paragraph which contains the given word.\nint getSGCount() Returns the number of Semicolon Groups in this Paragraph.\njava.util.ArrayList getSGList() Returns the array of Semicolon Groups of this Paragraph.\nint getWordCount() Returns the number of words in this Paragraph.\njava.lang.String parseParaKey(java.lang.String line) Extracts the keyword from a Semicolon Group represented as a string.\nvoid print() Prints the contents of this Paragraph to the standard output.\nvoid printAllSG() Prints all the contents of all Semicolon Groups, including references, without any special formatting.\nvoid printAllWords() Prints all of the words and phrases in the Paragraph on a separate line to the standard output.\nvoid setHeadNum(int num) Sets the Head number of this Paragraph.\nvoid setParaKey(java.lang.String key) Sets the keyword of this Paragraph.\nvoid setParaNum(int num) Sets the number of this Paragraph.\nvoid setPOS(java.lang.String p) Sets the part-of-speech of this Paragraph.\njava.lang.String toString() Converts to a string representation the Paragraph object."
    }, {
      "heading" : "Appendix B: The ELKB Java Documentation",
      "text" : "B - 17\nca.site.elkb Class Path\njava.lang.Object | +--ca.site.elkb.Path\nAll Implemented Interfaces:\njava.lang.Comparable\npublic class Path extends java.lang.Object implements java.lang.Comparable\nRepresents a path in Roget's Thesaurus between two words or phrases.\nConstructor Summary Path() Default constructor.\nPath(java.util.ArrayList path) Constructor that initialized this Path object with a Path.\nMethod Summary int compareTo(java.lang.Object other)\nCompares two paths.\njava.lang.String getKeyWord1() Returns the keyword of the the first word or phrase in this Path.\njava.lang.String getKeyWord2() Returns the keyword of the the second word or phrase in this Path.\njava.lang.String getPath() Returns the path between the first and second word or phrase.\njava.lang.String getPathInfo1() Returns the location in the ontology of the first word or phrase in this Path.\njava.lang.String getPathInfo2() Returns the location in the ontology of the second word or phrase in this Path.\njava.lang.String getPos1() Returns the part-of-speech of the the first word or phrase in this Path.\njava.lang.String getPos2() Returns the part-of-speech of the the second word or phrase in this Path.\njava.lang.String getWord1() Returns the first word or phrase in this Path.\njava.lang.String getWord2() Returns the second word or phrase in this Path."
    }, {
      "heading" : "Appendix B: The ELKB Java Documentation",
      "text" : "B - 18\nint length() Returns the number of elements in this Path.\nint size() Returns the length in this Path.\njava.lang.String toString() Converts to a string representation the Path object."
    }, {
      "heading" : "Appendix B: The ELKB Java Documentation",
      "text" : "B - 19\nca.site.elkb Class PathSet\njava.lang.Object | +--ca.site.elkb.PathSet\nAll Implemented Interfaces:\njava.lang.Comparable\npublic class PathSet extends java.lang.Object implements java.lang.Comparable\nA set that contains all of the paths between two words and phrases as well as the number of minimum length paths. This class is used to measure semantic distance.\nThe PathSet also contains the original strings before any morphological transformations of modifications of phrases These are contained in origWord1 and origWord2.\nConstructor Summary PathSet() Default constructor.\nPathSet(java.util.TreeSet pathSet) Constructor that initialized this PathSet object with a PathSet.\nMethod Summary int compareTo(java.lang.Object other)\nCompares two PathSets according to the length of the shortest path.\njava.util.TreeSet getAllPaths() Returns all Paths in this PathSet.\nint getMinLength() Returns the length of the shortest Path in this PathSet.\nint getMinPathCount() Returns the number of minimum length Paths in this PathSet.\njava.lang.String getOrigWord1() Returns the original form of the first word or phrase in this PathSet.\njava.lang.String getOrigWord2() Returns the original form of the second word or phrase in this PathSet.\njava.lang.String getPos1() Returns the part-of-speech of the first word or phrase in this PathSet.\njava.lang.String getPos2() Returns the part-of-speech of the second word or phrase in this PathSet."
    }, {
      "heading" : "Appendix B: The ELKB Java Documentation",
      "text" : "B - 20\njava.lang.String getWord1() Returns the first word or phrase after the morphological transformations are applied in this PathSet.\njava.lang.String getWord2() Returns the second word or phrase after the morphological transformations are applied in this PathSet.\njava.lang.String getWordPair() Converts to a string representation the PathSet object - used for debugging.\nvoid setOrigWord1(java.lang.String word) Sets the original form of the first word or phrase in this PathSet.\nvoid setOrigWord2(java.lang.String word) Sets the original form of the second word or phrase in this PathSet.\njava.lang.String toString() Converts to a string representation the PathSet object."
    }, {
      "heading" : "Appendix B: The ELKB Java Documentation",
      "text" : "B - 21\nca.site.elkb Class Reference\njava.lang.Object | +--ca.site.elkb.Reference\nAll Implemented Interfaces: java.io.Serializable Direct Known Subclasses:\nSemRel\npublic class Reference extends java.lang.Object implements java.io.Serializable\nRepresents a symbolic pointer to a location where a specific word or phrase can be found in Roget's Thesaurus. A reference is identified by a keyword, head number and part of speech sequence.\nAn example of a Reference is: obstetrics 167 n. This instance of a Reference is represented as:\n• Reference name: obstetrics • Head number: 167 • Part-of-speech: N.\nA Reference is always liked to an index entry, for example: stork. See Also:\nSerialized Form\nConstructor Summary Reference() Default constructor.\nReference(java.lang.String ref) Constructor that creates a Reference object by parsing a string.\nReference(java.lang.String name, int head, java.lang.String p) Constructor which sets the reference name, Head number and part-of-speech.\nReference(java.lang.String name, int head, java.lang.String p, java.lang.String entry) Constructor which sets the referebnce name, Head number, part-of-speech, and Index entry.\nMethod Summary int getHeadNum()\nReturns the Head number of this Reference.\njava.lang.String getIndexEntry() Returns the Index entry of this Reference."
    }, {
      "heading" : "Appendix B: The ELKB Java Documentation",
      "text" : "B - 22\njava.lang.String getPos() Returns the part-of-speech of this Reference.\njava.lang.String getRefName() Returns the name of this Reference.\nvoid print() Prints this Reference to the standard output.\nvoid setHeadNum(int head) Sets the Head number of this Reference.\nvoid setIndexEntry(java.lang.String entry) Sets the Index entry of this Reference.\nvoid setPos(java.lang.String p) Sets the part-of-speech of this Reference.\nvoid setRefName(java.lang.String name) Sets the name of this Reference.\njava.lang.String toString() Converts to a string representation the Reference object."
    }, {
      "heading" : "Appendix B: The ELKB Java Documentation",
      "text" : "B - 23\nca.site.elkb Class RogetClass\njava.lang.Object | +--ca.site.elkb.RogetClass\npublic class RogetClass extends java.lang.Object\nRepresents the topmost element in Roget's Thesaurus Tabular Synopsis of Categories. It is represented by its number, name, subclass name if it is a subclass of an original Roget Class, and range of Sections that it contains. For example, Class 4. Intellect: the exercise of the mind (Formation of ideas) is represented as:\n• Class number: 4 • Class number in string format: Class four • Class Name: Intellect: the exercise of the mind • First section: 16 • Last section: 22\nConstructor Summary RogetClass() Default constructor.\nRogetClass(int num, java.lang.String name) Constructor which sets the Class number and name.\nRogetClass(int num, java.lang.String name, int start, int end) Constructor which sets the Class number and name, as well as the first and last Section number.\nRogetClass(int num, java.lang.String strClassNum, java.lang.String strClassName) Constructor which sets the Class number, Class number in string format and Class name, while parsing the strings for the Class number and name.\nRogetClass(int num, java.lang.String snum, java.lang.String name, int start, int end) Constructor which sets the Class number, Class number in string format, Class name, as well as the first and last Section number.\nRogetClass(int num, java.lang.String snum, java.lang.String name, java.lang.String subClass) Constructor which sets the Class number, Class number in string format, Class and Sub-class name.\nRogetClass(int num, java.lang.String snum, java.lang.String name, java.lang.String subClass, int start, int end) Constructor which sets the Class number, Class number in string format, Class name, Sub-class name as well as the first and last Section number.\nMethod Summary"
    }, {
      "heading" : "Appendix B: The ELKB Java Documentation",
      "text" : "B - 24\nvoid addSection(ca.site.elkb.Section section) Adds a Section to this RogetClass.\njava.lang.String getClassName() Returns the name of this RogetClass.\nint getClassNum() Returns the number of this RogetClass.\nint getSectionEnd() Returns the number of the last section of this RogetClass.\njava.util.ArrayList getSectionList() Returns the array of Section objects in this RogetClass.\nint getSectionStart() Returns the number of the first section of this RogetClass.\njava.lang.String getStrClassNum() Returns the number of this RogetClass in string format.\njava.lang.String getSubClassName() Returns the Sub-class name of this RogetClass.\nint headCount() Returns the number of Heads of this RogetClass.\nvoid print() Prints the contents of this RogetClass to the standard output.\nint sectionCount() Returns the number of Sections of this RogetClass.\nvoid setClassName(java.lang.String name) Sets the name of this RogetClass.\nvoid setClassNum(int num) Sets the number of this RogetClass.\nvoid setSectionEnd(int end) Sets the number of the last section of this RogetClass.\nvoid setSectionStart(int start) Sets the number of the first section of this RogetClass.\nvoid setStrClassNum(java.lang.String snum) Sets the number of this RogetClass in string format.\nvoid setSubClassName(java.lang.String subClass) Sets the Sub-class name of this RogetClass.\njava.lang.String toString() Converts to a string representation the RogetClass object."
    }, {
      "heading" : "Appendix B: The ELKB Java Documentation",
      "text" : "B - 25\nca.site.elkb Class RogetELKB\njava.lang.Object | +--ca.site.elkb.RogetELKB\npublic class RogetELKB extends java.lang.Object\nMain class of the Roget's Thesaurus Electronic Lexical KnowledgeBase. It is made up of three major components:\n• the Index • the Tabular Synopsis of Categories • the Text\nRequired files: • elkbIndex.dat: The Index in binary file format. • rogetMap.rt: The Tabular Synopsis of Categories. • ./heads/head*: The 990 heads • AmBr.lst: The American to British spelling word list. • noun.exc, adj.exc, verb.exc, adv.exc: exception lists used for the morphological\ntransformations.\nThese files are found in the $HOME/roget_elkb directory.\nField Summary static java.lang.String CATEG\nLocation of the ELKB Tabular Synopsis of Categories.\nca.site.elkb.Category category The ELKB Tabular Synopisis of Categories.\nstatic java.lang.String ELKB_PATH Location of the ELKB data directory.\nstatic java.lang.String HEADS Location of the Heads.\nca.site.elkb.Index index The ELKB Index.\nstatic java.lang.String INDEX Location of the ELKB Index.\nca.site.elkb.RogetText text The ELKB Text.\nstatic java.lang.String USER_HOME Location of user's Home directory."
    }, {
      "heading" : "Appendix B: The ELKB Java Documentation",
      "text" : "B - 26\nConstructor Summary RogetELKB() Default constructor.\nMethod Summary java.util.TreeSet getAllPaths(java.lang.String strWord1,\njava.lang.String strWord2) Returns all the paths between two words or phrases.\njava.util.TreeSet getAllPaths(java.lang.String strWord1, java.lang.String strWord2, java.lang.String POS) Returns all the paths between two words or phrases of a given part-of-speech.\nstatic void main(java.lang.String[] args) Allows the ELKB to be used via the command line.\nca.site.elkb.Path path(java.lang.String strWord1, java.lang.String strRef1, java.lang.String strWord2, java.lang.String strRef2) Calculates the path between two senses of words or phrases.\njava.lang.String t1Relation(java.lang.String strWord1, int iHeadNum1, java.lang.String sRefName1, java.lang.String sPos1, java.lang.String strWord2) Determines the thesaural relation that exists between a specific sense of a words or phrases and another word or phrase.\njava.lang.String t1Relation(java.lang.String strWord1, java.lang.String strWord2) Determines the thesaural relation that exists between two words or phrases."
    }, {
      "heading" : "Appendix B: The ELKB Java Documentation",
      "text" : "B - 27\nca.site.elkb Class RogetText\njava.lang.Object | +--ca.site.elkb.RogetText\nAll Implemented Interfaces:\njava.io.Serializable\npublic class RogetText extends java.lang.Object implements java.io.Serializable\nRepresents the Text of Roget's Thesaurus. The following information is maintained for the Text:\n• number of Heads • number of Paragraphs • number of words and phrases • number of Semicolon Groups • number of Cross-references • number of See references\nThis information is also kept for all nouns, adjectives, verbs, adverbs and interjections. See Also:\nSerialized Form\nConstructor Summary RogetText() Default constructor.\nRogetText(int capacity) Constructor which specifies the number of Heads contained in this RogetText.\nRogetText(int capacity, java.lang.String fileName) Constructor that builds the RogetText object by specifying the number of Heads and using the information contained files which end with .txt.\nRogetText(int capacity, java.lang.String fileName, java.lang.String extension) Constructor that builds the RogetText object by specifying the number of Heads and using the information contained files which end with the given extension.\nRogetText(java.lang.String path) Constructor which specifies the directory in which the Heads are found."
    }, {
      "heading" : "Appendix B: The ELKB Java Documentation",
      "text" : "B - 28\nMethod Summary void addHead(ca.site.elkb.Head headObj)\nAdds a Head object to this RogetText.\nvoid addHead(java.lang.String fileName) Adds a Head which is contained in the specified file to this RogetText.\nint getAdjCount() Returns the number of adjectives in this RogetText.\nint getAdjCRefCount() Returns the number of adjective Cross-references in this RogetText.\nint getAdjParaCount() Returns the number of adjective Paragraphs in this RogetText.\nint getAdjSeeCount() Returns the number of adjective See referencs in this RogetText.\nint getAdjSGCount() Returns the number of ajective Semicolon Groups in this RogetText.\nint getAdvCount() Returns the number of adverbs in this RogetText.\nint getAdvCRefCount() Returns the number of adverb Cross-references in this RogetText.\nint getAdvParaCount() Returns the number of adverb Paragraphs in this RogetText.\nint getAdvSeeCount() Returns the number of adverb See referencs in this RogetText.\nint getAdvSGCount() Returns the number of adverb Semicolon Groups in this RogetText.\nint getCRefCount() Returns the number of Cross-references in this RogetText.\nca.site.elkb.Head getHead(int headNum) Returns the Head with the specified number.\nint getHeadCount() Returns the number of Heads in this RogetText.\nint getIntCount() Returns the number of interjections in this RogetText.\nint getIntCRefCount() Returns the number of interjection Cross-references in this RogetText.\nint getIntParaCount() Returns the number of interjection Paragraphs in this RogetText.\nint getIntSeeCount() Returns the number of interjection See referencs in this RogetText.\nint getIntSGCount() Returns the number of interjection Semicolon Groups in this RogetText.\nint getNCount()"
    }, {
      "heading" : "Appendix B: The ELKB Java Documentation",
      "text" : "B - 29\nReturns the number of nouns in this RogetText.\nint getNCRefCount() Returns the number of noun Cross-references in this RogetText.\nint getNParaCount() Returns the number of noun Paragraphs in this RogetText.\nint getNSeeCount() Returns the number of noun See referencs in this RogetText.\nint getNSGCount() Returns the number of noun Semicolon Groups in this RogetText.\nint getParaCount() Returns the number of Paragraphs in this RogetText.\nint getSeeCount() Returns the number of See referencs in this RogetText.\nint getSGCount() Returns the number of Semicolon Groups in this RogetText.\nint getVbCount() Returns the number of verbs in this RogetText.\nint getVbCRefCount() Returns the number of verb Cross-references in this RogetText.\nint getVbParaCount() Returns the number of verb Paragraphs in this RogetText.\nint getVbSeeCount() Returns the number of verb See referencs in this RogetText.\nint getVbSGCount() Returns the number of verb Semicolon Groups in this RogetText.\nint getWordCount() Returns the number of words and phrases in this RogetText.\nvoid printHead(int headNum) Prints the contents of a Head specified by its number to the standard output.\njava.lang.String toString() Converts to a string representation the RogetText object."
    }, {
      "heading" : "Appendix B: The ELKB Java Documentation",
      "text" : "B - 30\nca.site.elkb Class Section\njava.lang.Object | +--ca.site.elkb.Section\npublic class Section extends java.lang.Object\nRepresents a Roget's Thesaurus Section. A Section is defined by the following attributes:\n• Section number • Section number in string format • Section name • number of the first Head • number of the last Head • array of Heads\nA Section can contain Head or HeadInfo objects, depending on the use.\nConstructor Summary Section() Default constructor.\nSection(int number, java.lang.String name) Constructor which sets the Section number and name.\nSection(int number, java.lang.String name, int start, int end) Constructor which sets the Section number and name, as well as the number of the first and last Head.\nSection(int number, java.lang.String strNum, java.lang.String strName) Constructor which sets the Section number, name, and Section number in string format and Class name, while parsing the strings for the Section number and name.\nMethod Summary void addHeadInfo(ca.site.elkb.HeadInfo head)\nAdds a HeadInfo object to this Section.\nint getHeadEnd() Returns the number of the last Head of this Section.\njava.util.ArrayList getHeadInfoList() Returns the array of HeadInfo objects of this Section.\nint getHeadStart() Returns the number of the first Head of this Section.\njava.lang.String getSectionName() Returns the name of this Section."
    }, {
      "heading" : "Appendix B: The ELKB Java Documentation",
      "text" : "B - 31\nint getSectionNum() Returns the number of this Section.\njava.lang.String getStrSectionNum() Returns the number of this Section in string format.\nint headCount() Returns the number of Heads in this Section.\nvoid print() Prints the content of this Section to the standard output.\nvoid printHeadInfo() Prints the information regarding the Heads contained in this Section to the standard output.\nvoid setHeadEnd(int end) Sets the number of the last Head of this Section.\nvoid setHeadStart(int start) Sets the number of the first Head of this Section.\nvoid setSectionName(java.lang.String name) Sets the number of this Section in string format.\nvoid setSectionNum(int num) Sets the number of this Section.\nvoid setStrSectionNum(java.lang.String snum) Sets the number of this Section in string format.\njava.lang.String toString() Converts to a string representation the Section object."
    }, {
      "heading" : "Appendix B: The ELKB Java Documentation",
      "text" : "B - 32\nca.site.elkb Class SemRel java.lang.Object | +--ca.site.elkb.Reference | +--ca.site.elkb.SemRel\nAll Implemented Interfaces:\njava.io.Serializable\npublic class SemRel extends Reference\nRepresents a Roget's Thesaurus relation between a word or phrase. This can be a Cross-reference or a See reference. For example:\n• See drug taking • 646 perfect\nRelation types currently used by the ELKB are cref and see. See Also:\nSerialized Form\nConstructor Summary SemRel() Default constructor.\nSemRel(java.lang.String t, int headNum, java.lang.String refName) Constructor which sets the relation type, Head number and Reference name.\nMethod Summary java.lang.String getType()\nReturns the relation type.\nvoid print() Prints this relation to the standard output.\nvoid setType(java.lang.String t) Sets the relation type.\njava.lang.String toString() Converts to a string representation the SemRel object.\nMethods inherited from class ca.site.elkb.Reference\ngetHeadNum, getIndexEntry, getPos, getRefName, setHeadNum, setIndexEntry,"
    }, {
      "heading" : "Appendix B: The ELKB Java Documentation",
      "text" : "B - 33\nsetPos, setRefName"
    }, {
      "heading" : "Appendix B: The ELKB Java Documentation",
      "text" : "B - 34\nca.site.elkb Class SG\njava.lang.Object | +--ca.site.elkb.SG\npublic class SG extends java.lang.Object\nRepresents a Roget's Thesaurus Semicolon Group. For example:\n• zeal, ardour, ernestness, seriousness;\nA Semicolon Group is defined by the following attributes:\n• Head number • Paragraph number • Paragraph keyword • Part-of-speech • Semicolon Group number • number of Cross-references • number of See references • number of See references • list of word and phrases • list of special tags for the words and phrases • list of references\nConstructor Summary SG() Default constructor.\nSG(int numSG, int numP, int numH, java.lang.String text, java.lang.String p) Constructor that sets the Semicolon Group number, Paragraph number, Head number, the words and phases of the Semicolon Group and the part-of-speech.\nSG(int num, java.lang.String text) Constructor that sets the Semicolon Group number and the words and phases that it contains.\nMethod Summary void addSemRel(ca.site.elkb.SemRel rel)\nAdds a relation to this Semicolon Group\nvoid addWord(java.lang.String word) Adds a word or phrase to this Semicolon Group.\nvoid addWord(java.lang.String word, java.lang.String tag) Adds a word or phrase and its style tag to this Semicolon Group."
    }, {
      "heading" : "Appendix B: The ELKB Java Documentation",
      "text" : "B - 35\njava.lang.String format() Returns this Semicolon Group formatted in a string, including references, style tags and punctuation.\njava.util.ArrayList getAllWordList() Returns the list of words and phrases, including the references, contained in this Semicolon Group.\nint getCRefCount() Returns the number Cross-references in this Semicolon Group.\njava.lang.String getGroup() Returns a string containing all of the words and phrases in the Semicolon Group minus the references.\nint getHeadNum() Returns the Head number of this Semicolon Group.\njava.lang.String getOffset() Returns a symbolic adress of this Semicolon Group.\njava.util.ArrayList getOffsetList() Returns a list of Semicolon Groups with their symbolic adresses.\njava.lang.String getParaKey() Returns the Paragraph keyword of this Semicolon Group.\nint getParaNum() Returns the Paragraph number of this Semicolon Group.\njava.lang.String getPOS() Returns the part-of-speech of this Semicolon Group.\njava.lang.String getReference() Returns a string containing only the references of this Semicolon Group.\nint getSeeCount() Returns the number See refereces in this Semicolon Group.\njava.util.ArrayList getSemRelList() Returns the list of relations of this Semicolon Group.\nint getSGNum() Returns the number of this Semicolon Group.\njava.util.ArrayList getStyleTagList() Returns the list of style tags of this Semicolon Group.\nint getWordCount() Returns the number of words and phrases in this Semicolon Group.\njava.util.ArrayList getWordList() Returns the list of words and phrases, minus the references, contained in this Semicolon Group.\nvoid print() Prints this Semicolon Group to the standard output.\nvoid setHeadNum(int num) Sets the Head number of this Semicolon Group.\nvoid setParaKey(java.lang.String key)"
    }, {
      "heading" : "Appendix B: The ELKB Java Documentation",
      "text" : "B - 36\nSets the Paragraph keyword of this Semicolon Group.\nvoid setParaNum(int num) Sets the Paragraph number of this Semicolon Group.\nvoid setPOS(java.lang.String p) Sets the part-of-speech of this Semicolon Group.\nvoid setSGNum(int num) Sets the number of this Semicolon Group.\nvoid setText(java.lang.String text) Sets the words and phrases used in this Semicolon Group.\njava.lang.String toString() Converts to a string representation the SG object."
    }, {
      "heading" : "Appendix B: The ELKB Java Documentation",
      "text" : "B - 37\nca.site.elkb Class SubSection\njava.lang.Object | +--ca.site.elkb.SubSection\npublic class SubSection extends java.lang.Object\nRepresents a Roget's Thesaurus Sub-section. A Sub-section may or may not exist. Here is an example:\n• Class one: Abstract Relations • Section one: Existence • Sub-section title: Abstract • Head group:1 Existence - 2 Nonexistence\nSub-sections may contain several Head groups.\nConstructor Summary SubSection() Default constructor.\nSubSection(int start) Constructor which sets the number of the first Head.\nSubSection(int start, java.lang.String sInfo) Constructor which sets the number of the first Head and the name of the Section supplied as a string to be parsed.\nSubSection(java.lang.String sInfo) Constructor which sets the name of the Section by parsing a string.\nMethod Summary void addGroup(ca.site.elkb.Group group)\nAdds a Head Group to this Sub-section.\nint getGroupCount() Returns the number of Head groups in this Sub-section.\njava.util.ArrayList getGroupList() Returns the list of Head groups in this Sub-section.\nint getHeadCount() Returns the number of Heads in this Sub-section.\nint getHeadStart() Returns the number of the first Head in this Sub-section.\nvoid print() Displays the content of a Sub-section in a similar way to Roget's Thesaurus"
    }, {
      "heading" : "Appendix B: The ELKB Java Documentation",
      "text" : "B - 38\nTabular Synopisis of Categories to the standard output.\nvoid setHeadStart(int start) Sets the number of the first Head in this Sub-section.\njava.lang.String toString() Converts to a string representation the SubSection object."
    }, {
      "heading" : "Appendix B: The ELKB Java Documentation",
      "text" : "B - 39\nca.site.elkb Class Variant\njava.lang.Object | +--ca.site.elkb.Variant All Implemented Interfaces:\njava.io.Serializable\npublic class Variant extends java.lang.Object implements java.io.Serializable\nAllows to obtain a variant of an English spelling. A British spelling variant can be obtained form an American spelling and vice-versa.\nThe default American and British word list is AmBr.lst contained in the $HOME/roget_elkb directory. It is loaded by the default constructor.\nSee Also: Serialized Form\nField Summary static java.lang.String AMBR_FILE\nLocation of the default American and British spelling word list.\nstatic java.lang.String ELKB_PATH Location of the ELKB data directory.\nstatic java.lang.String USER_HOME Location of user's Home directory.\nConstructor Summary Variant() Default constructor.\nVariant(java.lang.String filename) Constructor that builds the Variant object using the information contained in the specified file.\nMethod Summary java.lang.String amToBr(java.lang.String american)\nReturns the British spelling of a word, or null if the word cannot be found.\njava.lang.String brToAm(java.lang.String british) Returns the American spelling of a word, or null if the word cannot be found."
    }, {
      "heading" : "Appendix C: The ELKB Graphical and Command Line Interfaces",
      "text" : "C - 1\nAppendix C: The ELKB Graphical and Command Line Interfaces\nThis appendix presents the graphical and command line interfaces to the ELKB along with their related documentation."
    }, {
      "heading" : "1 The Graphical User Interface",
      "text" : "The Graphical User Interface (GUI) is a configurable mechanism for querying the ELKB. The GUI is designed to be as versatile, intuitive and informative as the printed version of the Thesaurus. To use it, a user supplies a word or phrase that is looked up ELKB’s index. The interface returns a list of references if the given word or phrase is found. The user must select one to display the paragraph in which the word or phrase is contained. An example using the word please is shown Figure C1.\n.\nFigure C1: Screenshot of the GUI"
    }, {
      "heading" : "Appendix C: The ELKB Graphical and Command Line Interfaces",
      "text" : "C - 2\nFigure C1 shows the following parts of the GUI:\n• Index Word: the word or phrase to be looked up. By hitting Enter or the clicking on the Search Button in the Search Results box. A history of queried words and phrases is maintained by the GUI.\n• Search Button: searches the index for the word or phrase found in the Index Word box.\n• Search Results: the result of the search is displayed in this box, also labeled as Index Listings in Figure C1. The user clicks on the desired reference to display the matching paragraph.\n• Paragraph: a paragraph is displayed in this text box when the user clicks on a result in the Search Results, or when the user clicks on a head in the Taxonomy Tree. The GUI displays the semicolon group containing the Index Word in bold, and references to other heads in the Thesaurus as blue underlined text. A word or phrase in the Paragraph display can be selected by holding down the left mouse button while moving the mouse. If a word or phrase is selected, a menu appears with an option to perform a query on the selected text. If a user right clicks on a reference a popup menu appears with an option to follow the link. If the user clicks on the follow link menu item, the referenced paragraph appears in the Paragraph window. When text is selected, it can be copied to the system clipboard by pressing CTRL-C.\n• Taxonomy Tree: An alternative way to use the Thesaurus is to browse the words and phrases using the classification system. A user can expand a node in the tree by double clicking on it, or by clicking on the “+” beside the node. If a user double clicks on a head, the first paragraph of the head appears in the Paragraph window. A node collapses when the “-” beside it is clicked, hiding any sub nodes.\n• Side Bar: This bar can be moved left or right to modify the size of the Taxonomy Tree and the other half of the GUI.\n• Previous Paragraph: this button displays the paragraph that precedes the one currently shown in the Paragraph window as ordered in Roget’s Thesaurus.\n• Next Paragraph: this button displays the paragraph that follows the one currently shown in the Paragraph window as ordered in Roget’s Thesaurus.\n• Part of Speech: each paragraph belongs to a part of speech. The part of speech of the currently displayed paragraph appears in a drop down list box. A user can display all of the different parts of speech that exist in the current head by clicking the down arrow of the list box. Clicking on one of the choices displays the first paragraph of the selection in the Paragraph window.\n• Head: The head name and number of the currently displayed paragraph is shown here."
    }, {
      "heading" : "Appendix C: The ELKB Graphical and Command Line Interfaces",
      "text" : "C - 3"
    }, {
      "heading" : "2 The Command Line Interface",
      "text" : "The command line interface allows looking up a word or phrase, or calculating the distance between two words or phrases. Figure C2 shows the possible options of the interface, Figures C3 and C4 the steps for looking up the word please, and Figure C5 the distance between words God and Yahweh.\nFigure C2: Screenshot of the command line interface\nFigure C3: The references of the word please"
    }, {
      "heading" : "Appendix C: The ELKB Graphical and Command Line Interfaces",
      "text" : "C - 4\nFigure C4: The paragraph of the reference 7. please 826 VB.\nFigure C5: The distance between words God and Yahweh"
    }, {
      "heading" : "Appendix D: The Programs Developed for the Thesis",
      "text" : "D - 1"
    }, {
      "heading" : "Appendix D: The Programs Developed for the Thesis",
      "text" : "This appendix lists the programs developed for the thesis. The programs can be classified in three categories: preparation of the lexical material for the ELKB, testing and use of the ELKB and experiments. The programs implemented for this thesis are:\n• Preparation of the lexical material:\no format: Perl program that converts the Pearson source files into a format recognizable by the ELKB.\no getHeads.pl: Perl program that takes Pearson Text file converted by the format program and separates it into 990 files, one for each head.\no ELKBWords: Java Program that lists all of the words and phrases found in the ELKB as well as their paragraph keyword, head number and part-of-speech.\no createIndex.pl: Perl program that takes the output of ELKBWords and converts it into an Index file to be used by the ELKB.\no index2.pl: Perl program that removes errors from the output of createIndex.pl.\no MakeBinIndex: Java program that takes the output of index2.pl and transforms the Index file in a binary format to be used by the ELKB.\n• Testing of the ELKB:\no Driver: Java program that tests the various methods of the ELKB.\no TestELKB: Java program that implements the command-line interface of the ELKB.\no CERoget: Java program that implements the graphical user interface of the ELKB.\n• Experiments:\no Similarity: Java program used for the experiments on semantic similarity.\no similarity.pl: Perl program used for the experiments on semantic similarity.\no LexicalChain: Java program that builds lexical chains using the ELKB.\nIt is enough to run most of these programs to know how they should be used. The preparation of the lexical material requires special attention. I have supplied here the documentation to perform this task."
    }, {
      "heading" : "Appendix D: The Programs Developed for the Thesis",
      "text" : "D - 2\nThe Preparation of the Lexical Material for use by the ELKB.\nThe format, getHeads.pl, ELKBWords, createIndex.pl, index2.pl and MakeBinIndex programs must be used in the following manner to convert the Pearson source files. This procedure will create the Text and Index files to be used by the ELKB.\n1. Concatenate all of the rogetXXX.txt files supplies by Pearson Education. 2. Run the format script. Usage: format -t input_file output_file. The format\nprogram can be used to convert the format of the Pearson Index files by using the –i flag.\n3. Run the getHeads.pl script on the resulting file. This creates a heads directory that contains the 990 Heads.\n4. Copy the heads directory to the user’s home directory. 5. Run ELKBWords and re-direct the output to a file 6. Sort the file using sort. 7. Remove duplicate entries using uniq. 8. Remove errors by hand, these include:\n• the first few hundred lines • lines specified by Error: • lines containing At line:\n9. Run the createIndex.pl program on the resulting file. 10. Run the index2.pl script on the output of createIndex.pl"
    }, {
      "heading" : "Appendix E: Converting the Pearson Codes into HTML-like Tags",
      "text" : "E - 1"
    }, {
      "heading" : "Appendix E: Converting the Pearson Codes into HTML-like Tags",
      "text" : "The text files supplied by Pearson’s Education are not easy to read, and they use their own\nspecific codes. Even though these codes are explained in their documentation, it is preferable to\nuse easily understood HTML-like tags. Here are the first few lines of the Text file:\n#t#6Class one #L#6Abstract Relations #U#5Section one #V#1Existence #H#3[001] 1 Existence #S#1N. #H#3[001] 1 Existence #S#1N. #D#10 / /............../../..../....../././../.............../ #T#6existence, #5being, entity; absolute being, the absolute 965#6divineness#5; aseity, self-existence; monad, a being, an entity, ens, essence, #1$:#5quiddity; Platonic idea, universal; subsistence 360 #6life#5; survival, eternity 115 #6perpetuity#5; preexistence 119 #6priority#5; this life 121 #6present time#5; existence in space, prevalence 189 #6presence#5; entelechy, realization, becoming, evolution 147 #6conversion#5; creation 164 #6production#5; potentiality 469 #6possibility#5; ontology, metaphysics; realism, materialism, idealism, existentialism 449 #6philosophy#5.\nThe format Perl script performs the following sixteen steps to convert the Text file:\n[1] Replace the #t#6 codes indicating a Class number and italics by <classNumber>#<i>.\nA closing </i>#</classNumber> tag is added and the Class number is separated by #.\nexample: #t#6Class one → <classNumber>#<i>#Class one #</i>#</classNumber>\n[2] Replace the #L#6 codes indicating a Class title and italics by <classTitle>#<i>. A\nclosing </i>#</classTitle> tag is added and the Class title is separated by #.\nexample: #L#6Abstract Relations → <classTitle>#<i>#Abstract Relations #</i>#</classTitle>\n[3] Replace the #I#1 codes indicating a Sub-class title and small bold by\n<subClassTitle>#<size=-1>#<b>. A closing tag is added.\nexample: #I#14.1 Formation of ideas → <subClassTitle>#<size=-1>#<b> #4.1 Formation of ideas #\n</b>#</size>#</subClassTile>"
    }, {
      "heading" : "Appendix E: Converting the Pearson Codes into HTML-like Tags",
      "text" : "E - 2\n[4] Replace the #U#5 codes indicating a Section number and roman by <sectionNumber>.\nA closing </sectionNumber> tag is added.\nexample: #U#5Section one → <sectionNumber>#Section one #</sectionNumber>\n[5] Replace the #V#1 codes indicating a Section title and small bold by <sectionTitle>#\n<size=-1>. Closing </size>#</sectionTitle> tags are added.\nexample: #V#1Existence → <sectionTitle>#<size=-1>#<b>#Existence #</b>#</size>#</sectionTitle>\n[6] Replace the #H#3 codes indicating a Headword and large bold by <headword>#<b>. A\nclosing </b>#</headword> tag is added and every field is separated by a #.\nexample: #H#3[001] 1 Existence → <headword>#<b>#[001] #1# Existence #</b>#</headword>\n[7] Replace the #S#1 codes indicating the part of speech and small bold symbol by\n<pos>#<size=-1>#<b>. A closing </b>#</size>#</pos> tag is added after the part of\nspeech label.\nexample: #S#1N. → <pos>#<size=-1>#<b>#N.#</b>#</size>#</pos>\n[8] Replace the #D#10 /\n/............../../..../....../././../.............../ code by <br> that\nindicates a blank line.\nexample: #D#10 / /............../../..../....../././../.............../\n→ <br>\n[9] Replace the #T Paragraph code by <paragraph> and a new line.\nexample: #T → <paragraph>\n[10] Place every Semicolon Group on an individual line, label it with the <sg> </sg> tags\nand replace the #6 (italic) and #5 (roman) codes by <i> and </i>. Separate every word,\nphrase and final punctuation symbol with a comma.\nexample: #6existence, #5being, entity; → <sg><i>existence, </i>being, entity,;</sg>"
    }, {
      "heading" : "Appendix E: Converting the Pearson Codes into HTML-like Tags",
      "text" : "E - 3\n[11] Remove the #1$:#5 codes which appear often in the files but do not mean anything.\nexample: #1$:#5quiddity → quiddity\n[12] Label the Cross-references, <number>#6<string>#5, with the <cref> </cref> tags.\nexample: 360 #6life#5 → <cref>360 <i>life</i></cref>\n[13] Label the See-references, (#1see #6<string>#5), with the <see> </see> tags.\nexample: (#1see #6turmoil#5) → <see><i>turmoil</i></see>\n[14] Finish the paragraph with a </paragraph> tag.\n[15] Expand abbreviations when required.\n[16] Additional tags (derog), (e), (tdmk), and (vulg), which respectively indicate words\nor phrases that are derogatory, of French origin that require a final “e” if applied to a woman,\na registered trademark and vulgar, are replaced by (<derog>), (<e>), (<tdmk>), and\n(<vulg>).\nThe sample of the Pearson file after the substitutions looks like this:\n<classNumber>#1#</classNumber> <sectionNumber>#1#</sectionNumber> <headword>#<b>#[001] #1# Existence #</b>#</headword> <pos>#<size=-1>#<b>#N.#</b>#</size>#</pos> <br> <paragraph> <sg><i>existence, </i>being, entity,;</sg> <sg>absolute being, the absolute, <cref>965<i>divineness</i></cref>,;</sg> <sg>aseity, self-existence,;</sg> <sg>monad, a being, an entity, ens, essence, quiddity,;</sg> <sg>Platonic idea, universal,;</sg> <sg>subsistence, <cref>360 <i>life</i></cref>,;</sg> <sg>survival, eternity, <cref>115 <i>perpetuity</i></cref>,;</sg> <sg>preexistence, <cref>119 <i>priority</i></cref>,;</sg> <sg>this life, <cref>121 <i>present time</i></cref>,;</sg> <sg>existence in space, prevalence, <cref>189 <i>presence</i></cref>,;</sg> <sg>entelechy, realization, becoming, evolution, <cref> 147<i>conversion</i> </cref>,;</sg> <sg>creation, <cref>164 <i>production</i></cref>,;</sg> <sg>potentiality, <cref>469 <i>possibility</i></cref>,;</sg> <sg>ontology, metaphysics,;</sg> <sg>realism, materialism, idealism, existentialism, <cref>449 <i>philosophy </i></cref>,.</sg> </paragraph>"
    }, {
      "heading" : "Appendix F: Some Errors in the Pearson Source Files",
      "text" : "F - 1"
    }, {
      "heading" : "Appendix F: Some Errors in the Pearson Source Files",
      "text" : "This appendix lists errors that I have identified in the Pearson source files. 179 phrases where a space is missing and 26 words that are cut by a space have been found and corrected.\nFile Name Original string Corrected string roget016.txt andneedles and needles roget036.txt andNicolette and Nicolette roget028.txt anotherplace another place roget029.txt antimissilemissile antimissile missile roget029.txt antitankobstacles antitank obstacles roget029.txt armstraffic arms traffic roget023.txt artnouveau art nouveau roget013.txt aslead a slead roget013.txt bambooshoots bamboo shoots roget009.txt becomehorizontal become horizontal roget005.txt beunderpopulated be underpopulated roget021.txt blunderhead blunder head roget028.txt blunderhead blunder head roget026.txt bookcollection book collection roget020.txt bookwoman book woman roget021.txt byallusion by allusion roget011.txt caninetooth canine tooth roget017.txt choirleader choir leader roget004.txt cognizanceof cognizance of roget021.txt commonplacebook commonplace book roget016.txt deathby death by roget026.txt dechoix de choix roget022.txt deskwork desk work roget011.txt dogpaddle dog paddle roget011.txt dogsleigh dog sleigh roget035.txt emptystomach empty stomach roget033.txt excitedfeeling excited feeling roget035.txt fearof fear of roget024.txt fictionalbiography fictional biography roget020.txt fineadjustment fine adjustment roget020.txt flatteringhope flattering hope roget015.txt foodplant food plant roget021.txt goldendream golden dream roget016.txt gothrough go through roget036.txt hardbitten hard bitten roget014.txt hardwater hard water roget020.txt havea have a"
    }, {
      "heading" : "Appendix F: Some Errors in the Pearson Source Files",
      "text" : "F - 2\nFile Name Original string Corrected string roget040.txt havemercy have mercy roget039.txt headfor head for roget040.txt hedgepriest hedge priest roget013.txt icecream ice cream roget011.txt inkdrop ink drop roget024.txt inkslinger ink slinger roget030.txt inlast in last roget020.txt interlocutorydecree interlocutory decree roget035.txt inwishful in wishful roget029.txt inwrestling in wrestling roget034.txt ladykiller lady killer roget039.txt lawcourts law courts roget036.txt lawhusband law husband roget027.txt leadpollution lead pollution roget013.txt leapfrogger leap frogger roget034.txt legpull leg pull roget024.txt lightreading light reading roget002.txt lorryload lorry load roget002.txt lorryload lorry load roget011.txt lorryload lorry load roget021.txt lossofreason loss of reason roget038.txt lovepot love pot roget020.txt makeabsolute make absolute roget026.txt mechanicaladvantage mechanical advantage roget006.txt mellowfruitfulness mellow fruitfulness roget021.txt mentaldeficiency mental deficiency roget007.txt mentalweakness mental weakness roget036.txt mixedmarriage mixed marriage roget011.txt mouseproof mouse proof roget013.txt naturalfunctions natural functions roget029.txt needlegun needle gun roget037.txt ofGod of God roget037.txt ofhonour of honour roget028.txt ofParliament of Parliament roget001.txt ofreference of reference roget016.txt ofsmell of smell roget021.txt ofspeaking of speaking roget007.txt ofstrength of strength roget026.txt ofsubstance of substance roget016.txt oftorture of torture roget021.txt ofunsoundmind of unsound mind roget040.txt oncedelivered once delivered roget035.txt one'sbreath one's breath"
    }, {
      "heading" : "Appendix F: Some Errors in the Pearson Source Files",
      "text" : "F - 3\nFile Name Original string Corrected string roget025.txt one'slot one's lot roget033.txt one'smind one's mind roget016.txt one'snose one's nose roget030.txt one'spockets one's pockets roget032.txt onefor one for roget024.txt onehander one hander roget020.txt onesidedness one sidedness roget021.txt onesyllable one syllable roget023.txt onesyllable one syllable roget037.txt onlyoneself only oneself roget016.txt onthe on the roget008.txt outof out of roget012.txt paddlewheel paddle wheel roget016.txt painfulaftermath painful aftermath roget038.txt pamperedappetite pampered appetite roget016.txt pipeof pipe of roget016.txt pipetobacco pipe tobacco roget008.txt pistolshot pistol shot roget038.txt profitmaking profit making roget008.txt puddingbasin pudding basin roget021.txt puddinghead pudding head roget035.txt racialprejudice racial prejudice roget010.txt rainhat rain hat roget032.txt remainderman remainder man roget009.txt rubbingshoulders rubbing shoulders roget024.txt runthrough run through roget026.txt safeconduct safe conduct roget021.txt setbefore set before roget035.txt setdown set down roget020.txt sexprejudice sex prejudice roget028.txt shopfloor shop floor roget013.txt shortcrust short crust roget022.txt situationcomedy situation comedy roget010.txt skiboots ski boots roget036.txt slangwhang slang whang roget005.txt soonafter soon after roget029.txt staffwork staff work roget011.txt stationwaggon station wagon roget011.txt swallowhole swallow hole roget011.txt swordpoint sword point roget029.txt swordstick sword stick roget004.txt systemsanalyst systems analyst roget026.txt tablemat table mat"
    }, {
      "heading" : "Appendix F: Some Errors in the Pearson Source Files",
      "text" : "F - 4\nFile Name Original string Corrected string roget036.txt takeoffence take offence roget011.txt talkdown talk down roget024.txt talknineteen talk nineteen roget018.txt thatyou that you roget016.txt theagony the agony roget030.txt theascendant the ascendant roget021.txt thebend the bend roget040.txt thechurch the church roget012.txt theclappers the clappers roget022.txt theeducationally the educationally roget018.txt theeyes the eyes roget015.txt thefallen the fallen roget019.txt thehouse the house roget039.txt thelaw the law roget015.txt theLongKnives the Long Knives roget020.txt thematter the matter roget030.txt themoney the money roget029.txt theoffensive the offensive roget039.txt therap the rap roget030.txt therose the rose roget013.txt thescales the scales roget025.txt thescent the scent roget013.txt theshakes the shakes roget007.txt thespout the spout roget022.txt thetrumpets the trumpets roget038.txt theup the up roget013.txt theweight the weight roget016.txt ticklingsensation tickling sensation roget002.txt tieup tie up roget018.txt tiger'seye tiger's eye roget003.txt timeslip time slip roget005.txt timeslip time slip roget005.txt timewarp time warp roget016.txt tobaccochewer tobacco chewer roget017.txt tomtom tom tom roget007.txt topcondition top condition roget006.txt tothe to the roget007.txt toughguy tough guy roget008.txt townsperson towns person roget013.txt trencherwoman trencher woman roget013.txt turnthe turn the roget034.txt twicetold twice told roget024.txt typefoundry type foundry"
    }, {
      "heading" : "Appendix F: Some Errors in the Pearson Source Files",
      "text" : "F - 5\nFile Name Original string Corrected string roget022.txt tyremark tyre mark roget023.txt undDrang und Drang roget034.txt underone's under one's roget007.txt vicelike vice like roget020.txt voxpopuli vox populi roget015.txt wastepipe waste pipe roget026.txt wastepipe waste pipe roget040.txt watchnight watch night roget007.txt weakas weak as roget029.txt wholehogging whole hogging roget031.txt withdrawpermission withdraw permission roget009.txt withinside with inside roget014.txt withrain with rain\nTable F1: 179 phrases where a space is missing in the Pearson source files"
    }, {
      "heading" : "Appendix F: Some Errors in the Pearson Source Files",
      "text" : "F - 6\nFile Name Original string Corrected string roget030.txt decentralizatio n decentralization roget007.txt destruct ion destruct ion roget024.txt editio n edition roget009.txt extraterritoriali ty extraterritoriality roget006.txt fatherf. father figure. roget013.txt featherwei ght featherweight roget036.txt glorificatio n glorification roget019.txt impracticabilit y impracticability roget027.txt misappropriatio n misappropriation roget003.txt overfulfil ment overfulfilment roget026.txt overfulfil ment overfulfilment roget013.txt geog raphical geographical roget040.txt a rchdeacon archdeacon roget010.txt di sequilibrium disequilibrium roget027.txt ince ssant ince ssant roget022.txt suggestio n suggestion roget022.txt suggestio n suggestion roget007.txt superfecundatio n superfecundation roget022.txt suppressio n suppression roget022.txt suppressio n suppression roget021.txt technica l technical roget024.txt televisi on television roget021.txt telltal e telltale roget038.txt unconscientiousn ess unconscientiousness roget019.txt unpredictabilit y unpredictabilit y roget029.txt withdr awal withdrawal\nTable F2: 26 words and phrases with a space in the wrong place from the Pearson files"
    }, {
      "heading" : "Appendix G: The 646 American and British Spelling Variations",
      "text" : "G - 1"
    }, {
      "heading" : "Appendix G: The 646 American and British Spelling Variations",
      "text" : "This appendix shows the 646 American and British spelling variations used by the ELKB. It is a union of three publicly available word lists: The American British – British American Dictionary (Smith, 2003), Delphion’s American/British Patent Term (Derwent, 2001) and XPNDC – American and British Spelling Variations (XPNDC, 2003).\nAmerican British American British abridgment abridgement archeology archaeology accouterment accoutrement ardor ardour acknowledgment acknowledgement armor armour adapter adaptor armorer armourer advertisement advertizement armory armoury advisor adviser artifact artefact adz adze ashtray ash-tray aerospace plane aerospaceplane asphalt asphalte afterward afterwards ass arse aging ageing atchoo atishoo airily aerify ax axe airplane aeroplane B.S. B.Sc. airy aery back scratch backscratch alluvium alluvion backward backwards alright allright balk baulk aluminum aluminium ball gown ballgown ameba amoeba baloney boloney Americanize Americanise baritone barytone amid amidst bark barque among amongst barreled barelled amphitheater amphitheatre barreled barrelled analog analogue barreling barelling analyze analyse battle-ax battleaxe anemia anaemia bedeviled bedevilled anemic anaemic behavior behaviour anesthesia anaesthesia behavioral behavioural anesthetic anaesthetic behoove behove anesthetist anaesthetist belabor belabour annex annexe bell ringer bellringer antiaircraft anti-aircraft belly flop bellyflop apologize apologise beside besides apothegm apophthegm bicolor bicolour appall appal bisulfate bisulphate apprise apprise bladder wrack bladderwrack arbor arbour book collection bookcollection"
    }, {
      "heading" : "Appendix G: The 646 American and British Spelling Variations",
      "text" : "G - 2\nAmerican British American British bookkeeper book-keeper check cheque boric boracic checker chequer break dance breakdance chili arch chiliarch brier briar chili chilli buncombe bunkum choir stall choirstall burden burthen cigaret cigarette burglarize burglarise citrus citrous burned burnt civilization civilization by allusion byallusion clamor clamour cachexia cachexy clangor clangour cafe café clarinetist clarinetist caliber calibre claw back clawback caliper calliper clearstory clerestory calipers callipers clever stick cleverstick calisthenics callisthenics cloture closure call girl callgirl cogency coagency canceled cancelled colonize colonize canceling cancelling color colour canceling cancellling conjuror conjurer candor candour connection connexion cantaloupe cantaloup cornflower cornflour capitalize capitalise councilor councilor carburetor carburettor counseled counseled carcass carcass counseling counseling caroler caroller counselor counselor caroling carolling cozy cosy cat slick catslick crawfish crayfish catalog catalogue criticize criticize catalyze catalyse curb kerb categorize categorize cutlas cutlass catsup ketchup czar tsar caviler caviller dark fall darkfall cell phone cellphone daydream day-dream center centre deathbed repentance deathbedrepentance centerboard centreboard defense defence centerfold centrefold deflection deflexion centering centring deflexion deflection centerpiece centrepiece demeanor demeanour centimeter centimetre dependent dependant cesarean caesarean deviled devilled cesarian caesarian deviling devilling cesium caesium dialog dialogue chamomile camomile dialyze cialyse channeled channelled diarrhea ciarrhea characterize characterise dieresis diaeresis"
    }, {
      "heading" : "Appendix G: The 646 American and British Spelling Variations",
      "text" : "G - 3\nAmerican British American British discolor discolour epaulet epaulette disfavor disfavour epicenter epicentre disheveled dishevelled epilog epilogue disheveling dishevelling equaled equalled dishonor dishonour equaling equalling disk disc equalize equalise dissention dissension esophagus oesophagus distill distil esthete aesthete disulfide disulphide esthetic aesthetic dolor dolour estival aestival donut doughnut estrogen oestrogen doodad doodah estrus oestrus doom watch doomwatch ether aether draft draught etiological aetiological draftsman draughtsman etiology aetiology drafty draughty eurythmy eurhythmy dramatize dramatise fagot faggot dreamed dreamt fagoting faggoting driveling drivelling fantasize fantasise dryly drily favor favour drypoint dry-point favored favoured duelist duellist favorite favourite duelists duellists favoritism favouritism eager eagre fecal faecal ecology oecology feces faeces ecumenical oecumenical fervor fervour edema oedema fetal foetal edematous oedematous fete fête elite élite fetid foetid emphasize emphasise fetor foetor enameled enamelled fetus foetus enameling enamelling fiber fibre enamor enamour fiberboard fibreboard encyclopedia encyclopaedia fiberglass fibreglass encyclopedia encyclopedia flakey flaky endeavor endeavour flavor flavour enology oenology flavored flavoured enroll enrol floatation flotation enrollment enrolment font fount enthrall enthral foregather forgather entree entrée forego forgo enure inure form forme envelop envelope forward forwards eon aeon frog march frogmarch eons aeons fueled fuelled"
    }, {
      "heading" : "Appendix G: The 646 American and British Spelling Variations",
      "text" : "G - 4\nAmerican British American British fueling fuelling hosteler hosteller fulfill fulfil hosteling hostelling fulfillment fulfilment hostler ostler furor furore humor humour fuse fuze ill betide illbetide galipot gallipot immortalize immortalise gallows bird gallowsbird impanel empanel gantlet gauntlet in appetence inappetence garrote garotte in expectancy inexpectancy garroted garotted in wrestling inwrestling garroting garotting incase encase gasoline gasolene inclose enclose gayety gaiety indorse endorse gel gell inflection inflexion genuflection genuflexion inquire enquire glamor glamour inquiry enquiry glamorize glamorise instal install goiter goitre installment instalment gonorrhea gonorrhoea instill instil good-by goodbye insure ensure gram gramme intern interne gray grey jail gaol groveled grovelled jeweler jeweller groveler groveller jewelry jewellery groveling grovelling jewlry jewellery grueling gruelling jibe gybe gynecology gynaecology jimmy jemmy gypsy gipsy Jr Jnr. hair space hairspace judgment judgement Halloween Hallowe'en karat carat halyard halliard kidnaped kidnapped harbor harbour kidnaper kidnapper harmonize harmonise kidnaping kidnapping have mercy havemercy kilometer kilometre hell hag hellhag kneeled knelt hemoglobin haemoglobin knob stick knobstick hemophilia haemophilia know all knowall hemorrhage haemorrhage labeled labelled hemorrhoid haemorrhoid labor labour hold all holdall lackluster lacklustre homeopath homoeopath lady killer ladykiller homeostasis homoeostasis lave rock laverock homolog homologue lay stall laystall honor honour lead pollution leadpollution hosteled hostelled leaned leant"
    }, {
      "heading" : "Appendix G: The 646 American and British Spelling Variations",
      "text" : "G - 5\nAmerican British American British leaped leapt misjudgment misjudgement learned learnt miter mitre leg pull legpull mobilize mobilise lemongrass lemon modeled modelled leukemia leukaemia modeler modeller leveled levelled modeling modelling leveler leveller mold mould leveler leveller molding moulding leveling levelling mollusk mollusc libeled libelled molt moult libeling libelling mom mum libelous libellous monolog monologue license licence motorize motorise licorice liquorice mum chance mumchance light well lightwell mustache moustache limp back limpback naive naïve liter litre naturalize naturalise logorrhea logorrhoea naught nought long shore longshore neighbor neighbour louver louvre neighborhood neighbourhood low fellow lowfellow neighborly neighbourly Luster lustre neoclassical neo-classical M.S. M.Sc. net nett malodor malodour night watch nightwatch man hour manhour nite night man oeuvre manoeuvre niter nitre maneuver manoeuvre not respect notrespect marshaled marshalled note paper notepaper marveled marvelled ocher ochre marveling marvelling odor odour marvelous marvellous offense offence marvelously marvellously omelet omelette matinee matinée organize organise meager meagre organized organised medieval mediaeval orthopedics orthopaedics mega there megathere outmaneuver outmanoeuvre menorrhea menorrhoea paddy whack paddywhack mental deficiency mentaldeficiency pajamas pyjamas metaled metalled paleobotany palaeobotany metaling metalling Paleocene Palaeocene meter metre paleoclimatology palaeoclimatology mill pool millpool paleogeography palaeogeography millimeter millimetre paleography palaeography misbehavior misbehaviour paleolithic palaeolithic misdemeanor misdemeanour paleomagnetism palaeomagnetism"
    }, {
      "heading" : "Appendix G: The 646 American and British Spelling Variations",
      "text" : "G - 6\nAmerican British American British paleontology palaeontology primeval primaeval Paleozoic Palaeozoic program programme panatela panatella programed programmed paneled panelled programer programmer paneling panelling programing programming panelist panellist prolog prologue paralyze paralyse propellent propellant parameterize parametrize propellor propeller parlor parlour pudgy podgy pastel list pastellist pull through pullthrough pasteurized pasteurised pummeling pummelling pavior paviour pupilage pupillage pean paean pygmy pigmy peas pease quarreled quarrelled pedagog pedagogue quarreler quarreller pedagogy paedagogy quarreling quarrelling pedaled pedalled racial prejudice racialprejudice pedaling pedalling rancor rancour peddler pedlar raveled ravelled pederast paederast realize realise pediatric paediatric recognizance recognisance pediatrician paediatrician recognize recognise pediatrics paediatrics reconnoiter reconnoitre pedler pedlar remodeling remodelling pedophile paedophile retroflection retroflexion pedophilia paedophilia reveled revelled penciled pencilled reveler reveller penciling pencilling reveling revelling persnickety pernickety revery reverie philter philtre reviviscence revivescence pickaninny piccaninny rigor rigour picket piquet rivaling rivalling pill winks pilliwinks role rôle pillar box pillarbox roll mops rollmops pipe tobacco pipetobacco roller coaster rollercoaster pjamas pyjamas romanize romanise plow plough ruble rouble plowman ploughman rumor rumour plowshare ploughshare saber sabre polyethylene polythene safe conduct safeconduct popularize popularise saga more sagamore port fire portfire sally port sallyport practice practise saltier saltire pretense pretence saltpeter saltpetre pricey pricy sanitorium sanatorium"
    }, {
      "heading" : "Appendix G: The 646 American and British Spelling Variations",
      "text" : "G - 7\nAmerican British American British satirize satirise spelled spelt savior saviour spilled spilt savor savour spiraling spiralling savory savoury splendor splendour scalawag scallywag spoiled spoilt scalp lock scalplock Sr Snr scepter sceptre stanch staunch scimitar scimetar standardize standardise septicemia septicaemia stenosis stegnosis sepulcher sepulchre story storey sex prejudice sexprejudice stout fellow stoutfellow sheep track sheeptrack succor succour shooting gallery shootinggallery suffix ion suffixion shoveled shovelled sulfate sulphate show shew sulfide sulphide shrink pack shrinkpack sulfur sulphur shriveled shrivelled sulfureted sulphuretted signaler signaller swallow hole swallowhole signaling signalling symbolize symbolise siphon syphon synagog synagogue siren syren syneresis synaeresis skeptic sceptic synesthesia synaesthesia skeptical sceptical syphon siphon skepticism scepticism taffy toffee skillful skilful the fallen thefallen skillfully skilfully theater theatre skin flick skinflick thralldom thraldom slug slog throw stick throwstick slush slosh thru through smelled smelt tidbit titbit smoke duct smokeduct tike tyke smolder smoulder till ant tillant snail shell snailshell tire tyre snicker snigger tiro tyro sniveled snivelled titer titre sniveler sniveller toilet toilette sniveling snivelling tonite tonight snow pack snowpack toward towards snowplow snowplough toweling towelling soft back softback trammeled trammelled somber sombre traveled travelled soy sauce soysauce traveler traveller specialize specialise traveling travelling specialty speciality travelog travelogue specter spectre tricolor tricolour"
    }, {
      "heading" : "Appendix G: The 646 American and British Spelling Variations",
      "text" : "G - 8\nAmerican British trisulfate trisulphate troweled trowelled troweling trowelling tumor tumour tunneling tunnelling ultrahigh ultra-high ultramodern ultra-modern unraveled unravelled unraveled untravelled unraveling unravelling untrammeled untrammelled valor valour vapor vapour vaporize vaporise vaporware vapourware veranda verandah vial phial video pack videopack vigor vigour vise vice visually challenged visuallychallenged wagon waggon watercolor watercolour weed killer weedkiller whey face wheyface while whilst whiskey whisky willful wilful willie willy woolen woollen wooly woolly word stock wordstock worshiped worshipped worshiper worshipper worshiping worshipping yodeling yodelling\nTable G1: The 646 American and British Spelling Variations\nAppendix H: The 980-element Stop List\nH - 1"
    }, {
      "heading" : "Appendix H: The 980-element Stop List",
      "text" : "This 980-element stop list is a union of five publicly-available lists: Oracle 8 ConText, SMART, Hyperwave, and lists from the University of Kansas and Ohio State University.\n0 45 81 along b can 1 46 82 alpha back can't 10 47 83 already backed cannot 11 48 84 also backing cant 12 49 85 although backs caption 13 5 86 always barely case 14 50 87 am be cases 15 51 88 among became cause 16 52 89 amongst because causes 17 53 9 an become certain 18 54 90 and becomes certainly 19 55 91 another becoming changes 2 56 92 any been chi 20 57 93 anybody before circa 21 58 94 anyhow beforehand clear 22 59 95 anyone began clearly 23 6 96 anything begin cm 24 60 97 anyway beginning co 25 61 98 anyways behind co. 26 62 99 anywhere being com 27 63 a apart beings come 28 64 a's appear believe comes 29 65 able appreciate below con 3 66 about appropriate beside concerning 30 67 above are besides consequently 31 68 according area best consider 32 69 accordingly areas beta considering 33 7 across aren't better contain 34 70 actually around between containing 35 71 adj as beyond contains 36 72 after aside big corp 37 73 afterwards ask billion corresponding 38 74 again asked both could 39 75 against asking brief couldn't 4 76 ain't asks but course 40 77 all associated by currently 41 78 allow at c d 42 79 allows available c'mon db 43 8 almost away c's definitely 44 80 alone awfully came delta\nAppendix H: The 980-element Stop List\nH - 2\ndescribed et further he'd inc. latest despite eta furthered he'll indeed latter did etc furthering he's indicate latterly didn't even furthermore hello indicated lb didst evenly furthers help indicates lbs differ ever g hence inner least different every gamma henceforth inside less differently everybody gave her insofar lest do everyone general here instead let doer everything generally here's interest let's does everywhere get hereafter interested lets doesn't ex gets hereby interesting like doest exactly getting herein interests liked doeth example give hereupon into likely doing except given hers inward little don't f gives herself iota ln done face go hi is lo dost faces goes high isn't long doth fact going higher it longer down facts gone highest it'd longest downed fairly good him it'll look downing far goods himself it's looking downs felt got his its looks downwards few gotten hither itself ltd during fewer great hopefully iv m e fifteen greater how ix made each fifth greatest howbeit j mainly early fifty greetings however just make edu find group hundred k makes eg finds grouped hz kappa making eight first grouping i keep man eighteen five groups i'd keeps many eighty followed h i'll kept may either following had i'm kg maybe eleven follows hadn't i've km me else for happens ie know mean elsewhere former hardly if known meantime end formerly has ignored knows meanwhile ended forth hasn't ii l member ending forty hast iii lamda members ends found hath immediate large men enough four have important largely merely entirely fourteen haven't in last mi epsilon from having inasmuch lately might especially ft he inc later million\nAppendix H: The 980-element Stop List\nH - 3\nmine noone ourselves qv seventeen such miss nor out r seventy sup ml normally outside rather several sure mm not over rd shall t more nothing overall re shalt t's moreover novel own really she take most now oz reasonably she'd taken mostly nowhere p recent she'll taking mr nu part recently she's tau mrs number parted regarding should tell ms numbers particular regardless shouldn't ten mu o particularly regards show tends much obviously parting relatively showed th must of parts respectively showing than my off per rho shows thank myself often perhaps right sides thanks mz oh phi room sigma thanx n ok pi rooms simply that name okay place roughly since that'll namely old placed s six that's nay older places said sixteen that've nd oldest please same sixty thats near omega plus saw small the nearly omicron point say smaller thee necessary on pointed saying smallest their need once pointing says so theirs needed one points sec some them needing one's possible second somebody themselves needs ones pre secondly somehow then neither only present seconds someone thence never onto presented secs something there nevertheless open presenting see sometime there'd new opened presents seeing sometimes there'll newer opens presumably seem somewhat there're newest or pro seemed somewhere there's next order probably seeming soon there've nine ordered problem seems sorry thereafter nineteen ordering problems seen specified thereby ninety orders provides self specify therefore Nm other psi selves specifying therein No others put sensible state thereof nobody otherwise puts sent states thereon non ought q serious still theres none our que seriously stop thereupon nonetheless ours quite seven sub these\nAppendix H: The 980-element Stop List\nH - 4\ntheta turning we whole yea they turns we'd whom year they'd twelve we'll whomever years they'll twenty we're whomsoever yes they're twice we've whose yet they've two welcome whoso you thine u well whosoever you'd thing un wells why you'll things under went will you're think unfortunately were willing you've thinks unless weren't wish young third unlike what with younger thirteen unlikely what'll within youngest thirty until what's without your this unto what've won't yours thorough up whatever wonder yourself thoroughly upon whatsoever work yourselves those upsilon when worked z thou us whence working zero though use whenever works zeta thought used whensoever would thoughts useful where wouldn't thousand uses where's x three using whereafter xi through usually whereas xii throughout uucp whereby xiii thru v wherefore xiv thus value wherein xix thy various whereinto xv thyself very whereof xvi to vi whereon xvii today via wheresoever xviii together vii whereupon xx too viii wherever xxi took viz wherewith xxii toward vs whether xxiii towards w which xxiv tried want while xxix tries wanted whilst xxv trillion wanting whither xxvi truly wants who xxvii try was who'd xxviii trying wasn't who'll y turn way who's yd turned ways whoever ye\nAppendix I: The Rubenstein and Goodenough 65 Noun Pairs\nI - 1\nAppendix I: The Rubenstein and Goodenough 65 Noun Pairs\nThis appendix contains the Rubenstein and Goodenough (1965) 65 noun pairs and the semantic similarity scores for the ELKB as well as the WordNet-based measures. They are correlated to Rubenstein and Goodenough’s results.\nNoun Pair Rubenstein Goodenough ELKB WordNet Edges Hirst St.Onge Jiang Conrath Leacock Chodorow Lin Resnik\ngem – jewel 3.940 16.000 30.000 16.000 1.000 3.466 1.000 12.886 midday – noon 3.940 16.000 30.000 16.000 1.000 3.466 1.000 10.584 automobile – car 3.920 16.000 30.000 16.000 1.000 3.466 1.000 6.340 cemetery – graveyard 3.880 16.000 30.000 16.000 1.000 3.466 1.000 10.689\ncushion – pillow 3.840 16.000 29.000 4.000 0.662 2.773 0.975 9.891 boy – lad 3.820 16.000 29.000 5.000 0.231 2.773 0.824 7.769\ncock – rooster 3.680 16.000 30.000 16.000 1.000 3.466 1.000 11.277 implement – tool 3.660 16.000 29.000 4.000 0.546 2.773 0.935 5.998 forest – woodland 3.650 14.000 30.000 16.000 1.000 3.466 1.000 10.114 coast – shore 3.600 16.000 29.000 4.000 0.647 2.773 0.971 8.974 autograph – signature 3.590 16.000 29.000 4.000 0.325 2.773 0.912 10.807 journey – voyage 3.580 16.000 29.000 4.000 0.169 2.773 0.699 6.057\nserf – slave 3.460 16.000 27.000 5.000 0.261 2.079 0.869 9.360 grin – smile 3.460 16.000 30.000 16.000 1.000 3.466 1.000 9.198 glass – tumbler 3.450 16.000 29.000 6.000 0.267 2.773 0.873 9.453 cord – string 3.410 16.000 29.000 6.000 0.297 2.773 0.874 8.214 hill – mound 3.290 12.000 30.000 16.000 1.000 3.466 1.000 11.095 magician – wizard 3.210 14.000 30.000 16.000 1.000 3.466 1.000 9.708 furnace – stove 3.110 14.000 23.000 5.000 0.060 1.386 0.238 2.426 asylum – madhouse 3.040 16.000 29.000 4.000 0.662 2.773 0.978 11.277 brother – monk 2.740 14.000 29.000 4.000 0.294 2.773 0.897 10.489\nfood – fruit 2.690 12.000 23.000 0.000 0.088 1.386 0.119 0.699 bird – cock 2.630 12.000 29.000 6.000 0.159 2.773 0.693 5.980 bird – crane 2.630 14.000 27.000 5.000 0.139 2.079 0.658 5.980 oracle – sage 2.610 16.000 23.000 0.000 0.057 1.386 0.226 2.455 sage – wizard 2.460 14.000 25.000 2.000 0.060 1.674 0.236 2.455 brother – lad 2.410 14.000 26.000 3.000 0.071 1.856 0.273 2.455\ncrane – implement 2.370 0.000 26.000 3.000 0.086 1.856 0.394 3.443 magician – oracle 1.820 6.000 28.000 6.000 0.533 2.367 0.957 9.708 glass – jewel 1.780 12.000 24.000 2.000 0.064 1.520 0.249 2.426 cemetery – mound 1.690 0.000 20.000 0.000 0.055 1.068 0.076 0.699\ncar – journey 1.550 12.000 17.000 0.000 0.075 0.827 0.000 0.000 hill – woodland 1.480 0.000 25.000 2.000 0.060 1.674 0.132 1.183 crane – rooster 1.410 12.000 23.000 0.000 0.080 1.386 0.510 5.980\nfurnace – implement 1.370 6.000 25.000 2.000 0.081 1.674 0.299 2.426\nAppendix I: The Rubenstein and Goodenough 65 Noun Pairs\nI - 2\nNoun Pair Rubenstein Goodenough ELKB WordNet Edges Hirst St.Onge Jiang Conrath Leacock Chodorow Lin Resnik"
    }, {
      "heading" : "Appendix J: The WordSimilarity-353 Test Collection",
      "text" : "J - 1"
    }, {
      "heading" : "Appendix J: The WordSimilarity-353 Test Collection",
      "text" : "This appendix presents The WordSimilarity-353 Test Collection (Finkelstein et al., 2002; Gabrilovich 2002) and the semantic similarity scores for the ELKB as well as the WordNet-based measures. They are correlated to Finkelstein et al.’s results.\nWord Pair Gabr. ELKB WN Edges Hirst St.O.\nJiang Con.\nLea. Chod.\nLin Res.\ntiger – tiger 10.00 16.00 30.00 24.00 1.00 3.47 1.00 12.18 fuck – sex 9.44 28.00 3.00 0.18 2.37 0.78 8.27 journey – voyage 9.29 16.00 29.00 4.00 0.17 2.77 0.70 6.05 midday – noon 9.29 16.00 30.00 16.00 1.00 3.47 1.00 10.57 dollar – buck 9.22 16.00 30.00 16.00 1.00 3.47 1.00 10.31 money – cash 9.15 16.00 28.00 5.00 0.19 2.37 0.74 7.14 coast – shore 9.10 16.00 29.00 4.00 0.65 2.77 0.97 8.96 money – cash 9.08 16.00 28.00 5.00 0.19 2.37 0.74 7.14 money – currency 9.04 16.00 29.00 5.00 0.41 2.77 0.90 7.14 football – soccer 9.03 16.00 29.00 4.00 0.27 2.77 0.88 10.17 magician – wizard 9.02 14.00 30.00 16.00 1.00 3.47 1.00 9.70 type – kind 8.97 16.00 29.00 4.00 0.62 2.77 0.95 5.60 gem – jewel 8.96 16.00 30.00 16.00 1.00 3.47 1.00 12.87 car – automobile 8.94 16.00 30.00 16.00 1.00 3.47 1.00 6.33 street – avenue 8.88 16.00 29.00 4.00 0.21 2.77 0.81 8.09 asylum – madhouse 8.87 14.00 29.00 4.00 0.66 2.77 0.98 11.26 boy – lad 8.83 16.00 29.00 5.00 0.23 2.77 0.82 7.76 environment – ecology 8.81 14.00 29.00 4.00 0.17 2.77 0.74 7.14 furnace – stove 8.79 14.00 23.00 5.00 0.06 1.39 0.24 2.45 seafood – lobster 8.70 16.00 28.00 5.00 0.24 2.37 0.84 8.08 mile – kilometer 8.66 14.00 27.00 4.00 0.10 2.08 0.55 5.34 Maradona – football 8.62 OPEC – oil 8.59 4.00 17.00 0.00 0.05 0.83 0.00 0.00 king – queen 8.58 16.00 28.00 5.00 0.27 2.37 0.89 11.49 murder – manslaughter 8.53 14.00 28.00 5.00 0.17 2.37 0.76 7.84 money – bank 8.50 16.00 24.00 0.00 0.10 1.52 0.47 4.11 computer – software 8.50 14.00 16.00 0.00 0.06 0.76 0.00 0.00 Jerusalem – Israel 8.46 20.00 4.00 0.06 1.07 0.31 3.71 vodka – gin 8.46 14.00 28.00 5.00 0.12 2.37 0.70 8.43 planet – star 8.45 14.00 28.00 5.00 0.35 2.37 0.88 6.84 calculation – computation 8.44 16.00 30.00 16.00 1.00 3.47 1.00 8.88 money – dollar 8.42 16.00 26.00 3.00 0.18 1.86 0.73 7.14 law – lawyer 8.38 12.00 21.00 0.00 0.06 1.16 0.00 0.00 championship – tournament 8.36 6.00 22.00 0.00 0.04 1.27 0.00 0.00 seafood – food 8.34 14.00 29.00 16.00 0.29 2.77 0.83 5.69 weather – forecast 8.34 14.00 17.00 0.00 0.05 0.83 0.00 0.00 FBI – investigation 8.31 14.00 19.00 0.00 0.05 0.98 0.00 0.00 network – hardware 8.31 6.00 27.00 4.00 0.06 2.08 0.32 3.44 nature – environment 8.31 4.00 24.00 0.00 0.06 1.52 0.07 0.71"
    }, {
      "heading" : "Appendix J: The WordSimilarity-353 Test Collection",
      "text" : "J - 2\nWord Pair Gabr. ELKB WN Edges Hirst St.O.\nJiang Con.\nLea. Chod.\nLin Res.\nman – woman 8.30 16.00 27.00 4.00 0.13 2.08 0.59 4.81 money – wealth 8.27 16.00 29.00 4.00 0.96 2.77 1.00 8.87 psychology – Freud 8.21 12.00 0.00 0.04 0.52 0.00 0.00 news – report 8.16 16.00 29.00 5.00 0.83 2.77 0.99 6.99 vodka – brandy 8.13 14.00 28.00 5.00 0.14 2.37 0.73 8.43 war – troops 8.13 12.00 22.00 0.00 0.06 1.27 0.00 0.00 Harvard – Yale 8.13 28.00 5.00 0.17 2.37 0.79 10.17 bank – money 8.12 16.00 24.00 0.00 0.10 1.52 0.47 4.11 physics – proton 8.12 12.00 13.00 0.00 0.05 0.58 0.00 0.00 planet – galaxy 8.11 12.00 23.00 4.00 0.05 1.39 0.17 2.17 stock – market 8.08 16.00 24.00 4.00 0.10 1.52 0.40 3.04 psychology – psychiatry 8.08 16.00 24.00 2.00 0.11 1.52 0.62 6.51 planet – moon 8.08 16.00 27.00 4.00 0.25 2.08 0.82 6.84 planet – constellation 8.06 12.00 27.00 4.00 0.13 2.08 0.57 4.50 credit – card 8.06 16.00 25.00 2.00 0.07 1.67 0.37 4.41 hotel – reservation 8.03 6.00 20.00 0.00 0.05 1.07 0.07 0.71 planet – sun 8.02 12.00 27.00 4.00 0.28 2.08 0.84 6.84 tiger – jaguar 8.00 16.00 28.00 5.00 0.21 2.37 0.84 9.74 tiger – feline 8.00 14.00 28.00 6.00 0.25 2.37 0.85 8.41 closet – clothes 8.00 12.00 24.00 0.00 0.08 1.52 0.31 2.45 planet – astronomer 7.94 12.00 24.00 0.00 0.06 1.52 0.23 2.45 soap – opera 7.94 16.00 20.00 0.00 0.06 1.07 0.20 1.96 movie – theater 7.92 16.00 23.00 0.00 0.06 1.39 0.00 0.00 planet – space 7.92 12.00 23.00 3.00 0.07 1.39 0.19 1.96 treatment – recovery 7.91 6.00 24.00 0.00 0.08 1.52 0.28 2.25 liquid – water 7.89 16.00 29.00 6.00 0.99 2.77 1.00 6.19 life – death 7.88 16.00 28.00 5.00 0.23 2.37 0.81 6.95 baby – mother 7.85 14.00 26.00 3.00 0.22 1.86 0.76 6.01 aluminum – metal 7.83 29.00 4.00 0.21 2.77 0.79 7.09 cell – phone 7.81 6.00 26.00 3.00 0.13 1.86 0.68 7.21 lobster – food 7.81 14.00 27.00 3.00 0.15 2.08 0.67 5.69 dollar – yen 7.78 14.00 27.00 3.00 0.11 2.08 0.63 6.85 wood – forest 7.73 14.00 30.00 16.00 1.00 3.47 1.00 8.28 money – deposit 7.73 16.00 28.00 6.00 0.16 2.37 0.72 6.82 television – film 7.72 16.00 26.00 3.00 0.22 1.86 0.80 7.23 psychology – mind 7.69 16.00 24.00 0.00 0.09 1.52 0.41 3.39 game – team 7.69 12.00 23.00 0.00 0.07 1.39 0.00 0.00 admission – ticket 7.69 16.00 22.00 0.00 0.06 1.27 0.27 2.88 Jerusalem – Palestinian 7.65 16.00 0.00 0.04 0.76 0.06 0.71 Arafat – terror 7.65 dividend – payment 7.63 14.00 28.00 6.00 0.15 2.37 0.71 7.09 profit – loss 7.63 14.00 25.00 3.00 0.33 1.67 0.86 6.54 computer – keyboard 7.62 14.00 27.00 2.00 0.08 2.08 0.43 4.29 boxing – round 7.61 14.00 24.00 0.00 0.13 1.52 0.67 6.85 century – year 7.59 14.00 28.00 4.00 0.13 2.37 0.52 3.72 rock – jazz 7.59 16.00 28.00 5.00 0.17 2.37 0.78 8.67 computer – internet 7.58 23.00 5.00 0.06 1.39 0.31 3.44"
    }, {
      "heading" : "Appendix J: The WordSimilarity-353 Test Collection",
      "text" : "J - 3\nWord Pair Gabr. ELKB WN Edges Hirst St.O.\nJiang Con.\nLea. Chod.\nLin Res.\nmoney – property 7.57 14.00 28.00 6.00 0.34 2.37 0.88 6.96 tennis – racket 7.56 6.00 22.00 0.00 0.06 1.27 0.27 3.12 announcement – news 7.56 14.00 26.00 3.00 0.10 1.86 0.47 3.84 canyon – landscape 7.53 6.00 19.00 0.00 0.05 0.98 0.17 1.96 day – dawn 7.53 14.00 28.00 5.00 0.08 2.37 0.38 3.72 food – fruit 7.52 12.00 24.00 0.00 0.11 1.52 0.33 1.96 telephone – communication 7.50 14.00 21.00 0.00 0.08 1.16 0.00 0.00 currency – market 7.50 14.00 23.00 0.00 0.06 1.39 0.00 0.00 psychology – cognition 7.48 12.00 25.00 3.00 0.13 1.67 0.46 2.89 Marathon – sprint 7.47 14.00 20.00 0.00 0.05 1.07 0.19 2.25 seafood – sea 7.47 6.00 23.00 0.00 0.06 1.39 0.09 0.71 book – paper 7.46 16.00 28.00 5.00 0.12 2.37 0.58 5.18 book – library 7.46 16.00 24.00 2.00 0.07 1.52 0.28 2.45 Mexico – Brazil 7.44 26.00 3.00 0.08 1.86 0.53 6.05 media – radio 7.42 16.00 19.00 0.00 0.04 0.98 0.00 0.00 psychology – depression 7.42 12.00 21.00 0.00 0.06 1.16 0.28 2.80 jaguar – cat 7.42 14.00 29.00 4.00 0.33 2.77 0.91 9.74 fighting – defeating 7.41 4.00 0.00 movie – star 7.38 12.00 27.00 0.00 0.07 1.39 0.31 2.88 bird – crane 7.38 14.00 18.00 5.00 0.14 2.08 0.66 5.97 hundred – percent 7.38 20.00 0.00 0.07 0.90 0.21 1.81 dollar – profit 7.38 16.00 29.00 0.00 0.06 1.07 0.18 1.81 tiger – cat 7.35 14.00 28.00 4.00 0.36 2.77 0.92 9.74 physics – chemistry 7.35 14.00 23.00 2.00 0.23 2.37 0.81 7.04 country – citizen 7.31 12.00 27.00 5.00 0.07 1.39 0.10 0.71 money – possession 7.29 12.00 14.00 5.00 0.17 2.08 0.63 4.11 jaguar – car 7.27 6.00 30.00 0.00 0.06 0.63 0.08 0.71 cup – drink 7.25 14.00 27.00 5.00 0.19 2.08 0.77 7.08 psychology – health 7.23 12.00 20.00 0.00 0.05 1.07 0.00 0.00 museum – theater 7.19 6.00 24.00 2.00 0.06 1.52 0.24 2.45 summer – drought 7.16 6.00 27.00 4.00 0.07 2.08 0.36 3.72 phone – equipment 7.13 6.00 28.00 2.00 0.24 2.37 0.80 6.04 investor – earning 7.13 4.00 0.00 bird – cock 7.10 12.00 29.00 6.00 0.16 2.77 0.69 5.97 company – stock 7.08 14.00 25.00 2.00 0.12 1.67 0.46 3.35 tiger – carnivore 7.08 14.00 27.00 5.00 0.18 2.08 0.74 6.78 stroke – hospital 7.03 12.00 20.00 0.00 0.06 1.07 0.06 0.71 liability – insurance 7.03 8.00 26.00 3.00 0.19 1.86 0.79 8.28 game – victory 7.03 14.00 24.00 0.00 0.12 1.52 0.50 3.78 doctor – nurse 7.00 12.00 27.00 5.00 0.25 2.08 0.83 7.28 tiger – animal 7.00 14.00 27.00 2.00 0.12 2.08 0.55 4.32 psychology – anxiety 7.00 12.00 21.00 0.00 0.08 1.16 0.33 2.80 game – defeat 6.97 14.00 24.00 0.00 0.10 1.52 0.46 3.78 FBI – fingerprint 6.94 4.00 16.00 0.00 0.04 0.76 0.00 0.00 money – withdrawal 6.88 6.00 21.00 0.00 0.06 1.16 0.00 0.00 street – block 6.88 14.00 25.00 2.00 0.08 1.67 0.30 2.46 opera – performance 6.88 12.00 24.00 2.00 0.08 1.52 0.34 2.88"
    }, {
      "heading" : "Appendix J: The WordSimilarity-353 Test Collection",
      "text" : "J - 4\nWord Pair Gabr. ELKB WN Edges Hirst St.O.\nJiang Con.\nLea. Chod.\nLin Res.\ndrink – eat 6.87 5.00 drug – abuse 6.85 14.00 22.00 0.00 0.06 1.27 0.00 0.00 tiger – mammal 6.85 14.00 25.00 3.00 0.14 1.67 0.64 5.43 psychology – fear 6.85 6.00 21.00 0.00 0.08 1.16 0.33 2.80 cup – tableware 6.85 28.00 6.00 0.58 2.37 0.95 7.60 student – professor 6.81 14.00 23.00 0.00 0.07 1.39 0.28 2.45 football – basketball 6.81 14.00 28.00 5.00 0.16 2.37 0.76 8.45 concert – virtuoso 6.81 14.00 20.00 0.00 0.05 1.07 0.00 0.00 computer – laboratory 6.78 6.00 21.00 0.00 0.05 1.16 0.08 0.71 love – sex 6.77 12.00 29.00 4.00 0.18 2.77 0.78 8.27 television – radio 6.77 16.00 28.00 5.00 0.33 2.37 0.90 9.51 Problem – challenge 6.75 14.00 25.00 2.00 0.08 1.67 0.37 3.84 Arafat – Peace 6.73 movie – critic 6.73 12.00 20.00 0.00 0.06 1.07 0.00 0.00 bed – closet 6.72 6.00 28.00 4.00 0.15 2.37 0.70 6.74 psychology – science 6.71 14.00 29.00 4.00 0.24 2.77 0.81 6.51 fertility – egg 6.69 12.00 20.00 0.00 0.04 1.07 0.00 0.00 bishop – rabbi 6.69 12.00 26.00 3.00 0.16 1.86 0.74 7.52 lawyer – evidence 6.69 6.00 20.00 0.00 0.06 1.07 0.00 0.00 precedent – law 6.65 16.00 28.00 6.00 0.17 2.37 0.74 6.94 football – tennis 6.63 16.00 25.00 2.00 0.17 1.67 0.76 8.09 minister – party 6.63 16.00 25.00 2.00 0.07 1.67 0.26 2.45 professor – doctor 6.62 14.00 24.00 0.00 0.20 1.52 0.77 6.55 psychology – clinic 6.58 12.00 17.00 0.00 0.05 0.83 0.00 0.00 cup – coffee 6.58 14.00 25.00 2.00 0.11 1.67 0.61 6.12 water – seepage 6.56 6.00 22.00 0.00 0.05 1.27 0.00 0.00 government – crisis 6.56 6.00 23.00 0.00 0.06 1.39 0.00 0.00 space – world 6.53 16.00 26.00 4.00 0.07 1.86 0.19 1.96 Japanese – American 6.50 25.00 2.00 0.09 1.67 0.54 6.27 dividend – calculation 6.48 6.00 21.00 0.00 0.05 1.16 0.00 0.00 victim – emergency 6.47 6.00 23.00 0.00 0.05 1.39 0.07 0.71 luxury – car 6.47 8.00 18.00 0.00 0.05 0.90 0.00 0.00 tool – implement 6.46 16.00 29.00 4.00 0.55 2.77 0.94 5.99 competition – price 6.44 2.00 23.00 0.00 0.08 1.39 0.25 2.65 street – place 6.44 16.00 25.00 3.00 0.07 1.67 0.30 3.35 psychology – doctor 6.42 16.00 18.00 0.00 0.06 0.90 0.00 0.00 gender – equality 6.41 2.00 23.00 0.00 0.06 1.39 0.30 3.25 listing – category 6.38 4.00 23.00 0.00 0.07 1.39 0.00 0.00 discovery – space 6.34 12.00 23.00 0.00 0.06 1.39 0.00 0.00 oil – stock 6.34 14.00 22.00 0.00 0.09 1.27 0.43 5.19 video – archive 6.34 23.00 0.00 0.06 1.39 0.22 2.45 governor – office 6.34 17.00 22.00 0.00 0.06 1.27 0.24 2.45 train – car 6.31 16.00 25.00 4.00 0.18 1.67 0.71 5.50 record – number 6.31 14.00 29.00 6.00 0.32 2.77 0.84 5.87 shower – thunderstorm 6.31 16.00 24.00 0.00 0.08 1.52 0.56 6.85 brother – monk 6.27 14.00 29.00 4.00 0.29 2.77 0.90 10.48 nature – man 6.25 14.00 27.00 4.00 0.08 2.08 0.30 2.40"
    }, {
      "heading" : "Appendix J: The WordSimilarity-353 Test Collection",
      "text" : "J - 5\nWord Pair Gabr. ELKB WN Edges Hirst St.O.\nJiang Con.\nLea. Chod.\nLin Res.\nproduction – crew 6.25 8.00 24.00 0.00 0.06 1.52 0.00 0.00 family – planning 6.25 16.00 23.00 0.00 0.06 1.39 0.00 0.00 disaster – area 6.25 16.00 22.00 0.00 0.08 1.27 0.37 3.35 skin – eye 6.22 16.00 25.00 2.00 0.10 1.67 0.50 4.21 food – preparation 6.22 16.00 27.00 3.00 0.12 2.08 0.47 3.21 bread – butter 6.19 12.00 27.00 5.00 0.12 2.08 0.61 5.69 movie – popcorn 6.19 0.00 18.00 0.00 0.05 0.90 0.00 0.00 game – series 6.19 12.00 28.00 5.00 0.21 2.37 0.75 5.67 lover – quarrel 6.19 8.00 21.00 0.00 0.05 1.16 0.07 0.71 preservation – world 6.19 12.00 24.00 0.00 0.06 1.52 0.00 0.00 dollar – loss 6.09 6.00 21.00 0.00 0.07 1.16 0.19 1.81 weapon – secret 6.06 14.00 21.00 0.00 0.06 1.16 0.00 0.00 precedent – antecedent 6.04 16.00 25.00 4.00 0.05 1.67 0.30 3.84 shower – flood 6.03 16.00 26.00 3.00 0.09 1.86 0.60 7.56 registration – arrangement 6.00 4.00 25.00 2.00 0.07 1.67 0.32 3.12 arrival – hotel 6.00 2.00 21.00 0.00 0.05 1.16 0.07 0.71 announcement – warning 6.00 16.00 25.00 2.00 0.08 1.67 0.40 3.84 baseball – season 5.97 4.00 17.00 0.00 0.06 0.83 0.00 0.00 game – round 5.97 16.00 26.00 4.00 0.16 1.86 0.72 7.23 drink – mouth 5.96 12.00 24.00 0.00 0.07 1.52 0.25 2.40 energy – crisis 5.94 14.00 24.00 0.00 0.06 1.52 0.30 3.35 grocery – money 5.94 20.00 0.00 0.06 1.07 0.00 0.00 life – lesson 5.94 6.00 21.00 0.00 0.07 1.16 0.29 2.88 cucumber – potato 5.92 14.00 27.00 4.00 0.11 2.08 0.64 7.50 king – rook 5.92 16.00 28.00 5.00 0.20 2.37 0.85 10.93 reason – criterion 5.91 4.00 23.00 0.00 0.09 1.39 0.37 2.89 equipment – maker 5.91 6.00 22.00 0.00 0.07 1.27 0.10 0.71 cup – liquid 5.90 12.00 25.00 3.00 0.16 1.67 0.69 5.97 deployment – withdrawal 5.88 6.00 22.00 0.00 0.05 1.27 0.21 2.25 tiger – zoo 5.87 6.00 23.00 0.00 0.04 1.39 0.06 0.71 journey – car 5.85 12.00 17.00 0.00 0.07 0.83 0.00 0.00 precedent – example 5.85 16.00 29.00 4.00 0.23 2.77 0.83 7.81 smart – stupid 5.81 8.00 18.00 3.00 0.04 0.90 0.00 0.00 plane – car 5.77 16.00 24.00 3.00 0.21 1.52 0.75 5.55 planet – people 5.75 4.00 23.00 4.00 0.07 1.39 0.00 0.00 lobster – wine 5.70 12.00 21.00 0.00 0.07 1.16 0.33 3.21 money – laundering 5.65 21.00 0.00 0.05 1.16 0.00 0.00 Mars – scientist 5.63 4.00 20.00 0.00 0.06 1.07 0.09 0.71 decoration – valor 5.63 4.00 19.00 0.00 0.06 0.98 0.18 1.81 OPEC – country 5.63 2.00 24.00 5.00 0.08 1.52 0.35 3.34 summer – nature 5.63 6.00 22.00 0.00 0.07 1.27 0.22 1.81 tiger – fauna 5.62 12.00 27.00 2.00 0.12 2.08 0.55 4.32 psychology – discipline 5.58 4.00 28.00 6.00 0.22 2.37 0.77 6.01 glass – metal 5.56 16.00 26.00 3.00 0.09 1.86 0.40 3.21 alcohol – chemistry 5.54 2.00 20.00 0.00 0.06 1.07 0.00 0.00 disability – death 5.47 6.00 24.00 0.00 0.08 1.52 0.38 3.35 change – attitude 5.44 16.00 25.00 2.00 0.08 1.67 0.30 3.25"
    }, {
      "heading" : "Appendix J: The WordSimilarity-353 Test Collection",
      "text" : "J - 6\nWord Pair Gabr. ELKB WN Edges Hirst St.O.\nJiang Con.\nLea. Chod.\nLin Res.\narrangement – accommodation 5.41 14.00 27.00 4.00 0.08 2.08 0.43 4.92 territory – surface 5.34 6.00 25.00 2.00 0.16 1.67 0.56 3.39 credit – information 5.31 6.00 27.00 4.00 0.12 2.08 0.50 4.78 size – prominence 5.31 14.00 25.00 2.00 0.08 1.67 0.35 3.35 exhibit – memorabilia 5.31 4.00 21.00 0.00 0.05 1.16 0.24 2.88 territory – kilometer 5.28 4.00 22.00 0.00 0.06 1.27 0.00 0.00 death – row 5.25 6.00 24.00 0.00 0.06 1.52 0.22 2.25 man – governor 5.25 14.00 26.00 3.00 0.09 1.86 0.32 3.44 doctor – liability 5.19 6.00 21.00 0.00 0.06 1.16 0.00 0.00 impartiality – interest 5.16 6.00 22.00 0.00 0.05 1.27 0.00 0.00 energy – laboratory 5.09 14.00 20.00 0.00 0.06 1.07 0.00 0.00 secretary – senate 5.06 6.00 18.00 0.00 0.05 0.90 0.00 0.00 death – inmate 5.03 4.00 22.00 0.00 0.05 1.27 0.00 0.00 monk – oracle 5.00 12.00 23.00 0.00 0.06 1.39 0.23 2.45 cup – food 5.00 14.00 25.00 3.00 0.14 1.67 0.61 4.99 doctor – personnel 5.00 14.00 21.00 0.00 0.07 1.16 0.00 0.00 travel – activity 5.00 14.00 25.00 2.00 0.19 1.67 0.51 2.25 journal – association 4.97 4.00 24.00 0.00 0.05 1.52 0.22 2.65 car – flight 4.94 12.00 23.00 0.00 0.07 1.39 0.28 2.45 street – children 4.94 12.00 22.00 0.00 0.07 1.27 0.10 0.71 space – chemistry 4.88 6.00 26.00 3.00 0.06 1.86 0.23 2.88 situation – conclusion 4.81 4.00 25.00 0.00 0.10 1.67 0.32 2.25 tiger – organism 4.77 6.00 28.00 2.00 0.10 2.37 0.32 2.17 peace – plan 4.75 12.00 22.00 0.00 0.08 1.27 0.34 2.80 word – similarity 4.75 14.00 22.00 0.00 0.09 1.27 0.26 1.81 consumer – energy 4.75 6.00 21.00 0.00 0.06 1.16 0.00 0.00 ministry – culture 4.69 4.00 21.00 0.00 0.06 1.16 0.29 3.04 hospital – infrastructure 4.63 6.00 18.00 2.00 0.04 0.90 0.00 0.00 smart – student 4.62 6.00 18.00 0.00 0.05 0.90 0.00 0.00 investigation – effort 4.59 4.00 27.00 4.00 0.17 2.08 0.65 4.48 image – surface 4.56 16.00 26.00 3.00 0.11 1.86 0.37 3.39 life – term 4.50 14.00 28.00 5.00 0.11 2.37 0.48 3.72 computer – news 4.47 8.00 20.00 0.00 0.06 1.07 0.00 0.00 board – recommendation 4.47 6.00 17.00 0.00 0.06 0.83 0.00 0.00 start – match 4.47 6.00 24.00 4.00 0.09 1.52 0.42 3.78 lad – brother 4.46 14.00 26.00 3.00 0.07 1.86 0.27 2.45 food – rooster 4.42 12.00 18.00 0.00 0.06 0.90 0.09 0.71 coast – hill 4.38 4.00 26.00 2.00 0.15 1.86 0.69 6.36 observation – architecture 4.38 4.00 25.00 2.00 0.06 1.67 0.29 3.12 attempt – peace 4.25 8.00 24.00 0.00 0.06 1.52 0.00 0.00 deployment – departure 4.25 6.00 23.00 0.00 0.06 1.39 0.21 2.25 benchmark – index 4.25 12.00 27.00 4.00 0.07 2.08 0.50 6.15 consumer – confidence 4.13 4.00 21.00 0.00 0.05 1.16 0.00 0.00 start – year 4.06 6.00 27.00 4.00 0.11 2.08 0.49 3.72 focus – life 4.06 14.00 25.00 2.00 0.08 1.67 0.37 3.35 development – issue 3.97 12.00 27.00 4.00 0.16 2.08 0.61 4.18"
    }, {
      "heading" : "Appendix J: The WordSimilarity-353 Test Collection",
      "text" : "J - 7\nWord Pair Gabr. ELKB WN Edges Hirst St.O.\nJiang Con.\nLea. Chod.\nLin Res.\nday – summer 3.94 16.00 27.00 4.00 0.11 2.08 0.48 3.72 theater – history 3.91 6.00 24.00 0.00 0.06 1.52 0.00 0.00 situation – isolation 3.88 6.00 26.00 3.00 0.10 1.86 0.43 3.35 media – trading 3.88 16.00 23.00 0.00 0.05 1.39 0.21 2.25 profit – warning 3.88 8.00 20.00 0.00 0.06 1.07 0.18 1.81 chance – credibility 3.88 14.00 25.00 2.00 0.06 1.67 0.19 1.81 precedent – information 3.85 6.00 28.00 6.00 0.14 2.37 0.63 5.21 architecture – century 3.78 2.00 21.00 0.00 0.06 1.16 0.18 1.81 population – development 3.75 6.00 25.00 0.00 0.11 1.67 0.57 5.60 stock – live 3.73 16.00 0.00 cup – object 3.69 14.00 26.00 4.00 0.14 1.86 0.38 1.96 atmosphere – landscape 3.69 12.00 22.00 0.00 0.07 1.27 0.32 3.27 minority – peace 3.69 14.00 25.00 2.00 0.06 1.67 0.29 3.35 peace – atmosphere 3.69 6.00 26.00 3.00 0.08 1.86 0.41 4.26 morality – marriage 3.69 8.00 24.00 0.00 0.06 1.52 0.00 0.00 report – gain 3.63 4.00 22.00 0.00 0.08 1.27 0.23 1.81 music – project 3.63 14.00 26.00 3.00 0.10 1.86 0.41 3.12 seven – series 3.56 4.00 21.00 0.00 0.06 1.16 0.20 1.81 experience – music 3.47 12.00 23.00 0.00 0.08 1.39 0.35 2.89 school – center 3.44 16.00 28.00 2.00 0.12 2.37 0.59 5.33 announcement – production 3.38 4.00 23.00 0.00 0.08 1.39 0.33 2.88 five – month 3.38 4.00 23.00 0.00 0.08 1.39 0.35 2.97 money – operation 3.31 6.00 23.00 0.00 0.08 1.39 0.00 0.00 delay – news 3.31 8.00 22.00 0.00 0.07 1.27 0.21 1.81 morality – importance 3.31 2.00 26.00 3.00 0.13 1.86 0.57 4.23 governor – interview 3.25 4.00 18.00 0.00 0.05 0.90 0.00 0.00 practice – institution 3.19 16.00 28.00 6.00 0.21 2.37 0.82 8.52 century – nation 3.16 6.00 22.00 0.00 0.07 1.27 0.00 0.00 coast – forest 3.15 16.00 24.00 0.00 0.06 1.52 0.20 1.96 shore – woodland 3.08 6.00 25.00 2.00 0.06 1.67 0.21 1.96 drink – car 3.04 6.00 22.00 0.00 0.10 1.27 0.31 1.96 president – medal 3.00 4.00 16.00 0.00 0.05 0.76 0.00 0.00 prejudice – recognition 3.00 6.00 22.00 0.00 0.07 1.27 0.30 2.89 viewer – serial 2.97 4.00 20.00 0.00 0.06 1.07 0.23 2.45 Mars – water 2.94 12.00 23.00 0.00 0.07 1.39 0.24 1.96 peace – insurance 2.94 4.00 27.00 4.00 0.15 2.08 0.74 8.28 cup – artifact 2.92 0.00 27.00 5.00 0.15 2.08 0.45 2.45 media – gain 2.88 6.00 22.00 0.00 0.05 1.27 0.00 0.00 precedent – cognition 2.81 4.00 27.00 5.00 0.11 2.08 0.41 2.89 announcement – effort 2.75 4.00 20.00 0.00 0.06 1.07 0.00 0.00 crane – implement 2.69 0.00 26.00 3.00 0.09 1.86 0.39 3.44 line – insurance 2.69 6.00 26.00 3.00 0.11 1.86 0.51 4.79 drink – mother 2.65 16.00 25.00 2.00 0.09 1.67 0.34 3.21 opera – industry 2.63 6.00 18.00 0.00 0.06 0.90 0.18 1.81 volunteer – motto 2.56 0.00 17.00 0.00 0.04 0.83 0.00 0.00 listing – proximity 2.56 6.00 19.00 0.00 0.07 0.98 0.27 2.65 Arafat – Jackson 2.50"
    }, {
      "heading" : "Appendix J: The WordSimilarity-353 Test Collection",
      "text" : "J - 8\nWord Pair Gabr. ELKB WN Edges Hirst St.O.\nJiang Con.\nLea. Chod.\nLin Res.\nprecedent – collection 2.50 6.00 27.00 5.00 0.13 2.08 0.61 5.21 cup – article 2.40 6.00 26.00 3.00 0.55 1.86 0.95 7.52 sign – recess 2.38 8.00 26.00 5.00 0.07 1.86 0.25 2.45 problem – airport 2.38 4.00 20.00 0.00 0.05 1.07 0.00 0.00 reason – hypertension 2.31 4.00 23.00 0.00 0.06 1.39 0.37 4.26 direction – combination 2.25 6.00 24.00 0.00 0.08 1.52 0.27 3.12 Wednesday – news 2.22 4.00 18.00 0.00 0.07 0.90 0.21 1.81 cup – entity 2.15 0.00 25.00 3.00 0.13 1.67 0.16 0.71 cemetery – woodland 2.08 6.00 21.00 0.00 0.05 1.16 0.07 0.71 glass – magician 2.08 4.00 22.00 0.00 0.05 1.27 0.08 0.71 possibility – girl 1.94 6.00 22.00 0.00 0.06 1.27 0.00 0.00 cup – substance 1.92 6.00 25.00 2.00 0.13 1.67 0.44 3.21 forest – graveyard 1.85 6.00 21.00 0.00 0.05 1.16 0.07 0.71 stock – egg 1.81 14.00 24.00 2.00 0.10 1.52 0.53 4.99 energy – secretary 1.81 4.00 20.00 0.00 0.06 1.07 0.00 0.00 month – hotel 1.81 0.00 20.00 0.00 0.06 1.07 0.00 0.00 precedent – group 1.77 6.00 26.00 4.00 0.10 1.86 0.35 2.46 production – hike 1.75 2.00 23.00 0.00 0.07 1.39 0.24 2.25 stock – phone 1.62 12.00 24.00 2.00 0.09 1.52 0.41 4.29 holy – sex 1.62 6.00 22.00 0.00 0.05 1.27 0.00 0.00 stock – CD 1.31 25.00 2.00 0.07 1.67 0.40 4.29 drink – ear 1.31 6.00 23.00 0.00 0.07 1.39 0.21 1.96 delay – racism 1.19 4.00 24.00 0.00 0.05 1.52 0.21 2.25 stock – jaguar 0.92 12.00 25.00 2.00 0.09 1.67 0.52 5.47 stock – life 0.92 12.00 24.00 2.00 0.09 1.52 0.39 3.35 monk – slave 0.92 6.00 26.00 3.00 0.06 1.86 0.25 2.45 lad – wizard 0.92 4.00 26.00 3.00 0.07 1.86 0.27 2.45 sugar – approach 0.88 6.00 23.00 0.00 0.07 1.39 0.24 1.96 rooster – voyage 0.62 2.00 13.00 0.00 0.04 0.58 0.00 0.00 chord – smile 0.54 0.00 20.00 0.00 0.07 1.07 0.29 2.88 noon – string 0.54 6.00 19.00 0.00 0.05 0.98 0.00 0.00 professor – cucumber 0.31 0.00 18.00 0.00 0.05 0.90 0.19 2.17 king – cabbage 0.23 12.00 21.00 0.00 0.06 1.16 0.27 3.21 Correlation 1.00 0.54 0.27 0.34 0.35 0.36 0.36 0.37\nTable J1: Comparison of semantic similarity measures using the WordSimilarity-353 Test Collection"
    }, {
      "heading" : "Appendix K: TOEFL, ESL and RDWP questions",
      "text" : "K - 1\nAppendix K: TOEFL, ESL and RDWP questions\nThis appendix presents 80 TOEFL (ETS, 2003), 50 ESL (Tatsuki, 1998) and 100 RDWP questions (Turney, 2001; Lewis 2000-2001) as well as the answers given by my system using the ELKB and the WordNet-based system which uses the Hirst and St-Onge (1998) measure. The WordNet-based system is implemented using the Semantic Distance software package (Pedersen, 2002). Tad Stach collected the other 200 RDWP questions from the following Canadian issues of Reader’s Digest (Lewis 2000-2001): January, March, April, May, June, August and September 2000; January and May 2001.\n1 Semantic Distance measured using the ELKB 1.A. 80 TOEFL Questions Question 1 enormously | tremendously | appropriately | uniquely | decidedly enormously ADV. [enormously] to tremendously ADV. [tremendously], length = 4, 1 path(s) of this length enormously ADV. [enormously] to appropriately ADV. [appropriately], length = 14, 1 path(s) of this length uniquely is NOT IN THE INDEX enormously ADV. to decidedly ADV., length = 4, 1 path(s) of this length Roget thinks that enormously means tremendously CORRECT Question 2 provisions | stipulations | interrelations | jurisdictions | interpretations provisions N. [provisions] to stipulations N. [stipulations], length = 0, 1 path(s) of this length provisions N. [provisions] to interrelations N. [interrelations], length = 14, 3 path(s) of this length provisions N. [provisions] to jurisdictions N. [jurisdictions], length = 12, 3 path(s) of this length provisions N. to interpretations N., length = 12, 36 path(s) of this length Roget thinks that provisions means stipulations CORRECT Question 3 haphazardly | randomly | dangerously | densely | linearly haphazardly ADV. [haphazardly] to randomly ADV. [randomly], length = 0, 1 path(s) of this length haphazardly ADV. [haphazardly] to dangerously VB. [dangerously], length = 12, 1 path(s) of this length haphazardly ADV. [haphazardly] to densely ADJ. [densely], length = 16, 2 path(s) of this length linearly is NOT IN THE INDEX Roget thinks that haphazardly means randomly CORRECT Question 4 prominent | conspicuous | battered | ancient | mysterious prominent ADJ. [prominent] to conspicuous ADJ. [conspicuous], length = 0, 4 path(s) of this length prominent ADJ. [prominent] to battered ADJ. [battered], length = 10, 10 path(s) of this length prominent ADJ. [prominent] to ancient ADJ. [ancient], length = 4, 1 path(s) of this length prominent ADJ. to mysterious ADJ., length = 8, 2 path(s) of this length Roget thinks that prominent means conspicuous CORRECT Question 5 zenith | pinnacle | completion | outset | decline zenith N. [zenith] to pinnacle N. [pinnacle], length = 0, 3 path(s) of"
    }, {
      "heading" : "Appendix K: TOEFL, ESL and RDWP questions",
      "text" : "K - 2\nthis length zenith N. [zenith] to completion N. [completion], length = 0, 4 path(s) of this length zenith N. [zenith] to outset N. [outset], length = 8, 1 path(s) of this length zenith N. to decline N., length = 8, 3 path(s) of this length Roget thinks that zenith means completion TIE LOST INCORRECT Question 6 flawed | imperfect | tiny | lustrous | crude flawed ADJ. [flawed] to imperfect ADJ. [imperfect], length = 0, 5 path(s) of this length flawed N. [flawed] to tiny ADJ. [tiny], length = 12, 2 path(s) of this length flawed N. [flawed] to lustrous ADJ. [lustrous], length = 12, 3 path(s) of this length flawed ADJ. to crude ADJ., length = 2, 2 path(s) of this length Roget thinks that flawed means imperfect CORRECT Question 7 urgently | desperately | typically | conceivably | tentatively urgently ADV. [urgently] to desperately ADV. [desperately], length = 16, 1 path(s) of this length typically is NOT IN THE INDEX urgently ADV. [urgently] to conceivably ADV. [conceivably], length = 16, 1 path(s) of this length tentatively is NOT IN THE INDEX Roget thinks that urgently means desperately CORRECT Question 8 consumed | eaten | bred | caught | supplied consumed VB. [consumed] to eaten VB. [eaten], length = 0, 6 path(s) of this length consumed VB. [consumed] to bred VB. [bred], length = 8, 6 path(s) of this length consumed VB. [consumed] to caught VB. [caught], length = 0, 1 path(s) of this length consumed VB. to supplied N., length = 8, 9 path(s) of this length Roget thinks that consumed means eaten TIE BROKEN CORRECT Question 9 advent | coming | arrest | financing | stability advent N. [advent] to coming N. [coming], length = 0, 5 path(s) of this length advent N. [advent] to arrest N. [arrest], length = 12, 2 path(s) of this length advent N. [advent] to financing VB. [financing], length = 16, 28 path(s) of this length advent N. to stability N., length = 10, 5 path(s) of this length Roget thinks that advent means coming CORRECT Question 10 concisely | succinctly | powerfully | positively | freely succinctly (ANSWER) is NOT IN THE INDEX concisely ADV. [concisely] to powerfully ADV. [powerfully], length = 16, 6 path(s) of this length concisely ADV. [concisely] to positively ADV. [positively], length = 12, 4 path(s) of this length concisely ADV. to freely VB., length = 12, 6 path(s) of this length Roget thinks that concisely means freely INCORRECT Question 11 salutes | greetings | information | ceremonies | privileges salutes N. [salutes] to greetings N. [greetings], length = 0, 3 path(s) of this length salutes VB. [salutes] to information N. [information], length = 6, 1 path(s) of this length salutes N. [salutes] to ceremonies N. [ceremonies], length = 0, 1 path(s) of this length salutes VB. to privileges N., length = 12, 26 path(s) of this length Roget thinks that salutes means greetings TIE BROKEN CORRECT"
    }, {
      "heading" : "Appendix K: TOEFL, ESL and RDWP questions",
      "text" : "K - 3\nQuestion 12 solitary | alone | alert | restless | fearless solitary ADJ. [solitary] to alone ADJ. [alone], length = 0, 6 path(s) of this length solitary N. [solitary] to alert ADJ. [alert], length = 12, 10 path(s) of this length solitary N. [solitary] to restless ADJ. [restless], length = 8, 1 path(s) of this length solitary N. to fearless ADJ., length = 14, 9 path(s) of this length Roget thinks that solitary means alone CORRECT Question 13 hasten | accelerate | permit | determine | accompany hasten VB. [hasten] to accelerate VB. [accelerate], length = 2, 5 path(s) of this length hasten VB. [hasten] to permit N. [permit], length = 10, 6 path(s) of this length hasten VB. [hasten] to determine VB. [determine], length = 2, 1 path(s) of this length hasten VB. to accompany VB., length = 10, 12 path(s) of this length Roget thinks that hasten means accelerate TIE BROKEN CORRECT Question 14 perseverance | endurance | skill | generosity | disturbance perseverance N. [perseverance] to endurance N. [endurance], length = 2, 4 path(s) of this length perseverance N. [perseverance] to skill N. [skill], length = 10, 16 path(s) of this length perseverance N. [perseverance] to generosity N. [generosity], length = 14, 2 path(s) of this length perseverance N. to disturbance N., length = 14, 11 path(s) of this length Roget thinks that perseverance means endurance CORRECT Question 15 fanciful | imaginative | familiar | apparent | logical fanciful ADJ. [fanciful] to imaginative ADJ. [imaginative], length = 0, 2 path(s) of this length fanciful ADJ. [fanciful] to familiar ADJ. [familiar], length = 10, 10 path(s) of this length fanciful ADJ. [fanciful] to apparent ADJ. [apparent], length = 12, 13 path(s) of this length fanciful ADJ. to logical ADJ., length = 10, 32 path(s) of this length Roget thinks that fanciful means imaginative CORRECT Question 16 showed | demonstrated | published | repeated | postponed showed VB. [showed] to demonstrated VB. [demonstrated], length = 0, 15 path(s) of this length showed VB. [showed] to published VB. [published], length = 0, 7 path(s) of this length showed N. [showed] to repeated N. [repeated], length = 0, 2 path(s) of this length showed VB. to postponed VB., length = 12, 52 path(s) of this length Roget thinks that showed means demonstrated TIE BROKEN CORRECT Question 17 constantly | continually | instantly | rapidly | accidentally constantly ADV. [constantly] to continually ADV. [continually], length = 0, 1 path(s) of this length constantly ADV. [constantly] to instantly ADV. [instantly], length = 8, 1 path(s) of this length constantly ADV. [constantly] to rapidly ADV. [rapidly], length = 16, 2 path(s) of this length constantly ADV. to accidentally ADV., length = 14, 2 path(s) of this length Roget thinks that constantly means continually CORRECT Question 18 issues | subjects | training | salaries | benefits issues N. [issues] to subjects N. [subjects], length = 0, 1 path(s) of this length"
    }, {
      "heading" : "Appendix K: TOEFL, ESL and RDWP questions",
      "text" : "K - 4\nissues N. [issues] to training N. [training], length = 10, 149 path(s) of this length issues N. [issues] to salaries N. [salaries], length = 10, 14 path(s) of this length issues VB. to benefits N., length = 10, 62 path(s) of this length Roget thinks that issues means subjects CORRECT Question 19 furnish | supply | impress | protect | advise furnish VB. [furnish] to supply VB. [supply], length = 0, 2 path(s) of this length furnish VB. [furnish] to impress VB. [impress], length = 10, 15 path(s) of this length furnish VB. [furnish] to protect VB. [protect], length = 10, 6 path(s) of this length furnish VB. to advise VB., length = 10, 10 path(s) of this length Roget thinks that furnish means supply CORRECT Question 20 costly | expensive | beautiful | popular | complicated costly ADJ. [costly] to expensive ADJ. [expensive], length = 0, 2 path(s) of this length costly ADJ. [costly] to beautiful ADJ. [beautiful], length = 4, 1 path(s) of this length costly ADJ. [costly] to popular N. [popular], length = 8, 1 path(s) of this length costly ADJ. to complicated VB., length = 12, 2 path(s) of this length Roget thinks that costly means expensive CORRECT Question 21 recognized | acknowledged | successful | depicted | welcomed recognized VB. [recognized] to acknowledged VB. [acknowledged], length = 0, 5 path(s) of this length recognized VB. [recognized] to successful ADJ. [successful], length = 10, 91 path(s) of this length recognized VB. [recognized] to depicted VB. [depicted], length = 12, 16 path(s) of this length recognized VB. to welcomed VB., length = 0, 1 path(s) of this length Roget thinks that recognized means acknowledged TIE BROKEN CORRECT Question 22 spot | location | climate | latitude | sea spot N. [spot] to location N. [location], length = 2, 1 path(s) of this length spot VB. [spot] to climate N. [climate], length = 8, 2 path(s) of this length spot N. [spot] to latitude N. [latitude], length = 4, 1 path(s) of this length spot ADJ. to sea ADJ., length = 8, 6 path(s) of this length Roget thinks that spot means location CORRECT Question 23 make | earn | print | trade | borrow make VB. [make] to earn VB. [earn], length = 2, 5 path(s) of this length make VB. [make] to print VB. [print], length = 2, 9 path(s) of this length make VB. [make] to trade VB. [trade], length = 0, 5 path(s) of this length make VB. to borrow VB., length = 2, 9 path(s) of this length Roget thinks that make means trade INCORRECT Question 24 often | frequently | definitely | chemically | hardly often ADV. [often] to frequently ADV. [frequently], length = 0, 3 path(s) of this length often ADV. [often] to definitely ADV. [definitely], length = 14, 5 path(s) of this length chemically is NOT IN THE INDEX often ADV. to hardly ADV., length = 2, 1 path(s) of this length Roget thinks that often means frequently CORRECT Question 25 easygoing | relaxed | frontier | boring | farming easygoing ADJ. [easygoing] to relaxed VB. [relaxed], length = 6, 1"
    }, {
      "heading" : "Appendix K: TOEFL, ESL and RDWP questions",
      "text" : "K - 5\npath(s) of this length easygoing ADJ. [easygoing] to frontier N. [frontier], length = 16, 24 path(s) of this length easygoing ADJ. [easygoing] to boring ADJ. [boring], length = 8, 1 path(s) of this length easygoing ADJ. to farming N., length = 12, 2 path(s) of this length Roget thinks that easygoing means relaxed CORRECT Question 26 debate | argument | war | election | competition debate N. [debate] to argument N. [argument], length = 0, 4 path(s) of this length debate N. [debate] to war N. [war], length = 2, 3 path(s) of this length debate VB. [debate] to election N. [election], length = 10, 11 path(s) of this length debate N. to competition N., length = 2, 2 path(s) of this length Roget thinks that debate means argument CORRECT Question 27 narrow | thin | clear | freezing | poisonous narrow VB. [narrow] to thin VB. [thin], length = 0, 10 path(s) of this length narrow N. [narrow] to clear N. [clear], length = 0, 2 path(s) of this length narrow VB. [narrow] to freezing N. [freezing], length = 6, 4 path(s) of this length narrow N. to poisonous ADJ., length = 6, 2 path(s) of this length Roget thinks that narrow means thin TIE BROKEN CORRECT Question 28 arranged | planned | explained | studied | discarded arranged VB. [arranged] to planned VB. [planned], length = 0, 6 path(s) of this length arranged VB. [arranged] to explained VB. [explained], length = 4, 1 path(s) of this length arranged VB. [arranged] to studied VB. [studied], length = 4, 1 path(s) of this length arranged ADJ. to discarded N., length = 10, 30 path(s) of this length Roget thinks that arranged means planned CORRECT Question 29 infinite | limitless | relative | unusual | structural infinite ADJ. [infinite] to limitless ADJ. [limitless], length = 0, 2 path(s) of this length infinite ADJ. [infinite] to relative ADJ. [relative], length = 12, 6 path(s) of this length infinite ADJ. [infinite] to unusual ADJ. [unusual], length = 4, 1 path(s) of this length infinite ADJ. to structural ADJ., length = 12, 2 path(s) of this length Roget thinks that infinite means limitless CORRECT Question 30 showy | striking | prickly | entertaining | incidental showy ADJ. [showy] to striking ADJ. [striking], length = 2, 1 path(s) of this length showy ADJ. [showy] to prickly N. [prickly], length = 10, 4 path(s) of this length showy ADJ. [showy] to entertaining VB. [entertaining], length = 6, 2 path(s) of this length showy ADJ. to incidental N., length = 10, 2 path(s) of this length Roget thinks that showy means striking CORRECT Question 31 levied | imposed | believed | requested | correlated levied VB. [levied] to imposed VB. [imposed], length = 4, 1 path(s) of this length levied VB. [levied] to believed VB. [believed], length = 12, 48 path(s) of this length levied VB. [levied] to requested VB. [requested], length = 0, 5 path(s) of this length levied VB. to correlated VB., length = 12, 9 path(s) of this length Roget thinks that levied means requested INCORRECT Question 32"
    }, {
      "heading" : "Appendix K: TOEFL, ESL and RDWP questions",
      "text" : "K - 6\ndeftly | skillfully | prudently | occasionally | humorously deftly (PROBLEM) not found in the index!! Question 33 distribute | circulate | commercialize | research | acknowledge distribute VB. [distribute] to circulate VB. [circulate], length = 0, 2 path(s) of this length distribute VB. [distribute] to commercialize VB. [commercialize], length = 12, 1 path(s) of this length distribute VB. [distribute] to research N. [research], length = 12, 13 path(s) of this length distribute VB. to acknowledge VB., length = 10, 8 path(s) of this length Roget thinks that distribute means circulate CORRECT Question 34 discrepancies | differences | weights | deposits | wavelengths discrepancies N. [discrepancies] to differences N. [differences], length = 0, 2 path(s) of this length discrepancies N. [discrepancies] to weights VB. [weights], length = 14, 14 path(s) of this length discrepancies N. [discrepancies] to deposits N. [deposits], length = 14, 4 path(s) of this length discrepancies N. to wavelengths N., length = 16, 12 path(s) of this length Roget thinks that discrepancies means differences CORRECT Question 35 prolific | productive | serious | capable | promising prolific ADJ. [prolific] to productive ADJ. [productive], length = 0, 5 path(s) of this length prolific ADJ. [prolific] to serious ADJ. [serious], length = 10, 20 path(s) of this length prolific ADJ. [prolific] to capable ADJ. [capable], length = 12, 6 path(s) of this length prolific ADJ. to promising N., length = 10, 27 path(s) of this length Roget thinks that prolific means productive CORRECT Question 36 unmatched | unequaled | unrecognized | alienated | emulated unmatched ADJ. [unmatched] to unequaled ADJ. [unequaled], length = 2, 1 path(s) of this length unmatched ADJ. [unmatched] to unrecognized ADJ. [unrecognized], length = 16, 6 path(s) of this length unmatched ADJ. [unmatched] to alienated VB. [alienated], length = 14, 2 path(s) of this length unmatched ADJ. to emulated ADJ., length = 2, 1 path(s) of this length Roget thinks that unmatched means unequaled CORRECT Question 37 peculiarly | uniquely | partly | patriotically | suspiciously uniquely (ANSWER) is NOT IN THE INDEX peculiarly ADV. [peculiarly] to partly ADV. [partly], length = 8, 1 path(s) of this length patriotically is NOT IN THE INDEX suspiciously is NOT IN THE INDEX Roget thinks that peculiarly means partly INCORRECT Question 38 hue | color | glare | contrast | scent hue N. [hue] to color N. [color], length = 2, 4 path(s) of this length hue N. [hue] to glare VB. [glare], length = 10, 6 path(s) of this length hue N. [hue] to contrast N. [contrast], length = 10, 1 path(s) of this length hue N. to scent VB., length = 10, 4 path(s) of this length Roget thinks that hue means color CORRECT Question 39 hind | rear | curved | muscular |hairy hind ADJ. [hind] to rear ADJ. [rear], length = 2, 1 path(s) of this length hind N. [hind] to curved N. [curved], length = 4, 1 path(s) of this length hind N. [hind] to muscular N. [muscular], length = 14, 2 path(s) of this length hind ADJ. to hairy ADJ., length = 12, 2 path(s) of this length Roget thinks that hind means rear"
    }, {
      "heading" : "Appendix K: TOEFL, ESL and RDWP questions",
      "text" : "K - 7\nCORRECT Question 40 highlight | accentuate | alter | imitate | restore highlight VB. [highlight] to accentuate VB. [accentuate], length = 2, 2 path(s) of this length highlight VB. [highlight] to alter N. [alter], length = 12, 10 path(s) of this length highlight VB. [highlight] to imitate VB. [imitate], length = 4, 2 path(s) of this length highlight N. to restore VB., length = 10, 19 path(s) of this length Roget thinks that highlight means accentuate CORRECT Question 41 hastily | hurriedly | shrewdly | habitually | chronologically hastily ADV. [hastily] to hurriedly ADV. [hurriedly], length = 0, 1 path(s) of this length hastily ADV. [hastily] to shrewdly ADV. [shrewdly], length = 14, 2 path(s) of this length hastily ADV. [hastily] to habitually ADV. [habitually], length = 14, 2 path(s) of this length chronologically is NOT IN THE INDEX Roget thinks that hastily means hurriedly CORRECT Question 42 temperate | mild | cold | short | windy temperate ADJ. [temperate] to mild ADJ. [mild], length = 0, 3 path(s) of this length temperate ADJ. [temperate] to cold ADJ. [cold], length = 2, 4 path(s) of this length temperate ADJ. [temperate] to short N. [short], length = 2, 1 path(s) of this length temperate VB. to windy ADJ., length = 8, 2 path(s) of this length Roget thinks that temperate means mild CORRECT Question 43 grin | smile | exercise | rest | joke grin N. [grin] to smile N. [smile], length = 0, 4 path(s) of this length grin N. [grin] to exercise VB. [exercise], length = 10, 4 path(s) of this length grin N. [grin] to rest VB. [rest], length = 10, 16 path(s) of this length grin N. to joke N., length = 2, 2 path(s) of this length Roget thinks that grin means smile CORRECT Question 44 verbally | orally | overtly | fittingly | verbosely orally (ANSWER) is NOT IN THE INDEX overtly is NOT IN THE INDEX verbally ADV. [verbally] to fittingly ADV. [fittingly], length = 16, 4 path(s) of this length verbosely is NOT IN THE INDEX Roget thinks that verbally means fittingly INCORRECT Question 45 physician | doctor | chemist | pharmacist | nurse physician N. [physician] to doctor N. [doctor], length = 0, 3 path(s) of this length physician N. [physician] to chemist N. [chemist], length = 4, 1 path(s) of this length physician N. [physician] to pharmacist N. [pharmacist], length = 4, 1 path(s) of this length physician N. to nurse VB., length = 2, 2 path(s) of this length Roget thinks that physician means doctor CORRECT Question 46 essentially | basically | possibly | eagerly | ordinarily essentially ADV. [essentially] to basically ADV. [basically], length = 16, 5 path(s) of this length essentially ADV. [essentially] to possibly ADV. [possibly], length = 2, 1 path(s) of this length essentially ADV. [essentially] to eagerly ADV. [eagerly], length = 16, 5 path(s) of this length"
    }, {
      "heading" : "Appendix K: TOEFL, ESL and RDWP questions",
      "text" : "K - 8\nordinarily is NOT IN THE INDEX Roget thinks that essentially means possibly INCORRECT Question 47 keen | sharp | useful | simple | famous keen ADJ. [keen] to sharp ADJ. [sharp], length = 0, 12 path(s) of this length keen ADJ. [keen] to useful N. [useful], length = 10, 6 path(s) of this length keen ADJ. [keen] to simple ADJ. [simple], length = 10, 43 path(s) of this length keen ADJ. to famous ADJ., length = 12, 9 path(s) of this length Roget thinks that keen means sharp CORRECT Question 48 situated | positioned | rotating | isolated | emptying situated ADJ. [situated] to positioned ADJ. [positioned], length = 0, 2 path(s) of this length situated VB. [situated] to rotating VB. [rotating], length = 14, 51 path(s) of this length situated VB. [situated] to isolated N. [isolated], length = 6, 1 path(s) of this length situated VB. to emptying VB., length = 8, 4 path(s) of this length Roget thinks that situated means positioned CORRECT Question 49 principal | major | most | numerous | exceptional principal N. [principal] to major N. [major], length = 0, 7 path(s) of this length principal N. [principal] to most ADJ. [most], length = 6, 2 path(s) of this length principal N. [principal] to numerous ADJ. [numerous], length = 14, 4 path(s) of this length principal N. to exceptional ADJ., length = 6, 1 path(s) of this length Roget thinks that principal means major CORRECT Question 50 slowly | gradually | rarely | effectively | continuously slowly ADV. [slowly] to gradually ADV. [gradually], length = 4, 1 path(s) of this length slowly VB. [slowly] to rarely ADV. [rarely], length = 12, 4 path(s) of this length effectively is NOT IN THE INDEX slowly VB. to continuously ADV., length = 10, 8 path(s) of this length Roget thinks that slowly means gradually CORRECT Question 51 built | constructed | proposed | financed | organized built VB. [built] to constructed VB. [constructed], length = 0, 5 path(s) of this length built ADJ. [built] to proposed VB. [proposed], length = 10, 22 path(s) of this length built VB. [built] to financed VB. [financed], length = 10, 1 path(s) of this length built VB. to organized VB., length = 0, 2 path(s) of this length Roget thinks that built means constructed TIE BROKEN CORRECT Question 52 tasks | jobs | customers | materials | shops tasks N. [tasks] to jobs N. [jobs], length = 0, 9 path(s) of this length tasks N. [tasks] to customers N. [customers], length = 10, 21 path(s) of this length tasks N. [tasks] to materials N. [materials], length = 10, 60 path(s) of this length tasks N. to shops VB., length = 6, 2 path(s) of this length Roget thinks that tasks means jobs CORRECT Question 53 unlikely | improbable | disagreeable | different | unpopular unlikely ADJ. [unlikely] to improbable ADJ. [improbable], length = 0, 1 path(s) of this length unlikely VB. [unlikely] to disagreeable ADJ. [disagreeable], length = 16,"
    }, {
      "heading" : "Appendix K: TOEFL, ESL and RDWP questions",
      "text" : "K - 9\n30 path(s) of this length unlikely VB. [unlikely] to different N. [different], length = 12, 5 path(s) of this length unlikely VB. to unpopular ADJ., length = 16, 48 path(s) of this length Roget thinks that unlikely means improbable CORRECT Question 54 halfheartedly | apathetically | customarily | bipartisanly | unconventionally halfheartedly (PROBLEM) not found in the index!! Question 55 annals | chronicles | homes | trails | songs annals N. [annals] to chronicles N. [chronicles], length = 0, 3 path(s) of this length annals N. [annals] to homes VB. [homes], length = 12, 36 path(s) of this length annals N. [annals] to trails N. [trails], length = 4, 1 path(s) of this length annals N. to songs N., length = 12, 6 path(s) of this length Roget thinks that annals means chronicles CORRECT Question 56 wildly | furiously | distinctively | mysteriously | abruptly wildly VB. [wildly] to furiously ADV. [furiously], length = 16, 3 path(s) of this length distinctively is NOT IN THE INDEX mysteriously is NOT IN THE INDEX wildly VB. to abruptly ADV., length = 12, 1 path(s) of this length Roget thinks that wildly means abruptly INCORRECT Question 57 hailed | acclaimed | judged | remembered | addressed hailed VB. [hailed] to acclaimed VB. [acclaimed], length = 0, 2 path(s) of this length hailed N. [hailed] to judged VB. [judged], length = 12, 62 path(s) of this length hailed N. [hailed] to remembered VB. [remembered], length = 12, 46 path(s) of this length hailed N. to addressed N., length = 0, 2 path(s) of this length Roget thinks that hailed means acclaimed CORRECT Question 58 command | mastery | observation | love | awareness command N. [command] to mastery N. [mastery], length = 2, 1 path(s) of this length command VB. [command] to observation N. [observation], length = 6, 2 path(s) of this length command N. [command] to love VB. [love], length = 2, 7 path(s) of this length command VB. to awareness N., length = 12, 39 path(s) of this length Roget thinks that command means love TIE LOST INCORRECT Question 59 concocted | devised | cleaned | requested | supervised concocted VB. [concocted] to devised VB. [devised], length = 0, 3 path(s) of this length concocted VB. [concocted] to cleaned VB. [cleaned], length = 8, 2 path(s) of this length concocted VB. [concocted] to requested VB. [requested], length = 10, 76 path(s) of this length concocted VB. to supervised VB., length = 14, 2 path(s) of this length Roget thinks that concocted means devised CORRECT Question 60 prospective | potential | particular | prudent | prominent prospective ADJ. [prospective] to potential ADJ. [potential], length = 2, 1 path(s) of this length prospective N. [prospective] to particular N. [particular], length = 10, 14 path(s) of this length prospective ADJ. [prospective] to prudent ADJ. [prudent], length = 2, 1 path(s) of this length prospective ADJ. to prominent ADJ., length = 12, 3 path(s) of this length"
    }, {
      "heading" : "Appendix K: TOEFL, ESL and RDWP questions",
      "text" : "K - 10\nRoget thinks that prospective means potential CORRECT Question 61 generally | broadly | descriptively | controversially | accurately broadly (ANSWER) is NOT IN THE INDEX descriptively is NOT IN THE INDEX controversially is NOT IN THE INDEX generally ADV. to accurately ADV., length = 16, 5 path(s) of this length Roget thinks that generally means accurately INCORRECT Question 62 sustained | prolonged | refined | lowered | analyzed sustained VB. [sustained] to prolonged VB. [prolonged], length = 0, 3 path(s) of this length sustained VB. [sustained] to refined N. [refined], length = 6, 1 path(s) of this length sustained VB. [sustained] to lowered VB. [lowered], length = 10, 46 path(s) of this length analyzed is NOT IN THE INDEX Roget thinks that sustained means prolonged CORRECT Question 63 perilous | dangerous | binding | exciting | offensive perilous ADJ. [perilous] to dangerous ADJ. [dangerous], length = 0, 1 path(s) of this length perilous ADJ. [perilous] to binding VB. [binding], length = 10, 2 path(s) of this length perilous ADJ. [perilous] to exciting VB. [exciting], length = 14, 3 path(s) of this length perilous ADJ. to offensive ADJ., length = 10, 1 path(s) of this length Roget thinks that perilous means dangerous CORRECT Question 64 tranquillity | peacefulness | harshness | weariness | happiness tranquillity N. [tranquillity] to peacefulness N. [peacefulness], length = 0, 1 path(s) of this length tranquillity N. [tranquillity] to harshness N. [harshness], length = 8, 1 path(s) of this length tranquillity N. [tranquillity] to weariness N. [weariness], length = 8, 1 path(s) of this length tranquillity N. to happiness N., length = 10, 8 path(s) of this length Roget thinks that tranquillity means peacefulness CORRECT Question 65 dissipate | disperse | isolate | disguise | photograph dissipate VB. [dissipate] to disperse VB. [disperse], length = 0, 5 path(s) of this length dissipate VB. [dissipate] to isolate VB. [isolate], length = 10, 1 path(s) of this length dissipate VB. [dissipate] to disguise N. [disguise], length = 10, 1 path(s) of this length dissipate VB. to photograph N., length = 14, 4 path(s) of this length Roget thinks that dissipate means disperse CORRECT Question 66 primarily | chiefly | occasionally | cautiously | consistently chiefly (ANSWER) is NOT IN THE INDEX primarily ADV. [primarily] to occasionally ADV. [occasionally], length = 10, 1 path(s) of this length primarily ADV. [primarily] to cautiously ADV. [cautiously], length = 16, 3 path(s) of this length primarily ADV. to consistently ADJ., length = 12, 1 path(s) of this length Roget thinks that primarily means occasionally INCORRECT Question 67 colloquial | conversational | recorded | misunderstood | incorrect colloquial ADJ. [colloquial] to conversational ADJ. [conversational], length = 12, 8 path(s) of this length colloquial ADJ. [colloquial] to recorded VB. [recorded], length = 12, 128 path(s) of this length"
    }, {
      "heading" : "Appendix K: TOEFL, ESL and RDWP questions",
      "text" : "K - 11\ncolloquial ADJ. [colloquial] to misunderstood VB. [misunderstood], length = 12, 16 path(s) of this length colloquial ADJ. to incorrect ADJ., length = 12, 12 path(s) of this length Roget thinks that colloquial means recorded INCORRECT Question 68 resolved | settled | publicized | forgotten | examined resolved N. [resolved] to settled N. [settled], length = 0, 3 path(s) of this length resolved VB. [resolved] to publicized VB. [publicized], length = 12, 6 path(s) of this length resolved VB. [resolved] to forgotten N. [forgotten], length = 10, 24 path(s) of this length resolved VB. to examined VB., length = 12, 5 path(s) of this length Roget thinks that resolved means settled CORRECT Question 69 feasible | possible | permitted | equitable | evident feasible ADJ. [feasible] to possible ADJ. [possible], length = 0, 3 path(s) of this length feasible ADJ. [feasible] to permitted ADJ. [permitted], length = 2, 1 path(s) of this length feasible ADJ. [feasible] to equitable ADJ. [equitable], length = 16, 9 path(s) of this length feasible N. to evident ADJ., length = 12, 6 path(s) of this length Roget thinks that feasible means possible CORRECT Question 70 expeditiously | rapidly | frequently | actually | repeatedly expeditiously (PROBLEM) not found in the index!! Question 71 percentage | proportion | volume | sample | profit percentage N. [percentage] to proportion N. [proportion], length = 2, 2 path(s) of this length percentage N. [percentage] to volume N. [volume], length = 4, 1 path(s) of this length percentage N. [percentage] to sample N. [sample], length = 2, 1 path(s) of this length percentage N. to profit N., length = 2, 1 path(s) of this length Roget thinks that percentage means proportion TIE BROKEN CORRECT Question 72 terminated | ended | posed | postponed | evaluated terminated VB. [terminated] to ended VB. [ended], length = 0, 6 path(s) of this length terminated VB. [terminated] to posed VB. [posed], length = 14, 15 path(s) of this length terminated VB. [terminated] to postponed VB. [postponed], length = 12, 1 path(s) of this length terminated ADJ. to evaluated VB., length = 12, 2 path(s) of this length Roget thinks that terminated means ended CORRECT Question 73 uniform | alike | hard | complex | sharp uniform ADJ. [uniform] to alike ADJ. [alike], length = 2, 1 path(s) of this length uniform N. [uniform] to hard N. [hard], length = 4, 2 path(s) of this length uniform ADJ. [uniform] to complex N. [complex], length = 6, 5 path(s) of this length uniform ADJ. to sharp N., length = 6, 3 path(s) of this length Roget thinks that uniform means alike CORRECT Question 74 figure | solve | list | divide | express figure VB. [figure] to solve VB. [solve], length = 12, 10 path(s) of this length figure N. [figure] to list N. [list], length = 2, 4 path(s) of this length figure VB. [figure] to divide VB. [divide], length = 2, 1 path(s) of this length figure N. to express VB., length = 2, 2 path(s) of this length"
    }, {
      "heading" : "Appendix K: TOEFL, ESL and RDWP questions",
      "text" : "K - 12\nRoget thinks that figure means list INCORRECT Question 75 sufficient | enough | recent | physiological | valuable sufficient ADJ. [sufficient] to enough ADJ. [enough], length = 2, 1 path(s) of this length sufficient N. [sufficient] to recent ADJ. [recent], length = 14, 15 path(s) of this length sufficient ADJ. [sufficient] to physiological ADJ. [physiological], length = 16, 8 path(s) of this length sufficient ADJ. to valuable ADJ., length = 4, 2 path(s) of this length Roget thinks that sufficient means enough CORRECT Question 76 fashion | manner | ration | fathom | craze fashion N. [fashion] to manner N. [manner], length = 0, 5 path(s) of this length fashion N. [fashion] to ration VB. [ration], length = 10, 11 path(s) of this length fashion N. [fashion] to fathom VB. [fathom], length = 12, 22 path(s) of this length fashion N. to craze N., length = 0, 3 path(s) of this length Roget thinks that fashion means manner TIE BROKEN CORRECT Question 77 marketed | sold | frozen | sweetened | diluted marketed N. [marketed] to sold N. [sold], length = 0, 5 path(s) of this length marketed N. [marketed] to frozen N. [frozen], length = 10, 108 path(s) of this length marketed N. [marketed] to sweetened VB. [sweetened], length = 10, 10 path(s) of this length marketed N. to diluted ADJ., length = 12, 5 path(s) of this length Roget thinks that marketed means sold CORRECT Question 78 bigger | larger | steadier | closer | better bigger ADJ. [bigger] to larger ADJ. [larger], length = 0, 4 path(s) of this length bigger N. [bigger] to steadier ADJ. [steadier], length = 8, 1 path(s) of this length bigger ADJ. [bigger] to closer VB. [closer], length = 8, 7 path(s) of this length bigger N. to better VB., length = 2, 1 path(s) of this length Roget thinks that bigger means larger CORRECT Question 79 roots | origins | rituals | cure | function roots N. [roots] to origins N. [origins], length = 0, 3 path(s) of this length roots VB. [roots] to rituals N. [rituals], length = 6, 1 path(s) of this length roots VB. [roots] to cure VB. [cure], length = 8, 1 path(s) of this length roots N. to function N., length = 4, 1 path(s) of this length Roget thinks that roots means origins CORRECT Question 80 normally | ordinarily | haltingly | permanently | periodically normally (PROBLEM) not found in the index!! Final score: 63/80. 9 ties broken, 2 ties lost. Question word not in index: 4 times. Answer word not in index: 5 times. Other word not in index: 17 times. The following question words were not found in Roget: [deftly, halfheartedly, expeditiously, normally] The following answer words were not found in Roget: [succinctly, uniquely, orally, broadly, chiefly] Other words that were not found in Roget: [uniquely, linearly, typically, tentatively, chemically, patriotically, suspiciously, chronologically, overtly, verbosely, ordinarily,"
    }, {
      "heading" : "Appendix K: TOEFL, ESL and RDWP questions",
      "text" : "K - 13\neffectively, distinctively, mysteriously, descriptively, controversially, analyzed]\n1.B. 50 ESL Questions Question 1 rusty | corroded | black | dirty | painted rusty ADJ. [rusty] to corroded VB. [corroded], length = 6, 1 path(s) of this length rusty ADJ. [rusty] to black N. [black], length = 6, 9 path(s) of this length rusty ADJ. [rusty] to dirty ADJ. [dirty], length = 0, 1 path(s) of this length rusty ADJ. to painted ADJ., length = 2, 1 path(s) of this length Roget thinks that rusty means dirty INCORRECT Question 2 brass | metal | wood | stone | plastic brass N. [brass] to metal N. [metal], length = 0, 5 path(s) of this length brass N. [brass] to wood N. [wood], length = 4, 1 path(s) of this length brass N. [brass] to stone N. [stone], length = 2, 5 path(s) of this length brass N. to plastic N., length = 4, 2 path(s) of this length Roget thinks that brass means metal CORRECT Question 3 spin | twirl | ache | sweat | flush spin VB. [spin] to twirl VB. [twirl], length = 0, 2 path(s) of this length spin N. [spin] to ache VB. [ache], length = 12, 1 path(s) of this length spin N. [spin] to sweat N. [sweat], length = 0, 1 path(s) of this length spin VB. to flush VB., length = 4, 1 path(s) of this length Roget thinks that spin means twirl TIE BROKEN CORRECT Question 4 passage | hallway | ticket | entrance | room passage N. [passage] to hallway N. [hallway], length = 2, 2 path(s) of this length passage N. [passage] to ticket N. [ticket], length = 4, 2 path(s) of this length passage N. [passage] to entrance N. [entrance], length = 2, 5 path(s) of this length passage N. to room N., length = 2, 3 path(s) of this length Roget thinks that passage means entrance INCORRECT Question 5 yield | submit | challenge | boast | scorn yield VB. [yield] to submit VB. [submit], length = 0, 9 path(s) of this length yield VB. [yield] to challenge VB. [challenge], length = 4, 2 path(s) of this length yield VB. [yield] to boast VB. [boast], length = 8, 1 path(s) of this length yield VB. to scorn VB., length = 10, 11 path(s) of this length Roget thinks that yield means submit CORRECT Question 6 lean | rest | scrape | grate | refer lean VB. [lean] to rest VB. [rest], length = 0, 2 path(s) of this length lean VB. [lean] to scrape VB. [scrape], length = 2, 1 path(s) of this length lean N. [lean] to grate VB. [grate], length = 6, 2 path(s) of this length lean VB. to refer VB., length = 12, 15 path(s) of this length Roget thinks that lean means rest CORRECT Question 7 barrel | cask | bottle | box | case barrel N. [barrel] to cask N. [cask], length = 0, 2 path(s) of this length barrel N. [barrel] to bottle N. [bottle], length = 4, 3 path(s) of this length barrel N. [barrel] to box N. [box], length = 0, 9 path(s) of this length barrel N. to case N., length = 4, 4 path(s) of this length Roget thinks that barrel means box TIE LOST INCORRECT Question 8 nuisance | pest | garbage | relief | troublesome nuisance N. [nuisance] to pest N. [pest], length = 0, 4 path(s) of this length nuisance N. [nuisance] to garbage N. [garbage], length = 14, 4 path(s) of"
    }, {
      "heading" : "Appendix K: TOEFL, ESL and RDWP questions",
      "text" : "K - 14\nthis length nuisance N. [nuisance] to relief N. [relief], length = 8, 2 path(s) of this length nuisance N. to troublesome ADJ., length = 6, 1 path(s) of this length Roget thinks that nuisance means pest CORRECT Question 9 rug | carpet | sofa | ottoman | hallway rug N. [rug] to carpet N. [carpet], length = 2, 2 path(s) of this length rug N. [rug] to sofa N. [sofa], length = 12, 2 path(s) of this length rug N. [rug] to ottoman N. [ottoman], length = 12, 2 path(s) of this length rug N. to hallway N., length = 16, 2 path(s) of this length Roget thinks that rug means carpet CORRECT Question 10 tap | drain | boil | knock | rap tap VB. [tap] to drain VB. [drain], length = 0, 8 path(s) of this length tap N. [tap] to boil VB. [boil], length = 6, 1 path(s) of this length tap N. [tap] to knock N. [knock], length = 0, 10 path(s) of this length tap N. to rap N., length = 0, 5 path(s) of this length Roget thinks that tap means knock TIE LOST INCORRECT Question 11 split | divided | crushed | grated | bruised split VB. [split] to divided VB. [divided], length = 2, 8 path(s) of this length split VB. [split] to crushed VB. [crushed], length = 4, 1 path(s) of this length split N. [split] to grated VB. [grated], length = 6, 1 path(s) of this length split VB. to bruised VB., length = 10, 7 path(s) of this length Roget thinks that split means divided CORRECT Question 12 lump | chunk | stem | trunk | limb lump N. [lump] to chunk N. [chunk], length = 0, 3 path(s) of this length lump N. [lump] to stem N. [stem], length = 2, 1 path(s) of this length lump N. [lump] to trunk N. [trunk], length = 2, 2 path(s) of this length lump N. to limb N., length = 2, 3 path(s) of this length Roget thinks that lump means chunk CORRECT Question 13 outline | contour | pair | blend | block outline N. [outline] to contour N. [contour], length = 0, 7 path(s) of this length outline N. [outline] to pair N. [pair], length = 10, 41 path(s) of this length outline N. [outline] to blend VB. [blend], length = 10, 8 path(s) of this length outline N. to block VB., length = 2, 8 path(s) of this length Roget thinks that outline means contour CORRECT Question 14 swear | vow | explain | think | describe swear VB. [swear] to vow VB. [vow], length = 0, 4 path(s) of this length swear VB. [swear] to explain VB. [explain], length = 8, 1 path(s) of this length swear VB. [swear] to think VB. [think], length = 4, 3 path(s) of this length swear VB. to describe VB., length = 12, 77 path(s) of this length Roget thinks that swear means vow CORRECT Question 15 relieved | rested | sleepy | tired | hasty relieved VB. [relieved] to rested VB. [rested], length = 0, 6 path(s) of this length relieved VB. [relieved] to sleepy ADJ. [sleepy], length = 8, 1 path(s) of this length relieved VB. [relieved] to tired VB. [tired], length = 8, 3 path(s) of this length relieved VB. to hasty N., length = 2, 1 path(s) of this length"
    }, {
      "heading" : "Appendix K: TOEFL, ESL and RDWP questions",
      "text" : "K - 15\nRoget thinks that relieved means rested CORRECT Question 16 deserve | merit | need | want | expect deserve VB. [deserve] to merit VB. [merit], length = 0, 2 path(s) of this length deserve VB. [deserve] to need VB. [need], length = 10, 16 path(s) of this length deserve VB. [deserve] to want VB. [want], length = 10, 23 path(s) of this length deserve VB. to expect VB., length = 4, 1 path(s) of this length Roget thinks that deserve means merit CORRECT Question 17 haste | a hurry | anger | ear | spite haste N. [haste] to hurry N. [a hurry], length = 0, 6 path(s) of this length haste N. [haste] to anger N. [anger], length = 4, 1 path(s) of this length haste N. [haste] to ear N. [ear], length = 12, 10 path(s) of this length haste N. to spite VB., length = 12, 5 path(s) of this length Roget thinks that haste means a hurry CORRECT Question 18 stiff | firm | dark | drunk | cooked stiff N. [stiff] to firm N. [firm], length = 2, 4 path(s) of this length stiff ADJ. [stiff] to dark ADJ. [dark], length = 2, 1 path(s) of this length stiff ADJ. [stiff] to drunk ADJ. [drunk], length = 2, 3 path(s) of this length stiff ADJ. to cooked VB., length = 12, 34 path(s) of this length Roget thinks that stiff means firm TIE BROKEN CORRECT Question 19 verse | section | weed | twig | branch verse N. [verse] to section N. [section], length = 2, 4 path(s) of this length verse N. [verse] to weed VB. [weed], length = 10, 15 path(s) of this length verse N. [verse] to twig N. [twig], length = 4, 1 path(s) of this length verse N. to branch N., length = 4, 2 path(s) of this length Roget thinks that verse means section CORRECT Question 20 steep | sheer | bare | rugged | stone steep ADJ. [steep] to sheer ADJ. [sheer], length = 0, 3 path(s) of this length steep VB. [steep] to bare ADJ. [bare], length = 8, 1 path(s) of this length steep ADJ. [steep] to rugged ADJ. [rugged], length = 2, 1 path(s) of this length steep VB. to stone N., length = 8, 1 path(s) of this length Roget thinks that steep means sheer CORRECT Question 21 envious | jealous | enthusiastic | hurt | relieved envious ADJ. [envious] to jealous ADJ. [jealous], length = 0, 7 path(s) of this length envious ADJ. [envious] to enthusiastic ADJ. [enthusiastic], length = 12, 1 path(s) of this length envious ADJ. [envious] to hurt ADJ. [hurt], length = 2, 1 path(s) of this length envious ADJ. to relieved VB., length = 8, 2 path(s) of this length Roget thinks that envious means jealous CORRECT Question 22 paste | dough | syrup | block | jelly paste N. [paste] to dough N. [dough], length = 0, 2 path(s) of this length paste N. [paste] to syrup N. [syrup], length = 2, 1 path(s) of this length paste N. [paste] to block N. [block], length = 8, 1 path(s) of this length paste N. to jelly N., length = 2, 1 path(s) of this length Roget thinks that paste means dough CORRECT Question 23 scorn | refuse | enjoy | avoid | plan scorn VB. [scorn] to refuse VB. [refuse], length = 2, 1 path(s) of this length scorn VB. [scorn] to enjoy VB. [enjoy], length = 8, 1 path(s) of this length"
    }, {
      "heading" : "Appendix K: TOEFL, ESL and RDWP questions",
      "text" : "K - 16\nscorn N. [scorn] to avoid VB. [avoid], length = 10, 45 path(s) of this length scorn VB. to plan VB., length = 10, 24 path(s) of this length Roget thinks that scorn means refuse CORRECT Question 24 refer | direct | call | carry | explain refer VB. [refer] to direct VB. [direct], length = 2, 1 path(s) of this length refer VB. [refer] to call VB. [call], length = 2, 2 path(s) of this length refer VB. [refer] to carry VB. [carry], length = 10, 39 path(s) of this length refer VB. to explain VB., length = 4, 1 path(s) of this length Roget thinks that refer means call TIE LOST INCORRECT Question 25 limb | branch | bark | trunk | twig limb N. [limb] to branch N. [branch], length = 0, 8 path(s) of this length limb N. [limb] to bark N. [bark], length = 10, 10 path(s) of this length limb N. [limb] to trunk N. [trunk], length = 2, 5 path(s) of this length limb N. to twig N., length = 0, 7 path(s) of this length Roget thinks that limb means branch TIE BROKEN CORRECT Question 26 pad | cushion | board | block | tablet pad VB. [pad] to cushion VB. [cushion], length = 0, 5 path(s) of this length pad N. [pad] to board N. [board], length = 2, 5 path(s) of this length pad VB. [pad] to block VB. [block], length = 2, 7 path(s) of this length pad N. to tablet N., length = 2, 2 path(s) of this length Roget thinks that pad means cushion CORRECT Question 27 boast | brag | yell | complain | explain boast VB. [boast] to brag VB. [brag], length = 0, 9 path(s) of this length boast VB. [boast] to yell N. [yell], length = 12, 22 path(s) of this length boast VB. [boast] to complain VB. [complain], length = 8, 1 path(s) of this length boast VB. to explain VB., length = 10, 24 path(s) of this length Roget thinks that boast means brag CORRECT Question 28 applause | approval | fear | shame | friends applause N. [applause] to approval N. [approval], length = 0, 2 path(s) of this length applause N. [applause] to fear VB. [fear], length = 6, 1 path(s) of this length applause N. [applause] to shame VB. [shame], length = 8, 4 path(s) of this length applause N. to friends N., length = 8, 1 path(s) of this length Roget thinks that applause means approval CORRECT Question 29 sheet | leaf | book | block | tap sheet N. [sheet] to leaf N. [leaf], length = 0, 5 path(s) of this length sheet N. [sheet] to book N. [book], length = 0, 14 path(s) of this length sheet N. [sheet] to block N. [block], length = 4, 2 path(s) of this length sheet VB. to tap VB., length = 4, 1 path(s) of this length Roget thinks that sheet means book TIE LOST INCORRECT Question 30 stem | stalk | bark | column | trunk stem N. [stem] to stalk N. [stalk], length = 0, 4 path(s) of this length stem VB. [stem] to bark N. [bark], length = 10, 14 path(s) of this length stem N. [stem] to column N. [column], length = 2, 2 path(s) of this length stem N. to trunk N., length = 0, 4 path(s) of this length Roget thinks that stem means stalk CORRECT Question 31 seize | take | refer | request | yield seize VB. [seize] to take VB. [take], length = 0, 8 path(s) of this length"
    }, {
      "heading" : "Appendix K: TOEFL, ESL and RDWP questions",
      "text" : "K - 17\nseize VB. [seize] to refer VB. [refer], length = 12, 16 path(s) of this length seize VB. [seize] to request N. [request], length = 6, 1 path(s) of this length seize VB. to yield VB., length = 10, 18 path(s) of this length Roget thinks that seize means take CORRECT Question 32 trunk | chest | bag | closet | swing trunk N. [trunk] to chest N. [chest], length = 2, 1 path(s) of this length trunk N. [trunk] to bag N. [bag], length = 4, 2 path(s) of this length trunk N. [trunk] to closet N. [closet], length = 4, 2 path(s) of this length trunk N. to swing N., length = 4, 1 path(s) of this length Roget thinks that trunk means chest CORRECT Question 33 weed | unwanted plant | cloth | animal | vegetable weed N. [weed] to plant N. [unwanted plant], length = 0, 4 path(s) of this length weed VB. [weed] to cloth N. [cloth], length = 6, 1 path(s) of this length weed N. [weed] to animal N. [animal], length = 0, 1 path(s) of this length weed N. to vegetable N., length = 0, 4 path(s) of this length Roget thinks that weed means unwanted plant TIE BROKEN CORRECT Question 34 approval | endorsement | gift | statement | confession approval N. [approval] to endorsement N. [endorsement], length = 2, 1 path(s) of this length approval ADJ. [approval] to gift N. [gift], length = 10, 30 path(s) of this length approval ADJ. [approval] to statement N. [statement], length = 10, 30 path(s) of this length approval N. to confession N., length = 2, 1 path(s) of this length Roget thinks that approval means endorsement CORRECT Question 35 mass | lump | service | worship | element mass N. [mass] to lump N. [lump], length = 0, 11 path(s) of this length mass N. [mass] to service N. [service], length = 0, 14 path(s) of this length mass N. [mass] to worship N. [worship], length = 2, 3 path(s) of this length mass N. to element N., length = 4, 2 path(s) of this length Roget thinks that mass means service TIE LOST INCORRECT Question 36 swing | sway | bounce | break | crash swing VB. [swing] to sway VB. [sway], length = 0, 5 path(s) of this length swing VB. [swing] to bounce VB. [bounce], length = 2, 2 path(s) of this length swing VB. [swing] to break VB. [break], length = 2, 2 path(s) of this length swing N. to crash N., length = 2, 4 path(s) of this length Roget thinks that swing means sway CORRECT Question 37 sore | painful | red | hot | rough sore ADJ. [sore] to painful ADJ. [painful], length = 0, 5 path(s) of this length sore VB. [sore] to red VB. [red], length = 0, 3 path(s) of this length sore N. [sore] to hot N. [hot], length = 0, 3 path(s) of this length sore ADJ. to rough ADJ., length = 2, 3 path(s) of this length Roget thinks that sore means painful TIE BROKEN CORRECT Question 38 hinder | block | assist | relieve | yield hinder VB. [hinder] to block VB. [block], length = 2, 2 path(s) of this length hinder VB. [hinder] to assist VB. [assist], length = 8, 1 path(s) of this length hinder VB. [hinder] to relieve VB. [relieve], length = 8, 3 path(s) of this length hinder VB. to yield VB., length = 10, 40 path(s) of this length Roget thinks that hinder means block CORRECT"
    }, {
      "heading" : "Appendix K: TOEFL, ESL and RDWP questions",
      "text" : "K - 18\nQuestion 39 sticky | gooey | smooth | shiny | wet sticky ADJ. [sticky] to gooey ADJ. [gooey], length = 0, 2 path(s) of this length sticky N. [sticky] to smooth N. [smooth], length = 2, 1 path(s) of this length sticky ADJ. [sticky] to shiny ADJ. [shiny], length = 14, 2 path(s) of this length sticky ADJ. to wet VB., length = 10, 34 path(s) of this length Roget thinks that sticky means gooey CORRECT Question 40 confession | statement | service | plea | bargain confession N. [confession] to statement N. [statement], length = 0, 3 path(s) of this length confession N. [confession] to service N. [service], length = 4, 4 path(s) of this length confession N. [confession] to plea N. [plea], length = 2, 1 path(s) of this length confession N. to bargain N., length = 4, 1 path(s) of this length Roget thinks that confession means statement CORRECT Question 41 weave | intertwine | print | stamp | shake weave VB. [weave] to intertwine VB. [intertwine], length = 0, 5 path(s) of this length weave VB. [weave] to print VB. [print], length = 2, 2 path(s) of this length weave VB. [weave] to stamp VB. [stamp], length = 8, 1 path(s) of this length weave VB. to shake VB., length = 2, 1 path(s) of this length Roget thinks that weave means intertwine CORRECT Question 42 saucer | dish | box | frisbee | can saucer N. [saucer] to dish N. [dish], length = 2, 4 path(s) of this length saucer N. [saucer] to box N. [box], length = 4, 8 path(s) of this length frisbee is NOT IN THE INDEX saucer N. to can N., length = 4, 3 path(s) of this length Roget thinks that saucer means dish CORRECT Question 43 substance | thing | posture | level | score substance N. [substance] to thing N. [thing], length = 2, 13 path(s) of this length substance N. [substance] to posture N. [posture], length = 2, 1 path(s) of this length substance N. [substance] to level N. [level], length = 8, 2 path(s) of this length substance N. to score N., length = 8, 1 path(s) of this length Roget thinks that substance means thing TIE BROKEN CORRECT Question 44 firmly | steadfastly | reluctantly | sadly | hopefully steadfastly (ANSWER) is NOT IN THE INDEX firmly VB. [firmly] to reluctantly ADV. [reluctantly], length = 16, 6 path(s) of this length firmly ADV. [firmly] to sadly ADV. [sadly], length = 12, 1 path(s) of this length firmly VB. to hopefully ADV., length = 16, 3 path(s) of this length Roget thinks that firmly means sadly INCORRECT Question 45 twist | intertwine | clip | fasten | curl twist VB. [twist] to intertwine VB. [intertwine], length = 0, 2 path(s) of this length twist N. [twist] to clip N. [clip], length = 4, 2 path(s) of this length twist VB. [twist] to fasten VB. [fasten], length = 4, 3 path(s) of this length twist N. to curl N., length = 2, 4 path(s) of this length Roget thinks that twist means intertwine CORRECT Question 46"
    }, {
      "heading" : "Appendix K: TOEFL, ESL and RDWP questions",
      "text" : "K - 19\nscrape | grate | chop | mince | slice scrape VB. [scrape] to grate VB. [grate], length = 0, 4 path(s) of this length scrape VB. [scrape] to chop VB. [chop], length = 8, 1 path(s) of this length scrape VB. [scrape] to mince VB. [mince], length = 2, 1 path(s) of this length scrape VB. to slice N., length = 10, 25 path(s) of this length Roget thinks that scrape means grate CORRECT Question 47 grind | rub | slice | hit | tap grind VB. [grind] to rub VB. [rub], length = 2, 6 path(s) of this length grind VB. [grind] to slice VB. [slice], length = 4, 2 path(s) of this length grind VB. [grind] to hit VB. [hit], length = 10, 37 path(s) of this length grind VB. to tap ADJ., length = 10, 49 path(s) of this length Roget thinks that grind means rub CORRECT Question 48 swell | enlarge | move | curl | shake swell VB. [swell] to enlarge VB. [enlarge], length = 0, 4 path(s) of this length swell N. [swell] to move VB. [move], length = 6, 1 path(s) of this length swell VB. [swell] to curl VB. [curl], length = 10, 34 path(s) of this length swell N. to shake N., length = 4, 1 path(s) of this length Roget thinks that swell means enlarge CORRECT Question 49 harvest | intake | stem | lump | split harvest N. [harvest] to intake N. [intake], length = 4, 1 path(s) of this length harvest N. [harvest] to stem N. [stem], length = 8, 1 path(s) of this length harvest N. [harvest] to lump N. [lump], length = 2, 1 path(s) of this length harvest N. to split N., length = 4, 1 path(s) of this length Roget thinks that harvest means lump INCORRECT Question 50 approve | support | boast | scorn | anger approve VB. [approve] to support VB. [support], length = 2, 3 path(s) of this length approve VB. [approve] to boast VB. [boast], length = 4, 1 path(s) of this length approve VB. [approve] to scorn VB. [scorn], length = 2, 1 path(s) of this length approve VB. to anger N., length = 8, 1 path(s) of this length Roget thinks that approve means support TIE BROKEN CORRECT Final score: 41/50. 7 ties broken, 5 ties lost. Question word not in index: 0 times. Answer word not in index: 1 times. Other word not in index: 1 times. The following question words were not found in Roget: [] The following answer words were not found in Roget: [steadfastly] Other words that were not found in Roget: [frisbee]\n1.C. 20 RDWP Questions – January 2000: Nature Question 1 eddy | whirlpool | current | wave | wind eddy N. [eddy] to whirlpool N. [whirlpool], length = 0, 4 path(s) of this length eddy N. [eddy] to current N. [current], length = 2, 2 path(s) of this length eddy N. [eddy] to wave N. [wave], length = 2, 3 path(s) of this length eddy N. to wind N., length = 2, 5 path(s) of this length Roget thinks that eddy means whirlpool CORRECT Question 2 bough | branch | barricade | shaded area | fallen tree bough N. [bough] to branch N. [branch], length = 0, 5 path(s) of this length bough N. [bough] to barricade N. [barricade], length = 12, 1 path(s) of this length bough N. [bough] to shaded N. [shaded area], length = 10, 31 path(s) of this length bough N. [bough] to tree N. [fallen tree], length = 2, 2 path(s) of this length"
    }, {
      "heading" : "Appendix K: TOEFL, ESL and RDWP questions",
      "text" : "K - 20\nRoget thinks that bough means branch CORRECT Question 3 heath | overgrown open land | burned-over area | thin forest | pasture heath N. [heath] to land N. [overgrown open land], length = 0, 2 path(s) of this length heath N. [heath] to area N. [burned-over area], length = 4, 1 path(s) of this length heath N. [heath] to forest N. [thin forest], length = 0, 2 path(s) of this length heath N. to pasture N., length = 4, 2 path(s) of this length Roget thinks that heath means overgrown open land CORRECT Question 4 scud | run straight | move slowly | falter | vaporize scud VB. [scud] to run VB. [run straight], length = 2, 5 path(s) of this length scud VB. [scud] to move slowly VB. [move slowly], length = 8, 1 path(s) of this length scud VB. [scud] to falter VB. [falter], length = 8, 2 path(s) of this length scud N. to vaporize VB., length = 6, 1 path(s) of this length Roget thinks that scud means run straight CORRECT Question 5 williwaw | sudden windblast | rainsquall | songbird | meadow flower williwaw N. [williwaw] to sudden N. [sudden windblast], length = 14, 2 path(s) of this length rainsquall is NOT IN THE INDEX williwaw N. [williwaw] to songbird N. [songbird], length = 14, 2 path(s) of this length williwaw N. [williwaw] to flower N. [meadow flower], length = 4, 1 path(s) of this length Roget thinks that williwaw means meadow flower INCORRECT Question 6 verge | brink | middle path | bare ground | vantage point verge N. [verge] to brink N. [brink], length = 0, 3 path(s) of this length verge N. [verge] to path N. [middle path], length = 4, 2 path(s) of this length verge VB. [verge] to ground VB. [bare ground], length = 10, 77 path(s) of this length verge N. to vantage point N., length = 16, 9 path(s) of this length Roget thinks that verge means brink CORRECT Question 7 dale | valley | retreat | shelter | plain dale N. [dale] to valley N. [valley], length = 2, 1 path(s) of this length dale N. [dale] to retreat VB. [retreat], length = 16, 36 path(s) of this length dale N. [dale] to shelter N. [shelter], length = 14, 2 path(s) of this length dale N. to plain N., length = 2, 1 path(s) of this length Roget thinks that dale means valley CORRECT Question 8 limpid | clear | still | flat | luminous limpid ADJ. [limpid] to clear ADJ. [clear], length = 0, 3 path(s) of this length limpid ADJ. [limpid] to still VB. [still], length = 10, 23 path(s) of this length limpid ADJ. [limpid] to flat ADJ. [flat], length = 10, 31 path(s) of this length limpid ADJ. to luminous ADJ., length = 2, 1 path(s) of this length Roget thinks that limpid means clear CORRECT Question 9 floe | floating ice | frozen stream | lump | driftwood floe N. [floe] to ice N. [floating ice], length = 0, 4 path(s) of this length floe N. [floe] to frozen N. [frozen stream], length = 2, 2 path(s) of this length floe N. [floe] to lump N. [lump], length = 12, 4 path(s) of this length floe N. to driftwood N., length = 16, 6 path(s) of this length Roget thinks that floe means floating ice CORRECT Question 10 cascade | waterfall | thunder | swift current | edge cascade N. [cascade] to waterfall N. [waterfall], length = 0, 2 path(s) of this length cascade VB. [cascade] to thunder VB. [thunder], length = 12, 2 path(s) of this length cascade N. [cascade] to current N. [swift current], length = 4, 1 path(s) of this length cascade VB. to edge VB., length = 10, 8 path(s) of this length Roget thinks that cascade means waterfall CORRECT Question 11 undulation | rise and fall | faint motion | ebb and flow | quivering"
    }, {
      "heading" : "Appendix K: TOEFL, ESL and RDWP questions",
      "text" : "K - 21\nundulation N. [undulation] to rise and fall VB. [rise and fall], length = 10, 1 path(s) of this length undulation N. [undulation] to motion N. [faint motion], length = 2, 2 path(s) of this length undulation N. [undulation] to ebb and flow N. [ebb and flow], length = 4, 2 path(s) of this length undulation N. to quivering VB., length = 6, 1 path(s) of this length Roget thinks that undulation means faint motion INCORRECT Question 12 crag | steep rock | headland | barren hill | niche crag N. [crag] to steep N. [steep rock], length = 0, 1 path(s) of this length crag N. [crag] to headland N. [headland], length = 2, 1 path(s) of this length crag N. [crag] to hill N. [barren hill], length = 2, 2 path(s) of this length crag N. to niche N., length = 10, 3 path(s) of this length Roget thinks that crag means steep rock CORRECT Question 13 truss | cluster of flowers | main stem | bunch of grass | fallen petals truss VB. [truss] to cluster VB. [cluster of flowers], length = 4, 3 path(s) of this length truss N. [truss] to stem N. [main stem], length = 2, 2 path(s) of this length truss VB. [truss] to bunch VB. [bunch of grass], length = 2, 5 path(s) of this length truss VB. [truss] to fallen VB. [fallen petals], length = 8, 3 path(s) of this length Roget thinks that truss means bunch of grass INCORRECT Question 14 slough | deep mire | quicksand | shower | erosion slough N. [slough] to mire N. [deep mire], length = 0, 1 path(s) of this length slough N. [slough] to quicksand N. [quicksand], length = 2, 1 path(s) of this length slough VB. [slough] to shower N. [shower], length = 6, 5 path(s) of this length slough N. to erosion N., length = 10, 16 path(s) of this length Roget thinks that slough means deep mire CORRECT Question 15 lee | shelter | cove | grassland | riverbank lee N. [lee] to shelter N. [shelter], length = 2, 2 path(s) of this length lee N. [lee] to cove N. [cove], length = 14, 1 path(s) of this length lee N. [lee] to grassland N. [grassland], length = 14, 1 path(s) of this length riverbank is NOT IN THE INDEX Roget thinks that lee means shelter CORRECT Question 16 brackish | salty | dirty | rough | noisy brackish ADJ. [brackish] to salty ADJ. [salty], length = 0, 1 path(s) of this length brackish ADJ. [brackish] to dirty VB. [dirty], length = 10, 6 path(s) of this length brackish ADJ. [brackish] to rough ADJ. [rough], length = 4, 1 path(s) of this length brackish ADJ. to noisy ADJ., length = 10, 1 path(s) of this length Roget thinks that brackish means salty CORRECT Question 17 precipice | vertical rockface | wide gap | broken path | descent precipice N. [precipice] to vertical N. [vertical rockface], length = 2, 2 path(s) of this length precipice N. [precipice] to wide ADJ. [wide gap], length = 10, 22 path(s) of this length precipice N. [precipice] to broken VB. [broken path], length = 8, 1 path(s) of this length precipice N. to descent N., length = 2, 2 path(s) of this length Roget thinks that precipice means descent INCORRECT Question 18 chasm | deep fissure | wide opening | mountain pass | series of falls chasm N. [chasm] to fissure N. [deep fissure], length = 0, 2 path(s) of this length chasm N. [chasm] to wide opening ADJ. [wide opening], length = 10, 3 path(s) of this length chasm N. [chasm] to pass N. [mountain pass], length = 2, 1 path(s) of this length chasm N. [chasm] to falls N. [series of falls], length = 2, 1 path(s) of this length Roget thinks that chasm means deep fissure CORRECT Question 19 sediment | settles to the bottom | floats | holds together | covers rocks sediment N. [sediment] to the N. [settles to the bottom], length = 10, 24 path(s) of this length sediment N. [sediment] to floats VB. [floats], length = 10, 9 path(s) of this length sediment N. [sediment] to together VB. [holds together], length = 6, 1 path(s) of this length"
    }, {
      "heading" : "Appendix K: TOEFL, ESL and RDWP questions",
      "text" : "K - 22\nsediment N. [sediment] to rocks N. [covers rocks], length = 2, 1 path(s) of this length Roget thinks that sediment means covers rocks INCORRECT Question 20 torrent | violent flow | drift | swell | deep sound torrent N. [torrent] to violent N. [violent flow], length = 4, 1 path(s) of this length torrent N. [torrent] to drift VB. [drift], length = 8, 1 path(s) of this length torrent N. [torrent] to swell N. [swell], length = 4, 1 path(s) of this length torrent N. [torrent] to sound ADJ. [deep sound], length = 6, 2 path(s) of this length Roget thinks that torrent means violent flow CORRECT Final score: 15/20. 0 ties broken, 0 ties lost. The answer was not in the index 2 times. The question was not in the index 0 times. -- NEW STATS -- Question word not in index: 0 times. Answer word not in index: 0 times. Other word not in index: 2 times. The following question words were not found in Roget: [] The following answer words were not found in Roget: [] Other words that were not found in Roget: [rainsquall, riverbank]\n2 Semantic Distance measured using the Hirst and St-Onge WordNet-based meaure\n2.A. 80 TOEFL Questions Question 1 enormously | tremendously | appropriately | uniquely | decidedly enormously tremendously 16 enormously appropriately 0 enormously uniquely 0 enormously decidedly 0 WordNet thinks that the answer is tremendously CORRECT Question 2 provisions | stipulations | interrelations | jurisdictions | interpretations provisions stipulations 0 provisions interrelations 0 provisions jurisdictions 0 provisions interpretations 0 WordNet thinks that the answer is stipulations 4 answers tied [score = 0.25] Question 3 haphazardly | randomly | dangerously | densely | linearly haphazardly randomly 16 haphazardly dangerously 0 haphazardly densely 0 haphazardly linearly 0 WordNet thinks that the answer is randomly CORRECT Question 4 prominent | conspicuous | battered | ancient | mysterious prominent conspicuous 16 prominent battered 0 prominent ancient 0 prominent mysterious 0 WordNet thinks that the answer is conspicuous CORRECT"
    }, {
      "heading" : "Appendix K: TOEFL, ESL and RDWP questions",
      "text" : "K - 23\nQuestion 5 zenith | pinnacle | completion | outset | decline zenith pinnacle 2 zenith completion 0 zenith outset 0 zenith decline 0 WordNet thinks that the answer is pinnacle CORRECT Question 6 flawed | imperfect | tiny | lustrous | crude flawed imperfect 0 flawed tiny 0 flawed lustrous 0 flawed crude 0 WordNet thinks that the answer is imperfect 4 answers tied [score = 0.25] Question 7 urgently | desperately | typically | conceivably | tentatively urgently desperately 16 urgently typically 0 urgently conceivably 0 urgently tentatively 0 WordNet thinks that the answer is desperately CORRECT Question 8 consumed | eaten | bred | caught | supplied consumed eaten 16 consumed bred 0 consumed caught 0 consumed supplied 3 WordNet thinks that the answer is eaten CORRECT Question 9 advent | coming | arrest | financing | stability advent coming 16 advent arrest 0 advent financing 0 advent stability 0 WordNet thinks that the answer is coming CORRECT Question 10 concisely | succinctly | powerfully | positively | freely concisely succinctly 0 concisely powerfully 0 concisely positively 0 concisely freely 0 WordNet thinks that the answer is succinctly 4 answers tied [score = 0.25] Question 11 salutes | greetings | information | ceremonies | privileges salutes greetings 4 salutes information 3 ['ceremonies' not in WordNet.] salutes ceremonies salutes privileges 0 WordNet thinks that the answer is greetings CORRECT Question 12 solitary | alone | alert | restless | fearless solitary alone 16 solitary alert 0 solitary restless 0 solitary fearless 0 WordNet thinks that the answer is alone CORRECT Question 13 hasten | accelerate | permit | determine | accompany hasten accelerate 0 hasten permit 0 hasten determine 4"
    }, {
      "heading" : "Appendix K: TOEFL, ESL and RDWP questions",
      "text" : "K - 24\nhasten accompany 5 WordNet thinks that the answer is accompany INCORRECT Question 14 perseverance | endurance | skill | generosity | disturbance perseverance endurance 0 perseverance skill 0 perseverance generosity 0 perseverance disturbance 4 WordNet thinks that the answer is disturbance INCORRECT Question 15 fanciful | imaginative | familiar | apparent | logical fanciful imaginative 4 fanciful familiar 0 fanciful apparent 0 fanciful logical 0 WordNet thinks that the answer is imaginative CORRECT Question 16 showed | demonstrated | published | repeated | postponed showed demonstrated 16 showed published 3 showed repeated 4 showed postponed 0 WordNet thinks that the answer is demonstrated CORRECT Question 17 constantly | continually | instantly | rapidly | accidentally constantly continually 0 constantly instantly 0 constantly rapidly 0 constantly accidentally 0 WordNet thinks that the answer is continually 4 answers tied [score = 0.25] Question 18 issues | subjects | training | salaries | benefits issues subjects 16 issues training 4 ['salaries' not in WordNet.] issues salaries issues benefits 0 WordNet thinks that the answer is subjects CORRECT Question 19 furnish | supply | impress | protect | advise furnish supply 16 furnish impress 0 furnish protect 0 furnish advise 0 WordNet thinks that the answer is supply CORRECT Question 20 costly | expensive | beautiful | popular | complicated costly expensive 16 costly beautiful 0 costly popular 0 costly complicated 0 WordNet thinks that the answer is expensive CORRECT Question 21 recognized | acknowledged | successful | depicted | welcomed recognized acknowledged 16 recognized successful 0 recognized depicted 0 recognized welcomed 4 WordNet thinks that the answer is acknowledged CORRECT Question 22"
    }, {
      "heading" : "Appendix K: TOEFL, ESL and RDWP questions",
      "text" : "K - 25\nspot | location | climate | latitude | sea spot location 6 spot climate 0 spot latitude 3 spot sea 3 WordNet thinks that the answer is location CORRECT Question 23 make | earn | print | trade | borrow make earn 16 make print 6 make trade 4 make borrow 5 WordNet thinks that the answer is earn CORRECT Question 24 often | frequently | definitely | chemically | hardly often frequently 16 often definitely 0 often chemically 0 often hardly 0 WordNet thinks that the answer is frequently CORRECT Question 25 easygoing | relaxed | frontier | boring | farming easygoing relaxed 0 easygoing frontier 0 easygoing boring 0 easygoing farming 0 WordNet thinks that the answer is relaxed 4 answers tied [score = 0.25] Question 26 debate | argument | war | election | competition debate argument 16 debate war 0 debate election 0 debate competition 0 WordNet thinks that the answer is argument CORRECT Question 27 narrow | thin | clear | freezing | poisonous narrow thin 16 narrow clear 0 narrow freezing 0 narrow poisonous 0 WordNet thinks that the answer is thin CORRECT Question 28 arranged | planned | explained | studied | discarded arranged planned 3 arranged explained 0 arranged studied 0 arranged discarded 0 WordNet thinks that the answer is planned CORRECT Question 29 infinite | limitless | relative | unusual | structural infinite limitless 16 infinite relative 0 infinite unusual 0 infinite structural 0 WordNet thinks that the answer is limitless CORRECT Question 30 showy | striking | prickly | entertaining | incidental showy striking 0 showy prickly 0 showy entertaining 0 showy incidental 0"
    }, {
      "heading" : "Appendix K: TOEFL, ESL and RDWP questions",
      "text" : "K - 26\nWordNet thinks that the answer is striking 4 answers tied [score = 0.25] Question 31 levied | imposed | believed | requested | correlated levied imposed 16 levied believed 0 levied requested 0 levied correlated 0 WordNet thinks that the answer is imposed CORRECT Question 32 deftly | skillfully | prudently | occasionally | humorously deftly skillfully 0 deftly prudently 0 deftly occasionally 0 deftly humorously 0 WordNet thinks that the answer is skillfully 4 answers tied [score = 0.25] Question 33 distribute | circulate | commercialize | research | acknowledge distribute circulate 16 distribute commercialize 0 distribute research 0 distribute acknowledge 2 WordNet thinks that the answer is circulate CORRECT Question 34 discrepancies | differences | weights | deposits | wavelengths ['discrepancies' not in WordNet.] discrepancies differences ['discrepancies' not in WordNet.] discrepancies weights ['discrepancies' not in WordNet.] discrepancies deposits ['discrepancies' not in WordNet.] discrepancies wavelengths NO ANSWER FOUND Question 35 prolific | productive | serious | capable | promising prolific productive 16 prolific serious 0 prolific capable 0 prolific promising 0 WordNet thinks that the answer is productive CORRECT Question 36 unmatched | unequaled | unrecognized | alienated | emulated unmatched unequaled 4 unmatched unrecognized 0 unmatched alienated 0 unmatched emulated 0 WordNet thinks that the answer is unequaled CORRECT Question 37 peculiarly | uniquely | partly | patriotically | suspiciously peculiarly uniquely 0 peculiarly partly 0 peculiarly patriotically 0 peculiarly suspiciously 0 WordNet thinks that the answer is uniquely 4 answers tied [score = 0.25] Question 38 hue | color | glare | contrast | scent hue color 6 hue glare 2 hue contrast 0 hue scent 3 WordNet thinks that the answer is color CORRECT Question 39 hind | rear | curved | muscular |hairy hind rear 3"
    }, {
      "heading" : "Appendix K: TOEFL, ESL and RDWP questions",
      "text" : "K - 27\nhind curved 0 hind muscular 0 hind hairy 0 WordNet thinks that the answer is rear CORRECT Question 40 highlight | accentuate | alter | imitate | restore highlight accentuate 6 highlight alter 0 highlight imitate 0 highlight restore 0 WordNet thinks that the answer is accentuate CORRECT Question 41 hastily | hurriedly | shrewdly | habitually | chronologically hastily hurriedly 16 hastily shrewdly 0 hastily habitually 0 hastily chronologically 0 WordNet thinks that the answer is hurriedly CORRECT Question 42 temperate | mild | cold | short | windy temperate mild 16 temperate cold 4 temperate short 0 temperate windy 0 WordNet thinks that the answer is mild CORRECT Question 43 grin | smile | exercise | rest | joke grin smile 16 grin exercise 0 grin rest 0 grin joke 3 WordNet thinks that the answer is smile CORRECT Question 44 verbally | orally | overtly | fittingly | verbosely verbally orally 0 verbally overtly 0 verbally fittingly 0 verbally verbosely 0 WordNet thinks that the answer is orally 4 answers tied [score = 0.25] Question 45 physician | doctor | chemist | pharmacist | nurse physician doctor 16 physician chemist 4 physician pharmacist 4 physician nurse 4 WordNet thinks that the answer is doctor CORRECT Question 46 essentially | basically | possibly | eagerly | ordinarily essentially basically 16 essentially possibly 0 essentially eagerly 0 essentially ordinarily 0 WordNet thinks that the answer is basically CORRECT Question 47 keen | sharp | useful | simple | famous keen sharp 16 keen useful 0 keen simple 4 keen famous 0 WordNet thinks that the answer is sharp CORRECT"
    }, {
      "heading" : "Appendix K: TOEFL, ESL and RDWP questions",
      "text" : "K - 28\nQuestion 48 situated | positioned | rotating | isolated | emptying situated positioned 5 situated rotating 2 situated isolated 3 situated emptying 0 WordNet thinks that the answer is positioned CORRECT Question 49 principal | major | most | numerous | exceptional principal major 0 principal most 0 principal numerous 0 principal exceptional 0 WordNet thinks that the answer is major 4 answers tied [score = 0.25] Question 50 slowly | gradually | rarely | effectively | continuously slowly gradually 0 slowly rarely 0 slowly effectively 0 slowly continuously 0 WordNet thinks that the answer is gradually 4 answers tied [score = 0.25] Question 51 built | constructed | proposed | financed | organized built constructed 16 built proposed 0 built financed 0 built organized 5 WordNet thinks that the answer is constructed CORRECT Question 52 tasks | jobs | customers | materials | shops tasks jobs 16 tasks customers 0 tasks materials 0 tasks shops 0 WordNet thinks that the answer is jobs CORRECT Question 53 unlikely | improbable | disagreeable | different | unpopular unlikely improbable 16 unlikely disagreeable 0 unlikely different 0 unlikely unpopular 0 WordNet thinks that the answer is improbable CORRECT Question 54 halfheartedly | apathetically | customarily | bipartisanly | unconventionally ['halfheartedly' not in WordNet.] halfheartedly apathetically ['halfheartedly' not in WordNet.] halfheartedly customarily ['halfheartedly' not in WordNet.] halfheartedly bipartisanly ['halfheartedly' not in WordNet.] halfheartedly unconventionally NO ANSWER FOUND Question 55 annals | chronicles | homes | trails | songs annals chronicles 4 annals homes 0 annals trails 0 annals songs 0 WordNet thinks that the answer is chronicles CORRECT Question 56 wildly | furiously | distinctively | mysteriously | abruptly wildly furiously 0"
    }, {
      "heading" : "Appendix K: TOEFL, ESL and RDWP questions",
      "text" : "K - 29\nwildly distinctively 0 wildly mysteriously 0 wildly abruptly 0 WordNet thinks that the answer is furiously 4 answers tied [score = 0.25] Question 57 hailed | acclaimed | judged | remembered | addressed hailed acclaimed 16 hailed judged 4 hailed remembered 0 hailed addressed 6 WordNet thinks that the answer is acclaimed CORRECT Question 58 command | mastery | observation | love | awareness command mastery 16 command observation 2 command love 0 command awareness 2 WordNet thinks that the answer is mastery CORRECT Question 59 concocted | devised | cleaned | requested | supervised concocted devised 5 concocted cleaned 4 concocted requested 0 concocted supervised 0 WordNet thinks that the answer is devised CORRECT Question 60 prospective | potential | particular | prudent | prominent prospective potential 16 prospective particular 0 prospective prudent 0 prospective prominent 0 WordNet thinks that the answer is potential CORRECT Question 61 generally | broadly | descriptively | controversially | accurately generally broadly 16 generally descriptively 0 generally controversially 0 generally accurately 0 WordNet thinks that the answer is broadly CORRECT Question 62 sustained | prolonged | refined | lowered | analyzed sustained prolonged 16 sustained refined 0 sustained lowered 2 sustained analyzed 0 WordNet thinks that the answer is prolonged CORRECT Question 63 perilous | dangerous | binding | exciting | offensive perilous dangerous 16 perilous binding 0 perilous exciting 0 perilous offensive 0 WordNet thinks that the answer is dangerous CORRECT Question 64 tranquillity | peacefulness | harshness | weariness | happiness tranquillity peacefulness 4 tranquillity harshness 2 tranquillity weariness 3 tranquillity happiness 4 WordNet thinks that the answer is peacefulness 2 answers tied [score = 0.5]"
    }, {
      "heading" : "Appendix K: TOEFL, ESL and RDWP questions",
      "text" : "K - 30\nQuestion 65 dissipate | disperse | isolate | disguise | photograph dissipate disperse 16 dissipate isolate 0 dissipate disguise 0 dissipate photograph 0 WordNet thinks that the answer is disperse CORRECT Question 66 primarily | chiefly | occasionally | cautiously | consistently primarily chiefly 16 primarily occasionally 0 primarily cautiously 0 primarily consistently 0 WordNet thinks that the answer is chiefly CORRECT Question 67 colloquial | conversational | recorded | misunderstood | incorrect colloquial conversational 16 colloquial recorded 0 colloquial misunderstood 0 colloquial incorrect 0 WordNet thinks that the answer is conversational CORRECT Question 68 resolved | settled | publicized | forgotten | examined resolved settled 16 resolved publicized 0 resolved forgotten 0 resolved examined 0 WordNet thinks that the answer is settled CORRECT Question 69 feasible | possible | permitted | equitable | evident feasible possible 16 feasible permitted 0 feasible equitable 0 feasible evident 0 WordNet thinks that the answer is possible CORRECT Question 70 expeditiously | rapidly | frequently | actually | repeatedly expeditiously rapidly 0 expeditiously frequently 0 expeditiously actually 0 expeditiously repeatedly 0 WordNet thinks that the answer is rapidly 4 answers tied [score = 0.25] Question 71 percentage | proportion | volume | sample | profit percentage proportion 4 percentage volume 0 percentage sample 0 percentage profit 4 WordNet thinks that the answer is proportion 2 answers tied [score = 0.5] Question 72 terminated | ended | posed | postponed | evaluated terminated ended 16 terminated posed 4 terminated postponed 0 terminated evaluated 0 WordNet thinks that the answer is ended CORRECT Question 73 uniform | alike | hard | complex | sharp uniform alike 0 uniform hard 0"
    }, {
      "heading" : "Appendix K: TOEFL, ESL and RDWP questions",
      "text" : "K - 31\nuniform complex 2 uniform sharp 0 WordNet thinks that the answer is complex INCORRECT Question 74 figure | solve | list | divide | express figure solve 4 figure list 0 figure divide 4 figure express 0 WordNet thinks that the answer is solve 2 answers tied [score = 0.5] Question 75 sufficient | enough | recent | physiological | valuable sufficient enough 16 sufficient recent 0 sufficient physiological 0 sufficient valuable 0 WordNet thinks that the answer is enough CORRECT Question 76 fashion | manner | ration | fathom | craze fashion manner 16 fashion ration 0 fashion fathom 0 fashion craze 4 WordNet thinks that the answer is manner CORRECT Question 77 marketed | sold | frozen | sweetened | diluted marketed sold 5 marketed frozen 5 marketed sweetened 5 marketed diluted 4 WordNet thinks that the answer is sold 3 answers tied [score = 0.333333333333333] Question 78 bigger | larger | steadier | closer | better bigger larger 16 bigger steadier 0 bigger closer 0 bigger better 0 WordNet thinks that the answer is larger CORRECT Question 79 roots | origins | rituals | cure | function roots origins 0 roots rituals 0 roots cure 0 roots function 0 WordNet thinks that the answer is origins 4 answers tied [score = 0.25] Question 80 normally | ordinarily | haltingly | permanently | periodically normally ordinarily 16 normally haltingly 0 normally permanently 0 normally periodically 0 WordNet thinks that the answer is ordinarily CORRECT Total questions = 80, score = 62.3333333333333, correct = 57, ties = 18 Number of problem words not found in WordNet: 2 Number of other words not found in WordNet: 2 Problem words not in WordNet: halfheartedly discrepancies Other words not in WordNet: ceremonies salaries"
    }, {
      "heading" : "Appendix K: TOEFL, ESL and RDWP questions",
      "text" : "K - 32\n2.B. 50 ESL Questions Question 1 rusty | corroded | black | dirty | painted rusty corroded 0 rusty black 3 rusty dirty 0 rusty painted 0 WordNet thinks that the answer is black INCORRECT Question 2 brass | metal | wood | stone | plastic brass metal 6 brass wood 2 brass stone 3 brass plastic 0 WordNet thinks that the answer is metal CORRECT Question 3 spin | twirl | ache | sweat | flush spin twirl 16 spin ache 0 spin sweat 0 spin flush 4 WordNet thinks that the answer is twirl CORRECT Question 4 passage | hallway | ticket | entrance | room passage hallway 5 passage ticket 2 passage entrance 4 passage room 2 WordNet thinks that the answer is hallway CORRECT Question 5 yield | submit | challenge | boast | scorn yield submit 5 yield challenge 0 yield boast 0 yield scorn 0 WordNet thinks that the answer is submit CORRECT Question 6 lean | rest | scrape | grate | refer lean rest 5 lean scrape 2 lean grate 0 lean refer 3 WordNet thinks that the answer is rest CORRECT Question 7 barrel | cask | bottle | box | case barrel cask 16 barrel bottle 5 barrel box 5 barrel case 5 WordNet thinks that the answer is cask CORRECT Question 8 nuisance | pest | garbage | relief | troublesome nuisance pest 3 nuisance garbage 0 nuisance relief 2 nuisance troublesome 0 WordNet thinks that the answer is pest CORRECT Question 9 rug | carpet | sofa | ottoman | hallway rug carpet 16"
    }, {
      "heading" : "Appendix K: TOEFL, ESL and RDWP questions",
      "text" : "K - 33\nrug sofa 3 rug ottoman 3 rug hallway 0 WordNet thinks that the answer is carpet CORRECT Question 10 tap | drain | boil | knock | rap tap drain 3 tap boil 4 tap knock 16 tap rap 16 WordNet thinks that the answer is knock INCORRECT Question 11 split | divided | crushed | grated | bruised split divided 16 split crushed 5 split grated 4 split bruised 3 WordNet thinks that the answer is divided CORRECT Question 12 lump | chunk | stem | trunk | limb lump chunk 16 lump stem 0 lump trunk 3 lump limb 2 WordNet thinks that the answer is chunk CORRECT Question 13 outline | contour | pair | blend | block outline contour 4 outline pair 0 outline blend 3 outline block 3 WordNet thinks that the answer is contour CORRECT Question 14 swear | vow | explain | think | describe swear vow 4 swear explain 4 swear think 3 swear describe 0 WordNet thinks that the answer is vow 2 answers tied [score = 0.5] Question 15 relieved | rested | sleepy | tired | hasty relieved rested 0 relieved sleepy 0 relieved tired 3 relieved hasty 0 WordNet thinks that the answer is tired INCORRECT Question 16 deserve | merit | need | want | expect deserve merit 16 deserve need 5 deserve want 5 deserve expect 0 WordNet thinks that the answer is merit CORRECT Question 17 haste | a hurry | anger | ear | spite ['a hurry' not in WordNet.] haste a hurry haste anger 0 haste ear 0 haste spite 0 WordNet thinks that the answer is anger INCORRECT"
    }, {
      "heading" : "Appendix K: TOEFL, ESL and RDWP questions",
      "text" : "K - 34\nQuestion 18 stiff | firm | dark | drunk | cooked stiff firm 3 stiff dark 0 stiff drunk 16 stiff cooked 0 WordNet thinks that the answer is drunk INCORRECT Question 19 verse | section | weed | twig | branch verse section 4 verse weed 0 verse twig 0 verse branch 0 WordNet thinks that the answer is section CORRECT Question 20 steep | sheer | bare | rugged | stone steep sheer 16 steep bare 0 steep rugged 0 steep stone 2 WordNet thinks that the answer is sheer CORRECT Question 21 envious | jealous | enthusiastic | hurt | relieved envious jealous 16 envious enthusiastic 0 envious hurt 0 envious relieved 0 WordNet thinks that the answer is jealous CORRECT Question 22 paste | dough | syrup | block | jelly paste dough 0 paste syrup 3 paste block 0 paste jelly 3 WordNet thinks that the answer is syrup INCORRECT Question 23 scorn | refuse | enjoy | avoid | plan scorn refuse 0 scorn enjoy 0 scorn avoid 0 scorn plan 0 WordNet thinks that the answer is refuse 4 answers tied [score = 0.25] Question 24 refer | direct | call | carry | explain refer direct 4 refer call 2 refer carry 4 refer explain 2 WordNet thinks that the answer is direct 2 answers tied [score = 0.5] Question 25 limb | branch | bark | trunk | twig limb branch 16 limb bark 6 limb trunk 4 limb twig 5 WordNet thinks that the answer is branch CORRECT Question 26 pad | cushion | board | block | tablet pad cushion 5 pad board 3"
    }, {
      "heading" : "Appendix K: TOEFL, ESL and RDWP questions",
      "text" : "K - 35\npad block 5 pad tablet 16 WordNet thinks that the answer is tablet INCORRECT Question 27 boast | brag | yell | complain | explain boast brag 16 boast yell 0 boast complain 0 boast explain 3 WordNet thinks that the answer is brag CORRECT Question 28 applause | approval | fear | shame | friends applause approval 4 applause fear 0 applause shame 0 applause friends 0 WordNet thinks that the answer is approval CORRECT Question 29 sheet | leaf | book | block | tap sheet leaf 4 sheet book 3 sheet block 5 sheet tap 6 WordNet thinks that the answer is tap INCORRECT Question 30 stem | stalk | bark | column | trunk stem stalk 16 stem bark 5 stem column 5 stem trunk 5 WordNet thinks that the answer is stalk CORRECT Question 31 seize | take | refer | request | yield seize take 4 seize refer 5 seize request 0 seize yield 3 WordNet thinks that the answer is refer INCORRECT Question 32 trunk | chest | bag | closet | swing trunk chest 4 trunk bag 5 trunk closet 4 trunk swing 0 WordNet thinks that the answer is bag INCORRECT Question 33 weed | unwanted plant | cloth | animal | vegetable ['unwanted plant' not in WordNet.] weed unwanted plant weed cloth 0 weed animal 3 weed vegetable 4 WordNet thinks that the answer is vegetable INCORRECT Question 34 approval | endorsement | gift | statement | confession approval endorsement 5 approval gift 0 approval statement 5 approval confession 3 WordNet thinks that the answer is endorsement 2 answers tied [score = 0.5]"
    }, {
      "heading" : "Appendix K: TOEFL, ESL and RDWP questions",
      "text" : "K - 36\nQuestion 35 mass | lump | service | worship | element mass lump 4 mass service 5 mass worship 3 mass element 2 WordNet thinks that the answer is service INCORRECT Question 36 swing | sway | bounce | break | crash swing sway 16 swing bounce 5 swing break 5 swing crash 4 WordNet thinks that the answer is sway CORRECT Question 37 sore | painful | red | hot | rough sore painful 16 sore red 0 sore hot 4 sore rough 4 WordNet thinks that the answer is painful CORRECT Question 38 hinder | block | assist | relieve | yield hinder block 16 hinder assist 0 hinder relieve 0 hinder yield 0 WordNet thinks that the answer is block CORRECT Question 39 sticky | gooey | smooth | shiny | wet sticky gooey 4 sticky smooth 0 sticky shiny 0 sticky wet 16 WordNet thinks that the answer is wet INCORRECT Question 40 confession | statement | service | plea | bargain confession statement 4 confession service 3 confession plea 0 confession bargain 0 WordNet thinks that the answer is statement CORRECT Question 41 weave | intertwine | print | stamp | shake weave intertwine 5 weave print 4 weave stamp 4 weave shake 4 WordNet thinks that the answer is intertwine CORRECT Question 42 saucer | dish | box | frisbee | can saucer dish 16 saucer box 4 saucer frisbee 5 saucer can 4 WordNet thinks that the answer is dish CORRECT Question 43 substance | thing | posture | level | score substance thing 6 substance posture 3 substance level 3"
    }, {
      "heading" : "Appendix K: TOEFL, ESL and RDWP questions",
      "text" : "K - 37\nsubstance score 2 WordNet thinks that the answer is thing CORRECT Question 44 firmly | steadfastly | reluctantly | sadly | hopefully firmly steadfastly 16 firmly reluctantly 0 firmly sadly 0 firmly hopefully 0 WordNet thinks that the answer is steadfastly CORRECT Question 45 twist | intertwine | clip | fasten | curl twist intertwine 4 twist clip 3 twist fasten 4 twist curl 6 WordNet thinks that the answer is curl INCORRECT Question 46 scrape | grate | chop | mince | slice scrape grate 16 scrape chop 4 scrape mince 3 scrape slice 5 WordNet thinks that the answer is grate CORRECT Question 47 grind | rub | slice | hit | tap grind rub 4 grind slice 0 grind hit 5 grind tap 5 WordNet thinks that the answer is hit INCORRECT Question 48 swell | enlarge | move | curl | shake swell enlarge 4 swell move 5 swell curl 2 swell shake 0 WordNet thinks that the answer is move INCORRECT Question 49 harvest | intake | stem | lump | split harvest intake 0 harvest stem 0 harvest lump 0 harvest split 0 WordNet thinks that the answer is intake 4 answers tied [score = 0.25] Question 50 approve | support | boast | scorn | anger approve support 4 approve boast 0 approve scorn 0 approve anger 0 WordNet thinks that the answer is support CORRECT Total questions = 50, score = 31, correct = 29, ties = 5 Number of problem words not found in WordNet: 0 Number of other words not found in WordNet: 2 Problem words not in WordNet: Other words not in WordNet: a hurry unwanted plant"
    }, {
      "heading" : "Appendix K: TOEFL, ESL and RDWP questions",
      "text" : "K - 38\n2.C. 20 RDWP Questions – January 2000: Nature Question 1 eddy | whirlpool | current | wave | wind eddy whirlpool 16 eddy current 4 eddy wave 4 eddy wind 0 WordNet thinks that the answer is whirlpool CORRECT Question 2 bough | branch | barricade | shaded area | fallen tree bough branch 6 bough barricade 0 ['shaded area' not in WordNet.] bough shaded area ['fallen tree ' not in WordNet.] bough fallen tree WordNet thinks that the answer is branch CORRECT Question 3 heath | overgrown open land | burned-over area | thin forest | pasture ['overgrown open land' not in WordNet.] heath overgrown open land ['burned-over area' not in WordNet.] heath burned-over area ['thin forest' not in WordNet.] heath thin forest heath pasture 0 WordNet thinks that the answer is pasture INCORRECT Question 4 scud | run straight | move slowly | falter | vaporize ['run straight' not in WordNet.] scud run straight ['move slowly' not in WordNet.] scud move slowly scud falter 3 scud vaporize 0 WordNet thinks that the answer is falter INCORRECT Question 5 williwaw | sudden windblast | rainsquall | songbird | meadow flower ['williwaw' not in WordNet.] williwaw sudden windblast ['williwaw' not in WordNet.] williwaw rainsquall ['williwaw' not in WordNet.] williwaw songbird ['williwaw' not in WordNet.] williwaw meadow flower NO ANSWER FOUND Question 6 verge | brink | middle path | bare ground | vantage point verge brink 16 ['middle path' not in WordNet.] verge middle path ['bare ground' not in WordNet.] verge bare ground verge vantage point 0 WordNet thinks that the answer is brink CORRECT Question 7 dale | valley | retreat | shelter | plain dale valley 4 dale retreat 0 dale shelter 0 dale plain 0 WordNet thinks that the answer is valley CORRECT Question 8 limpid | clear | still | flat | luminous limpid clear 16 limpid still 0 limpid flat 0 limpid luminous 0 WordNet thinks that the answer is clear CORRECT Question 9 floe | floating ice | frozen stream | lump | driftwood ['floating ice' not in WordNet.] floe floating ice"
    }, {
      "heading" : "Appendix K: TOEFL, ESL and RDWP questions",
      "text" : "K - 39\n['frozen stream' not in WordNet.] floe frozen stream floe lump 0 floe driftwood 0 WordNet thinks that the answer is lump INCORRECT Question 10 cascade | waterfall | thunder | swift current | edge cascade waterfall 4 cascade thunder 2 ['swift current' not in WordNet.] cascade swift current cascade edge 3 WordNet thinks that the answer is waterfall CORRECT Question 11 undulation | rise and fall | faint motion | ebb and flow | quivering ['rise and fall' not in WordNet.] undulation rise and fall ['faint motion' not in WordNet.] undulation faint motion ['ebb and flow' not in WordNet.] undulation ebb and flow undulation quivering 0 WordNet thinks that the answer is quivering INCORRECT Question 12 crag | steep rock | headland | barren hill | niche ['steep rock' not in WordNet.] crag steep rock crag headland 2 ['barren hill' not in WordNet.] crag barren hill crag niche 0 WordNet thinks that the answer is headland INCORRECT Question 13 truss | cluster of flowers | main stem | bunch of grass | fallen petals ['cluster of flowers' not in WordNet.] truss cluster of flowers ['main stem' not in WordNet.] truss main stem ['bunch of grass' not in WordNet.] truss bunch of grass ['fallen petals ' not in WordNet.] truss fallen petals NO ANSWER FOUND Question 14 slough | deep mire | quicksand | shower | erosion ['deep mire' not in WordNet.] slough deep mire slough quicksand 0 slough shower 0 slough erosion 0 WordNet thinks that the answer is quicksand INCORRECT Question 15 lee | shelter | cove | grassland | riverbank lee shelter 0 lee cove 0 lee grassland 0 lee riverbank 0 WordNet thinks that the answer is shelter 4 answers tied [score = 0.25] Question 16 brackish | salty | dirty | rough | noisy brackish salty 4 brackish dirty 0 brackish rough 0 brackish noisy 0 WordNet thinks that the answer is salty CORRECT Question 17 precipice | vertical rockface | wide gap | broken path | descent ['vertical rockface' not in WordNet.] precipice vertical rockface ['wide gap' not in WordNet.] precipice wide gap ['broken path' not in WordNet.] precipice broken path precipice descent 3 WordNet thinks that the answer is descent INCORRECT"
    }, {
      "heading" : "Appendix K: TOEFL, ESL and RDWP questions",
      "text" : "K - 40\nQuestion 18 chasm | deep fissure | wide opening | mountain pass | series of falls ['deep fissure' not in WordNet.] chasm deep fissure ['wide opening' not in WordNet.] chasm wide opening chasm mountain pass 3 ['series of falls ' not in WordNet.] chasm series of falls WordNet thinks that the answer is mountain pass INCORRECT Question 19 sediment | settles to the bottom | floats | holds together | covers rocks ['settles to the bottom' not in WordNet.] sediment settles to the bottom sediment floats 2 ['holds together' not in WordNet.] sediment holds together ['covers rocks ' not in WordNet.] sediment covers rocks WordNet thinks that the answer is floats INCORRECT Question 20 torrent | violent flow | drift | swell | deep sound ['violent flow' not in WordNet.] torrent violent flow torrent drift 0 torrent swell 0 ['deep sound' not in WordNet.] torrent deep sound WordNet thinks that the answer is drift INCORRECT Total questions = 20, score = 7.25, correct = 7, ties = 1 Number of problem words not found in WordNet: 1 Number of other words not found in WordNet: 33 Problem words not in WordNet: williwaw Other words not in WordNet: covers rocks, wide opening, ebb and flow, settles to the bottom, vertical rockface, fallen petals, violent flow, burned-over area, barren hill, bunch of grass, deep fissure, main stem, bare ground, thin forest, wide gap, faint motion, overgrown open land, rise and fall, floating ice, cluster of flowers, deep mire, middle path, frozen stream, steep rock, run straight, broken path, holds together, deep sound, swift current, move slowly, series of falls, fallen tree, shaded area"
    }, {
      "heading" : "Appendix L: A Lexical Chain Building Example",
      "text" : "L - 1"
    }, {
      "heading" : "Appendix L: A Lexical Chain Building Example",
      "text" : "This appendix shows the step-by-step output of my lexical chain building program that uses the ELKB. St-Onge (1995) and Ellman (2000) also use this text, attributed to Einstein, to demonstrate their lexical chain building systems.\nStep 1: Choose a Set of Thesaural Relations\nChapter 5 presents the thesaural relations used by the ELKB.\nStep 2: Select a Set of Candidate Words\nWe suppose a very long train travelling along the rails with a constant velocity v and in the direction indicated in Figure 1. People travelling in this train will with advantage use the train as a rigid reference-body; they regard all events in reference to the train. Then every event which takes place along the line also takes place at a particular point of the train. Also, the definition of simultaneity can be given relative to the train in exactly the same way as with respect to the embankment.\nStep 3: Build All Proto-Chains for Each Candidate Word\nsuppose, takes [score: 2.0, sense: 480, line: 1] suppose, takes, takes [score: 3.0, sense: 480, line: 1] suppose, regard [score: 2.0, sense: 485, line: 1] suppose, regard, takes [score: 3.0, sense: 485, line: 1] suppose, regard, takes, takes [score: 4.0, sense: 485, line: 1] suppose, takes [score: 2.0, sense: 512, line: 1] suppose, takes, takes [score: 3.0, sense: 512, line: 1] suppose, takes [score: 2.0, sense: 510, line: 1] suppose, takes, takes [score: 3.0, sense: 510, line: 1] train, train [score: 2.0, sense: 534, line: 1] train, train, train [score: 3.0, sense: 534, line: 1] train, train, train, train [score: 4.0, sense: 534, line: 1] train, train, train, train, takes [score: 5.0, sense: 534, line: 1] train, train, train, train, takes, takes [score: 6.0, sense: 534, line: 1] train, train, train, train, takes, takes, train [score: 7.0, sense: 534, line: 1] train, train, train, train, takes, takes, train, train [score: 8.0, sense: 534, line: 1] train, train [score: 2.0, sense: 536, line: 1] train, train, train [score: 3.0, sense: 536, line: 1] train, train, train, train [score: 4.0, sense: 536, line: 1] train, train, train, train, takes [score: 5.0, sense: 536, line: 1] train, train, train, train, takes, takes [score: 6.0, sense: 536, line: 1] train, train, train, train, takes, takes, train [score: 7.0, sense: 536, line: 1] train, train, train, train, takes, takes, train, train [score: 8.0, sense: 536, line: 1] train, train [score: 2.0, sense: 284, line: 1] train, train, train [score: 3.0, sense: 284, line: 1]"
    }, {
      "heading" : "Appendix L: A Lexical Chain Building Example",
      "text" : "L - 2\ntrain, train, train, train [score: 4.0, sense: 284, line: 1] train, train, train, train, train [score: 5.0, sense: 284, line: 1] train, train, train, train, train, train [score: 6.0, sense: 284, line: 1] train, train [score: 2.0, sense: 217, line: 1] train, train, train [score: 3.0, sense: 217, line: 1] train, train, train, train [score: 4.0, sense: 217, line: 1] train, train, train, train, train [score: 5.0, sense: 217, line: 1] train, train, train, train, train, train [score: 6.0, sense: 217, line: 1] train, train [score: 2.0, sense: 267, line: 1] train, train, train [score: 3.0, sense: 267, line: 1] train, train, train, train [score: 4.0, sense: 267, line: 1] train, train, train, train, train [score: 5.0, sense: 267, line: 1] train, train, train, train, train, train [score: 6.0, sense: 267, line: 1] train, rails [score: 2.0, sense: 274, line: 1] train, rails, train [score: 3.0, sense: 274, line: 1] train, rails, train, train [score: 4.0, sense: 274, line: 1] train, rails, train, train, train [score: 5.0, sense: 274, line: 1] train, rails, train, train, train, train [score: 6.0, sense: 274, line: 1] train, rails, train, train, train, train, train [score: 7.0, sense: 274, line: 1] train, train [score: 2.0, sense: 837, line: 1] train, train, train [score: 3.0, sense: 837, line: 1] train, train, train, train [score: 4.0, sense: 837, line: 1] train, train, train, train, train [score: 5.0, sense: 837, line: 1] train, train, train, train, train, train [score: 6.0, sense: 837, line: 1] train, train [score: 2.0, sense: 268, line: 1] train, train, train [score: 3.0, sense: 268, line: 1] train, train, train, train [score: 4.0, sense: 268, line: 1] train, train, train, train, train [score: 5.0, sense: 268, line: 1] train, train, train, train, train, train [score: 6.0, sense: 268, line: 1] train, train [score: 2.0, sense: 238, line: 1] train, train, train [score: 3.0, sense: 238, line: 1] train, train, train, train [score: 4.0, sense: 238, line: 1] train, train, train, train, train [score: 5.0, sense: 238, line: 1] train, train, train, train, train, train [score: 6.0, sense: 238, line: 1] train, train [score: 2.0, sense: 357, line: 1] train, train, train [score: 3.0, sense: 357, line: 1] train, train, train, train [score: 4.0, sense: 357, line: 1] train, train, train, train, train [score: 5.0, sense: 357, line: 1] train, train, train, train, train, train [score: 6.0, sense: 357, line: 1] train, train [score: 2.0, sense: 72, line: 1] train, train, train [score: 3.0, sense: 72, line: 1] train, train, train, train [score: 4.0, sense: 72, line: 1] train, train, train, train, line [score: 5.0, sense: 72, line: 1] train, train, train, train, line, train [score: 6.0, sense: 72, line: 1] train, train, train, train, line, train, train [score: 7.0, sense: 72, line: 1] train, train [score: 2.0, sense: 658, line: 1] train, train, train [score: 3.0, sense: 658, line: 1] train, train, train, train [score: 4.0, sense: 658, line: 1] train, train, train, train, train [score: 5.0, sense: 658, line: 1] train, train, train, train, train, train [score: 6.0, sense: 658, line: 1] train, train [score: 2.0, sense: 461, line: 1] train, train, train [score: 3.0, sense: 461, line: 1] train, train, train, train [score: 4.0, sense: 461, line: 1] train, train, train, train, train [score: 5.0, sense: 461, line: 1]"
    }, {
      "heading" : "Appendix L: A Lexical Chain Building Example",
      "text" : "L - 3\ntrain, train, train, train, train, train [score: 6.0, sense: 461, line: 1] train, train [score: 2.0, sense: 277, line: 1] train, train, train [score: 3.0, sense: 277, line: 1] train, train, train, train [score: 4.0, sense: 277, line: 1] train, train, train, train, train [score: 5.0, sense: 277, line: 1] train, train, train, train, train, train [score: 6.0, sense: 277, line: 1] train, train [score: 2.0, sense: 742, line: 1] train, train, train [score: 3.0, sense: 742, line: 1] train, train, train, train [score: 4.0, sense: 742, line: 1] train, train, train, train, train [score: 5.0, sense: 742, line: 1] train, train, train, train, train, train [score: 6.0, sense: 742, line: 1] train, train [score: 2.0, sense: 71, line: 1] train, train, train [score: 3.0, sense: 71, line: 1] train, train, train, train [score: 4.0, sense: 71, line: 1] train, train, train, train, line [score: 5.0, sense: 71, line: 1] train, train, train, train, line, train [score: 6.0, sense: 71, line: 1] train, train, train, train, line, train, train [score: 7.0, sense: 71, line: 1] train, train [score: 2.0, sense: 228, line: 1] train, train, train [score: 3.0, sense: 228, line: 1] train, train, train, train [score: 4.0, sense: 228, line: 1] train, train, train, train, train [score: 5.0, sense: 228, line: 1] train, train, train, train, train, train [score: 6.0, sense: 228, line: 1] train, train [score: 2.0, sense: 273, line: 1] train, train, train [score: 3.0, sense: 273, line: 1] train, train, train, train [score: 4.0, sense: 273, line: 1] train, train, train, train, train [score: 5.0, sense: 273, line: 1] train, train, train, train, train, train [score: 6.0, sense: 273, line: 1] train, train [score: 2.0, sense: 362, line: 1] train, train, train [score: 3.0, sense: 362, line: 1] train, train, train, train [score: 4.0, sense: 362, line: 1] train, train, train, train, train [score: 5.0, sense: 362, line: 1] train, train, train, train, train, train [score: 6.0, sense: 362, line: 1] train, train [score: 2.0, sense: 441, line: 1] train, train, train [score: 3.0, sense: 441, line: 1] train, train, train, train [score: 4.0, sense: 441, line: 1] train, train, train, train, train [score: 5.0, sense: 441, line: 1] train, train, train, train, train, train [score: 6.0, sense: 441, line: 1] train, rails [score: 2.0, sense: 624, line: 1] train, rails, train [score: 3.0, sense: 624, line: 1] train, rails, train, train [score: 4.0, sense: 624, line: 1] train, rails, train, train, train [score: 5.0, sense: 624, line: 1] train, rails, train, train, train, line [score: 6.0, sense: 624, line: 1] train, rails, train, train, train, line, train [score: 7.0, sense: 624, line: 1] train, rails, train, train, train, line, train, train [score: 8.0, sense: 624, line: 1] train, rails, train, train, train, line, train, train, embankment [score: 9.0, sense: 624, line: 1] train, train [score: 2.0, sense: 669, line: 1] train, train, train [score: 3.0, sense: 669, line: 1] train, train, train, train [score: 4.0, sense: 669, line: 1] train, train, train, train, train [score: 5.0, sense: 669, line: 1] train, train, train, train, train, train [score: 6.0, sense: 669, line: 1] train, train [score: 2.0, sense: 67, line: 1] train, train, train [score: 3.0, sense: 67, line: 1] train, train, train, train [score: 4.0, sense: 67, line: 1] train, train, train, train, train [score: 5.0, sense: 67, line: 1]"
    }, {
      "heading" : "Appendix L: A Lexical Chain Building Example",
      "text" : "L - 4\ntrain, train, train, train, train, train [score: 6.0, sense: 67, line: 1] train, train [score: 2.0, sense: 278, line: 1] train, train, train [score: 3.0, sense: 278, line: 1] train, train, train, train [score: 4.0, sense: 278, line: 1] train, train, train, train, train [score: 5.0, sense: 278, line: 1] train, train, train, train, train, train [score: 6.0, sense: 278, line: 1] train, train [score: 2.0, sense: 288, line: 1] train, train, train [score: 3.0, sense: 288, line: 1] train, train, train, train [score: 4.0, sense: 288, line: 1] train, train, train, train, train [score: 5.0, sense: 288, line: 1] train, train, train, train, train, train [score: 6.0, sense: 288, line: 1] train, train [score: 2.0, sense: 40, line: 1] train, train, train [score: 3.0, sense: 40, line: 1] train, train, train, train [score: 4.0, sense: 40, line: 1] train, train, train, train, train [score: 5.0, sense: 40, line: 1] train, train, train, train, train, train [score: 6.0, sense: 40, line: 1] train, train [score: 2.0, sense: 369, line: 1] train, train, train [score: 3.0, sense: 369, line: 1] train, train, train, train [score: 4.0, sense: 369, line: 1] train, train, train, train, train [score: 5.0, sense: 369, line: 1] train, train, train, train, train, train [score: 6.0, sense: 369, line: 1] train, train [score: 2.0, sense: 610, line: 1] train, train, train [score: 3.0, sense: 610, line: 1] train, train, train, train [score: 4.0, sense: 610, line: 1] train, train, train, train, takes [score: 5.0, sense: 610, line: 1] train, train, train, train, takes, takes [score: 6.0, sense: 610, line: 1] train, train, train, train, takes, takes, train [score: 7.0, sense: 610, line: 1] train, train, train, train, takes, takes, train, train [score: 8.0, sense: 610, line: 1] train, train [score: 2.0, sense: 83, line: 1] train, train, train [score: 3.0, sense: 83, line: 1] train, train, train, train [score: 4.0, sense: 83, line: 1] train, train, train, train, train [score: 5.0, sense: 83, line: 1] train, train, train, train, train, train [score: 6.0, sense: 83, line: 1] train, train [score: 2.0, sense: 689, line: 1] train, train, train [score: 3.0, sense: 689, line: 1] train, train, train, train [score: 4.0, sense: 689, line: 1] train, train, train, train, train [score: 5.0, sense: 689, line: 1] train, train, train, train, train, train [score: 6.0, sense: 689, line: 1] train, train [score: 2.0, sense: 164, line: 1] train, train, train [score: 3.0, sense: 164, line: 1] train, train, train, train [score: 4.0, sense: 164, line: 1] train, train, train, train, train [score: 5.0, sense: 164, line: 1] train, train, train, train, train, train [score: 6.0, sense: 164, line: 1] travelling, travelling [score: 2.0, sense: 265, line: 1] travelling, travelling [score: 2.0, sense: 589, line: 1] travelling, travelling [score: 2.0, sense: 981, line: 1] travelling, travelling, takes [score: 3.0, sense: 981, line: 1] travelling, travelling, takes, takes [score: 4.0, sense: 981, line: 1] travelling, travelling [score: 2.0, sense: 75, line: 1] travelling, direction [score: 2.0, sense: 271, line: 1] travelling, direction, travelling [score: 3.0, sense: 271, line: 1] travelling, travelling [score: 2.0, sense: 276, line: 1] travelling, travelling [score: 2.0, sense: 295, line: 1]"
    }, {
      "heading" : "Appendix L: A Lexical Chain Building Example",
      "text" : "L - 5\ntravelling, travelling [score: 2.0, sense: 793, line: 1] travelling, travelling [score: 2.0, sense: 152, line: 1] travelling, travelling [score: 2.0, sense: 618, line: 1] travelling, travelling [score: 2.0, sense: 145, line: 1] travelling, travelling [score: 2.0, sense: 117, line: 1] travelling, travelling [score: 2.0, sense: 59, line: 1] travelling, travelling [score: 2.0, sense: 282, line: 1] travelling, velocity [score: 2.0, sense: 465, line: 1] travelling, velocity, travelling [score: 3.0, sense: 465, line: 1] travelling, travelling [score: 2.0, sense: 269, line: 1] travelling, travelling [score: 2.0, sense: 266, line: 1] travelling, travelling [score: 2.0, sense: 298, line: 1] travelling, travelling [score: 2.0, sense: 453, line: 1] travelling, travelling [score: 2.0, sense: 194, line: 1] travelling, travelling [score: 2.0, sense: 314, line: 1] travelling, travelling [score: 2.0, sense: 84, line: 1] travelling, travelling, rigid [score: 3.0, sense: 84, line: 1] travelling, travelling [score: 2.0, sense: 305, line: 1] travelling, travelling [score: 2.0, sense: 744, line: 1] rails, respect [score: 1.75, sense: 924, line: 1] constant, rigid [score: 2.0, sense: 494, line: 1] constant, line [score: 2.0, sense: 16, line: 1] direction, line [score: 2.0, sense: 693, line: 1] direction, regard [score: 2.0, sense: 9, line: 1] direction, regard, reference [score: 3.0, sense: 9, line: 1] direction, regard, reference, respect [score: 4.0, sense: 9, line: 1] direction, line [score: 2.0, sense: 281, line: 1] direction, line [score: 2.0, sense: 220, line: 1] direction, line [score: 2.0, sense: 547, line: 1] advantage, line [score: 2.0, sense: 640, line: 2] advantage, takes [score: 2.0, sense: 916, line: 2] advantage, takes, takes [score: 3.0, sense: 916, line: 2] regard, respect [score: 2.0, sense: 920, line: 2] regard, respect [score: 2.0, sense: 887, line: 2] regard, respect [score: 2.0, sense: 880, line: 2] regard, respect [score: 2.0, sense: 768, line: 2] regard, takes [score: 2.0, sense: 438, line: 2] regard, takes, takes [score: 3.0, sense: 438, line: 2] regard, reference [score: 2.0, sense: 10, line: 2] regard, reference, respect [score: 3.0, sense: 10, line: 2] regard, reference [score: 2.0, sense: 923, line: 2] regard, reference, respect [score: 3.0, sense: 923, line: 2] regard, respect [score: 2.0, sense: 866, line: 2] events, event [score: 2.0, sense: 725, line: 2] events, event [score: 2.0, sense: 526, line: 2] events, event [score: 2.0, sense: 1, line: 2] events, event [score: 2.0, sense: 124, line: 2] events, event [score: 2.0, sense: 157, line: 2]"
    }, {
      "heading" : "Appendix L: A Lexical Chain Building Example",
      "text" : "L - 6\nevents, event [score: 2.0, sense: 590, line: 2] events, event [score: 2.0, sense: 716, line: 2] events, event [score: 2.0, sense: 167, line: 2] events, event [score: 2.0, sense: 137, line: 2] events, event [score: 2.0, sense: 8, line: 2] events, event [score: 2.0, sense: 474, line: 2] events, event [score: 2.0, sense: 616, line: 2] events, event [score: 2.0, sense: 596, line: 2] events, event [score: 2.0, sense: 154, line: 2] events, event [score: 2.0, sense: 473, line: 2] takes, takes [score: 2.0, sense: 18, line: 3] takes, takes [score: 2.0, sense: 761, line: 3] takes, takes [score: 2.0, sense: 583, line: 3] takes, takes [score: 2.0, sense: 808, line: 3] takes, takes [score: 2.0, sense: 86, line: 3] takes, takes [score: 2.0, sense: 498, line: 3] takes, takes [score: 2.0, sense: 74, line: 3] takes, takes [score: 2.0, sense: 36, line: 3] takes, takes [score: 2.0, sense: 714, line: 3] takes, takes [score: 2.0, sense: 851, line: 3] takes, takes, respect [score: 3.0, sense: 851, line: 3] takes, takes [score: 2.0, sense: 825, line: 3] takes, takes [score: 2.0, sense: 490, line: 3] takes, takes [score: 2.0, sense: 828, line: 3] takes, takes [score: 2.0, sense: 622, line: 3] takes, takes [score: 2.0, sense: 148, line: 3] takes, takes [score: 2.0, sense: 791, line: 3] takes, takes [score: 2.0, sense: 859, line: 3] takes, takes [score: 2.0, sense: 173, line: 3] takes, takes [score: 2.0, sense: 20, line: 3] takes, takes [score: 2.0, sense: 672, line: 3] takes, takes [score: 2.0, sense: 959, line: 3] takes, takes [score: 2.0, sense: 833, line: 3] takes, takes [score: 2.0, sense: 963, line: 3] takes, takes [score: 2.0, sense: 955, line: 3] takes, takes [score: 2.0, sense: 296, line: 3] takes, takes [score: 2.0, sense: 458, line: 3] takes, takes [score: 2.0, sense: 673, line: 3] takes, takes [score: 2.0, sense: 57, line: 3] takes, takes [score: 2.0, sense: 638, line: 3] takes, takes, respect [score: 3.0, sense: 638, line: 3] takes, takes [score: 2.0, sense: 551, line: 3] takes, takes [score: 2.0, sense: 37, line: 3] takes, takes [score: 2.0, sense: 740, line: 3] takes, takes [score: 2.0, sense: 660, line: 3] takes, takes [score: 2.0, sense: 810, line: 3] takes, takes [score: 2.0, sense: 198, line: 3] takes, takes [score: 2.0, sense: 468, line: 3] takes, takes [score: 2.0, sense: 204, line: 3] takes, takes [score: 2.0, sense: 39, line: 3] takes, takes [score: 2.0, sense: 163, line: 3] takes, takes [score: 2.0, sense: 459, line: 3] takes, takes [score: 2.0, sense: 706, line: 3]"
    }, {
      "heading" : "Appendix L: A Lexical Chain Building Example",
      "text" : "L - 7\ntakes, takes [score: 2.0, sense: 310, line: 3] takes, takes [score: 2.0, sense: 619, line: 3] takes, takes [score: 2.0, sense: 852, line: 3] takes, takes [score: 2.0, sense: 671, line: 3] takes, takes [score: 2.0, sense: 712, line: 3] takes, takes [score: 2.0, sense: 900, line: 3] takes, takes [score: 2.0, sense: 187, line: 3] takes, takes [score: 2.0, sense: 708, line: 3] takes, takes [score: 2.0, sense: 788, line: 3] takes, takes [score: 2.0, sense: 786, line: 3] takes, takes [score: 2.0, sense: 831, line: 3] takes, takes [score: 2.0, sense: 188, line: 3] takes, takes [score: 2.0, sense: 65, line: 3] takes, takes [score: 2.0, sense: 508, line: 3] takes, takes [score: 2.0, sense: 525, line: 3] takes, takes [score: 2.0, sense: 542, line: 3] takes, takes [score: 2.0, sense: 46, line: 3] takes, takes [score: 2.0, sense: 745, line: 3] takes, takes [score: 2.0, sense: 189, line: 3] takes, takes [score: 2.0, sense: 823, line: 3] takes, takes [score: 2.0, sense: 108, line: 3] takes, takes [score: 2.0, sense: 192, line: 3] takes, takes [score: 2.0, sense: 144, line: 3] takes, takes [score: 2.0, sense: 721, line: 3] takes, takes [score: 2.0, sense: 627, line: 3] takes, takes [score: 2.0, sense: 682, line: 3] takes, takes [score: 2.0, sense: 516, line: 3] takes, takes [score: 2.0, sense: 915, line: 3] takes, takes [score: 2.0, sense: 603, line: 3] takes, takes [score: 2.0, sense: 891, line: 3] takes, takes [score: 2.0, sense: 584, line: 3] takes, takes [score: 2.0, sense: 78, line: 3] takes, takes [score: 2.0, sense: 457, line: 3] takes, takes [score: 2.0, sense: 308, line: 3] takes, takes [score: 2.0, sense: 829, line: 3] takes, takes [score: 2.0, sense: 304, line: 3] takes, takes [score: 2.0, sense: 917, line: 3] takes, takes [score: 2.0, sense: 858, line: 3] takes, takes [score: 2.0, sense: 165, line: 3] takes, takes [score: 2.0, sense: 910, line: 3] takes, takes [score: 2.0, sense: 802, line: 3] takes, takes [score: 2.0, sense: 172, line: 3] takes, takes [score: 2.0, sense: 767, line: 3] takes, takes [score: 2.0, sense: 370, line: 3] takes, takes [score: 2.0, sense: 662, line: 3] takes, takes [score: 2.0, sense: 311, line: 3] takes, takes [score: 2.0, sense: 881, line: 3] takes, takes [score: 2.0, sense: 773, line: 3] takes, takes [score: 2.0, sense: 979, line: 3] takes, takes [score: 2.0, sense: 704, line: 3] takes, takes [score: 2.0, sense: 854, line: 3] takes, takes, respect [score: 3.0, sense: 854, line: 3] takes, takes [score: 2.0, sense: 586, line: 3] takes, takes [score: 2.0, sense: 481, line: 3]"
    }, {
      "heading" : "Appendix L: A Lexical Chain Building Example",
      "text" : "L - 8\ntakes, takes [score: 2.0, sense: 664, line: 3] takes, takes [score: 2.0, sense: 985, line: 3] takes, takes [score: 2.0, sense: 211, line: 3] takes, takes [score: 2.0, sense: 836, line: 3] takes, takes [score: 2.0, sense: 895, line: 3] takes, takes [score: 2.0, sense: 550, line: 3] takes, takes [score: 2.0, sense: 986, line: 3] takes, takes [score: 2.0, sense: 807, line: 3] takes, takes [score: 2.0, sense: 855, line: 3] takes, takes [score: 2.0, sense: 31, line: 3] takes, takes [score: 2.0, sense: 178, line: 3] takes, takes [score: 2.0, sense: 388, line: 3] takes, line [score: 2.0, sense: 193, line: 3] takes, line, takes [score: 3.0, sense: 193, line: 3] takes, takes [score: 2.0, sense: 166, line: 3] takes, takes [score: 2.0, sense: 667, line: 3] takes, takes [score: 2.0, sense: 548, line: 3] takes, takes [score: 2.0, sense: 229, line: 3] takes, takes [score: 2.0, sense: 919, line: 3] takes, takes [score: 2.0, sense: 988, line: 3] takes, takes [score: 2.0, sense: 299, line: 3] takes, takes [score: 2.0, sense: 38, line: 3] takes, takes [score: 2.0, sense: 50, line: 3] takes, takes [score: 2.0, sense: 56, line: 3] takes, takes [score: 2.0, sense: 301, line: 3] takes, takes [score: 2.0, sense: 885, line: 3] takes, takes, respect [score: 3.0, sense: 885, line: 3] takes, takes [score: 2.0, sense: 634, line: 3] takes, takes [score: 2.0, sense: 958, line: 3] takes, takes [score: 2.0, sense: 782, line: 3] takes, takes [score: 2.0, sense: 882, line: 3] takes, takes [score: 2.0, sense: 605, line: 3] takes, takes [score: 2.0, sense: 889, line: 3] takes, takes [score: 2.0, sense: 676, line: 3] line, relative [score: 2.0, sense: 27, line: 3] line, definition [score: 2.0, sense: 236, line: 3]\nStep 4: Select the Best Proto-chain for Each Candidate Word\ntrain, rails, train, train, train, line, train, train, embankment [score: 9.0, sense: 624, line: 1] direction, regard, reference, respect [score: 4.0, sense: 9, line: 1] travelling, travelling, takes, takes [score: 4.0, sense: 981, line: 1] suppose, regard, takes, takes [score: 4.0, sense: 485, line: 1] regard, takes, takes [score: 3.0, sense: 438, line: 2] advantage, takes, takes [score: 3.0, sense: 916, line: 2] takes, takes, respect [score: 3.0, sense: 851, line: 3] constant, rigid [score: 2.0, sense: 494, line: 1] events, event [score: 2.0, sense: 725, line: 2] line, relative [score: 2.0, sense: 27, line: 3] rails, respect [score: 1.75, sense: 924, line: 1]"
    }, {
      "heading" : "Appendix L: A Lexical Chain Building Example",
      "text" : "L - 9\nStep 5: Select the Lexical Chains\ntrain, rails, train, train, train, line, train, train, embankment [score: 9.0, sense: 624, line: 1] suppose, regard, takes, takes [score: 4.0, sense: 485, line: 1] direction, reference, respect [score: 3.0, sense: 9, line: 1] travelling, travelling [score: 2.0, sense: 981, line: 1] constant, rigid [score: 2.0, sense: 494, line: 1] events, event [score: 2.0, sense: 725, line: 2]"
    }, {
      "heading" : "Appendix M: The First Two Levels of the WordNet 1.7.1 Noun Hierarchy",
      "text" : "M - 1\nAppendix M: The First Two Levels of the WordNet 1.7.1 Noun Hierarchy\nThis appendix presents the 9 unique beginners of WordNet 1.7.1 and the 161 first level hyponyms.\nentity, physical thing thing causal agent, cause, causal agency object, physical object substance, matter location subject, content, depicted object thing imaginary place anticipation body of water, water enclosure, natural enclosure expanse inessential, nonessential necessity, essential, requirement, requisite, necessary part, piece sky unit, building block variable\npsychological feature cognition, knowledge, noesis motivation, motive, need feeling\nabstraction time space attribute relation measure, quantity, amount, quantum set\nstate\nskillfulness cognitive state, state of mind cleavage medium ornamentation condition condition, status conditionality situation, state of affairs"
    }, {
      "heading" : "Appendix M: The First Two Levels of the WordNet 1.7.1 Noun Hierarchy",
      "text" : "M - 2\nrelationship relationship tribalism utopia dystopia wild, natural state, state of nature isomerism degree, level, stage, point office, power status, position being, beingness, existence nonbeing death employment, employ unemployment order disorder hostility, enmity, antagonism conflict illumination freedom representation, delegacy, agency dependence, dependance, dependency motion motionlessness, stillness dead letter, non-issue action, activity, activeness inaction, inactivity, inactiveness temporary state imminence, imminency, impendence, impendency, forthcomingness readiness, preparedness, preparation physiological state, physiological condition kalemia union, unification maturity, matureness immaturity, immatureness grace, saving grace, state of grace damnation, eternal damnation omniscience omnipotence perfection, flawlessness, ne plus ultra integrity, unity, wholeness imperfection, imperfectness receivership ownership obligation end, destruction, death revocation, annulment sale turgor homozygosity"
    }, {
      "heading" : "Appendix M: The First Two Levels of the WordNet 1.7.1 Noun Hierarchy",
      "text" : "M - 3\nheterozygosity polyvalence, polyvalency, multivalence, mulltivalency utilization\nevent\nmight-have-been nonevent happening, occurrence, natural event social event miracle migration Fall\nact, human action, human activity\naction nonaccomplishment, nonachievement leaning motivation, motivating assumption rejection forfeit, forfeiture, sacrifice activity wear, wearing judgment, judgement, assessment production stay residency, residence, abidance inactivity hindrance, interference stop, stoppage group action distribution legitimation waste, permissive waste proclamation, promulgation communication, communicating speech act\ngroup, grouping\narrangement straggle kingdom biological group community, biotic community world, human race, humanity, humankind, human beings, humans, mankind, man people social group collection, aggregation, accumulation, assemblage edition electron shell ethnic group, ethnos"
    }, {
      "heading" : "Appendix M: The First Two Levels of the WordNet 1.7.1 Noun Hierarchy",
      "text" : "M - 4\nrace association subgroup sainthood citizenry, people population multitude, masses, mass, hoi polloi, people circuit system series actinoid, actinide, actinon rare earth, rare-earth element, lanthanoid, lanthanide, lanthanon halogen\npossession\nproperty, belongings, holding, material possession territory, dominion, territorial dominion, province white elephant transferred property, transferred possession circumstances assets treasure liabilities\nphenomenon\nnatural phenomenon levitation metempsychosis, rebirth consequence, effect, outcome, result, event, issue, upshot luck, fortune, chance, hazard luck, fortune pulsation process"
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2003,
    "abstractText" : "WordNet proved that it is possible to construct a large-scale electronic lexical database on the<lb>principles of lexical semantics. It has been accepted and used extensively by computational<lb>linguists ever since it was released. Some of its applications include information retrieval,<lb>language generation, question answering, text categorization, text classification and word sense<lb>disambiguation. Inspired by WordNet's success, we propose as an alternative a similar resource,<lb>based on the 1987 Penguin edition of Roget’s Thesaurus of English Words and Phrases. Peter Mark Roget published his first Thesaurus over 150 years ago. Countless writers, orators<lb>and students of the English language have used it. Computational linguists have employed<lb>Roget’s for almost 50 years in Natural Language Processing. Some of the tasks they have used it<lb>for include machine translation, computing lexical cohesion in texts and constructing databases<lb>that can infer common sense knowledge. This dissertation presents Roget’s merits by explaining<lb>what it really is and how it has been used, while comparing its applications to those of WordNet.<lb>The NLP community has hesitated in accepting Roget’s Thesaurus because a proper machine-<lb>tractable version was not available. This dissertation presents an implementation of a machine-tractable version of the 1987 Penguin<lb>edition of Roget’s Thesaurus – the first implementation of its kind to use an entire current<lb>edition. It explains the steps necessary for taking a machine-readable file and transforming it into<lb>a tractable system. This involves converting the lexical material into a format that can be more<lb>easily exploited, identifying data structures and designing classes to computerize the Thesaurus.<lb>Roget’s organization is studied in detail and contrasted with WordNet’s. We show two applications of the computerized Thesaurus: computing semantic similarity<lb>between words and phrases, and building lexical chains in a text. The experiments are performed<lb>using well-known benchmarks and the results are compared to those of other systems that use<lb>Roget’s, WordNet and statistical techniques. Roget’s has turned out to be an excellent resource<lb>for measuring semantic similarity; lexical chains are easily built but more difficult to evaluate.<lb>We also explain ways in which Roget’s Thesaurus and WordNet can be combined. To my parents, who are my most valued treasure.",
    "creator" : "PScript5.dll Version 5.2"
  }
}