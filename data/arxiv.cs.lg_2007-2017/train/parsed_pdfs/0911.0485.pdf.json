{
  "name" : "0911.0485.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Novel Intrusion Detection using Probabilistic Neural Network and Adaptive Boosting",
    "authors" : [ "Tich Phuoc Tran", "Longbing Cao", "Dat Tran", "Cuong Duc Nguyen" ],
    "emails" : [ "lbcao}@it.uts.edu.au", "Dat.Tran@canberra.edu.au", "ndcuong@hcmiu.edu.vn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "techniques to solve Intrusion Detection problems within computer networks. Due to complex and dynamic nature of computer networks and hacking techniques, detecting malicious activities remains a challenging task for security experts, that is, currently available defense systems suffer from low detection capability and high number of false alarms. To overcome such performance limitations, we propose a novel Machine Learning algorithm, namely Boosted Subspace Probabilistic Neural Network (BSPNN), which integrates an adaptive boosting technique and a semi-parametric neural network to obtain good trade-off between accuracy and generalty. As the result, learning bias and generalization variance can be significantly minimized. Substantial experiments on KDD-99 intrusion benchmark indicate that our model outperforms other state-of-the-art learning algorithms, with significantly improved detection accuracy, minimal false alarms and relatively small computational complexity.\nKeywords- Intrusion Detection, Neural Network,\nAdaptive Boosting\nI. INTRODUCTION\nAs more and more corporations rely on computers and networks for communications and critical business transactions, securing digital information has become one of the largest concerns of the business community. A powerful security system is not only a requirement but essential to the livelihood of enterprises. In recent years, there has been a great deal of research conducted in this area to develop intelligent and automated security tools which can fight the latest cyber attacks. Alongside with static defense mechanisms such as keeping operating systems up-to-date or deploying firewalls at critical network segments for access control, more advanced defense systems, namely Intrusion Detection Systems (IDS), are becoming an important part of today’s network security architectures. Particularly, IDS can be used to monitor computers or networks for unauthorized activities based on network traffic or system usage behaviors, thereby detect if a system is targeted by a network attack such as a denial of service attack.\nThe majority of currently existing IDS face a number of challenges such as low detection rates which can miss serious intrusion attacks and high false alarm rates, which falsely classifies a normal connection as an attack and therefore obstructs legitimate user access to the network resources [1]. These problems are due to the sophistication of the attacks and their intended similarities to normal behavior. More intelligence is brought into IDS by means of Machine Learning (ML). Theoretically, it is possible for a ML algorithm to achieve the best performance, i.e. it can minimize the false alarm rate and maximize the detection accuracy. However, this normally requires infinite training sample sizes (theoretically) [2]. In practice, this condition is impossible due to limited computational power and real-time response requirement of IDS. IDS must be active in real time and they cannot allow much delay because this would cause a bottleneck to the whole network.\nTo overcome the above limitations of currently existing IDS, we propose an efficient Boosted Subspace Probabilistic Neural Network (BSPNN) to enhance the performance of intrusion detection for rare and complicated attacks. BSPNN combines and improves a Vector Quantized-Generalized Regression Neural Network (VQ-GRNN) with an ensemble technique to improve detection accuracy while minimizing computation overheads by tuning of models. Because this method combines the virtues of boosting and neural network technologies, it has both high data fitting capability and high system robustness. To evaluate our approach, substantial experiments are conducted on the KDD-99 intrusion detection benchmark. The proposed algorithm clearly demonstrates superior classification performance compared with other well-known techniques in terms of bias and variance for the real life problems.\nII. NETWORK INTRUSION DETECTION AND"
    }, {
      "heading" : "RELATED WORKS",
      "text" : "Because most computers today are connected to the Internet, network security has become a major concern for organizations throughout the world. Alongside the existing techniques for preventing intrusions such as\n83 http://sites.google.com/site/ijcsis/ ISSN 1947-5500\nencryption and firewalls, Intrusion Detection technology has established itself as an emerging research field that is concerned with detecting unauthorized access and abuse of computer systems from both internal users and external offenders. An Intrusion Detection System (IDS) is defined as a protection system that monitors computers or networks for unauthorized activities based on network traffic or system usage behaviors, thereby detecting if a system is targeted by a network attack such as a denial of service attack [4]. In response to those identified adversarial transactions, IDS can inform relevant authorities to take corrective actions.\nThere are a large number of IDS available on the market to complement firewalls and other defense techniques. These systems are categorized into two types of IDS, namely (1) misuse-based detection in which events are compared against pre-defined patterns of known attacks and (2) anomaly-based detection which relies on detecting the activities deviating from system “normal” operations.\nIn addition to the overwhelming volume of generated network data, rapidly changing technologies present a great challenge for today’s security systems with respect to attack detection speed, accuracy and system adaptability. In order to overcome such limitations, there has been considerable research conducted to apply ML algorithms to achieve a generalization capability from limited training data. That means, given known intrusion signatures, a security system should be able to detect similar or new attacks. Various techniques such as association rules, clustering, Naïve Bayes, Support Vector Machines, Genetic Algorithms, Neural Networks, and others have been developed to detect intrusions. This section provides a brief literature review on these technologies and related frameworks.\nOne of the rule-based methods which is commonly used by early IDS is the Expert System (ES) [3, 4]. In such a system, the knowledge of human experts is encoded into a set of rules. This allows more effective knowledge management than that of a human expert in terms of reproducibility, consistency and completeness in identifying activities that match the defined characteristics of misuse and attacks. However, ES suffers from low flexibility and robustness. Unlike ES, data mining approaches derive association rules and frequent episodes from available sample data, not from human experts. Using these rules, Lee et. al. developed a data mining framework for the purpose of intrusion detection [5, 6]. In particular, system usage behaviors are recorded and analyzed to generate rules which can recognize misuse attacks. The drawback of such frameworks is that they tend to produce a large number of rules and thereby, increase the complexity of the system.\nDecision trees are one of the most commonly used supervised learning algorithms in IDS [7-11] due to its simplicity, high detection accuracy and fast adaptation. Another high performing method is Artificial Neural Networks (ANN) which can model both linear and nonlinear patterns. ANN-based IDS [12-15] have achieved great successes in detecting difficult attacks. For unsupervised intrusion detection, data clustering methods can be applied [16, 17]. These methods involve computing a distance between numeric features and therefore they cannot easily deal with symbolic attributes, resulting in inaccuracy.\nAnother well-known ML techniques used in IDS is Naïve Bayes classifiers [7]. Because Naïve Bayes assumes that features are independent, which is often not the case for intrusion detection, correlated features may degrade its performance. In [18], the authors apply a Bayesian network for IDS. The network appears to be attack specific and its size grows rapidly as the number of features and attack types increase.\nBeside popular decision trees and ANN, Support Vector Machines (SVMs) are also a good candidate for intrusion detection systems [14, 19] which can provide real-time detection capability, deal with large dimensionality of data. SVMs plot the training vectors in high dimensional feature space through nonlinear mapping and labeling each vector by its class. The data is then classified by determining a set of support vectors, which are members of the set of training inputs that outline a hyperplane in the feature space.\nSeveral other AI paradigms including linear genetic programming [20] , Hidden Markov Model [21], Columbia Model [22] and Layered Conditional Random Fields [23] have been applied for the design of IDS.\nIII. BOOSTED SUBSPACE PROBABILISTIC NEURAL"
    }, {
      "heading" : "NETWORK (BSPNN)",
      "text" : ""
    }, {
      "heading" : "A. Bias-Variance-Computation Dilemma",
      "text" : "Several ML techniques have been adopted in the Network Security domain with certain success; however, there remain severe limitations. Firstly, we consider Artificial Neural Network (ANN) because of its wide popularity and well-known characteristics. As a flexible “model-free\" learning method, ANN can fit training data very well and thus provide a low learning bias. However, they are susceptible to overfitting, which can cause instability in generalization [24]. Recent remedies try to improve the model stability by reducing generalization variance at the cost of worse learning bias, i.e. allowing underfitting. However, underfitting is not acceptable for some applications requiring high classification accuracy. Therefore, a system which can achieve both stable generalization and accurate learning is imperative for applications as\n84 http://sites.google.com/site/ijcsis/ ISSN 1947-5500\nin Intrusion Detection [19]. Mathematically, both bias and variance may be reduced at the same time given infinite sized models. However, this is infeasible since computing resources must be limited in real life. We develop a learning algorithm which provides a good tradeoff for learning bias, generalization variance and computational requirement motivated by the need of an accurate detection system for Intrusion Detection."
    }, {
      "heading" : "B. Objectives",
      "text" : "This paper is inspired by a light-weight ANN model, namely Vector Quantized-Generalized Regression Neural Network (VQ-GRNN) [25], which reduces the nonparametric GRNN [26] to a semiparametric model by applying vector quantization techniques on the training data, i.e. clustering the input space into a smaller subspace. Compared with GRNN method which incorporates every training vector into its structure, VQ-GRNN only applies on a smaller number of clusters of input data. This significantly improves the robustness of the algorithm (low variance), but also controls its learning accuracy to some extent [24]. To make the VQ-GRNN suitable for Intrusion Detection problems, i.e. enhancing its accuracy, we propose the Boosted Subspace Probabilistic Neural Network (BSPNN) which combines VQ-GRNN and Ensemble Learning technique. Ensemble methods such as Boosting [27] iteratively learn multiple classifiers (base classifiers) on different distributions of training data. It particularly guides changes of the training data to direct further classifiers toward more “difficult cases”, i.e. putting more weights for previously misclassified instances. It then combines base classifiers in such a way that the composite – boosted learner – outperforms the single classifiers. Amongst popular boosting variants, we choose Adaptive Boosting or AdaBoost [28] to improve performance of VQ-GRNN. AdaBoost is the most widely adopted method which allows the designer to continue adding weak learners whose accuracy is only moderate until some desired low training error has been achieved. AdaBoost is “adaptive” in the sense that it does not require prior knowledge of the accuracy of these hypotheses [27]. Instead, it measures the accuracy of a base hypothesis at each iteration and sets its parameters accordingly.\nAlthough classifier combinations (as in boosting) can improve generalization performance, correlation between individual classifiers can be harmful to the final composite model. Moreover, it is widely accepted that generalization performance of a combined classifier is not necessarily achieved by combining classifiers with better individual performance but by including independent classifiers in the ensemble [9]. Therefore, such independence condition among individual classifiers which is normally termed as\northogonality, diversity or disagreement is required to obtain a good ensemble."
    }, {
      "heading" : "C. Model description",
      "text" : "As shown in Figure 1, the proposed BSPNN algorithm has two major modules: the Adaptive Booster and the Modified Probabilistic Classifier.\nGiven the input data , | 1 … where output vector 1 … , the BSPNN algorithm aims to produce a classifier F such that:\nIn this research, we implement F (referred to as\nAdaptive Booster), using SAMME algorithm [29].\nF learns by iteratively training a Modified Probabilistic Classifier f on weighted data samples S and their weights are updated by the Distribution Generator according to previously created models of f. This base learner f is actually a modified version of the emerging VQ-GRNN model [25] (called Modified GRNN Base learner) in which the input data space is reduced significantly (by the Weighted vector quantization module) and its output is computed by a linearly weighted mixture of Radial Basis Function (RBF). This process is repeated until F reaches a desired number of iterations or its Mean Squared Error (MSE) approaches an appropriate level. The base hypotheses returned from f are finally combined by the Hypothesis Aggregator:\n.\nThis combination depends not only on the misclassification error of previously added but also the diversity of the ensemble at that time. The Diversity Checker measures ensemble diversity by using Kohavi-Wolpert variance [30] (which is denoted by the hypothesis weighting coefficient ). To avoid any confusion, the adaptive booster F is called the master algorithm while f refers to the base learner. They are described in greater details in next sections.\n1) Adaptive Booster The Adaptive Booster iteratively produces base hypotheses on a weighted training dataset. The weights are updated adaptively based on the classification performance of component hypotheses. The generated hypotheses are then integrated via a weighted sum based on their diversity.\n85 http://sites.google.com/site/ijcsis/ ISSN 1947-5500\n2) Modified Probabilistic Classifier (Base Learner)\nThe Modified Probabilistic Classifier serves as the base learner which can be trained on { , _#} repeatedly by the Adatptive Booster to obtain the hypothesis\n86 http://sites.google.com/site/ijcsis/ ISSN 1947-5500\n$#: & '61, `1) In each boosting iteration, a base hypothesis is created with associated accuracy and diversity measures. From this information, the data weights are updated for the next iteration and the final weighting of that hypothesis in the joint classification is computed.\nWe adapt VQ-GRNN [25] as a base learner in our BSPNN model. VQ-GRNN is closely related to Specht’s GRNN [26] and PNN [31] classifiers. This adaptation of VQ-GRNN can produce confidence-rated outputs and it is modified such that it utilizes weights associated with training examples (to compute cluster center vectors and find a single smoothing factor) and incorporates these weights as penalties for misclassifications (e.g. weighted MSE). This modified version of VQ-GRNN is similar to the original one in that a single kernel bandwidth is tuned to achieve satisfactory learning. They both cluster close training vectors according to a very simple procedure related to vector quantization. A number of equally sized radial basis functions are placed at each and every center vector location. These functions are approximated:\n1 6 , a3 b c@ de\nf @1 6 g@ , a3\nThis approximation is reasonable because the vectors are close to each other in the input vector space. Using this idea, the VQ-GRNN’s equation can be generalized [25]:\nh1 3 ∑ c 1 6 g , a3 f\n∑ c 1 6 g , a3 f\nWhere g is the center vector for class i in the input space, , a is the radial basis function with centre x and the width parameter a , is the ouput related related to g , c is the number of vectors 2 associated with centre g . ∑ c i is the total number of training vectors.\nThe above formula can be extended to a multiclass classification problem by redefining the output vector as a K-dimensional vector (K is the number of classes):\n, … , * ^ where @ is the class membership probability of the k-th class of the vector . If the vector is of class k, then @ 1.0 and @A 0 for the remaining vector elements (D j Dk). An input vector x is classified to class-k if the k-th element of the output vector has the highest magnitude.\nTo suit ensemble learning, VQ-GRNN is adapted such that it incoperates the weights associated with each training vector into the learning process, i.e. using them in cluster center formation and Mean Square Error (MSE) calculation for realzing the smoothing factor a.\nSuch modifications make VQ-GRNN specially suited for boosting. In particular, the center vector g is computed as:\ng@ ∑ \" de\nc@ where c@ is the number of training vectors belonging to a cluster k; \" is the weight associated with .\nVQ-GRNN’s learning involves finding the optimal bandwidth a giving the minimum MSE. In our implementation, a Weighted MSE (WMSE) is used instead:\n\"l m ∑ '\" h 6 ) n\nwhere o and h are the associated weight and\nprediction of an example , , i = 1…N 3) Remarks on BSPNN\nThe high accuracy of BSPNN can be attributed to the boosting effects of SAMME method implemented in the Adaptive Booster module. By sufficiently handling the multiclass problem and using confidencerated predictions, SAMME can maximize the distribution margins of the training data [32]. Also, our implementation of Kohavi-Wolpert variance (KW) [30] in the reweighting of hypotheses in the joint classification can effectively enforce the ensemble diversity. The Modified Probabilistic Classifier has very fast adaptation and it is modified to better integrate with the Adaptive Booster module. Particularly, after being modified, it can produce confidence rated outputs and fully utilize the weights given by the booster into learning process. In the next sections, we apply BSPNN into specific Intrusion Detection problems.\nIV. APPLICATION TO NETWORK INTRUSION"
    }, {
      "heading" : "DETECTION",
      "text" : "Current IDS suffer from low detection accuracy and insufficient system robustness for new and rare security breaches. In this section, we apply our BSPNN to identify known and novel attacks in the KDD-99 dataset [1], containing TCP/IP connection records. Each record consisted of 41 attributes (features) and one target value (labeled data) which indicates whether a connection is Normal or an attack. There are 40 types of attacks, classified into four major categories, namely Probing (Probe) (collect information of target system prior to an attack), Denial of Service (DoS) (prevent legitimate requests to a network resource by consuming the bandwidth or overloading computational resources), User-to-Root (U2R) (attackers with normal user level access gain privileges of root user), and Remote-to-Local (R2L) (unauthorized users gain the ability to execute commands locally).\n87 http://sites.google.com/site/ijcsis/ ISSN 1947-5500\nTable 2 describes the components of KDD-99 dataset (referred to as Whole KDD): 10% KDD containing 26 known attack types (for training) and Corrected KDD containing 14 novel attacks (for testing)."
    }, {
      "heading" : "A. Experiment Setup",
      "text" : "1) Cost-Sensitive Evaluation Because an error on a particular class may not be equally serious as errors on other classes, we should consider misclassification cost for intrusion detection. Given a test set, the average cost of a classifier is calculated as below [1]:\nVGpq ∑ ∑ VGO l , r s VGpql , r t2 t ( 4) Where\nN: total number of connections in the dataset\nConfM(i,j): the entry at row i, column j in the confusion matrix.\nCostM(i,j): the entry at row i, column j in the cost matrix.\n2) Datasets Creation First, we consider anomaly detection where only normal connection records are available for training. Any connections that differ from these normal records are classified as “abnormal” without further specifying which attack categories it actually belongs to. For this purpose, we filter all known intrusions from the 10% KDD to form a pure normal dataset (Norm).\nFor misuse detection, we inject the 26 known attacks into Norm to classify 14 novel ones. For example, from the Probe attacks that appeared in the training set (ipsweep., nmap., portspeep., satan.), we aim to detect unseen Probe attacks that were only included in testing data (mscan., saint.). In [33], artificial anomalies are added to the training data to help the learner discover a boundary around the available training data. The method particularly changes the value of one feature of a connection while leaving other features unaltered. However, we do not adopt this method due to its high false alarm rate and its unconfirmed assumption that the boundary is very close to the known data and that they do not intersect one another. Instead, we group 26 known intrusions into 13 clusters V , … , V u (note that these clusters are not artificially generated but real incidents, available in “10% KDD” set) and use it for classification. Each cluster contains intrusions that require similar features\nfor effective detection and this method, as detailed in [33], is not influenced by cluster orders.\nIn our experiments, we first created 13 datasets v , … , v u , as shown in Table 3, by incrementally adding each cluster V into the normal dataset (Norm) to simulate the evolution of new intrusions:\nv@ GFw ` V @\nv@N ` V@\nThe BSPNN and other learning methods are then tested against the “Corrected KDD” testing set, containing both known and unknown attacks."
    }, {
      "heading" : "B. Experiment Result",
      "text" : "1) Anomaly Detection We train BSPNN on the pure Normal dataset (Norm)\nto detect anomalies in “Corrected KDD” testing set.\nTable 4 shows that our BSPNN obtains competitive detection rate compared with [33] while achieves significantly lower false alarm rate (1.1%), minimizing major drawbacks of anomaly detection.\n2) Misuse Detection To test the effect of having known intrusions in the training set on the overall performance, we run BSPNN on the 13 training sets: v , … , v u. Its detection rates (DR) on different attack categories are displayed in Figure 2. We could discover a general trend of increasing performance as more intrusions are added into training set. In particular, detection of R2L attacks requires less known intrusion data (DR starts rising at vx) than that of other classes.\nUsing the full training set (v u), we test our BSPNN against other existing methods, including the KDD-99 winner [8], the rule-based PNrule approach [34], the multi-class Support Vector Machine [19], the Layered Conditional Random Fields Framework (LCRF) [23], the Columbia Model [22] and the Decision Tree method [11]. Their Detection Rate (DR) and False Alarm Rate (FAR) are reported in Table 5, with highest DR and lowest FAR for each class in bold.\n88 http://sites.google.com/site/ijcsis/ ISSN 1947-5500\nTABLE III. CLUSTERS OF KNOWN INTRUSION\n8 back\n8y buffer_overflow, loadmodule, perl, rootkit\n8z ftp_write, warezclient, warezmaster\n8{ guess_passwd\n8| imap\n8} land\n8~ portsweep, satan\n8 ipsweep, nmap\n8 multihop\n8 neptune\n8 phf\n8 y pod, teardrop\n8 z spy, smurf\nTABLE IV. ANOMALY DETECTION RATE (DR) AND FALSE ALARM RATE (FAR) FOR ANOMALY DETECTION\nFor Probe and DoS attacks, BSPNN can achieve slightly better DR than other algorithms with very competitive FAR. Though improvement for detection of Normal class is not significant, our model can, in fact, get a remarkably low FAR. In addition, a clear performance superiority is claimed for BSPNN in the case of U2R and R2L classes.\nIt is also important to note that, since KDD-99 dataset is unbalanced (U2R and R2L appeared rarely),\nthe baseline models can only classify the major classes and performs poorly on other minor ones, while our BSPNN exhibits superior detection power for all classes. Significant improvement in detection of more dangerous attacks (U2R, R2L) leads to lower total weight of misclassification of 0.1523 compared with 0.2332 of the KDD-99 winner.\n89 http://sites.google.com/site/ijcsis/ ISSN 1947-5500\nVol. 6, No. 1, 2009 [18] C. Kruegel, D. Mutz, W. Robertson, and F. Valeur, \"Bayesian Event\nClassification for Intrusion Detection,\" Proc. 19th Annual Computer Security Applications Conference, pp. 14-23, 2003.\n[19] T. Ambwani, \"Multi class support vector machine implementation to intrusion detection,\" in Proc. of IJCNN, 2003, pp. 2300-2305.\n[20] D. Song, M. I. Heywood, and A. N. Zincir-Heywood, \"Training Genetic Programming on Half a Million Patterns: An Example from Anomaly\nDetection,\" IEEE Trans. Evolutionary Computation, vol. 9, pp. 225-239, 2005.\n[21] W. Wang, X. H. Guan, and X. L. Zhang, \"Modeling Program Behaviors by Hidden Markov Models for Intrusion Detection,\" Proc. International\nConference Machine Learning and Cybernetics, vol. 5, pp. 2830-2835, 2004.\n[22] W. Lee and S. Stolfo, \"A Framework for Constructing Features and Models for Intrusion Detection Systems,\" Information and System Security, vol. 4, pp. 227-261, 2000.\n[23] K. K. Gupta, B. Nath, and R. Kotagiri, \"Layered Approach using Conditional Random Fields for Intrusion Detection,\" IEEE Transactions on Dependable and Secure Computing, vol. 5, 2008.\n[24] A. Zaknich, Neural Networks for Intelligent Signal Processing. Sydney: World Scientific Publishing, 2003.\n[25] A. Zaknich, \"Introduction to the modified probabilistic neural network for general signal processing applications,\" IEEE Transactions on Signal\nProcessing, vol. 46, pp. 1980-1990, 1998.\n[26] D. F. Spetch, \"A general regression neural network,\" IEEE Transactions on Neural Networks, vol. 2, pp. 568-576, 1991.\n[27] R. E. Schapire, \"A brief introduction to boosting,\" in Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence, San Francisco, CA, 1999, pp. 1401-1406.\n[28] Y. Freund and R. Schapire, \"A decision-theoretic generation of on-line learning and an application to boosting,\" Journal of Computer and System Science, vol. 55, pp. 119–139, 1997.\n[29] J. Zhu, S. Rosset, H. Zhou, and T. Hastie, \"Multiclass adaboost,\" The Annals of Applied Statistics, vol. 2, pp. 1290--1306., 2005.\n[30] R. Kohavi and D. Wolpert, \"Bias plus variance decomposition for zeroone loss functions,\" in Proc. of International Conference on Machine Learning Italy, 1996, pp. 275-283.\n[31] D. F. Specht, \"Probabilistic neural networks,\" Neural Networks, vol. 3, pp. 109-118, 1990.\n[32] J. Huang, S. Ertekin, Y. Song, H. Zha, and C. L. Giles, \"Efficient Multiclass Boosting Classification with Active Learning,\" ICDM, 2007.\n[33] W. Fan, M. Miller, S. Stolfo, W. Lee, and P. Chan, \"Using artificial anomalies to detect unknown and known network intrusions,\" Knowledge and Information Systems, vol. 6, pp. 507–527, 2004.\n[34] R. Agarwal and M. V. Joshi, \"PNrule: A New Framework for Learning Classifier Models in Data Mining,\" in A Case-Study in Network Intrusion Detection, 2000.\n91 http://sites.google.com/site/ijcsis/ ISSN 1947-5500"
    } ],
    "references" : [ {
      "title" : "Results of the KDD’99 Classifier Learning,",
      "author" : [ "C. Elkan" ],
      "venue" : "ACM SIGKDD Explorations,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2000
    }, {
      "title" : "Machine Learning and Data Mining: Introduction to Principles and Algorithms",
      "author" : [ "I. Kononenko", "M. Kukar" ],
      "venue" : "Horwood Publishing Limited,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2007
    }, {
      "title" : "NIDX – an expert system for realtime network intrusion detection,\" in Proceeding of the Computer Networking",
      "author" : [ "D.S. Bauer", "M.E. Koblentz" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1988
    }, {
      "title" : "State transition analysis: a rulebased intrusion detection approach,",
      "author" : [ "K. Ilgun", "R. Kemmerer", "P. Porras" ],
      "venue" : "IEEE Transactions on Software Engineering,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1995
    }, {
      "title" : "Mining Audit Data to Build Intrusion Detection Models,",
      "author" : [ "W. Lee", "S. Stolfo", "K. Mok" ],
      "venue" : "Proc. Fourth International Conference Knowledge Discovery and Data Mining pp",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1999
    }, {
      "title" : "A Data Mining Framework for Building Intrusion Detection",
      "author" : [ "W. Lee", "S. Stolfo", "K. Mok" ],
      "venue" : "Model,\" Proc. IEEE Symp. Security and Privacy,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1999
    }, {
      "title" : "Naive Bayes vs. Decision Trees in Intrusion Detection Systems,",
      "author" : [ "N.B. Amor", "S. Benferhat", "Z. Elouedi" ],
      "venue" : "Proc. ACM Symp. Applied Computing,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2004
    }, {
      "title" : "Winning the KDD99 Classification Cup: Bagged Boosting,",
      "author" : [ "B. Pfahringer" ],
      "venue" : "SIGKDD Explorations,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2000
    }, {
      "title" : "The MP13 Approach to the KDD’99 Classifier Learning Contest,",
      "author" : [ "V. Miheev", "A. Vopilov", "I. Shabalin" ],
      "venue" : "SIGKDD Explorations,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2000
    }, {
      "title" : "KDD-99 Classifier Learning Contest: LLSoft’s Results Overview,",
      "author" : [ "I. Levin" ],
      "venue" : "SIGKDD Explorations,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2000
    }, {
      "title" : "Effective Value of Decision Tree with KDD 99 Intrusion Detection Datasets for Intrusion Detection",
      "author" : [ "J.-H. Lee", "S.-G. Sohn", "J.-H. Ryu", "T.-M. Chung" ],
      "venue" : "System,\" in 10th International Conference on Advanced Communication Technology",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2008
    }, {
      "title" : "Ucles, \"HIDE: A Hierarchical Network Intrusion Detection System Using Statistical Preprocessing and Neural Network Classification,",
      "author" : [ "Z. Zhang", "J. Li", "C.N. Manikopoulos", "J. Jorgenson" ],
      "venue" : "Proc. IEEE Workshop Information Assurance and Security,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2001
    }, {
      "title" : "Artificial neural networks for misuse",
      "author" : [ "J. Cannady" ],
      "venue" : "Proceedings of the National Information Systems Security Conference Arlington,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1998
    }, {
      "title" : "Intrusion detection using neural networks and support vector machines,",
      "author" : [ "S. Mukkamala", "G. Janoski", "A. Sung" ],
      "venue" : "in International Joint Conference on Neural Networks (IJCNN)",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2002
    }, {
      "title" : "Kanthamanon, \"Hybrid neural networks for intrusion detection",
      "author" : [ "C. Jirapummin", "N. Wattanapongsakorn" ],
      "venue" : "Proceedings of The 2002 International Technical Conference On Circuits/Systems,Computers and Communications,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2002
    }, {
      "title" : "Intrusion Detection with Unlabeled Data Using Clustering,",
      "author" : [ "L. Portnoy", "E. Eskin", "S. Stolfo" ],
      "venue" : "Proc. ACM Workshop Data Mining Applied to Security (DMSA),",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2001
    }, {
      "title" : "Fuzzy Clustering for Intrusion Detection,",
      "author" : [ "H. Shah", "J. Undercoffer", "A. Joshi" ],
      "venue" : "Proc. 12th IEEE International Conference Fuzzy Systems (FUZZ-IEEE ’03),",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2003
    }, {
      "title" : "Bayesian Event Classification for Intrusion Detection,",
      "author" : [ "C. Kruegel", "D. Mutz", "W. Robertson", "F. Valeur" ],
      "venue" : "Proc. 19th Annual Computer Security Applications Conference,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2003
    }, {
      "title" : "Multi class support vector machine implementation to intrusion detection,",
      "author" : [ "T. Ambwani" ],
      "venue" : "in Proc. of IJCNN,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2003
    }, {
      "title" : "Training Genetic Programming on Half a Million Patterns: An Example from Anomaly Detection,",
      "author" : [ "D. Song", "M.I. Heywood", "A.N. Zincir-Heywood" ],
      "venue" : "IEEE Trans. Evolutionary Computation,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2005
    }, {
      "title" : "Modeling Program Behaviors by Hidden Markov Models for Intrusion Detection,",
      "author" : [ "W. Wang", "X.H. Guan", "X.L. Zhang" ],
      "venue" : "Proc. International Conference Machine Learning and Cybernetics,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2004
    }, {
      "title" : "A Framework for Constructing Features and Models for Intrusion Detection Systems,",
      "author" : [ "W. Lee", "S. Stolfo" ],
      "venue" : "Information and System Security,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2000
    }, {
      "title" : "Layered Approach using Conditional Random Fields for Intrusion Detection,",
      "author" : [ "K.K. Gupta", "B. Nath", "R. Kotagiri" ],
      "venue" : "IEEE Transactions on Dependable and Secure Computing,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2008
    }, {
      "title" : "Neural Networks for Intelligent Signal Processing",
      "author" : [ "A. Zaknich" ],
      "venue" : "Sydney: World Scientific Publishing,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2003
    }, {
      "title" : "Introduction to the modified probabilistic neural network for general signal processing applications,",
      "author" : [ "A. Zaknich" ],
      "venue" : "IEEE Transactions on Signal Processing,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 1998
    }, {
      "title" : "A general regression neural network,",
      "author" : [ "D.F. Sp" ],
      "venue" : "IEEE Transactions on Neural Networks,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 1991
    }, {
      "title" : "A brief introduction to boosting,",
      "author" : [ "R.E. Schapire" ],
      "venue" : "Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 1999
    }, {
      "title" : "A decision-theoretic generation of on-line learning and an application to boosting,",
      "author" : [ "Y. Freund", "R. Schapire" ],
      "venue" : "Journal of Computer and System Science,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 1997
    }, {
      "title" : "Bias plus variance decomposition for zeroone loss functions,",
      "author" : [ "R. Kohavi", "D. Wolpert" ],
      "venue" : "in Proc. of International Conference on Machine Learning Italy,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 1996
    }, {
      "title" : "Probabilistic neural networks,",
      "author" : [ "D.F. Specht" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 1990
    }, {
      "title" : "Efficient Multiclass Boosting Classification with Active Learning,",
      "author" : [ "J. Huang", "S. Ertekin", "Y. Song", "H. Zha", "C.L. Giles" ],
      "venue" : null,
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2007
    }, {
      "title" : "Using artificial anomalies to detect unknown and known network intrusions,",
      "author" : [ "W. Fan", "M. Miller", "S. Stolfo", "W. Lee", "P. Chan" ],
      "venue" : "Knowledge and Information Systems,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2004
    }, {
      "title" : "PNrule: A New Framework for Learning Classifier Models in Data Mining,\" in A Case-Study in Network",
      "author" : [ "R. Agarwal", "M.V. Joshi" ],
      "venue" : "Intrusion Detection,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2000
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "The majority of currently existing IDS face a number of challenges such as low detection rates which can miss serious intrusion attacks and high false alarm rates, which falsely classifies a normal connection as an attack and therefore obstructs legitimate user access to the network resources [1].",
      "startOffset" : 294,
      "endOffset" : 297
    }, {
      "referenceID" : 1,
      "context" : "However, this normally requires infinite training sample sizes (theoretically) [2].",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 3,
      "context" : "An Intrusion Detection System (IDS) is defined as a protection system that monitors computers or networks for unauthorized activities based on network traffic or system usage behaviors, thereby detecting if a system is targeted by a network attack such as a denial of service attack [4].",
      "startOffset" : 283,
      "endOffset" : 286
    }, {
      "referenceID" : 2,
      "context" : "One of the rule-based methods which is commonly used by early IDS is the Expert System (ES) [3, 4].",
      "startOffset" : 92,
      "endOffset" : 98
    }, {
      "referenceID" : 3,
      "context" : "One of the rule-based methods which is commonly used by early IDS is the Expert System (ES) [3, 4].",
      "startOffset" : 92,
      "endOffset" : 98
    }, {
      "referenceID" : 4,
      "context" : "developed a data mining framework for the purpose of intrusion detection [5, 6].",
      "startOffset" : 73,
      "endOffset" : 79
    }, {
      "referenceID" : 5,
      "context" : "developed a data mining framework for the purpose of intrusion detection [5, 6].",
      "startOffset" : 73,
      "endOffset" : 79
    }, {
      "referenceID" : 6,
      "context" : "Decision trees are one of the most commonly used supervised learning algorithms in IDS [7-11] due to its simplicity, high detection accuracy and fast adaptation.",
      "startOffset" : 87,
      "endOffset" : 93
    }, {
      "referenceID" : 7,
      "context" : "Decision trees are one of the most commonly used supervised learning algorithms in IDS [7-11] due to its simplicity, high detection accuracy and fast adaptation.",
      "startOffset" : 87,
      "endOffset" : 93
    }, {
      "referenceID" : 8,
      "context" : "Decision trees are one of the most commonly used supervised learning algorithms in IDS [7-11] due to its simplicity, high detection accuracy and fast adaptation.",
      "startOffset" : 87,
      "endOffset" : 93
    }, {
      "referenceID" : 9,
      "context" : "Decision trees are one of the most commonly used supervised learning algorithms in IDS [7-11] due to its simplicity, high detection accuracy and fast adaptation.",
      "startOffset" : 87,
      "endOffset" : 93
    }, {
      "referenceID" : 10,
      "context" : "Decision trees are one of the most commonly used supervised learning algorithms in IDS [7-11] due to its simplicity, high detection accuracy and fast adaptation.",
      "startOffset" : 87,
      "endOffset" : 93
    }, {
      "referenceID" : 11,
      "context" : "ANN-based IDS [12-15] have achieved great successes in detecting difficult attacks.",
      "startOffset" : 14,
      "endOffset" : 21
    }, {
      "referenceID" : 12,
      "context" : "ANN-based IDS [12-15] have achieved great successes in detecting difficult attacks.",
      "startOffset" : 14,
      "endOffset" : 21
    }, {
      "referenceID" : 13,
      "context" : "ANN-based IDS [12-15] have achieved great successes in detecting difficult attacks.",
      "startOffset" : 14,
      "endOffset" : 21
    }, {
      "referenceID" : 14,
      "context" : "ANN-based IDS [12-15] have achieved great successes in detecting difficult attacks.",
      "startOffset" : 14,
      "endOffset" : 21
    }, {
      "referenceID" : 15,
      "context" : "For unsupervised intrusion detection, data clustering methods can be applied [16, 17].",
      "startOffset" : 77,
      "endOffset" : 85
    }, {
      "referenceID" : 16,
      "context" : "For unsupervised intrusion detection, data clustering methods can be applied [16, 17].",
      "startOffset" : 77,
      "endOffset" : 85
    }, {
      "referenceID" : 6,
      "context" : "Another well-known ML techniques used in IDS is Naïve Bayes classifiers [7].",
      "startOffset" : 72,
      "endOffset" : 75
    }, {
      "referenceID" : 17,
      "context" : "In [18], the authors apply a Bayesian network for IDS.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 13,
      "context" : "Beside popular decision trees and ANN, Support Vector Machines (SVMs) are also a good candidate for intrusion detection systems [14, 19] which can provide real-time detection capability, deal with large dimensionality of data.",
      "startOffset" : 128,
      "endOffset" : 136
    }, {
      "referenceID" : 18,
      "context" : "Beside popular decision trees and ANN, Support Vector Machines (SVMs) are also a good candidate for intrusion detection systems [14, 19] which can provide real-time detection capability, deal with large dimensionality of data.",
      "startOffset" : 128,
      "endOffset" : 136
    }, {
      "referenceID" : 19,
      "context" : "Several other AI paradigms including linear genetic programming [20] , Hidden Markov Model [21], Columbia Model [22] and Layered Conditional Random Fields [23] have been applied for the design of IDS.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 20,
      "context" : "Several other AI paradigms including linear genetic programming [20] , Hidden Markov Model [21], Columbia Model [22] and Layered Conditional Random Fields [23] have been applied for the design of IDS.",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 21,
      "context" : "Several other AI paradigms including linear genetic programming [20] , Hidden Markov Model [21], Columbia Model [22] and Layered Conditional Random Fields [23] have been applied for the design of IDS.",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 22,
      "context" : "Several other AI paradigms including linear genetic programming [20] , Hidden Markov Model [21], Columbia Model [22] and Layered Conditional Random Fields [23] have been applied for the design of IDS.",
      "startOffset" : 155,
      "endOffset" : 159
    }, {
      "referenceID" : 23,
      "context" : "However, they are susceptible to overfitting, which can cause instability in generalization [24].",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 18,
      "context" : "in Intrusion Detection [19].",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 24,
      "context" : "This paper is inspired by a light-weight ANN model, namely Vector Quantized-Generalized Regression Neural Network (VQ-GRNN) [25], which reduces the nonparametric GRNN [26] to a semiparametric model by applying vector quantization techniques on the training data, i.",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 25,
      "context" : "This paper is inspired by a light-weight ANN model, namely Vector Quantized-Generalized Regression Neural Network (VQ-GRNN) [25], which reduces the nonparametric GRNN [26] to a semiparametric model by applying vector quantization techniques on the training data, i.",
      "startOffset" : 167,
      "endOffset" : 171
    }, {
      "referenceID" : 23,
      "context" : "This significantly improves the robustness of the algorithm (low variance), but also controls its learning accuracy to some extent [24].",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 26,
      "context" : "Ensemble methods such as Boosting [27] iteratively learn multiple classifiers (base classifiers) on different distributions of training data.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 27,
      "context" : "Amongst popular boosting variants, we choose Adaptive Boosting or AdaBoost [28] to improve performance of VQ-GRNN.",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 26,
      "context" : "AdaBoost is “adaptive” in the sense that it does not require prior knowledge of the accuracy of these hypotheses [27].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 8,
      "context" : "Moreover, it is widely accepted that generalization performance of a combined classifier is not necessarily achieved by combining classifiers with better individual performance but by including independent classifiers in the ensemble [9].",
      "startOffset" : 234,
      "endOffset" : 237
    }, {
      "referenceID" : 24,
      "context" : "This base learner f is actually a modified version of the emerging VQ-GRNN model [25] (called Modified GRNN Base learner) in which the input data space is reduced significantly (by the Weighted vector quantization module) and its output is computed by a linearly weighted mixture of Radial Basis Function (RBF).",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 28,
      "context" : "The Diversity Checker measures ensemble diversity by using Kohavi-Wolpert variance [30] (which is denoted by the hypothesis weighting coefficient ).",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 24,
      "context" : "We adapt VQ-GRNN [25] as a base learner in our BSPNN model.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 25,
      "context" : "VQ-GRNN is closely related to Specht’s GRNN [26] and PNN [31] classifiers.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 29,
      "context" : "VQ-GRNN is closely related to Specht’s GRNN [26] and PNN [31] classifiers.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 24,
      "context" : "Using this idea, the VQ-GRNN’s equation can be generalized [25]:",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 30,
      "context" : "By sufficiently handling the multiclass problem and using confidencerated predictions, SAMME can maximize the distribution margins of the training data [32].",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 28,
      "context" : "Also, our implementation of Kohavi-Wolpert variance (KW) [30] in the reweighting of hypotheses in the joint classification can effectively enforce the ensemble diversity.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 0,
      "context" : "In this section, we apply our BSPNN to identify known and novel attacks in the KDD-99 dataset [1], containing TCP/IP connection records.",
      "startOffset" : 94,
      "endOffset" : 97
    }, {
      "referenceID" : 0,
      "context" : "KDD-99 COMPONENT DATASETS [1]",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 0,
      "context" : "Given a test set, the average cost of a classifier is calculated as below [1]:",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 31,
      "context" : "In [33], artificial anomalies are added to the training data to help the learner discover a boundary around the available training data.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 31,
      "context" : "Each cluster contains intrusions that require similar features for effective detection and this method, as detailed in [33], is not influenced by cluster orders.",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 31,
      "context" : "Table 4 shows that our BSPNN obtains competitive detection rate compared with [33] while achieves significantly lower false alarm rate (1.",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 7,
      "context" : "Using the full training set (v u), we test our BSPNN against other existing methods, including the KDD-99 winner [8], the rule-based PNrule approach [34], the multi-class Support Vector Machine [19], the Layered Conditional Random Fields Framework (LCRF) [23], the Columbia Model [22] and the Decision Tree method [11].",
      "startOffset" : 113,
      "endOffset" : 116
    }, {
      "referenceID" : 32,
      "context" : "Using the full training set (v u), we test our BSPNN against other existing methods, including the KDD-99 winner [8], the rule-based PNrule approach [34], the multi-class Support Vector Machine [19], the Layered Conditional Random Fields Framework (LCRF) [23], the Columbia Model [22] and the Decision Tree method [11].",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 18,
      "context" : "Using the full training set (v u), we test our BSPNN against other existing methods, including the KDD-99 winner [8], the rule-based PNrule approach [34], the multi-class Support Vector Machine [19], the Layered Conditional Random Fields Framework (LCRF) [23], the Columbia Model [22] and the Decision Tree method [11].",
      "startOffset" : 194,
      "endOffset" : 198
    }, {
      "referenceID" : 22,
      "context" : "Using the full training set (v u), we test our BSPNN against other existing methods, including the KDD-99 winner [8], the rule-based PNrule approach [34], the multi-class Support Vector Machine [19], the Layered Conditional Random Fields Framework (LCRF) [23], the Columbia Model [22] and the Decision Tree method [11].",
      "startOffset" : 255,
      "endOffset" : 259
    }, {
      "referenceID" : 21,
      "context" : "Using the full training set (v u), we test our BSPNN against other existing methods, including the KDD-99 winner [8], the rule-based PNrule approach [34], the multi-class Support Vector Machine [19], the Layered Conditional Random Fields Framework (LCRF) [23], the Columbia Model [22] and the Decision Tree method [11].",
      "startOffset" : 280,
      "endOffset" : 284
    }, {
      "referenceID" : 10,
      "context" : "Using the full training set (v u), we test our BSPNN against other existing methods, including the KDD-99 winner [8], the rule-based PNrule approach [34], the multi-class Support Vector Machine [19], the Layered Conditional Random Fields Framework (LCRF) [23], the Columbia Model [22] and the Decision Tree method [11].",
      "startOffset" : 314,
      "endOffset" : 318
    }, {
      "referenceID" : 31,
      "context" : "[33] BSPNN",
      "startOffset" : 0,
      "endOffset" : 4
    } ],
    "year" : 2009,
    "abstractText" : "This article applies Machine Learning techniques to solve Intrusion Detection problems within computer networks. Due to complex and dynamic nature of computer networks and hacking techniques, detecting malicious activities remains a challenging task for security experts, that is, currently available defense systems suffer from low detection capability and high number of false alarms. To overcome such performance limitations, we propose a novel Machine Learning algorithm, namely Boosted Subspace Probabilistic Neural Network (BSPNN), which integrates an adaptive boosting technique and a semi-parametric neural network to obtain good trade-off between accuracy and generalty. As the result, learning bias and generalization variance can be significantly minimized. Substantial experiments on KDD-99 intrusion benchmark indicate that our model outperforms other state-of-the-art learning algorithms, with significantly improved detection accuracy, minimal false alarms and relatively small computational complexity. KeywordsIntrusion Detection, Neural Network, Adaptive Boosting",
    "creator" : "PScript5.dll Version 5.2"
  }
}