{
  "name" : "1203.3477.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Scalable Method for Solving High-Dimensional Continuous POMDPs Using Local Approximation",
    "authors" : [ "Tom Erez", "William D. Smart" ],
    "emails" : [ "etom@cse.wustl.edu", "wds@cse.wustl.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Partially-Observable Markov Decision Processes (POMDPs) are typically solved by finding an approximate global solution to a corresponding belief-MDP. In this paper, we offer a new planning algorithm for POMDPs with continuous state, action and observation spaces. Since such domains have an inherent notion of locality, we can find an approximate solution using local optimization methods. We parameterize the belief distribution as a Gaussian mixture, and use the Extended Kalman Filter (EKF) to approximate the belief update. Since the EKF is a first-order filter, we can marginalize over the observations analytically. By using feedback control and state estimation during policy execution, we recover a behavior that is effectively conditioned on incoming observations despite the unconditioned planning. Local optimization provides no guarantees of global optimality, but it allows us to tackle domains that are at least an order of magnitude larger than the current state-of-the-art. We demonstrate the scalability of our algorithm by considering a simulated hand-eye coordination domain with 16 continuous state dimensions and 6 continuous action dimensions."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Partially-Observable Markov Decision Processes (POMDPs) offer a framework for studying decision making under uncertainty. The optimal behavior in a POMDP domain is expected to strike a balance between exploring the partially-observable world and acting in a goal-directed manner. Most of the POMDP literature is concerned with discrete domains, but in the past few years, as POMDP tools become more powerful, there is growing\n∗ {etom,wds}@cse.wustl.edu\ninterest in tackling continuous domains (Porta et al., 2006; Brooks, 2009).\nThe standard approach to solving POMDPs is to find an approximate solution to the fully-observable belief -MDP, whose states are probability distributions over the state space of the original POMDP (Kaelbling et al., 1998). In the discrete case, the resulting belief space is continuous but finite-dimensional, and belief update can be carried out exactly. However, the belief space of a continuous POMDP is infinite-dimensional, and must be approximated (Thrun, 2000).\nThe optimal value function of belief-MDPs is piecewiselinear and convex in the discrete case (Sondik, 1971), and this also holds for some cases of continuous state (Porta et al., 2006), as long as the observations and actions are discrete. This result was used to tackle domains with continuous hybrid-linear dynamics by Brunskill et al. (2008). Other combinations of the discrete and the continuous domains were also considered (Hoey & Poupart, 2005; Spaan & Vlassis, 2005). The richest domain tackled by continuous POMDPs is probably outdoor navigation (Brooks, 2009).\nHowever, in all the examples mentioned above, the belief domain is solved through global optimization. Since the volume of state space grows exponentially with the dimension of the state, it is unrealistic to seek a globally-optimal solution in domains above a certain size because of the curse of dimensionality. Some studies (e.g., Feng & Zilberstein, 2004) try to offset some of the computational burden by finding parts of belief space that can safely be ignored, but the fundamental problem of exponential scaling remains.\nIn contrast, continuous domains naturally admit a notion of distance, which allows the application of local optimization methods. Here, we present a method for approximating a locally-optimal solution to a POMDP in which state, action and observation space are continuous. This work is a departure from the current POMDP literature, as it offers a different trade-off between provable correctness and\nscalability. Since we employ a local method, guarantees or bounds for global optimality are impossible to obtain. However, local optimization is not subject to the curse of dimensionality, and can tackle domains that are outside the reach of global approaches.\nIn this paper, we use Differential Dynamic Programming (DDP) to solve for a locally-optimal policy (section 4). While DDP optimizes the open-loop (“blind”) policy, it also outputs a feedback policy (section 5) that is used to adapt the execution-time behavior to the actual incoming observations.\nWe approximate the belief space with a parametric distribution, specifically a Gaussian mixture, and use the Extended Kalman Filter (EKF) for belief update (Stengel, 1994). By virtue of the EKF being a first-order filter, we can analytically marginalize the belief update over the observations. By focusing only on the most-likely observation, we recover a deterministic update scheme (section 3). This seems counter-intuitive, since the goal of solving POMDPs is to generate behavior that responds to observations. However, note that observations are marginalized only for planning; during policy execution, the actual incoming observations are taken into account as they are used to estimate the agent’s hidden state. By coupling state estimation and feedback control, the agent’s behavior is again conditioned on incoming observations, allowing it to respond to the changing environment in real time.\nPOMDPs are often used to tackle domains with unilateral constraints, such as contacts (e.g., Hsiao et al., 2007). Since the EKF works by linearizing the dynamics, a single Gaussian would not be descriptive enough to handle such discontinuities. Since the distribution of the hidden state is truncated by a constraint manifold, we explicitly approximate the probability mass that aggregates on this manifold with a Gaussian of lower rank (section 3.2). We analytically account for the flow of probability mass between the two Gaussians using the equations of truncated normal distributions (section 3.2.1). While these approximations are used for belief propagation during planning, more accurate state estimation (e.g., a particle filter) can be employed during policy execution (section 5).\nThe scalability of the proposed method is unmatched by any existing technique, and allows the use of POMDPs in application domains that are too large to admit global solutions. In section 6.2, we apply our method to a simulated domain of hand-eye coordination with 16 continuous state dimensions and 6 continuous action dimensions.\nThe principles of deterministic planning through marginalized observations is not new: for example, Prentice & Roy (2009) also employ a single-Gaussian approximation of the belief state during planning. However, this approach requires samples that span the entire state space, and is hence bound by the curse of dimensionality. The notion\nof marginalizing the observations during local optimization has informed the solution method presented in Erez & Smart (2009). The method we present here is very similar to Miller et al. (2009), who used the term nominalbelief optimization, and Platt et al. (2010), who used the term maximum-likelihood observations. However, neither approaches can tackle domains with unilateral constraints (section 3.2)."
    }, {
      "heading" : "2 DEFINITIONS",
      "text" : "We consider a discrete-time POMDP defined by a tuple 〈S,A,Z, T,Ω, R,N〉, where: S,A and Z are the state space, action space and observation space, respectively; T (s′, s, a) = Pr(s′|s, a) is a transition function describing the probability of the next state given the current state and action; Ω(z, s, a) = Pr(z|s, a) is the observation function, describing the probability of an observation given the current state and action; and R is a time-dependent reward function Ri(s, a), with a terminal reward RN (s). In this paper we consider an undiscounted optimality criterion, where the agent’s goal is to maximize the expected cumulative reward within a fixed time horizon N . This formulation is a deviation from the common focus on discounted horizons, and we adopt it because it is useful for the local optimal control algorithm we employ (section 4)."
    }, {
      "heading" : "3 THE BELIEF DOMAIN",
      "text" : "The belief state b ∈ B is a probability distribution over S, where bi(s) is the likelihood of the true state being s at time i. In order to construct the belief domain of a given POMDP, we need to find a representation for b, and define the reward function and dynamics (belief update) over this space.\nGiven the current belief b, an action a and observation z, the updated belief b′ can be calculated by applying Bayes’s rule. In the continuous case B is infinite-dimensional, and Bayes’s rule yields an integral:\nb′(s′) ∝ ∫ s b(s)T (s′, s, a)Ω(z, s, a)ds.\nIn order to make this function computationally tractable, we must employ some approximation b̂ to the true belief b, and commit to some state estimation filter to update the approximated belief.\nThe reward associated with a belief is simply the expected value over this state distribution:\nRi(b, a) = E s∼b\n[ Ri(s, a) ] . (1)\nSince our optimality criterion employs a finite-horizon, our optimization focuses on the time-dependent policy π(b̂, i),\nmapping beliefs and time to actions. The optimal policy maximizes the cumulative reward:\nπ∗ = argmax π E [ N∑ i=1 Ri(bi, π(bi, i)) ] . (2)"
    }, {
      "heading" : "3.1 CONTINUOUS DYNAMICS",
      "text" : "In this section, we focus on nonlinear stochastic dynamics ds = f(s, a)dt + q(s, a)dξ, where ξ is a Wiener process. For a given state s and action a, integrating these dynamics over a small time-step τ results in a normal distribution over the next state s′:\nT (s′, s, a) = N (s′|F (s, a), Q(s, a)),\nwhere the mean is propagated with the Euler integration\nF = s+ τf(s, a), (3)\nand the covariance Q = τqTq is a time-scaling of the continuous process qdξ.\nSimilarly, we focus on observation distributions of the form: Ω(z, s, a) = N (z|w(s),W (s, a)), where w deterministically maps states to observations, and W describes how the current state and action affect the observation noise.\nGiven a Gaussian prior on the initial state, we approximate the infinite-dimensional b by a single Gaussian:\nb̂(s) = N (s|ŝ,Σ) = 1 (2π) k 2 |Σ| 12 e− 1 2 (s−ŝ) TΣ−1(s−ŝ),\nand denote its parameterization by:\nν = {ŝ,Σ} (4)\nwhere the covariance Σ belongs to the space of symmetric, positive-semidefinite matricesM ⊂ Rn×n. Therefore, the belief space B̂ is parameterized in this case by the product space ν ∈ S×M. In the limit of τ → 0, this approximation is accurate.\nIn order to approximate the belief update, we use the Extended Kalman Filter (EKF) (Stengel, 1994). Given the current belief b̂, action a and observation z, we calculate the partial derivatives around ŝ: ws = ∂w/∂s and Fs = ∂F/∂s. We find the uncorrected estimation uncertainty H = FsΣFTs +Q(ŝ, a) and calculate the new mean ŝ′ by the innovation process:\nŝ′ = F (ŝ, a)−K(z − w(ŝ)). (5)\nwhere K = Hws(wTsHws + W (ŝ, a)) −1 is the Kalman gain. Finally, the new covariance Σ′ is given by:\nΣ′ = Ψ(ŝ,Σ, a) =\nH −Hws(wTsHws +W (ŝ, a))−1wTsHT. (6)\nThe deterministic belief update is obtained by marginalizing equations (5) and (6) over the observation z. Equation (5) is linear in z, and so we can take the expectation by simply replacing z with its mean w(ŝ). The second term of equation (5) vanishes, and so the mean follows (3). By virtue of the EKF being a first-order filter, the calculation in (6) is independent of z. In summary, the deterministic belief update is formed by the combination of (3) and (6):\nb̂′ = {F (ŝ, a),Ψ(ŝ,Σ, a)}. (7)"
    }, {
      "heading" : "3.2 UNILATERAL CONSTRAINTS",
      "text" : "In the previous section, we made the assumption that F and w can be linearized WRT s. However, this assumption may be too restrictive for some domains; in particular, it excludes discontinuous dynamics that occur due to unilateral constraints. Since this category includes interesting domains of disambiguation by contact, object manipulation and locomotion, we extend our method to handle the nonGaussian beliefs that come about in such cases.\nIn this section we consider domains with non-penetration constraints Γ:\nds = f(s, a)dt+Q(s, a)dξ,\nΓ(s) ≥ 0. (8)\nIn the general case, the reaction forces that enforce these constraints can be calculated using complementarity methods (Stewart, 2000) or penalty methods (Drumwright, 2008). When Γ(s) = 0, we say that the constraint is active. In this paper, we consider domains where at most one constraint is active at any one time, and so we may focus on cases where Γ(s) is scalar.\nThe resulting belief b can no longer be described by a simple normal distribution: Γ describes an (n−1)-dimensional constraint manifold, and the belief distribution is truncated at this manifold, with some probability mass aggregating on it. We approximate this truncated distribution with a weighted mixture of two Gaussians: one describing the belief distribution in the unconstrained volume, and the other describing the aggregated belief on the constraint (hence degenerate in the direction locally perpendicular to the manifold). Using ν to parameteize a single Gaussian as in (4), we denote the parameterized belief\nb̂(s) = αN (s|ŝ1,Σ1) + (1− α)N (s|ŝ2,Σ2)\nby the shorthand\nb̂ = {ν1, ν2, α},\nwhere α ∈ [0, 1] is the relative weight of the first Gaussian. This is not an exact representation of the true belief; a Gaussian has infinite support, and therefore the unconstrained Gaussian has non-zero probability mass beyond\nthe constraint. However, this mass is small enough that, in practice, it has had no noticeable effect on our results.\nBelief update is done in two stages, as outlined in algorithm 1. In the first stage, we update the belief of each Gaussian independently using (7). Assuming that there is noise in the direction locally-perpendicular to the constraint, the second Gaussian is now full-rank. In the second stage, we re-approximate this two-Gaussian mixture, ensuring that the resulting mixture maintains the form described above — the probability mass above the constraint manifold is approximated with one Gaussian, and the belief that lies below the constraint is approximated with a second, degenerate Gaussian that lies on the manifold. The details of the computations required for the second stage are detailed in the next two subsections."
    }, {
      "heading" : "3.2.1 Truncation",
      "text" : "In order to re-adjust the belief to the constraint, we linearize the constraint function Γ ≈ Js + e ≥ 0 around the mean of each Gaussian. We compute the distributions on either side of the constraint analytically by considering truncated normal distributions (Boutilier, 2002; Toussaint, 2009). We can linearly rotate and re-scale the state space so as to ensure that the constraint manifold is locally perpendicular to the kth dimension of s, and that the uncertainty in this dimension is independent of the others. Therefore, we can focus our analysis on the one-dimensional case, assuming without loss of generality that the constraint does not affect any dimension but k.\nLet x ∼ N (µ, σ2). When bound to an interval x ∈ [l, u], its distribution becomes:\nPr(x) ∝ 1√ 2πσ2\nexp ( − (x− µ) 2\n2σ2\n) Θ(x− l)Θ(u− x),\nwhere Θ is the Heaviside function. The first two moments of the resulting distribution are:\nE(X | l < X < u) = µ+ σ φ(l̄)− φ(ū) Φ(ū)− Φ(l̄)\n(9a)\nVar(X | l < X < u) =\nσ2 [ 1 +\nl̄φ(l̄)− ūφ(ū) Φ(ū)− Φ(l̄) − ( φ(l̄)− φ(ū) Φ(ū)− Φ(l̄) )2] (9b)\nwhere l̄ = l−µσ , ū = u−µ σ , and φ(x̄), Φ(x̄) are the PDF and CDF of the normal distribution with zero mean and unit variance. The probability masses that aggregate on the constraints are Φ(l̄) and 1 − Φ(ū). We are interested in distributions over one-sided intervals, so either l = −∞ or u =∞, which further simplifies (9).\nAlgorithm 1 Deterministic Belief Update with Unilateral Constraints\nInput: b̂ = {ν1, ν2, α}, action a for i = 1, 2 do\nMarginalized EKF: Calculate ν′i by (7). Truncation: Calculate {νui , νli , αui } by (9).\nend for Reduction: Calculate ν′′1 , ν′′2 by (10). Adjustment: Project ν′′2 onto constraint by (11). Weight update: Calculate α′ by (12). Output: b̂′ = {ν′′1 , ν′′2 , α′}."
    }, {
      "heading" : "3.2.2 Mixture Reduction",
      "text" : "We use the truncation procedure described above to split each Gaussian in two, across the constraint. In order to maintain our form (one Gaussian unconstrained, one Gaussian on the constraint manifold), we reduce this fourGaussian mixture back to two, and project the second Gaussian onto the constraint.\nReducing a mixture of two Gaussians {ν1, ν2, α} results in a single Gaussian whose mean ŝ and covariance Σ are:\nŝ = αŝ1 + (1− α)ŝ2, (10a)\nΣ = αΣ1+(1−α)Σ2+α(1−α)(ŝ1−ŝ2)(ŝ1−ŝ2)T (10b)\nUsing these equations, we combine the two Gaussians above the constraint into a single Gaussian ν′′1 , and the two Gaussians below the constraint into ν′′2 . Assuming that the constraint is locally perpendicular to the kth dimension as above, we project ν′′2 onto the constraint by setting:\n(ŝ′′2)k = Γ(ŝ ′′ 2), and (Σ ′′ 2)k,k = 0. (11)\nFinally, the weight of the unconstrained Gaussian in the adjusted mixture is:\nα′ = ααu1 + (1− α)αu2 . (12)"
    }, {
      "heading" : "4 PLANNING",
      "text" : "The belief update schemes of the previous section (together with (1)) define a problem of deterministic optimal control in a high-dimensional continuous space, with non-linear dynamics and non-quadratic reward. This can be solved using a variety of techniques; Platt et al. (2010), for example, use the multiple shooting method. We use a local optimization scheme called Differential Dynamic Programming (DDP), an algorithm that has been successfully applied to real-world high-dimensional, non-linear control domains (e.g., Abbeel & Ng, 2005; Tassa et al., 2008). The interested reader may find an in-depth description of the algorithm in Jacobson & Mayne (1970).\nThe benefit of using DDP is that in addition to the locally-optimal trajectory and the open-loop action sequence which realizes this trajectory, it outputs a sequence of linear feedback gain matrices. These parameterize the policy (section 5) to create a feedback controller for the original POMDP."
    }, {
      "heading" : "5 POLICY EXECUTION",
      "text" : "Since a policy for a continuous POMDP is infinitedimensional, it also needs to be parameterized. In this paper we focus on policies that are locally-linear:\nπ(b̂, i) = āi + Li(b̂− b̄i) (13)\nfor some parameterized belief states b̄1:N , actions ā1:N−1 and feedback gain matrices L1:N−1. This parameterization corresponds to the output of Differential Dynamic Programming, as described in the previous section.\nThe policy is executed post-planning, as the agent interacts with the environment. Incoming observations are filtered by state estimation, and feedback control responds to changes in the perceived state and reacts appropriately. Thus, the agent’s behavior is conditioned on received observations even though these were marginalized during planning.\nAt this stage, we are no longer committed to the belief update schemes of section 3, and a more accurate filter (e.g., particle filter) can be used for state approximation. This new filter may employ a different representation of the belief state which may be richer than a single Gaussian."
    }, {
      "heading" : "6 RESULTS",
      "text" : "First, we demonstrate key features of our method by considering an example of planar navigation, roughly corresponding to domains considered by Roy & Thrun (1999) and Brooks (2009). Then, we demonstrate the scalability of our method by solving a 16-dimensional problem, first presented by Erez & Smart (2009)."
    }, {
      "heading" : "6.1 PLANAR NAVIGATION",
      "text" : "In this problem, a robot must move in a closed room from a start point to a target while avoiding obstacles. The robot cannot sense its position, but may localize itself by making contact with the walls. Here, state, action and observation are all two-dimensional, and the constraint is scalar. The resulting optimal behavior (figure 1(a)) is found in less than a minute: the robot avoids the obstacles by approaching the side wall, and then “cut” the corner on its way to the target at the target at the bottom wall. In order to study the effect of linearizing the constraint, we tested a case where the agent interacts with the curved segment of the constraint.\nAs figure 1(b) shows, the optimal path in this case follows the round corner without difficulty.\nThe disambiguating property of the contact with the wall is termed “coastal navigation” (Roy & Thrun, 1999), and our algorithm is able to identify and leverage this feature as it emerges in the optimal solution. We cannot offer a direct comparison of our results with Brooks (2009), since his experiments were conducted on real robots. Note the qualitative difference in our approach: Brooks’s method approximates the global optimum, and requires 8000 samples that are processed in ∼25 minutes, while our method finds a locally-optimal solution in less than one minute."
    }, {
      "heading" : "6.2 HAND-EYE COORDINATION",
      "text" : "This problem illustrates the scalability of our algorithm, since we believe it cannot be solved by any other POMDP technique. In addition, we demonstrate reactive behavior through feedback control. The resulting behavior is best illustrated by the supplementary movie1.\nThis domain is similar to the one we solved in Erez & Smart (2009), with few notable differences: first, here we introduce perceptual inhibition during saccadic motion; more significantly, the solution method of this paper is faster and more robust than the minimax approach presented there, since here we do not solve for the adversarial behavior.\nThis domain simulates the problem of an agent coordinating two “hands” and an “eye”. The task requires the agent to bring the hands from their starting positions to a target point at a specific time, while avoiding four obstacles in a planar scene. State transitions are subject to a fixed Gaussian process noise. The obstacles’ positions are unknown,\n1www.cse.wustl.edu/∼etom/uai2010.mp4\nso the agent must observe and estimate these as well.\nThe planar scene is illustrated in figure 2(a). The state is defined in terms of the following variables: se is the eye’s two-dimensional position, sh1 and sh2 are the positions of the hands, st is the target’s position, and {sli , i = 1 . . . 4} are the positions of four obstacles. Therefore, the state space has 16 continuous dimensions. Every state s is a concatenation of the 8 planar positions above. The action space A is 6-dimensional, specifying planar velocities for the hands and eye. As stated in the previous section, such a domain is infeasible for global optimization, and cannot be solved by any existing global POMDP algorithm.\nZ, the observation space, is identical to the state space. The observation noise covariance W is diagonal, allowing independent observation of each scene element. W is stateand action-dependent: the eye has the capacity to produce unambiguous observations in a small region around its current position, conceptually modelling foveated vision. The eye’s gaze locally reduces the observation noise:\nW?(s, a) = 1− e−‖se−s?‖ 2/2η + 0.01aTe ae (14)\nwhere ? stands for one of the scene elements: h1, h2, t, or any of the obstacles li. The parameter η determines the size of the fovea, and ae is the current actuation of the eye. The last RHS term in (14) models visual inhibition during saccadic eye movement, effectively eliminating the eye’s disambiguating effect during high-velocity eye movements. Thus, the eye produces valid observations only when it is close to an object, and moving slowly.\nThe reward function is the same as in Erez & Smart (2009) — it penalizes for distance between the hands and the target at the final time step, and for proximity between the hands and the obstacles at all other time steps; action (displacement of hands and eye) incurs a quadratic cost, where the eye’s action cost is negligibly small.\nThe covariance of the process noise Q is a constant diagonal matrix, where the noise in the X- and Y-direction are equal for every scene element. The process noise that affects the eye, obstacles and target is zero. From the agent’s perspective, this means that once observed, the positions of the target and obstacles can be trusted to remain unmoved, allowing the eye’s position to provide grounding for locating all other elements of the scene. Since process noise is uncorrelated between state dimensions and symmetric in both planar directions, the belief covariance can be decomposed and succinctly represented by a single scalar variance for each scene element (but the eye, whose position is certain). In all, B̂ has 23 dimensions.\nFigure 2(b) shows the resulting locally-optimal trajectories for both hands and the eye. Notice how the eye tracks each hand in turn as it passes close the obstacles, and how the hands time their approach to the obstacles to synchronize with the eye. Interestingly, in the optimal solution we can see the emergence of two distinct phases of eye behavior – smooth pursuit, where the eye tracks the hand, and saccades, where the eye rapidly moves from one gaze target to another. This behavior is in accordance with biological visual behavior (Cassin & Rubin, 2001).\nTo demonstrate the responsiveness of the resulting policy, we tested the agent’s feedback control in a modified scene, where the obstacles were shifted from their position during planning. Figure 2(c) shows the resulting behavior: the hands’ trajectories are adjusted as the eye perceives and updates the estimated position of the obstacles and hands (using EKF). Since feedback is specified over belief space, the behaving agent also responds to changes in the estimation uncertainty.\nIn our initial experiments, we observed mis-convergence to a local minimum, where the eye would not bother to saccade between the two hands, instead sticking to only one of them. This was remedied by employing a shapingcontinuation method (Erez & Smart, 2008) We first found a solution to a simpler problem, where the size of the notional fovea is large (η = 10). There, the wide field-ofview allowed a relatively unambiguous view of the entire scene, and enabled the formation of a trajectory that had the right coarse features (a move to the left, then a move to the right, then a move up), even as it was not required to perform precise saccades. As learning progresses, the size of the foveal region was gradually reduced, making exact eye movements more important. Every new problem instance was solved using the previous solution as a starting point. This process repeated for decreasing fovea radius (η = [1, 0.3, 0.05]) until we generated a solution to the original problem. The shaping sequence required running DDP to convergence 4 times, yet the optimal solution for this 16-dimensional domain was found in less than 3 minutes of MATLAB running on a single-core desktop computer."
    }, {
      "heading" : "7 DISCUSSION",
      "text" : "This paper offers a new perspective on solving continuous POMDPs. Instead of using global approximation in a belief-MDP, we marginalize the observations and cast the infinite-dimensional, stochastic belief domain in terms of a finite-dimensional optimal control problem. The approach we present here applies to domains whose dynamics are smooth (and hence Gaussian beliefs constitute a good approximation) or with one unilateral constraint (where we use a specifically-structured two-Gaussian mixture); however, our approximations may not hold for domains with more elaborate structure.\nWhile this method scales very well with state dimensionality, we chose to focus on domains where only one constraint is active at a time. Such cases are amenable to analytic manipulation using truncated normal distributions, as described above. If we extended this type of analysis to cases where more than one constraint may be active at once, we would be assigning a Gaussian to every combination of active constraints, and accounting for the flow of probability mass between all of them. This would intro-\nduce yet another set of approximations, and would be computationally reasonable only for a small number of jointlyactive constraints.\nOne natural extension of this work could employ local optimization from multiple starting points, creating a controller that uses a trajectory library (Stolle & Atkeson, 2006; Tassa et al., 2008). Such a scheme could extend the basin of attraction of our local controller (similar to Tedrake, 2009), and produce a better approximation of the globally-optimal policy. In particular, a multi-modal prior can be handled by finding the optimal behavior for each of the modes, and using state estimation during policy execution to choose the relevant case. If behavior is expected to encounter different variants of the same environment, which vary along a single parameter (e.g. ground incline, or object mass), we can extend a single solution and create a manifold controller (Erez & Smart, 2007) through some continuation approach.\nIn many real-life cases, an active constraint results in frictional forces, in addition to the reaction forces that maintain non-penetration. This can be incorporated into our method by using a different dynamical model for the initial belief update of the constrained Gaussian ν2, in particular one that incorporates friction. In cases where making contact (i.e., collision) is associated with a non-negligible impact dynamics of other degrees of freedom beyond the constrained one (e.g., foot-ground impact, or ball-racket impact), these impulses can be considered as we project the Gaussian that lies below the constraint manifold onto the linearized hyperplane."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work was supported by NSF award BCS 0924609"
    } ],
    "references" : [ {
      "title" : "Exploration and apprenticeship learning in reinforcement learning",
      "author" : [ "Abbeel", "Pieter", "Ng", "Andrew Y" ],
      "venue" : "In International Conference on Machine Learning (ICML), pp",
      "citeRegEx" : "Abbeel et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Abbeel et al\\.",
      "year" : 2005
    }, {
      "title" : "A POMDP formulation of preference elicitation problems",
      "author" : [ "Boutilier", "Craig" ],
      "venue" : "In Eighteenth national conference on Artificial intelligence,",
      "citeRegEx" : "Boutilier and Craig.,? \\Q2002\\E",
      "shortCiteRegEx" : "Boutilier and Craig.",
      "year" : 2002
    }, {
      "title" : "Parametric POMDPs",
      "author" : [ "Brooks", "Alex" ],
      "venue" : "VDM Verlag,",
      "citeRegEx" : "Brooks and Alex.,? \\Q2009\\E",
      "shortCiteRegEx" : "Brooks and Alex.",
      "year" : 2009
    }, {
      "title" : "Continuous-state POMDPs with hybrid dynamics",
      "author" : [ "Brunskill", "Emma", "Kaelbling", "Leslie", "Lozano-Perez", "Tomas", "Roy", "Nicholas" ],
      "venue" : "In Tenth International Symposium on Artificial Intelligence and Mathematics (ISAIM),",
      "citeRegEx" : "Brunskill et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Brunskill et al\\.",
      "year" : 2008
    }, {
      "title" : "Dictionary of Eye Terminology",
      "author" : [ "Cassin", "Barbara", "Rubin", "Melvin L" ],
      "venue" : "Triad Publishing Company (FL),",
      "citeRegEx" : "Cassin et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Cassin et al\\.",
      "year" : 2001
    }, {
      "title" : "Bipedal walking on rough terrain using manifold control",
      "author" : [ "T. Erez", "W.D. Smart" ],
      "venue" : "In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),",
      "citeRegEx" : "Erez and Smart,? \\Q2007\\E",
      "shortCiteRegEx" : "Erez and Smart",
      "year" : 2007
    }, {
      "title" : "What does shaping mean for computational reinforcement learning",
      "author" : [ "Erez", "Tom", "Smart", "William D" ],
      "venue" : "IEEE International Conference on Development and Learning (ICDL),",
      "citeRegEx" : "Erez et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Erez et al\\.",
      "year" : 2008
    }, {
      "title" : "Coupling perception and action using minimax optimal control",
      "author" : [ "Erez", "Tom", "Smart", "William D" ],
      "venue" : "In IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL),",
      "citeRegEx" : "Erez et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Erez et al\\.",
      "year" : 2009
    }, {
      "title" : "Region-based incremental pruning for POMDPs",
      "author" : [ "Feng", "Zhengzhu", "Zilberstein", "Shlomo" ],
      "venue" : "In The 20th conference on Uncertainty in artificial intelligence (UAI),",
      "citeRegEx" : "Feng et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Feng et al\\.",
      "year" : 2004
    }, {
      "title" : "Solving POMDPs with continuous or large discrete observation spaces",
      "author" : [ "Hoey", "Jesse", "Poupart", "Pascal" ],
      "venue" : "In International Joint Conference on Artificial Intelligence (IJCAI),",
      "citeRegEx" : "Hoey et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Hoey et al\\.",
      "year" : 2005
    }, {
      "title" : "Grasping POMDPs",
      "author" : [ "Hsiao", "Kaijen", "Kaelbling", "Leslie Pack", "Lozano-Perez", "Tomas" ],
      "venue" : "In IEEE International Conference on Robotics and Automation (ICRA),",
      "citeRegEx" : "Hsiao et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Hsiao et al\\.",
      "year" : 2007
    }, {
      "title" : "Planning and acting in partially observable stochastic domains",
      "author" : [ "Kaelbling", "Leslie Pack", "Littman", "Michael L", "Cassandra", "Anthony R" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Kaelbling et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Kaelbling et al\\.",
      "year" : 1998
    }, {
      "title" : "A POMDP framework for coordinated guidance of autonomous uavs for multitarget tracking",
      "author" : [ "Miller", "Scott A", "Harris", "Zachary A", "Chong", "Edwin K. P" ],
      "venue" : "EURASIP Journal of Advanced Signal Process,",
      "citeRegEx" : "Miller et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Miller et al\\.",
      "year" : 2009
    }, {
      "title" : "Belief space planning assuming maximum likelihood observations",
      "author" : [ "Platt", "Robert", "Tedrake", "Russ", "Kaelbling", "Leslie Pack", "Lozano-Perez", "Tomas" ],
      "venue" : "In Robotics: Science and Systems (R:SS),",
      "citeRegEx" : "Platt et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Platt et al\\.",
      "year" : 2010
    }, {
      "title" : "Point-based value iteration for continuous POMDPs",
      "author" : [ "Porta", "Josep M", "Vlassis", "Nikos", "Spaan", "Matthijs T.J", "Poupart", "Pascal" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Porta et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Porta et al\\.",
      "year" : 2006
    }, {
      "title" : "The belief roadmap: Efficient planning in belief space by factoring the covariance",
      "author" : [ "S. Prentice", "N. Roy" ],
      "venue" : "The International Journal of Robotics Research,",
      "citeRegEx" : "Prentice and Roy,? \\Q2009\\E",
      "shortCiteRegEx" : "Prentice and Roy",
      "year" : 2009
    }, {
      "title" : "Coastal navigation with mobile robots",
      "author" : [ "Roy", "Nicholas", "Thrun", "Sebastian" ],
      "venue" : "In Advances in Neural Processing Systems (NIPS),",
      "citeRegEx" : "Roy et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Roy et al\\.",
      "year" : 1999
    }, {
      "title" : "The Optimal Control of Partially Observable Markov Processes",
      "author" : [ "E.J. Sondik" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "Sondik,? \\Q1971\\E",
      "shortCiteRegEx" : "Sondik",
      "year" : 1971
    }, {
      "title" : "Planning with continuous actions in partially observable environments",
      "author" : [ "Spaan", "Matthijs T. J", "Vlassis", "Nikos A" ],
      "venue" : "In IEEE International Conference on Robotics and Automation (ICRA),",
      "citeRegEx" : "Spaan et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Spaan et al\\.",
      "year" : 2005
    }, {
      "title" : "Optimal Control and Estimation",
      "author" : [ "Stengel", "Robert F" ],
      "venue" : "Dover Publications,",
      "citeRegEx" : "Stengel and F.,? \\Q1994\\E",
      "shortCiteRegEx" : "Stengel and F.",
      "year" : 1994
    }, {
      "title" : "Rigid-body dynamics with friction and impact",
      "author" : [ "Stewart", "David E" ],
      "venue" : "SIAM Reviews,",
      "citeRegEx" : "Stewart and E.,? \\Q2000\\E",
      "shortCiteRegEx" : "Stewart and E.",
      "year" : 2000
    }, {
      "title" : "Policies based on trajectory libraries",
      "author" : [ "Stolle", "Martin", "Atkeson", "Chris" ],
      "venue" : "In IEEE International Conference on Robotics and Automation (ICRA),",
      "citeRegEx" : "Stolle et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Stolle et al\\.",
      "year" : 2006
    }, {
      "title" : "Receding horizon differential dynamic programming",
      "author" : [ "Tassa", "Yuval", "Erez", "Tom", "Smart", "William" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Tassa et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Tassa et al\\.",
      "year" : 2008
    }, {
      "title" : "LQR-trees: Feedback motion planning on sparse randomized trees",
      "author" : [ "Tedrake", "Russ" ],
      "venue" : "In Robotics: Science and Systems (R:SS),",
      "citeRegEx" : "Tedrake and Russ.,? \\Q2009\\E",
      "shortCiteRegEx" : "Tedrake and Russ.",
      "year" : 2009
    }, {
      "title" : "Monte carlo POMDPs",
      "author" : [ "S. Thrun" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Thrun,? \\Q2000\\E",
      "shortCiteRegEx" : "Thrun",
      "year" : 2000
    }, {
      "title" : "Pros and cons of truncated gaussian ep in the context of approximate inference control",
      "author" : [ "Toussaint", "Marc" ],
      "venue" : "NIPS workshop on Probabilistic Approaches for Robotics and Control,",
      "citeRegEx" : "Toussaint and Marc.,? \\Q2009\\E",
      "shortCiteRegEx" : "Toussaint and Marc.",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : "edu interest in tackling continuous domains (Porta et al., 2006; Brooks, 2009).",
      "startOffset" : 44,
      "endOffset" : 78
    }, {
      "referenceID" : 11,
      "context" : "The standard approach to solving POMDPs is to find an approximate solution to the fully-observable belief -MDP, whose states are probability distributions over the state space of the original POMDP (Kaelbling et al., 1998).",
      "startOffset" : 198,
      "endOffset" : 222
    }, {
      "referenceID" : 24,
      "context" : "However, the belief space of a continuous POMDP is infinite-dimensional, and must be approximated (Thrun, 2000).",
      "startOffset" : 98,
      "endOffset" : 111
    }, {
      "referenceID" : 17,
      "context" : "The optimal value function of belief-MDPs is piecewiselinear and convex in the discrete case (Sondik, 1971), and this also holds for some cases of continuous state (Porta et al.",
      "startOffset" : 93,
      "endOffset" : 107
    }, {
      "referenceID" : 14,
      "context" : "The optimal value function of belief-MDPs is piecewiselinear and convex in the discrete case (Sondik, 1971), and this also holds for some cases of continuous state (Porta et al., 2006), as long as the observations and actions are discrete.",
      "startOffset" : 164,
      "endOffset" : 184
    }, {
      "referenceID" : 3,
      "context" : "This result was used to tackle domains with continuous hybrid-linear dynamics by Brunskill et al. (2008). Other combinations of the discrete and the continuous domains were also considered (Hoey & Poupart, 2005; Spaan & Vlassis, 2005).",
      "startOffset" : 81,
      "endOffset" : 105
    }, {
      "referenceID" : 12,
      "context" : "The method we present here is very similar to Miller et al. (2009), who used the term nominalbelief optimization, and Platt et al.",
      "startOffset" : 46,
      "endOffset" : 67
    }, {
      "referenceID" : 12,
      "context" : "The method we present here is very similar to Miller et al. (2009), who used the term nominalbelief optimization, and Platt et al. (2010), who used the term maximum-likelihood observations.",
      "startOffset" : 46,
      "endOffset" : 138
    }, {
      "referenceID" : 22,
      "context" : "We use a local optimization scheme called Differential Dynamic Programming (DDP), an algorithm that has been successfully applied to real-world high-dimensional, non-linear control domains (e.g., Abbeel & Ng, 2005; Tassa et al., 2008).",
      "startOffset" : 189,
      "endOffset" : 234
    }, {
      "referenceID" : 13,
      "context" : "This can be solved using a variety of techniques; Platt et al. (2010), for example, use the multiple shooting method.",
      "startOffset" : 50,
      "endOffset" : 70
    }, {
      "referenceID" : 13,
      "context" : "This can be solved using a variety of techniques; Platt et al. (2010), for example, use the multiple shooting method. We use a local optimization scheme called Differential Dynamic Programming (DDP), an algorithm that has been successfully applied to real-world high-dimensional, non-linear control domains (e.g., Abbeel & Ng, 2005; Tassa et al., 2008). The interested reader may find an in-depth description of the algorithm in Jacobson & Mayne (1970).",
      "startOffset" : 50,
      "endOffset" : 453
    }, {
      "referenceID" : 24,
      "context" : "First, we demonstrate key features of our method by considering an example of planar navigation, roughly corresponding to domains considered by Roy & Thrun (1999) and Brooks (2009).",
      "startOffset" : 150,
      "endOffset" : 163
    }, {
      "referenceID" : 24,
      "context" : "First, we demonstrate key features of our method by considering an example of planar navigation, roughly corresponding to domains considered by Roy & Thrun (1999) and Brooks (2009). Then, we demonstrate the scalability of our method by solving a 16-dimensional problem, first presented by Erez & Smart (2009).",
      "startOffset" : 150,
      "endOffset" : 181
    }, {
      "referenceID" : 24,
      "context" : "First, we demonstrate key features of our method by considering an example of planar navigation, roughly corresponding to domains considered by Roy & Thrun (1999) and Brooks (2009). Then, we demonstrate the scalability of our method by solving a 16-dimensional problem, first presented by Erez & Smart (2009).",
      "startOffset" : 150,
      "endOffset" : 309
    }, {
      "referenceID" : 24,
      "context" : "The disambiguating property of the contact with the wall is termed “coastal navigation” (Roy & Thrun, 1999), and our algorithm is able to identify and leverage this feature as it emerges in the optimal solution. We cannot offer a direct comparison of our results with Brooks (2009), since his experiments were conducted on real robots.",
      "startOffset" : 95,
      "endOffset" : 282
    }, {
      "referenceID" : 22,
      "context" : "One natural extension of this work could employ local optimization from multiple starting points, creating a controller that uses a trajectory library (Stolle & Atkeson, 2006; Tassa et al., 2008).",
      "startOffset" : 151,
      "endOffset" : 195
    } ],
    "year" : 2010,
    "abstractText" : "Partially-Observable Markov Decision Processes (POMDPs) are typically solved by finding an approximate global solution to a corresponding belief-MDP. In this paper, we offer a new planning algorithm for POMDPs with continuous state, action and observation spaces. Since such domains have an inherent notion of locality, we can find an approximate solution using local optimization methods. We parameterize the belief distribution as a Gaussian mixture, and use the Extended Kalman Filter (EKF) to approximate the belief update. Since the EKF is a first-order filter, we can marginalize over the observations analytically. By using feedback control and state estimation during policy execution, we recover a behavior that is effectively conditioned on incoming observations despite the unconditioned planning. Local optimization provides no guarantees of global optimality, but it allows us to tackle domains that are at least an order of magnitude larger than the current state-of-the-art. We demonstrate the scalability of our algorithm by considering a simulated hand-eye coordination domain with 16 continuous state dimensions and 6 continuous action dimensions.",
    "creator" : "TeX"
  }
}