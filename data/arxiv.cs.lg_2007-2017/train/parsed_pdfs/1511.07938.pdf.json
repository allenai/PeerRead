{
  "name" : "1511.07938.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "LAB TESTS", "Narges Razavian", "David Sontag" ],
    "emails" : [ "razavian@cs.nyu.edu", "dsontag@cs.nyu.edu" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Representation learning and unsupervised feature discovery via deep learning has led to ground breaking advances in domains such as image processing (Krizhevsky et al., 2012), speech recognition (Graves & Schmidhuber, 2005), natural language processing (Mikolov et al., 2013), surpassing methods based on hand-engineered features in all benchmarks tested. Following recent availability of large electronic medical record datasets and other biological signals (Hsiao et al., 2014), discovery of early temporal disease signatures within lab values has become a possibility. In this paper, we are interested in discovering these signatures to perform early diagnosis of multiple preventable and treatable diseases.\nThere are many challenges associated with performing machine learning on observational medical data. Data is almost never missing at random and often sparse. Labels such as disease onsets, if they exist, are noisy. Many unobserved variables also affect the outcome. Each individual has a different baseline healthy state, and variations compared to their own baseline indicates whether they have deviated from their optimal health state. This last characteristic has inspired us to train a temporal convolution model (Le Cun et al., 1990; LeCun et al., 1998; Tompson et al., 2014; Krizhevsky et al., 2012) to learn variation patterns of labs as biological representations of healthy and diseased states. In the clinical domain, each biomarker varies with a different natural speed of change in the body. Therefore, in this paper we focus on multi-resolution deep convolutional architectures, inspired by Mnih et al. (2014).\nStrong biases exist in the frequency and timing of lab measurements. For instance, a person suspected to have diabetes is likely to have more Glucose lab tests ordered by the physician. As we will see later in the experiments, the utilization signals (i.e. how often and when each lab is ordered) are predictive of disease onset as well. But recent health care developments, such as Theranos lab testing startup or affordable wearables capable of measuring different chemicals at home, will likely result in the process of obtaining lab tests becoming both significantly cheaper and easier. As a result, we expect that this utilization signal will look very different in a few years from how it is today. Additionally, medical community has actively studied the causal effect of variations on different signals, such as glucose (Kilpatrick et al., 2007), cholesterol (Bangalore et al., 2015), blood pressure(Hata et al., 2013), and prostate-specific antigen (Roehrborn et al., 1996) on different disease onsets. A model trained on the imputed biological measurements rather than the healthcare utilization signals\nar X\niv :1\n51 1.\n07 93\n8v 4\n[ cs\n.L G\n] 1\n1 M\nar 2\n01 6\ncan better aid with hypothesis generation for such causal studies, which can lead to meaningful interventions. For these reasons, and to provide models that reveal the disease signatures (i.e. changes in the actual chemicals in the body prior to a disease onset), we consider models that work on an imputed version of the lab data.\nA generative model that captures all sources of bias could potentially remove the utilization effects using marginalization and inference. Examples of such models include Gaussian processes and generative models based on recurrent neural networks (Sutskever et al., 2011; 2009; Tang et al., 2014; Chung et al., 2015). However, for high dimensional structured continuous input time series, when the variables are not observed at the same time, marginalization can be prohibitively slow or not possible.\nInstead, we propose a convolution based formulation of multivariate nonparametric (kernel) regression, which is capable of inferring the structure of the input as part of the imputation task. This approach can obtain competitive results as Gaussian processes for univariate data, and is extremely fast to train for asynchronous multivariate data. Moreover, although we do not explore it in this paper, this approach to imputation is amenable to end-to-end training together with the supervised prediction task.\nOur paper is structured as follows: We first present our prediction model architecture, which is a multi-resolution convolution network with shared components for multi-task learning. We then present our imputation model architecture, which is based on a differentiable formulation of nonparametric (kernel) density estimation, for single time-series as well as multiple dependent timeseries. The final architecture is the combination of the imputation network with the prediction network. Our evaluation is performed on an original dataset of 298,000 individuals tracked for 8 years. We use temporal observations of 18 most commonly measured lab measurements and perform early (at least 3 months in advance) detection of 171 diseases and conditions. We compare our imputation and prediction results to an extensive set of baselines and various input signals, and show that the temporal signatures learned via convolution are significantly more predictive than baselines commonly used for early disease diagnosis.\nAlthough we present the new multi-resolution deep convolutional architecture and the multivariate nonparametric regression algorithms in the context of early diagnosis from lab tests, we emphasize that both of these algorithms are much more broadly applicable to prediction problems in machine learning with temporal, sparse and irregularly measured, multivariate data. User behavior modeling, financial data analysis, and providing useful service from wearables are among domains where the data exhibits similar characteristics and challenges."
    }, {
      "heading" : "2 TEMPORAL CONVOLUTIONAL NETWORK",
      "text" : "We formulate the task of diagnosis as a supervised multi-task classification task. Each individual has a variable-length history of lab observations (X) and diagnosis records (Y ). X is continuous valued, and Y is binary. We use a sliding window framework to deal with variable length input. At each time point t for each person X , the model looks at a backward window of 36 months of all D biomarkers of the input, X1:Dt−36:t, to predict the output. Output is a binary vector Y of size M , corresponding to M disease onsets each happening within the following months from t + 3 to t+ 3 + 24. In this paper we consider 18 commonly measured biomarkers, and predict 171 common diseases (i.e. D = 18, M = 171). To retain clinical validity for our early detection task, we have carefully designed our experimental setting, outcome definition and exclusion criteria, which we discuss in details in section 4.2.\nOur temporal convolution model is shown in figure 1. The input to the model can be raw(unimputed) observations; imputed observations; or the concatenation of the imputed data and the binary observation pattern. The choice of the input will allow us to analyze the nature of signals that better predicts each disease. Binary observation pattern only encodes the health-care interaction signal, which is subject to fast change as the health-care policies and the economy change. While currently useful, these health-care interaction signals will have different distributions in the era where preventive medicine is in full practice. Therefore, a model which relies on the chemical state of the body (i.e. imputed observations) would be required. We present the imputation network in section 3.3 and the full model of imputation and prediction is shown in figure 4.\nThe prediction part of network is shown in figure 1. Specifically, we defined X1:Dt−36:t to be the input of the network at time t. Let there be J number of filters (or patterns) in each of the levels of the multi-resolution convolution network. Each filter Kji (j = 1 : J) is of size 1 × L, corresponding to temporal filters of size L at different resolution levels i. The third level of resolution includes two layers of convolution corresponding to filters Kj3 and K j 5 . The output of the multi-resolution convolution network is a vector C = [C1, C2, C5] which is defined as follows:\nCd,j1 =f(b j 1 + (K j 1 ∗MaxPool(Xdt−36:t, p2))) (1)\nCd,j2 =f(b j 2 + (K j 2 ∗MaxPool(Xdt−36:t, p))) (2)\nCd,j3 =f(b j 3 + (K j 3 ∗X1:Dt−36:t)) (3)\nCd,j4 =MaxPool(C d,j 3 , p) (4)\nCd,j5 =f(b j 5 + J∑ k=1 Kj5 ∗ C d,k 4 ) (5)\nIn the above definition, ∗ is a standard convolution operation. f is a ReLU nonlinearity function (Nair & Hinton, 2010). The vector Ci is the concatenation of C d,j i for all biomarkers d = 1 : D and filters j = 1 : J , and MaxPool(X, p) corresponds to non-overlapping max pooling operation defined asMaxPool(Z, p)[i] = max(Z[p ·i : p ·(i+1)−1]) for each i = 1 : floor(length(Z)/p). The value of p is set to 3 in our case. bji is a bias term and is learned during training. After every convolution operation we use batch normalization (Ioffe & Szegedy, 2015).\nAfter the multi-resolution convolution is applied, the vector C represents the application of filters to all biomarkers, and we note that the filters are shared across all biomarkers. We then use 2 layers of hidden nodes to allow non-linear combination of filter activations on different biomarkers.\nh1 =f(W T 1 C + bh1) (6)\nh2 =f(W T 2 h1 + bh2) (7)\nWi is the weight of the hidden nodes and bhi is the bias associated with each layer. Each of the hidden layers are subject to Dropout(Srivastava et al., 2014) regularization (with probability 0.5) during training, and are followed by batch normalization.\nFinally, for each disease m = 1 : M , the model predicts the likelihood of the disease via logistic regression over h2.\nP (Ym = 1|X1:Dt−36:t) = σ(WTmh2 + bm) (8)\nThe loss function for each disease is the negative log likelihood of the true label, weighted by the inverse label ratio to handle class imbalance during multi-task batch training. Diseases are trained independently, but the gradient is propagated through the shared part of the network.\nFigure 1 is shown for imputed or un-imputed input. Prediction model for the case where input is the concatenation of imputed and binary observation mask is identical, except that the input is then of size 18× 2 (i.e. vertical concatenation) times length of the backward window (36 months). Specific architectural choices and values of hyper-parameters are described in section 4.3"
    }, {
      "heading" : "2.1 RELATED WORK",
      "text" : "Medical field has been dominated by traditional feature engineering methods. Only recently, attempts to learn the patterns has started to gain some attention. (Lasko et al., 2013) studied a method based on sparse auto-encoders to learn temporal variation features from 30-day uric acid observations, to distinguish between gout and leukemia. (Che et al., 2015) developed a training which allows prior domain knowledge to regularize the deeper layers of feed-forward network, for the task of multiple disease classification when datasets are small. To our knowledge, a full scale study of convolutional neural networks for the task of disease pattern discovery has not yet been performed.\nWithin the domain of temporal convolutional networks, (Abdel-Hamid et al., 2012; Sainath et al., 2013) were among the first to show significant gains in speech recognition tasks in large scale. Unlike speech domain where the input is fully observed, in our case we have sparse and asynchronously measured observations. Alternative models would be recurrent neural network(RNN) models such as variants of LSTM, however given the state of the body in the past few years, there is no clear evidence that longer term dependencies are necessary. In addition, the trends on the biomarkers which are directly learned via temporal convolution might provide more clinically interpretable results currently. For these reasons we focus on temporal multi-resolution convolution model in this paper."
    }, {
      "heading" : "3 IMPUTATION VIA DIFFERENTIABLE KERNEL REGRESSION",
      "text" : "In order to learn biological disease signatures, we now present our imputation model which we apply to the input prior to learning the variation patterns. Our model is based on nonparametric regression, which we formulate as differentiable functions of (univariate and multivariate) kernels and input. Using back-propagation (Rumelhart et al., 1988), we then show how one can learn the entire form of the kernel function instead of cross validating within a limited set of parametric family (such as Gaussian or Laplace). We compare our method to Gaussian processes and traditional nonparametric(kernel) regression which use standard kernel functions."
    }, {
      "heading" : "3.1 RELATED WORK ON UNIVARIATE KERNEL LEARNING",
      "text" : "Within the field of nonparametric methods, most existing work only cross validate over a few wellknown kernel functions such as Radial basis(Gaussian), Laplace, or other simple kernels, and fail to consider the entire space of legal kernels. In best case, attempts such as (Duvenaud et al., 2013), (Gönen & Alpaydın, 2011) learn a composition or combination of kernel families. The algorithms are slow, and in practice, the search algorithm is not comprehensive enough to guarantee recovery of the correct kernel. Additionally learning multivariate kernels are also not possible using these methods. Our proposed solution overcomes all these issues."
    }, {
      "heading" : "3.2 UNIVARIATE KERNEL REGRESSION: LEARNING THE KERNEL",
      "text" : "Imagine the input to be samples from D time series, each sampled irregularly. We denote the samples as x1\nt11 , x1 t12 , ..., x1t1n1 , ...,xD tD1 , xD tD2 , ..., xDtDnD , where xd refers to time series d and td1,... t d nd refer to the time points over which time series d is sampled. Kernel regression provides a general formalism for\nestimating any function with additive noise, provided that the signal is locally stationary. Let’s start from a single time series, x(t). Kernel regression assumes the following:\nx = f(t) +\n∼ N(0, σ2) Given observed samples xt1 ,...xtn from the series, general function regression with additive noise lets us estimate the value of x at a new time point tnew as follows.\nx(tnew) = Ex∼P (x|t=tnew)[x]\nEx∼P (x|t=tnew)[x] = ∫ x xP (x|t = tnew)dx = ∫ x x P (x, t = tnew) P (tnew) dx\nAt this point, one can use kernel density estimation to estimate the probabilities P (x, t = tnew) and P (tnew) from the training data. (Nadaraya, 1964) and (Watson, 1964) showed that using a positive semidefinite kernel function K(t, t′), the nonparametric regression formulation is reduced to:\nEx∼P (x|t=tnew)[x] = ∑n i=1 xtiK(tnew, ti)∑n i=1K(tnew, ti)\n(9)\nWe can now rewrite the nonparametric regression using convolution operator. To be able to use functional notation, we first write the sequence of observed samples xt1 ,...,xtn as a function: X̄train(t) = ∑n i=1 xtiδ(t, ti), where δ(t, τ0) = 1 when t = τ0, and 0 otherwise.\nDenoting convolution operator as ∗, i.e. (K ∗ f)(t) = ∫ τ K(t − τ)f(τ)dτ , the numerator of the\nkernel regression is equal to: ∑n i=1 xtiK(ti − tnew) = (K ∗ X̄train)(tnew). The denominator P (tnew) can similarly be written as a convolution of the kernel function with a sequence of 1s at each point at which we have a sample, denoted as I(X̄train : observed)(t) = ∑n i=1 δ(t, ti).\nn∑ i=1 K(tnew, ti) = (K ∗ I(X̄train : observed))(tnew)\nSo the kernel regression formulation of Nadaraya and Watson reduces to the following formulation.\nEx∼P (x|t=tnew)[x] = (K ∗ X̄train)(tnew)\n(K ∗ I(X̄train : observed))(tnew)\nThis formulation has previously been used in image processing literature under the name normalized convolution (Knutsson & Westin, 1993), however only parametric kernels have been considered before. By writing the kernel regression as a fully differentiable function, we can now learn K(τ) at each position τ within the kernel domain via back-propagation. We can also compose this differentiable kernel regression module within any subsequent differentiable operators and perform multiple tasks.\nIn this paper we use leave-one-out imputation mean squared error as the loss function. In practice, we assume the domain of K is bounded between [−M,M ], therefore the learning task will have 2M + 1 parameters. Figure 2 shows the architecture of this model."
    }, {
      "heading" : "3.3 MULTIVARIATE KERNEL REGRESSION: LEARNING THE TEMPORAL KERNEL AND DEPENDENCY STRUCTURE",
      "text" : "Let’s now assume that we have D time series, corresponding to each of the labs. We could attempt to model the full joint distribution of the time series, so that observations of related labs at nearby times could be used to infer the values of missing labs. Various multi-output extensions of Gaussian processes have been proposed previously. The dominant approaches rely on Bayesian formalization and process convolution (Boyle & Frean, 2004; Alvarez et al., 2010; Alvarez & Lawrence, 2009), and require known dependency structure on the multiple outputs. The main problem with the models is that they are only scalable under the sparse structure assumptions (Alvarez et al., 2011; Byron et al., 2009). In biological domains, observed variables are highly correlated due to many unobserved latent variables. In the general high-dimensional tightly correlated setting, considering that the parameters of the kernels need to be tuned via cross-validation, these models are not scalable.\nOne solution for unconstrained structure was proposed in (Wilson et al., 2012), however inference required Monte Carlo sampling or variational inference, which were inefficient. Alternatively, one could model the full joint distribution for a window of interest using a nonparametric graphical model (Fukumizu et al., 2007; Smola et al., 2007; Song et al., 2011). However, since the labs are measured asynchronously with significant missing data, inference and learning of these models can be extremely slow. Our proposed framework allows us to easily extend the univariate kernel regression to the multivariate setting, giving a very fast and – as we show in the experiments – accurate multivariate approach."
    }, {
      "heading" : "3.4 MULTIVARIATE KERNEL REGRESSION",
      "text" : "We extend the kernel K to be a matrix of size D × (2M + 1), and learn the kernel magnitude at (r,j,s) corresponding to kernel value between the imputed series at time r and series j at time s. Multivariate kernel regression becomes a 2D convolution of this kernel matrix with all time series’ observed points in the numerator, normalized by the 2D convolution of the kernel matrix with a binary matrix encoding which series at which time point does have a nonzero observation.\nExd∼P (xd|t=tnew)[x d] = (K ∗ X̄1..dtrain)(tnew) (K ∗ I(X̄1..dtrain : observed))(tnew)\nFigure 3 shows the model for one output variable. Similar to the univariate training, for each time series, for each observation, we mask that observation and optimize the mean squared error of the true value compared to the predicted value using multivariate kernel. In our current formulation, we learn a separate D × (2M + 1) sized kernel for each lab. Finally figure 4 shows the full imputation and prediction architecture together. For each lab, the multivariate kernel is learned via pre-training and optimization of MSE for that lab. We then fix the imputation parameters, and train the consequent prediction network. We note that end-to-end training of the entire network (imputation and prediction) using the only prediction network’s loss function will result in a different loss function than MSE for the imputation network. Joint training of the two networks, perhaps by optimizing both loss functions (negative log likelihood of predictions and mean squared error of imputations) is part of our future work.\nFinally, we note that at each time point, the input is truncated outside the backward window before imputation, therefore no information from the future is affecting the prediction."
    }, {
      "heading" : "4 EXPERIMENTS AND RESULTS",
      "text" : ""
    }, {
      "heading" : "4.1 DATA",
      "text" : "Our original dataset consisted of lab measurement and diagnosis information for 298,000 individuals. The lab measurements had the resolution of 1 month, and we used a backward window of 36 months for each prediction. We limited this paper’s input to comprehensive lab panel plus cholesterol and bilirubin (together 18 lab types), which are currently recommended annually and covered by insurance companies. The name and code of labs used in our analysis is included in Table 1. Each\nlab value was normalized by subtracting the mean and dividing by standard deviation across the entire dataset. We randomly divided individuals to a 100K training set, a 100K validation set, and a 98K test set. Validation set was used to select the best epoch/parameters for models and prediction results are presented on the test set, unseen during the training and validation.\nOutput corresponded to diagnosis information of these individuals. In our dataset, each disease diagnosis is recorded as an ICD9-CM (International Classification of Diseases, Ninth Revision, Clinical Modification) code. These codes are somewhat noisy, therefore we defined our prediction task carefully to improve the analysis quality as we describe next."
    }, {
      "heading" : "4.2 PREDICTION TASK SETUP",
      "text" : "Our goal is early diagnosis of diseases, for people who do not already have the disease. We required a 3 month gap between the end of the backward window (i.e. t), and the start of early diagnosis window. The purpose of the 3 month gap was to ensure that the clinical tests taken right before diagnosis of a disease would not allow our system to cheat in the prediction of that disease. Each output label was defined as positive if the diagnosis code for the disease was observed in at least 2 distinct months between 3 to 3 + 24 months after t. Using 24 months helps alleviate the noisy label problem. Requiring at least 2 observations of the noise also reduced the noise coming from ”up-coding” physicians (physicians who report their wrong suspected diagnosis as a diagnosis). For each disease, we excluded individuals who already have the disease by time t + 3. Our exclusion required only 1 diagnosis record instead of 2. This results in a more difficult, but also more clinically meaningful and interesting prediction task."
    }, {
      "heading" : "4.3 PREDICTION MODEL ARCHITECTURE DETAILS",
      "text" : "The specific architectural choices for the shared part of the prediction network is as follows: We set the number of filters to be 8 for all convolution modules, with the kernel length 3(months) and step size of 1. Each Max-pooling module has the horizontal length of 3 and vertical length of 1,\nwith step size of 3 in horizontal direction(i.e. no overlap). Each convolution module is followed by a batch normalization module (Ioffe & Szegedy, 2015) and then a ReLU nonlinearity (Nair & Hinton, 2010). We have 2 fully connected layers (with 100 nodes each) after the concatenation of outputs of all convolution layers. Each of the fully connected layers are followed by a batch normalization layer and a ReLu nonlinearity layer. We also add one Dropout module (Srivastava et al., 2014) (0.5 dropout probability) before each fully connected layer. After the last ReLu nonlinearity, corresponding to the output of the shared part of the network, for each 171 diseases we have the followings in order: A Dropout layer(0.5 dropout probability), a fully connected layer (of size 2 nodes corresponding to binary outcome), batch normalization layer and a Log Softmax Layer. Learning rate was selected from among the values [0.001, 0.01, 0.05, 0.1, 1] using validation set average (over all diseases) Area Under ROC curve after 10 epochs. Value of 0.01 was selected for the learning rate. Similarly learning rate decay of 0.95 was selected for learning rate decay from the list [0.8, 0.9, 0.95, 0.99]. Training was done using stochastic gradient descent, over mini-batches of size 256. We implemented the architecture using Torch (Collobert et al., 2011)."
    }, {
      "heading" : "4.4 DATA AUGMENTATION",
      "text" : "During the training of the kernel regression imputation, we randomly perturbed each time series by adding Gaussian noise with standard deviation of 0.01 to each lab observation, and also randomly perturbed the time of each observation by a random jump drawn from a Gaussian distribution in either direction with standard deviation of 2 (we take the floor of the continuous value to determine the integer number of months to shift). We found this step to be especially important for learning robust imputation kernels."
    }, {
      "heading" : "4.5 IMPUTATION RESULTS",
      "text" : "First, we pre-trained the imputation layer by optimizing the mean squared error. For each observation, we masked the value, and asked the network (or our classic baselines of Gaussian Processes (Rasmussen (2006)) and classic Kernel regression) to predict the masked value given the rest of the observations. In case of the multivariate network we masked the data observed on the entire month during the training. We note that without data augmentation this method would not learn the value of the kernel at the origin (i.e. t − t′ = 0), and that’s why randomly perturbing the time of the observations by a small amount is essential. Our baselines included univariate Gaussian Processes and univariate kernel regression. We used cross validation to select kernel family (Gaussian, Laplace and Triangular), kernel bandwidth, and in case of Gaussian Processes also the diagonal noise magnitude in the kernel matrix.\nNetworks for each lab were trained independently. For this part of the analysis we used a random subset of 10,000 patients. 8,000 individuals were selected for training (or cross validation, in case of the baselines), and the other 2000 participated in evaluation.\nTable 1 shows the quality of the imputation on the lab values. Univariate models perform similarly, and since this is the result on the cohort which is already normalized, the univariate models are not reliable for many labs at all. But learning and using multivariate kernel model leads to a distinct improvement in the imputation quality. In Figure 5, you can see the learned univariate kernel for Creatinine lab. Trying to compose known families of kernel (i.e. Laplace or a mixture of Laplace kernels seems to fit well for the shape) to recover this form is not guaranteed to lead to the optimal data driven kernel. In Figure 6 you can see the multivariate kernels learned with our multivariate kernel regression framework. Our results indicate that the kernels capture the relationship between different variables well. Interesting to note is the lab value we purposefully did not discard, which is a ratio of two other lab values(Urea nitrogen/Creatinine). Our formulation of multivariate kernel regression only allows linear construction at this level of depth, and we see that the ratio is approximated with positive weight for numerator(Urea nitrogen) and negative weight for denominator(Creatinine).\nKernel based imputation method has a property where the kernel is symmetric, looking into past and future for the imputation. Our training method optimizes the prediction of observations within and at the border of each lab time series, therefore allowing the kernels to adjust to the border cases where only past(or future) data is available. However, in the consequent disease prediction model we only use a 3-year backward window which is shorter than the typical time-span on which imputation was\nTable 1: RMSE for different lab values and different models. Models are: GP(Univariate Gaussian Processes), KR(Kernel Regression), ConvKR(Univariate convolution formulation of KR), and ConvKR multivariate.\nLab GP KR univar Conv KR univar Conv KR multivar\nCreatinine 0.397 0.406 0.433 0.096 Urea nitrogen 0.449 0.457 0.465 0.131 Potassium 0.995 1.011 1.010 0.170 Glucose 0.716 0.709 0.690 0.118 Alanine aminotransferase 0.653 0.677 0.679 0.127 Aspartate aminotransferase 0.708 0.720 0.710 0.130 Protein 1.142 1.194 1.220 0.206 Albumin 1.092 1.128 1.120 0.263 Cholesterol 0.621 0.631 0.651 0.118 Triglyceride 0.640 0.633 0.696 0.104 Cholesterol.in LDL 0.640 0.649 0.648 0.108 Calcium 1.614 1.652 1.703 0.260 Sodium 0.722 0.717 0.742 0.139 Chloride 0.672 0.674 0.688 0.113 Carbon dioxide 0.782 0.783 0.782 0.131 Urea nitrogen/Creatinine 0.601 0.606 0.600 0.075 Bilirubin 0.667 0.687 0.678 0.105 Albumin/Globulin 0.586 0.601 0.636 0.112\n0 5 -5\n5\n4\n3\n2\n7\n6\n1\nFigure 5: The kernel learned for univariate kernel regression for Creatinine biomarker. The x axis indicates time with t =0 at the center, and the y axis is the magnitude of the kernel value around the origin.\ntrained. While we do not explore in this paper, the interplay between the spans of the kernels, the quality of imputation at border cases, and their effect on sub-sequent disease prediction task is an interesting direction for further analysis."
    }, {
      "heading" : "4.6 PREDICTION RESULTS",
      "text" : "Figure 7, and table S1 in the Supplementary section show the area under ROC curve results of predicting each disease on the test set. As baselines we compare the results to multilayer perceptron (MLP) over the entire observations within the 36 month backward window, and logistic regression on maximum value of all 18 lab values over 36 months backward window. For each model, we compared three imputation setting: Raw input, without imputing unobserved values; Imputed input only; and a 2-channel input, composed of imputed input, next to binary observation mask. Results shown in Figure 7 are the best AUCs achieved on any of the imputation settings, per model.\nOur multilayer perceptron baseline had 2 hidden layers shared across all 171 diseases (100 hidden nodes each). Each disease is predicted using a logistic function of the last hidden layer, with its own parameters (implemented as Log Softmax in Torch environment). Batch normalization (Ioffe & Szegedy, 2015) was used after every hidden layer, and Dropout (with probability 0.5) was used before each hidden layer. We used cross-validation to optimize learning rate and learning rate decay for the baseline models. We also selected optimum learning rate and learning rate decay parameters for the convolution network via cross-validation, but fixed the architecture parameters to those described in section 2. As in the case with convolution network, we used weighted negative loglikelihood loss function to train the baselines.\nIn figure S1 and table S2 in Supplementary section, we show the disease classification AUC results on the full set of experiments, comparing different imputation and prediction methods. By imputation, we discard health-care utilization, which is a predictive signal. So it is expected that just using imputation would lower the early detection accuracy, and we observe this in our results as well. However, when we use two separate channels (imputed signal and binary observation mask), the results are comparable to the prediction on the unimputed input, which indicates that the our imputation layer is correctly separating biological signals from the utilization. The trends learned on the imputed channel are more interesting to medical research field, which studies core biological processes, while clinical intervention field is interested in any model that gives better predictive result, using all the signals including the utilization patterns.\nC O\nN V\nN E T B\ne st\nM LP\nB e st\nM a x L\no g it\nB e st\nV22.1 Supervis oth normal preg 626.4 Irregular menstruation 285.21 Anemia in chr kidney dis 585.9 Chronic kidney dis NOS 626.2 Excessive menstruation 585.3 Chr kidney dis stage III 584.9 Acute kidney failure NOS 218.9 Uterine leiomyoma NOS\nV01.6 Venereal dis contact 250.01 DMI wo cmp nt st uncntrl 627.2 Sympt fem climact state 626.8 Menstrual disorder NEC 250.02 DMII wo cmp uncntrld 593.9 Renal & ureteral dis NOS 314.01 Attn deficit w hyperact 428.0 CHF NOS V05.3 Need prphyl vc vrl hepat\n185. Malign neopl prostate 790.93 Elvtd prstate spcf antgn\n274.9 Gout NOS 362.52 Exudative macular degen 706.1 Acne NEC 600.00 BPH w/o urinary obs/LUTS\n511.9 Pleural effusion NOS 616.10 Vaginitis NOS 607.84 Impotence, organic orign 600.01 BPH w urinary obs/LUTS 285.29 Anemia-other chronic dis 314.00 Attn defic nonhyperact\n309.81 Posttraumatic stress dis 346.90 Migrne unsp wo ntrc mgrn\n620.2 Ovarian cyst NEC/NOS 427.31 Atrial fibrillation\n839.20 Dislocat lumbar vert-cl 728.87 Muscle weakness-general 250.00 DMII wo cmp nt st uncntr 425.4 Prim cardiomyopathy NEC 362.51 Nonexudat macular degen 443.9 Periph vascular dis NOS\n424.1 Aortic valve disorder\nC O\nN V\nN E T B\ne st\nM LP\nB e st\nM a x L\no g it\nB e st\n280.9 Iron defic anemia NOS 303.90 Alcoh dep NEC/NOS-unspec\n733.00 Osteoporosis NOS 793.80 Ab mammogram NOS 174.9 Malign neopl breast NOS 414.00 Cor ath unsp vsl ntv/gft\n739.7 Somat dysfunc upper extr 414.9 Chr ischemic hrt dis NOS\n781.2 Abnormality of gait 414.01 Crnry athrscl natve vssl\n286.9 Coagulat defect NEC/NOS 782.3 Edema\n278.01 Morbid obesity 492.8 Emphysema NEC 462. Acute pharyngitis 296.89 Bipolar disorder NEC 496. Chr airway obstruct NEC\n173.3 malignant neo skin(Face) 285.9 Anemia NOS\n780.2 Syncope and collapse 787.01 Nausea with vomiting 427.89 Cardiac dysrhythmias NEC 424.2 Nonrheum tricusp val dis\n433.10 Ocl crtd art wo infrct 682.9 Cellulitis NOS 611.72 Lump or mass in breast 733.90 Bone & cartilage dis NOS\n424.0 Mitral valve disorder 401.9 Hypertension NOS 110.1 Dermatophytosis of nail 216.9 Benign neoplasm skin NOS\n786.05 Shortness of breath 836.0 Tear med menisc knee-cur\n599.0 Urin tract infection NOS 715.90 Osteoarthros NOS-unspec 300.01 Panic dis w/o agorphobia 790.21 Impaired fasting glucose 716.90 Arthropathy NOS-unspec\n216.5 Benign neo skin trunk 702.0 Actinic keratosis\nC O\nN V\nN E T B\ne st\nM LP\nB e st\nM a x L\no g it\nB e st\n309.28 Adjust dis w anxiety/dep 722.0 Cervical disc displacmnt 780.53 Hypersom w slp apnea NOS 461.9 Acute sinusitis NOS\n703.0 Ingrowing nail 379.21 Vitreous degeneration\n401.1 Benign hypertension 402.90 Hyp hrt dis NOS w/o hf\n726.5 Enthesopathy of hip 783.1 Abnormal weight gain 366.16 Senile nuclear cataract 702.19 Other sborheic keratosis 739.1 Somat dysfunc cervic reg 794.31 Abnorm electrocardiogram 238.2 Unc behav neo skin\n272.4 Hyperlipidemia NEC/NOS 702.11 Inflamed sbrheic keratos\n780.57 Sleep apnea NOS 723.4 Brachial neuritis NOS\n300.4 Dysthymic disorder 715.09 General osteoarthrosis\n846.0 Sprain lumbosacral 728.71 Plantar fibromatosis 272.0 Pure hypercholesterolem 995.3 Allergy, unspecified 739.4 Somat dysfunc sacral reg 372.14 Chr allrg conjunctiv NEC\n739.3 Somat dysfunc lumbar reg 380.4 Impacted cerumen\n493.00 Extrinsic asthma NOS 241.0 Nontox uninodular goiter\n281.0 Pernicious anemia 268.9 Vitamin D deficiency NOS 724.02 Spin sten,lumbr wo claud 782.1 Nonspecif skin erupt NEC\n272.2 Mixed hyperlipidemia 695.3 Rosacea 564.1 Irritable bowel syndrome 722.10 Lumbar disc displacement\n785.1 Palpitations\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\nFigure 7: Area Under ROC curve for each disease, comparing the held out test score on our Convolution vs. Multi-layered Perceptron, vs. Logistic Regression over the maximum observed lab in the past 3 years. The actual AUC values are included in table S1\nClinically interesting to note is how well can different diseases be diagnosed at least 3 month in advance, and how many diseases can be detected with much better accuracy compared to current practices, by using the convolution method. In particular, heart failure, severe kidney diseases and liver problems, diabetes and hormone related conditions and prostate cancer are among the diseases which are well detectable early, from only 18 common lab measurements tracked in the past 3 years. Additionally, for patients with multiple existing diseases, side-effects of different medications in addition to their conditions can trigger other unexpected diseases. Monitoring the risks of all diseases is often neglected in the clinics today and our model is a reasonable solution for this task."
    }, {
      "heading" : "5 CONCLUSION",
      "text" : "In this work, we presented the first large scale application of convolutional neural networks for discovery of early temporal disease signatures for the task of disease prediction. We presented a novel approach to nonparametric imputation, which is essential to learning disease signatures that are biologically valid. Our results show significant improvement in the quality of early diagnosis, compared to methods currently used in most of the medical and clinical world, only using 18 lab measurement over the past 3 years.\nOur results indicate that onset of many diseases, including major heart, kidney, and liver diseases, prostate cancer, and diabetes are predictable with high quality in advance. For many of these diseases, early detection even by a few months can lead to significant gains in effectiveness of treatment, quality of life of the patients and their families, and reduction of financial burden on the healthcare systems. Our method also enables large-scale intervention programs to target the most high-risk population better than available baselines, therefore increasing the cost-effectiveness and applicability of the programs. Finally, for every disease presented, detailed analysis of the discovered predictive temporal patterns can lead to new medical insights, and is part of our future work."
    }, {
      "heading" : "ACKNOWLEDGEMENTS",
      "text" : "The authors gratefully acknowledge support by Independence Blue Cross. The Tesla K40s used for this research were donated by the NVIDIA Corporation."
    } ],
    "references" : [ {
      "title" : "Applying convolutional neural networks concepts to hybrid nn-hmm model for speech recognition",
      "author" : [ "Abdel-Hamid", "Ossama", "Mohamed", "Abdel-rahman", "Jiang", "Hui", "Penn", "Gerald" ],
      "venue" : "In Acoustics, Speech and Signal Processing (ICASSP),",
      "citeRegEx" : "Abdel.Hamid et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Abdel.Hamid et al\\.",
      "year" : 2012
    }, {
      "title" : "Sparse convolved gaussian processes for multi-output regression. In Advances in neural information processing",
      "author" : [ "Alvarez", "Mauricio", "Lawrence", "Neil D" ],
      "venue" : null,
      "citeRegEx" : "Alvarez et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Alvarez et al\\.",
      "year" : 2009
    }, {
      "title" : "Efficient multioutput gaussian processes through variational inducing kernels",
      "author" : [ "Alvarez", "Mauricio A", "Luengo", "David", "Titsias", "Michalis K", "Lawrence", "Neil D" ],
      "venue" : "In International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Alvarez et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Alvarez et al\\.",
      "year" : 2010
    }, {
      "title" : "Kernels for vector-valued functions: A review",
      "author" : [ "Alvarez", "Mauricio A", "Rosasco", "Lorenzo", "Lawrence", "Neil D" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Alvarez et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Alvarez et al\\.",
      "year" : 2011
    }, {
      "title" : "Visit-to-visit low-density lipoprotein cholesterol variability and risk of cardiovascular outcomes: Insights from the tnt trial",
      "author" : [ "Bangalore", "Sripal", "Breazna", "Andrei", "DeMicco", "David A", "Wun", "Chuan-Chuan", "Messerli", "Franz H" ],
      "venue" : "Journal of the American College of Cardiology,",
      "citeRegEx" : "Bangalore et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bangalore et al\\.",
      "year" : 2015
    }, {
      "title" : "Dependent gaussian processes",
      "author" : [ "Boyle", "Phillip", "Frean", "Marcus" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Boyle et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Boyle et al\\.",
      "year" : 2004
    }, {
      "title" : "Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity",
      "author" : [ "Byron", "M Yu", "Cunningham", "John P", "Santhanam", "Gopal", "Ryu", "Stephen I", "Shenoy", "Krishna V", "Sahani", "Maneesh" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Byron et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Byron et al\\.",
      "year" : 2009
    }, {
      "title" : "Deep computational phenotyping",
      "author" : [ "Che", "Zhengping", "Kale", "David", "Li", "Wenzhe", "Bahadori", "Mohammad Taha", "Liu", "Yan" ],
      "venue" : "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
      "citeRegEx" : "Che et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Che et al\\.",
      "year" : 2015
    }, {
      "title" : "A recurrent latent variable model for sequential data",
      "author" : [ "Chung", "Junyoung", "Kastner", "Kyle", "Dinh", "Laurent", "Goel", "Kratarth", "Courville", "Aaron", "Bengio", "Yoshua" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Chung et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2015
    }, {
      "title" : "Torch7: A matlab-like environment for machine learning",
      "author" : [ "Collobert", "Ronan", "Kavukcuoglu", "Koray", "Farabet", "Clément" ],
      "venue" : "In BigLearn, NIPS Workshop, number EPFL-CONF-192376,",
      "citeRegEx" : "Collobert et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2011
    }, {
      "title" : "Structure discovery in nonparametric regression through compositional kernel search",
      "author" : [ "Duvenaud", "David", "Lloyd", "James Robert", "Grosse", "Roger", "Tenenbaum", "Joshua B", "Ghahramani", "Zoubin" ],
      "venue" : "arXiv preprint arXiv:1302.4922,",
      "citeRegEx" : "Duvenaud et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Duvenaud et al\\.",
      "year" : 2013
    }, {
      "title" : "Kernel measures of conditional dependence",
      "author" : [ "Fukumizu", "Kenji", "Gretton", "Arthur", "Sun", "Xiaohai", "Schölkopf", "Bernhard" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Fukumizu et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Fukumizu et al\\.",
      "year" : 2007
    }, {
      "title" : "Multiple kernel learning algorithms",
      "author" : [ "Gönen", "Mehmet", "Alpaydın", "Ethem" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Gönen et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Gönen et al\\.",
      "year" : 2011
    }, {
      "title" : "Framewise phoneme classification with bidirectional lstm and other neural network architectures",
      "author" : [ "Graves", "Alex", "Schmidhuber", "Jürgen" ],
      "venue" : null,
      "citeRegEx" : "Graves et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2005
    }, {
      "title" : "Effects of visit-to-visit variability in systolic blood pressure on macrovascular and microvascular complications in patient with type 2 diabetes: the advance trial",
      "author" : [ "Hata", "Jun", "Arima", "Hisatomi", "Rothwell", "Peter M", "Woodward", "Mark", "Zoungas", "Sophia", "Anderson", "Craig", "Patel", "Anushka", "Neal", "Bruce", "Glasziou", "Paul", "Hamet", "Pavel" ],
      "venue" : "Circulation, pp. CIRCULATIONAHA–113,",
      "citeRegEx" : "Hata et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Hata et al\\.",
      "year" : 2013
    }, {
      "title" : "Trends in electronic health record system use among office-based physicians: United states, 2007-2012",
      "author" : [ "Hsiao", "Chun-Ju", "Hing", "Esther", "Ashman", "Jill" ],
      "venue" : "Natl Health Stat Report.,",
      "citeRegEx" : "Hsiao et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hsiao et al\\.",
      "year" : 2014
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "Ioffe", "Sergey", "Szegedy", "Christian" ],
      "venue" : "arXiv preprint arXiv:1502.03167,",
      "citeRegEx" : "Ioffe et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ioffe et al\\.",
      "year" : 2015
    }, {
      "title" : "Relating mean blood glucose and glucose variability to the risk of multiple episodes of hypoglycaemia in type 1 diabetes",
      "author" : [ "ES Kilpatrick", "AS Rigby", "K Goode", "Atkin", "SL" ],
      "venue" : "Diabetologia,",
      "citeRegEx" : "Kilpatrick et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Kilpatrick et al\\.",
      "year" : 2007
    }, {
      "title" : "Normalized and differential convolution",
      "author" : [ "Knutsson", "Hans", "Westin", "Carl-Fredrik" ],
      "venue" : "In Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Knutsson et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Knutsson et al\\.",
      "year" : 1993
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing",
      "author" : [ "Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E" ],
      "venue" : null,
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Computational phenotype discovery using unsupervised feature learning over noisy, sparse, and irregular clinical data. volume 8, pp. e66341",
      "author" : [ "Lasko", "Thomas A", "Denny", "Joshua C", "Levy", "Mia A" ],
      "venue" : "Public Library of Science,",
      "citeRegEx" : "Lasko et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Lasko et al\\.",
      "year" : 2013
    }, {
      "title" : "Handwritten digit recognition with a back-propagation network",
      "author" : [ "Le Cun", "B Boser", "Denker", "John S", "D Henderson", "Howard", "Richard E", "W Hubbard", "Jackel", "Lawrence D" ],
      "venue" : "In Advances in neural information processing systems. Citeseer,",
      "citeRegEx" : "Cun et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "Cun et al\\.",
      "year" : 1990
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "LeCun", "Yann", "Bottou", "Léon", "Bengio", "Yoshua", "Haffner", "Patrick" ],
      "venue" : null,
      "citeRegEx" : "LeCun et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1998
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg S", "Dean", "Jeff" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Recurrent models of visual attention",
      "author" : [ "Mnih", "Volodymyr", "Heess", "Nicolas", "Graves", "Alex" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Mnih et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2014
    }, {
      "title" : "On estimating regression",
      "author" : [ "Nadaraya", "Elizbar A" ],
      "venue" : "Theory of Probability & Its Applications,",
      "citeRegEx" : "Nadaraya and A.,? \\Q1964\\E",
      "shortCiteRegEx" : "Nadaraya and A.",
      "year" : 1964
    }, {
      "title" : "Rectified linear units improve restricted boltzmann machines",
      "author" : [ "Nair", "Vinod", "Hinton", "Geoffrey E" ],
      "venue" : "In Proceedings of the 27th International Conference on Machine Learning",
      "citeRegEx" : "Nair et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Nair et al\\.",
      "year" : 2010
    }, {
      "title" : "Gaussian processes for machine learning",
      "author" : [ "Rasmussen", "Carl Edward" ],
      "venue" : null,
      "citeRegEx" : "Rasmussen and Edward.,? \\Q2006\\E",
      "shortCiteRegEx" : "Rasmussen and Edward.",
      "year" : 2006
    }, {
      "title" : "Variability of repeated serum prostate-specific antigen (psa) measurements within less than 90 days in a well-defined patient",
      "author" : [ "Roehrborn", "Claus G", "Pickens", "G John", "Carmody", "Thomas" ],
      "venue" : "population. Urology,",
      "citeRegEx" : "Roehrborn et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Roehrborn et al\\.",
      "year" : 1996
    }, {
      "title" : "Learning representations by back-propagating errors",
      "author" : [ "Rumelhart", "David E", "Hinton", "Geoffrey E", "Williams", "Ronald J" ],
      "venue" : null,
      "citeRegEx" : "Rumelhart et al\\.,? \\Q1988\\E",
      "shortCiteRegEx" : "Rumelhart et al\\.",
      "year" : 1988
    }, {
      "title" : "Deep convolutional neural networks for lvcsr",
      "author" : [ "Sainath", "Tara N", "Mohamed", "Abdel-rahman", "Kingsbury", "Brian", "Ramabhadran", "Bhuvana" ],
      "venue" : "In Acoustics, Speech and Signal Processing (ICASSP),",
      "citeRegEx" : "Sainath et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Sainath et al\\.",
      "year" : 2013
    }, {
      "title" : "A hilbert space embedding for distributions",
      "author" : [ "Smola", "Alex", "Gretton", "Arthur", "Song", "Le", "Schölkopf", "Bernhard" ],
      "venue" : "In Algorithmic Learning Theory,",
      "citeRegEx" : "Smola et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Smola et al\\.",
      "year" : 2007
    }, {
      "title" : "Kernel embeddings of latent tree graphical models",
      "author" : [ "Song", "Le", "Xing", "Eric P", "Parikh", "Ankur P" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Song et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2011
    }, {
      "title" : "Dropout: A simple way to prevent neural networks from overfitting",
      "author" : [ "Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan" ],
      "venue" : null,
      "citeRegEx" : "Srivastava et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2014
    }, {
      "title" : "The recurrent temporal restricted boltzmann machine",
      "author" : [ "Sutskever", "Ilya", "Hinton", "Geoffrey E", "Taylor", "Graham W" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Sutskever et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2009
    }, {
      "title" : "Generating text with recurrent neural networks",
      "author" : [ "Sutskever", "Ilya", "Martens", "James", "Hinton", "Geoffrey E" ],
      "venue" : "In Proceedings of the 28th International Conference on Machine Learning",
      "citeRegEx" : "Sutskever et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2011
    }, {
      "title" : "Learning generative models with visual attention",
      "author" : [ "Tang", "Yichuan", "Srivastava", "Nitish", "Salakhutdinov", "Ruslan R" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Tang et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2014
    }, {
      "title" : "Joint training of a convolutional network and a graphical model for human pose estimation",
      "author" : [ "Tompson", "Jonathan J", "Jain", "Arjun", "LeCun", "Yann", "Bregler", "Christoph" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Tompson et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Tompson et al\\.",
      "year" : 2014
    }, {
      "title" : "Smooth regression analysis",
      "author" : [ "Watson", "Geoffrey S" ],
      "venue" : "Sankhyā: The Indian Journal of Statistics,",
      "citeRegEx" : "Watson and S.,? \\Q1964\\E",
      "shortCiteRegEx" : "Watson and S.",
      "year" : 1964
    }, {
      "title" : "Gaussian process regression networks",
      "author" : [ "Wilson", "Andrew", "Ghahramani", "Zoubin", "Knowles", "David A" ],
      "venue" : "In Proceedings of the 29th International Conference on Machine Learning",
      "citeRegEx" : "Wilson et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Wilson et al\\.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 19,
      "context" : "Representation learning and unsupervised feature discovery via deep learning has led to ground breaking advances in domains such as image processing (Krizhevsky et al., 2012), speech recognition (Graves & Schmidhuber, 2005), natural language processing (Mikolov et al.",
      "startOffset" : 149,
      "endOffset" : 174
    }, {
      "referenceID" : 23,
      "context" : ", 2012), speech recognition (Graves & Schmidhuber, 2005), natural language processing (Mikolov et al., 2013), surpassing methods based on hand-engineered features in all benchmarks tested.",
      "startOffset" : 86,
      "endOffset" : 108
    }, {
      "referenceID" : 15,
      "context" : "Following recent availability of large electronic medical record datasets and other biological signals (Hsiao et al., 2014), discovery of early temporal disease signatures within lab values has become a possibility.",
      "startOffset" : 103,
      "endOffset" : 123
    }, {
      "referenceID" : 22,
      "context" : "This last characteristic has inspired us to train a temporal convolution model (Le Cun et al., 1990; LeCun et al., 1998; Tompson et al., 2014; Krizhevsky et al., 2012) to learn variation patterns of labs as biological representations of healthy and diseased states.",
      "startOffset" : 79,
      "endOffset" : 167
    }, {
      "referenceID" : 37,
      "context" : "This last characteristic has inspired us to train a temporal convolution model (Le Cun et al., 1990; LeCun et al., 1998; Tompson et al., 2014; Krizhevsky et al., 2012) to learn variation patterns of labs as biological representations of healthy and diseased states.",
      "startOffset" : 79,
      "endOffset" : 167
    }, {
      "referenceID" : 19,
      "context" : "This last characteristic has inspired us to train a temporal convolution model (Le Cun et al., 1990; LeCun et al., 1998; Tompson et al., 2014; Krizhevsky et al., 2012) to learn variation patterns of labs as biological representations of healthy and diseased states.",
      "startOffset" : 79,
      "endOffset" : 167
    }, {
      "referenceID" : 19,
      "context" : ", 2014; Krizhevsky et al., 2012) to learn variation patterns of labs as biological representations of healthy and diseased states. In the clinical domain, each biomarker varies with a different natural speed of change in the body. Therefore, in this paper we focus on multi-resolution deep convolutional architectures, inspired by Mnih et al. (2014).",
      "startOffset" : 8,
      "endOffset" : 350
    }, {
      "referenceID" : 17,
      "context" : "Additionally, medical community has actively studied the causal effect of variations on different signals, such as glucose (Kilpatrick et al., 2007), cholesterol (Bangalore et al.",
      "startOffset" : 123,
      "endOffset" : 148
    }, {
      "referenceID" : 4,
      "context" : ", 2007), cholesterol (Bangalore et al., 2015), blood pressure(Hata et al.",
      "startOffset" : 21,
      "endOffset" : 45
    }, {
      "referenceID" : 14,
      "context" : ", 2015), blood pressure(Hata et al., 2013), and prostate-specific antigen (Roehrborn et al.",
      "startOffset" : 23,
      "endOffset" : 42
    }, {
      "referenceID" : 28,
      "context" : ", 2013), and prostate-specific antigen (Roehrborn et al., 1996) on different disease onsets.",
      "startOffset" : 39,
      "endOffset" : 63
    }, {
      "referenceID" : 35,
      "context" : "Examples of such models include Gaussian processes and generative models based on recurrent neural networks (Sutskever et al., 2011; 2009; Tang et al., 2014; Chung et al., 2015).",
      "startOffset" : 108,
      "endOffset" : 177
    }, {
      "referenceID" : 36,
      "context" : "Examples of such models include Gaussian processes and generative models based on recurrent neural networks (Sutskever et al., 2011; 2009; Tang et al., 2014; Chung et al., 2015).",
      "startOffset" : 108,
      "endOffset" : 177
    }, {
      "referenceID" : 8,
      "context" : "Examples of such models include Gaussian processes and generative models based on recurrent neural networks (Sutskever et al., 2011; 2009; Tang et al., 2014; Chung et al., 2015).",
      "startOffset" : 108,
      "endOffset" : 177
    }, {
      "referenceID" : 33,
      "context" : "Each of the hidden layers are subject to Dropout(Srivastava et al., 2014) regularization (with probability 0.",
      "startOffset" : 48,
      "endOffset" : 73
    }, {
      "referenceID" : 20,
      "context" : "(Lasko et al., 2013) studied a method based on sparse auto-encoders to learn temporal variation features from 30-day uric acid observations, to distinguish between gout and leukemia.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 7,
      "context" : "(Che et al., 2015) developed a training which allows prior domain knowledge to regularize the deeper layers of feed-forward network, for the task of multiple disease classification when datasets are small.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 0,
      "context" : "Within the domain of temporal convolutional networks, (Abdel-Hamid et al., 2012; Sainath et al., 2013) were among the first to show significant gains in speech recognition tasks in large scale.",
      "startOffset" : 54,
      "endOffset" : 102
    }, {
      "referenceID" : 30,
      "context" : "Within the domain of temporal convolutional networks, (Abdel-Hamid et al., 2012; Sainath et al., 2013) were among the first to show significant gains in speech recognition tasks in large scale.",
      "startOffset" : 54,
      "endOffset" : 102
    }, {
      "referenceID" : 29,
      "context" : "Using back-propagation (Rumelhart et al., 1988), we then show how one can learn the entire form of the kernel function instead of cross validating within a limited set of parametric family (such as Gaussian or Laplace).",
      "startOffset" : 23,
      "endOffset" : 47
    }, {
      "referenceID" : 10,
      "context" : "In best case, attempts such as (Duvenaud et al., 2013), (Gönen & Alpaydın, 2011) learn a composition or combination of kernel families.",
      "startOffset" : 31,
      "endOffset" : 54
    }, {
      "referenceID" : 2,
      "context" : "The dominant approaches rely on Bayesian formalization and process convolution (Boyle & Frean, 2004; Alvarez et al., 2010; Alvarez & Lawrence, 2009), and require known dependency structure on the multiple outputs.",
      "startOffset" : 79,
      "endOffset" : 148
    }, {
      "referenceID" : 3,
      "context" : "The main problem with the models is that they are only scalable under the sparse structure assumptions (Alvarez et al., 2011; Byron et al., 2009).",
      "startOffset" : 103,
      "endOffset" : 145
    }, {
      "referenceID" : 6,
      "context" : "The main problem with the models is that they are only scalable under the sparse structure assumptions (Alvarez et al., 2011; Byron et al., 2009).",
      "startOffset" : 103,
      "endOffset" : 145
    }, {
      "referenceID" : 39,
      "context" : "One solution for unconstrained structure was proposed in (Wilson et al., 2012), however inference required Monte Carlo sampling or variational inference, which were inefficient.",
      "startOffset" : 57,
      "endOffset" : 78
    }, {
      "referenceID" : 11,
      "context" : "Alternatively, one could model the full joint distribution for a window of interest using a nonparametric graphical model (Fukumizu et al., 2007; Smola et al., 2007; Song et al., 2011).",
      "startOffset" : 122,
      "endOffset" : 184
    }, {
      "referenceID" : 31,
      "context" : "Alternatively, one could model the full joint distribution for a window of interest using a nonparametric graphical model (Fukumizu et al., 2007; Smola et al., 2007; Song et al., 2011).",
      "startOffset" : 122,
      "endOffset" : 184
    }, {
      "referenceID" : 32,
      "context" : "Alternatively, one could model the full joint distribution for a window of interest using a nonparametric graphical model (Fukumizu et al., 2007; Smola et al., 2007; Song et al., 2011).",
      "startOffset" : 122,
      "endOffset" : 184
    }, {
      "referenceID" : 33,
      "context" : "We also add one Dropout module (Srivastava et al., 2014) (0.",
      "startOffset" : 31,
      "endOffset" : 56
    }, {
      "referenceID" : 9,
      "context" : "We implemented the architecture using Torch (Collobert et al., 2011).",
      "startOffset" : 44,
      "endOffset" : 68
    } ],
    "year" : 2016,
    "abstractText" : "Early diagnosis of treatable diseases is essential for improving healthcare, and many diseases’ onsets are predictable from annual lab tests and their temporal trends. We introduce a multi-resolution convolutional neural network for early detection of multiple diseases from irregularly measured sparse lab values. Our novel architecture takes as input both an imputed version of the data and a binary observation matrix. For imputing the temporal sparse observations, we develop a flexible, fast to train method for differentiable multivariate kernel regression. Our experiments on data from 298K individuals over 8 years, 18 common lab measurements, and 171 diseases show that the temporal signatures learned via convolution are significantly more predictive than baselines commonly used for early disease diagnosis.",
    "creator" : "LaTeX with hyperref package"
  }
}