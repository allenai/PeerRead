{
  "name" : "1502.08030.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Author Name Disambiguation by Using Deep Neural Network",
    "authors" : [ "Hung Nghiep Tran", "Tin Huynh", "Tien Do", "Linh Trung Ward", "Thu Duc" ],
    "emails" : [ "tiendv}@uit.edu.vn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: Digital Library, Bibliographic Data, Author Name Disambiguation, Machine Learning, Feature Learning, Deep Neural Network."
    }, {
      "heading" : "1 Introduction",
      "text" : "Author name ambiguity is a problem that occurs when a set of publication records contains ambiguous author names, i.e., the same author may appear under distinct names (synonymy), or distinct authors may have similar names (polysemy) [5]. This problem decreases the quality and reliability of information retrieved from digital libraries such as the impact of authors, the impact of organizations, etc. Therefore, author name disambiguation is a critical task in digital libraries.\nThere are two approaches to author name disambiguation: (1) grouping publication records of a same author by finding some similarity among them (author grouping methods) or (2) directly assigning them to their respective authors (author assignment methods) [5]. Both of them try to create, select and combine features based on the similarity of attributes (author names, keywords, etc.) by using some measures such as Jaccard, Jaro, etc., or some heuristics. However, most of those works are done manually based on experts’ knowledge. Each predefined feature set could perform very well on a specific dataset that experts ar X\niv :1\n50 2.\n08 03\n0v 2\n[ cs\n.D L\n] 2\n9 Ju\nl 2 01\n7\noriginally dealt with, but it could perform poorly on other datasets. To solve this problem, a method to learn features automatically from data is necessary.\nNeural networks, which have many layers, are called deep neural networks (DNN). Recent researches [3, 10, 15] have shown their strong ability in feature learning in many tasks. Internal features learned by the DNN are relatively stable for variants in data if the training data are sufficiently representative [15]. This helps dealing with citation errors1, which is an open challenge pointed out by Ferreira et al. [5]. Moreover, using neural network has the advantage that it would build a general model. This model could disambiguate author name incrementally when new publication records are incorporated into the dataset.\nIn this paper, we propose a new approach which uses deep neural network to learn features automatically for solving author name ambiguity. Additionally, we propose the general system architecture for author name disambiguation on any dataset. This system computes a representative for a dataset, and then uses a combination of many DNNs to learn features and disambiguate author names.\nThe remainder of the paper is organized as follows. Section 2 briefly presents related researches on author name disambiguation and feature learning using DNN. Section 3 will describe the proposed method and system architecture. In section 4, we will present the experiments, evaluation results and discussions. Finally, we conclude the paper and suggest future works in section 5."
    }, {
      "heading" : "2 Related Work",
      "text" : "Ferreira et al. [5] did a brief survey of author name disambiguation methods. According to their survey, existing methods have tried to create, select and combine features based on the similarity of attributes by using some stringmatching measures or some specific heuristic, such as the number of coauthor names in common, etc.\nBhattacharya and Getoor [1] proposed a combined similarity function defined on attributes and relational information. The method obtained a high F1 score around 0.99 in the CiteSeer collection, lower in the arXiv collection and only around 0.81 in the BioBase collection.\nIn another research, Torvik et al. [14] used a feature set resulting from the comparison between the common citation attributes along with medical subject headings, language, and affiliation of two references in MEDLINE dataset. In a subsequent work [13], Torvik and Smalheiser incorporated some features into their method to achieve better result.\nIn our previous research [9], we predefined a feature set to learn a similarity function specifically for Vietnamese author dataset, one of the most difficult case, and obtained around 0.98 of accuracy.\nIn those researches, the central task is predefining a feature set for a specific dataset. A good feature set will help improving accuracy on a specific dataset, but it need to be recalibrated for a new dataset. In this research, we aim at learning features automatically.\n1 Citation errors: errors in citation data which are sometimes impossible to detect.\nDNN could be regarded as feedforward neural network with more than one layer [12]. Recently, many training and initialization schemes have been proposed in order to improve learning speed on such deep network, e.g., such as RBM [8], sparse auto-encoders [11], and normalized initialization [6]. Deep learning using DNN has been a popular method for automatic feature learning in many tasks.\nCiresan et al. [3] was very successful in using a big DNN to learn features in image recognition. They built a deep convolution neural network and trained such network by simple online back-propagation. Their models greatly outperformed previous methods on many well-known datasets such as MNIST 2, NORB 3, etc. without using complicated image pre-processing techniques.\nYu et al. [15] used a simple deep feedforward neural network to learn features in speech recognition. They proved the model’s ability to extract discriminative internal features that are robust to variants in data. Their model outperformed state-of-the-art systems based on GMMs or shallow networks without the need for explicit model adaptation or feature normalization.\nHowever, to the best of our knowledge, there has not been any research that attempts to learn feature automatically by using DNN for author name disambiguation. Therefore, in this research, we will explore that approach."
    }, {
      "heading" : "3 Our Approach",
      "text" : "In this section, we describe our proposed method and the general system architecture for author name disambiguation on any dataset. This method uses a combination of many DNNs to learn features from a data representative, which could be computed automatically for any dataset."
    }, {
      "heading" : "3.1 Deep Neural Network",
      "text" : "DNN is a popular method for automatic feature learning [12]. Some recent researches have successfully exploited feature learning using DNN to achieve stateof-the-art performance in many tasks [3, 10,15]. There are many types of DNN. All of them are neural networks with many layers, but they are different in parameter initialization scheme, training algorithm, activation function, etc.\nIn this research, we use DNN with simple feedfoward architecture, a.k.a., multi-layer perceptron [12]. Figure 1 shows the general architecture of such network. The network has many layers stacked upon each other. Each neuron unit in each layer connects to every unit in the sequential layer. The number of units in the input layer corresponds with the number of basic features we use. Output layer contains two units which correspond with the case where two citations belong to the same author and otherwise, respectively.\nIf we denote the input and the ideal output of the DNN as x and y, respectively, a DNN can be interpreted as a directed graphical model that approximates the posterior probability py|x(y = c|x) of a class c given an input x. 2 http://yann.lecun.com/exdb/mnist/ 3 http://www.cs.nyu.edu/~ylclab/data/norb-v1.0/\nWe consider a DNN with L− 1 hidden layers and 1 output layer, which is a stack of L layers of log-linear models. Each hidden layer l models the posterior probabilities of a hidden vector hl given the preceding layer’s output vl. If hl consists of N l hidden units, each denoted as hlj , the posterior probabilities, with sigmoid activation function for simplicity, can be expressed as\npl(hl|vl) = Nl∏ j=1\n1\n1 + e −zlj(v\nl) , 0 < l < L (1)\nwhere zl(vl) = W l · vl + al, and W l and al represent the weight matrix and bias vector in the l − th layer, respectively.\nInternal features will be learned in each hidden layer [15]. Each hidden unit’s output represents an internal feature and each hidden layer’s output composes an internal feature vector. Starting with the basic feature input v1 = h0 = x, the output of each layer becomes the input of the next one, i.e., vl+1 = hl. The latter layer will learn more sophisticated features.\nIn the final layer, the class posterior probabilities are computed as a multinomial distribution using softmax\npy|x(y = c|x) = pL(y = c|vL) = ez\nL c (v L)∑ c′ e zL c′ (v L) (2)\nThis type of DNN has the vanishing gradient issue when being trained with the traditional activation function sigmoid(x) = 1/(1 + e−x) and backpropagation algorithm. To address this issue, we use the activation function softsign(x) = x/(1 + |x|) and the adaptive resilient backpropagation algorithm together with some training techniques [6].\nDNN’s ability in a specific case is affected by the network’s structure and its parameters. Network parameters could be learned by training using some optimization algorithms. Network structure includes two hyperparamters: the number of hidden layers and the number of hidden units in each layer. The number of units should be equal among hidden layers so that information could flow effectively [6]. In general, deeper and larger network will achieve better results. However, this makes training slower and is capable to yield overfitting.\nThose two hyperparameters are usually chosen based on experiments on the validation set [3, 15]. In this research, we determine the optimum network size based on experiments using k-fold cross-validation. We begin with a small network, and then change the number of hidden layers and the number of hidden units respectively to create networks at larger sizes. The optimum network structure is the one with the highest average validation accuracy."
    }, {
      "heading" : "3.2 Data Representative",
      "text" : "As we have shown in the previous subsection, DNN could learn internal features from data. In order to do this, data must be put into the input layer in a proper way. The input should be a good representative for data, i.e., it could describe details in data. We call that data representative the basic feature set.\nThere are many different ways to compute a data representative. One obvious way is to measure the similarity between all attributes of two publication records such as Author name, Affiliations, Coauthor, and Paper keyword, etc. using string-matching measures. We assume that the similarity between those attributes expresses how much two publications belong to the same author.\nAccording to some surveys reviewing string-matching measures to identify the duplication [2, 4], there are three types of measure: (1) Edit distance such as Levenshtein, Monger-Elkan, Jaro, and Jaro-Winkler; (2) Token-based such as Jaccard and TF/IDF and (3) Hybrid measures such as Mogne-Elkan for comparing two long strings. We employ all three types of measure.\nEach publication record has different attributes. In general case, we could apply computations to all available attributes, and use default value when they are unavailable. Therefore, how we build the data representative does not depend on a specific dataset.\nIn this research, we use these measures: Jaccard, Levenshtein, Jaro, JaroWinkler, Smith-Waterman, Mogne-Elkan. We apply them for these attributes: author name, co-author, affiliation, paper keyword, author interest keyword."
    }, {
      "heading" : "3.3 System Architecture",
      "text" : "In this subsection, we describe the general system architecture for author name disambiguation. The system could run on any dataset without expert’s modifications. Figure 2 shows the proposed system architecture. The system incorporates two components.\nThe first component takes the data and computes a data representative, i.e., a basic feature set x to represent data. There could be many representatives for the same data, so this component could be implemented in many ways. In this research, we use string-matching measures to compute the data representative. The computations could be performed on any dataset automatically.\nThe second component takes the basic feature set as its input, and then learns features in its hidden layers to disambiguate author names. The last layer of DNN computes the probabilities py|x(y = c|x) = pL(y = c|vL) to determine\nwhether two author instance names in a pair belong to the same author (when p ≥ 0.5) or not (when p < 0.5).\nIn this system architecture, we use multi-column DNN technique, which is illustrated in figure 3, to improve the generalization capabilities of the system [3]. This technique is similar to an ensemble method known as bootstrap aggregating or bagging.\nSpecifically, we will train N DNNs simultaneously using data retrieved randomly from the training set in a manner similar to k-fold cross-validation. After training, we have N distinct DNNs. In testing phase, we will apply all those DNNs simultaneously to each item in a separate test set. Then, we will take the final result by averaging results from those DNNs as\nPy|x(y = c|x) = py|x(y = c|x) =\nN∑ n=1 pny|x(y = c|x)\nN (3)\nwhere pn is the output of n− th DNN."
    }, {
      "heading" : "4 Experiment, Evaluation and Discussion",
      "text" : "We evaluate the effect of using automatic feature learning by DNNs for the author name disambiguation problem on Vietnamese author dataset (very ambiguous cases [5]), which we collected from online digital libraries. This section presents our experiment settings, evaluation results, and discussions."
    }, {
      "heading" : "4.1 Dataset",
      "text" : "Vietnamese author dataset:\nIn a previous work [9], we built a Vietnamese author dataset for checking author name ambiguity. Data was acquired from three online digital libraries that are ACM 4, IEEE Xplore 5, and MAS 6 by querying their search engine using names of 10 Vietnamese authors. For these authors, there are many different instance names, e.g., author ‘Hoang Kiem’ can have many instance names such as ‘Hoang Kiem’, ‘Kiem Hoang’, ‘Hoang Van Kiem’, ‘Kiem Van Hoang’, etc.\nQuery results are publications with different author instance names. We built the dataset by creating pairs of publications for each author. Based on our understanding about these authors, we manually labeled each pair with value 1 if ambiguous names in this pair actually were one person and value 0 otherwise.\nIn this research, we extend the dataset, so that, there are totally 30537 samples in the dataset. Table 1 shows the dataset details, the total number of pairs is counted without duplicated ones.\nDataset preparation:\nFor each pair of publication records, we compute all basic features. So each pair of publication records is represented as a basic feature vector with label 1 or 0. We use this vector as the input for the DNN.\nFrom the original dataset, we hold out 20% of the data as the test set for final performance evaluation. We use 5-fold cross-validation on the remaining 80% of the data to tune hyperparameters and avoid overfitting. Each split dataset contains almost equal percentage of random samples of one particular class. Those samples are picked randomly at uniform distribution.\n4 http://dl.acm.org 5 http://ieeexplore.ieee.org/Xplore/home.jsp 6 http://academic.research.microsoft.com/"
    }, {
      "heading" : "4.2 Tuning Hyperparameters",
      "text" : "On Vietnamese author dataset, we experiment with 5-fold cross-validation to choose network size. We begin with the smallest network size of 1 hidden layer and 10 hidden units. We use this network as the baseline for hyperparameters tuning and achieve average validation accuracy of 95.35%.\nThen we increase the number of hidden layers and hidden units respectively and conduct experiments at many network sizes. Table 2 shows five network sizes (the number of hidden layers × the number of hidden units in each layer) with the highest accuracy. The network with 7 hidden layers and 50 hidden units in each layer achieves the highest average validation accuracy."
    }, {
      "heading" : "4.3 Evaluation",
      "text" : "In our recent research [9], we proposed an approach based on a predefined feature set for Vietnamese author name and applied several classification models to that feature set. According to that research, k-NN, Random Forest, C4.5, SVM, and Naive Bayes, respectively, are the best suitable methods for the predefined feature set.\nIn this research, we compare the proposed method with those methods. The proposed method implements the system architecture that we have described. The DNNs use the hyperparameters that have been tuned. The other methods use the same settings and implementations as in our previous research [9].\nTable 3 shows evaluation results in terms of accuracy and error on a separated test set. Results show that the proposed method significantly outperforms methods that use predefined feature set. The proposed method achieves 99.31% in terms of accuracy. Whereas, the best method that uses predefined feature set achieves 98.17% in terms of accuracy. Prediction error rate decreases from 1.83% to 0.69%, i.e., it decreases by 1.14%, or 62.3% relatively compared with other methods that use predefined feature set."
    }, {
      "heading" : "4.4 Discussion",
      "text" : "Evaluation results clearly show benefits of learning features compared with predefining features in terms of accuracy. Moreover, automatic feature learning does\nnot require expert’s knowledge on specific dataset. DNN has been used to learn features successfully. However, due to its high capability to learn complex features, it is prone to overfitting.\nIn this research, we use many techniques to reduce overfitting. We extend Vietnamese author dataset. We use k-fold cross-validation for hyperparameter tuning and early stopping. Moreover, our system architecture uses multi-column technique to have a lower variance result.\nThe type of DNN we use has the vanishing gradient issue when being trained with the traditional sigmoid activation function and back-propagation algorithm. One solution is choosing a good activation function [6]. The hyperbolic tangent function tanh(x) = (1− e−2x)/(1 + e−2x) is better than the traditional sigmoid function thanks to its zero mean. Whereas, the softsign function softsign(x) = x/(1 + |x|) is better than tanh(x) thanks to its smoother asymptotic behavior. The rectifier function max(x, 0) is one of the best activation functions [6, 7].\nThe current DNN model is supervised, therefore, it needs labeled data, which is usually not easy to obtain. However, there are some techniques to pre-train DNN using unlabeled data, which are special kind of weight initialization methods, to improve performance much in terms of training time and accuracy, especially in case lack of labeled data.\nThe proposed method is prospective when data is integrated from heterogeneous sources, because in such case, it is difficult to predefine a feature set.\nOn the other hand, automatic feature learning using DNN has shown its ability in many complex tasks such as image recognition, where this method could recognize object just by using raw pixels [3]. Therefore, it is rational to think of creating such ‘pixels’ in author name disambiguation by encoding bibliographic data and use those ‘pixels’ as the basic data representative in the DNN."
    }, {
      "heading" : "5 Conclusion and Future Work",
      "text" : "In this paper, we have proposed a new approach which uses deep neural network to learn features automatically for solving author name ambiguity. Additionally, we have proposed the general system architecture for author name disambiguation on any dataset.\nWe have evaluated the proposed method on a Vietnamese author dataset. The results show that this method significantly outperforms other methods that\nuse predefined feature set. The proposed method achieves 99.31% in terms of accuracy. Prediction error rate decreases from 1.83% to 0.69%, i.e., it decreases by 1.14%, or 62.3% relatively compared with other methods that use predefined feature set.\nThe proposed method could be extended to solve some open challenges such as the lack of labeled training data, incremental and new-author disambiguation.\nIn the future, we will benchmark the proposed method on other datasets. We will also experiment with other activation functions and unsupervised pretraining methods on encoded bibliographic data.\nAcknowledgments. This research is funded by University of Information Technology, VNU-HCMC under grant number C2012-07."
    } ],
    "references" : [ {
      "title" : "Collective entity resolution in relational data",
      "author" : [ "I. Bhattacharya", "L. Getoor" ],
      "venue" : "ACM Trans. Knowl. Discov. Data 1(1)",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Adaptive name matching in information integration",
      "author" : [ "M. Bilenko", "R. Mooney", "W. Cohen", "P. Ravikumar", "S. Fienberg" ],
      "venue" : "IEEE Intell. Sys. 18(5), 16–23",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Multi-column deep neural networks for image classification",
      "author" : [ "D.C. Ciresan", "U. Meier", "J. Schmidhuber" ],
      "venue" : "CVPR. pp. 3642–3649",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A comparison of string distance metrics for name-matching tasks",
      "author" : [ "W.W. Cohen", "P.D. Ravikumar", "S.E. Fienberg" ],
      "venue" : "IIWeb. pp. 73–78",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "A brief survey of automatic methods for author name disambiguation",
      "author" : [ "A.A. Ferreira", "M.A. Gonçalves", "A.H. Laender" ],
      "venue" : "SIGMOD Rec. 41(2), 15–26",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Understanding the difficulty of training deep feedforward neural networks",
      "author" : [ "X. Glorot", "Y. Bengio" ],
      "venue" : "JLMR - Proceedings Track 9, 249–256",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Deep sparse rectifier neural networks",
      "author" : [ "X. Glorot", "A. Bordes", "Y. Bengio" ],
      "venue" : "AISTATS. pp. 315–323",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "A fast learning algorithm for deep belief nets",
      "author" : [ "G.E. Hinton", "S. Osindero", "Y.W. Teh" ],
      "venue" : "Neural Comput. 18(7), 1527–1554",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Vietnamese author name disambiguation for integrating publications from heterogeneous sources",
      "author" : [ "T. Huynh", "K. Hoang", "T. Do", "D. Huynh" ],
      "venue" : "The 5th ACIIDS - Vol. Part I. pp. 226–235. ACIIDS’13, Springer, Berlin, Heidelberg",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "NIPS 25, pp. 1106–1114",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Sparse feature learning for deep belief networks",
      "author" : [ "M. Ranzato", "Y.L. Boureau", "Y. LeCun" ],
      "venue" : "NIPS",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Parallel distributed processing: explorations in the microstructure of cognition, vol",
      "author" : [ "D.E. Rumelhart", "G.E. Hinton", "R.J. Williams" ],
      "venue" : "1. chap. Learning internal representations by error propagation, pp. 318–362. MIT Press, Cambridge, MA, USA",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1986
    }, {
      "title" : "Author name disambiguation in medline",
      "author" : [ "V.I. Torvik", "N.R. Smalheiser" ],
      "venue" : "ACM Trans. Knowl. Discov. Data 3(3), 11:1–11:29",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A probabilistic similarity metric for medline records: A model for author name disambiguation: Research articles",
      "author" : [ "V.I. Torvik", "M. Weeber", "D.R. Swanson", "N.R. Smalheiser" ],
      "venue" : "J. Am. Soc. Inf. Sci. Technol. 56(2), 140–158",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Feature learning in deep neural networks - a study on speech recognition tasks",
      "author" : [ "D. Yu", "M.L. Seltzer", "J. Li", "J.T. Huang", "F. Seide" ],
      "venue" : "CoRR abs/1301.3605",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : ", the same author may appear under distinct names (synonymy), or distinct authors may have similar names (polysemy) [5].",
      "startOffset" : 116,
      "endOffset" : 119
    }, {
      "referenceID" : 4,
      "context" : "There are two approaches to author name disambiguation: (1) grouping publication records of a same author by finding some similarity among them (author grouping methods) or (2) directly assigning them to their respective authors (author assignment methods) [5].",
      "startOffset" : 257,
      "endOffset" : 260
    }, {
      "referenceID" : 2,
      "context" : "Recent researches [3, 10, 15] have shown their strong ability in feature learning in many tasks.",
      "startOffset" : 18,
      "endOffset" : 29
    }, {
      "referenceID" : 9,
      "context" : "Recent researches [3, 10, 15] have shown their strong ability in feature learning in many tasks.",
      "startOffset" : 18,
      "endOffset" : 29
    }, {
      "referenceID" : 14,
      "context" : "Recent researches [3, 10, 15] have shown their strong ability in feature learning in many tasks.",
      "startOffset" : 18,
      "endOffset" : 29
    }, {
      "referenceID" : 14,
      "context" : "Internal features learned by the DNN are relatively stable for variants in data if the training data are sufficiently representative [15].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 4,
      "context" : "[5].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[5] did a brief survey of author name disambiguation methods.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 0,
      "context" : "Bhattacharya and Getoor [1] proposed a combined similarity function defined on attributes and relational information.",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 13,
      "context" : "[14] used a feature set resulting from the comparison between the common citation attributes along with medical subject headings, language, and affiliation of two references in MEDLINE dataset.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "In a subsequent work [13], Torvik and Smalheiser incorporated some features into their method to achieve better result.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 8,
      "context" : "In our previous research [9], we predefined a feature set to learn a similarity function specifically for Vietnamese author dataset, one of the most difficult case, and obtained around 0.",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 11,
      "context" : "DNN could be regarded as feedforward neural network with more than one layer [12].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 7,
      "context" : ", such as RBM [8], sparse auto-encoders [11], and normalized initialization [6].",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 10,
      "context" : ", such as RBM [8], sparse auto-encoders [11], and normalized initialization [6].",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 5,
      "context" : ", such as RBM [8], sparse auto-encoders [11], and normalized initialization [6].",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 2,
      "context" : "[3] was very successful in using a big DNN to learn features in image recognition.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 14,
      "context" : "[15] used a simple deep feedforward neural network to learn features in speech recognition.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "DNN is a popular method for automatic feature learning [12].",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 2,
      "context" : "Some recent researches have successfully exploited feature learning using DNN to achieve stateof-the-art performance in many tasks [3, 10,15].",
      "startOffset" : 131,
      "endOffset" : 141
    }, {
      "referenceID" : 9,
      "context" : "Some recent researches have successfully exploited feature learning using DNN to achieve stateof-the-art performance in many tasks [3, 10,15].",
      "startOffset" : 131,
      "endOffset" : 141
    }, {
      "referenceID" : 14,
      "context" : "Some recent researches have successfully exploited feature learning using DNN to achieve stateof-the-art performance in many tasks [3, 10,15].",
      "startOffset" : 131,
      "endOffset" : 141
    }, {
      "referenceID" : 11,
      "context" : ", multi-layer perceptron [12].",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 14,
      "context" : "Internal features will be learned in each hidden layer [15].",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 5,
      "context" : "To address this issue, we use the activation function softsign(x) = x/(1 + |x|) and the adaptive resilient backpropagation algorithm together with some training techniques [6].",
      "startOffset" : 172,
      "endOffset" : 175
    }, {
      "referenceID" : 5,
      "context" : "The number of units should be equal among hidden layers so that information could flow effectively [6].",
      "startOffset" : 99,
      "endOffset" : 102
    }, {
      "referenceID" : 2,
      "context" : "Those two hyperparameters are usually chosen based on experiments on the validation set [3, 15].",
      "startOffset" : 88,
      "endOffset" : 95
    }, {
      "referenceID" : 14,
      "context" : "Those two hyperparameters are usually chosen based on experiments on the validation set [3, 15].",
      "startOffset" : 88,
      "endOffset" : 95
    }, {
      "referenceID" : 1,
      "context" : "According to some surveys reviewing string-matching measures to identify the duplication [2, 4], there are three types of measure: (1) Edit distance such as Levenshtein, Monger-Elkan, Jaro, and Jaro-Winkler; (2) Token-based such as Jaccard and TF/IDF and (3) Hybrid measures such as Mogne-Elkan for comparing two long strings.",
      "startOffset" : 89,
      "endOffset" : 95
    }, {
      "referenceID" : 3,
      "context" : "According to some surveys reviewing string-matching measures to identify the duplication [2, 4], there are three types of measure: (1) Edit distance such as Levenshtein, Monger-Elkan, Jaro, and Jaro-Winkler; (2) Token-based such as Jaccard and TF/IDF and (3) Hybrid measures such as Mogne-Elkan for comparing two long strings.",
      "startOffset" : 89,
      "endOffset" : 95
    }, {
      "referenceID" : 2,
      "context" : "In this system architecture, we use multi-column DNN technique, which is illustrated in figure 3, to improve the generalization capabilities of the system [3].",
      "startOffset" : 155,
      "endOffset" : 158
    }, {
      "referenceID" : 4,
      "context" : "We evaluate the effect of using automatic feature learning by DNNs for the author name disambiguation problem on Vietnamese author dataset (very ambiguous cases [5]), which we collected from online digital libraries.",
      "startOffset" : 161,
      "endOffset" : 164
    }, {
      "referenceID" : 8,
      "context" : "In a previous work [9], we built a Vietnamese author dataset for checking author name ambiguity.",
      "startOffset" : 19,
      "endOffset" : 22
    }, {
      "referenceID" : 8,
      "context" : "In our recent research [9], we proposed an approach based on a predefined feature set for Vietnamese author name and applied several classification models to that feature set.",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 8,
      "context" : "The other methods use the same settings and implementations as in our previous research [9].",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 5,
      "context" : "One solution is choosing a good activation function [6].",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 5,
      "context" : "The rectifier function max(x, 0) is one of the best activation functions [6, 7].",
      "startOffset" : 73,
      "endOffset" : 79
    }, {
      "referenceID" : 6,
      "context" : "The rectifier function max(x, 0) is one of the best activation functions [6, 7].",
      "startOffset" : 73,
      "endOffset" : 79
    }, {
      "referenceID" : 2,
      "context" : "On the other hand, automatic feature learning using DNN has shown its ability in many complex tasks such as image recognition, where this method could recognize object just by using raw pixels [3].",
      "startOffset" : 193,
      "endOffset" : 196
    } ],
    "year" : 2017,
    "abstractText" : "Author name ambiguity is one of the problems that decrease the quality and reliability of information retrieved from digital libraries. Existing methods have tried to solve this problem by predefining a feature set based on expert’s knowledge for a specific dataset. In this paper, we propose a new approach which uses deep neural network to learn features automatically for solving author name ambiguity. Additionally, we propose the general system architecture for author name disambiguation on any dataset. We evaluate the proposed method on a dataset containing Vietnamese author names. The results show that this method significantly outperforms other methods that use predefined feature set. The proposed method achieves 99.31% in terms of accuracy. Prediction error rate decreases from 1.83% to 0.69%, i.e., it decreases by 1.14%, or 62.3% relatively compared with other methods that use predefined feature set (Table 3).",
    "creator" : "Hung Nghiep Tran"
  }
}