{
  "name" : "1505.02581.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Improving neural networks with bunches of neurons modeled by Kumaraswamy units: Preliminary study",
    "authors" : [ "Jakub M. Tomczak" ],
    "emails" : [ "JAKUB.TOMCZAK@PWR.EDU.PL" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Deep neural networks are quickly becoming a crucial element of high performance systems in many domains (Bengio et al., 2013), e.g., speech recognition, object recognition, natural language processing, multi-task and domain adaptation. Typical neural networks are based on sigmoid hidden units (Bengio, 2009), however, they can suffer from the vanishing gradient problem (Bengio et al., 1994). The issue may arise when lower layers of a neural network have gradients nearly 0 because higher layers are mostly saturated at 0 or 1. The vanishing gradients may drastically slow down the optimization procedure and eventually may lead to a poor local minimum.\nIn order to overcome issues associated with the sigmoid\nCopyright 2015 by the author.\nnon-linearity it is advocated to utilize other types of hidden units. Recently, deep neural networks with rectified linear units (ReLU) have seen success in different applications, e.g., signal processing (Zeiler et al., 2013), sentiment analysis (Glorot et al., 2011), object recognition (Jarrett et al., 2009), image analysis (Nair & Hinton, 2010). It has been shown that piece-wise linear units, such as ReLU, can compute highly complex and structured functions (Montufar et al., 2014). The practical success and theoretical results on ReLU have indicated a new direction for research. In (Maas et al., 2013) a leaky version of ReLU was proposed. The empirical evaluation on speech recognition task has shown slight improvement in comparison to sigmoid unit and ReLU. Further investigations with parametrized leaky ReLU (called Parametric ReLU) in (He et al., 2015) confirmed the presumption that simple ReLU may be to restrictive to learn fully successful representation. Recently, Agostinelli et al. (2015) went even further and proposed Adaptive Piecewise Linear Units (APLU), ReLU with a piecewise linear part for negative values. In the experiments it was shown that APLU can lead to significant performance increase.\nIn this work, we propose to improve neural networks by modeling neurons in a new manner. Our idea is to replicate a neuron with the same weights and biases in order to increase the robustness of learning a pattern. Moreover, we take into account the complex structure of single neuron which is represented by an additional parameter. A suitable fashion of modeling such bunch of neurons is application of a generalized Kumaraswamy distribution (KUM-G) (Cordeiro & de Castro, 2011; Nadarajah et al., 2012). The Kumaraswamy distribution (KUM) can be seen as an alternative distribution to the Beta distribution (Jones, 2009) and KUM-G determines a new class of distribution for given base probability measure. In our case, assuming single neuron in the bunch of neurons is modeled by a sigmoid function, we obtain novel non-linear activation function which we refer to as Kumaraswamy unit. For properly chosen parameters of KUM-G, the Kumaraswamy unit behaves similarly to ReLU.\nThe contribution of the paper is the following: i) we\nar X\niv :1\n50 5.\n02 58\n1v 1\n[ cs\n.L G\n] 1\n1 M\nay 2\n01 5\nintroduce an original non-linear activation function (Kumaraswamy unit) which follows from modeling a bunch of copies of the same neuron using the generalized Kumaraswamy distribution, ii) we provide close relationship between the Kumaraswamy unit and ReLU, iii) we provide an empirical evaluation of a single-layer neural network with the proposed hidden unit applied to MNIST dataset."
    }, {
      "heading" : "2. Modeling bunch of neurons: Kumaraswamy unit",
      "text" : "Preliminaries Let us focus on conventional feed-forward neural network with an input (visibles) v ∈ [0, 1]D×1 and an output y ∈ {0, 1}K×1 such that ∑ k yk = 1. In general, the network consists of L hidden layers, however, we restrict our considerations to one hidden layer for clarity. Therefore, the parameters of the network are θ = {c,d,W,U}, where c ∈ RM×1 and d ∈ RK×1 denote hidden and output biases, respectively, and W ∈ RM×D are input-to-hidden weights, and U ∈ RK×M are hiddento-output weights. The output of the network is modeled by the softmax unit:\np(yk = 1|v,θ) = exp(Uk·f(v; c,W) + dk)∑ l exp(Ul·f(v; c,W) + dl)\n(1)\nwhere Ui· denotes i-th row of the matrix U, and f(v; c,W) is the M -dimensional output of the hidden layer.\nThe activity of hidden units is modeled by some elementwise non-linear function f(·) . Typical activation function is the sigmoid function:\nσ(x) = 1\n1 + exp(−x) . (2)\nRecently, several alternatives to the sigmoid function have been used in numerous applications, such as, rectified linear unit (ReLU) (Jarrett et al., 2009):\nr(x) = max{0, x}, (3)\nor noisy rectified linear unit (Noisy ReLU) (Nair & Hinton, 2010):\nn(x) = max{0, x+N (0, v)}, (4)\nwhereN (·, ·) is a normal probability density function with zero mean and variance v.\nBunch of neurons Let us assume that activation of a neuron is modeled by a sigmoid unit. Moreover, let us presume that each neuron consists of a independent elements and there are b independent copies of the same neuron. Therefore, instead of single neuron we consider a bunch of neurons that try to reflect one pattern. A similar idea with replicas of a neuron was introduced in (Teh & Hinton, 2001)\nwhere the replication of sigmoid hidden units with the same weights and biases led to binomial units. Further, it turned out that binomial units with fixed offset to biases resulted in softplus units and its fast approximation, i.e., Noisy ReLU (Nair & Hinton, 2010). However, in our approach we copy the sigmoid hidden unit b times and additionally we introduce a second parameter, a, which corresponds to modeling complexity of the neuron itself. Increasing the value of b results in higher robustness of the bunch of neurons because it is less probable that the input signal will not activate any hidden unit in the bunch. On the other hand, increasing the value of a leads to higher failure probability of the neuron activation because it suffices that at least one element fails to deactivate the whole neuron.\nIt turns out that a suitable manner of modeling a bunch of neurons as described above is a generalized Kumaraswamy distribution (Cordeiro & de Castro, 2011; Nadarajah et al., 2012). The generalized Kumaraswamy distribution (KUMG) for given base distribution G(x) with a probability density function g(x) is defined as follows:\nKG(x|a, b) = 1− (1−G(x)a)b, (5)\nwhere a > 0 and b > 0 are shape parameters. The probability density function of KG(x|a, b) has a simple form:\nkG(x|a, b) = a b g(x)G(x)a−1(1−G(x)a)b−1. (6)\nFor integer-valued shape parameters a and b KUM-G has a nice interpretation of a system which consists of b independent components and each component is made up of a independent subcomponents. KUM-G perfectly fits to modeling chosen property of a complex system, such as, lifetime of an entire system (Nadarajah et al., 2012). We can clearly apply KUM-G to represent the bunch of neurons.\nKumaraswamy unit Assuming that single neuron activates according to the sigmoid function, we can take advantage of KUM-G to model the bunch of neurons which yields a new kind of non-linear activation function:\nKσ(x|a, b) = 1− (1− σ(x)a)b. (7)\nWe refer this resulting unit to as Kumaraswamy unit (Kumaraswamy(a, b)). Obviously, for a = b = 1 one recovers the sigmoid activation function. In the context of gradient-based learning algorithm it is important to compute a derivative of a hidden unit. Since the derivative of the sigmoid function can be easily calculated, the derivative of the Kumaraswamy unit can be obtained immediately (see Equation 6).\nAn intriguing property of the Kumaraswamy unit is that for properly chosen values of a and b it can behave like ReLU (see Figure 1 for comparison of sigmoid unit, ReLU, Noisy ReLu and the Kumaraswamy unit). We consider only two pairs of values of shape parameters, namely, (a, b) ∈ {(5, 6), (8, 30)}. The Kumaraswamy unit with a = 5 and b = 6 is the closest1 approximation of ReLU for value 0.5, while the second pair of values gives the closest approximation of ReLU in points 0.25 and 0.75. As we can evidently notice in Figure 1, the Kumaraswamy unit can behave similarly to ReLU but it returns values between 0 and 1 like the sigmoid function. We argue that such behavior may be crucial in training a neural network and is more biologically plausible.\nTraining Learning the parameters of the network θ for given dataD = {(vn, tn)}Nn=1 is performed by minimizing the cross-entropy loss. In the case of the neural network with output given by the softmax unit, the cross-entropy loss is equivalent to the negative conditional log-likelihood function:\n`(θ) = − ∑ n ∑ k tkn log p(ykn|vn,θ). (8)"
    }, {
      "heading" : "3. Experiments",
      "text" : "Goal In the experiment we aim at answering the following question:\n1In the Euclidean sense.\n• Is the Kumaraswamy unit preferable to sigmoid unit, ReLU or Noisy ReLU in training a single-layer neural network using stochastic gradient descent?\nWe want to point out that we try to verify only the impact of the proposed non-linearity on learning the neural network. We believe that positive answer to the stated question will give us a good starting point for further experiments with multi-layered neural networks, unsupervised pre-training and more sophisticated training techniques.\nData In the experiment we deal with the well-known MNIST dataset2 for hand-written digit classification. We split the dataset to 50,000 training images, 10,000 objects for validation, and 10,000 test examples. Each image consists of 28× 28 pixels (D = 784), and is labeled as one of ten possible digits (K = 10).\nLearning methodology In the experiment we focus on a single-layer neural network with 500 hidden units (M = 500). We train the model using stochastic gradient descent with momentum and a mini-batch size of 100 examples. The initial value of the momentum term is set according to the model selection with possible values in {0, 0.5}, and after 50 epochs it is set to 0.9. The learning rate is determined in the model selection procedure with possible values in {0.001, 0.01, 0.1}. During learning process, we apply the learning step policy in which the learning step is divided by 2 after each 10 epochs. Additionally, we add a weight decay to the learning objective and we explore the following values of the regularization coefficient: {0, 10−5, 10−4}. The maximum number of epochs is set to 100. The number of iterations over the training set is determined using early stopping according to the validation classification error, with a look ahead of 10 epochs. For all activation units the same initialization of the parameters is applied.\nEvaluation methodology In order to verify whether the Kumaraswamy unit is preferable to the sigmoid unit, ReLU or Noisy ReLU, we use two evaluation metrics, namely, the test classification error (Error) and the test crossentropy loss (Cross-Entropy).3 Additionally, we measure the mean activity of hidden units.\nResults The results for the considered activation nonlinear activation functions are gathered in Table 1. A single run of a training procedure for the considered units is presented in Figure 2. Moreover, in order to get better insight into the trained representation by the considered nonlinearities input-to-hidden weights are depicted in Figure 3\n2http://yann.lecun.com/exdb/mnist/ 3In the experiment we report the test cross-entropy loss di-\nvided by the number of test examples.\nand the mean activities of hidden units are demonstrated in Figure 4.\nDiscussion According to the results (see Table 1) we can conclude that the Kumaraswamy unit indeed tends to be preferable in comparison to sigmoid unit, ReLU and Noisy ReLU. The Kumaraswamy unit obtained the best results in terms of the test classification error and the test cross-entropy loss. The comparison of the two considered possible values of the scale parameters reveals that the Kumaraswamy(8, 30) performs better than the Kumaraswamy(5, 6), i.e., the bunches of 30 neurons with 8 elements seem to be more suitable in training a neural network.\nIt is a well-known fact that application of ReLU can cause saturation of some hidden units. However, we notice that the saturation of several hidden units resulted in faster convergence of ReLU and Noisy ReLU in comparison to other activation non-linearities (see Figure 2). For instance, ReLU obtained the minimum after 18 epochs while the sigmoid neural network converged after 65 iterations. It is worth noticing that the Kumaraswamy unit obtained the the best result (better by about 3% in comparison to others) just after 10 epochs.\nIn Figure 3 the learned features (input-to-hidden weights) for the considered types of hidden units are depicted. The features learned by the sigmoid neural network represent different patterns but some of them seem to be redundant, e.g., there are many patterns which look like diagonal stroke. On the contrary, ReLU and Noisy ReLU allow to obtain more diverse features. However, there are many neurons which are saturated, but in the case of Noisy ReLU this effect is less evident. Slightly different features in shape are obtained by Kumaraswamy units. These are more similar to the ones learned by ReLU, nevertheless, they are not saturated or degenerated as in the case of sigmoid units.\nNext, we investigate the impact of different non-linearities on activity of hidden units. The histograms of mean activity of neurons measured on the test set given in Figure 4 indicate that on average the ReLU, Noisy ReLU\nand Kumaraswamy units lead to larger number of neurons with activation smaller than 0.01 in comparison to the sigmoid units, namely, ReLU: 150, Noisy ReLU: 19, Kumaraswamy(5, 6): 7, Kumaraswamy(8, 30): 8, sigmoid: 0. However, Kumaraswamy unit has two advantages over ReLU and Noisy ReLU. First, there are less saturated neurons. Second, all Kumaraswamy units have mean activity less than 0.5 while in the case of ReLU and Noisy ReLU there are some units with very strong activation, i.e., above 0.5 or even larger than 1."
    }, {
      "heading" : "4. Conclusion",
      "text" : "In this work we introduced a new idea of improving neural networks with bunch of neurons, i.e., replicas of the same neurons which consist of independent elements. The bunch of neurons can be easily modeled by the generalized Kumaraswamy distribution which resulted in a formulation of new non-linear activation function we refer to as Kumaraswamy unit. A nice property of the Kumaraswamy unit is that for properly chosen parameters it can be approximately shaped as ReLU and it returns values between 0 and 1. In the experiment the performance of the neural network with the Kumaraswamy unit was compared with other activation functions, namely, sigmoid unit, ReLU and Noisy ReLU. The obtained results on MNIST seem to confirm the supremacy of the Kumaraswamy unit, nonetheless, this statement needs to be confirmed with more thorough studies. We believe that the performed experiment gives a good starting point for further research on the Kumaraswamy units applied to deep neural networks."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The research conducted by the author has been partially cofinanced by the Ministry of Science and Higher Education, Republic of Poland, grant No. B40020/I32."
    } ],
    "references" : [ {
      "title" : "Learning activation functions to improve deep neural networks",
      "author" : [ "Agostinelli", "Forest", "Hoffman", "Matthew", "Sadowski", "Peter", "Baldi", "Pierre" ],
      "venue" : "arXiv preprint arXiv:1412.6830,",
      "citeRegEx" : "Agostinelli et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Agostinelli et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning Deep Architectures for AI",
      "author" : [ "Bengio", "Yoshua" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "Bengio and Yoshua.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bengio and Yoshua.",
      "year" : 2009
    }, {
      "title" : "Learning long-term dependencies with gradient descent is difficult",
      "author" : [ "Bengio", "Yoshua", "Simard", "Patrice", "Frasconi", "Paolo" ],
      "venue" : "Neural Networks, IEEE Transactions on,",
      "citeRegEx" : "Bengio et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 1994
    }, {
      "title" : "Representation learning: A review and new perspectives",
      "author" : [ "Bengio", "Yoshua", "Courville", "Aaron", "Vincent", "Pascal" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2013
    }, {
      "title" : "A new family of generalized distributions",
      "author" : [ "Cordeiro", "Gauss M", "de Castro", "Mário" ],
      "venue" : "Journal of Statistical Computation and Simulation,",
      "citeRegEx" : "Cordeiro et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Cordeiro et al\\.",
      "year" : 2011
    }, {
      "title" : "Deep sparse rectifier networks",
      "author" : [ "Glorot", "Xavier", "Bordes", "Antoine", "Bengio", "Yoshua" ],
      "venue" : "In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics. JMLR W&CP Volume,",
      "citeRegEx" : "Glorot et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Glorot et al\\.",
      "year" : 2011
    }, {
      "title" : "Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification",
      "author" : [ "He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian" ],
      "venue" : "arXiv preprint arXiv:1502.01852,",
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "What is the best multi-stage architecture for object recognition",
      "author" : [ "Jarrett", "Kevin", "Kavukcuoglu", "Koray", "M Ranzato", "LeCun", "Yann" ],
      "venue" : "In Computer Vision,",
      "citeRegEx" : "Jarrett et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Jarrett et al\\.",
      "year" : 2009
    }, {
      "title" : "Kumaraswamys distribution: A beta-type distribution with some tractability advantages",
      "author" : [ "Jones", "MC" ],
      "venue" : "Statistical Methodology,",
      "citeRegEx" : "Jones and MC.,? \\Q2009\\E",
      "shortCiteRegEx" : "Jones and MC.",
      "year" : 2009
    }, {
      "title" : "Rectifier nonlinearities improve neural network acoustic models",
      "author" : [ "Maas", "Andrew L", "Hannun", "Awni Y", "Ng", "Andrew Y" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Maas et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Maas et al\\.",
      "year" : 2013
    }, {
      "title" : "On the number of linear regions of deep neural networks",
      "author" : [ "Montufar", "Guido F", "Pascanu", "Razvan", "Cho", "Kyunghyun", "Bengio", "Yoshua" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Montufar et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Montufar et al\\.",
      "year" : 2014
    }, {
      "title" : "General results for the kumaraswamy-g distribution",
      "author" : [ "Nadarajah", "Saralees", "Cordeiro", "Gauss M", "Ortega", "Edwin MM" ],
      "venue" : "Journal of Statistical Computation and Simulation,",
      "citeRegEx" : "Nadarajah et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Nadarajah et al\\.",
      "year" : 2012
    }, {
      "title" : "Rectified linear units improve restricted boltzmann machines",
      "author" : [ "Nair", "Vinod", "Hinton", "Geoffrey E" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Nair et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Nair et al\\.",
      "year" : 2010
    }, {
      "title" : "Rate-coded restricted boltzmann machines for face recognition",
      "author" : [ "Teh", "Yee Whye", "Hinton", "Geoffrey E" ],
      "venue" : "Advances in neural information processing systems,",
      "citeRegEx" : "Teh et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Teh et al\\.",
      "year" : 2001
    }, {
      "title" : "On rectified linear units for speech processing",
      "author" : [ "Zeiler", "Matthew D", "M Ranzato", "Monga", "Rajat", "M Mao", "K Yang", "Le", "Quoc Viet", "Nguyen", "Patrick", "A Senior", "Vanhoucke", "Vincent", "Dean", "Jeffrey", "Hinton", "Geoffrey E" ],
      "venue" : "In ICASSP,",
      "citeRegEx" : "Zeiler et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Zeiler et al\\.",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Deep neural networks are quickly becoming a crucial element of high performance systems in many domains (Bengio et al., 2013), e.",
      "startOffset" : 104,
      "endOffset" : 125
    }, {
      "referenceID" : 2,
      "context" : "Typical neural networks are based on sigmoid hidden units (Bengio, 2009), however, they can suffer from the vanishing gradient problem (Bengio et al., 1994).",
      "startOffset" : 135,
      "endOffset" : 156
    }, {
      "referenceID" : 14,
      "context" : ", signal processing (Zeiler et al., 2013), sentiment analysis (Glorot et al.",
      "startOffset" : 20,
      "endOffset" : 41
    }, {
      "referenceID" : 5,
      "context" : ", 2013), sentiment analysis (Glorot et al., 2011), object recognition (Jarrett et al.",
      "startOffset" : 28,
      "endOffset" : 49
    }, {
      "referenceID" : 7,
      "context" : ", 2011), object recognition (Jarrett et al., 2009), image analysis (Nair & Hinton, 2010).",
      "startOffset" : 28,
      "endOffset" : 50
    }, {
      "referenceID" : 10,
      "context" : "It has been shown that piece-wise linear units, such as ReLU, can compute highly complex and structured functions (Montufar et al., 2014).",
      "startOffset" : 114,
      "endOffset" : 137
    }, {
      "referenceID" : 9,
      "context" : "In (Maas et al., 2013) a leaky version of ReLU was proposed.",
      "startOffset" : 3,
      "endOffset" : 22
    }, {
      "referenceID" : 6,
      "context" : "Further investigations with parametrized leaky ReLU (called Parametric ReLU) in (He et al., 2015) confirmed the presumption that simple ReLU may be to restrictive to learn fully successful representation.",
      "startOffset" : 80,
      "endOffset" : 97
    }, {
      "referenceID" : 0,
      "context" : "Recently, Agostinelli et al. (2015) went even further and proposed Adaptive Piecewise Linear Units (APLU), ReLU with a piecewise linear part for negative values.",
      "startOffset" : 10,
      "endOffset" : 36
    }, {
      "referenceID" : 11,
      "context" : "A suitable fashion of modeling such bunch of neurons is application of a generalized Kumaraswamy distribution (KUM-G) (Cordeiro & de Castro, 2011; Nadarajah et al., 2012).",
      "startOffset" : 118,
      "endOffset" : 170
    }, {
      "referenceID" : 7,
      "context" : "Recently, several alternatives to the sigmoid function have been used in numerous applications, such as, rectified linear unit (ReLU) (Jarrett et al., 2009):",
      "startOffset" : 134,
      "endOffset" : 156
    }, {
      "referenceID" : 11,
      "context" : "It turns out that a suitable manner of modeling a bunch of neurons as described above is a generalized Kumaraswamy distribution (Cordeiro & de Castro, 2011; Nadarajah et al., 2012).",
      "startOffset" : 128,
      "endOffset" : 180
    }, {
      "referenceID" : 11,
      "context" : "KUM-G perfectly fits to modeling chosen property of a complex system, such as, lifetime of an entire system (Nadarajah et al., 2012).",
      "startOffset" : 108,
      "endOffset" : 132
    } ],
    "year" : 2015,
    "abstractText" : "Deep neural networks have recently achieved state-of-the-art results in many machine learning problems, e.g., speech recognition or object recognition. Hitherto, work on rectified linear units (ReLU) provides empirical and theoretical evidence on performance increase of neural networks comparing to typically used sigmoid activation function. In this paper, we investigate a new manner of improving neural networks by introducing a bunch of copies of the same neuron modeled by the generalized Kumaraswamy distribution. As a result, we propose novel nonlinear activation function which we refer to as Kumaraswamy unit which is closely related to ReLU. In the experimental study with MNIST image corpora we evaluate the Kumaraswamy unit applied to single-layer (shallow) neural network and report a significant drop in test classification error and test cross-entropy in comparison to sigmoid unit, ReLU and Noisy ReLU.",
    "creator" : "LaTeX with hyperref package"
  }
}