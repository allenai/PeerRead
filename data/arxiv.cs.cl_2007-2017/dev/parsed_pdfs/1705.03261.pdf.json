{
  "name" : "1705.03261.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Drug-drug Interaction Extraction via Recurrent Neural Network with Multiple Attention Layers",
    "authors" : [ "Zibo Yi", "Shasha Li", "Jie Yu", "Qingbo Wu" ],
    "emails" : [ "qingbo.wu}@nudt.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "I. INTRODUCTION\nDrug-drug interaction (DDI) is a situation when one drug increases or decreases the effect of another drug [1]. Adverse drug reactions may cause severe side effect, if two or more medicines were taken and their DDI were not investigated in detail. DDI is a common cause of illness, even a cause of death [2]. Thus, DDI databases for clinical medication decisions are proposed by some researchers. These databases such as SFINX [3], KEGG [4], CredibleMeds [5] help physicians and pharmacists avoid most adverse drug reactions.\nTraditional DDI databases are manually constructed by medical experts from clinical records, scientific research and drug specifications. For instance, The sentence “With combined use, clinicians should be aware, when phenytoin is added, of the potential for reexacerbation of pulmonary symptomatology due to lowered serum theophylline concentrations [6]”, which is from a pharmacotherapy report, describe the side effect of phenytoin and theophylline’s combined use. Then this information on specific medicines will be added to DDI databases. As drug-drug interactions have being increasingly found, manually constructing DDI database would consume a lot of manpower and resources.\nThere has been many efforts to automatically extract DDIs from natural language [1], [7]–[13], mainly medical literature and clinical records. These works can be divided into the following categories: • Text analysis and statistics based approach [1], [7], [14].\nThis kind of work utilizes NLP tools to analysis biomedical text’s semantics or statistics features (such as TFIDF) before the DDI decision. However, the semantics and statistics features are insufficient for understanding\nthe whole text. Even worse, NLP toolkits are imperfect and may propagate error to the classification. • Feature based machine learning approach [8], [11], [12], [15], [16]. Such method always need complex feature engineering. In addition, the quality of feature engineering have a great effect on the precision of DDI classification, which becomes the shortcoming of such method. • Deep learning based approach [9], [10], [13]. Deep learning neural networks, such as convolutional neural networks (CNN) and long short-term memory networks (LSTM), have been utilized for DDI extraction. Deep learning method avoids complicated feature engineering since CNN and LSTM can extract semantics features automatically through a well designed network.\nTo avoid complex feature engineering and NLP toolkits’ usage, we employ the deep learning approach for sentence comprehension as a whole. Our model takes in a sentence from biomedical literature which contains a drug pair and outputs what kind of DDI this drug pair belongs. This assists physicians refrain from improper combined use of drugs. In addition, the word and sentence level attentions are introduced to our model for better DDI predictions."
    }, {
      "heading" : "II. RELATED WORK",
      "text" : "In DDI extraction task, NLP methods or machine learning approaches are proposed by most of the work. Chowdhury [15] and Thomas et al. [12] proposed methods that use linguistic phenomenons and two-stage SVM to classify DDIs. FBK-irst [11] is a follow-on work which applies kernel method to the existing model and outperforms it.\nNeural network based approaches have been proposed by several works. Liu et al. [10] employ CNN for DDI extraction for the first time which outperforms the traditional machine learning based methods. Limited by the convolutional kernel size, the CNN can only extracted features of continuous 3 to 5 words rather than distant words. Liu et al. [9] proposed dependency-based CNN to handle distant but relevant words. Sahu et al. [13] proposed LSTM based DDI extraction approach and outperforms CNN based approach, since LSTM handles sentence as a sequence instead of slide windows. To conclude, Neural network based approaches have advantages of 1) less reliance on extra NLP toolkits, 2) simpler preprocessing procedure, 3) better performance than text analysis and machine learning methods.\nar X\niv :1\n70 5.\n03 26\n1v 1\n[ cs\n.C L\n] 9\nM ay\n2 01\n7\nDrug-drug interaction extraction is a relation extraction task of natural language processing. Relation extraction aims to determine the relation between two given entities in a sentence. In recent years, attention mechanism and various neural networks are applied to relation extraction [17]–[21]. Convolutional deep neural network are utilized for extracting sentence level features in [19]. Then the sentence level features are concatenated with lexical level features, which are obtained by NLP toolkit WordNet [22], followed by a multilayer perceptron (MLP) to classify the entities’ relation. A fixed work is proposed by Nguyen et al. [21]. The convolutional kernel is set various size to capture more n-gram features. In addition, the word and position embedding are trained automatically instead of keeping constant as in [19]. Wang et al. [20] introduce multi-level attention mechanism to CNN in order to emphasize the keywords and ignore the non-critical words during relation detection. The attention CNN model outperforms previous state-of-the-art methods.\nBesides CNN, Recurrent neural network (RNN) has been applied to relation extraction as well. Zhang et al. [18] utilize long short-term memory network (LSTM), a typical RNN model, to represent sentence. The bidirectional LSTM chronologically captures the previous and future information, after which a pooling layer and MLP have been set to extract feature and classify the relation. Attention mechanism is added to bidirectional LSTM in [17] for relation extraction. An attention layer gives each memory cell a weight so that classifier can catch the principal feature for the relation detection. The Attention based bidirectional LSTM has been proven better than previous work."
    }, {
      "heading" : "III. PROPOSED MODEL",
      "text" : "In this section, we present our bidirectional recurrent neural network with multiple attention layer model. The overview of our architecture is shown in Fig. 1. For a given instance, which describes the details about two or more drugs, the model represents each word as a vector in embedding layer. Then the bidirectional RNN layer generates a sentence matrix, each column vector in which is the semantic representation of the corresponding word. The word level attention layer transforms the sentence matrix to vector representation. Then sentence level attention layer generates final representation for the instance by combining several relevant sentences in view of the fact that these sentences have the same drug pair. Followed by a softmax classifier, the model classifies the drug pair in the given instance as specific DDI."
    }, {
      "heading" : "A. Embedding Layer",
      "text" : "Given a sentence S = (w1, w2, ..., wt) which contains specified two drugs wu, wv , each word is embedded in a d = dWE+2dPE dimensional space (dWE , dPE are the dimension of word embedding and position embedding). wi, pi are the one-hot representation (column vector) of word and relative distance. Ew, Ep are word and position embedding query matrix. After embedding layer the sentence is represented by S = (x1, x2, ..., xt), where\nxi = ((Ewwi)T, (Eppi−u)T, (Eppi−v)T)T (1)\nThen the word sequence is fed to the RNN layer. Note that the sentence will be filled with 0 if the length of which is less than t."
    }, {
      "heading" : "B. Bidirectional RNN Encoding Layer",
      "text" : "The words in the sequence are read by RNN’s gated recurrent unit (GRU) one by one. The GRU takes the current word xi and the previous GRU’s hidden state hi−1 as input. The current GRU encodes hi−1 and xi into a new hidden state hi (its dimension is dh, a hyperparameter), which can be regarded as informations the GRU remembered.\nFig. 2 shows the details in GRU. The reset gate ri selectively forgets informations delivered by previous GRU. Then the hidden state becomes h̃i. The update gate zi updates the informations according to h̃i and hi−1. The equations below describe these procedures. Note that ⊗ stands for element wise multiplication.\nri = σ(Wrxi + Urhi−1) (2)\nh̃i = Φ(Wxi + U(ri ⊗ hi−1)) (3)\nzi = σ(Wzxi + Uzhi−1) (4)\nhi = zi ⊗ hi−1 + ((1, 1, ..., 1)T − zi)⊗ h̃i (5)\nThe bidirectional RNN contains forward RNN and backward RNN. Forward RNN reads sentence from x1 to xt, generating −→ h 1, −→ h 2, ..., −→ h t. Backward RNN reads sentence from xt to x1, generating ←− h t, ←− h t−1, ..., ←− h 1. Then the encode result of this layer is\nH = ( −→ h 1 + ←− h 1, −→ h 2 + ←− h 2, ..., −→ h t + ←− h t) (6)\nWe apply dropout technique in RNN layer to avoid overfitting. Each GRU have a probability (denoted by Prdp, also a hyperparameter) of being dropped. The dropped GRU has no output and will not affect the subsequent GRUs. With bidirectional RNN and dropout technique, the input S = (x1, x2, ..., xt) is encoded into sentence matrix H ."
    }, {
      "heading" : "C. Word Level Attention",
      "text" : "The purpose of word level attention layer is to extract sentence representation (also known as feature vector) from encoded matrix. We use word level attention instead of max pooling, since attention mechanism can determine the importance of individual encoded word in each row of H . Let ω denotes the attention vector (column vector), a denotes the filter that gives each element in the row of H a weight. The following equations shows the attention operation, which is also illustrated in Fig. 1.\na = softmax(ωTtanh(H)) (7)\nh∗ = tanh(HaT) (8)\nh∗ denotes the final feature vector. Several approaches [13], [17] use this vector and softmax classifier for classification. Inspired by [23] we propose the sentence level attention to combine the information of other sentences for a improved DDI classification."
    }, {
      "heading" : "D. Sentence Level Attention",
      "text" : "The previous layers captures the features only from the given sentence. However, other sentences may contains informations that contribute to the understanding of this sentence. It is reasonable to look over other relevant instances when determine two drugs’ interaction from the given sentence.\nIn our implementation, the instances that have the same drug pair are believed to be relevant. The relevant instances set is denoted by S = {h∗1, h∗2, ..., h∗N}, where h∗i is the sentence feature vector. ei stands for how well the instance h∗i matches its DDI r (Vector representation of a specific DDI). A is a diagonal attention matrix, multiplied by which the feature vector h∗i can concentrate on those most representative features.\nei = h ∗T i Ar (9)\nαi = exp(ei)∑N\nk=1 exp(ek) (10)\nαi is the softmax result of ei. The final sentence representation is decided by all of the relevant sentences’ feature vector, as Eq. 11 shows.\ns = N∑ i=1 αih ∗ i (11)"
    }, {
      "heading" : "E. Classification and Training",
      "text" : "A given sentence S = (w1, w2, ..., wt) is finally represented by the feature vector s. Then we feed it to a softmax classifier. Let C denotes the set of all kinds of DDI. The output o ∈ R|C| is the probabilities of each class S belongs.\no = softmax(Ms+ d) (12)\nWe use cross entropy cost function and L2 regularization as the optimization objective. For i-th sentence, Yi denotes the one-hot representation of it’s label, where the model outputs oi. The cross entropy is li = −lnY Ti oi. For a mini-batchM = {S1, S2, ..., SM}, the optimization objective is:\nJ(θ) = − 1 |M| |M|∑ i=1 lnY Ti oi + λ||θ||2 (13)\nAll parameters in this model is:\nθ = {Ew, Ep,Wr, Ur,W,U,Wz, Uz, ω,A, r,M, d} (14)\nWe optimize the parameters of objective function J(θ) with Adam [24], which is a variant of mini-batch stochastic gradient descent. During each train step, the gradient of J(θ) is calculated. Then θ is adjusted according to the gradient. After the end of training, we have a model that is able to predict two drugs’ interactions when a sentence about these drugs is given."
    }, {
      "heading" : "IV. EXPERIMENTS",
      "text" : ""
    }, {
      "heading" : "A. Datasets and Evaluation Metrics",
      "text" : "We use the DDI corpus of the 2013 DDIExtraction challenge [25] to train and test our model. The DDIs in this corpus are classified as five categories. We give the definitions of these categories and their example sentences, as shown in Table I.\nThis standard dataset is made up of training set and testing set. We use the same metrics as in other drug-drug interaction extraction literature [9]–[13], [26]: the overall precision, recall, and F1 score on testing set. C denotes the set of {False, Mechanism, Effect, Advise, Int}. The precision and recall of each DDI c ∈ C are calculated by\nPc = #DDI is c and is classified as c\n#All the instances that are classified as c (15)\nRc = #DDI is c and is classified as c\n#All the instances that their DDI is c (16)\nThen the overall precision, recall, and F1 score are calculated by\nP = 1 |C| ∑ c∈C Pc, R = 1 |C| ∑ c∈C Rc, F1 = 2PR P +R (17)"
    }, {
      "heading" : "B. Hyperparameter Settings and Training",
      "text" : "We use TensorFlow to implement the proposed model. The input of each word is an ordered triple (word, relative position 1, relative position 2). The sentence, which is represented as a matrix, is fed to the model. The output of the model is a |C|-dimension vector representing the probabilities of\nbeing corresponding DDI. It is the network, parameters, and hyperparameters which decides the output vector. The network’s parameters are adjusted during training, where the hyperparameters are tuned by hand. The hyperparameters after tuning are as follows. The word embedding’s dimension dWE = 100, the position embedding’s dimension dPE = 10, the hidden state’s dimension dh = 230, the probability of dropout Prd = 0.5.\nThe word embedding is initialized by pre-trained word vectors using GloVe [27], while other parameters are initialized randomly. During each training step, a mini-batch (the minibatch size |M| = 60 in our implementation) of sentences is selected from training set. The gradient of objective function is calculated for parameters updating (See Section III-E).\nFig. 3 shows the training process. The objective function J(θ) is declining as the training mini-batches continuously sent to the model. As the testing mini-batches, the J(θ) function is fluctuating while its overall trend is descending. The instances in testing set are not participated in training so that J(θ) function is not descending so fast. However, training and testing instances have similar distribution in sample space, causing that testing instances’ J(θ) tends to be smaller along with the training process. J(θ) has inverse relationship with the performance measurement. The F1 score is getting fluctuating around a specific value after enough training steps. The\nreason why fluctuating range is considerable is that only a tiny part of the whole training or testing set has been calculated the F1 score. Testing the whole set during every step is time consuming and not necessary. We will evaluate the model on whole testing set in Section IV-C."
    }, {
      "heading" : "C. Experimental Results",
      "text" : "We save our model every 100 step and predict all the DDIs of the instances in the testing set. These predictions’ F1 score is shown in Fig. 4. To demonstrate the sentence level attention layer is effective, we drop this layer and then directly use h∗ for softmax classification (See Fig. 1). The result is shown with “RNN + dynamic word embedding + ATT” curve, which illustrates that the sentence level attention layer contributes to a more accurate model.\nWhether a dynamic or static word embedding is better for a DDI extraction task is under consideration. Nguyen et al. [21] shows that updating word embedding at the time of other parameters being trained makes a better performance in relation extraction task. We let the embedding be static when training, while other conditions are all the same. The “RNN + static word embedding + 2ATT” curve shows this case. We can draw a conclusion that updating the initialized word embedding trains more suitable word vectors for the task, which promotes the performance.\nWe compare our best F1 score with other state-of-theart approaches in Table II, which shows our model has competitive advantage in dealing with drug-drug interaction extraction. The predictions confusion matrix is shown in Table III. The DDIs other than false being classified as false makes most of the classification error. It may perform better if a classifier which can tells true and false DDI apart is trained. We leave this two-stage classifier to our future work. Another phenomenon is that the “Int” type is often classified as “Effect”. The “Int” sentence describes there exists interaction between two drugs and this information implies the two drugs’ combination will have good or bed effect. That’s the reason why “Int” and “Effect” are often obfuscated."
    }, {
      "heading" : "V. CONCLUSION AND FUTURE WORK",
      "text" : "To conclude, we propose a recurrent neural network with multiple attention layers to extract DDIs from biomedical text. The sentence level attention layer, which combines other sentences containing the same drugs, has been added to our model. The experiments shows that our model outperforms the state-of-the-art DDI extraction systems. Task relevant word embedding and two attention layers improved the performance to some extent.\nThe imbalance of the classes and the ambiguity of semantics cause most of the misclassifications. We consider that instance generation using generative adversarial networks would cover the shortage of instance in specific category. It is also reasonable to use distant supervision learning (which utilize other relevant material) for knowledge supplement and obtain a better performed DDI extraction system."
    }, {
      "heading" : "ACKNOWLEDGMENT",
      "text" : "This work is supported by the NSFC under Grant 61303191, 61303190, 61402504, 61103015."
    } ],
    "references" : [ {
      "title" : "Discovering drug-drug interactions: a text-mining and reasoning approach based on properties of drug metabolism",
      "author" : [ "L. Tari", "S. Anwar", "S. Liang", "J. Cai", "C. Baral" ],
      "venue" : "Bioinformatics, vol. 26, no. 18, pp. 547–53, 2010.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Incidence of adverse drug reactions in hospitalized patients: a meta-analysis of prospective studies",
      "author" : [ "J. Lazarou", "B.H. Pomeranz", "P.N. Corey" ],
      "venue" : "Jama, vol. 279, no. 15, pp. 1200–1205, 1998.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Sfinxa drug-drug interaction database designed for clinical decision support systems",
      "author" : [ "Y. Böttiger", "K. Laine", "M.L. Andersson", "T. Korhonen", "B. Molin", "M.- L. Ovesjö", "T. Tirkkonen", "A. Rane", "L.L. Gustafsson", "B. Eiermann" ],
      "venue" : "European journal of clinical pharmacology, vol. 65, no. 6, pp. 627–633, 2009.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Crediblemeds: Independent information on medicines",
      "author" : [ "P.R. Shankar" ],
      "venue" : "Australasian Medical Journal, vol. 7, no. 1, p. 149, 2014.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Enhanced theophylline clearance secondary to phenytoin therapy",
      "author" : [ "S.J. Sklar", "J.C. Wagner" ],
      "venue" : "Annals of Pharmacotherapy, vol. 19, no. 1, p. 34, 1985.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1985
    }, {
      "title" : "A novel algorithm for analyzing drug-drug interactions from medline literature",
      "author" : [ "Y. Lu", "D. Shen", "M. Pietsch", "C. Nagar", "Z. Fadli", "H. Huang", "Y.C. Tu", "F. Cheng" ],
      "venue" : "Scientific Reports, vol. 5, p. 17357, 2015.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A novel feature-based approach to extract drugdrug interactions from biomedical text",
      "author" : [ "Q.C. Bui", "P.M.A. Sloot", "E.M.V. Mulligen", "J.A. Kors" ],
      "venue" : "Bioinformatics, vol. 30, no. 23, pp. 3365–71, 2014.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Dependency-based convolutional neural network for drug-drug interaction extraction",
      "author" : [ "S. Liu", "K. Chen", "Q. Chen", "B. Tang" ],
      "venue" : "IEEE International Conference on Bioinformatics and Biomedicine, 2016, pp. 1074–1080.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Drug-drug interaction extraction via convolutional neural networks",
      "author" : [ "S. Liu", "B. Tang", "Q. Chen", "X. Wang" ],
      "venue" : "Computational and Mathematical Methods in Medicine, vol. 2016, pp. 1–8, 2016.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Fbk-irst: A multi-phase kernel based approach for drug-drug interaction detection and classification that exploits linguistic information",
      "author" : [ "M.F.M. Chowdhury", "A. Lavelli" ],
      "venue" : "Atlanta, Georgia, USA, vol. 351, p. 53, 2013.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Wbi-ddi: Drugdrug interaction extraction using majority voting",
      "author" : [ "P. Thomas", "M. Neves", "T. Rocktschel", "U. Leser" ],
      "venue" : "DDI Challenge at Semeval, 2013.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Drug-drug interaction extraction from biomedical text using long short term memory network",
      "author" : [ "S.K. Sahu", "A. Anand" ],
      "venue" : "arXiv preprint arXiv:1701.08303, 2017.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Vorobkalov, Retrieval of Drug-Drug Interactions Information from Biomedical Texts: Use of TF-IDF for Classification",
      "author" : [ "P.N.M.P. Melnikov" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2014
    }, {
      "title" : "Exploiting the scope of negations and heterogeneous features for relation extraction: A case study for drugdrug interaction extraction.",
      "author" : [ "M.F.M. Chowdhury", "A. Lavelli" ],
      "venue" : "in HLT-NAACL,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2013
    }, {
      "title" : "Extraction and classification of drug-drug interaction from biomedical text using a two-stage classifier",
      "author" : [ "M. Rastegar-Mojarad" ],
      "venue" : "2013.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Attentionbased bidirectional long short-term memory networks for relation classification",
      "author" : [ "P. Zhou", "W. Shi", "J. Tian", "Z. Qi", "B. Li", "H. Hao", "B. Xu" ],
      "venue" : "Meeting of the Association for Computational Linguistics, 2016, pp. 207–212.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Bidirectional long short-term memory networks for relation classification.",
      "author" : [ "S. Zhang", "D. Zheng", "X. Hu", "M. Yang" ],
      "venue" : "PACLIC,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2015
    }, {
      "title" : "Relation classification via convolutional deep neural network.",
      "author" : [ "D. Zeng", "K. Liu", "S. Lai", "G. Zhou", "J. Zhao" ],
      "venue" : "in COLING,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2014
    }, {
      "title" : "Relation classification via multi-level attention cnns",
      "author" : [ "L. Wang", "Z. Cao", "G.D. Melo", "Z. Liu" ],
      "venue" : "Meeting of the Association for Computational Linguistics, 2016, pp. 1298–1307.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Relation extraction: Perspective from convolutional neural networks",
      "author" : [ "T.H. Nguyen", "R. Grishman" ],
      "venue" : "The Workshop on Vector Space Modeling for Natural Language Processing, 2015, pp. 39–48.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Wordnet: An electronic lexical database",
      "author" : [ "D. Lin" ],
      "venue" : "Computational Linguistics, vol. 25, no. 2, pp. 292–296, 1999.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Neural relation extraction with selective attention over instances",
      "author" : [ "Y. Lin", "S. Shen", "Z. Liu", "H. Luan", "M. Sun" ],
      "venue" : "Meeting of the Association for Computational Linguistics, 2016, pp. 2124–2133.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "D. Kingma", "J. Ba" ],
      "venue" : "arXiv preprint arXiv:1412.6980, 2014.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "The ddi corpus: an annotated corpus with pharmacological substances and drug-drug interactions",
      "author" : [ "M. Herrero-Zazo", "I. Segura-Bedmar", "P. Martnez", "T. Declerck" ],
      "venue" : "Journal of Biomedical Informatics, vol. 46, no. 5, p. 914, 2013.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Drug drug interaction extraction from biomedical literature using syntax convolutional neural network.",
      "author" : [ "Z. Zhao", "Z. Yang", "L. Luo", "H. Lin", "J. Wang" ],
      "venue" : "Bioinformatics, vol. 32,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2016
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "J. Pennington", "R. Socher", "C. Manning" ],
      "venue" : "Conference on Empirical Methods in Natural Language Processing, 2014, pp. 1532–1543.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Drug-drug interaction (DDI) is a situation when one drug increases or decreases the effect of another drug [1].",
      "startOffset" : 107,
      "endOffset" : 110
    }, {
      "referenceID" : 1,
      "context" : "DDI is a common cause of illness, even a cause of death [2].",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 2,
      "context" : "These databases such as SFINX [3], KEGG [4], CredibleMeds [5] help physicians and pharmacists avoid most adverse drug reactions.",
      "startOffset" : 30,
      "endOffset" : 33
    }, {
      "referenceID" : 3,
      "context" : "These databases such as SFINX [3], KEGG [4], CredibleMeds [5] help physicians and pharmacists avoid most adverse drug reactions.",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 4,
      "context" : "For instance, The sentence “With combined use, clinicians should be aware, when phenytoin is added, of the potential for reexacerbation of pulmonary symptomatology due to lowered serum theophylline concentrations [6]”, which is from a pharmacotherapy report, describe the side effect of phenytoin and theophylline’s combined use.",
      "startOffset" : 213,
      "endOffset" : 216
    }, {
      "referenceID" : 0,
      "context" : "There has been many efforts to automatically extract DDIs from natural language [1], [7]–[13], mainly medical literature and clinical records.",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 5,
      "context" : "There has been many efforts to automatically extract DDIs from natural language [1], [7]–[13], mainly medical literature and clinical records.",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 11,
      "context" : "There has been many efforts to automatically extract DDIs from natural language [1], [7]–[13], mainly medical literature and clinical records.",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 0,
      "context" : "• Text analysis and statistics based approach [1], [7], [14].",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 5,
      "context" : "• Text analysis and statistics based approach [1], [7], [14].",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 12,
      "context" : "• Text analysis and statistics based approach [1], [7], [14].",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 6,
      "context" : "• Feature based machine learning approach [8], [11], [12], [15], [16].",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 9,
      "context" : "• Feature based machine learning approach [8], [11], [12], [15], [16].",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 10,
      "context" : "• Feature based machine learning approach [8], [11], [12], [15], [16].",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 13,
      "context" : "• Feature based machine learning approach [8], [11], [12], [15], [16].",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 14,
      "context" : "• Feature based machine learning approach [8], [11], [12], [15], [16].",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 7,
      "context" : "• Deep learning based approach [9], [10], [13].",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 8,
      "context" : "• Deep learning based approach [9], [10], [13].",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 11,
      "context" : "• Deep learning based approach [9], [10], [13].",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 13,
      "context" : "Chowdhury [15] and Thomas et al.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 10,
      "context" : "[12] proposed methods that use linguistic phenomenons and two-stage SVM to classify DDIs.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 9,
      "context" : "FBK-irst [11] is a follow-on work which applies kernel method to the existing model and outperforms it.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 8,
      "context" : "[10] employ CNN for DDI extraction for the first time which outperforms the traditional machine learning based methods.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 7,
      "context" : "[9] proposed dependency-based CNN to handle distant but relevant words.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 11,
      "context" : "[13] proposed LSTM based DDI extraction approach and outperforms CNN based approach, since LSTM handles sentence as a sequence instead of slide windows.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "In recent years, attention mechanism and various neural networks are applied to relation extraction [17]–[21].",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 19,
      "context" : "In recent years, attention mechanism and various neural networks are applied to relation extraction [17]–[21].",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 17,
      "context" : "Convolutional deep neural network are utilized for extracting sentence level features in [19].",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 20,
      "context" : "Then the sentence level features are concatenated with lexical level features, which are obtained by NLP toolkit WordNet [22], followed by a multilayer perceptron (MLP) to classify the entities’ relation.",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 19,
      "context" : "[21].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "In addition, the word and position embedding are trained automatically instead of keeping constant as in [19].",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 18,
      "context" : "[20] introduce multi-level attention mechanism to CNN in order to emphasize the keywords and ignore the non-critical words during relation detection.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "[18] utilize long short-term memory network (LSTM), a typical RNN model, to represent sentence.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "Attention mechanism is added to bidirectional LSTM in [17] for relation extraction.",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 11,
      "context" : "Several approaches [13], [17] use this vector and softmax classifier for classification.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 15,
      "context" : "Several approaches [13], [17] use this vector and softmax classifier for classification.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 21,
      "context" : "Inspired by [23] we propose the sentence level attention to combine the information of other sentences for a improved DDI classification.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 22,
      "context" : "We optimize the parameters of objective function J(θ) with Adam [24], which is a variant of mini-batch stochastic gradient descent.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 23,
      "context" : "We use the DDI corpus of the 2013 DDIExtraction challenge [25] to train and test our model.",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 7,
      "context" : "We use the same metrics as in other drug-drug interaction extraction literature [9]–[13], [26]: the overall precision, recall, and F1 score on testing set.",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 11,
      "context" : "We use the same metrics as in other drug-drug interaction extraction literature [9]–[13], [26]: the overall precision, recall, and F1 score on testing set.",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 24,
      "context" : "We use the same metrics as in other drug-drug interaction extraction literature [9]–[13], [26]: the overall precision, recall, and F1 score on testing set.",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 25,
      "context" : "The word embedding is initialized by pre-trained word vectors using GloVe [27], while other parameters are initialized randomly.",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 10,
      "context" : "WBI [12] Two stage SVM classification 0.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 9,
      "context" : "6090 FBK-ist [11] Hand crafted features + SVM 0.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 24,
      "context" : "6510 SCNN [26] Two stage syntax CNN 0.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 8,
      "context" : "[10] CNN + Pre-trained WE 0.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 7,
      "context" : "6975 DCNN [9] Dependency-based CNN + Pretrained WE 0.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 11,
      "context" : "[13] bidirectional LSTM + ATT 0.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "[21] shows that updating word embedding at the time of other parameters being trained makes a better performance in relation extraction task.",
      "startOffset" : 0,
      "endOffset" : 4
    } ],
    "year" : 2017,
    "abstractText" : "Drug-drug interaction (DDI) is a vital information when physicians and pharmacists prepare for the combined use of two or more drugs. Thus, several DDI databases are constructed to avoid mistakenly medicine administering. In recent years, automatically extracting DDIs from biomedical text has drawn researchers’ attention. However, the existing work need either complex feature engineering or NLP tools, both of which are insufficient for sentence comprehension. Inspired by the deep learning approaches in natural language processing, we propose a recurrent neural network model with multiple attention layers for DDI classification. We evaluate our model on 2013 SemEval DDIExtraction dataset. The experiments show that our model classifies most of the drug pairs into correct DDI categories, which outperforms the existing NLP or deep learning method.",
    "creator" : "LaTeX with hyperref package"
  }
}