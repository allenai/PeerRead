{
  "name" : "1602.02018.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Compressive spectral clustering",
    "authors" : [ "Nicolas Tremblay", "Gilles Puy", "Rémi Gribonval", "Pierre Vandergheynst" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "its Laplacian matrix to define a feature vector for each object, and run k-means on these features to separate objects into k classes. Each of these three steps becomes computationally intensive for large N and/or k. We propose to speed up the last two steps based on recent results in the emerging field of graph signal processing: graph filtering of random signals, and random sampling of bandlimited graph signals. We prove that our method, with a gain in computation time that can reach several orders of magnitude, is in fact an approximation of spectral clustering, for which we are able to control the error. We test the performance of our method on artificial and real-world network data.\n1. Introduction\nSpectral clustering (SC) is a fundamental tool in data mining [1]. Given a set of N data points {x1, . . . ,xN}, the goal is to partition this set into k weakly inter-connected clusters. Several spectral clustering algorithms exist, e.g., [2–5], but all follow the same scheme. First, compute weights Wij > 0 that model the similarity between pairs of data points (xi,xj). This gives rise to a graph G with N nodes and adjacency matrix W = (Wij)16i,j6N ∈ RN×N . Second, compute the first k eigenvectors Uk := (u1, . . . ,uk) ∈ RN×k of the Laplacian matrix L ∈ RN×N associated to G (see Sec. 2 for L’s definition). And finally, run k-means using the rows of Uk as feature vectors to partition the N data points into k clusters. This k-way scheme is a generalisation of Fiedler’s pioneering work [6].\nSC is mainly used in two contexts: 1) if the N data points show particular structures (e.g., concentric circles) for which naive k-means clustering fails; 2) if the input data is directly a graph G modeling a network [7], such as social, neuronal, or transportation networks. SC suffers nevertheless from three main computational bottlenecks for large N and/or k: the creation of the similarity matrix W; the partial eigendecomposition of the graph Laplacian matrix L; and k-means."
    }, {
      "heading" : "1.1. Related work",
      "text" : "Circumventing these bottlenecks has raised a significant interest in the past decade. Several authors have proposed ideas to tackle the eigendecomposition bottleneck, e.g., via the power method [8, 9], via a careful optimisation of diagonalisation algorithms in the context of SC [10], or via matrix column-subsampling such as in the Nyström method [11], the nSPEC and cSPEC methods of [12], or in [13, 14]. All these methods aim to quickly compute feature vectors, but k-means is still applied on N feature vectors. Other authors have proposed to circumvent k-means in high dimension by subsampling a few data points out of the N available ones, applying SC on its reduced similarity graph, and interpolating the results back on the complete dataset. One can find such methods in [15] and [12]’s eSPEC proposition, where two different interpolation methods are used. Both methods are heuristic: there is no proof that these methods approach the results of SC. Also, let us mention [16] that circumvents both the eigendecomposition and the k-means bottlenecks: the authors reduce the graph’s size by successive aggregation of nodes, apply SC on this small graph, and propagate the results on the complete graph using kernel k-means to control interpolation errors. The kernel is computed so that kernel k-means and SC share the same objective function [17]. Finally, we mention\nThis work was partly funded by the European Research Council, PLEASE project (ERC-StG-2011-277906), and by the Swiss National Science Foundation, grant 200021-154350/1 - Towards Signal Processing on Graphs.\nN. Tremblay, G. Puy, R. Gribonval and P. Vandergheynst are with INRIA Rennes - Bretagne Atlantique, Campus de Beaulieu, FR-35042 Rennes Cedex, France. N. Tremblay and P. Vandergheynst are also with the Institute of Electrical Engineering, Ecole Polytechnique Fédérale de Lausanne (EPFL), CH-1015 Lausanne, Switzerland.\n1\nar X\niv :1\n60 2.\n02 01\n8v 1\n[ cs\n.D S]\n5 F\neb 2\n01 6\nworks [18, 19] that concentrate on reducing the feature vectors’ dimension in the k-means problem, but do not sidestep the eigendecomposition nor the large N issues."
    }, {
      "heading" : "1.2. Contribution: compressive clustering",
      "text" : "In this work, inspired by recent advances in the emerging field of graph signal processing [20, 21], we circumvent SC’s last two bottlenecks and detail a fast approximate spectral clustering method for large datasets, as well as the supporting theory. We suppose that the Laplacian matrix L ∈ RN×N of G is given. Our method is made of two ingredients.\nThe first ingredient builds upon recent works [22,23] that avoid the costly computation of the eigenvectors of L by filtering O(log(k)) random signals on G that will then serve as feature vectors to perform clustering. We show in this paper how to incorporate the effects of non-ideal, but computationally efficient, graph filters on the quality of the feature vectors used for clustering.\nThe second ingredient uses a recent sampling theory of bandlimited graph-signals [24] to reduce the computational complexity of k-means. Using the fact that the indicator vectors of each cluster are approximately bandlimited on G, we prove that clustering a random subset of O(k log(k)) nodes of G using random features vectors of size O(log(k)) is sufficient to infer rapidly and accurately the cluster label of all N nodes of the graph. Note that the complexity of k-means is reduced to O(k2 log2(k)) instead of O(Nk2) for SC. One readily sees that this method scales easily to large datasets, as will be demonstrated on artifical and real-world datasets containing up to N = 106 nodes.\nThe proposed compressive spectral clustering method can be summarised as follows:\n• generate a feature vector for each node by filtering O(log(k)) random Gaussian signals on G; • sample O(k log(k)) nodes from the full set of nodes; • cluster the reduced set of nodes; • interpolate the cluster indicator vectors back to the complete graph.\n2. Background"
    }, {
      "heading" : "2.1. Graph signal processing",
      "text" : "Let G = (V, E ,W) be an undirected weighted graph with V the set of N nodes, E the set of edges, and W the weighted adjacency matrix such that Wij = Wji > 0 is the weight of the edge between nodes i and j.\nThe graph Fourier matrix. Consider the graph’s normalized Laplacian matrix L = I−D−1/2WD−1/2 where I is the identity matrix in dimension N , and D is diagonal with Dii = ∑ j 6=i Wij the strength of node i. L is real symmetric and positive semi-definite, therefore diagonalizable in an orthogonal basis. Its spectrum is composed of its set of sorted eigenvalues 0 = λ1 6 . . . 6 λN 6 2 [25], and of the orthonormal matrix U := (u1|u2| . . . |uN ) containing its eigenvectors. We denote by Λ ∈ RN×N the diagonal matrix containing the eigenvalues of L. By analogy to the continuous Laplacian operator whose eigenfunctions are the classical Fourier modes and eigenvalues their squared frequencies, the columns of U are considered as the graph’s Fourier modes, and { √ λl}l as its set of associated “frequencies” [20]. Other types of graph Fourier matrices have been proposed, e.g., [26], but in order to exhibit the link between graph signal processing and SC (that partially diagonalizes the Laplacian matrix), the Laplacian-based Fourier matrix appears more natural.\nGraph filtering. The graph Fourier transform x̂ of a signal x defined on the nodes of the graph (called a graph signal) reads: x̂ = Uᵀx. Given a continuous filter function h defined on [0, 2], its associated graph filter operator H ∈ RN×N is defined as H := h(L) = Uh(Λ)Uᵀ, where h(Λ) := diag(h(λ1), h(λ2), · · · , h(λN )). The signal x filtered by h is Hx. In the following, we consider ideal low-pass filters, denoted by hλc , that satisfy, for all λ ∈ [0, 2],\nhλc(λ) = 1, if λ 6 λc, and hλc(λ) = 0, if not.(1)\nDenote by Hλc the graph filter operator associated to hλc .\nAlgorithm 1 Spectral Clustering [2]\nInput: The Laplacian matrix L, the number of clusters k 1· Compute Uk ∈ RN×k, L’s first k eigenvectors: Uk = (u1|u2| · · · |uk). 2· Form the matrix Yk ∈ RN×k from Uk by normalizing each of Uk’s rows to unit length: (Yk)ij = (Uk)ij / √∑k j=1 U 2 ij . 3· Treat each node i as a point in Rk by defining its feature vector fi ∈ Rk as the transposed i-th row of Yk:\nfi := Y ᵀ kδi,\nwhere δi(j) = 1 if j = i and 0 otherwise. 4· To obtain k clusters, run k-means with the Euclidean distance: (2) Dij := ‖fi − fj‖\nFast graph filtering. In order to filter a signal by h without diagonalizing L, one may approximate h by a polynomial h̃ of order p satisfying\nh̃(λ) := p∑ l=0 αlλ l ' h(λ)\nfor all λ ∈ [0, 2], where α1, . . . , αp ∈ R. In matrix form, we have\nH̃ := h̃(L) = p∑ l=0 αlL l ' H.\nLet us highlight that we never compute the potentially dense matrix H̃ in practice. Indeed, we will only be interested in the result of the filtering operation, i.e., H̃x = ∑p l=0 αlL\nlx ≈ Hx for x ∈ RN , that can be obtained with only p successive matrix-vector multiplications with L. The computational complexity of filtering a signal is thus O(p#E), where #E is the number of edges of G."
    }, {
      "heading" : "2.2. Spectral clustering",
      "text" : "We choose here Ng et al.’s method [2] based on the normalized Laplacian as our standard SC method. The input is the adjacency matrix W representing the pairwise similarity of all the N objects to cluster1. After computing its Laplacian L, follow Alg. 1 to find k classes.\n3. Principles of CSC\nCompressive spectral clustering (CSC) circumvents two of SC’s bottlenecks, the partial diagonalisation of the Laplacian and the high-dimensional k-means, thanks to the following ideas.\n1) Perform a controlled estimation D̃ij of the spectral clustering distance Dij (see Eq (2)), without partially diagonalizing the Laplacian, by fast filtering a few random signals with the polynomial approximation h̃λk of the ideal low pass filter hλk (see Eq. (1)). A theorem recently published independently by two teams [22, 23] shows that this is possible when there is no normalisation step (step 2 in Alg. 1) and when the order p of the polynomial approximation tends to infinity, i.e., when\nh̃λk = hλk . In Sec. 3.1, we provide a first extension of this theorem that takes into account normalisation. A complete extension that also takes into account the errors due to a finite-order polynomial approximation will be presented in Sec. 4.2.\n2) Run k-means on n randomly selected feature vectors out of the N available ones - thus clustering the corresponding n nodes into k groups - and interpolate the result back on the full graph. To guarantee robust reconstruction, we take advantage of recent results of [24] on random sampling of\n1In network analysis, the raw data is directly W. In the case where one starts with a set of data points (x1, . . . ,xN ), the first step consists in deriving W from the pairwise similarities s(xi,xj). See [27] for several choices of similarity measure s and several ways to create W from the s(xi,xj).\nk-bandlimited graph signals. In Sec. 3.2, we explain why these results are applicable to clustering and show that it is sufficient to sample O(k log k) features only! Note that to cluster a dataset into k groups, one needs at least k samples. The above bound is thus optimal up to the extra log k factor."
    }, {
      "heading" : "3.1. Ideal filtering of random signals",
      "text" : "Definition 3.1 (Local cumulative coherence). Given a graph G, the local cumulative coherence of order k at node i is2 vk(i) := ‖Uᵀkδi‖ = √∑k j=1 U 2 ij.\nLet us define the diagonal matrix: Vk(i, i) = 1/vk(i). Note that we assume that vk(i) > 0. Indeed, in the pathologic cases where vk(i) = 0 for some nodes i, Step 2 of the standard SC algorithm cannot be run either. Now, consider the matrix R = (r1|r2| · · · |rd) ∈ RN×d consisting of d random signals ri, whose components are independent Bernouilli, Gaussian, or sparse (as in Theorem 1.1 of [28]) random variables. To fix ideas in the following, we consider the components as independent random Gaussian variables of mean zero and variance 1/d. Consider the coherence-normalized filtered version of R, VkHλkR ∈ RN×d, and define node i’s new feature vector f̃i ∈ Rd as the transposed i-th line of this filtered matrix:\nf̃i := (VkHλkR) ᵀδi.\nThe following theorem shows that, for large enough d, D̃ij := ∥∥∥f̃i − f̃j∥∥∥ = ‖(VkHλkR)ᵀ(δi − δj)‖\nis a good estimation of Dij with high probability.\nTheorem 3.2. Let ∈]0, 1] and β > 0 be given. If d is larger than 4 + 2β\n2/2− 3/3 logN,\nthen with probability at least 1−N−β, we have (1− )Dij 6 D̃ij 6 (1 + )Dij .\nfor all (i, j) ∈ {1, . . . , N}2.\nProof. The proof is an instance of the Johnson-Lindenstrauss lemma. Note that Hλk = UkU ᵀ k, and that Yk = VkUk. We rewrite ∥∥∥f̃i − f̃j∥∥∥ in a form that will let us apply the Johnson-Lindenstrauss\nlemma of norm conservation: (3) ∥∥∥f̃i − f̃j∥∥∥ = ∥∥RᵀHᵀλkVᵀk(δi − δj)∥∥ = ‖RᵀUkUᵀkVᵀk(δi − δj)‖ = ‖RᵀUk(fi − fj)‖\nwhere the fi are the standard SC feature vectors. Applying Theorem 1.1 of [28] (an instance of the Johnson-Lindenstrauss lemma) to ‖RᵀUk(fi − fj)‖, the following holds. If d is larger than:\n(4) 4 + 2β\n2/2− 3/3 logN,\nthen with probability at least 1−N−β , we have, ∀(i, j) ∈ {1, . . . , N}2:\n(1− ) ‖Uk(fi − fj)‖ 6 D̃ij 6 (1 + ) ‖Uk(fi − fj)‖ . As the columns of Uk are orthonormal, we end the proof:\n∀(i, j) ∈ [1, N ]2 ‖Uk(fi − fj)‖ = ‖fi − fj‖ = Dij .\nIn Sec. 4.2, we generalize this result to the real-world case where the low-pass filter is approximated by a finite order polynomial; we also prove that, as announced in the introduction, one only needs d = O(log k) features when using the downsampling scheme that we now detail.\n2Throughout this paper, ‖.‖ stands for the usual `2-norm."
    }, {
      "heading" : "3.2. Downsampling and interpolation",
      "text" : "For j = 1, . . . , k, let us denote by cj ∈ RN the ground-truth indicator vector of cluster Cj , i.e.,\n(cj)i := { 1 if i ∈ Cj , 0 otherwise,\n∀i ∈ {1, . . . , N}.\nTo estimate cj , one could run k-means on the N feature vectors {f̃1, . . . , f̃N} , as done in [22, 23]. Yet, this is still inefficient for large N . To reduce the computational cost further, we propose to run k-means on a small subset of n feature vectors only. The goal is then to infer the labels of all N nodes from the labels of the n sampled nodes. To this end, we need 1) a low-dimensional model that captures the regularity of the vectors cj , 2) to make sure that enough information is preserved after sampling to be able to recover the vectors cj , and 3) an algorithm that rapidly and accurately estimates the vectors cj by exploiting their regularity."
    }, {
      "heading" : "3.2.1. The low-dimensional model",
      "text" : "For a simple regular (with nodes of same degree) graph of k disconnected clusters, it is easy to check that {c1, . . . , ck} form a set of orthogonal eigenvectors of L with eigenvalue 0. All indicator vectors cj therefore live in span(Uk). For general graphs, we assume that the indicator vectors cj live close to span(Uk), i.e., the difference between any cj and its orthogonal projection onto span(Uk) is small. Experiments in Section 5 will confirm that it is a good enough model to recover the cluster indicator vectors.\nIn graph signal processing words, one can say that cj is approximately k-bandlimited, i.e., its k first graph Fourier coefficients bear most of its energy. There has been recently a surge of interest around adapting classical sampling theorems to such bandlimited graph signals [29–32]. We rely here on the random sampling strategy proposed in [24] to select a subset of n nodes."
    }, {
      "heading" : "3.2.2. Sampling and interpolation",
      "text" : "The subset of feature vectors is selected by drawing n indices Ω := {ω1, . . . , ωn} uniformly at random from {1, . . . , N} without replacement. Running k-means on the subset of features {f̃ω1 , . . . , f̃ωn} thus yields a clustering of the n sampled nodes into k clusters. We denote by crj ∈ Rn the resulting low-dimensional indicator vectors. Our goal is now to recover cj from c r j .\nConsider that k-means is able to correctly identify c1, . . . , ck ∈ RN using the original set of features {f1, . . . ,fN} with the SC algorithm (otherwise, CSC is doomed to fail from the start). Results in [22,23] show that k-means is also able to identify the clusters using the feature vectors {f̃1, . . . , f̃N}. This is explained theoretically by the fact that the distance between all pairs of feature vectors is preserved (see Theorem 3.2). Then, as choosing a subset {f̃ω1 , . . . , f̃ωn} of {f̃1, . . . , f̃N} does not change the distance between the feature vectors, we can hope that k-means correctly clusters the n sampled nodes, provided that each cluster is sufficiently sampled. Experiments in Sec. 5 will confirm this intuition. In this ideal situation, we have\n(5) crj = M cj ,\nwhere M ∈ Rn×N is the sampling matrix satisfying:\nMij := { 1 if j = ωi, 0 otherwise. (6)\nTo recover cj from its n observations c r j , Puy et al. [24] show that the solution to the optimisation problem\nmin x∈RN ∥∥Mx− crj∥∥22 + γ xᵀg(L)x,(7) is a faithful 3 estimation of cj , provided that cj is close to span(Uk) and that M satisfies the restricted isometry property (discussed in the next subsection). In (7), γ > 0 is a regularisation parameter\n3precise error bounds are provided in [24].\nand g a positive non-decreasing polynomial function (see Section 2.1 for the definition of g(L)). This reconstruction scheme is proved to be robust to: 1) observation noise, i.e., to imperfect clustering of the n nodes in our context; 2) model errors, i.e., the indicator vectors do not need to be exactly in span(Uk) for the method to work. Also, the performance is shown to depend on the ratio g(λk)/g(λk+1). The smaller it is, the better the reconstruction. To decrease this ratio, we decide to approximate the ideal high-pass filter gλk(λ) = 1−hλk(λ) for the reconstruction. Remark that this filter favors the recovery of signals living in span(Uk). The approximation g̃λk of gλk is obtained using a polynomial (as in Sec. 2.1), which permits us to find fast algorithms to solve (7)."
    }, {
      "heading" : "3.2.3. How many features to sample?",
      "text" : "We terminate this section by providing the theoretical number of features n one needs to sample in order to make sure that the indicator vectors can be faithfully recovered. This number is driven by the following quantity.\nDefinition 3.3 (Global cumulative coherence). The global cumulative coherence of order k of the graph G is νk := √ N · max16i6N {vk(i)} .\nIt is shown in [24] that νk ∈ [k1/2, N1/2].\nTheorem 3.4 ( [24]). Let M be a random sampling matrix constructed as in (6). For any δ, ∈ ]0, 1[,\n(1− δ) ‖x‖22 6 N\nn ‖Mx‖22 6 (1 + δ) ‖x‖ 2 2(8)\nfor all x ∈ span(Uk) with probability at least 1− provided that\nn > 6\nδ2 ν2k log\n( k ) .\nThe above theorem presents a sufficient condition on n ensuring that M satisfies the restricted isometry property (8). This condition is required to ensure that the solution of (7) is an accurate estimation of cj . The above theorem thus indicates that sampling O(ν 2 k log k) features is sufficient to recover the cluster indicator vectors. For a simple regular graph G made of k disconnected clusters, we have seen that Uk = (c1, . . . , ck) up to a normalisation of the vectors. Therefore, νk = N 1/2/mini{N1/2i }, where Ni is the size of the ith cluster. If the clusters have the same size Ni = N/k then νk = k 1/2, the lower bound on νk. In this simple optimal scenario, sampling O(ν2k log k) = O(k log k) features is thus sufficient to recover the cluster indicator vectors.\nThe attentive reader will have noticed that for graphs where ν2k ≈ N , no downsampling is possible. Yet, a simple solution exists in this situation: variable density sampling. Indeed, it is proved in [24] that, whatever the graph G, there always exists an optimal sampling distribution such that n = O(k log k) samples are sufficient to satisfy Eq. (8). This distribution depends on the profile of the local cumulative coherence and can be estimated rapidly (see [24] for more details). In this paper, we only consider uniform sampling to simplify the explanations, but keep in mind that in practice results will always be improved if one uses variable density sampling. Note also that one cannot expect to sample less than k nodes to find k clusters. Up to the extra log(k), our result is optimal.\n4. CSC in practice\nWe have detailed the two fundamental theoretical notions supporting our algorithm, presented in Alg. 2 and discussed in Sec. 4.1. However, some steps in Alg. 2 still need to be clarified. In particular, Sec. 4.2 provides an extension of Theorem 3.2 that takes into account the use of a non-ideal low-pass filter (to handle the practical case where the order of the polynomial approximation is finite). This theorem in fine explains and justifies Step 4 of Alg. 2. Then, in Sec. 4.3, we discuss important details of the method such as the estimation of λk (Step 1) and the choice of the polynomial approximation (Step 2). We finish this section with complexity considerations.\nAlgorithm 2 Compressive Spectral Clustering\nInput: The Laplacian matrix L, the number of clusters k; and parameters typically set to n = 2k log k, d = 4 log n, p = 50 and γ = 10−3. 1· Estimate L’s k-th eigenvalue λk as in Sec. 4.3.\n2· Compute the polynomial approximation h̃λk of order p of the ideal low-pass filter hλk . 3· Generate d random Gaussian signals of mean 0 and variance 1/d: R = (r1|r2| · · · |rd) ∈ RN×d. 4· Filter R with H̃λk = h̃λk(L) as in Sec. 2.1 and define, for each node i, its feature vector f̃i ∈ Rd:\nf̃i = [( H̃λkR )ᵀ δi ]/∥∥∥(H̃λkR)ᵀ δi∥∥∥. 5· Generate a random sampling matrix M ∈ Rn×N as in Eq. (6) and keep only n feature vectors: (f̃ω1 | . . . |f̃ωn)ᵀ = M(f̃1| . . . |f̃N )ᵀ. 6· Run k-means on the reduced dataset with the Euclidean distance: D̃rij =\n∥∥∥f̃ωi − f̃ωj∥∥∥ to obtain k reduced indicator vectors crj ∈ Rn, one for each cluster. 7· Interpolate each reduced indicator vector crj with the optimisation problem of Eq. (7), to obtain the k indicator vectors c̃j ∗ ∈ RN on the full set of nodes."
    }, {
      "heading" : "4.1. Algorithm",
      "text" : "As for SC (see Sec. 2.2), the algorithm starts with the adjacency matrix W of a graph G. After computing its Laplacian L, the CSC algorithm is summarized in Alg. 2. The output c̃∗j (i) is not binary and in fact quantifies how much node i belongs to cluster j, useful for fuzzy partitioning. To obtain an exact partition of the nodes, we normalize each indicator vector c̃∗j , and assign node i to the\ncluster j for which c̃∗j (i)/ ∥∥c̃∗j∥∥ is maximal. This is the procedure that we follow in the experiments of Sec. 5."
    }, {
      "heading" : "4.2. Non-ideal filtering of random signals",
      "text" : "In this section, we improve Theorem 3.2 by studying how the error of the polynomial approximation h̃λk of hλk propagates to the spectral distance estimation, and by taking into account the fact that k-means is performed on the reduced set of features (f̃ω1 | . . . |f̃ωn)ᵀ = M(f̃1| . . . |f̃N )ᵀ. We denote by MYk ∈ Rn×k the ideal reduced feature matrix. We have (fω1 | · · · |fωn)ᵀ = M(f1| · · · |fN )ᵀ = MYk. The actual distances we want to estimate using random signals are thus, for all (i, j) ∈ {1, . . . , n}2\nDrij := ∥∥fωi − fωj∥∥ = ∥∥YᵀkMᵀ(δri − δrj )∥∥ ,\nwhere the {δri } are here Diracs in n dimensions. Consider the random matrix R = (r1|r2| · · · |rd) ∈ RN×d constructed as in Sec. 3.1. Its filtered, normalized and reduced version is MVkH̃λkR ∈ Rn×d. The new feature vector f̃ωi ∈ Rd associated to node ωi is thus\nf̃ωi = (MVkH̃λkR) ᵀδri .\nThe normalisation of Step 4 of Alg. 2 approximates fastly the action of Vk in the above equation. More details and justifications are provided in the “Important remark” at the end of this section. The Euclidean distance between any two features reads as\nD̃rij := ∥∥∥f̃ωi − f̃ωj∥∥∥ = ∥∥∥RᵀH̃ᵀλkVᵀkMᵀ(δri − δrj )∥∥∥ .\nWe now study how well D̃rij estimates D r ij .\nApproximation error. Denote e(λ) the approximation error of the ideal low-pass filter:\n∀λ ∈ [0, 2], e(λ) := h̃λk(λ)− hλk(λ).\nIn the form of graph filter operators, one has\nh̃λk(L) = H̃λk = Hλk + E = hλk(L) + e(L).\nWe model the error e using two parameters: e1 (resp. e2) the maximal error for λ 6 λk (resp. λ > λk). We have\ne1 := sup λ∈{λ1,...,λk} |e(λ)|, e2 := sup λ∈{λk+1,...,λN} |e(λ)|.\nThe resolution parameter. In some cases, the ideal reduced spectral distance Drij may be null. In such cases, approximating Drij = 0 using a non-ideal filter is not possible. In fact, non-ideal filtering introduces an irreducible error on the estimation of the feature vectors that is not possible to compensate in general. We thus introduce a resolution parameter Drmin below which the distances Drij do not need to be approximated exactly, but should remain below D r min (up to a tolerated error). Theorem 4.1 (General norm conservation theorem). Let Drmin ∈ ] 0, √ 2 ]\nbe a chosen resolution parameter. For any δ ∈ ]0, 1], β > 0, if d is larger than\n16(2 + β) δ2 − δ3/3 log n,\nthen, for all (i, j) ∈ {1, . . . , n}2,\n(1− δ)Drij 6 D̃rij 6 (1 + δ)Drij , if Drij > Drmin, and\nD̃rij < (1 + δ)D r min, if D r ij < D r min,\nwith probability at least 1− 2n−β provided that√ |e21 − e22| + √ 2 e2\nDrmin mini{vk(i)} 6\nδ\n2 + δ .(9)\nProof. The proof follows the general ideas of Theorem 3.2’s proof, with a more careful use of the Johnson-Lindenstrauss lemma that takes into account error propagation. Recall that:\nD̃rij := ∥∥∥f̃ωi − f̃ωj∥∥∥ = ∥∥∥RᵀH̃ᵀλkVᵀkMᵀδrij∥∥∥ ,\nwhere δrij = δ r i − δrj . Given that H̃λk = Hλk + E and using the triangle inequality in the definition of D̃rij , we obtain∥∥RᵀHᵀλkVᵀkMᵀδrij∥∥ − ∥∥RᵀEᵀVᵀkMᵀδrij∥∥ 6 D̃rij 6 ∥∥RᵀEᵀVᵀkMᵀδrij∥∥ + ∥∥RᵀHᵀλkVᵀkMᵀδrij∥∥ , We continue the proof by bounding\n∥∥RᵀHᵀλkVᵀkMᵀ δrij∥∥ and ∥∥RᵀEᵀVᵀkMᵀ δrij∥∥ separately. Let δ ∈]0, 1]. To bound\n∥∥RᵀHᵀλkVᵀkMᵀδrij∥∥, we set = δ/2 in Theorem 3.2. This proves that if d is larger than\nd0 = 16(2 + β)\nδ2 − δ3/3 log n,\nthen with probability at least 1− n−β ,( 1− δ\n2\n) Drij 6 ∥∥RᵀHᵀλkVᵀkMᵀδrij∥∥ 6 (1 + δ2 ) Drij ,\nfor all (i, j) ∈ {1, . . . , n}2. To bound ∥∥RᵀEᵀVᵀkMᵀδrij∥∥, we use Theorem 1.1 in [28]. This theorem proves that if d > d0, then with probability at least 1− n−β ,∥∥RᵀEᵀVᵀkMᵀδrij∥∥ 6 (1 + δ2 )∥∥EᵀVᵀkMᵀδrij∥∥ ,\nfor all (i, j) ∈ {1, . . . , n}2. Using the union bound and (10), we deduce that, with probability at least 1− 2n−β ,(\n1− δ 2\n) Drij − ( 1 + δ\n2 )∥∥EᵀVᵀkMᵀδrij∥∥ 6 D̃rij 6 (1 + δ2 )∥∥EᵀVᵀkMᵀδrij∥∥ + (1 + δ2 ) Drij ,\nfor all (i, j) ∈ {1, . . . , n}2 provided that d > d0.\nThen, as e is bounded by e1 on the first k eigenvalues of the spectrum and by e2 on the remaining ones, we have∥∥EᵀVᵀkMᵀ δrij∥∥2 = ∥∥Ue(Λ)UᵀVᵀkMᵀ δrij∥∥2 = ∥∥e(Λ)UᵀVᵀkMᵀ δrij∥∥2\n= N∑ l=1 e(λl) 2 ∣∣(MVkul)ᵀδrij∣∣2\n6 e21 k∑ l=1 ∣∣(MVkul)ᵀδrij∣∣2 + e22 N∑ l=k+1 ∣∣(MVkul)ᵀδrij∣∣2 = e21\n∥∥UᵀkVᵀkMᵀδrij∥∥2 + e22 (∥∥UᵀVᵀkMᵀδrij∥∥2 − ∥∥UᵀkVᵀkMᵀδrij∥∥2) = (e21 − e22)\n∥∥UᵀkVᵀkMᵀδrij∥∥2 + e22 ∥∥UᵀVᵀkMᵀδrij∥∥2 = (e21 − e22) (Drij)2 + e22\n∥∥VᵀkMᵀδrij∥∥2 6 (e21 − e22) (Drij)2 +\n2 e22 mini{vk(i)2} .\nThe last step follows from the fact that∥∥VᵀkMᵀ δrij∥∥2 = N∑ l=1 1 vk(l)2 ∣∣(Mᵀδrij)(l)∣∣2 = 1vk(ωi)2 + 1vk(ωj)2 6 2mini{vk(i)}2\nDefine, for all (i, j) ∈ {1, . . . , n}2: eij := √ |e21 − e22|Drij +\n√ 2e2\nmini{vk(i)} .\nThus, the above inequality may be rewritten as:∥∥EᵀVᵀkMᵀ δrij∥∥ 6 eij , for all (i, j) ∈ {1, . . . , n}2, which combined with (10) yields(\n1− δ 2\n) Drij − ( 1 + δ\n2\n) eij 6 D̃ r ij 6 ( 1 + δ\n2\n) eij + ( 1 + δ\n2\n) Drij ,\nfor all (i, j) ∈ {1, . . . , n}2, with probability at least 1− 2n−β provided that d > d0. Let us now separate two cases. In the case where Drij > D r min > 0, we have\neij = eij Drij Drij =\n(√ |e21 − e22|+\n√ 2e2\nDrij mini{vk(i)}\n) Drij\n6 (√ |e21 − e22|+\n√ 2e2\nDrmin mini{vk(i)}\n) Drij\n6 δ\n2 + δ Drij .\nprovided that Eq. (9) holds. Combining the last inequality with (10) proves the first part of the theorem.\nIn the case where Drij < D r min, we have eij < √ |e21 − e22|Drmin +\n√ 2 e2\nmini{vk(i)} 6\nδ\n2 + δ Drmin.\nprovided that Eq. (9) holds. Combining the last inequality with (10) terminates the proof.\nConsequence of Theorem 4.1. All distances smaller (resp. larger) than the chosen resolution parameter Drmin are estimated smaller than (1 + δ)D r min (resp. correctly estimated up to a relative error δ). Moreover, for a fixed distance estimation error δ, the lower we decide to fix Drmin, the lower\nshould also be the errors e1 and/or e2 to ensure that Eq. (9) still holds, which implies an increase of the order p of the polynomial approximation of the ideal filter hλk , and ultimately, that means a higher computation time for the filtering operation of the random signals.\nImportant remark. The feature matrix VkH̃λkR can be easily computed if one knows the cut-off value λk and the local cumulative coherences vk(i). Unfortunately, this is not the case in practice. We propose a solution to estimate λk in Sec. 4.3. To estimate vk(i), one can use the results in Sec. 4 of [24] showing that vk(i) = ‖Uᵀkδi‖ ≈ ‖(HλkR)ᵀδi‖. Thus, a practical way to estimate VkH̃λkR is to first compute H̃λkR and then normalize its rows to unit length, as done in Step 4 of Alg. 2."
    }, {
      "heading" : "4.3. Polynomial approximation and estimation of λk",
      "text" : "The polynomial approximation. Theorem 4.1 uses a separate control on e(λ) below λk (with e1) and above λk (with e2). To have such a control in practice, one would need to use rational filters (ratio of two polynomials) to approximate hλk . Such filters have been introduced in the graph context [33], but they involve another optimisation step that would burden our main message. We prefer to simplify our analysis by using polynomials for which only the maximal error can be controlled. We write\n(10) em := max(e1, e2) = sup λ∈{λ1,...,λN}\n|e(λ)| .\nIn this easier case, one can show that Theorem 4.1 is still valid if Eq. (9) is replaced by\n(11)\n√ 2 em\nDrmin mini{vk(i)} 6\nδ\n2 + δ .\nIn our experiments, we could follow [34] and use truncated Chebychev polynomials to approximate the ideal filter, as these polynomials are known to require a small degree to ensure a given tolerated maximal error em. We prefer to follow [35] who suggest to use Jackson-Chebychev polynomials: Chebychev polynomials to which are added damping multipliers to alleviate the unwanted Gibbs oscillations around the cut-off frequency λk.\nThe polynomial’s order p. For a fixed δ, Drmin, and mini{vk(i)}, one should use the JacksonChebychev polynomial of smallest order p∗ ensuring that em satisfies Eq. (11), in order to optimize the computation time while making sure that Theorem 4.1 applies. Studying p∗ theoretically without computing the Laplacian’s complete spectrum (see Eq. (10)) is beyond the scope of this paper. Experimentally, p = 50 yields good results (see Fig. 1c).\nEstimation of λk. The fast filtering step is based on the polynomial approximation of hλk , which is itself parametrized by λk. Unless we compute the first k eigenvectors of L, thereby partly loosing our efficiency edge on other methods, we cannot know the value of λk with infinite precision. To estimate it efficiently, we use Alg. 1 in [24], where an estimation of λk is obtained as a by-product. This estimation uses the fact that Tr(RᵀHᵀλkHλkR) = Tr(R ᵀUkU ᵀ kR) ≈ k with high probability. Lemma 5.3 of [36] shows that this eigencount estimator yields the exact result (i.e., k) with probability 1− δ provided that d > 24k log(2/δ). In practice, experiments in [35] show that fewer random signals are sufficient in some cases. Inspired by these results and as in [24], we choose 2 logN random signals for this estimation task. Starting from λ = 2 and using the above estimator, the algorithm proceeds by dichotomy on λ until the interval [0, λ] contains k eigenvalues. For computational efficiency, we perform this eigencount task with Jackson-Chebychev polynomial approximation of the ideal low-pass filters."
    }, {
      "heading" : "4.4. Complexity considerations",
      "text" : "The complexity of steps 2, 3 and 5 of Alg. 2 are not detailed as they are insignificant compared to the others. First, note that fast filtering a graph signal costs O(p#E).4 Therefore, Step 1 costs O(p#E logN) per iteration of the dichotomy, and Step 4 costs O(p#E log n) (as d = O(log n)). Step 7 requires to solve Eq. (7) with the polynomial approximation of gλk(λ) = 1 − hλk(λ). When solved, e.g., by conjugate gradient or gradient descent, this step costs a fast filtering operation per\n4Recall that p is the order of the polynomial filter.\niteration of the solver and for each of the k classes. Step 7 thus costs O(p#Ek). Also, the complexity of k-means to cluster Q feature vectors of dimension r into k classes is O(kQr) per iteration. Therefore, Step 6 with Q = n and r = d = O(log(n)) costs O(kn log n). CSC’s complexity is thus O (kn log n+ p#E (logN + log n+ k)) . In practice, we are interested in sparse graphs: #E = O(N). Using the fact that n = O(k log k), CSC’s complexity simplifies to\nO ( k2 log2 k + pN (logN + k) ) .\nSC’s k-means step has a complexity of O(Nk2) per iteration. In many cases5 this sole task is more expensive than the CSC algorithm. On top of this, SC has the additional complexity of computing the first k eigenvectors of L, for which the cost of ARPACK - a popular eigenvalue solver - is at least O(k3) (see details in Sec. 3.2 of [37]). In general, the complexity is much larger with additional terms growing linearly with N .\nThis study suggests that CSC is faster than SC for large N and/or k. The above algorithms’ number of iterations are not taken into account as they are difficult to predict theoretically. Yet, the following experiments confirm the superiority of CSC over SC in terms of computational time.\n5. Experiments\nWe first perform well-controlled experiments on the Stochastic Block Model (SBM), a model of random graphs with community structures, that was showed suitable as a benchmark for SC in [38]. We also show performance results on a large real-world network. Implementation was done in Matlab R2015a, using the built-in function kmeans with 20 replicates, and the function eigs for SC. Experiments were done on a laptop with a 2.60 GHz Intel i7 dual-core processor running OS Fedora release 22 with 16 GB of RAM. The fast filtering part of CSC uses the gsp cheby op function of the GSP toolbox [39]. Equation (7) is solved using Matlab’s gmres function. All our results may be reproduced with the toolbox provided here [40]."
    }, {
      "heading" : "5.1. The Stochastic Block Model",
      "text" : "What distinguishes the SBM from Erdos-Renyi graphs is that the probability of connection between two nodes i and j is not uniform, but depends on the community label of i and j. More precisely, the probability of connection between nodes i and j equals q1 if they are in the same community, and q2 if not. In a first approach, we look at graphs with k communities, all of same size N/k. Furthermore, instead of considering the probabilities, one may fully characterize a SBM by providing their ratio = q2q1 , as well as the average degree s of the graph. The larger , the more difficult the community structure’s detection. In fact, Decelle et al. [41] show that a critical value c exists above which community detection is impossible at the large N limit: c = (s− √ s)/(s+ √ s(k − 1))."
    }, {
      "heading" : "5.2. Performance results",
      "text" : "In Figs. 1 a-d), we compare the recovery performance of CSC versus SC for different parameters. The performance is measured by the Adjusted Rand similarity index [42] between the SBM’s ground truth and the obtained partitions. It varies between −1 and 1. The higher it is, the better is the reconstruction. These figures show that the performance of CSC saturates at the default values of n, d, p and γ (see top of Alg. 2).\nWe also perform experiments on a SBM with N = 103, k = 20, s = 16 and hetereogeneous community sizes. More specifically, the list of community sizes is chosen to be: 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 50, 55, 60, 65, 70, 75, 80, 85, 90 and 95 nodes. In this scenario, there is no theoretical value of over which it is proven that recovery is impossible in the large N limit. Instead, we vary between 0 and 0.2 and show the recovery performance results with respect to n, d, p and γ in Fig. 2. Results are similar to the homogeneous case presented in Figs. 1 a-d).\nWe show in Fig. 1 e) the estimation results of λk for different values of . We see that it is overestimated in the SBM context. As long as the estimated value stays under λk+1, this overestimation\n5Roughly, all cases for which k2 > p(logN + k).\ndoes not have a strong impact on the method. On the other hand, as becomes larger than ∼ 0.06, our estimation of λk is larger than λk+1, which means that our feature vectors start to integrate some unwanted information from eigenvectors outside of span(Uk). Even though the impact of this additional information is application-dependent and in some cases insignificant, further efforts to improve the estimation of λk would be beneficial to our proposition.\nIn Figs. 1 f-g) we fix to c/4, n, d, p and γ to the values given in Alg. 2, and vary N and k. We compare the recovery performance and the time of computation of CSC, SC and Boutsidis’ power method [8]. The power method (PM), in a nutshell, 1) applies the Laplacian matrix to the power r to k random signals, 2) computes the left singular vectors of the N × k obtained matrix, to extract feature vectors, 3) applies k-means in high-dimension (like SC) with these feature vectors. In our experiments, we use r = 10. The recovery performances are nearly identical in all situations, even though CSC is only a few percents under SC and PM (Fig. f is zoomed around the high values of the recovery score). For the time of computation, the experiments confirm that all three methods are roughly linear in N and polynomial in k (Fig. g is plotted in log-log), with a lower exponent for CSC than for SC and PM; such that SC and PM are faster for k = 20 but CSC becomes up to an order of magnitude faster as k increases to 200. Note that the SBM is favorable to SC as Matlab’s function eigs converges very fast in this case, e.g., for N = 105, it finds the first k = 200 eigenvectors in less\nthan 2 minutes! PM sidesteps successfully the cost of eigs, but the cost of k-means in high-dimension is still a strong bottleneck.\nWe finally compare CSC and SC on a real-world dataset: the Amazon co-purchasing network [43]. It is an undirected connected graph comprising N = 334 863 nodes and #E = 925 872 edges. The results are presented in Fig.1 h) for three values of k. As there is no clear ground truth in this case, we use the modularity [44] to measure the algorithm’s clustering performance, a well-known cost function that measures how well a given partition separates a network in different communities. Note that the 20 replicates of k-means would not converge for SC with the default maximum number of iterations set to 100. For a fair comparison with CSC, we used only 2 replicates with a maximum number of iterations set to 1000 for SC’s k-means step. We see that for the same clustering performance, CSC is much faster than SC, especially as k increases. The PM algorithm on this dataset does not perform well: even though the features are estimated quickly, they apparently do not form clear classes such that its k-means step takes even longer than SC’s. For the three values of k, we stopped the PM algorithm after a time of computation exceeding SC’s.\n6. Conclusion\nBy graph filtering O(log k) random signals with polynomial filters approximating the low-pass ideal filter, we are able to construct feature vectors whose interdistances approach the standard SC feature distances. Then, building upon compressive sensing results, we show that one can sample O(k log k) nodes from the set of N nodes, cluster this reduced set of nodes and interpolate the result back to the whole graph. If the low-dimensional k-means result is correct, i.e., if Eq. (5) is verified, we guarantee that the interpolation is a good approximation of the SC result. To improve the clustering result of the reduced set of nodes, one could build upon the concept of community cores [45]. In fact, as the filtering and the low-dimensional clustering steps are fairly cheap to compute, one could repeat these steps for different random signals, keep the sets of nodes that are always classified together and use only these stable “cores” for interpolation. Also, note that the SC algorithm based on the combinatorial Laplacian L = D −W can also be successfully approximated by CSC. Using this Laplacian, the normalisation step of SC’s algorithm (Step 2 of Alg. 1) is no longer required [27]. To adapt CSC to this version of SC, one only needs to skip the normalisation step of CSC: simply change\nstep 4 of Alg.2 to f̃i = ( H̃λkR )ᵀ δi. In fact, the Fourier matrix (the eigenvector matrix of L) is still\northogonal in this case and all proofs stay valid. On the other hand, for the version of SC based on the random walk Laplacian L = I− D−1W [3], the Fourier matrix is no longer orthogonal, its inverse is no longer equal to its transpose, and the sampling theorems of [24] need yet to be extended to this case. Nevertheless, even without such potential improvements, our experiments show that CSC proves efficient and accurate in synthetic and real-world datasets; and could be preferred to SC for large N and/or k.\n. References\n[1] M. Nascimento and A. de Carvalho, “Spectral methods for graph clustering a survey,” European Journal of Operational Research, vol. 211, no. 2, pp. 221 – 231, 2011. [2] A. Ng, M. Jordan, and Y. Weiss, “On spectral clustering: Analysis and an algorithm,” in Advances in Neural\nInformation Processing Systems 14, T. Dietterich, S. Becker, and Z. Ghahramani, Eds. MIT Press, 2002, pp. 849–856. [3] J. Shi and J. Malik, “Normalized cuts and image segmentation,” Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 22, no. 8, pp. 888–905, 2000. [4] M. Belkin and P. Niyogi, “Laplacian eigenmaps for dimensionality reduction and data representation,” Neural\ncomputation, vol. 15, no. 6, pp. 1373–1396, 2003. [5] L. Zelnik-Manor and P. Perona, “Self-tuning spectral clustering,” in Advances in neural information processing\nsystems, 2004, pp. 1601–1608.\n[6] M. Fiedler, “Algebraic connectivity of graphs,” Czechoslovak mathematical journal, vol. 23, no. 2, pp. 298–305, 1973. [7] S. White and P. Smyth, “A spectral clustering approach to finding communities in graph.” in SDM, vol. 5. SIAM,\n2005, pp. 76–84.\n[8] K. P. Boutsidis, C. and A. Gittens, “Spectral clustering via the power method - provably.” in Proceedings of the\n32nd International Conference on Machine Learning (ICML-15), Lille, France, 2015, pp. 40–48.\n[9] F. Lin and W. W. Cohen, “Power iteration clustering.” in Proceedings of the 27th International Conference on Machine Learning (ICML-10), Haifa, Israel, 2010, pp. 655–662. [10] T.-Y. Liu, H.-Y. Yang, X. Zheng, T. Qin, and W.-Y. Ma, “Fast large-scale spectral clustering by sequential shrinkage optimization,” in Advances in Information Retrieval, 2007, pp. 319–330. [11] C. Fowlkes, S. Belongie, F. Chung, and J. Malik, “Spectral grouping using the nystrom method,” Pattern Analysis\nand Machine Intelligence, IEEE Transactions on, vol. 26, no. 2, pp. 214–225, 2004. [12] L. Wang, C. Leckie, K. Ramamohanarao, and J. Bezdek, “Approximate spectral clustering,” in Advances in\nKnowledge Discovery and Data Mining, 2009, pp. 134–146.\n[13] X. Chen and D. Cai, “Large scale spectral clustering with landmark-based representation.” in Proceedings of the 25th AAAI Conference on Artificial Intelligence, 2011. [14] T. Sakai and A. Imiya, “Fast spectral clustering with random projection and sampling,” in Machine Learning and\nData Mining in Pattern Recognition, 2009, pp. 372–384. [15] D. Yan, L. Huang, and M. Jordan, “Fast approximate spectral clustering,” in Proceedings of the 15th ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data Mining, ser. KDD ’09, New York, NY, USA, 2009,\npp. 907–916. [16] I. Dhillon, Y. Guan, and B. Kulis, “Weighted graph cuts without eigenvectors a multilevel approach,” Pattern\nAnalysis and Machine Intelligence, IEEE Transactions on, vol. 29, no. 11, pp. 1944–1957, 2007. [17] M. Filippone, F. Camastra, F. Masulli, and S. Rovetta, “A survey of kernel and spectral methods for clustering,”\nPattern Recognition, vol. 41, no. 1, pp. 176 – 190, 2008.\n[18] C. Boutsidis, A. Zouzias, M. W. Mahoney, and P. Drineas, “Stochastic dimensionality reduction for k-means clustering,” arXiv, abs/1110.2897, 2011. [19] M. B. Cohen, S. Elder, C. Musco, C. Musco, and M. Persu, “Dimensionality reduction for k-means clustering and\nlow rank approximation,” in Proceedings of the 47th Annual ACM on Symposium on Theory of Computing. ACM, 2015, pp. 163–172. [20] D. Shuman, S. Narang, P. Frossard, A. Ortega, and P. Vandergheynst, “The emerging field of signal processing\non graphs: Extending high-dimensional data analysis to networks and other irregular domains,” Signal Processing Magazine, IEEE, vol. 30, no. 3, pp. 83–98, 2013. [21] A. Sandryhaila and J. Moura, “Big data analysis with signal processing on graphs: Representation and processing\nof massive data sets with irregular structure,” Signal Processing Magazine, IEEE, vol. 31, no. 5, pp. 80–90, 2014. [22] N. Tremblay, G. Puy, P. Borgnat, R. Gribonval, and P. Vandergheynst, “Accelerated spectral clustering using graph\nfiltering of random signals,” in Acoustics, Speech and Signal Processing (ICASSP), IEEE International Conference\non, 2016, accepted. [23] D. Ramasamy and U. Madhow, “Compressive spectral embedding: sidestepping the SVD,” in Advances in Neural\nInformation Processing Systems 28, 2015, pp. 550–558. [24] G. Puy, N. Tremblay, R. Gribonval, and P. Vandergheynst, “Random sampling of bandlimited signals on graphs,”\narXiv, vol. abs/1511.05118, 2015.\n[25] F. Chung, Spectral graph theory. Amer Mathematical Society, 1997, no. 92. [26] A. Sandryhaila and J. Moura, “Discrete signal processing on graphs,” Signal Processing, IEEE Transactions on,\nvol. 61, no. 7, pp. 1644–1656, 2013.\n[27] U. von Luxburg, “A tutorial on spectral clustering,” Statistics and Computing, vol. 17, no. 4, pp. 395–416, 2007. [28] D. Achlioptas, “Database-friendly random projections: Johnson-lindenstrauss with binary coins,” Journal of Com-\nputer and System Sciences, vol. 66, no. 4, pp. 671 – 687, 2003.\n[29] S. Chen, R. Varma, A. Sandryhaila, and J. Kovacevic, “Discrete signal processing on graphs: Sampling theory,” Signal Processing, IEEE Transactions on, vol. 63, no. 24, pp. 6510–6523, 2015. [30] A. Anis, A. Gadde, and A. Ortega, “Efficient sampling set selection for bandlimited graph signals using graph\nspectral proxies,” arXiv, vol. abs/1510.00297, 2015. [31] M. Tsitsvero, S. Barbarossa, and P. D. Lorenzo, “Signals on graphs: Uncertainty principle and sampling,” arXiv,\nvol. abs/1507.08822, 2015. [32] A. Marques, S. Segarra, G. Leus, and A. Ribeiro, “Sampling of graph signals with successive local aggregations,”\nSignal Processing, IEEE Transactions on, vol. PP, no. 99, pp. 1–1, 2015. [33] X. Shi, H. Feng, M. Zhai, T. Yang, and B. Hu, “Infinite impulse response graph filters in wireless sensor networks,”\nSignal Processing Letters, IEEE, vol. 22, no. 8, pp. 1113–1117, 2015.\n[34] D. Shuman, P. Vandergheynst, and P. Frossard, “Chebyshev polynomial approximation for distributed signal\nprocessing,” in Distributed Computing in Sensor Systems and Workshops (DCOSS), International Conference on, 2011, pp. 1–8. [35] E. D. Napoli, E. Polizzi, and Y. Saad, “Efficient estimation of eigenvalue counts in an interval,” arXiv, vol. abs/1308.4275, 2013. [36] H. Avron and S. Toledo, “Randomized algorithms for estimating the trace of an implicit symmetric positive semi-\ndefinite matrix,” J. ACM, vol. 58, no. 2, pp. 8:1–8:34, 2011. [37] W.-Y. Chen, Y. Song, H. Bai, L. C.-J, and E. Chang, “Parallel spectral clustering in distributed systems,” Pattern\nAnalysis and Machine Intelligence, IEEE Transactions on, vol. 33, no. 3, pp. 568–586, 2011.\n[38] J. Lei and A. Rinaldo, “Consistency of spectral clustering in stochastic block models,” Ann. Statist., vol. 43, no. 1,\npp. 215–237, 2015.\n[39] N. Perraudin, J. Paratte, D. Shuman, V. Kalofolias, P. Vandergheynst, and D. Hammond, “Gspbox: A toolbox for signal processing on graphs,” arXiv, vol. abs/1408.5781, 2014. [40] http://perso.ens-lyon.fr/nicolas.tremblay/index.php?page=downloads. [41] A. Decelle, F. Krzakala, C. Moore, and L. Zdeborová, “Asymptotic analysis of the stochastic block model for\nmodular networks and its algorithmic applications,” Phys. Rev. E, vol. 84, p. 066106, 2011.\n[42] L. Hubert and P. Arabie, “Comparing partitions,” Journal of classification, vol. 2, no. 1, pp. 193–218, 1985. [43] J. Yang and J. Leskovec, “Defining and evaluating network communities based on ground-truth,” Knowledge and\nInformation Systems, vol. 42, no. 1, pp. 181–213, 2015.\n[44] M. E. J. Newman and M. Girvan, “Finding and evaluating community structure in networks,” Phys. Rev. E, vol. 69, p. 026113, 2004. [45] M. Seifi, I. Junier, J.-B. Rouquier, S. Iskrov, and J.-L. Guillaume, “Stable community cores in complex networks,”\nin Complex Networks, 2013, pp. 87–98."
    } ],
    "references" : [ {
      "title" : "Spectral methods for graph clustering a survey",
      "author" : [ "M. Nascimento", "A. de Carvalho" ],
      "venue" : "European Journal of Operational Research, vol. 211, no. 2, pp. 221 – 231, 2011.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "On spectral clustering: Analysis and an algorithm",
      "author" : [ "A. Ng", "M. Jordan", "Y. Weiss" ],
      "venue" : "Advances in Neural Information Processing Systems 14, T. Dietterich, S. Becker, and Z. Ghahramani, Eds. MIT Press, 2002, pp. 849–856.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Normalized cuts and image segmentation",
      "author" : [ "J. Shi", "J. Malik" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 22, no. 8, pp. 888–905, 2000.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Laplacian eigenmaps for dimensionality reduction and data representation",
      "author" : [ "M. Belkin", "P. Niyogi" ],
      "venue" : "Neural computation, vol. 15, no. 6, pp. 1373–1396, 2003.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Self-tuning spectral clustering",
      "author" : [ "L. Zelnik-Manor", "P. Perona" ],
      "venue" : "Advances in neural information processing systems, 2004, pp. 1601–1608.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Algebraic connectivity of graphs",
      "author" : [ "M. Fiedler" ],
      "venue" : "Czechoslovak mathematical journal, vol. 23, no. 2, pp. 298–305, 1973.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1973
    }, {
      "title" : "A spectral clustering approach to finding communities in graph.",
      "author" : [ "S. White", "P. Smyth" ],
      "venue" : "in SDM,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2005
    }, {
      "title" : "Spectral clustering via the power method - provably.",
      "author" : [ "C.K.P. Boutsidis", "A. Gittens" ],
      "venue" : "Proceedings of the 32nd International Conference on Machine Learning (ICML-15),",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2015
    }, {
      "title" : "Power iteration clustering.",
      "author" : [ "F. Lin", "W.W. Cohen" ],
      "venue" : "Proceedings of the 27th International Conference on Machine Learning (ICML-10), Haifa, Israel,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2010
    }, {
      "title" : "Fast large-scale spectral clustering by sequential shrinkage optimization",
      "author" : [ "T.-Y. Liu", "H.-Y. Yang", "X. Zheng", "T. Qin", "W.-Y. Ma" ],
      "venue" : "Advances in Information Retrieval, 2007, pp. 319–330.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Spectral grouping using the nystrom method",
      "author" : [ "C. Fowlkes", "S. Belongie", "F. Chung", "J. Malik" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 26, no. 2, pp. 214–225, 2004.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Approximate spectral clustering",
      "author" : [ "L. Wang", "C. Leckie", "K. Ramamohanarao", "J. Bezdek" ],
      "venue" : "Advances in Knowledge Discovery and Data Mining, 2009, pp. 134–146.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Large scale spectral clustering with landmark-based representation.",
      "author" : [ "X. Chen", "D. Cai" ],
      "venue" : "Proceedings of the 25th AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2011
    }, {
      "title" : "Fast spectral clustering with random projection and sampling",
      "author" : [ "T. Sakai", "A. Imiya" ],
      "venue" : "Machine Learning and Data Mining in Pattern Recognition, 2009, pp. 372–384.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Fast approximate spectral clustering",
      "author" : [ "D. Yan", "L. Huang", "M. Jordan" ],
      "venue" : "Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ser. KDD ’09, New York, NY, USA, 2009, pp. 907–916.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Weighted graph cuts without eigenvectors a multilevel approach",
      "author" : [ "I. Dhillon", "Y. Guan", "B. Kulis" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 29, no. 11, pp. 1944–1957, 2007.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1944
    }, {
      "title" : "A survey of kernel and spectral methods for clustering",
      "author" : [ "M. Filippone", "F. Camastra", "F. Masulli", "S. Rovetta" ],
      "venue" : "Pattern Recognition, vol. 41, no. 1, pp. 176 – 190, 2008.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Stochastic dimensionality reduction for k-means clustering",
      "author" : [ "C. Boutsidis", "A. Zouzias", "M.W. Mahoney", "P. Drineas" ],
      "venue" : "arXiv, abs/1110.2897, 2011.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Dimensionality reduction for k-means clustering and low rank approximation",
      "author" : [ "M.B. Cohen", "S. Elder", "C. Musco", "C. Musco", "M. Persu" ],
      "venue" : "Proceedings of the 47th Annual ACM on Symposium on Theory of Computing. ACM, 2015, pp. 163–172.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains",
      "author" : [ "D. Shuman", "S. Narang", "P. Frossard", "A. Ortega", "P. Vandergheynst" ],
      "venue" : "Signal Processing Magazine, IEEE, vol. 30, no. 3, pp. 83–98, 2013.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Big data analysis with signal processing on graphs: Representation and processing of massive data sets with irregular structure",
      "author" : [ "A. Sandryhaila", "J. Moura" ],
      "venue" : "Signal Processing Magazine, IEEE, vol. 31, no. 5, pp. 80–90, 2014.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Accelerated spectral clustering using graph filtering of random signals",
      "author" : [ "N. Tremblay", "G. Puy", "P. Borgnat", "R. Gribonval", "P. Vandergheynst" ],
      "venue" : "Acoustics, Speech and Signal Processing (ICASSP), IEEE International Conference on, 2016, accepted.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Compressive spectral embedding: sidestepping the SVD",
      "author" : [ "D. Ramasamy", "U. Madhow" ],
      "venue" : "Advances in Neural Information Processing Systems 28, 2015, pp. 550–558.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Random sampling of bandlimited signals on graphs",
      "author" : [ "G. Puy", "N. Tremblay", "R. Gribonval", "P. Vandergheynst" ],
      "venue" : "arXiv, vol. abs/1511.05118, 2015.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Spectral graph theory",
      "author" : [ "F. Chung" ],
      "venue" : "Amer Mathematical Society,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 1997
    }, {
      "title" : "Discrete signal processing on graphs",
      "author" : [ "A. Sandryhaila", "J. Moura" ],
      "venue" : "Signal Processing, IEEE Transactions on, vol. 61, no. 7, pp. 1644–1656, 2013.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A tutorial on spectral clustering",
      "author" : [ "U. von Luxburg" ],
      "venue" : "Statistics and Computing, vol. 17, no. 4, pp. 395–416, 2007.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Database-friendly random projections: Johnson-lindenstrauss with binary coins",
      "author" : [ "D. Achlioptas" ],
      "venue" : "Journal of Computer and System Sciences, vol. 66, no. 4, pp. 671 – 687, 2003.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Discrete signal processing on graphs: Sampling theory",
      "author" : [ "S. Chen", "R. Varma", "A. Sandryhaila", "J. Kovacevic" ],
      "venue" : "Signal Processing, IEEE Transactions on, vol. 63, no. 24, pp. 6510–6523, 2015.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Efficient sampling set selection for bandlimited graph signals using graph spectral proxies",
      "author" : [ "A. Anis", "A. Gadde", "A. Ortega" ],
      "venue" : "arXiv, vol. abs/1510.00297, 2015.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Signals on graphs: Uncertainty principle and sampling",
      "author" : [ "M. Tsitsvero", "S. Barbarossa", "P.D. Lorenzo" ],
      "venue" : "arXiv, vol. abs/1507.08822, 2015.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Sampling of graph signals with successive local aggregations",
      "author" : [ "A. Marques", "S. Segarra", "G. Leus", "A. Ribeiro" ],
      "venue" : "Signal Processing, IEEE Transactions on, vol. PP, no. 99, pp. 1–1, 2015.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Infinite impulse response graph filters in wireless sensor networks",
      "author" : [ "X. Shi", "H. Feng", "M. Zhai", "T. Yang", "B. Hu" ],
      "venue" : "Signal Processing Letters, IEEE, vol. 22, no. 8, pp. 1113–1117, 2015.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Chebyshev polynomial approximation for distributed signal processing",
      "author" : [ "D. Shuman", "P. Vandergheynst", "P. Frossard" ],
      "venue" : "Distributed Computing in Sensor Systems and Workshops (DCOSS), International Conference on, 2011, pp. 1–8.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Efficient estimation of eigenvalue counts in an interval",
      "author" : [ "E.D. Napoli", "E. Polizzi", "Y. Saad" ],
      "venue" : "arXiv, vol. abs/1308.4275, 2013.",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Randomized algorithms for estimating the trace of an implicit symmetric positive semidefinite matrix",
      "author" : [ "H. Avron", "S. Toledo" ],
      "venue" : "J. ACM, vol. 58, no. 2, pp. 8:1–8:34, 2011.",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Parallel spectral clustering in distributed systems",
      "author" : [ "W.-Y. Chen", "Y. Song", "H. Bai", "L.C.-J", "E. Chang" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 33, no. 3, pp. 568–586, 2011.  Compressive spectral clustering  15",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Consistency of spectral clustering in stochastic block models",
      "author" : [ "J. Lei", "A. Rinaldo" ],
      "venue" : "Ann. Statist., vol. 43, no. 1, pp. 215–237, 2015.",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Gspbox: A toolbox for signal processing on graphs",
      "author" : [ "N. Perraudin", "J. Paratte", "D. Shuman", "V. Kalofolias", "P. Vandergheynst", "D. Hammond" ],
      "venue" : "arXiv, vol. abs/1408.5781, 2014.",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Asymptotic analysis of the stochastic block model for modular networks and its algorithmic applications",
      "author" : [ "A. Decelle", "F. Krzakala", "C. Moore", "L. Zdeborová" ],
      "venue" : "Phys. Rev. E, vol. 84, p. 066106, 2011.",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Comparing partitions",
      "author" : [ "L. Hubert", "P. Arabie" ],
      "venue" : "Journal of classification, vol. 2, no. 1, pp. 193–218, 1985.",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 1985
    }, {
      "title" : "Defining and evaluating network communities based on ground-truth",
      "author" : [ "J. Yang", "J. Leskovec" ],
      "venue" : "Knowledge and Information Systems, vol. 42, no. 1, pp. 181–213, 2015.",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Finding and evaluating community structure in networks",
      "author" : [ "M.E.J. Newman", "M. Girvan" ],
      "venue" : "Phys. Rev. E, vol. 69, p. 026113, 2004.",
      "citeRegEx" : "44",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Stable community cores in complex networks",
      "author" : [ "M. Seifi", "I. Junier", "J.-B. Rouquier", "S. Iskrov", "J.-L. Guillaume" ],
      "venue" : "Complex Networks, 2013, pp. 87–98.",
      "citeRegEx" : "45",
      "shortCiteRegEx" : null,
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Spectral clustering (SC) is a fundamental tool in data mining [1].",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 1,
      "context" : ", [2–5], but all follow the same scheme.",
      "startOffset" : 2,
      "endOffset" : 7
    }, {
      "referenceID" : 2,
      "context" : ", [2–5], but all follow the same scheme.",
      "startOffset" : 2,
      "endOffset" : 7
    }, {
      "referenceID" : 3,
      "context" : ", [2–5], but all follow the same scheme.",
      "startOffset" : 2,
      "endOffset" : 7
    }, {
      "referenceID" : 4,
      "context" : ", [2–5], but all follow the same scheme.",
      "startOffset" : 2,
      "endOffset" : 7
    }, {
      "referenceID" : 5,
      "context" : "This k-way scheme is a generalisation of Fiedler’s pioneering work [6].",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 6,
      "context" : ", concentric circles) for which naive k-means clustering fails; 2) if the input data is directly a graph G modeling a network [7], such as social, neuronal, or transportation networks.",
      "startOffset" : 126,
      "endOffset" : 129
    }, {
      "referenceID" : 7,
      "context" : ", via the power method [8, 9], via a careful optimisation of diagonalisation algorithms in the context of SC [10], or via matrix column-subsampling such as in the Nyström method [11], the nSPEC and cSPEC methods of [12], or in [13, 14].",
      "startOffset" : 23,
      "endOffset" : 29
    }, {
      "referenceID" : 8,
      "context" : ", via the power method [8, 9], via a careful optimisation of diagonalisation algorithms in the context of SC [10], or via matrix column-subsampling such as in the Nyström method [11], the nSPEC and cSPEC methods of [12], or in [13, 14].",
      "startOffset" : 23,
      "endOffset" : 29
    }, {
      "referenceID" : 9,
      "context" : ", via the power method [8, 9], via a careful optimisation of diagonalisation algorithms in the context of SC [10], or via matrix column-subsampling such as in the Nyström method [11], the nSPEC and cSPEC methods of [12], or in [13, 14].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 10,
      "context" : ", via the power method [8, 9], via a careful optimisation of diagonalisation algorithms in the context of SC [10], or via matrix column-subsampling such as in the Nyström method [11], the nSPEC and cSPEC methods of [12], or in [13, 14].",
      "startOffset" : 178,
      "endOffset" : 182
    }, {
      "referenceID" : 11,
      "context" : ", via the power method [8, 9], via a careful optimisation of diagonalisation algorithms in the context of SC [10], or via matrix column-subsampling such as in the Nyström method [11], the nSPEC and cSPEC methods of [12], or in [13, 14].",
      "startOffset" : 215,
      "endOffset" : 219
    }, {
      "referenceID" : 12,
      "context" : ", via the power method [8, 9], via a careful optimisation of diagonalisation algorithms in the context of SC [10], or via matrix column-subsampling such as in the Nyström method [11], the nSPEC and cSPEC methods of [12], or in [13, 14].",
      "startOffset" : 227,
      "endOffset" : 235
    }, {
      "referenceID" : 13,
      "context" : ", via the power method [8, 9], via a careful optimisation of diagonalisation algorithms in the context of SC [10], or via matrix column-subsampling such as in the Nyström method [11], the nSPEC and cSPEC methods of [12], or in [13, 14].",
      "startOffset" : 227,
      "endOffset" : 235
    }, {
      "referenceID" : 14,
      "context" : "One can find such methods in [15] and [12]’s eSPEC proposition, where two different interpolation methods are used.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 11,
      "context" : "One can find such methods in [15] and [12]’s eSPEC proposition, where two different interpolation methods are used.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 15,
      "context" : "Also, let us mention [16] that circumvents both the eigendecomposition and the k-means bottlenecks: the authors reduce the graph’s size by successive aggregation of nodes, apply SC on this small graph, and propagate the results on the complete graph using kernel k-means to control interpolation errors.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 16,
      "context" : "The kernel is computed so that kernel k-means and SC share the same objective function [17].",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 17,
      "context" : "works [18, 19] that concentrate on reducing the feature vectors’ dimension in the k-means problem, but do not sidestep the eigendecomposition nor the large N issues.",
      "startOffset" : 6,
      "endOffset" : 14
    }, {
      "referenceID" : 18,
      "context" : "works [18, 19] that concentrate on reducing the feature vectors’ dimension in the k-means problem, but do not sidestep the eigendecomposition nor the large N issues.",
      "startOffset" : 6,
      "endOffset" : 14
    }, {
      "referenceID" : 19,
      "context" : "In this work, inspired by recent advances in the emerging field of graph signal processing [20, 21], we circumvent SC’s last two bottlenecks and detail a fast approximate spectral clustering method for large datasets, as well as the supporting theory.",
      "startOffset" : 91,
      "endOffset" : 99
    }, {
      "referenceID" : 20,
      "context" : "In this work, inspired by recent advances in the emerging field of graph signal processing [20, 21], we circumvent SC’s last two bottlenecks and detail a fast approximate spectral clustering method for large datasets, as well as the supporting theory.",
      "startOffset" : 91,
      "endOffset" : 99
    }, {
      "referenceID" : 21,
      "context" : "The first ingredient builds upon recent works [22,23] that avoid the costly computation of the eigenvectors of L by filtering O(log(k)) random signals on G that will then serve as feature vectors to perform clustering.",
      "startOffset" : 46,
      "endOffset" : 53
    }, {
      "referenceID" : 22,
      "context" : "The first ingredient builds upon recent works [22,23] that avoid the costly computation of the eigenvectors of L by filtering O(log(k)) random signals on G that will then serve as feature vectors to perform clustering.",
      "startOffset" : 46,
      "endOffset" : 53
    }, {
      "referenceID" : 23,
      "context" : "The second ingredient uses a recent sampling theory of bandlimited graph-signals [24] to reduce the computational complexity of k-means.",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 24,
      "context" : "6 λN 6 2 [25], and of the orthonormal matrix U := (u1|u2| .",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 19,
      "context" : "By analogy to the continuous Laplacian operator whose eigenfunctions are the classical Fourier modes and eigenvalues their squared frequencies, the columns of U are considered as the graph’s Fourier modes, and { √ λl}l as its set of associated “frequencies” [20].",
      "startOffset" : 258,
      "endOffset" : 262
    }, {
      "referenceID" : 25,
      "context" : ", [26], but in order to exhibit the link between graph signal processing and SC (that partially diagonalizes the Laplacian matrix), the Laplacian-based Fourier matrix appears more natural.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 1,
      "context" : "Given a continuous filter function h defined on [0, 2], its associated graph filter operator H ∈ RN×N is defined as H := h(L) = Uh(Λ)UT, where h(Λ) := diag(h(λ1), h(λ2), · · · , h(λN )).",
      "startOffset" : 48,
      "endOffset" : 54
    }, {
      "referenceID" : 1,
      "context" : "In the following, we consider ideal low-pass filters, denoted by hλc , that satisfy, for all λ ∈ [0, 2], hλc(λ) = 1, if λ 6 λc, and hλc(λ) = 0, if not.",
      "startOffset" : 97,
      "endOffset" : 103
    }, {
      "referenceID" : 1,
      "context" : "Algorithm 1 Spectral Clustering [2]",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 1,
      "context" : "l=0 αlλ l ' h(λ) for all λ ∈ [0, 2], where α1, .",
      "startOffset" : 29,
      "endOffset" : 35
    }, {
      "referenceID" : 1,
      "context" : "’s method [2] based on the normalized Laplacian as our standard SC method.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 21,
      "context" : "A theorem recently published independently by two teams [22, 23] shows that this is possible when there is no normalisation step (step 2 in Alg.",
      "startOffset" : 56,
      "endOffset" : 64
    }, {
      "referenceID" : 22,
      "context" : "A theorem recently published independently by two teams [22, 23] shows that this is possible when there is no normalisation step (step 2 in Alg.",
      "startOffset" : 56,
      "endOffset" : 64
    }, {
      "referenceID" : 23,
      "context" : "To guarantee robust reconstruction, we take advantage of recent results of [24] on random sampling of",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 26,
      "context" : "See [27] for several choices of similarity measure s and several ways to create W from the s(xi,xj).",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 27,
      "context" : "1 of [28]) random variables.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 27,
      "context" : "1 of [28] (an instance of the Johnson-Lindenstrauss lemma) to ‖RUk(fi − fj)‖, the following holds.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 21,
      "context" : ", f̃N} , as done in [22, 23].",
      "startOffset" : 20,
      "endOffset" : 28
    }, {
      "referenceID" : 22,
      "context" : ", f̃N} , as done in [22, 23].",
      "startOffset" : 20,
      "endOffset" : 28
    }, {
      "referenceID" : 28,
      "context" : "There has been recently a surge of interest around adapting classical sampling theorems to such bandlimited graph signals [29–32].",
      "startOffset" : 122,
      "endOffset" : 129
    }, {
      "referenceID" : 29,
      "context" : "There has been recently a surge of interest around adapting classical sampling theorems to such bandlimited graph signals [29–32].",
      "startOffset" : 122,
      "endOffset" : 129
    }, {
      "referenceID" : 30,
      "context" : "There has been recently a surge of interest around adapting classical sampling theorems to such bandlimited graph signals [29–32].",
      "startOffset" : 122,
      "endOffset" : 129
    }, {
      "referenceID" : 31,
      "context" : "There has been recently a surge of interest around adapting classical sampling theorems to such bandlimited graph signals [29–32].",
      "startOffset" : 122,
      "endOffset" : 129
    }, {
      "referenceID" : 23,
      "context" : "We rely here on the random sampling strategy proposed in [24] to select a subset of n nodes.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 21,
      "context" : "Results in [22,23] show that k-means is also able to identify the clusters using the feature vectors {f̃1, .",
      "startOffset" : 11,
      "endOffset" : 18
    }, {
      "referenceID" : 22,
      "context" : "Results in [22,23] show that k-means is also able to identify the clusters using the feature vectors {f̃1, .",
      "startOffset" : 11,
      "endOffset" : 18
    }, {
      "referenceID" : 23,
      "context" : "[24] show that the solution to the optimisation problem",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "3precise error bounds are provided in [24].",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 23,
      "context" : "It is shown in [24] that νk ∈ [k, N].",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 23,
      "context" : "4 ( [24]).",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 23,
      "context" : "Indeed, it is proved in [24] that, whatever the graph G, there always exists an optimal sampling distribution such that n = O(k log k) samples are sufficient to satisfy Eq.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 23,
      "context" : "This distribution depends on the profile of the local cumulative coherence and can be estimated rapidly (see [24] for more details).",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 1,
      "context" : "∀λ ∈ [0, 2], e(λ) := h̃λk(λ)− hλk(λ).",
      "startOffset" : 5,
      "endOffset" : 11
    }, {
      "referenceID" : 27,
      "context" : "1 in [28].",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 23,
      "context" : "4 of [24] showing that vk(i) = ‖Ukδi‖ ≈ ‖(HλkR)δi‖.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 32,
      "context" : "Such filters have been introduced in the graph context [33], but they involve another optimisation step that would burden our main message.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 33,
      "context" : "In our experiments, we could follow [34] and use truncated Chebychev polynomials to approximate the ideal filter, as these polynomials are known to require a small degree to ensure a given tolerated maximal error em.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 34,
      "context" : "We prefer to follow [35] who suggest to use Jackson-Chebychev polynomials: Chebychev polynomials to which are added damping multipliers to alleviate the unwanted Gibbs oscillations around the cut-off frequency λk.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 23,
      "context" : "1 in [24], where an estimation of λk is obtained as a by-product.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 35,
      "context" : "3 of [36] shows that this eigencount estimator yields the exact result (i.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 34,
      "context" : "In practice, experiments in [35] show that fewer random signals are sufficient in some cases.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 23,
      "context" : "Inspired by these results and as in [24], we choose 2 logN random signals for this estimation task.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 36,
      "context" : "2 of [37]).",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 37,
      "context" : "We first perform well-controlled experiments on the Stochastic Block Model (SBM), a model of random graphs with community structures, that was showed suitable as a benchmark for SC in [38].",
      "startOffset" : 184,
      "endOffset" : 188
    }, {
      "referenceID" : 38,
      "context" : "The fast filtering part of CSC uses the gsp cheby op function of the GSP toolbox [39].",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 39,
      "context" : "[41] show that a critical value c exists above which community detection is impossible at the large N limit: c = (s− √ s)/(s+ √ s(k − 1)).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 40,
      "context" : "The performance is measured by the Adjusted Rand similarity index [42] between the SBM’s ground truth and the obtained partitions.",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 7,
      "context" : "We compare the recovery performance and the time of computation of CSC, SC and Boutsidis’ power method [8].",
      "startOffset" : 103,
      "endOffset" : 106
    }, {
      "referenceID" : 41,
      "context" : "We finally compare CSC and SC on a real-world dataset: the Amazon co-purchasing network [43].",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 42,
      "context" : "As there is no clear ground truth in this case, we use the modularity [44] to measure the algorithm’s clustering performance, a well-known cost function that measures how well a given partition separates a network in different communities.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 43,
      "context" : "To improve the clustering result of the reduced set of nodes, one could build upon the concept of community cores [45].",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 26,
      "context" : "1) is no longer required [27].",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 2,
      "context" : "On the other hand, for the version of SC based on the random walk Laplacian L = I− D−1W [3], the Fourier matrix is no longer orthogonal, its inverse is no longer equal to its transpose, and the sampling theorems of [24] need yet to be extended to this case.",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 23,
      "context" : "On the other hand, for the version of SC based on the random walk Laplacian L = I− D−1W [3], the Fourier matrix is no longer orthogonal, its inverse is no longer equal to its transpose, and the sampling theorems of [24] need yet to be extended to this case.",
      "startOffset" : 215,
      "endOffset" : 219
    } ],
    "year" : 2017,
    "abstractText" : "Spectral clustering has become a popular technique due to its high performance in many contexts. It comprises three main steps: create a similarity graph between N objects to cluster, compute the first k eigenvectors of its Laplacian matrix to define a feature vector for each object, and run k-means on these features to separate objects into k classes. Each of these three steps becomes computationally intensive for large N and/or k. We propose to speed up the last two steps based on recent results in the emerging field of graph signal processing: graph filtering of random signals, and random sampling of bandlimited graph signals. We prove that our method, with a gain in computation time that can reach several orders of magnitude, is in fact an approximation of spectral clustering, for which we are able to control the error. We test the performance of our method on artificial and real-world network data.",
    "creator" : "LaTeX with hyperref package"
  }
}