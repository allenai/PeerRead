{
  "name" : "1412.5949.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "LARGE SCALE DISTRIBUTED DISTANCE METRIC LEARNING",
    "authors" : [ "Pengtao Xie", "Eric Xing" ],
    "emails" : [ "pengtaox@cs.cmu.edu", "epxing@cs.cmu.edu" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Learning a proper distance metric Xing et al. (2002); Bilenko et al. (2004); Bar-Hillel et al. (2005); Globerson & Roweis (2005); Weinberger et al. (2005); Davis et al. (2007); Guillaumin et al. (2009); Kostinger et al. (2012); Mensink et al. (2012) is essential for many distance based data mining and machine learning algorithms, such as retrieval Hoi et al. (2008), k-means clustering Xing et al. (2002) and k nearest neighbor (kNN) classification Weinberger et al. (2005). A commonality of these algorithms is that their accuracy depends critically on a good distance metric M between points, especially when the dataset is high-dimensional.\nAs first formulated in Xing et al. (2002), mathematically, a Distance Metric Learning (DML) problem can be formulated as semi-supervised learning problem where, given side information in the form of data pairs that are determined to be similar or dissimilar, we learn a metric M , which places similar data pairs close to each other, and dissimilar data pairs as far apart as possible. This leads to a quadratic program whose size grows super-linearly with the size of the data and of the side information. Specifically, let M defines a Mahalanobis distance (x − y)TM(x − y), where x, y are d-dimensional feature vectors and M ∈ Rd×d is a positive semidefinite matrix (to be learned). Because M is a d-by-d matrix, when the feature dimension d is huge — such as in web-scale problems where web pages are represented with bag-of-word (BOW) vectors that are millions of words long Ahmed et al. (2012), or in computer vision where hundreds of thousands of features are routinely extracted from images Lin et al. (2011) — the size of M quickly becomes intractable for a single machine; e.g., if d contains 1 million features, then M contains 1 trillion parameters. Storing thisM requires∼ 4 terabytes of memory, to say nothing of the massive computational cost of learning so many parameters. Even worse, the number of labeled data pairs can easily exceed billions or even trillions, particularly in web data. For instance, in Flickr, photos are organized into many interests groups (e.g., tiger, sushi, car) by the users — if we simply regard photos in the same group as similar, and those in different groups as dissimilar, we can rapidly generate a huge number of similar/dissimilar pairs, as Flickr contains billions of photos and over ten million groups.\nar X\niv :1\n41 2.\n59 49\nv1 [\ncs .L\nG ]\n1 8\nD ec\n2 01\n4\nFor problems involving big data volume, one would naturally speculate that modern distributed system should come for the rescue, and platforms such as Hadoop Foundation (2009) or Spark Zaharia et al. (2012) may straightforwardly offer to scale up the originally sequential algorithms intended for a single machine. But what makes DML on large-scale data nontrivial even with popular distributed platforms is that its basic formulation requires both substantial redesign of the original algorithm, and a more optimization-friendly parallel communication strategy not supported under the currently popular bulk synchronization parallelism (BSP) adopted by Hadoop and Spark. Specifically, DML is a semi-definite programming (SDP) problem Xing et al. (2002), which requires eigen-decomposition of the d-by-d Mahalanobis distance matrix M after each update. Because eigendecomposition requires O(d3) computation, it is outright infeasible when the feature dimension d is high, no matter how many machines are available. Furthermore, DML is a optimization problem with hard constraints (where each similar/dissimilar data pair corresponds to one constraint); this makes the distribution and parallel learning of parameters M very difficult, as costly, frequent inter-machine synchronization must be employed to keep the machines’ local views of M consistent with each other. A BSP model would make this operation very expensive.\nMotivated by these challenges, we explore and validate new approaches to performing distributed DML on large scale problems. On the algorithm side, inspired by ideas from Weinberger et al. (2005), we reformulate DML to make it tractable on high-dimensional data and amenable for distributed optimization — specifically, we re-represent M with an alternate decomposition LTL, and perform optimization directly over L. This not only preserves the semi-definite property of M , but also avoids the costly O(d3) eigen-decomposition. Moreover, to solve the inter-machine parameter synchronization challenge caused by the hard DML constraints, we use slack variables to relax those constraints, and then transform the relaxed constraints into a hinge loss function. This makes the distributed optimization much easier, yet does not hurt the effectiveness of the learned distance metric, as our validation will show.\nTo solve this reformulated DML problem, we build a distributed system that uses asynchronous stochastic gradient descent to do parameter learning of M . Similar and dissimilar data pairs are partitioned onto different worker machines and each worker stores a local view of the parameter. At each iteration, each worker randomly picks up a mini-batch of data pairs, computes a stochastic gradient and uses the gradient to update the local parameter copy. Because the gradients computed at each worker need to be seen and utilized by other workers, we use a centralized parameter server to synchronize the parameters among workers, which has been proven to be effective both empirically and theoretically Ahmed et al. (2012); Dean et al. (2012); Ho et al. (2013); Li et al. (2014).\nThe major contributions of this work are summarized as follows:\n• We design and implement a distributed framework to support large scale distance metric learning. To our best knowledge, this is the first work targeting the parallel and distributed metric learning on large datasets.\n• We use asynchronous stochastic gradient descent to optimize DML in distributed setting and implement an efficient centralized parameter server to synchronize the parameters across machines.\n• We provide an efficient and easy-to-use implementation of the framework, that we plan to open-source.\n• We conducted distributed distance metric learning on large datasets and demonstrate the efficiency and effectiveness of our system.\nIn Section 2, we discuss related work, before introducing our reformulation of distance metric learning in Section 3. Section 4 presents our distributed framework for parallel distance metric learning, with experimental results in Section 5."
    }, {
      "heading" : "2 RELATED WORKS",
      "text" : "Distance metric learning Xing et al. (2002); Bilenko et al. (2004); Bar-Hillel et al. (2005); Globerson & Roweis (2005); Weinberger et al. (2005); Davis et al. (2007); Guillaumin et al. (2009); Kostinger et al. (2012); Mensink et al. (2012) has been widely studied in many works. Given some data pairs labeled as similar or dissimilar, these algorithms try to learn distance metrics to make similar pairs\nclose to each other and separate dissimilar pairs apart. The most widely used distance metric is the Mahalanobis distance. Xing et al Xing et al. (2002) used semidefinite programming to learn a Mahalanobis distance metric for clustering under similarity and dissimilarity constraints. They aim to minimize the distance of similar pairs while separating dissimilar pairs with a certain margin. Weinberger et al Weinberger et al. (2005) and Mensink et al Mensink et al. (2012) employed a similar semidefinite formulation for k-nearest neighbor classification. The metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. Globerson and Roweis Globerson & Roweis (2005) proposed a formulation aiming to collapse all examples in the same class to a single point and push examples in other classes infinitely far away. Davis et al Davis et al. (2007) proposed information theoretic metric learning which minimizes the differential relative entropy between two multivariate Gaussians under the constraints on the distance function.\nAll the above-mentioned works are designed and implemented on single machine and cannot handle large scale data efficiently. Kostinger et al Kostinger et al. (2012) proposed a fast DML method based on likelihood-ratio test, which does not require iterative optimization procedures and hence is very efficient and scalable. However, distance metrics learned by this method yield poor performance empirically and this method is less flexible (can only deal with equivalence constraints). Our method learns effective distance metrics on multiple datasets and can flexibly deal with not only pair-wise constraints but also triple-wise constraints. In terms of the size of the problem, Mensink et al Mensink et al. (2012) achieved similar scale on ImageNet Deng et al. (2009) dataset as we do. However, in their work, DML was performed on a single machine, which takes days to finish. In our work, with the distributed framework, we can finish learning within 15 hours.\nDesigning and implementing parameter servers (PS) for distributed machine learning have been explored in several works Ahmed et al. (2012); Dean et al. (2012); Ho et al. (2013); Li et al. (2014) . Most of these designs contain a centralized server and a collection of workers. The central server maintains the global parameter and each worker has a local copy of the parameter. Workers compute local updates of the parameter and push the updates to the central server. The central server aggregates updates from workers, apply the update to the global parameter and push the fresh global parameter back to workers. The existing parameter servers differ in consistency control. In Bulk Synchronous Parallel (BSP) systems like Hadoop Dean & Ghemawat (2008), Spark Zaharia et al. (2012), workers must wait for each other at the end of every iteration. In asynchronous parameter server Ahmed et al. (2012); Dean & Ghemawat (2008), all workers work on their own pace and never waits. Staleness Synchronous Parallel Ho et al. (2013) seeks a balance between BSP and ASP. Workers are allowed to see different versions of the parameter, but the difference is bounded. None of the above works has studied distance metric learning (DML) before and it is unclear whether the parameter server based framework can be effective for distributed DML."
    }, {
      "heading" : "3 DISTANCE METRIC LEARNING",
      "text" : "In the original formulations Xing et al. (2002); Globerson & Roweis (2005); Weinberger et al. (2005) of distance metric learning, a semi-definite programming problem (SDP) needs to be solved. SDP requires eigen-decomposition of the Mahalanobis distance matrix, which is very hard to do on high dimensional data, if not impossible. To make DML scalable on large scale problems, similar to Weinberger et al. (2005), we do the reformulations in two ways. First, we factorize the Mahalanobis matrix M into M = LTL and do the optimization over L instead of M . The factorization ensures M still to be positive semi-definite, but avoids the eigen-decomposition of M . Second, we use slack variable to relax the constraints over dissimilar pairs and use hinge loss to replace the constraints. This can turn the original constrained problem into an unconstrained problem which makes its distributed optimization much easier without sacrificing the quality of learned distance metric."
    }, {
      "heading" : "3.1 REFORMULATION OF DISTANCE METRIC LEARNING",
      "text" : "Distance metric learning is a family of algorithms and has various formulations regarding the metric to learn, the form of distance supervision and how the objective function is defined. Among them, the most popular setting is: 1, metric: Mahalanobis distance (x − y)TM(x − y), where x and y are data samples and M is a symmetric and positive semidefinite matrix to be learned; 2, the form of distance supervision: pairs of data samples either labeled as similar or dissimilar; 3, learning\nobjective: learn a distance metric to place similar points as close as possible and dissimilar points as far as possible. Given a set of pairs labeled as similar S = {(xi, yi)}|S|i=1 and a set of pairs labeled dissimilar D = {(xi, yi)}|D|i=1, DML learns the Mahalanobis distance by optimizing the following problem\nminM ∑\n(x,y)∈S (x− y)TM(x− y)\ns.t. (x− y)TM(x− y) ≥ 1,∀(x, y) ∈ D M 0\n(1)\nwhere M 0 denotes that M is required to be positive semidefinite. This optimization problems tries to minimize the Mahalanobis distances between all pairs labeled as similar while separating dissimilar pairs with a margin 1. M is required to be positive semidefinite to ensure that Mahalanobis distance is a valid metric.\nWhile this problem can be solved using projected gradient descent (PGD), it turns out to be infeasible for high dimensional problems. In each iteration of PGD, one first computes the gradient of M w.r.t the objective function ∑ (x,y)∈S(x − y)TM(x − y), then updates M using gradient descent and finally project M onto the convex set specified by the two constraints. Doing projection requires the eigen-decomposition of M , whose complexity is O(d3), where d is the feature dimension of the data points. For high dimension problems where d can be hundreds of thousands or even millions, eigen-decomposition is extremely hard and inefficient, if not impossible. Note that a symmetric and positive semidefinite matrix M can always be factorized into M = LTL Weinberger et al. (2005), where L is a matrix of size k× d and k ≤ d. Replacing M with LTL, the problem defined in Eq.(1) can be written as\nminL ∑\n(x,y)∈S ‖L(x− y)‖2\ns.t. ‖L(x− y)‖2 ≥ 1,∀(x, y) ∈ D (2)\nThe constraint requires the distance between dissimilar pairs to be greater than a margin of 1, which is hard to enforce in the distributed setting where communication between machines is limited. We adopt a strategy similar to Weinberger et al. (2005), which introduces slack variables ξ to relax the hard constraint in Eq.(2) and get\nminL ∑\n(x,y)∈S ‖L(x− y)‖2 + λ ∑ (x,y)∈D ξx,y\ns.t. ‖L(x− y)‖2 ≥ 1− ξx,y, ξx,y ≥ 0,∀(x, y) ∈ D (3)\nUsing hinge loss, the constraint in Eq.(3) can be further eliminated minL ∑\n(x,y)∈S ‖L(x− y)‖2 + λ ∑ (x,y)∈D max(0, 1− ‖L(x− y)‖2) (4)\nTo this end, we turn the original constrained problem into an unconstrained one and the optimization becomes much easier. We use stochastic gradient method to do optimization."
    }, {
      "heading" : "4 DISTRIBUTED PARALLEL DISTANCE METRIC LEARNING",
      "text" : "In large scale distance metric learning problems, the number of similar and dissimilar pairs can be tens of millions. The dimensionality of features can be hundreds of thousands. Learning distance metrics on such large datasets on a single machine is infeasible. To solve this problem, we design a distributed framework to do parallel DML. We use asynchronous stochastic gradient descent to do the optimization. We partition the similar pairs and dissimilar pairs onto different machines. Each machine uses the data pairs it holds to update the model parameters in an asynchronous manner. Parameters on different machines are synchronized through a centralized parameter server. In terms of the form of distance supervision, we focus on pair-wise constraints (e.g., i is similar to j; j is dissimilar to k) in this work. Our framework can be easily extended to support triple-wise constraints Weinberger et al. (2005) (e.g., i is more similar to j than to k)."
    }, {
      "heading" : "4.1 PARAMETER SYNCHRONIZATION",
      "text" : "To learn L in a distributed environment with P worker machines, we partition the similarity pair S and dissimilar pair D into P pieces S1,S2,· · · ,SP and D1,D2,· · · ,DP and each machine holds one\npiece. Each machine p has a local copy Lp of the global parameter L. And different parameter copies are synchronized across machines to ensure they are as much the same as possible. Similar to Ahmed et al. (2012); Dean & Ghemawat (2008); Ho et al. (2013); Lee et al. (2014), we use a centralized parameter servers to synchronize these parameter copies. The central server holds the global parameter L. Each worker communicates with the central server and workers do not communicate with each other. At each iteration of the stochastic gradient descent algorithm, each worker p randomly samples a minibatch of data pairs from both the similar pair set Sp and the dissimilar pair set Dp it holds and computes a gradient update4Lp using the local parameter copy Lp and the minibatch. Worker p sends 4L to the central server. The central server aggregates updates received from workers and use them to update the central parameter L. The central server sends the updated L to each worker and the worker replaces its local parameter copy Lp with L."
    }, {
      "heading" : "4.2 IMPLEMENTATION DETAILS",
      "text" : "The centralized parameter server contains one central server and P workers. Workers communicate with the central server and they do not communicate with each other. The server maintains the global parameter and each worker has a local copy of the parameter. On the server side, there are two threads: 1) update thread; 2) communication thread. The server maintains two message queues to exchange messages with workers: 1) inbound message queue; 2) outbound message queue. The communication thread receives gradient updates from workers and puts them into the inbound message queue. The update thread takes a batch of gradient updates from the inbound message queue and uses them to update the central parameter. Then the update thread puts the updated central parameter to the outbound message queue. The communication thread takes updated parameter out of the outbound message queue and sends it to all workers.\nOn the worker side, there are three threads: 1) local computing thread; 2) remote update thread; 3) communication thread. The worker also maintains two message queues: 1) inbound message queue; 2) outbound message queue. At each iteration, the local computing thread takes a minibatch of data pairs, computes the gradient, uses the gradient to update the local parameter copy and puts the gradient into the outbound message queue. The communication thread sends the gradients in the outbound message queue to the server. On the other hand, it receives fresh parameters from the server and puts them into the inbound message queue. The remote update thread takes parameters out of the inbound message queue and uses them to replace the local parameter copy on this worker.\nThe server threads and worker threads execute in a best-effort manner, which means whenever some work is available to do, the threads do it without caring about other threads. For example, whenever there is a message in the outbound message queue, the communication thread sends them out. Whenever there is a message in the inbound message queue, the update thread uses it to update the parameter. Each thread behaves as if no other threads exist. The threads are coordinated indirectly by the message queues."
    }, {
      "heading" : "5 EXPERIMENTS",
      "text" : ""
    }, {
      "heading" : "5.1 DATASETS",
      "text" : "We use three datasets in our experiments. Table 1 summarizes the statistics of these datasets. The first one is the MNIST hidden written digits LeCun et al. (1998). It has 60K training images and 10K testing images. Each image is of size 32 × 32 and has a digit label. Images are represented with raw pixels, with a dimensionality of 780. We randomly sample 100K “similar” pairs and 100K “dissimilar” pairs from the training images. If two images are from the same digit, we label them as “similar”. If two images are from different digits, we label them as “dissimilar”.\nThe second dataset we use is ImageNet-63K, which has 63K training images randomly selected from the ImageNet Deng et al. (2009) dataset. Each image is associated with a class label and the total number of classes is 1000. Images are represented with Locality-constrained Linear Coding (LLC) Wang et al. (2010), with a dimensionality of 21504. We randomly sample 100K “similar” pairs and 100K “dissimilar” pairs from the training images. If two images are from the same class, we label them as “similar”. If two images are from different classes, we label them as “dissimilar”.\nThe third dataset is ImageNet-1M. It has 1 million training images and 63K testing images randomly selected from ImageNet Deng et al. (2009). The total number of classes is 1000. Images are represented with LLC features. We randomly sample 100M “similar” pairs and 100M “dissimilar” pairs to learn the distance metric."
    }, {
      "heading" : "5.2 EXPERIMENTAL SETUP",
      "text" : "In all the experiments, the tradeoff parameter λ is set to 1 and the threshold c is set to 1. For MNIST, we set k (the number of rows of L) to 600. At each iteration, we use a minibatch of 1000 pairs (500 similar pairs and 500 dissimilar pairs) to do the update. For ImageNet-63K, we set k to 10000. The number of model parameters is about 220 million. At each iteration, we use a mini-batch of 100 pairs (50 similar pairs and 50 dissimilar pairs) to do the update. For ImageNet-1M, we set k to 1000 to make the computation tractable. The mini-batch size is 1000 (500 similar pairs and 500 dissimilar pairs). Experiments on MNIST are done on machines each of which has 16 CPUs and 64G main memory. Experiments on ImageNet-63K and ImageNet-1M are performed on machines each of which has 64 CPUs and 128G main memory."
    }, {
      "heading" : "5.3 SCALABILITY OF OUR FRAMEWORK",
      "text" : "We evaluated the scalability of our framework w.r.t the number of CPU cores on three datasets. Figure 2(a) shows the convergence curves on MNIST dataset under different number of CPU cores. The horizontal axis corresponds to running time measured in minutes. The vertical axis corresponds to objective values. Figure 2(b) and 2(c) show the convergence curves on ImageNet-63K and ImageNet-1M respectively. As can be seen from the three figures, increasing the number of machines consistently increases the convergence speed.\nTo measure how our framework can speedup the training as we increase the number of machines (cores), for each machine setting we record the running time that the objective value is decreased to p, where p is the objective value achieved by one single machine at the end of training. The speedup factor of n machines is calculated as tn/t1, where tn is the running time of n machines and t1 is the runtime of 1 machine. Figure 3(a) shows the speedup factors on MNIST dataset. The horizontal axis corresponds to the number of cores. The vertical axis shows speed up factors. The blue curve is linear speedup (optimal speedup). The red curve shows the speed up factors achieved by our framework. As can be seen from this figure, our framework achieves near optimal speedup\nunder all machine settings with different CPU cores. Figure 3(b) and 3(c) shows the speedup on ImageNet-63K and ImageNet-1M dataset respectively. Our framework achieves speedups which are very close to linear under different number of cores. With 4 machines (256 cores in total), our framework achieves 3.6 times speedup on ImageNet-63K dataset and achieves 3.8 times speedup on ImageNet-1M dataset compared with those on 1 machine (16 cores in total). This demonstrates that our framework scales very well with the number of CPU cores (machines).\nThe scalability of our framework attributes to the sufficient use of CPU cores to do computation and efficient parameter synchronization among workers. We distribute the computation over all available machines and allocate a copy of the model parameters to each machine. Computing threads on each machine always have access to the parameters through the parameter copy, thereby, computation can be ceaselessly performed without blocking caused by the coordination and communication with other machines. Each machine runs as if no other machines exist. This ensures that all CPU cores can be sufficiently used. On the other hand, we synchronize the parameter copies of different workers using a centralized server, to ensure that each work’s update can be timely contributed to the global parameters and the computing threads on each worker can use the most-up-to-date parameters to do computation. The synchronization happens in background and does not require blocking computation."
    }, {
      "heading" : "5.4 QUALITY OF THE LEARNED DISTANCE METRIC",
      "text" : "We also measure the quality of the learned distance metric. We compare our reformulation in Eq.(4) with the original formulation Xing et al. (2002) in Eq.(1) (denoted by Xing2012) and with Information Theoretical Metric Learning (ITML) Davis et al. (2007) and the likelihood test based method (KISS) Kostinger et al. (2012). All methods (including our method) are implemented with MATLAB and runs on a single thread. For each method, we learn a distance metric on the MNIST training set. In ITML, we set the tradeoff parameter γ is set to 0.001. In KISS, the feature dimension is reduced to 600 using Principal Component Analysis (PCA) to ensure the covariance matrices are invertible.\nTo evaluate the effectiveness of the learned metric, we randomly sample 10K similar pairs and 10K dissimilar pairs from 100K held-out testing images and use the metric to judge whether these pairs are similar or dissimilar. If the distance is great that some threshold t, the pair is regarded as similar. Otherwise, the pair is regarded as dissimilar. We use two evaluation metrics: average precision and precision-recall curves.\nFigure 4(a) shows the average precision versus running time. Figure 4(b) shows the best precisionrecall curve achieved by each method. As can be seen from the two figures, our method is not only much faster, but also achieves better performance. Optimizing the original formulation in Eq.(1) (Xing2002) requires solving a linear constrained least square (LCLS) problem at each iteration, which is very costly. On the MNIST dataset, the number of variables to be optimized in LCLS is 680K and the number of constraints is 100K. Solving such a LCLS problem in each iteration is highly demanding. Besides, the original formulation requires eigen-decomposition of the Mahalanobis matrixM , which is very costly when the feature dimension is high. In ITML, the computational complexity on each data pairs is O(d2), where d is the feature dimension. In our method, the complexity is O(dk), where k is usually smaller than d. From Figure 4(a), we observe that in ITML, the precision is not consistently increasing as running time increases. This is because in each iteration of ITML, a single data pair is used to update the parameter, which may incur high variance. It is unclear how to extent ITML to mini-batch update to make the algorithm stable. KISS computes a distance metric in one shot and does not require iterative optimization, thereby, it is very fast. It learned a metric in 2 minutes. However, the metric yields very pool performance. The average precision is only 0.73 (Figure 4(a)) and the precision-recall curve (Figure 4(b)) is much worse than other methods. Our method is very efficient. The training is finished in about half a hour on a single thread while Xing2002 takes about 24 hours and ITML takes about 3 hours. With our distributed framework, the speed can be further improved significantly as shown in Figure 3(a). In addition, our method is very effective. It achieves an average precision of 0.90, which cannot be achieved by the other three methods.\nWe also evaluate the effectiveness of the distance metric learned on ImageNet-1M. From 63K heldout testing images, we randomly sample 100K similar pairs and 100K dissimilar pairs and compute precision-recall curves, which are shown in Figure 4(c). The blue curve is obtained from Euclidean distance computed on original feature vectors. The red curve corresponds to Mahalanobis distance computed under the learned distance metric. As can be seen from the figure, with distance metric learning, the performance is greatly improved."
    }, {
      "heading" : "6 CONCLUSIONS",
      "text" : "In this paper, we reformulate the original DML problem into an unconstrained optimization problem that is amenable for parallelization, and use it as the basis for a distributed framework to support large scale distance metric learning. Our framework makes use of asynchronous stochastic gradient descent for distributed optimization, where the computation is distributed across worker machines, which use a centralized parameter server to synchronize parameters between them. Experiments on three datasets demonstrate the efficiency and effectiveness of our framework, at previously-unseen problem scales with better accuracy than previous methods. We believe our work is the first to address distance metric learning at such large scales, in an intelligent distributed fashion."
    } ],
    "references" : [ {
      "title" : "Scalable inference in latent variable models",
      "author" : [ "J. der" ],
      "venue" : "In WSDM,",
      "citeRegEx" : "der,? \\Q2012\\E",
      "shortCiteRegEx" : "der",
      "year" : 2012
    }, {
      "title" : "Integrating constraints and metric learning in semi-supervised clustering",
      "author" : [ "Bilenko", "Mikhail", "Basu", "Sugato", "Mooney", "Raymond J" ],
      "venue" : "In ICML, pp",
      "citeRegEx" : "Bilenko et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Bilenko et al\\.",
      "year" : 2004
    }, {
      "title" : "Information-theoretic metric learning",
      "author" : [ "Davis", "Jason V", "Kulis", "Brian", "Jain", "Prateek", "Sra", "Suvrit", "Dhillon", "Inderjit S" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Davis et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Davis et al\\.",
      "year" : 2007
    }, {
      "title" : "Mapreduce: simplified data processing on large clusters",
      "author" : [ "Dean", "Jeffrey", "Ghemawat", "Sanjay" ],
      "venue" : "Communications of the ACM,",
      "citeRegEx" : "Dean et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Dean et al\\.",
      "year" : 2008
    }, {
      "title" : "Large scale distributed deep networks",
      "author" : [ "Dean", "Jeffrey", "Corrado", "Greg", "Monga", "Rajat", "Chen", "Kai", "Devin", "Matthieu", "Mao", "Mark", "Senior", "Andrew", "Tucker", "Paul", "Yang", "Ke", "Le", "Quoc V" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Dean et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Dean et al\\.",
      "year" : 2012
    }, {
      "title" : "Imagenet: A large-scale hierarchical image database",
      "author" : [ "Deng", "Jia", "Dong", "Wei", "Socher", "Richard", "Li", "Li-Jia", "Kai", "Fei-Fei" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Deng et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Deng et al\\.",
      "year" : 2009
    }, {
      "title" : "Metric learning by collapsing classes",
      "author" : [ "Globerson", "Amir", "Roweis", "Sam T" ],
      "venue" : "In NIPS, pp",
      "citeRegEx" : "Globerson et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Globerson et al\\.",
      "year" : 2005
    }, {
      "title" : "Is that you? metric learning approaches for face identification",
      "author" : [ "Guillaumin", "Matthieu", "Verbeek", "Jakob", "Schmid", "Cordelia" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "Guillaumin et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Guillaumin et al\\.",
      "year" : 2009
    }, {
      "title" : "More effective distributed ml via a stale synchronous parallel parameter server",
      "author" : [ "Ho", "Qirong", "Cipar", "James", "Cui", "Henggang", "Lee", "Seunghak", "Kim", "Jin Kyu", "Gibbons", "Phillip B", "Gibson", "Garth A", "Ganger", "Greg", "Xing", "Eric" ],
      "venue" : null,
      "citeRegEx" : "Ho et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Ho et al\\.",
      "year" : 2013
    }, {
      "title" : "Semi-supervised distance metric learning for collaborative image retrieval",
      "author" : [ "Hoi", "Steven CH", "Liu", "Wei", "Chang", "Shih-Fu" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Hoi et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Hoi et al\\.",
      "year" : 2008
    }, {
      "title" : "Large scale metric learning from equivalence constraints",
      "author" : [ "M Kostinger", "Hirzer", "Martin", "Wohlhart", "Paul", "Roth", "Peter M", "Bischof", "Horst" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Kostinger et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Kostinger et al\\.",
      "year" : 2012
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "LeCun", "Yann", "Bottou", "Léon", "Bengio", "Yoshua", "Haffner", "Patrick" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "LeCun et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1998
    }, {
      "title" : "Primitives for dynamic big model parallelism",
      "author" : [ "Lee", "Seunghak", "Kim", "Jin Kyu", "Zheng", "Xun", "Ho", "Qirong", "Gibson", "Garth A", "Xing", "Eric P" ],
      "venue" : "arXiv preprint arXiv:1406.4580,",
      "citeRegEx" : "Lee et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2014
    }, {
      "title" : "Scaling distributed machine learning with the parameter server",
      "author" : [ "Li", "Mu", "Andersen", "David G", "Park", "Jun Woo", "Smola", "Alexander J", "Ahmed", "Amr", "Josifovski", "Vanja", "Long", "James", "Shekita", "Eugene J", "Su", "Bor-Yiing" ],
      "venue" : "In Proc. OSDI,",
      "citeRegEx" : "Li et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2014
    }, {
      "title" : "Large-scale image classification: fast feature extraction and svm training",
      "author" : [ "Lin", "Yuanqing", "Lv", "Fengjun", "Zhu", "Shenghuo", "Yang", "Ming", "Cour", "Timothee", "Yu", "Kai", "Cao", "Liangliang", "Huang", "Thomas" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Lin et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2011
    }, {
      "title" : "Metric learning for large scale image classification: Generalizing to new classes at near-zero cost",
      "author" : [ "Mensink", "Thomas", "Verbeek", "Jakob", "Perronnin", "Florent", "Csurka", "Gabriela" ],
      "venue" : "In ECCV,",
      "citeRegEx" : "Mensink et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Mensink et al\\.",
      "year" : 2012
    }, {
      "title" : "Localityconstrained linear coding for image classification",
      "author" : [ "Wang", "Jinjun", "Yang", "Jianchao", "Yu", "Kai", "Lv", "Fengjun", "Huang", "Thomas", "Gong", "Yihong" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Wang et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2010
    }, {
      "title" : "Distance metric learning for large margin nearest neighbor classification",
      "author" : [ "Weinberger", "Kilian Q", "Blitzer", "John", "Saul", "Lawrence K" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Weinberger et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Weinberger et al\\.",
      "year" : 2005
    }, {
      "title" : "Distance metric learning with application to clustering with side-information",
      "author" : [ "Xing", "Eric P", "Jordan", "Michael I", "Russell", "Stuart", "Ng", "Andrew Y" ],
      "venue" : "In NIPS, pp",
      "citeRegEx" : "Xing et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Xing et al\\.",
      "year" : 2002
    }, {
      "title" : "Resilient distributed datasets: A faulttolerant abstraction for in-memory cluster computing",
      "author" : [ "Zaharia", "Matei", "Chowdhury", "Mosharaf", "Das", "Tathagata", "Dave", "Ankur", "Ma", "Justin", "McCauley", "Murphy", "Franklin", "Michael J", "Shenker", "Scott", "Stoica", "Ion" ],
      "venue" : "In Proceedings of the 9th USENIX conference on Networked Systems Design and Implementation,",
      "citeRegEx" : "Zaharia et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Zaharia et al\\.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "However, high dimensionality and large volume of pairwise constraints in modern big data can lead to prohibitive computational cost for both the original DML formulation in Xing et al. (2002) and later extensions.",
      "startOffset" : 75,
      "endOffset" : 192
    }, {
      "referenceID" : 0,
      "context" : "However, high dimensionality and large volume of pairwise constraints in modern big data can lead to prohibitive computational cost for both the original DML formulation in Xing et al. (2002) and later extensions. In this paper, we present a distributed algorithm for DML, and a large-scale implementation on a parameter server architecture. Our approach builds on a parallelizable reformulation of Xing et al. (2002), and an asynchronous stochastic gradient descent optimization procedure.",
      "startOffset" : 75,
      "endOffset" : 418
    }, {
      "referenceID" : 10,
      "context" : "Learning a proper distance metric Xing et al. (2002); Bilenko et al.",
      "startOffset" : 34,
      "endOffset" : 53
    }, {
      "referenceID" : 1,
      "context" : "(2002); Bilenko et al. (2004); Bar-Hillel et al.",
      "startOffset" : 8,
      "endOffset" : 30
    }, {
      "referenceID" : 1,
      "context" : "(2002); Bilenko et al. (2004); Bar-Hillel et al. (2005); Globerson & Roweis (2005); Weinberger et al.",
      "startOffset" : 8,
      "endOffset" : 56
    }, {
      "referenceID" : 1,
      "context" : "(2002); Bilenko et al. (2004); Bar-Hillel et al. (2005); Globerson & Roweis (2005); Weinberger et al.",
      "startOffset" : 8,
      "endOffset" : 83
    }, {
      "referenceID" : 1,
      "context" : "(2002); Bilenko et al. (2004); Bar-Hillel et al. (2005); Globerson & Roweis (2005); Weinberger et al. (2005); Davis et al.",
      "startOffset" : 8,
      "endOffset" : 109
    }, {
      "referenceID" : 1,
      "context" : "(2002); Bilenko et al. (2004); Bar-Hillel et al. (2005); Globerson & Roweis (2005); Weinberger et al. (2005); Davis et al. (2007); Guillaumin et al.",
      "startOffset" : 8,
      "endOffset" : 130
    }, {
      "referenceID" : 1,
      "context" : "(2002); Bilenko et al. (2004); Bar-Hillel et al. (2005); Globerson & Roweis (2005); Weinberger et al. (2005); Davis et al. (2007); Guillaumin et al. (2009); Kostinger et al.",
      "startOffset" : 8,
      "endOffset" : 156
    }, {
      "referenceID" : 1,
      "context" : "(2002); Bilenko et al. (2004); Bar-Hillel et al. (2005); Globerson & Roweis (2005); Weinberger et al. (2005); Davis et al. (2007); Guillaumin et al. (2009); Kostinger et al. (2012); Mensink et al.",
      "startOffset" : 8,
      "endOffset" : 181
    }, {
      "referenceID" : 1,
      "context" : "(2002); Bilenko et al. (2004); Bar-Hillel et al. (2005); Globerson & Roweis (2005); Weinberger et al. (2005); Davis et al. (2007); Guillaumin et al. (2009); Kostinger et al. (2012); Mensink et al. (2012) is essential for many distance based data mining and machine learning algorithms, such as retrieval Hoi et al.",
      "startOffset" : 8,
      "endOffset" : 204
    }, {
      "referenceID" : 1,
      "context" : "(2002); Bilenko et al. (2004); Bar-Hillel et al. (2005); Globerson & Roweis (2005); Weinberger et al. (2005); Davis et al. (2007); Guillaumin et al. (2009); Kostinger et al. (2012); Mensink et al. (2012) is essential for many distance based data mining and machine learning algorithms, such as retrieval Hoi et al. (2008), k-means clustering Xing et al.",
      "startOffset" : 8,
      "endOffset" : 322
    }, {
      "referenceID" : 1,
      "context" : "(2002); Bilenko et al. (2004); Bar-Hillel et al. (2005); Globerson & Roweis (2005); Weinberger et al. (2005); Davis et al. (2007); Guillaumin et al. (2009); Kostinger et al. (2012); Mensink et al. (2012) is essential for many distance based data mining and machine learning algorithms, such as retrieval Hoi et al. (2008), k-means clustering Xing et al. (2002) and k nearest neighbor (kNN) classification Weinberger et al.",
      "startOffset" : 8,
      "endOffset" : 361
    }, {
      "referenceID" : 1,
      "context" : "(2002); Bilenko et al. (2004); Bar-Hillel et al. (2005); Globerson & Roweis (2005); Weinberger et al. (2005); Davis et al. (2007); Guillaumin et al. (2009); Kostinger et al. (2012); Mensink et al. (2012) is essential for many distance based data mining and machine learning algorithms, such as retrieval Hoi et al. (2008), k-means clustering Xing et al. (2002) and k nearest neighbor (kNN) classification Weinberger et al. (2005). A commonality of these algorithms is that their accuracy depends critically on a good distance metric M between points, especially when the dataset is high-dimensional.",
      "startOffset" : 8,
      "endOffset" : 430
    }, {
      "referenceID" : 1,
      "context" : "(2002); Bilenko et al. (2004); Bar-Hillel et al. (2005); Globerson & Roweis (2005); Weinberger et al. (2005); Davis et al. (2007); Guillaumin et al. (2009); Kostinger et al. (2012); Mensink et al. (2012) is essential for many distance based data mining and machine learning algorithms, such as retrieval Hoi et al. (2008), k-means clustering Xing et al. (2002) and k nearest neighbor (kNN) classification Weinberger et al. (2005). A commonality of these algorithms is that their accuracy depends critically on a good distance metric M between points, especially when the dataset is high-dimensional. As first formulated in Xing et al. (2002), mathematically, a Distance Metric Learning (DML) problem can be formulated as semi-supervised learning problem where, given side information in the form of data pairs that are determined to be similar or dissimilar, we learn a metric M , which places similar data pairs close to each other, and dissimilar data pairs as far apart as possible.",
      "startOffset" : 8,
      "endOffset" : 642
    }, {
      "referenceID" : 1,
      "context" : "(2002); Bilenko et al. (2004); Bar-Hillel et al. (2005); Globerson & Roweis (2005); Weinberger et al. (2005); Davis et al. (2007); Guillaumin et al. (2009); Kostinger et al. (2012); Mensink et al. (2012) is essential for many distance based data mining and machine learning algorithms, such as retrieval Hoi et al. (2008), k-means clustering Xing et al. (2002) and k nearest neighbor (kNN) classification Weinberger et al. (2005). A commonality of these algorithms is that their accuracy depends critically on a good distance metric M between points, especially when the dataset is high-dimensional. As first formulated in Xing et al. (2002), mathematically, a Distance Metric Learning (DML) problem can be formulated as semi-supervised learning problem where, given side information in the form of data pairs that are determined to be similar or dissimilar, we learn a metric M , which places similar data pairs close to each other, and dissimilar data pairs as far apart as possible. This leads to a quadratic program whose size grows super-linearly with the size of the data and of the side information. Specifically, let M defines a Mahalanobis distance (x − y)M(x − y), where x, y are d-dimensional feature vectors and M ∈ Rd×d is a positive semidefinite matrix (to be learned). Because M is a d-by-d matrix, when the feature dimension d is huge — such as in web-scale problems where web pages are represented with bag-of-word (BOW) vectors that are millions of words long Ahmed et al. (2012), or in computer vision where hundreds of thousands of features are routinely extracted from images Lin et al.",
      "startOffset" : 8,
      "endOffset" : 1498
    }, {
      "referenceID" : 1,
      "context" : "(2002); Bilenko et al. (2004); Bar-Hillel et al. (2005); Globerson & Roweis (2005); Weinberger et al. (2005); Davis et al. (2007); Guillaumin et al. (2009); Kostinger et al. (2012); Mensink et al. (2012) is essential for many distance based data mining and machine learning algorithms, such as retrieval Hoi et al. (2008), k-means clustering Xing et al. (2002) and k nearest neighbor (kNN) classification Weinberger et al. (2005). A commonality of these algorithms is that their accuracy depends critically on a good distance metric M between points, especially when the dataset is high-dimensional. As first formulated in Xing et al. (2002), mathematically, a Distance Metric Learning (DML) problem can be formulated as semi-supervised learning problem where, given side information in the form of data pairs that are determined to be similar or dissimilar, we learn a metric M , which places similar data pairs close to each other, and dissimilar data pairs as far apart as possible. This leads to a quadratic program whose size grows super-linearly with the size of the data and of the side information. Specifically, let M defines a Mahalanobis distance (x − y)M(x − y), where x, y are d-dimensional feature vectors and M ∈ Rd×d is a positive semidefinite matrix (to be learned). Because M is a d-by-d matrix, when the feature dimension d is huge — such as in web-scale problems where web pages are represented with bag-of-word (BOW) vectors that are millions of words long Ahmed et al. (2012), or in computer vision where hundreds of thousands of features are routinely extracted from images Lin et al. (2011) — the size of M quickly becomes intractable for a single machine; e.",
      "startOffset" : 8,
      "endOffset" : 1615
    }, {
      "referenceID" : 0,
      "context" : "For problems involving big data volume, one would naturally speculate that modern distributed system should come for the rescue, and platforms such as Hadoop Foundation (2009) or Spark Zaharia et al.",
      "startOffset" : 77,
      "endOffset" : 176
    }, {
      "referenceID" : 0,
      "context" : "For problems involving big data volume, one would naturally speculate that modern distributed system should come for the rescue, and platforms such as Hadoop Foundation (2009) or Spark Zaharia et al. (2012) may straightforwardly offer to scale up the originally sequential algorithms intended for a single machine.",
      "startOffset" : 77,
      "endOffset" : 207
    }, {
      "referenceID" : 0,
      "context" : "For problems involving big data volume, one would naturally speculate that modern distributed system should come for the rescue, and platforms such as Hadoop Foundation (2009) or Spark Zaharia et al. (2012) may straightforwardly offer to scale up the originally sequential algorithms intended for a single machine. But what makes DML on large-scale data nontrivial even with popular distributed platforms is that its basic formulation requires both substantial redesign of the original algorithm, and a more optimization-friendly parallel communication strategy not supported under the currently popular bulk synchronization parallelism (BSP) adopted by Hadoop and Spark. Specifically, DML is a semi-definite programming (SDP) problem Xing et al. (2002), which requires eigen-decomposition of the d-by-d Mahalanobis distance matrix M after each update.",
      "startOffset" : 77,
      "endOffset" : 754
    }, {
      "referenceID" : 0,
      "context" : "For problems involving big data volume, one would naturally speculate that modern distributed system should come for the rescue, and platforms such as Hadoop Foundation (2009) or Spark Zaharia et al. (2012) may straightforwardly offer to scale up the originally sequential algorithms intended for a single machine. But what makes DML on large-scale data nontrivial even with popular distributed platforms is that its basic formulation requires both substantial redesign of the original algorithm, and a more optimization-friendly parallel communication strategy not supported under the currently popular bulk synchronization parallelism (BSP) adopted by Hadoop and Spark. Specifically, DML is a semi-definite programming (SDP) problem Xing et al. (2002), which requires eigen-decomposition of the d-by-d Mahalanobis distance matrix M after each update. Because eigendecomposition requires O(d) computation, it is outright infeasible when the feature dimension d is high, no matter how many machines are available. Furthermore, DML is a optimization problem with hard constraints (where each similar/dissimilar data pair corresponds to one constraint); this makes the distribution and parallel learning of parameters M very difficult, as costly, frequent inter-machine synchronization must be employed to keep the machines’ local views of M consistent with each other. A BSP model would make this operation very expensive. Motivated by these challenges, we explore and validate new approaches to performing distributed DML on large scale problems. On the algorithm side, inspired by ideas from Weinberger et al. (2005), we reformulate DML to make it tractable on high-dimensional data and amenable for distributed optimization — specifically, we re-represent M with an alternate decomposition LL, and perform optimization directly over L.",
      "startOffset" : 77,
      "endOffset" : 1618
    }, {
      "referenceID" : 0,
      "context" : "For problems involving big data volume, one would naturally speculate that modern distributed system should come for the rescue, and platforms such as Hadoop Foundation (2009) or Spark Zaharia et al. (2012) may straightforwardly offer to scale up the originally sequential algorithms intended for a single machine. But what makes DML on large-scale data nontrivial even with popular distributed platforms is that its basic formulation requires both substantial redesign of the original algorithm, and a more optimization-friendly parallel communication strategy not supported under the currently popular bulk synchronization parallelism (BSP) adopted by Hadoop and Spark. Specifically, DML is a semi-definite programming (SDP) problem Xing et al. (2002), which requires eigen-decomposition of the d-by-d Mahalanobis distance matrix M after each update. Because eigendecomposition requires O(d) computation, it is outright infeasible when the feature dimension d is high, no matter how many machines are available. Furthermore, DML is a optimization problem with hard constraints (where each similar/dissimilar data pair corresponds to one constraint); this makes the distribution and parallel learning of parameters M very difficult, as costly, frequent inter-machine synchronization must be employed to keep the machines’ local views of M consistent with each other. A BSP model would make this operation very expensive. Motivated by these challenges, we explore and validate new approaches to performing distributed DML on large scale problems. On the algorithm side, inspired by ideas from Weinberger et al. (2005), we reformulate DML to make it tractable on high-dimensional data and amenable for distributed optimization — specifically, we re-represent M with an alternate decomposition LL, and perform optimization directly over L. This not only preserves the semi-definite property of M , but also avoids the costly O(d) eigen-decomposition. Moreover, to solve the inter-machine parameter synchronization challenge caused by the hard DML constraints, we use slack variables to relax those constraints, and then transform the relaxed constraints into a hinge loss function. This makes the distributed optimization much easier, yet does not hurt the effectiveness of the learned distance metric, as our validation will show. To solve this reformulated DML problem, we build a distributed system that uses asynchronous stochastic gradient descent to do parameter learning of M . Similar and dissimilar data pairs are partitioned onto different worker machines and each worker stores a local view of the parameter. At each iteration, each worker randomly picks up a mini-batch of data pairs, computes a stochastic gradient and uses the gradient to update the local parameter copy. Because the gradients computed at each worker need to be seen and utilized by other workers, we use a centralized parameter server to synchronize the parameters among workers, which has been proven to be effective both empirically and theoretically Ahmed et al. (2012); Dean et al.",
      "startOffset" : 77,
      "endOffset" : 3053
    }, {
      "referenceID" : 0,
      "context" : "For problems involving big data volume, one would naturally speculate that modern distributed system should come for the rescue, and platforms such as Hadoop Foundation (2009) or Spark Zaharia et al. (2012) may straightforwardly offer to scale up the originally sequential algorithms intended for a single machine. But what makes DML on large-scale data nontrivial even with popular distributed platforms is that its basic formulation requires both substantial redesign of the original algorithm, and a more optimization-friendly parallel communication strategy not supported under the currently popular bulk synchronization parallelism (BSP) adopted by Hadoop and Spark. Specifically, DML is a semi-definite programming (SDP) problem Xing et al. (2002), which requires eigen-decomposition of the d-by-d Mahalanobis distance matrix M after each update. Because eigendecomposition requires O(d) computation, it is outright infeasible when the feature dimension d is high, no matter how many machines are available. Furthermore, DML is a optimization problem with hard constraints (where each similar/dissimilar data pair corresponds to one constraint); this makes the distribution and parallel learning of parameters M very difficult, as costly, frequent inter-machine synchronization must be employed to keep the machines’ local views of M consistent with each other. A BSP model would make this operation very expensive. Motivated by these challenges, we explore and validate new approaches to performing distributed DML on large scale problems. On the algorithm side, inspired by ideas from Weinberger et al. (2005), we reformulate DML to make it tractable on high-dimensional data and amenable for distributed optimization — specifically, we re-represent M with an alternate decomposition LL, and perform optimization directly over L. This not only preserves the semi-definite property of M , but also avoids the costly O(d) eigen-decomposition. Moreover, to solve the inter-machine parameter synchronization challenge caused by the hard DML constraints, we use slack variables to relax those constraints, and then transform the relaxed constraints into a hinge loss function. This makes the distributed optimization much easier, yet does not hurt the effectiveness of the learned distance metric, as our validation will show. To solve this reformulated DML problem, we build a distributed system that uses asynchronous stochastic gradient descent to do parameter learning of M . Similar and dissimilar data pairs are partitioned onto different worker machines and each worker stores a local view of the parameter. At each iteration, each worker randomly picks up a mini-batch of data pairs, computes a stochastic gradient and uses the gradient to update the local parameter copy. Because the gradients computed at each worker need to be seen and utilized by other workers, we use a centralized parameter server to synchronize the parameters among workers, which has been proven to be effective both empirically and theoretically Ahmed et al. (2012); Dean et al. (2012); Ho et al.",
      "startOffset" : 77,
      "endOffset" : 3073
    }, {
      "referenceID" : 0,
      "context" : "For problems involving big data volume, one would naturally speculate that modern distributed system should come for the rescue, and platforms such as Hadoop Foundation (2009) or Spark Zaharia et al. (2012) may straightforwardly offer to scale up the originally sequential algorithms intended for a single machine. But what makes DML on large-scale data nontrivial even with popular distributed platforms is that its basic formulation requires both substantial redesign of the original algorithm, and a more optimization-friendly parallel communication strategy not supported under the currently popular bulk synchronization parallelism (BSP) adopted by Hadoop and Spark. Specifically, DML is a semi-definite programming (SDP) problem Xing et al. (2002), which requires eigen-decomposition of the d-by-d Mahalanobis distance matrix M after each update. Because eigendecomposition requires O(d) computation, it is outright infeasible when the feature dimension d is high, no matter how many machines are available. Furthermore, DML is a optimization problem with hard constraints (where each similar/dissimilar data pair corresponds to one constraint); this makes the distribution and parallel learning of parameters M very difficult, as costly, frequent inter-machine synchronization must be employed to keep the machines’ local views of M consistent with each other. A BSP model would make this operation very expensive. Motivated by these challenges, we explore and validate new approaches to performing distributed DML on large scale problems. On the algorithm side, inspired by ideas from Weinberger et al. (2005), we reformulate DML to make it tractable on high-dimensional data and amenable for distributed optimization — specifically, we re-represent M with an alternate decomposition LL, and perform optimization directly over L. This not only preserves the semi-definite property of M , but also avoids the costly O(d) eigen-decomposition. Moreover, to solve the inter-machine parameter synchronization challenge caused by the hard DML constraints, we use slack variables to relax those constraints, and then transform the relaxed constraints into a hinge loss function. This makes the distributed optimization much easier, yet does not hurt the effectiveness of the learned distance metric, as our validation will show. To solve this reformulated DML problem, we build a distributed system that uses asynchronous stochastic gradient descent to do parameter learning of M . Similar and dissimilar data pairs are partitioned onto different worker machines and each worker stores a local view of the parameter. At each iteration, each worker randomly picks up a mini-batch of data pairs, computes a stochastic gradient and uses the gradient to update the local parameter copy. Because the gradients computed at each worker need to be seen and utilized by other workers, we use a centralized parameter server to synchronize the parameters among workers, which has been proven to be effective both empirically and theoretically Ahmed et al. (2012); Dean et al. (2012); Ho et al. (2013); Li et al.",
      "startOffset" : 77,
      "endOffset" : 3091
    }, {
      "referenceID" : 0,
      "context" : "For problems involving big data volume, one would naturally speculate that modern distributed system should come for the rescue, and platforms such as Hadoop Foundation (2009) or Spark Zaharia et al. (2012) may straightforwardly offer to scale up the originally sequential algorithms intended for a single machine. But what makes DML on large-scale data nontrivial even with popular distributed platforms is that its basic formulation requires both substantial redesign of the original algorithm, and a more optimization-friendly parallel communication strategy not supported under the currently popular bulk synchronization parallelism (BSP) adopted by Hadoop and Spark. Specifically, DML is a semi-definite programming (SDP) problem Xing et al. (2002), which requires eigen-decomposition of the d-by-d Mahalanobis distance matrix M after each update. Because eigendecomposition requires O(d) computation, it is outright infeasible when the feature dimension d is high, no matter how many machines are available. Furthermore, DML is a optimization problem with hard constraints (where each similar/dissimilar data pair corresponds to one constraint); this makes the distribution and parallel learning of parameters M very difficult, as costly, frequent inter-machine synchronization must be employed to keep the machines’ local views of M consistent with each other. A BSP model would make this operation very expensive. Motivated by these challenges, we explore and validate new approaches to performing distributed DML on large scale problems. On the algorithm side, inspired by ideas from Weinberger et al. (2005), we reformulate DML to make it tractable on high-dimensional data and amenable for distributed optimization — specifically, we re-represent M with an alternate decomposition LL, and perform optimization directly over L. This not only preserves the semi-definite property of M , but also avoids the costly O(d) eigen-decomposition. Moreover, to solve the inter-machine parameter synchronization challenge caused by the hard DML constraints, we use slack variables to relax those constraints, and then transform the relaxed constraints into a hinge loss function. This makes the distributed optimization much easier, yet does not hurt the effectiveness of the learned distance metric, as our validation will show. To solve this reformulated DML problem, we build a distributed system that uses asynchronous stochastic gradient descent to do parameter learning of M . Similar and dissimilar data pairs are partitioned onto different worker machines and each worker stores a local view of the parameter. At each iteration, each worker randomly picks up a mini-batch of data pairs, computes a stochastic gradient and uses the gradient to update the local parameter copy. Because the gradients computed at each worker need to be seen and utilized by other workers, we use a centralized parameter server to synchronize the parameters among workers, which has been proven to be effective both empirically and theoretically Ahmed et al. (2012); Dean et al. (2012); Ho et al. (2013); Li et al. (2014). The major contributions of this work are summarized as follows:",
      "startOffset" : 77,
      "endOffset" : 3109
    }, {
      "referenceID" : 12,
      "context" : "Distance metric learning Xing et al. (2002); Bilenko et al.",
      "startOffset" : 25,
      "endOffset" : 44
    }, {
      "referenceID" : 1,
      "context" : "(2002); Bilenko et al. (2004); Bar-Hillel et al.",
      "startOffset" : 8,
      "endOffset" : 30
    }, {
      "referenceID" : 1,
      "context" : "(2002); Bilenko et al. (2004); Bar-Hillel et al. (2005); Globerson & Roweis (2005); Weinberger et al.",
      "startOffset" : 8,
      "endOffset" : 56
    }, {
      "referenceID" : 1,
      "context" : "(2002); Bilenko et al. (2004); Bar-Hillel et al. (2005); Globerson & Roweis (2005); Weinberger et al.",
      "startOffset" : 8,
      "endOffset" : 83
    }, {
      "referenceID" : 1,
      "context" : "(2002); Bilenko et al. (2004); Bar-Hillel et al. (2005); Globerson & Roweis (2005); Weinberger et al. (2005); Davis et al.",
      "startOffset" : 8,
      "endOffset" : 109
    }, {
      "referenceID" : 1,
      "context" : "(2002); Bilenko et al. (2004); Bar-Hillel et al. (2005); Globerson & Roweis (2005); Weinberger et al. (2005); Davis et al. (2007); Guillaumin et al.",
      "startOffset" : 8,
      "endOffset" : 130
    }, {
      "referenceID" : 1,
      "context" : "(2002); Bilenko et al. (2004); Bar-Hillel et al. (2005); Globerson & Roweis (2005); Weinberger et al. (2005); Davis et al. (2007); Guillaumin et al. (2009); Kostinger et al.",
      "startOffset" : 8,
      "endOffset" : 156
    }, {
      "referenceID" : 1,
      "context" : "(2002); Bilenko et al. (2004); Bar-Hillel et al. (2005); Globerson & Roweis (2005); Weinberger et al. (2005); Davis et al. (2007); Guillaumin et al. (2009); Kostinger et al. (2012); Mensink et al.",
      "startOffset" : 8,
      "endOffset" : 181
    }, {
      "referenceID" : 1,
      "context" : "(2002); Bilenko et al. (2004); Bar-Hillel et al. (2005); Globerson & Roweis (2005); Weinberger et al. (2005); Davis et al. (2007); Guillaumin et al. (2009); Kostinger et al. (2012); Mensink et al. (2012) has been widely studied in many works.",
      "startOffset" : 8,
      "endOffset" : 204
    }, {
      "referenceID" : 8,
      "context" : "Xing et al Xing et al. (2002) used semidefinite programming to learn a Mahalanobis distance metric for clustering under similarity and dissimilarity constraints.",
      "startOffset" : 11,
      "endOffset" : 30
    }, {
      "referenceID" : 0,
      "context" : "(2002) used semidefinite programming to learn a Mahalanobis distance metric for clustering under similarity and dissimilarity constraints. They aim to minimize the distance of similar pairs while separating dissimilar pairs with a certain margin. Weinberger et al Weinberger et al. (2005) and Mensink et al Mensink et al.",
      "startOffset" : 93,
      "endOffset" : 289
    }, {
      "referenceID" : 0,
      "context" : "(2002) used semidefinite programming to learn a Mahalanobis distance metric for clustering under similarity and dissimilarity constraints. They aim to minimize the distance of similar pairs while separating dissimilar pairs with a certain margin. Weinberger et al Weinberger et al. (2005) and Mensink et al Mensink et al. (2012) employed a similar semidefinite formulation for k-nearest neighbor classification.",
      "startOffset" : 93,
      "endOffset" : 329
    }, {
      "referenceID" : 0,
      "context" : "(2002) used semidefinite programming to learn a Mahalanobis distance metric for clustering under similarity and dissimilarity constraints. They aim to minimize the distance of similar pairs while separating dissimilar pairs with a certain margin. Weinberger et al Weinberger et al. (2005) and Mensink et al Mensink et al. (2012) employed a similar semidefinite formulation for k-nearest neighbor classification. The metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. Globerson and Roweis Globerson & Roweis (2005) proposed a formulation aiming to collapse all examples in the same class to a single point and push examples in other classes infinitely far away.",
      "startOffset" : 93,
      "endOffset" : 627
    }, {
      "referenceID" : 0,
      "context" : "(2002) used semidefinite programming to learn a Mahalanobis distance metric for clustering under similarity and dissimilarity constraints. They aim to minimize the distance of similar pairs while separating dissimilar pairs with a certain margin. Weinberger et al Weinberger et al. (2005) and Mensink et al Mensink et al. (2012) employed a similar semidefinite formulation for k-nearest neighbor classification. The metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. Globerson and Roweis Globerson & Roweis (2005) proposed a formulation aiming to collapse all examples in the same class to a single point and push examples in other classes infinitely far away. Davis et al Davis et al. (2007) proposed information theoretic metric learning which minimizes the differential relative entropy between two multivariate Gaussians under the constraints on the distance function.",
      "startOffset" : 93,
      "endOffset" : 806
    }, {
      "referenceID" : 0,
      "context" : "(2002) used semidefinite programming to learn a Mahalanobis distance metric for clustering under similarity and dissimilarity constraints. They aim to minimize the distance of similar pairs while separating dissimilar pairs with a certain margin. Weinberger et al Weinberger et al. (2005) and Mensink et al Mensink et al. (2012) employed a similar semidefinite formulation for k-nearest neighbor classification. The metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. Globerson and Roweis Globerson & Roweis (2005) proposed a formulation aiming to collapse all examples in the same class to a single point and push examples in other classes infinitely far away. Davis et al Davis et al. (2007) proposed information theoretic metric learning which minimizes the differential relative entropy between two multivariate Gaussians under the constraints on the distance function. All the above-mentioned works are designed and implemented on single machine and cannot handle large scale data efficiently. Kostinger et al Kostinger et al. (2012) proposed a fast DML method based on likelihood-ratio test, which does not require iterative optimization procedures and hence is very efficient and scalable.",
      "startOffset" : 93,
      "endOffset" : 1151
    }, {
      "referenceID" : 0,
      "context" : "(2002) used semidefinite programming to learn a Mahalanobis distance metric for clustering under similarity and dissimilarity constraints. They aim to minimize the distance of similar pairs while separating dissimilar pairs with a certain margin. Weinberger et al Weinberger et al. (2005) and Mensink et al Mensink et al. (2012) employed a similar semidefinite formulation for k-nearest neighbor classification. The metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. Globerson and Roweis Globerson & Roweis (2005) proposed a formulation aiming to collapse all examples in the same class to a single point and push examples in other classes infinitely far away. Davis et al Davis et al. (2007) proposed information theoretic metric learning which minimizes the differential relative entropy between two multivariate Gaussians under the constraints on the distance function. All the above-mentioned works are designed and implemented on single machine and cannot handle large scale data efficiently. Kostinger et al Kostinger et al. (2012) proposed a fast DML method based on likelihood-ratio test, which does not require iterative optimization procedures and hence is very efficient and scalable. However, distance metrics learned by this method yield poor performance empirically and this method is less flexible (can only deal with equivalence constraints). Our method learns effective distance metrics on multiple datasets and can flexibly deal with not only pair-wise constraints but also triple-wise constraints. In terms of the size of the problem, Mensink et al Mensink et al. (2012) achieved similar scale on ImageNet Deng et al.",
      "startOffset" : 93,
      "endOffset" : 1703
    }, {
      "referenceID" : 0,
      "context" : "(2002) used semidefinite programming to learn a Mahalanobis distance metric for clustering under similarity and dissimilarity constraints. They aim to minimize the distance of similar pairs while separating dissimilar pairs with a certain margin. Weinberger et al Weinberger et al. (2005) and Mensink et al Mensink et al. (2012) employed a similar semidefinite formulation for k-nearest neighbor classification. The metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. Globerson and Roweis Globerson & Roweis (2005) proposed a formulation aiming to collapse all examples in the same class to a single point and push examples in other classes infinitely far away. Davis et al Davis et al. (2007) proposed information theoretic metric learning which minimizes the differential relative entropy between two multivariate Gaussians under the constraints on the distance function. All the above-mentioned works are designed and implemented on single machine and cannot handle large scale data efficiently. Kostinger et al Kostinger et al. (2012) proposed a fast DML method based on likelihood-ratio test, which does not require iterative optimization procedures and hence is very efficient and scalable. However, distance metrics learned by this method yield poor performance empirically and this method is less flexible (can only deal with equivalence constraints). Our method learns effective distance metrics on multiple datasets and can flexibly deal with not only pair-wise constraints but also triple-wise constraints. In terms of the size of the problem, Mensink et al Mensink et al. (2012) achieved similar scale on ImageNet Deng et al. (2009) dataset as we do.",
      "startOffset" : 93,
      "endOffset" : 1757
    }, {
      "referenceID" : 0,
      "context" : "(2002) used semidefinite programming to learn a Mahalanobis distance metric for clustering under similarity and dissimilarity constraints. They aim to minimize the distance of similar pairs while separating dissimilar pairs with a certain margin. Weinberger et al Weinberger et al. (2005) and Mensink et al Mensink et al. (2012) employed a similar semidefinite formulation for k-nearest neighbor classification. The metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. Globerson and Roweis Globerson & Roweis (2005) proposed a formulation aiming to collapse all examples in the same class to a single point and push examples in other classes infinitely far away. Davis et al Davis et al. (2007) proposed information theoretic metric learning which minimizes the differential relative entropy between two multivariate Gaussians under the constraints on the distance function. All the above-mentioned works are designed and implemented on single machine and cannot handle large scale data efficiently. Kostinger et al Kostinger et al. (2012) proposed a fast DML method based on likelihood-ratio test, which does not require iterative optimization procedures and hence is very efficient and scalable. However, distance metrics learned by this method yield poor performance empirically and this method is less flexible (can only deal with equivalence constraints). Our method learns effective distance metrics on multiple datasets and can flexibly deal with not only pair-wise constraints but also triple-wise constraints. In terms of the size of the problem, Mensink et al Mensink et al. (2012) achieved similar scale on ImageNet Deng et al. (2009) dataset as we do. However, in their work, DML was performed on a single machine, which takes days to finish. In our work, with the distributed framework, we can finish learning within 15 hours. Designing and implementing parameter servers (PS) for distributed machine learning have been explored in several works Ahmed et al. (2012); Dean et al.",
      "startOffset" : 93,
      "endOffset" : 2090
    }, {
      "referenceID" : 0,
      "context" : "(2002) used semidefinite programming to learn a Mahalanobis distance metric for clustering under similarity and dissimilarity constraints. They aim to minimize the distance of similar pairs while separating dissimilar pairs with a certain margin. Weinberger et al Weinberger et al. (2005) and Mensink et al Mensink et al. (2012) employed a similar semidefinite formulation for k-nearest neighbor classification. The metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. Globerson and Roweis Globerson & Roweis (2005) proposed a formulation aiming to collapse all examples in the same class to a single point and push examples in other classes infinitely far away. Davis et al Davis et al. (2007) proposed information theoretic metric learning which minimizes the differential relative entropy between two multivariate Gaussians under the constraints on the distance function. All the above-mentioned works are designed and implemented on single machine and cannot handle large scale data efficiently. Kostinger et al Kostinger et al. (2012) proposed a fast DML method based on likelihood-ratio test, which does not require iterative optimization procedures and hence is very efficient and scalable. However, distance metrics learned by this method yield poor performance empirically and this method is less flexible (can only deal with equivalence constraints). Our method learns effective distance metrics on multiple datasets and can flexibly deal with not only pair-wise constraints but also triple-wise constraints. In terms of the size of the problem, Mensink et al Mensink et al. (2012) achieved similar scale on ImageNet Deng et al. (2009) dataset as we do. However, in their work, DML was performed on a single machine, which takes days to finish. In our work, with the distributed framework, we can finish learning within 15 hours. Designing and implementing parameter servers (PS) for distributed machine learning have been explored in several works Ahmed et al. (2012); Dean et al. (2012); Ho et al.",
      "startOffset" : 93,
      "endOffset" : 2110
    }, {
      "referenceID" : 0,
      "context" : "(2002) used semidefinite programming to learn a Mahalanobis distance metric for clustering under similarity and dissimilarity constraints. They aim to minimize the distance of similar pairs while separating dissimilar pairs with a certain margin. Weinberger et al Weinberger et al. (2005) and Mensink et al Mensink et al. (2012) employed a similar semidefinite formulation for k-nearest neighbor classification. The metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. Globerson and Roweis Globerson & Roweis (2005) proposed a formulation aiming to collapse all examples in the same class to a single point and push examples in other classes infinitely far away. Davis et al Davis et al. (2007) proposed information theoretic metric learning which minimizes the differential relative entropy between two multivariate Gaussians under the constraints on the distance function. All the above-mentioned works are designed and implemented on single machine and cannot handle large scale data efficiently. Kostinger et al Kostinger et al. (2012) proposed a fast DML method based on likelihood-ratio test, which does not require iterative optimization procedures and hence is very efficient and scalable. However, distance metrics learned by this method yield poor performance empirically and this method is less flexible (can only deal with equivalence constraints). Our method learns effective distance metrics on multiple datasets and can flexibly deal with not only pair-wise constraints but also triple-wise constraints. In terms of the size of the problem, Mensink et al Mensink et al. (2012) achieved similar scale on ImageNet Deng et al. (2009) dataset as we do. However, in their work, DML was performed on a single machine, which takes days to finish. In our work, with the distributed framework, we can finish learning within 15 hours. Designing and implementing parameter servers (PS) for distributed machine learning have been explored in several works Ahmed et al. (2012); Dean et al. (2012); Ho et al. (2013); Li et al.",
      "startOffset" : 93,
      "endOffset" : 2128
    }, {
      "referenceID" : 0,
      "context" : "(2002) used semidefinite programming to learn a Mahalanobis distance metric for clustering under similarity and dissimilarity constraints. They aim to minimize the distance of similar pairs while separating dissimilar pairs with a certain margin. Weinberger et al Weinberger et al. (2005) and Mensink et al Mensink et al. (2012) employed a similar semidefinite formulation for k-nearest neighbor classification. The metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. Globerson and Roweis Globerson & Roweis (2005) proposed a formulation aiming to collapse all examples in the same class to a single point and push examples in other classes infinitely far away. Davis et al Davis et al. (2007) proposed information theoretic metric learning which minimizes the differential relative entropy between two multivariate Gaussians under the constraints on the distance function. All the above-mentioned works are designed and implemented on single machine and cannot handle large scale data efficiently. Kostinger et al Kostinger et al. (2012) proposed a fast DML method based on likelihood-ratio test, which does not require iterative optimization procedures and hence is very efficient and scalable. However, distance metrics learned by this method yield poor performance empirically and this method is less flexible (can only deal with equivalence constraints). Our method learns effective distance metrics on multiple datasets and can flexibly deal with not only pair-wise constraints but also triple-wise constraints. In terms of the size of the problem, Mensink et al Mensink et al. (2012) achieved similar scale on ImageNet Deng et al. (2009) dataset as we do. However, in their work, DML was performed on a single machine, which takes days to finish. In our work, with the distributed framework, we can finish learning within 15 hours. Designing and implementing parameter servers (PS) for distributed machine learning have been explored in several works Ahmed et al. (2012); Dean et al. (2012); Ho et al. (2013); Li et al. (2014) .",
      "startOffset" : 93,
      "endOffset" : 2146
    }, {
      "referenceID" : 0,
      "context" : "(2002) used semidefinite programming to learn a Mahalanobis distance metric for clustering under similarity and dissimilarity constraints. They aim to minimize the distance of similar pairs while separating dissimilar pairs with a certain margin. Weinberger et al Weinberger et al. (2005) and Mensink et al Mensink et al. (2012) employed a similar semidefinite formulation for k-nearest neighbor classification. The metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. Globerson and Roweis Globerson & Roweis (2005) proposed a formulation aiming to collapse all examples in the same class to a single point and push examples in other classes infinitely far away. Davis et al Davis et al. (2007) proposed information theoretic metric learning which minimizes the differential relative entropy between two multivariate Gaussians under the constraints on the distance function. All the above-mentioned works are designed and implemented on single machine and cannot handle large scale data efficiently. Kostinger et al Kostinger et al. (2012) proposed a fast DML method based on likelihood-ratio test, which does not require iterative optimization procedures and hence is very efficient and scalable. However, distance metrics learned by this method yield poor performance empirically and this method is less flexible (can only deal with equivalence constraints). Our method learns effective distance metrics on multiple datasets and can flexibly deal with not only pair-wise constraints but also triple-wise constraints. In terms of the size of the problem, Mensink et al Mensink et al. (2012) achieved similar scale on ImageNet Deng et al. (2009) dataset as we do. However, in their work, DML was performed on a single machine, which takes days to finish. In our work, with the distributed framework, we can finish learning within 15 hours. Designing and implementing parameter servers (PS) for distributed machine learning have been explored in several works Ahmed et al. (2012); Dean et al. (2012); Ho et al. (2013); Li et al. (2014) . Most of these designs contain a centralized server and a collection of workers. The central server maintains the global parameter and each worker has a local copy of the parameter. Workers compute local updates of the parameter and push the updates to the central server. The central server aggregates updates from workers, apply the update to the global parameter and push the fresh global parameter back to workers. The existing parameter servers differ in consistency control. In Bulk Synchronous Parallel (BSP) systems like Hadoop Dean & Ghemawat (2008), Spark Zaharia et al.",
      "startOffset" : 93,
      "endOffset" : 2706
    }, {
      "referenceID" : 0,
      "context" : "(2002) used semidefinite programming to learn a Mahalanobis distance metric for clustering under similarity and dissimilarity constraints. They aim to minimize the distance of similar pairs while separating dissimilar pairs with a certain margin. Weinberger et al Weinberger et al. (2005) and Mensink et al Mensink et al. (2012) employed a similar semidefinite formulation for k-nearest neighbor classification. The metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. Globerson and Roweis Globerson & Roweis (2005) proposed a formulation aiming to collapse all examples in the same class to a single point and push examples in other classes infinitely far away. Davis et al Davis et al. (2007) proposed information theoretic metric learning which minimizes the differential relative entropy between two multivariate Gaussians under the constraints on the distance function. All the above-mentioned works are designed and implemented on single machine and cannot handle large scale data efficiently. Kostinger et al Kostinger et al. (2012) proposed a fast DML method based on likelihood-ratio test, which does not require iterative optimization procedures and hence is very efficient and scalable. However, distance metrics learned by this method yield poor performance empirically and this method is less flexible (can only deal with equivalence constraints). Our method learns effective distance metrics on multiple datasets and can flexibly deal with not only pair-wise constraints but also triple-wise constraints. In terms of the size of the problem, Mensink et al Mensink et al. (2012) achieved similar scale on ImageNet Deng et al. (2009) dataset as we do. However, in their work, DML was performed on a single machine, which takes days to finish. In our work, with the distributed framework, we can finish learning within 15 hours. Designing and implementing parameter servers (PS) for distributed machine learning have been explored in several works Ahmed et al. (2012); Dean et al. (2012); Ho et al. (2013); Li et al. (2014) . Most of these designs contain a centralized server and a collection of workers. The central server maintains the global parameter and each worker has a local copy of the parameter. Workers compute local updates of the parameter and push the updates to the central server. The central server aggregates updates from workers, apply the update to the global parameter and push the fresh global parameter back to workers. The existing parameter servers differ in consistency control. In Bulk Synchronous Parallel (BSP) systems like Hadoop Dean & Ghemawat (2008), Spark Zaharia et al. (2012), workers must wait for each other at the end of every iteration.",
      "startOffset" : 93,
      "endOffset" : 2735
    }, {
      "referenceID" : 0,
      "context" : "(2002) used semidefinite programming to learn a Mahalanobis distance metric for clustering under similarity and dissimilarity constraints. They aim to minimize the distance of similar pairs while separating dissimilar pairs with a certain margin. Weinberger et al Weinberger et al. (2005) and Mensink et al Mensink et al. (2012) employed a similar semidefinite formulation for k-nearest neighbor classification. The metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. Globerson and Roweis Globerson & Roweis (2005) proposed a formulation aiming to collapse all examples in the same class to a single point and push examples in other classes infinitely far away. Davis et al Davis et al. (2007) proposed information theoretic metric learning which minimizes the differential relative entropy between two multivariate Gaussians under the constraints on the distance function. All the above-mentioned works are designed and implemented on single machine and cannot handle large scale data efficiently. Kostinger et al Kostinger et al. (2012) proposed a fast DML method based on likelihood-ratio test, which does not require iterative optimization procedures and hence is very efficient and scalable. However, distance metrics learned by this method yield poor performance empirically and this method is less flexible (can only deal with equivalence constraints). Our method learns effective distance metrics on multiple datasets and can flexibly deal with not only pair-wise constraints but also triple-wise constraints. In terms of the size of the problem, Mensink et al Mensink et al. (2012) achieved similar scale on ImageNet Deng et al. (2009) dataset as we do. However, in their work, DML was performed on a single machine, which takes days to finish. In our work, with the distributed framework, we can finish learning within 15 hours. Designing and implementing parameter servers (PS) for distributed machine learning have been explored in several works Ahmed et al. (2012); Dean et al. (2012); Ho et al. (2013); Li et al. (2014) . Most of these designs contain a centralized server and a collection of workers. The central server maintains the global parameter and each worker has a local copy of the parameter. Workers compute local updates of the parameter and push the updates to the central server. The central server aggregates updates from workers, apply the update to the global parameter and push the fresh global parameter back to workers. The existing parameter servers differ in consistency control. In Bulk Synchronous Parallel (BSP) systems like Hadoop Dean & Ghemawat (2008), Spark Zaharia et al. (2012), workers must wait for each other at the end of every iteration. In asynchronous parameter server Ahmed et al. (2012); Dean & Ghemawat (2008), all workers work on their own pace and never waits.",
      "startOffset" : 93,
      "endOffset" : 2853
    }, {
      "referenceID" : 0,
      "context" : "(2002) used semidefinite programming to learn a Mahalanobis distance metric for clustering under similarity and dissimilarity constraints. They aim to minimize the distance of similar pairs while separating dissimilar pairs with a certain margin. Weinberger et al Weinberger et al. (2005) and Mensink et al Mensink et al. (2012) employed a similar semidefinite formulation for k-nearest neighbor classification. The metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. Globerson and Roweis Globerson & Roweis (2005) proposed a formulation aiming to collapse all examples in the same class to a single point and push examples in other classes infinitely far away. Davis et al Davis et al. (2007) proposed information theoretic metric learning which minimizes the differential relative entropy between two multivariate Gaussians under the constraints on the distance function. All the above-mentioned works are designed and implemented on single machine and cannot handle large scale data efficiently. Kostinger et al Kostinger et al. (2012) proposed a fast DML method based on likelihood-ratio test, which does not require iterative optimization procedures and hence is very efficient and scalable. However, distance metrics learned by this method yield poor performance empirically and this method is less flexible (can only deal with equivalence constraints). Our method learns effective distance metrics on multiple datasets and can flexibly deal with not only pair-wise constraints but also triple-wise constraints. In terms of the size of the problem, Mensink et al Mensink et al. (2012) achieved similar scale on ImageNet Deng et al. (2009) dataset as we do. However, in their work, DML was performed on a single machine, which takes days to finish. In our work, with the distributed framework, we can finish learning within 15 hours. Designing and implementing parameter servers (PS) for distributed machine learning have been explored in several works Ahmed et al. (2012); Dean et al. (2012); Ho et al. (2013); Li et al. (2014) . Most of these designs contain a centralized server and a collection of workers. The central server maintains the global parameter and each worker has a local copy of the parameter. Workers compute local updates of the parameter and push the updates to the central server. The central server aggregates updates from workers, apply the update to the global parameter and push the fresh global parameter back to workers. The existing parameter servers differ in consistency control. In Bulk Synchronous Parallel (BSP) systems like Hadoop Dean & Ghemawat (2008), Spark Zaharia et al. (2012), workers must wait for each other at the end of every iteration. In asynchronous parameter server Ahmed et al. (2012); Dean & Ghemawat (2008), all workers work on their own pace and never waits.",
      "startOffset" : 93,
      "endOffset" : 2877
    }, {
      "referenceID" : 0,
      "context" : "(2002) used semidefinite programming to learn a Mahalanobis distance metric for clustering under similarity and dissimilarity constraints. They aim to minimize the distance of similar pairs while separating dissimilar pairs with a certain margin. Weinberger et al Weinberger et al. (2005) and Mensink et al Mensink et al. (2012) employed a similar semidefinite formulation for k-nearest neighbor classification. The metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. Globerson and Roweis Globerson & Roweis (2005) proposed a formulation aiming to collapse all examples in the same class to a single point and push examples in other classes infinitely far away. Davis et al Davis et al. (2007) proposed information theoretic metric learning which minimizes the differential relative entropy between two multivariate Gaussians under the constraints on the distance function. All the above-mentioned works are designed and implemented on single machine and cannot handle large scale data efficiently. Kostinger et al Kostinger et al. (2012) proposed a fast DML method based on likelihood-ratio test, which does not require iterative optimization procedures and hence is very efficient and scalable. However, distance metrics learned by this method yield poor performance empirically and this method is less flexible (can only deal with equivalence constraints). Our method learns effective distance metrics on multiple datasets and can flexibly deal with not only pair-wise constraints but also triple-wise constraints. In terms of the size of the problem, Mensink et al Mensink et al. (2012) achieved similar scale on ImageNet Deng et al. (2009) dataset as we do. However, in their work, DML was performed on a single machine, which takes days to finish. In our work, with the distributed framework, we can finish learning within 15 hours. Designing and implementing parameter servers (PS) for distributed machine learning have been explored in several works Ahmed et al. (2012); Dean et al. (2012); Ho et al. (2013); Li et al. (2014) . Most of these designs contain a centralized server and a collection of workers. The central server maintains the global parameter and each worker has a local copy of the parameter. Workers compute local updates of the parameter and push the updates to the central server. The central server aggregates updates from workers, apply the update to the global parameter and push the fresh global parameter back to workers. The existing parameter servers differ in consistency control. In Bulk Synchronous Parallel (BSP) systems like Hadoop Dean & Ghemawat (2008), Spark Zaharia et al. (2012), workers must wait for each other at the end of every iteration. In asynchronous parameter server Ahmed et al. (2012); Dean & Ghemawat (2008), all workers work on their own pace and never waits. Staleness Synchronous Parallel Ho et al. (2013) seeks a balance between BSP and ASP.",
      "startOffset" : 93,
      "endOffset" : 2978
    }, {
      "referenceID" : 17,
      "context" : "In the original formulations Xing et al. (2002); Globerson & Roweis (2005); Weinberger et al.",
      "startOffset" : 29,
      "endOffset" : 48
    }, {
      "referenceID" : 17,
      "context" : "In the original formulations Xing et al. (2002); Globerson & Roweis (2005); Weinberger et al.",
      "startOffset" : 29,
      "endOffset" : 75
    }, {
      "referenceID" : 17,
      "context" : "(2002); Globerson & Roweis (2005); Weinberger et al. (2005) of distance metric learning, a semi-definite programming problem (SDP) needs to be solved.",
      "startOffset" : 35,
      "endOffset" : 60
    }, {
      "referenceID" : 17,
      "context" : "(2002); Globerson & Roweis (2005); Weinberger et al. (2005) of distance metric learning, a semi-definite programming problem (SDP) needs to be solved. SDP requires eigen-decomposition of the Mahalanobis distance matrix, which is very hard to do on high dimensional data, if not impossible. To make DML scalable on large scale problems, similar to Weinberger et al. (2005), we do the reformulations in two ways.",
      "startOffset" : 35,
      "endOffset" : 372
    }, {
      "referenceID" : 17,
      "context" : "Note that a symmetric and positive semidefinite matrix M can always be factorized into M = LL Weinberger et al. (2005), where L is a matrix of size k× d and k ≤ d.",
      "startOffset" : 94,
      "endOffset" : 119
    }, {
      "referenceID" : 17,
      "context" : "We adopt a strategy similar to Weinberger et al. (2005), which introduces slack variables ξ to relax the hard constraint in Eq.",
      "startOffset" : 31,
      "endOffset" : 56
    }, {
      "referenceID" : 17,
      "context" : "Our framework can be easily extended to support triple-wise constraints Weinberger et al. (2005) (e.",
      "startOffset" : 72,
      "endOffset" : 97
    }, {
      "referenceID" : 8,
      "context" : "(2012); Dean & Ghemawat (2008); Ho et al. (2013); Lee et al.",
      "startOffset" : 32,
      "endOffset" : 49
    }, {
      "referenceID" : 8,
      "context" : "(2012); Dean & Ghemawat (2008); Ho et al. (2013); Lee et al. (2014), we use a centralized parameter servers to synchronize these parameter copies.",
      "startOffset" : 32,
      "endOffset" : 68
    }, {
      "referenceID" : 10,
      "context" : "The first one is the MNIST hidden written digits LeCun et al. (1998). It has 60K training images and 10K testing images.",
      "startOffset" : 49,
      "endOffset" : 69
    }, {
      "referenceID" : 5,
      "context" : "The second dataset we use is ImageNet-63K, which has 63K training images randomly selected from the ImageNet Deng et al. (2009) dataset.",
      "startOffset" : 109,
      "endOffset" : 128
    }, {
      "referenceID" : 5,
      "context" : "The second dataset we use is ImageNet-63K, which has 63K training images randomly selected from the ImageNet Deng et al. (2009) dataset. Each image is associated with a class label and the total number of classes is 1000. Images are represented with Locality-constrained Linear Coding (LLC) Wang et al. (2010), with a dimensionality of 21504.",
      "startOffset" : 109,
      "endOffset" : 310
    }, {
      "referenceID" : 5,
      "context" : "The second dataset we use is ImageNet-63K, which has 63K training images randomly selected from the ImageNet Deng et al. (2009) dataset. Each image is associated with a class label and the total number of classes is 1000. Images are represented with Locality-constrained Linear Coding (LLC) Wang et al. (2010), with a dimensionality of 21504. We randomly sample 100K “similar” pairs and 100K “dissimilar” pairs from the training images. If two images are from the same class, we label them as “similar”. If two images are from different classes, we label them as “dissimilar”. The third dataset is ImageNet-1M. It has 1 million training images and 63K testing images randomly selected from ImageNet Deng et al. (2009). The total number of classes is 1000.",
      "startOffset" : 109,
      "endOffset" : 718
    }, {
      "referenceID" : 16,
      "context" : "(4) with the original formulation Xing et al. (2002) in Eq.",
      "startOffset" : 34,
      "endOffset" : 53
    }, {
      "referenceID" : 2,
      "context" : "(1) (denoted by Xing2012) and with Information Theoretical Metric Learning (ITML) Davis et al. (2007) and the likelihood test based method (KISS) Kostinger et al.",
      "startOffset" : 82,
      "endOffset" : 102
    }, {
      "referenceID" : 2,
      "context" : "(1) (denoted by Xing2012) and with Information Theoretical Metric Learning (ITML) Davis et al. (2007) and the likelihood test based method (KISS) Kostinger et al. (2012). All methods (including our method) are implemented with MATLAB and runs on a single thread.",
      "startOffset" : 82,
      "endOffset" : 170
    } ],
    "year" : 2014,
    "abstractText" : "In large scale machine learning and data mining problems with high feature dimensionality, the Euclidean distance between data points can be uninformative, and Distance Metric Learning (DML) is often desired to learn a proper similarity measure (using side information such as example data pairs being similar or dissimilar). However, high dimensionality and large volume of pairwise constraints in modern big data can lead to prohibitive computational cost for both the original DML formulation in Xing et al. (2002) and later extensions. In this paper, we present a distributed algorithm for DML, and a large-scale implementation on a parameter server architecture. Our approach builds on a parallelizable reformulation of Xing et al. (2002), and an asynchronous stochastic gradient descent optimization procedure. To our knowledge, this is the first distributed solution to DML, and we show that, on a system with 256 CPU cores, our program is able to complete a DML task on a dataset with 1 million data points, 22-thousand features, and 200 million labeled data pairs, in 15 hours; and the learned metric shows great effectiveness in properly measuring distances.",
    "creator" : "LaTeX with hyperref package"
  }
}