{
  "name" : "1611.01779.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Alexey Dosovitskiy", "Vladlen Koltun" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "We present an approach to sensorimotor control in immersive environments. Our approach utilizes a high-dimensional sensory stream and a lower-dimensional measurement stream. The cotemporal structure of these streams provides a rich supervisory signal, which enables training a sensorimotor control model by interacting with the environment. The model is trained using supervised learning techniques, but without extraneous supervision. It learns to act based on raw sensory input from a complex three-dimensional environment. The presented formulation allows changing the agent’s goal at test time. We conduct extensive experiments in three-dimensional simulations based on the classical first-person game Doom. The results demonstrate that the presented approach outperforms sophisticated prior formulations, particularly on challenging tasks. The results also show that trained models successfully generalize across environments and goals. A model trained using the presented approach won the Full Deathmatch track of the Visual Doom AI Competition, which was held in previously unseen environments."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Machine learning problems are commonly divided into three classes: supervised, unsupervised, and reinforcement learning. In this view, supervised learning is concerned with learning input-output mappings, unsupervised learning aims to find hidden structure in data, and reinforcement learning deals with goal-directed behavior (Murphy, 2012). Reinforcement learning is compelling because it considers the natural setting of an organism acting in its environment. It is generally taken to comprise a class of problems (learning to act), the mathematical formalization of these problems (maximizing the expected discounted return), and a family of algorithmic approaches (optimizing an objective derived from the Bellman equation) (Kaelbling et al., 1996; Sutton & Barto, 2017).\nWhile reinforcement learning (RL) has achieved significant progress (Mnih et al., 2015), key challenges remain. One is sensorimotor control from raw sensory input in complex and dynamic threedimensional environments, learned directly from experience. Another is the acquisition of general skills that can be flexibly deployed to accomplish a multitude of dynamically specified goals (Lake et al., 2016).\nIn this work, we propose an approach to sensorimotor control that aims to assist progress towards meeting these challenges. Our approach departs from the reward-based formalization commonly used in RL. Instead of a monolithic state and a scalar reward, we consider a stream of sensory input {st} and a stream of measurements {mt}. The sensory stream is typically high-dimensional and may include the raw visual, auditory, and tactile input. The measurement stream has lower dimensionality and constitutes a set of data that pertain to the agent’s current state. In a physical system, measurements can include attitude, supply levels, and structural integrity. In a three-dimensional computer game, they can include health, ammunition levels, and the number of adversaries overcome.\nOur guiding observation is that the interlocked temporal structure of the sensory and measurement streams provides a rich supervisory signal. Given present sensory input, measurements, and goal, the agent can be trained to predict the effect of different actions on future measurements. Assuming that the goal can be expressed in terms of future measurements, predicting these provides all the information necessary to support action. This reduces sensorimotor control to supervised learning, while supporting learning from raw experience and without extraneous data. Supervision is pro-\nar X\niv :1\n61 1.\n01 77\n9v 1\n[ cs\n.L G\n] 6\nN ov\n2 01\n6\nvided by experience itself: by acting and observing the effects of different actions in the context of changing sensory inputs and goals.\nThis approach has two significant benefits. First, in contrast to an occasional scalar reward assumed in traditional RL, the measurement stream provides rich and temporally dense supervision that can stabilize and accelerate training. While a sparse scalar reward may be the only feedback available in a board game (Tesauro, 1994; Silver et al., 2016), a multidimensional stream of sensations is a more appropriate model for an organism that is learning to function in an immersive environment (Adolph & Berger, 2006).\nThe second advantage of the presented formulation is that it supports changing the agent’s goal at test time. Assuming that the goal can be expressed in terms of future measurements, the model can be trained to take the goal into account in its prediction of the future. At test time, the agent can predict future measurements given its current sensory input, measurements, and goal, and then simply select the action that best suits its present goal.\nWe evaluate the presented approach in immersive three-dimensional simulations that require visually navigating a complex three-dimensional environment, recognizing objects, and interacting with dynamic adversaries. We use the classical first-person game Doom, which introduced immersive three-dimensional games to popular culture (Kushner, 2003). The presented approach is given only raw visual input and the statistics shown to the player in the game, such as health and ammunition levels. No human gameplay is used, the model trains on raw experience.\nExperimental results demonstrate that the presented approach outperforms state-of-the-art deep RL models, particularly on complex tasks. Experiments further demonstrate that models learned by the presented approach generalize across environments and goals, and that the use of vectorial measurements instead of a scalar reward is beneficial. A model trained with the presented approach won the Full Deathmatch track of the Visual Doom AI Competition, which took place in previously unseen environments. The presented approach outperformed the second best submission, which employed a substantially more complex model and additional supervision during training, by more than 50%."
    }, {
      "heading" : "2 BACKGROUND",
      "text" : "The supervised learning (SL) perspective on learning to act by interacting with the environment dates back decades. Jordan & Rumelhart (1992) analyze this approach, review early work, and argue that the choice of SL versus RL should be guided by the characteristics of the environment. Their analysis suggests that RL may be more efficient when the environment provides only a sparse scalar reward signal, whereas SL can be advantageous when temporally dense multidimensional feedback is available.\nSutton (1988) analyzed temporal-difference (TD) learning and argued that it is preferable to SL for prediction problems in which the correctness of the prediction is revealed many steps after the prediction is made. Sutton’s influential analysis assumes a sparse scalar reward. TD and policy gradient methods have since come to dominate the study of sensorimotor learning (Kober et al., 2013; Mnih et al., 2015; Sutton & Barto, 2017). While the use of SL is natural in imitation learning (LeCun et al., 2005; Ross et al., 2013) or in conjunction with model-based RL (Levine & Koltun, 2013), the formulation of sensorimotor learning from raw experience as supervised learning is rare (Levine et al., 2016). Our work suggests that when the learner is exposed to dense multidimensional sensory feedback, direct future prediction can support effective sensorimotor coordination in complex dynamic environments.\nOur approach has similarities to Monte Carlo methods. The convergence of such methods was analyzed early on and they were seen as theoretically advantageous, particularly when function approximators are used (Bertsekas, 1995; Sutton, 1995; Singh & Sutton, 1996). The choice of TD learning over Monte Carlo methods was argued on practical grounds, based on empirical performance on canonical examples (Sutton, 1995). While the understanding of the convergence of both types of methods has since improved (Szepesvári & Littman, 1999; Tsitsiklis, 2002; Even-Dar & Mansour, 2003), the argument for TD versus Monte Carlo is to this day empirical (Sutton & Barto, 2017). Sharp negative examples exist (Bertsekas, 2010). Our work deals with the more general setting of vectorial feedback and parameterized goals, and shows that a simple Monte-Carlo-type method performs extremely well in a compelling instantiation of this setting.\nVector-valued feedback has been considered in the context of multi-objective decision-making (Gábor et al., 1998; Roijers et al., 2013). Transfer across related tasks has been analyzed by Konidaris et al. (2012). Parameterized goals have been studied in the context of continuous motor skills such as throwing darts at a target (da Silva et al., 2012; Kober et al., 2012; Deisenroth et al., 2014). A general framework for sharing value function approximators across both states and goals has been described by Schaul et al. (2015). Our work is related, but develops a new model and shows that it achieves state-of-the-art performance in complex and dynamic three-dimensional environments.\nLearning to act in simulated environments has been the focus of significant attention following the successful application of deep RL to Atari games by Mnih et al. (2015). A number of recent efforts applied related ideas to three-dimensional environments. Lillicrap et al. (2016) considered continuous and high-dimensional action spaces and learned control policies in the TORCS simulator. Mnih et al. (2016) described asynchronous variants of deep RL methods and demonstrated navigation in a three-dimensional labyrinth. Oh et al. (2016) augmented deep Q-networks with external memory and evaluated their performance on a set of tasks in Minecraft. In a recent technical report, Kulkarni et al. (2016b) proposed end-to-end training of successor representations and demonstrated navigation in the Doom game engine. In another recent report, Blundell et al. (2016) considered a nonparametric approach to control and conducted experiments in a three-dimensional labyrinth. We compare to state-of-the-art deep RL methods in Section 4 and demonstrate that the presented approach performs favorably.\nPrediction of future states in dynamical systems was considered by Littman et al. (2001) and Singh et al. (2003). Predictive representations in the form of generalized value functions were advocated by Sutton et al. (2011). More recently, Oh et al. (2015) learned to predict future frames in Atari games. Prediction of full sensory input in realistic three-dimensional environments remains an open challenge, although significant progress is being made (Mathieu et al., 2016; Finn et al., 2016; Kalchbrenner et al., 2016). Our work considers prediction of future values of meaningful measurements from rich sensory input and shows that such prediction supports effective visuomotor control."
    }, {
      "heading" : "3 MODEL",
      "text" : "Consider an agent that interacts with the environment over discrete time steps. At each time step t, the agent receives an observation ot and executes an action at based on this observation. We assume that the observations have the following structure: ot = 〈st,mt〉, where st is raw sensory input and mt is a set of measurements. In our experiments, st is an image: the agent’s view of its threedimensional environment. More generally, st can include input from multiple sensory modalities. The measurements mt can indicate the attitude, supply levels, and structural integrity in a physical system, or health, ammunition, and score in a computer game.\nThe distinction between sensory input st and measurements mt is somewhat artificial: both st and mt constitute sensory input in different forms. In our model, the measurement vector mt is distinguished from other sensations in two ways. First, the measurement vector is the part of the observation that the agent will aim to predict. At present, predicting full sensory streams is beyond our capabilities (although see the work of Kalchbrenner et al. (2016) and van den Oord et al. (2016) for impressive recent progress). We therefore designate a subset of sensations as measurements that will be predicted. Second, we assume that the agent’s goal can be defined in terms of future measurements. Specifically, let τ1, . . . , τn be a set of temporal offsets and let f = 〈mt+τ1 −mt, . . . ,mt+τn −mt〉 be the corresponding differences of future and present measurements. We assume that any goal that the agent will pursue can be defined as maximization of a function u(f ;g). Any parametric function can be used. Our experiments use goals that are expressed as linear combinations of future measurements: u(f ;g) = g>f , (1) where the vector g parameterizes the goal and has the same dimensionality as f . This model generalizes the standard reinforcement learning formulation: the scalar reward signal can be viewed as a measurement, and exponential decay is one possible configuration of the goal vector.\nTo predict future measurements, we use a parameterized nonlinear function approximator, denoted by P :\npat = P (ot, a,g;θ). (2)\nHere a ∈ A is an action, θ are the learned parameters of P , and pat is the resulting prediction. The dimensionality of pat matches the dimensionality of f and g. Note that the prediction is a function of the current observation, the considered action, and the goal. At test time, given learned parameters θ, the agent can choose the action that yields the best predicted outcome:\nat = argmax a∈A\ng>P (ot, a,g;θ). (3)\nThe goal vector used at test time may or may not be identical to one of the goals used during training."
    }, {
      "heading" : "3.1 TRAINING",
      "text" : "The predictor P is trained on experiences collected by the agent. No human trajectories are used. Starting with a random policy, the agent begins to interact with its environment. This interaction takes place over episodes that last for a fixed number of time steps or until a terminal event occurs.\nConsider a set of experiences collected by the agent, yielding a set D of training examples: D = {〈oi, ai,gi, fi〉}Ni=1. Here 〈oi, ai,gi〉 is the input and fi is the output of example i. The predictor is trained using a regression loss:\nL(θ) = N∑ i=1 ‖P (oi, ai,gi;θ)− fi‖2 . (4)\nA classification loss can be used for predicting categorical measurements, but this was not necessary in our experiments.\nAs the agent collects new experiences, the training setD and the predictor used by the agent change. We maintain an experience memory of the M most recent experiences out of which a mini-batch of N examples is randomly sampled for every iteration of the solver. The parameters of the predictor used by the agent are updated after every k new experiences. Additional details are provided in Appendix B.\nWe have evaluated two training regimes:\n1. Single goal: the goal vector is fixed throughout the training process.\n2. Randomized goals: the goal vector for each episode is randomly sampled from a set of goals.\nIn both regimes, the agent follows an ε-greedy policy: it acts greedily according to the current goal with probability 1− ε, and selects a random action with probability ε. The value of ε is initially set to 1 and is decreased during training according to a fixed schedule."
    }, {
      "heading" : "3.2 ARCHITECTURE",
      "text" : "The predictor P is a deep network parameterized by θ. The network architecture we use is shown in Figure 1. The network has three input modules: a perception module S(s), a measurement module M(m) and a goal module G(g). In our experiments, s is an image and the perception module S is implemented as a convolutional network. The measurement and goal modules are fully-connected networks. The outputs of the three input modules are concatenated, forming the joint input representation used for subsequent processing:\nj = J(s,m,g) = 〈S(s),M(m), G(g)〉. (5)\nFuture measurements are predicted based on this input representation. The network emits predictions of future measurements for all actions at once. This could be done by a fully-connected module that absorbs the input representation and outputs predictions. However, we found that introducing additional structure into the prediction module enhances its ability to learn the fine differences between the outcomes of different actions. To this end, we build on the ideas of Wang et al. (2016) and split the prediction module into two streams: an expectation stream E(j) and an action stream A(j). The expectation stream predicts the average of the future measurements over all potential actions. The action stream concentrates on the fine differences between actions: A(j) = 〈 A1(j), . . . , Aw(j) 〉 ,\nwhere w = |A| is the number of actions. We add a normalization layer at the end of the action stream that ensures that the average of the predictions of the action stream is zero for each future measurement:\nAi(j) = Ai(j)− 1 w w∑ j=1 Aj(j) (6)\nfor all i. The normalization layer subtracts the average over all actions from each prediction, forcing the expectation stream E to compensate by predicting these average values. The output of the expectation stream has dimensionality dim(f), where f is the vector of future measurements. The output of the action stream has dimensionality w · dim(f). The output of the network is a prediction of future measurements for each action, composed by summing the output of the expectation stream and the normalized action-conditional output of the action stream: 〈\nA1(j) + E(j), . . . , Aw(j) + E(j) 〉 . (7)\nThe output of the network has the same dimensionality as the output of the action stream."
    }, {
      "heading" : "4 EXPERIMENTS",
      "text" : "We evaluate the presented approach in immersive three-dimensional simulations based on the classical game Doom. In these simulations, the agent has a first-person view of the environment and must act based on the same visual information that is shown to human players in the game. To interface with the game engine, we use the ViZDoom platform developed by Kempka et al. (2016). One of the advantages of this platform is that it allows running the simulation at thousands of frames per second on a single CPU core, which enables training models on tens of millions of simulation steps in a single day.\nWe compare the presented approach to state-of-the-art deep RL methods in four scenarios of increasing difficulty, study generalization across environments and goals, and evaluate the importance of different aspects of the model."
    }, {
      "heading" : "4.1 SETUP",
      "text" : "Scenarios. We use four scenarios of increasing difficulty:\nD1 Gathering health kits in a square room. (“Basic”)\nD2 Gathering health kits and avoiding poison vials in a maze. (“Navigation”)\nD3 Defending against adversaries while gathering health and ammunition in a maze. (“Battle”)\nD4 Defending against adversaries while gathering health and ammunition in a more complicated maze. (“Battle 2”)\nThese scenarios are illustrated in Appendix A and in the supplementary video (http://bit.ly/ 2f9tacZ).\nThe first two scenarios are provided with the ViZDoom platform. In D1, the agent is in a square room and its health is declining at a constant rate. To survive, it must move around and collect health kits, which are distributed abundantly in the room. This task is easy: as long as the agent learns to avoid walls and keep traversing the room, performance is good. In D2, the agent is in a maze and its health is again declining at a constant rate. Here it must again collect health kits that increase its health, but it must also avoid blue poison vials that decrease health. This task is harder: the agent must learn to traverse irregularly shaped passageways, and to distinguish health kits from poison vials. In both tasks, the agent has access to three binary sub-actions: move forward, turn left, and turn right. Any combination of these three can be used at any given time, resulting in 8 possible actions. The only measurement provided to the agent in these scenarios is health.\nThe last two scenarios, D3 and D4, are more challenging and were designed by us using elements of the ViZDoom platform. Here the agent is armed and is under attack by alien monsters. The monsters spawn abundantly, move around in the environment, and shoot fireballs at the agent. Health kits and ammunition are sporadically distributed throughout the environment and can be collected by the agent. The environment is a simple maze in D3 and a more complex one in D4. In both scenarios, the agent has access to eight sub-actions: move forward, move backward, turn left, turn right, strafe left, strafe right, run, and shoot. Any combination of these sub-actions can be used, resulting in 256 possible actions. The agent is provided with three measurements: health, ammunition, and frag count (number of monsters killed).\nModels. We use two variants of the network architecture presented in Section 3. We refer to these variants as shallow and deep. Both follow the general structure shown in Figure 1, but differ in the exact configuration of the modules. The shallow version was configured to be as close as possible to the DQN model of Mnih et al. (2015), to ensure a fair comparison. This is the variant we use in all comparisons to prior work. The deep version has more layers but a much smaller number of parameters. This variant was used in experiments that do not involve prior work. Additional details are provided in Appendix B.\nTraining and testing. The agent is trained and tested over episodes. Each episode terminates after 525 steps (equivalent to 1 minute of real time) or when the agent’s health declines to zero. Reported measurements are averaged over the complete duration of each episode and across episodes."
    }, {
      "heading" : "4.2 RESULTS",
      "text" : "Comparison to prior work. We have compared the presented approach to three deep RL methods: DQN (Mnih et al., 2015), A3C (Mnih et al., 2016), and DSR (Kulkarni et al., 2016b). DQN is a standard baseline for visuomotor control due to its impressive performance on Atari games. A3C is more recent and is commonly regarded as the state of the art in this area. DSR is described in a recent technical report and we included it because the authors also used the ViZDoom platform in experiments, albeit with a simple task. Further details on the setup of the prior approaches are provided in Appendix C.\nThe performance of the different approaches during training is shown in Figure 2. In reporting the results of these experiments, we refer to our approach as DFP (direct future prediction). For the first two scenarios, all approaches were trained to maximize health. For these scenarios, Figure 2 reports average health over the course of training. For the last two scenarios, all approaches were trained to maximize a linear combination of the three normalized measurements (ammo, health, and frags) with coefficients (0.5, 0.5, 1). For these scenarios, Figure 2 reports average frags. Each data point in the plots averages information from 20,000 steps of testing.\nDQN, A3C, and DFP were trained for 50 million steps. The training procedure for DSR is much slower and can only process roughly 1 million simulation steps per day. For this reason, we were only able to evaluate DSR on the Basic scenario and were not able to perform extensive hyperparameter tuning. We report results for this technique after a week of training. (This week was sufficient\nto significantly exceed the number of training steps reported in the experiments of Kulkarni et al. (2016b), but not sufficient to approach the number of steps afforded by the other approaches.) For a fair comparison to prior work, DFP used the shallow version of our model.\nTable 1 reports the performance of the models after training. Each fully trained model was tested over 1 million simulation steps. (Consecutive episodes were automatically launched until the stated number of steps was reached for each method.) The table reports average health for scenarios D1 and D2, and av-\nerage frags for D3 and D4. We also report the training speed for each approach, in millions of simulation steps per day of training. The performance of the different models is additionally illustrated in the supplementary video.\nThe performance of DQN, A3C, and DFP is similar in the Basic scenario. As reported in Table 1, all three approaches achieve average health of around 95% during testing, with virtually identical performance by A3C and DFP. In the more complex Navigation scenario, a significant gap opens up between DQN and A3C; this is consistent with the experiments of Mnih et al. (2016). DFP achieves the best performance in this scenario, with a 5 percentage point advantage during testing. Note that in these first two scenarios DFP was only given a single measurement per time step, health.\nIn the more complex Battle and Battle 2 scenarios (D3 and D4), DFP dominates the other approaches. It outperforms A3C at test time by a factor of 3.6 in D3 and by a factor of 2.2 in D4. Note that the advantage of DFP is particularly significant in the scenarios that provide richer measurements: three measurements per time step in D3 and D4. The effect of multiple measurements is further evaluated in controlled experiments reported below.\nGeneralization across environments. We now evaluate how the behaviors learned by the presented approach generalize across different environments. To this end, we have created 100 ran-\ndomly textured versions of the mazes from scenarios D3 and D4. We used 90 of these for training and 10 for testing, with disjoint sets of textures in the training and testing environments. We call these scenarios D3-tx and D4-tx.\nTable 2 shows the performance of the approach for different combinations of training and testing regimes. For example, the entry in the D4-tx row of the D3 column shows the performance (in average number of frags) of a model trained in D3 and tested in D4-tx. Not surprisingly, a model trained in the simple D3 environment does not learn sufficient in-\nvariance to surface appearance to generalize well to other environments. Training in the more complex multi-texture environment in D4 yields better generalization: the trained model performs well in D3 and exhibits non-trivial performance in D3-tx and D4-tx. Finally, exposing the model to significant variation in surface appearance in D3-tx or D4-tx during training yields very good generalization.\nThe last column of Table 2 additionally reports the performance of a higher-capacity model trained in D4-tx. This model has the same architecture but each layer is wider by a factor of two. This combination is referred to as D4-tx-L. As shown in the table, this model performs even better.\nVisual Doom AI Competition. To further evaluate the presented approach, we enrolled in the Visual Doom AI Competition, held during September 2016. The competition evaluated sensorimotor control models that act based on raw visual input. The competition had the form of a tournament: the submitted agents play multiple games against each other, their performance measured by aggregate frags. The competition included two tracks. The Limited Deathmatch track was held in a known environment that was given to the participants in advance at training time. The Full Deathmatch track evaluated generalization to previously unseen environments and took place in multiple new environments that were not available to the participating teams at training time. We only enrolled in the Full Deathmatch track. Our model was trained using a variant of the D4-tx-L regime.\nOur model won, outperforming the second best submission by more than 50%. That submission, described by Lample & Chaplot (2016), constitutes a strong baseline. It is a deep recurrent Q-network that incorporates an LSTM and was trained using reward shaping and extra supervision from the game engine. Specifically, the authors took advantage of the ability provided by the ViZDoom platform to use the internal configuration of the game, including ground-truth knowledge of the presence of enemies in the field of view, during training. The authors’ report shows that this additional supervision improved performance significantly. Our model, which is simpler, achieved even higher performance without such additional supervision.\nGoal-agnostic training. We now evaluate the ability of the presented approach to learn behaviors that adapt to varying goals at test time. These experiments are performed in the Battle scenario. We use three training regimes: (a) fixed goal vector during training, (b) random goal vector with each value sampled uniformly from [0, 1] for every episode, (c) random goal vector with each value sampled uniformly from [−1, 1] for every episode. Intuitively, the second regime assumes that the agent will be maximizing all of the measurements at test time, to unknown degrees. The third regime makes no assumptions as to whether a measured quantity will be desirable or not.\nThe results are shown in Table 3. Each column corresponds to a training regime and each row to a different test-time goal. Goals are given by the weights of the three measurements (ammo, health, and frags) four seconds into the future. The first test-time goal in Table 3 is the goal vector used in the battle scenarios in the prior experiments, the second seeks to hoard ammunition, the third is a pacifist (maximize ammo and health, minimize frags), the fourth seeks to aimlessly drain ammunition. Each cell in the table reports the average value of each of the three measurements when a model that was trained as specified by the column was then tested as specified by the row.\nWe draw two main conclusions. First, on the main task (first row), goal-agnostic training performs as well as goal-driven training or even slightly better. Without knowing the final goal in advance, the agent learns to perform the task as well as when the goal was known at training time. Second,\nall agents, even the one trained with a fixed goal, generalize to new goals. Agents trained with randomized goals generalize better than the one trained with a fixed goal.\nAblation study. We now perform an ablation study using the D3-tx scenario. Specifically, we evaluate the importance of vectorial feedback versus scalar reward, and the effect of predicting measurements at multiple temporal offsets. The results are summarized in Table 4. The table reports the performance (in average frags at test time) of our full model (predicting three measurements at six tempo-\nral offsets) and of ablated variants that only predict frags (a scalar reward) and/or only predict at the farthest temporal offset. As the results demonstrate, predicting multiple measurements significantly improves the performance of the learned model, even when it is evaluated by only one of those measurements. Predicting measurements at multiple future times is also beneficial. This supports the intuition that a dense flow of multivariate measurements is a better training signal than a scalar reward."
    }, {
      "heading" : "5 DISCUSSION",
      "text" : "We presented an approach to sensorimotor control in immersive environments. Our approach is simple and demonstrates that supervised learning techniques can be adapted to learning to act in complex and dynamic three-dimensional environments given raw sensory input and intrinsic measurements. The model trains on raw experience, by interacting with the environment without extraneous supervision. Natural supervision is provided by the cotemporal structure of the sensory and measurement streams. Our experiments have demonstrated that this simple approach outperforms sophisticated deep reinforcement learning formulations on challenging tasks in immersive environments. Experiments have further demonstrated that the use of multivariate measurements provides a significant advantage over conventional scalar rewards and that the trained model can effectively adapt to new goals.\nThe presented work can be extended in multiple ways that are important for broadening the range of behaviors that can be learned. First, the presented model is purely reactive: it acts based on the current frame only, with no explicit facilities for memory and no test-time retention of internal representations. Recent work has explored memory-based models (Oh et al., 2016) and integrating such ideas with the presented approach may yield substantial advances. Second, significant progress in behavioral sophistication will likely require temporal abstraction and hierarchical organization of learned skills (Barto & Mahadevan, 2003; Kulkarni et al., 2016a). Third, the presented model was developed for discrete action spaces; applying the presented ideas to continuous action spaces would be interesting (Lillicrap et al., 2016). Finally, predicting features learned directly from rich sensory input can blur the distinction between sensory and measurement streams (Mathieu et al., 2016; Finn et al., 2016; Kalchbrenner et al., 2016)."
    }, {
      "heading" : "A SCENARIOS",
      "text" : "Figure A1 illustrates the four scenarios used in our experiments. Additional illustration is provided in the supplementary video (http://bit.ly/2f9tacZ).\nD1: Basic D2: Navigation\nD3: Battle D4: Battle 2\nFigure A1: Example frames from the four scenarios.\nB IMPLEMENTATION DETAILS\nB.1 NETWORK ARCHITECTURES\nThe detailed architectures of two network variants – shallow and deep – are shown in Tables B2 and B3. The shallow network follows the architecture of Mnih et al. (2015) as closely as possible. The deep one has more layers, but each layer is much narrower. The large one is the same as the deep one, but all layers are wider by a factor of two. The large and deep variants have fewer parameters than the shallow one: approximately 0.8 million in the deep network and 2.0 million in the large network, as compared to 6.4 million in the shallow network. In all networks we use the leaky ReLU nonlinearity LReLU(x) = x · δ[x > 0] + 0.2x · δ[x < 0] after each non-terminal layer. We initalized the weights of the network as proposed by He et al. (2015).\nWe empirically validate the architectural choices in the D3-tx regime. We compare the shallow and deep architectures, as well as three variants of the deep architecture:\n• No split: no expectation/action split, simply predict future measurements with a fullyconnected network.\n• Late split: predict both the expectation and the action-conditional differences with a single module.\n• Early split: predict the expectation and the action-conditional differences with two separate networks.\nThe results are reported in Table B1. The deep architecture performs slightly better than the shallow one, even though it has 8 times fewer parameters. All modifications of the deep architecture hurt performance, showing that (a) the two-stream formulation is beneficial, and (b) both too much and too little interaction between the two streams is detrimental to performance.\nshallow deep no split late split early split Score 10.9 11.4 9.7 8.3 8.2\nTable B1: Evaluation of different network architectures.\nModule Input dimension Channels Kernel Stride\nPerception\n84× 84× 1 32 8 4 21× 21× 32 64 4 2 10× 10× 64 64 3 1 10 · 10 · 64 512 − −\nMeasurement 3 128 − − 128 128 − − 128 128 − −\nGoal 3 · 6 128 − − 128 128 − − 128 128 − − Expectation 512 + 128 + 128 512 − − 512 3 · 6 − −\nAction 512 + 128 + 128 512 − − 512 3 · 6 · 256 − −\nTable B2: The shallow architecture.\nModule Input dimension Channels Kernel Stride\nPerception\n160× 120× 1 8 5 4 40× 30× 32 16 3 2 20× 15× 64 32 3 2 10× 8× 64 64 3 2 5 · 4 · 64 64 − −\nMeasurement 3 64 − − 64 64 − − 64 64 − −\nGoal 3 · 6 64 − − 64 64 − − 64 64 − − Expectation 64 + 64 + 64 128 − −\n128 128 − − 128 3 · 6 − −\nAction 64 + 64 + 64 128 − −\n128 128 − − 128 3 · 6 · 256 − −\nTable B3: The deep architecture.\nB.2 OTHER DETAILS\nThe raw sensory input to the agent is the observed image, in grayscale, without any additional preprocessing. The resolution is 84 × 84 pixels for the shallow model and 160 × 120 pixels for the deep one. We normalized the measurements by their standard deviations under random exploration.\nWe performed frame skipping during both training and testing. The agent observes the environment and selects an action every 4-th frame. The selected action is repeated during the skipped frames. This accelerates training without sacrificing accuracy. In the paper, “step” always refers to steps after frame skipping (equivalent to every 4-th step before frame skipping). When played by a human, Doom runs at 35 frames per second, so one step of the agent is equivalent to 114 milliseconds of real time. Therefore, frame skipping has the added benefit of bringing the reaction time of the agent closer to that of a human.\nWe set the temporal offsets τ1, . . . , τn of the predicted future measurements to 1, 2, 4, 8, 16, 32 steps in all experiments. The longest temporal offset corresponds to 3.66 seconds of real time.\nWe used an experience memory ofM = 20,000 steps, and sampled a mini-batch ofN = 64 samples after every k = 64 new experiences added. We added the experiences to the memory using 8 copies of the agent running in parallel. The networks in all experiments were trained using the Adam algorithm (Kingma & Ba, 2015) with β1 = 0.95, β2 = 0.999, and ε = 10−4. The initial learning rate is set to 10−4 and is gradually decreased during training. The shallow networks were trained for 800,000 mini-batch iterations (or 51.2 million steps), the deep ones for 1,000,000 iterations, the large one for 2,500,000 iterations."
    }, {
      "heading" : "C BASELINES",
      "text" : "We compared our approach to three prior methods: DQN (Mnih et al., 2015), DSR (Kulkarni et al., 2016b), and A3C (Mnih et al., 2016). We used the authors’ implementations of DQN (https://github.com/kuz/DeepMind-Atari-Deep-Q-Learner) and DSR (https://github.com/Ardavans/DSR), and an independent implementation of A3C (https://github.com/muupan/async-rl). For scenarios D1 and D2 we used the change in health as reward. For D3 and D4 we used a linear combination of changes of the three measurements with the same coefficients as for the presented approach: (0.5, 0.5, 1). For DQN we tested three learning rates: the default one (0.00025) and two alternatives (0.00005 and 0.00002). Other DQN hyperparameters were left at their default values. DSR was tested with the default hyperparameters only due to its restrictive running time. For A3C, which trains faster, we performed a search over a set of learning rates ({2, 4, 8, 16, 32} · 10−4) for the first two tasks; for the last two tasks we trained 20 models with random learning rates sampled log-uniformly between 10−4 and 10−2 and random β (entropy regularization) sampled log-uniformly between 10−4 and 10−1. For all baselines we report the best results we were able to obtain."
    } ],
    "references" : [ {
      "title" : "Motor development",
      "author" : [ "Karen E. Adolph", "Sarah E. Berger" ],
      "venue" : "In Handbook of Child Psychology,",
      "citeRegEx" : "Adolph and Berger.,? \\Q2006\\E",
      "shortCiteRegEx" : "Adolph and Berger.",
      "year" : 2006
    }, {
      "title" : "Recent advances in hierarchical reinforcement learning",
      "author" : [ "Andrew G. Barto", "Sridhar Mahadevan" ],
      "venue" : "Discrete Event Dynamic Systems,",
      "citeRegEx" : "Barto and Mahadevan.,? \\Q2003\\E",
      "shortCiteRegEx" : "Barto and Mahadevan.",
      "year" : 2003
    }, {
      "title" : "A counterexample to temporal differences learning",
      "author" : [ "Dimitri P. Bertsekas" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Bertsekas.,? \\Q1995\\E",
      "shortCiteRegEx" : "Bertsekas.",
      "year" : 1995
    }, {
      "title" : "Pathologies of temporal difference methods in approximate dynamic programming",
      "author" : [ "Dimitri P. Bertsekas" ],
      "venue" : "In IEEE Conference on Decision and Control,",
      "citeRegEx" : "Bertsekas.,? \\Q2010\\E",
      "shortCiteRegEx" : "Bertsekas.",
      "year" : 2010
    }, {
      "title" : "Model-free episodic control",
      "author" : [ "Charles Blundell", "Benigno Uria", "Alexander Pritzel", "Yazhe Li", "Avraham Ruderman", "Joel Z. Leibo", "Jack Rae", "Daan Wierstra", "Demis Hassabis" ],
      "venue" : null,
      "citeRegEx" : "Blundell et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Blundell et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning parameterized skills",
      "author" : [ "Bruno Castro da Silva", "George Konidaris", "Andrew G. Barto" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Silva et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Silva et al\\.",
      "year" : 2012
    }, {
      "title" : "Multi-task policy search for robotics",
      "author" : [ "Marc Peter Deisenroth", "Peter Englert", "Jan Peters", "Dieter Fox" ],
      "venue" : "In ICRA,",
      "citeRegEx" : "Deisenroth et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Deisenroth et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning rates for Q-learning",
      "author" : [ "Eyal Even-Dar", "Yishay Mansour" ],
      "venue" : null,
      "citeRegEx" : "Even.Dar and Mansour.,? \\Q2003\\E",
      "shortCiteRegEx" : "Even.Dar and Mansour.",
      "year" : 2003
    }, {
      "title" : "Unsupervised learning for physical interaction through video prediction",
      "author" : [ "Chelsea Finn", "Ian J. Goodfellow", "Sergey Levine" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Finn et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Finn et al\\.",
      "year" : 2016
    }, {
      "title" : "Multi-criteria reinforcement learning",
      "author" : [ "Zoltán Gábor", "Zsolt Kalmár", "Csaba Szepesvári" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Gábor et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Gábor et al\\.",
      "year" : 1998
    }, {
      "title" : "Delving deep into rectifiers: Surpassing humanlevel performance on ImageNet classification",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Forward models: Supervised learning with a distal teacher",
      "author" : [ "Michael I. Jordan", "David E. Rumelhart" ],
      "venue" : "Cognitive Science,",
      "citeRegEx" : "Jordan and Rumelhart.,? \\Q1992\\E",
      "shortCiteRegEx" : "Jordan and Rumelhart.",
      "year" : 1992
    }, {
      "title" : "ViZDoom: A Doom-based AI research platform for visual reinforcement learning",
      "author" : [ "Michał Kempka", "Marek Wydmuch", "Grzegorz Runc", "Jakub Toczek", "Wojciech Jaśkowski" ],
      "venue" : "In IEEE Conference on Computational Intelligence and Games,",
      "citeRegEx" : "Kempka et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kempka et al\\.",
      "year" : 2016
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Kingma and Ba.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Reinforcement learning to adjust parametrized motor primitives to new situations",
      "author" : [ "Jens Kober", "Andreas Wilhelm", "Erhan Oztop", "Jan Peters" ],
      "venue" : "Autonomous Robots,",
      "citeRegEx" : "Kober et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Kober et al\\.",
      "year" : 2012
    }, {
      "title" : "Reinforcement learning in robotics: A survey",
      "author" : [ "Jens Kober", "J. Andrew Bagnell", "Jan Peters" ],
      "venue" : "IJRR, 32(11),",
      "citeRegEx" : "Kober et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Kober et al\\.",
      "year" : 2013
    }, {
      "title" : "Transfer in reinforcement learning via shared features",
      "author" : [ "George Konidaris", "Ilya Scheidwasser", "Andrew G. Barto" ],
      "venue" : null,
      "citeRegEx" : "Konidaris et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Konidaris et al\\.",
      "year" : 2012
    }, {
      "title" : "Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation",
      "author" : [ "Tejas D. Kulkarni", "Karthik Narasimhan", "Ardavan Saeedi", "Joshua B. Tenenbaum" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Kulkarni et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kulkarni et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep successor reinforcement learning",
      "author" : [ "Tejas D. Kulkarni", "Ardavan Saeedi", "Simanta Gautam", "Samuel J. Gershman" ],
      "venue" : null,
      "citeRegEx" : "Kulkarni et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kulkarni et al\\.",
      "year" : 2016
    }, {
      "title" : "Masters of Doom: How Two Guys Created an Empire and Transformed Pop Culture",
      "author" : [ "David Kushner" ],
      "venue" : "Random House,",
      "citeRegEx" : "Kushner.,? \\Q2003\\E",
      "shortCiteRegEx" : "Kushner.",
      "year" : 2003
    }, {
      "title" : "Building machines that learn and think like people",
      "author" : [ "Brenden M. Lake", "Tomer D. Ullman", "Joshua B. Tenenbaum", "Samuel J. Gershman" ],
      "venue" : null,
      "citeRegEx" : "Lake et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lake et al\\.",
      "year" : 2016
    }, {
      "title" : "Playing FPS games with deep reinforcement learning",
      "author" : [ "Guillaume Lample", "Devendra Singh Chaplot" ],
      "venue" : null,
      "citeRegEx" : "Lample and Chaplot.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lample and Chaplot.",
      "year" : 2016
    }, {
      "title" : "Off-road obstacle avoidance through end-toend learning",
      "author" : [ "Yann LeCun", "Urs Muller", "Jan Ben", "Eric Cosatto", "Beat Flepp" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "LeCun et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 2005
    }, {
      "title" : "Guided policy search",
      "author" : [ "Sergey Levine", "Vladlen Koltun" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Levine and Koltun.,? \\Q2013\\E",
      "shortCiteRegEx" : "Levine and Koltun.",
      "year" : 2013
    }, {
      "title" : "Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection",
      "author" : [ "Sergey Levine", "Peter Pastor", "Alex Krizhevsky", "Deirdre Quillen" ],
      "venue" : "ISER,",
      "citeRegEx" : "Levine et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Levine et al\\.",
      "year" : 2016
    }, {
      "title" : "Continuous control with deep reinforcement learning",
      "author" : [ "Timothy P. Lillicrap", "Jonathan J. Hunt", "Alexander Pritzel", "Nicolas Heess", "Tom Erez", "Yuval Tassa", "David Silver", "Daan Wierstra" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Lillicrap et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lillicrap et al\\.",
      "year" : 2016
    }, {
      "title" : "Predictive representations of state",
      "author" : [ "Michael L. Littman", "Richard S. Sutton", "Satinder P. Singh" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Littman et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Littman et al\\.",
      "year" : 2001
    }, {
      "title" : "Deep multi-scale video prediction beyond mean square error",
      "author" : [ "Michaël Mathieu", "Camille Couprie", "Yann LeCun" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Mathieu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Mathieu et al\\.",
      "year" : 2016
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A. Rusu", "Joel Veness", "Marc G. Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K. Fidjeland", "Georg Ostrovski", "Stig Petersen", "Charles Beattie", "Amir Sadik" ],
      "venue" : null,
      "citeRegEx" : "Mnih et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2015
    }, {
      "title" : "Asynchronous methods for deep reinforcement learning",
      "author" : [ "Volodymyr Mnih", "Adrià Puigdomènech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P. Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu" ],
      "venue" : null,
      "citeRegEx" : "Mnih et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2016
    }, {
      "title" : "Machine Learning: A Probabilistic Perspective",
      "author" : [ "Kevin P. Murphy" ],
      "venue" : null,
      "citeRegEx" : "Murphy.,? \\Q2012\\E",
      "shortCiteRegEx" : "Murphy.",
      "year" : 2012
    }, {
      "title" : "Action-conditional video prediction using deep networks in Atari games",
      "author" : [ "Junhyuk Oh", "Xiaoxiao Guo", "Honglak Lee", "Richard L. Lewis", "Satinder P. Singh" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Oh et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Oh et al\\.",
      "year" : 2015
    }, {
      "title" : "Control of memory, active perception, and action in Minecraft",
      "author" : [ "Junhyuk Oh", "Valliappa Chockalingam", "Satinder P. Singh", "Honglak Lee" ],
      "venue" : null,
      "citeRegEx" : "Oh et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Oh et al\\.",
      "year" : 2016
    }, {
      "title" : "A survey of multi-objective sequential decision-making",
      "author" : [ "Diederik M. Roijers", "Peter Vamplew", "Shimon Whiteson", "Richard Dazeley" ],
      "venue" : null,
      "citeRegEx" : "Roijers et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Roijers et al\\.",
      "year" : 2013
    }, {
      "title" : "Learning monocular reactive UAV control in cluttered natural environments",
      "author" : [ "Stéphane Ross", "Narek Melik-Barkhudarov", "Kumar Shaurya Shankar", "Andreas Wendel", "Debadeepta Dey", "J. Andrew Bagnell", "Martial Hebert" ],
      "venue" : "In ICRA,",
      "citeRegEx" : "Ross et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Ross et al\\.",
      "year" : 2013
    }, {
      "title" : "Universal value function approximators",
      "author" : [ "Tom Schaul", "Daniel Horgan", "Karol Gregor", "David Silver" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Schaul et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Schaul et al\\.",
      "year" : 2015
    }, {
      "title" : "Mastering the game of Go with deep neural networks and tree",
      "author" : [ "David Silver", "Aja Huang", "Chris J. Maddison", "Arthur Guez", "Laurent Sifre", "George van den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot", "Sander Dieleman", "Dominik Grewe" ],
      "venue" : "search. Nature,",
      "citeRegEx" : "Silver et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Silver et al\\.",
      "year" : 2016
    }, {
      "title" : "Reinforcement learning with replacing eligibility traces",
      "author" : [ "Satinder P. Singh", "Richard S. Sutton" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Singh and Sutton.,? \\Q1996\\E",
      "shortCiteRegEx" : "Singh and Sutton.",
      "year" : 1996
    }, {
      "title" : "Learning predictive state representations",
      "author" : [ "Satinder P. Singh", "Michael L. Littman", "Nicholas K. Jong", "David Pardoe", "Peter Stone" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Singh et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Singh et al\\.",
      "year" : 2003
    }, {
      "title" : "Learning to predict by the methods of temporal differences",
      "author" : [ "Richard S. Sutton" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Sutton.,? \\Q1988\\E",
      "shortCiteRegEx" : "Sutton.",
      "year" : 1988
    }, {
      "title" : "Generalization in reinforcement learning: Successful examples using sparse coarse coding",
      "author" : [ "Richard S. Sutton" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Sutton.,? \\Q1995\\E",
      "shortCiteRegEx" : "Sutton.",
      "year" : 1995
    }, {
      "title" : "Reinforcement Learning: An Introduction",
      "author" : [ "Richard S. Sutton", "Andrew G. Barto" ],
      "venue" : "MIT Press,",
      "citeRegEx" : "Sutton and Barto.,? \\Q2017\\E",
      "shortCiteRegEx" : "Sutton and Barto.",
      "year" : 2017
    }, {
      "title" : "Horde: a scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction",
      "author" : [ "Richard S. Sutton", "Joseph Modayil", "Michael Delp", "Thomas Degris", "Patrick M. Pilarski", "Adam White", "Doina Precup" ],
      "venue" : "In AAMAS,",
      "citeRegEx" : "Sutton et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 2011
    }, {
      "title" : "A unified analysis of value-function-based reinforcement learning algorithms",
      "author" : [ "Csaba Szepesvári", "Michael L. Littman" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Szepesvári and Littman.,? \\Q1999\\E",
      "shortCiteRegEx" : "Szepesvári and Littman.",
      "year" : 1999
    }, {
      "title" : "TD-gammon, a self-teaching backgammon program, achieves master-level play",
      "author" : [ "Gerald Tesauro" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Tesauro.,? \\Q1994\\E",
      "shortCiteRegEx" : "Tesauro.",
      "year" : 1994
    }, {
      "title" : "On the convergence of optimistic policy iteration",
      "author" : [ "John N. Tsitsiklis" ],
      "venue" : null,
      "citeRegEx" : "Tsitsiklis.,? \\Q2002\\E",
      "shortCiteRegEx" : "Tsitsiklis.",
      "year" : 2002
    }, {
      "title" : "WaveNet: A generative model for raw audio",
      "author" : [ "Aäron van den Oord", "Sander Dieleman", "Heiga Zen", "Karen Simonyan", "Oriol Vinyals", "Alex Graves", "Nal Kalchbrenner", "Andrew W. Senior", "Koray Kavukcuoglu" ],
      "venue" : null,
      "citeRegEx" : "Oord et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Oord et al\\.",
      "year" : 2016
    }, {
      "title" : "Dueling network architectures for deep reinforcement learning",
      "author" : [ "Ziyu Wang", "Tom Schaul", "Matteo Hessel", "Hado van Hasselt", "Marc Lanctot", "Nando de Freitas" ],
      "venue" : null,
      "citeRegEx" : "Wang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "The shallow network follows the architecture",
      "author" : [ ],
      "venue" : null,
      "citeRegEx" : "B3.,? \\Q2015\\E",
      "shortCiteRegEx" : "B3.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 30,
      "context" : "In this view, supervised learning is concerned with learning input-output mappings, unsupervised learning aims to find hidden structure in data, and reinforcement learning deals with goal-directed behavior (Murphy, 2012).",
      "startOffset" : 206,
      "endOffset" : 220
    }, {
      "referenceID" : 28,
      "context" : "While reinforcement learning (RL) has achieved significant progress (Mnih et al., 2015), key challenges remain.",
      "startOffset" : 68,
      "endOffset" : 87
    }, {
      "referenceID" : 20,
      "context" : "Another is the acquisition of general skills that can be flexibly deployed to accomplish a multitude of dynamically specified goals (Lake et al., 2016).",
      "startOffset" : 132,
      "endOffset" : 151
    }, {
      "referenceID" : 44,
      "context" : "While a sparse scalar reward may be the only feedback available in a board game (Tesauro, 1994; Silver et al., 2016), a multidimensional stream of sensations is a more appropriate model for an organism that is learning to function in an immersive environment (Adolph & Berger, 2006).",
      "startOffset" : 80,
      "endOffset" : 116
    }, {
      "referenceID" : 36,
      "context" : "While a sparse scalar reward may be the only feedback available in a board game (Tesauro, 1994; Silver et al., 2016), a multidimensional stream of sensations is a more appropriate model for an organism that is learning to function in an immersive environment (Adolph & Berger, 2006).",
      "startOffset" : 80,
      "endOffset" : 116
    }, {
      "referenceID" : 19,
      "context" : "We use the classical first-person game Doom, which introduced immersive three-dimensional games to popular culture (Kushner, 2003).",
      "startOffset" : 115,
      "endOffset" : 130
    }, {
      "referenceID" : 15,
      "context" : "TD and policy gradient methods have since come to dominate the study of sensorimotor learning (Kober et al., 2013; Mnih et al., 2015; Sutton & Barto, 2017).",
      "startOffset" : 94,
      "endOffset" : 155
    }, {
      "referenceID" : 28,
      "context" : "TD and policy gradient methods have since come to dominate the study of sensorimotor learning (Kober et al., 2013; Mnih et al., 2015; Sutton & Barto, 2017).",
      "startOffset" : 94,
      "endOffset" : 155
    }, {
      "referenceID" : 22,
      "context" : "While the use of SL is natural in imitation learning (LeCun et al., 2005; Ross et al., 2013) or in conjunction with model-based RL (Levine & Koltun, 2013), the formulation of sensorimotor learning from raw experience as supervised learning is rare (Levine et al.",
      "startOffset" : 53,
      "endOffset" : 92
    }, {
      "referenceID" : 34,
      "context" : "While the use of SL is natural in imitation learning (LeCun et al., 2005; Ross et al., 2013) or in conjunction with model-based RL (Levine & Koltun, 2013), the formulation of sensorimotor learning from raw experience as supervised learning is rare (Levine et al.",
      "startOffset" : 53,
      "endOffset" : 92
    }, {
      "referenceID" : 24,
      "context" : ", 2013) or in conjunction with model-based RL (Levine & Koltun, 2013), the formulation of sensorimotor learning from raw experience as supervised learning is rare (Levine et al., 2016).",
      "startOffset" : 163,
      "endOffset" : 184
    }, {
      "referenceID" : 2,
      "context" : "The convergence of such methods was analyzed early on and they were seen as theoretically advantageous, particularly when function approximators are used (Bertsekas, 1995; Sutton, 1995; Singh & Sutton, 1996).",
      "startOffset" : 154,
      "endOffset" : 207
    }, {
      "referenceID" : 40,
      "context" : "The convergence of such methods was analyzed early on and they were seen as theoretically advantageous, particularly when function approximators are used (Bertsekas, 1995; Sutton, 1995; Singh & Sutton, 1996).",
      "startOffset" : 154,
      "endOffset" : 207
    }, {
      "referenceID" : 40,
      "context" : "The choice of TD learning over Monte Carlo methods was argued on practical grounds, based on empirical performance on canonical examples (Sutton, 1995).",
      "startOffset" : 137,
      "endOffset" : 151
    }, {
      "referenceID" : 45,
      "context" : "While the understanding of the convergence of both types of methods has since improved (Szepesvári & Littman, 1999; Tsitsiklis, 2002; Even-Dar & Mansour, 2003), the argument for TD versus Monte Carlo is to this day empirical (Sutton & Barto, 2017).",
      "startOffset" : 87,
      "endOffset" : 159
    }, {
      "referenceID" : 3,
      "context" : "Sharp negative examples exist (Bertsekas, 2010).",
      "startOffset" : 30,
      "endOffset" : 47
    }, {
      "referenceID" : 30,
      "context" : "Sutton (1988) analyzed temporal-difference (TD) learning and argued that it is preferable to SL for prediction problems in which the correctness of the prediction is revealed many steps after the prediction is made.",
      "startOffset" : 0,
      "endOffset" : 14
    }, {
      "referenceID" : 9,
      "context" : "Vector-valued feedback has been considered in the context of multi-objective decision-making (Gábor et al., 1998; Roijers et al., 2013).",
      "startOffset" : 93,
      "endOffset" : 135
    }, {
      "referenceID" : 33,
      "context" : "Vector-valued feedback has been considered in the context of multi-objective decision-making (Gábor et al., 1998; Roijers et al., 2013).",
      "startOffset" : 93,
      "endOffset" : 135
    }, {
      "referenceID" : 14,
      "context" : "Parameterized goals have been studied in the context of continuous motor skills such as throwing darts at a target (da Silva et al., 2012; Kober et al., 2012; Deisenroth et al., 2014).",
      "startOffset" : 115,
      "endOffset" : 183
    }, {
      "referenceID" : 6,
      "context" : "Parameterized goals have been studied in the context of continuous motor skills such as throwing darts at a target (da Silva et al., 2012; Kober et al., 2012; Deisenroth et al., 2014).",
      "startOffset" : 115,
      "endOffset" : 183
    }, {
      "referenceID" : 27,
      "context" : "Prediction of full sensory input in realistic three-dimensional environments remains an open challenge, although significant progress is being made (Mathieu et al., 2016; Finn et al., 2016; Kalchbrenner et al., 2016).",
      "startOffset" : 148,
      "endOffset" : 216
    }, {
      "referenceID" : 8,
      "context" : "Prediction of full sensory input in realistic three-dimensional environments remains an open challenge, although significant progress is being made (Mathieu et al., 2016; Finn et al., 2016; Kalchbrenner et al., 2016).",
      "startOffset" : 148,
      "endOffset" : 216
    }, {
      "referenceID" : 5,
      "context" : "Vector-valued feedback has been considered in the context of multi-objective decision-making (Gábor et al., 1998; Roijers et al., 2013). Transfer across related tasks has been analyzed by Konidaris et al. (2012). Parameterized goals have been studied in the context of continuous motor skills such as throwing darts at a target (da Silva et al.",
      "startOffset" : 94,
      "endOffset" : 212
    }, {
      "referenceID" : 4,
      "context" : "Parameterized goals have been studied in the context of continuous motor skills such as throwing darts at a target (da Silva et al., 2012; Kober et al., 2012; Deisenroth et al., 2014). A general framework for sharing value function approximators across both states and goals has been described by Schaul et al. (2015). Our work is related, but develops a new model and shows that it achieves state-of-the-art performance in complex and dynamic three-dimensional environments.",
      "startOffset" : 119,
      "endOffset" : 318
    }, {
      "referenceID" : 4,
      "context" : "Parameterized goals have been studied in the context of continuous motor skills such as throwing darts at a target (da Silva et al., 2012; Kober et al., 2012; Deisenroth et al., 2014). A general framework for sharing value function approximators across both states and goals has been described by Schaul et al. (2015). Our work is related, but develops a new model and shows that it achieves state-of-the-art performance in complex and dynamic three-dimensional environments. Learning to act in simulated environments has been the focus of significant attention following the successful application of deep RL to Atari games by Mnih et al. (2015). A number of recent efforts applied related ideas to three-dimensional environments.",
      "startOffset" : 119,
      "endOffset" : 647
    }, {
      "referenceID" : 4,
      "context" : "Parameterized goals have been studied in the context of continuous motor skills such as throwing darts at a target (da Silva et al., 2012; Kober et al., 2012; Deisenroth et al., 2014). A general framework for sharing value function approximators across both states and goals has been described by Schaul et al. (2015). Our work is related, but develops a new model and shows that it achieves state-of-the-art performance in complex and dynamic three-dimensional environments. Learning to act in simulated environments has been the focus of significant attention following the successful application of deep RL to Atari games by Mnih et al. (2015). A number of recent efforts applied related ideas to three-dimensional environments. Lillicrap et al. (2016) considered continuous and high-dimensional action spaces and learned control policies in the TORCS simulator.",
      "startOffset" : 119,
      "endOffset" : 756
    }, {
      "referenceID" : 4,
      "context" : "Parameterized goals have been studied in the context of continuous motor skills such as throwing darts at a target (da Silva et al., 2012; Kober et al., 2012; Deisenroth et al., 2014). A general framework for sharing value function approximators across both states and goals has been described by Schaul et al. (2015). Our work is related, but develops a new model and shows that it achieves state-of-the-art performance in complex and dynamic three-dimensional environments. Learning to act in simulated environments has been the focus of significant attention following the successful application of deep RL to Atari games by Mnih et al. (2015). A number of recent efforts applied related ideas to three-dimensional environments. Lillicrap et al. (2016) considered continuous and high-dimensional action spaces and learned control policies in the TORCS simulator. Mnih et al. (2016) described asynchronous variants of deep RL methods and demonstrated navigation in a three-dimensional labyrinth.",
      "startOffset" : 119,
      "endOffset" : 885
    }, {
      "referenceID" : 4,
      "context" : "Parameterized goals have been studied in the context of continuous motor skills such as throwing darts at a target (da Silva et al., 2012; Kober et al., 2012; Deisenroth et al., 2014). A general framework for sharing value function approximators across both states and goals has been described by Schaul et al. (2015). Our work is related, but develops a new model and shows that it achieves state-of-the-art performance in complex and dynamic three-dimensional environments. Learning to act in simulated environments has been the focus of significant attention following the successful application of deep RL to Atari games by Mnih et al. (2015). A number of recent efforts applied related ideas to three-dimensional environments. Lillicrap et al. (2016) considered continuous and high-dimensional action spaces and learned control policies in the TORCS simulator. Mnih et al. (2016) described asynchronous variants of deep RL methods and demonstrated navigation in a three-dimensional labyrinth. Oh et al. (2016) augmented deep Q-networks with external memory and evaluated their performance on a set of tasks in Minecraft.",
      "startOffset" : 119,
      "endOffset" : 1015
    }, {
      "referenceID" : 4,
      "context" : "Parameterized goals have been studied in the context of continuous motor skills such as throwing darts at a target (da Silva et al., 2012; Kober et al., 2012; Deisenroth et al., 2014). A general framework for sharing value function approximators across both states and goals has been described by Schaul et al. (2015). Our work is related, but develops a new model and shows that it achieves state-of-the-art performance in complex and dynamic three-dimensional environments. Learning to act in simulated environments has been the focus of significant attention following the successful application of deep RL to Atari games by Mnih et al. (2015). A number of recent efforts applied related ideas to three-dimensional environments. Lillicrap et al. (2016) considered continuous and high-dimensional action spaces and learned control policies in the TORCS simulator. Mnih et al. (2016) described asynchronous variants of deep RL methods and demonstrated navigation in a three-dimensional labyrinth. Oh et al. (2016) augmented deep Q-networks with external memory and evaluated their performance on a set of tasks in Minecraft. In a recent technical report, Kulkarni et al. (2016b) proposed end-to-end training of successor representations and demonstrated navigation in the Doom game engine.",
      "startOffset" : 119,
      "endOffset" : 1180
    }, {
      "referenceID" : 4,
      "context" : "In another recent report, Blundell et al. (2016) considered a nonparametric approach to control and conducted experiments in a three-dimensional labyrinth.",
      "startOffset" : 26,
      "endOffset" : 49
    }, {
      "referenceID" : 4,
      "context" : "In another recent report, Blundell et al. (2016) considered a nonparametric approach to control and conducted experiments in a three-dimensional labyrinth. We compare to state-of-the-art deep RL methods in Section 4 and demonstrate that the presented approach performs favorably. Prediction of future states in dynamical systems was considered by Littman et al. (2001) and Singh et al.",
      "startOffset" : 26,
      "endOffset" : 369
    }, {
      "referenceID" : 4,
      "context" : "In another recent report, Blundell et al. (2016) considered a nonparametric approach to control and conducted experiments in a three-dimensional labyrinth. We compare to state-of-the-art deep RL methods in Section 4 and demonstrate that the presented approach performs favorably. Prediction of future states in dynamical systems was considered by Littman et al. (2001) and Singh et al. (2003). Predictive representations in the form of generalized value functions were advocated by Sutton et al.",
      "startOffset" : 26,
      "endOffset" : 393
    }, {
      "referenceID" : 4,
      "context" : "In another recent report, Blundell et al. (2016) considered a nonparametric approach to control and conducted experiments in a three-dimensional labyrinth. We compare to state-of-the-art deep RL methods in Section 4 and demonstrate that the presented approach performs favorably. Prediction of future states in dynamical systems was considered by Littman et al. (2001) and Singh et al. (2003). Predictive representations in the form of generalized value functions were advocated by Sutton et al. (2011). More recently, Oh et al.",
      "startOffset" : 26,
      "endOffset" : 503
    }, {
      "referenceID" : 4,
      "context" : "In another recent report, Blundell et al. (2016) considered a nonparametric approach to control and conducted experiments in a three-dimensional labyrinth. We compare to state-of-the-art deep RL methods in Section 4 and demonstrate that the presented approach performs favorably. Prediction of future states in dynamical systems was considered by Littman et al. (2001) and Singh et al. (2003). Predictive representations in the form of generalized value functions were advocated by Sutton et al. (2011). More recently, Oh et al. (2015) learned to predict future frames in Atari games.",
      "startOffset" : 26,
      "endOffset" : 536
    }, {
      "referenceID" : 46,
      "context" : "(2016) and van den Oord et al. (2016) for impressive recent progress).",
      "startOffset" : 19,
      "endOffset" : 38
    }, {
      "referenceID" : 47,
      "context" : "To this end, we build on the ideas of Wang et al. (2016) and split the prediction module into two streams: an expectation stream E(j) and an action stream A(j).",
      "startOffset" : 38,
      "endOffset" : 57
    }, {
      "referenceID" : 12,
      "context" : "To interface with the game engine, we use the ViZDoom platform developed by Kempka et al. (2016). One of the advantages of this platform is that it allows running the simulation at thousands of frames per second on a single CPU core, which enables training models on tens of millions of simulation steps in a single day.",
      "startOffset" : 76,
      "endOffset" : 97
    }, {
      "referenceID" : 28,
      "context" : "The shallow version was configured to be as close as possible to the DQN model of Mnih et al. (2015), to ensure a fair comparison.",
      "startOffset" : 82,
      "endOffset" : 101
    }, {
      "referenceID" : 28,
      "context" : "We have compared the presented approach to three deep RL methods: DQN (Mnih et al., 2015), A3C (Mnih et al.",
      "startOffset" : 70,
      "endOffset" : 89
    }, {
      "referenceID" : 29,
      "context" : ", 2015), A3C (Mnih et al., 2016), and DSR (Kulkarni et al.",
      "startOffset" : 13,
      "endOffset" : 32
    }, {
      "referenceID" : 17,
      "context" : "to significantly exceed the number of training steps reported in the experiments of Kulkarni et al. (2016b), but not sufficient to approach the number of steps afforded by the other approaches.",
      "startOffset" : 84,
      "endOffset" : 108
    }, {
      "referenceID" : 28,
      "context" : "In the more complex Navigation scenario, a significant gap opens up between DQN and A3C; this is consistent with the experiments of Mnih et al. (2016). DFP achieves the best performance in this scenario, with a 5 percentage point advantage during testing.",
      "startOffset" : 132,
      "endOffset" : 151
    }, {
      "referenceID" : 32,
      "context" : "Recent work has explored memory-based models (Oh et al., 2016) and integrating such ideas with the presented approach may yield substantial advances.",
      "startOffset" : 45,
      "endOffset" : 62
    }, {
      "referenceID" : 25,
      "context" : "Third, the presented model was developed for discrete action spaces; applying the presented ideas to continuous action spaces would be interesting (Lillicrap et al., 2016).",
      "startOffset" : 147,
      "endOffset" : 171
    }, {
      "referenceID" : 27,
      "context" : "Finally, predicting features learned directly from rich sensory input can blur the distinction between sensory and measurement streams (Mathieu et al., 2016; Finn et al., 2016; Kalchbrenner et al., 2016).",
      "startOffset" : 135,
      "endOffset" : 203
    }, {
      "referenceID" : 8,
      "context" : "Finally, predicting features learned directly from rich sensory input can blur the distinction between sensory and measurement streams (Mathieu et al., 2016; Finn et al., 2016; Kalchbrenner et al., 2016).",
      "startOffset" : 135,
      "endOffset" : 203
    } ],
    "year" : 2016,
    "abstractText" : "We present an approach to sensorimotor control in immersive environments. Our approach utilizes a high-dimensional sensory stream and a lower-dimensional measurement stream. The cotemporal structure of these streams provides a rich supervisory signal, which enables training a sensorimotor control model by interacting with the environment. The model is trained using supervised learning techniques, but without extraneous supervision. It learns to act based on raw sensory input from a complex three-dimensional environment. The presented formulation allows changing the agent’s goal at test time. We conduct extensive experiments in three-dimensional simulations based on the classical first-person game Doom. The results demonstrate that the presented approach outperforms sophisticated prior formulations, particularly on challenging tasks. The results also show that trained models successfully generalize across environments and goals. A model trained using the presented approach won the Full Deathmatch track of the Visual Doom AI Competition, which was held in previously unseen environments.",
    "creator" : "LaTeX with hyperref package"
  }
}