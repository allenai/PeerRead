{
  "name" : "1704.04100.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Cross-lingual and cross-domain discourse segmentation of entire documents",
    "authors" : [ "Chloé Braud" ],
    "emails" : [ "chloe.braud@gmail.com", "lacroix@di.ku.dk", "soegaard@di.ku.dk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 4.\n04 10\n0v 2\n[ cs\n.C L\n] 2\n4 A\npr 2\n01 7\nDiscourse segmentation is a crucial step in building end-to-end discourse parsers. However, discourse segmenters only exist for a few languages and domains. Typically they only detect intra-sentential segment boundaries, assuming gold standard sentence and token segmentation, and relying on high-quality syntactic parses and rich heuristics that are not generally available across languages and domains. In this paper, we propose statistical discourse segmenters for five languages and three domains that do not rely on gold preannotations. We also consider the problem of learning discourse segmenters when no labeled data is available for a language. Our fully supervised system obtains 89.5% F1 for English newswire, with slight drops in performance on other domains, and we report supervised and unsupervised (cross-lingual) results for five languages in total."
    }, {
      "heading" : "1 Introduction",
      "text" : "Discourse segmentation is the first step in building a discourse parser. The goal is to identify the minimal units — called Elementary Discourse Units (EDU)— in the documents that will then be linked by discourse relations. For example, the sentences (1a) and (1b)1 are each segmented into two EDUs, then respectively linked by a CONTRAST and an ATTRIBUTION relation. The EDUs are mostly clauses and may cover a full sentence. This step is crucial: making a segmentation error leads to an error in the final analysis. Discourse segmentation can also inform other tasks, such as argumentation\n1The examples come from the RST Discourse Treebank.\nmining, anaphora resolution, or speech act assignment (Sidarenka et al., 2015).\n(1) a. [Such trappings suggest a glorious past] [but give no hint of a troubled present.]\nb. [He said] [the thrift will to get regulators to reverse the decision.]\nWe focus on the Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) – and resources such as the RST Discourse Treebank (RST-DT) (Carlson et al., 2001) – in which discourse structures are trees covering the documents. Most recent works on RST discourse parsing focuses on the task of tree building, relying on a gold discourse segmentation (Ji and Eisenstein, 2014; Feng and Hirst, 2014; Li et al., 2014; Joty et al., 2013). However, discourse parsers’ performance drops by 12-14% when relying on predicted segmentation (Joty et al., 2015), underscoring the importance of discourse segmentation. State-of-theart performance for discourse segmentation on the RST-DT is about 91% in F1 with predicted parses (Xuan Bach et al., 2012), but these systems rely on a gold segmentation of sentences and words, therefore probably overestimating performance in the wild. We propose to build discourse segmenters without making any data assumptions. Specifically, rather than segmenting sentences, our systems segment documents directly.\nFurthermore, only a few systems have been developed for languages other than English and domains other than the Wall Street Journal texts from the RST-DT. We are the first to perform experiments across 5 languages, and 3 non-newswire English domains. Since our goal is to provide a system usable for low-resource languages, we only use language-independent resources: here, the Universal Dependencies (UD) (Nivre et al., 2016)\nPart-of-Speech (POS) tags, for which annotations exist for about 50 languages. For the cross-lingual experiments, we also rely on cross-lingual word embeddings induced from parallel data. With a shared representation, we can transfer model parameters across languages, or learn models jointly through multi-task learning.\nContributions: We (i) propose a general statistical discourse segmenter (ii) that does not assume gold sentences and tokens, and (iii) evaluate it across 5 languages and 3 domains.\nWe make our code available at\nhttps://bitbucket.org/chloebt/discourse."
    }, {
      "heading" : "2 Related work",
      "text" : "For English RST-DT, the best discourse segmentation results were presented in Xuan Bach et al. (2012) (F1 91.0% with automatic parse, 93.7 with gold parse) – and in Joty et al. (2015) for the Instructional corpus (Subba and Di Eugenio, 2009) (F1 80.9% on 10-fold). Segmenters based on handwritten rules have been developed for Brazilian Portuguese (Pardo and Nunes, 2008) (51.3% to 56.8%, depending on the genre), Spanish (da Cunha et al., 2010, 2012) (80%) and Dutch (van der Vliet, 2010) (73% with automatic parse, 82% with gold parse).2\nMost statistical discourse segmenters are based on classifiers (Fisher and Roark, 2007; Joty et al., 2015). Subba and Di Eugenio (2007) were the first to use a neural network, and Sporleder and Lapata (2005) to model the task as a sequence prediction problem. In this work, we do sequence prediction using a neural network.\nAll these systems rely on a quite large range of lexical and syntactic features (e.g. token, POS tags, lexicalized production rules). Sporleder and Lapata (2005) present arguments for a knowledge-lean system that can be used for low-resourced languages. Their system, however, still relies on several tools and gold annotations (e.g. POS tagger, chunker, list of connectives, gold sentences). In contrast, we present what is to the best of our knowledge the first work on discourse segmentation that is directly applicable to low-resource languages, presenting results for scenarios where no labeled data is available for the target language.\n2 For German (Sidarenka et al., 2015) propose a segmenter in clauses (that may be EDU or not).\nPrevious work, relying on gold sentence boundaries, also only considers intra-sentential segment boundaries. We move to processing entire documents, motivated by the fact that sentence boundaries are not easily detected across all languages."
    }, {
      "heading" : "3 Discourse segmentation",
      "text" : "Nature of the EDUs Discourse segmentation is the first step in annotating a discourse corpus. The annotation guidelines define what is the nature of the EDUs, broadly relying on lexical and syntactic clues. If sentences and independent clauses are always minimal units, some fine distinctions make the task difficult.\nIn the English RST-DT (Carlson and Marcu, 2001), lexical information is crucial: for instance, the presence of the discourse connective “but” in example (1a)3 indicates the beginning of an EDU. In addition, clausal complements of verbs are generally not treated as EDUs. Exceptions are the complements of attribution verbs, as in (1b), and the infinitival clauses marking a PURPOSE relation as the second EDU in (2a). Note that, in this latter example, the first infinitival clause (“to cover up . . .”) is, however, not considered as an EDU. This fine distinction corresponds to one of the main difficulties of the task. Another one is linked to coordination: coordinated clauses are generally segmented as in (2b), but not coordinated verb phrases as in (2c).\n(2) a. [A grand jury has been investigating whether officials at Southern Co. ac-\ncounting conspired to cover up their accounting for spare parts] [to evade federal income taxes.]\nb. [they parcel out money] [so that their clients can find temporary living\nquarters,] [buy food] (. . .) [and replaster walls.]\nc. [Under Superfund, those] [who owned, generated or transported hazardous waste] [are liable for its cleanup, (. . .)]\nFinally, in a multi-lingual and multi-domain setting, note that all the corpora do not follow the same rules: for example, the relation ATTRIBUTION is only annotated in the English RST-DT\n3All the examples given come from (Carlson et al., 2001).\nand the corpora for Brazilian Portuguese, consequently, complements of attribution verbs are not segmented in the other corpora.\nBinary task As in previous studies, we view segmentation as a binary task at the word level: a word is either an EDU boundary (label B, beginning an EDU) or not (label I, inside an EDU). This design choice is motivated by the fact that, in RST corpora, the EDUs cover the documents entirely, and that EDUs mostly are adjacent spans of text. An exception is when embedded EDUs break up another EDU, as in Example (3). The units 1 and 3 form in fact one EDU. We follow previous work on treating this as three segments, but note that this may not be the optimal solution.\n(3) [But maintaining the key components (. . .)]1 [– a stable exchange rate and high levels of imports –]2 [will consume enormous amounts (. . .).]3\nDocument-level segmentation Contrary to previous studies, we do not assume gold sentences: Since sentence boundaries are EDU boundaries, our system jointly predicts sentence and intrasentential EDU boundaries."
    }, {
      "heading" : "4 Cross-lingual/-domain segmentation",
      "text" : "Data is scarce for discourse. In order to build statistical segmenters for new, low-resourced languages and domains, we propose to combine corpora within a multi-task learning setting (Section 5) leveraging data from well-resourced languages or domains. Models are trained on several (source) languages (resp. domains) – each viewed as an auxiliary task – for building a system for a (target) language (resp. domain).\nCross-domain For cross-domain experiments, the models are trained on all the other (source) domains and parameters are tuned on data for the target domain. This allows us to improve performance when only few data points (i.e. development set) are annotated for a specific domain (semi-supervised setting).\nCross-lingual For cross-lingual experiments, we tune our system’s parameters by training a system on the data for three languages with sufficient amounts of data (namely, German, Spanish and Brazilian Portuguese), and using English data as a development set. We then train a new model also using multi-task learning (with these tuned parameters) using only source training data, and report\nperformance on the target test set. This allows us to estimate performance when no data is available for the language of interest (unsupervised adaptation)."
    }, {
      "heading" : "5 Multi-task learning",
      "text" : "Our models perform sequence labeling based on a stacked k-layer bi-directional LSTM, a variant of LSTMs (Hochreiter and Schmidhuber, 1997) that reads the input in both regular and reversed order, allowing to take into account both left and right contexts (Graves and Schmidhuber, 2005). For our task, this enables us, for example, to distinguish between coordinated nouns and clauses. This model takes as input a sequence of words (and, here, POS tags) represented by vectors (initialized randomly or, for words, using pre-trained embedding vectors). The sequence goes through an embedding layer, and we compute the predictions of the forward and backward states for the k stacked layers. At the upper level, we compute the softmax predictions for each word based on a linear transformation. We use a logistic loss.\nWe also investigate joint training of multiple languages and domains for discourse segmentation. We thus try to leverage languages and domains regularities by sharing the architecture and parameters through multi-task training, where an auxiliary task is a source language (resp. domain) different from the target language (resp. domain) of interest. Specifically, we train models based on hard parameters sharing (Caruana, 1993; Collobert et al., 2011; Klerke et al., 2016; Plank et al., 2016):4 each task is associated with a specific output layer, whereas the inner layers – the stacked LSTMs – are shared across the tasks. At training time, we randomly sample data points from one task and do forward predictions. During backpropagation, we modify the weights of the shared layers and the task-specific outer layer. The model is optimized for one target task (corresponding to the development data used). Except for the outer layer, the target task model is thus regularized by the induction of auxiliary models."
    }, {
      "heading" : "6 Corpora",
      "text" : "Table 1 summarizes statistics about the data. For English, we use four corpora, allowing us to evaluate cross-domain performance: the RST-DT\n4We used a modified version of (Plank et al., 2016) fixing the random seed and using standard SGD.\n(En-DT) composed of Wall Street Journal articles; the SFU review corpus5 (En-SFU-DT) containing product reviews; the instructional corpus (En-Instr-DT) (Subba and Di Eugenio, 2009) built on instruction manuals; and the GUM corpus6 (En-Gum-DT) containing interviews, news, travel guides and how-tos.\nFor cross-lingual experiments, we use annotated corpora for Spanish (EsDT) (da Cunha et al., 2011),7 German (DeDT) (Stede, 2004; Stede and Neumann, 2014), Dutch (Nl-DT) (Vliet et al., 2011; Redeker et al., 2012) and, for Brazilian Portuguese, we merged four corpora (Pt-DT) (Cardoso et al., 2011; Collovini et al., 2007; Pardo and Seno, 2005; Pardo and Nunes, 2003, 2004) as done in (Maziero et al., 2015).\nThree other RST corpora exist, but we were not able to obtain cross-lingual word embeddings for Basque (Iruskieta et al., 2013) and Chinese (Wu et al., 2016), and could not obtain the data for Tamil (Subalalitha and Parthasarathi, 2012)."
    }, {
      "heading" : "7 Experiments",
      "text" : "Data We use the official test sets for the En-DT (38 documents) and the Es-DT (84). For the others, we randomly choose 38 documents as test set, and either keep the rest as development set (NlDT) or split it into a train and a development set.\nBaselines As baselines at the document level, we report the scores obtained (a) when only considering the sentence boundaries predicted using UDPipe (Straka et al., 2016) (UDP-S),8 and (b) when EDU boundaries are added after each token\n5https://www.sfu.ca/∼mtaboada 6https://corpling.uis.georgetown.edu/gum/ 7We only use the test set from the annotator A. 8http://ufal.mff.cuni.cz/udpipe\nPoS-tagged with “PUNCT” (UDP-P), marking either an inter- or an intra-sentential boundary.\nSystems As described in Section 3, our systems are either mono-lingual or mono-domain (mono), or based on a joint training across languages or domains (cross). The “mono” systems are built for the languages and domains represented by enough data (upper part of Table 1). The “cross” models are trained using multi-task learning.\nParameters The hyper-parameters are tuned on the development set: number of iterations i ∈ {10, 20, 30}, Gaussian noise σ ∈ {0.1, 0.2}, and number of dimensions d ∈ {50, 500}. We fix the number n of stacked hidden layers to 2 and the size of the hidden layers h to 100 after experimenting on the En-DT.9 Our final models use σ = 0.2 and d = 500.\nRepresentation We use tokens and POS tags as input data.10 The aim is to build a representation considering the current word and its context, i.e. its POS and the surrounding words/POS. We use the pre-trained UDPipe models to postag the documents for all languages. We experiment with randomly initialized and pre-trained cross-lingual word embeddings built on Europarl (Levy et al., 2017), keeping either the full 500 dimensions, or the first 50 ones.\nResults Our systems are evaluated using F1 over the boundaries (B labels), disregarding the first word of each document. Our scores are summarized in Table 2.\nOur supervised, monolingual systems unsurprisingly give the best performance, with F1 above\n9With n ∈ {1, 2, 3} and h ∈ {100, 200, 400}). 10A document is a sequence alternating words and POS. The tokens are labeled with a B or an I, the POS, always labeled with an I, are inserted after each token they refer to.\n80%. The results are generally linked to the size of the corpora, the larger the better. Only exception is the En-SFU-DT, which, however, include more varied annotation (the authors stated that the annotations “have not been checked for reliability”).\nThe (semi-supervised) cross-domain setting allows us to present the scores one can expect when only 25 documents are annotated for a new domain (i.e. the development set for the target domain), and to give the first results on the En-Gum-DT, but here, our model is actually outperformed by the sentence-based baseline (UDP-S).\nThe (unsupervised) cross-lingual models are generally largely better than UDPipe. These are scores that one can expect when doing crosslingual transfer to build a discourse segmenter for a new language for which no annotated data are available. The performance is still quite high, demonstrating the coherence between the annotation schemes, and the potential of cross-lingual transfer. We acknowledge that this is a small set of relatively similar Indo-European languages, however.\nNote that the sentence-based baseline has a high precision (e.g. 96.6 on Es-DT against 59.8 for the cross-lingual system), but a much lower recall, since it mainly predicts the sentence boundaries. On corpora that mostly contain sentential EDUs (e.g. Nl-DT, see Table 1), this is a good strategy. Using the punctuation (UDP-P) could be a better approximation for corpora with more varied EDUs, see the large gain for the Pt-DT and the En-Instr-DT.\nOur scores are not directly comparable with sentence-level state-of-the-art systems (see Section 2). However, for En-DT, our best system correctly identifies 950 sentence boundaries out of 991, but gets only 84.5% in F1 for intrasentential boundaries,11 thus lower than the stateof-the-art (91.0%). This is because we consider much less information, and because the system was not optimized for this task. Interestingly, our simple system beats HILDA (Hernault et al., 2010) (74.1% in F1), is as good as the other neural network based system (Subba and Di Eugenio, 2007), and is close to SPADE (Soricut and Marcu, 2003) (85.2% in F1) (Joty et al., 2015), while all of these systems use parse tree information.\nFinally, looking at the errors of our system on\n11This score ignores the sentences containing only one EDU (Sporleder and Lapata, 2005).\nthe En-DT, we found that most of them are on the tokens “to” (30 out of 94 not predicted as ’B’) and “and” (24 out of 103), as expected given the annotation guidelines (see Section 3). These words are highly ambiguous regarding discourse segmentation (e.g. in the test set, 42.3% of “and” indicates a boundary). We also found errors with coordinated verb phrases – e.g. “[when rates are rising] [and shift out at times]” – that should be split (Carlson et al., 2001), a distinction hard to make without syntactic trees. Finally, since we use predicted POS tags, our system learns from noisy data and makes errors due to postagging and tokenisation errors."
    }, {
      "heading" : "8 Conclusion",
      "text" : "We proposed new discourse segmenters with good performance for many languages and domains, at the document level, within a fully predicted setting and using only language independent tools."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We would like to thank the anonymous reviewers for their comments. This research is funded by the ERC Starting Grant LOWLANDS No. 313695."
    } ],
    "references" : [ {
      "title" : "CSTNews - a discourse-annotated corpus for single and multi",
      "author" : [ "Paula C.F. Cardoso", "Erick G. Maziero", "Mara Luca Castro Jorge", "Eloize R.M. Seno", "Ariani Di Felippo", "Lucia Helena Machado Rino", "Maria das Gracas Volpe Nunes", "Thiago A.S. Pardo" ],
      "venue" : null,
      "citeRegEx" : "Cardoso et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Cardoso et al\\.",
      "year" : 2011
    }, {
      "title" : "Discourse tagging reference manual",
      "author" : [ "Lynn Carlson", "Daniel Marcu." ],
      "venue" : "Technical report, University of Southern California Information Sciences Institute.",
      "citeRegEx" : "Carlson and Marcu.,? 2001",
      "shortCiteRegEx" : "Carlson and Marcu.",
      "year" : 2001
    }, {
      "title" : "Building a discourse-tagged corpus in the framework of Rhetorical Structure Theory",
      "author" : [ "Lynn Carlson", "Daniel Marcu", "Mary Ellen Okurowski." ],
      "venue" : "Proceedings of the Second SIGdial Workshop on Discourse and Dialogue.",
      "citeRegEx" : "Carlson et al\\.,? 2001",
      "shortCiteRegEx" : "Carlson et al\\.",
      "year" : 2001
    }, {
      "title" : "Multitask learning: a knowledgebased source of inductive bias",
      "author" : [ "Rich Caruana." ],
      "venue" : "Proceedings of ICML.",
      "citeRegEx" : "Caruana.,? 1993",
      "shortCiteRegEx" : "Caruana.",
      "year" : 1993
    }, {
      "title" : "Natural language processing (almost) from scratch",
      "author" : [ "Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa." ],
      "venue" : "The Journal of Machine Learning Research 12:2493–2537.",
      "citeRegEx" : "Collobert et al\\.,? 2011",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2011
    }, {
      "title" : "Summ-it: Um corpus anotado com informaçoes discursivas visandoa sumarizaçao automática",
      "author" : [ "Sandra Collovini", "Thiago I Carbonel", "Juliana Thiesen Fuchs", "Jorge César Coelho", "Lúcia Rino", "Renata Vieira." ],
      "venue" : "Proceedings of TIL.",
      "citeRegEx" : "Collovini et al\\.,? 2007",
      "shortCiteRegEx" : "Collovini et al\\.",
      "year" : 2007
    }, {
      "title" : "DiSeg: Un segmentador discursivo automático para el español",
      "author" : [ "Iria da Cunha", "Eric SanJuan", "Juan-Manuel TorresMoreno", "Marina Lloberas", "Irene Castellón." ],
      "venue" : "Procesamiento del lenguaje natural 45:145–152.",
      "citeRegEx" : "Cunha et al\\.,? 2010",
      "shortCiteRegEx" : "Cunha et al\\.",
      "year" : 2010
    }, {
      "title" : "DiSeg 1.0: The first system for Spanish discourse segmentation",
      "author" : [ "Iria da Cunha", "Eric SanJuan", "Juan-Manuel TorresMoreno", "Marina Lloberes", "Irene Castellón" ],
      "venue" : "Expert Syst. Appl. 39(2):1671–1678",
      "citeRegEx" : "Cunha et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Cunha et al\\.",
      "year" : 2012
    }, {
      "title" : "On the development of the RST Spanish Treebank",
      "author" : [ "Iria da Cunha", "Juan-Manuel Torres-Moreno", "Gerardo Sierra." ],
      "venue" : "Proceedings of LAW.",
      "citeRegEx" : "Cunha et al\\.,? 2011",
      "shortCiteRegEx" : "Cunha et al\\.",
      "year" : 2011
    }, {
      "title" : "A lineartime bottom-up discourse parser with constraints and post-editing",
      "author" : [ "Vanessa Wei Feng", "Graeme Hirst." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Feng and Hirst.,? 2014",
      "shortCiteRegEx" : "Feng and Hirst.",
      "year" : 2014
    }, {
      "title" : "The utility of parse-derived features for automatic discourse segmentation",
      "author" : [ "Seeger Fisher", "Brian Roark." ],
      "venue" : "Proceedings ACL.",
      "citeRegEx" : "Fisher and Roark.,? 2007",
      "shortCiteRegEx" : "Fisher and Roark.",
      "year" : 2007
    }, {
      "title" : "Framewise phoneme classification with bidirectional lstm and other neural network architectures",
      "author" : [ "Alex Graves", "Jrgen Schmidhuber." ],
      "venue" : "Neural Networks pages 5–6.",
      "citeRegEx" : "Graves and Schmidhuber.,? 2005",
      "shortCiteRegEx" : "Graves and Schmidhuber.",
      "year" : 2005
    }, {
      "title" : "HILDA: A discourse parser using support vector machine classification",
      "author" : [ "Hugo Hernault", "Helmut Prendinger", "David A. duVerle", "Mitsuru Ishizuka." ],
      "venue" : "Dialogue and Discourse 1:1–33.",
      "citeRegEx" : "Hernault et al\\.,? 2010",
      "shortCiteRegEx" : "Hernault et al\\.",
      "year" : 2010
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural Computation 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "The RST Basque Treebank: an online search interface to check rhetorical relations",
      "author" : [ "Mikel Iruskieta", "Marı́a J. Aranzabe", "Arantza Diaz de Ilarraza", "Itziar Gonzalez-Dios", "Mikel Lersundi", "Oier Lopez de la Calle" ],
      "venue" : "In Proceedings of the 4th Workshop",
      "citeRegEx" : "Iruskieta et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Iruskieta et al\\.",
      "year" : 2013
    }, {
      "title" : "Representation learning for text-level discourse parsing",
      "author" : [ "Yangfeng Ji", "Jacob Eisenstein." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Ji and Eisenstein.,? 2014",
      "shortCiteRegEx" : "Ji and Eisenstein.",
      "year" : 2014
    }, {
      "title" : "Codra: A novel discriminative framework for rhetorical analysis",
      "author" : [ "Shafiq Joty", "Giuseppe Carenini", "Raymond T. Ng." ],
      "venue" : "Computational Linguistics 41:3.",
      "citeRegEx" : "Joty et al\\.,? 2015",
      "shortCiteRegEx" : "Joty et al\\.",
      "year" : 2015
    }, {
      "title" : "Combining intra- and multi-sentential rhetorical parsing for documentlevel discourse analysis",
      "author" : [ "Shafiq R. Joty", "Giuseppe Carenini", "Raymond T. Ng", "Yashar Mehdad." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Joty et al\\.,? 2013",
      "shortCiteRegEx" : "Joty et al\\.",
      "year" : 2013
    }, {
      "title" : "Improving sentence compression by learning to predict gaze",
      "author" : [ "Sigrid Klerke", "Yoav Goldberg", "Anders Søgaard." ],
      "venue" : "Proceedings of NAACL.",
      "citeRegEx" : "Klerke et al\\.,? 2016",
      "shortCiteRegEx" : "Klerke et al\\.",
      "year" : 2016
    }, {
      "title" : "A strong baseline for learning cross-lingual word embeddings from sentence alignments",
      "author" : [ "Omer Levy", "Anders Søgaard", "Yoav Goldberg." ],
      "venue" : "Proceedings of EACL.",
      "citeRegEx" : "Levy et al\\.,? 2017",
      "shortCiteRegEx" : "Levy et al\\.",
      "year" : 2017
    }, {
      "title" : "Recursive deep models for discourse parsing",
      "author" : [ "Jiwei Li", "Rumeng Li", "Eduard H. Hovy." ],
      "venue" : "Proceedings of EMNLP.",
      "citeRegEx" : "Li et al\\.,? 2014",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2014
    }, {
      "title" : "Rhetorical Structure Theory: Toward a functional theory of text organization",
      "author" : [ "William C. Mann", "Sandra A. Thompson." ],
      "venue" : "Text 8:243–281.",
      "citeRegEx" : "Mann and Thompson.,? 1988",
      "shortCiteRegEx" : "Mann and Thompson.",
      "year" : 1988
    }, {
      "title" : "Adaptation of discourse parsing models for Portuguese language",
      "author" : [ "Erick G. Maziero", "Graeme Hirst", "Thiago A.S. Pardo." ],
      "venue" : "Proceedings of the Brazilian Conference on Intelligent Systems (BRACIS).",
      "citeRegEx" : "Maziero et al\\.,? 2015",
      "shortCiteRegEx" : "Maziero et al\\.",
      "year" : 2015
    }, {
      "title" : "Universal dependencies 1.3. LINDAT/CLARIN digital library at Institute of Formal and Applied Linguistics, Charles",
      "author" : [ "Veronika Vincze", "Jing Xian Wang", "Jonathan North Washington", "Zdeněk Žabokrtský", "Daniel Zeman", "Hanzhi Zhu" ],
      "venue" : null,
      "citeRegEx" : "Vincze et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Vincze et al\\.",
      "year" : 2016
    }, {
      "title" : "A construção de um corpus de textos cientı́ficos em Português do Brasil e sua marcação retórica",
      "author" : [ "Thiago A.S. Pardo", "Maria das Graças Volpe Nunes." ],
      "venue" : "Technical report, Universidade de São Paulo.",
      "citeRegEx" : "Pardo and Nunes.,? 2003",
      "shortCiteRegEx" : "Pardo and Nunes.",
      "year" : 2003
    }, {
      "title" : "Relações retóricas e seus marcadores superficiais: Análise de um corpus de textos cientı́ficos em Português do Brasil",
      "author" : [ "Thiago A.S. Pardo", "Maria das Graças Volpe Nunes." ],
      "venue" : "Relatório Técnico NILC .",
      "citeRegEx" : "Pardo and Nunes.,? 2004",
      "shortCiteRegEx" : "Pardo and Nunes.",
      "year" : 2004
    }, {
      "title" : "On the development and evaluation of a Brazilian Portuguese discourse parser",
      "author" : [ "Thiago A.S. Pardo", "Maria das Graças Volpe Nunes." ],
      "venue" : "Revista de Informática Teórica e Aplicada 15(2):43–64.",
      "citeRegEx" : "Pardo and Nunes.,? 2008",
      "shortCiteRegEx" : "Pardo and Nunes.",
      "year" : 2008
    }, {
      "title" : "Rhetalho: Um corpus de referłncia anotado retoricamente",
      "author" : [ "Thiago A.S. Pardo", "Eloize R.M. Seno." ],
      "venue" : "Proceedings of Encontro de Corpora.",
      "citeRegEx" : "Pardo and Seno.,? 2005",
      "shortCiteRegEx" : "Pardo and Seno.",
      "year" : 2005
    }, {
      "title" : "Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss",
      "author" : [ "Barbara Plank", "Anders Søgaard", "Yoav Goldberg." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Plank et al\\.,? 2016",
      "shortCiteRegEx" : "Plank et al\\.",
      "year" : 2016
    }, {
      "title" : "Multilayer discourse annotation of a dutch text corpus",
      "author" : [ "Gisela Redeker", "Ildik Berzlnovich", "Nynke van der Vliet", "Gosse Bouma", "Markus Egg." ],
      "venue" : "Proceedings of LREC.",
      "citeRegEx" : "Redeker et al\\.,? 2012",
      "shortCiteRegEx" : "Redeker et al\\.",
      "year" : 2012
    }, {
      "title" : "Discourse segmentation of german texts",
      "author" : [ "Uladzimir Sidarenka", "Andreas Peldszus", "Manfred Stede." ],
      "venue" : "Journal of Language Technology and Computational Linguistics 30(1):71–98.",
      "citeRegEx" : "Sidarenka et al\\.,? 2015",
      "shortCiteRegEx" : "Sidarenka et al\\.",
      "year" : 2015
    }, {
      "title" : "Sentence level discourse parsing using syntactic and lexical information",
      "author" : [ "Radu Soricut", "Daniel Marcu." ],
      "venue" : "Proceedings of NAACL.",
      "citeRegEx" : "Soricut and Marcu.,? 2003",
      "shortCiteRegEx" : "Soricut and Marcu.",
      "year" : 2003
    }, {
      "title" : "Discourse chunking and its application to sentence compression",
      "author" : [ "Caroline Sporleder", "Mirella Lapata." ],
      "venue" : "Proceedings of HLT/EMNLP.",
      "citeRegEx" : "Sporleder and Lapata.,? 2005",
      "shortCiteRegEx" : "Sporleder and Lapata.",
      "year" : 2005
    }, {
      "title" : "The potsdam commentary corpus",
      "author" : [ "Manfred Stede." ],
      "venue" : "Proceedings of the ACL Workshop on Discourse Annotation.",
      "citeRegEx" : "Stede.,? 2004",
      "shortCiteRegEx" : "Stede.",
      "year" : 2004
    }, {
      "title" : "Potsdam commentary corpus 2.0: Annotation for discourse research",
      "author" : [ "Manfred Stede", "Arne Neumann" ],
      "venue" : "In Proceedings of LREC",
      "citeRegEx" : "Stede and Neumann.,? \\Q2014\\E",
      "shortCiteRegEx" : "Stede and Neumann.",
      "year" : 2014
    }, {
      "title" : "UDPipe: Trainable Pipeline for Processing CoNLL-U Files Performing Tokenization, Morphological Analysis, POS Tagging and Parsing",
      "author" : [ "Milan Straka", "Jan Hajič", "Straková." ],
      "venue" : "Proceedings of LREC.",
      "citeRegEx" : "Straka et al\\.,? 2016",
      "shortCiteRegEx" : "Straka et al\\.",
      "year" : 2016
    }, {
      "title" : "An approach to discourse parsing using sangati and Rhetorical Structure Theory",
      "author" : [ "C N Subalalitha", "Ranjani Parthasarathi." ],
      "venue" : "Proceedings of the Workshop on Machine Translation and Parsing in Indian Languages (MTPIL-2012).",
      "citeRegEx" : "Subalalitha and Parthasarathi.,? 2012",
      "shortCiteRegEx" : "Subalalitha and Parthasarathi.",
      "year" : 2012
    }, {
      "title" : "Automatic discourse segmentation using neural networks",
      "author" : [ "Rajen Subba", "Barbara Di Eugenio." ],
      "venue" : "Workshop on the Semantics and Pragmatics of Dialogue.",
      "citeRegEx" : "Subba and Eugenio.,? 2007",
      "shortCiteRegEx" : "Subba and Eugenio.",
      "year" : 2007
    }, {
      "title" : "An effective discourse parser that uses rich linguistic information",
      "author" : [ "Rajen Subba", "Barbara Di Eugenio." ],
      "venue" : "Proceedings of ACL-HLT.",
      "citeRegEx" : "Subba and Eugenio.,? 2009",
      "shortCiteRegEx" : "Subba and Eugenio.",
      "year" : 2009
    }, {
      "title" : "Syntax-based discourse segmentation of Dutch text",
      "author" : [ "Nynke van der Vliet." ],
      "venue" : "15th Student Session, ESSLLI.",
      "citeRegEx" : "Vliet.,? 2010",
      "shortCiteRegEx" : "Vliet.",
      "year" : 2010
    }, {
      "title" : "Building a discourse-annotated Dutch text corpus",
      "author" : [ "Nynke Van Der Vliet", "Ildikó Berzlnovich", "Gosse Bouma", "Markus Egg", "Gisela Redeker." ],
      "venue" : "S. Dipper and H. Zinsmeister (Eds.), Beyond Semantics, Bochumer Linguistische Arbeitsberichte 3.",
      "citeRegEx" : "Vliet et al\\.,? 2011",
      "shortCiteRegEx" : "Vliet et al\\.",
      "year" : 2011
    }, {
      "title" : "A new ranking method for Chinese discourse tree building",
      "author" : [ "Yunfang Wu", "Fuqiang Wan", "Yifeng Xu", "Xueqiang Lü." ],
      "venue" : "Acta Scientiarum Naturalium Universitatis Pekinensis 52(1):65–74.",
      "citeRegEx" : "Wu et al\\.,? 2016",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2016
    }, {
      "title" : "A reranking model for discourse segmentation using subtree features",
      "author" : [ "Ngo Xuan Bach", "Nguyen LeMinh", "Akira Shimazu." ],
      "venue" : "Proceedings of Sigdial.",
      "citeRegEx" : "Bach et al\\.,? 2012",
      "shortCiteRegEx" : "Bach et al\\.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 30,
      "context" : "mining, anaphora resolution, or speech act assignment (Sidarenka et al., 2015).",
      "startOffset" : 54,
      "endOffset" : 78
    }, {
      "referenceID" : 21,
      "context" : "ory (RST) (Mann and Thompson, 1988) – and",
      "startOffset" : 10,
      "endOffset" : 35
    }, {
      "referenceID" : 2,
      "context" : "resources such as the RST Discourse Treebank (RST-DT) (Carlson et al., 2001) – in which discourse structures are trees covering the documents.",
      "startOffset" : 54,
      "endOffset" : 76
    }, {
      "referenceID" : 15,
      "context" : "Most recent works on RST discourse parsing focuses on the task of tree building, relying on a gold discourse segmentation (Ji and Eisenstein, 2014; Feng and Hirst, 2014; Li et al., 2014; Joty et al., 2013).",
      "startOffset" : 122,
      "endOffset" : 205
    }, {
      "referenceID" : 9,
      "context" : "Most recent works on RST discourse parsing focuses on the task of tree building, relying on a gold discourse segmentation (Ji and Eisenstein, 2014; Feng and Hirst, 2014; Li et al., 2014; Joty et al., 2013).",
      "startOffset" : 122,
      "endOffset" : 205
    }, {
      "referenceID" : 20,
      "context" : "Most recent works on RST discourse parsing focuses on the task of tree building, relying on a gold discourse segmentation (Ji and Eisenstein, 2014; Feng and Hirst, 2014; Li et al., 2014; Joty et al., 2013).",
      "startOffset" : 122,
      "endOffset" : 205
    }, {
      "referenceID" : 17,
      "context" : "Most recent works on RST discourse parsing focuses on the task of tree building, relying on a gold discourse segmentation (Ji and Eisenstein, 2014; Feng and Hirst, 2014; Li et al., 2014; Joty et al., 2013).",
      "startOffset" : 122,
      "endOffset" : 205
    }, {
      "referenceID" : 16,
      "context" : "However, discourse parsers’ performance drops by 12-14% when relying on predicted segmentation (Joty et al., 2015), underscoring the importance of discourse segmentation.",
      "startOffset" : 95,
      "endOffset" : 114
    }, {
      "referenceID" : 26,
      "context" : "Segmenters based on handwritten rules have been developed for Brazilian Portuguese (Pardo and Nunes, 2008) (51.",
      "startOffset" : 83,
      "endOffset" : 106
    }, {
      "referenceID" : 33,
      "context" : "For English RST-DT, the best discourse segmentation results were presented in Xuan Bach et al. (2012) (F1 91.",
      "startOffset" : 83,
      "endOffset" : 102
    }, {
      "referenceID" : 13,
      "context" : "7 with gold parse) – and in Joty et al. (2015) for the Instructional corpus (Subba and Di Eugenio, 2009) (F1 80.",
      "startOffset" : 28,
      "endOffset" : 47
    }, {
      "referenceID" : 10,
      "context" : "Most statistical discourse segmenters are based on classifiers (Fisher and Roark, 2007; Joty et al., 2015).",
      "startOffset" : 63,
      "endOffset" : 106
    }, {
      "referenceID" : 16,
      "context" : "Most statistical discourse segmenters are based on classifiers (Fisher and Roark, 2007; Joty et al., 2015).",
      "startOffset" : 63,
      "endOffset" : 106
    }, {
      "referenceID" : 10,
      "context" : "Most statistical discourse segmenters are based on classifiers (Fisher and Roark, 2007; Joty et al., 2015). Subba and Di Eugenio (2007) were the first to use a neural network, and",
      "startOffset" : 64,
      "endOffset" : 136
    }, {
      "referenceID" : 32,
      "context" : "Sporleder and Lapata (2005) present arguments for a knowledge-lean system that can be used for low-resourced languages.",
      "startOffset" : 0,
      "endOffset" : 28
    }, {
      "referenceID" : 30,
      "context" : "2 For German (Sidarenka et al., 2015) propose a segmenter in clauses (that may be EDU or not).",
      "startOffset" : 13,
      "endOffset" : 37
    }, {
      "referenceID" : 1,
      "context" : "In the English RST-DT (Carlson and Marcu, 2001), lexical information is crucial: for instance, the presence of the discourse connective “but” in example (1a) indicates the beginning of an EDU.",
      "startOffset" : 22,
      "endOffset" : 47
    }, {
      "referenceID" : 2,
      "context" : "All the examples given come from (Carlson et al., 2001).",
      "startOffset" : 33,
      "endOffset" : 55
    }, {
      "referenceID" : 13,
      "context" : "Our models perform sequence labeling based on a stacked k-layer bi-directional LSTM, a variant of LSTMs (Hochreiter and Schmidhuber, 1997) that reads the input in both regular and reversed order, allowing to take into account both left and right contexts (Graves and Schmidhuber, 2005).",
      "startOffset" : 104,
      "endOffset" : 138
    }, {
      "referenceID" : 11,
      "context" : "Our models perform sequence labeling based on a stacked k-layer bi-directional LSTM, a variant of LSTMs (Hochreiter and Schmidhuber, 1997) that reads the input in both regular and reversed order, allowing to take into account both left and right contexts (Graves and Schmidhuber, 2005).",
      "startOffset" : 255,
      "endOffset" : 285
    }, {
      "referenceID" : 3,
      "context" : "Specifically, we train models based on hard parameters sharing (Caruana, 1993; Collobert et al., 2011; Klerke et al., 2016; Plank et al., 2016): each task is associated with a specific output layer, whereas the inner layers – the stacked LSTMs – are shared across the tasks.",
      "startOffset" : 63,
      "endOffset" : 143
    }, {
      "referenceID" : 4,
      "context" : "Specifically, we train models based on hard parameters sharing (Caruana, 1993; Collobert et al., 2011; Klerke et al., 2016; Plank et al., 2016): each task is associated with a specific output layer, whereas the inner layers – the stacked LSTMs – are shared across the tasks.",
      "startOffset" : 63,
      "endOffset" : 143
    }, {
      "referenceID" : 18,
      "context" : "Specifically, we train models based on hard parameters sharing (Caruana, 1993; Collobert et al., 2011; Klerke et al., 2016; Plank et al., 2016): each task is associated with a specific output layer, whereas the inner layers – the stacked LSTMs – are shared across the tasks.",
      "startOffset" : 63,
      "endOffset" : 143
    }, {
      "referenceID" : 28,
      "context" : "Specifically, we train models based on hard parameters sharing (Caruana, 1993; Collobert et al., 2011; Klerke et al., 2016; Plank et al., 2016): each task is associated with a specific output layer, whereas the inner layers – the stacked LSTMs – are shared across the tasks.",
      "startOffset" : 63,
      "endOffset" : 143
    }, {
      "referenceID" : 28,
      "context" : "We used a modified version of (Plank et al., 2016) fixing the random seed and using standard SGD.",
      "startOffset" : 30,
      "endOffset" : 50
    }, {
      "referenceID" : 33,
      "context" : ", 2011), German (DeDT) (Stede, 2004; Stede and Neumann, 2014), Dutch (Nl-DT) (Vliet et al.",
      "startOffset" : 23,
      "endOffset" : 61
    }, {
      "referenceID" : 34,
      "context" : ", 2011), German (DeDT) (Stede, 2004; Stede and Neumann, 2014), Dutch (Nl-DT) (Vliet et al.",
      "startOffset" : 23,
      "endOffset" : 61
    }, {
      "referenceID" : 40,
      "context" : ", 2011), German (DeDT) (Stede, 2004; Stede and Neumann, 2014), Dutch (Nl-DT) (Vliet et al., 2011; Redeker et al., 2012) and, for Brazilian Portuguese, we merged four corpora (Pt-DT) (Cardoso et al.",
      "startOffset" : 77,
      "endOffset" : 119
    }, {
      "referenceID" : 29,
      "context" : ", 2011), German (DeDT) (Stede, 2004; Stede and Neumann, 2014), Dutch (Nl-DT) (Vliet et al., 2011; Redeker et al., 2012) and, for Brazilian Portuguese, we merged four corpora (Pt-DT) (Cardoso et al.",
      "startOffset" : 77,
      "endOffset" : 119
    }, {
      "referenceID" : 0,
      "context" : ", 2012) and, for Brazilian Portuguese, we merged four corpora (Pt-DT) (Cardoso et al., 2011; Collovini et al., 2007; Pardo and Seno, 2005; Pardo and Nunes, 2003, 2004) as done in (Maziero et al.",
      "startOffset" : 70,
      "endOffset" : 167
    }, {
      "referenceID" : 5,
      "context" : ", 2012) and, for Brazilian Portuguese, we merged four corpora (Pt-DT) (Cardoso et al., 2011; Collovini et al., 2007; Pardo and Seno, 2005; Pardo and Nunes, 2003, 2004) as done in (Maziero et al.",
      "startOffset" : 70,
      "endOffset" : 167
    }, {
      "referenceID" : 27,
      "context" : ", 2012) and, for Brazilian Portuguese, we merged four corpora (Pt-DT) (Cardoso et al., 2011; Collovini et al., 2007; Pardo and Seno, 2005; Pardo and Nunes, 2003, 2004) as done in (Maziero et al.",
      "startOffset" : 70,
      "endOffset" : 167
    }, {
      "referenceID" : 22,
      "context" : ", 2007; Pardo and Seno, 2005; Pardo and Nunes, 2003, 2004) as done in (Maziero et al., 2015).",
      "startOffset" : 70,
      "endOffset" : 92
    }, {
      "referenceID" : 14,
      "context" : "not able to obtain cross-lingual word embeddings for Basque (Iruskieta et al., 2013) and Chinese (Wu et al.",
      "startOffset" : 60,
      "endOffset" : 84
    }, {
      "referenceID" : 41,
      "context" : ", 2013) and Chinese (Wu et al., 2016), and could not obtain the data for Tamil (Subalalitha and Parthasarathi, 2012).",
      "startOffset" : 20,
      "endOffset" : 37
    }, {
      "referenceID" : 36,
      "context" : ", 2016), and could not obtain the data for Tamil (Subalalitha and Parthasarathi, 2012).",
      "startOffset" : 49,
      "endOffset" : 86
    }, {
      "referenceID" : 35,
      "context" : "Baselines As baselines at the document level, we report the scores obtained (a) when only considering the sentence boundaries predicted using UDPipe (Straka et al., 2016) (UDP-S), and (b) when EDU boundaries are added after each token",
      "startOffset" : 149,
      "endOffset" : 170
    }, {
      "referenceID" : 19,
      "context" : "We experiment with randomly initialized and pre-trained cross-lingual word embeddings built on Europarl (Levy et al., 2017), keeping either the full 500 dimensions, or the first 50 ones.",
      "startOffset" : 104,
      "endOffset" : 123
    }, {
      "referenceID" : 12,
      "context" : "Interestingly, our simple system beats HILDA (Hernault et al., 2010) (74.",
      "startOffset" : 45,
      "endOffset" : 68
    }, {
      "referenceID" : 31,
      "context" : "1% in F1), is as good as the other neural network based system (Subba and Di Eugenio, 2007), and is close to SPADE (Soricut and Marcu, 2003) (85.",
      "startOffset" : 115,
      "endOffset" : 140
    }, {
      "referenceID" : 16,
      "context" : "2% in F1) (Joty et al., 2015), while all of these systems use parse tree information.",
      "startOffset" : 10,
      "endOffset" : 29
    }, {
      "referenceID" : 32,
      "context" : "This score ignores the sentences containing only one EDU (Sporleder and Lapata, 2005).",
      "startOffset" : 57,
      "endOffset" : 85
    }, {
      "referenceID" : 2,
      "context" : "“[when rates are rising] [and shift out at times]” – that should be split (Carlson et al., 2001), a distinction hard to make without syntactic trees.",
      "startOffset" : 74,
      "endOffset" : 96
    } ],
    "year" : 2017,
    "abstractText" : "Discourse segmentation is a crucial step in building end-to-end discourse parsers. However, discourse segmenters only exist for a few languages and domains. Typically they only detect intra-sentential segment boundaries, assuming gold standard sentence and token segmentation, and relying on high-quality syntactic parses and rich heuristics that are not generally available across languages and domains. In this paper, we propose statistical discourse segmenters for five languages and three domains that do not rely on gold preannotations. We also consider the problem of learning discourse segmenters when no labeled data is available for a language. Our fully supervised system obtains 89.5% F1 for English newswire, with slight drops in performance on other domains, and we report supervised and unsupervised (cross-lingual) results for five languages in total.",
    "creator" : "LaTeX with hyperref package"
  }
}