{
  "name" : "1610.09756.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Towards Deep Learning in Hindi NER: An approach to tackle the Labelled Data Sparsity",
    "authors" : [ "Vinayak Athavale", "Ameya Prabhu", "Manish Shrivastava" ],
    "emails" : [ "vinayak.athavale@research.iiit.ac.in,", "vshreenivasbharadwaj@gmail.com", "monik.pamecha@djsce.edu.in,", "ameya.prabhu@research.iiit.ac.in", "m.shrivastava@iiit.ac.in" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Named entity recognition (NER) is a very important task in Natural Language Processing. In the NER task, the objective is to find and cluster named entities in text into any desired categories such as person names (PER), organizations (ORG), locations (LOC), time expressions, etc. NER is an important precursor to tasks like Machine Translation, Question Answering , Topic Modelling and Information Extraction among others. Various methods have been used in the past for NER including Hidden Markov models, Conditional Random fields, Feature engineering approaches using Support Vector Machines, Max Entropy classifiers for finally classifying outputs and more recently neural network based approaches.\nDevelopment of an NER system for Indian languages is a comparatively difficult task. Hindi and many other Indian languages provide some inherent difficulties in many NLP related tasks. The\nstructure of the languages contain many complexities like free-word ordering (which affect n-gram based approaches significantly), no capitalization information and its inflectional nature (affecting hand-engineered approaches significantly). Also, in Indian languages there are many word constructions that can be classified as Named Entities (Words with Sandhis) etc and these constraints vary from language to language hence carefully crafted rules need to be made for each language which is a very time consuming and expensive task.\nAnother major problem in Indian languages is the fact that we have sparse availability of annotated data for indian languages. The task is hard for rule-based NLP tools, and the sparsity of labelled data renders many of the statistical approaches like Deep Learning unusable. This complexity in the task is a significant challenge to solve- Can we develop tools which can generalize to other languages(unlike rule based approaches) but still can perform well on this task?\nOn the other hand, RNNs and its variants have consistently performed better than other approaches on English NER and many other sequence labelling tasks. We believe RNN’s would be a very effective method compared to fixedwindow approaches as the memory cell could take much larger parts of the sentence into context thus solving the problem of sentences being freely ordered to a large extent. We propose a method to be able to model the NER task using RNN based approaches using the unsupervised data available and achieve good improvements in accuracies over many other models without any hand-engineered features or any rule-based approach. We would learn word-vectors that capture a large number of precise semantic and syntactic word relationships from a large unlabelled corpus and use them to initialize RNNs thus allowing us to leverage the capabilities of RNNs on the currently availar X iv :1 61 0.\n09 75\n6v 1\n[ cs\n.C L\n] 3\n1 O\nct 2\n01 6\nable data. We believe to the best of our knowledge,that this would the first approach capable of using RNNs for NER in Hindi data. We believe learning based approaches like these could generalize to other Indian languages without having to handcraft features or develop dependence on other NLP related tools. Our model uses no language specific features or gazetteers or dictionaries. We use a small amount of supervised training data along with some unannotated corpus for training word embeddings still we achieve accuracies on par with the state of the art results on the CoNLL 2003 dataset for English and achieve 77.48% accuracy on ICON 2013 NLP tools corpus for Hindi language.\nOverall, our paper is mainly divided into the following sections:\n• In Section 1 we begin with an introduction to the task of NER and briefly describe our approach.\n• In Section 2, we mention the issues with hindi NER and provide an overview of the past approaches to NER.\n• In Section 3, we descibe our proposed RNN based approach to the task of NER and the creation of word embeddings for NER which are at the core of our model.\n• In Section 4 We explain our experimental setup, describe the dataset for both Hindi and English and give results and observations of testing on both the datasets.\n• In Section 5 We give our conclusions from the experiments and also describe methods to extend our approach to other languages."
    }, {
      "heading" : "2 Related Work",
      "text" : "NER task has been extensively studied in the literature. Previous approaches in NER can be roughly classified into Rule based approaches and learning based approaches. Rule based approaches include the system developed by Ralph Grishman in 1995 which used a large dictionary of Named Entities (R. Grishman et al., 1995). Another model was built for NER using large lists of names of people, location etc in 1996(Wakao et al., 1996). A huge disadvantage of these systems is that a huge list needed to be made and the output for any entity not seen before could not be determined.\nThey lacked in discovering new named entities, not present in the dictionary available and also cases where the word appeared in the dictionary but was not a named entity. This is an even bigger problem for indian languages which would frequently be agglutinative in nature hence creation of dictionaries would be rendered impossible. People either used feature learning based approaches using Hand-crafted features like Capitalization etc. Then, they gave these features to a Machine learning based classifier like Support Vector Machine (SVM)(Takeuchi et al., 2002), Naive Bayes (NB) or Maximum Entropy (ME) classifiers. Some then posed this problem as a sequence labelling problem terming the context is very important in determining the entities. Then, the handcrafted series were used in sequences using Machine learning methods such as Hidden Markov Models (HMM)(Bikel et al., 1997), Conditional Random Field (CRF) (Das et al., 2013) and Decision Trees (DT)(Isozaki et al., 2001).\nMany attempts have been made to combine the above two approaches to achieve better performance. An example of this is (Srihari et al., 2000) who use a combination of both handcrafted rules along with HMM and ME. More recent approaches for Indian language and Hindi NER are based on CRFs and include (Das et al., 2013) and (Sharnagat et al., 2013).\nThe recent RNN based approaches for NER include ones by (Lample et al., 2016). Also, there are many approaches which combine NER with other tasks like (Collobert et al., 2011) (POS Tagging and NER along with Chunking and SRL tasks) and (Luo et al., 2015) (combining Entity Linking and NER) which have produced state-ofthe-art results on English datasets."
    }, {
      "heading" : "3 Proposed Approach",
      "text" : "Owing to the recent success in deep learning frameworks, we sought to apply the techniques to Indian langauge data like Hindi. But, the main challenge in these approaches is to learn inspite of the sparsity of labelled data, one of the core problems of adapting deep-learning approaches to this domain.\nWe propose to leverage the vast amount of unlabelled data available in this domain. The recurrent neural networks (RNN)s trained generally have to learn the recurrent layer as well as the embedding layer for every word. The embedding layer usually\ntakes a large amount of data to create good embeddings. We formulate a two stage methodology to utilize the unlabelled data:\nIn the first stage we utilize unlabelled corpora. We learn Skip-gram (Mikolov et al., 2013) based embeddings and GloVe (Pennington et al., 2014) embeddings on those corpora. We use the Wikipedia corpus for Hindi as a source to train these models. By that, we get wordvectors which will be used in the second stage.\nIn the second stage, as illustrated in Figure 1, we use the deep-learning based models. We initialize their embedding layers with the wordvectors for every word. Then, we train the network end-to-end on the labelled data. As various approaches have proved, a good initialization is crucial to learning good models and train faster (Sutskever et al., 2013). We apply this approach to use word-vectors to counter the sparsity of labelled data. The idea behind this is that the models would require much lesser data for convergence and would give much better results than when the embeddings are randomly initialized.\nTo get both previous and subsequent context for making predictions we use Bi-Directional RNN’s (Schuster et al., 1997). We know that Vanilla RNN’s suffer from not being able to model long term dependencies (Bengio et al., 1994) Hence we use the LSTM variant of the RNN (Hochreiter et al., 1997) which helps the RNN model Long dependencies better."
    }, {
      "heading" : "3.1 Generating Word Embeddings for Hindi",
      "text" : "Word2Vec based approaches use the idea that words which occur in similar context are similar. Thus, they can be clustered together. There are two models introduced by: (Mikolov et al., 2013) CBOW and Skipgram. The latter is shown to perform better in English corpuses on a variety of tasks, hence is more generalizable. Thus, we use the skip-gram based approach.\nMost recent method for generating wordvectors was GloVe, which is similar in nature to that of Skipgram based model. It trains embeddings with local window context using co-occurrence matrices. The GloVe model is trained on the nonzero entries of a global co-occurrence matrix of all words in the corpus, which creates a frequency cooccurrence matrix which contains information on how frequently words co-occur with one another in a given corpus. GloVe is shown to be a very effective method, and is used widely thus is shown to be generalizable to multiple tasks in English.\nFor English language, we use the pretrained word embeddings using the aforementioned approaches, since they are widely used and pretty effective. The links for downloading the vectors is provided1. However, for Hindi languge- we train the two methods and generate word vectors. We start with One hot encoding for the words and random initializations for their wordvectors and then\n1Links for download note:webpage not maintained by us https://github.com/3Top/word2vec-api\ntrain them to finally arrive at the word vectors We use the Hindi dump from LTRC IIIT Hyderabad Corpus for training. The data is 385 MB in size and the encoding used is the UTF-8 format (The Unsupervised training corpus contains 27 million tokens and 500,000 distinct tokens). The training Hindi word embeddings were trained using a window of context size of 5. The trained model is then used to generate the embeddings for the words in the vocabulary. The data would be released along with the paper at our website along with the wordvectors and their training code2. For a comparative study of performance of these methods, we also compare between the Skip-gram based wordvectors and GloVe vectors as embeddings to evalauate their performance on Hindi language."
    }, {
      "heading" : "3.2 Network Architecture",
      "text" : "The architecture of the neural networks is described in Figure 2. We trained deep neural networks consisting of either one or two recurrent layers since the labelled dataset was small. In the architecture, we have an embedding layer followed by one or two recurrent layers as specified in the experiments followed by the softmax layer. We experimented with three different kinds of recurrent layers: Vannilla RNN, LSTM and Bidirectional LSTM to test which one would be the most suitable for NER task. For the embedding\n2http://example.com\nlayer, it is initialized with the concatenation of the wordvector and the one-hot vector indicating its POS Tag. The POS Tagging task is generally considered as a very useful feature for entity recognition, so it was a reliable feature. This hypothesis was validated when the inclusion of POS tags into the embedding improved the accuracies by 3-4%.\nThis setup was trained end-to-end using Adam optimizer (Kingma et al., 2015) and batch size of 128 using dropout layer with the dropout value of 0.5 after each of the recurrent layers. We have used Dropout training (Srivastava et al., 2014) to reduce overfitting in our models and help in combining the predictions of the Bi-Directional LSTM. The key idea in dropouts is to randomly drop units with their connections from the neural network during training."
    }, {
      "heading" : "4 Experiments",
      "text" : "We perform extensive experimentation to validate our methodology. We have described the datasets we use and the experimental setup in detail in this section. We then present our results and provide a set of observations made for those results."
    }, {
      "heading" : "4.1 Datasets",
      "text" : "We test the effectiveness of our approach on ICON 2013 NLP tools contest dataset for Hindi langauge, along with cross-validating our methodology on the well-established CoNLL 2003 English named entity recognition dataset ( Sang et al., 2003) ."
    }, {
      "heading" : "4.1.1 ICON 2013 NLP Tools Contest Dataset",
      "text" : "We used the ICON 2013 NLP tools contest dataset for NER recognition. The dataset contains words annotated with part-of-speech (POS) tags and corresponding named entity labels in Shakti Standard Form (SSF) format (Bharti et al., 2009) . The dataset primarily contains 11 entity types: Organization (ORG), Person (PER), Location (LOC), Entertainment, Facilities, Artifact, Living things, Locomotives, Plants, Materials and Diseases. Rest of the corpus was tagged as non-entities (O). The dataset was randomly divided into three splits: Train, Development and Test in the ratios 70%, 17% and 13%. The training set consists of 3,199 sentences comprising 56,801 tokens, development set contains 707 sentences comprising 12,882 tokens and test set contains 571 setences comprising of 10,396 tokens. We use the F1-measure to evaluate our performance against other approaches."
    }, {
      "heading" : "4.1.2 CoNLL 2003 Dataset",
      "text" : "We perform extensive experiments on the CoNLL 2003 dataset for Named Entity Recognition. The dataset is primarily a collection of Reuters newswire articles annotated for NER with four entity types: Person (PER), Location(LOC), Organization(ORG), Miscellaneous (MISC) along with non entity elements tagged as (O). The data is provided with a training set contains 15,000 sentences consisting of approximately 203,000 tokens, along with a development set containing 3466 sentences consisting of around 51,000 tokens and a test set containing 3684 sentences comprising of approximately 46,435 tokens. We use the standard evaluation scripts provided along with the dataset for assesing the performance of our methodology. The scripts use the F1-score to evaluate the performance of models."
    }, {
      "heading" : "4.2 Experimental Setup",
      "text" : "We use this architecture for the network because of the constraint on the dataset size caused by sparsity of labelled data. We used a NVIDIA 970 GTX GPU and a 4.00 GHz Intel i7-4790 processor with 64GB RAM to train our models. As the datasets in this domain expand, we would like to scale up our approach to bigger architectures. The results obtained on ICON 2013 NLP Tools dataset are summarized in Table 2. We cross-validated our approach with English language using the CoNLL 2003 dataset. The results are summarized in Table 1, We are able to achieve state-of-the-art accuracies without using additional information like Gazetters, Chunks along with not using any handcrafted features which are considered essential for NER task as chunking provides us data about the phrases and Gazetteers provide a list of words which have high likelihood of being a named entity"
    }, {
      "heading" : "4.3 Observations",
      "text" : "The neural networks which did not have wordvector based initializations could not perform well on the NER task as predicted. This can be attributed to the sparsity of the data available in the NER task. We also observe that networks consisting of one recurrent layer perform equally good or even better than networks having two recurrent layers. We believe this would be a validation to our hypothesis that increasing the number of parameters can lead to overfitting. We could see Significant\nimprovement in performance after using LSTMRNN instead of Vanilla RNN’s which can be attributed to the ability of LSTM’s to model long dependencies. Also, the bidirectional RNN achieved significant improvement of accuracy over the others suggesting that incorporating context of words around (of both ahead and back) of the word is very useful. We provide only 1 layer in our best model to be released along with the paper. 3"
    }, {
      "heading" : "5 Conclusion",
      "text" : "We show that the performance of Deep learning based approaches on the task for entity recognition can significantly outperform many other approaches involving rule based systems or handcrafted features. The bidirectional LSTM incorporates features of varied distances providing a bigger context relieving the problem of freeword ordering too. Also, given the sparsity of data, our proposed method effectively leverages LSTM based approaches by incorporating pre-\n3Code available at: www.example.com\ntrained word embeddings instead of learning it from data since it could be learnt in an unsupervied learning setting. We could extend this approach to many Indian Languages as we do not need a very large annotated corpus. When larger labelled datasets are devleoped, in the new system we would like to explore more deep neural network architectures and try learning the neural networks from scratch.\n4"
    } ],
    "references" : [ {
      "title" : "Neural Architectures for Named Entity Recognition",
      "author" : [ "Subramanian", "Kazuya Kawakami", "Chris Dyer" ],
      "venue" : "Proceedings of NAACL",
      "citeRegEx" : "Subramanian et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Subramanian et al\\.",
      "year" : 2016
    }, {
      "title" : "Natural Language Processing (almost) from Scratch",
      "author" : [ "Collobert", "Jason Weston", "Leon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Collobert et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2011
    }, {
      "title" : "Distributed Representations of Words and Phrases and their Compositionality",
      "author" : [ "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean" ],
      "venue" : "Proceedings of Neural Information Processing Systems,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "GloVe: Global Vectors for Word Representation",
      "author" : [ "Richard Socher", "Christopher D. Mannin" ],
      "venue" : "Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Pennington et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Long Short-Term Memory",
      "author" : [ "Jrgen Schmidhuber" ],
      "venue" : "Journal Neural Computation archive Volume 9 Issue",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Adam: A Method for Stochastic Optimization. International Conference for Learning Representations",
      "author" : [ "Jimmy Ba" ],
      "venue" : null,
      "citeRegEx" : "Kingma and Ba.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Bidirectional Recurrent Neural Networks",
      "author" : [ "Kuldip K. Paliwal" ],
      "venue" : "IEEE TRANSACTIONS ON SIGNAL PROCESSING,",
      "citeRegEx" : "Schuster and Paliwal.,? \\Q1997\\E",
      "shortCiteRegEx" : "Schuster and Paliwal.",
      "year" : 1997
    }, {
      "title" : "Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition. CoNLL-2003",
      "author" : [ "Fien De Meulder" ],
      "venue" : null,
      "citeRegEx" : "Sang and Meulder.,? \\Q2003\\E",
      "shortCiteRegEx" : "Sang and Meulder.",
      "year" : 2003
    }, {
      "title" : "The NYU system for MUC-6 or Wheres the Syntax",
      "author" : [ ],
      "venue" : "Sixth Message Understanding Conference",
      "citeRegEx" : "Grishman.,? \\Q1995\\E",
      "shortCiteRegEx" : "Grishman.",
      "year" : 1995
    }, {
      "title" : "Japanese named entity recognition based on a simple rule generator and decision tree learning. proceedings of the Association for Computational Linguistics, pages 306-313.",
      "author" : [ ],
      "venue" : null,
      "citeRegEx" : "Isozaki.,? \\Q2001\\E",
      "shortCiteRegEx" : "Isozaki.",
      "year" : 2001
    }, {
      "title" : "Use of Support Vector Machines in extended named entity recognition. CoNLL-2002",
      "author" : [ "Nigel Collier" ],
      "venue" : null,
      "citeRegEx" : "Takeuchi and Collier,? \\Q2002\\E",
      "shortCiteRegEx" : "Takeuchi and Collier",
      "year" : 2002
    }, {
      "title" : "Nymble: a high performance learning name-finder",
      "author" : [ "Scott Miller", "Richard Schwartz", "Ralph Weischedel" ],
      "venue" : "Fifth conference on Applied natural language processing,",
      "citeRegEx" : "Bikel et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Bikel et al\\.",
      "year" : 1997
    }, {
      "title" : "Phrase clustering for discriminative learning",
      "author" : [ "Lin et al.2009] Dekang Lin", "Xiaoyun Wu" ],
      "venue" : "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing",
      "citeRegEx" : "Lin and Wu.,? \\Q2009\\E",
      "shortCiteRegEx" : "Lin and Wu.",
      "year" : 2009
    }, {
      "title" : "Joint named entity recognition and disambiguation",
      "author" : [ "Luo et al.2015] Gang Luo", "Xiaojiang Huang", "ChinYew Lin", "Zaiqing Nie" ],
      "venue" : "Proc. EMNLP,",
      "citeRegEx" : "Luo et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Luo et al\\.",
      "year" : 2015
    }, {
      "title" : "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
      "author" : [ "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Srivastava et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2014
    }, {
      "title" : "SSF: Shakti Standard Format Guide",
      "author" : [ "Rajeev Sangal", "Dipti Misra Sharma" ],
      "venue" : null,
      "citeRegEx" : "Bharati et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bharati et al\\.",
      "year" : 2009
    }, {
      "title" : "Named Entity Recognizer for Indian Languages",
      "author" : [ "S Malarkodi C", "K Marimuthu" ],
      "venue" : null,
      "citeRegEx" : "Lalitha et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Lalitha et al\\.",
      "year" : 2013
    }, {
      "title" : "A Hybrid Approach for Named Entity and Sub-Type Tagging",
      "author" : [ "R. Srihari et al.2000] Srihari", "C. Niu", "W. Li" ],
      "venue" : "Proceedings of the sixth conference on applied natural language processing,",
      "citeRegEx" : "Srihari et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Srihari et al\\.",
      "year" : 2000
    }, {
      "title" : "On the importance of initialization and momentum in deep learning",
      "author" : [ "James Martens", "George Dahl", "Geoffrey Hinton" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Sutskever et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2013
    }, {
      "title" : "Learning Long-Term Dependencies with Gradient Descent is difficult",
      "author" : [ "Bengio et al.1994] Yoshua Bengio", "Patrice Simard", "Paolo Frasconi" ],
      "venue" : "IEEE Transactions on Neural Networks",
      "citeRegEx" : "Bengio et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 1994
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "Markov Models (HMM)(Bikel et al., 1997), Conditional Random Field (CRF) (Das et al.",
      "startOffset" : 19,
      "endOffset" : 39
    }, {
      "referenceID" : 17,
      "context" : "An example of this is (Srihari et al., 2000) who use a combination of both handcrafted rules along with HMM and ME.",
      "startOffset" : 22,
      "endOffset" : 44
    }, {
      "referenceID" : 1,
      "context" : "Also, there are many approaches which combine NER with other tasks like (Collobert et al., 2011) (POS Tagging and NER along with Chunking and SRL tasks) and (Luo et al.",
      "startOffset" : 72,
      "endOffset" : 96
    }, {
      "referenceID" : 13,
      "context" : ", 2011) (POS Tagging and NER along with Chunking and SRL tasks) and (Luo et al., 2015) (combining Entity Linking and NER) which have produced state-ofthe-art results on English datasets.",
      "startOffset" : 68,
      "endOffset" : 86
    }, {
      "referenceID" : 2,
      "context" : "We learn Skip-gram (Mikolov et al., 2013) based embeddings and GloVe (Pennington et al.",
      "startOffset" : 19,
      "endOffset" : 41
    }, {
      "referenceID" : 3,
      "context" : ", 2013) based embeddings and GloVe (Pennington et al., 2014) embeddings on those corpora.",
      "startOffset" : 35,
      "endOffset" : 60
    }, {
      "referenceID" : 18,
      "context" : "As various approaches have proved, a good initialization is crucial to learning good models and train faster (Sutskever et al., 2013).",
      "startOffset" : 109,
      "endOffset" : 133
    }, {
      "referenceID" : 19,
      "context" : "We know that Vanilla RNN’s suffer from not being able to model long term dependencies (Bengio et al., 1994) Hence we use the LSTM variant of the RNN (Hochreiter et al.",
      "startOffset" : 86,
      "endOffset" : 107
    }, {
      "referenceID" : 2,
      "context" : "There are two models introduced by: (Mikolov et al., 2013)",
      "startOffset" : 36,
      "endOffset" : 58
    }, {
      "referenceID" : 14,
      "context" : "We have used Dropout training (Srivastava et al., 2014) to reduce overfitting in our models and help in combining the predictions of the Bi-Directional LSTM.",
      "startOffset" : 30,
      "endOffset" : 55
    } ],
    "year" : 2017,
    "abstractText" : "In this paper we describe an end to end Neural Model for Named Entity Recognition (NER) which is based on BiDirectional RNN-LSTM’s. Almost all NER systems for Hindi use Language Specific features and handcrafted rules with gazetteers. Our model is language independent and uses no domain specific features or any handcrafted rules. Our models rely on semantic information in the form of word vectors which are learnt by an unsupervised learning algorithm on an unannotated corpus. Our model attained state of the art performance in both English and Hindi which is a morphologically rich language without the use of any morphological analysis or without using gazetteers of any sort.",
    "creator" : "LaTeX with hyperref package"
  }
}