{
  "name" : "1705.11168.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Are distributional representations ready for the real world? Evaluating word vectors for grounded perceptual meaning",
    "authors" : [ "Li Lucy", "Jon Gauthier" ],
    "emails" : [ "lucy3@stanford.edu", "jon@gauthiers.net" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Distributional approaches to meaning representation have enabled a substantial amount of progress in natural language processing over the past years. They center around a classic insight from at least as early as Harris (1954); Firth (1957):\nYou shall know a word by the company it keeps. (Firth, 1957, p. 11)\nPopular distributional analysis methods which exploit this intuition such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) have been critical to the success of many recent\nAll project code available at github.com/lucy3/ grounding-embeddings.\nlarge-scale natural language processing applications (e.g. Turney and Pantel, 2010; Turian et al., 2010; Collobert and Weston, 2008; Socher et al., 2013; Goldberg, 2016). These methods operationalize distributional meaning via tasks where words are optimized to predict words which cooccur with them in text corpora. These methods yield compact word representations — vectors in some high-dimensional space — which are optimized to solve these prediction tasks. These vector representations form the foundation of practically all modern deep learning models applied within natural language processing.\nDespite the success of distributional representations in standard natural language processing tasks, a small but growing consensus within the artificial intelligence community suggests that these methods cannot be sufficient to induce adequate representations of words and concepts (Kiela et al., 2016; Gauthier and Mordatch, 2016; Lazaridou et al., 2015). These sorts of claims, which often draw on experimental evidence from cognitive science (see e.g. Barsalou, 2008), are used to back up arguments for multimodal learning (at the weakest) or complete embodiment (at the strongest). Kiela et al. (2016) claim the following:\n. . . the best way for acquiring humanlevel semantics is to have machines learn through (physical) experience: if we want to teach a system the true meaning of “bumping into a wall,” we simply have to bump it into walls repeatedly.\nDiscussions like the one above have an intuitive pull: certainly “bump” is best understood through a sense of touch, just as “loud” is best understood through a sense of sound. It seems inefficient — or perhaps just wrong — to learn these sorts of concepts from distributional evidence.\nar X\niv :1\n70 5.\n11 16\n8v 1\n[ cs\n.C L\n] 3\n1 M\nay 2\n01 7\nDespite the intuitive pull, there is not much evidence from a computational perspective that grounded or multimodal learning actually earns us anything in terms of general meaning representation. Will our robots and chat-bots be worse off for not having physically bumped into walls before they hold discussions on wall-collisions? Will our representation of the concept loud somehow be faulty unless we explicitly associate it with certain decibel levels experienced in the real world? Before we proceed to embed our learning agents in multimodal games and robot-shells, it is important that we have some concrete idea of how grounding actually affects meaning.\nThis paper presents a thorough analysis of the contents of distributional word representations with respect to this question. Our results suggest that several common distributional word representations may indeed be deficient in the sort of grounded meaning necessary for languageenabled agents deployed in the real world."
    }, {
      "heading" : "2 Related work",
      "text" : "This paper uses semantic norm datasets to evaluate the content of distributional word representations. Semantic norm datasets consist of concepts and norms concerning their perceptual and conceptual features, as provided by human participants. They are a popular resource within psychology and cognitive science as models of human concept representation, and have been used to explain psycholinguistic phenomena from semantic priming and interference (Vigliocco et al., 2004) to the structure of early word learning in child language acquisition (Hills et al., 2009). Andrews et al. (2009) show how “experiential” semantic norm information can be used to model human judgments of concept similarity. They show that this semantic norm data provides information distinct from the information found in basic word representations. Our work extends the findings of Andrews et al. to a larger semantic norm dataset and evaluates particular implications within natural language processing.\nA small NLP literature has compared distributional representations with semantic norm datasets and other external resources. Rubinstein et al. (2015) confirm that word representations are especially effective at predicting taxonomic features versus attributive features. Collell and Moens (2016) find that word representations fail to pre-\ndict many visual features of concepts, and show how representations from computer vision models can help improve these predictions. Several studies have used distributional representations to reconstruct aspects of these semantic norm datasets (Herbelot and Vecchi, 2015; Fagarasan et al., 2015; Erk, 2016).\nThe majority of the NLP work in this space has focused on the downstream task of augmenting word representations with novel grounded information, often evaluating on standard semantic similarity datasets (Agirre et al., 2009; Bruni et al., 2012; Faruqui et al., 2015; Bulat et al., 2016). Young et al. (2014) develop an alternative operationalization of denotational meaning using image captioning datasets, and demonstrate gains over distributional representations on textual similarity and entailment datasets.\nThis applied work has demonstrated that something worthwhile is indeed gained by augmenting distributional representations with some orthogonal grounded or multimodal information. We believe it is critical to analyze the original successes and failures of distributional representations in order to motivate this move to grounded meaning representation."
    }, {
      "heading" : "3 Meaning representations",
      "text" : ""
    }, {
      "heading" : "3.1 Distributional meaning",
      "text" : "This paper examines representations produced by two popular unsupervised distributional methods. Table 1 shows the statistics of the corpora used to generate these vectors.\nGloVe: GloVe (Pennington et al., 2014) estimates word representations wi by using them to reconstruct a word-word co-occurrence matrix X collected from a large text corpus:\nL = V∑ i,j=1 f(Xij) ( wTi wj + bi + bj − logXij )2 (1)\nhere f(Xij) is a weighting function on word pairs and bi, bj are learned per-word bias terms.\nWe use two pre-trained GloVe vector datasets: one trained on a concatenation of Wikipedia 2014 and Gigaword 5 (GloVe-WG), and another trained on a Common Crawl dump (GloVe-CC).1\nword2vec: word2vec (Mikolov et al., 2013) estimates word representations by optimizing a skip-gram objective to predict all words wj within a context window c of a word wi given their word representations:\nJ = 1\nT T∑ i=1 ∑ i−c≤j≤i+c log p(wj | wi) (2)\nwhere T is the total number of words in a corpus. We use a publicly available word2vec dataset trained on the Google News corpus.2"
    }, {
      "heading" : "3.2 Semantic norms",
      "text" : "Semantic feature norm datasets consist of reports from human participants about the semantic features of various natural kinds. A proportion of the features contained in these datasets are properties of concepts which may be obvious to humans but are perhaps difficult to find written in text corpora. For this reason, we selected two semantic norm datasets to serve as gold-standard comparisons of concept meaning. Table 2 displays basic statistics about the semantic norm datasets we use in this paper.\nMcRae Our initial experiments use the semantic norm dataset from McRae et al. (2005), which consists of 541 concrete noun concepts with associated feature norms, collected from 725 participants. For a given concept, the McRae dataset includes all feature norms which were reported independently by at least five participants (2,526 in total). After removing concepts indicated to have ambiguous meanings to mitigate\n1nlp.stanford.edu/projects/glove 2code.google.com/archive/p/word2vec\npolysemy effects (such as tank (army) and tank (container)) and one concept without a GloVe representation (dunebuggy), we had a resulting set of 515 concepts for analysis. The dataset groups features into several perceptual and non-perceptual categories: taxonomic, encyclopedic, function, visual-motion, visual-form and - surface, visual-colour, sound, tactile, and taste (McRae et al., 2005). We use the McRae dataset and feature categories to perform basic pilot analyses and form hypotheses about the nature of the distributional representations tested.\nCSLB We reproduce and extend our results on a second semantic norm dataset collected by the Cambridge Centre for Speech, Language and the Brain (CSLB; Devereux et al., 2014). CSLB contains 638 concepts provided by 123 participants. Their data collection closely followed McRae et al. (2005), though features were included if at least 2 participants named that feature. We removed concepts with two-word names, ambiguous meanings, or missing vector representations to yield a vocabulary of 597 concepts from this dataset. CSLB also includes a feature categorization schema, though the categories are broader than those in McRae: visual perceptual, other perceptual, functional, taxonomic, and encyclopedic.\nThe mapping between the two categorization schemes is far from perfect. While some perceptual features in McRae are categorized as perceptual features in CSLB, other features (e.g. those related to swimming, flying, eating) are reclassified as “functional” in CSLB. The two datasets disagree on abstract conceptual properties as well. For example, CSLB classifies is for - football as a functional property, while McRae classifies the comparable feature associated - with football games as encyclopedic.\nThe encyclopedic category is somewhat difficult to distinguish in both datasets. It is composed mainly of abstract factual features, but also contains attributive features such as is cold - blooded and does use electricity as well as is scary and is cool.\nMeanwhile, the functional category mixes features for behaviors associated with the concept (does dive) as well as functions that people perform on or with the concept (is hit). This classification system may need some readjustments to provide a clear understanding of what is\nperceptual and what is conceptual, and it may be that some features, such as has a steering - wheel, are both.\nGiven the significant noise of this classification scheme, we focus our investigation on a single contrast between features in clearly perceptual categories (visual, tactile, sound, etc.) and nonperceptual categories (functional and taxonomic). Because the encyclopedic category contains an ambiguous mix of both sorts, we exclude it from our formal predictions later in the paper."
    }, {
      "heading" : "4 The feature view",
      "text" : "We first investigate how well distributional word representations directly encode information about semantic norms.3 For each feature in a semantic norm dataset, we construct a binary classification problem which predicts the presence or absence of the feature for each concept. Concretely, for each feature fi we have a label vector yi ∈ {0, 1}nc , where nc is the total number of concepts in the dataset, and yij is 1 when concept j has feature fi and 0 otherwise. We build label vectors only for features with five or more associated concepts. After filtering, we have nf = 267 label vectors in the McRae dataset and nf = 775 in CSLB.\nFor each feature, we construct a binary logistic regression model pi which predicts the presence or absence of the feature for a concept given its word representation xj :\npi(yij | xj) = σ(wTi xj) (3)\nThis base model is extremely prone to overfitting, as most features have only several associated concepts — that is, each classifier has only a few positive examples — and the input word representations are of a high dimensionality. In order to prevent overfitting, we add an independent L2 regularization term to each regression model. For each feature fi, we use leave-one-out crossvalidation to select the regularization parameter λi which maximizes the following modified logistic\n3The remainder of this paper describes a general analysis performed on both the McRae and CSLB datasets. We used McRae as a pilot dataset to form hypotheses, and checked these hypotheses on the CSLB dataset as a test set. All of the graphs and numbers reported in this paper correspond to results on CSLB.\nobjective:\nLi(λi) = 1 |fi| ∑ xj∈fi log pi,λi−j (yij = 1 | xj) + 1\nnc − |fi| ∑ xk 6∈fi log pi,λi−j (yik = 0 | xk)  (4)\nHere pi,λi−j (·) represents a regression model (Equation (3)) trained without example (xj , yij) in the training set and with regularization parameter λi. The first term of the summand calculates the log-probability of the left-out concept having the desired feature, and the second term calculates the average log-probability that any other concept (outside of the feature group fi) does not have the feature. The regularization terms λi are selected independently for each feature to maximize the objective Li.\nAfter fitting the regularized logistic regression models, we calculate a set of “feature fit” metrics. For each feature fi, we evaluate the binary F1 score of its classifier’s predictions pi(yi). Figure 1 shows each feature as a point in a swarm-plot (grouped by feature category).\nPilot tests with the McRae dataset suggested that the categories associated with strictly perceptual features were not well encoded in the distributional representations relative to strictly nonperceptual categories (taxonomic and functional features).\nWe use the CSLB dataset as a test set for this prediction. We perform a bootstrap confidence interval test on the difference between the median feature fit scores for CSLB features in nonperceptual and perceptual categories. The 95% confidence intervals on this bootstrap are positive for two of the three representations tested (GloVeCC and word2vec).4 Figure 1 shows the feature fit scores on CSLB evaluated with GloVe-CC, and the word2vec evaluation effectively shows the same result: taxonomic and functional features score higher on average than strictly perceptual features. This comparison failed on GloVe-WG, however, where features classed as “functional” scored far lower on average than those in perceptual categories. Across all three sets of distributional representations, the median score of ency-\n4GloVe-CC: (7.67%, 24.0%); word2vec: (7.13%, 20.6%); GloVe-WG: (-1.25%, 15.7%).\nclopedic features was well below all other feature categories.\nIt is obvious from Figure 1 that each category contains a wide range of feature fit values. As discussed earlier in Section 3.2, this categorization of features is far from perfect. Many of the lower-scoring features classed as “encyclopedic” are simple attributive features not deserving of the category label, such as is fresh and is filling. Many of the higher-scoring encyclopedic features seem genuinely encyclopedic, such as is found on farms; other highscoring features are arguably “functional,” such as does grow on trees. Many of the higher scoring visual perceptual features state structural part-whole relations, such as has legs and has an engine.\nTable 3 provides more examples of low- and high-scoring features in each category. Despite the rather noisy classification scheme used in this dataset, we still managed to find a regular trend in two of three evaluations, matching our expectations from prior pilot experiments. We believe that a revised classification scheme could help to\ndemonstrate a clear difference between perceptual and non-perceptual features in all three datasets."
    }, {
      "heading" : "4.1 Matching word representation sources",
      "text" : "For each feature, we compare its feature fit score evaluated with GloVe-CC word vectors and its score evaluated with word2vec vectors in Figure 2.\n40 50 60 70 80 90 100\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 m(GloVeCC,CSLB)\n0.2\n0.0\n0.2\n0.4\n0.6\nm (G\nlo V\ne C\nC , W\nor dN\net )\n(a) Concepts plotted according to the correlation between their GloVe-CC embeddings and two other sources (CSLB, horizontal axis; WordNet, vertical axis). The points are colored according to their feature fit.\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 m(GloVeCC,CSLB)\n40\n50\n60\n70\n80\n90\nM ed\nia n \nfe at\nur e \nfit  s\nco re\n(b) A direct comparison of m(GloVe-CC,CSLB) (horizontal axis) and the median feature fit scores associated with concept (vertical axis). See main text for statistical tests.\nFigure 3: Concept view results.\nThe trend in the figure suggests that both representations have similar feature fit deficiencies and strengths, though the trend becomes weaker near the (100%, 100%) corner — the two representations correlate well at low feature fit scores, and seem to fan out at higher scores. A large group of points also sit in the figure at y = 100 and x = 100; these features are perfectly captured by one representation and not by the other.\nThis correlation is somewhat surprising, given that the word2vec and GloVe vectors are the products of different algorithms executed on very different corpora. There are two likely explanations behind this correlation:\n1. Some features in the CSLB semantic norm data are unusually difficult, or are perhaps missing associated concepts. GloVe and word2vec correlate in performance because they don’t match these noisy or incomplete features.\n2. There are systematic deficiencies in the word vectors due to their shared reliance on the distributional method.\nIt is difficult to differentiate these two explanations on these small semantic norm datasets, but we hope to distinguish these in the future by testing new predictions for concepts not covered in these datasets. We will return to this idea in the conclusion of the paper."
    }, {
      "heading" : "5 The concept view",
      "text" : "The previous section demonstrated that several classes of perceptual features are not well encoded on average by distributional word representations, and that these deficiencies systematically match across representations. How does this deficiency in feature representation carry over into computations on the word representations themselves?\nWe evaluate the matching between distributional representations and representations from other sources by comparing their predictions of word-word similarity. For distributional word representations, we compute word-word similarity by cosine distance:\nsim(i, j) = cos(xi, xj) (5)\nWe derive compact concept representations from the semantic norm datasets with LSA (Landauer et al., 1998). We compute a truncated SVD on the feature matrix Y ∈ {0, 1}nc×nf , which is the concatenation of the binary feature label vectors introduced in Section 4. We define conceptconcept similarity by the cosine distance between their corresponding LSA vectors.\nAs a secondary data source, we also compute word-word similarity judgments from the WordNet taxonomy (Miller, 1995). We use the Resnik metric (Resnik et al., 1999) to compute the similarity between concept names ci, cj :\nsimresnik(ci, cj) = max c∈S(ci,cj) − log p(c) (6)\nwhere S(ci, cj) selects the common ancestors of the concepts in the WordNet taxonomy, and p(c) is the unigram probability of a concept as computed on an external corpus. This selects the ancestor of the two concepts in the taxonomy which has maximal information content (surprisal). We use WordNet as additional verification that the trends observed between semantic norms and distributional representations are non-coincidental.\nWe use these similarity metrics to compute pairwise distance measures for concepts present in the semantic norm datasets. For each metric, we produce a symmetric pairwise distance matrix D ∈ Rnc×nc , where an element Dij indicates the distance between concepts i and j according to the metric.\nWe next compute how well each concept’s pairwise similarity is correlated between the various metrics. For a given concept, we compute the Pearson correlation between the concept’s GloVe/word2vec pairwise distance vector and the LSA and WordNet pairwise distance vectors.5 The correlation values of interest are m(GloVe/word2vec,CSLB) and m(GloVe/word2vec,WordNet) — that is, the correlations between the pairwise distance vectors for GloVe/word2vec and CSLB and between the pairwise distance vectors for GloVe/word2vec and WordNet.\nFigure 3a plots both of these correlation values evaluated with GloVe-CC for all concepts. The two m measures are evidently positively correlated, though with some noise (r = 0.6160). This is to be expected, as the CSLB dataset and WordNet overlap only partially in the semantic features they encode.\nEach concept in Figure 3 is colored according to the median feature fit score of its associated features. In Figure 3b, we show this feature fit metric on the vertical axis. There is a positive relationship here between feature fit scores and the correlation metric m(GloVe-CC,CSLB) (r = 0.3323). Because the correlation between m(·,CSLB) and feature fit metrics is weaker than expected, we run post-hoc multiple regression significance tests for each distributional representation. An F-test shows that the regression feature m(·,CSLB) significantly improves predictions of feature fit val-\n5The Pearson correlation between two vectors is equivalent to the cosine distance between their mean-centered forms.\nues relative to a baseline model for all three representations6,7.\nThere is substantial variance in the predictions of the distributional representations due to factors outside of the scope of the semantic norm data. The mismatch in predictions between distributional representations is nevertheless a statistically significant predictor of feature fit metrics. This suggests that the feature-level deficiencies discovered in the previous section have concrete implications in terms of word-word similarity measures."
    }, {
      "heading" : "5.1 Domain-level analysis",
      "text" : "We next investigate whether some domains of concepts are particularly affected by the deficiencies discussed in the previous sections. We perform agglomerative clustering on concepts from the CSLB dataset using a custom distance metric:\nd(i, j) = ||LSAi−LSAj ||2+α(FFi−FFj)2 (7)\nwhere LSAi is the LSA vector representation computed from the semantic norm data for concept i as introduced earlier in this section, and FFi is the median feature-fit score for a concept i. We select the weight αmanually to produce the most semantically coherent clusters.\n6The baseline regression model predicts a concept’s feature fit from these baseline features: log(word frequency in Brown corpus), log(# associated features), log(total # feature reports for the concept), # WordNet senses.\n7GloVe-CC: F ∗ = 41.297, p < 10−9; GloVe-WG: F ∗ = 68.783, p < 10−15, word2vec: F ∗ = 41.27, p < 10−9\nFigure 4 shows the distribution of feature fit scores for each of the resulting 40 domains. We find that settings of αwhich yield semantically coherent clusters also yield groups of concepts with very low variance in feature fit scores. In Table 4 we list select domains and their median feature fit scores. This clustering suggests that deficiencies at the feature level affect entire coherent semantic domains of concepts."
    }, {
      "heading" : "6 Conclusion",
      "text" : "This paper has analyzed how well various standard distributional representations encode aspects of grounded meaning. We chose to use semantic norm datasets as a gold standard of grounded meaning, and tested how word representations predicted features within these datasets. We grouped these features into high-level categories and found that, despite large within-category variance, several standard distributional representations underperformed on average in predicting perceptual features. The difference in prediction performance proved statistically significant on two of the three representations we evaluated. These deficiencies in feature encoding matched between GloVe and word2vec representations trained on different corpora, suggesting that certain classes of features may be poorly represented by distributional methods in general.\nWe also examined the consequences of these deficiencies in feature encoding for the word representations themselves. We compared the wordword similarity predictions made with distributional representations with those made with the semantic norm dataset and with WordNet, and found that words having features badly encoded within the distributional representations were also\nlikely to make different similarity predictions than the predictions from these two corpora. A final domain-level concept analysis suggested that some semantic domains are particularly impacted by these issues in feature encoding.\nThe semantic norm datasets used in this paper are subject to saliency biases: they only contain the concept-feature mappings which experimental subjects think to mention when queried. These saliency effects add noise to our results, as mentioned in Section 4.1, and may have caused us to generally underestimate the performance of distributional models within all feature categories. In future work, we plan to repeat the sorts of tests conducted in this paper while avoiding possible saliency confounds. We also plan to develop a causal explanation for the deficiencies in the word embeddings found in this paper, showing how cooccurrence information (or lack thereof) present in the training corpus can bias performance on these tasks. Both of these studies will verify that the results we have found are due entirely to deficiencies in distributional methods rather than in the datasets used here.\nWe think these deficiencies should be worrying: if neural models of language are to have any knowledge about concepts, it ought to be in their word embeddings. Our findings show that these embeddings are lacking in basic features of perceptual meaning. These results suggest that distributional meaning (as operationalized by modern distributional models) may miss out on fundamental elements of semantics. We hope they will help motivate further work in developing multimodal representations which can prepare us to deploy more fluent language agents in the real world."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We thank Christopher D. Manning, Peng Qi, Pakapol Supaniratisai, Keenon Werling, and members of the Stanford, University of Washington, and Berkeley NLP communities for useful discussions, and the anonymous reviewers for their insightful comments."
    } ],
    "references" : [ {
      "title" : "A Study on Similarity and Relatedness Using Distributional and WordNet-based Approaches",
      "author" : [ "Eneko Agirre", "Enrique Alfonseca", "Keith B. Hall", "Jana Kravalova", "Marius Pasca", "Aitor Soroa." ],
      "venue" : "HLTNAACL.",
      "citeRegEx" : "Agirre et al\\.,? 2009",
      "shortCiteRegEx" : "Agirre et al\\.",
      "year" : 2009
    }, {
      "title" : "Integrating experiential and distributional data to learn semantic representations",
      "author" : [ "Mark Andrews", "Gabriella Vigliocco", "David Vinson." ],
      "venue" : "Psychological review 116(3):463.",
      "citeRegEx" : "Andrews et al\\.,? 2009",
      "shortCiteRegEx" : "Andrews et al\\.",
      "year" : 2009
    }, {
      "title" : "Grounded cognition",
      "author" : [ "Lawrence W Barsalou." ],
      "venue" : "Annu. Rev. Psychol. 59:617–645.",
      "citeRegEx" : "Barsalou.,? 2008",
      "shortCiteRegEx" : "Barsalou.",
      "year" : 2008
    }, {
      "title" : "Distributional Semantics in Technicolor",
      "author" : [ "Elia Bruni", "Gemma Boleda", "Marco Baroni", "Nam-Khanh Tran." ],
      "venue" : "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume",
      "citeRegEx" : "Bruni et al\\.,? 2012",
      "shortCiteRegEx" : "Bruni et al\\.",
      "year" : 2012
    }, {
      "title" : "Vision and feature norms: Improving automatic feature norm learning through cross-modal maps",
      "author" : [ "Luana Bulat", "Douwe Kiela", "Stephen Clark." ],
      "venue" : "HLT-NAACL.",
      "citeRegEx" : "Bulat et al\\.,? 2016",
      "shortCiteRegEx" : "Bulat et al\\.",
      "year" : 2016
    }, {
      "title" : "Is an image worth more than a thousand words? on the fine-grain semantic differences between visual and linguistic representations",
      "author" : [ "Guillem Collell", "Marie-Francine Moens." ],
      "venue" : "Proceedings of COLING 2016, the 26th Interna-",
      "citeRegEx" : "Collell and Moens.,? 2016",
      "shortCiteRegEx" : "Collell and Moens.",
      "year" : 2016
    }, {
      "title" : "A unified architecture for natural language processing: Deep neural networks with multitask learning",
      "author" : [ "Ronan Collobert", "Jason Weston." ],
      "venue" : "Proceedings of the 25th international conference on Machine learning. ACM, pages 160–167.",
      "citeRegEx" : "Collobert and Weston.,? 2008",
      "shortCiteRegEx" : "Collobert and Weston.",
      "year" : 2008
    }, {
      "title" : "The centre for speech, language and the brain (cslb) concept property norms",
      "author" : [ "Barry J Devereux", "Lorraine K Tyler", "Jeroen Geertzen", "Billi Randall." ],
      "venue" : "Behavior research methods 46(4):1119–1127.",
      "citeRegEx" : "Devereux et al\\.,? 2014",
      "shortCiteRegEx" : "Devereux et al\\.",
      "year" : 2014
    }, {
      "title" : "What do you know about an alligator when you know the company it keeps? Semantics and Pragmatics 9(17):1–63",
      "author" : [ "Katrin Erk." ],
      "venue" : "https://doi.org/10.3765/sp.9.17.",
      "citeRegEx" : "Erk.,? 2016",
      "shortCiteRegEx" : "Erk.",
      "year" : 2016
    }, {
      "title" : "From distributional semantics to feature norms: grounding semantic models in human perceptual data",
      "author" : [ "Luana Fagarasan", "Eva Maria Vecchi", "Stephen Clark." ],
      "venue" : "IWCS.",
      "citeRegEx" : "Fagarasan et al\\.,? 2015",
      "shortCiteRegEx" : "Fagarasan et al\\.",
      "year" : 2015
    }, {
      "title" : "Retrofitting Word Vectors to Semantic Lexicons",
      "author" : [ "Manaal Faruqui", "Jesse Dodge", "Sujay Kumar Jauhar", "Chris Dyer", "Eduard Hovy" ],
      "venue" : null,
      "citeRegEx" : "Faruqui et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Faruqui et al\\.",
      "year" : 2015
    }, {
      "title" : "A synopsis of linguistic theory, 1930-1955",
      "author" : [ "John Rupert Firth." ],
      "venue" : "Studies in linguistic analysis .",
      "citeRegEx" : "Firth.,? 1957",
      "shortCiteRegEx" : "Firth.",
      "year" : 1957
    }, {
      "title" : "A paradigm for situated and goal-driven language learning",
      "author" : [ "Jon Gauthier", "Igor Mordatch." ],
      "venue" : "arXiv preprint arXiv:1610.03585 .",
      "citeRegEx" : "Gauthier and Mordatch.,? 2016",
      "shortCiteRegEx" : "Gauthier and Mordatch.",
      "year" : 2016
    }, {
      "title" : "A primer on neural network models for natural language processing",
      "author" : [ "Yoav Goldberg." ],
      "venue" : "Journal of Artificial Intelligence Research 57:345–420.",
      "citeRegEx" : "Goldberg.,? 2016",
      "shortCiteRegEx" : "Goldberg.",
      "year" : 2016
    }, {
      "title" : "Distributional structure",
      "author" : [ "Zellig S Harris." ],
      "venue" : "Word 10(2-3):146–162.",
      "citeRegEx" : "Harris.,? 1954",
      "shortCiteRegEx" : "Harris.",
      "year" : 1954
    }, {
      "title" : "Building a shared world: mapping distributional to modeltheoretic semantic spaces",
      "author" : [ "Aurélie Herbelot", "Eva Maria Vecchi." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Herbelot and Vecchi.,? 2015",
      "shortCiteRegEx" : "Herbelot and Vecchi.",
      "year" : 2015
    }, {
      "title" : "Categorical structure among shared features in networks of early-learned nouns",
      "author" : [ "Thomas T Hills", "Mounir Maouene", "Josita Maouene", "Adam Sheya", "Linda Smith." ],
      "venue" : "Cognition 112(3):381–396.",
      "citeRegEx" : "Hills et al\\.,? 2009",
      "shortCiteRegEx" : "Hills et al\\.",
      "year" : 2009
    }, {
      "title" : "Virtual embodiment: A scalable long-term strategy for artificial intelligence research",
      "author" : [ "Douwe Kiela", "Luana Bulat", "Anita L Vero", "Stephen Clark." ],
      "venue" : "arXiv preprint arXiv:1610.07432 .",
      "citeRegEx" : "Kiela et al\\.,? 2016",
      "shortCiteRegEx" : "Kiela et al\\.",
      "year" : 2016
    }, {
      "title" : "An introduction to latent semantic analysis",
      "author" : [ "Thomas K Landauer", "Peter W Foltz", "Darrell Laham." ],
      "venue" : "Discourse processes 25(2-3):259–284.",
      "citeRegEx" : "Landauer et al\\.,? 1998",
      "shortCiteRegEx" : "Landauer et al\\.",
      "year" : 1998
    }, {
      "title" : "Combining language and vision with a multimodal skip-gram model",
      "author" : [ "Angeliki Lazaridou", "Nghia The Pham", "Marco Baroni." ],
      "venue" : "HLT-NAACL.",
      "citeRegEx" : "Lazaridou et al\\.,? 2015",
      "shortCiteRegEx" : "Lazaridou et al\\.",
      "year" : 2015
    }, {
      "title" : "Semantic feature production norms for a large set of living and nonliving things",
      "author" : [ "Ken McRae", "George S Cree", "Mark S Seidenberg", "Chris McNorgan." ],
      "venue" : "Behavior research methods 37(4):547–559.",
      "citeRegEx" : "McRae et al\\.,? 2005",
      "shortCiteRegEx" : "McRae et al\\.",
      "year" : 2005
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean." ],
      "venue" : "Advances in neural information processing systems. pages 3111–3119.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Wordnet: a lexical database for english",
      "author" : [ "George A Miller." ],
      "venue" : "Communications of the ACM 38(11):39–",
      "citeRegEx" : "Miller.,? 1995",
      "shortCiteRegEx" : "Miller.",
      "year" : 1995
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D. Manning." ],
      "venue" : "Empirical Methods in Natural Language Processing (EMNLP). pages 1532– 1543. http://www.aclweb.org/anthology/D14-1162.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Semantic similarity in a taxonomy: An information-based measure and its application to problems of ambiguity in natural language",
      "author" : [ "Philip Resnik" ],
      "venue" : "J. Artif. Intell. Res.(JAIR) 11:95–130.",
      "citeRegEx" : "Resnik,? 1999",
      "shortCiteRegEx" : "Resnik",
      "year" : 1999
    }, {
      "title" : "How Well Do Distributional Models Capture Different Types of Semantic Knowledge",
      "author" : [ "Dana Rubinstein", "Effi Levi", "Roy Schwartz", "Ari Rappoport" ],
      "venue" : null,
      "citeRegEx" : "Rubinstein et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Rubinstein et al\\.",
      "year" : 2015
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts" ],
      "venue" : "In Proceedings of the conference on",
      "citeRegEx" : "Socher et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Word representations: a simple and general method for semi-supervised learning",
      "author" : [ "Joseph Turian", "Lev Ratinov", "Yoshua Bengio." ],
      "venue" : "Proceedings of the 48th annual meeting of the association for computational linguistics. Association for Computational",
      "citeRegEx" : "Turian et al\\.,? 2010",
      "shortCiteRegEx" : "Turian et al\\.",
      "year" : 2010
    }, {
      "title" : "From frequency to meaning: Vector space models of semantics",
      "author" : [ "Peter D Turney", "Patrick Pantel." ],
      "venue" : "Journal of artificial intelligence research 37:141–188.",
      "citeRegEx" : "Turney and Pantel.,? 2010",
      "shortCiteRegEx" : "Turney and Pantel.",
      "year" : 2010
    }, {
      "title" : "Representing the meanings of object and action words: The featural and unitary semantic space hypothesis",
      "author" : [ "Gabriella Vigliocco", "David P Vinson", "William Lewis", "Merrill F Garrett." ],
      "venue" : "Cognitive psychology 48(4):422–488.",
      "citeRegEx" : "Vigliocco et al\\.,? 2004",
      "shortCiteRegEx" : "Vigliocco et al\\.",
      "year" : 2004
    }, {
      "title" : "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
      "author" : [ "Peter Young", "Alice Lai", "Micah Hodosh", "Julia Hockenmaier." ],
      "venue" : "Transactions of the Association for Computational Linguis-",
      "citeRegEx" : "Young et al\\.,? 2014",
      "shortCiteRegEx" : "Young et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 13,
      "context" : "They center around a classic insight from at least as early as Harris (1954); Firth (1957):",
      "startOffset" : 63,
      "endOffset" : 77
    }, {
      "referenceID" : 11,
      "context" : "They center around a classic insight from at least as early as Harris (1954); Firth (1957):",
      "startOffset" : 78,
      "endOffset" : 91
    }, {
      "referenceID" : 21,
      "context" : "Popular distributional analysis methods which exploit this intuition such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al.",
      "startOffset" : 86,
      "endOffset" : 108
    }, {
      "referenceID" : 23,
      "context" : ", 2013) and GloVe (Pennington et al., 2014) have been critical to the success of many recent",
      "startOffset" : 18,
      "endOffset" : 43
    }, {
      "referenceID" : 27,
      "context" : "large-scale natural language processing applications (e.g. Turney and Pantel, 2010; Turian et al., 2010; Collobert and Weston, 2008; Socher et al., 2013; Goldberg, 2016).",
      "startOffset" : 53,
      "endOffset" : 169
    }, {
      "referenceID" : 6,
      "context" : "large-scale natural language processing applications (e.g. Turney and Pantel, 2010; Turian et al., 2010; Collobert and Weston, 2008; Socher et al., 2013; Goldberg, 2016).",
      "startOffset" : 53,
      "endOffset" : 169
    }, {
      "referenceID" : 26,
      "context" : "large-scale natural language processing applications (e.g. Turney and Pantel, 2010; Turian et al., 2010; Collobert and Weston, 2008; Socher et al., 2013; Goldberg, 2016).",
      "startOffset" : 53,
      "endOffset" : 169
    }, {
      "referenceID" : 13,
      "context" : "large-scale natural language processing applications (e.g. Turney and Pantel, 2010; Turian et al., 2010; Collobert and Weston, 2008; Socher et al., 2013; Goldberg, 2016).",
      "startOffset" : 53,
      "endOffset" : 169
    }, {
      "referenceID" : 17,
      "context" : "Despite the success of distributional representations in standard natural language processing tasks, a small but growing consensus within the artificial intelligence community suggests that these methods cannot be sufficient to induce adequate representations of words and concepts (Kiela et al., 2016; Gauthier and Mordatch, 2016; Lazaridou et al., 2015).",
      "startOffset" : 282,
      "endOffset" : 355
    }, {
      "referenceID" : 12,
      "context" : "Despite the success of distributional representations in standard natural language processing tasks, a small but growing consensus within the artificial intelligence community suggests that these methods cannot be sufficient to induce adequate representations of words and concepts (Kiela et al., 2016; Gauthier and Mordatch, 2016; Lazaridou et al., 2015).",
      "startOffset" : 282,
      "endOffset" : 355
    }, {
      "referenceID" : 19,
      "context" : "Despite the success of distributional representations in standard natural language processing tasks, a small but growing consensus within the artificial intelligence community suggests that these methods cannot be sufficient to induce adequate representations of words and concepts (Kiela et al., 2016; Gauthier and Mordatch, 2016; Lazaridou et al., 2015).",
      "startOffset" : 282,
      "endOffset" : 355
    }, {
      "referenceID" : 2,
      "context" : "Barsalou, 2008), are used to back up arguments for multimodal learning (at the weakest) or complete embodiment (at the strongest). Kiela et al. (2016) claim the following:",
      "startOffset" : 0,
      "endOffset" : 151
    }, {
      "referenceID" : 29,
      "context" : "They are a popular resource within psychology and cognitive science as models of human concept representation, and have been used to explain psycholinguistic phenomena from semantic priming and interference (Vigliocco et al., 2004) to the structure of early word learning in child language acquisition (Hills et al.",
      "startOffset" : 207,
      "endOffset" : 231
    }, {
      "referenceID" : 16,
      "context" : ", 2004) to the structure of early word learning in child language acquisition (Hills et al., 2009).",
      "startOffset" : 78,
      "endOffset" : 98
    }, {
      "referenceID" : 1,
      "context" : "Andrews et al. (2009) show how “experiential” semantic norm information can be used to model human judgments of concept similarity.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 24,
      "context" : "Rubinstein et al. (2015) confirm that word representations are especially effective at predicting taxonomic features versus attributive features.",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 5,
      "context" : "Collell and Moens (2016) find that word representations fail to pre# word tokens # word types",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 15,
      "context" : "Several studies have used distributional representations to reconstruct aspects of these semantic norm datasets (Herbelot and Vecchi, 2015; Fagarasan et al., 2015; Erk, 2016).",
      "startOffset" : 112,
      "endOffset" : 174
    }, {
      "referenceID" : 9,
      "context" : "Several studies have used distributional representations to reconstruct aspects of these semantic norm datasets (Herbelot and Vecchi, 2015; Fagarasan et al., 2015; Erk, 2016).",
      "startOffset" : 112,
      "endOffset" : 174
    }, {
      "referenceID" : 8,
      "context" : "Several studies have used distributional representations to reconstruct aspects of these semantic norm datasets (Herbelot and Vecchi, 2015; Fagarasan et al., 2015; Erk, 2016).",
      "startOffset" : 112,
      "endOffset" : 174
    }, {
      "referenceID" : 0,
      "context" : "The majority of the NLP work in this space has focused on the downstream task of augmenting word representations with novel grounded information, often evaluating on standard semantic similarity datasets (Agirre et al., 2009; Bruni et al., 2012; Faruqui et al., 2015; Bulat et al., 2016).",
      "startOffset" : 204,
      "endOffset" : 287
    }, {
      "referenceID" : 3,
      "context" : "The majority of the NLP work in this space has focused on the downstream task of augmenting word representations with novel grounded information, often evaluating on standard semantic similarity datasets (Agirre et al., 2009; Bruni et al., 2012; Faruqui et al., 2015; Bulat et al., 2016).",
      "startOffset" : 204,
      "endOffset" : 287
    }, {
      "referenceID" : 10,
      "context" : "The majority of the NLP work in this space has focused on the downstream task of augmenting word representations with novel grounded information, often evaluating on standard semantic similarity datasets (Agirre et al., 2009; Bruni et al., 2012; Faruqui et al., 2015; Bulat et al., 2016).",
      "startOffset" : 204,
      "endOffset" : 287
    }, {
      "referenceID" : 4,
      "context" : "The majority of the NLP work in this space has focused on the downstream task of augmenting word representations with novel grounded information, often evaluating on standard semantic similarity datasets (Agirre et al., 2009; Bruni et al., 2012; Faruqui et al., 2015; Bulat et al., 2016).",
      "startOffset" : 204,
      "endOffset" : 287
    }, {
      "referenceID" : 0,
      "context" : "The majority of the NLP work in this space has focused on the downstream task of augmenting word representations with novel grounded information, often evaluating on standard semantic similarity datasets (Agirre et al., 2009; Bruni et al., 2012; Faruqui et al., 2015; Bulat et al., 2016). Young et al. (2014) develop an alternative operationalization of denotational meaning using image captioning datasets, and demonstrate gains over distributional representations on textual similarity and entailment datasets.",
      "startOffset" : 205,
      "endOffset" : 309
    }, {
      "referenceID" : 23,
      "context" : "GloVe: GloVe (Pennington et al., 2014) estimates word representations wi by using them to reconstruct a word-word co-occurrence matrix X collected from a large text corpus:",
      "startOffset" : 13,
      "endOffset" : 38
    }, {
      "referenceID" : 21,
      "context" : "word2vec: word2vec (Mikolov et al., 2013) estimates word representations by optimizing a skip-gram objective to predict all words wj within a context window c of a word wi given their word representations:",
      "startOffset" : 19,
      "endOffset" : 41
    }, {
      "referenceID" : 20,
      "context" : "McRae Our initial experiments use the semantic norm dataset from McRae et al. (2005), which consists of 541 concrete noun concepts with associated feature norms, collected from 725 participants.",
      "startOffset" : 65,
      "endOffset" : 85
    }, {
      "referenceID" : 20,
      "context" : "The dataset groups features into several perceptual and non-perceptual categories: taxonomic, encyclopedic, function, visual-motion, visual-form and surface, visual-colour, sound, tactile, and taste (McRae et al., 2005).",
      "startOffset" : 199,
      "endOffset" : 219
    }, {
      "referenceID" : 7,
      "context" : "CSLB We reproduce and extend our results on a second semantic norm dataset collected by the Cambridge Centre for Speech, Language and the Brain (CSLB; Devereux et al., 2014).",
      "startOffset" : 144,
      "endOffset" : 173
    }, {
      "referenceID" : 7,
      "context" : "CSLB We reproduce and extend our results on a second semantic norm dataset collected by the Cambridge Centre for Speech, Language and the Brain (CSLB; Devereux et al., 2014). CSLB contains 638 concepts provided by 123 participants. Their data collection closely followed McRae et al. (2005), though features were included if at least 2 participants named that feature.",
      "startOffset" : 151,
      "endOffset" : 291
    }, {
      "referenceID" : 18,
      "context" : "We derive compact concept representations from the semantic norm datasets with LSA (Landauer et al., 1998).",
      "startOffset" : 83,
      "endOffset" : 106
    }, {
      "referenceID" : 22,
      "context" : "As a secondary data source, we also compute word-word similarity judgments from the WordNet taxonomy (Miller, 1995).",
      "startOffset" : 101,
      "endOffset" : 115
    } ],
    "year" : 2017,
    "abstractText" : "Distributional word representation methods exploit word co-occurrences to build compact vector encodings of words. While these representations enjoy widespread use in modern natural language processing, it is unclear whether they accurately encode all necessary facets of conceptual meaning. In this paper, we evaluate how well these representations can predict perceptual and conceptual features of concrete concepts, drawing on two semantic norm datasets sourced from human participants. We find that several standard word representations fail to encode many salient perceptual features of concepts, and show that these deficits correlate with word-word similarity prediction errors. Our analyses provide motivation for grounded and embodied language learning approaches, which may help to remedy these deficits.",
    "creator" : "LaTeX with hyperref package"
  }
}