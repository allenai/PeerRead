{
  "name" : "1403.0667.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "The Hidden Convexity of Spectral Clustering∗",
    "authors" : [ "James Voss", "Mikhail Belkin", "Luis Rademacher" ],
    "emails" : [ "vossj@cse.ohio-state.edu", "mbelkin@cse.ohio-state.edu", "lrademac@cse.ohio-state.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Geometrically, the proposed algorithms can be interpreted as hidden basis recovery by means of function optimization. We give a complete characterization of the contrast functions admissible for provable basis recovery. We show how these conditions can be interpreted as a “hidden convexity” of our optimization problem on the sphere; interestingly, we use efficient convex maximization rather than the more common convex minimization. We also show encouraging experimental results on real and simulated data.\nkeywords: spectral clustering, convex maximization, basis recovery"
    }, {
      "heading" : "1 Introduction",
      "text" : "Partitioning a dataset into classes based on a similarity between data points, known as cluster analysis, is one of the most basic and practically important problems in data analysis and machine learning. It has a vast array of applications from speech recognition to image analysis to bioinformatics and to data compression. There is an extensive literature on the subject, including a number of different methodologies as well as their various practical and theoretical aspects [11].\nIn recent years spectral clustering—a class of methods based on the eigenvectors of a certain matrix, typically the graph Laplacian constructed from data—has become a widely used method for cluster analysis. This is due to the simplicity of the algorithm, a number of desirable properties it exhibits and its amenability to theoretical analysis. In its simplest form, spectral bi-partitioning is an attractively straightforward algorithm based on thresholding the second bottom eigenvector of the Laplacian matrix of a graph. However, the more practically significant problem of multiway spectral clustering is considerably more complex. While hierarchical methods based on a sequence of binary splits have been used, the most common approaches use k-means or weighted k-means clustering in the spectral space or related iterative procedures [17, 15, 2, 25]. Typical algorithms for multiway spectral clustering follow a two-step process:\n∗A short version of this paper previously appeared in the proceedings of the Thirtieth AAAI Conference on Artificial Intelligence [20]. Implementations of the algorithms proposed in this paper can be found at https:// github.com/vossj/HBR-Spectral-Clustering. †Department of Computer Science and Engineering, the Ohio State University\nar X\niv :1\n40 3.\n06 67\nv3 [\ncs .L\nG ]\n4 M\nay 2\n01 6\n1. Spectral embedding: A similarity graph for the data is constructed based on the data’s feature representation. If one is looking for k clusters, one constructs the embedding using the bottom k eigenvectors of the graph Laplacian (normalized or unnormalized) corresponding to that graph.\n2. Clustering: In the second step, the embedded data (sometimes rescaled) is clustered, typically using the conventional/spherical k-means algorithms or their variations.\nIn the first step, the spectral embedding given by the eigenvectors of Laplacian matrices has a number of interpretations. The meaning can be explained by spectral graph theory as relaxations of multiway cut problems [19]. In the extreme case of a similarity graph having k connected components, the embedded vectors reside in Rk, and vectors corresponding to the same connected component are mapped to a single point. There are also connections to other areas of machine learning and mathematics, in particular to the geometry of the underlying space from which the data is sampled [4].\nWe propose a new class of algorithms for the second step of multiway spectral clustering. The starting point is that when the k clusters are perfectly separate, the spectral embedding using the bottom k eigenvectors has a particularly simple geometric form. For the unnormalized (or asymmetric normalized) Laplacian, it is simply a (weighted) orthogonal basis in k-dimensional space, and recovering the basis vectors is sufficient for cluster identification. This view of spectral clustering as basis recovery is related to previous observations that the spectral embedding generates a discrete weighted simplex (see [21, 12] for some applications). For the symmetric normalized Laplacian, the structure is slightly more complex, but is still suitable for our analysis. Moreover, our proposed algorithms can be used without modification.\nThe proposed approach relies on an optimization problem resembling certain Independent Component Analysis techniques, such as FastICA (see [10] for a broad overview). Specifically, the problem of identifying k clusters reduces to maximizing a certain “admissible” contrast function over a (k − 1)-sphere. Our main theoretical contribution is to formulate a general version of the basis recovery problem arising in spectral clustering, and to characterize the set of admissible contrast functions for guaranteed recovery1 (Section 2). Rather than the more usual convex minimization, our analysis is based on convex maximization over a (hidden) convex domain. Interestingly, while maximizing a convex function over a convex domain is generally difficult (even maximizing a positive definite quadratic form over the continuous cube [0, 1]n is NP-hard2), our setting allows for efficient optimization.\nBased on this theoretical connection between clusters and local maxima of contrast functions over the sphere, we propose practical algorithms for cluster recovery through function maximization. We discuss the choice of contrast functions and provide running time analysis. We also provide a number of encouraging experimental results on synthetic and real-world data sets.\nWe also note connections to recent work on geometric recovery. Anderson et al. [1] use the method of moments to recover a continuous simplex given samples from the uniform probability distribution. Like in our work, Anderson et al. use efficient enumeration of local maxima of a function over the sphere. Also, one of the results of Hsu and Kakade [9] shows recovery of parameters in a Gaussian Mixture Model using the moments of order three, and this result can be thought of as a case of the basis recovery problem.\nThe paper is structured as follows: In Section 2, we provide our main technical results on basis recovery and briefly outline its connection to spectral clustering. In Sections 3 and 4 we\n1Interestingly, there are no analogous recovery guarantees in the ICA setting except for the special case of cumulant functions as contrasts. In particular, typical versions of FastICA are known to have spurious maxima [22].\n2This follows from [7] together with Fact 6 below.\nintroduce spectral clustering and formulate it in terms of basis learning. In Section 5 we provide the main theoretical results for basis recovery in the spectral clustering setting, and discuss algorithic implementation details. Our experimental results are given in Section 6. Finally in Section 7, we handle the deferred proof details and discuss the admissibility of normalized graph Laplacians for our framework."
    }, {
      "heading" : "2 Basis Recovery and Spectral Clustering",
      "text" : "In this section, we provide our main technical results on hidden basis recovery. Then, we briefly discuss how our results will apply to the spectral clustering setting.\nA Note on Notation. In what follows, we will use the following notations. For a matrix B, bij indicates the element in its ith row and jth column. The ith row vector of B is denoted bi • , and the jth column vector of B is denoted b • j . For a vector v, ‖v‖ denotes its standard Euclidean 2-norm. Given two vectors u and v, u • v denotes their dot product. We denote the set {1, 2, . . . , k} by [k]. We denote by 1S the indicator vector for the set S, i.e. the vector which is 1 for indices in S and 0 otherwise. The null space of a matrix M is denoted N (M). We denote the unit sphere in Rd by Sd−1. For points p1, . . . , pm, conv(p1, . . . , pm) will denote their convex hull. All angles are given in radians, and ∠(u,v) denotes the angle between the vectors u and v in the domain [0, π]. We use 7→ to define anonymous functions; for instance t 7→ t2 is the function f : R → R defined by f(t) := t2. Finally, for X a subspace of Rd, PX denotes the square matrix corresponding to the orthogonal projection from Rd to X ."
    }, {
      "heading" : "2.1 Basis Recovery via Convex Maximization",
      "text" : "The main technical results of this section deal with reconstructing a hidden basis by simple optimization techniques. For this purpose, we introduce the following class of functions.\nDefinition 1. A function F : Rd → R is said to be an orthogonal basis encoding function (orthogonal BEF) if there exists an orthonormal basis z1, . . . , zd of Rd and functions gi : R→ R such that F (u) = ∑d i=1 gi(u • zi).\nWe will assume throughout that the functions gi (and hence F ) are continuously differentiable. In this section, we provide conditions under which recovery of the hidden basis z1, . . . , zd (up to sign) can be guaranteed for an orthogonal BEF using simple function maximization techniques. To motivate our conditions, it will be useful to first consider a classic problem which fits into the orthogonal BEF framework: the eigendecomposition of positive definite symmetric matrices.\nExample 2 (Symmetric PSD Matrix Eigendecompositions). Let A be a symmetric positive semidefinite matrix with eigendecomposition A = ∑d i=1 λiziz T i . The function FA : Rd → R defined by FA(u) := u TAu = ∑d i=1 λi(u • zi)\n2 is an orthogonal BEF with the functions gi : R→ R defined as gi(x) := λix\n2. If the eigenvalues are ordered such that λ1 > λ2 > . . . > λd, then the directions ±z1 are the maxima (local and global) of FA on the domain Sd−1. Further, after ±z1 is recovered, we may maximize FA in the orthogonal complement of z1 to recover ±z2. This deflationary procedure can be extended to recover all eigenvectors of FA (see Algorithm 1 for the idea).\nHowever, when A has repeated eigenvalues, then its eigendecomposition is no longer uniquely defined. For the identity matrix I, any orthonormal basis in Rd can be used to form its eigenvectors, and the function FI(u) = 1 for any choice of u ∈ Sd−1. In general, the hidden basis recovery problem arising in the eigendecomposition problem is only uniquely defined when there are no repeat eigenvalues.\nAlgorithm 1 The deflationary scheme for hidden basis recovery. This is an abstract algorithm which when given access to an orthogonal BEF F (u) = ∑d i=1 gi(u • zi) satisfying Assumption 3, it recovers and returns estimates of the hidden basis directions z1, . . . , zd up to unknown signs and potentially an unknown permutation.\n1: for i← 1 to d do 2: Find z̃i a local maximizer of F on Sd−1 ∩ span({z̃1, z̃2, . . . , z̃i−1})⊥ 3: end for 4: return z̃1, . . . , z̃d.\nAs pointed out by the Example 2, we will need to understand the conditions under which a deflationary approach to maximizing a BEF F on Sd−1 (see Algorithm 1) can be guaranteed to recover the hidden basis z1, . . . , zd. We also wish that the hidden basis z1, . . . , zd be uniquely defined by the BEF F . It turns out that the following assumption is sufficient for performing guaranteed basis recovery.\nAssumption 3 (Strict convexity). For all i ∈ [d], t 7→ gi(sign(t) √ |t|) is strictly convex.\nMore formally, we have the following result.\nTheorem 4. Suppose that F is an orthogonal BEF satisfying Assumption 3. Then, the set of local maxima of F on the unit sphere is non-empty and contained in the set {±z1, . . . ,±zd}.\nThe Assumption 3 is sufficient for hidden basis recovery in the sense of the following Corollary. Its proof is an exercise in induction on the number of recovered vectors z̃j , where the inductive step is a result of Theorem 4.\nCorollary 5. If F is an orthogonal BEF satisfying Assumption 3, then the abstract Algorithm 1 returns vectors z̃1, . . . , z̃d which recover the directions z1, . . . , zd up to a choice of signs and permutation. More precisely, there exists sign si ∈ {±1} and a permutation p of [d] such that zi = siz̃p(i) for each i ∈ [d].\nBefore proceding with the proof of Theorem 4, it is worth discussing the importance of strict convexity in Assumption 3. In the case of the matrix eigendecomposition Example 2 with the identity matrix I, we constructed an orthogonal BEF with contrast functions gi(t) = t2 which satisfy that each gi(sign(t) √ |t|) = t is convex but not strictly convex. The function FI(u) is constant on the unit sphere, and there is no uniquely defined hidden basis (or eigenvector basis) for the identity matrix. In this sense, it does not suffice for t 7→ g(sign(t) √ |t|) to be convex.\nInterestingly, the only issue which can arise when strict convexity is relaxed to convexity in Assumption 3 is that the function F may plateau (become constant) on regions within the unit sphere Sd−1. Strict convexity is one way to ensure that this does not happen. Nevertheless, the problem of recovering the eigendecomposition of a positive definite symmetric matrix A (Example 2) is a limit case of our framework. Moreover, Algorithm 1 can be used to perform eigenvector recovery since one does not require uniqueness of the eigenvector basis.\nThe intuition behind Assumption 3 is captured in the proof of Theorem 4. The main idea is to introduce a change of variable and recast maximization of F over the unit sphere as a convex maximization problem defined over a (hidden) convex domain.\nProof of Theorem 4. We will use the following Fact about convex maximization (see [16, Chapter 32] for an overview of concepts related to convex maximization).\nFor a convex set K, a point x ∈ K is said to be an extreme point if x is not equal to a strict convex combination of two other points in K.\nFact 6. Suppose that K is a closed and bounded convex set. Let f : K → R be a strictly convex function. Then, the set of local maxima of f on K is non-empty and contained in the set of extreme points of K.\nAs z1, . . . , zd form an orthonormal basis of the space, we may simplify notation and work in the coordinate system in which z1, . . . , zd are the canonical vectors e1, . . . , ed. We define ∆\nd−1 := conv(e1, . . . , ed) a (hidden) simplex, and Q d−1 + := {u ∈ Sd | ui ≥ 0 for all i ∈ [d]} the restriction of the sphere onto the positive orthant. By the symmetries of the problem, it suffices to show that the set S of local maxima of F with respect to Qd−1+ is non-empty and that S ⊂ {e1, . . . , ed}.\nThe main idea is to use the change of variable ψ : Qd−1+ → ∆d−1 defined by ψi(u) = u2i . Since\nF ◦ ψ−1(x) = d∑\ni=1\ngi(ψ −1 i (x)) = d∑ i=1 gi( √ xi) , (1)\nthen by Assumption 3, F ◦ ψ−1 : ∆d−1 → R is a strictly convex function defined on a closed and bounded convex domain. By Fact 6, we note that the set S′ of local maxima of F ◦ψ−1 on ∆d−1 is nonempty and contained in the set {e1, . . . , ed} of extreme points of ∆d−1. Pulling back to Qd−1+ , we see that S = ψ−1(S′) is a non-empty subset of {e1, . . . , ed}."
    }, {
      "heading" : "2.2 Spectral Clustering as Basis Recovery",
      "text" : "It turns out that orthogonal basis recovery has direct implications for spectral clustering. In particular, when an n-vertex similarity graph G has k connected components corresponding to the desired clusters, it will be seen in section 4 that the spectral embedding into Rk maps each vertex vi in the j\nth connected component onto a ray protruding from the origin in a direction zj . It happens that the directions z1, . . . , zk are orthogonal. We let xi denote the embedded points and we construct the function\nFg(u) := 1\nn n∑ i=1 g(|u • xi|) ,\nfrom the embedded data and the contrast function g : R→ R. To see that Fg is actually an orthogonal BEF, we consider the following theoretical construction: Let S1, . . .Sk be the vertex index sets corresponding to the distinct components of the graph G, and define the functions gj : R→ R for all j ∈ [k] by gj(t) = 1n ∑ i∈Sj g(t‖xi‖). Then, it may be verified\nthat Fg(u) = ∑k\nj=1 gj(u • zj), which takes on the form of an orthogonal BEF. In particular, we will be able to recover the directions z1, . . . , zk corresponding to the desired clusters by maximizing the function Fg on the unit sphere Sk−1.\nDue to the special form of orthogonal BEF which arises in spectral clustering, we will have slightly stronger guarantees. In particular, it will be seen (Theorem 9 and Theorem 18) all of the directions of ±z1, . . . ,±zk are strict local maximum of Fg on Sk−1 instead of just some."
    }, {
      "heading" : "3 Spectral Clustering Problem Statement",
      "text" : "Let G = (V,A) denote a similarity graph where V is a set of n vertices and A is an adjacency matrix with non-negative weights. Two vertices i, j ∈ V are incident if aij > 0, and the value of aij is interpreted as a measure of the similarity between the vertices. In spectral clustering, the goal is to partition the vertices of a graph into sets S1, . . . ,Sk such that these vertex sets form natural clusters in the graph. In the most basic setting, G consists of k connected components, and the\nnatural clusters should be the components themselves. In this case, if i′ ∈ Si and j′ ∈ Sj then ai′j′ = 0 whenever i 6= j. For convenience, we can consider the vertices of V to be indexed such that all indices in Si precede all indices in Sj when i < j. The matrix A takes on the form:\nA =  AS1 0 · · · 0 0 AS2 · · · 0 ... ... . . .\n... 0 0 · · · ASk  , a block diagonal matrix. In this setting, spectral clustering can be viewed as a technique for reorganizing a given similarity matrix A into such a block diagonal matrix.\nIn practice, G rarely consists of k truly disjoint connected components. Instead, one typically observes a matrix Ã = A+E where E is a perturbation from the clean setting. The goal of spectral clustering is to permute the rows and columns of Ã to form a matrix which is nearly block diagonal and to recover the corresponding clusters."
    }, {
      "heading" : "4 The Spectral Embedding",
      "text" : "Given an n-vertex similarity graph G = (V,A), let D be the diagonal degree matrix with non-zero entries dii = ∑ j∈V aij . The graph Laplacian is defined as L := D − A. The following well known property of the graph Laplacian (see [19] for a review) helps shed light on its importance: Given u ∈ Rn,\nuTLu = 1\n2 ∑ i,j∈V aij(ui − uj)2 . (2)\nThe graph Laplacian L is symmetric positive semi-definite as equation (2) cannot be negative. Further, u is a 0-eigenvector of L (or equivalently, u ∈ N (L)) if and only if uTLu = 0. When G consists of k connected components with indices in the sets S1, . . . ,Sk, inspection of equation (2) gives that u ∈ N (L) precisely when u is piecewise constant on each Si. In particular,\n{|S1|−1/21S1 , . . . , |Sk| −1/21Sk} (3)\nis an orthonormal basis for N (L). In general, letting X ∈ Rn×k contain an orthogonal basis of N (L), it cannot be guaranteed that the rows of X will act as indicators of the various classes, as the columns of X have only been characterized up to a rotation within the subspace N (L). However, the rows of X are contained in a scaled orthogonal basis of Rk with the basis directions corresponding to the various classes. We formulate this result as follows (see [21], [18, Proposition 5], and [15, Proposition 1] for related statements).\nProposition 7. Let the similarity graph G = (V,A) contain k connected components with indices in the sets S1, . . . ,Sk, let n = |V |, and let L be the graph Laplacian of G. Then, N (L) has dimensionality k. Let X = (x •1, . . . , x •k) contain k scaled, orthogonal column vectors forming a basis of N (L) such that ‖x • j‖ = √ n for each j ∈ [k]. Then, there exist weights w1, . . . , wk with wj = |Sj | n and mutually orthogonal vectors z1, . . . , zk ∈ R\nk such that whenever i ∈ Sj, the row vector xi • =\n1√ wj zTj .\nProof. We define the matrix MSi := 1Si1 T Si . PN (L) can be constructed from any orthonormal basis of N (L). Using the two bases {|S1|−1/21S1 , . . . , |Sk| −1/21Sk} and { 1√nx •1, . . . , 1√ n x •k} yields:\nPN (L) = k∑\ni=1\n|Si|−1MSi and PN (L) = 1\nn XXT .\nThus for i, j ∈ V , 1nxi • • xj • = (PN (L))ij . In particular, if there exists ` ∈ [k] such that i, j ∈ S`, then 1nxi • • xj • = |S`|\n−1. When i and j belong to separate clusters, then xi • ⊥ xj • . If i, j ∈ Sj , then\ncos(∠(xi • , xj • )) = xi • • xj •\n‖xi •‖‖xj •‖ =\n|S`|−1\n|S`|−1/2|S`|−1/2 = 1 ,\nimplies that xi • and xj • are in the same direction. As they also have the same magnitude, xi • and xj • coincide for any two indices i and j belonging to the same component of G.\nThus letting wi := |Si| n for i = 1, . . . , k, there are k perpendicular vectors z1, . . . , zk correspond-\ning to the k connected components of G such that xi • = 1√ w` zT` for all i ∈ S`.\nProposition 7 demonstrates that using the null space of the graph Laplacian, the k connected components in G are mapped to k scaled, orthogonal basis vectors in Rk. Of course, under a perturbation of A, the interpretation of Proposition 7 must change. In particular, G will no longer consist of k connected components, and instead of using only vectors in N (L), X must be constructed using the eigenvectors corresponding to the lowest k eigenvalues of L. With the perturbation of A comes a corresponding perturbation of the eigenvectors in X. Using the perturbation theory of symmetric matrices, it can be shown that when the perturbation is not too large, the structure of X is approximately maintained (see [5, 19]).\nDue to different properties of the resulting spectral embeddings, normalized graph Laplacians are often used in place of L for spectral clustering, in particular the symmetric normalized Laplacian Lsym := D −1/2LD−1/2 and the asymmetric normalized Laplacian Lrw := D −1L. These normalized Laplacians are often viewed as more stable to perturbations of the graph structure. Further, spectral clustering with Lsym has a nice interpretation as a relaxation of the NP-hard multi-way normalized graph cut problem [25], and the use of Lrw has connections to the theory of Markov chains [6, 14].\nFor simplicity, we focus first on the unnormalized graph Laplacian L. However, when G consists of k connected components, N (Lrw) happens to be identical to N (L). The algorithms which we will propose for spectral clustering turn out to be equally valid when using any of L, Lsym, or Lrw, though the structure of N (Lsym) gives rise to a slightly more complicated ray-based basis structure. The discussion of N (Lsym) and its admissibility are deferred to Section 7."
    }, {
      "heading" : "5 Basis Recovery for Spectral Clustering",
      "text" : "We now focus on the second step of spectral clustering, which is clustering the points embedded by the Laplacian embedding into the desired clusters. In particular, we will now demonstrate that the embedded data (the rows of X in Proposition 7) may be used to construct a function optimization problem whereby the maxima structure of the function can be used to recover the desired clusters.\nConstruction 8. Given a graph G with n vertices and k connected components, let X; S1, . . . ,Sk; w1, . . . , wk; z1, . . . , zk; and L as in Proposition 7. We construct a function Fg : Sk−1 → R on the\nunit sphere using a contrast function g : [0,∞)→ R where it is assumed that t 7→ g( √ t) is strictly convex. We construct Fg as\nFg(u) := 1\nn n∑ i=1 g(|u • xi • |) . (4)\nUsing Proposition 7, this may be equivalently written as\nFg(u) = k∑ i=1 wig( 1√ wi |u • zi|) . (5)\nIn Construction 8, the vectors z1, . . . , zk form an unseen orthonormal basis of Rk, and each weight wi = |Si| n is the fraction of the rows of X indexed as x` • which are embedded from the i th component of G and which coincide with the point 1√wi z T i . Since each embedded point in the i th cluster lies on the line through zi and −zi, it suffices to recover the basis directions z1, . . . , zk up to sign in order to cluster the points. Our idea is to show that Fg is an orthogonal BEF which satisfies Assumption 3 with the directions z1, . . . , zk corresponding to the BEF basis. As such, we will be able to use the maxima structure of Fg on Sk−1 in order to recover the hidden basis and thence the desired clustering.\nWe use equation (5) to see that Fg is a special form of orthogonal BEF with the functions gi (see Definition 1) defined by gi(t) := wig(\n1√ wi |t|). Further, since t 7→ g(\n√ t) is strictly convex, we\nsee that t 7→ gi(sign(t) √ |t|) is strictly convex for all i ∈ [k], and hence Fg satisfies Assumption 3. However, due to the special form of Fg , each of the directions {±zi : i ∈ [k]} are maxima of Fg over Sk−1 (as opposed to just some, cf. Theorem 4).\nTheorem 9. Let Fg : Sk−1 → R and z1, . . . , zk be defined as in Construction 8. Then, the set {±zi : i ∈ [k]} is a complete enumeration of the local maxima of Fg.\nWe defer the proof of Theorem 9 to section 7.2. We also provide and prove the analogous result for when Fg is constructed using the Laplacian embedding arising from Lrw or Lsym in section 7.2.\nAs Fg is an orthogonal BEF, it follows from the discussion in section 2 that by enumerating the local maxima of Fg using a deflationary scheme, we may recover the hidden basis z1, . . . , zk corresponding to the graph clusters. By Theorem 9, we get slightly more flexibility in our algorithmic design since it is known that each of the directions z1, . . . , zk is a local maximum of Fg on Sk−1, and therefore we have room to relax the orthogonality constraint from the prototypical deflationary scheme (Algorithm 1) when designing algorithms for hidden basis recovery in the spectral clustering setting."
    }, {
      "heading" : "5.1 Proposed Algorithms",
      "text" : "We now design a new class of algorithms for spectral clustering. Given a similarity graph G = (V,A) containing n vertices, define a graph Laplacian L̃ among L, Lrw, and Lsym (reader’s choice). Viewing G as a perturbation of a graph consisting of k connected components, construct X ∈ Rn×k such that x • i gives the eigenvector corresponding to the i th smallest eigenvalue of L̃ with scaling ‖x • i‖ = √ n.\nWith X in hand, choose a contrast function g satisfying the strict convexity condition from Assumption 3. From g, the function Fg(u) = 1 n ∑n i=1 g(u • xi • ) is defined on Sk−1 using the rows of X. The local maxima of Fg correspond to the desired clusters of the graph vertices. Since Fg is a symmetric function, if Fg has a local maximum at u, Fg also has a local maximum at −u. However, the directions u and −u correspond to the same line through the origin of Rk and form an equivalence class, with each such equivalence class corresponding to a cluster.\nOur first goal is to find local maxima of Fg corresponding to distinct equivalence classes. We will use that the desired maxima of Fg should be approximately orthogonal to each other. Once we have obtained local maxima u1, . . . ,uk of Fg , we cluster the vertices of G by placing vertex i in the jth cluster using the rule j = arg max` |u` • xi • |. We sketch two algorithmic ideas in HBRopt and HBRenum (where HBR stands for hidden basis recovery).\nAlgorithm 2 Finds the local maxima of Fg defined from the embedded vertices xi • which we want to cluster. The second input η is the learning rate (step size).\n1: function HBRopt(X, η) 2: C ← {} 3: for i← 1 to k do 4: Draw u uniformly from Sk−1 ∩ span(C)⊥ 5: repeat 6: u← u + η(∇Fg(u)− uuT∇Fg(u)) (= u + ηPu⊥∇Fg(u)) 7: u← Pspan(C)⊥u 8: u← u‖u‖ 9: until Convergence\n10: Let C ← C ∪ {u} 11: end for 12: return C 13: end function\nHBRopt is a form of projected gradient ascent which more fully implements the deflationary scheme of Algorithm 1. The parameter η is the learning rate. Each iteration of the repeat-until loop moves u in the direction of steepest ascent. For gradient ascent in Rk, one would expect step 6 of HBRopt to read u ← u + η∇Fg(u). However, gradient ascent is being performed for a function Fg defined on the unit sphere, but the gradient described by ∇Fg is for the function Fg with domain Rk. The more expanded formula ∇Fg(u)−uuT∇Fg(u) is the projection of ∇Fg onto the plane tangent to Sk−1 at u. This update keeps u near the sphere.\nWe may draw u uniformly at random from Sk−1 ∩ span(C)⊥ by first drawing u from Sk−1 uniformly at random, projecting u onto span(C)⊥, and then normalizing u. It is important that u stay near the orthogonal complement of span(C) in order to converge to a new cluster rather than converging to a previously found optimum of Fg . Step 7 enforces this constraint during the update step.\nIn contrast to HBRopt, HBRenum more directly uses the point separation implied by the orthogonality of the approximate cluster centers. Since each embedded data point should be near to a cluster center, the data points themselves are used as test points. Instead of directly enforcing orthogonality between cluster means, a parameter δ > 0 specifies the minimum allowable angle between found cluster means.\nBy pre-computing the values of Fg(xi •/‖xi •‖) outside of the while loop, HBRenum can be run in O(kn2) time. HBRenum is likely to be slower than HBRopt which takes O(k2nt) time where t is the average number of iterations to convergence. The number of clusters k cannot exceed (and is usually much smaller than) the number of graph vertices n.\nHBRenum has a couple of nice features which may make it preferable on smaller data sets. Each center found by HBRenum will always be within a cluster of data points even when the optimization landscape is distorted under perturbation. In addition, the maxima found by HBRenum are based on a more global outlook, which may be useful in the noisy setting.\nAlgorithm 3 Finds the local maxima of Fg defined from the points xi • needed for clustering. The second input δ controls how far a point needs to be from previously found cluster centers to be a candidate future cluster center.\n1: function HBRenum(X, δ) 2: C ← {} 3: while |C| < k do 4: j ← arg maxi{Fg( xi •‖xi • ‖) | ∠( xi •‖xi • ‖ ,u) > δ ∀u ∈ C} 5: C ← C ∪ { xj •‖xj • ‖} 6: end while 7: return C 8: end function"
    }, {
      "heading" : "5.2 Choosing a Contrast Function",
      "text" : "There are many possible choices of contrast g which are admissible for spectral clustering under Theorem 9 including the following:\ngsig(t) = − 1\n1 + exp(−|t|) gp(t) = |t|p where p ∈ (2,∞)\nggau = e −t2 gabs(t) = −|t| ght(t) = log cosh(t)\nIn choosing contrasts, it is instructive to first consider the function g2(y) = y 2 (which relaxes the criterion that t 7→ g( √ |t|) be strictly convex to plain convexity and is thus not admissible).\nNoting that Fg2(u) = ∑k i=1wi( 1√ wi u • zi) 2 = 1, we see that Fg2 is constant on the unit sphere. We see that the distinguishing power of a contrast function for spectral clustering comes from our assumption that t 7→ g( √ |t|) is strictly convex. Intuitively, “more strictly convex” contrasts have better resolving power but are also more sensitive to outliers and perturbations of the data. Indeed, if g grows too rapidly, a small number of outliers far from the origin could significantly distort the maxima structure of Fg .\nDue to this tradeoff, gsig and gabs could be important practical choices for the contrast function. Both gsig( √ |t|) and gabs( √ |t|) have a strong convexity structure near the origin. As gsig is a\nbounded function, it should be very robust to perturbations. In comparison, gabs( √ |t|) = − √ |t| maintains a stronger convexity structure over a much larger region of its domain, and gabs(t) has only a linear rate of growth as t → ∞. This is a much slower growth rate than is present for instances in gp with p > 2."
    }, {
      "heading" : "6 Clustering Experiments",
      "text" : "We now discuss our test results on our proposed spectral clustering algorithms on a variety of real and simulated data. The implementations for our spectral clustering algorithms are available on github: https://github.com/vossj/HBR-Spectral-Clustering."
    }, {
      "heading" : "6.1 An Illustrating Example",
      "text" : "Figure 1 illustrates our function optimization framework for spectral clustering. In this example, random points pi were generated from 3 concentric circles: 200 points were drawn uniformly at\nrandom from a radius 1 circle, 350 points from a radius 3 circle, and 700 points from a radius 5 circle. The points were then radially perturbed. The generated points are displayed in Figure 1 (a). The similarity matrix A was constructed as aij = exp(−14‖pi−pj‖\n2), and the Laplacian embedding was performed using Lrw.\nFigure 1 (b) depicts the clustering process with the contrast gsig on the resulting embedded points. In this depiction, the embedded data sufficiently encodes the desired orthogonal basis structure that all local maxima of Fgsig correspond to desired clusters. The value of Fgsig is displayed by the grayscale heat map on the unit sphere in Figure 1 (b), with lighter shades of gray indicate greater values of Fgsig . The cluster labels were produced using HBRopt. The rays protruding from the sphere correspond to the basis directions recovered by HBRopt, and the recovered labels are indicated by the color and symbol used to display each data point."
    }, {
      "heading" : "6.2 Image Segmentation Examples",
      "text" : "Spectral clustering was first applied to image segmentation by Shi and Malik [17], and it has remained a popular application of spectral clustering. The goal in image segmentation is to divide an image into regions which represent distinct objects or features of the image. Figure 2 and Figure 3 show several segmentations produced by HBRopt-gabs and spherical k-means on several example images from the BSDS300 test set [13].\nFor this example application, we used a relatively simple notion of similarity based only on the color and proximity of the image’s pixels. Let pi denote the i\nth pixel. Each pi has a location xi and an RGB color ci = (ri, gi, bi)\nT . We used the following similarity between any two distinct pixels pi and pj :\naij =\n{ e− 1 α2 ‖xi−xj‖2e − 1 β2 ‖ci−cj‖2 if ‖xi − xj‖ < R\n0 if ‖xi − xj‖ ≥ R (6)\nfor some parameters α, β, and radius R. By enforcing that aij is 0 for points which are not too close, we build a sparse similarity matrix which greatly speeds up the computations. As the similarity measure decays exponentially with distance, the zeroed entries would be very small anyway.\nDetermining the number of clusters to use in spectral clustering is an unsolved problem. However, the BSDS300 data set includes hand labeled segmentations. From the hand labeled segmen-\ntations for a particular image, one human segmentation was chosen at random and the number of segments from that segmentation was used as the number of clusters k for spectral clustering to search for. No other information from the human segmentations was used in generating the image segmentations.\nIn order to reduce the effect of salt and pepper type noise, the images were preprocessed using 9×9 median filtering prior to constructing the similarity matrices. The similarity from equation (6) was constructed with common fixed values of α, β, and R across all images. Spectral clustering was performed using HBRopt under the contrast function gabs and the Lrw embedding.\nQualitatively, we found that for the same embedding, k-means is more likely to over segment large regions within the image, in effect balancing the cluster sizes. In contrast, our proposed HBRopt algorithm tended to segment out additional small regions within the image more frequently."
    }, {
      "heading" : "6.3 Stochastic Block Model with Imbalanced Clusters",
      "text" : "We construct a similarity graph A = diag(A1, A2, A3) + E where each Ai is a symmetric matrix corresponding to a cluster and E is a small perturbation. We set A1 = A2 to be 10× 10 matrices with entries 0.1. We set A3 to be a 1000 × 1000 matrix which is symmetric, approximately 95% sparse with randomly chosen non-zero locations set to 0.001. When performing this experiment 50 times, HBRopt-gsig obtained a mean accuracy of 99.9%. In contrast, spherical k-means with\nrandomly chosen starting points obtained a mean accuracy of only 42.1%. It turns out that splitting the large cluster is in fact optimal in terms of the spherical k-means objective function but leads to poor classification performance. Our method does not suffer from that shortcoming."
    }, {
      "heading" : "6.4 Performance Evaluation on UCI Datasets",
      "text" : "We compare spectral clustering performance on a number of data sets with unbalanced cluster sizes. In particular, we use the E. coli, flags, glass, Iris, thyroid disease, and car evaluation data sets which are part of the UCI machine learning repository [3]. We also use the standardized gene expression data set [24, 23], which is also referred to as cell cycle. For the flags data set, we used religion as the ground truth labels, and for thyroid disease, we used the new-thyroid data.\nFor all data sets, we only used fields for which there were no missing values, we normalized the data such that every field had unit standard deviation, and we constructed the similarity matrix A using a Gaussian kernel k(yi,yj) = exp(−α‖yi−yj‖2). The parameter α was chosen separately for each data set in order to create a good embedding. The choices of α were: 0.25 for E. coli, 32\nfor glass, 0.5 for Iris, 32 for thyroid disease, 128 for flags, 0.25 for car evaluation, and 0.125 for cell cycle.\nThe spectral embedding was performed using the symmetric normalized Laplacian Lsym. Then, the clustering performance of our proposed algorithms HBRopt and HBRenum (implemented with δ = 3π/8 radians) were compared with the following baselines:\n• oracle-centroids: The means µj = 1|Sj | ∑ i∈Sj xi • ‖xi • ‖ are set using the ground truth labels for\nall j ∈ [k]. Points are assigned to their nearest cluster mean in cosine distance.\n• k-means-cosine: Spherical k-means (standard matlab kmeans library function called using the cosine distance and using the default k-means++ mean initialization) is run with a random initialization of the means, (cf. [15]).\nWe report the clustering accuracy of each algorithm in Table 1. The accuracy is computed using the best matching between the clusters and the true labels. The reported results consist of the mean performance over a set of 25 runs for each algorithm. The number of clusters being searched for was set to the ground truth number of clusters. In most cases, our proposed algorithms show improvement in performance over spherical k-means."
    }, {
      "heading" : "7 Basis Recovery With Each Laplacian Embedding",
      "text" : "We have already argued that graph Laplacians L and Lrw can be used for spectral clustering within our BEF framework, and we have asserted that Lsym can also be used. We now discuss how orthogonal BEF recovery can be used for spectral clustering in the setting where G consists of k connected components using any of the graph Laplacians. First, in section 7.1, we show how the Laplacian embedding for the symmetric normalized Laplacian Lsym differs and generalizes upon the embedding structure arising for L and Lrw (cf. Proposition 7). Then, in section 7.2, we prove that the spectral embedding induced by any of the discussed graph Laplacians gives rise to an optimization problem on Sk−1 in which the local maxima enumerate the desired clusters for spectral clustering. More precisely, we prove a generalization of Theorem 9 which includes embeddings generated using L, Lrw, and Lsym.\nThe discussion in this section highlights the differences between using Lsym and using either L or Lrw for the proposed spectral algorithms. Whereas taking an orthogonal basis of N (L) or N (Lrw) produces embedded points which are orthogonal and of fixed norm within any particular class, using N (Lsym) produces embedded points along perpendicular rays but with varying intra-class norms as will be seen in Proposition 10. Despite these differences, when given a contrast function\ng meeting the strict convexity criterion from Assumption 3, the proposed algorithms HBRopt and HBRenum which worked for spectral clustering using L and Lrw also work for spectral clustering using Lsym."
    }, {
      "heading" : "7.1 Null Space Structure of the Normalized Laplacians",
      "text" : "We now investigate the null space structure of the normalized graph Laplacians. We will first describe the null space structures Lsym and Lrw for a graph G consisting of k components. Then, we will show how the null space structures of Lsym, Lrw, and L can all be viewed within a single, more generalized notion of a graph embedding.\nLet G = (V,A) be an n-vertex graph containing k connected components such that the ith\ncomponent has vertices with indices in the set Si. For any set C ⊂ V , we define δD(C) := ∑ i∈C dii . (7)\nwhere D = diag(d11, d22, . . . , dnn) is a diagonal matrix with strictly positive entries. For now, we will take D to be the diagonal degree matrix D such that dii = dii = ∑n j=1 aij . Then, δD(C) is the sum of vertex degrees for vertices in the set C. Using this definition, we are able to characterize the embedding structure of Lsym.\nProposition 10. Let G be a similarity graph consisting of k connected components for which Lsym is well defined. Let the vertex indices be partitioned into sets S1, . . . ,Sk corresponding to the k connected components. Then, dim(N (Lsym)) = k. If X = (x •1, . . . , x •k) contains a scaled basis of N (Lsym) in its columns such that ‖x • i‖ = √ n, then there exist k mutually orthogonal unit vectors z1, . . . , zk such that whenever i ∈ Sj, the row vector\nxi • = √ ndiiδD(Sj)−1zTj . (8)\nProof. An important property of the symmetric normalized Laplacian [19, Proposition 3] is that for all u ∈ Rn,\nuTLsymu = 1\n2 ∑ i,j∈V aij\n( ui\nd 1/2 ii\n− uj d\n1/2 jj\n)2 . (9)\nLsym is positive semi-definite, and u is a 0-eigenvector of Lsym if and only if plugging u into equation (9) yields 0. Let ySj be the vector such that\nySj =\n{ d\n1/2 ii if i ∈ Sj .\n0 otherwise . (10)\nThen, B = (δD(S1)−1/2yS1 , . . . , δD(Sd) −1/2ySd) contains an orthonormal basis for N (Lsym) in its columns. Defining MSi = ySiy T Si , we get:\nPN (L) = BB T = k∑ i=1 δD(Si)−1MSi . (11)\nBut PN (Lsym) can be constructed from any orthonormal basis of N (Lsym). In particular, PN (Lsym) = 1 nXX T as well. Hence, 1nxi • • xj • = (PN (L))ij = δD(S`) −1d 1/2 ii d 1/2 jj precisely when there exists ` ∈ [k] such that i, j ∈ S`. Otherwise, xi • ⊥ xj • .\nNote that for i, j ∈ S`,\ncos(∠(xi • , xj, • )) = xi • • xj •\n‖xi •‖‖xj •‖\n= nδD(S`)−1d 1/2 ii d 1/2 jj\n(n1/2δD(S`) −1/2 d 1/2 ii )(n 1/2 δD(S`) −1/2 d 1/2 jj )\n= 1 .\nThus, points from the same cluster lie on the same ray from the origin. It follows that there are k mutually orthogonal unit vectors, z1, . . . , zk such that xi • = √ ndiiδD(S`)−1zT` for each i ∈ S`.\nWe will make use of the close connection between the eigenvector structure of Lrw and Lsym in order to characterize the Laplacian embedding structure of Lrw. The following fact can be found in the tutorial [19, Proposition 3].\nFact 11. (λ,u) is an eigenvalue-eigenvector pair of Lrw if and only if (λ,D 1/2u) is an eigenvalueeigenvector pair for Lsym.\nBy using Fact 11 and Proposition 10, we obtain the embedding structure for Lrw.\nProposition 12. Let the similarity graph G = (V,A) contain k connected components with indices in the sets S1, . . . ,Sk, let n = |V |, and let Lrw be well defined for G. Then, N (Lrw) has dimensionality k. Let X = (x •1, . . . , x •k) contain k scaled, orthogonal column vectors forming a basis of N (Lrw) such that ‖x • j‖ = √ n for each j ∈ [k]. Then, there exist weights w1, . . . , wk with wj = |Sj | n and mutually orthogonal vectors z1, . . . , zk ∈ R\nk such that whenever i ∈ Sj, the row vector xi • =\n1√ wj zTj .\nProof. By Fact 11, we may construct an orthogonal basis of N (Lrw) using a particular choice of orthogonal basis of N (Lsym). In particular, we define the vectors ySj the same as in the proof of Proposition 10, and we obtain that the vectors ỹSj := D −1/2ySj are 0-eigenvectors of Lrw. Using equation (10), we see that ySj = 1Sj . In particular, it follows that {|S1| −1/21S1 , . . . , |Sk|\n−1/21Sk} is an orthonormal basis of N (Lrw). From the discussion around equation (3), it follows that N (L) and N (Lrw) are the same space in this setting where G consists of k connected components. Our desired result thus follows from Proposition 7.\nWe note that the Propositions 7, 10, and 12 are closely. From Propositions 7 and 12, we see that L and Lrw give rise to the same embedding structure when G consists of k connected components. Further, we may place the embedding structure for L (or equivalently Lrw) into the notation used for describing the ray structure of Lsym. In particular, if we let D = I, we see that δI(Sj) = |Sj |. Recalling that wj = |Sj |n , we see (by replacing D with I in equation (8)) that √ nIiiδI(Sj)−1zTj = 1√wj z T j , which is the required replacement to recreate the statements of Proposition 7 and Proposition 12. In particular, we may create a generalized notion of a graph embedding which captures all of the Laplacian embeddings.\nDefinition 13. Let G be a similarity graph consisting of n vertices and k connected components such indices partitioned into sets S1, . . . ,Sk corresponding to the connected components. Let D = diag(d11, . . . , dnn) be a positive definite matrix. Let ϕ be a map which takes the i\nth vertex of G to a point xi ∈ Rk. If there exists an orthonormal basis z1, . . . , zk of Rk such that xi = √ ndiiδD(Sj)−1zj for each i ∈ Sj , then we call ϕ a (G, D)-orthogonal embedding. We see by Proposition 10, the Laplacian embedding induced by Lsym is a (G,D)-orthogonal embedding; and by Propositions 7 and 12, the Laplacian embedding induced by L and Lrw are (G, I)-orthogonal embedding."
    }, {
      "heading" : "7.2 Maxima Structure of the Resulting BEFs",
      "text" : "In this section, we demonstrate that by performing function maximization over the directional projections of embedded data arising from any of the Laplacian embeddings, we are able to recover the desired clusters for spectral clustering. We will make use of the following construction.\nConstruction 14. Let G be a similarity graph consisting of n vertices and k connected components with indices partitioned into the sets S1, . . . ,Sk. We suppose that D = diag(d11, . . . , dnn) is a positive definite matrix. We suppose that x1, . . . ,xn is a (G,D)-orthogonal embedding of the vertices of G such that xi = √ ndiiδD(Sj)−1zj for each i in Sj. Parallel to the text of section 5, we construct a function Fg : Sk−1 → R from a continuous contrast function g : [0,∞) → R where it is assumed that t 7→ g( √ t) is strictly convex (cf. Assumption 3). We construct Fg as\nFg(u) := 1\nn n∑ i=1 g(|u • xi|)\n= 1\nn k∑ j=1 ∑ i∈Sj g (‖xi‖ · |u • zj |) . (12)\nFirst, we make a couple of comments about Construction 14. Using the discussion at the end of section 7.1, when D = I the embedded points xi can be obtained from the rows of X in Proposition 7, and they thus correspond to the embedded points arising from L. For this choice of D = I, Construction 14 is thus a strict generalization of Construction 8. However, Construction 14 also captures Lrw (with D = I by Proposition 12) and Lsym (with D = D by Proposition 10).\nWe now wish to generalize Theorem 9 by showing that the local maxima of Fg from Construction 14 are precisely the directions ±z1, . . . ,±zk. We will first argue that Fg has no extraneous maxima, and then that the direction ±z1, . . . ,±zk actually are maxima. To see that Fg has no extraneous local maxima, we need only demonstrate that Fg is an orthogonal BEF satisfying Assumption 3 and apply Theorem 4.\nLemma 15. Let Fg and z1, . . . , zk be as in Construction 14. Then, the local maxima of Fg is contained in the set {±zi | i ∈ [k]}.\nProof. Define gi : R→ R by gi(t) := 1n ∑ j∈Si g(‖xj‖ · |t|). Using equation (12), we obtain\nFg(u) = 1\nn k∑ i=1 ∑ j∈Si g (‖xj‖ · |u • zi|) = k∑ i=1 gi(u • zi) .\nSince t 7→ g( √ t) is strictly convex, it follows that t 7→ gi(sign(t) √ |t|) is strictly convex for all i ∈ [k]. By Theorem 4, the local maxima of Fg are contained in {±zi : i ∈ [k]}.\nWhat remains to be seen is that the directions {±zi | i ∈ [k]} are local maxima of Fg . For notational simplicity, we identify z1, . . . , zk with the canonical directions e1, . . . , ek in an unknown coordinate system so that ui is shorthand for u • ei. In our proofs, we exploit the convexity structure induced by the change of variable introduced in the proof of Theorem 4, namely ψ defined by ψi(u) := u 2 i which maps the domain Sk−1 onto the simplex ∆k−1 := conv(e1, . . . , ek).\nLemma 16. Let x1, . . . ,xn be as in Construction 14 with the added assumption that zi = ei for each i ∈ [k]. Let h : [0,∞) → R be a strictly convex function. Let H : ∆k−1 → R be given by H(u) = 1n ∑k i=1 ∑ j∈Si h(ui‖xj‖\n2). Then the set {ei | i ∈ [k]} is contained in the set of strict local maxima of H.\nProof. By the symmetries of H, it suffices to show that e1 is a strict local maximum of H. To see this, choose u 6= e1 from a neighborhood of e1 relative to ∆k−1 to be specified later. Let Λu = {i | i ∈ [k] \\ {1}, ui 6= 0}. Then,\nH(e1)−H(u)\n= 1\nn ∑ j∈S1 h(‖xj‖2) + k∑ i=2 ∑ j∈Si h(0)− k∑ i=1 ∑ j∈Si h(ui‖xj‖2)  = 1\nn ∑ j∈S1 ( h(‖xj‖2)− h(u1‖x2j‖) )\n− k∑\ni=2 ∑ j∈Si ( h(ui‖xj‖2)− h(0) ) = 1\nn ∑ j∈S1 ‖xj‖2(1− u1) h(‖xj‖2)− h(u1‖x2j‖) ‖xj‖2(1− u1)\n− ∑ i∈Λu ∑ j∈Si ui‖xj‖2 h(ui‖xj‖2)− h(0) ui‖xj‖2  . We have written H(e1) −H(u) as a weighted sum of difference quotients (slopes). We would like to apply Lemma 20 in order to demonstrate that there is a neighborhood B of e1 relative to ∆ k−1 such that u ∈ B \\ {e1} implies H(e1)−H(u) > 0. First, we notice that for each xj , u breaks the interval into left and right pieces, yielding two slopes of interest:\nm`ij(u) = h(ui‖xj‖2)− h(0)\nui‖xj‖2\nand\nmrij(u) = h(‖xj‖2)− h(ui‖xj‖2) ‖xj‖2(1− ui) .\nThus,\nH(e1)−H(u) = 1\nn ∑ j∈S1 ‖xj‖2(1− u1)mr1j(u)\n− ∑ i∈Λu ∑ j∈Si ui‖xj‖2m`ij(u)  . Let B = {u | ui < minj‖xj‖ 2\nmaxj‖xj‖2 for all i 6= 1} \\ {e1}. Then, fixing u ∈ B and i 6= 1, we have that ui‖xj1‖2 < ‖xj2‖2 for any j1 ∈ Si and j2 ∈ S1. Let m`max(u) := max{m`ij(u) | i ∈ Λu, j ∈ Si} and mrmin(u) := min{mr1j(u) | j ∈ S1}. From Lemma 20, it follows that m`max(u) < mrmin(u) for all\nu ∈ B. Thus,\nH(e1)−H(u) ≥ 1\nn ∑ j∈S1 ‖xj‖2(1− u1)mrmin(u)\n− ∑ i∈Λu ∑ j∈Si ui‖xj‖2m`max(u)  = (1− u1)mrmin(u)−\nk∑ i=2 uim ` max(u)\n= (1− u1)[mrmin(u)−m`min(u)] > 0\nwhere the first equality uses that ∑\nj∈Si‖xj‖ 2 = n (∑ j∈Si djj ) δD(Sj) = n for all j ∈ [k]. It follows\nthat e1 is a local maximum of H.\nTheorem 17. In Construction 14, {±zi | i ∈ [k]} is a complete enumeration of the local maxima of Fg.\nProof. Let Λ denote the set of local maxima of Fg . That Λ ⊂ {±zi | i ∈ [k]} is immediate from Lemma 15. To see that Λ ⊃ {±zi : i ∈ [k]}, we note that there is a natural mapping between ∆k−1 and a quadrant of Sk−1.\nThe set {±zi | i ∈ [k]} gives an unknown, orthonormal basis of our space. We may without loss of generality work in the coordinate system where e1, . . . , ek coincide with z1, . . . , zk. Let Q1 = Sk−1 ∩ [0,∞)k−1 give the first quadrant of the unit sphere. By the symmetries of the problem, it suffices to show that {e1, . . . , ek} are maxima of Fg . However, the map ψ : Q1 → ∆k−1 defined by (ψ(u))i = u 2 i is a homeomorphism. Defining H : ∆\nk−1 → R by H(t) = Fg(ψ−1(t)), then t ∈ ∆k−1 is a local maximum of H if and only if ψ−1(t) is a local maximum of Fg relative to Q1.\nNote that H(t) = 1n ∑k i=1 ∑ j∈Si g( √ ti‖xj‖2). As y 7→ g( √ y) is convex, it follows by Lemma 16\nthat {ei}ki=1 are local maxima of H. Hence, using the symmetries of Fg , {±zi | i ∈ [k]} ⊃ Λ.\nWith Theorem 17 in hand, it is now straight forward to generalize Theorem 9 to demonstrate that the spectral embedding arising from any of the graph Laplacians is compatible with the proposed BEF function maximization framework for clustering within the embedded space.\nTheorem 18. Suppose that G is a graph consisting of n vertices and k connected components with indices in the sets S1, . . . ,Sk. Let L be a (well defined) graph Laplacian chosen among L, Lrw, or Lsym constructed from G. If X ∈ Rn×k is such that its columns x • i form an orthogonal subspace of N (L) scaled such that ‖x • i‖ = √ n, then there exists an orthonormal basis z1, . . . , zk of Rk such that\n1. For each j ∈ Si, xTj • lies on the ray starting at the origin and going through zi.\n2. If we define Fg : Sk−1 → R by Fg(u) = 1n ∑n\ni=1 g(u • xi • ) from a contrast g : [0,∞) → R satisfying that t 7→ g( √ t) is strictly convex, then the directions {±zi | i ∈ [k]} provide a complete enumeration of the local maxima of Fg on Sk−1.\nProof. Part 1 follows from the combination of Propositions 7, 10, and 12. Part 2 follows from Theorem 17 along with the observation that Construction 14 captures the given Fg irregardless of which of the 3 graph Laplacians is used to construct Fg (see Construction 14 and the surrounding discussion)."
    }, {
      "heading" : "A Facts About Convex Functions",
      "text" : "In this section, intervals can be open, half open, or closed. There is a large literature studying the properties of convex functions. As strict convexity is considered more special than convexity, results are typically stated in terms of convex functions. The following characterization of strict convexity is a version of Proposition 1.1.4 of [8] for strictly convex functions, and can be proven in a similar fashion.\nLemma 19. For an interval I, let f : I → R be a strictly convex function. Then, fixing any x0 ∈ I, the slope function defined by m(x) := f(x)−f(x0)x−x0 is strictly increasing on I \\ {x0}.\nThe following result is largely a consequence of Lemma 19.\nLemma 20. Let I be an interval and let f : I → R be a convex function. Suppose that (a, b) ⊂ I and (c, d) ⊂ I are such that a ≤ c and b ≤ d with at least one of the inequalities being strict. Then,\nf(b)− f(a) b− a < f(d)− f(c) d− c\nProof. If c = a, then f(d)−f(a)d−a = f(d)−f(c) d−c trivially. Otherwise, a < c, and by Lemma 19, we have that f(d)−f(a)d−a < f(d)−f(c) d−c By similar reasoning, f(b)−f(a) b−a ≤ f(d)−f(a) d−a (with equality if and only if d = b). As by assumption, a = b and c = d cannot both hold, it follows that f(b)−f(a)b−a ≤ f(d)−f(a)\nd−a ≤ f(d)−f(c)\nd−c with at least one of the inequalities being strict.\nThe following result comes from Remark 4.2.2 of Hiriart-Urruty and Lemaréchal [8].\nLemma 21. Given an interval I and a function f : I → R, then the left derivative ∂−f is leftcontinuous and the right derivative ∂+f is right-continuous respectively whenever they are defined (that is, finite)."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work was supported by NSF grants IIS 1117707, CCF 1350870, and CCF 1422830."
    } ],
    "references" : [ {
      "title" : "Efficient learning of simplices",
      "author" : [ "J. Anderson", "N. Goyal", "L. Rademacher" ],
      "venue" : "COLT, pages 1020–1045,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Learning spectral clustering, with application to speech separation",
      "author" : [ "F.R. Bach", "M.I. Jordan" ],
      "venue" : "Journal of Machine Learning Research, 7:1963–2001,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Laplacian eigenmaps for dimensionality reduction and data representation",
      "author" : [ "M. Belkin", "P. Niyogi" ],
      "venue" : "Neural Comput., 15(6):1373–1396,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "The rotation of eigenvectors by a perturbation",
      "author" : [ "C. Davis", "W.M. Kahan" ],
      "venue" : "iii. SIAM Journal on Numerical Analysis, 7(1):1–46,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1970
    }, {
      "title" : "Identification of almost invariant aggregates in reversible nearly uncoupled markov chains",
      "author" : [ "P. Deuflhard", "W. Huisinga", "A. Fischer", "C. Schütte" ],
      "venue" : "Linear Algebra and its Applications, 315(1):39–59,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "On the 0–1-maximization of positive definite quadratic forms",
      "author" : [ "P. Gritzmann", "V. Klee" ],
      "venue" : "Operations Research Proceedings 1988, pages 222–227. Springer,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Convex Analysis and Minimization Algorithms: Part 1: Fundamentals, volume 1",
      "author" : [ "J.-B. Hiriart-Urruty", "C. Lemaréchal" ],
      "venue" : "Springer,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Learning mixtures of spherical Gaussians: Moment methods and spectral decompositions",
      "author" : [ "D. Hsu", "S.M. Kakade" ],
      "venue" : "Proceedings of the 4th conference on Innovations in Theoretical Computer Science (ITCS), pages 11–20. ACM,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Independent component analysis, volume 46",
      "author" : [ "A. Hyvärinen", "J. Karhunen", "E. Oja" ],
      "venue" : "John Wiley & Sons,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Algorithms for clustering data",
      "author" : [ "A.K. Jain", "R.C. Dubes" ],
      "venue" : "Prentice-Hall, Inc., Upper Saddle River, NJ, USA,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "Spectral clustering as mapping to a simplex",
      "author" : [ "P. Kumar", "N. Narasimhan", "B. Ravindran" ],
      "venue" : "2013 ICML workshop on Spectral Learning,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics",
      "author" : [ "D.R. Martin", "C. Fowlkes", "D. Tal", "J. Malik" ],
      "venue" : "ICCV, pages 416–425,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "A random walks view of spectral segmentation",
      "author" : [ "M. Meilă", "J. Shi" ],
      "venue" : "AI and Statistics (AISTATS),",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "On spectral clustering: Analysis and an algorithm",
      "author" : [ "A.Y. Ng", "M.I. Jordan", "Y. Weiss" ],
      "venue" : "Advances in neural information processing systems, 2:849–856,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Convex analysis",
      "author" : [ "R.T. Rockafellar" ],
      "venue" : "Princeton Landmarks in Mathematics. Princeton University Press, Princeton, NJ,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Normalized cuts and image segmentation",
      "author" : [ "J. Shi", "J. Malik" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence, 22(8):888–905,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "A comparison of spectral clustering algorithms",
      "author" : [ "D. Verma", "M. Meilă" ],
      "venue" : "Technical report, University of Washington CSE Department, Seattle, WA 98195-2350, 2003. doi=10.1.1.57.6424, Accessed online via CiteSeerx 5 Mar",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A tutorial on spectral clustering",
      "author" : [ "U. Von Luxburg" ],
      "venue" : "Statistics and computing, 17(4):395–416,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "The hidden convexity of spectral clustering",
      "author" : [ "J. Voss", "M. Belkin", "L. Rademacher" ],
      "venue" : "Thirtieth AAAI Conference on Artificial Intelligence, pages 2108–2114,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Perron cluster analysis and its connection to graph partitioning for noisy data",
      "author" : [ "M. Weber", "W. Rungsarityotin", "A. Schliep" ],
      "venue" : "Konrad-Zuse-Zentrum für Informationstechnik Berlin,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "A study of the fixed points and spurious solutions of the deflation-based fastica algorithm",
      "author" : [ "T. Wei" ],
      "venue" : "Neural Computing and Applications, pages 1–12,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Model-based clustering and data transformations for gene expression data supplementary web site",
      "author" : [ "K.Y. Yeung", "C. Fraley", "A. Murua", "A.E. Raftery", "W.L. Ruzzo" ],
      "venue" : "http://faculty. washington.edu/kayee/model/, 2001. Accessed: 20 Jan",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Model-based clustering and data transformations for gene expression data",
      "author" : [ "K.Y. Yeung", "C. Fraley", "A. Murua", "A.E. Raftery", "W.L. Ruzzo" ],
      "venue" : "Bioinformatics, 17(10):977–987,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Multiclass spectral clustering",
      "author" : [ "S.X. Yu", "J. Shi" ],
      "venue" : "Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on (ICCV), pages 313–319,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "There is an extensive literature on the subject, including a number of different methodologies as well as their various practical and theoretical aspects [11].",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 15,
      "context" : "While hierarchical methods based on a sequence of binary splits have been used, the most common approaches use k-means or weighted k-means clustering in the spectral space or related iterative procedures [17, 15, 2, 25].",
      "startOffset" : 204,
      "endOffset" : 219
    }, {
      "referenceID" : 13,
      "context" : "While hierarchical methods based on a sequence of binary splits have been used, the most common approaches use k-means or weighted k-means clustering in the spectral space or related iterative procedures [17, 15, 2, 25].",
      "startOffset" : 204,
      "endOffset" : 219
    }, {
      "referenceID" : 1,
      "context" : "While hierarchical methods based on a sequence of binary splits have been used, the most common approaches use k-means or weighted k-means clustering in the spectral space or related iterative procedures [17, 15, 2, 25].",
      "startOffset" : 204,
      "endOffset" : 219
    }, {
      "referenceID" : 23,
      "context" : "While hierarchical methods based on a sequence of binary splits have been used, the most common approaches use k-means or weighted k-means clustering in the spectral space or related iterative procedures [17, 15, 2, 25].",
      "startOffset" : 204,
      "endOffset" : 219
    }, {
      "referenceID" : 18,
      "context" : "Typical algorithms for multiway spectral clustering follow a two-step process: ∗A short version of this paper previously appeared in the proceedings of the Thirtieth AAAI Conference on Artificial Intelligence [20].",
      "startOffset" : 209,
      "endOffset" : 213
    }, {
      "referenceID" : 17,
      "context" : "The meaning can be explained by spectral graph theory as relaxations of multiway cut problems [19].",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 2,
      "context" : "There are also connections to other areas of machine learning and mathematics, in particular to the geometry of the underlying space from which the data is sampled [4].",
      "startOffset" : 164,
      "endOffset" : 167
    }, {
      "referenceID" : 19,
      "context" : "This view of spectral clustering as basis recovery is related to previous observations that the spectral embedding generates a discrete weighted simplex (see [21, 12] for some applications).",
      "startOffset" : 158,
      "endOffset" : 166
    }, {
      "referenceID" : 10,
      "context" : "This view of spectral clustering as basis recovery is related to previous observations that the spectral embedding generates a discrete weighted simplex (see [21, 12] for some applications).",
      "startOffset" : 158,
      "endOffset" : 166
    }, {
      "referenceID" : 8,
      "context" : "The proposed approach relies on an optimization problem resembling certain Independent Component Analysis techniques, such as FastICA (see [10] for a broad overview).",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 0,
      "context" : "Interestingly, while maximizing a convex function over a convex domain is generally difficult (even maximizing a positive definite quadratic form over the continuous cube [0, 1]n is NP-hard2), our setting allows for efficient optimization.",
      "startOffset" : 171,
      "endOffset" : 177
    }, {
      "referenceID" : 0,
      "context" : "[1] use the method of moments to recover a continuous simplex given samples from the uniform probability distribution.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "Also, one of the results of Hsu and Kakade [9] shows recovery of parameters in a Gaussian Mixture Model using the moments of order three, and this result can be thought of as a case of the basis recovery problem.",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 20,
      "context" : "In particular, typical versions of FastICA are known to have spurious maxima [22].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 5,
      "context" : "This follows from [7] together with Fact 6 below.",
      "startOffset" : 18,
      "endOffset" : 21
    }, {
      "referenceID" : 17,
      "context" : "The following well known property of the graph Laplacian (see [19] for a review) helps shed light on its importance: Given u ∈ Rn, uLu = 1 2 ∑",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 19,
      "context" : "We formulate this result as follows (see [21], [18, Proposition 5], and [15, Proposition 1] for related statements).",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 3,
      "context" : "Using the perturbation theory of symmetric matrices, it can be shown that when the perturbation is not too large, the structure of X is approximately maintained (see [5, 19]).",
      "startOffset" : 166,
      "endOffset" : 173
    }, {
      "referenceID" : 17,
      "context" : "Using the perturbation theory of symmetric matrices, it can be shown that when the perturbation is not too large, the structure of X is approximately maintained (see [5, 19]).",
      "startOffset" : 166,
      "endOffset" : 173
    }, {
      "referenceID" : 23,
      "context" : "Further, spectral clustering with Lsym has a nice interpretation as a relaxation of the NP-hard multi-way normalized graph cut problem [25], and the use of Lrw has connections to the theory of Markov chains [6, 14].",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 4,
      "context" : "Further, spectral clustering with Lsym has a nice interpretation as a relaxation of the NP-hard multi-way normalized graph cut problem [25], and the use of Lrw has connections to the theory of Markov chains [6, 14].",
      "startOffset" : 207,
      "endOffset" : 214
    }, {
      "referenceID" : 12,
      "context" : "Further, spectral clustering with Lsym has a nice interpretation as a relaxation of the NP-hard multi-way normalized graph cut problem [25], and the use of Lrw has connections to the theory of Markov chains [6, 14].",
      "startOffset" : 207,
      "endOffset" : 214
    }, {
      "referenceID" : 15,
      "context" : "Spectral clustering was first applied to image segmentation by Shi and Malik [17], and it has remained a popular application of spectral clustering.",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 11,
      "context" : "Figure 2 and Figure 3 show several segmentations produced by HBRopt-gabs and spherical k-means on several example images from the BSDS300 test set [13].",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 22,
      "context" : "We also use the standardized gene expression data set [24, 23], which is also referred to as cell cycle.",
      "startOffset" : 54,
      "endOffset" : 62
    }, {
      "referenceID" : 21,
      "context" : "We also use the standardized gene expression data set [24, 23], which is also referred to as cell cycle.",
      "startOffset" : 54,
      "endOffset" : 62
    }, {
      "referenceID" : 13,
      "context" : "[15]).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 6,
      "context" : "4 of [8] for strictly convex functions, and can be proven in a similar fashion.",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 6,
      "context" : "2 of Hiriart-Urruty and Lemaréchal [8].",
      "startOffset" : 35,
      "endOffset" : 38
    } ],
    "year" : 2016,
    "abstractText" : "In recent years, spectral clustering has become a standard method for data analysis used in a broad range of applications. In this paper we propose a new class of algorithms for multiway spectral clustering based on optimization of a certain “contrast function” over the unit sphere. These algorithms, partly inspired by certain Independent Component Analysis techniques, are simple, easy to implement and efficient. Geometrically, the proposed algorithms can be interpreted as hidden basis recovery by means of function optimization. We give a complete characterization of the contrast functions admissible for provable basis recovery. We show how these conditions can be interpreted as a “hidden convexity” of our optimization problem on the sphere; interestingly, we use efficient convex maximization rather than the more common convex minimization. We also show encouraging experimental results on real and simulated data. keywords: spectral clustering, convex maximization, basis recovery",
    "creator" : "LaTeX with hyperref package"
  }
}