{"conference": "ICLR 2017 conference submission", "title": "Lossy Image Compression with Compressive Autoencoders", "abstract": "We propose a new approach to the problem of optimizing autoencoders for lossy image compression. New media formats, changing hardware technology, as well as diverse requirements and content types create a need for compression algorithms which are more flexible than existing codecs. Autoencoders have the potential to address this need, but are difficult to optimize directly due to the inherent non-differentiabilty of the compression loss. We here show that minimal changes to the loss are sufficient to train deep autoencoders competitive with JPEG 2000 and outperforming recently proposed approaches based on RNNs. Our network is furthermore computationally efficient thanks to a sub-pixel architecture, which makes it suitable for high-resolution images. This is in contrast to previous work on autoencoders for compression using coarser approximations, shallower architectures, computationally expensive methods, or focusing on small images.", "reviews": [{"is_meta_review": true, "comments": "The paper proposes a neural approach to learning an image compression-decompression scheme as an auto-encoder. While the idea is certainly interesting and well-motivated, in practice, it turns out to achieve effectively identical rates to JPEG-2000.\n\nNow, as the authors argue, there is some value to the fact that this scheme was learned automatically rather than by expert design---which means it has benefits beyond the compression of natural images (e.g., it could be used to automatically learning a compression scheme for signals for which we don't have as much domain knowledge). However, I still believe that this makes the paper unsuitable for publication in its current form because of the following reasons---\n\n1. Firstly, the fact that the learned encoder is competitive---and not clearly better---than JPEG 2000 means that the focus of the paper should more be about the aspects in which the encoder is similar to, and the aspects in which it differs, from JPEG 2000. Is it learning similar filters or completely different ones ? For what kinds of textures does it do better and for what kinds does it do worse (the paper could show the best and worst 10 patches at different bit-rates) ?\n\n2. Secondly, I think it's crucial that the paper demonstrate that the benefits come from a better coding scheme (as opposed to just a better decoder), as suggested in my initial pre-review question. How would a decoder trained on JPEG-2000 codes (and perhaps also on encoded random projections) do worse or better ?\n\n3. Finally, I think the fact that it does as well/worse than JPEG-2000 significantly diminishes the case for using a 'deep' auto-encoder. JPEG-2000 essentially uses a wavelet transform, which is a basis that past studies have shown could be recovered using a simple sparse dictionary algorithm like K-SVD. This is why I feel that the method needs to clearly outperform JPEG-2000, or show comparisons to (or atleast discuss) a well-crafted traditional/generative model-based baseline.", "IS_META_REVIEW": true}, {"TITLE": "ICLR committee final decision", "OTHER_KEYS": "ICLR 2017 pcs", "comments": "This paper optimizes autoencoders for lossy image compression. Minimal adaptation of the loss makes autoencoders competitive with JPEG2000 and computationally efficient, while the generalizability of trainable autoencoders offers the added promise of adaptation to new domains without domain knowledge.\n The paper is very clear and the authors have tried to give additional results to facilitate replication. The results are impressive. While another ICLR submission that is in the same space does outperform JPEG2000, this contributions nevertheless also offers state-of-the-art performance and should be of interest to many ICLR attendees.", "IS_META_REVIEW": false, "DATE": "06 Feb 2017", "is_meta_review": false}, {"TITLE": "Revision", "OTHER_KEYS": "Lucas Theis", "comments": "Dear reviewers, we made the following changes to our paper:\n\n\u2013 added direct comparison with VAE/quantization approximation of Balle et al. (Figure 10)\n\u2013\u00a0added another control to Figure 3 (incremental training vs fixed small learning rate)\n\u2013\u00a0added a motivation and reference for MOS tests\n\u2013\u00a0added more detail to appendix to make reimplementation easier\n\u2013 improved caption of Figure 3A\n\nMinor:\n\u2013\u00a0added number of scales used in Gaussian scale mixtures\n\u2013\u00a0fixed reference to Figure 4 (which pointed to Figure 9 before)\n\nToderici et al. kindly provided us with their results which include entropy encoding*. We will rerun MOS experiments and intend to update quantitative comparison figures before the meeting.\n\n* ", "IS_META_REVIEW": false, "DATE": "20 Jan 2017", "is_meta_review": false}, {"TITLE": "Loss function", "OTHER_KEYS": "(anonymous)", "comments": "Thank you for the important work. I'm trying to reproduce the results.\n\nThe loss function in eq 2. scales the distortion by a beta hyper parameter. In your experiment you mention training three variants, high bit-rate: beta=0.01, medium: 0.05 and low: 0.2\n\nSo, the high bit-rate variant weights the distortion the least? That does not make sense to me. Did you mean 1/0.01, 1/0.05 and 1/0.2 or something else?\n\nAlso, figure 3 B shows the loss on the y-axis. This goes from 0.01 to 0.03. How can the loss be so low if it includes -logQ, which from figure 4 seems to be around 0.3 to 2.0 (bpp), if I'm interpreting this right.\n\nAlso, you mention you use mean squared error, but not on which scale. Do you calculate mse on the normalized images or on the input/output uint8 images?", "IS_META_REVIEW": false, "DATE": "10 Jan 2017", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Final Review", "is_meta_review": false, "comments": "This work proposes a new approach for image compression using auto encoders. The results are impressive, besting the state of the art in this field.\n\nPros:\n+ Very clear paper. It should be possible to replicate these results should one be inclined to do so.\n+ The results, when compared to other work in this field are very promising. I need to emphasize, and I think the authors should have emphasized this fact as well: this is very new technology and it should not be surprising it's not better than the state of the art in image compression. It's definitely better than other neural network approaches to compression, though.\n\nCons:\n- The training procedure seems clunky. It requires multiple training stages, freezing weights, etc.\n- The motivation behind Figure 1 is a bit strange, as it's not clear what it's trying to illustrate, and may confuse readers (it talks about effects on JPEG, but the paper discusses a neural network architecture, not DCT quantization)", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Review", "is_meta_review": false, "comments": "This paper proposes an autoencoder approach to lossy image compression by minimizing the weighted sum of reconstruction error and code length. The architecture consists of a convolutional encoder and a sub-pixel convolutional decoder. Experiments compare PSNR, SSIM, and MS-SSIM performance against JPEG, JPEG-2000, and a recent RNN-based compression approach. A mean opinion score test was also conducted.\n\nPros:\n+ The paper is clear and well-written.\n+ The decoder architecture takes advantage of recent advances in convolutional approaches to image super-resolution.\n+ The proposed approaches to quantization and rate estimation are sensible and well-justified.\n\nCons:\n- The experimental baselines do not appear to be entirely complete.\n\nThe task of using autoencoders to perform compression is important and has a large practical impact. Though directly optimizing the rate-distortion tradeoff is not an entirely novel enterprise, there are enough differences (e.g. the quantization approach and sub-pixel convolutional decoder) to sufficiently distinguish this from earlier work. I am not an image compression expert but the approach and results both seem compelling. The main shortcoming is that the implementation of Toderici et al. 2016b appears to be incomplete, and there is no comparison to Balle et al. 2016. Overall, I feel that the fact that this architecture achieves competitive performance with JPEG-2000 while simultaneously setting the stage for future work that varies the encoder/decoder size and data domain means the community will find this work to be of significant interest.\n\nI have no further specific comments at this time as they were answered sufficiently in the pre-review questions.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Official Review", "is_meta_review": false, "comments": "The paper proposes a neural approach to learning an image compression-decompression scheme as an auto-encoder. While the idea is certainly interesting and well-motivated, in practice, it turns out to achieve effectively identical rates to JPEG-2000.\n\nNow, as the authors argue, there is some value to the fact that this scheme was learned automatically rather than by expert design---which means it has benefits beyond the compression of natural images (e.g., it could be used to automatically learning a compression scheme for signals for which we don't have as much domain knowledge). However, I still believe that this makes the paper unsuitable for publication in its current form because of the following reasons---\n\n1. Firstly, the fact that the learned encoder is competitive---and not clearly better---than JPEG 2000 means that the focus of the paper should more be about the aspects in which the encoder is similar to, and the aspects in which it differs, from JPEG 2000. Is it learning similar filters or completely different ones ? For what kinds of textures does it do better and for what kinds does it do worse (the paper could show the best and worst 10 patches at different bit-rates) ?\n\n2. Secondly, I think it's crucial that the paper demonstrate that the benefits come from a better coding scheme (as opposed to just a better decoder), as suggested in my initial pre-review question. How would a decoder trained on JPEG-2000 codes (and perhaps also on encoded random projections) do worse or better ?\n\n3. Finally, I think the fact that it does as well/worse than JPEG-2000 significantly diminishes the case for using a 'deep' auto-encoder. JPEG-2000 essentially uses a wavelet transform, which is a basis that past studies have shown could be recovered using a simple sparse dictionary algorithm like K-SVD. This is why I feel that the method needs to clearly outperform JPEG-2000, or show comparisons to (or atleast discuss) a well-crafted traditional/generative model-based baseline.", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Regarding eq 8", "OTHER_KEYS": "(anonymous)", "comments": "Thank you for this important and inspiring work.\n\nI have some questions regarding the bound in eq. 8.\n\nMy understanding:\n\nThe bound in eq. 8 is used in the loss function as a proxy for the non-differentiable entropy of the codes. If the bound is minimized, a bound on the entropy is minimized.\n\nThe bound in eq. 8 comes directly from the equality in eq. 7, by applying Jensen's inequality. So, the bound only holds if the equality in eq. 7 holds.\n\nHowever, q(z+u) is a parametric approximation (the GSM model) of Q(z), not an equality.\n\nDoes the bound still hold? Under which circumstances?\n\n(I have a feeling that the missing term on the RHS of eq. 8 to make the bound an equality is a KL term between int q(z+u) du and Q(z), in which case the bound still holds as the KL is positive, but I have not been able to show it.)\n", "IS_META_REVIEW": false, "DATE": "14 Dec 2016", "is_meta_review": false}, {"TITLE": "Some questions regarding the paper", "OTHER_KEYS": "(anonymous)", "comments": "Great and very interesting work. I have some questions.\n\n1. It seems that the introduced scale parameters for different bit-rates are also trainable. How do you train and update the scale parameters for different bit-rates? Could you explain it in more details? Also, could you elaborate more about Figure 3A? What does the x-axis of the graph means? \n\n2. What was the intuition for incremental training? Do you have some insights why incremental manner is beneficial for training? Also, for figure 3B, it seems that the loss are not quite stable before 100 thousand iterations but suddenly the loss become very stable. Is this due to decreased learning rate at that point? \n\n3. Regarding the GSM modeling of quantized coefficients, how many mixtures did you use and how did you train GSM (e.g., EM algorithm) ? Also, isn't it a burden to train GSM to fit the quantized coefficients at every iteration? \n\nThanks.", "IS_META_REVIEW": false, "DATE": "14 Dec 2016", "is_meta_review": false}, {"TITLE": "Some more questions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "IS_META_REVIEW": false, "DATE": "04 Dec 2016", "is_meta_review": false}, {"TITLE": "Vs JPEG2000", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "IS_META_REVIEW": false, "DATE": "03 Dec 2016", "is_meta_review": false}, {"TITLE": "Pre-review questions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "is_meta_review": false}], "SCORE": 5, "authors": "Lucas Theis, Wenzhe Shi, Andrew Cunningham, Ferenc Husz\u00e1r", "KEYWORDS": "A simple approach to train autoencoders to compress images as well or better than JPEG 2000.", "accepted": true, "id": ""}
