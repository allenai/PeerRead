{
  "name" : "1512.06034.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Ontology-driven Information Extraction",
    "authors" : [ "Weronika T. Adrian", "Nicola Leone", "Marco Manna" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: unstructured data, ontologies, semantic information extraction, table recognition, semantic views"
    }, {
      "heading" : "1 Introduction",
      "text" : "Context and Motivation. By its nature, the Web has been conceived as an enormous distributed source of information which behaves as an open system to facilitate data sharing. However, the concrete way how the Web has been populated gave rise to a large amount of knowledge which is accessible only to humans but not to computers. A large slice of this knowledge is destined to remain only human-readable. But there is another relevant portion of it which could be automatically manipulated to be processed by computers. This is the case, for example, of homogeneous unstructured data (HUD) which are collections of unstructured documents that share common properties, such as similar layout, common file format, or common domain of values, just to mention a few.\nBuilding on their common properties, it would be desirable to automatically process HUD to access the main information they contain through a semantic layer which is typically given in the form of an ontology, and that we call semantic view. The problem of identifying and extracting information from unstructured documents is widely studied in the field of information and knowledge management, and is referred to as Information Extraction (IE) [5,3,7,2]. However, most of the existing approaches to IE are mainly syntactic, and do not offer a uniform, clear, and semantic view of the relevant information.\nar X\niv :1\n51 2.\n06 03\n4v 1\n[ cs\n.A I]\n1 8\nD ec\n2 01\nContribution. To offer a semantic view of a collection of HUD (even if encoded as pdf files), we propose and implement a system, named KnowRex, which splits the entire process in two different phases, called design and runtime. During the first one, the designer (i) defines the target schema for the semantic view of the original data, (ii) fixes an object model to offer a structured representation of the documents, (iii) arranges a suite of annotation units (such as named entity extractors, natural language processing tools, and annotation tools based on thesauri or regular expressions) to define the “leaves” of the object model, (iv) chooses and calibrates one of the software programs that partition unstructured documents into two-dimensional grids, and (v) provides formal rules to structure the documents, and to construct the semantic view. During the runtime phase, the system processes the documents as prescribed in the design phase to instantiate the object model first, and then the target schema. In particular, the process that provides a structured representation of the documents can be thought as a kind of IE task which is heavily driven by domain knowledge and semantics, while the process that constructs the semantic view from the structured version of the documents takes care of reorganizing the extracted knowledge to facilitate data analysis. To sharpen our system, we considered curriculum vitae in the Europass style to offer a semantic view of them and be able, for example, to select those which exhibit required skills. The main contributions of the paper are: I We present an ontology-based approach to IE which allows for extracting semantically rich information from unstructured data sharing some common features. To this end, we integrate and extend recent technologies and results from the fields of classical information extraction, table recognition, ontologies, text annotation, and logic programming. I We design and implement a system, named KnowRex, which realizes our ontology-based approach to IE, and provides access to HUD via semantic views. I On the application side, we have successfully applied KnowRex to offer a semantic view to curriculum vitae in the Europass style.\nRelated Work. The literature of the academic and commercial worlds offers a variety of approaches and tools to IE that are either programmed manually, or learned semi-automatically. A main shortcoming of these approaches, however, is their lack of understanding of extracted information. More recently, some works have shown the promise of deducing and encoding formal knowledge in the form of ontologies [9,8,1,6]. These approaches use ontologies either to improve the extraction phase as a way to present the results of the extraction, or to allow matching different representations across sources. Our approach follows the line of combining several techniques to obtain comprehensive results [11,4]. To better recognize named entities, we have utilized StanfordNER (http:// nlp.stanford.edu/software/CRF-NER.shtml), although during the tests, we also evaluated Alchemy (http://www.alchemyapi.com), DBpedia Spotlight (http:// dbpedia.org/spotlight), Extractiv (http://extractiv.com), OpenCalais (http: //www.opencalais.com), and Lupedia (http://www.old.ontotext.com/lupedia). The notion of semantic descriptors introduced in Section 4 has been inherited from HiLeX [10]. However, we have refined and extended their shape."
    }, {
      "heading" : "2 System Overview",
      "text" : "KnowRex is a framework that allows to develop systems for Semantic Information Extraction (i.e., information extraction based on the meaning of data). In our approach, what drives the whole process is a semantic view of the input data. We start developing a new project with KnowRex by deciding what information we want to obtain in the end, and how we want to organize it. It is often possible to semi-formally model the organization of data within a document (e.g. the DOM model for (X)HTML an XML languages). In KnowRex, however, we take a step further and allow users to define the final semantic view that is independent from the initial structure. This approach is closer to practical use cases, in which specialists are asked to populate existing knowledge (or data) bases by extracting appropriate information from a collection of documents.\nWhen we consider homogeneous unstructured data, we can assume that input documents (in a given collection) share some specific features, hereafter called a “template”. Based on a template one can define an object model that will capture the sort of data contained within the documents and, to some extend, the way it is organized. If there exists a model for a template, then the information extraction from a set of documents complying to this template will populate the object model with instances from each input document. The other important part of the process is formulating a mapping from the object model to a target schema. Such a mapping allows to reorganize objects extracted from the documents and transform them into instances of the desired semantic view (see Figure 1).\nIt is possible to define more semantic views for the same collection of data (e.g. for different use cases). One can even imagine defining several object models for the same input and target schema. KnowRex framework ensures flexibility in these respects, by separating the stages of extraction and the mapping to target schema and enabling reuse of the components.\nWithin KnowRex, several tools and techniques have been used, namely:\n1. a bi-dimensional processor for recognizing structural elements of documents, 2. one- and bi-dimensional tokenizers for identifying basic elements of text, 3. annotators (third-party semantic annotators, natural language processing\ntools, pattern recognition tools etc.) that label single words or phrases as belonging to particular categories, 4. semantic descriptors that allow to build the object model from the objects obtained from structural and semantic analysis, and 5. logical rules that allow to formulate a mapping between knowledge representations (the object model and the target schema).\nDeployment of a new project with KnowRex is relatively easy and consists in adapting the core of the system and the external tools to work on specific data to obtain desired results. Development of a new project is divided into two phases: design and runtime. In the former, the designer works on a conceptual level defining the object model (by assuming a certain template) and the target schema, and by setting the tools and writing rules that will govern the data and information transformations. For the extraction step, the bi-dimensional\nprocessing tools, the annotators and the descriptors must be adapted, and for the mapping to target schema, logic rules must be defined. These design choices, described in details in Section 4, are materialized and applied in the runtime phase to extract information from the actual documents and populate semantic view(s) defined for them.\nKnowRex consists of a core system and a set of external tools (see Fig. 2). The Semantic Information Extraction is governed by three main components: Bi-Dimensional Unit, Annotation Unit and Language Unit, that are configured during the design phase, and during runtime are responsible for consecutive stages of document analysis, information extraction and processing.\nThe Bi-Dimensional Unit is responsible for a structural analysis of the input documents. The more information about the expected layout, structure or typical features of the input documents is given, the better representation will be obtained, and better quality of information extraction during later stages can be achieved. In the Annotation Unit, the system uses external tools to identify objects of certain classes within the document. The Language Unit is responsible for high-level extraction of information. The semantic descriptors used here work with the results of the annotation and bi-dimensional stages. They take as input information about the structure of the document, the objects identified by semantic annotators, their placement within the document, proximity to each other etc., and build more complex objects for the object model."
    }, {
      "heading" : "3 Automatic curriculum analysis",
      "text" : "For testing our framework, we selected the European standard style for Curriculum Vitae documents called Europass (see Figure 3). This choice ensures that the input documents have similar two-column layout and organization of data. Despite some differences between single Europass CV documents, they can be seen as HUD, and we can assume a certain template, which consists in: (i) two-column layout, (ii) same file format (actually a pdf that contains no information about sections/subsections which must be reconstructed at runtime), (iii) fixed set of labels (in the left column), (iv) common domain of values (personal information, education, work experience etc.).\nThe problem of recruiters and the goal of our system is to extract appropriate information from a collection of documents, and enter it into the target database. Some information can be localized by identifying appropriate sections and labels in the left column (e.g., name, surname, address etc.). Also, a part from the driving license, all the information needed for relation candidate are grouped together. For other information, it may be necessary to combine the knowledge about the structure of the input with the semantics of data. For instance, for work and education, it is possible to locate the institutions by analyzing the labels in the first column and extracting everything that follows them on the right. However, it would be beneficial to use also the semantic annotators that can recognize particular phrases as names of schools or companies to get more precise information. Finally, some information may be dispersed through the document. For instance, one may want to extract information about the candi-\n! !\"#$%&'()*&+%#,'-*%&\n\"#$%&!'()*+%,!-!./$'()*+%,! .\"#%&*/')01#*'&\n011$*%%+*%, 2#(!3#*&$4!5/66#\n7*8*9:4'*+%, ;<=!>?<!@AB!CD=E\nFG)(#8 HI(1$#('J)(&I/'#6(8I#&\nK(&#4'(8#&L 348#%:\nM*'1*$ \"*)(8*\n.%#/)\"23\"#*\"&4\"\nN(&*% >EI?EE=G9$*%*'&\nO66/9(&#4'!4$!94%#&#4'!:*81 P*%*($6:!Q!7*(6:#'R!0%%#%&('&\nS(#'!(6&#T#&#*%!('1!$*%94'%#U#8#&#*% P*%*($6:V!.*)('&#6!7*6:'484R#*%W!X4R#6!3$4R$())#'RW!Y'4H8*1R*!F'R#'**$#'RZ 7*(6:#'RV!['#\\-MK[-X#'/\\W!08R4$#&:)%!('1!N(&(!.&$/6&/$*%W!O9*$(&#'R!.L%&*)%W!.*)('&#6!]*U\nK()*!('1!(11$*%%!4^!*)984L*$ 0M_!['#T*$%#&L!4^!.6#*'6*!('1!7*6:'484RL\n7L9*!4^!U/%#'*%%!4$!%*6&4$ .6#*'6*!('1!*1/6(&#4'\nN(&*% DI?EEDG=I?EED\nO66/9(&#4'!4$!94%#&#4'!:*81 .4^&H($*!N*T*849*$\nS(#'!(6&#T#&#*%!('1!$*%94'%#U#8#&#*% ]*U!3$4R$())#'R!+:&)8W!6%%W!`(T(%6$#9&W!9:9W!(`(\\W!)L%a8,\nK()*!('1!(11$*%%!4^!*)984L*$ _48#1(Lb:*6c!0M\n7L9*!4^!U/%#'*%%!4$!%*6&4$ d'^4$)(&#4'!7*6:'484RL\n3(R*!>->!G!b/$$#6/8/)!T#&(*!4^! ./$'()*+%,!\"#$%&!'()*+%,! \"4$!)4$*!#'^4$)(&#4'!4'!F/$49(%%!R4!&4!:&&9V--*/$49(%%I6*1*^49I*/$49(I*/ e!F/$49*('!['#4'W!?EE@G?E>E!!!?@ED?E>E\nFig. 3. Fragment of a Curriculum Vitae PDF-document in the Europass style.\ndates’ skills, and this may be given in several ways. There exist dedicated sections in the Europass style, but these are not always filled by candidates. Thus, the skills may be also extracted from the other parts of the document. For instance, one may recognize “practical skills” as gained during the work experience, and “theoretical skills”, if they are listed within the education section."
    }, {
      "heading" : "4 The Design Phase in KnowRex",
      "text" : "During the design phase, a KnowRex project is configured to perform operations on a collection of HUD, to obtain information desired by user, in a particular form. To do it, the designer should reflect on what they have and what the want to obtain (see Figure 4). The former means identifying a template which a vague concept for describing the common features of HUD. A template must be formalized in the form of object model. Elements of it will be extracted by different components (two-dimensional processing tools, annotators, and semantic descriptors). The target schema, in turn, is typically formalized as an ontology or database schema. The designer configures the system and arranges the external tools so that the object model can be built. Then, they write logic rules that map the object model into target schema. The result of the design phase is used at runtime to process the actual documents to create the semantic view."
    }, {
      "heading" : "4.1 Definition of the Target Schema",
      "text" : "This step is crucial for the definition of a desired output of the system. The designer has to decide how to organize the information that will be extracted from the documents. The target schema may be either for a relational database or for an ontology. For the target database, we may consider only the portion of information relevant. Let us assume the following target schema for the considered use case: candidate(Id, Name, Surname, Phone, Email, Address, Gender, Nationality, License); workExperience(Id, Company, BusinessSector, StartDate, EndDate); candWE(IdCandidate, IdWorkExperience). The schema should be consistent and realistic i.e., it should be easy to populate it manually, only by analyzing the input documents."
    }, {
      "heading" : "4.2 Definition of the Object Model",
      "text" : "By considering the template and the target schema, the designer fixes an object model for HUD, which consists of a hierarchical forest-like structure. To define it, we have used the ontology language OntoDLP [12] in which one can define object types, relation types and express relationships between objects. Object types are preceded by keyword entity, and the subclass relationship is expressed via the term isa. Objects may have zero or more attributes which are specified in the type definition, by giving their names and types. By default, a class inherits attributes from its superclass. Relation types can be defined by keyword relation, and giving a name and attributes for this relation (see Section 5 for examples).\nWithin the object model, a few types of objects are identified. First, there are concepts that belong to an ontology representation of a document. This representation is independent of the use case, it is present in KnowRex by default and does not need configuration. It contains one-dimensional objects such as\ntoken (basic elements of text) and delimiters, such as start and end of a line. It also provides two-dimensional concepts, empty and filled cells, to represent the basic elements of the document structure.\nentity onto logyObject . entity oneDimObject i sa onto logyObject .\nentity token i sa oneDimObject . entity d e l i m i t e r i sa oneDimObject .\nentity s ta r tOfL ine i sa d e l i m i t e r . entity endOfLine i sa d e l i m i t e r .\nentity biDimObject i sa onto logyObject . entity c e l l i sa biDimObject .\nentity emptyCell i sa c e l l . entity f i l l e d C e l l i sa c e l l ( va lue : s t r i n g ) .\nIn the second group of concepts, there are categories that can be identified within the content of the document. For a CV use case, we can think of places, persons, companies, schools, skills, different professional terms, e.g. names of programming languages, languages etc. This set of concepts is defined by a designer and heavily depends on the use case, e.g.:\nentity semanticCategory ( va lue : s t r i n g ) . entity person i sa semanticCategory . entity p lace i sa semanticCategory . entity date i sa semanticCategory . entity company i sa semanticCategory . entity e d u c a t i o n I n s t i t u t i o n i sa semanticCategory .\nFinally, there is a group of concepts that describe domain-dependent elements of the structure of the document. These are concepts typically appearing in the considered HUD, such as section headlines, typical labels etc. These concepts are also specified by a designer, e.g.:\nentity domainObject . entity e u c v l a b e l i sa domainObject .\nentity eucv name labe l i sa e u c v l a b e l . entity eucv phone labe l i sa e u c v l a b e l . entity e u c v e m a i l l a b e l i sa e u c v l a b e l .\nentity eucv l abe l box i sa domainObject . entity eucv name labe l box i sa eucv l abe l box .\nentity eucv phone labe l box i sa eucv l abe l box ."
    }, {
      "heading" : "4.3 Arrangement of the Semantic Annotators",
      "text" : "In this step, the designer selects the annotators to be used, then chooses classes that should be searched for, and configures each annotator: provides a mapping from the tool’s output to the object model, and sets the tool’s specific properties. In the case of Europass CV analysis, we have selected: StanfordNER, a custom annotator for recognizing e-mail addresses and dates, a dictionary-based annotator for recognizing skills defined in the European e-Competence Framework 3,\n3 See http://www.ecompetences.eu/.\nand a label annotator based on pattern recognition that recognizes labels typical for Europass CV. Decisions about the arrangement of annotators are made experimentally. Sometimes, it is beneficial to use more than one tool for recognizing the same category. The resulting potential redundancy is not harmful, instead the recall of extraction may improve."
    }, {
      "heading" : "4.4 Two-dimensional Document Analysis",
      "text" : "Knowing the context in which certain phrase appears is helpful for semantic information extraction. In some input data formats, e.g. pdf documents, the information about the structure is lost; while visible to human eye, it is not obvious for a machine. Thus, we need to recover the structure to obtain a meaningful representation of the input documents. To this end, this step configures an external two-dimensional processor and a refinement module inside KnowRex. As a two-dimensional processor, we have used Quablo (http://www.quablo.eu/) that can recognize a set of regular tables within a pdf document. The representation obtained from this tool is then improved by a special module that works with domain concepts, such as labels of the Europass template. The module produces improved structure, merging appropriate cells (for example, if a label spans across two cells, these cells will be merged). Moreover, one- and twodimensional tokenizers (tools inside the KnowRex core) are used to identify the basic one- and two-dimensional objects of the document. In the end, we obtain a grid representation of the document that consists of two-dimensional objects (cells) containing one-dimensional ones (text fragments, delimiters)."
    }, {
      "heading" : "4.5 Semantic Descriptors Specification",
      "text" : "While the semantic annotators identify single words or phrases as belonging to specific classes (producing the “leaves” of the object model), and the twodimensional processing adds structure to the input, the semantic descriptors can combine and use the above information to build more complex objects. Semantic descriptors are rules that organize two-dimensional and one-dimensional objects into descriptions to extract additional information. This is done on several levels. To help the intuition, we illustrate the semantic descriptors by examples.\nFirst, a designer should identify parts of the document that will help to localize other data portions, e.g.:\n<eucv_email_label_box()> ::- <filledCell()> CONTAINS <eucv_email_label()>\nWith this simple descriptor, we intend to create a (two-dimensional) concept eucv email label box that defines a cell in which there is a (one-dimensional) eucv email label. The object we want to extract always resides on the left-handside of the operator “::-” (in the head of a descriptor), while on the right (in the body), there are objects that must be found. In this example, we look for a cell within which there is a particular domain concept, an eucv email label. If we find a cell with this label inside, the cell can be recognized as a eucv email label box.\nDescriptors can join several cells that appear in a document one after another (horizontally or vertically). This is useful, if we want to say that there exist a particular object, if there is a specific sequence of cells, e.g.:\n<candidateEmail(E)> ::- <eucv_email_label_box()>\n(<filledCell(X)> CONTAINS <email(X)> {E:=X;})\nThis description should be read as: “A candidateEmail is a two-dimensional object that captures two cells: the first is an eucv email label box and is followed (horizontally) by a filledCell that contains a (one-dimensional) object email with value X. The new object spans across both cells, and the value of the object email becomes the value of candidateEmail.” By using the context (first there is a box with an e-mail label, and then there is a cell with an e-mail address), we ensure that, even if the CV contains a few e-mail addresses, we select the correct one, because if the e-mail address appears in this place, it must be in the Personal Information section, and thus it is the e-mail of the candidate.\nWe can also aggregate the concepts and attributes extracted by other semantic descriptors to build more complex ones, e.g.:\n<personalInformation(N, S, A, P, E, Nt G)> ::|\n<candidateName(X)> {N:=X;} <candidateSurname(X)> {S:=X;} <candidateAddress(X)> {A:=X;} <candidatePhone(X)> {P:=X;}\n<candidateNationality(X)> {Nt:=X;} <candidateGender(X)> {G:=X;}\nThis semantic descriptor aggregates results of other descriptors that extract single information about a candidate. It describes a sequence of concepts that must appear one after another vertically, which we mark with the “::|” operator.\nWithin cells, we can create one-dimensional descriptors by using the operator “::”. A recurrence structure “(sequence of terms )+” and a keyword “...” that allows to skip irrelevant data allow to build complex descriptions such as:\n<list_of_skills(S)> :: {S:=[];} <startOfLine> ...\n(<IndustryTerm(S1)> {S&=S1;} ...)+ <endOfLine>\nThis descriptor works on one-dimensional objects that are all located in one cell (treated as a single line thanks to the two-dimensional processing). Here, we want to create a list, so we initialize the attribute S:=[]. Then, we look for a concept IndustryTerm, recognized by a semantic annotator, append its attribute value to S ({S&=S1;}), and place the term in a recurrence structure. The expression (<IndustryTerm(S1)>{S&=S1;} ...)+ means that there may be some objects after the IndustryTerm that we ignore, and if we find another object IndustryTerm, we append its attribute value to the list again. By using the keyword “...” before the recurrence, we say that we can skip some objects, i.e., the recurrence structure may appear anywhere between the startOfLine and the endOfLine. The descriptor creates a new object list of skills that stores as an attribute a list of IndustryTerm objects’ attributes.\nFinally, semantic descriptors may use the information about the placement of objects within the document (e.g. presence of a given object within specific section) to extract new objects that are not explicitly defined in text, e.g.:\n<list_of_practical_skills(S)> ::- <eucv_work_act_resp_label_box()>\n(<filledCell(X)> CONTAINS <list_of_skills(X)> {S:=X;})\nIn this example, we use a eucv work act resp label box, a domain concept that represents a cell containing “Activities and Responsibilities” label for selected Work Experience. This way, we look for lists of skills present only within the Work Experience subsections (and not for example within Education ones) and we can call them practical skills."
    }, {
      "heading" : "4.6 Defining a Mapping from Object Model to Target Schema",
      "text" : "The design phase in KnowRex is completed with the definition of a mapping from object model classes to the concepts of the target schema. This mapping, written in a form of Datalog rules, is used to automatically create a semantic view of the (structured) input documents during the runtime phase. In the head of rules, there are concepts from the target schema, and in the body – objects from the object model (and auxiliary objects such as candidate ID). Partial mapping of the object model to the target schema is as follows:\ncandidate(Id,N,S,P,E,A,G,Nt,D,L) :- ID:cv_candidate_id(Id),\nPI:personalInformation(N,S,A,P,E,Nt,D,G), CDL:candidateDrivingLicence(L).\nworkExperience(WExpId, Company, BusinessSector, Start, End) :-\nC:company(WExpId,Company,BusinessSector), WED:workExperienceDates(WExpId,Start,End).\nWhen the design phase ends, all the decisions are saved in the configuration files of the system. The semantic descriptors are translated into logic rules."
    }, {
      "heading" : "5 The Runtime Phase of the System",
      "text" : "Once the design of the project is done, KnowRex can be run over a collection of input documents. The flow of the operations and the relations between the design and runtime phases may be observed in Figure 5.\nIn the stage of the document analysis, first, a two-dimensional processor is used. Its output is then refined according to domain knowledge (specific labels, structure elements or keywords). This improved structure is analyzed by one- and two-dimensional tokenizers, tools hidden from a user, that identify the atomic one- and two-dimensional components of a document (tokens and cells). A logical fact base is obtained that represent the document as a two-dimensional “grid”. KnowRex uses a two-dimensional representation of objects that helps localize them within the documents. The definitions of the position relations in OntoDLP are as follows:\nrelation p o s i t i o n ( obj : ontologyObject , s t a r t : int , end : int ) . relation onePos i t i on ( obj : oneDimObject , s t a r t : int , end : int ) . relation b i P o s i t i o n ( obj : biDimObject , x s t a r t : int , y s t a r t : int , xend : int , yend : int ) . relation belongsTo ( obj : oneDimObject , obj2 : biDimObject )\nAt the end of the two-dimensional processing stage, an ontological model of the document is obtained. It contains information about positions of the one- and two-dimensional objects within the document. For each two-dimensional object, a relation biPosition is added that specify the row and column on the document “grid”, where the object appears, e.g.:\nfilled19:filledCell(’anna@w3.org’). biPosition(filled19,1,8,2,9).\nFor all one-dimensional objects (that are located inside the two-dimensional cells), two relations are added: belongsTo that identifies the containing cell by its id, and onePosition which denotes the position of the object within a cell:\ntk123:token(’manager’).onePosition(tk123,0,6).belongsTo(tk123,filled80). tk124:token(’of’).onePosition(tk124,6,7).belongsTo(tk124,filled80).\nThis representation is normalized i.e., the positions of blank spaces are omitted and the tokens follow one another. Such a representation is a reference for semantic annotators that may treat blank spaces differently.\nThen comes the annotation stage, in which selected semantic annotators are run over the identified cells and label the parts of text as objects belonging to different classes (such as Places, Persons, IndustryTerms, etc.) The representation of the identified objects (new logic facts that carry information about the annotator that found the object) is added to the fact base, e.g.:\nannS2:email(’anna@w3.org’).one_position(0,10).belongs_to(annS2,filled19).\nOnce the annotation stage is finished, the semantic descriptors which have been compiled into logic rules are executed over the facts representing the objects within a document. Each descriptor is transformed into a set of logical rules that first extract the portion of the document complying to the descriptor body, and then create a new object, specified in the descriptor head.\nEach descriptor is internally represented as an automaton. After setting the initial configuration, each element of a descriptor is treated as a transition that allows to go from one state to the next one. The condition that one object must appear after another in a document is realized by checking the positions of the objects using biPosition and onePosition relations. The relation belongsTo checks the conditions expressed by the CONTAINS keyword. The attributes of the objects are passed between the rules by using variables.\nFor instance, the descriptor from Example 2 in Section 4.5:\n<candidateEmail(E)> ::- {E:=’’;} <eucv_email_label_box()>\n<filledCell(X)> CONTAINS <email(X)> {E:=X;}\nis translated into the following logic rules:\n1. Extracting candidate email from the document:\ninit_conf_candidateEmail(0,\"\"). conf_candidateEmail(1, \"\", Xe, Ys, Xe_1, Ye) :-\ninit_conf_candidateEmail(0,\"\"), eu_cv_email_label_box(Id, Lc4), bi_position(Id, Xe, Ys, Xe_1, Ye).\nconf_candidateEmail(2, Lc1, Xs, Ys, Xe_1, Ye) :-\nconf_candidateEmail(1, Gl3, Xs, Ys, Xe, Ye), filledCell(Id, Lc1), bi_position(Id, Xe, Ys, Xe_1, Ye).\nconf_candidateEmail(3, Lc2, Xe, Ys, Xe_1, Ye) :-\nconf_candidateEmail(1, Gl3, Xs, Ys, Xe, Ye), filledCell(Id, Lc1), bi_position(Id, Xe, Ys, Xe_1, Ye), email(IdContains, Lc2), belongs_to(IdContains,Id).\n2. Creating a new object for the object model, with its position:\naux_candidateEmail(AutoGen,Gl3, Xs, Ys, Xe, Ye) :-\nconf_candidateEmail(3,Gl3, Xs, Ys, Xe, Ye), AutoGen=#newID.\nAutoGen : candidateEmail(Gl3) :-\naux_candidateEmail(AutoGen,Gl3, Xs, Ys, Xe, Ye).\nbi_position(AutoGen, Xs, Ys, Xe, Ye) :-\naux_candidateEmail(AutoGen,Gl3, Xs, Ys, Xe, Ye).\nThese rules use the output of the extraction and create an object in OntoDLP, together with its one- or bi-dimensional position (and optionally, belonging to a cell, if it is a one-dimensional object).\nFinally, the extracted objects are transformed into the instances of the semantic view (see Figure 6) with use of the mapping defined in the design phase. Technically, this is done by additional logic rules that create instances for the target representation from the objects (in OntoDLP) of the object model."
    }, {
      "heading" : "6 Discussion and Conclusion",
      "text" : "We have described an ontology-based approach for extracting and organizing semantically rich information from HUD. This approach has been implemented in a system called KnowRex, which has been tested on curricula in the Europass style, stored as pdf files. Roughly, the design phase has been carried out in two man-weeks. From our preliminary analysis over 80 CVs, it appeared that the two-dimensional structure recognition and the recall of third-party annotators are the main bottlenecks. With initial configuration for the Europass template, Quablo worked well for about 50% of documents. For further 20% of documents, satisfying results were obtained by small adjustments of the tool (margin toleration etc.). While the precision of the semantic annotators is satisfying, their sometimes low recall may be compensated by adjusting dictionary-based annotators. Logical rules (semantic descriptors and mapping rules) worked as expected on the found objects without loss of precision. KnowRex is sufficiently flexible and modular to be suitable for various scenarios in which HUD is available."
    }, {
      "heading" : "Acknowledgement",
      "text" : "The work has been supported by the Calabrian Region within the project ”THT – Talent Hunter Technology” (CUP: J84E07000540005)."
    } ],
    "references" : [ {
      "title" : "Ontology guided information extraction from unstructured text",
      "author" : [ "R. Anantharangachar", "S. Ramani", "S. Rajagopalan" ],
      "venue" : "CoRR abs/1302.1335",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Introduction to information extraction: Basic notions and current trends",
      "author" : [ "W.T. Balke" ],
      "venue" : "Datenbank-Spektrum 12(2), 81–88",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A Survey of Web Information Extraction Systems",
      "author" : [ "C.H. Chang", "M. Kayed", "M.R. Girgis", "K.F. Shaalan" ],
      "venue" : "IEEE Trans. on Kn. and Data Eng. 18(10), 1411–1428",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Aggregating semantic annotators",
      "author" : [ "L. Chen", "S. Ortona", "G. Orsi", "M. Benedikt" ],
      "venue" : "Proc. VLDB Endow. 6(13), 1486–1497",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A Comparative Study of Information Extraction Strategies",
      "author" : [ "R. Feldman", "Y. Aumann", "M. Finkelstein-Landau", "E. Hurvitz", "Y. Regev", "A. Yaroshevich" ],
      "venue" : "Proc. of CICLing, Mexico City, Mexico. pp. 21–34",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Little knowledge rules the web: Domain-centric result page extraction",
      "author" : [ "T. Furche", "G. Gottlob", "G. Grasso", "G. Orsi", "C. Schallhart", "C. Wang" ],
      "venue" : "Web Reasoning and Rule Systems, LNCS, vol. 6902, pp. 61–76",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Information extraction from text",
      "author" : [ "J. Jiang" ],
      "venue" : "Mining Text Data, pp. 11–41",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "An ontology-based retrieval system using semantic indexing",
      "author" : [ "S. Kara", "O. Alan", "O. Sabuncu", "S. Akpinar", "N.K. Cicekli", "F.N. Alpaslan" ],
      "venue" : "Information Systems 37(4), 294 – 305",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Ontology based information extraction from text",
      "author" : [ "V. Karkaletsis", "P. Fragkou", "G. Petasis", "E. Iosif" ],
      "venue" : "Knowledge-Driven Multimedia Information Extraction and Ontology Evolution, LNCS, vol. 6050, pp. 89–109",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "The HıLεX system for semantic information extraction",
      "author" : [ "M. Manna", "E. Oro", "M. Ruffolo", "M. Alviano", "N. Leone" ],
      "venue" : "Trans. on Large-Scale Data- and KnowledgeCentered Systems V, vol. 7100, pp. 91–125",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Ontology-based web information extraction",
      "author" : [ "Q. Mo", "Chen", "Y.h." ],
      "venue" : "Communications and Information Processing, CCIS, vol. 288, pp. 118–126",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Disjunctive logic programming with types and objects: The DLV+ system",
      "author" : [ "F. Ricca", "N. Leone" ],
      "venue" : "J. Applied Logic 5(3), 545–573",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "The problem of identifying and extracting information from unstructured documents is widely studied in the field of information and knowledge management, and is referred to as Information Extraction (IE) [5,3,7,2].",
      "startOffset" : 204,
      "endOffset" : 213
    }, {
      "referenceID" : 2,
      "context" : "The problem of identifying and extracting information from unstructured documents is widely studied in the field of information and knowledge management, and is referred to as Information Extraction (IE) [5,3,7,2].",
      "startOffset" : 204,
      "endOffset" : 213
    }, {
      "referenceID" : 6,
      "context" : "The problem of identifying and extracting information from unstructured documents is widely studied in the field of information and knowledge management, and is referred to as Information Extraction (IE) [5,3,7,2].",
      "startOffset" : 204,
      "endOffset" : 213
    }, {
      "referenceID" : 1,
      "context" : "The problem of identifying and extracting information from unstructured documents is widely studied in the field of information and knowledge management, and is referred to as Information Extraction (IE) [5,3,7,2].",
      "startOffset" : 204,
      "endOffset" : 213
    }, {
      "referenceID" : 8,
      "context" : "More recently, some works have shown the promise of deducing and encoding formal knowledge in the form of ontologies [9,8,1,6].",
      "startOffset" : 117,
      "endOffset" : 126
    }, {
      "referenceID" : 7,
      "context" : "More recently, some works have shown the promise of deducing and encoding formal knowledge in the form of ontologies [9,8,1,6].",
      "startOffset" : 117,
      "endOffset" : 126
    }, {
      "referenceID" : 0,
      "context" : "More recently, some works have shown the promise of deducing and encoding formal knowledge in the form of ontologies [9,8,1,6].",
      "startOffset" : 117,
      "endOffset" : 126
    }, {
      "referenceID" : 5,
      "context" : "More recently, some works have shown the promise of deducing and encoding formal knowledge in the form of ontologies [9,8,1,6].",
      "startOffset" : 117,
      "endOffset" : 126
    }, {
      "referenceID" : 10,
      "context" : "Our approach follows the line of combining several techniques to obtain comprehensive results [11,4].",
      "startOffset" : 94,
      "endOffset" : 100
    }, {
      "referenceID" : 3,
      "context" : "Our approach follows the line of combining several techniques to obtain comprehensive results [11,4].",
      "startOffset" : 94,
      "endOffset" : 100
    }, {
      "referenceID" : 9,
      "context" : "The notion of semantic descriptors introduced in Section 4 has been inherited from HiLeX [10].",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 11,
      "context" : "To define it, we have used the ontology language OntoDLP [12] in which one can define object types, relation types and express relationships between objects.",
      "startOffset" : 57,
      "endOffset" : 61
    } ],
    "year" : 2015,
    "abstractText" : "Homogeneous unstructured data (HUD) are collections of unstructured documents that share common properties, such as similar layout, common file format, or common domain of values. Building on such properties, it would be desirable to automatically process HUD to access the main information through a semantic layer – typically an ontology – called semantic view. Hence, we propose an ontology-based approach for extracting semantically rich information from HUD, by integrating and extending recent technologies and results from the fields of classical information extraction, table recognition, ontologies, text annotation, and logic programming. Moreover, we design and implement a system, named KnowRex, that has been successfully applied to curriculum vitae in the Europass style to offer a semantic view of them, and be able, for example, to select those which exhibit required skills.",
    "creator" : "LaTeX with hyperref package"
  }
}