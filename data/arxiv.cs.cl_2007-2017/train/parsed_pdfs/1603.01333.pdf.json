{
  "name" : "1603.01333.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Joint Learning Templates and Slots for Event Schema Induction",
    "authors" : [ "Lei Sha", "Sujian Li", "Baobao Chang", "Zhifang Sui" ],
    "emails" : [ "szf@pku.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 3.\n01 33\n3v 1\n[ cs\n.C L\n] 4\nM ar\nAutomatic event schema induction (AESI) means to extract meta-event from raw text, in other words, to find out what types (templates) of event may exist in the raw text and what roles (slots) may exist in each event type. In this paper, we propose a joint entity-driven model to learn templates and slots simultaneously based on the constraints of templates and slots in the same sentence. In addition, the entities’ semantic information is also considered for the inner connectivity of the entities. We borrow the normalized cut criteria in image segmentation to divide the entities into more accurate template clusters and slot clusters. The experiment shows that our model gains a relatively higher result than previous work."
    }, {
      "heading" : "1 Introduction",
      "text" : "Event schema is a high-level representation of a bunch of similar events. It is very useful for the traditional information extraction (IE)(Sagayam et al., 2012) task. An example of event schema is shown in Table 1. Given the bombing schema, we only need to find proper words to fill the slots when extracting a bombing event.\nThere are two main approaches for AESI task. Both of them use the idea of clustering the potential event arguments to find the event schema. One of them is probabilistic graphical model (Chambers, 2013; Cheung, 2013). By incorporating templates and slots as latent topics, probabilistic graphical models learns those templates and slots\nBombing Template Perpetrator: person Victim: person Target: public Instrument: bomb"
    }, {
      "heading" : "2 Task Definition",
      "text" : "Our model is an entity-driven model. This model represents a document d as a series of entities Ed = {ei|i = 1, 2, · · · }. Each entity is a quadruple e = (h, p, d, f). Here, h represents the head word of an entity, p represents its predicate, and d represents the dependency path between the predicate and the head word, f contains the features of the entity (such as the direct hypernyms of the head word), the sentence id where e occurred and the document id where e occurred. A simple example is Fig 1.\nOur ultimate goal is to assign two labels, a slot variable s and a template variable t, to each entity. After that, we can summarize all of them to get event schemas."
    }, {
      "heading" : "3 Automatic Event Schema Induction",
      "text" : ""
    }, {
      "heading" : "3.1 Inner Connectivity Between Entities",
      "text" : "We focus on two types of inner connectivity: (1) the likelihood of two entities to belong to the same template; (2) the likelihood of two entities to belong to the same slot;"
    }, {
      "heading" : "3.1.1 Template Level Connectivity",
      "text" : "It is easy to understand that entities occurred near each other are more likely to belong to the same template. Therefore, (Chambers and Jurafsky, 2011) uses PMI to measure the correlation of two words in the same document, but it cannot put two words from different documents together. In the Bayesian model of (Chambers, 2013), p(predicate) is the key factor to decide the template, but it ignores the fact that entities occurring nearby should belong to\nthe same template. In this paper, we try to put two measures together. That is, if two entities occurred nearby, they can belong to the same template; if they have similar meaning, they can also belong to the same template. We use PMI to measure the distance similarity and use word vector (Mikolov et al., 2013) to calculate the semantic similarity.\nA word vector can well represent the meaning of a word. So we concatenate the word vector of the j-th entity’s head word and its predicate, denoted as vechp(i). We use the cosine distance coshp(i, j) to measure the difference of two vectors.\nThen we can get the template level connectivity formula as shown in Eq 1. The PMI(i, j) is calculated by the head words of entity mention i and j.\nWT (i, j) = PMI(i, j) + coshp(i, j) (1)"
    }, {
      "heading" : "3.1.2 Slot Level Connectivity",
      "text" : "If two entities can play similar role in an event, they are likely to fill the same slot. We know that if two entities can play similar role, their head words may have the same hypernyms. We only consider the direct hypernyms here. Also, their predicates may have similar meaning and the entities may have the same dependency path to their predicate. Therefore, we give the factors equal weights and add them together to get the slot level similarity.\nWS(i, j) = cosp(i, j) + δ(dependi = dependj)\n+ δ(hypernymi ∩ hypernymj 6= φ) (2)\nHere, the δ(·) has value 1 when the inner expression is true and 0 otherwise. The “hypernym” is derived from Wordnet(Miller, 1995), so it is a set of direct hypernyms. If two entities’ head words have at least one common direct hypernym, then they may belong to the same slot. And again cosp(i, j) represents the cosine distance between the predicates’ word vector of entity i and entity j."
    }, {
      "heading" : "3.2 Template and Slot Clustering Using Normalized Cut",
      "text" : "Normalized cut intend to maximize the intra-class similarity while minimize the inter class similarity, which deals well with the connectivity between entities.\nWe represent each entity as a point in a highdimension space. The edge weight between two points is their template level similarity / slot level similarity. Then the larger the similarity value is, the more likely the two entities (point) belong to the same template / slot, which is also our basis intuition.\nFor simplicity, denote the entity set as E = {e1, · · · , e|E|}, and the template set as T . We use the |E| × |T | partition matrix XT to represent the template clustering result. Let XT = [XT1 , · · · ,XT|T | ], where XTl is a binary indicator for template l(Tl).\nXT (i, l) =\n{\n1 ei ∈ Tl 0 otherwise (3)\nUsually, we define the degree matrix DT as: DT (i, i) = ∑\nj∈E WT (i, j), i = 1, · · · , |E|. Obviously, DT is a diagonal matrix. It contains information about the weight sum of edges attached to each vertex. Then we have the template clustering optimization as shown in Eq 4 according to (Shi and Malik, 2000).\nmax ε1(XT ) = 1\n|T |\n|T | ∑\nl=1\nXTTlWTXTl XTTlDTXTl\ns.t. XT ∈ {0, 1} |E|×|T | XT 1|T | = 1|E|\n(4)\nwhere 1|E| represents the |E| × 1 vector of all 1’s. For the slot clustering, we have a similar optimization as shown in Eq 5.\nmax ε2(XS) = 1\n|S|\n|S| ∑\nl=1\nXTSlWSXSl XTSlDSXSl\ns.t. XS ∈ {0, 1} |E|×|S| XS1|S| = 1|E|\n(5)\nwhere S represents the slot set, XS is the slot clustering result with XS = [XS1 , · · · ,XS|S| ], where XSl is a binary indicator for slot l(Sl).\nXS(i, l) =\n{\n1 ei ∈ Sl 0 otherwise (6)"
    }, {
      "heading" : "3.3 Joint Model With Sentence Constraints",
      "text" : "For event schema induction, we find an important property and we name it “Sentence constraint”. The\nentities in one sentence often belong to one template but different slots.\nThe sentence constraint contains two types of constraint, “template constraint” and “slot constraint”.\n1. Template constraint: Entities in the same sentence are usually in the same template. Hence we should make the templates taken by a sentence as few as possible.\n2. Slot constraint: Entities in the same sentence are usually in different slots. Hence we should make the slots taken by a sentence as many as possible.\nBased on these consideration, we can add an extra item to the optimization object. Let Nsentence be the number of sentences. Define Nsentence× |E| matrix J as the sentence constraint matrix, the entries of J is as following:\nJ(i, j) =\n{\n1 ei ∈ Sentencej 0 otherwise (7)\nEasy to show, the product GT = JTXT represents the relation between sentences and templates. In matrix GT , the (i, j)-th entry represents how many entities in sentence i are belong to Tj .\nUsing GT , we can construct our objective. To represent the two constraints, the best objective we have found is the trace value: tr(GTGTT ). Each entry on the diagonal of matrix GTGTT is the square sum of all the entries in the corresponding line in GT , and the larger the trace value is, the less templates the sentence would taken. Since tr(GTGTT ) is the sum of the diagonal elements, we only need to maximize the value tr(GTGTT ) to meet the template constraint. For the same reason, we need to minimize the value tr(GSG T S ) to meet the slot constraint.\nGenerally, we have the following optimization objective:\nε3(XT ,XS) = tr ( XTT JJ TXT )\ntr (\nXTS JJ TXS\n) (8)\nThe whole joint model is shown in Eq 9. The solving\nmethod is in the attachment file.\nXT ,XS = argmax XT ,XS ε1(XT ) + ε2(XS) + ε3(XT ,XS)\ns.t. XT ∈ {0, 1} |E|×|T | XT 1|T | = 1|E|\nXS ∈ {0, 1} |E|×|S| XS1|S| = 1|E|\n(9)"
    }, {
      "heading" : "4 Experiment",
      "text" : ""
    }, {
      "heading" : "4.1 Dataset",
      "text" : "In this paper, we use MUC-4(Sundheim, 1991) as our dataset, which is the same as previous works (Chambers and Jurafsky, 2011; Chambers, 2013). MUC-4 corpus contains 1300 documents in the training set, 200 in development set (TS1, TS2) and 200 in testing set (TS3, TS4) about Latin American news of terrorism events. We ran several times on the 1500 documents (training/dev set) and choose the best |T | and |S| as |T | = 6, |S| = 4. Then we report the performance of test set. For each document, it provides a series of hand-constructed event schemas, which are called gold schemas. With these gold schemas we can evaluate our results. The MUC-4 corpus contains six template types: Attack, Kidnapping, Bombing, Arson, Robbery, and Forced Work Stoppage, and for each template, there are 25 slots. Since most previous works do not evaluate their performance on all the 25 slots, they instead focus on 4 main slots like Table 1, we will also focus on these four slots. We use the Stanford CoreNLP toolkit to parse the MUC-4 corpus."
    }, {
      "heading" : "4.2 Performance",
      "text" : "Fig 2 shows two examples of our learned schemas: Bombing and Attacking. The five words in each slot are the five randomly picked entities from the mapped slots. The templates and slots that were joint learned seem reasonable.\nWe compare our results with four works (Chambers and Jurafsky, 2011; Cheung, 2013; Chambers, 2013; Nguyen et al., 2015) as is shown in Table 2. Our model has outperformed all of the previous methods. The improvement of recall is due to the normalized cut criteria, which can better use the inner connectivity between entities. The sentence constraint improves the result one step further.\nBombing\nPerpetrator Victim Target Instrument\nAttack\nPerpetrator Victim Target Instrument\nEl salvador\nThe guerrillas The drag mafia Drug traffickers\nThe Atlacatl battalion\nThe police chief\nStudents\nThe Peruvian embassy\nThe diplomat\nsoldiers\nministry\nThe embassy\nThe police station\norganization\nbridge\nexplosives car bomb dynamite\nincendiary bomb\nvehicle bomb\ntroops\ncriminals combat\nmurder person\ndriver\nsoldiers children civilians\njournalists\norganization\nhelicopter\nperson\nlivestock ministray building\nvehicles\nrifles\nweapons\ngun\nexplosives\nmachinegun"
    }, {
      "heading" : "5 Related Works",
      "text" : "Also, there are some state-of-the-art works using probabilistic graphic model (Chambers, 2013; Cheung, 2013; Nguyen et al., 2015)."
    }, {
      "heading" : "6 Conclusion",
      "text" : "This paper presented a joint entity-driven model to induct event schemas automatically.\nThis model uses word embedding as well as PMI to measure the inner connection of entities and uses normalized cut for more accurate clustering. Finally, our model uses sentence constraint to extract templates and slots simultaneously. The experiment has proved the effectiveness of our model."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This research is supported by National Key Basic Research Program of China (No.2014CB340504) and National Natural Science Foundation of China (No.61375074,61273318). The contact authors of this paper are Sujian Li and Baobao Chang."
    }, {
      "heading" : "1 The Model",
      "text" : "Template clustering optimization is shown in Eq 1.\nmax ε1(XT ) = 1\n|T |\n|T | ∑\nl=1\nXTTlWTXTl XTTlDTXTl\ns.t. XT ∈ {0, 1} |E|×|T | XT1|T | = 1|E|\n(1)\nHere, 1|E| represents the |E| × 1 vector of all 1’s. Slot clustering optimization is shown in Eq 2.\nmax ε2(XS) = 1\n|S|\n|S| ∑\nl=1\nXTSlWSXSl XTSlDSXSl\ns.t. XS ∈ {0, 1} |E|×|S| XS1|S| = 1|E|\n(2)\nHere, S represents the slot set, XS is the slot clustering result with XS = [XS1 , · · · , XS|S| ], where XSl is a binary indicator for slot l(Sl).\nXS(i, l) =\n{\n1 ei ∈ Sl 0 otherwise (3)\nThe original sentence constraint model is shown as follows:\nε3(XT , XS) = tr ( XTT JJ TXT )\ntr (\nXTS JJ TXS\n) (4)\nHowever, this form of objective is hard to optimize, we can transfer the slot constraint objective tr(GSG T S ) (GS = J\nTXS) to something that should be maximized. Since tr(GSG T S ) = tr ( XTS JJ TXS) ) , to minimize tr ( XTS JJ TXS) ) is the same as to maximize tr (\nXTS (E − JJ T )XS)\n)\n(E = 1 · 1T ). 1 represents an all 1\nvector. It can be proved that tr (\nXTS (E − JJ T )XS)\n)\nis positive.\n1\nGenerally, we have the following optimization objective:\nmax ε3(XT , XS) = tr ( XTT JJ TXT ) tr ( XTS (E − JJ T )XS) )\ns.t. XT ∈ {0, 1} |E|×|T | XT1|T | = 1|E|\nXS ∈ {0, 1} |E|×|S| XS1|S| = 1|E|\n(5)\nThe whole joint model is shown in Eq 6. The first item represents the goodness of the templates clustering. The second item represents the goodness of the slot clustering. The third item is the sentence constraint item. However, this model is too complex to be solved by normal optimization method. Therefore, we use the Alternating Maximization Procedure[2] to solve this problem in the following section.\nXT , XS = argmax XT ,XS ε1(XT ) + ε2(XS) + ε3(XT , XS)\ns.t. XT ∈ {0, 1} |E|×|T | XT1|T | = 1|E|\nXS ∈ {0, 1} |E|×|S| XS1|S| = 1|E|\n(6)"
    }, {
      "heading" : "2 Solving Method: Alternating Maximization",
      "text" : "Procedure(AMP)\nIn this section, the detailed solving method of the complex model shown in Eq 6 will be illustrated. The ultimate objective in Eq 6 is the combination of optimization objective in Eq 1, Eq 2 and Eq 5.\nThe first two items in Eq 6 is the form of generalized Rayleigh quotient and can be solved using the method in [3], which mainly contains two steps: 1) find the continuous optimal value 2) discretization. We use the AMP method to get the numerical solution of Eq 6. The AMP algorithm can be viewed as a joint maximization method by fixing one argument and maximizing over the other. After we fixed XS or XT , we can transform the objective to the form of generalized Rayleigh quotient which could be solved by the method in [3].\nWhen XT is fixed The first term in Eq 6 is a constant in this case, so that we ignore it for simplicity. Let f(XT ) = tr ( XTT JJ TXT ) , then Eq 6 becomes:\nmax ε(XS ;XT ) = 1\n|S|\n|S| ∑\nl=1\nXTSlWSXSl XTSlDSXSl + f(XT )\n|S| ∑\nl=1\nXTSl(E − JJ T )XSl (7)\nWe can reduce the fractions to a common denominator, then Eq 7 becomes:\n|S| ∑\nl=1\n1 |S|X T Sl WSXSl + f(XT ) ∗X T Sl (E − JJT )XSlX T Sl DSXSl\nXTSlDSXSl (8)\n2\nNote that the term XTSl(E−JJ T )XSlX T Sl DSXSl is a scalar, so that we can take it as a trace of a 1× 1 matrix as shown in Eq 9.\nXTSl(E − JJ T )XSlX T Sl DSXSl\n= tr(XTSl(E − JJ T )XSlX T Sl DSXSl) = ΩSX T Sl (E − JJT )DSXSl\n(9)\nHere, ΩS = X T Sl XSl is a diagonal matrix. Each diagonal entry is the number of entities in the corresponding slot. In order to represent Eq 8 to the form of Eq 10, we need to keep D∗S = DS , and the W ∗S is as Eq 11. In order to keep W ∗ S a symmetric matrix, we add 1 2 of Eq 9 to both sides of XTSlWSXSl .\nε(XS ;XT ) =\n|S| ∑\nl=1\nXTSlW ∗ SXSl XTSlD ∗ SXSl\n(10)\n\n    \n    \nW ∗S = 1\n2 f(XT )DS(E − JJ\nT )ΩS + 1\n|S| WS\n+ 1\n2 f(XT )ΩS(E − JJ\nT )DS\nD∗S = DS\n(11)\nWhen XS is fixed Using the same method as the above, in order to get the form of Eq 12, the value of W ∗T and D ∗ T are calculated as Eq 13.\nε(XT ;XS) =\n|T | ∑\nl=1\nXTTlW ∗ TXTl XTTlD ∗ TXTl\n(12)\n\n     \n     \nW ∗T = 1\n2f(XS) JJTDTΩT +\n1\n|T | WT\n+ 1\n2f(XS) ΩTDTJJ\nT\nD∗T =DT\n(13)\nStopping criteria According to [3], if XT , XS is a feasible solution to Eq 6, so is {XTRT , XSRS |R T TRT = I, R T SRS = I}, and they have the same objective value: ε(XTRT , XSRS) = ε(XT , XS). Therefore, if Eq 14 is satisfied, the loop ends.\n‖XnewT −X old T RT ‖ = 0 ‖XnewS −X old S RS‖ = 0\n(14)\nWe can get the closed form of RT and RS as shown in Eq 15.\nRT = (X (new)T T X new T ) −1X (new)T T X old T RS = (X (new)T S X new S ) −1X (new)T S X old S\n(15)\n3\nTherefore, the ultimate stop criteria becomes ‖RTTRT − I‖ + ‖R T SRS − I‖ < ǫ, ǫ is very close to 0. The total algorithm of the whole process is shown as Algorithm 1. Since the optimization objective is a differentiable function, the convergence to the optimum solution can be guaranteed by [2, 1].\nAlgorithm 1: The pseudo code of the optimum value finding process\nInput: Template level similarity matrix, WT ; Slot level similarity matrix, WS ; sentence constraint matrix, J . Output: The partition matrix of template, XT ; The partition matrix of slot, XS ; begin\nRandomly initialize XT and XS ; while ‖RTTRT − I‖+ ‖R T SRS − I‖ > ǫ do\nFix XT , calculate Eq 11; Find XS which can maximize Eq 10; Fix XS , calculate Eq 13; Find XT which can maximize Eq 12; Calculate RT and RS by Eq 15;\nend while Discretize XT and XS ; return XT and XS\nend"
    }, {
      "heading" : "3 Experiment Setting",
      "text" : "The ΩT and ΩS in Eq 13 and Eq 11 can be seen as a prior of the template cluster size and slot cluster size. We use the most näıve prior that all clusters are of the same size."
    } ],
    "references" : [ {
      "title" : "The elements of statistical learning",
      "author" : [ "Trevor Hastie", "Robert Tibshirani", "Jerome Friedman", "T Hastie", "J Friedman", "R Tibshirani" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2009
    }, {
      "title" : "Alternating maximization procedure for finding the global maximum of directed information",
      "author" : [ "Iddo Naiss", "Haim H Permuter" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2010
    }, {
      "title" : "Multiclass spectral clustering",
      "author" : [ "Stella X Yu", "Jianbo Shi" ],
      "venue" : "In Computer Vision,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2003
    } ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "Automatic event schema induction (AESI) means to extract meta-event from raw text, in other words, to find out what types (templates) of event may exist in the raw text and what roles (slots) may exist in each event type. In this paper, we propose a joint entity-driven model to learn templates and slots simultaneously based on the constraints of templates and slots in the same sentence. In addition, the entities’ semantic information is also considered for the inner connectivity of the entities. We borrow the normalized cut criteria in image segmentation to divide the entities into more accurate template clusters and slot clusters. The experiment shows that our model gains a relatively higher result than previous work.",
    "creator" : "LaTeX with hyperref package"
  }
}