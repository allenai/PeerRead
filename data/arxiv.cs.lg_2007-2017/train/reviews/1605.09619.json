{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2016", "title": "Horizontally Scalable Submodular Maximization", "abstract": "A variety of large-scale machine learning problems can be cast as instances of constrained submodular maximization. Existing approaches for distributed submodular maximization have a critical drawback: The capacity - number of instances that can fit in memory - must grow with the data set size. In practice, while one can provision many machines, the capacity of each machine is limited by physical constraints. We propose a truly scalable approach for distributed submodular maximization under fixed capacity. The proposed framework applies to a broad class of algorithms and constraints and provides theoretical guarantees on the approximation factor for any available capacity. We empirically evaluate the proposed algorithm on a variety of data sets and demonstrate that it achieves performance competitive with the centralized greedy solution.", "histories": [["v1", "Tue, 31 May 2016 13:18:30 GMT  (464kb,D)", "http://arxiv.org/abs/1605.09619v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.DC cs.DM cs.LG", "authors": ["mario lucic", "olivier bachem", "morteza zadimoghaddam", "andreas krause 0001"], "accepted": true, "id": "1605.09619"}
