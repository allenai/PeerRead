{
  "name" : "1302.4980.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Accounting for Context in Plan Recognition, with Application to Traffic Monitoring",
    "authors" : [ "David V. Pynadath", "Michael P. Wellman" ],
    "emails" : [ "@engin.urnich.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Typical approaches to plan recognition start from a representation of an agent's possible plans, and reason evidentially from observations of the agent's actions to assess the plausibility of the various candidates. A more expansive view of the task (consistent with some prior work) ac counts for the context in which the plan was gen erated, the mental state and planning process of the agent, and consequences of the agent's ac tions in the world. We present a general Bayesian framework encompassing this view, and focus on how context can be exploited in plan recogni tion. We demonstrate the approach on a prob lem in traffic monitoring, where the objective is to induce the plan of the driver from observation of vehicle movements. Starting from a model of how the driver generates plans, we show how the highway context can appropriately influence the recognizer's interpretation of observed driver be havior.\n1 INTRODUCTION\nThe problem of plan recognition is to induce the plan of ac tion driving an agent's behavior, based on partial observa tion of its behavior up to the current time. Deriving the un derlying plan can be useful for many purposes-predicting the agent's future behavior, interpreting its past behavior, or generating actions designed to influence the plan itself. Researchers in AI have studied plan recognition for several kinds of tasks, including discourse analysis (Grosz & Sid ner, 1990), collaborative planning (Huber & Durfee, 1993 ), and adversarial planning (Azarewicz et at., 1989). These works have employed a great variety of reasoning tech niques, operating on similarly various plan representations and adopting varied assumptions about observability.\nThe common theme underlying these diverse motivations and approaches is that the object to be induced is a plan, and\nthat this plan is the cause of observed behavior. If there is anything special about the task of plan recognition as op posed to recognition in general, it must be due to special properties of plans: how they are constituted, and how they cause the behavior we observe and wish to predict, inter pret, and influence.\nIn this paper, we focus on one of these special properties the context in which the plan is generated-and how it can be exploited in the recognition process. W hereas most previous approaches have emphasized the relationship be tween plans and their observable effects,1 we argue that it is equally necessary to consider evidence that would bear on which plan would have been appropriate for the agent to generate. We demonstrate this point through an example application in traffic monitoring, where the interpretation of an individual vehicle's action depends on the surround ing highway context. Our techniques for reasoning about plan-generation context are based on Bayesian networks, as part of a general Bayesian framework for plan recognition. This contribution can be considered a variant extension of the model of Charniak and Goldman ( 1993), and of the ap proach advocated by Huber et al. (1994).\n2 PLAN RECOGNITION\n2.1 TOWARDS A GENERAL BAYESIAN FRAMEWORK\nOne of the aims of our work is to elucidate the fundamen tal elements of plan recognition, and to develop a general Bayesian framework for approaches to this task. Achiev ing generality is complicated by the diversity of represen tations for plans and techniques for plan generation; there fore, we present the framework at multiple levels of speci ficity. The most abstract specification is designed to accom modate most conceivable versions of plan recognition, and by introducing further distinctions we taxonornize the ap-\n1 Although, as we point out in the discussion below, several of these approaches can also accommodate the sort of context infor mation we are concerned with.\nproaches.\nThe framework for plan recognition is distinguished from uncertain reasoning in general by two special features of plans. First, plans are structured linguistic objects. Plan languages considered in AI research range from simple se quences of action tokens to general-purpose programming languages. In either case, the recognizer can and should ex ploit the structure of plans in inducing them from partial ob servations ofthe actions comprising the plan. Another way to say this is that plans are descriptions of action patterns, and therefore any general pattern-recognition technique is automatically a plan recognition technique for the class of plans corresponding to the class of patterns associated with the given technique.\nThe second special feature of plans is that they are ratio nal constructions. They are synthesized by a rational agent with some beliefs, preferences, and capabilities, that is, a mental state. Knowing the agent's mental state and its ra tionality properties strongly constrains the possible plans it will construct. (The degree of constraint depends on the power of the rationality theory we adopt.) The rational ori gin of plans is what distinguishes plan recognition from pat tern recognition. If the observations available include evi dence bearing on the beliefs, preferences, and capabilities of the agent, then the recognizer should combine this with evidence from the observed actions in reasoning about the entire plan.\nOur framework is Bayesian in that we start from a causal theory of how the agent's mental state causes its plan and executing its plan causes activity, and reason from observed effects to underlying causes. Our recognizer has uncertain a priori knowledge about the agent's mental state, the world state, and the world's dynamics, which can be summarized (at least in principle) by a probability distribution. It then makes partial observations about the world, and uses this evidence to induce properties of the agent and its plan.\nThe remainder of this section describes our framework in more detail. We demonstrate the utility of the framework by showing how extensions to the underlying conception of plans and planning generate corresponding extensions to plan recognition. Examples from our explorations of plan recognition in a highway traffic domain illustrate our appli cation of the framework to a concrete problem.\n2.2 PLANNING MODEL\nWe begin with a model of the planning agent operating in the world. As it begins planning, the agent has a certain mental state, consisting of its preferences (e.g., goals), be liefs (e.g., about the state of its environment), and capabil ities (e.g., available actions). We assume the actual plan ning process to be some rational procedure for generating the plan that will best satisfy the agent's preferences based\nContext in Plan Recognition 473\non its beliefs, subject to its capabilities. This plan then de termines (perhaps with some uncertainty) the actions taken by the agent in the world.\nMost plan-recognition work concentrates only on this last step, the relationship between a plan and the actions taken in the world. Typical approaches start from a representa tion of the possible plans, and prune the set of possibilities based on the actions observed. For example, Kautz ( 1986) connects plans and actions through event hierarchies, which place the plan at the top of a taxonomy of subplans and ac tions. Vilain ( 1990) presents a context-free grammar repre sentation of these event hierarchies as an alternative model. Lin and Goebel ( 1991) restrict the constraint language, per mitting use of a faster, specialized message-passing recog nition algorithm.\nGiven the reduced set of possible plans that could explain the observations, the plan recognizer must apply some pref erence criterion for choosing among them. For instance, Kautz's approach prefers explanations that involve fewer plans. The algorithm of Lin and Goebel prefers plan scenar ios that are more general. However, given two explanations containing the same number of plans, at the same levels of generality, neither algorithm has a basis for a choice either way. To borrow an example from Chamiak and Goldman, suppose we hear that Jack packed a bag and went to the air port. Depending on the exact event hierarchy, neither algo rithm may be able to decide whether Jack is in the process of taking a trip or conducting a terrorist bombing.\nThe average reader would probably not consider the latter possibility, since people are much more likely to take a trip than bomb an airplane. Charniak and Goldman account for this behavior in their recognition procedure by including prior probabilities on plans. This allows them to distinguish among equally possible, but unequally plausible explana tions for observed activity. The recognition model of Car berry (1990), based on the Dempster-Shafer theory of ev idential reasoning instead of Bayesian techniques, takes a similar approach by using threshold plausibility and differ ence levels of belief to distinguish among competing hy potheses. Similar distinctions could be supported in lin guistic approaches as well, perhaps based on probabilistic grammars (Wetherell, 1980).\n2.3 MENTAL STATE\nIn a particular case, we typically have information avail able to us that would augment these prior probabilities. For instance, we may know that Jack belongs to a terrorist or ganization, which would make the bombing explanation of his actions more plausible. To account for this sort of knowledge, the plan-recognition framework should accom modate all possible information about the agent's plan se lection process, beginning with its mental state. We can break down an agent's mental state into three distinct com-\n47 4 Pynadath and Wellman\nponents:\nBeliefs. The agent's knowledge of the state of the world and its dynamics. Beliefs may be incomplete, uncer tain, or incorrect.\nPreferences. The agent's desires about the world. These may be simple goals, or arbitrarily graded degrees of utility.\nCapabilities. The agent's self-model of its available ac tions. Strictly speaking, this should be knowledge of capabilities, but we stick to the more concise term.\nWe may have knowledge about any of these components of mental state. Looking back at Jack's situation, if we know that he belongs to a terrorist organization, then we might infer that his training included a lot of information about bombs, airport security, and other matters that are not widely known. Similarly, we may conclude that his goals are vastly different from those of a typical person going to the airport. For example, we may expect that Jack's goals include gaining worldwide attention for his group. Finally, his terrorist background may be such that he has a repertoire of available actions, such as conceal-bomb, beyond that of the vanilla air traveler.\nPlan selection also relies on the agent's beliefs about the current world state. For instance, if Jack knows that there is an important diplomat on an outgoing flight, then he prob ably believes that bombing that plane will generate even more attention for his organization. Notice that the world state affects plan selection only through the agent's beliefs. If Jack did not know about any diplomats, then the fact that they are present is irrelevant to his planning. By the same token, if Jack believes that a diplomat is on the plane even if none are present, it is his erroneous belief that we must consider.\n2.4 PLAN EXECUTION\nOnce we have accounted for the agent's plan-generation process, we need to consider the effects of the plan's exe cution. In many plan-recognition domains, the external ob server finds the agent's actions inaccessible. In such cases, the recognizer observes actions only indirectly, via their ef fects on the world (which themselves are typically only par tially observable). These restricted observations then form the basis of inference.\nThus, observations of the state of the world provide two types of evidence about the plan. First, as mentioned in Sec tion 2.3, the world influences the agent's initial mental state, which provides the context for plan generation. Second, changes in the world state reflect the effects of the agent's actions, which result from executing its plan.\n3 THE PLAN-RECOGNITION NETWORK\nTo perform plan recognition tasks, we generate a Bayesian network representing the causal planning model and use it to support evidential reasoning from observations to plan hypotheses. The structure of the Bayesian network is based on the framework depicted in Figure 1. That diagram can itself be viewed as a Bayesian network, albeit with rather broad random variables. To make this operational, we re place each component of the model with a subnetwork that captures intermediate structure for the particular problem. The limited connections among the subnetworks reflect the dependency structure of our generic planning model.\nTo illustrate this plan-recognition framework, consider the example problem of a driver on the highway, trying to pre dict the actions of the other drivers. Since these actions are normally limited to a small set of maneuvers (e.g. lane changes, passing, exiting), recognition of a driver's maneu vering plan would greatly assist in the prediction of future actions. To this end, we have worked on a probabilistic model of the maneuvers of a single car. We can then use this model to identify the current maneuver of an observed car and/or predict future actions, given only partial informa tion. The subnetwork descriptions below first present the general construction techniques and then provide a specific instantiation for this specific traffic domain.\n3.1 CONTEXT\nThe network, like the causal model, begins with the initial world state. We must include all possibly observable events that are relevant to formation-the process by which the agent's mental state is affected by the world. By including these events, the recognition procedure can take advantage of partial information about the agent's mental state. Note that even though the initial world state model may itself include inaccessible variables, the context subnetwork in cludes only those which are observable. However, we may wish to simplify the network by providing more compact intermediate results derived from inaccessible variables.\nOne of the motivations for maintaining a separate initial state subnetwork is to distinguish between our contextual observations and those of the agent. Therefore, we may have an unobservable node representing an aspect of the world state accessible to the agent, and an observable node representing a related feature accessible to us. The depen dency between these nodes is essentially a sensor model. If we are fortunate enough to have perfect sensors, then the context variables become redundant, since they will simply echo the values of the actual variables, and can be elimi nated.\nIn this model, the initial world state is defined as causally prior to all agent behavior. Therefore, the corresponding\nrandom variables can have links only from other such vari ables, representing dependencies within the state. Any de pendency links connecting a node from the initial state to any node outside this subnetwork must be directed to the outside node.\nThis treatment of context differs from the work of Huber et al. ( 1994), where the initial situation depends on the agent's mental state and not the other way around as it is here. This was possible given the planning model employed in that work, that of the Procedural Reasoning System (PRS) (In grand et al., 1992). In the PRS model, plan selection is a function of current goal and situation. Because these con text variables have no predecessors or substructure, the di rection of links can be reversed without changing the rest of the dependency structure. However, the agent's men tal state considered here may be more complex, especially in terms of its preference structure. Even if the agent has only simple goals, there are potential interactions among the goals that could affect the planning process. Hopefully, by following the causal structure in creating the network and placing the context prior to the plan, we can represent these interactions without greatly complicating the depen dency structure.\nIn the traffic domain, the driver must consider several as pects of the initial world state in rationally choosing a plan. First of all, the current position and speed of the car are im portant factors, and we assume that both are observable, to\nthe driver as well as to us. We also assume perfect sensors, but an extension to incorporate sensor noise is straightfor ward, as described above. The random variables x posi tion andy position of Figure 2 represent the car's lane po sition and distance from the highway's start, respectively. The driver can be in one of three lanes or may be off the highway, either preparing to enter or having just exited. The random variable y speed, denoting the car's speed, initially depends on the current node, since the farther left the lane, the faster the car is usually traveling.\nWe can also observe the presence of other cars around the driver of interest, who must consider them in choosing a maneuver. For instance, if there is a car blocking the driver's front, then a passing maneuver is more likely. We can observe any cars to the driver's immediate front, back, left, and right, as well as in the four diagonal directions. In the Bayesian network, the Boolean random variable left clr? represents the presence of any car to the immediate left of the driver. There are similar variables for the right, front, and back, as well as the four diagonal directions. The vari ables indexed tO in the first column of nodes in Figure 2 con stitute the context subnetwork.\n3.2 MENTAL STATE\nThe subnetwork representing the agent's beliefs about the world state must include random variables for all aspects of the context that the agent can observe and that factor\n476 Pynadath and Wellman\nFigure 2: Planning process subnetwork\ninto its decision-making. There may be some agent be liefs that are independent of any real-world variable. Unless we can observe these (perhaps through communication with the agent), there is no advantage in using additional random variables. Instead, we can fold the uncertainty in these be liefs into the plan subnetwork. However, agent beliefs will typically depend on the some aspect of the actual state of the world, although we can model the agent as being arbitrar ily uncertain or deluded. As mentioned in Section 3.1, this dependency represents the imperfection and/or incomplete ness of sensors. If the agent's sensors were perfect, then we could eliminate the nodes for the agent's belief variables, as they would take on the same values as the context variables.\nThe agent's knowledge of its capabilities is usually inde pendent of the world state, as are its preferences in most cases. Simple goals can be represented as separate Boolean variables, though it may be useful to combine a set of mu tually exclusive variables into a single variable with several possible values. More complex preference structures will require more complex subnetwork structures. The agent's capabilities can be represented in a similar fashion.\nThe model of agent formation is greatly simplified in our traffic domain. Because of our assumption of perfect sen sors, the driver's beliefs about the world correspond to the actual values in our simplified model. In addition, the agent's beliefs about its capabilities are not represented ex plicitly in our traffic network. Instead, the driver is as sumed to know all of the possible plans (as described in Section 3.3.1). The planning process also assumes that the driver has complete knowledge of how the plans can best satisfy its preferences in the current context. Thus the plan selection mechanism implicitly represents the driver's be-\nliefs about its capabilities.\nWe model the driver preferences with two goals. First, a driver has the explicit goal of getting from one exit to an other, though the intended exits are unknown to an exter nal observer. The random variable exit position in Figure 2 represents the driver's desired exit. All of the possible exit positions are farther along the highway than the values of y position. If this were not the case, then the current position would provide evidence that the desired exit is probably not one that has been passed. Therefore, there would be a de pendency, but to simplify the network, we make the sets of y and exit positions disjoint.\nSecond, there may be some constraint on the travel time between these exits, or the driver might have some target speed which is preferred for the duration of travel. How ever, we can usually translate the former into a desired speed because of the fixed positions of the exits. Therefore, our model uses only the random variable target y speed in Figure 2, with its values clustered around the speed limit. If the car has been on the highway for enough time, then its current speed should provide some clue as to the driver's target speed. We could model this with a link from y speed. On the other hand, if we have been observing the car and its maneuvers for some time, then these past observations should provide more conclusive evidence as to its target speed. Thus, we can make the target speed independent of current speed and encode our past observations in the prior probabilities.\nThis network also contains the intermediate belief random variables, at exit? and at target?, in the second column of nodes in Figure 2. These reflect the driver's belief about the proximity of the desired exit and the desirability of the current speed, respectively. The at exit? variable depends only on the current position and the preferred exit, and is true only when the former is immediately before the latter. The at target? variable depends only on the current and preferred speeds, and its value indicates whether the current speed is too slow, too fast, or just right, with respect to the driver's desired cruising speed.\n3.3 PLANNING PROCESS\n3.3.1 Plan Variables\nThe plan subnetwork is comprised of random variables col lectively representing the current plan. For instance, in Kautz's event hierarchies, there is a taxonomy of plans and actions. The children of a certain plan correspond to pos sible subplans or actions, while other links indicate neces sary components. If our planning model is based on such event hierarchies, we may designate one Boolean variable corresponding to each element in the taxonomy, indicating the presence of the corresponding plan. Or we may com bine certain mutually exclusive subplans into a single ran-\ndom variable, which takes on a different value depending on the actual subplan present.\nSuch hierarchies are based on the subsumption relation, re quiring a dependency link from the more general node to the more specific. The conditional probability table can repre sent the distribution of the specific values, given the gen eral. In particular, because of the subsumption relation, we can set the conditional probability of a child node given that its parent node is false to zero.\nIn the traffic domain, we can classify driving maneuvers ac cording to the lane changes involved. The simplest plan is to simply continue driving in the same lane. At the next level of complexity, a driver can shift one lane to the left or right. We consider entering and exiting the highway as specific instances of these one-lane shifts. The driver could also shift two lanes to the left or right, where this could again involve entering or exiting the highway. As a final option, the driver may choose a passing maneuver, which we view as two successive lane shifts of opposite direction. In Figure 2, the variable gen maneuver represents the gen eral driving maneuver and takes on a value corresponding to the chosen plan.\nWe can also classify driving plans according to the acceler ation. Depending on the current and desired speed, a driver may decide to speed up, slow down, or maintain current speed, indicated by the value of the variable ace maneuver of Figure 2. The acceleration maneuver depends on the lane maneuver if we do not consider the plan selection mecha nism. For instance, a deceleration is more likely as a part of a right lane change plan than as a part of a plan to pass. However, the two variables are independent given the ini tial context, as indicated in the network.\nThe variable spec pass in Figure 2 indicates the direction of the pass, if one is taking place. Since passing in a spe cific direction is a subplan of the general passing maneuver which gen maneuver can represent, this is an example of the subsumption relation found in event hierarchies. If the driver decides to pass, there are the options of passing on the left and passing on the right. And even if the driver chooses to pass, there may be cars blocking both lanes, forcing the driver to wait for another opportunity to pass. This variable clearly depends on gen maneuver, since the more general passing maneuver is its parent and the conditional proba bility table represents a subsumption relation as described above. In other words, if a passing maneuver is not cho sen, then spec pass will be neither pass on left nor pass on right.\n3.3.2 Plan Selection\nLinks from the agent's mental state into the plan subnet work represent the agent's planning process. For hierarchi cal planning, we start with the most general plan nodes and proceeding to the most specific, determine which aspects of\nContext in Plan Recognition 477\nthe mental state influence the agent's choice. For instance, suppose the agent's decision-making procedure consists of a set of condition-action rules. Then, any plan choices in the action portion of a rule depend on all of the context variables that appear in the conditions of the rule. By con necting only parts of the mental state relevant to particular choices, we keep the dependency structure as simple as pos sible.\nWe must then specify the conditional probabilities of the plan variables given the relevant aspects of the agent's men tal state. If the agent is a deterministic planner, then the con ditional probability given a particular mental state instanti ation will be 1 for a single instantiation of the plan subnet work and 0 for all others. For nondeterministic planners, we must determine the conditional probabilities from whatever agent model we have.\nIf in fact we have no opportunity to observe anything about the initial world state or the agent's mental state, then we may collapse the initial state and mental state subnetworks into prior probabilities for the top-level plan variables. The plan recognition networks (PRNs) of Charniak and Gold man ( 1993) use such priors to model the agent's plan selec tion process. These prior probabilities represent the same distribution as the explicit planning process subnetwork, but since the initial nodes are unobservable, we can merge the nodes into the plan subnetwork without losing informa tion.\nWe can now model a driver's plan selection with some re liability. In our Bayesian network, the conditional proba bility table must specify the likelihood of certain maneu vers under every possible combination of world situation and driver mental state. Under most situations, there will be one maneuver that is clearly preferable, though there is still uncertainty. For example, suppose that the driver is cur rently traveling below the desired speed and that there is an other car directly in front while the lane to the left is clear. Then it is likely that driver will pass the car on the left. The complete plan selection subnetwork is shown in Figure 2. This model of the driver's decision process is based in part on the driving model underlying the BATmobile (Bayesian Automated Taxi) project, described by Forbes, et al. ( 1995).\nThe acceleration maneuver depends only on the preferabil ity of the current speed. Thus the sole link to ace maneu ver is from at target?. If the driver is at the target speed, then the current speed will be maintained. If the current speed is too low, then the driver will choose an acceleration maneuver. Likewise, if the current speed is too fast, then a deceleration maneuver will be chosen.\nThe lane change maneuver also depends on the preferability of the current speed. For instance, a car traveling at its target speed is unlikely to change lanes. However, there are other factors in the initial world state to consider. Obviously, the current lane is important, since a car in the leftmost lane\n478 Pynadath and Wellman\ncannot change lanes to the left. In addition, the driver will consider any cars to the front or back. If there is a car block ing the front and the driver's current speed is too low, then a simple acceleration could cause a collision. The driver may instead choose to change lanes to the left. But a decision to change lanes must also consider the presence of cars to the driver's left or right, or any cars coming up from the back left or right. The links to the gen maneuver node represent these dependencies.\nIf the driver decides to pass, a direction must be chosen. Passing on the left is preferable to passing on the right, but the current situation may not allow it. For instance, any cars to the driver's left or to the front left could block the passing attempt. The same is true on the right side. If enough pass ing avenues are blocked, then the driver may decide to delay the passing attempt or to perform the initial lane change and wait to complete the pass.\n3.3.3 Agent Communication\nModeling agent communication depends greatly on the spe cific protocol adopted, and the relationship between the ob server and the observed. If a trusted agent directly an nounces particular aspects of its planning process, then we could simply instantiate the corresponding variables. Other types of communication would require nodes to represent beliefs we attribute to the agent, based on its communica tion actions. Note that we are not modeling here the planned character of communication acts; to do so we would treat them as we do actions in general.\nThe only communication allowed in our traffic model is through the driver's tum indicator, which provides a sim ple mechanism for a driver to announce the intended lane change. The variables signal mx? in the fourth column of Figure 2 represent the state of the driver's tum signal dur ing stage x of the maneuver. Clearly, both the general ma neuver and the specific direction of any passing attempt in fluence any signal. For instance, when performing a left lane change, signal mO? is likely to take the value Left and signal m1? the value Off. Of course, many drivers fail to signal their maneuvers, and sometimes they signal erroneously. These possibilities are considered when deter mining the conditional probability tables. However, drivers are usually consistent in their signaling habits. For instance, when performing a pass on the left, someone who fails to signal the initial left lane change is unlikely to signal the subsequent right change. The link between the two signal variables represents this consistency.\n3.4 PLAN EXECUTION\nThe agent's plan execution process is reflected in the model by dependencies from its plan subnetwork to another sub network describing its activity. This is analogous to links in event hierarchies connecting plans to their component ob-\nFigure 3: Plan execution subnetwork\nservable actions. In PRS (Ingrand et al., 1992; Lee et al., 1994), Knowledge Areas (KAs) specify a sequence of ac tions associated with a plan, corresponding to links from the plan node to corresponding action nodes. Either of these can be cast in Bayesian networks, representing the likeli hood of the component's appearance given the plan in the conditional probability table for that node.\nAll of these methods for modeling the dependency of the agent's activity on its plan are acceptable. We require only that the agent's activity be conditionally independent of the initial world state and the agent's mental state given the plan. That is, we assume that the plan is a sufficient speci fication of activity.\nThe activity subnetwork in the traffic model includes the in dividual transitions in lane and speed, which are completely unobservable. At each step, the driver can change one lane to the left or right, or remain in the same lane. The driver can also increase, decrease, or maintain speed. All of the plans we consider produce a two-step action sequence. For instance, a plan to shift one lane to the left produces a left lane change followed by a \"remain in lane\" act. The lat act mx variables in Figure 3 represent the lane changes at step x, while fwd act mx represents the acceleration at step x.\nOur definition of the lane maneuvers completely determines the lane changes of the action sequences. The individual shifts depend on the general lane maneuver, as well as on the specific passing plan, but not on the acceleration maneu ver. Likewise, the individual accelerations are independent of the general lane changes and the specific passing maneu vers if given the overall acceleration plan.\n3.5 WORLD DYNAMICS\nThe relationship between the observed and actual actions of the agent is similar to that of the observed and actual world states. If we have perfect sensors, we do not need a separate observed activity subnetwork; otherwise, we have to model sensor noise in the links from the actual nodes.\nIn some cases, the agent's activity is completely inaccessi-\nble, though we might still be able to observe effects of this activity. These effects are dictated by the dynamics of our world, which specify how the agent's actions alter the sit uation. Therefore, we must model how subsequent world states depend on the initial world state and the agent's ac tivity. It is possible that a world state depends on the entire world history, but if the the plan is sufficiently structured (e.g., sequential actions) then we may be able to simplify this dependency. If we express the effects model in accord with standard AI approaches, we can restrict the effects to depend only on background and direct effects and, given these, to be conditionally independent of the plan itself, as well as further removed activity and indirect effects (Well man, 1990).\nWe can make effects conditionally independent of future ac tions and effects simply by ensuring that links never point backward in time, but this could make actions dependent on past world states. So far, we have had links move from plans to activity and from activity to effects, so adding links in the opposite direction would go against the flow in Fig ure 1. If, as described above, the plan is sufficient for deter mining activity, the current action is conditionally indepen dent of the previous world states given the current plan, as well as the actions performed so far.\nDepending on our domain, we may able to make a Markov assumption with respect to activity and the effects. In such cases, the current action would be conditionally indepen dent of actions more than one time step back in the action sequence, given the current plan and the action immediately previous. If the effects have a similar property, they should not depend on any world states or actions more than one step previous. Although this would greatly localize the de pendencies, this may not always be possible, depending on the types of observations available and the set of state vari ables in the model.\nSince there is no directly observable activity in the traffic model, most of our inference will come from observed ef fects. We must now model the dynamics of the traffic world, beginning with the changes in the position and speed of the car. We can view the actions of the driver to be transitions between world states. To simplify the model, we ignore ob servations taking place while the driver is performing an ac tion. Thus, evidence is available only at the completion of a component action, and there are three stages of observable variables, including the context, as can be seen in Figure 4.\nFinally, we must define the dependencies of these effects. Most of the observable variables depend on the driver's pre vious action, as well as their own previous values. For instance, the driver's lane is completely determined if we know what lane change just took place, as well as the lane value just before the change. Likewise, the driver's speed depends on the previous speed and whatever acceleration action took place, although this is clearly not a determinis-\nContext in Plan Recognition 479\nFigure 4: Observation subnetwork\ntic relationship.\nThe presence of other cars is a bit more complex, due to the driver's movements. For instance, after a left change, a car that was to the front and left is now probably directly in front. But if the driver stays in the same lane, then we must check whether there was a car blocking the front in the previous world state. Therefore, each clearance vari able depends on the previous action, as well as all relevant clearance variables from the previous state. To simplify the network, we ignore the presence of other cars in the evi dence. We do consider them when modeling plan selection, but since the driver's actions do not directly affect the other drivers' positions, we ignore these effects. As with the con text, we assume perfect sensors, so there is no distinction between the actual and observed effects.\n3.6 PLAN RECOGNITION\nOnce we have created the belief network, we can per form recognition tasks by fixing any observed variables and querying the network about the relevant variables. We re ceive evidence only about the variables in the bottom half of Figure I, though, as described before, these may corre spond exactly to actual variables in the planning model.\nOnce we fix the values of the known variables in the net work, we can propagate the information throughout the net work and observe the posterior probabilities at the nodes of interest. For instance, we may be interested in determining the plan chosen by the agent, in which case we would ex amine the nodes in the plan subnetwork. Alternatively, we can predict future agent activity or effects by examining the probabilities Of those variables.\nOnce we have constructed the entire traffic maneuver net work, shown in Figure 5, we can handle plan recognition in a wide range of useful driving situations. For instance, sup pose we are trying to predict the behavior of the car behind us as we are driving in the middle lane of a three-lane high way. We observe the car move into the rightmost lane, and we want to determine if it is passing us, or preparing to exit,\nor perhaps simply moving into the slower-moving lane.\nThus, in the context, we have observed front clr? tO to be false and x position tO to be the middle lane. The only ob served effect is that x position t1 is the right lane. If we want to infer the driver's plan, we can examine the gen ma neuver? node to see that the posterior probability of a one lane right shift is 0.64, while that of a pass is 0.35. The for mer is more plausible since we assume that drivers prefer to pass on the left-hand side, so passing on the right has a rela tively low prior probability. The only remaining maneuver with nonzero probability is an exit. All of the other plans have zero probability, since the observed change in lanes vi olates their definitions.\nIf we are not interested in the driver's plan, but only in the future lane position, then we can examine the x position t2 node. The posterior probability that the car will still be in the right lane is 0.65, while the probability that it will move to the middle lane is 0.34. The difference between these be liefs and that of the maneuvers arises from the nature of the passing maneuver. Even if the car decides to pass, it may not be able to do so immediately do to surrounding cars. In such a case, it will remain in its current lane until it can com plete the maneuver. Thus, there is a slight probability that the car will stay in the right lane even if the driver has de cided to pass.\nGiven no other contextual observations, it is reasonable to predict that the car will remain in the right lane. However, if we also observed that there was another car to our left, thus blocking the car behind us from passing on the left, we can\ninstantiate the frontL clr? tO variable to be false. Repeat ing our observation of the nodes of interest, we find that the posterior probability that the car is passing has increased to 0.53, while that for the car simply shifting one lane to the right has dropped to 0.46. The probabilities for x position t2 have changed as well, to 0.51 and 0.48 respectively. If we made our final decisions based simply on maximum proba bilities, we would predict that the car was passing us. No tice that, without knowing about the car to our left, our pre diction would be that the car was not passing, but the obser vation of that aspect of the context changes our belief.\nThus, we are able to perform valuable inference with only a limited subset of the possible observations. If we were to also observe that there were no other cars nearby, other than those already considered, then we could instantiate the re maining clearance context variables to be true. Doing so in creases the posterior probability that the maneuver is a pass to 0.6 1, while decreasing that for a one-lane right shift to 0.39.\n4 CONCLUSION\nThe traffic application presented above illustrates several aspects of our plan-recognition framework, highlighting the importance of accounting for context. Our assumption of rationality on the part of the agent allowed us to model the relationship between an agent's plan and its mental state. By modeling a driver's decision process, observations of the initial state provided strong evidence about the result ing plan. We were also able to model plan execution in a\nmanner similar to other approaches to recognition. The re sulting network was able to perform useful inference, even when given only partial observations.\nAlthough the traffic example is a very specific domain, we believe that the general structure of Figure 1 is applicable to a broad class of plan-recognition tasks. Even with our restrictive assumptions, the network captures an extensive model of planning behavior. The driver observes the world, generates a plan, performs a sequence of actions, and these actions produce changes in the world state. To summarize, we have augmented the common plan execution model em ployed in recognition to include plan formulation, and the result is encouraging.\nHowever, the generality and scalability of our framework remains to be seen. The driver we considered had two goals, an intended exit and a target driving speed. Other drivers, and agents in other domains, will most likely have more complex preferences and a more complex decision process. The decision process may involve a more elaborate plan ning theory, which may be difficult to capture in our model. In addition, the major issue of communication is as yet un explored within our model. In future work, we intend to push on these issues, by increasing the scale and complexity of the underlying process we are modeling.\nAcknowledgments\nThis work was supported in part by Grant F49620-94-10027 from the Air Force Office of Scientific Research, and by the Transportation Research Board IVHS-IDEA Pro gram. Marcus J. Huber provided many helpful suggestions in discussions of this work. The authors also thank the anonymous reviewers for their comments on the exposition.\nReferences\nAzarewicz, J., Fala, G., & Heithecker, C. 1989. Template based multi-agent plan recognition for tactical situation as sessment. In Proceedings of the Sixth Conference on Arti ficial Intelligence Applications, 247-254.\nCarberry, S. 1990. Incorporating default inferences into plan recognition. In Proceedings of AAAI-90, 471-478, Boston, MA. The MIT Press.\nCharniak, E., & Goldman, R. P. 1993. A Bayesian model of plan recognition. Artificial Intelligence, 64(1), 53-79.\nForbes, J., Huang, T., Kanazawa, K., & Russell, S. 1995. The BATmobile: Towards a Bayesian automated taxi. To appear in IJCAI-95.\nGrosz, B. J., & Sidner, C. L. 1990. Plans for discourse. In Cohen, P. R., Morgan, J., & Pollack, M. E. (eds), Intentions in Communication, chapter 20, 417-444. MIT Press, Cam bridge, MA.\nContext in Plan Recognition 481\nHuber, M. J., & Durfee, E. H. 1993. Observational uncer tainty in plan recognition among interating robots. In Work ing Notes: Workshop on Dynamically Interacting Robots, 68-75, Chambery, France.\nHuber, M. J., Durfee, E. H., & Wellman, M.P. 1994. The automated mapping of plans for plan recognition. In Pro ceedings of the Tenth Conference on Uncertainty in Artifi cial Intelligence, 344-351.\nIngrand, F. F., Georgeff, M.P., & Rao, A. S. 1992. An ar chitecture for real-time reasoning and system control. IEEE Expert, 7(6), 34-44.\nKautz, H. A., & Allen, J. F. 1986. Generalized plan recog nition. In Proceedings of AAAI-86, 32-37.\nLee, J., Huber, M. J., Durfee, E. H., & Kenny, P. G. 1994. UM-PRS: An implementation of the procedural reasoning system for multirobot applications. In Proceedings of the AIAAINASA Conference on Intelligent Robotics in Field, Factory, Service, and Space, 842-849.\nLin, D., & Goebel, R. 1991. A message passing algorithm for plan recognition. In Proceedings of the Twelfth Interna tional Joint Conference on Artificial Intelligence, 280-285.\nVilain, M. 1990. Getting serious about parsing plans: A grammatical analysis of plan recognition. In Proceedings of AAAI-90, 190-197, Boston, MA. The MIT Press.\nWellman, M.P. 1990. The STRIPS assumption for plan ning under uncertainty. In Proceedings of the National Con ference on Artificial Intelligence, 198-203, Boston, MA. AAAI.\nWetherell, C. S. 1980. Probabilistic languages: a review and some open questions. Computing Surveys, 12(4), 361- 379."
    } ],
    "references" : [ {
      "title" : "Incorporating default inferences",
      "author" : [ "S. Carberry" ],
      "venue" : null,
      "citeRegEx" : "Carberry,? \\Q1990\\E",
      "shortCiteRegEx" : "Carberry",
      "year" : 1990
    }, {
      "title" : "A Bayesian model of plan recognition",
      "author" : [ "P." ],
      "venue" : "Artificial Intelligence, 64(1), 53-79. Forbes, J., Huang, T., Kanazawa, K., & Russell, S. 1995. The BATmobile: Towards a Bayesian automated taxi. To appear in IJCAI-95.",
      "citeRegEx" : "P.,? 1993",
      "shortCiteRegEx" : "P.",
      "year" : 1993
    }, {
      "title" : "Plans for discourse",
      "author" : [ "B.J. Grosz", "C.L. Sidner" ],
      "venue" : null,
      "citeRegEx" : "Grosz and Sidner,? \\Q1990\\E",
      "shortCiteRegEx" : "Grosz and Sidner",
      "year" : 1990
    }, {
      "title" : "Observational uncer­ tainty in plan recognition among interating robots",
      "author" : [ "M.J. Huber", "E.H. Durfee" ],
      "venue" : "In Work­ ing Notes: Workshop on Dynamically Interacting Robots,",
      "citeRegEx" : "Huber and Durfee,? \\Q1993\\E",
      "shortCiteRegEx" : "Huber and Durfee",
      "year" : 1993
    }, {
      "title" : "The automated mapping of plans for plan recognition",
      "author" : [ "M.J. Huber", "E.H. Durfee", "M.P. Wellman" ],
      "venue" : "In Pro­ ceedings of the Tenth Conference on Uncertainty in Artifi­ cial Intelligence,",
      "citeRegEx" : "Huber et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Huber et al\\.",
      "year" : 1994
    }, {
      "title" : "An ar­ chitecture for real-time reasoning and system control",
      "author" : [ "F.F. Ingrand", "M.P. Georgeff", "A.S. Rao" ],
      "venue" : "IEEE Expert,",
      "citeRegEx" : "Ingrand et al\\.,? \\Q1992\\E",
      "shortCiteRegEx" : "Ingrand et al\\.",
      "year" : 1992
    }, {
      "title" : "Generalized plan recog­ nition",
      "author" : [ "H.A. Kautz", "J.F. Allen" ],
      "venue" : "In Proceedings of AAAI-86,",
      "citeRegEx" : "Kautz and Allen,? \\Q1986\\E",
      "shortCiteRegEx" : "Kautz and Allen",
      "year" : 1986
    }, {
      "title" : "UM-PRS: An implementation of the procedural reasoning system for multirobot applications",
      "author" : [ "G.P." ],
      "venue" : "Proceedings of the AIAAINASA Conference on Intelligent Robotics in Field, Factory, Service, and Space, 842-849.",
      "citeRegEx" : "P.,? 1994",
      "shortCiteRegEx" : "P.",
      "year" : 1994
    }, {
      "title" : "A message passing algorithm for plan recognition",
      "author" : [ "D. Lin", "R. Goebel" ],
      "venue" : "In Proceedings of the Twelfth Interna­ tional Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "Lin and Goebel,? \\Q1991\\E",
      "shortCiteRegEx" : "Lin and Goebel",
      "year" : 1991
    }, {
      "title" : "The STRIPS assumption for plan­ ning under uncertainty",
      "author" : [ "M.P. Wellman" ],
      "venue" : "Proceedings of the National Con­ ference on Artificial Intelligence, 198-203, Boston, MA. AAAI.",
      "citeRegEx" : "Wellman,? 1990",
      "shortCiteRegEx" : "Wellman",
      "year" : 1990
    }, {
      "title" : "Probabilistic languages: a review and some open questions",
      "author" : [ "C.S. Wetherell" ],
      "venue" : "Computing Surveys, 12(4), 361379.",
      "citeRegEx" : "Wetherell,? 1980",
      "shortCiteRegEx" : "Wetherell",
      "year" : 1980
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "In this paper, we focus on one of these special properties­ the context in which the plan is generated-and how it can be exploited in the recognition process. W hereas most previous approaches have emphasized the relationship be­ tween plans and their observable effects,1 we argue that it is equally necessary to consider evidence that would bear on which plan would have been appropriate for the agent to generate. We demonstrate this point through an example application in traffic monitoring, where the interpretation of an individual vehicle's action depends on the surround­ ing highway context. Our techniques for reasoning about plan-generation context are based on Bayesian networks, as part of a general Bayesian framework for plan recognition. This contribution can be considered a variant extension of the model of Charniak and Goldman ( 1993), and of the ap­ proach advocated by Huber et al. (1994).",
      "startOffset" : 8,
      "endOffset" : 912
    }, {
      "referenceID" : 10,
      "context" : "Similar distinctions could be supported in lin­ guistic approaches as well, perhaps based on probabilistic grammars (Wetherell, 1980).",
      "startOffset" : 116,
      "endOffset" : 133
    } ],
    "year" : 2011,
    "abstractText" : "Typical approaches to plan recognition start from a representation of an agent's possible plans, and reason evidentially from observations of the agent's actions to assess the plausibility of the various candidates. A more expansive view of the task (consistent with some prior work) ac­ counts for the context in which the plan was gen­ erated, the mental state and planning process of the agent, and consequences of the agent's ac­ tions in the world. We present a general Bayesian framework encompassing this view, and focus on how context can be exploited in plan recogni­ tion. We demonstrate the approach on a prob­ lem in traffic monitoring, where the objective is to induce the plan of the driver from observation of vehicle movements. Starting from a model of how the driver generates plans, we show how the highway context can appropriately influence the recognizer's interpretation of observed driver be­ havior.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}