{
  "name" : "1608.04808.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning Latent Local Conversation Modes for Predicting Community Endorsement in Online Discussions",
    "authors" : [ "Hao Fang", "Hao Cheng", "Mari Ostendorf" ],
    "emails" : [ "hfang@uw.edu", "chenghao@uw.edu", "ostendorf@uw.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Online discussion forums provide a platform for people with shared interests (online communities) to discuss current events and common concerns. Many forums provide a mechanism for readers to indicate positive/negative reactions to comments in the discussion, with up/down votes, “liking,” or indicating whether a comment is useful. The cumulative reaction, which we will refer to as “community endorsement,” can be useful to readers for prioritizing what they read or in gathering information for decision making. This paper introduces the task of automatically predicting the level of endorsement of a comment based on the response structure of the discussion and the text of the comment. To address this task, we introduce a neural network architecture that learns latent discussion structure (or, conversation) modes and adjusts the relative dependence on\ntext vs. structural cues in classification. The neural network framework is also useful for combining text with the disparate features that characterize the submission context of a comment, i.e. relative timing in the discussion, response structure (characterized by graph features), and author indexing.\nThe idea of conversation modes stems from the observation that regions of a discussion can be qualitatively different: low vs. high activity, many participants vs. a few, etc. Points of high activity in the discussion (comments that elicit many responses) tend to have higher community endorsement, but some points of high activity are due to controversy. We hypothesize that these cases can be distinguished by the submission context, which we characterize with a vector of graph and timing features extracted from the local subgraph of a comment. The context vectors are modeled as a weighted combination of latent basis vectors corresponding to the different modes, where bases are learned using the weak supervision signal of community endorsement. We further hypothesize that the nature of the submission context impacts the relative importance of the actual text in a comment; hence, a mode-dependent gating mechanism is introduced to weight the contribution of text features in estimating community endorsement.\nThe model is assessed in experiments on Reddit discussion forum data, using karma (the difference in numbers of up and down votes) as a proxy for community endorsement, showing benefits from both the latent modes and the gating. As described further below, the prediction task differs somewhat from prior work on popularity prediction in two respects. First, the data is not constrained to control\nar X\niv :1\n60 8.\n04 80\n8v 2\n[ cs\n.S I]\n2 8\nSe p\n20 16\nfor either submission context or comment/post content, but rather the goal is to learn different context modes that impact the importance of the message. Second, the use of the full discussion thread vs. a limited time window puts a focus on participant interaction in understanding community endorsement."
    }, {
      "heading" : "2 Related Work",
      "text" : "The cumulative response of readers to social media and online content has been studied using a variety of measurements, including: the volume of comments in response to blog posts (Yano and Smith, 2010) and news articles (Tasgkias et al., 2009; Tatar et al., 2011), the number of Twitter shares of news articles (Bandari et al., 2012), the number of reshares on Facebook (Cheng et al., 2014) and retweets on Twitter (Suh et al., 2010; Hong et al., 2011; Tan et al., 2014; Zhao et al., 2015), and the difference in the number of reader up and down votes on posts and comments in Reddit discussion forums (Lakkaraju et al., 2013; Jaech et al., 2015). An advantage of working with the Reddit data is that both positive and negative reactions are accounted for, so the total (karma in Reddit) is a reasonable proxy for community endorsement.\nFor all the different types of measures, a challenge in predicting the cumulative reaction is that the cases of most interest are at the tails of a Zipfian distribution. Various prediction tasks have been proposed with this in mind, including regression on a log score (Bandari et al., 2012), classification into 3-4 groups (e.g. none, low, high) (Tasgkias et al., 2009; Hong et al., 2011; Yano and Smith, 2010), a binary decision as to whether the score will double given a current score (Lakkaraju et al., 2013), and relative ranking of comments (Tan et al., 2014; Jaech et al., 2015). In our work, we take the approach of classification, but use a finer grain quantization with bins automatically determined by the score distribution.\nThe work on cumulative reaction has mostly considered two different scenarios: predicting responses before a comment/document has been published vs. after a limited lookahead time for extracting features based on the initial response. While the framework proposed here could handle either scenario, the experiments reported allow the classifier to use a longer future window, until most of the discussion\nhas played out. This provides insight into the difficulty of the task and illustrates that volume of responses alone does not reliably predict endorsement.\nA few studies investigate language factors that may impact popularity through carefully controlled experiments. To tease apart the factor of content quality, Lakkaraju et al. (2013) predict resharing of duplicated image submissions, investigating both the submission context (community, time of day, resubmission statistics) and language factors. Our work differs in that content is not controlled and the submission context includes the response structure and relative timing of the comment within the discussion. Tan et al. (2014) futher control the author and temporal factors in addition to the topic of the content, by ranking pairs of tweets with almost identical content made by the same author within a limited time window. Jaech et al. (2015) control the temporal factor for ranking Reddit comments made in a time-limited window and study different language factors. Here, rather than manually controlling the submission context, we propose a model to discover latent modes of submission context (relative timing, response structure) and analyze its utility in predicting community endorsement. Furthermore, we study how the usefulness of language information in estimating the community endorsement varies depending on submission context."
    }, {
      "heading" : "3 Data and Task",
      "text" : "Data: Reddit (https://www.reddit.com) is a discussion forum with thousands of subcommunities organized as subreddits. Users can initiate a tree-structured discussion thread by making a post in a subreddit. Comments are made either directly to the root post or to other comments within the thread, sometimes triggering sub-discussions. Each comment can receive upvotes and downvotes from registered users; the difference is shown as the karma score beside the comment. The graph structure of a Reddit disccussion thread is shown in\nTask: In many discussion forums, including the those explored here, community endorsement (i.e., karma in Reddit) has a heavy-tailed Zipfian distribution, with most comments getting minimal endorsement and high endorsement comments being rare. Since the high endorsement comments are of most interest, we do not want to treat this as a regression problem using a mean squared error (MSE) objective.2 Instead, we quantize the karma into J + 1 discrete levels and design a task consisting of J binary classification subtasks which individually predict whether a comment has karma of at least level-j for each level j = 1, . . . , J given the text of the comment and the structure of the full discussion thread. (All samples have karma at least level-0.)\nKarma scores are quantized into 8 levels of community endorsement according to statistics computed over a large collection of comments in the subreddit. The quantization process is similar to the head-tail break rule described in (Jiang, 2013). First, comments with karma no more than 1 are labeled as level-0, indicating that these comments receive no more upvotes than downvotes.3 Then, we compute the median karma score for the rest of the comments, and label those with below-than-median karma as level-1. This process is repeated through level-6, and the remaining comments are labeled as\n2A prediction error of 50 is minimal for a comment with karma of 500 but substantial for a comment with karma of 1, and the low karma comments dominate the overall MSE.\n3The inital karma score of a comment is 1.\nlevel-7. The resulting data distributions are shown in Fig. 2. Note that the quantization is subreddit dependent, since the distribution and range of karma tends to vary for different subreddits.\nEvaluation metric: Since we use a quantization scheme following a binary thresholding process, we can compute the F1 score for each level-j subtask (j = 1, 2, . . . , 7) by treating comments whose predicted level is lower than j as negative samples and others as positive samples. To evaluate the overall prediction performance, the seven F1 scores are aggregated via a macro average, which effectively puts a higher weight on the higher endorsement levels."
    }, {
      "heading" : "4 Model Description",
      "text" : "The proposed model utilizes two kinds of information for a comment to predict its quantized karma: (1) the submission context encoded by a set of graph and timing statistics, and (2) the textual content of the comment itself. Both sources of information are first embedded in a continuous space by a neural network as illustrated in Fig. 3, where c ∈ RC and d ∈ RD encode the submission context and the textual content, respectively. As described further below, the two vectors are transformed for use in the final decision function to c̃, a linear combination of latent basis vectors, and d̃, a context-dependent weighted version of the text features.\nSubmission context modes: Reddit discussions have a variety of conversation structures, including sections involving many contributors or just a few. Based on observations that high karma com-\nments seem to co-occur with active points of discussions, we identify a set of features to represent the submission context of a comment, specifically aiming to characterize relative timing of the comment within the discussion, participant response to the comment, and whether the comment author is the original poster (see Table 1 for the full list). The features are normalized to zero mean and unit variance based on the training set.\nIn this paper, instead of controlling for the submission context, we let the model learn latent modes of submission context and examine how the learned context modes relate to different levels of community endorsement. The proposed model learns K latent basis vectors b1, · · · ,bK ∈ RC for characterizing the submission context of a particular comment in the discussion. Given the raw submission context feature vector x ∈ RN , the model computes a vector c ∈ RC as c = LReL(Px), where P ∈ RC×N is a projection matrix, and LReL(·) is the leaky rectified linear function (Mass et al., 2013) with 0.1 as the slope of the negative part. Coefficients for these\nK latent bases are then estimated as\nak = softmax(v T tanh(U [c; bk])),\nwhere v ∈ RC and U ∈ RC×2C are parameters to be learned. The final submission context embedding is obtained as c̃ = ∑K k=1 ak · bk ∈ RC .\nThe computation of basis coefficients is similar to the attention mechanism that has been used in the context of machine translation (Bahdanau et al., 2015), constituency parsing (Vinyals et al., 2015), question answering and language modeling (Weston et al., 2015; Sukhbaatar et al., 2015). To the best of our knowledge, this is the first attempt to use the attention mechanism for latent basis learning.\nText embeddings: Recurrent neural networks (RNNs) have been widely used to obtain sequence embeddings for different applications in recent years (Sutskever et al., 2014; Cheng et al., 2015; Palangi et al., 2016). In this paper, we use a bidirectional RNN to encode each sentence, and concatenate the hidden layers at the last time step of each direction as the sentence embedding. For comments with multiple sentences, we average the sentence embeddings into a single vector as the textual content embedding d ∈ RD.\nFor the t-th token in a sentence, the hidden layers of the bi-directional RNN are computed as\nh (l) t = GRU(zt,h (l) t−1), h (r) t = GRU(zt,h (r) t+1),\nwhere zt ∈ RD is the token input vector, h(l)t ∈ RD and h(r)t ∈ RD are the hidden layers for the leftto-right and right-to-left directions, respectively, and\nGRU(·, ·) denotes the gated recurrent unit (GRU), which is proposed by Cho et al. (2014) as a simpler alternative to the long short-term memory unit (Hochreiter and Schmidhuber, 1997) for addressing the vanishing gradient issue in RNNs. For consistency of the model and consideration of computation speed, we replace the hyperbolic tangent function in the GRU with the LReL function. Although not shown in Fig. 3, weight matrices in the bi-directional RNN are jointly learned with all other parameters.\nTo generate the token input vector to the RNN, we utilize the lemma and part-of-speech (POS) tag of each token (obtained with the Stanford CoreNLP toolkit (Manning et al., 2014)), in addition to its word form. A token embedding zt ∈ RD for the t-th token in a sentence is computed as\nzt = E wordewordt +E posepost +E lemmaelemmat ,\nwhere et’s are one-hot encoding vectors for the token, and E’s are parameters to be learned. The dimensions of these one-hot encoding vectors are determined by the size of the corresponding vocabularies, which include all observed types except singletons. Thus, these embedding matrices E’s have the same first dimension D but different second dimensions. This type of additive token embedding has been used in (Botha and Blunsom, 2014; Fang et al., 2015) to integrate various types of information about the token. Moreover, it reduces the tuning space since we only need to make a single decision on the dimensionality of the token embedding.\nGating mechanism: For estimating comment karma levels, the textual content should provide additional information beyond the submission context. However, we hypothesize that the usefulness of textual content may vary under different submission contexts since structure reflects size of the readership. Therefore, we design a context-dependent gating mechanism in the proposed model to weight the textual factors. A scalar gate value is estimated from the submission context embedding c̃, i.e., g = sigmoid(wT c̃), where w ∈ RC is the parameter to be learned. The textual content embedding d ∈ RD is scaled by the gate value g before being fed to the output layer, i.e., d̃ = g · d.\nDecision function: The estimated probability distribution y = [y0, . . . , y7] over all quantized karma\nlevels is computed by the softmax output layer, i.e., y = softmax(Q [ c̃; d̃ ] ), where Q ∈ RJ×(C+D) is\nthe weight matrix to be learned. The hypothesized level for a comment is L̂ = argmaxjyj . For each level-j subtask, both the label L and the hypothesis L̂ are converted to binary values by checking the condition whether they are no less than j."
    }, {
      "heading" : "5 Parameter Learning",
      "text" : "To train the proposed model, each comment is treated as an independent sample, and the objective is the maximum log-likelihood of these samples. We use mini-batch stochastic gradient descent with a batch size of 32, where the gradients are computed with the back-propagation algorithm (Rumelhart et al., 1986). Specifically, the Adam algorithm is used (Kingma and Ba, 2015). The initial learning rate is selected from the range of [0.0010, 0.0100], with a step size of 0.0005, according to the log-likelhood of the validation data at the first epoch. The learning rate is halved at each epoch once the log-likelihood of the validation data decreases. The whole training procedure terminates when the log-likelihood decreases for the second time.\nEach comment is treated as a data sample, and assigned to a partition number in {0, 1, . . . , 9} according to the thread it belongs to. Each partition has roughly the same number of threads. We use partitions 4–9 as training data, partitions 2–3 as validation data, and partitions 0–1 as test data, The training data are shuffled at the beginning of each epoch.\nAs discussed in Section 3, there are many more low-level comments than high-level comments, and the evaluation metric effectively puts more emphasis on high-level comments. Therefore, rather than using the full training and validation sets, we subsample the low-level comments (level-0, level-1, level2, level-3) such that each level has roughly the same number of samples as level-4. Since the three subreddits studied in this paper vary in their sizes, to eliminate the factor of training data size, we use similar amounts of training (∼90K comments) and validation (∼30K comments) data for these subreddits. Note that we do not subsample the test data, i.e., 192K for AskMen, 463K for AskWomen, and 1,167K for Politics."
    }, {
      "heading" : "6 Experiments",
      "text" : "In this section, we present the performance of the proposed model and conduct contrastive experiments to study model variants in two dimensions. For the submission context features, we compare representations obtained via feedforward neural networks to that obtained by a learned combination of latent basis vectors. In terms of textual features, we compare a model which uses no text, contextindependent text features, and a context-depending gating mechanism. Finally, we analyze the learned latent submission context modes, as well as contextdependent gate values that reflect the amount of textual information used by the full model."
    }, {
      "heading" : "6.1 Model Configuration",
      "text" : "All parameters in the neural networks except bias terms are initialized randomly according to the Gaussian distribution N (0, 10−2). We tune the number of latent bases K and the number of hidden layer neurons C and D based on the macro F1 scores on the validation data. For the full model, the best configuration uses K = 8, C = 32 and D = 64 for all subreddits, except Politics where D = 32."
    }, {
      "heading" : "6.2 Main Results",
      "text" : "The performance of the full model on individual levels is presented in Fig. 4. As expected, the lowest level comments are easier to classify. Detection of high-level comments is most reliable in the Politics subreddit, but still difficult.\nTable 2 compares models variants that only\nuse the submission context features. The SubtreeSize baseline uses a multinominal logistic regression model to predict the level according to the subtree size feature alone, whereas the ConvStruct uses the same model but with all conversation structure features defined in Tabel 1. All baselines are stronger than predicting based on prior distributions, which has F1 scores in the 11- 17 range. The model Feedfwd-n is a feedforward neural network with n hidden layers; it uses the submission context feature c in Fig. 3 for prediction. The model LatentBases represents the submission context information by a linear combination of latent bases; it uses c̃ in Fig. 3 for prediction. Compared with Feedfwd-1 in terms of the number of model parameters, Feedfwd-2, Feedfwd-3 and LatentBases have C2, 2C2, and (2C2+K) extra parameters, respectively. These models have similar performance, but there is a slight improvement by increasing model capacity. While the proposed method does not give a significant performance gain, it leads to a more interpretable model.\nTable 3 studies the effect of adding text and introducing the gating mechanism. The un-gated variant uses d instead of d̃ for prediction. Without the gating mechanism, textual information provides significant improvement for AskMen and AskWomen but not for Politics. With the introduced dynamic gating mechanism, the textual information is used more effectively for all three subreddits."
    }, {
      "heading" : "6.3 Analysis",
      "text" : "In this subsection, we analyze the learned submission context modes and the gate values that control the amount of textual information to be used by the\nmodel for predicting comment karma level.\nSubmission context modes: To study the submission context modes, we assign each comment to a cluster according to which basis vector receives the highest weight: argmaxk=1,...,Kak. The label distribution for each cluster is shown in Fig. 5. It can be observed that some clusters are dominated by level-0 comments, and others are dominated by level-7 comments. In Fig. 6, we visualize the learned clusters by projecting the raw conversation structure features x to a 2-dimensional space using the tSNE algorithm (van der Maaten and Hinton, 2008). For purposes of illustrating cross-domain similarities, we group the clusters dominated by level-0 and level-1 comments into a low endorsement cluster, those dominated by level-6 and level-7 into a high endorsement cluster, and the rest as the medium endorsement cluster. It can be seen that the learned clusters split the comments with a consistent pattern, with the higher endorsement comments towards the\nright and the low endorsement comments to the left. In Fig. 7, we show mean values of four selected submission context features for each latent mode of AskWomen, where units of time are in hours. High karma comments tend to be submitted early in the discussion, and the number of children (direct replies) is similar to or greater than the height of its subtree (corresponding to a broad subtree). Low and medium karma comments have a ratio of number of children to subtree height that is less than one. Low karma comments tend to come later in the discussion overall (time since root) but also later in terms of the group of responses to a parent comment (time since parent). These trends hold for all three subreddits. All subreddits have within-group differences in the mode characteristics, particularly the low-karma modes. For AskWomen, graph cluster B corresponds to comments made at the end of a discussion, which are more likely to be low karma because there are fewer readers and less opportunity for a new contribution. Cluster C comments come earlier in the discussion but have small subtrees compared to other early comments.\nText gate: In Table 4, we show the mean gate values g for each group of latent modes. Since gate values are not comparable across subreddits due to dynamic range of feature values, the values shown are scaled by the value for the low-level mode. We observe a consistent trend across all subreddits: lower gate values for higher karma. Recall that the high karma comments typically spawn active discussions. Thus, a possible explanation is that users may be biased to endorse comments that others are endorsing, making the details of the content less important."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In summary, this work has addressed the problem of predicting community endorsement of comments in a discussion forum using a new neural network architecture that integrates submission context features (including relative timing and response struc-\nture) with features extracted from the text of a comment. The approach represents the submission context in terms of a linear combination of latent basis vectors that characterize the dynamic conversation mode, which gives results similar to using a deep network but is more interpretable. The model also includes a dynamic gate for the text content, and analysis shows that when response structure is available to the predictor, the content of a comment has the most utility for comments that are not in active regions of the discussion. These results are based on characterizing quantized levels of karma with a series of binary classifiers. Quantized karma prediction could also be framed as an ordinal regression task, which would involve a straightforward change to the neural network learning objective.\nThis work differs from related work on popularity prediction in that the task does not control for content of a post/comment, nor limit the time window of the submission. With fewer controls, it is more difficult to uncover the aspects of textual content that contribute to endorsement, but by conditioning on submission context we can begin to understand herd effects of endorsement. The task described here also differs from previous work in that the full (or almost full) discussion thread is available in extracting features characterizing the response to the comment, but the modeling framework would also be useful with a limited window lookahead. The results using the full discussion tree also show the limits of using response volume to measure endorsement.\nA limitation of this work is that the submission context is represented only in terms of the relative timing and graph structure in a discussion thread and does not use the text within earlier or responding comments. Prior work has shown that the relevance of a comment to the preceding discussion matters (Jaech et al., 2015), and clearly the sentiment expressed in responses should provide important cues. Capturing these different sources of information in a gated framework is of interest for future work."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This paper is based on work supported by the DARPA DEFT Program. Views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "In Proc. Int. Conf. Learning Representations (ICLR)",
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "The pulse of news in social media: forecasting popularity",
      "author" : [ "Bandari et al.2012] Roja Bandari", "Sitaram Asur", "Bernardo Huberman" ],
      "venue" : "In Proc. Int. AAAI Conf. Web and Social Media (ICWSM)",
      "citeRegEx" : "Bandari et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bandari et al\\.",
      "year" : 2012
    }, {
      "title" : "Compositional morphology for word representations and language modelling",
      "author" : [ "Botha", "Blunsom2014] Jan A. Botha", "Phil Blunsom" ],
      "venue" : "In Proc. Int. Conf. Machine Learning (ICML)",
      "citeRegEx" : "Botha et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Botha et al\\.",
      "year" : 2014
    }, {
      "title" : "Can cascade be predicted",
      "author" : [ "Cheng et al.2014] Justin Cheng", "Lada Adamic", "P. Alex Dow", "Jon Michael Kleinberg", "Jure Leskovec" ],
      "venue" : "In Proc. Int. Conf. World Wide Web (WWW),",
      "citeRegEx" : "Cheng et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2014
    }, {
      "title" : "Open-domain name error detection using a multi-task RNN",
      "author" : [ "Cheng et al.2015] Hao Cheng", "Hao Fang", "Mari Ostendorf" ],
      "venue" : "In Proc. Conf. Empirical Methods Natural Language Process. (EMNLP)",
      "citeRegEx" : "Cheng et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning phrase representations using RNN encoder-decoder for statistical machine translation",
      "author" : [ "Cho et al.2014] Kyunghyun Cho", "Bart van Merriënboer", "Caglar Gulcehre", "Dzmitry Bahadanau", "Fethhi Bougares", "Holger Schwenk", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Cho et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Exponential language modeling using morphological features and multi-task learning",
      "author" : [ "Fang et al.2015] Hao Fang", "Mari Ostendorf", "Peter Baumann", "Janet Pierrehumbert" ],
      "venue" : "IEEE Trans. Audio, Speech, and Language Process.,",
      "citeRegEx" : "Fang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Fang et al\\.",
      "year" : 2015
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Hochreiter et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter et al\\.",
      "year" : 1997
    }, {
      "title" : "Predicting popular messages in Twitter",
      "author" : [ "Hong et al.2011] Liangjie Hong", "Ovidiu Dan", "Brian Davison" ],
      "venue" : "In Proc. Int. Conf. World Wide Web (WWW),",
      "citeRegEx" : "Hong et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Hong et al\\.",
      "year" : 2011
    }, {
      "title" : "Talking to the crowd: What do people react to in online discussions",
      "author" : [ "Jaech et al.2015] Aaron Jaech", "Vicky Zayats", "Hao Fang", "Mari Ostendorf", "Hannaneh Hajishirzi" ],
      "venue" : "In Proc. Conf. Empirical Methods Natural Language Process. (EMNLP),",
      "citeRegEx" : "Jaech et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Jaech et al\\.",
      "year" : 2015
    }, {
      "title" : "Head/tail break: A new classification scheme for data with a heavy-tailed distribution",
      "author" : [ "Bin Jiang" ],
      "venue" : "The Professional Geographer,",
      "citeRegEx" : "Jiang.,? \\Q2013\\E",
      "shortCiteRegEx" : "Jiang.",
      "year" : 2013
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Kingma", "Ba2015] Diederik Kingma", "Jimmy Ba" ],
      "venue" : "In Proc. Int. Conf. Learning Representations (ICLR)",
      "citeRegEx" : "Kingma et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2015
    }, {
      "title" : "What’s in a name? Understanding the interplay between titles, content, and communities in social media",
      "author" : [ "Julian McAuley", "Jure Leskovec" ],
      "venue" : "In Proc. Int. AAAI Conf. Web and Social Media (ICWSM)",
      "citeRegEx" : "Lakkaraju et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Lakkaraju et al\\.",
      "year" : 2013
    }, {
      "title" : "The Stanford CoreNLP natural language processing toolkit",
      "author" : [ "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky" ],
      "venue" : "In Proc. Annu. Meeting Assoc. for Computational Linguistics:",
      "citeRegEx" : "Manning et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Manning et al\\.",
      "year" : 2014
    }, {
      "title" : "Rectifier nonlinearities improve neural network acoustic models",
      "author" : [ "Mass et al.2013] Andrew L. Mass", "Awni Y. Hannun", "Andrew Y. Ng" ],
      "venue" : "In Proc. Int. Conf. Machine Learning (ICML)",
      "citeRegEx" : "Mass et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mass et al\\.",
      "year" : 2013
    }, {
      "title" : "Deep sentence embedding using long short-term memory networks: Analysis and application to information",
      "author" : [ "Li Deng", "Yelong Shen", "Jianfeng Gao", "Xiaodong He", "Jianshu Chen", "Xinying Song", "Rabab Ward" ],
      "venue" : null,
      "citeRegEx" : "Palangi et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Palangi et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning representations by back-propogating",
      "author" : [ "Geoffrey E. Hinton", "Ronald J. Williams" ],
      "venue" : null,
      "citeRegEx" : "Rumelhart et al\\.,? \\Q1986\\E",
      "shortCiteRegEx" : "Rumelhart et al\\.",
      "year" : 1986
    }, {
      "title" : "Want to be retweeted? large scale analytics on factors impacting retweet in twitter network",
      "author" : [ "Suh et al.2010] B. Suh", "L. Hong", "P. Pirolli", "E.H. Chi" ],
      "venue" : "In Proc. IEEE Inter. Conf. on Social Computing (SocialCom),",
      "citeRegEx" : "Suh et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Suh et al\\.",
      "year" : 2010
    }, {
      "title" : "End-toend memory networks",
      "author" : [ "Arthur Szlam", "Jason Weston", "Rob Fergus" ],
      "venue" : "In Proc. Annu. Conf. Neural Inform. Process. Syst. (NIPS),",
      "citeRegEx" : "Sukhbaatar et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sukhbaatar et al\\.",
      "year" : 2015
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Oriol Vinyals", "Quoc V. Le" ],
      "venue" : "In Proc. Annu. Conf. Neural Inform. Process. Syst. (NIPS),",
      "citeRegEx" : "Sutskever et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "The effect of wording on message propagation: Topic- and author-controlled natural experiments on Twitter",
      "author" : [ "Tan et al.2014] Chenhao Tan", "Lillian Lee", "Bo Pang" ],
      "venue" : "In Proc. Annu. Meeting Assoc. for Computational Linguistics (ACL),",
      "citeRegEx" : "Tan et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Tan et al\\.",
      "year" : 2014
    }, {
      "title" : "Predicting the volume of comments on online news stories",
      "author" : [ "Wouter Weerkamp", "Maarten de Rijke" ],
      "venue" : "In Proc. CIKM,",
      "citeRegEx" : "Tasgkias et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Tasgkias et al\\.",
      "year" : 2009
    }, {
      "title" : "Predicting the polularity of online articles based on user comments",
      "author" : [ "Jeremie Leguay", "Panayotis Antoniadis", "Arnaud Limbourg", "Marcelo Dias de Amorim", "Serge Fdida" ],
      "venue" : "In Proc. Inter. Conf. on Web",
      "citeRegEx" : "Tatar et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Tatar et al\\.",
      "year" : 2011
    }, {
      "title" : "Visualizing data using t-SNE",
      "author" : [ "van der Maaten", "Geoffrey Hinton" ],
      "venue" : "Machine Learning Research,",
      "citeRegEx" : "Maaten et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Maaten et al\\.",
      "year" : 2008
    }, {
      "title" : "Grammar as a foreign language",
      "author" : [ "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton" ],
      "venue" : "In Proc. Annu. Conf. Neural Inform. Process. Syst. (NIPS),",
      "citeRegEx" : "Vinyals et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "What’s worthy of comment? content and comment volume in political blogs",
      "author" : [ "Yano", "Smith2010] Tae Yano", "Noah A. Smith" ],
      "venue" : "In Proc. Int. AAAI Conf. Weblogs and Social Media (ICWSM)",
      "citeRegEx" : "Yano et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Yano et al\\.",
      "year" : 2010
    }, {
      "title" : "SEISMIC: A self-exciting point process model for predicting Tweet popularity",
      "author" : [ "Zhao et al.2015] Qingyuan Zhao", "Murat A. Erdogdu", "Hera Y. He", "Anand Rajaraman", "Jure Leskovec" ],
      "venue" : "In Proc. ACM SIGKDD Conf. Knowledge Discovery and Data Min-",
      "citeRegEx" : "Zhao et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 21,
      "context" : "The cumulative response of readers to social media and online content has been studied using a variety of measurements, including: the volume of comments in response to blog posts (Yano and Smith, 2010) and news articles (Tasgkias et al., 2009; Tatar et al., 2011), the number of Twitter shares of news articles (Bandari et al.",
      "startOffset" : 221,
      "endOffset" : 264
    }, {
      "referenceID" : 22,
      "context" : "The cumulative response of readers to social media and online content has been studied using a variety of measurements, including: the volume of comments in response to blog posts (Yano and Smith, 2010) and news articles (Tasgkias et al., 2009; Tatar et al., 2011), the number of Twitter shares of news articles (Bandari et al.",
      "startOffset" : 221,
      "endOffset" : 264
    }, {
      "referenceID" : 1,
      "context" : ", 2011), the number of Twitter shares of news articles (Bandari et al., 2012), the number of reshares on Facebook (Cheng et al.",
      "startOffset" : 55,
      "endOffset" : 77
    }, {
      "referenceID" : 3,
      "context" : ", 2012), the number of reshares on Facebook (Cheng et al., 2014) and retweets on Twitter (Suh et al.",
      "startOffset" : 44,
      "endOffset" : 64
    }, {
      "referenceID" : 17,
      "context" : ", 2014) and retweets on Twitter (Suh et al., 2010; Hong et al., 2011; Tan et al., 2014; Zhao et al., 2015), and the difference in the number of reader up and down votes on posts and comments in Reddit discussion forums (Lakkaraju et al.",
      "startOffset" : 32,
      "endOffset" : 106
    }, {
      "referenceID" : 8,
      "context" : ", 2014) and retweets on Twitter (Suh et al., 2010; Hong et al., 2011; Tan et al., 2014; Zhao et al., 2015), and the difference in the number of reader up and down votes on posts and comments in Reddit discussion forums (Lakkaraju et al.",
      "startOffset" : 32,
      "endOffset" : 106
    }, {
      "referenceID" : 20,
      "context" : ", 2014) and retweets on Twitter (Suh et al., 2010; Hong et al., 2011; Tan et al., 2014; Zhao et al., 2015), and the difference in the number of reader up and down votes on posts and comments in Reddit discussion forums (Lakkaraju et al.",
      "startOffset" : 32,
      "endOffset" : 106
    }, {
      "referenceID" : 26,
      "context" : ", 2014) and retweets on Twitter (Suh et al., 2010; Hong et al., 2011; Tan et al., 2014; Zhao et al., 2015), and the difference in the number of reader up and down votes on posts and comments in Reddit discussion forums (Lakkaraju et al.",
      "startOffset" : 32,
      "endOffset" : 106
    }, {
      "referenceID" : 12,
      "context" : ", 2015), and the difference in the number of reader up and down votes on posts and comments in Reddit discussion forums (Lakkaraju et al., 2013; Jaech et al., 2015).",
      "startOffset" : 120,
      "endOffset" : 164
    }, {
      "referenceID" : 9,
      "context" : ", 2015), and the difference in the number of reader up and down votes on posts and comments in Reddit discussion forums (Lakkaraju et al., 2013; Jaech et al., 2015).",
      "startOffset" : 120,
      "endOffset" : 164
    }, {
      "referenceID" : 1,
      "context" : "Various prediction tasks have been proposed with this in mind, including regression on a log score (Bandari et al., 2012), classification into 3-4 groups (e.",
      "startOffset" : 99,
      "endOffset" : 121
    }, {
      "referenceID" : 21,
      "context" : "none, low, high) (Tasgkias et al., 2009; Hong et al., 2011; Yano and Smith, 2010), a binary decision as to whether the score will double given a current score (Lakkaraju et al.",
      "startOffset" : 17,
      "endOffset" : 81
    }, {
      "referenceID" : 8,
      "context" : "none, low, high) (Tasgkias et al., 2009; Hong et al., 2011; Yano and Smith, 2010), a binary decision as to whether the score will double given a current score (Lakkaraju et al.",
      "startOffset" : 17,
      "endOffset" : 81
    }, {
      "referenceID" : 12,
      "context" : ", 2011; Yano and Smith, 2010), a binary decision as to whether the score will double given a current score (Lakkaraju et al., 2013), and relative ranking of comments (Tan et al.",
      "startOffset" : 107,
      "endOffset" : 131
    }, {
      "referenceID" : 20,
      "context" : ", 2013), and relative ranking of comments (Tan et al., 2014; Jaech et al., 2015).",
      "startOffset" : 42,
      "endOffset" : 80
    }, {
      "referenceID" : 9,
      "context" : ", 2013), and relative ranking of comments (Tan et al., 2014; Jaech et al., 2015).",
      "startOffset" : 42,
      "endOffset" : 80
    }, {
      "referenceID" : 11,
      "context" : "To tease apart the factor of content quality, Lakkaraju et al. (2013) predict resharing of duplicated image submissions, investigating both the submission context (community, time of day, resubmission statistics) and language factors.",
      "startOffset" : 46,
      "endOffset" : 70
    }, {
      "referenceID" : 11,
      "context" : "To tease apart the factor of content quality, Lakkaraju et al. (2013) predict resharing of duplicated image submissions, investigating both the submission context (community, time of day, resubmission statistics) and language factors. Our work differs in that content is not controlled and the submission context includes the response structure and relative timing of the comment within the discussion. Tan et al. (2014) futher control the author and temporal factors in addition to the topic of the content, by ranking pairs of tweets with almost identical content made by the same author within a limited time window.",
      "startOffset" : 46,
      "endOffset" : 421
    }, {
      "referenceID" : 9,
      "context" : "Jaech et al. (2015) control the temporal factor for ranking Reddit comments made in a time-limited window and study different language factors.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 10,
      "context" : "The quantization process is similar to the head-tail break rule described in (Jiang, 2013).",
      "startOffset" : 77,
      "endOffset" : 90
    }, {
      "referenceID" : 14,
      "context" : "Given the raw submission context feature vector x ∈ RN , the model computes a vector c ∈ RC as c = LReL(Px), where P ∈ RC×N is a projection matrix, and LReL(·) is the leaky rectified linear function (Mass et al., 2013) with 0.",
      "startOffset" : 199,
      "endOffset" : 218
    }, {
      "referenceID" : 0,
      "context" : "The computation of basis coefficients is similar to the attention mechanism that has been used in the context of machine translation (Bahdanau et al., 2015), constituency parsing (Vinyals et al.",
      "startOffset" : 133,
      "endOffset" : 156
    }, {
      "referenceID" : 24,
      "context" : ", 2015), constituency parsing (Vinyals et al., 2015), question answering and language modeling (Weston et al.",
      "startOffset" : 30,
      "endOffset" : 52
    }, {
      "referenceID" : 18,
      "context" : ", 2015), question answering and language modeling (Weston et al., 2015; Sukhbaatar et al., 2015).",
      "startOffset" : 50,
      "endOffset" : 96
    }, {
      "referenceID" : 19,
      "context" : "Text embeddings: Recurrent neural networks (RNNs) have been widely used to obtain sequence embeddings for different applications in recent years (Sutskever et al., 2014; Cheng et al., 2015; Palangi et al., 2016).",
      "startOffset" : 145,
      "endOffset" : 211
    }, {
      "referenceID" : 4,
      "context" : "Text embeddings: Recurrent neural networks (RNNs) have been widely used to obtain sequence embeddings for different applications in recent years (Sutskever et al., 2014; Cheng et al., 2015; Palangi et al., 2016).",
      "startOffset" : 145,
      "endOffset" : 211
    }, {
      "referenceID" : 15,
      "context" : "Text embeddings: Recurrent neural networks (RNNs) have been widely used to obtain sequence embeddings for different applications in recent years (Sutskever et al., 2014; Cheng et al., 2015; Palangi et al., 2016).",
      "startOffset" : 145,
      "endOffset" : 211
    }, {
      "referenceID" : 13,
      "context" : "To generate the token input vector to the RNN, we utilize the lemma and part-of-speech (POS) tag of each token (obtained with the Stanford CoreNLP toolkit (Manning et al., 2014)), in addition to its word form.",
      "startOffset" : 155,
      "endOffset" : 177
    }, {
      "referenceID" : 5,
      "context" : "GRU(·, ·) denotes the gated recurrent unit (GRU), which is proposed by Cho et al. (2014) as a simpler alternative to the long short-term memory unit (Hochreiter and Schmidhuber, 1997) for addressing the vanishing gradient issue in RNNs.",
      "startOffset" : 71,
      "endOffset" : 89
    }, {
      "referenceID" : 6,
      "context" : "This type of additive token embedding has been used in (Botha and Blunsom, 2014; Fang et al., 2015) to integrate various types of information about the token.",
      "startOffset" : 55,
      "endOffset" : 99
    }, {
      "referenceID" : 16,
      "context" : "We use mini-batch stochastic gradient descent with a batch size of 32, where the gradients are computed with the back-propagation algorithm (Rumelhart et al., 1986).",
      "startOffset" : 140,
      "endOffset" : 164
    }, {
      "referenceID" : 9,
      "context" : "Prior work has shown that the relevance of a comment to the preceding discussion matters (Jaech et al., 2015), and clearly the sentiment expressed in responses should provide important cues.",
      "startOffset" : 89,
      "endOffset" : 109
    } ],
    "year" : 2016,
    "abstractText" : "Many social media platforms offer a mechanism for readers to react to comments, both positively and negatively, which in aggregate can be thought of as community endorsement. This paper addresses the problem of predicting community endorsement in online discussions, leveraging both the participant response structure and the text of the comment. The different types of features are integrated in a neural network that uses a novel architecture to learn latent modes of discussion structure that perform as well as deep neural networks but are more interpretable. In addition, the latent modes can be used to weight text features thereby improving prediction accuracy.",
    "creator" : "LaTeX with hyperref package"
  }
}