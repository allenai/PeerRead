{
  "name" : "1604.04428.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Resisting Adversarials for Convolutional Neural Networks using Internal Projection",
    "authors" : [ "Harm Berntsen", "Wouter Kuijper", "Tom Heskes" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Convolutional Neural Networks (CNNs) have been shown to work well on image classification tasks [19]. However, CNNs are vulnerable to adversarial images [16,23]. In this paper we introduce a novel type of network structure and training procedure that results in classifiers that are provably, quantitatively more robust to adversarial samples. Adversarial images can be found by perturbing a normal image in such a subtle way that the change is usually imperceptible by the naked eye [7,23].\nThe main idea of our approach is to force the network to make predictions on what the given instance of the class under consideration would look like and subsequently test those predictions. Technically we achieve this by chopping the classifier network into three stages: estimation, projection and comparison.\nThe first stage estimates a vector of parameters (displacement, rotation, scale and, possibly, various object specific internal deformations) from the image. The second stage generates an image based on the estimated parameters. The third stage compares the projected image with the actual image and delivers a likelihood value which can be turned into a verdict using a threshold. The working hypothesis is that this network structure improves robustness against adversarial samples.\nThere are two intuitions behind this working hypothesis. The first is that parameter estimation is a smoother task than classification. Meaning that an orbit through the multidimensional output space can be expected to have a smooth corresponding orbit through the input space. In other words: it is possible to meaningfully interpolate parameters for a model of a given class but it is\nar X\niv :1\n60 4.\n04 42\n8v 2\n[ cs\n.L G\n] 1\n4 Ju\nl 2 01\n6\nmuch harder to meaningfully interpolate between models of two or more different classes.\nThe second intuition behind our working hypothesis is that, by forcing the network to draw a new image using only the estimated parameter vector and subsequently comparing this new image to the original, we are having the network give a “proof” of the presence of the object. By carrying out this comparison only through myopic, local features we ensure that, in order to get enough probability mass to make the threshold, the network must be fairly precise in reproducing the internal details of the objects. In effect we force the network to learn much more than just a discerning set of features, we force it to learn also the detailed internal structure of the object, thereby making it inherently more robust against adversarial input.\nIn this paper we lay the conceptual groundwork and give initial experimental results. We hope that this will enable further research on combining our approach with other, orthogonal, approaches like adversarial training and on applying this method, or refinements inspired by it, on real- world tasks.\nThe source code for training the networks and to generate adversarial images is available at https://github.com/hberntsen/resisting-adversarials."
    }, {
      "heading" : "1.1 Related Work",
      "text" : "Neural networks recognise objects in a different way than humans. As Ullman et al. [26] point out: “. . . the human recognition system uses features and learning processes, which are critical for recognition, but are not used by current models”. They show that where humans can recognise internal components of the objects in the image, current neural networks do not. With knowledge about the internal representation of the objects, false detections can be rejected when it is not consistent with the internal representation of the object. This corresponds with the sensitivity to adversarial images with an imperceptible change that have been shown in [23] and various work since [16]. They show that the smoothness assumption does not hold for neural networks; an imperceptible change in the query image can flip the classification. Goodfellow et al. argue that the primary cause for this is the linear behaviour of the networks in high-dimensional spaces [7] as opposite to the nonlinearity suspected in [23]. The adversarial images are not isolated, spurious points in the pixel space but appear in large regions of the space [24]. Moreover, adversarial images can be efficiently computed using gradient ascent, starting from any input [7].\nThough the existence of adversarial examples is universal [23], neural networks can be made more robust against them. One way is to include adversarial examples in the training data [7,10,14,23], e.g. by assigning them to an additional rubbish class. Apart from increasing the robustness it can also increase the accuracy on non-adversarial examples. Another approach is to adapt the model of the network to improve robustness [4,8]. In [4] the authors identify features that are causally related with the classes. Their learning procedure could be seen as a way to train a classifier that is robust against adversarial examples. In [8] the authors test several denoising architectures to reduce the effects of\nadversarial examples. They conclude that the sensitivity is more related to the training procedure and objective function than to model topology and present a new training procedure.\nWe use 3D models to train the classifier. Though this is artificial data, it can be used as training material for real data, e.g. for object detection [18,22] or even aligning 3D models within an 2D image [1,15,21]. The work of [1] does this using HOG descriptors, while [15,21] use neural networks. They have trained a CNN to predict the viewpoint of 3D models and were successful in applying this model to real-world images."
    }, {
      "heading" : "2 Network Architectures",
      "text" : "In this section we describe the network architectures that we use to test the robustness of our approach. The task of each network is the same: classify the image. Our data consists of greyscale ImageNet images where a part of the image is overlaid with an alpha-blended instance of a 3D model. We use three 3D models that are parametrised by their Euler rotation. The neural network has to recognise the 3D models in all those rotations and emit which 3D model, if any, is visible in the query image. We compare the robustness against adversarial images using three concrete network structures. We will refer to the three 3D models as positive classes and refer to the ‘None’ class as the negative class."
    }, {
      "heading" : "2.1 Networks",
      "text" : "Direct Classification To set a baseline, we train a network to map the query images directly to a probability distribution over the classes. This network is based on AlexNet [12], which has been shown to work well in various situations [17,20,21,25]. To adapt AlexNet to a reduced set of classes and smaller query image, we use a reduced version of AlexNet from [2] which uses smaller layers. We replaced the last layer with a softmax classification layer. The softmax layer has four outputs, three for the positive classes and one to indicate the negative class.\nDirect Classification + Parameter Estimation The Direct Classification + Parameter Estimation network is a variant of the Direct Classification network that has an additional output: the parameters of the model. This additional output forces the neural network to develop a better understanding of the 3D models it has to recognise. The parameter estimation is only used to guide the training process and is not used after the network has been trained.\nTriple-staged We will first describe the triple-staged network as if it is specific to one single class. We expand this design later to a configuration for multiple classes. The triple-staged network contains three stages: (1) estimation, (2) projection, (3) comparison, shown in Fig. 1. Each stage was trained separately and finally merged into one network.\nThe first stage maps the query image to a parameter vector that describes a 3D model. In our running example the parameters describe just the Euler rotation of a 3D model but in general this can also include scale, pan, and internal parameters such as dimensions, and rotational and linear joints. The estimations are clipped to their valid range. The network structure of this stage is the same as the direct classification + parameter estimation network without the task to predict the class.\nThe second stage of the network projects the parameter vector to a 2D image that contains the rendered 3D model in front of a black background. The alpha channel indicates to which degree each pixel belongs to the 3D model. In [5], it was shown that a deep, deconvolutional neural network can be trained to generate images that are parametrised by a broad set of classes and viewpoints. Due to our smaller set of classes and parameters, we use a downscaled variant of the 1s-S network from [5]. The first and second stage together form an autoencoder where the bottleneck contains an understandable instantiation vector of the input. This concept was already applied in the context of transforming autoencoders [9].\nThe final stage has to compare each projected image with the query image. Here we follow [27], which shows how to compare image patches using CNNs. We adapted the 2-channel structure to create a network that compares 10×10 pixel image patches with respect to the alpha channel. This network is convoluted over the output of the second stage, giving it only local data to work with. The network was trained to emit a binary output that indicates whether the original and projected image patch should be considered equal. Fig. 2 visualises how this network works.\nThe combination of the three stages is capable of determining the presence of a single class in the query image. This does not scale well since a separate network has to be trained for each and every class. This issue is addressed by adding the class to generate as a parameter to the projection stage of the network. Each stage can now be trained on data of all the 3D models at once. The class parameter improves scalability of the triple-staged network since only one network needs to be trained for multiple classes. To generate a classification for a query image, it is provided as an input to the network multiple times with a different class parameter. If there is any class where the output of the network rises above the threshold Θ, we use the class that yields the highest similarity score. Otherwise we judge that none of the classes are visible in the query image.\nSimilarly, an optimisation we applied is to supply the class parameter to the prediction stage of the network. We add the class as additional binary channels to the query image. Without the class information passed to this network, the network would internally need to determine which class is visible in the query image. We found that supplying the class information to the network increased the robustness of the triple-staged network. Figure 3 shows the triple-staged network architecture we used."
    }, {
      "heading" : "2.2 Rationale",
      "text" : "The main feature of the triple-staged architecture is to be robust against adversarial samples that cause the network to indicate that a certain class is visible when it is not. To let the neural network produce a false positive classification, an\nadversary needs to perturb the image such that it ultimately fools the comparator stage of the network. However, in order to do so, it must pass through both the estimator and the projector stage. Any attempt to generate a false positive will start drawing another 3D model over the existing one because the comparator compares the query image with the stable internal projection of the class in question. Ultimately then, the ‘false positive’ class will be evidently visible in the query image.\nSince the comparator network directly consumes the query image, this network could still be susceptible to adversarial perturbations of the query image in much the same way as a normal classifier would be. In order to reduce susceptibility, we limited the input space of the network to a single 10× 10 pixel patch. This is enough to learn the general concept of two patches being “similar” (modulo some minor deviations and/or artefacts) but it is not enough to learn longer range correlations in the query image (that would give the adversarial a clear gradient to follow in generating adversarial input). We convolve this local network across the whole image, hence an adversary would need to simultaneously fool sufficiently many individual, local patch comparisons to make a significant impact on the overall similarity mass."
    }, {
      "heading" : "3 Experiment Set-up",
      "text" : "In this section we describe how we will test the network architectures from the previous section for their robustness."
    }, {
      "heading" : "3.1 Training method",
      "text" : "As objects to recognise, we use parametrised 3D models. We rendered 64 × 64 pixel greyscale images of three 3D models: a Monkey (the Suzanne model from [3]), Penguin [13] and an Aeroplane [6] using Blender [3]. We took the rotation of the 3D model over three axes as our parameter space though our method is not limited to this. The rotations were uniformly sampled from the range of [−0.5, 0.5] radians. To give the 3D models a ‘natural’ background, we use\nalpha-composition to blend the 3D model in front of randomly sampled images from the ImageNet dataset [19]. This reduces overfitting of the network on the otherwise black background. We generated 4 × 104 samples for each class. The None class simply consists of random ImageNet images.\nWe used Caffe [11] for the network implementations. The direct classification networks were trained using all 16×104 samples. We left the predicted parameter vector for the negative samples undefined. Each stage of the triple-staged was trained separately. The estimator stage was only trained on the subset of positive samples. The second stage was trained on the original data as rendered through Blender. The input of this stage consists of the binary encoding of the class and the rotation parameters.\nThe data for the third stage of the network was generated by passing data through the first two stages of the network. This resulted in a new dataset with the query image, ground truth class, projected image and projected alpha mask. From this data we generated a balanced dataset where half of the samples should be considered the same and the other half of the samples is not. The samples that are considered different compare the query image, which is either a random ImageNet image or one of the 3D models in front of an ImageNet background, against one of the projected images by the second stage of the network. From this training set we sampled 10 × 10 pixel patches where the projected alpha mask indicated that at least 1% of the pixels belonged to the model. The other samples do not matter since their comparison is cancelled out by the multiplication with the projected alpha (visualised in Fig. 2)."
    }, {
      "heading" : "3.2 Adversarial Image Generation",
      "text" : "When we want to generate an adversarial query image x̃, we search for a minimal perturbation of the original image x that is sufficient to flip the classifier towards a chosen adversarial target class value y. To do this we adopt the fast gradient sign method of [7]. The fast gradient sign method can efficiently generate adversarial images using backpropagation. Our aim is to generate adversarial samples that flip the classification to another positive class value y. Specifically, we perturb an image by computing\nx̃ = clip(x− sign(∇xJ(θ, x, y)), 0, 255),\nwith J the loss function over the query image x and network parameters θ. Since we use 8-bit greyscale images with a range of [0, 255], each pixel of the image will be minimally perturbed. This function is applied as often as needed to flip the classification of the network to the target y."
    }, {
      "heading" : "4 Results",
      "text" : "We test our networks on a separate test set which consists of 10000 samples of each class. The backgrounds are sourced from the ImageNet validation set. We\nfirst measure the classification performance of the networks on non-adversarial images, see Table 1 for the results. The direct classification networks have the lowest error rate, followed by the triple-staged network. With Θ = 0.2 orΘ = 0.7, the ‘None’ class is chosen more/less often. Although choosing Θ somewhere in the middle seems to be the best option purely in terms of minimizing classification error, our data clearly shows that there is a trade-off to consider concerning robustness to adversarial samples versus classification error.\nTo compare the networks under adversarial conditions, we measure how many iterations of adversarial perturbation it takes to change the classification and how much the image was changed. Here we follow [23] who measure the amount of perturbation in adversarial sample for original sample as distortion which is defined as:\n1\n255\n√∑ i (x̃i − xi) 2\nn ,\nwhere x is the original image, x̃ is the distorted image and n is the number of pixels.\nWe performed experiments with both false positive and false negative adversarial images. To generate a false positive adversarial image, we start from a test image containing one of our 3D objects, say a Monkey. Following the procedure explained in Section 3.2, we then construct an adversarial image that makes the network believe that the image belongs to the other class, once for a Penguin and once for an Aeroplane. We repeat this procedure for all test images. Figure 4 shows the results for the false positive adversarial images. The figure shows that for the direct classification networks the required changes are limited: the median of their distortion is still below 0.1 which indicates that the adversarial image is still very similar to the original one. The examples in Fig. 5a show this. In contrast, the triple-staged network requires significantly more effort to change the classification. The higher the threshold, the more the query image needs to look like the internally projected image. Figure 5b shows false positive adversarial samples for the Θ = 0.70 network. The triple-staged network structure requires the adversary to generate images that really start to look like the adversary class. As Table 1 shows, the error rate on normal samples is still reasonable at this threshold.\nWhen we generate false positives, we start with an image that contains one of the classes and perturb it to an image that is classified as ‘None’. For the direct classification networks this is the 4th class they can predict. In the case of the triple-staged network, the output for every class has to be below the threshold Θ. Figure 6 shows that the number of required iterations is significantly higher for Θ = 0.20 compared to the other networks. Note that in contrast to the false positives, the false negative adversarial samples are better resisted using a lower threshold. By lowering the threshold, the triple-staged network is less likely to switch to the ‘None’ class, requiring more work from the adversary."
    }, {
      "heading" : "5 Discussion",
      "text" : "We have adapted the classical network structure for classifier tasks and shown significant improvements in robustness against adversarial samples. In order not to pollute the results we have not taken into account other types of solutions against adversarial samples such as adversarial training. This does not however mean these techniques would not be useful also in our setting. As future work we therefore plan to incorporate adversarial training into our approach.\nFuture work could also apply our technique to include more parameters including internal deformations, using joints etc. We have only tested three 3D models with a limited parameter space. In [15,21] it was already shown that it is possible to estimate viewpoints of 3D models in real-world images. Dosovitskiy et al. [5] have shown that a deconvolutional neural can generate images based on many classes and viewpoints. This opens up possibilities to expand our work to a real-world situation.\nFor the present work we opted to train the network in three separate stages. This allowed us quite a bit of control over the network architecture which, in\nturn, allowed us a shorter route to testing the working hypothesis. Nevertheless, as future work, it would be interesting to develop end-to-end training methods for which the architecture would be more emergent and less explicit. As an obvious first step we could conceive of training the first two stages end-to-end, as an autoencoder, instead of using manually parameterized 3D models."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We would like to thank Daan van Beek for the lively discussions during the preconceptual stage of this work, and also for his detailed comments on a draft of this paper."
    } ],
    "references" : [ {
      "title" : "Seeing 3d chairs: exemplar part-based 2d-3d alignment using a large dataset of cad models",
      "author" : [ "M. Aubry", "D. Maturana", "A. Efros", "B. Russell", "J. Sivic" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2014
    }, {
      "title" : "Adversarial background augmentation improves object localisation using convolutional neural networks. Master’s thesis, Radboud",
      "author" : [ "H. Berntsen" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2015
    }, {
      "title" : "Visual causal feature learning",
      "author" : [ "K. Chalupka", "P. Perona", "F. Eberhardt" ],
      "venue" : "UAI (2015),",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2015
    }, {
      "title" : "Learning to generate chairs, tables and cars with convolutional neural networks",
      "author" : [ "A. Dosovitskiy", "J.T. Springenberg", "M. Tatarchenko", "T. Brox" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2015
    }, {
      "title" : "Antonov an - 71 - 3d model - .obj, .mb",
      "author" : [ "P. Fraga" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2015
    }, {
      "title" : "Explaining and harnessing adversarial examples",
      "author" : [ "I.J. Goodfellow", "J. Shlens", "C. Szegedy" ],
      "venue" : "CoRR (2015),",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2015
    }, {
      "title" : "Towards deep neural network architectures robust to adversarial examples",
      "author" : [ "S. Gu", "L. Rigazio" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2014
    }, {
      "title" : "Transforming auto-encoders",
      "author" : [ "G.E. Hinton", "A. Krizhevsky", "S.D. Wang" ],
      "venue" : "ICANN",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Learning with a strong adversary",
      "author" : [ "R. Huang", "B. Xu", "D. Schuurmans", "C.S. ri" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2015
    }, {
      "title" : "Caffe: Convolutional architecture for fast feature embedding",
      "author" : [ "Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2014
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "K.Q. (eds.) Advances in Neural Information Processing Systems",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2012
    }, {
      "title" : "Tux (armatured) - 3d models - kator legaz",
      "author" : [ "K. Legaz" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2015
    }, {
      "title" : "A unified gradient regularization family for adversarial examples",
      "author" : [ "C. Lyu", "K. Huang", "H.N. Liang" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2015
    }, {
      "title" : "Deep exemplar 2d-3d detection by adapting from real to rendered views",
      "author" : [ "F. Massa", "B.C. Russell", "M. Aubry" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2015
    }, {
      "title" : "Deep neural networks are easily fooled: High confidence predictions for unrecognizable",
      "author" : [ "A. Nguyen", "J. Yosinski", "J. Clune" ],
      "venue" : "images. In: Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2015
    }, {
      "title" : "Zero-shot learning by convex combination of semantic embeddings",
      "author" : [ "M. Norouzi", "T. Mikolov", "S. Bengio", "Y. Singer", "J. Shlens", "A. Frome", "G. Corrado", "J. Dean" ],
      "venue" : "In: International Conference on Learning Representations",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2014
    }, {
      "title" : "Learning deep object detectors from 3d models",
      "author" : [ "X. Peng", "B. Sun", "K. Ali", "K. Saenko" ],
      "venue" : "http://www.cv-foundation.org/openaccess/content_",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2015
    }, {
      "title" : "Deep inside convolutional networks: Visualising image classification models and saliency maps",
      "author" : [ "K. Simonyan", "A. Vedaldi", "A. Zisserman" ],
      "venue" : "Workshop at International Conference on Learning Representations",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2014
    }, {
      "title" : "Render for cnn: Viewpoint estimation in images using cnns trained with rendered 3d model views",
      "author" : [ "H. Su", "C.R. Qi", "Y. Li", "L.J. Guibas" ],
      "venue" : "The IEEE International Conference on Computer Vision (ICCV)",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2015
    }, {
      "title" : "From Virtual to Reality: Fast Adaptation of Virtual Object Detectors to Real Domains",
      "author" : [ "B. Sun", "K. Saenko" ],
      "venue" : "Proceedings of the British Machine Vision Conference. BMVA Press",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2014
    }, {
      "title" : "Intriguing properties of neural networks",
      "author" : [ "C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I. Goodfellow", "R. Fergus" ],
      "venue" : "In: International Conference on Learning Representations",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2014
    }, {
      "title" : "Exploring the space of adversarial images",
      "author" : [ "P. Tabacof", "E. Valle" ],
      "venue" : "CoRR (2015),",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2015
    }, {
      "title" : "Improving image classification with location context",
      "author" : [ "K.D. Tang", "M. Paluri", "L. Fei-Fei", "R. Fergus", "L.D. Bourdev" ],
      "venue" : null,
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2015
    }, {
      "title" : "Atoms of recognition in human and computer vision",
      "author" : [ "S. Ullman", "L. Assif", "E. Fetaya", "D. Harari" ],
      "venue" : "Proceedings of the National Academy of Sciences",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2016
    }, {
      "title" : "Learning to compare image patches via convolutional neural networks",
      "author" : [ "S. Zagoruyko", "N. Komodakis" ],
      "venue" : "The IEEE Conference on Computer Vision and Pattern Recognition",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : "However, CNNs are vulnerable to adversarial images [16,23].",
      "startOffset" : 51,
      "endOffset" : 58
    }, {
      "referenceID" : 20,
      "context" : "However, CNNs are vulnerable to adversarial images [16,23].",
      "startOffset" : 51,
      "endOffset" : 58
    }, {
      "referenceID" : 5,
      "context" : "Adversarial images can be found by perturbing a normal image in such a subtle way that the change is usually imperceptible by the naked eye [7,23].",
      "startOffset" : 140,
      "endOffset" : 146
    }, {
      "referenceID" : 20,
      "context" : "Adversarial images can be found by perturbing a normal image in such a subtle way that the change is usually imperceptible by the naked eye [7,23].",
      "startOffset" : 140,
      "endOffset" : 146
    }, {
      "referenceID" : 23,
      "context" : "[26] point out: “.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "This corresponds with the sensitivity to adversarial images with an imperceptible change that have been shown in [23] and various work since [16].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 14,
      "context" : "This corresponds with the sensitivity to adversarial images with an imperceptible change that have been shown in [23] and various work since [16].",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 5,
      "context" : "argue that the primary cause for this is the linear behaviour of the networks in high-dimensional spaces [7] as opposite to the nonlinearity suspected in [23].",
      "startOffset" : 105,
      "endOffset" : 108
    }, {
      "referenceID" : 20,
      "context" : "argue that the primary cause for this is the linear behaviour of the networks in high-dimensional spaces [7] as opposite to the nonlinearity suspected in [23].",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 21,
      "context" : "The adversarial images are not isolated, spurious points in the pixel space but appear in large regions of the space [24].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 5,
      "context" : "Moreover, adversarial images can be efficiently computed using gradient ascent, starting from any input [7].",
      "startOffset" : 104,
      "endOffset" : 107
    }, {
      "referenceID" : 20,
      "context" : "Though the existence of adversarial examples is universal [23], neural networks can be made more robust against them.",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 5,
      "context" : "One way is to include adversarial examples in the training data [7,10,14,23], e.",
      "startOffset" : 64,
      "endOffset" : 76
    }, {
      "referenceID" : 8,
      "context" : "One way is to include adversarial examples in the training data [7,10,14,23], e.",
      "startOffset" : 64,
      "endOffset" : 76
    }, {
      "referenceID" : 12,
      "context" : "One way is to include adversarial examples in the training data [7,10,14,23], e.",
      "startOffset" : 64,
      "endOffset" : 76
    }, {
      "referenceID" : 20,
      "context" : "One way is to include adversarial examples in the training data [7,10,14,23], e.",
      "startOffset" : 64,
      "endOffset" : 76
    }, {
      "referenceID" : 2,
      "context" : "Another approach is to adapt the model of the network to improve robustness [4,8].",
      "startOffset" : 76,
      "endOffset" : 81
    }, {
      "referenceID" : 6,
      "context" : "Another approach is to adapt the model of the network to improve robustness [4,8].",
      "startOffset" : 76,
      "endOffset" : 81
    }, {
      "referenceID" : 2,
      "context" : "In [4] the authors identify features that are causally related with the classes.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 6,
      "context" : "In [8] the authors test several denoising architectures to reduce the effects of",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 16,
      "context" : "for object detection [18,22] or even aligning 3D models within an 2D image [1,15,21].",
      "startOffset" : 21,
      "endOffset" : 28
    }, {
      "referenceID" : 19,
      "context" : "for object detection [18,22] or even aligning 3D models within an 2D image [1,15,21].",
      "startOffset" : 21,
      "endOffset" : 28
    }, {
      "referenceID" : 0,
      "context" : "for object detection [18,22] or even aligning 3D models within an 2D image [1,15,21].",
      "startOffset" : 75,
      "endOffset" : 84
    }, {
      "referenceID" : 13,
      "context" : "for object detection [18,22] or even aligning 3D models within an 2D image [1,15,21].",
      "startOffset" : 75,
      "endOffset" : 84
    }, {
      "referenceID" : 18,
      "context" : "for object detection [18,22] or even aligning 3D models within an 2D image [1,15,21].",
      "startOffset" : 75,
      "endOffset" : 84
    }, {
      "referenceID" : 0,
      "context" : "The work of [1] does this using HOG descriptors, while [15,21] use neural networks.",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 13,
      "context" : "The work of [1] does this using HOG descriptors, while [15,21] use neural networks.",
      "startOffset" : 55,
      "endOffset" : 62
    }, {
      "referenceID" : 18,
      "context" : "The work of [1] does this using HOG descriptors, while [15,21] use neural networks.",
      "startOffset" : 55,
      "endOffset" : 62
    }, {
      "referenceID" : 10,
      "context" : "This network is based on AlexNet [12], which has been shown to work well in various situations [17,20,21,25].",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 15,
      "context" : "This network is based on AlexNet [12], which has been shown to work well in various situations [17,20,21,25].",
      "startOffset" : 95,
      "endOffset" : 108
    }, {
      "referenceID" : 17,
      "context" : "This network is based on AlexNet [12], which has been shown to work well in various situations [17,20,21,25].",
      "startOffset" : 95,
      "endOffset" : 108
    }, {
      "referenceID" : 18,
      "context" : "This network is based on AlexNet [12], which has been shown to work well in various situations [17,20,21,25].",
      "startOffset" : 95,
      "endOffset" : 108
    }, {
      "referenceID" : 22,
      "context" : "This network is based on AlexNet [12], which has been shown to work well in various situations [17,20,21,25].",
      "startOffset" : 95,
      "endOffset" : 108
    }, {
      "referenceID" : 1,
      "context" : "To adapt AlexNet to a reduced set of classes and smaller query image, we use a reduced version of AlexNet from [2] which uses smaller layers.",
      "startOffset" : 111,
      "endOffset" : 114
    }, {
      "referenceID" : 3,
      "context" : "In [5], it was shown that a deep, deconvolutional neural network can be trained to generate images that are parametrised by a broad set of classes and viewpoints.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 3,
      "context" : "Due to our smaller set of classes and parameters, we use a downscaled variant of the 1s-S network from [5].",
      "startOffset" : 103,
      "endOffset" : 106
    }, {
      "referenceID" : 7,
      "context" : "This concept was already applied in the context of transforming autoencoders [9].",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 24,
      "context" : "Here we follow [27], which shows how to compare image patches using CNNs.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 11,
      "context" : "We rendered 64 × 64 pixel greyscale images of three 3D models: a Monkey (the Suzanne model from [3]), Penguin [13] and an Aeroplane [6] using Blender [3].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 4,
      "context" : "We rendered 64 × 64 pixel greyscale images of three 3D models: a Monkey (the Suzanne model from [3]), Penguin [13] and an Aeroplane [6] using Blender [3].",
      "startOffset" : 132,
      "endOffset" : 135
    }, {
      "referenceID" : 9,
      "context" : "We used Caffe [11] for the network implementations.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 5,
      "context" : "To do this we adopt the fast gradient sign method of [7].",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 20,
      "context" : "Here we follow [23] who measure the amount of perturbation in adversarial sample for original sample as distortion which is defined as:",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 13,
      "context" : "In [15,21] it was already shown that it is possible to estimate viewpoints of 3D models in real-world images.",
      "startOffset" : 3,
      "endOffset" : 10
    }, {
      "referenceID" : 18,
      "context" : "In [15,21] it was already shown that it is possible to estimate viewpoints of 3D models in real-world images.",
      "startOffset" : 3,
      "endOffset" : 10
    }, {
      "referenceID" : 3,
      "context" : "[5] have shown that a deconvolutional neural can generate images based on many classes and viewpoints.",
      "startOffset" : 0,
      "endOffset" : 3
    } ],
    "year" : 2016,
    "abstractText" : "We introduce a novel artificial neural network architecture that integrates robustness to adversarial input in the network structure. The main idea of our approach is to force the network to make predictions on what the given instance of the class under consideration would look like and subsequently test those predictions. By forcing the network to redraw the relevant parts of the image and subsequently comparing this new image to the original, we are having the network give a “proof” of the presence of the object.",
    "creator" : "LaTeX with hyperref package"
  }
}