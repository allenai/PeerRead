{
  "name" : "1206.6388.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Canonical Trends: Detecting Trend Setters in Web Data",
    "authors" : [ "Felix Bießmann", "Jens-Michalis Papaioannou", "Mikio Braun", "Andreas Harth" ],
    "emails" : [ "felix.biessmann@tu-berlin.de", "jensmicha@googlemail.com", "mikio.braun@tu-berlin.de", "harth@kit.edu" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Temporal information is a fundamental aspect of many datasets. Several commercial o↵erings are based on temporal variation of web sources for data mining1. The news domain is a prime example for an industry where time matters. Sources that break a story gain reputation and economic benefits. We thus consider the problem of identifying trendsetting news sources based on temporal correlations found in data.\nOur definition of a trend setter is simple. If a single\n1See e.g. http://www.google.com/trends/correlate/\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nweb source publishes content that will later on dominate the content of a pool of other websites, we consider this source as a trend setter; this approach is similar to causality based graph analyses such as in (Lozano and Sindhwani., 2010). In order to test the trendsetting behavior of a web source we first extract a time series of features from each web source. Then we learn for each web source a convolution in this feature space that predicts the content of all other web sources of interest.\nOur contributions are as follows:\n• We present an approach that detects the canonical trends (CTs) in a pool of web sources. The canonical trends capture the information cascades with the highest impact on that collection of web sources.\n• We propose an unsupervised algorithm that identifies web sources which predict these trends before they arise. The features necessary to predict the trends are automatically learned; they help to identify information cascades and their temporal dynamics.\n• We evaluate the approach on a dataset of news items from 96 popular technology newsfeeds, collected over several months, showing that our approach predicts the temporal evolution of news items better than classical topic detection.\nAs an example data set we collected data from the most influential technology news websites. Bag-ofWord (BoW) features were extracted for each website.\nWe then predict the total information of all websites at time point t using only the information of one web source at prior time points t ⌧ . Our results show that some news sites can predict the future temporal dynamics of the tech-news-sphere well, while others fail to do so. The prediction performance can be interpreted as how much a given news site can be considered as a trend setter and can be used to rank sites according to this criterion: The better a news site predicts the future information of all other news sites, the more influential the news site is."
    }, {
      "heading" : "2. Related Work",
      "text" : "In the following we discuss some alternative approaches towards analysis of temporal dynamics in web data graphs. The authors of (Sun et al., 2007) use the temporal dynamics within a communication network graph to partition the nodes of the graph into groups. The method first extracts adjacency matrices of the graph for di↵erent time points and then tries to compress this time series of connections. This is done by finding similar connection patterns over time and group them together. The motivation of this approach is very di↵erent from ours and a direct comparison of these two approaches is not possible. But there is a similarity that is worth noting: If one web source predicts the content of all other nodes perfectly, we can focus on this single node only and forget about the rest of the network. Thus the representation found by our approach can be seen as an optimal compression of the graph, too.\nOther approaches towards network data graphs evolving over time investigate the di↵usion of influential items, so called memes (Leskovec et al., 2009; Yang & Leskovec, 2010; Gomez Rodriguez et al., 2011). In (Leskovec et al., 2009; Yang & Leskovec, 2010) the authors focus on di↵usion of n-grams in blogs and news media. The method proposed in (Yang & Leskovec, 2010) finds those n-grams that are repeated often, i.e. that account for a large volume of a graph. This objective is very similar to that of this study. The objective of our method is to predict the content of a pool of web sources optimally. This is equivalent to finding nodes that maximize the variance explained of a pool of other web sources. Similar to (Yang & Leskovec, 2010) we use a linear model. A decisive advantage of our approach is that it straightforwardly extends to non-linear dependencies (see section 5.2). Another important di↵erence is that in (Yang & Leskovec, 2010) information transmission is modeled as an indicator function in, meaning information has been transmitted at a certain time lag or not. In our approach\nwe do not restrict the analysis to a binary transmission scheme. Instead we learn a gradual information transmission model from the data. Another related approach is (Gomez Rodriguez et al., 2011). Here the authors analyze the temporal dynamics of information cascades in a temporally evolving graph, in particular how n-grams di↵use through a network. The cascades are represented as time stamps of selected n-grams. Di↵erent generative models are fitted to the data using convex optimization. A central assumption is that the transmission rates can be estimated independently for each cascade. This assumption is similar to our approach: We analyze the temporal dynamics of single web sources independently.\nDespite a number of similarities between (Leskovec et al., 2009; Yang & Leskovec, 2010; Gomez Rodriguez et al., 2011) and our method we emphasize an important di↵erence: All of the above approaches require that the relevant items of information are selected prior to the analysis. For example in (Leskovec et al., 2009; Yang & Leskovec, 2010) the authors analyze a large data set containing millions of n-grams. But only 1000 information cascades are selected for the final analysis according to some heuristics. Thus the result can depend on data selection during preprocessing. Our approach is di↵erent in that it takes the full data set and automatically learns the relevant features. Another crucial di↵erence is that the above approaches do not model dependencies between information cascades. In real data sets it is very likely that on piece of information is highly correlated with another piece of information. The method proposed here takes into account the dependencies between features and models the full multivariate temporal dynamics between web sources."
    }, {
      "heading" : "3. Canonical Trends",
      "text" : "For our approach we extract from each web source f 2 {1, 2, . . . , F} in our collection of F web sources the corresponding features x\nf (t) 2 RW at time points t = {0, 1, . . . , T}. For the sake of simplicity we here assume regularly sampled time points. In our application example we will extract Bag-of-Words features, see section 6.2.1, but our approach is readily applicable to other feature representations such as n-grams or collections of hyperlinks. After feature extraction we store the multivariate feature time series in a sparse matrix\nX\nf = [x f (t = 1), . . . , x f\n(t = T )] 2 RW⇥T . (1)\nWe are interested not in the dynamics of a single web source but rather the temporal variation of many\nnodes in the web graph. The joint time series of all web sources Y\nf can be obtained as the average across all X\nf\nY\nf\n= 1/(F 1) X\nf 0 6=f X f\n0 2 RW⇥T (2)\nwhere f 0 denotes the indices of all web sources except f . We now represent a canonical trend (CT) y\nf (t) as a combination w\ny\n2 RW of features\ny\nf (t) = w> y Y f (:, t) (3)\nIn our application example of BoW features we chose a linear feature combination as the optimal tradeo↵ between a too simplistic modeling of single word occurrences2 and a computationally costly n-gram representation as e.g. in (Leskovec et al., 2009). If the relationships between single features are more complex, the linear feature combination w\ny can be replaced by arbitrary non-linear feature combinations simply by using appropriate kernel functions (see section 5.2)."
    }, {
      "heading" : "4. Canonical Trend Prediction",
      "text" : "The aim of our approach is to predict the temporal variation of the overall trend y\nf (t) using the information published in the past N\n⌧ hours by a single news feed X\nf . This means we want to find a temporal convolution w\nx (⌧) that uses the information of x(t ⌧), ⌧ 2 {1, . . . , N\n⌧ } to predict the canonical trend y\nf (t). The optimal prediction of y f (t) based on the content published in the past N\n⌧ hours on a single web source x\nf\n(t) can be formulated as\nŷ\nf\n(t) = X\n⌧\nw\nx (⌧)>X f (:, t ⌧). (4)\nNeglecting the amplitude of y f (t) and ŷ f (t), minimizing the least-squares error of eq. 4 is equivalent to maximizing the correlation between y\nf (t) and ŷ f (t)\nargmax w\nx\n(⌧),w\ny\nCorr(y f (t), ŷ f (t)). (5)\nThe optimal w x (⌧) and w y can be computed simultaneously using canonical correlation analysis (CCA) (Hotelling, 1936). CCA has proven very useful for a wide variety of applications ranging from signal processing (Akaike, 1976) over e cient computation of causality measures (Otter, 1991). The mathematical properties of CCA are as well understood (Jordan, 1875) as its statistical convergence criteria (Anderson, 1999; Fukumizu et al., 2007). Instead of standard CCA we use an extension, temporal kernel CCA\n2See e.g. http://www.google.com/trends/correlate/.\n(tkCCA), that can deal with high dimensional data, small sample sizes and time delayed non-linear dependencies between data (Bießmann et al., 2010). The interpretation of w\ny and w x (⌧) is straightforward. In our application example they are the directions in the BoW feature space that maximize the correlation between a single feed and all other news feeds (or equivalently – assuming normalized time series – minimize the prediction error between the two). CCA simultaneously optimizes w\ny and w x (⌧) such that the correlation between y\nf (t) and ŷ f (t) is invariant with respect to all linear transformations of the data3. This is why the correlation coe cient in eq. 5 is called canonical. The projection w\ny maps the data into their respective canonical subspace. We thus refer to the time series y\nf (t) as the canonical trend (CT) in the BoW feature space.\nThe correlation coe cient in eq. 5 is obtained from a convolved time series. The convolution in eq. 4 sums over all time lags ⌧ . Often it can give valuable insights in the temporal dynamics between variables if one computes a time lag dependent correlation coe - cient ⇢(⌧)\n⇢(⌧) = Corr(w x (⌧)>X f (:, t ⌧), w> y Y f (:, t)). (6)\nWe will refer to ⇢(⌧) as the canonical correlogram, in complete analogy to a standard crosscorrelogram. The main di↵erence is that standard cross-correlograms are typically computed between two univariate signals. The canonical correlogram is computed between high dimensional multivariate time series, projected into their canonical subspace. The canonical correlogram ⇢(⌧) and the coe cients of the convolution w\nx (⌧) reflect the temporal dynamics in the canonical subspace. An illustrative toy data example is shown in figure 1, for an explanation see section 6.1."
    }, {
      "heading" : "5. Canonical Trend Algorithm",
      "text" : "Informally our approach consists of three steps:\n1. Extract feature matrix X f for each feed\n2. Temporal Embedding of single news feed X f\n3. Kernel CCA between X f and all other feeds Y f\nIn the following we describe steps two and three in detail. Data collection and feature extraction are described in section 6.2.1.\n3Or invariant w.r.t. non-linear transformations in the case of kernel CCA"
    }, {
      "heading" : "5.1. Temporal Embedding",
      "text" : "The temporal embedding is done by creating for each feed f a new representation X̃\nf in which we add copies of the data in X\nf\n, shifted back in time by a time lag ⌧\nX̃\nf\n=\n2\n64 X f,⌧= N ⌧\n... X\nf,⌧= 1\n3\n75 2 RWN⌧⇥T . (7)"
    }, {
      "heading" : "5.2. Kernel CCA",
      "text" : "The temporal embedding operation will increase the dimensionality of our data by a factor of N\n⌧ , the number of time lags. However using the well known kernel trick (Aizerman et al., 1964) we can e ciently compute CCA in kernel space. A main advantage of this trick is that computation of non-linear dependencies becomes a linear problem in kernel space, see e.g. (Fyfe & Lai, 2000). Another crucial advantage of kCCA for the given problem setting is that it reduces the problem size substantially: Estimating w\ny and w x (⌧) in the input space requires the inversion of covariance matrices of size (W +WN\n⌧ )2, where W denotes the number of features. In kernel space we only have to deal with matrices of size (2T )2, where T denotes the number of samples. For the sake of simplicity we consider linear kernels here, but non-linear dependencies can be easily estimated by replacing the linear kernel with other kernel functions. When using linear kernels the CCA solution in input space is a linear expansion of data points\nw\nx (⌧) = X f,⌧ ↵, (8)\nw\ny = Y f . (9)\nThe coe cients ↵ and the eigenvectors of the generalized eigenvalue problem \n0 K x K y\nK\ny\nK\nx\n0\n ↵\n=\n L\nx 0 0 L\ny\n ↵\n(10)\nwhere K x = X̃> f X̃ f 2 RT⇥T is the linear kernel matrix of X̃\nf andK y = Y > f Y f 2 RT⇥T is the linear kernel matrix of Y\nf . The eigenvalue is the canonical correlation on the training data set4, which yields the same result as eq. 5. The matrices on the right hand side are computed as L\nx = K2 x + I and L y = K2 y + I, where  is a regularization parameter controlling the complexity of the solution. For very noisy data  will\n4We consider here only the first dimension of the canonical subspace corresponding to the first eigenvalue; multidimensional canonical subspaces can be found by solving eq. 10 for more than one eigenvalue.\nAlgorithm 1 Canonical Trend Algorithm\nInput: Data {X f=1 , . . . , X f=F } 2 RW⇥T , optimal time lag N\n⌧ , optimal regularizer  Loop over all news feeds for f = 1 to F do\nAverage over all news feeds except f Y\nf\n= 1/(1 F ) P\nf 0 6=f Xf 0\nTemporal Embedding (eq. 7) X̃\nf = [X f (:, t N ⌧ )>, . . . , X f (:, t 1)>]> Compute Kernels K\nx = X̃> f X̃ f\nK\ny = Y > f Y f\nCross-validation loop for fold = 1 to 10 do Pick Training indices Tr 2 {1, . . . , T} Pick Test indices Te 2 {1, . . . , T} \\ Tr N\n⌧\n↵, = kCCA(K x (Tr,Tr),K y (Tr,Tr),) Predict Test data c\nf,fold = >K y (Tr,Te)K x\n(Te,Tr)↵ end for\nend for Rank Feeds according to 1/10 P fold c f,fold\nbe large and thus the optimal ↵ and will be a vector with very similar entries ↵\ni , and the same will be true for . This e↵ectively means that eq. 8 and eq. 9 will reduce to computing the empirical mean of the data. After optimization of N\n⌧ ,  and eq. 10 we can recover the canonical projection w\ny according to eq. 8 and the canonical convolution w\nx (⌧) according to eq. 9. We then could compute ŷ\nf (t) according to eq. 4 and the overall trend y\nf (t) using eq. 3. In practice however this is suboptimal in terms of computational cost. Instead of recovering w\ny\n, w\nx (⌧) and computing y\nf (t), ŷ f (t), we can stay in kernel space to evaluate the models. This yields a substantial computational speedup once the kernels are computed. The complete canonical trend detection algorithm is summarized in algorithm 1."
    }, {
      "heading" : "5.3. Model evaluation for time series",
      "text" : "In order to obtain meaningful prediction accuracies we apply 10-fold cross-validation: we split the available data into training and test data, estimate ↵ and on the training set and compute the prediction accuracy in eq. 5 on test data. When performing crossvalidation on time series data special care has to be taken. In contrast to standard classification settings, where one can simply randomly pick a certain subset of the data, the temporal dependencies in time series data do not allow for such a simple resampling. For proper cross-validation we split the time series in 10\nblocks of equal length. Due to the temporal embedding (see eq. 7) consecutive blocks will overlap by N\n⌧\nsamples. Thus we discarded the first N ⌧ samples from the training block adjacent to the test data block. This ensured that no data point that we tested on was used for training the KCCA model. We estimated the optimal time lag and regularization parameters using 10- fold cross-validation (nested within the training data set) and a grid search over time lags ⌧ 2 {1, 2, . . . , 10} and  2 {10 5, 10 4, . . . , 101}. Optimal regularizers  were in the range of 10 3 to 10 1, the optimal time lag was ⌧ = 5 hours."
    }, {
      "heading" : "5.4. Comparison with other approaches",
      "text" : "The relevant contribution of the CT algorithm is that it maximizes the co-variation of single web sources X\nf\nand other web sources Y f . This is accomplished by a joint factorization of X̃\nf and Y f (see eq. 10). An alternative approach for topic detection is latent semantic analysis (LSA) (Deerwester et al., 1990) in which only a single matrix of BoW features is factorized. In LSA the strongest topic v\ny,f 2 RW is that subspace in the BoW space, here the row space of Y\nf , that captures most variance\nargmax v\ny,f\n(v> y,f Y f Y > f v y,f ), s.t. v> y,f v y,f = 1. (11)\nThe strongest topic v x,f in the single feed BoW space X\nf is found analogously. Informally the relationship between LSA and CT is similar to the relationship between principal component analysis (PCA) (Pearson, 1901) and CCA: PCA maximizes the variance within one web source X\nf (or a collection of web sources Y f ) while CCA maximizes the co-variation between multiple web sources X\nf and Y f . We compared the canonical trend predictions (eq. 5) with the correlation between v>\ny,f\nY\nf and v> x,f X f obtained by LSA on X f\nand Y\nf separately. As an additional sanity check we also shu✏ed the data in time and thereby destroyed the temporal dependencies between X\nf and Y f (results shown in table 1, middle column). All analyses were performed analogously on this surrogate data set, in order to show that the prediction accuracies were indeed meaningful and not just overfitted."
    }, {
      "heading" : "6. Results",
      "text" : "We first illustrate our approach on a toy data set. Thereafter we present some results on real data extracted from technology news feeds."
    }, {
      "heading" : "6.1. Canonical Trends: A toy data example",
      "text" : "For illustrative purposes we consider an event that has been reported extensively on. In 2010 a volcano on Iceland erupted and produced a large ash cloud. Due to this cloud a lot of flights had to be cancelled for security reasons, as the ash could damage aircraft turbines. In the course of the events, every news page on the web reported on the eruption and its consequence. Not every news page used the same words but the overall trend across all news pages included words like eruption, volcano, iceland, aircraft, traffic etc. that co-occurred increasingly. We model this trend in the BoW feature representation time series of two di↵erent web sources X 2 R3 and Y 2 R3. The trend is reflected in the di↵erent dimensions of X with a weighting w⇤\nx = [0.05, 0.9, 0.4]> corresponding to the words Phone, Volcano, Airplane and analogously it is reflected in the dimensions of Y with the weighting w⇤\ny = [0.9, 0.05, 0.6]> corresponding to the words Cloud, iPad, Ash. So one BoW dimension did not carry relevant information (Phone, iPad) and the other two dimensions did carry relevant information,\nrespectively. The toy data was generated from an underlying trend s(t) 2 R1, reflecting the volcano eruption and its consequences on air tra c, by\nX(:, t) = w⇤ x\ns(t 3) + p 1 ✏\nx\n(t) (12)\nY (:, t) = w⇤ y\ns(t) + p\n1 ✏ y (t)\nwhere ✏(t) ⇠ N (0, 1) was noise drawn from a standard normal distribution in R3 and = 0.9 was the signal to noise ratio of the trend. The BoW time series are shown in the right panels of figure 1. X is generated from the latent trend variable s(t) with a temporal lag of 3 temporal units so that X will be ahead of Y by 3 time samples. Note that the dimensions in X were not related to the dimensions of Y . This is a realistic setting: In practice this is di cult to define all possible trends a priori, even with the help of a semantic dictionary. But the increased co-occurrence of the above mentioned trend-relevant words, that is the temporal co-variation in the canonical subspace defined by w⇤\nx\nand w⇤ y , captures the trend very well.\nThis canonical subspace is robustly found by the canonical trend detection algorithm. The optimal convolution w\nx (⌧) and the projection w y are plotted on the left of figure 1. They clearly reflect the structure of w⇤\nx and w⇤ y\nthat gave rise to the trend in the BoW space. In the case of w⇤\nx , the canonical trend detection yields a convolution, rather than a simple projection. The additional temporal dimension indicates the temporal dependency structure in the canonical subspace. At a time lag of 3 temporal units, the web source X predicts the web source Y best. So the optimal BoW features for X, corresponding to w⇤\nx , are found at a time lag of 3. The canonical correlogram (see eq. 6) for our toy data example is plotted in the middle panel on the left and shows a strong peak at ⌧ = 3, indicating that X published the relevant information 3 time units before Y ."
    }, {
      "heading" : "6.2. Trend setter detection in News feeds",
      "text" : ""
    }, {
      "heading" : "6.2.1. Data Collection",
      "text" : "We collected data from 96 news feeds5during the year of 2011. Bag-of-Word (BoW) features were extracted using standard natural language processing tools6. After removal of stop words and stemming our BoW dictionary contained W ⇡ 105 words. The time series of each word was tf-idf normalized. The feature time series were then stored in sparse matrices X\nf 2 RW⇥T where f = {1, . . . , F = 96} denotes news feed and t = {1, . . . , T} denotes the time in hours. Time stamps\n5http://beta.wunderfacts.com/ 6http://www.nltk.org/\nof all news web sources were set to CET. For the sake of comprehensibility in the results presented here we focus on the month of October in 2011. In this month a clearly detectable trend were reports of Steve Jobs’ death."
    }, {
      "heading" : "6.2.2. Canonical Trends in News feeds",
      "text" : "As we obtain a canonical projection w y for each pool of web sources Y\nf , the canonical trends that are predicted by each news feed X\nf could potentially di↵er. In practice however, the canonical trends are very similar. Figure 2 shows in the top panel the median and 25th/75th percentiles of all canonical trends in October 2011. The percentiles are very close to the median trend, indicating a large similarity of the di↵erent canonical trends. Reports on Steve Jobs’ death mark a pronounced peak in the first week reflected in all canonical trends. Also note that the trends clearly reflect the weekly publishing activity on the news feeds, five peaks each week and a trough reflecting the weekend. Our results show that the temporal dynamics in the canonical subspace can be easily interpreted and authentically reflect the impact of relevant information cascades in large web graphs."
    }, {
      "heading" : "6.2.3. Canonical Trend Prediction",
      "text" : "We investigated how well we can predict the trends in a pool of web sources from a single web source. Table 1 shows the prediction accuracy as canonical correlation (see eq. 5) for the ten best predictors, i.e. the trend setter news feeds, summarized as 25th/50th/75th percentiles across cross validation folds. Using the information published at t ⌧, ⌧ = {1, . . . , 5}, meaning five to one hours before all other feeds, the listed news feeds could predict the overall trend at time t with high accuracy. For instance the web site http://businessinsider.com predicted the content of all other news websites in the data set in more than 50% of the cases tested with a correlation coe cient of 0.8. The trend prediction ŷ\nf (t) of the top trendsetter http://businessinsider.com is shown in the bottom panel of figure 2. The time course clearly captures the temporal variation of the overall trend, depicted above in the top panel of fig. 2. In the top panel of figure 3 the time lag dependent features of w\nx (⌧) are depicted. The words to which the canonical trend\ndetection algorithm assigned high weights were associated with Steve Jobs or Apple. The corresponding canonical correlogram ⇢(⌧) has a pronounced peak at ⌧ = 5hrs.\nIt is important to note that the temporal dynamics of single features in w\nx (⌧) can be di↵erent than those of ⇢(⌧). One reason for this is that w\nx (⌧) is nonseparable, meaning that it does not factorize into a single temporal component and one component that describes the dependencies in the BoW feature space. So in order to get the full picture of the temporal dynamics between X\nf and Y f one has to look at the time courses of all features in w\nx (⌧). However we can identify relevant features from w\nx (⌧) by picking those with the highest absolute weights, summed over time lags. And we can extract the overall temporal dynamics from the canonical subspace from ⇢(⌧).\nWe compared the predictions from the canonical trend algorithm to predictions obtained with a standard topic detection method (LSA, see section 5.4). The results are shown in table 1, right column. In the LSA setting, we extracted topics from the BoW time series of single news feeds and the average BoW time series separately. Predictions of the strongest topics in all news feeds based on the strongest topics in a single feed are lower than the CT predictions. This suggests that canonical trends found in a single web source generalize better to a pool of web sources. This is expected from the di↵erent objective functions of CT (see eq. 5) and LSA (see eq. 11)."
    }, {
      "heading" : "7. Conclusion and Outlook",
      "text" : "We presented a simple, e cient and purely data driven method for detecting news trends and trend setters in web data. By making use of the kernel trick we can e ciently exploit the full multivariate structure of temporal dependencies in the canonical subspace of web graph features such as the BoW representation. Both the detected trends and the features learned by the algorithm authentically reflect the true impact of information cascades in temporally evolving graphs. Future work includes more empirical evaluations to study temporal correlation not only from BoW features but also from auxiliary data, such as frequency of retweets along the lines of (Lerman & Hogg, 2010), which predict popularity of content based on early user interest. Another useful feature representation could be named entities along the lines of (Gabrilovich et al., 2004). Independent of the feature representation employed it is important to note that the CT algorithm is unsupervised. The objective of the CT algorithm, maximal co-variation (eq. 5), does not necessarily yield the most interesting trends. Some information that is highly relevant might not be reflected as the main oscillation in all news feeds. However the criterion used in our approach, maximal variance explained, is useful if one is interested in the web sources that have the strongest overall impact. For more detailed analyses the trend of interest could be manually defined (for instance by picking only a few words of interest). Future research will also have to investigate temporal interactions between multidimensional canonical trend subspaces. Moreover we here assumed that the temporal dependencies between web sources are stationary in the analysis period. In general this might not be the case. Web source dependencies can be highly non-stationary. These non-stationarities have to be investigated using appropriate methods, as for instance (von Bünau et al., 2009)."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We thank Klaus-Robert Müller and Manuel GomezRodriguez for helpful discussions. M.B. and F.B. would like to acknowledge support for this project from the BMBF project ALICE, “Autonomous Learning in Complex Environments” (01IB10003B)."
    } ],
    "references" : [ {
      "title" : "Theoretical foundations of the potential function method in pattern recognition learning",
      "author" : [ "A Aizerman", "EM Braverman", "L. Rozonoer" ],
      "venue" : "Automation and Remote Control,",
      "citeRegEx" : "Aizerman et al\\.,? \\Q1964\\E",
      "shortCiteRegEx" : "Aizerman et al\\.",
      "year" : 1964
    }, {
      "title" : "Asymptotic theory for canonical correlation analysis",
      "author" : [ "Anderson", "TW" ],
      "venue" : "Journal of Multivariate Analysis,",
      "citeRegEx" : "Anderson and TW.,? \\Q1999\\E",
      "shortCiteRegEx" : "Anderson and TW.",
      "year" : 1999
    }, {
      "title" : "tkCCA and its application in multimodal neuronal data analysis",
      "author" : [ "F Bießmann", "FC Meinecke", "A Gretton", "A Rauch", "G Rainer", "NK Logothetis", "Müller", "KR" ],
      "venue" : "Machine Learning Journal,",
      "citeRegEx" : "Bießmann et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Bießmann et al\\.",
      "year" : 2010
    }, {
      "title" : "Indexing by Latent Semantic Analysis",
      "author" : [ "SC Deerwester", "ST Dumais", "TK Landauer", "GW Furnas", "Harshman", "RA" ],
      "venue" : "Journal of the American Society of Information Science,",
      "citeRegEx" : "Deerwester et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "Deerwester et al\\.",
      "year" : 1990
    }, {
      "title" : "Statistical consistency of kernel CCA",
      "author" : [ "K Fukumizu", "FR Bach", "A. Gretton" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Fukumizu et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Fukumizu et al\\.",
      "year" : 2007
    }, {
      "title" : "ICA using kernel canonical correlation analysis",
      "author" : [ "C Fyfe", "P. Lai" ],
      "venue" : "In Proc. Int. Workshop on Independent Component Analysis,",
      "citeRegEx" : "Fyfe and Lai,? \\Q2000\\E",
      "shortCiteRegEx" : "Fyfe and Lai",
      "year" : 2000
    }, {
      "title" : "Newsjunkie: providing personalized newsfeeds via analysis of information novelty",
      "author" : [ "E Gabrilovich", "S Dumais", "E. Horvitz" ],
      "venue" : "In Proceedings of the WWW,",
      "citeRegEx" : "Gabrilovich et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Gabrilovich et al\\.",
      "year" : 2004
    }, {
      "title" : "Uncovering the temporal dynamics of di↵usion networks",
      "author" : [ "M Gomez Rodriguez", "D Balduzzi", "B. Schölkopf" ],
      "venue" : "Proceedings of ICML pp",
      "citeRegEx" : "Rodriguez et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Rodriguez et al\\.",
      "year" : 2011
    }, {
      "title" : "Relations between two sets of variates",
      "author" : [ "H. Hotelling" ],
      "venue" : "Biometrika, 28(3):321–377,",
      "citeRegEx" : "Hotelling,? \\Q1936\\E",
      "shortCiteRegEx" : "Hotelling",
      "year" : 1936
    }, {
      "title" : "Using a model of social dynamics to predict popularity of news",
      "author" : [ "K Lerman", "T. Hogg" ],
      "venue" : "In Proceedings of the WWW,",
      "citeRegEx" : "Lerman and Hogg,? \\Q2010\\E",
      "shortCiteRegEx" : "Lerman and Hogg",
      "year" : 2010
    }, {
      "title" : "Memetracking and the dynamics of the news cycle",
      "author" : [ "J Leskovec", "L Backstrom", "J. Kleinberg" ],
      "venue" : "In Proceedings of KDD, pp",
      "citeRegEx" : "Leskovec et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Leskovec et al\\.",
      "year" : 2009
    }, {
      "title" : "On wiener-granger causality, information and canonical correlation",
      "author" : [ "Otter", "PW" ],
      "venue" : "Economics Letters,",
      "citeRegEx" : "Otter and PW.,? \\Q1991\\E",
      "shortCiteRegEx" : "Otter and PW.",
      "year" : 1991
    }, {
      "title" : "On lines and planes of closest fit to systems of points in space",
      "author" : [ "K. Pearson" ],
      "venue" : "Philosophical Magazine,",
      "citeRegEx" : "Pearson,? \\Q1901\\E",
      "shortCiteRegEx" : "Pearson",
      "year" : 1901
    }, {
      "title" : "Graphscope: parameter-free mining of large time-evolving graphs",
      "author" : [ "J Sun", "C Faloutsos", "S Papadimitriou", "Yu", "PS" ],
      "venue" : "In Proceedings of SIGKDD,",
      "citeRegEx" : "Sun et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2007
    }, {
      "title" : "Modeling information di↵usion in implicit networks",
      "author" : [ "J Yang", "J. Leskovec" ],
      "venue" : "In Proceedings of the ICDM, pp",
      "citeRegEx" : "Yang and Leskovec,? \\Q2010\\E",
      "shortCiteRegEx" : "Yang and Leskovec",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 13,
      "context" : "The authors of (Sun et al., 2007) use the temporal dynamics within a communication network graph to partition the nodes of the graph into groups.",
      "startOffset" : 15,
      "endOffset" : 33
    }, {
      "referenceID" : 10,
      "context" : "Other approaches towards network data graphs evolving over time investigate the di↵usion of influential items, so called memes (Leskovec et al., 2009; Yang & Leskovec, 2010; Gomez Rodriguez et al., 2011).",
      "startOffset" : 127,
      "endOffset" : 203
    }, {
      "referenceID" : 10,
      "context" : "In (Leskovec et al., 2009; Yang & Leskovec, 2010) the authors focus on di↵usion of n-grams in blogs and news media.",
      "startOffset" : 3,
      "endOffset" : 49
    }, {
      "referenceID" : 10,
      "context" : "Despite a number of similarities between (Leskovec et al., 2009; Yang & Leskovec, 2010; Gomez Rodriguez et al., 2011) and our method we emphasize an important di↵erence: All of the above approaches require that the relevant items of information are selected prior to the analysis.",
      "startOffset" : 41,
      "endOffset" : 117
    }, {
      "referenceID" : 10,
      "context" : "For example in (Leskovec et al., 2009; Yang & Leskovec, 2010) the authors analyze a large data set containing millions of n-grams.",
      "startOffset" : 15,
      "endOffset" : 61
    }, {
      "referenceID" : 10,
      "context" : "in (Leskovec et al., 2009).",
      "startOffset" : 3,
      "endOffset" : 26
    }, {
      "referenceID" : 8,
      "context" : "y can be computed simultaneously using canonical correlation analysis (CCA) (Hotelling, 1936).",
      "startOffset" : 76,
      "endOffset" : 93
    }, {
      "referenceID" : 4,
      "context" : "The mathematical properties of CCA are as well understood (Jordan, 1875) as its statistical convergence criteria (Anderson, 1999; Fukumizu et al., 2007).",
      "startOffset" : 113,
      "endOffset" : 152
    }, {
      "referenceID" : 2,
      "context" : "(tkCCA), that can deal with high dimensional data, small sample sizes and time delayed non-linear dependencies between data (Bießmann et al., 2010).",
      "startOffset" : 124,
      "endOffset" : 147
    }, {
      "referenceID" : 0,
      "context" : "However using the well known kernel trick (Aizerman et al., 1964) we can e ciently compute CCA in kernel space.",
      "startOffset" : 42,
      "endOffset" : 65
    }, {
      "referenceID" : 3,
      "context" : "An alternative approach for topic detection is latent semantic analysis (LSA) (Deerwester et al., 1990) in which only a single matrix of BoW features is factorized.",
      "startOffset" : 78,
      "endOffset" : 103
    }, {
      "referenceID" : 12,
      "context" : "Informally the relationship between LSA and CT is similar to the relationship between principal component analysis (PCA) (Pearson, 1901) and CCA: PCA maximizes the variance within one web source X",
      "startOffset" : 121,
      "endOffset" : 136
    } ],
    "year" : 2012,
    "abstractText" : "Much information available on the web is copied, reused or rephrased. The phenomenon that multiple web sources pick up certain information is often called trend. A central problem in the context of web data mining is to detect those web sources that are first to publish information which will give rise to a trend. We present a simple and e cient method for finding trends dominating a pool of web sources and identifying those web sources that publish the information relevant to a trend before others. We validate our approach on real data collected from influential technology news feeds.",
    "creator" : "Preview"
  }
}