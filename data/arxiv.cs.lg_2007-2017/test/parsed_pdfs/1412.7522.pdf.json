{
  "name" : "1412.7522.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "BRAIN DECODING", "Orhan Firat", "Emre Aksan", "Fatos T. Yarman Vural" ],
    "emails" : [ "orhan.firat@ceng.metu.edu.tr", "eaksan@ceng.metu.edu.tr", "vural@ceng.metu.edu.tr", "ioztekin@ku.edu.tr" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Revealing the relation between brain activity and stimuli is one of the milestones to be reached in order to understand the neural code. Data driven approaches such as Multi-Voxel Pattern Analysis (MVPA) in systems neuroscience help to achieve this goal by formulating it as a machine learning task. When presented a stimulus and the simultaneous brain activity is recorded for a subject, the relation between the recorded signal and the category of the stimulus may provide percepts for the underlying cognitive process. And the classification problem of predicting the stimulus from the brain recording is called brain decoding. Brain decoding using functional magnetic resonance imaging (fMRI) is a challenging problem that has received much attention recently because of its non-invasive nature, see Haxby et al. (2014) for a review.\nThe major difficulty for the machine learning tasks using fMRI data is the scarcity of labelled samples compared to the high dimensional input spaces. In a typical task related fMRI experiment, the number of labelled samples over several time points reaches at most hundreds while dimension of the input space (number of voxels) easily exceeds thousands even if a small region of interest is considered. Therefore, in order to improve the classification accuracy and significance, there has been a\nar X\niv :1\n41 2.\n75 22\nv4 [\ncs .L\nG ]\n1 2\ngreat deal of effort spent to either reduce the dimensionality Viviani et al. (2005); Sidhu et al. (2012); Mwangi et al. (2014) or employ spatial/temporal structures Battle et al. (2006); Pereira & Botvinick (2011); Firat et al. (2013) for a better representation. As a result, many brain decoding systems rely on cleverly hand-crafted features Norman et al. (2006); Haynes & Rees (2006); Shirer et al. (2012); Hausfeld et al. (2014) to represent cognitive processes. Another issue related to the low number of labelled samples is the labelling procedure which is strongly coupled with the experimental design. Within the time points acquired during an experiment, only few are assigned to corresponding class labels by considering timing of the stimulus. For example in a block design, a predetermined number of time points within a block are averaged to obtain a smoothed intensity value Mitchell et al. (2008) and assigned to a single class label. Similarly, in an event related design, class labels are assigned to time points according to the prior knowledge of the peaks of hemo-dynamic response function (e.g. 2-3 time points after the stimulus) Norman et al. (2006); Oztekin & Badre (2011). The rest of the unlabelled samples are generally thrown away and not used further in the decoding tasks. A typical fMRI experiment for braind decoding is depicted in Figure 1.\nRecent improvements in unsupervised feature learning and transfer learning points out the importance of employing unlabelled data for a better classification Raina et al. (2007); Erhan et al. (2010), especially for the cases that we do not have enough labelled data. It is hypothesised that learning the data generating distribution p(x) by leveraging unlabelled data improves further discriminative tasks p(y|x), when p(x) and p(y|x) share some structure Bengio et al. (2013). We experimented this hypothesis by proposing that the discarded unlabelled samples of fMRI data still contain information and can be exploited in brain decoding tasks. Learning of p(x) is reformulated as learning a set of basis functions that can be used to represent temporal behaviour of brain activity. Likewise, and learning of p(y|x) is formulated as a mapping between learned representations and class labels. In this paper, we attack the brain decoding problem from perspectives of dimensionality reduction and leveraging the unlabelled data. In order to represent noisy and redundant fMRI data, we employ an unsupervised feature learning algorithm that can automatically extract multiple levels of temporal features using unlabelled data. Such approaches are common in computer vision Le et al. (2011) but have not been adapted for neuroimaging data, especially for brain decoding. By incorporating unlabelled data and very little prior knowledge, we learn two levels of temporal filters for fMRI data representation without hand-crafting them. Further, we integrate learned temporal representations into a deep temporal convolutional neural network (CNN) which have demonstrated many successes recently Krizhevsky et al. (2012). We employ the temporal representations to capture the temporal information by convolution following spatial pooling within CNN layers to reduce dimensionality.\nBy leveraging unlabelled data and the representational power of CNNs, we are able to train robust classifiers. The constructed feature spaces are automatically formed by temporal representations and have a reduced dimensionality thanks to spatial pooling. The proposed method substantially improves classical and enhanced multi-voxel pattern analysis (MVPA) methods for brain decoding on a recognition memory experiment with nine participants. Although there exist few studies that employ and analyse deep learning methods for neuroimaging data Cadieu et al. (2014); Hjelm et al. (2014); Plis et al. (2014), we believe that the potential of deep learning should be further studied thoroughly for brain decoding tasks as well."
    }, {
      "heading" : "2 UNSUPERVISED LEARNING OF TEMPORAL REPRESENTATIONS AND CONVOLUTIONAL ARCHITECTURE",
      "text" : "In this section, we describe proposed unsupervised learning architecture for deep temporal representations. fMRI data is composed of 3-dimensional brain volumes across time {ti}ni=1, where each 3D volume is formed by stacking several 2D slices (scans) and n is the total length of the experiment across runs (see Figure 1). Each pixel in these 2D images actually represents the intensity of a small volume of brain tissue (voxel) at a time instant ti. The intensity value of a voxel at location j at a time instant i can be denoted as vji , where j = 1, . . . ,m and m is the total number of voxels in the experiment. A typical fMRI experiment consists of several runs in which the subjects are presented task specific stimuli at predetermined time points. Each of these stimulus corresponds to a class (semantic class label) and the data acquired at that instant is assigned to corresponding class label (orange vertical lines in Figure 1). The entire experimental data can be represented by a voxel×time matrix V T , where columns are representing the brain volumes in terms of voxels (space), the rows stand for voxel time series (time). Further in the classification, the labelled samples are separated into two sets, where the first set is employed in training the classifier and the second set is used for generalization test. Conventional MVPA methods discard columns of the data matrix V T that do not have do not have class labels and represent the cognitive states by using raw voxel intensity values as features. Although the discarded data do not carry label information, it carries the temporal structure (activation pattern) of each voxel across time. In order to improve representation power of features, it is promising to capture and employ the temporal activation pattern as a substitute for raw intensity values.\nIn this study we use the entire V T matrix (except the columns separated for test) to learn temporal filters in two layers. Our aim is to learn a number of activation patterns delimited by short timewindow and able to reflect temporal regularities in the data without hand-crafting them. For the aim of learning temporal filters in an unsupervised fashion, we employ sparse autoencoders Kavukcuoglu et al. (2009). An autoencoder is a neural network trained by back-propagation which attempts to reconstruct its input by setting the target values to be equal to the inputs, x ≈ x̃. Let x be the dataset of time windows of length τ1 that are randomly sampled from the rows of the V T matrix. Autoencoder consists of two consecutive functions. First, an encoder function fθ1(x) is applied on x with parameters θ1 which maps x to a hidden representation h. Second, a decoder function gθ2(fθ1(x)) mapping the hidden representations to the reconstruction x̃ with parameters θ2. By enforcing a sparsity constraint (activation around zero) to hidden layer neurons via the cost function, autoencoder learns a compact, non-linear and possibly over-complete representation of its input x. The number of hidden neurons is equal to the number of filters (bases) to be learned and will be referred as k1 for the first layer temporal representations. The sparse autoencoder having k1 hidden neurons is trained to minimize reconstruction error using back-propagation by minimizing the following cost function,\nJsparse(Θ) = JNN (Θ) + βJρ̂ + λ||Θ||22, (1)\nwhere JNN = 12 ∑ i ||x̃(i) − x(i)||22 is the neural network reconstruction term and λ||Θ||22 is the L2 regularization term on parameters and β is the hyper-parameter controlling importance of sparsity in the model. Sparsity term Jρ̂ is the crucial term in our autoencoder. Let a(.) be an element-wise non-linear activation function and ρ̂j = E[aj(x)] be the expected activation of hidden unit j over the dataset. By enforcing the constraint ρ̂j = ρ where ρ is the sparsity hyper-parameter, hidden layer activations can be adjusted to be sparse. In order to measure the sparsity cost, KullbackLeibler divergence between average activation of a unit ρ̂ and sparsity parameter ρ is calculated\nas Jρ̂ = ∑k1 j KL(ρ||ρ̂j). The optimization of the cost function (1) yields the model parameters Θ = {θ1, θ2}, and columns of the transition weights θ1 of encoder function fθ1(x), constitutes the temporal filters. After learning k1 number of filters in the first layer, V T matrix is convolved along the time axis (1D full temporal convolution) with the learned filters and k1 number of response matrices are extracted. Note that resulting response matrices are the same size as V T with a full convolution.\nIn a convolutional neural network, a general processing block is constructed by three consecutive operations; convolution, pooling and applying an element-wise non-linearity Krizhevsky et al. (2012). Further, these blocks can be repeated consecutively by feeding output of one block to the next one Le et al. (2011). We construct our temporal convolutional model with two processing blocks as shown in Figure 2 with dashed boxes. To complete our first processing block, we determine a spatial pooling function µ, a pooling range in a vicinity δ1 and point-wise non-linearity function σ. Considering the capillary structure of the brain and point spread function of fMRI medium, it is expected that nearby voxels exhibit correlated activations Pereira & Botvinick (2011). Therefore for the choice of µ we employ max pooling function on the columns of response matrices. Note that the columns of the response matrices correspond to spatial domain of fMRI data. As we increase the pooling range without any overlaps between pooling functions, the final dimensionality decreases. As an example, a spatial pooling with range δ1 = 2 on a response matrix with m rows (voxels) and n columns (time points) response matrix results in an m/2 rows and n columned pooled response matrix where the maximum of each of 2 closest voxel responses are taken 1. We finalize the first processing block by applying σ, which is set to hyperbolic tangent, to all elements of the pooled response matrices.\nThe second level of our deep temporal convolutional net takes the pooled response matrices as input and repeats the same pipeline conducted in the first block. The only difference for the second block is the number of input matrices, which is k1 number of m/δ1 rowed and n columned pooled response matrices. For each of these matrices, a separate k2 number of temporal filters are learned again by employing sparse autoencoders for each. Same procedure is followed k1 times: first collecting τ2 length time windows from the rows of an input response matrix then training a sparse autoencoder having k2 number of hidden units. Each k2 filters are then convolved with the corresponding first level pooled response matrix. Repeating this process for all inputs of the second block then applying pooling and non-linearity similarly, we obtain k1 × k2 number of second level pooled response matrices. Note that second level pooled response matrices have m/(δ1 × δ2) rows and n columns, where δ2 is the pooling range for second processing block. As mentioned above, the class labels are carried along the columns of the initial V T matrix. After processing raw input data with the proposed temporal convolutional model, the resulting second level pooled response matrices have the same number of columns n, hence still carries the class label information. Finally at this point, all the second level pooled response matrices are concatenated along their time axis resulting our final representation matrix which has (m×k2)/(δ1×δ2) rows and n columns. We then separate the training and test data by extracting corresponding columns of V T matrix, according to the experiment design class labels positions.\n1Pooling functions are applied to closest voxel pairs in the 3D-brain volume, not in the consecutive elements of VT matrix columns. Pooling regions in Figure 2 are drawn for the ease of understanding."
    }, {
      "heading" : "3 EXPERIMENTS ON FMRI DATA",
      "text" : "fMRI recording was conducted during a recognition memory task on nine participants. Each participant is shown a list of words belonging to a specified semantic category in the working memory encoding phase within ten categories, namely fruits, vegetables, furniture, animals, herbs, clothes, body parts, chemical elements, colors and tools. Following a delay period where the participant solves mathematical problems, a test probe is presented and the participant executes a yes/no response indicating whether the word belongs to the current study list (e.g., see Oztekin & Badre (2011)). For the decoding task, we focused on anterior lateral temporal cortex region having 1024 voxels (m). fMRI data consists of 2400 time points (n) in 8 runs, with 240 class labels for the memory encoding phase and 240 class labels for the memory retrieval phase (for each of ten classes, 24 samples for both memory encoding and memory retrieval). The classification task we seek to accomplish is to predict class labels of the samples in the memory retrieval phase by using samples in the memory encoding phase. Measurements recorded in the memory encoding phase are used as labelled training samples and measurements in the memory retrieval phase are used as test samples. The recording was conducted using a 3T Siemens scanner with a 2 seconds TR (repetition time), meaning that we obtained a separate brain volume each 2 seconds (0.5 Hz sampling rate). Apart from slice-scan time correction, 3D motion correction and registration, no additional pre-processing was employed."
    }, {
      "heading" : "3.1 MODEL DETAILS",
      "text" : "The proposed model is tested using fMRI experiment data with various architectural choices. Our main consideration was to reduce the final representation dimensions with varying pooling ranges. For this purpose, we adjusted the pooling ranges δ1 and δ2 to retract number of dimensions comparable with the standard MVPA method (which is equal to number of voxels m) and even lower. It is well known that, hemo-dynamic response function (HRF) peaks around 4-6 seconds after a stimulus and returns to baseline after 10-12 seconds Huettel et al. (2004). Considering the experimental protocol TR as 2sec, a full HRF could be captured with a temporal window size of 6 samples. Therefore the sampling window length for the first convolutional block τ1 (resp. input unit size for the first autoencoder, filter length), is set to 6. It is also common for CNNs that the higher level receptive fields get wider to capture more abstract regularities Krizhevsky et al. (2012), so we increased the second block temporal sampling window length τ2, to 9 samples (7-10 range give similar results). Note that the collected windows that overlap with the test samples are discarded for fairness. The necessity to explicity increasing the window length of the second layer compared to standard CNNs is that we do not conduct convolution and pooling in the same domain, namely we conduct pooling on spatial domain not in time domain. Another important difference compared to common CNNs is the summation of the response maps for each filter. The pattern we seek for brain decoding is the\nactivity peaks corresponding to the cognitive task which is very sensitive to noise and comparably small signal in magnitude. We observed that summation of the response maps for each filter results very smoothed activity peaks in the response map, which makes it difficult for decoding task to distinguish between class specific activations. Therefore, we avoided such summations for all of the learned filters. Lastly, we observed a rapid overfitting as we fine-tune the whole model with labelled data, which is expected with such high capacity models, hence we did not finetune our pre-trained model."
    }, {
      "heading" : "3.2 SELECTION OF HYPER-PARAMETERS WITHOUT A VALIDATION SET",
      "text" : "A major strait of CNNs and many other deep learning methods is the large number of hyperparameters to be tuned Bengio et al. (2013). Without a proper cross-validation set, as in our case, these hyper parameters are left to be tuned with trial and error fashion which we avoided in this study. As a heuristic, for the hyper-parameters of the autoencoders in both levels k1, k2, β, ρ, λ, we propose following numerical quality assurance. It is obvious that the hidden representations of an autoencoder are expected to learn different regularities from the input, hence, the representation power will be increased and diversified as the learned filters are decorrelated. We searched in a parameter search space for these hyper-parameter combinations. For k1, the inverval 9-25 and for k2 the interval 4-9, for sparsity parameter ρ 0.01-0.3 with multiples of 3 and 1,3,5 for β. For each learned filter bank with an hyper-parameter combination, we calculated the correlation matrix of learned filters. In fully decorrelated filters case, the diagonalized correlation matrix should be close to the identity matrix (all eigenvalues should be equal to one). Therefore, we calculated the L1 distance between the eigenvalues of the correlation matrix of the learned filters to a vector of ones. For example, the hyper-parameters k1, β and ρ for the first layer autoencoder are calculated as follows,\narg min k1,β,ρ ||diag(I)− eigs(Rθ1)||1 (2)\nwhere I is identity matrix and Rθ1 is the correlation matrix for filters θ1 (columns of the encoder parameter matrix). We determine the hyper-parameter combination having the lowest distance, as k1 = 16, k2 = 4, ρ = 0.03 and β = 1. The first layer temporal representations are illustrated in Figure 3, where we observe several HRF alike filters. Moreover, the filters learned directly from unlabelled data are capable of representing several other activation patterns such as linear trends, boxcar, rapid dips and peaks, even some baseline trends. Note that the proposed model takes raw fMRI intensity values as input without any normalization or pre-processing such as whitening or z-scores."
    }, {
      "heading" : "3.3 TESTING PROCEDURES AND COMPARISON",
      "text" : "For the validity of the proposed model, we map the raw input fMRI data into the learned representation space with our pre-trained temporal CNN. Further, in the representation space, training and test samples are extracted and final design matrix is formed for classification. In order to compare proposed method with other MVPA methods, we employ a k-nearest neighbor classifier for all methods. For the proposed model, we also evaluate a single level temporal convolutional network to monitor the impact of the depth. Three different MVPA approaches are taken into consideration. The baseline is Raw MVPA method where raw intensity values are directly fed to classifier. To make a fair comparison, we further employ temporal information in classical MVPA method in two different ways. First, to account for temporal activation pattern in a hand-crafted fashion, we convolved the raw intensity values with a double gamma HRF function spanning 6 samples, which we called as HRF MVPA. After convolving with the HRF, resulting response matrix is used in a similar way with Raw MVPA. Second, we considered a 6 sample time window from onset of the stimulus and concatenated all of the intensity values acquired during that period, which we called as Temporal MVPA (T-MVPA). This process yields a single feature vector for each sample which is 6 times larger than MVPA and HRF MVPA."
    }, {
      "heading" : "3.4 SIGNIFICANCE TESTS AND OVERFITTING ANALYSIS",
      "text" : "The fewer number of samples (as in our case) may result in greater variability of the estimation. We conducted simple, yet diagnostic tests to determine how precise the proposed method performs. Sta-\ntistical significance measures the improbability of the observed classification accuracy given the null hypothesis is true. Pereira et al. (2009) proposes Binomial test (see Alpaydin (2010)) to estimate the significance of a classifier provided that the samples are drawn independently. In our experiment, the trials were clearly separated by irrelevant tasks between memory encoding and memory decoding phases, which makes it plausible to assume that precondition holds. The null hypothesis was regarded as randomness, i.e., the classifier performs at chance level (10% for general, 50% for one-vs-all fashion). The largest p-value the tests yielded was 0.0004, and the null hypothesis was rejected for all methods proposed.\nLearning curve is a modest diagnosis tool, telling how fast a model learns with respect to training data size and whether it suffers more from variance or bias error (see Ng notes). For an ideal setting, both test and training error curves need to be converging to each other. A large gap between them is an indication of high-variance problem. With very few number of samples in relatively high dimensional feature space of fMRI data, a model can easily over fit. In terms of data, getting more training examples or using smaller set of features are possible solutions likely to help the highvariance problem. However, for fMRI case, it is usually hard to increase number of samples due to the physical limitations of MRI, forcing us to learn fewer number of representative features. To be able to observe effect of different feature sets, learning curves of two methods are presented in Figure 4. We mixed samples of different runs to get a larger pool of samples. At each step, 20 new samples (uniformly distributed) are inserted into the training set, and test and training errors are plotted. The gap between test and training errors of MVPA remains much larger as amount of training data increases. However, by using the temporal representations learned by our model, we are able to get a method suffering much less from high-variance problem. However, the end point of the plots (training with all samples) is indicating that both methods still requires more data before convergence."
    }, {
      "heading" : "4 RESULTS",
      "text" : "In this section, we present and analyse performance results of the proposed approach. Experimental results are analysed by considering final feature space dimensions (last column of Table I) in classifiers by comparing classification accuracies. Overall results are also illustrated in Table 1. For the classical and temporal MVPA approaches (first three rows of Table 1), Raw MVPA approach is taken as baseline method. HRF convolved MVPA model performed poorly compared to Raw MVPA results only for two subjects (indicated by Sub) and improved performance up to 8% for the rest of the participants. This is expected as we employ temporal information. However, without adjusting HRF to the regularities in the data, it remains rather hand-crafted. Similarly T-MVPA method use temporal information without any HRF assumption and improves performance compared to other MVPA methods.\nProposed temporal CNN architecture is tested with varying depth (single layer and two layer rows in Table 1) and pooling ranges (δ1 and δ2 columns). A single layer (shallow) temporal feature learning\nand convolution architecture with varying pooling range between 2 to 16 yields a feature space with dimensions from 8192 to 1024. For all nine subjects, single layer architecture outperforms classical and temporal MVPA methods substantially and achieving performance up to 60%s. By appropriate pooling, the feature dimensions of the single layer architecture retracted to 1024 where we still observe 20% improvement.\nIn order to analyse the impact of the depth and further reduce feature dimensionality, two layer architecture is tested with varying pooling ranges in the second level. The proposed model in two layers reduces feature dimensions down to 256 where we still observe better performance in 8 subjects compared to the single layer architecture with lowest dimensions. We did not observe any performance improvements by increasing depth more than two, as model gets more complex and starts to overfit rapidly. The results suggest that employing temporal structures with appropriate pooling for brain decoding gives rise to better classification accuracies. Hierarchical convolutional models pre-trained with a slightly larger amount of unlabelled data are appropriate candidates to employ temporal information in an efficient way. Furthermore, increasing depth of such temporal convolutional architectures up to some extent, makes it possible to reduce feature dimensionality, and the learned filters become more complex and non-linear, resulting a better classification performance."
    }, {
      "heading" : "5 CONCLUSION",
      "text" : "In this study, we have considered a novel approach for brain decoding on fMRI data using unsupervised feature learning and convolutional neural networks. By leveraging unlabelled data and employing multi-layer temporal CNNs, we learn multiple layers of temporal filters which represent the activation patterns of voxels under experimental conditions. By making use of deep temporal representations, we train powerful and robust brain decoding models, which may initiate a slight departure from conventional MVPA approaches generally relying on voxel intensity values as features or carefully hand-crafting them. As evidence of the power of proposed model, we conducted a recognition memory experiment on 9 subjects and observed significant performance improvements. The proposed model has potential to further improvements by incorporating spatial structures with spatial convolution and pooling, or learning spatio-temporal filters all-together which is left as a future work. Also, hyper-parameter selection and preventing overfitting is partially and naively handled in this study, which should be further analysed in details."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "Authors thank all Beyincik members for insightful discussions on fMRI data pre-processing. This work is supported by TUBITAK Project, 112E315."
    } ],
    "references" : [ {
      "title" : "Introduction to Machine Learning",
      "author" : [ "Alpaydin", "Ethem" ],
      "venue" : "ISBN 026201243X,",
      "citeRegEx" : "Alpaydin and Ethem.,? \\Q2010\\E",
      "shortCiteRegEx" : "Alpaydin and Ethem.",
      "year" : 2010
    }, {
      "title" : "Temporal and cross-subject probabilistic models for fmri prediction tasks",
      "author" : [ "Battle", "Alexis", "Chechik", "Gal", "Koller", "Daphne" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Battle et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Battle et al\\.",
      "year" : 2006
    }, {
      "title" : "Representation learning: A review and new perspectives",
      "author" : [ "Bengio", "Yoshua", "Courville", "Aaron", "Vincent", "Pascal" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2013
    }, {
      "title" : "Deep neural networks rival the representation of primate it cortex for core visual object recognition",
      "author" : [ "Cadieu", "Charles F", "Hong", "Ha", "Yamins", "Daniel L K", "Pinto", "Nicolas", "Ardila", "Diego", "Solomon", "Ethan A", "Majaj", "Najib J", "DiCarlo", "James J" ],
      "venue" : "PLoS computational biology,",
      "citeRegEx" : "Cadieu et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Cadieu et al\\.",
      "year" : 2014
    }, {
      "title" : "Why does unsupervised pre-training help deep learning",
      "author" : [ "Erhan", "Dumitru", "Bengio", "Yoshua", "Courville", "Aaron", "Manzagol", "Pierre-Antoine", "Vincent", "Pascal", "Samy" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Erhan et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Erhan et al\\.",
      "year" : 2010
    }, {
      "title" : "Functional mesh learning for pattern analysis of cognitive processes",
      "author" : [ "Firat", "Orhan", "Ozay", "Mete", "Onal", "Itir", "Oztekiny", "Ilke", "Vural", "Fatos T Yarman" ],
      "venue" : "In Cognitive Informatics & Cognitive Computing (ICCI* CC),",
      "citeRegEx" : "Firat et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Firat et al\\.",
      "year" : 2013
    }, {
      "title" : "Multiclass fmri data decoding and visualization using supervised self-organizing maps",
      "author" : [ "Hausfeld", "Lars", "Valente", "Giancarlo", "Formisano", "Elia" ],
      "venue" : null,
      "citeRegEx" : "Hausfeld et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hausfeld et al\\.",
      "year" : 2014
    }, {
      "title" : "Decoding neural representational spaces using multivariate pattern analysis",
      "author" : [ "Haxby", "James V", "Connolly", "Andrew C", "Guntupalli", "J. Swaroop" ],
      "venue" : "Annual Review of Neuroscience,",
      "citeRegEx" : "Haxby et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Haxby et al\\.",
      "year" : 2014
    }, {
      "title" : "Decoding mental states from brain activity in humans",
      "author" : [ "Haynes", "John-Dylan", "Rees", "Geraint" ],
      "venue" : "Nature reviews. Neuroscience,",
      "citeRegEx" : "Haynes et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Haynes et al\\.",
      "year" : 2006
    }, {
      "title" : "Restricted boltzmann machines for neuroimaging: An application in identifying intrinsic networks",
      "author" : [ "Hjelm", "R. Devon", "Calhoun", "Vince D", "Salakhutdinov", "Ruslan", "Allen", "Elena A", "Adali", "Tulay", "Plis", "Sergey M" ],
      "venue" : null,
      "citeRegEx" : "Hjelm et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hjelm et al\\.",
      "year" : 2014
    }, {
      "title" : "Functional magnetic resonance imaging, volume 1. Sinauer Associates",
      "author" : [ "Huettel", "Scott A", "Song", "Allen W", "McCarthy", "Gregory" ],
      "venue" : null,
      "citeRegEx" : "Huettel et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Huettel et al\\.",
      "year" : 2004
    }, {
      "title" : "Learning invariant features through topographic filter maps",
      "author" : [ "Kavukcuoglu", "Koray", "M Ranzato", "Fergus", "Rob", "LeCun", "Yann" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Kavukcuoglu et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Kavukcuoglu et al\\.",
      "year" : 2009
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis",
      "author" : [ "Le", "Quoc V", "Zou", "Will Y", "Yeung", "Serena Y", "Ng", "Andrew Y" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Le et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Le et al\\.",
      "year" : 2011
    }, {
      "title" : "Predicting human brain activity associated with the meanings of nouns",
      "author" : [ "Mitchell", "Tom M", "Shinkareva", "Svetlana V", "Carlson", "Andrew", "Chang", "Kai-Min", "Malave", "Vicente L", "Mason", "Robert A", "Just", "Marcel Adam" ],
      "venue" : null,
      "citeRegEx" : "Mitchell et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Mitchell et al\\.",
      "year" : 2008
    }, {
      "title" : "A review of feature reduction techniques in neuroimaging",
      "author" : [ "Mwangi", "Benson", "Tian", "TianSiva", "Soares", "JairC" ],
      "venue" : "Neuroinformatics, 12(2):229–244,",
      "citeRegEx" : "Mwangi et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mwangi et al\\.",
      "year" : 2014
    }, {
      "title" : "Advice for applying machine learning. Accessed: 04/01/2014",
      "author" : [ "Ng", "Andrew" ],
      "venue" : "URL http://see. stanford.edu/materials/aimlcs229/ML-advice.pdf",
      "citeRegEx" : "Ng and Andrew.,? \\Q2014\\E",
      "shortCiteRegEx" : "Ng and Andrew.",
      "year" : 2014
    }, {
      "title" : "Beyond mind-reading: multi-voxel pattern analysis of fMRI data",
      "author" : [ "Norman", "Kenneth A", "Polyn", "Sean M", "Detre", "Greg J", "Haxby", "James V" ],
      "venue" : "Trends in cognitive sciences,",
      "citeRegEx" : "Norman et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Norman et al\\.",
      "year" : 2006
    }, {
      "title" : "Distributed Patterns of Brain Activity that Lead to Forgetting",
      "author" : [ "Oztekin", "Ilke", "Badre", "David" ],
      "venue" : "Frontiers in human neuroscience,",
      "citeRegEx" : "Oztekin et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Oztekin et al\\.",
      "year" : 2011
    }, {
      "title" : "Information mapping with pattern classifiers: a comparative study",
      "author" : [ "Pereira", "Francisco", "Botvinick", "Matthew" ],
      "venue" : null,
      "citeRegEx" : "Pereira et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Pereira et al\\.",
      "year" : 2011
    }, {
      "title" : "Machine learning classifiers and fmri: A tutorial overview",
      "author" : [ "Pereira", "Francisco", "Mitchell", "Tom", "Botvinick", "Matthew" ],
      "venue" : "NeuroImage, 45:S199 – S209,",
      "citeRegEx" : "Pereira et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Pereira et al\\.",
      "year" : 2009
    }, {
      "title" : "Deep learning for neuroimaging: a validation study",
      "author" : [ "Plis", "Sergey M", "Hjelm", "Devon", "Salakhutdinov", "Ruslan", "Allen", "Elena A", "Bockholt", "Henry Jeremy", "Long", "Jeffrey D", "Johnson", "Hans J", "Paulsen", "Jane", "Turner", "Jessica A", "Calhoun", "Vince D" ],
      "venue" : "Frontiers in Neuroscience,",
      "citeRegEx" : "Plis et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Plis et al\\.",
      "year" : 2014
    }, {
      "title" : "Self-taught learning: transfer learning from unlabeled data",
      "author" : [ "Raina", "Rajat", "Battle", "Alexis", "Lee", "Honglak", "Packer", "Benjamin", "Ng", "Andrew Y" ],
      "venue" : "In Proceedings of the 24th international conference on Machine learning,",
      "citeRegEx" : "Raina et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Raina et al\\.",
      "year" : 2007
    }, {
      "title" : "Decoding subject-driven cognitive states with whole-brain connectivity patterns. Cerebral cortex (New York, N.Y",
      "author" : [ "W R Shirer", "S Ryali", "E Rykhlevskaia", "V Menon", "Greicius", "M D" ],
      "venue" : null,
      "citeRegEx" : "Shirer et al\\.,? \\Q1991\\E",
      "shortCiteRegEx" : "Shirer et al\\.",
      "year" : 1991
    }, {
      "title" : "Kernel principal component analysis for dimensionality reduction in fmri-based diagnosis of adhd",
      "author" : [ "Sidhu", "Gagan S", "Asgarian", "Nasimeh", "Greiner", "Russell", "Brown", "Matthew R G" ],
      "venue" : "Frontiers in Systems Neuroscience,",
      "citeRegEx" : "Sidhu et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Sidhu et al\\.",
      "year" : 2012
    }, {
      "title" : "Functional principal component analysis of fmri data",
      "author" : [ "Viviani", "Roberto", "Grön", "Georg", "Spitzer", "Manfred" ],
      "venue" : "Human brain mapping,",
      "citeRegEx" : "Viviani et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Viviani et al\\.",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "Brain decoding using functional magnetic resonance imaging (fMRI) is a challenging problem that has received much attention recently because of its non-invasive nature, see Haxby et al. (2014) for a review.",
      "startOffset" : 173,
      "endOffset" : 193
    }, {
      "referenceID" : 17,
      "context" : "great deal of effort spent to either reduce the dimensionality Viviani et al. (2005); Sidhu et al.",
      "startOffset" : 63,
      "endOffset" : 85
    }, {
      "referenceID" : 17,
      "context" : "(2005); Sidhu et al. (2012); Mwangi et al.",
      "startOffset" : 8,
      "endOffset" : 28
    }, {
      "referenceID" : 11,
      "context" : "(2012); Mwangi et al. (2014) or employ spatial/temporal structures Battle et al.",
      "startOffset" : 8,
      "endOffset" : 29
    }, {
      "referenceID" : 1,
      "context" : "(2014) or employ spatial/temporal structures Battle et al. (2006); Pereira & Botvinick (2011); Firat et al.",
      "startOffset" : 45,
      "endOffset" : 66
    }, {
      "referenceID" : 1,
      "context" : "(2014) or employ spatial/temporal structures Battle et al. (2006); Pereira & Botvinick (2011); Firat et al.",
      "startOffset" : 45,
      "endOffset" : 94
    }, {
      "referenceID" : 1,
      "context" : "(2014) or employ spatial/temporal structures Battle et al. (2006); Pereira & Botvinick (2011); Firat et al. (2013) for a better representation.",
      "startOffset" : 45,
      "endOffset" : 115
    }, {
      "referenceID" : 1,
      "context" : "(2014) or employ spatial/temporal structures Battle et al. (2006); Pereira & Botvinick (2011); Firat et al. (2013) for a better representation. As a result, many brain decoding systems rely on cleverly hand-crafted features Norman et al. (2006); Haynes & Rees (2006); Shirer et al.",
      "startOffset" : 45,
      "endOffset" : 245
    }, {
      "referenceID" : 1,
      "context" : "(2014) or employ spatial/temporal structures Battle et al. (2006); Pereira & Botvinick (2011); Firat et al. (2013) for a better representation. As a result, many brain decoding systems rely on cleverly hand-crafted features Norman et al. (2006); Haynes & Rees (2006); Shirer et al.",
      "startOffset" : 45,
      "endOffset" : 267
    }, {
      "referenceID" : 1,
      "context" : "(2014) or employ spatial/temporal structures Battle et al. (2006); Pereira & Botvinick (2011); Firat et al. (2013) for a better representation. As a result, many brain decoding systems rely on cleverly hand-crafted features Norman et al. (2006); Haynes & Rees (2006); Shirer et al. (2012); Hausfeld et al.",
      "startOffset" : 45,
      "endOffset" : 289
    }, {
      "referenceID" : 1,
      "context" : "(2014) or employ spatial/temporal structures Battle et al. (2006); Pereira & Botvinick (2011); Firat et al. (2013) for a better representation. As a result, many brain decoding systems rely on cleverly hand-crafted features Norman et al. (2006); Haynes & Rees (2006); Shirer et al. (2012); Hausfeld et al. (2014) to represent cognitive processes.",
      "startOffset" : 45,
      "endOffset" : 313
    }, {
      "referenceID" : 1,
      "context" : "(2014) or employ spatial/temporal structures Battle et al. (2006); Pereira & Botvinick (2011); Firat et al. (2013) for a better representation. As a result, many brain decoding systems rely on cleverly hand-crafted features Norman et al. (2006); Haynes & Rees (2006); Shirer et al. (2012); Hausfeld et al. (2014) to represent cognitive processes. Another issue related to the low number of labelled samples is the labelling procedure which is strongly coupled with the experimental design. Within the time points acquired during an experiment, only few are assigned to corresponding class labels by considering timing of the stimulus. For example in a block design, a predetermined number of time points within a block are averaged to obtain a smoothed intensity value Mitchell et al. (2008) and assigned to a single class label.",
      "startOffset" : 45,
      "endOffset" : 792
    }, {
      "referenceID" : 1,
      "context" : "(2014) or employ spatial/temporal structures Battle et al. (2006); Pereira & Botvinick (2011); Firat et al. (2013) for a better representation. As a result, many brain decoding systems rely on cleverly hand-crafted features Norman et al. (2006); Haynes & Rees (2006); Shirer et al. (2012); Hausfeld et al. (2014) to represent cognitive processes. Another issue related to the low number of labelled samples is the labelling procedure which is strongly coupled with the experimental design. Within the time points acquired during an experiment, only few are assigned to corresponding class labels by considering timing of the stimulus. For example in a block design, a predetermined number of time points within a block are averaged to obtain a smoothed intensity value Mitchell et al. (2008) and assigned to a single class label. Similarly, in an event related design, class labels are assigned to time points according to the prior knowledge of the peaks of hemo-dynamic response function (e.g. 2-3 time points after the stimulus) Norman et al. (2006); Oztekin & Badre (2011).",
      "startOffset" : 45,
      "endOffset" : 1053
    }, {
      "referenceID" : 1,
      "context" : "(2014) or employ spatial/temporal structures Battle et al. (2006); Pereira & Botvinick (2011); Firat et al. (2013) for a better representation. As a result, many brain decoding systems rely on cleverly hand-crafted features Norman et al. (2006); Haynes & Rees (2006); Shirer et al. (2012); Hausfeld et al. (2014) to represent cognitive processes. Another issue related to the low number of labelled samples is the labelling procedure which is strongly coupled with the experimental design. Within the time points acquired during an experiment, only few are assigned to corresponding class labels by considering timing of the stimulus. For example in a block design, a predetermined number of time points within a block are averaged to obtain a smoothed intensity value Mitchell et al. (2008) and assigned to a single class label. Similarly, in an event related design, class labels are assigned to time points according to the prior knowledge of the peaks of hemo-dynamic response function (e.g. 2-3 time points after the stimulus) Norman et al. (2006); Oztekin & Badre (2011). The rest of the unlabelled samples are generally thrown away and not used further in the decoding tasks.",
      "startOffset" : 45,
      "endOffset" : 1077
    }, {
      "referenceID" : 20,
      "context" : "Recent improvements in unsupervised feature learning and transfer learning points out the importance of employing unlabelled data for a better classification Raina et al. (2007); Erhan et al.",
      "startOffset" : 158,
      "endOffset" : 178
    }, {
      "referenceID" : 3,
      "context" : "(2007); Erhan et al. (2010), especially for the cases that we do not have enough labelled data.",
      "startOffset" : 8,
      "endOffset" : 28
    }, {
      "referenceID" : 2,
      "context" : "It is hypothesised that learning the data generating distribution p(x) by leveraging unlabelled data improves further discriminative tasks p(y|x), when p(x) and p(y|x) share some structure Bengio et al. (2013). We experimented this hypothesis by proposing that the discarded unlabelled samples of fMRI data still contain information and can be exploited in brain decoding tasks.",
      "startOffset" : 189,
      "endOffset" : 210
    }, {
      "referenceID" : 12,
      "context" : "Such approaches are common in computer vision Le et al. (2011) but have not been adapted for neuroimaging data, especially for brain decoding.",
      "startOffset" : 46,
      "endOffset" : 63
    }, {
      "referenceID" : 12,
      "context" : "Further, we integrate learned temporal representations into a deep temporal convolutional neural network (CNN) which have demonstrated many successes recently Krizhevsky et al. (2012). We employ the temporal representations to capture the temporal information by convolution following spatial pooling within CNN layers to reduce dimensionality.",
      "startOffset" : 159,
      "endOffset" : 184
    }, {
      "referenceID" : 3,
      "context" : "Although there exist few studies that employ and analyse deep learning methods for neuroimaging data Cadieu et al. (2014); Hjelm et al.",
      "startOffset" : 101,
      "endOffset" : 122
    }, {
      "referenceID" : 3,
      "context" : "Although there exist few studies that employ and analyse deep learning methods for neuroimaging data Cadieu et al. (2014); Hjelm et al. (2014); Plis et al.",
      "startOffset" : 101,
      "endOffset" : 143
    }, {
      "referenceID" : 3,
      "context" : "Although there exist few studies that employ and analyse deep learning methods for neuroimaging data Cadieu et al. (2014); Hjelm et al. (2014); Plis et al. (2014), we believe that the potential of deep learning should be further studied thoroughly for brain decoding tasks as well.",
      "startOffset" : 101,
      "endOffset" : 163
    }, {
      "referenceID" : 11,
      "context" : "For the aim of learning temporal filters in an unsupervised fashion, we employ sparse autoencoders Kavukcuoglu et al. (2009). An autoencoder is a neural network trained by back-propagation which attempts to reconstruct its input by setting the target values to be equal to the inputs, x ≈ x̃.",
      "startOffset" : 99,
      "endOffset" : 125
    }, {
      "referenceID" : 12,
      "context" : "In a convolutional neural network, a general processing block is constructed by three consecutive operations; convolution, pooling and applying an element-wise non-linearity Krizhevsky et al. (2012). Further, these blocks can be repeated consecutively by feeding output of one block to the next one Le et al.",
      "startOffset" : 174,
      "endOffset" : 199
    }, {
      "referenceID" : 12,
      "context" : "In a convolutional neural network, a general processing block is constructed by three consecutive operations; convolution, pooling and applying an element-wise non-linearity Krizhevsky et al. (2012). Further, these blocks can be repeated consecutively by feeding output of one block to the next one Le et al. (2011). We construct our temporal convolutional model with two processing blocks as shown in Figure 2 with dashed boxes.",
      "startOffset" : 174,
      "endOffset" : 316
    }, {
      "referenceID" : 12,
      "context" : "In a convolutional neural network, a general processing block is constructed by three consecutive operations; convolution, pooling and applying an element-wise non-linearity Krizhevsky et al. (2012). Further, these blocks can be repeated consecutively by feeding output of one block to the next one Le et al. (2011). We construct our temporal convolutional model with two processing blocks as shown in Figure 2 with dashed boxes. To complete our first processing block, we determine a spatial pooling function μ, a pooling range in a vicinity δ1 and point-wise non-linearity function σ. Considering the capillary structure of the brain and point spread function of fMRI medium, it is expected that nearby voxels exhibit correlated activations Pereira & Botvinick (2011). Therefore for the choice of μ we employ max pooling function on the columns of response matrices.",
      "startOffset" : 174,
      "endOffset" : 770
    }, {
      "referenceID" : 10,
      "context" : "It is well known that, hemo-dynamic response function (HRF) peaks around 4-6 seconds after a stimulus and returns to baseline after 10-12 seconds Huettel et al. (2004). Considering the experimental protocol TR as 2sec, a full HRF could be captured with a temporal window size of 6 samples.",
      "startOffset" : 146,
      "endOffset" : 168
    }, {
      "referenceID" : 10,
      "context" : "It is well known that, hemo-dynamic response function (HRF) peaks around 4-6 seconds after a stimulus and returns to baseline after 10-12 seconds Huettel et al. (2004). Considering the experimental protocol TR as 2sec, a full HRF could be captured with a temporal window size of 6 samples. Therefore the sampling window length for the first convolutional block τ1 (resp. input unit size for the first autoencoder, filter length), is set to 6. It is also common for CNNs that the higher level receptive fields get wider to capture more abstract regularities Krizhevsky et al. (2012), so we increased the second block temporal sampling window length τ2, to 9 samples (7-10 range give similar results).",
      "startOffset" : 146,
      "endOffset" : 582
    }, {
      "referenceID" : 2,
      "context" : "A major strait of CNNs and many other deep learning methods is the large number of hyperparameters to be tuned Bengio et al. (2013). Without a proper cross-validation set, as in our case, these hyper parameters are left to be tuned with trial and error fashion which we avoided in this study.",
      "startOffset" : 111,
      "endOffset" : 132
    }, {
      "referenceID" : 19,
      "context" : "Pereira et al. (2009) proposes Binomial test (see Alpaydin (2010)) to estimate the significance of a classifier provided that the samples are drawn independently.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 19,
      "context" : "Pereira et al. (2009) proposes Binomial test (see Alpaydin (2010)) to estimate the significance of a classifier provided that the samples are drawn independently.",
      "startOffset" : 0,
      "endOffset" : 66
    } ],
    "year" : 2015,
    "abstractText" : "Functional magnetic resonance imaging produces high dimensional data, with a less then ideal number of labelled samples for brain decoding tasks (predicting brain states). In this study, we propose a new deep temporal convolutional neural network architecture with spatial pooling for brain decoding which aims to reduce dimensionality of feature space along with improved classification performance. Temporal representations (filters) for each layer of the convolutional model are learned by leveraging unlabelled fMRI data in an unsupervised fashion with regularized autoencoders. Learned temporal representations in multiple levels capture the regularities in the temporal domain and are observed to be a rich bank of activation patterns which also exhibit similarities to the actual hemodynamic responses. Further, spatial pooling layers in the convolutional architecture reduce the dimensionality without losing excessive information. By employing the proposed temporal convolutional architecture with spatial pooling, raw input fMRI data is mapped to a non-linear, highly-expressive and low-dimensional feature space where the final classification is conducted. In addition, we propose a simple heuristic approach for hyper-parameter tuning when no validation data is available. Proposed method is tested on a ten class recognition memory experiment with nine subjects. The results support the efficiency and potential of the proposed model, compared to the baseline multi-voxel pattern analysis techniques.",
    "creator" : "LaTeX with hyperref package"
  }
}