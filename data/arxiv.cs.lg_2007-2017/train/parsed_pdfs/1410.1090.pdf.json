{
  "name" : "1410.1090.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Explain Images with Multimodal Recurrent Neural Networks",
    "authors" : [ "Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang", "Alan L. Yuille" ],
    "emails" : [ "mjhustc@ucla.edu,", "wei.xu@baidu.com,", "yangyi05@baidu.com,", "wangjiang03@baidu.com,", "yuille@stat.ucla.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Obtaining sentence level descriptions for images is becoming an important task and has many applications, such as early childhood education, image retrieval, and navigation for the blind. Thanks to the rapid development of computer vision and natural language processing technologies, recent works have made significant progress for this task (see a brief review in Section 2). Many of these works treat it as a retrieval task. They extract features for both sentences and images, and map them to the same semantic embedding space. These methods address the tasks of retrieving the sentences given the query image or retrieving the images given the query sentences. But they can only label the query image with the sentence annotations of the images already existing in the datasets, thus lack the ability to describe new images that contain previously unseen combinations of objects and scenes.\nIn this work, we propose a multimodal Recurrent Neural Networks (m-RNN) model to address both the task of generating novel sentences descriptions for images, and the task of image and sentence retrieval. The whole m-RNN architecture contains a language model part, an image part and a multimodal part. The language model part learns the dense feature embedding for each word in the dictionary and stores the semantic temporal context in recurrent layers. The image part contains a deep Convulutional Neural Network (CNN) [17] which extracts image features. The multimodal part connects the language model and the deep CNN together by a one-layer representation. Our m-RNN model is learned using a perplexity based cost function (see details in Section 4). The errors are backpropagated to the three parts of the m-RNN model to update the model parameters simultaneously. To the best of our knowledge, this is the first work that incorporates the Recurrent Neural Network in a deep multimodal architecture.\nIn the experiments, we validate our model on three benchmark datasets: IAPR TC-12 [8], Flickr 8K [28], and Flickr 30K [13]. we show that our method significantly outperforms the state-of-the-art methods in both the task of generating sentences and the task of image and sentence retrieval when using the same image feature extraction networks. Our model is extendable and has the potential to be further improved by incorporating more powerful deep networks for the image and the sentence.\nar X\niv :1\n41 0.\n10 90\nv1 [\ncs .C\nV ]\n4 O\nct 2\n01 4"
    }, {
      "heading" : "2 Related Work",
      "text" : "Deep model for computer vision and natural language. The deep neural network structure develops rapidly in recent years in both the field of computer vision and natural language. For computer vision, Krizhevsky et. al [17] proposed a deep convolutional neural networks with 8 layers (denoted as AlexNet) for image classification tasks and outperformed previous methods by a large margin. Recently, Girshick et. al [7] proposed a object detection framework based on AlexNet. For natural language, the Recurrent Neural Network shows the state-of-the-art performance in many tasks, such as speech recognition and word embedding learning [22, 23, 24].\nImage-sentence retrieval. Many works treat the task of describe images as a retrieval task and formulate the problem as a ranking or embedding learning problem [12, 6, 30]. They will first extract the word and sentence features (e.g. Socher et.al [30] uses dependency tree Recursive Neural network to extract sentence features) as well as the image features. Then they optimize a ranking cost to learn an embedding model that maps both the language feature and the image feature to a common semantic feature space. In this way, they can directly calculate the distance between images and sentences. Most recently, Karpathy et.al [15] showed that object level image features based on object detection results will generate better results than image features extracted at the global level.\nGenerating novel sentence descriptions for images. There are generally two categories of methods for this task. The first category assumes a specific rule of the language grammar. They parse the sentence and divide it into several parts [25, 10]. Then each part is associated to a object or an attribute in the image (e.g. [18] uses a Conditional Random Field model and [5] uses a Markov Random Field model). This kind of method generates sentences that are syntactically correct. Another category of methods, which is more related to our method, learns a probability density over the space of multimodal inputs (i.e. sentences and images), using for example, Deep Boltzmann Machines [31], and topic models [1, 14]. They can generate sentences with richer and more flexible structure than the first group. The probability of generating sentences given the corresponding image can serves as the affinity metric for retrieval. Our method falls into this category. More close related to our tasks and method is the work of Kiros et al. [16], which is built on a Log-BiLinear model [26]. It needs a fixed length of context (i.e. five words), whereas in our model, the temporal context is stored in a recurrent architecture, which allows arbitrary context length."
    }, {
      "heading" : "3 Model Architecture",
      "text" : ""
    }, {
      "heading" : "3.1 Simple recurrent neural network",
      "text" : "We briefly introduce the simple Recurrent Neural Network (RNN) or Elman network [4] that is widely used for many natural language processing tasks, such as speech recognition [22, 23]. Its architecture is shown in Figure 2(a). It has three types of layers in each time frame: the input word\nlayer w, the recurrent layer r and the output layer y. The activation of input, recurrent and output layers at time t is denoted as w(t), r(t), and y(t) respectively. w(t) is the one-hot representation of the current word. This representation is binary, and has the same dimension of the vocabulary size with only one non-zero element. y(t) can be calculated as follows:\nx(t) = [w(t) r(t− 1)]; r(t) = f1(U · x(t)); y(t) = g1(V · r(t)); (1) where x(t) as a vector that concatenates w(t) and r(t − 1), f1(.) and g1(.) are element-wised sigmoid and softmax function respectively, and U, V are weights which will be learned.\nThe size of RNN is adaptive to the length of the input sequence and the recurrent layers connect the sub-networks in different time frames. Accordingly, when we do the backpropagation, we need to propagate the error through recurrent connections back in time [29]."
    }, {
      "heading" : "3.2 Our m-RNN model",
      "text" : "The structure of our multimodal Recurrent Neural Network (m-RNN) is shown in Figure 2(b). The m-RNN model is much deeper than the simple RNN model. It has six layers in each time frame: the input word layer, two word embedding layers, the recurrent layer, the multimodal layer, and the softmax layer).\nThe two word embedding layers embed the one-hot input into a dense word representation. It has several advantages. Firstly, it will significantly lower the number of parameters in the networks because the dense word vector (128 dimension) is much smaller than the one-hot word vector. Secondly, the dense word embedding encodes the semantic meanings of the words [21]. The semantically relevant words can be found by calculating the Euclidean distance between two dense word vectors in embedding layers.\nMost of the sentence-image multimodal models [15, 6, 30, 16] use pre-computed word embedding vectors as the initialization of their model. In contrast, we randomly initialize our word embedding layers and learn them from the training data. We show that this random initialization is sufficient for our architecture to generate the state-of-the-art results. We treat the activation of the word embedding layer 2 (see Figure 2(b)) as the final word representation, which directly inputs in the multimodal layer.\nAfter the two word embedding layers, we have a recurrent layer with 256 dimensions. The calculation of the recurrent layer is slightly different from the calculation for the simple RNN. Instead of concatenating the word representation at time t (denoted as w(t)) and the recurrent layer activation at time t− 1 (denoted as r(t− 1)), we first map r(t− 1) into the same vector space as w(t) and add them together: r(t) = f2(Ur · r(t− 1) +w(t)); (2) We set f2(.) as the Rectified Linear Unit (ReLU), inspired by its the recent success when training very deep structure in computer vision field [17]. This differs from the simple RNN where the sigmoid function is adopted (see Section 3.1). ReLU is faster, and harder to saturate or overfit the data than non-linear functions like the sigmoid. When backpropagation through time (BPTT) [29] is conducted for RNN with sigmoid function, the vanishing gradient problem appears since even the simplest RNN model can have a large temporal depth. Previous methods [22, 23] used heuristics, such as truncated BPTT, to avoid this problem. Truncated BPTT stops the BPTT after k time steps, where k is a hand-defined hyperparameter. Because of the good properties of ReLU, we do not need to stop the BPTT at an early stage, which leads to a better and more efficient utilization of the data than truncated BPTT.\nAfter the recurrent layer, we set up a 512 dimensional multimodal layer that connect the language model part and the image part of the m-RNN model (see Figure 2(b)). The language model part includes the word embedding layer 2 (the final word representation) and the recurrent layer (the sentence context). The image part contains the image feature extraction network. Here we connect the seventh layer of AlexNet [17] to the multimodal layer (please refer to Section 5 for more details). But our framework can use any image features. We map the feature vector for each layer to the same feature space and add them together to obtain the feature vector for the multimodal layer:\nm(t) = g2(Vw ·w(t) +Vr · r(t) +VI · I); (3) where m denotes the multimodal layer feature vector, I denotes the image feature, g2(.) is the element-wised scaled hyperbolic tangent function [19]:\ng2(x) = 1.7159 · tanh( 2\n3 x) (4)\nThis function forces the gradients into the most non-linear value range and accelerates the training process than the basic hyperbolic tangent function.\nAs the simple RNN, our m-RNN model has a softmax layer that will generate the probability distribution of the next word. The dimension of this layer is the vocabulary size M , which is different for different datasets."
    }, {
      "heading" : "4 Training the m-RNN",
      "text" : "For training our m-RNN model we adopt a cost function based on the Perplexity of the sentences in the training set given their corresponding images. Perplexity is a standard measure for evaluating language model. The perplexity for one word sequence (i.e. a sentences) w1:L is calculated as follows:\nlog2 PPL(w1:L|I) = − 1\nL L∑ n=1 log2 P (wn|w1:n−1, I) (5)\nwhere L is the length of the word sequences, PPL(w1:L|I) denotes the perplexity of the sentence w1:L given the image I. P (wn|w1:n−1, I) is the probability of generating the word wn given I and previous words w1:n−1. It corresponds to the feature vector of the SoftMax layer of our model.\nThe cost function of our model is the average log-likelihood of the words given their context words and corresponding images in the training sentences plus a regularization term. It can be calculated by the perplexity:\nC = 1 N N∑ i=1 L · log2 PPL(w (i) 1:L|I (i)) + ‖θ‖22 (6)\nwhere N is the number of words in the training set and θ is the model parameters.\nOur training objective is to minimize this cost function, which is equivalent to maximize the probability of the model to generate the sentences in the training set given their corresponding images. The cost function is differentiable and we use backpropagation to learn the model parameters."
    }, {
      "heading" : "5 Learning of Sentence and Image Features",
      "text" : "The architecture of our model allows the gradients from the loss function to be backpropagated to both the language modeling part (i.e. the word embedding layers and the recurrent layer) as well as the image part (e.g. the AlexNet [17]).\nFor the language modeling part, as mentioned above, we randomly initialize the language modeling layers and learn their parameters. For the image part, we connect the seventh layer of a pre-trained Convolutional Neural Network [17, 3] (denoted as AlexNet). The same features extracted from the seventh layer of AlexNet (also denoted as decaf features [3]) are widely used by previous multimodal methods [16, 6, 15, 30]. A recent multimodal retrieval work [15] showed that using the RCNN object detection framework [7] combined with the decaf features significantly improves the performance. In the experiments, we show that our method performs much better than [15] when the same image features are used, and is better than or comparable to their results even when they use more sophisticated features based on object detection.\nWe can update the AlexNet according to the gradient backpropagated from the multimodal layer. In this paper, we fix the image features and the deep CNN network in the training stage due to a shortage of data (The datasets we used in the experiment have less than 30K images). In future work, we will apply our method on large datasets and finetune the parameters of the deep CNN network in the training stage."
    }, {
      "heading" : "6 Sentence Generation, Image and Sentence Retrieval",
      "text" : "We can use the trained m-RNN model for three tasks: 1) Sentences generation; 2) Sentence retrieval (retrieving most relevant sentences to the given image); 3) Image retrieval (retrieving most relevant images to the given sentence);\nThe sentence generation process is straightforward. Start from the start sign “##START##” or arbitrary number of reference words (e.g. we can input the first K words in the reference sentence to the model and then start to generate new words), our model can calculate the probability distribution of the next word: P (w|w1:n−1, I). Then we can sample from this probability distribution to pick the next word. In practice, we find that selecting the word with the maximum probability performs slightly better than sampling. After that, we input the picked word to the model and continue the process until the model outputs the end sign “##END##”.\nFor the retrieval tasks, we use our model to calculate the perplexity of generating a sentence given an image. The perplexity can be treated as an affinity measurement between sentences and images. For the image retrieval task, we rank the images based on their perplexity with the query sentence and output the top ranked ones.\nThe sentence retrieval task is trickier because there might be some sentences that have high probability for any image query (e.g. sentences consists of many frequently appeared words). Instead of looking at the perplexity or the probability of generating the sentences given the query image, we use the normalized probability for each sentence: P (w1:L|I)/P (w1:L). P (w1:L) =∑\nI′ P (w1:L|I ′ ) · P (I′), where I′ are images sampled from the training set. We approximate P (I′)\nby a constant and ignore this term. P (w1:L|I) = PPL(w1:L|I)−L."
    }, {
      "heading" : "7 Experiments",
      "text" : ""
    }, {
      "heading" : "7.1 Datasets",
      "text" : "We test our method on three benchmark datasets with sentence level annotations: IAPR TC-12 [8], Flickr 8K [28], and Flickr 30K [13].\nIAPR TC-12 Benchmark. This dataset consists of around 20,000 images taken from locations around the world. This includes images of different sports and actions, people, animals, cities, landscapes, and so on. For each image, they provide at least one sentence annotation. On average, there are about 1.7 sentences annotations for one image. We adopt the publicly available separation\nof training and testing set as previous works [9, 16]. There are 17,665 images for training and 1962 images for testing.\nFlickr8K Benchmark. This dataset consists of 8,000 images extracted from Flickr. For each image, it provides five sentences annotations. The grammar of the annotations for this dataset is simpler than that for the IAPR TC-12 dataset. We adopt the standard separation of training, validation and testing set which is provided by the dataset. There are 6,000 images for training, 1,000 images for validation and 1,000 images for testing.\nFlickr30K Benchmark. This dataset is a recent extension of Flickr8K. For each image, it also provides five sentences annotations. It consists of 158,915 crowd-sourced captions describing 31,783 images. The grammar and style for the annotations of this dataset is similar to Flickr8K. We follow the previous work [15] which used 1,000 images for testing. This dataset, as well as the Flick8K dataset, are commonly used for the image-sentence retrieval tasks."
    }, {
      "heading" : "7.2 Evaluation metrics",
      "text" : "Sentence Generation. Following previous works, we adopt sentence perplexity and BLEU scores (i.e. B-1, B-2, and B-3) [27, 20] as the evaluation metrics. BLEU scores were originally designed for automatic machine translation where they rate the quality of a translated sentences given several references sentences. We can treat the sentence generation task as the “translation” of the content of images to sentences. BLEU remains the standard evaluation metric for sentence generation methods for images, though it has drawbacks. For some images, the reference sentences might not contains all the possible descriptions in the image and BLEU might penalize some correctly generated sentences. To conduct a fair comparison, we adopt the same sentence generation steps and experiment settings as [16], and generate as many words as there are in the reference sentences to calculate BLEU. Note that our model does not need to know the length of the reference sentence because we add a end sign ”##END##” at the end of every training sentences and we can stop the generation process when our model outputs the end sign.\nSentence Retrieval and Image Retrieval For Flickr8K and Flickr30K datasets, we adopted the same evaluation metrics as previous works [30, 6, 15] for both the tasks of sentences retrieval and image retrieval. They used R@K (K = 1, 5, 10) as the measurements, which are the recall rates of the first retrieved groundtruth sentences (sentence retrieval task) or images (image retrieval task). Higher R@K usually mean better retrieval performance. Since we care most about the top-ranked retrieved results, the R@K with small K are more important. The Med r is another score we used, which is the median rank of the first retrieved groundtruth sentences or images. Lower Med r usually means better performance.\nFor IAPR TC-12 datasets, we adopt exactly the same evaluation metrics as [16] and plot the mean number of matches of the retrieved groundtruth sentences or images with respect to the percentage of the retrieved sentences or images for the testing set. For sentences retrieval task, [16] used a shortlist of 100 images which are the nearest neighbors of the query image in the feature space. This shortlist strategy makes the task harder because similar images might have similar descriptions and it is often harder to find subtle differences among the sentences and pick the most suitable one. Although there are no published R@K scores and Med r score for this dataset available for the best of our knowledge, we also report these scores of our method for future comparison."
    }, {
      "heading" : "7.3 Results on IAPR TC-12",
      "text" : "The results of the sentence generation task are shown in Table 1. BACK-OFF GT2 and GT3 are n-grams methods with Katz backoff and Good-Turing discounting [2, 16]. Ours-RNN-Base serves as a baseline method for our m-RNN model. It has the same architecture with m-RNN except that we will not input the image features to the network.\nTo conduct a fair comparison, we followed the same experimental settings of [16], include the context length to calculate the BLEU scores and perplexity. These two evaluation metrics are not necessarily correlated to each other for the following reasons. As mentioned in Section 4, perplexity is calculated according to the conditional probability of the word in a sentence given all of its previous reference words. Therefore, a strong language model that successfully captures the distributions of words in sentences can have a low perplexity without the image content. But the content of the gen-\nerated sentences might be unrelated to images. From Table 1, we can see that although our baseline method of RNN generates a very low perplexity, its BLEU score is not very high, indicating that it failed to generate sentences with high quality.\nWe show that our m-RNN model performs much better than our baseline RNN model in terms of both perplexity and BLEU score. It also outperforms the state-of-the-art methods in terms of perplexity, B-1, B-3, and a comparable result for B-2 1.\nFor retrieval tasks, as mentioned in Section 7.2, we draw a recall accuracy curve with respect to the percentage of retrieved images (sentence retrieval task) or sentences (sentence retrieval task) in Figure 3. For sentence retrieval task, we used a shortlist of 100 images as the three comparing methods shown in [16]. The first method, bowdecaf, is a strong image based bag-of-words baseline. The second and the third models are all multimodal deep models. Our m-RNN model significantly outperforms these three methods in this task.\nSince there are no publicly available results of R@K and median rank in this dataset, we report R@K scores of our method in Table 2 for future comparisons. The result shows that 20.9% topranked retrieved images and 13.2% top-ranked retrieved sentences are groundtruth.\n1[16] further improve their results after the publication. The perplexity of MLBL-F and LBL now are 9.90 and 9.29 respectively."
    }, {
      "heading" : "7.4 Results on flickr8K",
      "text" : "This dataset was widely used as a benchmark dataset of image and sentence retrieval. The R@K and Med r of different methods are shown in Table 3. Our model outperforms the state-of-theart methods (i.e Socher-decaf, DeViSE-decaf, DeepFE-decaf) by a large margin when using the same image features (i.e. decaf features). We also list the performance of methods using more sophisticated features in Table 3. “-avg-rcnn” denotes methods with features of the average CNN activation of all objects above a detection confidence threshold. DeepFE-rcnn [15] uses a fragment mapping strategy to better exploit the object detection results. The results show that using these features will improve the performance. Even without the help from the object detection methods, however, our method performs better than these methods in most of the evaluation metrics. We will develop our framework using better image features in the future work.\nWe report the results of generated sentences in Table 5. There is no publicly available algorithm that reported results on this dataset. So we compared our m-RNN model with the Ours-RNN-Base model. The m-RNN model performs much better than this baseline both in terms of the perplexity and BLEU scores."
    }, {
      "heading" : "7.5 Results on flickr30K",
      "text" : "This dataset is a new dataset and there are only a few methods report their retrieval results on it so far. We first show the R@K evaluation metric in Table 4. Our method outperforms the state-of-theart methods in most of the evaluation metrics. The results of the sentence generation task with a comparison of our RNN baseline are shown in Table 5."
    }, {
      "heading" : "8 Conclusion",
      "text" : "We propose a multimodal Recurrent Neural Network (m-RNN) framework that performs at the state-of-the-art in three tasks: sentence generation, sentence retrieval given query image and image\nretrieval given query sentence. Our m-RNN can be extended to use more complex image features (e.g. object detection features) and more sophisticated language models."
    } ],
    "references" : [ {
      "title" : "Matching words and pictures",
      "author" : [ "K. Barnard", "P. Duygulu", "D. Forsyth", "N. De Freitas", "D.M. Blei", "M.I. Jordan" ],
      "venue" : "JMLR, 3:1107–1135",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "A survey of smoothing techniques for me models",
      "author" : [ "S.F. Chen", "R. Rosenfeld" ],
      "venue" : "TSAP, 8(1):37–50",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Decaf: A deep convolutional activation feature for generic visual recognition",
      "author" : [ "J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell" ],
      "venue" : "arXiv preprint arXiv:1310.1531",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Finding structure in time",
      "author" : [ "J.L. Elman" ],
      "venue" : "Cognitive science, 14(2):179–211",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "Every picture tells a story: Generating sentences from images",
      "author" : [ "A. Farhadi", "M. Hejrati", "M.A. Sadeghi", "P. Young", "C. Rashtchian", "J. Hockenmaier", "D. Forsyth" ],
      "venue" : "In ECCV,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2010
    }, {
      "title" : "et al",
      "author" : [ "A. Frome", "G.S. Corrado", "J. Shlens", "S. Bengio", "J. Dean", "T. Mikolov" ],
      "venue" : "Devise: A deep visual-semantic embedding model. In NIPS, pages 2121–2129",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Rich feature hierarchies for accurate object detection and semantic segmentation",
      "author" : [ "R. Girshick", "J. Donahue", "T. Darrell", "J. Malik" ],
      "venue" : "CVPR",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "The iapr tc-12 benchmark: A new evaluation resource for visual information systems",
      "author" : [ "M. Grubinger", "P. Clough", "H. Müller", "T. Deselaers" ],
      "venue" : "International Workshop OntoImage, pages 13–23",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Multiple instance metric learning from automatically labeled bags of faces",
      "author" : [ "M. Guillaumin", "J. Verbeek", "C. Schmid" ],
      "venue" : "ECCV, pages 634–647",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "From image annotation to image description",
      "author" : [ "A. Gupta", "P. Mannem" ],
      "venue" : "ICONIP",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Choosing linguistics over vision to describe images",
      "author" : [ "A. Gupta", "Y. Verma", "C. Jawahar" ],
      "venue" : "AAAI",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Framing image description as a ranking task: Data",
      "author" : [ "M. Hodosh", "P. Young", "J. Hockenmaier" ],
      "venue" : "models and evaluation metrics. JAIR, 47:853–899",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Learning cross-modality similarity for multinomial data",
      "author" : [ "Y. Jia", "M. Salzmann", "T. Darrell" ],
      "venue" : "ICCV, pages 2407–2414",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Deep fragment embeddings for bidirectional image sentence mapping",
      "author" : [ "A. Karpathy", "A. Joulin", "L. Fei-Fei" ],
      "venue" : "arXiv:1406.5679",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Multimodal neural language models",
      "author" : [ "R. Kiros", "R. Zemel", "R. Salakhutdinov" ],
      "venue" : "ICML",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "NIPS, pages 1097–1105",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Baby talk: Understanding and generating image descriptions",
      "author" : [ "G. Kulkarni", "V. Premraj", "S. Dhar", "S. Li", "Y. Choi", "A.C. Berg", "T.L. Berg" ],
      "venue" : "CVPR",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Efficient backprop",
      "author" : [ "Y.A. LeCun", "L. Bottou", "G.B. Orr", "K.-R. Müller" ],
      "venue" : "Neural networks: Tricks of the trade, pages 9–48. Springer",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics",
      "author" : [ "C.-Y. Lin", "F.J. Och" ],
      "venue" : "ACL, page 605",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "T. Mikolov", "K. Chen", "G. Corrado", "J. Dean" ],
      "venue" : "arXiv preprint arXiv:1301.3781",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Recurrent neural network based language model",
      "author" : [ "T. Mikolov", "M. Karafiát", "L. Burget", "J. Cernockỳ", "S. Khudanpur" ],
      "venue" : "INTERSPEECH, pages 1045–1048",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Extensions of recurrent neural network language model",
      "author" : [ "T. Mikolov", "S. Kombrink", "L. Burget", "J. Cernocky", "S. Khudanpur" ],
      "venue" : "ICASSP, pages 5528–5531",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean" ],
      "venue" : "NIPS, pages 3111–3119",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Midge: Generating image descriptions from computer vision detections",
      "author" : [ "M. Mitchell", "X. Han", "J. Dodge", "A. Mensch", "A. Goyal", "A. Berg", "K. Yamaguchi", "T. Berg", "K. Stratos", "H. Daumé III" ],
      "venue" : "EACL",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Three new graphical models for statistical language modelling",
      "author" : [ "A. Mnih", "G. Hinton" ],
      "venue" : "ICML, pages 641–648. ACM",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu" ],
      "venue" : "ACL, pages 311–318",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Collecting image annotations using amazon’s mechanical turk",
      "author" : [ "C. Rashtchian", "P. Young", "M. Hodosh", "J. Hockenmaier" ],
      "venue" : "NAACL-HLT workshop 2010, pages 139–147",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Learning representations by back-propagating errors",
      "author" : [ "D.E. Rumelhart", "G.E. Hinton", "R.J. Williams" ],
      "venue" : "Cognitive modeling",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "Grounded compositional semantics for finding and describing images with sentences",
      "author" : [ "R. Socher", "Q. Le", "C. Manning", "A. Ng" ],
      "venue" : "TACL",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Multimodal learning with deep boltzmann machines",
      "author" : [ "N. Srivastava", "R. Salakhutdinov" ],
      "venue" : "NIPS, pages 2222–2230",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "The effectiveness of our model is validated on three benchmark datasets (IAPR TC-12 [8], Flickr 8K [28], and Flickr 30K [13]).",
      "startOffset" : 84,
      "endOffset" : 87
    }, {
      "referenceID" : 26,
      "context" : "The effectiveness of our model is validated on three benchmark datasets (IAPR TC-12 [8], Flickr 8K [28], and Flickr 30K [13]).",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 15,
      "context" : "The image part contains a deep Convulutional Neural Network (CNN) [17] which extracts image features.",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 7,
      "context" : "In the experiments, we validate our model on three benchmark datasets: IAPR TC-12 [8], Flickr 8K [28], and Flickr 30K [13].",
      "startOffset" : 82,
      "endOffset" : 85
    }, {
      "referenceID" : 26,
      "context" : "In the experiments, we validate our model on three benchmark datasets: IAPR TC-12 [8], Flickr 8K [28], and Flickr 30K [13].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 15,
      "context" : "al [17] proposed a deep convolutional neural networks with 8 layers (denoted as AlexNet) for image classification tasks and outperformed previous methods by a large margin.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 6,
      "context" : "al [7] proposed a object detection framework based on AlexNet.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 20,
      "context" : "For natural language, the Recurrent Neural Network shows the state-of-the-art performance in many tasks, such as speech recognition and word embedding learning [22, 23, 24].",
      "startOffset" : 160,
      "endOffset" : 172
    }, {
      "referenceID" : 21,
      "context" : "For natural language, the Recurrent Neural Network shows the state-of-the-art performance in many tasks, such as speech recognition and word embedding learning [22, 23, 24].",
      "startOffset" : 160,
      "endOffset" : 172
    }, {
      "referenceID" : 22,
      "context" : "For natural language, the Recurrent Neural Network shows the state-of-the-art performance in many tasks, such as speech recognition and word embedding learning [22, 23, 24].",
      "startOffset" : 160,
      "endOffset" : 172
    }, {
      "referenceID" : 11,
      "context" : "Many works treat the task of describe images as a retrieval task and formulate the problem as a ranking or embedding learning problem [12, 6, 30].",
      "startOffset" : 134,
      "endOffset" : 145
    }, {
      "referenceID" : 5,
      "context" : "Many works treat the task of describe images as a retrieval task and formulate the problem as a ranking or embedding learning problem [12, 6, 30].",
      "startOffset" : 134,
      "endOffset" : 145
    }, {
      "referenceID" : 28,
      "context" : "Many works treat the task of describe images as a retrieval task and formulate the problem as a ranking or embedding learning problem [12, 6, 30].",
      "startOffset" : 134,
      "endOffset" : 145
    }, {
      "referenceID" : 28,
      "context" : "al [30] uses dependency tree Recursive Neural network to extract sentence features) as well as the image features.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 13,
      "context" : "al [15] showed that object level image features based on object detection results will generate better results than image features extracted at the global level.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 23,
      "context" : "They parse the sentence and divide it into several parts [25, 10].",
      "startOffset" : 57,
      "endOffset" : 65
    }, {
      "referenceID" : 9,
      "context" : "They parse the sentence and divide it into several parts [25, 10].",
      "startOffset" : 57,
      "endOffset" : 65
    }, {
      "referenceID" : 16,
      "context" : "[18] uses a Conditional Random Field model and [5] uses a Markov Random Field model).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 4,
      "context" : "[18] uses a Conditional Random Field model and [5] uses a Markov Random Field model).",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 29,
      "context" : "sentences and images), using for example, Deep Boltzmann Machines [31], and topic models [1, 14].",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 0,
      "context" : "sentences and images), using for example, Deep Boltzmann Machines [31], and topic models [1, 14].",
      "startOffset" : 89,
      "endOffset" : 96
    }, {
      "referenceID" : 12,
      "context" : "sentences and images), using for example, Deep Boltzmann Machines [31], and topic models [1, 14].",
      "startOffset" : 89,
      "endOffset" : 96
    }, {
      "referenceID" : 14,
      "context" : "[16], which is built on a Log-BiLinear model [26].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "[16], which is built on a Log-BiLinear model [26].",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 3,
      "context" : "We briefly introduce the simple Recurrent Neural Network (RNN) or Elman network [4] that is widely used for many natural language processing tasks, such as speech recognition [22, 23].",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 20,
      "context" : "We briefly introduce the simple Recurrent Neural Network (RNN) or Elman network [4] that is widely used for many natural language processing tasks, such as speech recognition [22, 23].",
      "startOffset" : 175,
      "endOffset" : 183
    }, {
      "referenceID" : 21,
      "context" : "We briefly introduce the simple Recurrent Neural Network (RNN) or Elman network [4] that is widely used for many natural language processing tasks, such as speech recognition [22, 23].",
      "startOffset" : 175,
      "endOffset" : 183
    }, {
      "referenceID" : 27,
      "context" : "Accordingly, when we do the backpropagation, we need to propagate the error through recurrent connections back in time [29].",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 19,
      "context" : "Secondly, the dense word embedding encodes the semantic meanings of the words [21].",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 13,
      "context" : "Most of the sentence-image multimodal models [15, 6, 30, 16] use pre-computed word embedding vectors as the initialization of their model.",
      "startOffset" : 45,
      "endOffset" : 60
    }, {
      "referenceID" : 5,
      "context" : "Most of the sentence-image multimodal models [15, 6, 30, 16] use pre-computed word embedding vectors as the initialization of their model.",
      "startOffset" : 45,
      "endOffset" : 60
    }, {
      "referenceID" : 28,
      "context" : "Most of the sentence-image multimodal models [15, 6, 30, 16] use pre-computed word embedding vectors as the initialization of their model.",
      "startOffset" : 45,
      "endOffset" : 60
    }, {
      "referenceID" : 14,
      "context" : "Most of the sentence-image multimodal models [15, 6, 30, 16] use pre-computed word embedding vectors as the initialization of their model.",
      "startOffset" : 45,
      "endOffset" : 60
    }, {
      "referenceID" : 15,
      "context" : ") as the Rectified Linear Unit (ReLU), inspired by its the recent success when training very deep structure in computer vision field [17].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 27,
      "context" : "When backpropagation through time (BPTT) [29] is conducted for RNN with sigmoid function, the vanishing gradient problem appears since even the simplest RNN model can have a large temporal depth.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 20,
      "context" : "Previous methods [22, 23] used heuristics, such as truncated BPTT, to avoid this problem.",
      "startOffset" : 17,
      "endOffset" : 25
    }, {
      "referenceID" : 21,
      "context" : "Previous methods [22, 23] used heuristics, such as truncated BPTT, to avoid this problem.",
      "startOffset" : 17,
      "endOffset" : 25
    }, {
      "referenceID" : 15,
      "context" : "Here we connect the seventh layer of AlexNet [17] to the multimodal layer (please refer to Section 5 for more details).",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 17,
      "context" : ") is the element-wised scaled hyperbolic tangent function [19]:",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 15,
      "context" : "the AlexNet [17]).",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 15,
      "context" : "For the image part, we connect the seventh layer of a pre-trained Convolutional Neural Network [17, 3] (denoted as AlexNet).",
      "startOffset" : 95,
      "endOffset" : 102
    }, {
      "referenceID" : 2,
      "context" : "For the image part, we connect the seventh layer of a pre-trained Convolutional Neural Network [17, 3] (denoted as AlexNet).",
      "startOffset" : 95,
      "endOffset" : 102
    }, {
      "referenceID" : 2,
      "context" : "The same features extracted from the seventh layer of AlexNet (also denoted as decaf features [3]) are widely used by previous multimodal methods [16, 6, 15, 30].",
      "startOffset" : 94,
      "endOffset" : 97
    }, {
      "referenceID" : 14,
      "context" : "The same features extracted from the seventh layer of AlexNet (also denoted as decaf features [3]) are widely used by previous multimodal methods [16, 6, 15, 30].",
      "startOffset" : 146,
      "endOffset" : 161
    }, {
      "referenceID" : 5,
      "context" : "The same features extracted from the seventh layer of AlexNet (also denoted as decaf features [3]) are widely used by previous multimodal methods [16, 6, 15, 30].",
      "startOffset" : 146,
      "endOffset" : 161
    }, {
      "referenceID" : 13,
      "context" : "The same features extracted from the seventh layer of AlexNet (also denoted as decaf features [3]) are widely used by previous multimodal methods [16, 6, 15, 30].",
      "startOffset" : 146,
      "endOffset" : 161
    }, {
      "referenceID" : 28,
      "context" : "The same features extracted from the seventh layer of AlexNet (also denoted as decaf features [3]) are widely used by previous multimodal methods [16, 6, 15, 30].",
      "startOffset" : 146,
      "endOffset" : 161
    }, {
      "referenceID" : 13,
      "context" : "A recent multimodal retrieval work [15] showed that using the RCNN object detection framework [7] combined with the decaf features significantly improves the performance.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 6,
      "context" : "A recent multimodal retrieval work [15] showed that using the RCNN object detection framework [7] combined with the decaf features significantly improves the performance.",
      "startOffset" : 94,
      "endOffset" : 97
    }, {
      "referenceID" : 13,
      "context" : "In the experiments, we show that our method performs much better than [15] when the same image features are used, and is better than or comparable to their results even when they use more sophisticated features based on object detection.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 7,
      "context" : "We test our method on three benchmark datasets with sentence level annotations: IAPR TC-12 [8], Flickr 8K [28], and Flickr 30K [13].",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 26,
      "context" : "We test our method on three benchmark datasets with sentence level annotations: IAPR TC-12 [8], Flickr 8K [28], and Flickr 30K [13].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 8,
      "context" : "of training and testing set as previous works [9, 16].",
      "startOffset" : 46,
      "endOffset" : 53
    }, {
      "referenceID" : 14,
      "context" : "of training and testing set as previous works [9, 16].",
      "startOffset" : 46,
      "endOffset" : 53
    }, {
      "referenceID" : 13,
      "context" : "We follow the previous work [15] which used 1,000 images for testing.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 25,
      "context" : "B-1, B-2, and B-3) [27, 20] as the evaluation metrics.",
      "startOffset" : 19,
      "endOffset" : 27
    }, {
      "referenceID" : 18,
      "context" : "B-1, B-2, and B-3) [27, 20] as the evaluation metrics.",
      "startOffset" : 19,
      "endOffset" : 27
    }, {
      "referenceID" : 14,
      "context" : "To conduct a fair comparison, we adopt the same sentence generation steps and experiment settings as [16], and generate as many words as there are in the reference sentences to calculate BLEU.",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 28,
      "context" : "Sentence Retrieval and Image Retrieval For Flickr8K and Flickr30K datasets, we adopted the same evaluation metrics as previous works [30, 6, 15] for both the tasks of sentences retrieval and image retrieval.",
      "startOffset" : 133,
      "endOffset" : 144
    }, {
      "referenceID" : 5,
      "context" : "Sentence Retrieval and Image Retrieval For Flickr8K and Flickr30K datasets, we adopted the same evaluation metrics as previous works [30, 6, 15] for both the tasks of sentences retrieval and image retrieval.",
      "startOffset" : 133,
      "endOffset" : 144
    }, {
      "referenceID" : 13,
      "context" : "Sentence Retrieval and Image Retrieval For Flickr8K and Flickr30K datasets, we adopted the same evaluation metrics as previous works [30, 6, 15] for both the tasks of sentences retrieval and image retrieval.",
      "startOffset" : 133,
      "endOffset" : 144
    }, {
      "referenceID" : 14,
      "context" : "For IAPR TC-12 datasets, we adopt exactly the same evaluation metrics as [16] and plot the mean number of matches of the retrieved groundtruth sentences or images with respect to the percentage of the retrieved sentences or images for the testing set.",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 14,
      "context" : "For sentences retrieval task, [16] used a shortlist of 100 images which are the nearest neighbors of the query image in the feature space.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 1,
      "context" : "BACK-OFF GT2 and GT3 are n-grams methods with Katz backoff and Good-Turing discounting [2, 16].",
      "startOffset" : 87,
      "endOffset" : 94
    }, {
      "referenceID" : 14,
      "context" : "BACK-OFF GT2 and GT3 are n-grams methods with Katz backoff and Good-Turing discounting [2, 16].",
      "startOffset" : 87,
      "endOffset" : 94
    }, {
      "referenceID" : 14,
      "context" : "To conduct a fair comparison, we followed the same experimental settings of [16], include the context length to calculate the BLEU scores and perplexity.",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 24,
      "context" : "059 LBL [26] 20.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 14,
      "context" : "068 MLBL-B-DeCAF [16] 24.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 14,
      "context" : "098 MLBL-F-DeCAF [16] 21.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 10,
      "context" : "[11] / 0.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 9,
      "context" : "01 Gupta & Mannem [10] / 0.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 14,
      "context" : "For sentence retrieval task, we used a shortlist of 100 images as the three comparing methods shown in [16].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 14,
      "context" : "[16] further improve their results after the publication.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "DeepFE-rcnn [15] uses a fragment mapping strategy to better exploit the object detection results.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 28,
      "context" : "0 500 Socher-decaf [30] 4.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 28,
      "context" : "0 29 Socher-avg-rcnn [30] 6.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 5,
      "context" : "7 25 DeViSE-avg-rcnn [6] 4.",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 13,
      "context" : "6 29 DeepFE-decaf [15] 5.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 13,
      "context" : "5 32 DeepFE-rcnn [15] 12.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 5,
      "context" : "0 500 DeViSE-avg-rcnn [6] 4.",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 13,
      "context" : "6 29 DeepFE-rcnn [15] 16.",
      "startOffset" : 17,
      "endOffset" : 21
    } ],
    "year" : 2014,
    "abstractText" : "In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel sentence descriptions to explain the content of images. It directly models the probability distribution of generating a word given previous words and the image. Image descriptions are generated by sampling from this distribution. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. The effectiveness of our model is validated on three benchmark datasets (IAPR TC-12 [8], Flickr 8K [28], and Flickr 30K [13]). Our model outperforms the state-of-the-art generative method. In addition, the m-RNN model can be applied to retrieval tasks for retrieving images or sentences, and achieves significant performance improvement over the state-of-the-art methods which directly optimize the ranking objective function for retrieval.",
    "creator" : "LaTeX with hyperref package"
  }
}