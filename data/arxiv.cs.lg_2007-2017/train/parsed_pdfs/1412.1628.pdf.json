{
  "name" : "1412.1628.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Fisher Kernel for Deep Neural Activations",
    "authors" : [ "Donggeun Yoo", "Sunggyun Park", "Joon-Young Lee" ],
    "emails" : [ "dgyoo@rcv.kaist.ac.kr,", "sunggyun@kaist.ac.kr,", "jylee@rcv.kaist.ac.kr,", "iskweon77@kaist.ac.kr" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Image representation is one of the most important factors that affect performance on visual recognition tasks. Barbu et al. [3] introduced an interesting experiment that a simple classifier along with human brain-scan data substantially outperforms the state-of-the-art methods in recognizing action from video clips.\nWith a success of local descriptors [19], many researches devoted deep study to global image representation based on a Bag-of-Word (BOW) model [29] that aggregates abundant local statistics captured by hand-designed local descriptors. The BOW representation is further improved with VLAD [14] and Fisher kernel [24, 23] by adding higher order statistics. One major benefit of these global representations based on local descriptors is their invariance property to scale changes, location changes, occlusions and background clutters.\nIn recent computer vision researches, drastic advances of visual recognition are achieved by deep convolutional neural networks (CNNs) [5], which jointly learn the whole feature hierarchies starting from image pixels to the final class posterior with stacked non-linear processing layers. A deep representation is quite efficient since its intermediate templates are reused. However, the deep CNN is nonlinear and have millions of parameters to be estimated. It requires strong computing power for the optimization and large training data to be generalized well. The recent presence of large scale ImageNet [6] database and the raise of parallel computing contribute to the breakthrough in visual recognition. Krizhevsky et al. [17] achieved an impressive result using a CNN in large-scale image classification.\nInstead of training a CNN for a specific task, intermediate activations extracted from a CNN pre-trained on independent large data have been successfully applied as a generic image representation. Combining the CNN activations with a classifier has shown impressive performance in wide visual recognition tasks such as object classification [26, 8, 22, 13, 4], object detection [11, 13], scene classification [26, 12, 36], fine-grained classification [26, 34], attribute recognition [35], image retrieval [2], and domain transfer [8].\nFor utilizing CNN activations as a generic image representation, a straightforward way is to extract the responses from the first or second fully connected layer of a pretrained CNN by feeding an image and to represent the image with the responses [8, 2, 13, 11]. However, this representation is vulnerable to geometric variations. There are techniques to address the problem. A common practice is exploiting multiple jitterred images (random crops and flips) for data augmentation. Though the data augmentation has been used to prevent over-fitting [17], recent researches show that average pooling, augmenting data and averaging the multiple activation vectors in a test stage, also helps to achieve better geometric invariance of CNNs while improving classification performance by +2.92% in [4] and +3.3%\nar X\niv :1\n41 2.\n16 28\nv2 [\ncs .C\nV ]\n1 9\nD ec\nin [26] on PASCAL VOC 2007. A different experiment for enhancing the geometric invariance on CNN activations was also presented. Gong et al. [12] proposed a method to exploit multi-scale CNN activations in order to achieve geometric invariance characteristic while improving recognition accuracy. They extracted dense local patches at three different scales and fed each local patch into a pre-trained CNN. The CNN activations are aggregated at finer scales via VLAD encoding which was introduced in [14], and then the encoded activations are concatenated as a single vector to obtain the final representation.\nIn this paper, we introduce a multi-scale pyramid pooling to improve the discriminative power of CNN activations robust to geometric variations. A pipeline of the proposed method is illustrated in Figure 1. Similar to [12], we also utilize multi-scale CNN activations, but present a different pooling method that shows better performance in our experiments. Specifically, we suggest an efficient way to obtain abundant amount of multi-scale local activations from a CNN, and aggregate them using the state-of-the-art Fisher kernel [24, 23] with a simple but important scale-wise normalization, so called multi-scale pyramid pooling. Our proposal demonstrates substantial improvements on both scene and object classification tasks compared to the previous representations including a single activation, the average pooling [26, 4], and the VLAD of activations [12]. Also, we demonstrate object confidence maps which is useful for object detection/localization though only category-level labels without specific object bounding boxes are used in training.\nAccording to our empirical observations, replacing a VLAD kernel with a Fisher kernel does not present significant impact, however it shows meaningful performance improvements when our pooling mechanism that takes an average pooling after scale-wise normalization is applied.\nIt implies that the performance improvement of our representation does not come just from the superiority of Fisher kernel but from the careful consideration of neural activation’s property dependent on scales."
    }, {
      "heading" : "2. Multi-scale Pyramid Pooling",
      "text" : "In this section, we first review the Fisher kernel framework and then introduce a multi-scale pyramid pooling which adds a Fisher kernel based pooling layer on top of a pre-trained CNN."
    }, {
      "heading" : "2.1. Fisher Kernel Review",
      "text" : "The Fisher kernel framework on a visual vocabulary is proposed by Perronnin et al. in [23]. It extends the conventional Bag-of-Words model to a probabilistic generative model. It models the distribution of low-level descriptors using a Gaussian Mixture Model (GMM) and represents an image by considering the gradient with respect to the model parameters. Although the number of local descriptors varies across images, the consequent Fisher vector has a fixed-length, therefore it is possible to use discriminative classifiers such as a linear SVM.\nLet x denote a d-dimensional local descriptor and Gλ = {gk, k = 1...K} denote a pre-trained GMM with K Gaussians where λ = {ωk, µk, σk, k = 1...K}. For each visual word gk, two gradient vectors, Gµk ∈ <d and Gσk ∈ <d, are computed by aggregating the gradients of the local descriptors extracted from an image with respect to the mean and the standard deviation of the kth Gaussian. Then, the final image representation, Fisher vector, is obtained by concatenating all the gradient vectors. Accordingly, the Fisher kernel framework represents an image with a 2Kddimensional Fisher vector G ∈ <2Kd.\nIntuitively, a Fisher vector includes the information\n…\nScale\npyramid Reform\nConv. 1\n(11 11 3 96)\nFC 6\n(6 6 256 4,096)\nMulti-scale\ndense activations\nActivation\npyramid\nabout directions of model parameters to best fit the local descriptors of an image to the GMM. The fisher kernel framework is further improved in [24] by the additional two-stage normalizations: power-normalization with the factor of 0.5 followed by `2-normalization. Refer to [24] for the theoretical proofs and details."
    }, {
      "heading" : "2.2. Dense CNN Activations",
      "text" : "To obtain multi-scale activations from a CNN without modification, previous approach cropped local patches and fed the patches into a network after resizing the patches to the fixed size of CNN input. However, when we extract multi-scale local activations densely, the approach is quite inefficient since many redundant operations are performed in convolutional layers for overlapped regions.\nTo extract dense CNN activations without redundant operations, we simply replace the fully connected layers of an existing CNN with equivalent multiple convolution filters along spatial axises. When an image larger than the fixed size is fed, the modified network outputs multiple activation vectors where each vector is CNN activations from the corresponding local patch. The procedure is illustrated in Fig. 2. With this method, thousands of dense local activations (4,410 per image) from multiple scale levels are extracted in a reasonable extraction time (0.46 seconds per image) as shown in Table 1."
    }, {
      "heading" : "2.3. Multi-scale Pyramid Pooling (MPP)",
      "text" : "For representing an image, we first generate a scale pyramid for the input image where the minimum scale image has a fixed size of a CNN and each scale image has two times larger resolution than the previous scale image. We feed all the scaled images into a pre-trained CNN and extract dense CNN activation vectors. Then, all the activation vectors are merged into a single vector by our multi-scale pyramid pooling.\nIf we consider each activation vector as a local descriptor, it is straightforward to aggregate all the local activations into a Fisher vector as explained in Sec. 2.1. However, CNN activations have different scale properties compared to SIFT-like local descriptors, as will be explained in Sec. 3. To adopt the Fisher kernel suitable to CNN activation characteristics, we introduce adding a multi-scale pyramid pooling layer on top of the modified CNN as follows.\nGiven a scale pyramid S containing N scaled image and local activation vectors xs extracted from each scale s ∈ S, we first apply PCA to reduce the dimension of activation vectors and obtain x′s. Then, we aggregate the local activation vectors x′s of each scale s to each Fisher vector Gs. After Fisher encoding, we have N Fisher vectors and they are merged into one global vector by average pooling after `2-normalization as\nGS = 1 N ∑ s∈S Gs ‖Gs‖2 s.t. Gs = 1 |x′s| ∑ x∈x′s ∇λ logGλ(x), (1) where |·| denotes the cardinality of a set. We use an average pooling since it is a natural pooling scheme for Fisher kernel rather than vector concatenation. Following the Improved Fisher Kernel framework [24], we finally apply power normalization and `2-normalization to the Fisher vector GS . The overall pipeline of MPP is illustrated in Figure 1.\n20\n30\n40\n50\nm A\nP (%\n)\nSIFT−Fisher\n1( 12 ) 2( 65 )\n3( 17 3) 4( 45 6)\n5( 1,\n03 6)\n6( 2,\n35 4)\n7( 4,\n95 0)\nImage Scale (Num. of Desc.)\n60\n65\n70\n75\n80\nm A\nP (%\n)\nCNN−Fisher\n1( 1) 2( 9)\n3( 64 ) 4( 19 6) 5( 48 4) 6( 1, 15 6) 7( 2, 50 0)\nImage Scale (Num. of Desc.)\nFigure 3. Classification performance of SIFT-Fisher and CNNFisher according to image scale on PASCAL VOC 2007. The tick labels of the horizontal axis denote image scales and their average number of local descriptors."
    }, {
      "heading" : "3. Analysis of Multi-scale CNN Activations",
      "text" : "We compare scale characteristics between traditional local features and CNN activations. It tells us that it is not suitable to directly adopt a Fisher kernel framework to multi-scale local CNN activations for representing an image. To investigate the best way for aggregating the CNN activations into a global representation, we perform empirical studies and conclude that applying scale-wise normalization of Fisher vectors is very important.\nA naive way to obtain a Fisher vector G′S given multiscale local activationsX = {x ∈ xs, s ∈ S} is to aggregate them as\nG′S = 1 |X| ∑ s∈S ∑ x∈xs ∇λ logGλ(x). (2)\nHere, every multi-scale local activation vector is pooled to one Fisher vector with an equal weight of 1/|X|.\nTo better combine a Fisher kernel with mid-level neural activations, the property of CNN activations according to patch scale should be took in consideration. In the traditional use of Fisher kernel on visual classification tasks, the hand-designed local descriptors such as SIFT [19] have been often densely computed in multi-scale. This local descriptor encodes low-level gradient information within an local region and captures detailed textures or shapes within a small region rather than the global structure within a larger region. In contrast, a mid-level neural activation extracted from a higher layer of CNNs (e.g. FC6 or FC7 of [17]) represents higher level structure information which is closer to class posteriors. As shown in the CNN visualization proposed by Zeiler and Fergus in [33], image regions strongly activated by a certain CNN filter of the fifth layer usually capture a category-level entire object.\nTo figure out the different scale properties between the Fisher vector of traditional SIFT (SIFT-Fisher) and that of neural activation from FC7 (CNN-Fisher), we conduct an\n60\n65\n70\n75\nA cc\nu ra\ncy (\n% )\nMIT Indoor 67\n1 ~ 4 1 ~ 5 1 ~ 6 1 ~ 7 Image scales\nNaive Fisher Proposed MPP\n65\n70\n75\n80\nm A\nP (\n% )\nPASCAL VOC 2007\n1 ~ 4 1 ~ 5 1 ~ 6 1 ~ 7 Image scales\nNaive Fisher Proposed MPP 84\n86\n88\n90\n92\n94\nA cc\nu ra\ncy (\n% )\nOxford 102 Flowers\n1 ~ 4 1 ~ 5 1 ~ 6 1 ~ 7 Image scales\nNaive Fisher Proposed MPP\nFigure 4. Classification performance of our multi-scale pyramid pooling in Eq. (1) and the naive Fisher pooling in Eq. (2). The tick labels of the horizontal axis scale levels in a scale pyramid.\nempirical analysis with scale-wise classification scores on PASCAL VOC 2007 [9]. For the analysis, we first diversify dataset into seven different scale levels from the smallest scale of 227×227 resolution to the biggest scale of 1, 816× 1, 816 resolution and extract both dense SIFT descriptors and local activation vectors in the seventh layer (FC7) of our CNN. Then, we follow the standard framework to encode Fisher vectors and to train an independent linear SVM for each scale, respectively.\nIn Fig. 3, we show the results of classification performances using SIFT-Fisher and CNN-Fisher according to scale. The figure demonstrates clear contrast between SIFTFisher and CNN-Fisher. CNN-Fisher performs worst at the largest image scale since local activations come from small image regions in an original image, while SIFTFisher performs best at the same scale since SIFT properly captures low-level contents within such small regions. If we aggregate the CNN activations of all scales into one Fisher vector by Eq. (2), the poorly performing 2,500 activations will have dominant influence with the large weight of 2,500/4,410 in the image representation.\nOne possible strategy for aggregating multi-scale CNN activations is to choose activations of a set of scales relatively performing well. However, the selection of good scales is dependent on dataset and the activations from the large image scale can also contribute to geometric invariance property if we balance the influence of each scale. We empirically examined various combinations of pooling as will be shown in Sec. 4 and we found that scale-wise Fisher vector normalization followed by an simple average pooling is effective to balance the influence.\nWe perform an experiment to compare our pooling method in Eq. (1) to the naive Fisher pooling in Eq. (2). In the experiment, we apply both of two pooling methods with five different numbers of scales and perform classification on PASCAL VOC 2007. Despite the simplicity of our multi-scale pyramid pooling, it demonstrates superior\nperformances as depicted in Fig. 4. The performance of the naive Fisher kernel pooling in Eq. (2) deteriorates rapidly when finer scale levels are involved. This is because indistinctive neural activations from finer scale levels become dominant in forming a Fisher vector. Our representation, however, exhibits stable performance that the accuracy is constantly increasing and finally being saturated. It verify that our pooling method aggregates multi-scale CNN activations effectively."
    }, {
      "heading" : "4. Experiments",
      "text" : ""
    }, {
      "heading" : "4.1. Datasets",
      "text" : "To evaluate our proposal as a generic image representation, we conduct three different visual recognition tasks with following datasets.\nMIT Indoor 67 [25] is used for a scene classification task. The dataset contains 15,620 images with 67 indoor scene classes in total. It is a challenging dataset because many indoor classes are characterized by the objects they contain (e.g. different type of stores) rather than their spatial properties. The performance is measured with top-1 accuracy.\nPASCAL VOC 2007 [9] is used for an object classification task. It consists of 9,963 images of 20 object classes in total. The task is quite difficult since the scales of the objects fluctuate and multiple objects of different classes are often contained in the same image. The performance is measured with (11-points interpolated) mean average precision.\nOxford 102 Flowers [21] is used for a fine-grained object classification task, which distinguishes the sub-classes of the same object class. This dataset consists of 8,189 images with 102 flower classes. Each class consists of various numbers of images from 20 to 258. The performance is measured with top-1 accuracy."
    }, {
      "heading" : "4.2. Pre-trained CNNs",
      "text" : "We use two CNNs pre-trained on the ILSVRC’12 dataset [6] to extract multi-scale local activations. One is the Caffe reference model [15] composed of five convolutional layers and three fully connected layers. This model performed 19.6% top-5 error when a single center-crop of each validation image are used for evaluation on the ILSVRC’12 dataset. Henceforth, we denote this model by “Alex” since it is nearly the same architecture of Krizhevsky et al.’s CNN [17].\nThe other one is Chatfield et al.’s CNN-S model [4] (“CNNS”, henceforth). This model, a simplified version of\nthe OverFeat [27], is also composed of five convolutional layers (three in [27]) and three fully connected layers. It shows 15.5% top-5 error on the ILSVRC’12 dataset with the same center-crop. Compared to Alex, it uses 7×7 smaller filters but dense stride of 2 in the first convolutional layer.\nOur experiments are conducted mostly with the Alex by default. The CNNS is used only for the PASCAL VOC 2007 dataset to compare our method with [4], which demonstrates excellent performance with the CNNS. Both of the two pre-trained models are available online [31]."
    }, {
      "heading" : "4.3. Implementation Details",
      "text" : "We use an image pyramid of seven scales by default since the seven scales can cover large enough scale variations and performance in all datasets as shown in Fig. 4.\nThe overall procedure of our image representation is as follows. Given an image, we make an image pyramid containing seven scaled images. Each image in the pyramid has twice resolution than the previous scale starting from the standard size defined in each CNN (e.g. 227×227 for Alex). We then feed each scale image to the CNN and obtain 4,410 vectors of 4,096 dimensional dense CNN activations from the seventh layer. The dimensionality of each activation vector is reduced to 128 by PCA where a projection is trained with 256,000 activation vectors sampled from training images. A visual vocabulary (GMM of 256 Gaussian distributions) is also trained with the same samples. Consequently, one 65,536 dimensional Fisher vector is computed by Eq. (1), and further power- and `2-normalization follow. One-versus-rest linear SVMs with a quadratic regularizer and a hinge loss are trained finally.\nOur system is mostly implemented using open source libraries including VLFeat [30] for a Fisher kernel framework and MatConvNet [31] for CNNs."
    }, {
      "heading" : "4.4. Results and Analysis",
      "text" : "We perform comprehensive experiments to compare various methods on the three recognition tasks. We first show the performance of our method and baseline methods. Then, we compare our result with state-of-the-art methods for each dataset. For simplicity, we use a notation protocol “A(B)” where A denotes a pooling method and B denotes descriptors to be pooled by A. The notations are summarized in Table 2.\nWe compare our method with several baseline methods. The baseline methods include intermediate CNN activations from a pre-trained CNN with a standard input, an average pooling with multiple jittered images, and modified versions of our method. The comparison results for each dataset are summarized in Table 3(a), 4(a), 6(a). As expected, the most basic representation, Alex-FC7, performs the worst for all datasets. The average pooling in AP10 and AP50 improves the performance +1.39%∼+3%, how-\never the improvement is bounded regardless of the number of data augmentation. The other two baseline methods (MPP w/o SN and CSF) exploit multi-scale CNN activations and they show better results than single-scale representations. Compared to the AP10, the performance gains from multi-scale activations exceed +10%, +1%, and +5% for each dataset. It shows that image representation based on CNN activations can be enriched by utilizing multi-scale local activations.\nEven though baseline methods exploiting multi-scale CNN activations show substantial improvements compared to the single-scale baselines, we can also verify that handling multi-scale activations is important for further improvement. Compared to the naive Fisher kernel pooling (NFK) in Eq. (2), our MPP achieves an extra but significant performance gain of +4.18%, +4.58%, and 2.84% for each dataset. Instead of pooling multi-scale activations as our MPP, concatenating encoded Fisher vectors can be another option as done in Gong et al.’s method [12]. The concatenation (CSF) also improves the performance, however the CSF without an additional dimension reduction raises the dimensionality proportional to the number of scales and the MPP still outperforms the CSF for all datasets. The comprehensive test with various pooling strategies so far shows that the proposed image representation can be used as a primary image representation in wide visual recognition tasks.\nWe also apply the spatial pyramid (SP) kernel [18] to our representation. We construct a spatial pyramid into four sub-regions (whole, top, middle, bottom) and it increases the dimensionality of our representation four times. The results are unequable but the differences are marginal for all datasets. This result is not surprising because the rich activations from smaller image scales already cover the global layout. It makes the SP kernel redundant.\nIn Table 3(b), we compare our result with various stateof-the-art methods on Indoor 67. Similar to ours, Gong et al. [12] proposed a pooling method for multi-scale CNN activations. They performed VLAD pooling at each scale and concatenated them. Compared to [12], our representation largely outperforms the method with a gain of +7.07%. The performance gap possibly comes from 1) the large number of scales, 2) the superiority of the Fisher kernel, and 3) the details of pooling strategy. While they use only three scales, we extract seven-scale activations with a quite efficient way (Fig. 2). Though adding local activations from very finer scales such as 6 or 7 in a naive way may harm the performance, it actually contribute to a better invariance property by the proposed MPP. In addition, as our experiment of the “CSF” was shown, the MPP is more suitable for aggregating multi-scale activations than the concatenation. It implies that our better performance does not just come from the superior Fisher kernel, but from the better handling of multi-scale neural activations.\nThe record holder in the Indoor 67 dataset has been Zuo et al. [37] who combined the Alex-FC6 and their complementary features so called DSFL. The DSFL learns discriminative and shareable filters with a target dataset. When we stack an additional MPP at the Pool5 layer, we already achieve a state-of-the-art performance (77.76%) with a pretrained Alex only. We also stack the DSFL feature1 over our representation and the result shows the performance of 80.78%. It shows that our representation is also improved by combining complementary features.\nThe results on VOC 2007 is summarized in Table 4(b). There are two methods ([22] and [26]) that use the same Alex network. Razavian et al. [26] performed target data augmentation and Oquab et al. [22] used a multi-layer perceptron (MLP) instead of a linear SVM with ground truth bounding boxes. Our representation outperforms the two methods using the pre-trained Alex without data augmentation or the use of bounding box annotations. The gains are +1.84% and +2.34% respectively.\nThere are recent methods outperforming our method. All of them are adopting better CNNs for the source task (i.e. ImageNet classification) or the target task, such as Spatial Pyramid Pooling (SPP) network [13], Multi-label CNN [32] and the CNNS [4]. Our basic MPP(Alex-FC7) demonstrates slightly lower precisions (79.54%) compared to them, however we use the basic Alex CNN without finetuning on VOC 2007. When our representation is equipped with the superior CNNS [4], which is not fine-tuned on VOC 2007, our representation (81.40%) reaches nearly statof-the-art performance and our method is further improved to 82.13% by stacking MPP(CNNS-FC8). The performance is still lower than [4], who conduct target data augmentation and fine-tuning. We believe our method can be further improved by additional techniques such as fine-tuning, target data augmentation, or use of ground truth bounding boxes, we leave the issue as future work because our major focus is a generic image representation with a pre-trained CNN.\nTable 6(b) shows the classification performances on 102 Flowers. Our method (91.28%) outperforms the previous state-of-the-art method [26] (86.80%). Without the use of a powerful CNN representation, various previous methods show much lower performances.\nTable 5 shows per-class performances on VOC 2007. Compared to state-of-the-art methods, our method performs best in 6 classes among 20 classes. It is interesting that the 6 classes include “bottle”, “pottedplant”, and “tvmonitor”, which are the representative small objects in the VOC 2007 dataset. The results clearly demonstrates the benefit of our MPP that aggregates activations from very finer-scales as well, which are prone to harm the performance if it is handled inappropriately.\n1Pre-computed DSFL vectors for the MIT Indoor 67 dataset are provided by the authors."
    }, {
      "heading" : "4.5. Weakly-Supervised Object Confidence Map",
      "text" : "One interesting feature of our method is that we can present object confidence maps for object classification tasks, though we train the SVM classifiers without bounding box annotation but only with class-level labels. To recover confidence maps, we trace how much weight is given to each local patch and accumulate all the weights of local activations. Tracing the weight of local activations is possible because our final representation can be formed regardless of the number of scales and the number of local activation vectors. To trace the weight of each patch, we compute our final representation per patch using the corresponding single activation vector only and compute the score from the pre-trained SVM classifiers we used for object classification.\nFig. 5 and Fig. 6 show several examples of object confidence map on the VOC 2007 test images. In the figures, we can verify our image representation encodes the discriminative image patches well, despite large within-class variations as well as substantial geometric changes. As we discussed in Sec. 4.4, the images containing small-size objects also present the accurate confidence maps. These maps may further be utilized as an considerable cue for object detection/localization and also be useful for analyzing image representation."
    }, {
      "heading" : "5. Discussion",
      "text" : "We have proposed the multi-scale pyramid pooling for better use of neural activations from a pre-trained CNN. There are several conclusions we can derive through our study. First, we should take the scale characteristic of neural activations into consideration for the successful combination of a Fisher kernel and a CNN. The activations become uninformative as a patch size becomes smaller, however they can contribute to better scale invariance when they meet a simple scale-wise normalization. Second, dense deep neural activations from multiple scale levels are extracted with reasonable computations by replacing the fully connection with equivalent multiple convolution filters. It enables us to pool the truly multi-scale activations and to achieve significant performance improvements on the visual recognition tasks. Third, reasonable object-level confidence maps can be obtained from our image representation even though only class-level labels are given for supervision, which can be further applied to object detection or localization tasks. In the comprehensive experiments on three different recognition tasks, the results suggest that our proposal can be used as a primary image representation for better performances in various visual recognition tasks."
    } ],
    "references" : [ {
      "title" : "Efficient object detection and segmentation for fine-grained recognition",
      "author" : [ "A. Angelova", "S. Zhu" ],
      "venue" : "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2013
    }, {
      "title" : "Neural codes for image retrieval",
      "author" : [ "A. Babenko", "A. Slesarev", "A. Chigorin", "V. Lempitsky" ],
      "venue" : "In Proceedings of European Conference on Computer Vision (ECCV),",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2014
    }, {
      "title" : "Seeing is worse than believing: Reading people’s minds better than computer-vision methods recognize actions",
      "author" : [ "A. Barbu", "D.P. Barrett", "W. Chen", "S. Narayanaswamy", "C. Xiong", "J.J. Corso", "C.D. Fellbaum", "C. Hanson", "S.J. Hanson", "S. Hélie", "E. Malaia", "B.A. Pearlmutter", "J.M. Siskind", "T.M. Talavage", "R.B. Wilbur" ],
      "venue" : "In Proceedings of European Conference on Computer Vision (ECCV),",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2014
    }, {
      "title" : "Return of the devil in the details: Delving deep into convolutional nets",
      "author" : [ "K. Chatfield", "K. Simonyan", "A. Vedaldi", "A. Zisserman" ],
      "venue" : "In Proceedings of British Machine Vision Conference (BMVC),",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2014
    }, {
      "title" : "Backpropagation applied to handwritten zip code recognition",
      "author" : [ "Y.L. Cun", "B. Boser", "J.S. Denker", "D. Henderson", "R. Howard", "W. Hubbard", "L. Jackel" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1989
    }, {
      "title" : "Imagenet: A large-scale hierarchical image database",
      "author" : [ "J. Deng", "W. Dong", "R. Socher", "L.J. Li", "K. Li", "L.F. Fei" ],
      "venue" : "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2009
    }, {
      "title" : "Mid-level visual element discovery as discriminative mode seeking",
      "author" : [ "C. Doersch", "A. Gupta", "A.A. Efros" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2013
    }, {
      "title" : "DeCAF: A deep convolutional activation feature for generic visual recognition",
      "author" : [ "J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell" ],
      "venue" : "In Computing Research Repository (CoRR),",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2013
    }, {
      "title" : "The PASCAL Visual Object Classes Challenge",
      "author" : [ "M. Everingham", "L. Van Gool", "C.K.I. Williams", "J. Winn", "A. Zisserman" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2007
    }, {
      "title" : "Mining midlevel features for image classification",
      "author" : [ "B. Fernando", "É. Fromont", "T. Tuytelaars" ],
      "venue" : "International Journal on Computer Vision (IJCV),",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2014
    }, {
      "title" : "Rich feature hierarchies for accurate object detection and semantic segmentation",
      "author" : [ "R.B. Girshick", "J. Donahue", "T. Darrell", "J. Malik" ],
      "venue" : "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2014
    }, {
      "title" : "Multi-scale orderless pooling of deep convolutional activations features",
      "author" : [ "Y. Gong", "L. Wang", "R. Guo", "S. Lazebnik" ],
      "venue" : "In Proceedings of European Conference on Computer Vision (ECCV),",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2014
    }, {
      "title" : "Spatial pyramid pooling in deep convolutional networks for visual recognition",
      "author" : [ "K. He", "X. Zhang", "S. Ren", "J. Sun" ],
      "venue" : "In Proceedings of European Conference on Computer Vision (ECCV),",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2014
    }, {
      "title" : "Aggregating local descriptors into a compact image representation",
      "author" : [ "H. Jegou", "M. Douze", "C. Schmid", "P. Pérez" ],
      "venue" : "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2010
    }, {
      "title" : "Caffe: Convolutional architecture for fast feature embedding",
      "author" : [ "Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell" ],
      "venue" : "arXiv preprint arXiv:1408.5093,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2014
    }, {
      "title" : "Blocks that shout: Distinctive parts for scene classification",
      "author" : [ "M. Juneja", "A. Vedaldi", "C.V. Jawahar", "A. Zisserman" ],
      "venue" : "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2013
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2013
    }, {
      "title" : "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories",
      "author" : [ "S. Lazebnik", "C. Schmid", "J. Ponce" ],
      "venue" : "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2006
    }, {
      "title" : "Distinctive image features from scale-invariant keypoints",
      "author" : [ "D.G. Lowe" ],
      "venue" : "International Journal on Computer Vision (IJCV),",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2004
    }, {
      "title" : "Generalized max pooling",
      "author" : [ "N. Murray", "F. Perronnin" ],
      "venue" : "Computing Research Repository (CoRR),",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2014
    }, {
      "title" : "Automated flower classification over a large number of classes",
      "author" : [ "M.-E. Nilsback", "A. Zisserman" ],
      "venue" : "In Proceedings of the Indian Conference on Computer Vision, Graphics and Image Processing,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2008
    }, {
      "title" : "Learning and transferring mid-level image representations using convolutional neural networks",
      "author" : [ "M. Oquab", "L. Bottou", "I. Laptev", "J. Sivic" ],
      "venue" : "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2014
    }, {
      "title" : "Fisher kernels on visual vocabularies for image categorization",
      "author" : [ "F. Perronnin", "C.R. Dance" ],
      "venue" : "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2007
    }, {
      "title" : "Improving the fisher kernel for large-scale image classification",
      "author" : [ "F. Perronnin", "J. Sánchez", "T. Mensink" ],
      "venue" : "In Proceedings of European Conference on Computer Vision (ECCV),",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2010
    }, {
      "title" : "Recognizing indoor scenes",
      "author" : [ "A. Quattoni", "A.Torralba" ],
      "venue" : "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2009
    }, {
      "title" : "CNN features off-the-shelf: an astounding baseline for recognition",
      "author" : [ "A.S. Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson" ],
      "venue" : "In Computing Research Repository (CoRR),",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2014
    }, {
      "title" : "Overfeat: Integrated recognition, localization and detection using convolutional networks",
      "author" : [ "P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun" ],
      "venue" : "In Proceedings of International Conference on Learning Representations (ICLR),",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2014
    }, {
      "title" : "Unsupervised discovery of mid-level discriminative patches",
      "author" : [ "S. Singh", "A. Gupta", "A.A. Efros" ],
      "venue" : "In Proceedings of European Conference on Computer Vision (ECCV),",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2012
    }, {
      "title" : "Video google: A text retrieval approach to object matching in videos",
      "author" : [ "J. Sivic", "A. Zisserman" ],
      "venue" : "In Proceedings of IEEE International Conference on Computer Vision (ICCV),",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2003
    }, {
      "title" : "VLFeat: An open and portable library of computer vision algorithms",
      "author" : [ "A. Vedaldi", "B. Fulkerson" ],
      "venue" : "http://www. vlfeat.org/,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2008
    }, {
      "title" : "MatConvNet: Convolutional neural networks for matlab",
      "author" : [ "A. Vedaldi", "K. Lenc" ],
      "venue" : "http://www.vlfeat.org/ matconvnet/,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2014
    }, {
      "title" : "Cnn: Single-label to multi-label",
      "author" : [ "Y. Wei", "W. Xia", "J. Huang", "B. Ni", "J. Dong", "Y. Zhao", "S. Yan" ],
      "venue" : "In Computing Research Repository (CoRR),",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2014
    }, {
      "title" : "Visualizing and understanding convolutional networks",
      "author" : [ "M.D. Zeiler", "R. Fergus" ],
      "venue" : "In Proceedings of European Conference on Computer Vision (ECCV),",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2014
    }, {
      "title" : "Part-based R-CNNs for fine-grained category detection",
      "author" : [ "N. Zhang", "J. Donahue", "R.B. Girshick", "T. Darrell" ],
      "venue" : "In Proceedings of European Conference on Computer Vision (ECCV),",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2014
    }, {
      "title" : "PANDA: Pose aligned networks for deep attribute modeling",
      "author" : [ "N. Zhang", "M. Paluri", "M. Ranzato", "T. Darrell", "L.D. Bourdev" ],
      "venue" : "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2014
    }, {
      "title" : "Learning deep features for scene recognition using places database",
      "author" : [ "B. Zhou", "A. Lapedriza", "J. Xiao", "A. Torralba", "A. Oliva" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2014
    }, {
      "title" : "Learning discriminative and shareable features for scene classification",
      "author" : [ "Z. Zuo", "G. Wang", "B. Shuai", "L. Zhao", "Q. Yang", "X. Jiang" ],
      "venue" : "In Proceedings of European Conference on Computer Vision (ECCV),",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "[3] introduced an interesting experiment that a simple classifier along with human brain-scan data substantially outperforms the state-of-the-art methods in recognizing action from video clips.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 18,
      "context" : "With a success of local descriptors [19], many researches devoted deep study to global image representation based on a Bag-of-Word (BOW) model [29] that aggregates abundant local statistics captured by hand-designed local descriptors.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 28,
      "context" : "With a success of local descriptors [19], many researches devoted deep study to global image representation based on a Bag-of-Word (BOW) model [29] that aggregates abundant local statistics captured by hand-designed local descriptors.",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 13,
      "context" : "The BOW representation is further improved with VLAD [14] and Fisher kernel [24, 23] by adding higher order statistics.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 23,
      "context" : "The BOW representation is further improved with VLAD [14] and Fisher kernel [24, 23] by adding higher order statistics.",
      "startOffset" : 76,
      "endOffset" : 84
    }, {
      "referenceID" : 22,
      "context" : "The BOW representation is further improved with VLAD [14] and Fisher kernel [24, 23] by adding higher order statistics.",
      "startOffset" : 76,
      "endOffset" : 84
    }, {
      "referenceID" : 4,
      "context" : "In recent computer vision researches, drastic advances of visual recognition are achieved by deep convolutional neural networks (CNNs) [5], which jointly learn the whole feature hierarchies starting from image pixels to the final class posterior with stacked non-linear processing layers.",
      "startOffset" : 135,
      "endOffset" : 138
    }, {
      "referenceID" : 5,
      "context" : "The recent presence of large scale ImageNet [6] database and the raise of parallel computing contribute to the breakthrough in visual recognition.",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 16,
      "context" : "[17] achieved an impressive result using a CNN in large-scale image classification.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 25,
      "context" : "Combining the CNN activations with a classifier has shown impressive performance in wide visual recognition tasks such as object classification [26, 8, 22, 13, 4], object detection [11, 13], scene classification [26, 12, 36], fine-grained classification [26, 34], attribute recognition [35], image retrieval [2], and domain transfer [8].",
      "startOffset" : 144,
      "endOffset" : 162
    }, {
      "referenceID" : 7,
      "context" : "Combining the CNN activations with a classifier has shown impressive performance in wide visual recognition tasks such as object classification [26, 8, 22, 13, 4], object detection [11, 13], scene classification [26, 12, 36], fine-grained classification [26, 34], attribute recognition [35], image retrieval [2], and domain transfer [8].",
      "startOffset" : 144,
      "endOffset" : 162
    }, {
      "referenceID" : 21,
      "context" : "Combining the CNN activations with a classifier has shown impressive performance in wide visual recognition tasks such as object classification [26, 8, 22, 13, 4], object detection [11, 13], scene classification [26, 12, 36], fine-grained classification [26, 34], attribute recognition [35], image retrieval [2], and domain transfer [8].",
      "startOffset" : 144,
      "endOffset" : 162
    }, {
      "referenceID" : 12,
      "context" : "Combining the CNN activations with a classifier has shown impressive performance in wide visual recognition tasks such as object classification [26, 8, 22, 13, 4], object detection [11, 13], scene classification [26, 12, 36], fine-grained classification [26, 34], attribute recognition [35], image retrieval [2], and domain transfer [8].",
      "startOffset" : 144,
      "endOffset" : 162
    }, {
      "referenceID" : 3,
      "context" : "Combining the CNN activations with a classifier has shown impressive performance in wide visual recognition tasks such as object classification [26, 8, 22, 13, 4], object detection [11, 13], scene classification [26, 12, 36], fine-grained classification [26, 34], attribute recognition [35], image retrieval [2], and domain transfer [8].",
      "startOffset" : 144,
      "endOffset" : 162
    }, {
      "referenceID" : 10,
      "context" : "Combining the CNN activations with a classifier has shown impressive performance in wide visual recognition tasks such as object classification [26, 8, 22, 13, 4], object detection [11, 13], scene classification [26, 12, 36], fine-grained classification [26, 34], attribute recognition [35], image retrieval [2], and domain transfer [8].",
      "startOffset" : 181,
      "endOffset" : 189
    }, {
      "referenceID" : 12,
      "context" : "Combining the CNN activations with a classifier has shown impressive performance in wide visual recognition tasks such as object classification [26, 8, 22, 13, 4], object detection [11, 13], scene classification [26, 12, 36], fine-grained classification [26, 34], attribute recognition [35], image retrieval [2], and domain transfer [8].",
      "startOffset" : 181,
      "endOffset" : 189
    }, {
      "referenceID" : 25,
      "context" : "Combining the CNN activations with a classifier has shown impressive performance in wide visual recognition tasks such as object classification [26, 8, 22, 13, 4], object detection [11, 13], scene classification [26, 12, 36], fine-grained classification [26, 34], attribute recognition [35], image retrieval [2], and domain transfer [8].",
      "startOffset" : 212,
      "endOffset" : 224
    }, {
      "referenceID" : 11,
      "context" : "Combining the CNN activations with a classifier has shown impressive performance in wide visual recognition tasks such as object classification [26, 8, 22, 13, 4], object detection [11, 13], scene classification [26, 12, 36], fine-grained classification [26, 34], attribute recognition [35], image retrieval [2], and domain transfer [8].",
      "startOffset" : 212,
      "endOffset" : 224
    }, {
      "referenceID" : 35,
      "context" : "Combining the CNN activations with a classifier has shown impressive performance in wide visual recognition tasks such as object classification [26, 8, 22, 13, 4], object detection [11, 13], scene classification [26, 12, 36], fine-grained classification [26, 34], attribute recognition [35], image retrieval [2], and domain transfer [8].",
      "startOffset" : 212,
      "endOffset" : 224
    }, {
      "referenceID" : 25,
      "context" : "Combining the CNN activations with a classifier has shown impressive performance in wide visual recognition tasks such as object classification [26, 8, 22, 13, 4], object detection [11, 13], scene classification [26, 12, 36], fine-grained classification [26, 34], attribute recognition [35], image retrieval [2], and domain transfer [8].",
      "startOffset" : 254,
      "endOffset" : 262
    }, {
      "referenceID" : 33,
      "context" : "Combining the CNN activations with a classifier has shown impressive performance in wide visual recognition tasks such as object classification [26, 8, 22, 13, 4], object detection [11, 13], scene classification [26, 12, 36], fine-grained classification [26, 34], attribute recognition [35], image retrieval [2], and domain transfer [8].",
      "startOffset" : 254,
      "endOffset" : 262
    }, {
      "referenceID" : 34,
      "context" : "Combining the CNN activations with a classifier has shown impressive performance in wide visual recognition tasks such as object classification [26, 8, 22, 13, 4], object detection [11, 13], scene classification [26, 12, 36], fine-grained classification [26, 34], attribute recognition [35], image retrieval [2], and domain transfer [8].",
      "startOffset" : 286,
      "endOffset" : 290
    }, {
      "referenceID" : 1,
      "context" : "Combining the CNN activations with a classifier has shown impressive performance in wide visual recognition tasks such as object classification [26, 8, 22, 13, 4], object detection [11, 13], scene classification [26, 12, 36], fine-grained classification [26, 34], attribute recognition [35], image retrieval [2], and domain transfer [8].",
      "startOffset" : 308,
      "endOffset" : 311
    }, {
      "referenceID" : 7,
      "context" : "Combining the CNN activations with a classifier has shown impressive performance in wide visual recognition tasks such as object classification [26, 8, 22, 13, 4], object detection [11, 13], scene classification [26, 12, 36], fine-grained classification [26, 34], attribute recognition [35], image retrieval [2], and domain transfer [8].",
      "startOffset" : 333,
      "endOffset" : 336
    }, {
      "referenceID" : 7,
      "context" : "For utilizing CNN activations as a generic image representation, a straightforward way is to extract the responses from the first or second fully connected layer of a pretrained CNN by feeding an image and to represent the image with the responses [8, 2, 13, 11].",
      "startOffset" : 248,
      "endOffset" : 262
    }, {
      "referenceID" : 1,
      "context" : "For utilizing CNN activations as a generic image representation, a straightforward way is to extract the responses from the first or second fully connected layer of a pretrained CNN by feeding an image and to represent the image with the responses [8, 2, 13, 11].",
      "startOffset" : 248,
      "endOffset" : 262
    }, {
      "referenceID" : 12,
      "context" : "For utilizing CNN activations as a generic image representation, a straightforward way is to extract the responses from the first or second fully connected layer of a pretrained CNN by feeding an image and to represent the image with the responses [8, 2, 13, 11].",
      "startOffset" : 248,
      "endOffset" : 262
    }, {
      "referenceID" : 10,
      "context" : "For utilizing CNN activations as a generic image representation, a straightforward way is to extract the responses from the first or second fully connected layer of a pretrained CNN by feeding an image and to represent the image with the responses [8, 2, 13, 11].",
      "startOffset" : 248,
      "endOffset" : 262
    }, {
      "referenceID" : 16,
      "context" : "Though the data augmentation has been used to prevent over-fitting [17], recent researches show that average pooling, augmenting data and averaging the multiple activation vectors in a test stage, also helps to achieve better geometric invariance of CNNs while improving classification performance by +2.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 3,
      "context" : "92% in [4] and +3.",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 25,
      "context" : "in [26] on PASCAL VOC 2007.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 11,
      "context" : "[12] proposed a method to exploit multi-scale CNN activations in order to achieve geometric invariance characteristic while improving recognition accuracy.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "The CNN activations are aggregated at finer scales via VLAD encoding which was introduced in [14], and then the encoded activations are concatenated as a single vector to obtain the final representation.",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 11,
      "context" : "Similar to [12], we also utilize multi-scale CNN activations, but present a different pooling method that shows better performance in our experiments.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 23,
      "context" : "Specifically, we suggest an efficient way to obtain abundant amount of multi-scale local activations from a CNN, and aggregate them using the state-of-the-art Fisher kernel [24, 23] with a simple but important scale-wise normalization, so called multi-scale pyramid pooling.",
      "startOffset" : 173,
      "endOffset" : 181
    }, {
      "referenceID" : 22,
      "context" : "Specifically, we suggest an efficient way to obtain abundant amount of multi-scale local activations from a CNN, and aggregate them using the state-of-the-art Fisher kernel [24, 23] with a simple but important scale-wise normalization, so called multi-scale pyramid pooling.",
      "startOffset" : 173,
      "endOffset" : 181
    }, {
      "referenceID" : 25,
      "context" : "Our proposal demonstrates substantial improvements on both scene and object classification tasks compared to the previous representations including a single activation, the average pooling [26, 4], and the VLAD of activations [12].",
      "startOffset" : 189,
      "endOffset" : 196
    }, {
      "referenceID" : 3,
      "context" : "Our proposal demonstrates substantial improvements on both scene and object classification tasks compared to the previous representations including a single activation, the average pooling [26, 4], and the VLAD of activations [12].",
      "startOffset" : 189,
      "endOffset" : 196
    }, {
      "referenceID" : 11,
      "context" : "Our proposal demonstrates substantial improvements on both scene and object classification tasks compared to the previous representations including a single activation, the average pooling [26, 4], and the VLAD of activations [12].",
      "startOffset" : 226,
      "endOffset" : 230
    }, {
      "referenceID" : 22,
      "context" : "in [23].",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 14,
      "context" : "With Caffe reference model [15], FC7 activations are extracted from 100 random images of PASCAL VOC 2007.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 23,
      "context" : "The fisher kernel framework is further improved in [24] by the additional two-stage normalizations: power-normalization with the factor of 0.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 23,
      "context" : "Refer to [24] for the theoretical proofs and details.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 23,
      "context" : "Following the Improved Fisher Kernel framework [24], we finally apply power normalization and `2-normalization to the Fisher vector G .",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 18,
      "context" : "In the traditional use of Fisher kernel on visual classification tasks, the hand-designed local descriptors such as SIFT [19] have been often densely computed in multi-scale.",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 16,
      "context" : "FC6 or FC7 of [17]) represents higher level structure information which is closer to class posteriors.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 32,
      "context" : "As shown in the CNN visualization proposed by Zeiler and Fergus in [33], image regions strongly activated by a certain CNN filter of the fifth layer usually capture a category-level entire object.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 8,
      "context" : "empirical analysis with scale-wise classification scores on PASCAL VOC 2007 [9].",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 24,
      "context" : "MIT Indoor 67 [25] is used for a scene classification task.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 8,
      "context" : "PASCAL VOC 2007 [9] is used for an object classification task.",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 20,
      "context" : "Oxford 102 Flowers [21] is used for a fine-grained object classification task, which distinguishes the sub-classes of the same object class.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 5,
      "context" : "We use two CNNs pre-trained on the ILSVRC’12 dataset [6] to extract multi-scale local activations.",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 14,
      "context" : "One is the Caffe reference model [15] composed of five convolutional layers and three fully connected layers.",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 16,
      "context" : "’s CNN [17].",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 3,
      "context" : "’s CNN-S model [4] (“CNNS”, henceforth).",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 26,
      "context" : "This model, a simplified version of the OverFeat [27], is also composed of five convolutional layers (three in [27]) and three fully connected layers.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 26,
      "context" : "This model, a simplified version of the OverFeat [27], is also composed of five convolutional layers (three in [27]) and three fully connected layers.",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 3,
      "context" : "The CNNS is used only for the PASCAL VOC 2007 dataset to compare our method with [4], which demonstrates excellent performance with the CNNS.",
      "startOffset" : 81,
      "endOffset" : 84
    }, {
      "referenceID" : 30,
      "context" : "Both of the two pre-trained models are available online [31].",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 29,
      "context" : "Our system is mostly implemented using open source libraries including VLFeat [30] for a Fisher kernel framework and MatConvNet [31] for CNNs.",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 30,
      "context" : "Our system is mostly implemented using open source libraries including VLFeat [30] for a Fisher kernel framework and MatConvNet [31] for CNNs.",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 11,
      "context" : "’s method [12].",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 17,
      "context" : "We also apply the spatial pyramid (SP) kernel [18] to our representation.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 11,
      "context" : "[12] proposed a pooling method for multi-scale CNN activations.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "Compared to [12], our representation largely outperforms the method with a gain of +7.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 36,
      "context" : "[37] who combined the Alex-FC6 and their complementary features so called DSFL.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "There are two methods ([22] and [26]) that use the same Alex network.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 25,
      "context" : "There are two methods ([22] and [26]) that use the same Alex network.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 25,
      "context" : "[26] performed target data augmentation and Oquab et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "[22] used a multi-layer perceptron (MLP) instead of a linear SVM with ground truth bounding boxes.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "ImageNet classification) or the target task, such as Spatial Pyramid Pooling (SPP) network [13], Multi-label CNN [32] and the CNNS [4].",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 31,
      "context" : "ImageNet classification) or the target task, such as Spatial Pyramid Pooling (SPP) network [13], Multi-label CNN [32] and the CNNS [4].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 3,
      "context" : "ImageNet classification) or the target task, such as Spatial Pyramid Pooling (SPP) network [13], Multi-label CNN [32] and the CNNS [4].",
      "startOffset" : 131,
      "endOffset" : 134
    }, {
      "referenceID" : 3,
      "context" : "When our representation is equipped with the superior CNNS [4], which is not fine-tuned on VOC 2007, our representation (81.",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 3,
      "context" : "The performance is still lower than [4], who conduct target data augmentation and fine-tuning.",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 25,
      "context" : "28%) outperforms the previous state-of-the-art method [26] (86.",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 23,
      "context" : "[24] 10’ No.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 25,
      "context" : "[26] ’14 No.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "[22] ’14 No.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 31,
      "context" : "[32] ’14 Yes.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 3,
      "context" : "[4] ’14 Yes.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 36,
      "context" : "56 Ours MPP(Alex-FC7)+DSFL[37] Yes.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 27,
      "context" : "[28] ’12 Part+GIST+DPM+SP No.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "[16] ’13 IFK+Bag-of-Parts No.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 6,
      "context" : "[7] ’13 IFK+MidlevelRepresent.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 36,
      "context" : "[37] ’14 DSFL No.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 36,
      "context" : "[37] ’14 DSFL+Alex-FC6 Yes.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 35,
      "context" : "[36] ’14 Alex-FC7 Yes.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 35,
      "context" : "[36] ’14 Alex-FC7 Yes.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 25,
      "context" : "[26] ’14 AP(Alex)+PT+TargetAug.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[12] ’14 VLAD Concat.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "[24] 10’ IFK(SIFT+color) No.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "[13] ’14 SPPNET-FC7 No.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 31,
      "context" : "[32] ’14 Multi-label CNN Yes.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 25,
      "context" : "[26] ’14 AP(Alex)+PT+TA No.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "[22] ’14 Alex-FC7+MLP No.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 3,
      "context" : "[4] ’14 AP(CNNS-FC7)+TA No.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[4] ’14 AP(CNNS-FC7)+TA Yes.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 20,
      "context" : "Nilsback and Zisserman [21] ’08 Multple kernel learning Yes.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 0,
      "context" : "70 Angelova and Zhu [1] ’13 Seg+DenseHoG+LLC+MaxPooling Yes.",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 19,
      "context" : "70 Murray and Perronnin [20] ’14 GMP of IFK(SIFT+color) No.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 9,
      "context" : "[10] ’14 Bag-of-FLH Yes.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 25,
      "context" : "[26] ’14 AP(Alex)+PT+TA No.",
      "startOffset" : 0,
      "endOffset" : 4
    } ],
    "year" : 2014,
    "abstractText" : "Compared to image representation based on low-level local descriptors, deep neural activations of Convolutional Neural Networks (CNNs) are richer in mid-level representation, but poorer in geometric invariance properties. In this paper, we present a straightforward framework for better image representation by combining the two approaches. To take advantages of both representations, we propose an efficient method to extract a fair amount of multi-scale dense local activations from a pre-trained CNN. We then aggregate the activations by Fisher kernel framework, which has been modified with a simple scale-wise normalization essential to make it suitable for CNN activations. Replacing the direct use of a single activation vector with our representation demonstrates significant performance improvements: +17.76 (Acc.) on MIT Indoor 67 and +7.18 (mAP) on PASCAL VOC 2007. The results suggest that our proposal can be used as a primary image representation for better performances in visual recognition tasks.",
    "creator" : "LaTeX with hyperref package"
  }
}