{
  "name" : "1505.04657.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Mining User Opinions in Mobile App Reviews: A Keyword-based Approach",
    "authors" : [ "Phong Minh Vu", "Tam The Nguyen", "Hung Viet Pham", "Tung Thanh Nguyen" ],
    "emails" : [ "phong.vu@aggiemail.usu.edu", "tam.nguyen@aggiemail.usu.edu", "hung.pham@aggiemail.usu.edu", "tung.nguyen@usu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords—App Review, Opinion Mining, Keyword\nI. INTRODUCTION\nAs the use of mobile devices such as smartphones and tablets explodes, developing software applications (often called apps) for those devices is becoming a highly popular and profitable business in software development. However, it is also a highly competitive business, as millions and counting apps of different categories are available on major app markets like Apple App Stores or Google Play. Since revenue and profit of mobile apps are often proportional to the size of their userbases, improving user satisfaction to retain existing users and attract new ones is of important to mobile apps developers.\nTo improve user satisfaction, mobile app developers could rely on relevant user opinions such as complaints or suggestions. As most online app markets provide a rating and reviewing mechanism for users of mobile apps, user reviews on app markets provide an important source for such opinions. However, manual analysis of those reviews for useful opinions is often challenging and time-consuming. First, due to the sheer size of its userbase, a popular app (with million of users) often gets thousands of reviews each day. In addition, not all reviews are worth reading. Prior research reports that more than 60% of reviews are noise, i.e. do not contain useful opinions [1].\nIn this paper, we propose M.A.R.K, a semi-automated framework for mining user opinions in app reviews. Considering this mining task as an information retrieval problem, M.A.R.K follows a keyword-based approach. That is, it allows a review analyst to specify his/her interests/concerns by keywords. Then, it uses those keywords to search for and visualize\nthe most relevant reviews, expecting them to contain opinions usefully matched the analyst’s specified interests. The key departure of M.A.R.K from a typical information retrieval system is that it employs several automated, customized techniques for extracting keywords from raw reviews, ranking those keywords based on review ratings and occurrence frequencies, grouping related keywords, searching for reviews that are relevant to a set of keywords, visualizing their occurrences over time, and reporting if such occurrences contain unusual patterns.\nLet us illustrate M.A.R.K and those techniques via an example. Assume that a review analyst is interested in negative user opinions about Facebook Messenger (one of the most popular apps on Google Play with around 500 millions to 1 billion users as of May 2015) in a given time frame. Initially, he has no idea about what aspects of the app get negative opinions. So, M.A.R.K extracts all potential keywords from raw reviews of Facebook Messenger in the given time frame (using its customized keyword extraction technique discussed in details in Section III-B3). Then, it ranks those keywords based on a negative scoring scheme (discussed in details in Section III-B) and presents that list to the review analyst. Table I illustrates the top ones among them.\nAs seen in the table, several keywords are related and indicate a more general concern/issue. For example both keywords crash and freeze could be used to describe the app’s status when an unrecoverable error occurs. Or, battery and drain often go together to describe the bad energy consumption of the app. Therefore, the review analyst can use the grouping functionality of M.A.R.K to divide the listed keywords in to clusters (i.e. sub-groups), each for a more general concern. Table II illustrates the resulted clusters produced for Facebook Messenger.\nar X\niv :1\n50 5.\n04 65\n7v 1\n[ cs\n.I R\n] 1\n8 M\nay 2\n01 5\nThis clustering task is based on Word2Vec, a distributed, vector-based representation of words [2]. Word2Vec represents each word in a vocabulary as a high dimensional vector learned from a large corpus of text. Words having similar or related syntactic roles or semantic meanings often have similar vectors. Thus, M.A.R.K divides a group of keywords into smaller sub-groups of related ones by applying K-mean [3], a similarity-based clustering algorithm on the vectors representing those keywords. Details about Word2Vec and M.A.R.K’s grouping technique will be discussed in Section IV-B.\nRather than dividing a given set of keywords into smaller subsets, M.A.R.K can expand a small set of keywords into a bigger one, also based on the vector-based similarity of the keywords. This case often happens when the analyst has some ideas about the opinions he is looking for and want to explore them in a broader context. For example, when analyzing the keywords in Table I, the analyst sees the keyword battery and get interested in the energy consumption aspect of this app. He assumes keywords battery and drain are related to this topic, but expects users to use other keywords to describe it. Therefore, he requests M.A.R.K to expand his initial keyword set {battery, drain}. Figure 1 shows the expanding results, with newly added keywords like heat, power, and usage.\nOnce the analyst, either via clustering or expanding, specifies a set of keywords that match his interest/concern (such as ones in Figure 1), M.A.R.K queries its review database and returns ones that are most relevant to that keyword set. This querying task is based on the standard Vector Space Model. That is, M.A.R.K applies the tf.idf weighting scheme on the keywords and measures the relevancy between the given keyword set to a review as the cosine similarity of their tf.idf feature vectors. Table III illustrates some reviews queried for the keyword set in Figure 1. As seen, those reviews contain (negative) user opinions about the energy consumption aspect of this app.\nM.A.R.K can also visualize and analyze the occurrences of a keyword set overtime for abnormal patterns. Prior works [1], [4] suggest that there are sudden changes that often happen when a new version of an app is released, which contain some defects or issues that make many users unsatisfied. For example, Figure 6 illustrates the analysis result of M.A.R.K on the occurrences of the keywords related to energy consumption.\nAs seen, given a suitable threshold, M.A.R.K has detected an unusual increase of those keywords at Feb 2015, which was after the release of a new version of Facebook Messenger. Further investigating in this problem, we found that this happened because there was a critical error in the syncing functionality of this app (Section VI-D1). To detect such abnormalities, M.A.R.K considers the keyword occurrence counts as a timeseries, computes its moving average (SMA), and differences between actual counts and its SMA values. If a difference value is significant higher than the standard deviation of SMA values, it would indicate a sudden change in the corresponding occurrence count.\nIn general, this paper has the following contributions: (i) M.A.R.K, a semi-automated framework for review analysis of mobile apps; (ii) a keyword extraction technique with several customized regulation algorithms for user reviews of mobile apps; (iii) a keyword ranking technique based on review ratings and term frequencies; (iv) A keyword grouping technique based on a distributed, vector-based representation of words; (v) A technique for searching reviews relevant to a given keyword set; (vi) A technique for detecting abnormalities of keyword occurrences based on time-series analysis; and (vii) an empirical evaluation and several case studies for the accuracy and usefulness of M.A.R.K on our dataset of 2 millions Google Play Reviews.\nThe remaining of this paper is organized as the following. Section II discusses an architecture overview of M.A.R.K framework. Section III describes M.A.R.K’s keyword extraction and its customized regulation algorithms. Section IV discusses keyword analysis techniques used in M.A.R.K. Section V presents our trend abnormalities detection and visualization technique. Section VI presents our empirical evaluations and case studies. Related works are discussed in Section VII and we conclude our study in Section VIII."
    }, {
      "heading" : "II. SYSTEM OVERVIEW",
      "text" : "Figure 2 illustrates M.A.R.K’s system architecture. In general, M.A.R.K has three main components: Keyword Extractor, Keyword Analyzers, and Further Analysis & Visualization. Keyword Extractor takes raw reviews from designated app stores1 and extract keywords from them. After that, the keyword data is stored in a common database, which can be retrieved for further analysis by Keyword Analyzers (Keyword Dividing, Keywords Expanding, Keywords Ranking). The results of these processes are a keyword set or several clusters of keywords, which in turn can be used as input for searching relevant reviews or discovering abnormalities in their trends. Details of each techniques will be discussed further in the following sections."
    }, {
      "heading" : "III. DATA COLLECTION AND PREPROCESSING",
      "text" : "In this section, we describe how M.A.R.K process raw review data and store them for our following analysis. This includes: (i) pointing out the problems that we encountered when collecting words from reviews: too many misspelled words, acronyms and teen code, too many reviews written in other languages rather than English, not all parts of a sentence have interesting information; (ii) proposing a preprocessing\n1Google Play, Apple App Store, Amazon, Microsoft\nprocedure to solve the above problems and gather information we need for the later analyses."
    }, {
      "heading" : "A. Data Collecting",
      "text" : "Most App Markets provide user reviews to developers and third parties. While the provided information could be varied from one to another, most of them include: • Text: user’s textual review. • Rating: User’s sentiment about the app, range from 1\nto 5, in which 5 is the best and 1 is the worst. • Creation Time: the time when this review is created.\nIn our work, we need these information of each review, however, M.A.R.K Framework still crawl every other information provided by each App Market and store them for any future usage.\nMost markets limit the number of reviews to be retrieved by their API (For example, Google Play API limit the reviews to 500 at a time), therefore our crawler has to work continuously to acquire all the reviews of a time period."
    }, {
      "heading" : "B. Reviews Preprocessing and Keywords Extraction",
      "text" : "1) Misspelled words, acronym and teen-code: Most of the time, users of mobile apps have to input their reviews via a mobile device. However, the “fat fingers” problem2 makes it difficult for typing in small devices and leads to many typos when mobile device users want to input something. This also apply to the reviews people write with their smartphones on app markets. Through manual inspection, we found a lot of misspelled words in the reviews data. Table IV shows top 10 misspelled words in 300,000 reviews we collected during January 2015 on Google Play. We expect that there will be more words like them if the reviews number reaches millions. It is interesting that not all of the misspelled words we have found came from mistake. For instance, “frnds” is a common teen-code used by many young people on social media. Instagram, Facebook, Snapchat and many other apps in this area have these kinds of expression from users. This could lead to a major problem for our approach: Normalizing terms to find keywords.\nThere are many spelling correctors such as FAROO3 or Peter Norvig’s corrector4 to consider for correcting these\n2http://en.wikipedia.org/wiki/Typographical error 3http://blog.faroo.com/2012/06/07/improved-edit-distance-based-spelling-\ncorrection/ 4http://norvig.com/spell-correct.html\nwords. However, we have no control of how the corrected words turn out to be, and they could mistake foreign words with misspelled words, thus introduce noise to our non-English reviews classification process in the next section. To control the output, we make our own list of incorrect-to-correct words map without using any spell corrector.\nIn order to find the incorrect words, we use the US/UK English dictionary provided by Microsoft WinEdt project5. To make the vocabulary consistent, we only use the US part and map all the UK words into their US counterparts.\nAfter that, we find all the terms that is not in the dictionary and occurred more than 20 times in our sample of 300,000 reviews from Google Play. A team of 4 researchers will then read through all the terms and decide whether if a term is misspelled or not. If a term is not so obvious like “intagram→ instagram”, then the team will query all reviews that contain that term and read through them. If all 4 researchers agree on the correct form of a term, that term will be mapped to its correct form.\nA by-product of our process is an additional dictionary of app-review specific words that are not in any English dictionaries, such as app names, abbreviations, domain specific words like “reinstall” or “hashtag” or “xml”. In our work, it acts as a complementary dictionary for the original US one for classifying non-English reviews and spelling correction.\n2) Non-English Reviews Classifier: Not all users of apps speak English. Many of them came from different countries and write reviews in their own languages. For example, one popular language that are used in US region is Spanish. The terms extracted from those reviews introduce noises to our analysis and need to be removed. By manual inspection, we have found many reviews that are not in English but still use ASCII characters (Table V), therefore removing just the reviews with several non-ASCII characters like has done before in the work of Binfu et. al. [4] will not suffice. We need a better technique to exclude those reviews from later analyses.\n5English (US) dictionary: http://www.winedt.org/Dict/\nWith the dictionaries and the most common misspelled words map, it is possible to detect reviews with some percentages of foreign words. However, to make sure that we do not remove the English reviews that have several words not present in our dictionary, we compute the ratio of the English words that go consecutively together (bi-gram) with the total of words in each review. Our assumption is that English speakers should not make too many mistakes in a row on purpose. This ratio score, combined with the percentages of English words in the reviews (uni-gram) should provide us a good metric to classify foreign reviews apart from just some spelling mistakes. In Evaluation section, we experiment with this technique to find the best thresholds.\n3) Terms Normalization: From our manual inspection, we also found that the most interesting features/concerns that are described by users come from actions or functionalities related to the apps. Therefore we are only interested in Verbs and Nouns. We disregard Ajectives within this study because they usually just express users sentiments and has little meaning to the functions or features of apps( For example: “funny”, “bad”, “nice”. ). Other parts of speech like Prepositions, Modal Auxiliary and such do not contribute to the general understanding of user opinions at all. Therefore, we need to identify those parts and exclude them from our analysis.\nAnother thing is, in an English sentence, a Verb or Noun could come in various form such as plural or singular and many tenses. For reviews of apps, most forms and tenses of a word express the same functionality or action meaning (I.e: ”deleted account” is the same action as ”delete account”), therefore we should consider them as one term. It is best to just stem them into one base forms. In this kind of problem, Snowball Stemmer [5] is often used. However, it is known to over-stem words to a degree that the words lost their original semantic. For example, “conspiracy”, “conspirator”, “conspire”, “conspired” and “conspiring” are all stemmed into “conspir”. This is not suitable for keyword approach because we require the words to retain original information. Lemmatization tools such as Stanford Lemmatizer [6] may works well but it is too slow since it will try to lemmatize all words, not just Nouns and Verbs.\nOur final solution is to design our own Customized Stemmer (Figure 3), which inspired from Snowball and lemmatization tools with the common English rules. Our Stemmer uses the WSJ-trained Stanford Part of Speech (PoS) tagger [7] to find PoS for each word in each sentence. Within the results,it selects all plural Nouns and normally tensed Verbs. For irregular verbs which rules can not be applied, it map them directly to their basic form using an Irregular Verb List6. The main algorithm applies basic English rules and some chosen rules from Snowball Stemmer for transforming\n6http://www.usingenglish.com/reference/irregular-verbs/\nselected Nouns and Verbs to infer their base forms (Table VI). After the respective rule is applied, there is a chance that the word is over-stemmed. To counter this problem, the algorithm completes the word by a tri-gram model for probability of occurrence of the last 3 characters for English words. It further improves the correctness of the result by applying the FAROO Spelling Corrector for its fast nature before output the final result. Both tri-gram model and FAROO spelling corrector are trained on the First Billion characters form Wikipedia Dataset7.\nWe evaluate our algorithm and compare it with the widely used Stanford Lemmatizer in Section VI-B2.\n4) Data storage and Vector-based representations: In M.A.R.K, we have a step of grouping keywords to find common concerns among them. To do such task, we need to find words with common meanings or relationships and put them together. Simple statistical models such as topic modeling [8] can capture topics based on the frequency of a word in different documents, however, it is not intended to capture semantic regularities of words. Recent studies on vector-space distributed representation of words had suggested that words can have multiple degrees of similarity [9]. Exploiting this property of word vectors could help us to group semantically similar keywords together. We discuss more on this in Section IV-B. In our work, we use Word2vec tool to learn the vectorrepresentations of words and store them in a database for later use.\n7http://mattmahoney.net/dc/textdata\nAfter normalization step, in every app, we count the occurrence of each keyword over time for each rating level. This information can be later used to rank keywords and analyze their trends. The time unit is day, thus each word will have 5 vectors of daily occurrences for five rating levels. It is also relatively simple to just update the daily occurrences when new reviews arrive. The following definitions explain how we store our data: • A = {a1, a2, ..., aN} is a discrete set of N vocabular-\nies for N apps from app stores. • ai = {w1, w2, ..., wk} is a set of k keywords found in\nreviews of ai • wk = {v,D} where v is the vector-based repre-\nsentation of wk AND D = {r1, r2, r3, r4, r5} contains word counts rx for 5 each rating level, rx = {d1, d2, ..., dj} contains count for each day of j days."
    }, {
      "heading" : "IV. KEYWORD ANALYSIS",
      "text" : "In this section, we describe our approach to analyze keywords extracted from reviews to infer user opinions. The section includes: (i) Using rating and frequency of keywords to rank them and find the most popular keywords that users often mention in their concerns; (ii) grouping keywords to find common concerns; (iii) expanding a keyword set to a more comprehensive set; (iv) searching for reviews that are relevant to a keyword set the best.\nA. Finding keywords of interest\nIn this section, we propose a technique to rank keywords based on their importance. Our assumption is that developers and users often express their concerns via keywords related to that concern. However, the number of concerns extracted from user’s reviews could be many, and address all of them could be time consuming. Therefore, they should be prioritized so developers can choose to address the most important ones first. The problem is defined below:\nDefinition 1: Given a keyword vocabulary V , an importance measure λ, return all keywords u ∈ V in descending order of λ(u).\nFrom the review’s information, we know that the ratings associated with user’s reviews is a measurement for sentiment of that reviews. At keyword level, if many users decide to give low ratings to reviews contain a particular keyword, that keyword could expresses the feature that they are concerning about. Through manual inspection of 400 reviews chosen randomly in our dataset, we found that users often use the\nsame terms on the reviews that talk about the same topic. For example, when users complain about the excessive energy consumption rate of some apps, they often refer to it as “battery hog” or “drain battery” and give low star rating to the review. This phenomenon also appeared when people complain about apps suddenly stop working or responding, and they called it “freeze” because of the symptoms.\nWe run a simple counter to count the number of time some words appear in each rating of more than 2 millions Google Play reviews (Figure 4). As given by the results, the terms “hog” and “freeze” and “drain” appeared mostly in reviews with rating less than 3, which further support our observation.\nTo discover the relationship between ratings and keywords, we propose a simple contrast ranking technique using just ratio and the difference between positive and negative rating counts of keywords (Equation 1). This technique is relatively faster and simpler than the commonly used Skewness [10] or Pearson Correlation [11] which are widely used. Pearson Correlation put more weight on too high or too low ratings, thus it is more susceptible to the influence of outliers. This complication is not needed since rating of 2 and 1 are both negative ratings (or 4 and 5 for the positive side) so we do not need to discriminate them. Skewness scoring technique, on the other hand, calculates the moment coefficients of rating counts ranging from 1 to 5. It can be used to express the negative/positive alignment of keywords but it also has to constantly calculate sample third central moment and cubic of sample standard deviation, which requires a lot of computational effort.\nOur simple technique can overcome those setbacks by only taking the contrast of negative/positive counts in consideration. This contrast score includes: the ratio between negative ratings and positive ratings counts to discover negative/positive alignments; the difference between those counts to reflect the impact of the concern to users. This is because we assume that if a concern affect users more than other concerns, then they would mention its keywords more frequently in their reviews.\nCm = (Nn −Np)× NnNp (1)\n• Nn: Number of reviews below rating 3 • Np: Number of reviews above rating 3 To evaluate if our simple contrast score can classify positive and negative keywords as well as Pearson Correlation and Skewness, we compute the intersection between keywords with negative alignment of the three methods as shown in Table VII. Overall, there is no significant different between the classification capability of our techniques versus the others."
    }, {
      "heading" : "B. Dividing keywords into groups",
      "text" : "In this section, we explain our grouping technique using vector-space distributed representation of words. We define the problem as below:\nDefinition 2: Given a keyword set S, a similarity measure sim, a threshold δ, add to S any keyword v ∈ V − S such that ∀u ∈ S, sim(u, v) >= δ.\nGenerally, a concern could be expressed by not just one keyword, as for the battery consumption problem stated above. This leads to the need of developers to discover sets of keywords that describe each concern. In such set, all keywords need to have a common semantical meaning or relationship. As explained in Section III-B4, simple statistical models do not describe such information. However, the vector-space distributed representations can explicitly encode many linguistic regularities, patterns and context information in many degrees, including semantic meanings [12]. To learn those vectors, we use Word2Vec tool [2] for its scalability to expand to billions of words, opens up an opportunity for future analysis tasks of hundreds of millions reviews from multiple app stores.\nIn the properties of vectors, each keyword usually expresses one atomic feature of app or a part of a concern as in the battery consumption problem. Together, several keywords can make much more sense of a general concern, feature than just a single word. We expect to find that common relationship from their average vector. Therefore, in this work we choose K-mean Clustering Algorithm [3] to cluster the keywords into common concerns. Ideally, the words with similar semantic meaning will be in the same cluster by using similarity between their vector representations. We estimate such similarity using Cosine Similarity [13]. We evaluate this grouping technique in Section VI-C1."
    }, {
      "heading" : "C. Expanding keywords into bigger set",
      "text" : "As an opposite analysis of grouping a known set of keywords into several subsets, in this section we propose a technique to expand a small keyword set into a larger and more comprehensive one. We define this problem below:\nDefinition 3: Given a vocabulary of keywords V , a keyword set S, a similarity measure ψ, a threshold α, add to S any keyword v ∈ V − S such that u = mean(S), ψ(u, v) >= α.\nAs said, concern could be described by many keywords, sometime developers have in mind of what keywords they really want to look into but do not know if their set of keywords is sufficient or not. This leads back to the problem of finding keywords that have similar meaning or close relationship but in this case, developers already know a part of what they are looking for.\nOur solution for this problem is looking for words that are semantically similar to the original set to a degree. From the last section, we know that the distributed representation of words trained by Word2vec hold multi-degree context information of words, including its semantic meaning. This kind of information is needed to find semantical relation between the words provided by developers and the words in our vocabulary.\nHowever, most words relate to each other in one way or another, therefore we need to make sure that the expanding set of words should hold a common ground of understanding. Therefore, every time the expanding algorithm (Figure 5) iterate through the vocabulary, it compute Cosine Similarity on the average vector of the set to the new word’s vector. This should ensure the new word will only be added to the group if\nit share a common relationship with that group and the main topic of the group should shift every time it includes a new word.\nThe threshold for similarity is set by developers, ranging from 0 to 1. In evaluation section, we explain a case study using most popular keywords from a prior study and find the most suitable similarity threshold."
    }, {
      "heading" : "D. Searching reviews based on keyword set",
      "text" : "Finally, after having a set of keywords that can describe a concern, developers need an effective way to search for the reviews that contain those keywords. We propose a review searching technique from a given set of keywords that ranks the reviews based on their relevancy to the set. We formulate this problem below:\nDefinition 4: Given a collection of reviews R, a keyword set S, an relevancy measure ω, return all review r ∈ R in descending order of ω(r, S).\nGenerally, it is not certain that all the keywords contribute equally to the main concern. Some keywords could relate to several concerns at once, like “usage” for example, could both describe user’s opinions about data usage or battery usage or even memory usage. In contrast, when people mention “drain”, they are most likely to refer only to battery problem. Therefore, we assume that keywords that appear everywhere in the corpus do not have crucial contribution to any particular topic and the reviews contain them should not be as important as the reviews contain specific keywords to a topic of interest.\nIn general, this problem of ranking the importance of words can be solved with Term Frequency - Inverse Document Frequency (tf.idf) [13] scoring. The tf.idf score for a word is proportion to the number of times a word appears in the document, but is reduced by the frequency of the word in the corpus. If a word appear in many documents, its score will be less than the words that appear in just a few documents.\nHowever, just the importance of keywords is not enough to rank the importance of an entire review. If a review express a lot about an user’s concern, it could contain more keywords of that concern than other reviews. Therefore, to search for important reviews of a concern, we need to combine both tf.idf and the number of keywords appeared in each review. We choose to compute Cosine Similarity on tf.idf of the concern keywords set and the keywords appear in each review to rank them as it also addresses the number of common items in both sets and is the simplest and most commonly used technique for this problem. We evaluate this technique in Section VI."
    }, {
      "heading" : "V. TREND AND ABNORMALITIES DISCOVERY",
      "text" : "In this section we describe our method to detect abnormalities in trend of a topic of concern, which is defined by a set of keywords, based on sudden changes in its timeseries.\nTime-series of keywords are the series of occurrences of given words overtime. Figure 6(a) shows a time-series of battery concern. If users mention a set of keywords in their reviews more than usual in a period, they may have experienced some difficulties related to the topic of those keywords. From the prior studies [1] [4], we know that sometime a new release could cause the change in occurrence of words or reviews of an app. For example, if a release introduces a new buggy main feature to the app, users are expected to complain about that feature more than when it was still fine. Therefore, analyze the time-series could give developers a better understanding of current trends in user opinions, thus they can address newly appeared problems in a timely manner.\nHowever, the occurrence of keywords each day is entirely a matter of word choice from users who wrote reviews for that day. Even if there is no big concern going on, the timeseries itself still fluctuates every day and make unwanted noise in the overall trend. There are several techniques to smooth the time-series and filter out short-term noise, but the most commonly used is Simple Moving Average [14] for its simple and sufficient nature. Simple Moving Average acts like a lowpass filter, calculating the average values of a sliding window to highlight long term trends. Sudden changes in the observed value would not be reflected on moving average data, this cause a large difference between them given the change is big enough. With this information, we can detect any sudden change in user concerns and alert the developers in time.\nDefinition 5: Given a time-series T , a threshold δ, a sudden change measure ξ, return all day d ∈ T such as ξ(d, T ) > δ.\nTo detect sudden change using Simple Moving Average (SMA), we first assume that big changes rarely happen, or else it will become a trend and the smoothed data would reflect that. Moreover, when such change happens, there should be a big difference between time-series’ standard deviation to moving average and the actual offset between the time-series and moving average on the day it occurs. Given a time unit, if the ratio of them passes a given threshold δ, we can consider the change on that time unit as a significant change, and alert developers about the growing up problem. The formula for this ratio is given in equation (2)\nρi = Vi − µi σ\n(2)\n• Vi: observed occurrence on day i on the time-series • µi: Simple Moving Average on day i • ρi: ratio of difference between observed value with\nSMA to Standard deviation on day i • σ: Standard deviation of the time-series to Simple\nMoving Average"
    }, {
      "heading" : "VI. EMPIRICAL EVALUATION AND CASE STUDIES",
      "text" : "In this section, we present and discuss our empirical evaluation and case studies with M.A.R.K on a sample of more than 2 millions reviews collected from Google Play. In our empirical evaluation, we focus on the accuracy of M.A.R.K in its keyword extraction tasks. Our case studies evaluate the correctness and usefulness of M.A.R.K in its keyword-based analysis tasks, i.e. identifying the most negative\nkeywords, clustering of those keywords, expanding of userprovided keywords, querying of relevant reviews, detecting of breakout trends of given keywords."
    }, {
      "heading" : "A. Data collection",
      "text" : "We have crawled 2 millions of reviews from 95 apps (Table VIII) on Googleplay from January 2015 to the beginning of May 2015 using an open-source Google Play Crawler8. The apps that was chosen to be crawled in this study are based on the number of download times and manual inspection on their review increment rates. Because Google limits the total number of new reviews can be downloaded each time, which is 500 reviews, we had to crawl the reviews of the chosen apps continuously overtime to get access to the most recent ones. Overall, we have an average of more than twenty thousands reviews for each apps, however, this number is varying between them because some apps have more active users than others."
    }, {
      "heading" : "B. Keyword Extractor",
      "text" : "1) Non-English reviews classifier: To evaluate our classifier and find suitable bi-gram and uni-gram thresholds, we manually labeled 400 reviews for testing classification accuracy. These reviews are sampled randomly from our dataset and labeled as 245/145 English Reviews/non-English reviews. To begin evaluation, we run a greedy test with both threshold from 0.0 to 1.0, on the increment of 0.01 to find the best accuracy. In the final results, we found that bi-gram threshold and uni-gram threshold are best at 0.39 and 0.64, respectively, where the accuracy reached 86.5%. Table IX shows some of the calculated scores for some of our examples, with the upper half are accepted English reviews and the lower half are nonEnglish ones.\nFinally, we applied these thresholds on the whole dataset and discovered 239,980 non-English reviews, which contribute 11.39% to our dataset.\n2) Customized Stemmer: To evaluate our Customized Stemmer, we pick a list of random 1000 words that were PoS tagged by Stanford PoStagger from our reviews dataset and manually give them their respective base-forms. This test set is then used as input for our Stemmer and the widely used Stanford Lemmatizer tool for comparison purpose. Overall, our algorithm is able to stem correctly 97.9% words while Stanford Lemmatizer can only get 90.8% right. This is partially because of our domain specific dictionary found in Section III-B1 that we are using for our spelling corrector."
    }, {
      "heading" : "C. Keywords Analyzer",
      "text" : "1) Keywords Grouping: To evaluate our grouping technique, we apply it on top 100 negative keywords based on\n8https://github.com/Akdeniz/google-play-crawler\nour contrast score. Table X shows the resulted clusters after grouping. After that, we asked a team of 8 Computer Science researchers to identify the keywords that do not really belong to each cluster. Each researcher will cross out the words they see unfit. The combination of their results is used to calculate acceptance rates for each words. We take the average as our accuracy measure.\nOverall, the average accuracy for this grouping approach is 83.11% with the highest accuracy reach 87.5% for concern about connection. Generally, the words in each cluster mostly describe a common concern, as we expected. It is also interesting that Facebook and Snapchat are able to have their own topics from top 100 highest ranked words. This could be explained by the popularity of these apps compare to other apps, thus give them a bigger part of review numbers in our data. Moreover, upon manual inspection of several recent releases, both apps suffered badly from negative reviews of their updates, which could partially explain why many of their keywords appeared on the top negative words. This result suggests that their developers should be more careful in quality control of their future releases.\n2) Keywords Expanding: In this case study, we choose some words from the topics found by Wiscom [4] to expand. Like grouping keyword, we asked a team of 8 researchers to verify our results in the same manner. To simplify our case, we use only one starter word instead of a set of words. In their opinion, if a word is not relate to the starter word, then they cross it out. To find the most suitable similarity threshold, we evaluate our technique with several thresholds from 0.7 to 0.95 with 0.05 increment.\nThe evaluation results in Table XI show that our expanding technique is able to capture the semantic similarities of the group and choose the suitable words to add into it with high accuracy (average of 89.72%) at similarity threshold of 0.75. We choose 0.75 because it is the similarity threshold that gives us highest accuracy without losing too many words. Starting from 0.8, some topics could not find any word whose similarity\ncould pass the threshold.\nMoreover, the expanded words mostly improve the understanding dimensions to the starter words, make its topic more comprehensive and user’s like. Therefore, we conclude that our technique would help developers to map their initial ideas into how user really express their concerns.\nD. Visualization and further Analysis\n1) Abnormalities Discovery: To evaluate our abnormalities detection technique on time-series, we use the Keywords Expanding tool to obtain a set of keywords that relate to the word “battery” in Facebook Messenger app the manually choose the keywords that represent the battery consumption concern the most. In the end, our set of keywords contains: “heat”, “hog”, “usage”, “consumption”, “consume”, “battery”, “drain”, “hogger”, “overheat”, “eater”, “eat”, “drainer”, “power”. Figure 6(a) shows the time-series and moving average for this set over the time period of January 2015 to early May 2015.\nThe reason why we choose this concern is because of an observed complaining trend about battery consumption in early February. Our further investigation lead to a confirmation from a developer at Facebook that it was a syncing issue on Android that affect people’s battery life and an update was soon brought on 13th February to address this problem9. Our time-series also shows a big spike during those days and the trend went normal again after the release date.\nIn the scope of this study, we suggest developers set threshold for ratio score at 2, however, it is up to them to set a more suitable threshold. Figure 6(b) shows that the ratio score during 12th and 13th of February raised far excess our threshold of 2 while 119 other days could not pass it. If this threshold was chosen, developers of Facebook would be able to discover this trend and address the problem without having\n9http://androidforums.com/threads/facebook-messenger-batterydrain.902687/\nto read through all reviews or search for the unknowns on online forums.\n2) Reviews Searching: We use the same set of keywords about Battery Consumption from Section VI-D1 to evaluate our searching technique on Facebook Messenger. To calculate the accuracy of our technique, a team of 4 researchers read through top 50 returned reviews to labeled the reviews as relevant to the topic or not. We then calculate the accuracy for top 10, 15, 20, 25, 30, 45, 50 results as shown in Figure 7. Overall, the accuracy is high as most reviews in top 10 belong to Battery Consumption concern while the accuracy for top 50 is still near to 90%. The results indicate that our keywords-based searching technique can provide reliable reviews to deepen developers’ understanding of user concerns."
    }, {
      "heading" : "VII. RELATED WORK",
      "text" : "There are a number of empirical and exploratory studies on the importance of app’s reviews in app development process.\nIn [15], Vasa et al. made an exploratory study about how users input their reviews on app stores and what could affect the way they write reviews. Later, Hoon et al. [16] analyzed nearly 8 millions reviews on Apple AppStore to discover several statistical characteristics to suggest developers constantly watching for the changes in user’s expectations to adapt their apps. Again on Apple App Store, an emprical study about user’s feedback was made by Pagano et al. [17]. Similarly, Khalid et al. suggest that there are at least 12 types of complaints about iOS apps [18]. They explored various aspects that influent user reviews such as time of release, topics and several properties including quality and constructiveness to understand their impacts on apps.\nOther than reviews, price and rating of apps can also affect how people provide their feedbacks, as suggested in [19] by Iacob et al. Meanwhile, Bavota et al. [20] studied the relationship between API changes and their faulty level with app ratings. Recently, Martin et al. [21] reported the sampling bias researchers might encounter when mining information from app reviews.\nThus so far, there are very few works in mining useful information from user’s reviews on app markets. One of the earliest work is from Chandy et al. [22] who proposed a classification model for spamming reviews on Apple AppStore using a simple latent model. On the other hand, Carreno et al. [23] extract changes or additional requirements for new versions automatically using information retrieval techniques. Our work is different from theirs in the main goals: We focus on extracting user opinions based on keywords to map the way developers express their concerns to user’s way.\nMore to the mining techniques, in [24], Guzman et al.\nextracts features from app reviews in form of collocations and summarizes them with Latent Dirichlet Allocation (LDA) [8] and their sentiment. In their work, they score the sentiment of reviews by using SentiStrength [25], which is a lexical sentiment approach. The keywords extraction approach in our work is very close to the meaning of features extraction, but with extra flexibility for developers and users to map their expressions. Instead of a collocation of two words, our keywords can come in a set and carries an entirely different dimension of information. We rank our keywords using ratings from users, which is a more domain specific approach for review analysis than Guzman’s general lexical sentiment approach.\nAnother work, which is more closely related to our work, is Wiscom [4]. Their work on sentiment analysis of words using Linear Regression Model is comparable to our ranking scheme but with a different intention. We try to address the impact of keyword’s concerns to users while they want to address the impact on sentiment of a keywords to discover inconsistent review. On ”meso level” they use a LDA model to analyze topics of user reviews based on their distribution. Similarly, we group the keywords using K-mean clustering on vector-space representation of words, but our approach focus on exploiting different layers of semantic meaning for words inside the corpus, which give us a different perspective of user opinions. To the best of our knowledge, their work is also the first work to mention the use of timeseries on reviews for analysis, however, their approach was to use root cause analysis based on the observed busts in negative or positive comments, which does not address the problems that may lie inside normal stream of comments. Our approach using keywords can automatically discover changes in trends of given concerns regardless of the total reviews’ number.\nInterestingly, Iacob et al [26] designed a prototype (MARA) to retrieve app feature requests from comments using a set of linguistic rules. The rules were derived manually from actual text of reviews and the retrieved features are further analyzed using LDA to find common topics among them. This approach shares one similar aim with us: to extract features. However, their feature level stay at the phrase level while our features can be described by keywords, which may be more intuitive for both developers and users.\nOne of the most recent work in extracting information from reviews is AR-Miner [1]. Chen et al. propose a computational framework to extract and rank informative reviews at sentence level. They adopt the semi-supervised algorithm Expectation Maximization for Naive Bayes (EMNB) [27] to classify between informative and non-informative reviews. To rank the reviews, they use a ranking schema based on several meta-data of reviews and try to suggest the most informative ones. Our work is different from their work in both purpose and approach: Our purpose is to extract user opinion, while they want to find and rank most informative reviews, so the benefit for developers is different. Moreover, we focus on keywords level because their distributed representations can discover more detailed semantic meanings of user’s reviews.\nFrom an empirical study suggested that there are errorprone permissions reported in user reviews, Gomez et al. [28] developed a static error-proneness checker for app based on permissions. This work uses LDA to identify topics in reviews and reported permissions. As other works, it is different\nfrom our main purpose and approach of mining keyword’s semantical meaning.\nFinally, our framework is the first to provide a reliable way to search for most relevant reviews based on keywords approach, which is yet to be mentioned by any prior work."
    }, {
      "heading" : "VIII. CONCLUSION",
      "text" : "In this work, we proposed M.A.R.K as a semi-automated framework to collect and mine user opinions from App Markets using a keyword-based approach. We developed and applied several automated, customized techniques for our main tasks, including: extracting keywords from raw-reviews, ranking them, grouping them based on their semantic meaning, searching for most relevant reviews to a set of keywords, visualizing their occurrence over time with reports of unusual patterns.\nFrom our observations on the difficulties of processing raw reviews, we proposed an original technique to classify nonEnglish ones and developed a customized Stemmer to normalize their keywords. Our evaluations show that the classifier is able to correctly label 86.5% reviews in our test set, and using it, we found 11.39% are non-English in our 2 millions Google Play reviews. Our customized Stemmer is also proved to be of higher accuracy for stemming app reviews data than the general purpose lematization tool from Stanford for our test set.\nExploiting the multi-degree semantical meaning property of distributed vector-space representation of words, our Grouping and Expanding techniques can discover highly related keywords that express common concerns. Our case studies show that these techniques is able to reach 83.11% accuracy for grouping and 89.72% accuracy for expanding tasks.\nUsing a set of keywords discovered from the above steps, M.A.R.K can help developers to search for most relevance reviews to that set. In our case study of searching for battery consumption concern in Facebook Messenger, we found that 90% of top 50 returned reviews satisfy the query.\nIn the tasks of discovering abnormalities of keywords occurrence in a time period, our analysis technique utilizes Simple Moving Average to alert developers when abnormal patterns appear. Our case study suggests that this technique is able to detect correctly a real problem in Facebook Message that annoyed many of its users. We conclude that it has the potential to reduce developer’s effort in real life situation.\nFinally, from the evaluations and case studies, we suggest that using M.A.R.K Framework would help developers to map their concerns/interests with user’s via the common expression of keywords, thus save them time and effort for discovering and understanding user’s opinions."
    }, {
      "heading" : "ACKNOWLEDGMENT",
      "text" : "The authors would like to thank Dr. Young-Woo Kwon, Anand Ashok Bora, Sima Mehri Kotchaki, and Jiin Kim at Utah State University for helping us in our evaluation process.\nWe also give our special thank to Mr. Ty Nguyen, Dr. Tam Vu and Dr. Thang Dinh for their contribution to our work."
    }, {
      "heading" : "C. Cosine Similarity",
      "text" : "similarity = cos(θ) = A ·B ‖A‖ ‖B‖\n(7)\n• A,B: two vectors of attributes.\n• cos(θ): Cosine Similarity"
    }, {
      "heading" : "D. Term Frequency - Document Frequency",
      "text" : "tf.idf = N1+log(nt) (8)\n• N : term frequency\n• nt: document frequency\nE. Simple Moving Average\nµi =\n∑i−1 j=i−w Vj\nw (9)\n• µi: Simple Moving Average on day i • w: Sliding window’s size in days • Vj : observed value on the timeseries on day j\nσ = √√√√ 1 N N∑ i=1 (Vi − µi)2 (10)\n• σ: Standard deviation of the timeseries to Simple Moving Average\n• N : length of the time series in days"
    } ],
    "references" : [ {
      "title" : "Arminer: Mining informative reviews for developers from mobile app marketplace",
      "author" : [ "N. Chen", "J. Lin", "S.C.H. Hoi", "X. Xiao", "B. Zhang" ],
      "venue" : "Proceedings of the 36th International Conference on Software Engineering, ser. ICSE 2014. New York, NY, USA: ACM, 2014, pp. 767–778. [Online]. Available: http://doi.acm.org/10.1145/ 2568225.2568263",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "T. Mikolov", "K. Chen", "G. Corrado", "J. Dean" ],
      "venue" : "CoRR, vol. abs/1301.3781, 2013. [Online]. Available: http://arxiv.org/abs/1301.3781",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Some methods for classification and analysis of multivariate observations",
      "author" : [ "J. MacQueen" ],
      "venue" : "Proceedings of the fifth Berkeley symposium on mathematical statistics and probability, vol. 1, no. 14. Oakland, CA, USA., 1967, pp. 281–297.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1967
    }, {
      "title" : "Why people hate your app: Making sense of user feedback in a mobile app store",
      "author" : [ "B. Fu", "J. Lin", "L. Li", "C. Faloutsos", "J. Hong", "N. Sadeh" ],
      "venue" : "Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ser. KDD ’13. New York, NY, USA: ACM, 2013, pp. 1276–1284. [Online]. Available: http://doi.acm.org/10.1145/2487575.2488202",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Snowball stemmer",
      "author" : [ "M. Porter", "R. Boulton" ],
      "venue" : "2001.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "The Stanford CoreNLP natural language processing toolkit",
      "author" : [ "C.D. Manning", "M. Surdeanu", "J. Bauer", "J. Finkel", "S.J. Bethard", "D. McClosky" ],
      "venue" : "Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, 2014, pp. 55–60. [Online]. Available: http://www.aclweb.org/anthology/P/P14/P14-5010",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Featurerich part-of-speech tagging with a cyclic dependency network",
      "author" : [ "K. Toutanova", "D. Klein", "C.D. Manning", "Y. Singer" ],
      "venue" : "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, ser. NAACL ’03. Stroudsburg, PA, USA: Association for Computational Linguistics, 2003, pp. 173–180. [Online]. Available: http://dx.doi.org/10.3115/1073445.1073478",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Latent dirichlet allocation",
      "author" : [ "D.M. Blei", "A.Y. Ng", "M.I. Jordan" ],
      "venue" : "J. Mach. Learn. Res., vol. 3, pp. 993–1022, Mar. 2003. [Online]. Available: http://dl.acm.org/citation.cfm?id=944919.944937",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Statistical language models based on neural networks",
      "author" : [ "T. Mikolov" ],
      "venue" : "Presentation at Google, Mountain View, 2nd April, 2012.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Oehlert, A first course in design and analysis of experiments",
      "author" : [ "W. G" ],
      "venue" : "WH Freeman New York,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2000
    }, {
      "title" : "Probability and statistics for engineers and scientists",
      "author" : [ "R.E. Walpole", "R.H. Myers", "S.L. Myers", "K. Ye" ],
      "venue" : "Macmillan New York,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1993
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean" ],
      "venue" : "CoRR, vol. abs/1310.4546, 2013. [Online]. Available: http://arxiv.org/abs/1310. 4546",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Introduction to information retrieval",
      "author" : [ "C.D. Manning", "P. Raghavan", "H. Schütze" ],
      "venue" : "Cambridge university press Cambridge,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2008
    }, {
      "title" : "The scientist and engineer’s guide to digital signal processing",
      "author" : [ "S.W. Smith" ],
      "venue" : "1997.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "A preliminary analysis of mobile app user reviews",
      "author" : [ "R. Vasa", "L. Hoon", "K. Mouzakis", "A. Noguchi" ],
      "venue" : "Proceedings of the 24th Australian Computer-Human Interaction Conference, ser. OzCHI ’12. New York, NY, USA: ACM, 2012, pp. 241–244. [Online]. Available: http://doi.acm.org/10.1145/2414536.2414577",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "An analysis of the mobile app review landscape: Trends and implications",
      "author" : [ "L. Hoon", "R. Vasa", "J.-G. Schneider", "J. Grundy" ],
      "venue" : "Tech. rep., Faculty of Information and Communication Technologies, Swinburne University of Technology, Melbourne, Australia, Tech. Rep., 2013.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "User feedback in the appstore: An empirical study",
      "author" : [ "D. Pagano", "W. Maalej" ],
      "venue" : "Requirements Engineering Conference (RE), 2013 21st IEEE International, July 2013, pp. 125–134.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "What do mobile app users complain about? a study on free ios apps",
      "author" : [ "H. Khalid", "E. Shihab", "M. Nagappan", "A. Hassan" ],
      "venue" : "2014.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "What are you complaining about?: a study of online reviews of mobile applications",
      "author" : [ "C. Iacob", "V. Veerappa", "R. Harrison" ],
      "venue" : "Proceedings of the 27th International BCS Human Computer Interaction Conference. British Computer Society, 2013, p. 29.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "The impact of api change- and faultproneness on the user ratings of android apps",
      "author" : [ "G. Bavota", "M. Linares-Vasquez", "C. Bernal-Cardenas", "M. Di Penta", "R. Oliveto", "D. Poshyvanyk" ],
      "venue" : "Software Engineering, IEEE Transactions on, vol. 41, no. 4, pp. 384–407, April 2015.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Identifying spam in the ios app store",
      "author" : [ "R. Chandy", "H. Gu" ],
      "venue" : "Proceedings of the 2Nd Joint WICOW/AIRWeb Workshop on Web Quality, ser. WebQuality ’12. New York, NY, USA: ACM, 2012, pp. 56–59. [Online]. Available: http://doi.acm.org/10.1145/2184305. 2184317",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Analysis of user comments: An approach for software requirements evolution",
      "author" : [ "L. Galvis Carreno", "K. Winbladh" ],
      "venue" : "Software Engineering (ICSE), 2013 35th International Conference on, May 2013, pp. 582– 591.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "How do users like this feature? a fine grained sentiment analysis of app reviews",
      "author" : [ "E. Guzman", "W. Maalej" ],
      "venue" : "Requirements Engineering Conference (RE), 2014 IEEE 22nd International. IEEE, 2014, pp. 153–162.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Sentiment strength detection in short informal text",
      "author" : [ "M. Thelwall", "K. Buckley", "G. Paltoglou", "D. Cai", "A. Kappas" ],
      "venue" : "Journal of the American Society for Information Science and Technology, vol. 61, no. 12, pp. 2544–2558, 2010.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Retrieving and analyzing mobile apps feature requests from online reviews",
      "author" : [ "C. Iacob", "R. Harrison" ],
      "venue" : "Proceedings of the 10th Working Conference on Mining Software Repositories, ser. MSR ’13. Piscataway, NJ, USA: IEEE Press, 2013, pp. 41–44. [Online]. Available: http://dl.acm.org/citation.cfm?id=2487085.2487094",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Text classification from labeled and unlabeled documents using em",
      "author" : [ "K. Nigam", "A.K. McCallum", "S. Thrun", "T. Mitchell" ],
      "venue" : "Machine learning, vol. 39, no. 2-3, pp. 103–134, 2000.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "A recommender system of buggy app checkers for app store moderators",
      "author" : [ "M. Gomez", "R. Rouvoy", "M. Monperrus", "L. Seinturier" ],
      "venue" : "Ph.D. dissertation, Inria Lille, 2014.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "do not contain useful opinions [1].",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 1,
      "context" : "This clustering task is based on Word2Vec, a distributed, vector-based representation of words [2].",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 2,
      "context" : "K divides a group of keywords into smaller sub-groups of related ones by applying K-mean [3], a similarity-based clustering algorithm on the vectors representing those keywords.",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 0,
      "context" : "Prior works [1], [4] suggest that there are sudden changes that often happen when a new version of an app is released, which contain some defects or issues that make many users unsatisfied.",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 3,
      "context" : "Prior works [1], [4] suggest that there are sudden changes that often happen when a new version of an app is released, which contain some defects or issues that make many users unsatisfied.",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 3,
      "context" : "[4] will not suffice.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "In this kind of problem, Snowball Stemmer [5] is often used.",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 5,
      "context" : "Lemmatization tools such as Stanford Lemmatizer [6] may works well but it is too slow since it will try to lemmatize all words, not just Nouns and Verbs.",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 6,
      "context" : "Our Stemmer uses the WSJ-trained Stanford Part of Speech (PoS) tagger [7] to find PoS for each word in each sentence.",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 7,
      "context" : "Simple statistical models such as topic modeling [8] can capture topics based on the frequency of a word in different documents, however, it is not intended to capture semantic regularities of words.",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 8,
      "context" : "Recent studies on vector-space distributed representation of words had suggested that words can have multiple degrees of similarity [9].",
      "startOffset" : 132,
      "endOffset" : 135
    }, {
      "referenceID" : 9,
      "context" : "This technique is relatively faster and simpler than the commonly used Skewness [10] or Pearson Correlation [11] which are widely used.",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 10,
      "context" : "This technique is relatively faster and simpler than the commonly used Skewness [10] or Pearson Correlation [11] which are widely used.",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 11,
      "context" : "However, the vector-space distributed representations can explicitly encode many linguistic regularities, patterns and context information in many degrees, including semantic meanings [12].",
      "startOffset" : 184,
      "endOffset" : 188
    }, {
      "referenceID" : 1,
      "context" : "To learn those vectors, we use Word2Vec tool [2] for its scalability to expand to billions of words, opens up an opportunity for future analysis tasks of hundreds of millions reviews from multiple app stores.",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 2,
      "context" : "Therefore, in this work we choose K-mean Clustering Algorithm [3] to cluster the keywords into common concerns.",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 12,
      "context" : "We estimate such similarity using Cosine Similarity [13].",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 12,
      "context" : "idf) [13] scoring.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 0,
      "context" : "From the prior studies [1] [4], we know that sometime a new release could cause the change in occurrence of words or reviews of an app.",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 3,
      "context" : "From the prior studies [1] [4], we know that sometime a new release could cause the change in occurrence of words or reviews of an app.",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 13,
      "context" : "There are several techniques to smooth the time-series and filter out short-term noise, but the most commonly used is Simple Moving Average [14] for its simple and sufficient nature.",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 3,
      "context" : "2) Keywords Expanding: In this case study, we choose some words from the topics found by Wiscom [4] to expand.",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 3,
      "context" : "KEYWORD EXPANDING RESULTS FOR SEVERAL POPULAR TOPICS FROM WISCOM [4].",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 14,
      "context" : "In [15], Vasa et al.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 15,
      "context" : "[16] analyzed nearly 8 millions reviews on Apple AppStore to discover several statistical characteristics to suggest developers constantly watching for the changes in user’s expectations to adapt their apps.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "[17].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "suggest that there are at least 12 types of complaints about iOS apps [18].",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 18,
      "context" : "Other than reviews, price and rating of apps can also affect how people provide their feedbacks, as suggested in [19] by Iacob et al.",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 19,
      "context" : "[20] studied the relationship between API changes and their faulty level with app ratings.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "[22] who proposed a classification model for spamming reviews on Apple AppStore using a simple latent model.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "[23] extract changes or additional requirements for new versions automatically using information retrieval techniques.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 22,
      "context" : "More to the mining techniques, in [24], Guzman et al.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 7,
      "context" : "extracts features from app reviews in form of collocations and summarizes them with Latent Dirichlet Allocation (LDA) [8] and their sentiment.",
      "startOffset" : 118,
      "endOffset" : 121
    }, {
      "referenceID" : 23,
      "context" : "In their work, they score the sentiment of reviews by using SentiStrength [25], which is a lexical sentiment approach.",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 3,
      "context" : "Another work, which is more closely related to our work, is Wiscom [4].",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 24,
      "context" : "Interestingly, Iacob et al [26] designed a prototype (MARA) to retrieve app feature requests from comments using a set of linguistic rules.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 0,
      "context" : "One of the most recent work in extracting information from reviews is AR-Miner [1].",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 25,
      "context" : "They adopt the semi-supervised algorithm Expectation Maximization for Naive Bayes (EMNB) [27] to classify between informative and non-informative reviews.",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 26,
      "context" : "[28] developed a static error-proneness checker for app based on permissions.",
      "startOffset" : 0,
      "endOffset" : 4
    } ],
    "year" : 2017,
    "abstractText" : "To improve user satisfaction, mobile app developers are interested in relevant user opinions such as complaints or suggestions. An important source for such opinions is user reviews on online app markets. However, manual review analysis for useful opinions is often challenging due to the large amount and the noisy-nature of user reviews. To address this problem, we propose M.A.R.K, a keyword-based framework for semiautomated review analysis. The key task of M.A.R.K is to analyze reviews for keywords of potential interest which developers can use to search for useful opinions. We have developed several techniques for that task including: 1) keyword extracting with customized regularization algorithms; 2) keyword grouping with distributed representation; and 3) keyword ranking with ratings and frequencies analysis. Our empirical evaluation and case studies show that M.A.R.K can identify keywords of high interest and provide developers with useful user opinions. Keywords—App Review, Opinion Mining, Keyword",
    "creator" : "LaTeX with hyperref package"
  }
}