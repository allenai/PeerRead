{
  "name" : "1409.5079.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Will it rain tomorrow?",
    "authors" : [ "Bilal Ahmed" ],
    "emails" : [ "bahmad@student.unimelb.edu.au" ],
    "sections" : [ {
      "heading" : null,
      "text" : "With the availability of high precision digital sensors and cheap storage medium, it is not uncommon to find large amounts of data collected on almost all measurable attributes, both in nature and man-made habitats. Weather in particular has been an area of keen interest for researchers to develop more accurate and reliable prediction models. This paper presents a set of experiments which involve the use of prevalent machine learning techniques to build models to predict the day of the week given the weather data for that particular day i.e. temperature, wind, rain etc., and test their reliability across four cities in Australia {Brisbane, Adelaide, Perth, Hobart}. The results provide a comparison of accuracy of these machine learning techniques and their reliability to predict the day of the week by analysing the weather data. We then apply the models to predict weather conditions based on the available data."
    }, {
      "heading" : "1. Introduction",
      "text" : "Weather is perhaps the most commonly encountered natural phenomenon which affects a large proportion of the human population on a daily basis. Given the large number of variables which may contribute to the overall weather of a given location, it is quite challenging to accurately predict what the weather would be like on a given day and the day of the week based on the given weather conditions.\nFor our experiments we train our classifiers using historical data to:\n1. Predict the day of the week {Mon, Tue, Wed, Thu, Fri, Sat, Sun} by analysing the\ngiven weather conditions for that day which includes temperature, rain, wind and time of the year among other attributes.\n2. Predict weather conditions for a given day i.e. the likelihood of rain, wind and\ntemperature range.\n3. Test the robustness of these models by applying them across various cities in\nAustralia and compare their results."
    }, {
      "heading" : "2. Classifiers",
      "text" : "The following classification algorithms have been used to build prediction models to perform the experiments:\n Naïve Bayes (NB) classifier is a simple probabilistic classifier based on applying\nBayes' theorem with strong (naive) independence assumptions .i.e. the classifier assumes that the presence (or absence) of a particular feature of a class is unrelated to the presence (or absence) of any other feature, given the class variable. It is simple to build and fast to make decisions. It efficiently accommodates new data by changing the associated probabilities.\n Random Forests (RF) classifier is a variant of the decision tree classification\nmodel. It operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes output by individual trees. This method is similar to bagging in many respects but the construction of each tree is different to the standard decision tree method. Random Forests are shown to be one of the best classification methods experimentally.\n J48 classifier is a variant of the decision tree classification model and is based on\nC4.5 algorithm. The C4.5 algorithm generates a classification-decision tree for the given data-set by recursive partitioning of data. The decision is grown using depth-first strategy. J48 employs two pruning methods to reduce the size of the generated decision trees. The first is known as sub-tree replacement and the second is\ntermed sub-tree raising.\n IB1classifier is an instance based learner, based on simple Euclidean distance. IB1\nuses a simple distance measure to find the training instance closest to the given test instance, and predicts the same class as the training instance. If multiple instances are the same (smallest) distance to the test instance, the first one found is used."
    }, {
      "heading" : "3. Methodology",
      "text" : "The classifiers described in Section 2 are trained on a range of datasets to predict the day of the week based on the weather conditions. The algorithms are compared based on the accuracy of their results.\nWe further investigate the correlation between the discretisation techniques and the accuracy of the results.\n3.1. Pre-processing\nFollowing steps have been applied to pre-process\nthe data-sets.\n3.1.1. Missing Values\nThe missing values for attributes in the dataset are replaced with the modes and means based on existing data. The ReplaceMissiongValues1 filter in Weka is used to replace values for missing attributes in the dataset. Adding the missing values provides a more complete dataset for the classifiers to be trained on.\n3.1.2. Discretisation\nThe following two techniques were applied to discretise the attributes which were originally in continuous form.\n1. Unsupervised Discretisation is used to discretise attributes into the following\n“groups” or bins:\n 10 bins – High resolution\n 4 bins – Medium resolution\n 2 bins – Low resolution\n 1 bin (similar to supervised discretisation)\n2. Supervised Discretisation: The classifiers are also trained on data discretised using\nsupervised discretisation technique.\nFor instance following results are obtained when\n1 weka.filters.unsupervised.attribute.ReplaceMissingValues\nunsupervised discretisation is applied to the attribute (F4) which represents the aggregate precipitation (in mm), given in the training data set for Brisbane.\n Rainfall data discretised into 10 separate categories or ranges, i.e. from 0 to 0.5 mm,\n0.5 to 5.5 and so on.\n Rainfall data discretised into 4 separate\ncategories or ranges.\n Rainfall data discretised into 2 separate\ncategories or range\nAs can be observed higher bin values provide higher resolution in terms of categorisation. For example by discretising data into 10 bins we get a much higher resolution as compared to when the attribute values are discretised into 2 bins. In the latter case, the data is divided into two very broad categories .i.e. (0 - 0.5mm) and (0.5mm – higher) and hence provides results of coarse resolution for the attribute. For instance, using the 2 bins approach we can only predict how likely it was to rain either more or less than 0.5 mm, since we only have two categories (0 – 0.5mm) and (0.5 mm – higher). On the other hand discretising into 10 bins provides\nhigher resolution results; which does not necessarily mean higher accuracy. We use this knowledge when we try to predict rain, temperature and wind for a given day as part of our experiments. This is further discussed in Section 4.2 and the results in Section 4.2 (Result Set 2) further elaborate on this discussion.\nThe choice of discretisation resolution depends on the task (context) and on the type of data used."
    }, {
      "heading" : "4. Results",
      "text" : "4.1 Result Set 1 – Predicting the day of the week {Mon … Sun}, {Weekday, Sat, Sun} and {Weekday, Weekend} using training and development data\nThe following section outlines the results of the experiments.\n1. Predicting the day of the week {Mon, Tue, Wed, Thu, Fri, Sat, Sun} by analysing the given\nweather conditions.\nResult: Discretising the Year attribute (F1) into 2 Bins coupled with Random Forest classifier yielded the highest accuracy, at 16.01 % for Brisbane data. The second and third best performing algorithms have been marked in the following table (Table 1):\nResult: The following table (Table 2) shows the results of the prediction algorithms as they are applied across the four cities. Random Forest classifier performs best on Adelaide weather data, yielding 19.04 % accuracy when the data is\ndiscretised into 10 bins.\nUnsupervised Discretisation of all Attributes (F1-F20) into 10 Bins for Brisbane, Adelaide, Perth and Hobart\nDiscretisation\nNaïve Bayes Simple\nRandom Forests J48 IB1\nBrisbane 14.02% 13.24% 14.48% 13.05%\nAdelaide 15.42% 17.95% 19.04% 16.93%\nPerth 12.10% 13.53% 14.36% 12.44%\nHobart 13.81% 11.89% 13.39% 8.75%\nis it to rain? In addition to rainfall we also try to predict the temperature and wind velocity for a given day.\nIn the following section we provide the results of our experiments.\n4.2 Result Set 2 – Predicting rain, average temperature and maximum wind for a given day using training and development data\nIn Table 5 (below) we have used the prediction models to predict how likely it was to rain, what the average temperature and maximum wind velocity would be, on a given day in Perth.\nBy discretising the data into 10 bins we can not only say whether or not it will rain on a given day, but we can also predict how much it will rain, if it does. This provides results of higher resolution and hence adds more meaning to the results. The following table shows the different categories or ranges for the Rainfall attribute.\nUnsupervised Discretisation into 10 Bins\nNominal Label Rainfall in mm\na = '(-inf-0.05]' 0 – 0.05 mm\nb = '(0.05-0.4]' 0.05 mm – 0.4 mm\nc = '(0.4-1.05]' 0.4 mm – 1.05mm\nd = '(1.05-2.85]' 1.05 mm – 2.85 mm\ne = '(2.85-5.05]' 2.85 mm – 5.05 mm\nf = '(5.05-8.15]' 5.05 mm – 8.15 mm\ng = '(8.15-10.85]' 8.15 mm – 10.85 mm\nh = '(10.85-17.05]' 10.85 mm – 17.05 mm\ni = '(17.05-30.85]' 17.05 mm – 30.85 mm\nj = '(30.85-inf)' 30.85 mm - higher\nSimilarly, we can not only predict whether it would be warm on a given day, but we can also predict how warm it is going to be or how windy it is going to be. This additional resolution or “degree” adds much more meaning to results as compared to just answering “Yes” /“No” type questions.\nObservations: In Table 5 we can see that the accuracy of predictions goes down as the resolution of results goes up. In other words, we can predict with higher accuracy between a smaller numbers of choices (coarse resolution). But as we increase the number of choices (higher resolution) the accuracy goes down.\nAll of the four classifiers performed exceptionally well on Perth and Adelaide data for predicting the average temperature on a given day. With J48 classifiers yielding 93.36% accuracy on Perth data discretized into 4 bins (Table 5).\nPlease refer to Table 4.2 Result Set 2 provided in the Appendix, to view the complete set of results across the four cities."
    }, {
      "heading" : "5. Conclusions",
      "text" : "The choice of discretisation technique(s) and classifier algorithm(s) used predominantly depends on the context and type of available data. Some algorithms are more suitable for nominal values while others perform best with numerical data.\nIt is hard to make a clear judgment based on the results obtained as part of this experiment, but in most cases Random Forests and J48 yielded in higher accuracy; slightly better results as compared to IB1 and noticeably better Naïve Bayes simple. Although in some instances Naïve Bayes yielded much higher accuracy, while the others were down.\nFrom what we have observed in the test results, the accuracy of predictions goes down as the resolution of results goes up and vice versa. In other words, we can predict with higher accuracy between a smaller numbers of choices (coarse resolution). For instance, predicting between weekends and weekdays i.e. between 2 choices, resulted in much figures as compared to predicting the day of the week i.e. between 7 different choices.\nOne of the guiding principles is to ensure we provide as much meaning to our results as possible and to strike a balance between the resolution and accuracy of the results."
    } ],
    "references" : [ {
      "title" : "WEKA - Machine Learning Algorithms in Java, Chapter 8, Data Mining: Practical Machine Learning",
      "author" : [ "I.H. Witten", "E. Frank" ],
      "venue" : "Tools and Techniques with Java Implementations,",
      "citeRegEx" : "Witten and Frank,? \\Q2000\\E",
      "shortCiteRegEx" : "Witten and Frank",
      "year" : 2000
    } ],
    "referenceMentions" : [ ],
    "year" : 2013,
    "abstractText" : "With the availability of high precision digital sensors and cheap storage medium, it is not uncommon to find large amounts of data collected on almost all measurable attributes, both in nature and man-made habitats. Weather in particular has been an area of keen interest for researchers to develop more accurate and reliable prediction models. This paper presents a set of experiments which involve the use of prevalent machine learning techniques to build models to predict the day of the week given the weather data for that particular day i.e. temperature, wind, rain etc., and test their reliability across four cities in Australia {Brisbane, Adelaide, Perth, Hobart}. The results provide a comparison of accuracy of these machine learning techniques and their reliability to predict the day of the week by analysing the weather data. We then apply the models to predict weather conditions based on the available data.",
    "creator" : "Microsoft® Word 2010"
  }
}