{
  "name" : "1708.06266.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Probabilistic Relation Induction in Vector Space Embeddings",
    "authors" : [ "Zied Bouraoui", "Shoaib Jameel", "Steven Schockaert" ],
    "emails" : [ "BouraouiZ@Cardiff.ac.uk", "JameelS1@Cardiff.ac.uk", "SchockaertS1@Cardiff.ac.uk" ],
    "sections" : [ {
      "heading" : "Introduction",
      "text" : "A wide variety of methods have been proposed for representing words in low-dimensional vector spaces (Deerwester et al. 1990; Turney and Pantel 2010; Mikolov, Yih, and Zweig 2013; Pennington, Socher, and Manning 2014). While the primary motivation for most of these works has been to model similarity, recently there has been an increasing interest in the use of such vector space embeddings for learning other types of lexical relations. In particular, it has been observed that many syntactic and semantic relationships can be modelled as vector translations. For instance, there may be a vector r that models the ‘captial of’ relation, such that e.g. pparis − pfrance ≈ ptokyo − pjapan ≈ ... ≈ r, where pw denotes the representation of word w.\nAlthough this remarkable property of word embeddings is now well-established, so far it has mainly been used as a tool for evaluating the quality of word embedding models (Pennington, Socher, and Manning 2014; Vylomova et al. 2016). In particular, it remains unclear to what extent, or in what way, vector space embeddings can be used for learning relations in tasks such as knowledge base completion (Bordes et al. 2013; West et al. 2014). To address this question, we focus on the following relation induction problem: given a set {(s1, t1), ..., (sn, tn)} of entity pairs that are related in a given way, identify new entity pairs (s, t) that are likely to be related in the same way. Throughout the paper, we will refer to s and t as the source and target word respectively.\nCopyright c© 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nThe context of knowledge base completion affects the relation induction task in a number of ways. First, it means that we need a model that can produce faithful confidence scores. This is important because adding incorrect information to a deductive system can have far-reaching consequences. Having faithful confidence scores means that we can repair any inconsistencies that arise in an informed way, and that we can qualify inference results that rely on automatically learned pieces of knowledge. Second, the number of training pairs n is typically quite small, which means that common approaches, such as training a support vector machine (SVM) on the set of vector differences ti − si (Vylomova et al. 2016), may not be an ideal solution.\nIn this paper, we propose a probabilistic model that relies on two main ideas. First, it assumes that the sets of valid source and target words can be modeled as Gaussians. This prevents the model from identifying pairs (s, t) in which s or t are not of the correct type (e.g. identifying the capital of something that is not a country), which addresses an important limitation of pure translation based models. Second, it assumes that the set of translations t−swhich correspond to valid word pairs can also be modelled as a Gaussian. By considering a probability distribution over possible translations, rather than a single translation, the model is intuitively able to ignore features of word meaning that are irrelevant for the considered relation and to appropriately weight the remaining features. We also consider a variant of our model that is not based on translations, and merely assumes that there is a linear mapping between source and target words, which we formalize using Bayesian linear regression."
    }, {
      "heading" : "Related Work",
      "text" : ""
    }, {
      "heading" : "Predicting Relations",
      "text" : "At least three different types of approaches have been studied for predicting relations that are missing from a given knowledge base. First, there is a large body of work on relation extraction from text. Among others, in recent years a number of approaches have been developed that are specifically targeted at completing knowledge bases. These methods essentially learn how to extract the considered relations by using their known instances as a form of distant supervision (Mintz et al. 2009; Riedel, Yao, and McCallum 2010; Surdeanu et al. 2012).\nar X\niv :1\n70 8.\n06 26\n6v 1\n[ cs\n.A I]\n2 1\nA ug\n2 01\n7\nThe second type of approaches rely on modeling statistical dependencies among the known instances of the considered relations, e.g. if we already know that “person A works for company B” and that “company B is based in country C”, we can plausibly derive that “A lives in C”. To exploit such dependencies, some approaches rely on learning latent representations (Kok and Domingos 2007; Speer, Havasi, and Lieberman 2008; Nickel, Tresp, and Kriegel 2012; Riedel et al. 2013; Bordes et al. 2013; Wang et al. 2014; Yang et al. 2015), while others learn probabilistic rules (Schoenmackers et al. 2010; Lao, Mitchell, and Cohen 2011; Wang et al. 2015).\nThe third type of approaches, which are the focus of this paper and are reviewed in more detail below, rely on vector space representations of words or entities to induce plausible relation instances. These vector space representations summarize the linguistic contexts in which the words/entities occur, and relations are thus essentially induced by comparing linguistic contexts. A standard approach is to model relations as translations in the vector space (Mikolov, Yih, and Zweig 2013), although various other approaches have also been investigated (Weeds et al. 2014).\nThese three types of methods are highly complementary. While relation extraction methods can predict very finegrained relations, they require that at least one sentence in the corpus states the relation explicitly. Statistical methods can predict relations even without access to a text corpus, but they are limited to predicting what can plausibly derived from what is already known. From a knowledge base completion point of view, the main appeal of word embeddings is that they may be able to reveal commonsense relationships which are rarely stated explicitly in text."
    }, {
      "heading" : "Modeling Relations in a Vector Space",
      "text" : "As already mentioned in the introduction, various syntactic and semantic relations can be modeled as vector translations in a word embedding (Mikolov, Yih, and Zweig 2013). Among others, it has been shown that word embeddings can be used to complete analogy questions of the form a:b::c:?, asking for a word that relates to c in the same way that b relates to a (e.g. france:wine::germany:?), by predicting the word w that maximizes cos(pb − pa + pc, pw).\nSeveral types of interpretable features can be modeled as directions in word embeddings. For example, in (Rothe and Schütze 2016), it was shown that word embeddings can be decomposed in orthogonal subspaces that capture particular semantic properties, including a one-dimensional subspace (i.e. a direction) that encodes polarity. Along similar lines, in (Kim and de Marneffe 2013) it was found that the direction defined by a word and its antonym (e.g. “good” and “bad”) can be used to derive adjectival scales (e.g. bad < okay < good < excellent). In (Gupta et al. 2015), it was shown that many types of numerical attributes can be predicted from word embeddings (e.g. GDP, fertility rate and CO2 emissions of a country) using linear regression, again supporting the view that directions can model meaningful relations. Finally, in (Derrac and Schockaert 2015) an unsupervised method was proposed to decompose domain-specific vector spaces into interpretable directions. For instance, in a space\nof movies, directions modeling terms such as “scary”, “romantic” or “hilarious” were found.\nSeveral authors have focused on extracting hyperpnym relations from word embeddings. In (Baroni et al. 2012), To decide whether a word h is a hypernym of w, in (Baroni et al. 2012) it is proposed to use an SVM with a polynomial kernel, using the concatenation of h and w as feature vector. In (Roller, Erk, and Boleda 2014) it was shown that vector differences can lead to good results with a linear SVM, provided that the vectors are normalized, and that the squared differences of each coordinate are added as additional features. Intuitively, this allows the SVM classifier to express that h and w need to be different in particular aspects (using the vector differences) but similar in other aspects (using the squared differences). Some authors have also proposed to identify hypernyms by using word embedding models that represent words as regions or densities (Erk 2009; Vilnis and McCallum 2015; Jameel and Schockaert 2017).\nBeyond hypernyms, most work has focused on completing analogies. The problem of relation induction, where we are given a set of correct instances instead of just one in the analogy task, was studied in (Vylomova et al. 2016), where a linear SVM trained on vector differences was used. While strong results were obtained for several relations in a controlled setting (e.g. predicting which among a given set of relations a word pair belongs to), many false positives were obtained when random word pairs were added as negative examples. To alleviate this issue, a number of heuristics were proposed to generate more informative negative examples. A variant of the relation induction problem was also studied in (Drozd, Gladkova, and Matsuoka 2016), where the focus was on predicting the target word t given a valid source word s (as in analogy completion), given a set of training instances (as in relation induction). Two strong baselines were introduced in this paper, which will be discussed below as part of our experimental methodology."
    }, {
      "heading" : "Modeling Relations",
      "text" : "In this section, we propose two models for relation induction. We assume that we are given a set of pairs {(s1, t1), ..., (sn, tn)} as training data, and we need to determine whether a given pair of words (s, t) are related in the same way."
    }, {
      "heading" : "Translation Model",
      "text" : "The first model is based on the common view that relations can be modeled as vector translations. The source words s1, ..., sn typically belong to some semantic or syntactic category, and as a result their representations typically belong to some particular subspace of the word embedding. This is illustrated in Figure 1 for the ‘has body covering’ relation, where the source words all represent animals and the target words represent body covering types.\nIf a relation can be modeled as a translation, it means that the source subspace and the target subspace have to be aligned. However, this is rarely perfectly the case. In fact, in most cases the source and target space even have a different number of dimensions. In the example from Figure 1,\nwe can see that there is no vector that perfectly models the relation, although all valid word pairs (si, ti) define a translation which is more or less horizontal. This can be naturally modeled by representing the relation as a probability distribution over vector translations. In this example, this distribution would have a large ‘horizontal variance’ but a very small ‘vertical variance’. Note that by considering probability distributions over translations, as special cases we can represent relations that are modeled as directions or in terms of similarity.\nIntuitively, we want to accept (s, t) as a valid relation instance if (i) the translation t− s has a sufficiently high probability and (ii) s and t are of the correct type. Let us write δs and δt be the event that the source word s and target word t are of the correct type, and let θst be the event that s and t are in the considered relation. Note that θst entails δs and δt. We evaluate the probability that (s, t) is a valid instance as follows:\nP (θst|ps, pt) = P (δs|ps) · P (δt|pt) · P (θst|ps, pt, δs, δt)\n∝ f(ps|δs) f(ps) · f(pt|δt) f(pt) · f(pt − ps|θst) f(pt − ps|δs, δt)\n(1)\nTo evaluate the latter expression we have to make a number of assumptions. First, we assume that the overall distribution of the words in the word embedding follows a multivariate Gaussian distribution. Given the typical vocabulary sizes, we can use the sample mean and covariance to estimate the parameters of this Gaussian, and thus evaluate f(ps) and f(pt). We also assume that f(ps|δs) follows a multivariate Gaussian distribution. However, as the number of training instances n is often small, and in particular smaller than the number of dimensions, the sample covariance matrix is not a reliable estimator. To alleviate this problem, we will restrict ourselves to diagonal covariance matrices. We then have:\nf(ps|δs) = m∏ i=1 f(xsi |δs)\nwhere m is the number of dimensions in the word embedding, xsi is the i\nth coordinate of ps, and f(xsi |δs) follows a univariate Gaussian distribution with an unknown mean and variance. Using a Bayesian approach, we estimate f(xsi |δs)\nas: ∫ G(xsi ;µ, σ 2)NIχ2(µ, σ2|µ0, κ0, ν0, σ20)dµdσ\nwhereG represents the Gaussian distribution and NIχ2 is the normal inverse χ2 distribution. This integral has an analytical solution, which is given as follows if we use flat priors on the parameters:\ntn−1 ( xi, (n+ 1) ∑n j=1(x sj i − xi)2\nn(n− 1) ) with xi = 1n ∑n j=1 x sj i and tn−1 the Student t-distribution with n− 1 degrees of freedom. The density f(pt|δt) is evaluated in the same way. If we assume that the translations pt − ps also follow a Gaussian distribution, we can estimate f(pt−ps|θst) in a similar way. In particular, we estimate f(pt − ps|θst) as ∏m i=1 f(x s i − xti|θst), where xsi is again the ith coordinate of ps and similar for xti. Each univariate Gaussian f(x s i − xti|θst) is then again estimated using the t-distribution, from the set of data points {xs1i − x t1 i , ..., x sn i − x tn i }. We similarly estimate\nf(pt − ps|δs, δt) as ∏m i=1 f(x s i − xti|δs, δt). The mean of f(xsi − xti|δs, δt) is the same as the mean of f(xsi − xti|θst), but the variance is estimated from the differences xsli − x tk i corresponding to n randomly sampled source words sl and target words tk.\nNote that if the assumption that the considered relation corresponds to a translation is wrong, we can expect the variance of f(xsi −xti|θst) and f(xsi −xti|δs, δt) to be similar, in which case the last factor in (1) evaluates to approximately 1. In other words, the model implicitly takes into account how much the translation assumption appears to be satisfied."
    }, {
      "heading" : "Regression Model",
      "text" : "The translation model relies on the assumption that the source and target spaces are aligned. For a relation such as ‘capital of’, there is a direct connection between each source word and its corresponding target word, i.e. the representation of a country should be similar to the representation of its capital city. In such cases, we can indeed expect this alignment assumption to hold. There are many types of relations, however, for which the connection between source and target word is more implicit. Consider, for instance, the\nproblem of wine-food pairing. We can expect that in a subspace with wines there are directions that correspond to features such as ‘sweetness’, ‘acidity’ and ‘amount of tannins’. Similarly, in a subspace of food types, there may be directions corresponding to features such as ‘healthy’ or prototypes such as ‘meat’, ‘fish’ and ‘tomato’. The mere fact that such features are represented in the word embedding should be enough to predict reasonable wine-food pairings, even if the wine and food spaces are not aligned. Figure 2 illustrates this situation for the earlier example of animal body coverings. Clearly, the lack of alignment between the animal and covering spaces means that translation vectors are no longer a reliable indicator of whether the relation holds. Instead we have to rely on the weaker assumption that there is a linear mapping from the source to the target space.\nTaking this view, in this section we treat relation induction as a linear regression problem. However, two issues need to be addressed. First, we can only fit a linear regression model if the number of training examples is higher than the number of dimensions, which will often not be the case. We address this issue by using a low-rank approximation of the source space. Second, we need to explicitly represent how certain we are about the predictions of the linear regression model. If the source word s is not a linear combination of the source words s1, ..., sn in the training data, then our model should capture the fact that the available training data is not sufficient to make a reliable prediction. Furthermore, even if s is (approximately) a linear combination of s1, ..., sn, we may only be able to predict particular features of the target space. To capture both sources of uncertainty, we make use of a Bayesian linear regression model.\nIn particular, we now model the probability that (s, t) is a valid instance of the considered relation as follows:\nP (θst|ps, pt) = P (δs|ps) · P (δt|pt) · P (θst|ps, pt, δs, δt)\n∝ f(ps|δs) f(ps) · f(pt|δt) f(pt) · f(pt|ps, θst) f(pt|ps, δs, δt)\n= f(ps|δs) f(ps) · f(pt|δt) f(pt) · f(pt|ps, θst) f(pt|δt)\n= f(ps|δs) f(ps) · f(pt|ps, θst) f(pt)\n(2)\nThe densities f(ps|δs), f(ps) and f(pt) are estimated as before. We estimate f(pt|ps, θst) as ∏m i=1 f(x t i|ps, θst), where xti is again the i th coordinate of pt.\nEach univariate density f(xti|ps, θst) is estimated using a Bayesian linear regression model that predicts the possible representations of the target word from ps. However, this is only feasible if ps has at most n− 2 coordinates. Therefore, we use a low-rank approximation of the source word representations, as follows. Let S be a matrix whose rows are the vectors ps1 , ..., psn and let A = UΣV\nT be the SVD decomposition of A. Let v1, ..., vk be the first k row vectors of V , for some k < n − 1. For a given vector p, we can think of pS = (p · v1, ..., p · vk) as the representation of p in the source subspace. Given that we typically need far fewer dimensions to represent the source space than the total number\nof dimensions in the word embedding, we should be able to predict the target word from pSs , even for relatively small values of k. In any case, the choice of k represents a trade-off: the lower the value of k, the better we can characterize the uncertainty underlying our predictions, but the less information we have for making predictions. In the experiments, we have used k = n−12 . We estimate f(x\nt i|ps, θst) as follows:∫\nG(xti; p ∗ sβ, σ 2)·\nG(β; (XTX)−1XT bi, (XTX)−1σ2)· NIχ2(σ2|ν0, σ20)dβdσ\nwhere bi = (xt1i , ..., x tn i ), X is composed of the first k columns of UΣ (with U and Σ the matrices from the SVD decomposition of A) with an additional 1 appended at the end of each row for the bias term, and p∗s is the vector p S s with an additional 1 appended. Assuming a flat prior on the residual variance σ2, the parameters ν0 and σ20 can be estimated from the training data as:\nν0 = n− k − 1\nσ20 = 1\nn− k − 1 (bi −Xβ̂)T (bi −Xβ̂)\nwith β̂ the least squares solution."
    }, {
      "heading" : "Evaluation",
      "text" : "In this section, we experimentally compare the two proposed models with a number of baseline methods from the literature. The relations we consider are taken from three standard benchmark datasets, each containing a mixture of syntactic and semantic relationships: (i) the Google Analogy Test Set (Google), which contains 14 types of relations with a varying number of instances per relation (Mikolov et al. 2013), (ii) the Bigger Analogy Test Set (BATS), wich contains 40 relations with 50 instances per relation (Gladkova, Drozd, and Matsuoka 2016), and (iii) the DiffVec Test Set (DV), which contains 36 relations with a varying number of instances per relation (Vylomova et al. 2016). We report results for two embeddings that have been learned using Skipgram, one from the Wikipedia dump of 2 November 2015 (SG-Wiki) and one from a 100B words Google News data set1 (SG-GN). We also use two embeddings that have been learned with GloVe, one from the same Wikipedia dump (GloVe-Wiki) and one from the 840B words Common Crawl data set2 (GloVe-CC).\nFor relations with at least 10 instances, we use 10-fold cross validation, whereas for relations with less than 10 instances, we use a leave-one-out evaluation. Note that the test fold only contains positive examples. To generate negative examples, we use four strategies. First, for each pair (s, t) in the test fold, we add (t, s) as a negative example. Second, for each source word s in the test fold, we randomly sample two tail words from the test fold (provided that the test fold\n1https://code.google.com/archive/p/ word2vec/\n2https://nlp.stanford.edu/projects/glove/\ncontains enough pairs), which do not occur together with s, and for each such tail word t, we add (s, t) as a negative example. Third, for each positive example, we randomly select a pair from the other relations. Finally, for each positive example, we generate a random word pair from the words available in the dataset. This ensures that the evaluation involves negative examples that consist of related words, as well as negative examples that consist of unrelated words.\nIf we consider the task as a classification task, i.e. deciding for an unseen pair (s, t) whether it has the considered relation, we need to select a threshold, as the considered methods only produce a confidence score (i.e. (1) for the translation model and (2) for the regression model). To choose this threshold, we randomly select 10% of the 9 training folds as validation data, and select the average score of the pairs just above and below the cut-off that optimizes the F1 score3. In the results below, we separately report precision, recall and F1. We can also evaluate this task as a ranking problem, where we merely evaluate to what extent each method assigns the highest score to the correct pairs. In that case, we use mean average precision (MAP)."
    }, {
      "heading" : "Baselines",
      "text" : "The first baseline we consider is the 3CosAvg method proposed in (Drozd, Gladkova, and Matsuoka 2016), which essentially treats the relation induction problem like an analogy completion problem, where we use the average translation vector across all pairs (si, ti) from the training data. In particular, this method assigns the following score to the test pair (s, t):\nscore3CA(t, s) = cos ( pt, ps + ∑ i pti − psi n ) Despite its simplicity, 3CosAvg was found to be a remarkably strong baseline. Another method proposed in (Drozd, Gladkova, and Matsuoka 2016), called LRCos, is based on the assumption that (s, t) is likely correct if cos(ps, pt) is high and t is of the correct type, where a logistic regression classifier was trained on the target words {t1, ..., tn} to predict the probability that t is a valid ‘target word’. To adapt this method to our setting, we also need to consider the probability that s is a valid ‘source word’ (which is not needed in the analogy completion setting considered in (Drozd, Gladkova, and Matsuoka 2016), since there s is always given as a valid source word). To allow for a more direct comparison with our methods, instead of using a logistic regression classifier, we will use our Bayesian estimation for the probability that s and t are of the correct type. In particular, we use the score scoreLRC(t, s) defined as follows:\nP (ps|δs) P (ps) · P (pt|δt) P (pt) · cos(ps, pt)\nAs our final baseline, we train a linear SVM classifier using the training pairs (s1, t1), ..., (sn, tn) as positive examples.\n3Another possibility would be to choose priors that maximize the likelihood of the training data (and a random sample of negative examples). However, selecting a cut-off based on validation data allows for a more direct comparison with the baselines.\nFollowing (Vylomova et al. 2016), we use negative examples of the form (ti, si), obtained by swapping the position of source and target word, as well as negative examples of the form (si, tj), obtained by swapping ti by the target word of another instance (while ensuring that (si, tj) does not appear in the training data as well). Finally, we also add n random word pairs as negative examples. The C parameter of the SVM is tuned for each relation separately (choosing values from {0.01, 0.1, 1, 10, 100}), by using the same validation data that is used for selecting the thresholds in the other models. To address class imbalance, negative examples were weighted by the ratio of positive to negative examples."
    }, {
      "heading" : "Results",
      "text" : "The results are summarized in Table 1. As can be observed, our translation model consistently outperforms all other methods in both MAP and F1 score. Moreover, the regression model consistently outperforms the baselines in terms of MAP score, and outperforms the baselines in for the Google and DV test sets in terms of F1 score (but not for the BATS test set). Among the baselines, 3CosAvg is clearly outperformed by LRCos and SVM. On average, LRCos is the strongest baseline (except for the GloVe-Wiki embedding, where it is outperformed by SVM). This highlights the importance of explicitly modeling the fact that source and target words are expected to belong to a given type. Models which are only trained on translation vectors, such as in the SVM approach, cannot capture this. On the other hand, LRCos relies on cosine similarity to connect source and target words, which is far from optimal, as is evidenced by the large difference in performance between LRCos and our translation model.\nTo compare the performance of the methods across different types of relations, Table 2 contains the MAP scores for the relations from the DiffVec and BATS test sets, for the SG-GN word embedding. For the BATS dataset, the translation model consistently outperforms the baseline across all relations (including the relations that are not shown in the table). In the case of DiffVec there are a few exceptions, as can be seen in Table 2, but in such cases the differences with the translation model are small. The regression model also outperforms the baselines in most cases, but there are a few exceptions where it performs much worse (e.g. Lvc for DiffVec, and Hypernyms-animals, Meronymssubstance, Synonyms-intensity and Antonyms-binary in the case of BATS).\nWhile the regression model is outperformed by the translation model on average, there are several cases where it performs better. For relations such as Event, Hyper and Mero from DiffVec, where the number of examples is rather large (resp. 3583, 1173, 2825), we can see that the regression model actually substantially outperforms the translation model. The main weakness of the regression model is that it needs more training data: while a vector translation can be estimated from a single training example, learning an arbitrary linear mapping requires the number of training examples to be larger than the number of dimensions. While this can be addressed by using a low-dimensional approximation of the source word, doing so means that information is lost.\nOn the other hand, the main weakness of the translation model is that its underlying assumption is rather strong, and there are indeed some relations in the test sets that simply cannot be faithfully modeled in terms of translations. To illustrate this point, in Figure 3 we first show an example of a relation for which the translation assumption is clearly satisfied, the superlative relationship from the BATS test set. In particular, Figure 3a shows the first two principal components4 of the representations of some word pairs (si, ti) that have this relationship (where related words are connected with a line). Clearly, this is the kind of plot that we would expect for a relation that satisfies the translation assumption. Figure 3b illustrates the same relation in a different way. Here a word pair (s, t) is represented as a point (x, y), where x corresponds to the first principal component of the source\n4Specifically, we obtained these coordinates based on an SVD decomposition of the representations of the relevant source and target words {s1, ..., sn, t1, ..., tn}.\nword s, and y corresponds to the first principal component of t. If the considered relationship satisfies the translation assumption, we would expect these points to lie on a line with a slope of 1, which is here (approximately) the case (as it is for other principal components).\nFigure 4 shows a similar plot to Figure 3 but for the mero relation from the DiffVec test set. As is clearly illustrated by this figure, the translation assumption is not valid for this relation. The fact that the regression model performs quite well for this relation means that it can nonetheless be described using a linear model."
    }, {
      "heading" : "Conclusions",
      "text" : "We have proposed two probabilistic models for identifying word pairs that are in a given relation. The first model is based on the common assumption that lexical relations correspond to vector translations in a word embedding. The other model is based on linear regression, relying on the\nTa bl\ne 2:\nM A\nP sc\nor es\nfo rt\nhe in\ndi vi\ndu al\nre la\ntio ns\nof D\niff V\nec an\nd B\nA T\nS fo\nrS G\n-G N\nD iff\nV ec\n3C A\nL R\nC SV\nM Tr\nan s\nR eg r A ct io n: O bj ec tA ttr ib ut e 0. 10 7 0. 07 8 0. 27 8 0. 13 0 0. 25 1 O bj ec t:S ta te 0. 07 5 0. 59 0 0. 27 0 0. 56 7 0. 49 8 O bj ec t:T yp ic al A ct io n 0. 06 9 0. 51 9 0. 36 2 0. 56 0 0. 48 0 A ct io n/ A ct iv ity :G oa l 0. 12 2 0. 49 0 0. 29 0 0. 51 5 0. 47 5 A ge nt :G oa l 0. 07 3 0. 54 3 0. 43 9 0. 60 2 0. 57 8 C au se :C om pe ns at or yA ct io n 0. 06 6 0. 58 8 0. 41 2 0. 62 2 0. 66 4 C au se :E ff ec t 0. 10 3 0. 46 4 0. 24 1 0. 48 4 0. 44 4 E na bl in gA ge nt :O bj ec t 0. 10 6 0. 50 6 0. 33 8 0. 53 9 0. 49 0 In st ru m en t:G oa l 0. 09 4 0. 37 1 0. 23 2 0. 41 9 0. 43 7 In st ru m en t:I nt en de dA ct io n 0. 07 2 0. 53 3 0. 35 5 0. 62 9 0. 58 5 Pr ev en tio n 0. 09 2 0. 65 5 0. 55 3 0. 70 9 0. 61 5 C ol le ct iv e no un 0. 12 6 0. 57 5 0. 38 6 0. 56 3 0. 68 5 E ve nt 0. 20 2 0. 71 7 0. 40 4 0. 73 7 0. 94 0 H yp er 0. 25 9 0. 55 0 0. 38 5 0. 74 6 0. 91 1 L vc 0. 07 2 0. 70 9 0. 77 2 0. 73 5 0. 22 0 M er o 0. 29 0 0. 54 8 0. 39 5 0. 66 9 0. 82 5 N ou n Si ng pl ur 0. 25 3 0. 58 5 0. 32 6 0. 95 8 0. 85 2 Pr efi x re 0. 20 4 0. 49 7 0. 30 7 0. 72 1 0. 68 9 C on ce al m en t 0. 08 2 0. 55 2 0. 27 4 0. 55 1 0. 49 6 E xp re ss io n 0. 05 5 0. 81 8 0. 50 8 0. 81 0 0. 82 2 K no w le dg e 0. 06 9 0. 69 0 0. 50 7 0. 71 7 0. 69 8 Pl an 0. 08 3 0. 54 8 0. 28 5 0. 56 6 0. 62 2 R ep re se nt at io n 0. 10 0 0. 50 0 0. 39 8 0. 48 5 0. 38 8 Si gn :S ig ni fic an t 0. 09 3 0. 38 4 0. 30 1 0. 39 3 0. 38 0 A tta ch m en t 0. 09 1 0. 54 2 0. 23 4 0. 65 4 0. 52 6 C on tig ui ty 0. 10 5 0. 53 3 0. 29 5 0. 62 6 0. 61 3 It em :L oc at io n 0. 10 1 0. 61 8 0. 31 2 0. 71 6 0. 69 4 L oc :A ct io n/ A ct iv ity 0. 07 6 0. 73 6 0. 51 1 0. 75 7 0. 72 7 L oc :I ns tr um en t/A ss oc ia te dI te m 0. 07 5 0. 47 6 0. 33 4 0. 48 7 0. 50 0 L oc :P ro ce ss /P ro du ct 0. 10 7 0. 40 7 0. 56 9 0. 47 8 0. 65 6 Se qu en ce 0. 11 1 0. 40 5 0. 27 1 0. 42 4 0. 40 9 Ti m e: A ct io n/ A ct iv ity 0. 10 1 0. 55 9 0. 34 7 0. 55 4 0. 60 6 V er b 3r d 0. 16 8 0. 60 9 0. 39 7 0. 97 8 0. 95 8 V er b 3r d Pa st 0. 17 7 0. 62 5 0. 30 2 0. 95 0 0. 92 6 V er b Pa st 0. 18 5 0. 63 8 0. 31 6 0. 98 5 0. 90 1 V nD er iv 0. 34 9 0. 44 5 0. 25 9 0. 82 8 0. 79 2\nB A\nT S\n3C A\nL R\nC SV\nM Tr\nan s\nR eg r R eg ul ar pl ur al s 0. 18 6 0. 58 6 0. 40 2 0. 87 6 0. 79 1 pl ur al s -o rt ho gr ap hi c ch an ge s 0. 19 5 0. 60 1 0. 35 5 0. 77 2 0. 65 2 C om pa ra tiv e de gr ee 0. 10 6 0. 65 4 0. 47 3 0. 96 1 0. 88 3 Su pe rl at iv e de gr ee 0. 09 3 0. 70 5 0. 60 5 0. 93 2 0. 86 8 In fin iti ve :3 Ps .S g 0. 11 6 0. 63 8 0. 53 6 1. 00 0 0. 96 8 In fin iti ve :p ar tic ip le 0. 16 1 0. 60 3 0. 49 5 0. 91 6 0. 79 3 In fin iti ve :p as t 0. 14 6 0. 58 2 0. 45 5 0. 95 7 0. 72 2 Pa rt ic ip le :3 Ps .S g 0. 09 9 0. 59 6 0. 57 7 0. 87 4 0. 76 0 Pa rt ic ip le :p as t 0. 16 1 0. 59 8 0. 45 0 0. 87 2 0. 73 1 3P s. Sg :p as t 0. 13 1 0. 69 6 0. 55 6 0. 98 4 0. 94 9 N ou n+ le ss 0. 07 9 0. 58 2 0. 43 3 0. 62 0 0. 63 4 U n+ ad j 0. 11 0 0. 55 6 0. 35 4 0. 77 4 0. 69 2 A dj +l y 0. 09 6 0. 62 5 0. 49 5 0. 89 5 0. 82 4 O ve r+ ad h. /V ed 0. 09 2 0. 62 7 0. 37 5 0. 74 4 0. 76 5 A dj +n es s 0. 07 7 0. 71 7 0. 56 8 0. 83 8 0. 83 2 R e+ ve rb 0. 13 8 0. 66 2 0. 37 6 0. 82 8 0. 74 6 V er b+ ab le 0. 08 5 0. 62 8 0. 56 9 0. 75 8 0. 76 2 V er b+ er 0. 07 5 0. 64 6 0. 61 3 0. 79 3 0. 70 0 V er b+ at io n 0. 11 5 0. 58 0 0. 44 1 0. 78 4 0. 76 2 V er b+ m en t 0. 09 9 0. 54 3 0. 47 4 0. 78 2 0. 69 4 H yp er ny m s an im al s 0. 22 3 0. 70 5 0. 64 4 0. 85 2 0. 28 4 H yp er ny m s m is c 0. 20 2 0. 64 9 0. 54 5 0. 77 8 0. 23 9 H yp on ym s m is c 0. 31 3 0. 44 9 0. 30 4 0. 68 6 0. 28 1 M er on ym s su bs ta nc e 0. 17 0 0. 51 3 0. 36 3 0. 61 2 0. 26 7 M er on ym s m em be r 0. 13 4 0. 55 8 0. 32 5 0. 68 2 0. 66 0 M er on ym s pa rt -w ho le 0. 25 5 0. 52 5 0. 39 1 0. 65 0 0. 25 9 Sy no ny m s in te ns ity 0. 25 8 0. 50 1 0. 29 1 0. 66 8 0. 22 8 Sy no ny m s ex ac t 0. 27 0 0. 45 5 0. 26 0 0. 51 8 0. 19 1 A nt on ym s gr ad ab le 0. 25 1 0. 48 7 0. 31 6 0. 56 7 0. 27 6 A nt on ym s bi na ry 0. 21 9 0. 44 0 0. 30 6 0. 49 7 0. 23 9 C ap ita ls 0. 10 6 0. 68 2 0. 46 5 0. 75 2 0. 73 5 C ou nt ry :la ng ua ge 0. 08 8 0. 64 3 0. 52 4 0. 66 8 0. 71 7 U K ci ty :c ou nt y 0. 05 3 0. 78 5 0. 58 7 0. 87 7 0. 71 9 N at io na lit ie s 0. 05 9 0. 77 4 0. 60 3 0. 85 2 0. 62 6 O cc up at io n 0. 07 3 0. 66 0 0. 53 3 0. 68 1 0. 78 0 A ni m al s yo un g 0. 11 5 0. 60 1 0. 45 8 0. 68 7 0. 75 4 A ni m al s so un ds 0. 08 7 0. 65 0 0. 45 4 0. 66 1 0. 76 8 A ni m al s sh el te r 0. 11 6 0. 66 5 0. 55 8 0. 67 9 0. 50 1 th in g: co lo r 0. 09 8 0. 77 2 0. 66 3 0. 76 1 0. 79 9 m al e: fe m al e 0. 14 7 0. 61 1 0. 47 1 0. 84 0 0. 72 0\nweaker assumption that there is a linear relationship between the source and target words of the considered relation. Both models implicitly factor in whether their underlying assumption is satisfied, and could thus easily be used in combination with each other, or with additional models. In our experimental evaluation, we have found both models to outperform existing approaches, with the translation model outperforming the regression model on average.\nThere are several interesting avenues for future work. First, a number of variants of the proposed models can be developed. For example, a model based on vector concatenations could intuitively model similar kinds of relationships as the regression model. However, in the case of vector concatenations, we can no longer use a diagonal covariance matrix, as that would mean that no interactions between source and target words are being captured. One solution could be to use a low-rank approximation of the vector concatenations and estimate full covariance matrices in a lower-dimensional space. Another interesting option to explore would be to estimate prior probabilities from coarser grained relations for which more training data is available. For example, we could learn a generic model for causal relations, and use that as a prior for the specific types of causal relationships that are considered in the DiffVec test set. It may even be useful to learn priors capturing e.g. syntactic relations, which would intuitively amount to finding a subspace of the embedding that relates to syntactic features."
    } ],
    "references" : [ {
      "title" : "Entailment above the word level in distributional semantics",
      "author" : [ "Baroni" ],
      "venue" : "In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,",
      "citeRegEx" : "Baroni,? \\Q2012\\E",
      "shortCiteRegEx" : "Baroni",
      "year" : 2012
    }, {
      "title" : "Translating embeddings for modeling multi-relational data",
      "author" : [ "Bordes" ],
      "venue" : null,
      "citeRegEx" : "Bordes,? \\Q2013\\E",
      "shortCiteRegEx" : "Bordes",
      "year" : 2013
    }, {
      "title" : "Indexing by latent semantic analysis",
      "author" : [ "Deerwester" ],
      "venue" : "Journal of the American Society for Information Science",
      "citeRegEx" : "Deerwester,? \\Q1990\\E",
      "shortCiteRegEx" : "Deerwester",
      "year" : 1990
    }, {
      "title" : "Inducing semantic relations from conceptual spaces: a data-driven approach to plausible reasoning",
      "author" : [ "Derrac", "J. Schockaert 2015] Derrac", "S. Schockaert" ],
      "venue" : "Artificial Intelligence",
      "citeRegEx" : "Derrac et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Derrac et al\\.",
      "year" : 2015
    }, {
      "title" : "Word embeddings, analogies, and machine learning: Beyond king - man + woman = queen",
      "author" : [ "Gladkova Drozd", "A. Matsuoka 2016] Drozd", "A. Gladkova", "S. Matsuoka" ],
      "venue" : "In Proceedings of the 26th International Conference on Computational Linguistics,",
      "citeRegEx" : "Drozd et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Drozd et al\\.",
      "year" : 2016
    }, {
      "title" : "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn’t",
      "author" : [ "Drozd Gladkova", "A. Matsuoka 2016] Gladkova", "A. Drozd", "S. Matsuoka" ],
      "venue" : "In Proceedings of the Student Research Workshop at NAACL",
      "citeRegEx" : "Gladkova et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Gladkova et al\\.",
      "year" : 2016
    }, {
      "title" : "Distributional vectors encode referential attributes",
      "author" : [ "Gupta" ],
      "venue" : "In Proc. EMNLP,",
      "citeRegEx" : "Gupta,? \\Q2015\\E",
      "shortCiteRegEx" : "Gupta",
      "year" : 2015
    }, {
      "title" : "Modeling context words as regions: An ordinal regression approach to word embedding",
      "author" : [ "Jameel", "S. Schockaert 2017] Jameel", "S. Schockaert" ],
      "venue" : "In Proceedings of the 21st Conference on Computational Natural Language Learning,",
      "citeRegEx" : "Jameel et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Jameel et al\\.",
      "year" : 2017
    }, {
      "title" : "Deriving adjectival scales from continuous space word representations",
      "author" : [ "Kim", "de Marneffe 2013] Kim", "J.-K", "de Marneffe", "M.-C" ],
      "venue" : "In Proc. EMNLP,",
      "citeRegEx" : "Kim et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2013
    }, {
      "title" : "Statistical predicate invention",
      "author" : [ "Kok", "S. Domingos 2007] Kok", "P. Domingos" ],
      "venue" : "In Proc. ICML,",
      "citeRegEx" : "Kok et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Kok et al\\.",
      "year" : 2007
    }, {
      "title" : "Random walk inference and learning in a large scale knowledge base",
      "author" : [ "Mitchell Lao", "N. Cohen 2011] Lao", "T. Mitchell", "W.W. Cohen" ],
      "venue" : null,
      "citeRegEx" : "Lao et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Lao et al\\.",
      "year" : 2011
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Mikolov" ],
      "venue" : "In International Conference on Learning Representations",
      "citeRegEx" : "Mikolov,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov",
      "year" : 2013
    }, {
      "title" : "Linguistic regularities in continuous space word representations",
      "author" : [ "Yih Mikolov", "T. Zweig 2013] Mikolov", "W.-t. Yih", "G. Zweig" ],
      "venue" : "In Proc. NAACL-HLT,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Distant supervision for relation extraction without labeled data",
      "author" : [ "Mintz" ],
      "venue" : "In Proceedings of the 47th Annual Meeting of the ACL,",
      "citeRegEx" : "Mintz,? \\Q2009\\E",
      "shortCiteRegEx" : "Mintz",
      "year" : 2009
    }, {
      "title" : "Factorizing YAGO: Scalable machine learning for linked data",
      "author" : [ "Tresp Nickel", "M. Kriegel 2012] Nickel", "V. Tresp", "H.-P. Kriegel" ],
      "venue" : "In Proceedings of the 21st International Conference on World Wide Web,",
      "citeRegEx" : "Nickel et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Nickel et al\\.",
      "year" : 2012
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Socher Pennington", "J. Manning 2014] Pennington", "R. Socher", "C.D. Manning" ],
      "venue" : null,
      "citeRegEx" : "Pennington et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Relation extraction with matrix factorization and universal schemas",
      "author" : [ "Riedel" ],
      "venue" : "In Proc. HLT-NAACL,",
      "citeRegEx" : "Riedel,? \\Q2013\\E",
      "shortCiteRegEx" : "Riedel",
      "year" : 2013
    }, {
      "title" : "Modeling relations and their mentions without labeled text",
      "author" : [ "Yao Riedel", "S. McCallum 2010] Riedel", "L. Yao", "A. McCallum" ],
      "venue" : "In Proc. ECML/PKDD,",
      "citeRegEx" : "Riedel et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Riedel et al\\.",
      "year" : 2010
    }, {
      "title" : "Inclusive yet selective: Supervised distributional hypernymy detection",
      "author" : [ "Erk Roller", "S. Boleda 2014] Roller", "K. Erk", "G. Boleda" ],
      "venue" : "In Proc. COLING,",
      "citeRegEx" : "Roller et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Roller et al\\.",
      "year" : 2014
    }, {
      "title" : "Word embedding calculus in meaningful ultradense subspaces",
      "author" : [ "Rothe", "S. Schütze 2016] Rothe", "H. Schütze" ],
      "venue" : "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Rothe et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Rothe et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning first-order horn clauses from web text",
      "author" : [ "Schoenmackers" ],
      "venue" : "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Schoenmackers,? \\Q2010\\E",
      "shortCiteRegEx" : "Schoenmackers",
      "year" : 2010
    }, {
      "title" : "Analogyspace: reducing the dimensionality of common sense knowledge",
      "author" : [ "Havasi Speer", "R. Lieberman 2008] Speer", "C. Havasi", "H. Lieberman" ],
      "venue" : "In Proceedings of the 23rd AAAI Conference on Artificial intelligence,",
      "citeRegEx" : "Speer et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Speer et al\\.",
      "year" : 2008
    }, {
      "title" : "Multi-instance multilabel learning for relation extraction",
      "author" : [ "Surdeanu" ],
      "venue" : "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language",
      "citeRegEx" : "Surdeanu,? \\Q2012\\E",
      "shortCiteRegEx" : "Surdeanu",
      "year" : 2012
    }, {
      "title" : "From frequency to meaning: Vector space models of semantics",
      "author" : [ "Turney", "P.D. Pantel 2010] Turney", "P. Pantel" ],
      "venue" : "Journal of Artificial Intelligence Research 37:141–188",
      "citeRegEx" : "Turney et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Turney et al\\.",
      "year" : 2010
    }, {
      "title" : "Word representations via gaussian embedding",
      "author" : [ "Vilnis", "L. McCallum 2015] Vilnis", "A. McCallum" ],
      "venue" : null,
      "citeRegEx" : "Vilnis et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Vilnis et al\\.",
      "year" : 2015
    }, {
      "title" : "Take and took, gaggle and goose, book and read: Evaluating the utility of vector differences for lexical relation learning",
      "author" : [ "Vylomova" ],
      "venue" : "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguis-",
      "citeRegEx" : "Vylomova,? \\Q2016\\E",
      "shortCiteRegEx" : "Vylomova",
      "year" : 2016
    }, {
      "title" : "Knowledge graph embedding by translating on hyperplanes",
      "author" : [ "Wang" ],
      "venue" : null,
      "citeRegEx" : "Wang,? \\Q2014\\E",
      "shortCiteRegEx" : "Wang",
      "year" : 2014
    }, {
      "title" : "Efficient inference and learning in a large knowledge base - reasoning with extracted information using a locally groundable first-order probabilistic logic. Machine Learning 100(1):101–126",
      "author" : [ "Wang" ],
      "venue" : null,
      "citeRegEx" : "Wang,? \\Q2015\\E",
      "shortCiteRegEx" : "Wang",
      "year" : 2015
    }, {
      "title" : "Learning to distinguish hypernyms and co-hyponyms",
      "author" : [ "Weeds" ],
      "venue" : "In Proceedings of the 25th International Conference on Computational Linguistics,",
      "citeRegEx" : "Weeds,? \\Q2014\\E",
      "shortCiteRegEx" : "Weeds",
      "year" : 2014
    }, {
      "title" : "Knowledge base completion via search-based question answering",
      "author" : [ "West" ],
      "venue" : "In Proceedings of the 23rd International Conference on World Wide Web,",
      "citeRegEx" : "West,? \\Q2014\\E",
      "shortCiteRegEx" : "West",
      "year" : 2014
    }, {
      "title" : "Embedding entities and relations for learning and inference in knowledge bases",
      "author" : [ "Yang" ],
      "venue" : "In Proceedings of the International Conference on Learning Representations",
      "citeRegEx" : "Yang,? \\Q2015\\E",
      "shortCiteRegEx" : "Yang",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "The relations we consider are taken from three standard benchmark datasets, each containing a mixture of syntactic and semantic relationships: (i) the Google Analogy Test Set (Google), which contains 14 types of relations with a varying number of instances per relation (Mikolov et al. 2013), (ii) the Bigger Analogy Test Set (BATS), wich contains 40 relations with 50 instances per relation (Gladkova, Drozd, and Matsuoka 2016), and (iii) the DiffVec Test Set (DV), which contains 36 relations with a varying number of instances per relation (Vylomova et al.",
      "startOffset" : 270,
      "endOffset" : 291
    } ],
    "year" : 2017,
    "abstractText" : "Word embeddings have been found to capture a surprisingly rich amount of syntactic and semantic knowledge. However, it is not yet sufficiently well-understood how the relational knowledge that is implicitly encoded in word embeddings can be extracted in a reliable way. In this paper, we propose two probabilistic models to address this issue. The first model is based on the common relations-as-translations view, but is cast in a probabilistic setting. Our second model is based on the much weaker assumption that there is a linear relationship between the vector representations of related words. Compared to existing approaches, our models lead to more accurate predictions, and they are more explicit about what can and cannot be extracted from the word embedding.",
    "creator" : "LaTeX with hyperref package"
  }
}