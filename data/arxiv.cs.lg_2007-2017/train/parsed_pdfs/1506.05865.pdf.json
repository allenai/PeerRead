{
  "name" : "1506.05865.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "LCSTS: A Large Scale Chinese Short Text Summarization Dataset",
    "authors" : [ "Baotian Hu", "Qingcai Chen", "Fangze Zhu" ],
    "emails" : [ "zhufangze123}@gmail.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Nowadays, individuals or organizations can easily share or post information to the public on the social network. Take the popular Chinese microblogging website (Sina Weibo) as an example, the People’s Daily, one of the media in China, posts more than tens of weibos (analogous to tweets) each day. Most of these weibos are wellwritten and highly informative because of the text length limitation (less than140 Chinese characters). Such data is regarded as naturally annotated web resources (Sun, 2011). If we can mine these high-quality data from these naturally annotated web resources, it will be beneficial to the research that has been hampered by the lack of data.\n1http://icrc.hitsz.edu.cn/Article/show/139.html\nIn the Natural Language Processing (NLP) community, automatic text summarization is a hot and difficult task. A good summarization system should understand the whole text and re-organize the information to generate coherent, informative, and significantly short summaries which convey important information of the original text (Hovy and Lin, 1998), (Martins, 2007). Most of traditional abstractive summarization methods divide the process into two phrases (Bing et al., 2015). First, key textual elements are extracted from the original text by using unsupervised methods or linguistic knowledge. And then, unclear extracted components are rewritten or paraphrased to produce a concise summary of the original text by using linguistic rules or language generation techniques. Although extensive researches have been done, the linguistic quality of abstractive summary is still far from satisfactory. Recently, deep learning methods have shown potential abilities to learn representation (Hu et al., 2014; Zhou et al., 2015) and generate language (Bahdanau et al., 2014; Sutskever et al., 2014) from large scale data by utilizing GPUs. Many researchers realize that we are closer to generate abstractive summarizations by using the deep learning methods. However, the publicly available and high-quality large scale summarization data set is still very rare and not easy to be constructed manually. For example, the popular document summarization dataset DUC2, TAC3 and TREC4 have only hundreds of human written English text summarizations. The problem is even worse for Chinese. In this pa-\n2http://duc.nist.gov/data.html 3http://www.nist.gov/tac/2015/KBP/ 4http://trec.nist.gov/\nar X\niv :1\n50 6.\n05 86\n5v 4\n[ cs\n.C L\n] 1\n9 Fe\nb 20\n16\nper, we take one step back and focus on constructing LCSTS, the Large-scale Chinese Short Text Summarization dataset by utilizing the naturally annotated web resources on Sina Weibo. Figure 1 shows one weibo posted by the People’s Daily. In order to convey the import information to the public quickly, it also writes a very informative and short summary (in the blue circle) of the news. Our goal is to mine a large scale, high-quality short text summarization dataset from these texts.\nThis paper makes the following contributions: (1) We introduce a large scale Chinese short text summarization dataset. To our knowledge, it is the largest one to date; (2) We provide standard splits for the dataset into large scale training set and human labeled test set which will be easier for benchmarking the related methods; (3) We explore the properties of the dataset and sample 10,666 instances for manually checking and scoring the quality of the dataset; (4) We perform recurrent neural network based encoder-decoder method on the dataset to generate summary and get promising results, which can be used as one baseline of the task."
    }, {
      "heading" : "2 Related Work",
      "text" : "Our work is related to recent works on automatic text summarization and natural language processing based on naturally annotated web resources, which are briefly introduced as follows.\nAutomatic Text Summarization in some form has been studied since 1950. Since then, most researches are related to extractive summarizations by analyzing the organization of the words in the document (Nenkova and McKeown, 2011) (Luhn, 1998); Since it needs labeled data sets for supervised machine learning methods and labeling dataset is very intensive, some researches focused on the unsupervised methods (Mihalcea, 2004). The scale of existing data sets are usually very\nsmall (most of them are less than 1000). For example, DUC2002 dataset contains 567 documents and each document is provided with two 100-words human summaries. Our work is also related to the headline generation, which is a task to generate one sentence of the text it entitles. Colmenares et.al construct a 1.3 million financial news headline dataset written in English for headline generation (Colmenares et al., 2015). However, the data set is not publicly available.\nNaturally Annotated Web Resources based Natural Language Processing is proposed by Sun (Sun, 2011). Naturally Annotated Web Resources is the data generated by users for communicative purposes such as web pages, blogs and microblogs. We can mine knowledge or useful data from these raw data by using marks generated by users unintentionally. Jure et.al track 1.6 million mainstream media sites and blogs and mine a set of novel and persistent temporal patterns in the news cycle (Leskovec et al., 2009). Sepandar et.al use the users’ naturally annotated pattern ‘we feel’ and ‘i feel’ to extract the ‘Feeling’ sentence collection which is used to collect the world’s emotions. In this work, we use the naturally annotated resources to construct the large scale Chinese short text summarization data to facilitate the research on text summarization."
    }, {
      "heading" : "3 Data Collection",
      "text" : "A lot of popular Chinese media and organizations have created accounts on the Sina Weibo. They use their accounts to post news and information. These accounts are verified on the Weibo and labeled by a blue ‘V’. In order to guarantee the quality of the crawled text, we only crawl the verified organizations’ weibos which are more likely to be clean, formal and informative. There are a lot of human intervention required in each step. The process of the data collection is shown as Figure 2 and\nsummarized as follows: 1) We first collect 50 very popular organization users as seeds. They come from the domains of politic, economic, military, movies, game and etc, such as People’s Daily, the Economic Observe press, the Ministry of National Defense and etc. 2) We then crawl fusers followed by these seed users and filter them by using human written rules such as the user must be blue verified, the number of followers is more than 1 million and etc. 3) We use the chosen users and text crawler to crawl their weibos. 4) we filter, clean and extract (short text, summary) pairs. About 100 rules are used to extract high quality pairs. These rules are concluded by 5 peoples via carefully investigating of the raw text. We also remove those paris, whose short text length is too short (less than 80 characters) and length of summaries is out of [10,30]."
    }, {
      "heading" : "4 Data Properties",
      "text" : "The dataset consists of three parts shown as Table 1 and the length distributions of texts are shown as Figure 3.\nPart I is the main content of LCSTS that contains 2,400,591 (short text, summary) pairs. These pairs can be used to train supervised learning model for summary generation.\nPart II contains the 10,666 human labeled (short text, summary) pairs with the score ranges from 1 to 5 that indicates the relevance between the short text and the corresponding summary. ‘1’ denotes “ the least relevant ” and ‘5’ denotes “the most relevant”. For annotating this part, we recruit 5 volunteers, each pair is only labeled by one annotator. These pairs are randomly sampled from Part I and are used to analysize the distribution of pairs in the Part I. Figure 4 illustrates examples of different scores. From the examples we can see that pairs scored by 3, 4 or 5 are very relevant to the corresponding summaries. These summaries are highly informative, concise and significantly short compared to original text. We can also see that many words in the summary do not appear in the original text, which indicates the significant difference of our dataset from sentence compression datasets. The summaries of pairs scored by 1 or 2 are highly abstractive and relatively hard to conclude the summaries from the short text. They are more likely to be headlines or comments instead of summaries. The statistics show that the percent of score 1 and 2 is less than 20% of the\ndata, which can be filtered by using trained classifier.\nPart III contains 1,106 pairs. For this part, 3 annotators label the same 2000 texts and we extract the text with common scores. This part is independent from Part I and Part II. In this work, we use pairs scored by 3, 4 and 5 of this part as the test set for short text summary generation task."
    }, {
      "heading" : "5 Experiment",
      "text" : "Recently, recurrent neural network (RNN) have shown powerful abilities on speech recognition (Graves et al., 2013), machine translation (Sutskever et al., 2014) and automatic dialog response (Shang et al., 2015). However, there is rare research on the automatic text summarization by using deep models. In this section, we use RNN as encoder and decoder to generate the summary of short text. We use the Part I as the training set\nand the subset of Part III, which is scored by 3, 4 and 5, as test set.\nTwo approaches are used to preprocess the data: 1) character-based method, we take the Chinese character as input, which will reduce the vocabulary size to 4,000. 2) word-based method, the\ntext is segmented into Chinese words by using jieba5. The vocabulary is limited to 50,000. We adopt two deep architectures, 1) The local context is not used during decoding. We use the RNN as encoder and it’s last hidden state as the input of decoder, as shown in Figure 5; 2) The context is used during decoding, following (Bahdanau et al., 2014), we use the combination of all the hidden states of encoder as input of the decoder, as shown in Figure 6. For the RNN, we adopt the gated recurrent unit (GRU) which is proposed by (Chung et al., 2015) and has been proved comparable to LSTM (Chung et al., 2014). All the parameters (including the embeddings) of the two architectures are randomly initialized and ADADELTA (Zeiler, 2012) is used to update the learning rate. After the model is trained, the beam search is used to generate the best summaries in the process of decoding and the size of beam is set to 10 in our experiment.\n5https://pypi.python.org/pypi/jieba/\nFor evaluation, we adopt the ROUGE metrics proposed by (Lin and Hovy, 2003), which has been proved strongly correlated with human evaluations. ROUGE-1, ROUGE-2 and ROUGE-L are used. Because the standard Rouge package 6 is used for evaluating English summarization systems, we transform the Chinese words to numerical IDs to adapt to the systems. All the models are trained on the GPUs tesla M2090 for about one week.Table 2 lists the experiment results. As we can see in Figure 7, the summaries generated by RNN with context are very close to human written summaries, which indicates that if we feed enough data to the RNN encoder and decoder, it may generate summary almost from scratch.\nThe results also show that the RNN with context outperforms RNN without context on both character and word based input. This result indicates that the internal hidden states of the RNN encoder can be combined to represent the context of words in summary. And also the performances of the character-based input outperform the wordbased input. As shown in Figure 8, the summary generated by RNN with context by inputing the character-based short text is relatively good, while\n6http://www.berouge.com/Pages/default.aspx\nthe the summary generated by RNN with context on word-based input contains many UNKs. This may attribute to that the segmentation may lead to many UNKs in the vocabulary and text such as the person name and organization name. For example, “愿景光电子” is a company name which is not in the vocabulary of word-based RNN, the RNN summarizer has to use the UNKs to replace the “愿景光电子” in the process of decoding."
    }, {
      "heading" : "6 Conclusion and Future Work",
      "text" : "We constructed a large-scale Chinese short text summarization dataset and performed RNN-based methods on it, which achieved some promising results. This is just a start of deep models on this task and there is much room for improvement. We take the whole short text as one sequence, this may not be very reasonable, because most of short texts contain several sentences. A hierarchical RNN (Li et al., 2015) is one possible direction. The rare word problem is also very important for the generation of the summaries, especially when the input is word-based instead of character-based. It is also a hot topic in the neural generative models such as neural translation machine(NTM) (Luong et al., 2014), which can benefit to this task. We also plan to construct a large document summarization data set by using naturally annotated web resources."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work is supported by National Natural Science Foundation of China: 61473101, 61173075 and 61272383, Strategic Emerging Industry Development Special Funds of Shenzhen: JCYJ20140417172417105, JCYJ20140508161040764 and JCYJ20140627163809422. We thank to Baolin Peng, Lin Ma, Li Yu and the anonymous reviewers for their insightful comments."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Abstractive multi-document summarization via phrase selection and merging",
      "author" : [ "Bing et al.2015] Lidong Bing", "Piji Li", "Yi Liao", "Wai Lam", "Weiwei Guo", "Rebecca Passonneau" ],
      "venue" : "In Proceedings of the ACL-IJCNLP,",
      "citeRegEx" : "Bing et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bing et al\\.",
      "year" : 2015
    }, {
      "title" : "Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555",
      "author" : [ "Chung et al.2014] Junyoung Chung", "Çaglar Gülçehre", "KyungHyun Cho", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Chung et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2014
    }, {
      "title" : "Gated feedback recurrent neural networks. CoRR, abs/1502.02367",
      "author" : [ "Chung et al.2015] Junyoung Chung", "Çaglar Gülçehre", "KyungHyun Cho", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Chung et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2015
    }, {
      "title" : "Heads: Headline generation as sequence prediction using an abstract feature-rich space",
      "author" : [ "Marina Litvak", "Amin Mantrach", "Fabrizio Silvestri" ],
      "venue" : "In Proceddings of 2015 Conference",
      "citeRegEx" : "Colmenares et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Colmenares et al\\.",
      "year" : 2015
    }, {
      "title" : "Speech recognition with deep recurrent neural networks. CoRR, abs/1303.5778",
      "author" : [ "Graves et al.2013] Alex Graves", "Abdel-rahman Mohamed", "Geoffrey E. Hinton" ],
      "venue" : null,
      "citeRegEx" : "Graves et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2013
    }, {
      "title" : "Automated text summarization and the summarist system",
      "author" : [ "Hovy", "Lin1998] Eduard Hovy", "Chin-Yew Lin" ],
      "venue" : "In Proceedings of a Workshop on Held at Baltimore, Maryland: October",
      "citeRegEx" : "Hovy et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Hovy et al\\.",
      "year" : 1998
    }, {
      "title" : "Convolutional neural network architectures for matching natural language sentences",
      "author" : [ "Hu et al.2014] Baotian Hu", "Zhengdong Lu", "Hang Li", "Qingcai Chen" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Hu et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2014
    }, {
      "title" : "Meme-tracking and the dynamics of the news cycle",
      "author" : [ "Lars Backstrom", "Jon Kleinberg" ],
      "venue" : "In Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’09,",
      "citeRegEx" : "Leskovec et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Leskovec et al\\.",
      "year" : 2009
    }, {
      "title" : "A hierarchical neural autoencoder",
      "author" : [ "Li et al.2015] Jiwei Li", "Minh-Thang Luong", "Dan Jurafsky" ],
      "venue" : null,
      "citeRegEx" : "Li et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2015
    }, {
      "title" : "Automatic evaluation of summaries using n-gram co-occurrence statistics",
      "author" : [ "Lin", "Hovy2003] Chin-Yew Lin", "E.H. Hovy" ],
      "venue" : "In Proceedings of 2003 Language Technology Conference (HLTNAACL",
      "citeRegEx" : "Lin et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2003
    }, {
      "title" : "The automatic creation of literature abstracts",
      "author" : [ "H.P. Luhn" ],
      "venue" : "IBM Journal of Research and Development,",
      "citeRegEx" : "Luhn.,? \\Q1998\\E",
      "shortCiteRegEx" : "Luhn.",
      "year" : 1998
    }, {
      "title" : "Addressing the rare word problem in neural machine translation. CoRR, abs/1410.8206",
      "author" : [ "Luong et al.2014] Thang Luong", "Ilya Sutskever", "Quoc V. Le", "Oriol Vinyals", "Wojciech Zaremba" ],
      "venue" : null,
      "citeRegEx" : "Luong et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2014
    }, {
      "title" : "Graph-based ranking algorithms for sentence extraction, applied to text summarization",
      "author" : [ "Rada Mihalcea" ],
      "venue" : "In Proceedings of the 42nd Annual Meeting of the Association",
      "citeRegEx" : "Mihalcea.,? \\Q2004\\E",
      "shortCiteRegEx" : "Mihalcea.",
      "year" : 2004
    }, {
      "title" : "Neural responding machine for short-text conversation. CoRR, abs/1503.02364",
      "author" : [ "Shang et al.2015] Lifeng Shang", "Zhengdong Lu", "Hang Li" ],
      "venue" : null,
      "citeRegEx" : "Shang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Shang et al\\.",
      "year" : 2015
    }, {
      "title" : "Natural language procesing based on naturaly annotated web resources",
      "author" : [ "Mao Song Sun" ],
      "venue" : "Journal of Chinese Information Processing,",
      "citeRegEx" : "Sun.,? \\Q2011\\E",
      "shortCiteRegEx" : "Sun.",
      "year" : 2011
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Oriol Vinyals", "Quoc V. V Le" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Sutskever et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "ADADELTA: an adaptive learning rate method. CoRR, abs/1212.5701",
      "author" : [ "Matthew D. Zeiler" ],
      "venue" : null,
      "citeRegEx" : "Zeiler.,? \\Q2012\\E",
      "shortCiteRegEx" : "Zeiler.",
      "year" : 2012
    }, {
      "title" : "Answer sequence learning with neural networks for answer selection in community question answering",
      "author" : [ "Zhou et al.2015] Xiaoqiang Zhou", "Baotian Hu", "Qingcai Chen", "Buzhou Tang", "Xiaolong Wang" ],
      "venue" : "In Proceedings of the ACL-IJCNLP,",
      "citeRegEx" : "Zhou et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "Such data is regarded as naturally annotated web resources (Sun, 2011).",
      "startOffset" : 59,
      "endOffset" : 70
    }, {
      "referenceID" : 1,
      "context" : "Most of traditional abstractive summarization methods divide the process into two phrases (Bing et al., 2015).",
      "startOffset" : 90,
      "endOffset" : 109
    }, {
      "referenceID" : 7,
      "context" : "Recently, deep learning methods have shown potential abilities to learn representation (Hu et al., 2014; Zhou et al., 2015) and generate language (Bahdanau et al.",
      "startOffset" : 87,
      "endOffset" : 123
    }, {
      "referenceID" : 18,
      "context" : "Recently, deep learning methods have shown potential abilities to learn representation (Hu et al., 2014; Zhou et al., 2015) and generate language (Bahdanau et al.",
      "startOffset" : 87,
      "endOffset" : 123
    }, {
      "referenceID" : 0,
      "context" : ", 2015) and generate language (Bahdanau et al., 2014; Sutskever et al., 2014) from large scale data by utilizing GPUs.",
      "startOffset" : 30,
      "endOffset" : 77
    }, {
      "referenceID" : 16,
      "context" : ", 2015) and generate language (Bahdanau et al., 2014; Sutskever et al., 2014) from large scale data by utilizing GPUs.",
      "startOffset" : 30,
      "endOffset" : 77
    }, {
      "referenceID" : 11,
      "context" : "Since then, most researches are related to extractive summarizations by analyzing the organization of the words in the document (Nenkova and McKeown, 2011) (Luhn, 1998); Since it needs labeled data sets for supervised machine learning methods and labeling dataset is very intensive, some researches focused on the unsupervised methods (Mihalcea, 2004).",
      "startOffset" : 156,
      "endOffset" : 168
    }, {
      "referenceID" : 13,
      "context" : "Since then, most researches are related to extractive summarizations by analyzing the organization of the words in the document (Nenkova and McKeown, 2011) (Luhn, 1998); Since it needs labeled data sets for supervised machine learning methods and labeling dataset is very intensive, some researches focused on the unsupervised methods (Mihalcea, 2004).",
      "startOffset" : 335,
      "endOffset" : 351
    }, {
      "referenceID" : 4,
      "context" : "3 million financial news headline dataset written in English for headline generation (Colmenares et al., 2015).",
      "startOffset" : 85,
      "endOffset" : 110
    }, {
      "referenceID" : 15,
      "context" : "Naturally Annotated Web Resources based Natural Language Processing is proposed by Sun (Sun, 2011).",
      "startOffset" : 87,
      "endOffset" : 98
    }, {
      "referenceID" : 8,
      "context" : "6 million mainstream media sites and blogs and mine a set of novel and persistent temporal patterns in the news cycle (Leskovec et al., 2009).",
      "startOffset" : 118,
      "endOffset" : 141
    }, {
      "referenceID" : 5,
      "context" : "Recently, recurrent neural network (RNN) have shown powerful abilities on speech recognition (Graves et al., 2013), machine translation (Sutskever et al.",
      "startOffset" : 93,
      "endOffset" : 114
    }, {
      "referenceID" : 16,
      "context" : ", 2013), machine translation (Sutskever et al., 2014) and automatic dialog response (Shang et al.",
      "startOffset" : 29,
      "endOffset" : 53
    }, {
      "referenceID" : 14,
      "context" : ", 2014) and automatic dialog response (Shang et al., 2015).",
      "startOffset" : 38,
      "endOffset" : 58
    }, {
      "referenceID" : 0,
      "context" : "We use the RNN as encoder and it’s last hidden state as the input of decoder, as shown in Figure 5; 2) The context is used during decoding, following (Bahdanau et al., 2014), we use the combination of all the hidden states of encoder as input of the decoder, as shown in Figure 6.",
      "startOffset" : 150,
      "endOffset" : 173
    }, {
      "referenceID" : 3,
      "context" : "For the RNN, we adopt the gated recurrent unit (GRU) which is proposed by (Chung et al., 2015) and has been proved comparable to LSTM (Chung et al.",
      "startOffset" : 74,
      "endOffset" : 94
    }, {
      "referenceID" : 2,
      "context" : ", 2015) and has been proved comparable to LSTM (Chung et al., 2014).",
      "startOffset" : 47,
      "endOffset" : 67
    }, {
      "referenceID" : 17,
      "context" : "All the parameters (including the embeddings) of the two architectures are randomly initialized and ADADELTA (Zeiler, 2012) is used to update the learning rate.",
      "startOffset" : 109,
      "endOffset" : 123
    }, {
      "referenceID" : 9,
      "context" : "A hierarchical RNN (Li et al., 2015) is one possible direction.",
      "startOffset" : 19,
      "endOffset" : 36
    }, {
      "referenceID" : 12,
      "context" : "It is also a hot topic in the neural generative models such as neural translation machine(NTM) (Luong et al., 2014), which can benefit to this task.",
      "startOffset" : 95,
      "endOffset" : 115
    } ],
    "year" : 2016,
    "abstractText" : "Automatic text summarization is widely regarded as the highly difficult problem, partially because of the lack of large text summarization data set. Due to the great challenge of constructing the large scale summaries for full text, in this paper, we introduce a large corpus of Chinese short text summarization dataset constructed from the Chinese microblogging website Sina Weibo, which is released to the public1. This corpus consists of over 2 million real Chinese short texts with short summaries given by the author of each text. We also manually tagged the relevance of 10,666 short summaries with their corresponding short texts. Based on the corpus, we introduce recurrent neural network for the summary generation and achieve promising results, which not only shows the usefulness of the proposed corpus for short text summarization research, but also provides a baseline for further research on this topic.",
    "creator" : "LaTeX with hyperref package"
  }
}