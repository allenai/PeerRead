{
  "name" : "1706.05140.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "An Automatic Approach for Document-level Topic Model Evaluation",
    "authors" : [ "Shraey Bhatia", "Jey Han Lau", "Timothy Baldwin" ],
    "emails" : [ "shraeybhatia@gmail.com,", "jeyhan.lau@gmail.com,", "tb@ldwin.net" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Topic models such as latent Dirichlet allocation (Blei et al., 2003) jointly learn latent topics (in the form of multinomial distributions over words) and topic allocations to individual documents (in the form of multinomial distributions over topics), and provide a powerful means of document collection navigation and visualisation (Newman et al., 2010a; Chaney and Blei, 2012; Smith et al., 2017). One property of LDA-style topic models that has contributed to their popularity is that they are highly configurable, and can be structured to capture a myriad of statistical dependencies, such as between topics (Blei and Lafferty, 2006), between documents associated with the same individual (Rosen-Zvi et al., 2004), or between documents associated with individuals in different network relations (Wang and Blei, 2011). This has led to a wealth of topic models of different types, and the need for methods to evaluate different styles of topic model over the same document collections. Test data perplexity is the obvious solution, but it has been shown to correlate poorly with direct\nhuman assessment of topic model quality (Chang et al., 2009), motivating the need for automatic topic model evaluation methods which emulate human assessment. Research in this vein has focused primarily on evaluating the quality of individual topics (Newman et al., 2010b; Mimno et al., 2011; Aletras and Stevenson, 2013; Lau et al., 2014; Fang et al., 2016) and largely ignored evaluation of topic allocations to individual documents, and it has become widely accepted that topic-level evaluation is a reliable indicator of the intrinsic quality of the overall topic model (Lau et al., 2014). We challenge this assumption, and demonstrate that topic model evaluation should operate at both the topic and document levels.\nOur primary contributions are as follows: (1) we empirically demonstrate that there can be large discrepancies between topic- and document-level topic model evaluation; (2) we demonstrate that previously-proposed document-level evaluation approaches can be misleading, and propose an alternative evaluation method; and (3) we propose an automatic approach to topic model evaluation based on analysis of document-level topic distributions, which we show to correlate strongly with manual annotations."
    }, {
      "heading" : "2 Related Work",
      "text" : "Perplexity or held-out likelihood has long been used as an intrinsic metric to evaluate topic models (Wallach et al., 2009). Chang et al. (2009) proposed two human judgement tasks, at the topic and document levels, and showed that there is low correlation between perplexity and direct human evaluations of topic model quality. The two tasks took the form of “intruder” tasks, whereby subjects were tasked with identifying an intruder topic word for a given topic, or an intruder topic for a given document. Specifically, in the word intrusion\nar X\niv :1\n70 6.\n05 14\n0v 1\n[ cs\n.C L\n] 1\n6 Ju\nn 20\n17\ntask, an intruder word was added to the top-5 topic words, and annotators were asked to identify the intruder word. Similarly in the topic intrusion task, a document and 4 topics were presented — the top-3 topics corresponding to the document and a random intruder topic — and subjects were asked to spot the intruder topic. The intuition behind both methods is that the higher the quality of the topic or topic allocation for a given document, the easier it should be to detect the intruder.\nNewman et al. (2010b) proposed to measure topic coherence directly in the form of “observed coherence”, in which human judges rated topics directly on an ordinal 3-point scale. They experimented with a range of different methods to automate the rating task, and reported the best results by simply aggregating pointwise mutual information (pmi) scores for different pairings of topic words, based on a sliding window over English Wikipedia.\nBuilding on the work of Chang et al. (2009), Lau et al. (2014) proposed an improved method for estimating observed coherence based on normalised pmi (npmi), and further automated the word intruder detection task based on a combination of word association features (pmi, npmi, CP1, and CP2) in a learn-to-rank model (Joachims, 2006). Additionally, the authors showed a strong correlation between word intrusion and observed coherence, and suggested that it is possible to perform topic model evaluation based on aggregation of word intrusion or observed coherence scores across all topics."
    }, {
      "heading" : "3 Datasets and Topic Models",
      "text" : "We use two document collections for our experiments: APNEWS and the British National Corpus (“BNC”: Burnard (1995)). APNEWS is a collection of Associated Press1 news articles from 2009 to 2016, while BNC is an amalgamation of extracts from different sources such as books, journals, letters, and pamphlets. We sample 50K and 15K documents from APNEWS and BNC, respectively, to create two datasets for our experiments.\nIn terms of preprocessing, we use Stanford CoreNLP (Manning et al., 2014) to tokenise words and sentences. We additionally remove stop words,2 lower-case all word tokens, filter word types which occur less than 10 times, and exclude\n1https://www.ap.org/en-gb/ 2We use Mallet’s stop word list: https://github.\ncom/mimno/Mallet/tree/master/stoplists\nthe top 0.1% most frequent word types. Statistics for each of the preprocessed datasets are provided in Table 1.\nSimilarly to Chang et al. (2009), we base our analysis on a representative selection of topic models, each of which we train over APNEWS and BNC to generate 100 topics: • lda (Blei et al., 2003) uses a symmetric\nDirichlet prior to model both document-level topic mixtures and topic-level word mixtures. It is one of the most commonly used topic model implementations and serve as a benchmark for comparison. We use Mallet’s implementation of lda for our experiments. Note that Mallet implements various enhancements to the basic LDA model, including the use of an asymmetric–symmetric prior. • ctm (Blei and Lafferty, 2006) is an extension\nof lda that uses a logistic normal prior over topic proportions instead of a Dirichlet prior to model correlations between different topics and reduce overlap in topic content. • hca (Buntine and Mishra, 2014) is an exten-\nsion to LDA to capture word burstiness (Doyle and Elkan, 2009), based on the observation that there tends to be higher likelihood of generating a word which has already been seen recently. Word generation is modelled by a Pitman–Yor process (Chen et al., 2011). • ntm (Cao et al., 2015) is a neural topic\nmodel, where topic–word multinomials are modelled as a look-up layer of words, and topic–document multinomials are modelled as a look-up layer of documents. The output layer of the network is given by the dot product of the two vectors. There are 2 variants of ntm: unsupervised and supervised. We use only the unsupervised variant in our experiments. • cluster is a baseline topic model, specifi-\ncally designed to produce highly coherent topics but “bland” topic allocations. We represent word types in the documents with pre-trained word2vec vectors (Mikolov et al., 2013a,b), pre-trained on Google News,3 and create word clusters using k-means clustering (k = 100) to generate the topics. We derive the multinomial distribution for each topic based on the cosine distance to the cluster centroid, and\n3Available from: https://code.google.com/ archive/word2vec.\nlinear normalisation across all words. To generate the topic allocation for a given document, we first calculate a document representation based on the mean of the word2vec vectors of its content words. For each cluster, we represent them by calculating the mean word2vec vectors of its top-10 words. Given the document vector and clusters/topics, we calculate the similarity of the document to each cluster based on cosine similarity, and finally (linearly) normalise the similarities to generate a probability distribution."
    }, {
      "heading" : "4 Topic-level Evaluation: Topic Coherence",
      "text" : "Pointwise mutual information (and its normalised variant npmi) is a common association measure to estimate topic coherence (Newman et al., 2010b; Mimno et al., 2011; Aletras and Stevenson, 2013; Lau et al., 2014; Fang et al., 2016). Although the method is successful in assessing topic quality, it tells us little about the association between documents and topics. As we will see, a topic model can produce topics that are coherent — in terms of npmi association — but poor descriptor of the overall concepts in the document collection.\nWe first compute topic coherence for all 5 topic models over APNEWS and BNC using npmi (Lau et al., 2014) and present the results in Table 2.4 We see that lda and cluster perform consistently well across both datasets. hca performs well over\n4We use the following open source toolkit to compute topic coherence: https://github.com/jhlau/ topic_interpretability.\nAPNEWS but poorly over BNC. Both ctm and ntm topics appear to have low coherence over the two datasets.\nBased on these results, one would conclude that cluster is a good topic model, as it produces very coherent topics. To better understand the nature and quality of the topics, we present a random sample of lda and cluster topics in Table 3.\nLooking at the topics, we see that cluster tends to include different inflectional forms of the same word (e.g. prohibited, probihiting) and nearsynonyms/sister words (e.g. river, lake, creeks) in a single topic. This explains the strong npmi association of the cluster topics. On the other hand, lda discovers related words that collectively describe concepts rather than just clustering (near) synonyms. This suggests that the topic coherence metric alone may not completely capture topic model quality, leading us to also investigate the topic distribution associated with documents from our collections."
    }, {
      "heading" : "5 Human Evaluation of Document-level Topic Allocations",
      "text" : "In this section, we describe a series of manual evaluations of document-level topic allocations, in order to get a more holistic evaluation of the true quality of the different topic models (in line with the original work of Chang et al. (2009))."
    }, {
      "heading" : "5.1 Topic Intrusion",
      "text" : "The goal of the topic intrusion task is to examine whether the document–topic allocations from a given topic model accord with manual judgements. We formulate the task similarly to Chang et al. (2009), in presenting the human judges with a snippet from each document, along with four topics. The four topics comprise the top-3 highest probability topics related to document, and one intruder topic. Each annotator is required to pick the topic that is least representative of the document, with the expectation that the better the topic model, the more readily they should be able to pick the intruder topic. The intruder topic is sampled randomly, subject to the following conditions: (1) it should be a low probability topic for the target document; and (2) it should be a high probability topic for at least one other document. The first constraint is intended to ensure that the intruder topic is unrelated to the target document, while the second constraint is intended to select a topic that is highly\nassociated with some documents, and hence likely to be coherent and not a junk topic. Each topic is represented by its top-10 most probable words, and the target document is presented in the form of the first three sentences, with an option to view more of the document if further context is needed.\nWe used Amazon Mechanical Turk to collect the human judgements, with five document–topic combinations forming a single HIT, one of which acts as a quality control. The control items were sourced from an earlier annotation task where subjects were asked to score the top-5 topics for a target document on a scale of 0–3. The 50 topscoring documents from this annotation task, with their top-3 topics, were chosen as controls. The intruder topic for the control was generated by randomly selecting 10 words from the corpus vocabulary. In order to pass quality control, each worker had to correctly select the intruder topic for the control document–topic item over 60% of time (across all HITs they completed). Each document–topic pair was rated by 10 annotators initially, and for HITs where less than 3 annotations passed quality control, we reposted them for a second round of annotation.\nFor our annotation task, we randomly sampled 100 documents from each of our two datasets, for\neach of which we generate document–topic items based on the five different topic models. In total, therefore, we annotated 1000 (100 documents × 2 collections× 5 topic models) document–topic combinations. After quality control, the final dataset contains an average of 5.4 and 5.5 valid intruder topic annotations for APNEWS and BNC, respectively.\nChang et al. (2009) proposed topic log odds (“TLO”) as a means of evaluating the topic intrusion task. The authors defined topic log odds for a document–topic pair as the difference in the logprobability assigned to the intruder and the logprobability assigned to the topic chosen by a given annotator, which they then averaged across annotators to get a TLO score for a single document. Separately, Chang et al. (2009) proposed model precision as a means of evaluating the word intrusion task, whereby they simply calculated the proportion of annotators who correctly selected the intruder word for a given topic. In addition to presenting results based on TLO, we apply the model precision methodology in our evaluation of the topic intrusion task, in calculating the proportion of annotators who correctly selected the intruder topic for a given document, which we then average across documents to derive a model score.\nThe results of the human annotation task are summarised in Tables 4 and 5. Looking at model precision for APNEWS first, we see that lda outperforms the other topic models. ctm and hca perform credibly, whereas ntm and cluster are quite poor. Moving on to BNC, we see a drop in score for lda, to a level comparable with ctm. cluster improves slightly higher than BNC, whereas hca drops considerably (despite being designed specifically to deal with word burstiness in the longer documents characteristic of BNC). Figure 1 shows boxplots for topic-level model precision, and reflects a similar trend.\nLooking next to TLO in Table 5, we see a totally different picture, with cluster being rated as the best topic model by a clear margin. This exposes a flaw in the TLO formulation, in the case of adversarial topic models such as clusterwhich assign near-uniform probabilities across all topics. This results in the difference in probability mass being\nvery close to the upper bound of zero in all cases, meaning that even for random topic selection, TLO is near perfect. We can also see this in Figure 2, where the boxes for cluster have nearly zero range. Indeed, if we combined the results for TLO with those for topic coherence, we would (very wrongly!) conclude that cluster performs best over both document collections. More encouragingly, for the other four topic models, the results for TLO are much more consistent with those based on model precision."
    }, {
      "heading" : "5.2 Direct Annotation of Topic Assignments",
      "text" : "Newman et al. (2010b) proposed a more direct approach to topic coherence, by asking people to rate topics directly based on the top-N words. Taking inspiration from their methodology, we propose to directly annotate each topic assigned to a target document. We present the human annotators with the target document and the top-ranked (high-\nest probability) topic from each of the five topic models, and ask them to rate each topic on an ordinal scale of 0–3. At the model level, we take the mean rating over all document–topic pairings for that topic model (based, once again, on 100 documents per collection).5 We summarise the findings in Table 6.\nWe observe that, in the case of APNEWS, lda does considerably better than ctm and hca, whereas for BNC, lda and ctm are quite close, with hca close behind. cluster and ntm do poorly across both datasets. The overall trend for APNEWS of lda > ctm > hca > cluster > ntm is consistent with the model precision results in Table 4. In the case of BNC, the observation of ctm ≈ lda > hca > cluster > ntm is also broadly the same, except that hca does not do as well over the topic intrusion task. Here, we are more interested in the relative performance of topic models than absolute numbers, although the low absolute scores are an indication that it is a difficult annotation task.\nBroadly combined across the two evaluation methodologies, lda and ctm are top-performing, hca gets mixed results, and cluster and ntm are the lowest performers. These results generally agree with the model precision findings, demonstrating that model precision is a more robust metric than TLO."
    }, {
      "heading" : "6 Automatic Evaluation",
      "text" : "A limitation of the topic intrusion task is that it requires manual annotation, making it ill-suited for large-scale or automatic evaluation. We present the first attempt to automate the prediction of the intruder topic, with the aim of developing an approach to topic model evaluation which comple-\n5The 100 documents used for this task were different to the ones used in Section 5.1.\nments topic coherence (as motivated in Sections 4 and 5)."
    }, {
      "heading" : "6.1 Methodology",
      "text" : "We build a support vector regression (SVR) model (Joachims, 2006) to rank topics given a document to select the intruder topic. We first explain an intuition of the features that are driving the SVR.\nTo rank topics for a document, we need to first compute the probability of a topic t given document d, i.e. P (t|d). We can invert the condition using Bayes rule:\nP (t|d) = P (d|t)P (t) P (d)\n∝ P (d|t)P (t)\nWe can omit P (d) as the probability of document d is constant for the topics that we are ranking.\nNext we represent topic t using its top-N highest probability words, giving:\nP (t|d) ∝ P (d|w1, ..., wN )P (w1, ..., wN ) ∝ logP (d|w1, ..., wN )+\nlogP (w1, ..., wN )\nThe first term logP (d|w1, ..., wN ) can be interpreted from an information retrieval perspective, where we are computing the relevance of document d given query terms w1, w2, ..., wN . This term constitutes the first feature for the SVR. We use Indri6 to index the document collection, and compute logP (d|w1, ..., wN ) given a set of query words and a document.7\nWe estimate the second term, logP (w1, ..., wN ), using the pairwise probability of the topic words:∑\n0<i≤m ∑ i+1≤j≤m log #(wi, wj) #(·)\nwhere m denotes the number of topic words used, #(wi, wj) is the number of documents where word wi and wj co-occur, and #(·) is the total number of documents. We explore using two values of m here: 5 and 10.8 These two values constitute the second and third features of the SVR.\nTo train the SVR, we sample 1700 random documents and split them into 1600/100 documents for the training and test partitions, respectively.\n6http://www.lemurproject.org 7N = 10. 8That is, if m = 5, we compute pairwise probabilities\nusing the top-5 topic words.\nThe test documents are the same 100 documents that were previously used for intruder topics (Section 5.1). As the intruder topics are artificially generated, we can sample additional documents to create a larger training set for the SVR; the ability to generate arbitrary training data is a strength of our method.\nWe pool together all 5 topic models when training the SVR, thereby generating 8000 training and 500 development and testing instances for each dataset. For each document, the SVR is trained to rank the topics in terms of their likelihood of being an intruder topic.9 The top-ranking topic is selected as the system-predicted intruder word, and model precision is computed as before (Section 5.1).10"
    }, {
      "heading" : "6.2 System results",
      "text" : "In Figure 3, we present the human vs. system mean model precision on the test partition for each of the topic models. We see that the trend line for the system model precision very closely tracks that of human model precision. In general, the best systems — lda and ctm — and the worst systems — ntm and cluster — are predicted correctly. The correlation between the two is very high, at r = 0.88 and 0.87 for APNEWS and BNC, respectively. This suggests that the automated method is a reliable means of evaluating document-level topic model quality.\n9We use the default hyper-parameter values for the SVR (C = 0.01), and hence do no require a development set for tuning.\n10Note that the system model precision for each document– topic combination is a binary value as there is only 1 system — as opposed to multiple annotators — selecting an intruder word."
    }, {
      "heading" : "7 Discussion",
      "text" : "To better understand the differences between human- and system-predicted intruder topics, we present a number of documents and their associated topics in Table 7, focusing specifically on: (a) intruder topics that humans struggle to identify but our automatic method reliably detects; and (b) conversely, intruder topics which humans readily identify but our method struggles to detect.\nLooking at the topics across the two types of errors, we notice that there are often multiple “bad” topics for these documents: occasionally the annotators are able to single out the worst topic while the system fails (1st and 2nd document), but sometimes the opposite happens (3rd and 4th document). In the first case, the top-ranking topic (church, gay, ...) from the topic model is associated with the document because of the service, but actually capturing a very different aspect of religion to what is discussed in the document, which leads our method astray. A similar effect is seen with the second document. In the case of the third and fourth documents, there is actually content further down in the document which is relevant to the topics the human annotators select, but it is not apparent in the document snippet presented to the annotators. That is, the effect is caused by resource limitations for the annotation task, that our automated method does not suffer from.\nWhen we aggregate the top-level model precision values for a topic model, these differences are averaged out (hence the strong correlation in Section 6.2), but these qualitative analyses reveal that there are still slight disparities between human\nError Type: High human MP Low system MP\nDocument more than 2,000 attendees are expected to attend public funeral services for former nevada gov. kenny guinn . a catholic mass on tuesday morning will be followed by a memorial reception at palace station . the two-term governor who served from 1999 to 2007 died thursday after falling from the roof of his las vegas home while making repairs . he was 73 . guinn ’s former chief of staff pete ernaut says attendance to the services will be limited only by the size of the venues . services start at 10 a.m. at st. joseph , husband of mary roman catholic church ...\nTopics\n0: church gay marriage religious catholic same-sex couples pastor members bishop 1: died family funeral honor memorial father death wife cemetery son 2: casino las vegas nevada gambling casinos ford vehicles cars car X: students college student campus education tuition universities colleges high degree\nDocument the milwaukee art museum is exhibiting more than 70 works done by 19th century portrait painter thomas sully . it ’s the first retrospective of the artist in 30 years and the first to present the artist ’s portraits and subject pictures . sully was known for employing drama and theatricality to his works . in some of his full-length portraits , he composed his figures as if they were onstage . some of his subjects even seem to be trying to directly engage the viewer . milwaukee art museum director daniel keegan says the exhibit provides a new look ...\nTopics\n0: china art chinese arts artist painting artists cuba world beijing 1: show music film movie won festival tickets game band play 2: online information internet book video media facebook phone computer technology X: kelley family letter leave absence left united jay weeks director\nannotators and the automated method in intruder topic selection.\nTo further understand how the topics relate to the documents in different topic models, we present documents with the corresponding topics for different topic models in Table 8.\nIn the human annotation task, we use the top-10 most probable words to represent a topic. We use 10 words as it is the standard approach to visualising topics, but this is an important hyper-parameter which needs to be investigated further (Lau and Baldwin, 2016), which we leave to future work."
    }, {
      "heading" : "8 Conclusion",
      "text" : "We demonstrate empirically that there can be large discrepancies between topic coherence and document–topic associations. By way of designing an artificial topic model, we showed that a topic model can simultaneously produce topics that are coherent but be largely undescriptive of the document collection. We propose a method to automatically predict document-level topic quality and found encouraging correlation with manual evaluation, suggesting that it can be used as an alternative approach for extrinsic topic model evaluation."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This research was supported in part by the Australian Research Council."
    } ],
    "references" : [ {
      "title" : "Evaluating topic coherence using distributional semantics",
      "author" : [ "Nikos Aletras", "Mark Stevenson." ],
      "venue" : "Proceedings of the Tenth International Workshop on Computational Semantics (IWCS-10). Potsdam, Germany, pages 13–22.",
      "citeRegEx" : "Aletras and Stevenson.,? 2013",
      "shortCiteRegEx" : "Aletras and Stevenson.",
      "year" : 2013
    }, {
      "title" : "Latent Dirichlet allocation",
      "author" : [ "David M Blei", "Andrew Y Ng", "Michael I Jordan." ],
      "venue" : "Journal of Machine Learning Research 3:993–1022.",
      "citeRegEx" : "Blei et al\\.,? 2003",
      "shortCiteRegEx" : "Blei et al\\.",
      "year" : 2003
    }, {
      "title" : "Experiments with non-parametric topic models",
      "author" : [ "Wray L Buntine", "Swapnil Mishra." ],
      "venue" : "Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. pages 881–890.",
      "citeRegEx" : "Buntine and Mishra.,? 2014",
      "shortCiteRegEx" : "Buntine and Mishra.",
      "year" : 2014
    }, {
      "title" : "User guide for the British National Corpus",
      "author" : [ "Lou Burnard" ],
      "venue" : null,
      "citeRegEx" : "Burnard.,? \\Q1995\\E",
      "shortCiteRegEx" : "Burnard.",
      "year" : 1995
    }, {
      "title" : "A novel neural topic model and its supervised extension",
      "author" : [ "Ji." ],
      "venue" : "Proceedings of AAAI 2015. pages 2210–2216.",
      "citeRegEx" : "Ji.,? 2015",
      "shortCiteRegEx" : "Ji.",
      "year" : 2015
    }, {
      "title" : "Visualizing topic models",
      "author" : [ "Allison June-Barlow Chaney", "David M. Blei." ],
      "venue" : "Proceedings of the 6th International Conference on Weblogs and Social Media (ICWSM 2012). Dublin, Ireland.",
      "citeRegEx" : "Chaney and Blei.,? 2012",
      "shortCiteRegEx" : "Chaney and Blei.",
      "year" : 2012
    }, {
      "title" : "Reading tea leaves: How humans interpret topic models",
      "author" : [ "Jonathan Chang", "Sean Gerrish", "Chong Wang", "Jordan L. Boyd-Graber", "David M. Blei." ],
      "venue" : "Advances in Neural Information Processing Systems 21 (NIPS-09). Vancouver, Canada, pages 288–296.",
      "citeRegEx" : "Chang et al\\.,? 2009",
      "shortCiteRegEx" : "Chang et al\\.",
      "year" : 2009
    }, {
      "title" : "Sampling table configurations for the hierarchical poisson-dirichlet process",
      "author" : [ "Changyou Chen", "Lan Du", "Wray Buntine." ],
      "venue" : "Machine Learning and Knowledge Discovery in Databases pages 296–311.",
      "citeRegEx" : "Chen et al\\.,? 2011",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2011
    }, {
      "title" : "Accounting for burstiness in topic models",
      "author" : [ "Gabriel Doyle", "Charles Elkan." ],
      "venue" : "Proceedings of the 26th Annual International Conference on Machine Learning. pages 281–288.",
      "citeRegEx" : "Doyle and Elkan.,? 2009",
      "shortCiteRegEx" : "Doyle and Elkan.",
      "year" : 2009
    }, {
      "title" : "Using word embedding to evaluate the coherence of topics from Twitter data",
      "author" : [ "Anjie Fang", "Craig Macdonald", "Iadh Ounis", "Philip Habel." ],
      "venue" : "Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information",
      "citeRegEx" : "Fang et al\\.,? 2016",
      "shortCiteRegEx" : "Fang et al\\.",
      "year" : 2016
    }, {
      "title" : "Training linear SVMs in linear time",
      "author" : [ "Thorsten Joachims." ],
      "venue" : "Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. pages 217–226.",
      "citeRegEx" : "Joachims.,? 2006",
      "shortCiteRegEx" : "Joachims.",
      "year" : 2006
    }, {
      "title" : "The sensitivity of topic coherence evaluation to topic cardinality",
      "author" : [ "Jey Han Lau", "Timothy Baldwin." ],
      "venue" : "Proceedings of NAACL-HLT . pages 483–487.",
      "citeRegEx" : "Lau and Baldwin.,? 2016",
      "shortCiteRegEx" : "Lau and Baldwin.",
      "year" : 2016
    }, {
      "title" : "Machine reading tea leaves: Automatically evaluating topic coherence and topic model quality",
      "author" : [ "Jey Han Lau", "David Newman", "Timothy Baldwin." ],
      "venue" : "Proceedings of EACL 2014. pages 530–539.",
      "citeRegEx" : "Lau et al\\.,? 2014",
      "shortCiteRegEx" : "Lau et al\\.",
      "year" : 2014
    }, {
      "title" : "The Stanford CoreNLP natural language processing toolkit",
      "author" : [ "Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky." ],
      "venue" : "Association for Computational Linguistics (ACL) System Demonstrations.",
      "citeRegEx" : "Manning et al\\.,? 2014",
      "shortCiteRegEx" : "Manning et al\\.",
      "year" : 2014
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "Proceedings of Workshop at the International Conference on Learning Representations, 2013. Scottsdale, USA.",
      "citeRegEx" : "Mikolov et al\\.,? 2013a",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean." ],
      "venue" : "Advances in Neural Information Processing Systems. pages 3111–3119.",
      "citeRegEx" : "Mikolov et al\\.,? 2013b",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Optimizing semantic coherence in topic models",
      "author" : [ "David Mimno", "Hanna Wallach", "Edmund Talley", "Miriam Leenders", "Andrew McCallum." ],
      "venue" : "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP",
      "citeRegEx" : "Mimno et al\\.,? 2011",
      "shortCiteRegEx" : "Mimno et al\\.",
      "year" : 2011
    }, {
      "title" : "Visualizing document collections and search results using topic mapping",
      "author" : [ "David Newman", "Timothy Baldwin", "Lawrence Cavedon", "Sarvnaz Karimi", "David Martinez", "Justin Zobel." ],
      "venue" : "Journal of Web Semantics 8(2–3):169–175.",
      "citeRegEx" : "Newman et al\\.,? 2010a",
      "shortCiteRegEx" : "Newman et al\\.",
      "year" : 2010
    }, {
      "title" : "Automatic evaluation of topic coherence",
      "author" : [ "David Newman", "Jey Han Lau", "Karl Grieser", "Timothy Baldwin." ],
      "venue" : "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Lin-",
      "citeRegEx" : "Newman et al\\.,? 2010b",
      "shortCiteRegEx" : "Newman et al\\.",
      "year" : 2010
    }, {
      "title" : "The author-topic model for authors and documents",
      "author" : [ "Michal Rosen-Zvi", "Thomas Griffiths", "Mark Steyvers", "Padhraic Smyth." ],
      "venue" : "Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence. pages 487–494.",
      "citeRegEx" : "Rosen.Zvi et al\\.,? 2004",
      "shortCiteRegEx" : "Rosen.Zvi et al\\.",
      "year" : 2004
    }, {
      "title" : "Evaluating visual representations for topic understanding and their effects on manually generated labels",
      "author" : [ "Alison Smith", "Tak Yeon Lee", "Forough PoursabziSangdeh", "Jordan Boyd-Graber", "Kevin Seppi", "Niklas Elmqvist", "Leah Findlater." ],
      "venue" : "Transac-",
      "citeRegEx" : "Smith et al\\.,? 2017",
      "shortCiteRegEx" : "Smith et al\\.",
      "year" : 2017
    }, {
      "title" : "Evaluation methods for topic models",
      "author" : [ "Hanna M Wallach", "Iain Murray", "Ruslan Salakhutdinov", "David Mimno." ],
      "venue" : "Proceedings of the 26th International Conference on Machine Learning (ICML 2009). Montreal, Canada, pages 1105–1112.",
      "citeRegEx" : "Wallach et al\\.,? 2009",
      "shortCiteRegEx" : "Wallach et al\\.",
      "year" : 2009
    }, {
      "title" : "Collaborative topic modeling for recommending scientific articles",
      "author" : [ "Chong Wang", "David M. Blei." ],
      "venue" : "Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. pages 448–456.",
      "citeRegEx" : "Wang and Blei.,? 2011",
      "shortCiteRegEx" : "Wang and Blei.",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Topic models such as latent Dirichlet allocation (Blei et al., 2003) jointly learn latent topics (in the form of multinomial distributions over words) and topic allocations to individual documents (in the form of multinomial distributions over topics), and provide a powerful means of document collection navigation and visualisation (Newman et al.",
      "startOffset" : 49,
      "endOffset" : 68
    }, {
      "referenceID" : 17,
      "context" : ", 2003) jointly learn latent topics (in the form of multinomial distributions over words) and topic allocations to individual documents (in the form of multinomial distributions over topics), and provide a powerful means of document collection navigation and visualisation (Newman et al., 2010a; Chaney and Blei, 2012; Smith et al., 2017).",
      "startOffset" : 273,
      "endOffset" : 338
    }, {
      "referenceID" : 5,
      "context" : ", 2003) jointly learn latent topics (in the form of multinomial distributions over words) and topic allocations to individual documents (in the form of multinomial distributions over topics), and provide a powerful means of document collection navigation and visualisation (Newman et al., 2010a; Chaney and Blei, 2012; Smith et al., 2017).",
      "startOffset" : 273,
      "endOffset" : 338
    }, {
      "referenceID" : 20,
      "context" : ", 2003) jointly learn latent topics (in the form of multinomial distributions over words) and topic allocations to individual documents (in the form of multinomial distributions over topics), and provide a powerful means of document collection navigation and visualisation (Newman et al., 2010a; Chaney and Blei, 2012; Smith et al., 2017).",
      "startOffset" : 273,
      "endOffset" : 338
    }, {
      "referenceID" : 19,
      "context" : "One property of LDA-style topic models that has contributed to their popularity is that they are highly configurable, and can be structured to capture a myriad of statistical dependencies, such as between topics (Blei and Lafferty, 2006), between documents associated with the same individual (Rosen-Zvi et al., 2004), or between documents associated with individuals in different network relations (Wang and Blei, 2011).",
      "startOffset" : 293,
      "endOffset" : 317
    }, {
      "referenceID" : 22,
      "context" : ", 2004), or between documents associated with individuals in different network relations (Wang and Blei, 2011).",
      "startOffset" : 89,
      "endOffset" : 110
    }, {
      "referenceID" : 6,
      "context" : "Test data perplexity is the obvious solution, but it has been shown to correlate poorly with direct human assessment of topic model quality (Chang et al., 2009), motivating the need for automatic topic model evaluation methods which emulate human assessment.",
      "startOffset" : 140,
      "endOffset" : 160
    }, {
      "referenceID" : 18,
      "context" : "Research in this vein has focused primarily on evaluating the quality of individual topics (Newman et al., 2010b; Mimno et al., 2011; Aletras and Stevenson, 2013; Lau et al., 2014; Fang et al., 2016) and largely ignored evaluation of topic allocations to individual documents, and it has become widely accepted that topic-level evaluation is a reliable indicator of the intrinsic quality of the overall topic model (Lau et al.",
      "startOffset" : 91,
      "endOffset" : 199
    }, {
      "referenceID" : 16,
      "context" : "Research in this vein has focused primarily on evaluating the quality of individual topics (Newman et al., 2010b; Mimno et al., 2011; Aletras and Stevenson, 2013; Lau et al., 2014; Fang et al., 2016) and largely ignored evaluation of topic allocations to individual documents, and it has become widely accepted that topic-level evaluation is a reliable indicator of the intrinsic quality of the overall topic model (Lau et al.",
      "startOffset" : 91,
      "endOffset" : 199
    }, {
      "referenceID" : 0,
      "context" : "Research in this vein has focused primarily on evaluating the quality of individual topics (Newman et al., 2010b; Mimno et al., 2011; Aletras and Stevenson, 2013; Lau et al., 2014; Fang et al., 2016) and largely ignored evaluation of topic allocations to individual documents, and it has become widely accepted that topic-level evaluation is a reliable indicator of the intrinsic quality of the overall topic model (Lau et al.",
      "startOffset" : 91,
      "endOffset" : 199
    }, {
      "referenceID" : 12,
      "context" : "Research in this vein has focused primarily on evaluating the quality of individual topics (Newman et al., 2010b; Mimno et al., 2011; Aletras and Stevenson, 2013; Lau et al., 2014; Fang et al., 2016) and largely ignored evaluation of topic allocations to individual documents, and it has become widely accepted that topic-level evaluation is a reliable indicator of the intrinsic quality of the overall topic model (Lau et al.",
      "startOffset" : 91,
      "endOffset" : 199
    }, {
      "referenceID" : 9,
      "context" : "Research in this vein has focused primarily on evaluating the quality of individual topics (Newman et al., 2010b; Mimno et al., 2011; Aletras and Stevenson, 2013; Lau et al., 2014; Fang et al., 2016) and largely ignored evaluation of topic allocations to individual documents, and it has become widely accepted that topic-level evaluation is a reliable indicator of the intrinsic quality of the overall topic model (Lau et al.",
      "startOffset" : 91,
      "endOffset" : 199
    }, {
      "referenceID" : 12,
      "context" : ", 2016) and largely ignored evaluation of topic allocations to individual documents, and it has become widely accepted that topic-level evaluation is a reliable indicator of the intrinsic quality of the overall topic model (Lau et al., 2014).",
      "startOffset" : 223,
      "endOffset" : 241
    }, {
      "referenceID" : 21,
      "context" : "Perplexity or held-out likelihood has long been used as an intrinsic metric to evaluate topic models (Wallach et al., 2009).",
      "startOffset" : 101,
      "endOffset" : 123
    }, {
      "referenceID" : 6,
      "context" : "Chang et al. (2009) proposed two human judgement tasks, at the topic and document levels, and showed that there is low correlation between perplexity and direct human evaluations of topic model quality.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 10,
      "context" : "(2014) proposed an improved method for estimating observed coherence based on normalised pmi (npmi), and further automated the word intruder detection task based on a combination of word association features (pmi, npmi, CP1, and CP2) in a learn-to-rank model (Joachims, 2006).",
      "startOffset" : 259,
      "endOffset" : 275
    }, {
      "referenceID" : 6,
      "context" : "Building on the work of Chang et al. (2009), Lau et al.",
      "startOffset" : 24,
      "endOffset" : 44
    }, {
      "referenceID" : 6,
      "context" : "Building on the work of Chang et al. (2009), Lau et al. (2014) proposed an improved method for estimating observed coherence based on normalised pmi (npmi), and further automated the word intruder detection task based on a combination of word association features (pmi, npmi, CP1, and CP2) in a learn-to-rank model (Joachims, 2006).",
      "startOffset" : 24,
      "endOffset" : 63
    }, {
      "referenceID" : 3,
      "context" : "We use two document collections for our experiments: APNEWS and the British National Corpus (“BNC”: Burnard (1995)).",
      "startOffset" : 100,
      "endOffset" : 115
    }, {
      "referenceID" : 13,
      "context" : "In terms of preprocessing, we use Stanford CoreNLP (Manning et al., 2014) to tokenise words and sentences.",
      "startOffset" : 51,
      "endOffset" : 73
    }, {
      "referenceID" : 6,
      "context" : "Similarly to Chang et al. (2009), we base our",
      "startOffset" : 13,
      "endOffset" : 33
    }, {
      "referenceID" : 1,
      "context" : "• lda (Blei et al., 2003) uses a symmetric Dirichlet prior to model both document-level topic mixtures and topic-level word mixtures.",
      "startOffset" : 6,
      "endOffset" : 25
    }, {
      "referenceID" : 2,
      "context" : "• hca (Buntine and Mishra, 2014) is an extension to LDA to capture word burstiness (Doyle and Elkan, 2009), based on the observation that there tends to be higher likelihood of generating a word which has already been seen recently.",
      "startOffset" : 6,
      "endOffset" : 32
    }, {
      "referenceID" : 8,
      "context" : "• hca (Buntine and Mishra, 2014) is an extension to LDA to capture word burstiness (Doyle and Elkan, 2009), based on the observation that there tends to be higher likelihood of generating a word which has already been seen recently.",
      "startOffset" : 83,
      "endOffset" : 106
    }, {
      "referenceID" : 7,
      "context" : "Word generation is modelled by a Pitman–Yor process (Chen et al., 2011).",
      "startOffset" : 52,
      "endOffset" : 71
    }, {
      "referenceID" : 18,
      "context" : "Pointwise mutual information (and its normalised variant npmi) is a common association measure to estimate topic coherence (Newman et al., 2010b; Mimno et al., 2011; Aletras and Stevenson, 2013; Lau et al., 2014; Fang et al., 2016).",
      "startOffset" : 123,
      "endOffset" : 231
    }, {
      "referenceID" : 16,
      "context" : "Pointwise mutual information (and its normalised variant npmi) is a common association measure to estimate topic coherence (Newman et al., 2010b; Mimno et al., 2011; Aletras and Stevenson, 2013; Lau et al., 2014; Fang et al., 2016).",
      "startOffset" : 123,
      "endOffset" : 231
    }, {
      "referenceID" : 0,
      "context" : "Pointwise mutual information (and its normalised variant npmi) is a common association measure to estimate topic coherence (Newman et al., 2010b; Mimno et al., 2011; Aletras and Stevenson, 2013; Lau et al., 2014; Fang et al., 2016).",
      "startOffset" : 123,
      "endOffset" : 231
    }, {
      "referenceID" : 12,
      "context" : "Pointwise mutual information (and its normalised variant npmi) is a common association measure to estimate topic coherence (Newman et al., 2010b; Mimno et al., 2011; Aletras and Stevenson, 2013; Lau et al., 2014; Fang et al., 2016).",
      "startOffset" : 123,
      "endOffset" : 231
    }, {
      "referenceID" : 9,
      "context" : "Pointwise mutual information (and its normalised variant npmi) is a common association measure to estimate topic coherence (Newman et al., 2010b; Mimno et al., 2011; Aletras and Stevenson, 2013; Lau et al., 2014; Fang et al., 2016).",
      "startOffset" : 123,
      "endOffset" : 231
    }, {
      "referenceID" : 12,
      "context" : "We first compute topic coherence for all 5 topic models over APNEWS and BNC using npmi (Lau et al., 2014) and present the results in Table 2.",
      "startOffset" : 87,
      "endOffset" : 105
    }, {
      "referenceID" : 6,
      "context" : "ations of document-level topic allocations, in order to get a more holistic evaluation of the true quality of the different topic models (in line with the original work of Chang et al. (2009)).",
      "startOffset" : 172,
      "endOffset" : 192
    }, {
      "referenceID" : 6,
      "context" : "We formulate the task similarly to Chang et al. (2009), in presenting the human judges with a snippet from each document, along with four topics.",
      "startOffset" : 35,
      "endOffset" : 55
    }, {
      "referenceID" : 10,
      "context" : "We build a support vector regression (SVR) model (Joachims, 2006) to rank topics given a document to select the intruder topic.",
      "startOffset" : 49,
      "endOffset" : 65
    }, {
      "referenceID" : 11,
      "context" : "We use 10 words as it is the standard approach to visualising topics, but this is an important hyper-parameter which needs to be investigated further (Lau and Baldwin, 2016), which we leave to future work.",
      "startOffset" : 150,
      "endOffset" : 173
    } ],
    "year" : 2017,
    "abstractText" : "Topic models jointly learn topics and document-level topic distribution. Extrinsic evaluation of topic models tends to focus exclusively on topic-level evaluation, e.g. by assessing the coherence of topics. We demonstrate that there can be large discrepancies between topicand documentlevel model quality, and that basing model evaluation on topic-level analysis can be highly misleading. We propose a method for automatically predicting topic model quality based on analysis of documentlevel topic allocations, and provide empirical evidence for its robustness.",
    "creator" : "LaTeX with hyperref package"
  }
}