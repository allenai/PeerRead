{
  "name" : "1605.09128.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Control of Memory, Active Perception, and Action in Minecraft",
    "authors" : [ "Junhyuk Oh", "Valliappa Chockalingam", "Satinder Singh", "Honglak Lee" ],
    "emails" : [ "JUNHYUK@UMICH.EDU", "VALLI@UMICH.EDU", "BAVEJA@UMICH.EDU", "HONGLAK@UMICH.EDU" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Deep learning approaches (surveyed in LeCun et al., 2015; Schmidhuber, 2015) have made advances in many lowlevel perceptual supervised learning problems (Krizhevsky et al., 2012; Girshick et al., 2014; Simonyan & Zisserman, 2015). This success has been extended to reinforcement learning (RL) problems that involve visual perception. For example, the Deep Q-Network (DQN) (Mnih et al., 2015) architecture has been shown to successfully learn to play many Atari 2600 games in the Arcade Learning Environment (ALE) benchmark (Bellemare et al., 2013) by learning visual features useful for control directly from raw pixels using Q-Learning (Watkins & Dayan, 1992).\nProceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s).\nTo p-\nD ow n V ie w\nFi rs\ntPe\nrs on\nV ie\nw\n(a) t=3 (b) t=10 (c) t=11 (d) t=19\nFigure 1. Example task in Minecraft. In this task, the agent should visit the red block if the indicator (next to the start location) is yellow. Otherwise, if the indicator is green, it should visit the blue block. The top row shows the agent’s first-person observation. The bottom row visualizes the map and the agent’s location; this is not available to the agent. (a) The agent observes the yellow indicator. (b) The agent looks left and sees the blue block, (c) but it decides to keep going straight having previously seen the yellow indicator. (d) Finally, it visits the red block and receives a positive reward.\nRecently, researchers have explored problems that require faculties associated with higher-level cognition (e.g., inferring simple general purpose algorithms: Graves et al., 2014, and, Q&A: Weston et al., 2015). Most of these advances, however, are restricted to the supervised learning setting, which provides clear error signals. In this paper, we are interested in extending this success to similarly cognition-inspired RL tasks. Specifically, this paper introduces a set of tasks in Minecraft1, a flexible 3D world in which an agent can collect resources, build structures, and survive attacks from enemies. Our RL tasks (one example is illustrated in Figure 1) not only have the usual RL challenges of partial observability, high-dimensional (visual) perception, and delayed reward, but also require an agent to develop movement policies by learning how to use its active perception to observe useful information and collect reward. In addition, our RL tasks require an agent to learn to use any memory it possesses including its interaction with active perception which feeds observations into\n1https://minecraft.net/\nar X\niv :1\n60 5.\n09 12\n8v 1\n[ cs\n.A I]\n3 0\nM ay\n2 01\nmemory. We note that for simplicity we hereafter refer to these cognition-inspired tasks as cognitive tasks but acknowledge that they form at best a very limited exploration of the range of cognitive faculties in humans.\nIn this work, we aim to not only systematically evaluate the performance of different neural network architectures on our tasks, but also examine how well such architectures generalize to unseen or larger topologies (Minecraft maps). The empirical results show that existing DRL architectures (Mnih et al., 2015; Hausknecht & Stone, 2015) perform worse on unseen or larger maps compared to training sets of maps, even though they perform reasonably well on the training maps. Motivated by the lack of generalization of existing architectures on our tasks, we also propose new memory-based DRL architectures. Our proposed architectures store recent observations into their memory and retrieve relevant memory based on the temporal context, whereas memory retrieval in existing architectures used in RL problems is not conditioned on the context. In summary, we show that our architectures outperform existing ones on most of the tasks as well as generalize better to unseen maps by exploiting their new memory mechanisms."
    }, {
      "heading" : "2. Related Work",
      "text" : "Neural Networks with External Memory. Graves et al. (2014) introduced a Neural Turing Machine (NTM), a differentiable external memory architecture, and showed that it can learn algorithms such as copy and reverse. Zaremba & Sutskever (2015) proposed RL-NTM that has a nondifferentiable memory to scale up the addressing mechanism of NTM and applied policy gradient to train the architecture. Joulin & Mikolov (2015) implemented a stack using neural networks and demonstrated that it can infer several algorithmic patterns. Sukhbaatar et al. (2015b) proposed a Memory Network (MemNN) for Q&A and language modeling tasks, which stores all inputs and retrieves relevant memory blocks depending on the question.\nDeep Reinforcement Learning. Neural networks have been used to learn features for RL tasks for a few decades (e.g., Tesauro, 1995 and Lange & Riedmiller, 2010). Recently, Mnih et al. (2015) proposed a Deep Q-Network (DQN) for training deep convolutional neural networks (CNNs) through Q-Learning in an end-to-end fashion; this achieved state-of-the-art performance on Atari games. Guo et al. (2014) used slow Monte-Carlo Tree Search (MCTS) (Kocsis & Szepesvári, 2006) to generate a relatively small amount of data to train fast-playing convolutional networks in Atari games. Schulman et al. (2015), Levine et al. (2016), and Lillicrap et al. (2016) have successfully trained deep neural networks to directly learn policies and applied their architectures to robotics problems. In addition, there are deep RL approaches to tasks other than Atari such as learning algorithms (Zaremba\net al., 2016) and text-based games (Sukhbaatar et al., 2015a; Narasimhan et al., 2015). There have also been a few attempts to learn state-transition models using deep learning to improve exploration in RL (Oh et al., 2015; Stadie et al., 2015). Most recently, Mnih et al. (2016) proposed asynchronous DQN and showed that it can learn to explore a 3D environment similar to Minecraft. Unlike their work, we focus on a systematic evaluation of the ability to deal with partial observability, active perception, and external memory in different neural network architectures as well as generalization across size and maps.\nModel-free Deep RL for POMDPs. Building a modelfree agent in partially observable Markov decision processes (POMDPs) is a challenging problem because the agent needs to learn how to summarize history for actionselection. To deal with such a challenge, Bakker et al. (2003) used a Long Short-Term Memory (LSTM) network (Hochreiter & Schmidhuber, 1997) in an offline policy learning framework to show that a robot controlled by an LSTM network can solve T-Mazes where the robot should go to the correct destination depending on the traffic signal at the beginning of the maze. Wierstra et al. (2010) proposed a Recurrent Policy Gradient method and showed that an LSTM network trained using this method outperforms other methods in several tasks including TMazes. More recently, Zhang et al. (2016) introduced continuous memory states to augment the state and action space and showed it can memorize salient information through Guided Policy Search (Levine & Koltun, 2013). Hausknecht & Stone (2015) proposed Deep Recurrent QNetwork (DRQN) which consists of an LSTM on top of a CNN based on the DQN framework and demonstrated improved handling of partial observability in Atari games.\nDeparture from Related Work. The architectures we introduce use memory mechanisms similar to MemNN, but our architectures have a layer that constructs a query for memory retrieval based on temporal context. Our architectures are also similar to NTM in that a recurrent controller interacts with an external memory, but ours have a simpler writing and addressing mechanism which makes them easier to train. Most importantly, our architectures are used in an RL setting and must learn from a delayed reward signal, whereas most previous work in exploring architectures with memory is in the supervised learning setting with its much more direct and undelayed error signals. We describe details of our architectures in Section 4.\nThe tasks we introduce are inspired by the T-maze experiments (Bakker et al., 2003) as well as MazeBase (Sukhbaatar et al., 2015a), which has natural language descriptions of mazes available to the agent. Unlike these previous tasks, our mazes have high-dimensional visual observations with deep partial observability due to the nature of the 3D worlds. In addition, the agent has to learn how\nbest to control its active perception system to collect useful information at the right time in our tasks; this is not necessary in previous work."
    }, {
      "heading" : "3. Background: Deep Q-Learning",
      "text" : "Denote the state, immediate reward, and action at time t as st, rt, at respectively. In the DQN framework, every transition Tt = (st, st+1, at, rt) is stored in a replay memory. For (each) iteration i, the deep neural network (with parameters θ) is trained to approximate the action-value function from transitions {(s, s′, a, r)} by minimizing the loss functions Li (θi) as follows:\nLi (θ) = Es,a∼πθ [ (yi −Q (s, a; θ))2 ] ∇θLi (θ) = Es,a∼πθ [(yi −Q (s, a; θ))∇θQ (s, a; θ)]\nwhere yi = Es′∼πθ [r + γmaxa′ Q (s′, a′; θ′)] is the target Q-value estimated by a target Q-network (θ′). In practice, the expectation terms are approximated by sampling a mini-batch of transitions from the replay memory. The parameter of target Q-network (θ′) is synchronized with the learned network (θ) after a fixed number of iterations."
    }, {
      "heading" : "4. Architectures",
      "text" : "The importance of retrieving a prior observation from memory depends on the current context. For example, in the maze of Figure 1 where the color of the indicator block determines the desired target color, the indicator information is important only when the agent is seeing a potential target and has to decide whether to approach it or find a different target. Motivated by the lack of “context-dependent memory retrieval” in existing DRL architectures, we present three new memory-based architectures in this section.\nOur proposed architectures (Figure 3c-e) consist of convolutional networks for extracting high-level features from images (§4.1), a memory that retains a recent history of observations (§4.2), and a context vector used both for memory retrieval and (in part for) action-value estimation (§4.3). Depending on how the context vector is constructed, we obtain three new architectures: Memory Q-Network (MQN), Recurrent Memory Q-Network (RMQN), and Feedback Recurrent Memory Q-Network (FRMQN)."
    }, {
      "heading" : "4.1. Encoding",
      "text" : "For each time-step, a raw observation (pixels) is encoded to a fixed-length vector as follows:\net = ϕenc (xt) (1)\nwhere xt ∈ Rc×h×w is h × w image with c channels, and et ∈ Re is the encoded feature at time t. In this work, we use a CNN to encode the observation."
    }, {
      "heading" : "4.2. Memory",
      "text" : "The memory operations in the proposed architectures are similar to those proposed in MemNN. Write. The encoded features of last M observations are linearly transformed and stored into the memory as key and value memory blocks as illustrated in Figure 2a. More formally, two types of memory blocks are defined as follows:\nMkeyt = W keyEt (2) Mvalt = W valEt (3)\nwhere Mkeyt ,M val t ∈ Rm×M are memory blocks with mdimensional embeddings, and Wkey,Wval ∈ Rm×e are parameters of the linear transformations for keys and values respectively. Et = [et−1, et−2, ..., et−M ] ∈ Re×M is the concatenation of features of the last M observations. Read. The reading mechanism of the memory is based on soft attention (Graves, 2013; Bahdanau et al., 2015) as illustrated in Figure 2b. Given a context vector ht ∈ Rm (§4.3), the memory module draws soft attention over memory locations (and implicitly time) by computing the innerproduct between the context and all key memory blocks as follows:\npt,i = exp\n( h>t M key t [i] ) ∑M j=1 exp ( h>t M key t [j]\n) (4) where pt,i ∈ R is an attention weight for i-th memory block (t−i time-step). The output of the read operation is the linear sum of the value memory blocks based on the attention weights as follows:\not = Mvalt pt (5)\nwhere ot ∈ Rm and pt ∈ RM are the retrieved memory and the attention weights respectively."
    }, {
      "heading" : "4.3. Context",
      "text" : "To retrieve useful information from memory, the context vector should capture relevant spatio-temporal information from the observations. To this end, we present three different architectures for constructing the context vector:\nMQN: ht = Wcet (6) RMQN: [ht, ct] = LSTM (et,ht−1, ct−1) (7)\nFRMQN: [ht, ct] = LSTM ([et, ot−1] ,ht−1, ct−1) (8)\nwhere ht, ct ∈ Rm are a context vector and a memory cell of LSTM respectively, and [et, ot−1] denotes concatenation of the two vectors as input for LSTM. MQN is a feedforward architecture that constructs the context based on only the current observation, which is very similar to MemNN except that the current input is used for memory retrieval in the temporal context of an RL problem. RMQN is a recurrent architecture that captures spatio-temporal information from the history of observations using LSTM. This architecture allows for retaining temporal information through LSTM as well as external memory. Finally, FRMQN has a feedback connection from the retrieved memory to the context vector as illustrated in Figure 4. This allows the FRMQN architecture to refine its context based on the previously retrieved memory so that it can do more complex reasoning as time goes on. Note that feedback connections are analogous to the idea of multiple hops in MemNN in the sense that the architecture retrieves memory blocks multiple times based on the previously retrieved memory. However, FRMQN retrieves memory blocks through time, while MemNN does not.\nFinally, the architectures estimate action-values by incorporating the retrieved memory and the context vector:\nqt = ϕ q (ht, ot) (9)\nwhere qt ∈ Ra is the estimated action-value, and ϕq is a multi-layer perceptron (MLP) taking two inputs. In the results we report here, we used an MLP with one hidden layer as follows: gt = f ( Whht + ot ) ,qt = W\nqgt where f is a rectified linear function (Nair & Hinton, 2010) applied only to half of the hidden units for easy optimization by following Sukhbaatar et al. (2015b)."
    }, {
      "heading" : "5. Experiments",
      "text" : "The experiments, baselines, and tasks are designed to investigate how useful context-dependent memory retrieval is for generalizing to unseen maps, and when memory feedback connections in FRMQN are helpful. Game play videos can be found in the supplementary material and at the following website: https://sites.google.com/a/umich.edu/ junhyuk-oh/icml2016-minecraft. Next, we describe aspects that are common to all tasks and our training methodology. Environment. In all the tasks, episodes terminate either when the agent finishes the task or after 50 steps. An agent receives -0.04 reward at every time step. The agent’s initial looking direction is randomly selected among four directions: north, south, east, and west. For tasks where there is randomness (e.g., maps, spawn points), we randomly sampled an instance after every episode. Actions. The following six actions are available: Look left/right (±90◦ in yaw), Look up/down (±45◦ in pitch), and Move forward/backward. Moving actions move the agent one block forward or backward in the direction it is facing. The pitch is limited to [−45◦, 0◦]. Baselines. We compare our three architectures with two baselines: DQN (Mnih et al., 2015) (see Figure 3a) and DRQN (Hausknecht & Stone, 2015) (see Figure 3b). DQN is a CNN architecture that takes a fixed number of frames as input. DRQN is a recurrent architecture that has an LSTM layer on top of the CNN. Note that DQN cannot take more\nthan the number of frames used during training because its first convolution layer takes a fixed number of observations. However, DRQN and our architectures can take arbitrary number of input frames using their recurrent layers. Additionally, our architectures can use an arbitrarily large size of memory during evaluation as well.\nTraining details. Input frames from Minecraft are captured as 32 × 32 RGB images. All the architectures use the same 2-layer CNN architecture as described in the supplementary material. In the DQN and DRQN architectures, the last convolutional layer is followed by a fullyconnected layer with 256 hidden units. In our architectures, the last convolution layer is given as the encoded feature for memory blocks. In addition, 256 LSTM units are used in DRQN, RMQN, and FRMQN. More details including hyperparameters for Deep Q-Learning are described in the supplementary material. Our implementation is based on Torch7 (Collobert et al., 2011), a public DQN implementation (Mnih et al., 2015), and a Minecraft Forge Mod.2"
    }, {
      "heading" : "5.1. I-Maze: Description and Results",
      "text" : "Task. Our I-Maze task was inspired by T-Mazes which have been used in animal cognition experiments (Olton, 1979). Maps for this task (see Figure 5a) have an indicator at the top that has equal chance of being yellow or green. If the indicator is yellow, the red block gives +1 reward and the blue block gives -1 reward; if the indicator is green, the red block gives -1 and the blue block gives +1 reward. Thus, the agent should memorize the color of the indicator at the beginning while it is in view and visit the correct goal depending on the indicator-color. We varied the length of the vertical corridor to l = {5, 7, 9} during training. The last 12 frames were given as input for all architectures, and\n2http://files.minecraftforge.net/\nthe size of memory for our architectures was 11. Performance on the training set. We observed two stages of behavior during learning from all the architectures: 1) early in the training the discount factor and time penalty led to the agent to take a chance by visiting any goal, and 2) later in the training the agent goes to the correct goal by learning the correlation between the indicator and the goal. As seen in the learning curves in Figure 6a, our architectures converge more quickly than DQN and DRQN to the correct behavior. In particular, we observed that DRQN takes many more epochs to reach the second stage after the first stage has been reached. This is possibly due to the long time interval between seeing the indicator and the goals. Besides, the indicator block is important only when the agent is at the bottom end of the vertical corridor and needs to decide which way to go (see Figure 5a). In other words, the indicator information does not affect the agent’s decision making along its way to the end of the corridor. This makes it even more difficult for DRQN to retain the indicator information for a long time. On the other hand, our architectures can handle these problems by storing the history of observations into memory and retrieving such information when it is important, based on the context. Generalization performance. To investigate generalization performance, we evaluated the architectures on maps that have vertical corridor lengths {4, 6, 8, 10, 15, 20, 25, 30, 35, 40} that were not present in the training maps. More specifically, testing on {6, 8} sizes of maps and the rest of the sizes of maps can evaluate interpolation and extrapolation performance, respectively (Schaul et al., 2015). Since some unseen maps are larger than the training maps, we used 50 last frames as input during evaluation on the unseen maps for all architectures except for DQN, which can take only 12 frames as discussed in the experimental setup. The size of memory for our architectures is set to 49. The performance on the unseen set of maps is visualized in Figure 6b. Although the generalization performances of all architectures are highly variable even after training performance converges, it can be seen that FRMQN consistently outperforms the other architectures in terms of average reward. To further investigate the performance for different lengths of the vertical corridor, we measured the performance on each size of map in Table 1. It turns out that all architectures perform well on {6, 8} sizes of maps, which indicates that they can interpolate within the training set of maps. However, our architectures extrapolate to larger maps significantly better than the two baselines. Analysis of memory retrieval. Figure 7a visualizes FRMQN’s memory retrieval on a large I-Maze, where FRMQN sharply retrieves the indicator information only when it reaches the end of the corridor where it then makes a decision of which goal block to visit. This is a reasonable strategy because the indicator information is important only\nwhen it is at the end of the vertical corridor. This qualitative result implies that FRMQN learned a general strategy that looks for the indicator, goes to the end of the corridor, and retrieves the indicator information when it decides which goal block to visit. We observed similar policies learned by MQN and RMQN, but the memory attention for the indicator was not as sharp as FRMQN’s attention and so they visit wrong goals in larger I-Mazes more often.\nThe results on I-Maze shown above suggest that solving a task on a set of maps does not guarantee solving the same task on similar but unseen maps, and such generalization performance highly depends on the feature representation learned by deep neural networks. The extrapolation result shows that context-dependent memory retrieval in our architectures is important for learning a general strategy when the importance of an observational-event depends highly on the temporal context."
    }, {
      "heading" : "5.2. Pattern Matching: Description and Results",
      "text" : "Task. As illustrated in Figure 5b, this map consists of two 3 × 3 rooms. The visual patterns of the two rooms are either identical or different with equal probability. If the two\nrooms have the exact same color patterns, the agent should visit the blue block. If the rooms have different color patterns, the agent should visit the red block. The agent receives a +1 reward if it visits the correct block and a -1 reward if it visits the wrong block. This pattern matching task requires more complex reasoning (comparing two visual patterns given at different time steps) than the I-Maze task above. We generated 500 training and 500 unseen maps in such a way that there is little overlap between the two sets of visual patterns. Details of the map generation process are described in the supplementary material. The last 10 frames were given as input for all architectures, and the size of memory was set to 9.\nPerformance on the training set. The results plotted in Figure 6c and Table 2 show that MQN and FRMQN successfully learned to go to the correct goal block for all runs in the training maps. We observed that DRQN always learned a sub-optimal policy that goes to any goal regardless of the visual patterns of the two rooms. Another observation is the training performances of DQN and RMQN are a bit unstable; they often learned the same suboptimal policy, whereas MQN and FRMQN consistently learned to go to the correct goal across different runs. We hypothesize that it is not trivial for a neural network to compare two visual patterns observed in different time-steps unless the network can model high-order interactions between two specific observations for visual matching, which might be the reason why DQN and DRQN fail more often. Context-dependent memory retrieval mechanism in our architectures can alleviate this problem by retrieving two visual patterns corresponding to the observations of the two rooms before decision making.\nGeneralization performance. Table 2 and Figure 6d show that FRMQN achieves the highest success rate on the unseen set of maps. Interestingly, MQN fails to generalize to unseen visual patterns. We observed that MQN pays attention to the two visual patterns before choosing one of the goals through its memory retrieval. However, since the retrieved memory is just a convex combination of two visual patterns, it is hard for MQN to compare the similarity between them. Thus, we believe that MQN simply overfits to the training maps by memorizing the weighted sum of pairs of visual patterns in the training set of maps. On the other hand, FRMQN can utilize retrieved memory as well as its recurrent connections to compare visual patterns over time. Analysis of memory retrieval. An example of FRMQN’s memory retrieval is visualized in Figure 7b. FRMQN pays attention to both rooms, gradually moving weight from one to the other as time progresses, which means that the context vector is repeatedly refined based on the encoded features of the room retrieved through its feedback connections. Given this visualization and its good generalization performance, we hypothesize that FRMQN utilizes its feedback connection to compare the two visual features over time rather than comparing them at a single time-step. This result supports our view that feedback connections can play an important role in tasks where more complex reasoning is required with retrieved memories."
    }, {
      "heading" : "5.3. Random Mazes: Description and Results",
      "text" : "Task. A random maze task consists of randomly generated walls and goal locations as shown in Figure 5c and 5d. We present 4 classes of tasks using random mazes.\n• Single Goal: The task is to visit the blue block which gives +1 reward while avoiding the red block that gives -1 reward.\n• Sequential Goals: The task is to visit the red block first and then the blue block later which gives +0.5 and +1 reward respectively. If an agent visits the colored blocks in the reverse order, it receives -0.5 and -1 reward respectively.\n• Single Goal with Indicator: If the indicator is yellow, the task is to visit the red block. If the indicator is green, the task is to visit the blue block. Visiting the correct block results in +1 reward and visiting the incorrect block results in -1 reward.\n• Sequential Goals with Indicator: If the indicator is yellow, the task is to visit the blue block first and then the red block. If the indicator is green, the task is to visit the red block first and then the blue block. Visiting the blocks in the correct order results in +0.5 for the first block and +1 reward for the second block. Visiting the blocks in the reverse order results in -0.5 and -1 reward respectively.\nWe randomly generated 1000 maps used for training and two types of unseen evaluation sets of maps: 1000 maps of the same sizes present in the training maps and 1000 larger maps. The last 10 frames were given as input for all architectures, and the size of memory was set to 9. Performance on the training set. In this task, the agent not only needs to remember important information while traversing the maps (e.g., an indicator) but it also has to search for the goals as different maps have different obstacle and goal locations. Table 3 shows that RMQN and FRMQN achieve higher asymptotic performances than the other architectures on the training set of maps. Generalization performance. For the larger-sized unseen maps, we terminated episodes after 100 steps rather than 50 steps and used a time penalty of−0.02 considering their size. During evaluation, we used 10 frames as input for DQN and DRQN and 30 frames for MQN, RMQN, and\nFRMQN; these choices gave the best results for each architecture.\nThe results in Table 3 show that, as expected, the performance of all the architectures worsen in unseen maps. From the learning curves (see Figure 6e-g), we observed that generalization performance on unseen maps does not improve after some epochs, even though training performance is improving. This implies that improving policies on a fixed set of maps does not necessarily guarantee better performance on new environments. However, RMQN and FRMQN generalize better than the other architectures in most of the tasks. In particular, compared to the other architectures, DRQN’s performance is significantly degraded on unseen maps. In addition, while DQN shows good generalization performance on the Single Goal task which primarily requires search, on the other tasks it tends to go to any goal regardless of important information (e.g., color of indicator). This can be seen through the higher failure rate (the number of incorrectly completed episodes divided by the total number of episodes) of DQN on indicator tasks in Table 3.\nTo investigate how well the architectures handle partial observability, we measured precision (proportion of correct goal visits to all goal visits) versus the distance between goal and indicator in Single Goal with Indicator task, which is visualized in Figure 8. Notably, the gap between our architectures (RMQN and FRMQN) and the other architectures becomes larger as the distance increases. This result implies that our architectures are better at handling partial observability than the other architectures, because large distance between indicator and goal is more likely to introduce deeper partial observability (i.e., long-term dependency).\nCompared to MQN, the RMQN and FRMQN architectures achieve better generalization performance which suggests that the recurrent connections in the latter two architectures\nare a crucial component for handling random topologies. In addition, FRMQN and RMQN achieve similar performances, which implies that the feedback connection may not be always helpful in these tasks. We note that given a retrieved memory (e.g., indicator), the reasoning required for these tasks is simpler than the reasoning required for Pattern Matching task. Analysis of memory retrieval. An example of memory retrieval in FRMQN is visualized in Figure 7c. It retrieves memory that contains important information (e.g., indicator) before it visits a goal block. The memory retrieval strategy is reasonable and is an evidence that the proposed architectures make it easier to generalize to large-scale environments by better handling partial observability."
    }, {
      "heading" : "6. Discussion",
      "text" : "In this paper, we introduced three classes of cognitioninspired tasks in Minecraft and compared the performance of two existing architectures with three architectures that we proposed here. We emphasize that unlike most evaluations of RL algorithms, we trained and evaluated architectures on disjoint sets of maps so as to specifically consider the applicability of learned value functions to unseen (interpolation and extrapolation) maps.\nIn summary, our main empirical result is that contextdependent memory retrieval, particularly with a feedback connection from the retrieved memory, can more effectively solve our set of tasks that require control of active perception and external physical movement actions. Our architectures, particularly FRQMN, also show superior ability relative to the baseline architectures when learning value functions whose behavior generalizes better from training to unseen environments. In future work, we intend to take advantage of the flexibility of the Minecraft domain to construct even more challenging cognitive tasks to further evaluate our architectures."
    }, {
      "heading" : "Acknowledgement",
      "text" : "This work was supported by NSF grant IIS-1526059. Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the views of the sponsor."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua" ],
      "venue" : "In International Conference on Learning Representations,",
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "A robot that reinforcement-learns to identify and memorize important previous observations",
      "author" : [ "Bakker", "Bram", "Zhumatiy", "Viktor", "Gruener", "Gabriel", "Schmidhuber", "Jürgen" ],
      "venue" : "In Intelligent Robots and Systems,",
      "citeRegEx" : "Bakker et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Bakker et al\\.",
      "year" : 2003
    }, {
      "title" : "The arcade learning environment: An evaluation platform for general agents",
      "author" : [ "M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling" ],
      "venue" : "Journal of Artificial Intelligence Research, 47:253–279,",
      "citeRegEx" : "Bellemare et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bellemare et al\\.",
      "year" : 2013
    }, {
      "title" : "Torch7: A matlab-like environment for machine learning",
      "author" : [ "Collobert", "Ronan", "Kavukcuoglu", "Koray", "Farabet", "Clément" ],
      "venue" : "In BigLearn, Advances in the Neural Information Processing System Workshop,",
      "citeRegEx" : "Collobert et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2011
    }, {
      "title" : "Rich feature hierarchies for accurate object detection and semantic segmentation",
      "author" : [ "Girshick", "Ross", "Donahue", "Jeff", "Darrell", "Trevor", "Malik", "Jitendra" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Girshick et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Girshick et al\\.",
      "year" : 2014
    }, {
      "title" : "Generating sequences with recurrent neural networks",
      "author" : [ "Graves", "Alex" ],
      "venue" : "arXiv preprint arXiv:1308.0850,",
      "citeRegEx" : "Graves and Alex.,? \\Q2013\\E",
      "shortCiteRegEx" : "Graves and Alex.",
      "year" : 2013
    }, {
      "title" : "Neural turing machines",
      "author" : [ "Graves", "Alex", "Wayne", "Greg", "Danihelka", "Ivo" ],
      "venue" : "arXiv preprint arXiv:1410.5401,",
      "citeRegEx" : "Graves et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep learning for real-time atari game play using offline monte-carlo tree search planning",
      "author" : [ "Guo", "Xiaoxiao", "Singh", "Satinder", "Lee", "Honglak", "Lewis", "Richard L", "Wang", "Xiaoshi" ],
      "venue" : "In Advances in the Neural Information Processing System,",
      "citeRegEx" : "Guo et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep recurrent q-learning for partially observable mdps",
      "author" : [ "Hausknecht", "Matthew", "Stone", "Peter" ],
      "venue" : "In AAAI Fall Symposium on Sequential Decision Making for Intelligent Agents,",
      "citeRegEx" : "Hausknecht et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hausknecht et al\\.",
      "year" : 2015
    }, {
      "title" : "Long shortterm memory",
      "author" : [ "Hochreiter", "Sepp", "Schmidhuber", "Jürgen" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Hochreiter et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter et al\\.",
      "year" : 1997
    }, {
      "title" : "Bandit based monte-carlo planning",
      "author" : [ "Kocsis", "Levente", "Szepesvári", "Csaba" ],
      "venue" : "In European Conference on Machine Learning,",
      "citeRegEx" : "Kocsis et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Kocsis et al\\.",
      "year" : 2006
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E" ],
      "venue" : "In Advances in the Neural Information Processing System,",
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Deep auto-encoder neural networks in reinforcement learning",
      "author" : [ "Lange", "Sascha", "Riedmiller", "Martin" ],
      "venue" : "In International Joint Conference on Neural Networks,",
      "citeRegEx" : "Lange et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Lange et al\\.",
      "year" : 2010
    }, {
      "title" : "Guided policy search",
      "author" : [ "Levine", "Sergey", "Koltun", "Vladlen" ],
      "venue" : "In Proceedings of the International Conference on Machine Learning,",
      "citeRegEx" : "Levine et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Levine et al\\.",
      "year" : 2013
    }, {
      "title" : "End-to-end training of deep visuomotor policies",
      "author" : [ "Levine", "Sergey", "Finn", "Chelsea", "Darrell", "Trevor", "Abbeel", "Pieter" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Levine et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Levine et al\\.",
      "year" : 2016
    }, {
      "title" : "Continuous control with deep reinforcement learning",
      "author" : [ "Lillicrap", "Timothy P", "Hunt", "Jonathan J", "Pritzel", "Alexander", "Heess", "Nicolas", "Erez", "Tom", "Tassa", "Yuval", "Silver", "David", "Wierstra", "Daan" ],
      "venue" : "In International Conference on Learning Representations,",
      "citeRegEx" : "Lillicrap et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lillicrap et al\\.",
      "year" : 2016
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "Dharshan", "Wierstra", "Daan", "Legg", "Shane", "Hassabis", "Demis" ],
      "venue" : "Nature, 518(7540):529–533,",
      "citeRegEx" : "Dharshan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Dharshan et al\\.",
      "year" : 2015
    }, {
      "title" : "Rectified linear units improve restricted boltzmann machines",
      "author" : [ "Nair", "Vinod", "Hinton", "Geoffrey E" ],
      "venue" : "In Proceedings of the International Conference on Machine Learning,",
      "citeRegEx" : "Nair et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Nair et al\\.",
      "year" : 2010
    }, {
      "title" : "Language understanding for text-based games using deep reinforcement learning",
      "author" : [ "Narasimhan", "Karthik", "Kulkarni", "Tejas", "Barzilay", "Regina" ],
      "venue" : "In Conference on Empirical Methods on Natural Language Processing,",
      "citeRegEx" : "Narasimhan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Narasimhan et al\\.",
      "year" : 2015
    }, {
      "title" : "Action-conditional video prediction using deep networks in atari games",
      "author" : [ "Oh", "Junhyuk", "Guo", "Xiaoxiao", "Lee", "Honglak", "Lewis", "Richard L", "Singh", "Satinder" ],
      "venue" : "In Advances in the Neural Information Processing System,",
      "citeRegEx" : "Oh et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Oh et al\\.",
      "year" : 2015
    }, {
      "title" : "Mazes, maps, and memory",
      "author" : [ "Olton", "David S" ],
      "venue" : "American Psychologist,",
      "citeRegEx" : "Olton and S.,? \\Q1979\\E",
      "shortCiteRegEx" : "Olton and S.",
      "year" : 1979
    }, {
      "title" : "Universal value function approximators",
      "author" : [ "Schaul", "Tom", "Horgan", "Daniel", "Gregor", "Karol", "Silver", "David" ],
      "venue" : "In Proceedings of the International Conference on Machine Learning,",
      "citeRegEx" : "Schaul et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Schaul et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep learning in neural networks: An overview",
      "author" : [ "Schmidhuber", "Jürgen" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "Schmidhuber and Jürgen.,? \\Q2015\\E",
      "shortCiteRegEx" : "Schmidhuber and Jürgen.",
      "year" : 2015
    }, {
      "title" : "Trust region policy optimization",
      "author" : [ "Schulman", "John", "Levine", "Sergey", "Moritz", "Philipp", "Jordan", "Michael I", "Abbeel", "Pieter" ],
      "venue" : "In Proceedings of the International Conference on Machine Learning,",
      "citeRegEx" : "Schulman et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Schulman et al\\.",
      "year" : 2015
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "Simonyan", "Karen", "Zisserman", "Andrew" ],
      "venue" : "In International Conference on Learning Representations,",
      "citeRegEx" : "Simonyan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Simonyan et al\\.",
      "year" : 2015
    }, {
      "title" : "Incentivizing exploration in reinforcement learning with deep predictive models",
      "author" : [ "Stadie", "Bradly C", "Levine", "Sergey", "Abbeel", "Pieter" ],
      "venue" : "arXiv preprint arXiv:1507.00814,",
      "citeRegEx" : "Stadie et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Stadie et al\\.",
      "year" : 2015
    }, {
      "title" : "Mazebase: A sandbox for learning from games",
      "author" : [ "Sukhbaatar", "Sainbayar", "Szlam", "Arthur", "Synnaeve", "Gabriel", "Chintala", "Soumith", "Fergus", "Rob" ],
      "venue" : "arXiv preprint arXiv:1511.07401,",
      "citeRegEx" : "Sukhbaatar et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sukhbaatar et al\\.",
      "year" : 2015
    }, {
      "title" : "End-to-end memory networks",
      "author" : [ "Sukhbaatar", "Sainbayar", "Weston", "Jason", "Fergus", "Rob" ],
      "venue" : "In Advances in the Neural Information Processing System,",
      "citeRegEx" : "Sukhbaatar et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sukhbaatar et al\\.",
      "year" : 2015
    }, {
      "title" : "Temporal difference learning and tdgammon",
      "author" : [ "Tesauro", "Gerald" ],
      "venue" : "Communications of the ACM,",
      "citeRegEx" : "Tesauro and Gerald.,? \\Q1995\\E",
      "shortCiteRegEx" : "Tesauro and Gerald.",
      "year" : 1995
    }, {
      "title" : "Recurrent policy gradients",
      "author" : [ "Wierstra", "Daan", "Förster", "Alexander", "Peters", "Jan", "Schmidhuber", "Jürgen" ],
      "venue" : "Logic Journal of IGPL,",
      "citeRegEx" : "Wierstra et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Wierstra et al\\.",
      "year" : 2010
    }, {
      "title" : "Reinforcement learning neural turing machines",
      "author" : [ "Zaremba", "Wojciech", "Sutskever", "Ilya" ],
      "venue" : "arXiv preprint arXiv:1505.00521,",
      "citeRegEx" : "Zaremba et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zaremba et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning simple algorithms from examples",
      "author" : [ "Zaremba", "Wojciech", "Mikolov", "Tomas", "Joulin", "Armand", "Fergus", "Rob" ],
      "venue" : "In Proceedings of the International Conference on Machine Learning,",
      "citeRegEx" : "Zaremba et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zaremba et al\\.",
      "year" : 2016
    }, {
      "title" : "Policy learning with continuous memory states for partially observed robotic control",
      "author" : [ "Zhang", "Marvin", "Levine", "Sergey", "McCarthy", "Zoe", "Finn", "Chelsea", "Abbeel", "Pieter" ],
      "venue" : "In International Conference on Robotics and Automation,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2016
    }, {
      "title" : "Map Generation for Pattern Matching There are a total of 512 possible visual patterns in a 3 × 3 room with blocks of two colors. We randomly picked 250 patterns and generated two maps for each pattern: one that",
      "author" : [ "Lillicrap et al", "2016). A" ],
      "venue" : null,
      "citeRegEx" : "al. and A.2.,? \\Q2016\\E",
      "shortCiteRegEx" : "al. and A.2.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : ", 2015; Schmidhuber, 2015) have made advances in many lowlevel perceptual supervised learning problems (Krizhevsky et al., 2012; Girshick et al., 2014; Simonyan & Zisserman, 2015).",
      "startOffset" : 103,
      "endOffset" : 179
    }, {
      "referenceID" : 4,
      "context" : ", 2015; Schmidhuber, 2015) have made advances in many lowlevel perceptual supervised learning problems (Krizhevsky et al., 2012; Girshick et al., 2014; Simonyan & Zisserman, 2015).",
      "startOffset" : 103,
      "endOffset" : 179
    }, {
      "referenceID" : 2,
      "context" : ", 2015) architecture has been shown to successfully learn to play many Atari 2600 games in the Arcade Learning Environment (ALE) benchmark (Bellemare et al., 2013) by learning visual features useful for control directly from raw pixels using Q-Learning (Watkins & Dayan, 1992).",
      "startOffset" : 139,
      "endOffset" : 163
    }, {
      "referenceID" : 6,
      "context" : "Graves et al. (2014) introduced a Neural Turing Machine (NTM), a differentiable external memory architecture, and showed that it can learn algorithms such as copy and reverse.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 6,
      "context" : "Graves et al. (2014) introduced a Neural Turing Machine (NTM), a differentiable external memory architecture, and showed that it can learn algorithms such as copy and reverse. Zaremba & Sutskever (2015) proposed RL-NTM that has a nondifferentiable memory to scale up the addressing mechanism of NTM and applied policy gradient to train the architecture.",
      "startOffset" : 0,
      "endOffset" : 203
    }, {
      "referenceID" : 6,
      "context" : "Graves et al. (2014) introduced a Neural Turing Machine (NTM), a differentiable external memory architecture, and showed that it can learn algorithms such as copy and reverse. Zaremba & Sutskever (2015) proposed RL-NTM that has a nondifferentiable memory to scale up the addressing mechanism of NTM and applied policy gradient to train the architecture. Joulin & Mikolov (2015) implemented a stack using neural networks and demonstrated that it can infer several algorithmic patterns.",
      "startOffset" : 0,
      "endOffset" : 378
    }, {
      "referenceID" : 6,
      "context" : "Graves et al. (2014) introduced a Neural Turing Machine (NTM), a differentiable external memory architecture, and showed that it can learn algorithms such as copy and reverse. Zaremba & Sutskever (2015) proposed RL-NTM that has a nondifferentiable memory to scale up the addressing mechanism of NTM and applied policy gradient to train the architecture. Joulin & Mikolov (2015) implemented a stack using neural networks and demonstrated that it can infer several algorithmic patterns. Sukhbaatar et al. (2015b) proposed a Memory Network (MemNN) for Q&A and language modeling tasks, which stores all inputs and retrieves relevant memory blocks depending on the question.",
      "startOffset" : 0,
      "endOffset" : 511
    }, {
      "referenceID" : 31,
      "context" : "In addition, there are deep RL approaches to tasks other than Atari such as learning algorithms (Zaremba et al., 2016) and text-based games (Sukhbaatar et al.",
      "startOffset" : 96,
      "endOffset" : 118
    }, {
      "referenceID" : 18,
      "context" : ", 2016) and text-based games (Sukhbaatar et al., 2015a; Narasimhan et al., 2015).",
      "startOffset" : 29,
      "endOffset" : 80
    }, {
      "referenceID" : 19,
      "context" : "There have also been a few attempts to learn state-transition models using deep learning to improve exploration in RL (Oh et al., 2015; Stadie et al., 2015).",
      "startOffset" : 118,
      "endOffset" : 156
    }, {
      "referenceID" : 25,
      "context" : "There have also been a few attempts to learn state-transition models using deep learning to improve exploration in RL (Oh et al., 2015; Stadie et al., 2015).",
      "startOffset" : 118,
      "endOffset" : 156
    }, {
      "referenceID" : 6,
      "context" : "Guo et al. (2014) used slow Monte-Carlo Tree Search (MCTS) (Kocsis & Szepesvári, 2006) to generate a relatively small amount of data to train fast-playing convolutional networks in Atari games.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 6,
      "context" : "Guo et al. (2014) used slow Monte-Carlo Tree Search (MCTS) (Kocsis & Szepesvári, 2006) to generate a relatively small amount of data to train fast-playing convolutional networks in Atari games. Schulman et al. (2015), Levine et al.",
      "startOffset" : 0,
      "endOffset" : 217
    }, {
      "referenceID" : 6,
      "context" : "Guo et al. (2014) used slow Monte-Carlo Tree Search (MCTS) (Kocsis & Szepesvári, 2006) to generate a relatively small amount of data to train fast-playing convolutional networks in Atari games. Schulman et al. (2015), Levine et al. (2016), and Lillicrap et al.",
      "startOffset" : 0,
      "endOffset" : 239
    }, {
      "referenceID" : 6,
      "context" : "Guo et al. (2014) used slow Monte-Carlo Tree Search (MCTS) (Kocsis & Szepesvári, 2006) to generate a relatively small amount of data to train fast-playing convolutional networks in Atari games. Schulman et al. (2015), Levine et al. (2016), and Lillicrap et al. (2016) have successfully trained deep neural networks to directly learn policies and applied their architectures to robotics problems.",
      "startOffset" : 0,
      "endOffset" : 268
    }, {
      "referenceID" : 6,
      "context" : "Guo et al. (2014) used slow Monte-Carlo Tree Search (MCTS) (Kocsis & Szepesvári, 2006) to generate a relatively small amount of data to train fast-playing convolutional networks in Atari games. Schulman et al. (2015), Levine et al. (2016), and Lillicrap et al. (2016) have successfully trained deep neural networks to directly learn policies and applied their architectures to robotics problems. In addition, there are deep RL approaches to tasks other than Atari such as learning algorithms (Zaremba et al., 2016) and text-based games (Sukhbaatar et al., 2015a; Narasimhan et al., 2015). There have also been a few attempts to learn state-transition models using deep learning to improve exploration in RL (Oh et al., 2015; Stadie et al., 2015). Most recently, Mnih et al. (2016) proposed asynchronous DQN and showed that it can learn to explore a 3D environment similar to Minecraft.",
      "startOffset" : 0,
      "endOffset" : 781
    }, {
      "referenceID" : 1,
      "context" : "To deal with such a challenge, Bakker et al. (2003) used a Long Short-Term Memory (LSTM) network (Hochreiter & Schmidhuber, 1997) in an offline policy learning framework to show that a robot controlled by an LSTM network can solve T-Mazes where the robot should go to the correct destination depending on the traffic signal at the beginning of the maze.",
      "startOffset" : 31,
      "endOffset" : 52
    }, {
      "referenceID" : 1,
      "context" : "To deal with such a challenge, Bakker et al. (2003) used a Long Short-Term Memory (LSTM) network (Hochreiter & Schmidhuber, 1997) in an offline policy learning framework to show that a robot controlled by an LSTM network can solve T-Mazes where the robot should go to the correct destination depending on the traffic signal at the beginning of the maze. Wierstra et al. (2010) proposed a Recurrent Policy Gradient method and showed that an LSTM network trained using this method outperforms other methods in several tasks including TMazes.",
      "startOffset" : 31,
      "endOffset" : 377
    }, {
      "referenceID" : 1,
      "context" : "To deal with such a challenge, Bakker et al. (2003) used a Long Short-Term Memory (LSTM) network (Hochreiter & Schmidhuber, 1997) in an offline policy learning framework to show that a robot controlled by an LSTM network can solve T-Mazes where the robot should go to the correct destination depending on the traffic signal at the beginning of the maze. Wierstra et al. (2010) proposed a Recurrent Policy Gradient method and showed that an LSTM network trained using this method outperforms other methods in several tasks including TMazes. More recently, Zhang et al. (2016) introduced continuous memory states to augment the state and action space and showed it can memorize salient information through Guided Policy Search (Levine & Koltun, 2013).",
      "startOffset" : 31,
      "endOffset" : 575
    }, {
      "referenceID" : 1,
      "context" : "To deal with such a challenge, Bakker et al. (2003) used a Long Short-Term Memory (LSTM) network (Hochreiter & Schmidhuber, 1997) in an offline policy learning framework to show that a robot controlled by an LSTM network can solve T-Mazes where the robot should go to the correct destination depending on the traffic signal at the beginning of the maze. Wierstra et al. (2010) proposed a Recurrent Policy Gradient method and showed that an LSTM network trained using this method outperforms other methods in several tasks including TMazes. More recently, Zhang et al. (2016) introduced continuous memory states to augment the state and action space and showed it can memorize salient information through Guided Policy Search (Levine & Koltun, 2013). Hausknecht & Stone (2015) proposed Deep Recurrent QNetwork (DRQN) which consists of an LSTM on top of a CNN based on the DQN framework and demonstrated improved handling of partial observability in Atari games.",
      "startOffset" : 31,
      "endOffset" : 776
    }, {
      "referenceID" : 1,
      "context" : "The tasks we introduce are inspired by the T-maze experiments (Bakker et al., 2003) as well as MazeBase (Sukhbaatar et al.",
      "startOffset" : 62,
      "endOffset" : 83
    }, {
      "referenceID" : 0,
      "context" : "The reading mechanism of the memory is based on soft attention (Graves, 2013; Bahdanau et al., 2015) as illustrated in Figure 2b.",
      "startOffset" : 63,
      "endOffset" : 100
    }, {
      "referenceID" : 26,
      "context" : "In the results we report here, we used an MLP with one hidden layer as follows: gt = f ( Wht + ot ) ,qt = W gt where f is a rectified linear function (Nair & Hinton, 2010) applied only to half of the hidden units for easy optimization by following Sukhbaatar et al. (2015b). (a) I-Maze (b) Pattern Matching",
      "startOffset" : 248,
      "endOffset" : 274
    }, {
      "referenceID" : 3,
      "context" : "Our implementation is based on Torch7 (Collobert et al., 2011), a public DQN implementation (Mnih et al.",
      "startOffset" : 38,
      "endOffset" : 62
    }, {
      "referenceID" : 21,
      "context" : "More specifically, testing on {6, 8} sizes of maps and the rest of the sizes of maps can evaluate interpolation and extrapolation performance, respectively (Schaul et al., 2015).",
      "startOffset" : 156,
      "endOffset" : 177
    } ],
    "year" : 2016,
    "abstractText" : "In this paper, we introduce a new set of reinforcement learning (RL) tasks in Minecraft (a flexible 3D world). We then use these tasks to systematically compare and contrast existing deep reinforcement learning (DRL) architectures with our new memory-based DRL architectures. These tasks are designed to emphasize, in a controllable manner, issues that pose challenges for RL methods including partial observability (due to first-person visual observations), delayed rewards, high-dimensional visual observations, and the need to use active perception in a correct manner so as to perform well in the tasks. While these tasks are conceptually simple to describe, by virtue of having all of these challenges simultaneously they are difficult for current DRL architectures. Additionally, we evaluate the generalization performance of the architectures on environments not used during training. The experimental results show that our new architectures generalize to unseen environments better than existing DRL architectures.",
    "creator" : "LaTeX with hyperref package"
  }
}