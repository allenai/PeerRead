{
  "name" : "1706.00741.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Prosodic Event Recognition using Convolutional Neural Networks with Context Information",
    "authors" : [ "Sabrina Stehwien", "Ngoc Thang Vu" ],
    "emails" : [ "sabrina.stehwien@ims.uni-stuttgart.de", "thang.vu@ims.uni-stuttgart.de" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Prosodic Event Recognition (PER) refers to the task of automatically localizing pitch accents and phrase boundary tones in speech data and often deals with labelling specific segments, such as words or syllables. PER is important for for the analysis of human discourse and speech due to the interaction between prosody and meaning in languages such as English. For example, knowing what word in an utterance is pitch accented provides important insight into discourse structure such as focus, givenness and contrast [1, 2]. Phrasing information and boundary tones for example relate to the syntactic structure [3].\nA substantial amount of research has dealt with the impact of prosodic information for a wide range of language understanding tasks such as automatic speech recognition [4, 5, 6, 7] and understanding [8, 9, 10]. Furthermore, since manual prosodic annotation is expensive, it is desirable to have reliable, automatic annotation methods to aid linguistic and speech processing research on a large scale.\nMost PER methods consist of two stages: feature extraction and preprocessing, and statistical modelling or classification. PER distinguishes two subtasks: detection typically refers to the binary classification task (presence or absence of a prosodic event), while prosodic event classification encompasses the full multi-class labelling of prosodic event types [11] e.g. as described in the ToBI standard [12]. Typically the recognition of pitch accents is modelled separately from phrase boundaries, although the acoustic features are quite similar [13, 14, 15].\nMany approaches focus on finding appropriate acoustic representations of prosody [13, 11]. These features generally describe the fundamental frequency (f0) and energy and can be either frame-based [16] or grouped across segments [17]. Often acoustic-prosodic features also include the duration of certain segments [13, 18, 19]. Most successful methods that rely on acoustic features also benefit from the addition of lexicosyntactic information [20, 13, 19]. Since prosodic events usually span several segments, many cited approaches add features\nrepresenting the surrounding segment, while others explicitly focus on context modelling [21, 14, 22].\nRecent work has shown that convolutional neural networks (CNN) are suitable for the detection of prominence: Shahin et al. [23] combine the output of a CNN that learns high-level features representations from 27 frame-based Mel-spectral features with global (or aggregated) f0, energy and duration features across syllables for lexical stress detection. Wang et al. [24] train a CNN on continuous wavelet transformations of the fundamental frequency for the detection of pitch accents and phrase boundaries in a speaker-dependent task.\nAs previously pointed out in [19, 17], the large number of different approaches and task descriptions renders the comparison of PER performance methods quite difficult. Thus, our results are compared only to approaches that use the Boston University Radio News Corpus (BURNC) [25] and purely acoustic features. Some selected work with similar focus is listed in the following. Good results for pitch accent detection were reported by Sun [19], namely 84.7% on one speaker (f2b) of BURNC using acoustic features only. Wang et al. [24] use CNNs to detect pitch accents and phrase boundaries on the f2b speaker, obtaining 86.9% and 89.5% accuracy respectively. Ren et al. [26] obtain 83.6% accuracy in speaker-independent pitch accent detection on two female speakers in BURNC. The more difficult task is prosodic event type classification. Rosenberg [27] reports almost 64% accuracy for pitch accents and 72.9% for phrase boundaries in experiments that aimed at classifying 5 ToBI types each in 10-fold cross-validation experiments. Chen et al. [15] apply their neural-based method to speaker-independent setups using 4 speakers of BURNC and distinguishing 4 event types. They report 68.2% recognition accuracy using only acoustic-prosodic features. An early example of a neural network approach was proposed in [16], and relied only on frame-based acoustic features such as f0 and energy.\nIn this work, we use a CNN that learns high-level feature representations on its own from low-level acoustic descriptors. This way we can rely only on frame-based features that are readily obtained from the speech signal. The only segmental information used in this work is the time-alignment at the word level. We address the notion of explicit context modelling with CNNs in a simple and efficient way. We apply this method to both the detection and classification of pitch accents and intonational phrase boundaries. An additional challenge to PER is the generalization across different speakers due to the large variation in prosodic parameters. For this reason, we not only test the performance of the proposed method on one speaker for comparability, but also as leave-one-speaker-out cross-validation results. We report recognition accuracies comparable to similar previous work and show that our model generalizes well across speakers. ar X\niv :1\n70 6.\n00 74\n1v 1\n[ cs\n.C L\n] 2\nJ un\n2 01\n7"
    }, {
      "heading" : "2. Model",
      "text" : "We apply a CNN model as illustrated in Figure 1 for PER. The task is set up as a supervised learning task in which each word is labelled as carrying a prosodic event or not. The input to the CNN is a feature representation of the audio signal of the current word and (optionally) its context. The signal is divided into s overlapping frames and represented by a d-dimensional feature vector for each frame. Thus, for each utterance, a matrix W ∈ Rd×s is formed as input. The number of frames s depends on the duration (signal length) of the word as well as the context window size and the frame shift. For the convolution operation we use 2D kernels K (with width |K|) spanning all d features. The following equation expresses the convolution:\n(W ∗K)(x, y) = d∑\ni=1 |K|∑ j=1 W (i, j) ·K(x− i, y − j) (1)\nWe apply two convolution layers in order to expand the input information. After the convolution, max pooling is used to find the most salient features. All resulting feature maps are concatenated to one feature vector which is fed into the softmax layer. The softmax layer has either 2 units for binary classification or c classes for multi-class classification. For regularization, we also apply dropout [28] to this last layer."
    }, {
      "heading" : "2.1. Acoustic Features",
      "text" : "The features used in this work were chosen to be simple and fast to obtain. We extract acoustic features from the speech signal using the OpenSMILE toolkit [29]. In this work, two different feature sets are used: a prosody feature set consisting of 5 features from the OpenSMILE catalogue (smoothed f0, RMS\nenergy, PCM loudness, voicing probability and Harmonics-toNoise Ratio), and a Mel feature set consisting of 27 features extracted from the Mel-frequency spectrum (similar to [23]). The features are computed for each 20ms frame with a 10ms shift. These two features sets are used both separately and jointly (concatenated) in the reported experiments. The time intervals that indicate the word boundaries provided in the corpus are used to create the input feature matrices by grouping all frames for each word into one input matrix. Afterwards, zero padding is added to ensure that all matrices have the same size."
    }, {
      "heading" : "2.2. Position Indicator Feature",
      "text" : "The following describes the extension of the acoustic features by a position indicator for PER. This type of feature has been proposed for use in neural network models for relation classification [30, 31]. Previous work has demonstrated the benefits of adding context information to PER [14, 21]. The most straighforward approach is to add features that represent the right and left neighbouring segments to form a type of acoustic context window [11, 13, 24]. The caveat of using context windows as input to our CNN model is, however, that it also adds a substantial amount of noise. The learning method of CNNs is to look for patterns in the whole input and learn abstract global representations of these. The neighbouring words may have prosodic events or other prosodic prominence characteristics that “distract” from the current word. This effect may be amplified by the fact that the words have variable lengths. For this reason we add position features (or indicators) that are appended as an extra feature to the input matrices (see Figure 1). These features indicate the parts of the matrix that represent the current word. The rest of the matrix consists of zeros in this dimension. In the first convolution layer we ensure that the kernels always span the position-indicating feature dimension. Thus, the model is constantly informed whether the |K| current frames belong to the current word or the neighbouring words."
    }, {
      "heading" : "3. Experimental Setup",
      "text" : ""
    }, {
      "heading" : "3.1. Data",
      "text" : "The dataset used in this work is a subset of BURNC that has been manually labelled with prosodic events according to the ToBI labelling standard [12]. The speech data was recorded from 3 female and 2 male speakers, adding up to around 2 hours and 45 minutes of speech. Table 1 shows the number of words for each speaker in the datasets used for pitch accent and phrase boundary recognition in this work1.\nFor the speaker-dependent experiments, the largest speaker subset f2b is used in line with previous methods [19, 24]. We test our models using 10-fold cross-validation and validated on 1000 words from the respective training set. In the speaker-independent case, the models were trained and tested\n1Since the two tasks are trained and tested separately, we judge the mismatch in the two datasets as inconsequential to our experiments.\nusing leave-one-speaker-out cross-validation and validated on 500 words from a speaker of the same gender for early stopping2. All experiments are repeated 3 times and the results are averaged.\nThe Boston corpus contains different ToBI types of pitch accents and phrase boundaries. For the binary classification task (detection) all labels are grouped together as one class. For the classification task, we distinguish 5 different ToBI types of pitch accents and phrase boundaries (as in [27]), where the downstepped accents are collapsed into the non-downstepped ones: The pitch accent classes are (1) H* and !H*, (2) L*, (3) L+H* and L+!H*, (4) L*+H and L*+!H and (5) H+!H*. The boundary tones considered in this work mark the boundaries of intonational phrases: L-L%, L-H%, H-L%, !H-L%, !H-L% and H-H%. Uncertain events, where the annotator was unsure if there is an accent or boundary tone, are ignored for both detection and classification. Uncertain types, where the annotator was unsure of the event type, are ignored for classification."
    }, {
      "heading" : "3.2. Hyperparameters",
      "text" : "The classification model is a 2-layer CNN. The first layer consists of 100 2-dimensional kernels of the shape 6×d and a stride of 4× 1, with d as the number of features. The kernels encompass the whole feature set to ensure that all features are learnt simultaneously. The second layer consists of 100 kernels of the shape 4×1 and a stride of 2×1. The max pooling size is set so that the output of each max pooling on each of the 100 feature maps has the shape x. Thus, this hyperparameter varies depending on the dimensions of the input matrix, but is kept constant due to the zero padding in each individual experiment. Dropout with p = 0.2 is applied before the softmax layer. The models are trained for 50 epochs with an adaptive learning rate (Adam [32]) and L2 regularization."
    }, {
      "heading" : "4. Results",
      "text" : "We report results for each experiment with three context variations: no context (1 word), right and left context words (3 words) and right and left context words with position features (3 words + PF)."
    }, {
      "heading" : "4.1. Pitch Accent Recognition",
      "text" : "Table 2 shows the results for pitch accent recognition on the single-speaker dataset and Table 3 shows the results obtained in speaker-independent experiments. The model yields up to\n2This way we avoid a too large mismatch between the validation and test data.\n84% detection performance when considering only the current word with no additional context in the speaker-dependent setup and almost 82% in the speaker-independent experiments. The classification task is more difficult, especially in the speakerindependent case (68%). The results show a large drop in performance, down to the majority class baseline level, when extending the input to include the right and left context words. After adding the position indicating features, the accuracies of all tasks increases and exceeds those obtained from the singleword input in the speaker-independent case. We obtain up to 86.3% accuracy in pitch accent detection on f2b, which is comparable to the best previously reported results on purely acoustic input. This indicates that not only is the position indicator crucial when adding context to our specific model, but that it constitutes a strong modelling technique. Speaker-independent pitch accent classification remains the most difficult task, although the accuracy obtained in this work (69%) matches up to that of comparable methods.\nWe observe that in both the speaker-dependent and speakerindependent settings, the prosody feature set performs best, while the Mel and combined prosody + Mel feature yield similar results.\nWe also report the accuracies per speaker for the speakerindependent experiments using the prosody feature set and the position indicator features in Table 4. The results show that even though the speaker f2b constitutes the largest speaker subset leaving the least amount of data for training, the model does not perform much worse than on data from other speakers. Overall, there does not appear to be a distinctively “easy” or “difficult” speaker."
    }, {
      "heading" : "4.2. Phrase Boundaries",
      "text" : "The results for phrase boundary recognition appear to follow a similar pattern as for pitch accent recognition. In this task, we also observe a drop in performance when extending from the 1- word to the 3-word input windows, although this effect is not as pronounced in the case of phrase boundaries. Adding position indicator features improves the results in all cases.\nFor the speaker-dependent task, the combined prosody and\nMel feature set yields the best performance, while the small prosody feature set appears to be the best choice in the speakerindependent task. These differences, however, are not as pronounced as in the case of pitch accents. In the f2b experiments we obtain 90.5% and 88.8% accuracy for detection and classification, respectively and in the speaker-independent setup we obtain almost 90% accuracy for detection and 87.3% for classification.\nIn contrast to the pitch accent recognition results, we observe that the accuracies are lowest on speaker f1a, and highest on speaker m1b in both tasks (see Table 7)."
    }, {
      "heading" : "4.3. Discussion",
      "text" : "An interesting result in the above work is the impact of adding context frames without position features on the two presented tasks. We observe that adding “uninformed” context information is more detrimental to the recognition of pitch accents than to phrase boundaries. While we have not further examined this effect in the present study, it may be explained as follows. Pitch accents are rather local phenomena occurring on stressed syllables and are more frequent in the data. Intonational phrase boundary tones as described by the ToBI standard3 not only span longer stretches of speech (since these consist of an intermediate phrase accent and an intonational phrase boundary tone) but are also more sparse since they only occur at the end of intonational phrases. This means that the model may be less sensitive to local events or changes in neighbouring segments and that it is less likely for phrase boundaries to occur in two succeeding words as in the case of pitch accents.\nThe effect of using the various feature sets in our experiments shows that the smallest feature set (prosody) works best\n3http://www.speech.cs.cmu.edu/tobi/ToBI.0.html\nin almost all cases, with speaker-dependent phrase boundary recognition as the only exception. These differences, however, are small. The features used in this work were chosen to be quite simple, leaving room for further investigation with respect to the acoustic features on the individual tasks.\nA widely-used measure to enable the generalization of prosodic models across speakers is speaker normalization in the form of z-scoring [11, 15, 33]. In our experiments we observe a large drop in performance after z-scoring the features, both for the speaker-dependent and the speaker-independent case. This effect holds across tasks (see Table 8) using the prosody feature set4. This may be due to the fact that the CNN looks for relative patterns in the data independent of their absolute position and values; and prosodic events are characterized by relative changes in speech. Normalizing the values may lead to a loss of fine differences in the data since the range of the values is decreased by z-scoring. The CNN performance in our experiments, however, appears to benefit from the original differences."
    }, {
      "heading" : "5. Conclusion",
      "text" : "This paper presents experimental results using CNNs for wordbased PER on low-level acoustic features, while emphasizing the effect of including context information. We show that the model performs well just by learning from simple framebased features, and that the performance can be increased by adding position indicating features to the input that represents the word and its context. Our model generalizes well from a speaker-dependent setup to a speaker-independent setting, yielding 86.3% and 83.6% accuracy, respectively, for pitch accent detection. Even in the more challenging task of classifying ToBI types, we obtain results across speakers that are comparable to previous related work, that is 69% accuracy for pitch accents and 87.3% for phrase boundaries. Futhermore, the presented method can be readily applied to other datasets. Although a more detailed analysis is necessary to evaluate the performance on individual event types, we conclude that this method is quite suitable to the task, especially given its efficiency.\n4We observe this on the Mel feature set as well."
    }, {
      "heading" : "6. References",
      "text" : "[1] J. Hirschberg and J. B. Pierrehumbert, “The intonational structur-\ning of discourse,” in 24th Annual Meeting of the Association for Computational Linguistics, Columbia University, New York, New York, USA, July 10-13, 1986., 1986, pp. 136–144.\n[2] E. Selkirk, “Sentence prosody: Intonation, stress and phrasing,” in The handbook of phonological theory, J. A. Goldsmith, Ed. Oxford: Blackwell, 1995, pp. 550–569.\n[3] H. Truckenbrodt, “On the relation between syntactic phrases and phonological phrases,” Linguistic Inquiry, vol. 30, no. 2, pp. 219– 255, 1999.\n[4] A. Waibel, Prosody and Speech Recognition. Morgan Kaufmann, 1988.\n[5] K. Vicsi and G. Szaszák, “Using prosody to improve automatic speech recognition,” Speech Communication, vol. 52, no. 5, pp. 413–426, 2010.\n[6] S. Ananthakrishnan and S. Narayanan, “Improved speech recognition using acoustic and lexical correlates of pitch accent in a n-best rescoring framework,” in Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, Honolulu, Hawaii, 2007, pp. 873–876.\n[7] K. Chen, M. Hasegawa-Johnson, A. Cohen, S. Borys, S.-S. Kim, J. Cole, and J.-Y. Choi, “Prosody dependent speech recognition on radio news corpus of American English,” IEEE Transactions on Audio, Speech and Language Processing, vol. 14, no. 1, pp. 232–245, 2006.\n[8] R. Kompe, Prosody in Speech Understanding Systems, J. Siekmann and J. G. Carbonell, Eds. Secaucus, NJ, USA: SpringerVerlag New York, Inc., 1997.\n[9] E. Shriberg and A. Stolcke, “Prosody modeling for automatic speech recognition and understanding,” in Mathematical Foundations of Speech and Language Processing. Springer, 2004, pp. 105–114.\n[10] A. Batliner, B. Möbius, G. Möhler, A. Schweitzer, and E. Nöth, “Prosodic models, automatic speech understanding, and speech synthesis: towards the common ground,” in Proceedings of the European Conference on Speech Communication and Technology (Aalborg, Denmark), vol. 4. ISCA, 2001, pp. 2285–2288.\n[11] A. Rosenberg and J. Hirschberg, “Detecting pitch accent using pitch-corrected energy-based predictors,” in Proceedings of Interspeech, 2007, pp. 2777–2780.\n[12] K. Silverman, M. Beckman, J. Pitrelli, M. Ostendorf, and C. Wightman, “ToBI: A standard for labelling English prosody,” in Proceedings of ICSLP, 1992, pp. 867–870.\n[13] A. Schweitzer and B. Möbius, “Experiments on automatic prosodic labeling,” in Proceedings of Interspeech, 2009, pp. 2515–2518.\n[14] A. Rosenberg, R. Fernandez, and B. Ramabhadran, “Modeling phrasing and prominence using deep recurrent learning,” in Proceedings of Interspeech, 2015, pp. 3066–3070.\n[15] K. Chen, M. Hasegawa-Johnson, and A. Cohen, “An automatic prosody labeling system using ann-based syntactic-prosodic model and gmm-based acoustic-prosodic model,” in Proceedings of ICASSP, 2004, pp. 509–512.\n[16] P. Taylor, “Using neural networks to locate pitch accents,” in Proceedings of the 4th European Conference on Speech Communication and Technology, 1995.\n[17] A. Rosenberg and J. Hirschberg, “Detecting pitch accents at the word, syllable and vowel level,” in HLT-NAACL, 2009.\n[18] F. Tamburini, “Prosodic prominence detection in speech,” in ISSPA2003, 2003, pp. 385–338.\n[19] X. Sun, “Pitch accent prediction using ensemble machine learning,” in Proceedings of ICSLP-2002, 2002, pp. 16–20.\n[20] S. Ananthakrishnan and S. S. Narayanan, “Automatic prosodic event detection using acoustic, lexical and syntactic evidence,” in IEEE Transactions on Audio, Speech and Language Processing, vol. 16, no. 1, 2008, pp. 216–228.\n[21] G.-A. Levow, “Context in multi-lingual tone and pitch accent recognition,” in Proceedings of Interspeech, 2005, pp. 1809– 1812.\n[22] J. Zhao, W.-Q. Zhang, H. Yuan, M. T. Johnson, J. Liu, and S. Xia, “Exploiting contextual information for prosodic event detection using auto-context,” EURASIP J. Audio, Speech and Music Processing, vol. 2013, p. 30, 2013.\n[23] M. Shahin, J. Epps, and B. Ahmed, “Automatic classification of lexical stress in English and Arabic languages using deep learning,” in Proceedings of Interspeech, 2016, pp. 175–179.\n[24] X. Wang, S. Takaki, and J. Yamagishi, “Enhance the word vector with prosodic information for the recurrent neural network based tts system,” in Proceedings of Interspeech, 2016, pp. 2856–2860.\n[25] M. Ostendorf, P. Price, and S. Shattuck-Hufnagel, “The Boston University Radio News Corpus,” Boston University, Technical Report ECS-95-001, 1995.\n[26] K. Ren, S.-S. Kim, M. Hasegawa-Johnson, and J. Cole, “Speakerindependent automatic detection of pitch accent,” in ISCA International Conference on Speech Prosody, 2004, pp. 521–524.\n[27] A. Rosenberg, “Classification of prosodic events using quantized contour modeling,” in Proceedings of HLT-NAACL, 2010, pp. 721–724.\n[28] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, “Dropout: a simple way to prevent neural networks from overfitting,” Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929–1958, 2014.\n[29] F. Eyben, F. Weninger, F. Groß, and B. Schuller, “Recent developments in opensmile, the Munich open-source multimedia feature extractor,” in Proceedings of the 21st ACM international conference on Multimedia, 2013, pp. 835–838.\n[30] N. T. Vu, H. Adel, P. Gupta, and H. Schütze, “Combining recurrent and convolutional neural networks for relation classification,” in Proceedings of HLT-NAACL 2016, 2016, pp. 534–539.\n[31] D. Zhang and D. Wang, “Relation classification via recurrent neural network,” arXiv preprint arXiv:508.01006v1, 2015.\n[32] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” arXiv preprint arXiv:1412.6980, 2017.\n[33] K. Schweitzer, M. Walsh, B. Möbius, and H. Schütze, “Frequency of occurrence effects on pitch accent realisation,” in Proceedings of Interspeech, 2010, pp. 138–141."
    } ],
    "references" : [ {
      "title" : "The intonational structuring of discourse",
      "author" : [ "J. Hirschberg", "J.B. Pierrehumbert" ],
      "venue" : "24th Annual Meeting of the Association for Computational Linguistics, Columbia University, New York, New York, USA, July 10-13, 1986., 1986, pp. 136–144.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1986
    }, {
      "title" : "Sentence prosody: Intonation, stress and phrasing",
      "author" : [ "E. Selkirk" ],
      "venue" : "The handbook of phonological theory, J. A. Goldsmith, Ed. Oxford: Blackwell, 1995, pp. 550–569.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "On the relation between syntactic phrases and phonological phrases",
      "author" : [ "H. Truckenbrodt" ],
      "venue" : "Linguistic Inquiry, vol. 30, no. 2, pp. 219– 255, 1999.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Prosody and Speech Recognition",
      "author" : [ "A. Waibel" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1988
    }, {
      "title" : "Using prosody to improve automatic speech recognition",
      "author" : [ "K. Vicsi", "G. Szaszák" ],
      "venue" : "Speech Communication, vol. 52, no. 5, pp. 413–426, 2010.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Improved speech recognition using acoustic and lexical correlates of pitch accent in a n-best rescoring framework",
      "author" : [ "S. Ananthakrishnan", "S. Narayanan" ],
      "venue" : "Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, Honolulu, Hawaii, 2007, pp. 873–876.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Prosody dependent speech recognition on radio news corpus of American English",
      "author" : [ "K. Chen", "M. Hasegawa-Johnson", "A. Cohen", "S. Borys", "S.-S. Kim", "J. Cole", "J.-Y. Choi" ],
      "venue" : "IEEE Transactions on Audio, Speech and Language Processing, vol. 14, no. 1, pp. 232–245, 2006.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Prosody in Speech Understanding",
      "author" : [ "R. Kompe" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1997
    }, {
      "title" : "Prosody modeling for automatic speech recognition and understanding",
      "author" : [ "E. Shriberg", "A. Stolcke" ],
      "venue" : "Mathematical Foundations of Speech and Language Processing. Springer, 2004, pp. 105–114.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Prosodic models, automatic speech understanding, and speech synthesis: towards the common ground",
      "author" : [ "A. Batliner", "B. Möbius", "G. Möhler", "A. Schweitzer", "E. Nöth" ],
      "venue" : "Proceedings of the European Conference on Speech Communication and Technology (Aalborg, Denmark), vol. 4. ISCA, 2001, pp. 2285–2288.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Detecting pitch accent using pitch-corrected energy-based predictors",
      "author" : [ "A. Rosenberg", "J. Hirschberg" ],
      "venue" : "Proceedings of Interspeech, 2007, pp. 2777–2780.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "ToBI: A standard for labelling English prosody",
      "author" : [ "K. Silverman", "M. Beckman", "J. Pitrelli", "M. Ostendorf", "C. Wightman" ],
      "venue" : "Proceedings of ICSLP, 1992, pp. 867–870.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Experiments on automatic prosodic labeling",
      "author" : [ "A. Schweitzer", "B. Möbius" ],
      "venue" : "Proceedings of Interspeech, 2009, pp. 2515–2518.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Modeling phrasing and prominence using deep recurrent learning",
      "author" : [ "A. Rosenberg", "R. Fernandez", "B. Ramabhadran" ],
      "venue" : "Proceedings of Interspeech, 2015, pp. 3066–3070.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "An automatic prosody labeling system using ann-based syntactic-prosodic model and gmm-based acoustic-prosodic model",
      "author" : [ "K. Chen", "M. Hasegawa-Johnson", "A. Cohen" ],
      "venue" : "Proceedings of ICASSP, 2004, pp. 509–512.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Using neural networks to locate pitch accents",
      "author" : [ "P. Taylor" ],
      "venue" : "Proceedings of the 4th European Conference on Speech Communication and Technology, 1995.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Detecting pitch accents at the word, syllable and vowel level",
      "author" : [ "A. Rosenberg", "J. Hirschberg" ],
      "venue" : "HLT-NAACL, 2009.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Prosodic prominence detection in speech",
      "author" : [ "F. Tamburini" ],
      "venue" : "ISSPA2003, 2003, pp. 385–338.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Pitch accent prediction using ensemble machine learning",
      "author" : [ "X. Sun" ],
      "venue" : "Proceedings of ICSLP-2002, 2002, pp. 16–20.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Automatic prosodic event detection using acoustic, lexical and syntactic evidence",
      "author" : [ "S. Ananthakrishnan", "S.S. Narayanan" ],
      "venue" : "IEEE Transactions on Audio, Speech and Language Processing, vol. 16, no. 1, 2008, pp. 216–228.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Context in multi-lingual tone and pitch accent recognition",
      "author" : [ "G.-A. Levow" ],
      "venue" : "Proceedings of Interspeech, 2005, pp. 1809– 1812.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Exploiting contextual information for prosodic event detection using auto-context",
      "author" : [ "J. Zhao", "W.-Q. Zhang", "H. Yuan", "M.T. Johnson", "J. Liu", "S. Xia" ],
      "venue" : "EURASIP J. Audio, Speech and Music Processing, vol. 2013, p. 30, 2013.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Automatic classification of lexical stress in English and Arabic languages using deep learning",
      "author" : [ "M. Shahin", "J. Epps", "B. Ahmed" ],
      "venue" : "Proceedings of Interspeech, 2016, pp. 175–179.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Enhance the word vector with prosodic information for the recurrent neural network based tts system",
      "author" : [ "X. Wang", "S. Takaki", "J. Yamagishi" ],
      "venue" : "Proceedings of Interspeech, 2016, pp. 2856–2860.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "The Boston University Radio News Corpus",
      "author" : [ "M. Ostendorf", "P. Price", "S. Shattuck-Hufnagel" ],
      "venue" : "Boston University, Technical Report ECS-95-001, 1995.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Speakerindependent automatic detection of pitch accent",
      "author" : [ "K. Ren", "S.-S. Kim", "M. Hasegawa-Johnson", "J. Cole" ],
      "venue" : "ISCA International Conference on Speech Prosody, 2004, pp. 521–524.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Classification of prosodic events using quantized contour modeling",
      "author" : [ "A. Rosenberg" ],
      "venue" : "Proceedings of HLT-NAACL, 2010, pp. 721–724.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Dropout: a simple way to prevent neural networks from overfitting",
      "author" : [ "N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov" ],
      "venue" : "Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929–1958, 2014.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 1929
    }, {
      "title" : "Recent developments in opensmile, the Munich open-source multimedia feature extractor",
      "author" : [ "F. Eyben", "F. Weninger", "F. Groß", "B. Schuller" ],
      "venue" : "Proceedings of the 21st ACM international conference on Multimedia, 2013, pp. 835–838.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Combining recurrent and convolutional neural networks for relation classification",
      "author" : [ "N.T. Vu", "H. Adel", "P. Gupta", "H. Schütze" ],
      "venue" : "Proceedings of HLT-NAACL 2016, 2016, pp. 534–539.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Relation classification via recurrent neural network",
      "author" : [ "D. Zhang", "D. Wang" ],
      "venue" : "arXiv preprint arXiv:508.01006v1, 2015.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "D.P. Kingma", "J. Ba" ],
      "venue" : "arXiv preprint arXiv:1412.6980, 2017.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Frequency of occurrence effects on pitch accent realisation",
      "author" : [ "K. Schweitzer", "M. Walsh", "B. Möbius", "H. Schütze" ],
      "venue" : "Proceedings of Interspeech, 2010, pp. 138–141.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "For example, knowing what word in an utterance is pitch accented provides important insight into discourse structure such as focus, givenness and contrast [1, 2].",
      "startOffset" : 155,
      "endOffset" : 161
    }, {
      "referenceID" : 1,
      "context" : "For example, knowing what word in an utterance is pitch accented provides important insight into discourse structure such as focus, givenness and contrast [1, 2].",
      "startOffset" : 155,
      "endOffset" : 161
    }, {
      "referenceID" : 2,
      "context" : "Phrasing information and boundary tones for example relate to the syntactic structure [3].",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 3,
      "context" : "A substantial amount of research has dealt with the impact of prosodic information for a wide range of language understanding tasks such as automatic speech recognition [4, 5, 6, 7] and understanding [8, 9, 10].",
      "startOffset" : 169,
      "endOffset" : 181
    }, {
      "referenceID" : 4,
      "context" : "A substantial amount of research has dealt with the impact of prosodic information for a wide range of language understanding tasks such as automatic speech recognition [4, 5, 6, 7] and understanding [8, 9, 10].",
      "startOffset" : 169,
      "endOffset" : 181
    }, {
      "referenceID" : 5,
      "context" : "A substantial amount of research has dealt with the impact of prosodic information for a wide range of language understanding tasks such as automatic speech recognition [4, 5, 6, 7] and understanding [8, 9, 10].",
      "startOffset" : 169,
      "endOffset" : 181
    }, {
      "referenceID" : 6,
      "context" : "A substantial amount of research has dealt with the impact of prosodic information for a wide range of language understanding tasks such as automatic speech recognition [4, 5, 6, 7] and understanding [8, 9, 10].",
      "startOffset" : 169,
      "endOffset" : 181
    }, {
      "referenceID" : 7,
      "context" : "A substantial amount of research has dealt with the impact of prosodic information for a wide range of language understanding tasks such as automatic speech recognition [4, 5, 6, 7] and understanding [8, 9, 10].",
      "startOffset" : 200,
      "endOffset" : 210
    }, {
      "referenceID" : 8,
      "context" : "A substantial amount of research has dealt with the impact of prosodic information for a wide range of language understanding tasks such as automatic speech recognition [4, 5, 6, 7] and understanding [8, 9, 10].",
      "startOffset" : 200,
      "endOffset" : 210
    }, {
      "referenceID" : 9,
      "context" : "A substantial amount of research has dealt with the impact of prosodic information for a wide range of language understanding tasks such as automatic speech recognition [4, 5, 6, 7] and understanding [8, 9, 10].",
      "startOffset" : 200,
      "endOffset" : 210
    }, {
      "referenceID" : 10,
      "context" : "PER distinguishes two subtasks: detection typically refers to the binary classification task (presence or absence of a prosodic event), while prosodic event classification encompasses the full multi-class labelling of prosodic event types [11] e.",
      "startOffset" : 239,
      "endOffset" : 243
    }, {
      "referenceID" : 11,
      "context" : "as described in the ToBI standard [12].",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 12,
      "context" : "Typically the recognition of pitch accents is modelled separately from phrase boundaries, although the acoustic features are quite similar [13, 14, 15].",
      "startOffset" : 139,
      "endOffset" : 151
    }, {
      "referenceID" : 13,
      "context" : "Typically the recognition of pitch accents is modelled separately from phrase boundaries, although the acoustic features are quite similar [13, 14, 15].",
      "startOffset" : 139,
      "endOffset" : 151
    }, {
      "referenceID" : 14,
      "context" : "Typically the recognition of pitch accents is modelled separately from phrase boundaries, although the acoustic features are quite similar [13, 14, 15].",
      "startOffset" : 139,
      "endOffset" : 151
    }, {
      "referenceID" : 12,
      "context" : "Many approaches focus on finding appropriate acoustic representations of prosody [13, 11].",
      "startOffset" : 81,
      "endOffset" : 89
    }, {
      "referenceID" : 10,
      "context" : "Many approaches focus on finding appropriate acoustic representations of prosody [13, 11].",
      "startOffset" : 81,
      "endOffset" : 89
    }, {
      "referenceID" : 15,
      "context" : "These features generally describe the fundamental frequency (f0) and energy and can be either frame-based [16] or grouped across segments [17].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 16,
      "context" : "These features generally describe the fundamental frequency (f0) and energy and can be either frame-based [16] or grouped across segments [17].",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 12,
      "context" : "Often acoustic-prosodic features also include the duration of certain segments [13, 18, 19].",
      "startOffset" : 79,
      "endOffset" : 91
    }, {
      "referenceID" : 17,
      "context" : "Often acoustic-prosodic features also include the duration of certain segments [13, 18, 19].",
      "startOffset" : 79,
      "endOffset" : 91
    }, {
      "referenceID" : 18,
      "context" : "Often acoustic-prosodic features also include the duration of certain segments [13, 18, 19].",
      "startOffset" : 79,
      "endOffset" : 91
    }, {
      "referenceID" : 19,
      "context" : "Most successful methods that rely on acoustic features also benefit from the addition of lexicosyntactic information [20, 13, 19].",
      "startOffset" : 117,
      "endOffset" : 129
    }, {
      "referenceID" : 12,
      "context" : "Most successful methods that rely on acoustic features also benefit from the addition of lexicosyntactic information [20, 13, 19].",
      "startOffset" : 117,
      "endOffset" : 129
    }, {
      "referenceID" : 18,
      "context" : "Most successful methods that rely on acoustic features also benefit from the addition of lexicosyntactic information [20, 13, 19].",
      "startOffset" : 117,
      "endOffset" : 129
    }, {
      "referenceID" : 20,
      "context" : "Since prosodic events usually span several segments, many cited approaches add features representing the surrounding segment, while others explicitly focus on context modelling [21, 14, 22].",
      "startOffset" : 177,
      "endOffset" : 189
    }, {
      "referenceID" : 13,
      "context" : "Since prosodic events usually span several segments, many cited approaches add features representing the surrounding segment, while others explicitly focus on context modelling [21, 14, 22].",
      "startOffset" : 177,
      "endOffset" : 189
    }, {
      "referenceID" : 21,
      "context" : "Since prosodic events usually span several segments, many cited approaches add features representing the surrounding segment, while others explicitly focus on context modelling [21, 14, 22].",
      "startOffset" : 177,
      "endOffset" : 189
    }, {
      "referenceID" : 22,
      "context" : "[23] combine the output of a CNN that learns high-level features representations from 27 frame-based Mel-spectral features with global (or aggregated) f0, energy and duration features across syllables for lexical stress detection.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "[24] train a CNN on continuous wavelet transformations of the fundamental frequency for the detection of pitch accents and phrase boundaries in a speaker-dependent task.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "As previously pointed out in [19, 17], the large number of different approaches and task descriptions renders the comparison of PER performance methods quite difficult.",
      "startOffset" : 29,
      "endOffset" : 37
    }, {
      "referenceID" : 16,
      "context" : "As previously pointed out in [19, 17], the large number of different approaches and task descriptions renders the comparison of PER performance methods quite difficult.",
      "startOffset" : 29,
      "endOffset" : 37
    }, {
      "referenceID" : 24,
      "context" : "Thus, our results are compared only to approaches that use the Boston University Radio News Corpus (BURNC) [25] and purely acoustic features.",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 18,
      "context" : "Good results for pitch accent detection were reported by Sun [19], namely 84.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 23,
      "context" : "[24] use CNNs to detect pitch accents and phrase boundaries on the f2b speaker, obtaining 86.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 25,
      "context" : "[26] obtain 83.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 26,
      "context" : "Rosenberg [27] reports almost 64% accuracy for pitch accents and 72.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 14,
      "context" : "[15] apply their neural-based method to speaker-independent setups using 4 speakers of BURNC and distinguishing 4 event types.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "An early example of a neural network approach was proposed in [16], and relied only on frame-based acoustic features such as f0 and energy.",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 27,
      "context" : "For regularization, we also apply dropout [28] to this last layer.",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 28,
      "context" : "We extract acoustic features from the speech signal using the OpenSMILE toolkit [29].",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 22,
      "context" : "In this work, two different feature sets are used: a prosody feature set consisting of 5 features from the OpenSMILE catalogue (smoothed f0, RMS energy, PCM loudness, voicing probability and Harmonics-toNoise Ratio), and a Mel feature set consisting of 27 features extracted from the Mel-frequency spectrum (similar to [23]).",
      "startOffset" : 319,
      "endOffset" : 323
    }, {
      "referenceID" : 29,
      "context" : "This type of feature has been proposed for use in neural network models for relation classification [30, 31].",
      "startOffset" : 100,
      "endOffset" : 108
    }, {
      "referenceID" : 30,
      "context" : "This type of feature has been proposed for use in neural network models for relation classification [30, 31].",
      "startOffset" : 100,
      "endOffset" : 108
    }, {
      "referenceID" : 13,
      "context" : "Previous work has demonstrated the benefits of adding context information to PER [14, 21].",
      "startOffset" : 81,
      "endOffset" : 89
    }, {
      "referenceID" : 20,
      "context" : "Previous work has demonstrated the benefits of adding context information to PER [14, 21].",
      "startOffset" : 81,
      "endOffset" : 89
    }, {
      "referenceID" : 10,
      "context" : "The most straighforward approach is to add features that represent the right and left neighbouring segments to form a type of acoustic context window [11, 13, 24].",
      "startOffset" : 150,
      "endOffset" : 162
    }, {
      "referenceID" : 12,
      "context" : "The most straighforward approach is to add features that represent the right and left neighbouring segments to form a type of acoustic context window [11, 13, 24].",
      "startOffset" : 150,
      "endOffset" : 162
    }, {
      "referenceID" : 23,
      "context" : "The most straighforward approach is to add features that represent the right and left neighbouring segments to form a type of acoustic context window [11, 13, 24].",
      "startOffset" : 150,
      "endOffset" : 162
    }, {
      "referenceID" : 11,
      "context" : "The dataset used in this work is a subset of BURNC that has been manually labelled with prosodic events according to the ToBI labelling standard [12].",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 18,
      "context" : "For the speaker-dependent experiments, the largest speaker subset f2b is used in line with previous methods [19, 24].",
      "startOffset" : 108,
      "endOffset" : 116
    }, {
      "referenceID" : 23,
      "context" : "For the speaker-dependent experiments, the largest speaker subset f2b is used in line with previous methods [19, 24].",
      "startOffset" : 108,
      "endOffset" : 116
    }, {
      "referenceID" : 26,
      "context" : "For the classification task, we distinguish 5 different ToBI types of pitch accents and phrase boundaries (as in [27]), where the downstepped accents are collapsed into the non-downstepped ones: The pitch accent classes are (1) H* and !H*, (2) L*, (3) L+H* and L+!H*, (4) L*+H and L*+!H and (5) H+!H*.",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 31,
      "context" : "The models are trained for 50 epochs with an adaptive learning rate (Adam [32]) and L2 regularization.",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 10,
      "context" : "A widely-used measure to enable the generalization of prosodic models across speakers is speaker normalization in the form of z-scoring [11, 15, 33].",
      "startOffset" : 136,
      "endOffset" : 148
    }, {
      "referenceID" : 14,
      "context" : "A widely-used measure to enable the generalization of prosodic models across speakers is speaker normalization in the form of z-scoring [11, 15, 33].",
      "startOffset" : 136,
      "endOffset" : 148
    }, {
      "referenceID" : 32,
      "context" : "A widely-used measure to enable the generalization of prosodic models across speakers is speaker normalization in the form of z-scoring [11, 15, 33].",
      "startOffset" : 136,
      "endOffset" : 148
    } ],
    "year" : 2017,
    "abstractText" : "This paper demonstrates the potential of convolutional neural networks (CNN) for detecting and classifying prosodic events on words, specifically pitch accents and phrase boundary tones, from frame-based acoustic features. Typical approaches use not only feature representations of the word in question but also its surrounding context. We show that adding position features indicating the current word benefits the CNN. In addition, this paper discusses the generalization from a speaker-dependent modelling approach to a speaker-independent setup. The proposed method is simple and efficient and yields strong results not only in speaker-dependent but also speaker-independent cases.",
    "creator" : "LaTeX with hyperref package"
  }
}