{
  "name" : "1405.3713.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Contextual Abductive Reasoning with Side-Effects",
    "authors" : [ "Luı́s Moniz Pereira", "Emmanuelle-Anna Dietz", "Steffen Hölldobler" ],
    "emails" : [ "lmp@fct.unl.pt", "dietz@iccl.tu-dresden.de", "sh@iccl.tu-dresden.de" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n40 5.\n37 13\nv2 [\ncs .A"
    }, {
      "heading" : "1 Introduction",
      "text" : "In the context of abductive reasoning, whenever discovering abductive explanations for some given primary observation, one may wish to check too whether some other given additional secondary observations are true, as logical consequences of the abductive explanations found for the primary observation. In other words, whether the secondary observations are plausible in the abductive context of the primary one, is a common scientific reasoning task. Thus, for example, one may attempt to find abductive explanations for such secondary observations strictly within the context of the given abductive explanations found for the primary observation – that is, disallowing new abductions – or, nevertheless, allowing additional abductions as long as they are consistent with the primary ones. As it were, the explanations of secondary observational\n∗ lmp@fct.unl.pt † {dietz,sh}@iccl.tu-dresden.de\n2 consequences may be consumers, but not producers, of the abductions produced in explaining the primary observation. We show this type of reasoning requires the characterization of a new abduction concept and mechanism, that of contextual abduction. We examine and formalize its variants, and employ these to understand and justify belief bias of human reasoning in addressing syllogisms.\nOur starting point is a psychological study carried out by Evans et al. (1983) about deductive reasoning which demonstrated possibly conflicting processes in human reasoning. Participants were presented different syllogisms and had to decide whether they were logically valid. Consider one of them, Sadd :\nPREMISE1 No addictive things are inexpensive. PREMISE2 Some cigarettes are inexpensive. CONCLUSION Therefore, some addictive things are not cigarettes.\nEven though the conclusion does not necessarily follow from the premises,1 participants assumed the syllogism to be logically valid. They were explicitly asked to logically validate or invalidate these syllogisms, but didn’t seem to have the intellectual capability to do so. Even worse, they were not at all aware about their inabilities. Repeatedly, the majority of people were proffering with certainty of confidence the wrong answer. Evans et al. (1983) concluded that this happened because they were being unduly influenced by their own beliefs, their belief bias. In (Evans 2010; Evans 2012), the conflict between logic and belief in human reasoning is discussed extensively.\nWe will show how the belief bias effect can be explained by abductive reasoning and its corresponding side-effects. This forms the basis for investigating contextual side-effects, (strict) possible side-effects, contextual contestable side-effects, and (jointly supported) contextual relevant consequences."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "We define the necessary notations, simplified for our present purposes but generalizable as usual (cf. Kowalski (2011)). We restrict ourselves to datalog programs, i.e. the set of terms consists only of constants and variables. A (first-order) logic program P is a finite set of clauses:\nA ← A1 ∧ . . . ∧ An ∧ ¬B1 ∧ . . . ∧ ¬Bm,\nwhere A and Ai, 1 ≤ i ≤ n, are atoms and ¬Bj , 1 ≤ j ≤ m, are negated atoms. A is the head and A1 ∧ . . . ∧ An ∧ ¬B1 ∧ . . . ∧ ¬Bm is the body of the clause. In the sequel we will abbreviate the body of a clause by simply writing body. A ← ⊤ and A ← ⊥ are special cases of clauses denoting positive and negative facts, respectively. If an argument is written with an upper case letter, it is a variable; otherwise it is a constant. In the sequel, if not denoted otherwise, we assume P to be ground, containing all the ground instances of its clauses. The set of all atoms occurring in P is atoms(P). An atom is undefined in P if it is not the head of some clause in P and the corresponding set of these atoms is undef(P).\n1 In abstract, it could be conceived that all addictive things would be just the (expensive) cigarettes. It does not follow because the conclusion supposes an existence which is not warranted by the premises."
    }, {
      "heading" : "2.1 Three-Valued Łukasiewicz Semantics",
      "text" : "We use the three-valued Łukasiewicz (1920) semantics. Table 1 defines the corresponding truth tables. Interpretations are represented by pairs 〈I⊤, I⊥〉, such that"
    }, {
      "heading" : "I⊤ = {A ∈ BP | A is mapped to ⊤} and I⊥ = {A ∈ BP | A is mapped to ⊥},",
      "text" : "where BP is the Herbrand base with respect to a given program P . A model of P is an interpretation which maps each clause occurring in P to ⊤.\nOne should observe that in contrast to two-valued logic, A ← B andA∨¬B are not equivalent under three-valued Łukasiewicz Semantics. Consider, for instance, an interpretation I such that I(A) = I(B) = U. Then, I(A ∨ ¬B) = U whereas I(A ←Ł B) = ⊤."
    }, {
      "heading" : "2.2 Weak Completion Semantics",
      "text" : "Weak completion semantics has first been introduced by Hölldobler and Kencana Ramli (2009a) and seems to adequately model Byrne’s (1989) suppression task (Dietz et al. 2012) and Wason’s (1968) selection task (Dietz et al. 2013). Consider the following transformation for given P :1. Replace all clauses in P with the same head A ← body1, A ← body2, . . .\nby the single expression A ← body1 ∨ body2,∨ . . . . 2. if A ∈ undef(P) then add A ← ⊥. 3. Replace all occurrences of ← by ↔.\nThe resulting set of equivalences is called the completion of P (Clark 1978). If Step 2 is omitted, then the resulting set is called the weak completion of P (wcP).\nHölldobler and Kencana Ramli (2009b) showed that the model intersection property holds for weakly completed programs. This guarantees the existence of a least model for every program. In computational logic, least models can often be computed as least fixed points of an appropriate semantic operator (Apt and van Emden 1982). Stenning and van Lambalgen (2008) devised such an operator which has been generalized for first-order programs by Hölldobler and Kencana Ramli (2009a): Let I be an interpretation and ΦP(I) = 〈J⊤, J⊥〉, where\nJ⊤= {A |there exists A ← body ∈ P with I(body) = ⊤}, J⊥= {A |there exists A ← body ∈ P and for all A ← body ∈ P we find I(body) = ⊥}.\nAs shown in (Hölldobler and Kencana Ramli 2009a) the least fixed point of ΦP is identical to the least model of the weak completion of P (lmŁwcP) under three-valued Łukasiewicz semantics. Starting with the empty interpretation I = 〈∅, ∅〉, lmŁwcP can be computed by iterating ΦP .2\n2 Weak completion semantics corresponds to well-founded semantics (Van Gelder et al. 1991) for tight logic programs (Dietz et al. 2013).\n4"
    }, {
      "heading" : "3 Reasoning in an Appropriate Logical Form",
      "text" : "Stenning and van Lambalgen (2005; 2008) proposed to model human reasoning by a two step process: Firstly, human reasoning should be modeled by setting up an appropriate representation and, secondly, the reasoning process should be modeled with respect to this representation. In this section we discuss the first step and show how to model syllogisms in logic programs.\n3.1 Integrity Constraints\nPREMISE1 of Sadd is\nNo addictive things are inexpensive. (1)\nand is equivalent to\nIf something is inexpensive, then it is not addictive. (2)\nThe consequence is the negation of something is addictive. As weak completion semantics does not allow negative heads in clauses, for every negative conclusion ¬p(X) we introduce an auxiliary formula p′(X), which denotes the negation of p and the clause p(X) ← ¬p′(X).\nWe obtain the following preliminary representation of the first premise of Sadd with regard to addictive: add ′(X) ← inex (X), and add(X) ← ¬add ′(X), where add(X), add ′(X), and inex (X) stand for X is addictive, not addictive, and inexpensive.\nWith the introduction of these auxiliary atoms, the need for integrity constraints arises. A least model of the weak completion that contains both add(X) and add ′(X) in I⊤ should be invalidated as a model in general. This condition can be represented by a set of integrity constraints IC, which contains clauses of the following form: U ← IC Body, where the implication is understood as usual. For our example above, ICadd contains one clause: U ← add(X) ∧ add ′(X). However, the Φ operator does not consider clauses of this form, thus such denial ICs are not evaluated by it. We apply a two step approach to take them into consideration. First, we compute the least model of the given program and second, we verify whether it does satisfy the requirements of the IC. Given an interpretation I and a set of integrity constraints IC, I satisfies IC if and only if all clauses in IC are true under I . For the following examples, whenever there exists a p(X) and its p′(X) counterpart in P , we implicitly assume that ICp: U ← p(X) ∧ p′(X).3"
    }, {
      "heading" : "3.2 Abnormalities & Background Knowledge",
      "text" : "A direct representation of PREMISE2 is\nThere exists a cigarette which is inexpensive. (3)\nAdditionally, it is commonly known that\nCigarettes are addictive. (4)\nAs discussed in (Evans et al. 1983), humans seem to have a background knowledge or belief, which, in this context, assuming (4), we imply that\nCigarettes are inexpensive (compared to other addictive things),\n3 This view on ICs corresponds to the definition applied for the well-founded semantics in (Pereira et al. 1991).\n5 which implies (3) and biases the reasoning towards a representation. The preliminary program representing the first two premises, is the non-ground program Ppreadd:\nadd ′(X) ← inex (X), add(X) ← ¬add ′(X), inex(X) ← cig(X), cig(a) ← ⊤.\nand its corresponding least model is: 〈{cig(a), inex (a), add ′(a)}, {add(a)}〉. This model contradicts the commonly known assumption of (4). This background knowledge can be expressed in the program by applying Stenning and van Lambalgen’s idea to implement conditionals by a normal default permission for implications.This can be achieved by adding an abnormality predicate to the antecedent of the implication and initially assuming that the abnormality predicate is false. Following this idea, the initial PREMISE1 in Sadd is extended to:"
    }, {
      "heading" : "If something is inexpensive and not abnormal, then it is not addictive.",
      "text" : "Nothing (as a rule) is abnormal (regarding (1)).\nThis belief-bias together with the idea to represent conditionals by a normal default permission for implication leading to this rendering\nIf something is a cigarette and not abnormal, then it is inexpensive. (5) Nothing (as a rule) is abnormal (regarding (3)).\nTogether with (4), it leads to\nIf something is a cigarette, then it is abnormal (regarding (1)).\nFinally, the information in the premises of Sadd is encoded as the non-ground program Padd :\nadd ′(X) ← inex (X) ∧ ¬ab1(X), add(X) ← ¬add ′(X), inex (X) ← cig(X) ∧ ¬ab2(X), ab2(X) ← ⊥, ab1(X) ← ⊥, ab1(X) ← cig(X), cig(a) ← ⊤.\nPadd represents the contextual background knowledge of syllogism Sadd ."
    }, {
      "heading" : "4 Abduction and Predictability",
      "text" : "In scientific methodology, a prediction can be made by adding hypotheses to knowledge known about the world. As specified by the classical hypothetic-deductive method (Hempel 1966), scientific inquiry is carried out in three stages: hypotheses generation, prediction, and evaluation. One or more hypotheses may be generated, by abduction, to explain observed events. A generated set of hypotheses (or assumptions) can thence be employed for predicting unseen events by means of deduction, on the implicit condition of not making further abductions (cf. Pereira and Pinto (2011)). The predicted events can then hopefully be tested against reality, in the form of such observable deduced side-effects, in order to evaluate the plausibility of the set of hypotheses.\nAbduction, or inference to the best explanation (its usual designation in the philosophy of science), is a reasoning method whereby one chooses those hypotheses that would, if true, best explain observed evidence or enable to satisfy some query, whilst meeting attending constraints. Abduction has been well studied in the field of computational logic – and logic programming in particular – for a few decades now. Abduction, when added to logic programs, offers a formalism to declaratively express problems in a variety of areas and empowers many applications,\n6 e.g. in decision-making, diagnosis, planning, belief revision, and conditional reasoning. In logic programs, abductive hypotheses (or abducibles) are named given atoms of the program which have no rules, and whose truth value is not initially assumed, and hence unknown.\nThe approach presented in (Torasso et al. 1991) treats abduction with completion semantics and does not study side-effects. In the following we consider abduction with weak completion semantics and introduce the examination of side-effects in contexts afforded by abduction."
    }, {
      "heading" : "4.1 Abductive Framework",
      "text" : "Following Kakas et al. (1993) we consider an abductive framework consisting of a program P as knowledge base, a collection of atoms A of abducibles syntactically represented by the set of the (positive and negative) facts for each undefined ground atom in P , a set of integrity constraints IC, and the logical consequence relation |=lmwcŁ , where P |= lmwc Ł F if and only if lmŁwcP(F ) = ⊤ for the formula F . An observation is a set of (at least one) literals. The truth value of abducibles may be independently assumed true or false, via either their positive or negated form, as the case may be, in order to produce an abductive explanation for an observation – or solution to a query –, which is a consistent set of assumed hypotheses in the form of abducibles.\nAn abductive solution is a consistent set of abducible instances that, when substituted by their assigned truth value in P , affords us with a model of P (for the specific semantics used on P), which satisfies the observation (or query) and any imposed integrity constraints – a so-called abductive model. In our notation this amounts to adding to P the corresponding positive and negative facts representing a solution’s abducibles.\nWhat we dub observation is analogous to a query whose explanation is desired, not necessarily something actually observed.\nDefinition 1 Let 〈P ,A, IC, |=lmwcŁ 〉 be an abductive framework,O be an observation, and E be an explanation which is a (consistent) subset of A, a set of integrity constraints IC, and the consequence relation |=lmwcŁ , defined for all formulas F .\nO is explained by E given P and IC iff P∪E |=lmwcŁ O, where P 6|= lmwc Ł O and lmŁwc (P∪E)\nsatisfies IC. O is explained given P and IC iff there exists an E such that O is explained by E given P\nand IC.\nIn abduction, as for its deduction counterpart, credulous and skeptical reasoning varieties are distinguished. Credulous reasoning consists in finding if there exists at least one model of the program – according to some pre-established semantics – which entails the observation to be explained. Skeptical reasoning demands that every model of the program entails the observation.\nF follows skeptically from P , IC and O iff O can be explained given P and IC, and for all minimal (or some alternative preference criterion instead) explanations E for O it holds that P ∪ E |=lmwcŁ F . F follows credulously from P , IC and O iff there exists a minimal (or some alternative preference criterion instead) explanation E for O and it holds that P ∪ E |=lmwcŁ F .\nBecause the satisfaction of integrity constraints (ICs) can require abductions, we must allow ICs to be actively productive of abductions, and not just use ICs to subsequently disallow abductive\n7 solutions that invalidate the least model with respect to them. However, we cannot actively promote that an atom is required to be false or else unknown, one of the two, in order for ICs to be satisfied. The only possibility is to impose its falsity: because ICs have the form of denials, we can think of U ← A as just an atom that we wish must never be explained by any explanation.\nAs our underlying semantics is three-valued, we may but might not make each undefined atom an abducible, so that we allow for unknown, non-abducible susceptible knowledge, which can be guaranteed by adding the clause A ← A for the predicate under consideration. Furthermore, when making use of skeptical reasoning, we conclude that an abducible is unknown if it does not have the same binary truth value in all models. For instance consider the program P = {A ← B,A ← C} and observation OA = {A}, for which there are two minimal explanation EB = {B ← ⊤} and EC = {C ← ⊤}. Under skeptical reasoning A does not follow from all minimal explanations and thus B and C stay unknown.\nIn previous approaches, weak completion semantics was used for cases expressed in propositional logic, and abduction under skeptical reasoning seemed adequate (Hölldobler et al. 2011)."
    }, {
      "heading" : "4.2 Usual Contextual Abduction",
      "text" : "One important extension of abduction pertains to the issue that, whenever discovering abductive solutions, i.e. explanations, for some given primary observation, one may wish to check too whether some other given additional secondary observations are true, being a logical consequence of the abductive explanations found for the primary observation. In other words, whether the secondary observations are plausible in the abductive context of the primary one. Indeed, often, besides needing to abductively discover which hypotheses to assume in order to satisfy some condition, we may also want to know some of the side-effects of those assumptions.\nWe address the issue of relaxing or loosening the implicit condition about additional abductions not being permitted whilst considering the observable side-effects explained by deductive prediction. In other words, prediction may be allowed recourse to additional assumptions, but nevertheless must make use of at least some of the initial explanations. If several such explanations exist for the observations concerned, then we might want to define alternative conditions that are less strict with regard to each set or to the collection of sets. Reuse of contextual abductions, by resorting to an implementation of tabled abduction for well-founded semantics, is reported in (Saptawijaya and Pereira 2013).\nLet’s consider again Padd , its weak completion consists of the following equivalences:\nadd(a) ↔ ¬add ′(a), add ′(a) ↔ inex (a) ∧ ¬ab1(a), inex (a) ↔ cig(a) ∧ ¬ab2(a), ab2(a) ↔ ⊥, ab1(a) ↔ cig(a), 4 cig(a) ↔ ⊤.\nIts least model, lmŁwcPadd , is 〈{cig(a), inex (a), add(a), ab1(a)}, {add ′(a), ab2(a)}〉, from which we cannot derive the CONCLUSION in Sadd . Obviously, the CONCLUSION is something about an object which is not a. The first part of this conclusion is an observation, let’s say about b: Oadd(b) = {add(b)}, which we need to explain as described in the previous subsection. The set of abducibles with respect to b is: APadd = {cig(b) ← ⊤, cig(b) ← ⊥}. Oadd(b) is true if add ′(b) is false which is false if inex (b) is false or ab1(b) is true. Either inex (b) is false but then cig(b) is false or ab1(b) is true but then cig(b) is true. For Oadd(b) we have two minimal\n4 ⊥ ∨ cig(a) is semantically equivalent to cig(a) under Łukasiewicz Semantics.\n8 explanations E¬cig(b) = {cig(b) ← ⊥} and Ecig(b) = {cig(b) ← ⊤}. The corresponding least models of the weak completion are:\nlmŁwc (Padd ∪ E¬cig(b)) = 〈{. . . , add(b), . . . }, {. . . , cig(b), inex (b), . . . }〉, lmŁwc (Padd ∪ Ecig(b)) = 〈{. . . , add(b), cig(b), inex (b), . . . }, {. . . }〉.\nUnder credulous reasoning we conclude, given explanation E¬cig(b), that the CONCLUSION of Sadd is true, as there exists something addictive which is not a cigarette. I.e., there is a least model which abductively explains that an observed addictive b is not a cigarette. However, we could also explain Oadd(b) by Ecig(b), which is an equally justified explanation. We would prefer our formalism to reflect that the first premise describes the usual and the second premise describes the exceptional case. That is, an inexpensive cigarette is meant to be the exception not the rule in the context of things that are addictive. This exceptional case should then only be considered when more is known about b. This preference is not expressed in our framework yet. The following section discusses and proposes a solution to this issue."
    }, {
      "heading" : "5 Inspection Points",
      "text" : "Until now, we assumed the possible observations as given, and easily identified from the corresponding context which was the exceptional and which the usual case. However, this is not explicitly encoded in our logic programs yet, and needs be syntactically indicated. For this purpose we investigate and apply inspection points, originally presented by Pereira and Pinto (2011).\nIn (Pereira and Pinto 2011), the authors present the concept of inspection points in abductive logic programming and show how one can employ it to investigate side-effects of interest in order to help choose among abductive solutions. In what follows we discuss this approach and show how inspection points can be modeled for our previous examples accordingly, in what concerns abducibles. Given an atom A, we introduce the following two reserved (meta-)predicates:\ninspect(A) and inspect¬(A),\nwhich are special cases of abducibles. They differ from the usual abducibles in the way that they can only be abduced whenever A or ¬A have been abduced somewhere else already. That is, an abductive solution or explanation E is only valid when for each inspect(A) (respectively inspect¬(A)) it contains, it also contains a corresponding A (respectively ¬A). That is, in a solution the consumers, here represented by the inspection points inspect(A) and inspect¬(A), respectively, must have a matching producer. The producers correspond to the usual abducibles.\nOne should observe that for a treatment of inspection points for all literals in a program and not just the abducible ones, we would simply need to adopt the program transformation technique of Pereira and Pinto (2011), which recursively relays inspection of non-terminal literals to the base inspection of terminals. Let us consider again the example of Section 4.2 where we stated that we would prefer to distinguish between the usual case and the exceptional case. We can now represent this in our logic program by replacing our ab1-clause accordingly. The new non-ground program, P insp add , is:\n(Padd \\ {ab1(X) ← cig(X)}) ∪ {ab1(X) ← inspect(cig(X))}.\nSuppose, b is addictive, i.e. Oadd(b). As cig(b) is undefined, inspect(cig(b)) becomes false, and ab1(b) will be false rather than unknown, that is, its falsity is obtained because nothing is known about cig(b). The only minimal explanation for Oadd(b) is now generated by inex(b)\n9 being false, which is achieved by lmŁwc (P insp add ∪ E¬cig(b)):\n〈{. . . , add(b), . . . }, {. . . , add ′(b), inspect(cig(b)), ab1(b), inex (b), ab2(b), . . . }〉.\nEven under skeptical reasoning, there exists an addictive thing which is not a cigarette."
    }, {
      "heading" : "6 The Expressive Power of Inspection Points in Contextual Abduction",
      "text" : "Inspection points allow us to specify various definitions, which we will provide – relative to explanation E of observation O, set of abducibles A and background knowledge P –, to the effect of relaxing the impermissibility of convoking additional hypotheses to explain side-effects in context E . Consider the following program, Pfire :\nstorm ← lightning ∧ ¬ab1, ffire ← inspect(lightning) ∧ ¬ab3, storm ← tempest ∧ ¬ab2, ffire ← barbecue ∧ ¬ab3, ab3 ← ¬dry leaves , rained ← inspect¬(dry leaves) ∧ ¬ab4,\nsmoke ← fire ∧ inspect(ffighters), sirens ← inspect(fire) ∧ffighters ,\nab1 ← ⊥, ab2 ← ⊥, ab3 ← ⊥, ab4 ← ⊥.\nwhere ffire means forest fire and ffighters means fire fighters. The set of abducibles, APfire , is:\ntempest ← ⊤, tempest ← ⊥, barbecue ← ⊤, barbecue ← ⊥,\nlightning ← ⊤, lightning ← ⊥, inspect(lightning) ← ⊤, inspect(lightning) ← ⊥,\ndry leaves ← ⊤, dry leaves ← ⊥, inspect¬(dry leaves) ← ⊤, inspect¬(dry leaves) ← ⊥,\nfire ← ⊤, fire ← ⊥, inspect(fire) ← ⊤, inspect(fire) ← ⊥,\nffighters ← ⊤, ffighters ← ⊥, inspect(ffighters) ← ⊤, inspect(ffighters) ← ⊥.\nFor simplicity, we assume that ICfire = ∅. In the sequel, we provide definitions and abductive examples which clarify how inspection points enrich the expressiveness of logic programs."
    }, {
      "heading" : "6.1 Contextual Side-effects",
      "text" : "Consider the following definition which includes various notions of contextual side-effects:\nDefinition 2 Given background knowledge P and set of integrity constraints IC, let O1 be an observation, E1 be an explanation of O1, O2 be an observation, and E2 be an explanation of O2.\nO2 is a necessary contextual side-effect of O1 given P and IC iff for all E1 there exists E2 such that, for any inspect(A), inspect¬(A) ∈ E2 for which respectively A, ¬A 6∈ E2, and some such exists in E2, then, A, ¬A ∈ E1, accordingly. O2 is a strict necessary contextual side-effect of O1 given P and IC iff O2 is a necessary contextual side-effect of O1 given P , and E1 ⊆ E2 for any E1.\n10\nO2 is a possible contextual side-effect of explained O1 given P and IC iff there exists an E1 and an E2 such that for any inspect(A), inspect¬(A) ∈ E2 for which respectivelyA, ¬A 6∈ E2, and some such exists in E2, then A, ¬A ∈ E1, accordingly. O2 is a strict possible contextual side-effect of O1 given P and IC iff O2 is a necessary contextual side-effect of O1 given P , and for some E1, E1 ⊆ E2.\nThe idea behind the definition for necessary contextual side-effects is that every explanation E1 for O1 affords us with one complete explanation by which any (inspection) incomplete explanation E2 for O2 can be necessarily completed. The idea behind the definition for possible contextual side-effects is that there is some explanation E1 for O1 which affords us with one complete explanation by which some (inspection) incomplete explanation E2 for O2 can be completed.\nConsider again Pfire , where there are two rules for ffire: one explanation is barbecue and another one is lightning . However, we assume that a lightning causing a forest fire is much more unlikely than a barbecue, and therefore, lightning , when not observed directly, only counts as a plausible explanation when it has been abduced by some other observation. This is expressed by inspect(lightning). Assume that we observe a storm and know that nothing is abnormal with respect to ab3, thus the leaves are dry: Ostorm,dry = {storm, dry leaves}. The minimal explanations are Elightning = {lightning ← ⊤} and Etempest = {tempest ← ⊤}. We have only enough evidence to explain Offire if we abduce Elightning for Ostorm,dry . By Definition 2, Offire is a strict possible contextual side-effect of explained Ostorm,dry given Pfire and ICfire ."
    }, {
      "heading" : "6.2 Contestable Contextual Side-effects",
      "text" : "Whereas up till now we stipulated cases where known explanations gave grounds for strengthening the plausibility of contextual side-effects, we next turn our attention to cases where the latter are to some extent contested and made implausible, by appealing to their negation. Analogously to Definition 2, its counterpart, contestable contextual side-effects, is defined as follows:\nDefinition 3 Given background knowledge P and set of integrity constraints IC, let O1 be an observation, E1 be an explanation of O1, O2 be an observation, and E2 be an explanation of ¬O2.\nO2 is a necessarily contested contextual side-effect of O1 given P and IC iff for all explanations E1 there exists E2 such that, for any inspect(A), inspect¬(A) ∈ E2, for which respectively A, ¬A 6∈ E2, and some such exists in E2, then, A, ¬A ∈ E1, accordingly. O2 is a possibly contested contextual side-effect of O1 given P and IC iff there exists an E1 and an E2 such that for any inspect(A), inspect¬(A) ∈ E2 for which respectivelyA, ¬A 6∈ E2, and some such exists in E2, then A, ¬A ∈ E1, accordingly.\nThe idea behind necessarily contested contextual side-effects, is, that every explanation E1 forO1 affords us with one complete explanation under which some incomplete explanation E2 for ¬O2 is necessarily completed. The idea behind the definition for the possibly contested contextual side-effects, is, that there is at least one explanation E1 for O1 which affords us with one complete explanation under which some incomplete explanation E2 for ¬O2 is necessarily completed.\nConsider rained ← inspect¬(dry leaves) in Pfire : it states that if, for some other observation we explained that the leaves are not dry, then it rained. Thus, when we observe a forest fire, then, one part of the abduced explanation will be, in any case, independent of whether there was a lightning or a barbecue, that the leaves are dry. However this explanation will lead to\n11\ninspect¬(dry leaves) being false, which makes rained false as well. O¬rained is a consequence that follows from the explanation for Offire . According to Definition 3, O¬rained is a necessarily contested contextual side-effect of Off given Pfire and ICfire .\nAnother variation of contestable side-effects is abductive rebuttal. In this case, the side-effect directly contradicts an observation. That is to say, it is the case that O2 ≡ ¬O1 in Definition 3. The second observation, O2 has to be the head of some clause and the negation of O1.5"
    }, {
      "heading" : "6.3 Contextual Relevant Consequences",
      "text" : "We identify two notions of contextual relevant consequences and define them as follows:\nDefinition 4 Given background knowledge P and set of integrity constraints IC, let O1 be an observation, E1 be an explanation of O1, O2 be an observation, and E2 be an explanation of O2.\nO2 is a necessary contextual relevant consequence of O1 given P and IC iff for all E2 there exists E1 consistent with E2 such that there exists either inspect(A) or inspect¬(A) ∈ E2 for which respectively A, ¬A 6∈ E2, but in E1. O2 is a possible contextual relevant consequence of O1 given P and IC iff there exists E2 there exists E1 consistent with E2 such that there exists either inspect(A) or inspect¬(A) ∈ E2 for which respectively A, ¬A 6∈ E2, but in E1.\nAssume we only observe Ostorm given Pfire and do not know whether the leaves are dry. Then Elightning = {lightning ← ⊤} is only partly explainingOffire , as additionally we need to abduce that the leaves are dry. However, as this explanation is not decorated with the inspect predicate, it can be abduced straightforwardly by Offire . Accordingly, Offire is a possible contextual relevant consequence of Ostorm given Pfire and ICfire . The case of contested relevant contextual consequence, where mere intersection is the case, could too be analogously expressed."
    }, {
      "heading" : "6.4 Jointly Supported Contextual Relevant Consequences",
      "text" : "It might be the case that two observations contain side-effects of each other, simultaneously. That is, more generally, we can allow for each observation to need inspection of the abducibles of the other observation; that is, they are mutually plausibly explained together, but not each by itself!\nDefinition 5 Given background knowledge P and set of integrity constraints IC, let O1, O2 be observations.\nO1 and O2 are necessarily jointly supported contextual relevant consequences given P and IC iff O1 is a necessary contextual relevant consequence of O2 given P and IC, and O2 is a necessary contextual relevant consequence of O1 given P and IC as defined in Definition 4. O1 and O2 are possibly jointly supported contextual relevant consequences given P and IC iff O1 is a possible contextual relevant consequence of O2 given P and IC, and O2 is a possible contextual relevant consequence of O1 given P and IC, as defined in Definition 4.\nLet us observe Osmoke in Pfire. Then we can abduce fire, but ffighters needs to be explained by some other observation. On the other hand, by observing Osirens , we can abduce ffighters but not fire. Accordingly, Osmoke and Osir are necessarily jointly supported contextual relevant consequences given Pfire and ICfire .\n5 As negative heads in our programs are not allowed, we can model these cases as described in Section 3.1.\n12"
    }, {
      "heading" : "7 Conclusion and Future Work",
      "text" : "Weak completion semantics is based on a previously proposed approach that seems to adequately model the Wason selection task and Byrne’s suppression task. Yet it seems to adequately model another human reasoning task which includes the belief-bias effect. Taking our formalization as starting point, we showed by running examples the need and use for possible extensions in abductive reasoning. Introducing the concept of inspection points in our current framework by applying reserved (meta-)predication for all abductive ground atoms, makes it possible to differentiate between consumed and produced abducibles. This distinction allows us to implement the concepts of contextual side-effect, (jointly supported) contextual relevant consequences and contestable contextual side-effects. Belief bias can thus be modeled using mechanisms described and formalized more abstractly, which deal with contextual abductive reasoning by means of taking side-effects under consideration, applicable to a larger scope of problems, via the notion of inspection points (references to other published examples are given). Our abstract formalism opens therefore the way to a wider use, not restricted to psychological modeling. When examining abductive explanatory plausibility and contextual counterfactual reasoning new questions raise on whether new observations should be explained by possible or by necessary side-effects. They again might explain new and possibly unexpected side-effects. Additionally, we need to explore how to deal with inconsistency. Another aspect is about choosing the most appropriate explanations. In our examples, we follow Occam’s razor with respect to competing explanations, that is, we only consider the minimal explanations. However, there are other measures of preferences which might be more appropriate (e.g. from a human reasoning point of view) and which should be further investigated. In a future extension, some abducibles can be abduced in a three-valued way if we’re trying to make the top goal unknown. This is typical of fault-finding, which concerns two separate problems: (1) finding the abducibles which, if unknown for some faulty components, would make unknown the predicted output using the correct model of the artifact, that happens to be at odds with the faulty output behavior of the artifact; (2) abducing the faulty behavior of components (using a model of the artifact comprising the modeling of faults) to actually predict the faulty behavior. This requires abducing normality of a component as a default, and going back over that normality to change it to unknown, whose technicalities are beyond the scope of this paper. In a psychological modeling case, the artifact we might be aiming to find faults about can be some psychological model. We then may have available some real person’s specific behavior that the model is not consistent with, i.e. it wrongly predicts the negation of that behavior. Consequently, we want at least render our model consistent with that behavior by forcing some abducible or other to be unknown, and thus its specific prediction is unknown, rather than the negation of the person’s behavior. A second step is to improve and correct the model to make the right prediction. This is comparable to introducing a component’s faulty behavior model into a correct model of the artifact, whereas now the faulty artifact is the (incorrect) model, and the missing faulty component model refers to a missing model part that would produce the correct behavior prediction."
    }, {
      "heading" : "8 Acknowledgments",
      "text" : "We thank Pierangelo Dell’Acqua for his comments. This work was partly funded by DAAD’s IPID program, financed by the German Federal Ministry of Education and Research (BMBF).\n13"
    } ],
    "references" : [ {
      "title" : "Contributions to the theory of logic programming",
      "author" : [ "K.R. APT", "M.H. VAN EMDEN" ],
      "venue" : "Journal of the ACM 29, 3, 841–862.",
      "citeRegEx" : "APT and EMDEN,? 1982",
      "shortCiteRegEx" : "APT and EMDEN",
      "year" : 1982
    }, {
      "title" : "Suppressing valid inferences with conditionals",
      "author" : [ "R.M.J. BYRNE" ],
      "venue" : "Cognition 31, 61–83.",
      "citeRegEx" : "BYRNE,? 1989",
      "shortCiteRegEx" : "BYRNE",
      "year" : 1989
    }, {
      "title" : "Negation as failure",
      "author" : [ "K.L. CLARK" ],
      "venue" : "Logic and Data Bases, H. Gallaire and J. Minker, Eds. Vol. 1. Plenum Press, New York, NY, 293–322.",
      "citeRegEx" : "CLARK,? 1978",
      "shortCiteRegEx" : "CLARK",
      "year" : 1978
    }, {
      "title" : "A computational logic approach to the suppression task",
      "author" : [ "DIETZ", "E.-A.", "S. HÖLLDOBLER", "M. RAGNI" ],
      "venue" : "Proceedings of the 34th Annual Conference of the Cognitive Science Society, CogSci 2013, N. Miyake, D. Peebles, and R. P. Cooper, Eds. Austin, TX: Cognitive Science Society, 1500–1505.",
      "citeRegEx" : "DIETZ et al\\.,? 2012",
      "shortCiteRegEx" : "DIETZ et al\\.",
      "year" : 2012
    }, {
      "title" : "A computational logic approach to the abstract and the social case of the selection task",
      "author" : [ "DIETZ", "E.-A.", "S. HÖLLDOBLER", "M. RAGNI" ],
      "venue" : "Proceedings of the 11th International Symposium on Logical Formalizations of Commonsense Reasoning, COMMONSENSE 2013. Aeya Nappa, Cyprus.",
      "citeRegEx" : "DIETZ et al\\.,? 2013",
      "shortCiteRegEx" : "DIETZ et al\\.",
      "year" : 2013
    }, {
      "title" : "Modeling the suppression task under weak completion and well-founded semantics",
      "author" : [ "DIETZ", "E.-A.", "S. HÖLLDOBLER", "C. WERNHARD" ],
      "venue" : "Journal of Applied Non-Classical Logics.",
      "citeRegEx" : "DIETZ et al\\.,? 2013",
      "shortCiteRegEx" : "DIETZ et al\\.",
      "year" : 2013
    }, {
      "title" : "Reasoning and imagination",
      "author" : [ "J. EVANS" ],
      "venue" : "Thinking Twice: Two Minds in One Brain. OUP Oxford.",
      "citeRegEx" : "EVANS,? 2010",
      "shortCiteRegEx" : "EVANS",
      "year" : 2010
    }, {
      "title" : "Biases in deductive reasoning",
      "author" : [ "J. EVANS" ],
      "venue" : "Cognitive Illusions: A Handbook on Fallacies and Biases in Thinking, Judgement and Memory, R. Pohl, Ed. Psychology Press.",
      "citeRegEx" : "EVANS,? 2012",
      "shortCiteRegEx" : "EVANS",
      "year" : 2012
    }, {
      "title" : "On the conflict between logic and belief in syllogistic reasoning",
      "author" : [ "J. EVANS", "J.L. BARSTON", "P. POLLARD" ],
      "venue" : "Memory & Cognition 11, 3, 295–306.",
      "citeRegEx" : "EVANS et al\\.,? 1983",
      "shortCiteRegEx" : "EVANS et al\\.",
      "year" : 1983
    }, {
      "title" : "Philosophy of Natural Science",
      "author" : [ "C.G. HEMPEL" ],
      "venue" : "Prentice Hall, Englewood Cliffs, NJ.",
      "citeRegEx" : "HEMPEL,? 1966",
      "shortCiteRegEx" : "HEMPEL",
      "year" : 1966
    }, {
      "title" : "Logic programs under three-valued Łukasiewicz semantics",
      "author" : [ "S. HÖLLDOBLER", "C.D. KENCANA RAMLI" ],
      "venue" : "Logic Programming, 25th International Conference, ICLP 2009, P. M. Hill and D. S. Warren, Eds. Lecture Notes in Computer Science, vol. 5649. Springer, Heidelberg, 464–478.",
      "citeRegEx" : "HÖLLDOBLER and RAMLI,? 2009a",
      "shortCiteRegEx" : "HÖLLDOBLER and RAMLI",
      "year" : 2009
    }, {
      "title" : "Logics and networks for human reasoning",
      "author" : [ "S. HÖLLDOBLER", "C.D. KENCANA RAMLI" ],
      "venue" : "International Conference on Artificial Neural Networks, ICANN 2009, Part II, C. Alippi, M. M. Polycarpou, C. G. Panayiotou, and G. Ellinas, Eds. Lecture Notes in Computer Science, vol. 5769. Springer, Heidelberg, 85–94.",
      "citeRegEx" : "HÖLLDOBLER and RAMLI,? 2009b",
      "shortCiteRegEx" : "HÖLLDOBLER and RAMLI",
      "year" : 2009
    }, {
      "title" : "An abductive model for human reasoning",
      "author" : [ "S. HÖLLDOBLER", "T. PHILIPP", "C. WERNHARD" ],
      "venue" : "Logical Formalizations of Commonsense Reasoning, Papers from the AAAI 2011 Spring Symposium. AAAI Spring Symposium Series Technical Reports. AAAI Press, Cambridge, MA, 135–138.",
      "citeRegEx" : "HÖLLDOBLER et al\\.,? 2011",
      "shortCiteRegEx" : "HÖLLDOBLER et al\\.",
      "year" : 2011
    }, {
      "title" : "Abductive logic programming",
      "author" : [ "A.C. KAKAS", "R.A. KOWALSKI", "F. TONI" ],
      "venue" : "Journal of Logic and Computation 2, 6, 719–770.",
      "citeRegEx" : "KAKAS et al\\.,? 1993",
      "shortCiteRegEx" : "KAKAS et al\\.",
      "year" : 1993
    }, {
      "title" : "Computational Logic and Human Thinking: How to be Artificially Intelligent",
      "author" : [ "R. KOWALSKI" ],
      "venue" : "Cambridge University Press, Cambridge.",
      "citeRegEx" : "KOWALSKI,? 2011",
      "shortCiteRegEx" : "KOWALSKI",
      "year" : 2011
    }, {
      "title" : "O logice trójwartościowej",
      "author" : [ "J. ŁUKASIEWICZ" ],
      "venue" : "Ruch Filozoficzny 5, 169–171. English translation: On three-valued logic. In: Łukasiewicz J. and Borkowski L. (ed.). (1990). Selected Works, Amsterdam: North Holland, pp. 87–88.",
      "citeRegEx" : "ŁUKASIEWICZ,? 1920",
      "shortCiteRegEx" : "ŁUKASIEWICZ",
      "year" : 1920
    }, {
      "title" : "Hypothetical reasoning with well founded semantics",
      "author" : [ "L.M. PEREIRA", "J.N. APARÍCIO", "J.J. ALFERES" ],
      "venue" : "Scandinavian Conference on Artificial Intelligence: Proc. of the SCAI’91, B. Mayoh, Ed. IOS Press, Amsterdam, 289–300.",
      "citeRegEx" : "PEREIRA et al\\.,? 1991",
      "shortCiteRegEx" : "PEREIRA et al\\.",
      "year" : 1991
    }, {
      "title" : "Inspecting side-effects of abduction in logic programs",
      "author" : [ "L.M. PEREIRA", "A.M. PINTO" ],
      "venue" : "Logic Programming, Knowledge Representation, and Nonmonotonic Reasoning: Essays in honour of Michael Gelfond, M. Balduccini and T. C. Son, Eds. LNAI, vol. 6565. Springer, 148–163.",
      "citeRegEx" : "PEREIRA and PINTO,? 2011",
      "shortCiteRegEx" : "PEREIRA and PINTO",
      "year" : 2011
    }, {
      "title" : "Tabled abduction in logic programs",
      "author" : [ "A. SAPTAWIJAYA", "L.M. PEREIRA" ],
      "venue" : "Theory and Practice of Logic Programming 13, 4-5-Online-Supplement.",
      "citeRegEx" : "SAPTAWIJAYA and PEREIRA,? 2013",
      "shortCiteRegEx" : "SAPTAWIJAYA and PEREIRA",
      "year" : 2013
    }, {
      "title" : "Semantic interpretation as computation in nonmonotonic logic: The real meaning of the suppression task",
      "author" : [ "K. STENNING", "M. VAN LAMBALGEN" ],
      "venue" : "Cognitive Science 6, 29, 916–960.",
      "citeRegEx" : "STENNING and LAMBALGEN,? 2005",
      "shortCiteRegEx" : "STENNING and LAMBALGEN",
      "year" : 2005
    }, {
      "title" : "Human Reasoning and Cognitive Science",
      "author" : [ "K. STENNING", "M. VAN LAMBALGEN" ],
      "venue" : "A Bradford Book. MIT Press, Cambridge, MA.",
      "citeRegEx" : "STENNING and LAMBALGEN,? 2008",
      "shortCiteRegEx" : "STENNING and LAMBALGEN",
      "year" : 2008
    }, {
      "title" : "On the relationship between abduction and deduction",
      "author" : [ "P. TORASSO", "L. CONSOLE", "L. PORTINALE", "D.T. DUPRÉ" ],
      "venue" : "Journal of Logic and Computation 1, 5, 661–690.",
      "citeRegEx" : "TORASSO et al\\.,? 1991",
      "shortCiteRegEx" : "TORASSO et al\\.",
      "year" : 1991
    }, {
      "title" : "The well-founded semantics for general logic programs",
      "author" : [ "A. VAN GELDER", "K.A. ROSS", "J.S. SCHLIPF" ],
      "venue" : "Journal of the ACM 38, 3, 619–649.",
      "citeRegEx" : "GELDER et al\\.,? 1991",
      "shortCiteRegEx" : "GELDER et al\\.",
      "year" : 1991
    }, {
      "title" : "Reasoning about a rule",
      "author" : [ "P. WASON" ],
      "venue" : "Quarterly Journal of Experimental Psychology 20, 3, 273–281.",
      "citeRegEx" : "WASON,? 1968",
      "shortCiteRegEx" : "WASON",
      "year" : 1968
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "In (Evans 2010; Evans 2012), the conflict between logic and belief in human reasoning is discussed extensively.",
      "startOffset" : 3,
      "endOffset" : 27
    }, {
      "referenceID" : 7,
      "context" : "In (Evans 2010; Evans 2012), the conflict between logic and belief in human reasoning is discussed extensively.",
      "startOffset" : 3,
      "endOffset" : 27
    }, {
      "referenceID" : 6,
      "context" : "Our starting point is a psychological study carried out by Evans et al. (1983) about deductive reasoning which demonstrated possibly conflicting processes in human reasoning.",
      "startOffset" : 59,
      "endOffset" : 79
    }, {
      "referenceID" : 6,
      "context" : "Our starting point is a psychological study carried out by Evans et al. (1983) about deductive reasoning which demonstrated possibly conflicting processes in human reasoning. Participants were presented different syllogisms and had to decide whether they were logically valid. Consider one of them, Sadd : PREMISE1 No addictive things are inexpensive. PREMISE2 Some cigarettes are inexpensive. CONCLUSION Therefore, some addictive things are not cigarettes. Even though the conclusion does not necessarily follow from the premises, participants assumed the syllogism to be logically valid. They were explicitly asked to logically validate or invalidate these syllogisms, but didn’t seem to have the intellectual capability to do so. Even worse, they were not at all aware about their inabilities. Repeatedly, the majority of people were proffering with certainty of confidence the wrong answer. Evans et al. (1983) concluded that this happened because they were being unduly influenced by their own beliefs, their belief bias.",
      "startOffset" : 59,
      "endOffset" : 915
    }, {
      "referenceID" : 14,
      "context" : "Kowalski (2011)).",
      "startOffset" : 0,
      "endOffset" : 16
    }, {
      "referenceID" : 15,
      "context" : "We use the three-valued Łukasiewicz (1920) semantics.",
      "startOffset" : 24,
      "endOffset" : 43
    }, {
      "referenceID" : 3,
      "context" : "Weak completion semantics has first been introduced by Hölldobler and Kencana Ramli (2009a) and seems to adequately model Byrne’s (1989) suppression task (Dietz et al. 2012) and Wason’s (1968) selection task (Dietz et al.",
      "startOffset" : 154,
      "endOffset" : 173
    }, {
      "referenceID" : 4,
      "context" : "2012) and Wason’s (1968) selection task (Dietz et al. 2013).",
      "startOffset" : 40,
      "endOffset" : 59
    }, {
      "referenceID" : 2,
      "context" : "The resulting set of equivalences is called the completion of P (Clark 1978).",
      "startOffset" : 64,
      "endOffset" : 76
    }, {
      "referenceID" : 1,
      "context" : "Weak completion semantics has first been introduced by Hölldobler and Kencana Ramli (2009a) and seems to adequately model Byrne’s (1989) suppression task (Dietz et al.",
      "startOffset" : 122,
      "endOffset" : 137
    }, {
      "referenceID" : 1,
      "context" : "Weak completion semantics has first been introduced by Hölldobler and Kencana Ramli (2009a) and seems to adequately model Byrne’s (1989) suppression task (Dietz et al. 2012) and Wason’s (1968) selection task (Dietz et al.",
      "startOffset" : 122,
      "endOffset" : 193
    }, {
      "referenceID" : 1,
      "context" : "Weak completion semantics has first been introduced by Hölldobler and Kencana Ramli (2009a) and seems to adequately model Byrne’s (1989) suppression task (Dietz et al. 2012) and Wason’s (1968) selection task (Dietz et al. 2013). Consider the following transformation for given P : 1. Replace all clauses in P with the same head A ← body1, A ← body2, . . . by the single expression A ← body1 ∨ body2,∨ . . . . 2. if A ∈ undef(P) then add A ← ⊥. 3. Replace all occurrences of ← by ↔. The resulting set of equivalences is called the completion of P (Clark 1978). If Step 2 is omitted, then the resulting set is called the weak completion of P (wcP). Hölldobler and Kencana Ramli (2009b) showed that the model intersection property holds for weakly completed programs.",
      "startOffset" : 122,
      "endOffset" : 684
    }, {
      "referenceID" : 1,
      "context" : "Weak completion semantics has first been introduced by Hölldobler and Kencana Ramli (2009a) and seems to adequately model Byrne’s (1989) suppression task (Dietz et al. 2012) and Wason’s (1968) selection task (Dietz et al. 2013). Consider the following transformation for given P : 1. Replace all clauses in P with the same head A ← body1, A ← body2, . . . by the single expression A ← body1 ∨ body2,∨ . . . . 2. if A ∈ undef(P) then add A ← ⊥. 3. Replace all occurrences of ← by ↔. The resulting set of equivalences is called the completion of P (Clark 1978). If Step 2 is omitted, then the resulting set is called the weak completion of P (wcP). Hölldobler and Kencana Ramli (2009b) showed that the model intersection property holds for weakly completed programs. This guarantees the existence of a least model for every program. In computational logic, least models can often be computed as least fixed points of an appropriate semantic operator (Apt and van Emden 1982). Stenning and van Lambalgen (2008) devised such an operator which has been generalized for first-order programs by Hölldobler and Kencana Ramli (2009a): Let I be an interpretation and ΦP(I) = 〈J, J〉, where J= {A |there exists A ← body ∈ P with I(body) = ⊤}, J= {A |there exists A ← body ∈ P and for all A ← body ∈ P we find I(body) = ⊥}.",
      "startOffset" : 122,
      "endOffset" : 1008
    }, {
      "referenceID" : 1,
      "context" : "Weak completion semantics has first been introduced by Hölldobler and Kencana Ramli (2009a) and seems to adequately model Byrne’s (1989) suppression task (Dietz et al. 2012) and Wason’s (1968) selection task (Dietz et al. 2013). Consider the following transformation for given P : 1. Replace all clauses in P with the same head A ← body1, A ← body2, . . . by the single expression A ← body1 ∨ body2,∨ . . . . 2. if A ∈ undef(P) then add A ← ⊥. 3. Replace all occurrences of ← by ↔. The resulting set of equivalences is called the completion of P (Clark 1978). If Step 2 is omitted, then the resulting set is called the weak completion of P (wcP). Hölldobler and Kencana Ramli (2009b) showed that the model intersection property holds for weakly completed programs. This guarantees the existence of a least model for every program. In computational logic, least models can often be computed as least fixed points of an appropriate semantic operator (Apt and van Emden 1982). Stenning and van Lambalgen (2008) devised such an operator which has been generalized for first-order programs by Hölldobler and Kencana Ramli (2009a): Let I be an interpretation and ΦP(I) = 〈J, J〉, where J= {A |there exists A ← body ∈ P with I(body) = ⊤}, J= {A |there exists A ← body ∈ P and for all A ← body ∈ P we find I(body) = ⊥}.",
      "startOffset" : 122,
      "endOffset" : 1125
    }, {
      "referenceID" : 4,
      "context" : "1991) for tight logic programs (Dietz et al. 2013).",
      "startOffset" : 31,
      "endOffset" : 50
    }, {
      "referenceID" : 8,
      "context" : "As discussed in (Evans et al. 1983), humans seem to have a background knowledge or belief, which, in this context, assuming (4), we imply that",
      "startOffset" : 16,
      "endOffset" : 35
    }, {
      "referenceID" : 16,
      "context" : "3 This view on ICs corresponds to the definition applied for the well-founded semantics in (Pereira et al. 1991).",
      "startOffset" : 91,
      "endOffset" : 112
    }, {
      "referenceID" : 9,
      "context" : "As specified by the classical hypothetic-deductive method (Hempel 1966), scientific inquiry is carried out in three stages: hypotheses generation, prediction, and evaluation.",
      "startOffset" : 58,
      "endOffset" : 71
    }, {
      "referenceID" : 9,
      "context" : "As specified by the classical hypothetic-deductive method (Hempel 1966), scientific inquiry is carried out in three stages: hypotheses generation, prediction, and evaluation. One or more hypotheses may be generated, by abduction, to explain observed events. A generated set of hypotheses (or assumptions) can thence be employed for predicting unseen events by means of deduction, on the implicit condition of not making further abductions (cf. Pereira and Pinto (2011)).",
      "startOffset" : 59,
      "endOffset" : 469
    }, {
      "referenceID" : 21,
      "context" : "The approach presented in (Torasso et al. 1991) treats abduction with completion semantics and does not study side-effects.",
      "startOffset" : 26,
      "endOffset" : 47
    }, {
      "referenceID" : 13,
      "context" : "Following Kakas et al. (1993) we consider an abductive framework consisting of a program P as knowledge base, a collection of atoms A of abducibles syntactically represented by the set of the (positive and negative) facts for each undefined ground atom in P , a set of integrity constraints IC, and the logical consequence relation |= Ł , where P |= lmwc Ł F if and only if lmŁwcP(F ) = ⊤ for the formula F .",
      "startOffset" : 10,
      "endOffset" : 30
    }, {
      "referenceID" : 18,
      "context" : "Reuse of contextual abductions, by resorting to an implementation of tabled abduction for well-founded semantics, is reported in (Saptawijaya and Pereira 2013).",
      "startOffset" : 129,
      "endOffset" : 159
    }, {
      "referenceID" : 17,
      "context" : "In (Pereira and Pinto 2011), the authors present the concept of inspection points in abductive logic programming and show how one can employ it to investigate side-effects of interest in order to help choose among abductive solutions.",
      "startOffset" : 3,
      "endOffset" : 27
    }, {
      "referenceID" : 17,
      "context" : "For this purpose we investigate and apply inspection points, originally presented by Pereira and Pinto (2011). In (Pereira and Pinto 2011), the authors present the concept of inspection points in abductive logic programming and show how one can employ it to investigate side-effects of interest in order to help choose among abductive solutions.",
      "startOffset" : 85,
      "endOffset" : 110
    }, {
      "referenceID" : 17,
      "context" : "One should observe that for a treatment of inspection points for all literals in a program and not just the abducible ones, we would simply need to adopt the program transformation technique of Pereira and Pinto (2011), which recursively relays inspection of non-terminal literals to the base inspection of terminals.",
      "startOffset" : 194,
      "endOffset" : 219
    } ],
    "year" : 2014,
    "abstractText" : "The belief bias effect is a phenomenon which occurs when we think that we judge an argument based on our reasoning, but are actually influenced by our beliefs and prior knowledge. Evans, Barston and Pollard carried out a psychological syllogistic reasoning task to prove this effect. Participants were asked whether they would accept or reject a given syllogism. We discuss one specific case which is commonly assumed to be believable but which is actually not logically valid. By introducing abnormalities, abduction and background knowledge, we adequately model this case under the weak completion semantics. Our formalization reveals new questions about possible extensions in abductive reasoning. For instance, observations and their explanations might include some relevant prior abductive contextual information concerning some side-effect or leading to a contestable or refutable side-effect. A weaker notion indicates the support of some relevant consequences by a prior abductive context. Yet another definition describes jointly supported relevant consequences, which captures the idea of two observations containing mutually supportive side-effects. Though motivated with and exemplified by the running psychology application, the various new general abductive context definitions are introduced here and given a declarative semantics for the first time, and have a much wider scope of application. Inspection points, a concept introduced by Pereira and Pinto, allows us to express these definitions syntactically and intertwine them into an operational semantics.",
    "creator" : "LaTeX with hyperref package"
  }
}