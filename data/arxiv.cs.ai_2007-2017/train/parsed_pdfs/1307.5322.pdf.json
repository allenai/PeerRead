{
  "name" : "1307.5322.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n30 7.\n53 22\nv1 [\ncs .A\nI] 1"
    }, {
      "heading" : "1 Introduction",
      "text" : "As ontologies became more prevalent and extensively used in domains such as biomedicine and geography, there is a growing need to automatically discover semantic correspondences between ontologies, through ontology matching [4, 7, 9, 17], in order to pursue the goal of a semantic web [16]. This is especially relevant when a lack of coordination in ontology development results in the independent creation of ontologies for the same or related domains. The widely use Web Ontology Language (OWL) provides a way to represent ontologies with a well-defined semantics, which could include mappings between other ontologies.\nIn recent years, there has been a growing interest in efficient and effective matching methods for large ontologies [12, 15, 13, 18, 6].\nThe Ontology Alignment Evaluation Initiative (OAEI) [8] has been the major playfield for ontology alignment, with the participation of state of the art ontology matching\nsystems in several ontology alignment challenges. After the recent introduction of the large biomedical track, an important finding of OAEI is that, although ontology matching can be seen as an offline process, some systems are not scalable enough to handle large ontologies and usually run out of memory. Another important finding is that most of the alignments produced are incoherent, i.e. lead to unsatisfable classes or properties. With respect to large ontologies alignments, the degree of incoherency is typically higher, and only one participant, LogMap [11, 12], detects incoherencies and uses repair techniques to improve the quality of the resulting alignment. The goal of a repairing process is to restore coherency by minimally changing the input. However, reasoning-based techniques aggravate the scalability problem, which restricts their application with more effective and complex matching strategies.\nTo the best of our knowledge, there are only two systems that perform alignment repair: LogMap and ALCOMO [15]. Besides performing repair operations during the matching process, LogMap provides a repair facility that applies a local repairing process over the input alignment. This process is incomplete, i.e. it may produce an incoherent alignment, but overcomes the scalability problem. ALCOMO is a repair system that provides a complete global repair process, but fails to handle large ontologies. A more efficient incomplete process is also provided, but it continues to fail for some large ontologies alignments (see Sections 4 and 5 for more details).\nThe OAEI large biomedical track consists of finding alignments between the Foundational Model of Anatomy (FMA), SNOMED CT, and the National Cancer Institute Thesaurus (NCI). These ontologies are semantically rich and contain tens of thousands of classes. Since there is no gold standard alignment, a silver standard alignment based on the UMLS Metathesaurus [1] is provided for evaluating each matching problem. Repaired versions of the silver standard alignments produced by the repair facility of LogMap and ALCOMO are also provided for evaluating the systems in competition.\nAfter analyzing the results provided by the repair facilities of LogMap and ALCOMO with respect to large biomedical track we identify two main problems: (1) ALCOMO and LogMap failed to repair all the incoherencies caused by disjointness restrictions between classes, which are the main cause of incoherency in alignments; and (2) in some cases, ALCOMO and LogMap are far from minimizing the set of mappings removed from the alignments.\nIn this paper, we propose a new repair algorithm that minimizes both the incoherence of the resulting alignment and the number of matches removed from the input alignment. To overcome the scalability problem, we use heuristics to determine nearoptimal solutions, and filtering methods that take advantage of the confidence values of the mappings. Moreover, we introduce a modularization based technique that allows the extraction of the core fragments of the ontologies that contain only the necessary classes and relations for repairing all the incoherencies caused by disjoint restrictions.\nThe paper is organized as follows: Section 2 describes our setting and introduces the notation used. Section 3 presents our module for the extraction of core fragments and its properties. Section 4 describes our repair algorithm and main methods. Section 5 presents and discusses the obtained results; and finally Section 6 is dedicated to final remarks and future work."
    }, {
      "heading" : "2 Our Setting",
      "text" : "We use A, B, ...X, Y, Z to denote classes, O,O′,O1,O2, ... to denote ontologies andM,M′, ... to denote sets of mappings, also called an alignment, between classes.\nIn an ontology matching setting we say that an alignment M between ontologies O1 and O2 is coherent if there is no class or property in O1 or O2 that is unsatisfiable due to M (see [15] for a formal definition).\nWith respect to superclass relations we use A ⊑ B and A ⊑d B to denote superclass and direct superclass relations between classes, respectively. A class B is a direct superclass of A if A ⊑ B and there is no C such that A ⊑ C, C ⊑ B and B @ C. The last condition was added due to the possible existence of cycles.\nWe assume that ontologies are coherent and don’t have cycles with respect to subclass relation between classes. The semantic inference is denoted by the symbol . For instance, given two ontologies O1 and O2, and a set of mapping M, we write O1 ∪ O2 ∪ M A ⊑ B to denote that A ⊑ B is inferred with respect to the resulting merged ontology. To denote conjunction we use the symbol ∧. With respect to incoherency detection, given two disjoint classes B and C, we say that a class A is incoherent if O1 ∪ O2 ∪M (A ⊑ B) ∧ (A ⊑ C). Since we assume that ontologies are coherent, we also say that M is incoherent.\nOur analysis of the alignments produced for the OAEI large biomedical track by the participant ontology matching systems show that most of the incoherency found is caused by disjointness restrictions. For this reason, we only consider incoherent alignments due to subclass/disjointness conflicts. That is, when a class is subsumed by disjoint classes due to the alignment. Thus, our incoherency detection is incomplete. Moreover, as LogMap, we just consider named classes, and sub/superclass and equivalent relations between them during the incoherence detection and repair process. We followed this strategy to ensure scalability while still improving the coherency degree of the alignments.\nAn implementation of our algorithms was done as part of AgreementMakerLight [3], a lightweight version of AgreementMaker [2], a successful ontology matching platform. During the development of our algorithms we took into account the very efficient and scalable methods provided by AgreementMakerLight to explore the relationship information of the input ontologies. For instance, the cost of checking if a class is subsumed by another class becomes negligible using the AgreementMakerLight HashMaps-based data structures."
    }, {
      "heading" : "3 Ontology Modularization",
      "text" : "In order to resolve an incoherence we need to determine which mappings are culprits. The determination of all possible culprits represents a very demanding task to be performed when dealing with large ontologies. Ontology modularization techniques have been proposed and implemented to overcome the issue of scalability [10, 12, 5].\nIn our work we also use modularization techniques. We introduce the following extraction module that suits our repair setting.\nDefinition 1 Let O1 and O2 be ontologies, M a set of mappings, O′1 ⊆ O1 and O′2 ⊆ O2. O′1 and O′2 are core fragments of O1,O2 and M if they satisfy the following conditions:\n1. if A and B are disjoint classes of O1 ∪ O2 then {A, B} ⊆ O′1 ∪ O′2;\n2. if A is a class and occurs in M then A ∈ O′1 ∪ O′2;\n3. if A is a class of O1 ∪ O2 such that\n(a) O1 ∪ O2 ∪M (A ⊑d B) ∧ (A ⊑d C), where B and C are distinct classes, and;\n(b) there is no class D that satisfies O1∪O2∪M (D ⊑d B′)∧(D ⊑d C′), where B′ and C′ are distinct classes, O1 ∪ O2 ∪M D ⊑ A and O1 ∪ O2 ∪M 2 A ⊑ D,\nthen A ∈ O′1 ∪ O′2;\n4. if O1 ∪ O2 ∪M A ⊑ B and {A, B} ⊆ O′1 ∪ O′2, then O′1 ∪ O′2 ∪M A ⊑ B;\nWe also called the checkset of M the set of classes that satisfy Condition (3).\nThe idea behind the presented module is to compute fragments, smaller than the original ontologies, that still allow the determination of all possible culprits of incoherencies. The module defines a set of core classes composed of: the classes that occur in a disjoint relation (Condition 1) or in a mapping (Condition 2), and; the classes that have more than one direct superclass and don’t have a subclass with more than one direct superclass (Condition 3). Condition (4) guarantees that the subclass relations between core classes are maintained.\nThe following proposition shows that all the mappings responsible for incoherencies between two matched ontologies can be determined using the respective core fragments.\nProposition 1 Let O1 and O2 be ontologies, M a set of mappings, O′1 and O′2 the respective core fragments, M′ ⊆ M, B and C disjoint classes.\nThere is a class A such that O1 ∪ O2 ∪M′ (A ⊑ B) ∧ (A ⊑ C) if and only if there is a class A′ such that O′1 ∪ O′2 ∪M′ (A′ ⊑ B) ∧ (A′ ⊑ C).\nProof. (“→”) (reductio ad absurdum) Let us assume there is A such that O1 ∪O2 ∪ M′ (A ⊑ B)∧(A ⊑ C) but there is no A′ such thatO′1∪O′2∪M′ (A′ ⊑ B)∧(A′ ⊑ C). Thus, A′ < O′1 ∪ O′2. There are two cases:\n1. If O1 ∪ O2 ∪ M′ B ⊑ C (the C ⊑ B case is analogous) then by Conditions (1) and (4) of Definition 1 we have that O′1 ∪ O′2 ∪ M′ (B ⊑ B) ∧ (B ⊑ C). Contradiction.\n2. Otherwise, there is a class X with more than one direct superclass such that O1 ∪ O2 ∪M ′ (A ⊑ X) ∧ (X ⊑ B) ∧ (X ⊑ C). If X ∈ O′1 ∪ O′2 then we have\na contradiction. If X < O′1 ∪ O′2 then by Condition (3) of Definition 1 there is a class Y ∈ O′1 ∪ O′2 and O1 ∪ O2 ∪M′ Y ⊑ X. By Condition (4) we have that O′1 ∪ O ′ 2 ∪M ′ (Y ⊑ B) ∧ (Y ⊑ C). Contradiction.\n(“←”) Trivial since A′ ∈ O1 ∪ O2.\nProposition 1 is mainly based on the fact that if a class is incoherent with respect to a disjoint then it must have a superclass with more than one direct superclass. Unless one of the disjoint classes subsumes the other class.\nMoreover, given Proposition 1 result, a checkset (Definition 1) denotes a complete set of classes to check the coherency of a mapping set wrt to disjoint restrictions.\nTable 1 shows the size of the core fragments computed for each of the matching problems of the OAEI large biomedical track. In all of the matching problems the size of the core fragments is significantly smaller than the original ontologies. In comparison to the module proposed by [10] and implemented by LogMap2, which computes fragments that contain 37% of the classes in FMA and 38% of the classes in NCI, there is a considerably improvement - only 5% of the total classes of FMA and NCI.\nGiven the previous result, the checkset denotes a set of classes that need to be checked for incoherencies. This way, instead of looking of all the culprits for each incoherent class of the input ontologies, we just need to look for the culprits for each incoherent class in the checkset. Table 1 shows the size of the computed checkset is also significantly smaller than the size of the respective input ontologies."
    }, {
      "heading" : "4 Alignment Repair",
      "text" : "Given an incoherent alignment, the goal of a repair procedure is to remove mappings from the input alignment in such way that the resulting set is coherent. Typically, a repair procedure ensures minimal impact on the input by, for instance, minimizing the number of removed mappings or the sum of confidence values of the removed mappings. There are two main approaches to alignment repair: global and local.\nA global repair determines the minimal impact by considering all the classes and relations of the matched ontologies. Although this approach produces better results, it is usually not scalable for large ontologies. This approach is followed by ALCOMO.\nA local repair is performed by determining the minimal impact in small subsets of the matched ontologies. This approach is more efficient, but produces a bigger impact in the input alignment than the global approach. LogMap follows this approach and applies it during its ontology matching process.\nOur repairing process is divided in three main tasks: the computation of the conflict set of mappings; the filtering of conflict sets; and finally, the removal of mappings."
    }, {
      "heading" : "4.1 Conflict sets of mappings",
      "text" : "Our implementation takes advantage of the fragments extraction proposed in Section 3, but also of the AgreementMakerLight data structures. In order to compute all the possible culprits of an incoherency, for each class in the checkset we do a full depthfirst search in the core fragments structure. This way, we are able to determine all the minimal sets of mappings, called conflict sets, that are culprits of the coherencies.\nFormally, given ontologies O1 and O2, and a set of mappings M we compute for each checkset class A and disjoint classes B and C, the minimal set of mappings M′ ⊆ M such that O1 ∪ O2 ∪M′ (A ⊑ B) ∧ (A ⊑ C).\nNotice that, in order to remove all the found incoherencies, we need to remove at least one mapping from each conflict set. Using a global approach, the goal is determine a minimal set of mapping that intersect all conflict sets. This way, we are able to minimize the number of removed mappings."
    }, {
      "heading" : "4.2 Filtering",
      "text" : "Ontology matching systems typically provide alignments with confidence values, between 0 and 1, associated to each of its mappings. These values are computed during the ontology matching and they are typically good reliability indicators. They can also be used in the repairing process when, for instance, we need to decide which mapping to remove in a conflict set.\nOur repair algorithm uses that information to resolve possible ties during the selection process (see Section 4.3) but also uses it to perform an initial filtering of the conflict sets. The main idea is to resolve conflict sets that appear to have a straightforward solution based on the respective confidence values. For instance, when a conflict set contains a mapping with a very low confidence value with respect to the other mappings in the set. The problem consists in establishing a value for which the lowest confidence value in a conflict set should be compared with the other confidence values. Since this value should indicate how reliable are the confidence values, we call it confidence interval. Thus, given a confidence interval ǫ, we filter all the conflicts sets by: (1) ordering them by their highest confidence mapping, and then; (2) removing the lowest confidence mapping if there is no other mapping within its confidence interval. That is, given the lowest and the second lowest confidence values c1 and c2, the lowest confidence mapping is removed if c1 + ǫ < c2 − ǫ."
    }, {
      "heading" : "4.3 Removing Mappings",
      "text" : "Given all conflicting sets (or only part of them after filtering) we need to determine which set of mappings should be removed. The task of computing a global minimal set of mappings, which corresponds to computing a minimal set of mappings that intersect all conflicting sets, is non-scalable. For this reason we employed two main approaches: (1) compute all disjoint clusters of conflicting sets. That is, we divide the initial set of conflicting sets into sets of conflicting sets that have at least one mapping in common. This way we are able to determine the mappings to be removed for each of these clusters independently. In some cases, this allows us to check if the resulting repair is in fact a global minimal. However, since it is not scalable approach and some matching problems have a huge number of conflict sets, it may not applicable to every case. For instance, with respect to OAEI large biomedical track and UMLS-based reference alignments (see Table 1) we computed 54 and 3 initial independent clusters for the FMA-NCI and FMA-SNOMED matching problems, respectively. For the SNOMEDNCI case weren’t able to employ this approach due to efficiency issues. (2) compute\nand remove the mappings that belongs to the highest number of unresolved conflict sets. This heuristics is very efficient and typically delivers the optimal solution because usually the mapping that belongs to the highest number of conflicts sets also belongs to the optimal solution. A similar strategy has been applied for repairing inconsistent databases [14]. However, when there are many mappings that belong to approximately the same number of conflicts sets, this heuristics fails to return the optimal solution. To overcome part of this problem, we resolve possible ties by performing a depth-firstsearch to determine which alternative resolves the highest number of conflict sets. The depth of this search is pre-defined."
    }, {
      "heading" : "4.4 The Repair Algorithm",
      "text" : "Algorithm 1 shows a description of our repair algorithm. Its input consists of: (1) a list of conflicting sets of mappings, C. This list contains all the conflicting sets for a given pair of ontologies and input alignment, as described in Section 4.1. Thus, instead of taking as input the matched ontologies, the core algorithm receives the corresponding conflicting sets; (2) the initial set of mappings, setMaps. This set is used to keep track of the removed mappings and to be returned after the repairing process; (3) a confidence interval, ǫ, for which a filtering will be performed as described in Section 4.2; (4) a search depth value, sDepth. This value establishes the depth of search when dealing with ties as described in Section 4.3, and finally; (5) a boolean, dis jCon f licts, that sets if the clusters of disjoint conflict sets are computed during the repair process, as described in 4.3.\nThe algorithm starts by checking if the initial filtering is performed. If so, the method FilterCon f licts inputs the list of conflict sets and the confidence interval, and returns a filtered list of conflicting sets of mappings as described in Section 4.2.\nIn the case that dis jCon f licts is set to true, an initial computation of the clusters of disjoint conflicting sets is performed. Notice, that this method returns a set of clusters of conflicting sets of mappings.\nThen, we enter in the main cycle of the algorithm, which will run until there is no unresolved conflicting set. In each of the steps, one cluster is selected to be resolved. In the case that dis jCon f licts is set to f alse, PC will always contain only one element until all the conflicting sets are resolved. Given the selected cluster, the selection of which mapping to delete is performed by the method WorstMapping, as described in Section 4.3. A description of this method is shown in Algorithm 2.\nAfter removing the selected mapping, the conflicting sets that contain the removed mapping are marked as resolved and removed from the respective lists. This task is performed by the method RemoveMapping. If the dis jCon f licts is set to true a clustering process is performed over the remaining conflicting sets."
    }, {
      "heading" : "5 Evaluation and Discussion",
      "text" : "In this section we identify the results produced by our implementation as AMLR. Our evaluation was done in a server with 16Gb of RAM. However, all the alignments pro-\nProcedure: Repair Input: C : List of conflicting sets of mappings; setMaps : A set of mappings; ǫ : A confidence interval; sDepth : search depth; dis jCon f licts : a boolean Output: A set of repaired mappings.\n1: if ǫ ≥ 0 then 2: C := FilterConflicts(C, ǫ) 3: end if 4: if dis jCon f licts = true then 5: PC := DisjointConflictsSets(C) 6: else 7: PC := {C} 8: end if 9: while |PC| > 0 do\n10: S := an element of PC 11: PC := PC \\ S 12: w : = WorstMapping(S, setMaps, sDepth) 13: setMaps : = setMaps \\ w 14: S : = RemoveMapping(S, w) 15: if |S| > 0 and dis jCon f licts = true then 16: PS := DisjointConflictLists(S) 17: PC := PC ∪ PS 18: else if |S| > 0 then 19: PC := {S} 20: end if 21: end while 22: return setMaps\nAlgorithm 1: Description of the repair algorithm.\nduced by AMLR can be produced using a 4Gb of RAM desktop without running out of memory.\nWe conducted experiments using the three OAEI large biomedical track matching problems: FMA-NCI, FMA-SNMD and SNMD-NCI (see Table 1 for details). We also considered the UMLS-based reference alignments that are used to evaluate the OAEI competitors systems, and their repaired versions produced by ALCOMO and the repair facility of LogMap. Since the last OAEI competition, a new version of LogMap was presented, LogMap2. For this reason, we also performed the evaluation with respect to the repair facility of LogMap2. To evaluate the precision and recall more accurately, we also consider the OAEI Anatomy Track problem for which there is a more accurate and coherent reference alignment.\nWith respect to the efficiency of our implementation, the time of execution is directly related to the number of conflict sets. The repair of the UMLS-based reference alignments of FMA-NCI, FMA-SNMD and SNMD-NCI, took less than 10 seconds, 15 minutes and 3 hours, respectively. The repair of the alignments produced by LogMap and LogMap2 for SNMD-NCI were executed in less than 45min. However, consider-\nTotal UMLS-based Align Core Fragments Checkset FMA - NCI 145712 3024 7325 (5%) 4159 (3%)\nFMA - SNMD 201452 9008 42875(21%) 29855 (15%) SNMD - NCI 189188 18844 63492(34%) 42918 (23%)\ning that ontology matching can be seen as an offline process, these are quite satisfactory results.\nIn order to check the degree of coherency of the alignments we use the JENA API and Pellet OWL Reasoner. This is a very memory and processing intensive task, requiring the use of the 8 core 16 GB server. For instance, it took more than 10 hours on average to check the coherency of an alignment produced for the FMA - SNMD matching problem.\nWe divide our evaluation in two main parts: (1) we evaluate AMLR by repairing the UMLS-based alignments provided for the OAEI Large biomedical Track, and comparing the number of mappings removed and the degree of coherency with the correspondent repairs produced by LogMap, LogMap2 and ALCOMO. (2) we evaluate the precision, recall and coherency degree of AMLR by repairing the alignments produced by OMSZ for the OAEI Large biomedical and Anatomy Tracks. We also compare these results with the repairs produced by LogMap2 and ALCOMO."
    }, {
      "heading" : "5.1 Repairing Silver Standard Alignments",
      "text" : "The construction of a gold or a silver standard alignment for an ontology matching problem is a very complex task. Even after several automated and manual refinements, alignments still contain errors or incomplete information. In the case of large ontologies that problem is even bigger since manually refinement becomes impractical. The OAEI Large biomedical track uses a silver standard alignment built from the UMLS Metathesaurus. Since the resulting silver standard alignment was incoherent, repaired versions of the alignment were produced by ALCOMO and LogMap, and used to evaluate the competing matching systems. Notice that the given silver standard alignment produced does not have confidence values associated to each of the mappings. Thus, the repair algorithms can not take advantage of that information.\nIn this context, we evaluate the quality of AMLR repairs by: (1) determining the degree of incoherency of the alignment by counting the number of incoherent classes; (2) determining the impact in the input alignment by counting the number of removed mappings; (3) comparing its results with ALCOMO, LogMap and LogMap2; (4) using AMLR to improve the results of ALCOMO, LogMap and LogMap2.\nWith respect to size of the conflict sets of mappings, we computed 931, 25351 and 73515 conflict sets for FMA-NCI, FMA-SNOMED and SNOMED-NCI matching problems, respectively. Notice that, given Proposition 1 these sets include all the possible culprits of an incoherency caused by a disjoint restriction.\nTable 2 shows the result of this evaluation."
    }, {
      "heading" : "5.1.1 FMA-NCI",
      "text" : "With respect to the number of mappings AMLR and LogMap2 produce close results, with 2901 and 2902 mappings, respectively. ALCOMO removes 80 mappings more. However, with respect to incoherency, ALCOMO produces a repair with only 10 incoherent classes, the same number as AMLR. LogMap and LogMap2 produce alignments with a high number of incoherent classes. Thus, AMLR produces the best results with respect to number of mappings removed and the coherence degree.\nTo show that the repaired alignment provided by the other systems could be improved by AMLR, we also repaired their respective alignments. The results show that AMLR considerably improves the incoherence degree of LogMap and LogMap2 by reducing it to 10 incoherent classes, as AMLR and ALCOMO. Moreover, AMLR produce optimal and near-optimal repairs for LogMap and LogMap2 repaired alignments, respectively. This was possible by applying the cluster strategy described in Section 4.3.\nWith respect to ALCOMO, AMLR did not remove any mappings, which was expected since ALCOMO already had the same number of incoherent classes as AMLR.\nMoreover, we were able to produce an optimal repair for LogMap case, and, at least, near-optimal minimal repairs for the remaining alignments produced by LogMap, LogMap2 and ALCOMO."
    }, {
      "heading" : "5.1.2 FMA-SNMD",
      "text" : "With respect to the number of mappings AMLR produces by far the best results, with 8349 mappings. The second best is ALCOMO with 8132. With respect to incoherency, AMLR is the only one that produces a fully coherent alignment. Moreover, only ALCOMO produces a comparable lower number of incoherent classes. LogMap and LogMap2 did not produce a quality alignment.\nIn this case we also repaired the resulting alignments of the other systems. In all of the cases we are able to considerably improve their results. For instance, by removing 6, 4 and 14 mappings from the LogMap, Logmap2 and ALCOMO alignments, respectively, we were able to achieve fully coherent alignments."
    }, {
      "heading" : "5.1.3 SNMD-NCI",
      "text" : "The SNMD-NCI task is very demanding in terms of memory, so both ALCOMO and our incoherency check were unable to provide results. This was excepted since the UMLS-based alignment for this matching problem has more than double the number of mappings with respect to the FMA-SNMD case, which already took an average of 10 hours to verify the coherency of each alignment.\nNevertheless, with respect to the number of mappings AMLR produced an alignment with less mappings than LogMap and LogMap2. However, by applying AMLR over the repairs produced by LogMap and LogMap2 we also obtained a lower number of mappings. Given the results of FMA-NCI and FMA-SNMD cases, this indicates that those alignment have a much higher degree of incoherence. For instance, AMLR removes 324 mappings from the LogMap alignment, which indicates that the majority of the incoherencies found by AMLR were still in LogMap alignment.\nThis evaluation clearly shows that AMLR obtains the best results with respect to the impact on the input alignment, and with respect to incoherency. Moreover, they also show that AMLR provides a better alternative for obtaining a more accurate silver standard alignment for the OAEI Large biomedical Track."
    }, {
      "heading" : "5.2 Repairing alignments",
      "text" : "Besides ensuring the coherency of an alignment, a repair procedure is also used to improve the quality of alignment in terms of f-measure. Since by its nature the repair procedure can not improve the recall, its goal is to improve precision without decreasing recall. Thus, its application produces better results when the input alignment has low precision.\nTo evaluate the impact of AMLR on the f-measure of the input alignments we consider the alignments produced by OMSZ for OAEI Anatomy and Large biomedical Tracks. With respect to the Anatomy track we use the gold standard alignment provided, which is coherent and regarded as accurate. In this case we use an alignment produced by OMSZ in an initial phase of its matching process, where precision is low. With respect to the Large biomedical track, since we show in Section 5.1 that AMLR produces much better results than the remaining systems, we used the repaired alignments produced by AMLR for the UMLS-based alignments as the reference alignments.\nWe also evaluate the results of an initial filtering of the conflicting sets as described in Section 4.2. For this purpose, we compare the results of AMLR with four different settings: no filtering, and filtering with confidence intervals of 0.1, 0.05 and 0.0, respectively. Notice that with the confidence interval set to 0.0 all the conflicting sets that have mappings with distinct confidence values will be filtered.\nTables 3, 4 and 5 show the results of this evaluation."
    }, {
      "heading" : "5.2.1 Anatomy",
      "text" : "Given the coherency degree of the resulting alignments (see Table 3), we conclude once again that AMLR produces the best results, with 0 incoherent classes. LogMap2 produces an alignment with almost as many incoherent classes as the initial non-repaired alignment.\nWith respect to f-measure values, since the initial alignment is small the resulting values are closer to the initial alignment values. However, it is AMLR who produces\nthe best results, 67.1% f-measure in one of its settings, which represents a significant 0.7% improvement over the initial f-measure value. Notice that the worst of the four settings of AMLR has the same f-measure as ALCOMO, 66.7%, and still a better fmeasure than LogMap2, 66.6%."
    }, {
      "heading" : "5.2.2 FMA-NCI",
      "text" : "In this evaluation (see Table 4) ALCOMO and AMLR produced similar results with respect to coherency and f-measure. These results were excepted since the initial alignment already had a high precision value. Both ALCOMO and AMLR produce alignments with 83.8% f-measure or more, and with only 2 incoherent classes. LogMap2 produces the worst results by producing an alignment with 147 incoherent classes and the lowest f-measure."
    }, {
      "heading" : "5.2.3 FMA-SNOMED",
      "text" : "In this case ALCOMO didn’t finish after 10 hours and, thus, didn’t provide any result. The results show (see Table 5) that the different settings of OMSZ may produce very distinct results. For instance, by not applying any filter, OMSZ produces an alignment with a f-measure 1.1% higher than the initial alignment. However, by applying a filter\nwith a confidence interval of 0.0 or 0.05, OMSZ produces a worst alignment with respect to f-measure. LogMap2 also produces an alignment with a lower f-measure than the initial alignment.\nThe contrasting results produce by the different confidence intervals can be explained by the number of conflicting sets filtered. In this case, we filter 2845, 6398, 13832 conflicting sets on a total of 13932 with respect the confidence intervals of 0.1, 0.05 and 0.0, respectively. In the case of a confidence interval of 0.0 most of the conflicting sets were filtered. Thus, given the high number of conflicting sets, this filtering produced an alignment with a lower recall.\nWith respect to the coherency degree, as in the previous cases, OMSZ produces much better results than the initial alignment and LogMap2."
    }, {
      "heading" : "5.2.4 SNOMED-NCI",
      "text" : "In this case ALCOMO ran out of memory and, thus, didn’t provide any result. As in Section 5.1.3 we were not able to determine the coherency degree of the alignments.\nThe results show (see Table 6) that OMSZ produces better results than the initial alignment. Its best settings produce an alignment with a 0.9% f-measure improvement. LogMap2 also improves the f-measure of the initial alignment, but by just 0.3%.\nWith respect to different settings of AMLR tested, the results show that is not clear how to set the confidence interval. However, it is clear that by filtering the conflicting sets we can obtain better and more efficient results."
    }, {
      "heading" : "6 Conclusions and Future Work",
      "text" : "In this paper we presented a new modularization based technique to extract the core fragments of the ontologies involved in alignment incoherencies, and a new repair algorithm that uses heuristics and filtering strategies to determine near-optimal solutions to provide a coherent alignment.\nWe did an extensive evaluation where we compared our implementation to the state of the art repairing systems. The results show that our repair implementation produces better results with respect to coherency, i.e. number of incoherent classes, and impact in the input alignment, i.e the number of mappings removed. In fact, our implementation produced remarkably better results than the repaired silver standard alignments of the OAEI Large biomedical Track. Thus, proving to be a better alternative for producing coherent silver standard alignments.\nThe results also show that our filtering strategy can obtain good results when mappings are associated with confidence values. However, the selection of an optimal confidence intervalf is not straightforward.\nAs ongoing work, we are adding parallel strategies to our implementation to take advantage of the current multi-core computers, and, hence, to improve the efficiency of the repairing process. The increase in the efficiency could also be used to achieve better results by, for instance, performing a deeper search when looking for the mapping to be removed. We are also integrating our repair algorithm in OMSZ. Our aim is to create a repair module in OMSZ that can be called during the matching process, to overcome the loss of recall caused by applying repair on the final alignment only.\nAs for future work, we want to consider for repair other restrictions and properties between classes besides disjoint restriction (e.g. allValuesFrom and someValuesFrom OWL restrictions)."
    } ],
    "references" : [ {
      "title" : "The unified medical language system (umls): integrating biomedical terminology",
      "author" : [ "O. Bodenreider" ],
      "venue" : "Nucleic Acids Research, 32(Database-Issue):267–270,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Agreementmaker: efficient matching for large real-world schemas and ontologies",
      "author" : [ "I.F. Cruz", "F.P. Antonelli", "C. Stroe" ],
      "venue" : "Proc. VLDB Endow., 2(2):1586–1589, Aug.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "The agreementmakerlight ontology matching system",
      "author" : [ "E.S.M.P.I.C. Daniel Faria", "Catia Pesquita", "F. Couto" ],
      "venue" : "In The 12th International Conference on Ontologies, DataBases, and Applications of Semantics, ODBASE",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2013
    }, {
      "title" : "Matching directories and owl ontologies with aroma",
      "author" : [ "J. David", "F. Guillet", "H. Briand" ],
      "venue" : "Proceedings of the 15th ACM international conference on Information and knowledge management, CIKM ’06, pages 830–831, New York, NY, USA,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Ontology module extraction for ontology reuse: an ontology engineering perspective",
      "author" : [ "P. Doran", "V.A.M. Tamma", "L. Iannone" ],
      "venue" : "M. J. Silva, A. H. F. Laender, R. A. Baeza-Yates, D. L. McGuinness, B. Olstad, Ø. H. Olsen, and A. O. Falcão, editors, CIKM, pages 61–70. ACM,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Instance-based matching of large ontologies using locality-sensitive hashing",
      "author" : [ "S. Duan", "A. Fokoue", "O. Hassanzadeh", "A. Kementsietsidis", "K. Srinivas", "M.J. Ward" ],
      "venue" : "P. Cudré-Mauroux, J. Heflin, E. Sirin, T. Tudorache, J. Euzenat, M. Hauswirth, J. X. Parreira, J. Hendler, G. Schreiber, A. Bernstein, and E. Blomqvist, editors, International Semantic Web Conference (1), volume 7649 of Lecture Notes in Computer Science, pages 49–64. Springer,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Yam: a schema matcher factory",
      "author" : [ "F. Duchateau", "R. Coletta", "Z. Bellahsene", "R.J. Miller" ],
      "venue" : "Proceedings of the 18th ACM conference on Information and knowledge management, CIKM ’09, pages 2079–2080, New York, NY, USA,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Results of the ontology alignment evaluation initiative",
      "author" : [ "J. Euzenat", "A. Ferrara", "W.R. van Hage", "L. Hollink", "C. Meilicke", "A. Nikolov", "D. Ritze", "F. Scharffe", "P. Shvaiko", "H. Stuckenschmidt", "O. Sváb-Zamazal", "C.T. dos Santos" ],
      "venue" : "CEUR Workshop Proceedings. CEUR-WS.org,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2011
    }, {
      "title" : "Ontology matching",
      "author" : [ "J. Euzenat", "P. Shvaiko" ],
      "venue" : "Springer,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Extracting modules from ontologies: A logic-based approach",
      "author" : [ "B.C. Grau", "I. Horrocks", "Y. Kazakov", "U. Sattler" ],
      "venue" : "C. Golbreich, A. Kalyanpur, and B. Parsia, editors, OWLED, volume 258 of CEUR Workshop Proceedings. CEUR-WS.org,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Logmap: Logic-based and scalable ontology matching",
      "author" : [ "E. Jiménez-Ruiz", "B.C. Grau" ],
      "venue" : "L. Aroyo, C. Welty, H. Alani, J. Taylor, A. Bernstein, L. Kagal, N. F. Noy, and E. Blomqvist, editors, International Semantic Web Conference (1), volume 7031 of Lecture Notes in Computer Science, pages 273–288. Springer,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Large-scale interactive ontology matching: Algorithms and implementation",
      "author" : [ "E. Jiménez-Ruiz", "B.C. Grau", "Y. Zhou", "I. Horrocks" ],
      "venue" : "L. D. Raedt, C. Bessière, D. Dubois, P. Doherty, P. Frasconi, F. Heintz, and P. J. F. Lucas, editors, ECAI, volume 242 of Frontiers in Artificial Intelligence and Applications, pages 444– 449. IOS Press,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Gomma: a component-based infrastructure for managing and analyzing life science ontologies and their evolution",
      "author" : [ "T. Kirsten", "A. Gross", "M. Hartung", "E. Rahm" ],
      "venue" : "J. Biomedical Semantics, 2:6,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Efficient approximation algorithms for repairing inconsistent databases",
      "author" : [ "A. Lopatenko", "L. Bravo" ],
      "venue" : "R. Chirkova, A. Dogac, M. T. Özsu, and T. K. Sellis, editors, ICDE, pages 216–225. IEEE,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Alignment Incoherence in Ontology Matching",
      "author" : [ "C. Meilicke" ],
      "venue" : "University Mannheim,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Journal on data semantics xi. chapter Exploring the Semantic Web as Background Knowledge for Ontology Matching, pages 156–190",
      "author" : [ "M. Sabou", "M. D’Aquin", "E. Motta" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2008
    }, {
      "title" : "Ontology matching: State of the art and future challenges",
      "author" : [ "P. Shvaiko", "J. Euzenat" ],
      "venue" : "IEEE Trans. Knowl. Data Eng., 25(1):158–176,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Matching large ontologies based on reduction anchors",
      "author" : [ "P. Wang", "Y. Zhou", "B. Xu" ],
      "venue" : "T. Walsh, editor, IJCAI, pages 2343–2348. IJCAI/AAAI,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "As ontologies became more prevalent and extensively used in domains such as biomedicine and geography, there is a growing need to automatically discover semantic correspondences between ontologies, through ontology matching [4, 7, 9, 17], in order to pursue the goal of a semantic web [16].",
      "startOffset" : 224,
      "endOffset" : 237
    }, {
      "referenceID" : 6,
      "context" : "As ontologies became more prevalent and extensively used in domains such as biomedicine and geography, there is a growing need to automatically discover semantic correspondences between ontologies, through ontology matching [4, 7, 9, 17], in order to pursue the goal of a semantic web [16].",
      "startOffset" : 224,
      "endOffset" : 237
    }, {
      "referenceID" : 8,
      "context" : "As ontologies became more prevalent and extensively used in domains such as biomedicine and geography, there is a growing need to automatically discover semantic correspondences between ontologies, through ontology matching [4, 7, 9, 17], in order to pursue the goal of a semantic web [16].",
      "startOffset" : 224,
      "endOffset" : 237
    }, {
      "referenceID" : 16,
      "context" : "As ontologies became more prevalent and extensively used in domains such as biomedicine and geography, there is a growing need to automatically discover semantic correspondences between ontologies, through ontology matching [4, 7, 9, 17], in order to pursue the goal of a semantic web [16].",
      "startOffset" : 224,
      "endOffset" : 237
    }, {
      "referenceID" : 15,
      "context" : "As ontologies became more prevalent and extensively used in domains such as biomedicine and geography, there is a growing need to automatically discover semantic correspondences between ontologies, through ontology matching [4, 7, 9, 17], in order to pursue the goal of a semantic web [16].",
      "startOffset" : 285,
      "endOffset" : 289
    }, {
      "referenceID" : 11,
      "context" : "In recent years, there has been a growing interest in efficient and effective matching methods for large ontologies [12, 15, 13, 18, 6].",
      "startOffset" : 116,
      "endOffset" : 135
    }, {
      "referenceID" : 14,
      "context" : "In recent years, there has been a growing interest in efficient and effective matching methods for large ontologies [12, 15, 13, 18, 6].",
      "startOffset" : 116,
      "endOffset" : 135
    }, {
      "referenceID" : 12,
      "context" : "In recent years, there has been a growing interest in efficient and effective matching methods for large ontologies [12, 15, 13, 18, 6].",
      "startOffset" : 116,
      "endOffset" : 135
    }, {
      "referenceID" : 17,
      "context" : "In recent years, there has been a growing interest in efficient and effective matching methods for large ontologies [12, 15, 13, 18, 6].",
      "startOffset" : 116,
      "endOffset" : 135
    }, {
      "referenceID" : 5,
      "context" : "In recent years, there has been a growing interest in efficient and effective matching methods for large ontologies [12, 15, 13, 18, 6].",
      "startOffset" : 116,
      "endOffset" : 135
    }, {
      "referenceID" : 7,
      "context" : "The Ontology Alignment Evaluation Initiative (OAEI) [8] has been the major playfield for ontology alignment, with the participation of state of the art ontology matching",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 10,
      "context" : "With respect to large ontologies alignments, the degree of incoherency is typically higher, and only one participant, LogMap [11, 12], detects incoherencies and uses repair techniques to improve the quality of the resulting alignment.",
      "startOffset" : 125,
      "endOffset" : 133
    }, {
      "referenceID" : 11,
      "context" : "With respect to large ontologies alignments, the degree of incoherency is typically higher, and only one participant, LogMap [11, 12], detects incoherencies and uses repair techniques to improve the quality of the resulting alignment.",
      "startOffset" : 125,
      "endOffset" : 133
    }, {
      "referenceID" : 14,
      "context" : "To the best of our knowledge, there are only two systems that perform alignment repair: LogMap and ALCOMO [15].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 0,
      "context" : "Since there is no gold standard alignment, a silver standard alignment based on the UMLS Metathesaurus [1] is provided for evaluating each matching problem.",
      "startOffset" : 103,
      "endOffset" : 106
    }, {
      "referenceID" : 14,
      "context" : "In an ontology matching setting we say that an alignment M between ontologies O1 and O2 is coherent if there is no class or property in O1 or O2 that is unsatisfiable due to M (see [15] for a formal definition).",
      "startOffset" : 181,
      "endOffset" : 185
    }, {
      "referenceID" : 2,
      "context" : "An implementation of our algorithms was done as part of AgreementMakerLight [3], a lightweight version of AgreementMaker [2], a successful ontology matching platform.",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 1,
      "context" : "An implementation of our algorithms was done as part of AgreementMakerLight [3], a lightweight version of AgreementMaker [2], a successful ontology matching platform.",
      "startOffset" : 121,
      "endOffset" : 124
    }, {
      "referenceID" : 9,
      "context" : "Ontology modularization techniques have been proposed and implemented to overcome the issue of scalability [10, 12, 5].",
      "startOffset" : 107,
      "endOffset" : 118
    }, {
      "referenceID" : 11,
      "context" : "Ontology modularization techniques have been proposed and implemented to overcome the issue of scalability [10, 12, 5].",
      "startOffset" : 107,
      "endOffset" : 118
    }, {
      "referenceID" : 4,
      "context" : "Ontology modularization techniques have been proposed and implemented to overcome the issue of scalability [10, 12, 5].",
      "startOffset" : 107,
      "endOffset" : 118
    }, {
      "referenceID" : 9,
      "context" : "In comparison to the module proposed by [10] and implemented by LogMap2, which computes fragments that contain 37% of the classes in FMA and 38% of the classes in NCI, there is a considerably improvement - only 5% of the total classes of FMA and NCI.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 13,
      "context" : "A similar strategy has been applied for repairing inconsistent databases [14].",
      "startOffset" : 73,
      "endOffset" : 77
    } ],
    "year" : 2013,
    "abstractText" : "Ontology Matching aims to find a set of semantic correspondences, called an alignment, between related ontologies. In recent years, there has been a growing interest in efficient and effective matching methods for large ontologies. However, most of the alignments produced for large ontologies are logically incoherent. It was only recently that the use of repair techniques to improve the quality of ontology alignments has been explored. In this paper we present a novel technique for detecting incoherent concepts based on ontology modularization, and a new repair algorithm that minimizes the incoherence of the resulting alignment and the number of matches removed from the input alignment. An implementation was done as part of a lightweight version of AgreementMaker system, a successful ontology matching platform, and evaluated using a set of four benchmark biomedical ontology matching tasks. Our results show that our implementation is efficient and produces better alignments with respect to their coherence and f-measure than the state of the art repairing tools. They also show that our implementation is a better alternative for producing coherent silver standard alignments.",
    "creator" : "LaTeX with hyperref package"
  }
}