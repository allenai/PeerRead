{
  "name" : "1503.01444.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Partial Sum Minimization of Singular Values in Robust PCA: Algorithm and Applications",
    "authors" : [ "Tae-Hyun Oh", "Jean-Charles Bazin" ],
    "emails" : [ "thoh.kaist.ac.kr@gmail.com,", "yuwing@gmail.com,", "woo.kim@kaist.ac.kr,", "iskweon77@kaist.ac.kr", "jean-charles.bazin@inf.ethz.ch" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms—Robust principal component analysis, rank minimization, sparse and low-rank decomposition, truncated nuclear norm, alternating direction method of multipliers."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "VArious low-level vision applications, including HighDynamic Range (HDR) [35], [36], photometric stereo [3], [23], batch image alignment [38] and factorization-based structure from motion [5], [41], can be formulated as a low-rank matrix recovery problem. Low-rank matrix approximation methods, such as Principal Component Analysis (PCA) [29] and matrix factorization [7], [18], [40], [48] are widely used to find the best approximation of an underlying low-rank structure of data. However many of these approaches are error-prone due to the presence of outliers. To recover the low-rank matrix while rejecting outliers, a rank minimization based Robust Principal Component Analysis (RPCA) [9] has been proposed and gained much interest in computer vision [28], [38], [45], [47].\nRPCA [9] aims to recover a low-rank matrix A ∈ Rm×n from corrupted observations O = A+E, where E ∈ Rm×n represents errors with arbitrary magnitude and distribution. The rank minimization approach [9], [10], [39], [43] assumes E is sparse and formulates the problem as:\nmin A,E\nrank(A) + λ‖E‖0, s.t. O = A+E, (1)\nwhere ‖·‖0 denotes the l0-norm, and λ is the relative weight between the two terms. Unfortunately, solving the problem\n• T.-H. Oh, Y.-W. Tai, H. Kim and I.S. Kweon (corresponding author) are with the Department of Electrical Engineering, KAIST, Daejeon, Republic of Korea. E-mail: thoh.kaist.ac.kr@gmail.com, yuwing@gmail.com, hyeongwoo.kim@kaist.ac.kr, iskweon77@kaist.ac.kr • J.-C. Bazin is with the Department of Computer Science, ETH Zurich, Switzerland. E-mail: jean-charles.bazin@inf.ethz.ch\nin Eq. (1) is an NP-hard problem. The rank minimization approach instead solves an approximated problem by convex surrogate as:\nmin A,E\n‖A‖∗ + λ‖E‖1, s.t. O = A+E, (2)\nwhere ‖A‖∗ = ∑\ni σi(A) is the nuclear norm of A, σi(A) represents the i-th singular value of A (sorted in decreasing order) , and ‖E‖1 is the l1-norm of E. Eq. (2) can be solved effectively by various methods [31], [44]. Wright et al. [43] and Candès et al. [9] proved that, under mild conditions, the unique solution of Eq. (2) exactly corresponds to the solution of the original NP-hard problem in Eq. (1). Yet, when the number of observations in O is very limited, experiments show that the converged solution includes some outliers as inliers and vice versa. Such limited number of observations is not uncommon in many computer vision problems due to practical reasons. For example, in HDR context, often only 2-4 differently exposed images are captured and photometric stereo requires only 3 images in theory. Moreover, we also observe that the converged solution can be degenerated. For instance, in the photometric stereo problem [45], the solution of A might have a rank lower than the theoretical rank of 3.\nIn this paper, based on the prior knowledge about the rank of A, we propose an alternative objective function which minimizes the Partial Sum of Singular Values (abbreviated to PSSV) of A:\nmin A,E\n‖A‖p=N + λ‖E‖1, s.t.O = A+E, (3)\nwhere ‖A‖p = ∑min(m,n)\ni=p+1 σi(A) and N is the target rank of A which can be derived from the problem definition, e.g. N = 1 for background subtraction, N = 3 for photometric stereo. Eq. (3) minimizes the rank of residual errors of A\nar X\niv :1\n50 3.\n01 44\n4v 2\n[ cs\n.C V\n] 1\n3 A\nug 2\n01 5\n2 against the target rank, instead of the nuclear norm. A major drawback of using the nuclear norm to approximate the l0-norm of singular values is that the nuclear norm minimizes not only the rank of A, but also the variance of A by simultaneously minimizing all the singular values of A including the singular values within the target rank N , i.e. σ1≤i≤N . Consequently, the low-rank structure of A may not be well approximated under the environment that does not follow the assumption of large number of inputs.\nAlthough Eq. (3) is non-convex, we observe in our experiments that Eq. (3) encourages the resulting low-rank matrix to have a rank close to N even with deficient observations. For example, when the singular values of A within the target rank are small, the nuclear norm can result in a rank deficient matrix A, i.e. whose rank is smaller than the target rank. In contrast, because our work does not minimize the subspace variance of A within the target rank, we are not biased to the solution with smaller variance of A. Thus, the low-rank matrix A can be more accurately estimated. Further analyses and discussions about this claim are provided in the later sections of our paper.\nIn contrast to matrix factorization methods where the a priori rank constraint is enforced as a hard constraint via matrix projection, we encourage the rank constraint as a soft constraint and propose the Partial Singular Value Thresholding (PSVT) to solve our partial sum singular value objective function. As analyzed in our study, the PSVT operator encourages the result A to meet the target rank even when all the singular values are small.\nThis work is the extension of our previous conference paper [33]. We empirically study the proposed objective function in many low-level vision problems, e.g. HDR imaging, motion boundary detection, photometric stereo, image alignment, and image recovery where the theoretical rank of A is known and the number of observations is limited (except the image recovery application). Our experimental analyses show that our formulation, described in Eq. (3), converges to a solution more robust to outliers than the solution obtained by the objective function in Eq. (2) in rank minimization, when the number of observations is limited. Empirically, we also find that the solutions of Eq. (2) (nuclear norm) and Eq. (3) (our PSSV) are almost identical when more than a sufficient number of samples is observed.\nIn short summary, our contributions are as follows:\n• We present a partial sum objective function and its corresponding minimization method for RPCA. • We empirically study the partial sum objective function and claim that it outperforms the nuclear norm rank minimization when the number of observed samples is very limited. • We present the convergence property of the proposed algorithm to minimize the proposed partial sum objective function consisting of PSSV and sparse term, and provide its proof. • We apply our technique on various low-level vision problems and demonstrate superior results over previous works."
    }, {
      "heading" : "2 RELATED WORKS",
      "text" : "In this section, we briefly review early works related to RPCA, then we discuss some recent advances in RPCA and its applications in computer vision. We will also review some recent matrix factorization based works for low-rank approximation. We invite readers to refer to Candès et al. [9] for a thoughtful review of RPCA.\nIn conventional PCA [29], the goodness-of-fit of data is evaluated by the l2-norm which is very sensitive to outliers. Early works in RPCA tried to reduce the effects of outliers by random sampling [19], robust M-estimator [12], [13], or alternating minimization [30] to identify outliers or penalize data with large errors. These methods share some limitations: either they are sensitive to the choice of parameters or their algorithms are not scalable enough in running time.\nRecent advances in RPCA showed that the heuristic nuclear norm solution [9], [39], [43] converges to a solution which is robust to sparse outliers. Candès et al. [9] proved that the original RPCA problem as in Eq. (1) can be solved by instead solving the convex relaxation version in Eq. (2), and it provides a unique and exact solution of Eq. (1) as long as E is sparse and random and the underlying rank(A) is lower than a certain upper bound1. To solve Eq. (2), various methods have been proposed [31], [44]. Among them, Alternating Direction Method of Multipliers (ADMM, or also called inexact augmented Lagrange multiplier) [31] has shown to be computationally efficient. Also, Zhou et al. [49] and Agarwal et al. [1] proved that convex approximation by nuclear norm can still achieve bounded and stable results even under small noise measurements.\nBesides the standard nuclear norm relaxation, some works study variants of the nuclear norm to enhance performance of rank minimization [11], [20], [24]. Chen et al. [11] and Gaiffas et al. [20] proposed an adaptive weighted nuclear norm. They suggested a non-trivial update scheme to update the adaptive weighted nuclear norm and claimed to achieve higher accuracy in low-rank matrix approximation in comparison with the standard nuclear norm. Hu et al. [24] proposed the truncated nuclear norm (TNN) for the matrix completion problem which shares a similar objective function with our PSSV objective function. Since the TNN is non-convex (which is not easy to directly solve), they aim to avoid direct minimization by locally approximating TNN as minX,W ‖X‖∗ −Tr(AlWB l ), s.t. X = W by alternatively minimizing Al, Bl, X and W based on the singular value thresholding (SVT) operator [8]. This alternating scheme requires outer iterations and additional SVD computations. Instead of utilizing this alternating scheme, we propose the PSVT operator to directly minimize the partial sum of singular value term. Although our objective function is also non-convex, our proposed PSVT produces the closedform solution to the sum of the PSSV and proximity term. In that sense, every sub-problem of our problem has a closed-form solution. Thus, our optimization problem can be solved efficiently. Moreover, while the approach of Hu et al. is dedicated only to matrix completion, we show that our work can be successfully applied for several computer\n1. The bound depends on the matrix size.\n3 vision tasks spanning from image alignment to photometric stereo and HDR imaging.\nAnother branch of low-rank framework is based on Matrix Factorization (MF). Several robust MF methods based on l1-norm have been suggested [7], [18], [48]. A benefit of matrix factorization approaches is that they can easily enforce the rank constraint by the explicit bilinear matrix form. The target rank constraint is enforced as a hard constraint via matrix reprojection or orthogonal procrustes. Cabral et al. [7] revisited the relationship between nuclear norm regularization and bilinear MF model [2], and proposed a rank continuation heuristic to avoid local minima. Compared with MFs, our target rank constraint is expressed as a soft constraint which provides flexibility to balance between the rank constraint and other constraints used in different computer vision problems.\nThe robustness and scalability of the rank minimization algorithm for RPCA [9], [31], [44] have inspired many applications in computer vision, such as background subtraction [9], image and video restoration [28], image alignment [38], regular texture analysis [47], and robust photometric stereo [45]. These applications are based on the observation that the underlying structures of clean data are linearly correlated, which forms a low-rank data matrix. The rank minimization proposed by Candès et al. [9] is general in the sense that it does not require to know a priori the rank of clean data. However, as briefly mentioned in the introduction, in some applications, the rank of clean data can be determined by the problem definition, and this motivates us to investigate how the prior rank information can be fully utilized in the context of RPCA.\nThe success of rank minimization based RPCA comes from the blessing of dimensionality of input matrix [16], [43], implying large amount of observations. However, when the number of observations is limited, which is common in practice, results from RPCA might be degenerated, e.g. correct samples might be considered as outliers and vice versa. As discussed in the introduction, this happens because the standard nuclear norm minimizes not only the rank of the matrix, but also the variance of data distribution of the matrix. To overcome this limitation, we introduce an alternative objective function that can efficiently deal with a deficient number of samples in the rank minimization problem. Our work can be considered as an addendum to the standard rank minimization approach when the target rank or the approximate target rank is known. The proposed alternative objective function can control the rank with a simple and efficient minimizer. We demonstrate the effectiveness of our proposed objective function through thoughtful experiments."
    }, {
      "heading" : "3 PARTIAL SUM MINIMIZATION BY THE PSVT OPERATOR",
      "text" : ""
    }, {
      "heading" : "3.1 Derivation of Partial Sum of Singular Values",
      "text" : "Our partial sum formulation in Eq. (3) is originated from the following objective function:\nargmin A,E\n|rank(A)−N | + λ‖E‖0, s.t. O = A+E. (4)\nEq. (4) aims to recover a low-rank matrix A close to the target rank N and a sparse error matrix E.\nSince the above objective function is also an NP-hard problem, we relax it with an alternative representation in order to effectively deal with it. The relaxation is similar to the method presented by Candès et al. [9]. We should also properly interpret the target rank N . We relax it with a projection operator to enforce a rank-N matrix in a matrix interpretation manner. From the relaxation, the PSSV objective function, which is the first term in Eq. (4), can be derived as follows:\n|‖A‖∗ − ‖PN (A)‖∗| = ∣∣∣∣∣∣ min(m,n)∑\ni=1\nσi(A)− N∑\ni=1\nσi(A) ∣∣∣∣∣∣\n=\nmin(m,n)∑\ni=N+1\nσi(A) = ‖A‖p=N ,\n(5)\nwhere ‖ · ‖p=N denotes the PSSV with the target rank N , and Pr(·) is the matrix projection operator to rank-r matrix defined as follows. Definition 1. [Projection operator]\nPr(X) = U 1:rXV1:r, (6)\nwhere U1:r and V1:r are the matrices consisting of the singular vectors corresponding to the r largest singular values of X."
    }, {
      "heading" : "3.1.1 From rank constraint to projection",
      "text" : "Eq. (5) leads us to the PSSV objective function in Eq. (3). In this section, we show the relationship between the target rank N and the projection operator in Eq. (5). We first introduce a rank representation.\nLemma 1. Let A ∈ Rm×n and rank(A) ≥ r, then there exist matrices C ∈ Rr×m and B ∈ Rn×r such that CC = B B = I ∈ Rr×r and\nrank(CAB) = r. (7)\nProof. Let UDV be SVD of A. Suppose C = U 1:r and B = V1:r, where U1:r and V1:r are the matrices consisting of the singular vectors corresponding to the r largest singular values. C and B satisfy rank(CAB) = r, which concludes the proof.\nThe constant r can be represented in the matrix form with Lemma 1. Now, we show the characteristics of the presented solution by SVD in Lemma 1 with Lemma 2.\nLemma 2. For any u = {w|w⊥span{u1, · · · ,uk−1}}, v = {w|w⊥span{v1, · · · ,vk−1}} and A ∈ Rm×n,\nσk = max u,v |u Av| ‖u‖ ‖v‖ . (8)\nLemma 2 is the well-known Variational Characterization of Singular Values (or Courant-Fischer Min-max principle for singular values). By Lemma 2, we see thatC andB satisfying Lemma 1 are also the unique solution of the following problem:\nmax C,B\n‖CAB‖∗ s.t. CC = B B = I. (9)\n4 Outlier\n(a)\nInlier\nInlier\n(b)\nFig. 1: Illustrations of the potential problems in minimizing nuclear norm (all singular values). The ground truth subspace (green) is a 1D line corrupted with sparse outliers and noise. In (a), the estimated subspace is biased to the estimated axis that has a smaller nuclear norm but a second singular value larger than the ground truth coordinate. In (b), some inliers located on the ground truth sub-space are regarded as outliers to achieve a smaller singular value.\n0 1 2 3 4 0\n1\n2\n3\n4\n5\n6\nx\nM ag\nni tu\nde\nNuclear norm Partial sum\n(a)\n0 1 2 3 4 0\n1\n2\n3\n4\n5\nx\nM ag\nni tu\nde\nNuclear norm Partial sum\n(b)\nFig. 2: A toy example comparison between the nuclear norm solution and the PSSV solution. Y-axis represents the magnitude of the nuclear norm and PSSV. The red dots represent the minimum points of the magnitude. The graphs show the nuclear norm and the PSSV of 2 × 2 matrices A = [1 1; 3x] in (a) and [1 1; 1x] in (b) with x varying from 0 to 4. In (a), the minimum point of nuclear norm is at x = 1 where the singular values of A are equal to [3.4142, 0.5858] (i.e., rank-2). As for the PSSV, the minimum point is at x = 3 with singular values equal to [4.4721, 0.0000] (i.e., rank1). In this toy example, the nuclear norm favors a rank-2 solution over a rank-1 solution because the rank-2 solution provides the minimum nuclear norm. In contrast, the PSSV achieves the lowest rank (rank-1) solution. In (b), when the basis of the first row of A is partially supported by another sample (second row), the nuclear norm and the PSSV both achieve the rank-1 solution at minima.\nThough the solution satisfying Lemma 1 is not unique, the solution of Eq. (9) can be a way to represent the target rank constraint. Therefore, we relax the target rank constant to the nuclear norm representation in Eq. (9) with U1:r and V1:r which satisfy both Lemma 1 and Eq. (9). In summary, we show the first term in Eq. (4) can be relaxed as PSSV by Lemmas 1 and 2, and the introduced projection operator Pr(·) (see Definition 1) favors preserving the information of the r largest singular values. Intuitively, when rank(A) ≥ r, as σi>N (A) → 0 (namely minimized), rank(A) → N . Of course, if rank(O) < N , i.e. if the inputs are degenerated, the rank of A cannot be increased to meet the target rank. This is a fundamental limitation. In common cases where input data contains noise or outliers, the inequality condition rank(A) ≥ r is easily satisfied, because data corruptions increase the rank of input data."
    }, {
      "heading" : "3.1.2 Why the partial sum of singular values?",
      "text" : "A major advantage of using the PSSV over the nuclear norm is that it does not minimize the variance of data distribution within the target rank. Minimizing the nuclear norm can favor a solution that has a lower nuclear norm, but the singular values in residual ranks (singular values above the target rank N . Let N=1 here) can be still large as illustrated in Fig. 1-(a) and Fig. 2-(a). This bias can\ndegrade the accuracy of the estimated low-rank subspace. The bias phenomenon by a convex surrogate is common, and it could be corrected by non-convex relaxation [46]. An additional issue is that if the ground truth has a large variance but a sparse distribution within the ground truth subspace, some inliers can be regarded as outliers in order to reduce the singular values within the target rank, as illustrated in Fig. 1-(b) and at the minimum point of nuclear norm in Fig. 2-(a). These two problems are not an issue when there is a lot of observed data to support the correct estimation of A. However, when observed data is limited, minimizing nuclear norm can easily cause bias since there is not a sufficient number of truth samples to support large variance of A within the target rank. In contrast, the PSSV does not assume small variance of A, and it only minimizes variances in residual rank which corresponds to minimizing the noise variance of observed data. Note that the original rank operator, rank(A), in Eq. (1) does not favor small variance solution."
    }, {
      "heading" : "3.2 Optimization by ADMM",
      "text" : "Our partial sum objective function in Eq. (3) forms a constrained optimization problem. To solve this type of problems, Lin et al. [31] proposed an ADMM method (or called inexact augmented Lagrange multipliers, iALM). The augmented Lagrangian function of Eq. (3) is formulated by:\nLμ(A,E,Z) = ‖A‖p=N + λ‖E‖1 (10) +〈Z,O−A−E〉+ μ\n2 ‖O−A−E‖2F ,\nwhere μ is a positive scalar, Z ∈ Rm×n is an estimate of the Lagrange multiplier, ‖ · ‖F denotes the Frobenius norm, and 〈·, ·〉 represents the inner product operator. Directly minimizing the Lagrangian function might be particularly challenging. According to a recent development of alternating direction method [31], Eq. (10) can be solved by minimizing each variable alternatively while fixing the other variables. The optimization problem can be divided into two sub-problems:"
    }, {
      "heading" : "A∗ =argmin",
      "text" : "A Lμk(A,Ek,Zk)\n= argmin A\nμ−1k ‖A‖p=N + 1\n2\n∥∥A− (O−Ek + μ−1k Zk) ∥∥2 F ,\n(11)"
    }, {
      "heading" : "E∗ =argmin",
      "text" : "E Lμk(Ak+1,E,Zk)\n= argmin E\nλμ−1k ‖E‖1 + 1\n2\n∥∥E− (O−Ak+1 + μ−1k Zk) ∥∥2 F ,\n(12) where k indicates the iteration index (see Alg. 1)."
    }, {
      "heading" : "3.3 Solving A∗",
      "text" : "To minimize Eq. (11), we define the Partial Singular Value Thresholding (PSVT) operator PN,τ [·]. Before defining the PSVT, we first introduce the von Neumann’s lemma (see details in de Sá et al. [14]).\nLemma 3 (von Neumann [14]). For any matrices B,Z ∈ Rm×n and vectors of the singular values σ(·), the following\n5 equality holds:\nmax {〈 UZV ,B 〉 |U ∈ Um,V ∈ Un } = 〈σ(Z), σ(B)〉, (13)\nwhere Un denotes the set of n×n unitary matrices, and 〈A,B〉 = Tr(ATB), for any matrix A ∈ Rm×n. Hence\n〈A,B〉 ≤ 〈σ(A), σ(B)〉 . (14)\nMoreover, equality holds in Eq. (14) iff there exists a simultaneous SVD U and V of A and B in the following form:\nA = Udiag (σ(A))V and B = Udiag (σ(B))V . (15)\nThe von Neumann’s lemma shows that 〈A,B〉 is always bounded by the inner product of σ(A) and σ(B). Notice that the maximum value of 〈A,B〉 can be only achieved when A has the same singular vector matrices U and V as B. This fact is useful to derive the PSVT.\nTheorem 1 (PSVT). Let τ > 0, l = min(m,n) and X,Y ∈ Rm×n which can be decomposed by SVD. Y can be considered as the sum of two matrices, Y = Y1 +Y2 = UY 1DY 1V Y 1 + UY 2DY 2V Y 2, where UY 1,VY 1 are the singular vector matrices corresponding to the N largest singular values by SVD, and UY 2,VY 2 from the (N+1)-th to the last singular values. Define a minimization problem for the PSSV as\nargmin X\n1 2 ‖X−Y‖2F + τ‖X‖p=N . (16)\nThen, the optimal solution of Eq. (16) can be expressed by the PSVT operator defined as:\nPN,τ [Y] =UY (DY 1 + Sτ [DY 2])V Y =Y1 +UY 2Sτ [DY 2]V Y 2,\n(17)\nwhere\nDY 1 = diag(σ1, · · · , σN , 0, · · · , 0), DY 2 = diag(0, · · · , 0, σN+1, · · · , σl),\nand Sτ [x] = sign(x) · max(|x| − τ, 0) is the soft-thresholding operator [17], [22].\nProof. Let’s consider X = UXDXV X = ∑l i=1 σi(X)uiv i where UX = [u1, · · · , um] ∈ Um, VX = [v1, · · · , vn] ∈ Un and DX = diag(σ(X)), where the singular values σ(·) = [σ1(·), · · · , σl(·)] ≥ 0 are sorted in a non-increasing order. Also we define the function J(X) as the objective function of Eq. (16). The first term of Eq. (16) can be derived as follows:\n1 2 ‖X−Y‖2F = 1 2\n( ‖Y‖2F − 2〈X,Y〉+ ‖X‖2F )\n= 1\n2\n( ‖Y‖2F − 2 l∑\ni=1\nσi(X)u i Yvi +\nl∑\ni=1\nσi(X) 2\n)\n= 1\n2 ‖Y‖2F +\n1\n2\nl∑\ni=1\n( −2σi(X)u i Yvi + σi(X)2 ) .\n(18)\nIn the minimization of Eq. (18) with respect to X, ‖Y‖2F is regarded as a constant and thus can be ignored. For a more\ndetailed representation, we change the parameterization of X to (UX ,VX ,DX) and minimize the function:\nJ(UX ,VX ,DX) = (19)\n1\n2\nl∑\ni=1\n( −2σi(X)u i Yvi + σi(X)2 ) + τ l∑\ni=N+1\nσi(X).\nFrom von Neumann’s lemma, the upper bound of u i Yvi is given as σi(Y) = max { u i Yvi } for all i when UX = UY and VX = VY . The lower envelope of J(UX ,VX ,DX) is obtained at UX = UY and VX = VY . Then Eq. (19) becomes a function depending only on DX as follows:\nJ(UY ,VY ,DX) (20)\n= 1\n2\nl∑\ni=1\n( −2σi(X)σi(Y) + σi(X)2 ) + τ l∑\ni=N+1\nσi(X)\n= 1\n2\n( N∑\ni=1\n( −2σi(X)σi(Y) + σi(X)2 )\n+ l∑\ni=N+1\n( −2σi(X)σi(Y) + σi(X)2 + 2τσi(X) ) ) .\nSince Eq. (20) consists of simple quadratic equations for each σi(X) independently, it is trivial to show that the minimum of Eq. (20) is obtained at D̂X = diag(σ̂(X)) by derivative in a feasible domain as the first-order optimality condition, where σ̂i(X) is defined as\nσ̂i(X) = { σi(Y), if i < N + 1, max (σi(Y)− τ, 0) , otherwise. (21)\nHence, the solution of Eq. (16) is X∗ = UY D̂XVTY. This result exactly corresponds to the PSVT operator where a feasible solution X∗ = UY (DY 1 + Sτ [DY 2])V Y exists.\nOur proposed PSVT can be regarded as a special case of solving the weighted nuclear norm based objective function of Chen et al. [11] and Gaı̈ffas et al. [20]. But we would like to notice that our method suggests how the weighted parameter (defined in their literatures) can be determined to encourage the rank constraint. Also, notice that our proposed PSVT operator provides a closed-form solution for systems of the same form as Eq. (16) (e.g. Eq. (11)). While Eq. (11) is a non-convex function, the PSVT provides a global optimal solution for the sub-problem of A (see the proof of Theorem 1).\nAs an analysis of PSVT, when τ = ∞, the optimal solution of Eq. (16) is a low-dimensional projection of Y known as singular value projection [27] which enforces the target rank constraint through projection. When σi < τ for 1 ≤ i ≤ N , conventional SVT [8] projects these σi to zero resulting in a more deficient rank of A than the target rank while our PSVT does not lead to rank deficient matrices. Hence, PSVT implicitly encourages the resulting matrix A to meet the target rank even when all the σi are small, which occasionally happens when the number of observed samples is limited.\n6"
    }, {
      "heading" : "3.4 Solving E∗",
      "text" : "As suggested by Hale et al. [22], the solution to the sub– problem in Eq. (12) can be obtained as:\nSτ [Y] = argmin X\n1 2 ‖X−Y‖2F + τ‖X‖1, (22)\nwhere Sτ [x] = sign(x)max(|x|−τ, 0) is the soft-thresholding operator [17], [22], and x ∈ R. This operator can be extended to vectors and matrices by applying it element-wise. The soft-thresholding (shrinkage) method is shown to be very effective in minimizing l1-norm and the proximity term, and guarantees that the solution is the global minimum for the equations of the same form as Eq. (22) (e.g. Eq. (12)) [17], [22]."
    }, {
      "heading" : "3.5 Updating A∗ and E∗",
      "text" : "At each iteration k, Ak and Ek can be updated with the operators Sτ [·] and PN,τ [·] as:\nAk+1 = PN,μ−1k [O−Ek + μk −1Zk], Ek+1 = Sλμ−1k [O−Ak+1 + μk −1Zk].\n(23)\nThe iterations are terminated when the equality constraint is satisfied (in all the experiments, ‖O−A−E‖F‖O‖F < 1e\n−7). Experiments showed that updating Ak and Ek for only one iteration in the inner loop is sufficient to produce a satisfying accurate solution of Eq. (3). This method is called the inexact ALM [31] and is designed for computational efficiency.\nWe summarize the overall algorithm in Alg. 1 (For more details, refer to the report of Lin et al. [31]).\nAlgorithm 1 ADMM for the PSSV based RPCA\nInput : O ∈ Rm×n, λ > 0, the constraint rank N . Initialize A0 = E0 = 0, Z as suggested in [31], μ0 > 0, ρ > 1 and k = 0. // Outer loop while not converged do\n// Inner loop while not converged do\nAk+1 = PN,μ−1 k [O−Ek + μk−1Zk]. Ek+1 = Sλμ−1 k [O−Ak+1 + μk−1Zk].\nend while Zk+1 = Zk + μk(O−Ak+1 −Ek+1). μk+1 = ρμk . k = k + 1.\nend while Output : (Ak,Ek)."
    }, {
      "heading" : "3.6 Convergence Analysis",
      "text" : "To the best of our knowledge, the general convergence property of ADMM which alternates between non-convex (solvingA∗) and convex (solving E∗) functions has not been answered yet. The ADMM for non-convex problems can be considered as a local optimization method, which aims to converge to a point with better objective value [4].\nIn our problem, each sub-problem has a closed-form solution and the objective value is always decreasing with respect to the primal variables optimized in each subproblem iteration2. Our empirical convergence tests showed\n2. It does not mean a monotonic decrease of the Lagrangian function, which is not necessarily monotone due to the dual update.\nthat our ADMM based algorithm has a strong convergence behavior (see Sec. 4.1). Although the global optimal solution is not guaranteed, all of our experiments showed that our algorithm converges to a solution which is very close to the nuclear norm solution, when the number of observations is more than sufficient. It also converges to a better solution than the nuclear norm solution when the number of observations is limited, even with all zero initializations.\nBesides the empirical behavior, we provide the convergence property for Alg. 1 in Proposition 1. It shows that any accumulation point (limit point) generated along the iterations satisfies the first-order necessary optimal condition, a KKT (Karush-Kuhn-Tucker) point.\nProposition 1 (Convergence). Let Sk = (Ak,Ek,Yk, Ŷk), where Ŷk+1 = Yk + μk(O − Ak+1 − Ek) and {Sk}∞k=1 is a set of intermediate solutions of Alg. 1. Suppose that {Yk}∞k=1 and {Ŷk}∞k=1 are bounded, lim\nk→∞ (Yk+1 −Yk) = 0, and μk is\nnon-decreasing, then any accumulation point of {Sk}∞k=1 satisfies the following KKT conditions: (C1) Y∗ ∈ ∂C‖A∗‖p, (C2) Y∗ ∈ ∂‖λE∗‖1, (C3)O−A∗−E∗ = 0, (C4) ∂C‖A∗‖p∩∂‖λE∗‖1 = ∅. In particular, whenever {Sk}∞k=1 converges, it converges to a KKT point of Eq. (3).\nThe proofs can be found in the supplementary material. The conditions for non-decreasing μk and the boundness of the sequence are already satisfied by Alg. 1 (see Lemma 1 in Lin et al. [31]). Proposition 1 is established for a single iteration algorithm in the inner loop, i.e. iALM. When the inner loop iterates until convergence (exactly solving the inner loop), it may lead a simpler proof than the above result. We remain further theoretical analyses of convergence as future work."
    }, {
      "heading" : "4 EXPERIMENT RESULTS",
      "text" : "We compare the performance of the proposed method against RPCA (nuclear norm) [9] with synthetic data sets and real world application examples. In all the experiments, we use the default parameters recommended by Candès et al. [9] for both their approach and ours, i.e. λ = 1/ √ max(m,n) and ρ = 1.5, except if explicitly stated\n7\notherwise. The code of the proposed method is available on our project website 3."
    }, {
      "heading" : "4.1 Synthetic Dataset",
      "text" : "We compare our method (PSSV) with RPCA (nuclear norm) on synthetic data by evaluating the success ratio and convergence behaviors. To synthesize a ground-truth lowrank matrix AGT ∈ Rm×n of rank-N , we perform a linear combination of N arbitrary orthogonal basis vector. The weight vector used to span each column vector of AGT is randomly sampled from the uniform distribution U [0, 1]. To generate sparse outliers, we select m×n×r entries from AGT , where r denotes the corruption ratio. Larger r means more outlier entries. The selected entries are corrupted by random noise from U [0, 1]. We run each of the tests over 50 trials and report the overall average errors of the trials. We refer to ‖AGT−Â‖F‖AGT ‖F as the normalized root mean squared error (NRMSE)."
    }, {
      "heading" : "4.1.1 Comparison of Success Ratio",
      "text" : "We verify the robustness of RPCA (nuclear norm) and the proposed method (PSSV) with respect to the number of observations, data dimension and corruption ratio. We examine the performance by counting the number of successes. If the recovered Â has a NRMSE smaller than 0.01, we consider the estimation of A and E is successful. We compare the success ratio with varying column size n (i.e. the number of observations), and row size m (i.e. data dimension). The magnitude in Fig. 3 indicates the success\n3. http://thoh.kaist.ac.kr\npercentage. A larger blue area indicates a more robust performance of the algorithm.\nWe also perform experiments where we fix m = 10, 000 and vary n and r. The comparison between RPCA and our method with rank-1,2,3,5 and -10 constraints is shown in Fig. 3. As n decreases (i.e. the number of observations decreases), the success ratio of RPCA decreases more rapidly than our method. When more observations are available (over n = 25 in Fig. 3 and Fig. 5), both methods show a similar behavior.\nFig. 4 shows the success ratio of RPCA and ours for the varying row m cases. We fix n = 16, and vary m and r. Our method can successfully recover A and E with input data contaminated up to 15% of severe corruption for the rank1 case in Fig. 4-(b), and leads to more robust results than RPCA despite 5% higher corruption for the rank-3 case in Fig. 4-(d)."
    }, {
      "heading" : "4.1.2 Rank Deficiency",
      "text" : "We verify whether the recovered Â obtained by RPCA and our method is rank deficient. Our objective function minimizes the rank of A up to the target rank. Thus, the result rank of Â should not be lower than the target rank. In practice, rank deficiency is crucial for quality of the final solution in some applications (e.g. photometric stereo). We measure the ratio σN (Â)/σ1(Â) (similar to the inverse value of the condition number) for the rank-N constraint case. We only test for rank-N = 3 as a typical example of photometric stereo. If the ratio is lower than 0.01, we consider that the recovered matrix has a rank lower than N . In Fig. 6, the red regions mean that the rank of the recovered matrix is lower than the target rank. The experiments empirically validate that the rank obtained by our method is bounded for almost all of the regions, while RPCA has regions whose rank is lower than the target rank. This happens when observations do not support its true subspaces well.\n8 Method Objective function Constraint Eriksson et al. [18] min\nU,V ‖O−UV‖1 –\nZheng et al. [48] min U,V ‖O−UV‖1 + λ‖V‖∗ U U = I LMaFit [40] min\nA,U,V ‖O−A‖1 A = UV\nSVP [27] based RPCA\nmin A,E ‖E‖1 O = A + E, rank(A) = N\nRPCA [9] min A,E ‖A‖∗ + λ‖E‖1 O = A+ E WNNM [11] based RPCA min A,E ‖A‖w,∗ + λ‖E‖1 O = A+ E Our method min A,E ‖A‖p=N + λ‖E‖1 O = A+ E\nTABLE 1: Summary of the compared methods. O,A,E ∈ Rm×n,U ∈ Rm×N ,V ∈ RN×n, ‖ · ‖w,∗ is the weighted nuclear norm and the weight coefficients in w is determined adaptively as suggested by Chen et al . [11]."
    }, {
      "heading" : "4.1.3 Sensitivity to Initialization",
      "text" : "Since the proposed objective function is non-convex, the converged solution may be different according to the initialization. To study the sensitivity of the optimization against the initialization, we conducted 1000 experiments with random initialization on a rank-3 matrixO ∈ R10000×50 with 5% outliers. The distribution of NRMSE is shown in Fig. 7. While the convergence of non-convex problem to an optimum is hard to be guaranteed, most solutions are concentrically distributed in regions near the ground-truth solution with small errors."
    }, {
      "heading" : "4.1.4 Comparisons with other low-rank approximations",
      "text" : "We provide additional comparisons with the singular value projection (SVP) [27] based and the weighted nuclear norm (WNNM) [11] based methods, and low-rank matrix approximation approaches by MF. The formulations are summarized in Table 1. The SVP and WNNM are reformulated based on RPCA framework for fair comparison. MF methods enforce the target rank N constraint of data matrix (O = UV) by factorizing it into a product of rank-N basis (U) and coefficient (V) as hard constraint. Among the existing MF based methods, we compare with the state-of-the-art methods of LMaFit [40], Zheng et al. [48] and Eriksson et al. [18], with the default recommended parameters.\nSince the method of Eriksson et al. can only handle small size examples, we perform separate experiments for small and large scales. We synthetically generate data matrices R30×7 with rank-2 for small scale or R5000×20 with rank-3 for large scale, and varying corruption ratio in [0.05, 0.20]. NRMSE is displayed in Fig. 8-(a,b).\nCompared to our method, their approach also minimizes the nuclear norm in addition to the hard target rank constraint. As discussed previously, since minimizing the nuclear norm also implicitly minimizes the variance of the estimated low-rank matrix, their estimated low-rank matrix could be biased by this assumption. On the other hand, since our PSSV objective function does not have this assumption, and since the target rank is penalized softly, our method converges to more accurate solutions compared to the solutions of LMaFit, Zheng et al. and Eriksson et al.\nWe have also conducted experiments for the undersampled cases on subspace: e.g. a ground truth data is spanned with 3 basis axis (true and target rank are 3), but the distribution along the third basis axis has a very small variance (small singular value). Thus, although the underlying matrix is a rank-3 matrix, it is very close\n0 5 10 15 20 10\n−8\n10 −6\n10 −4\n10 −2\n10 0\nOutlier ratio (%)\nRPCA SVP LMaFit Zheng Eriksson WNNM γ=0.5 WNNM γ=1 Ours\n(a) Small scale, well sampled\n0 5 10 15 20 10\n−8\n10 −6\n10 −4\n10 −2\n10 0\nOutlier ratio (%)\nRPCA SVP LMaFit Zheng WNNM γ=0.5 WNNM γ=1 Ours\n(b) Large scale, well sampled\n0 5 10 15 20 10\n−5\n10 −4\n10 −3\n10 −2\n10 −1\n10 0\nOutlier ratio (%)\nRPCA SVP LMaFit Zheng Eriksson WNNM γ=0.5 WNNM γ=1 Ours\n(c) Small scale, under sampled\n0 5 10 15 20 10\n−8\n10 −6\n10 −4\n10 −2\n10 0\nOutlier ratio (%)\nRPCA SVP LMaFit Zheng WNNM γ=0.5 WNNM γ=1 Ours\n(d) Large scale, under sampled\nFig. 8: Accuracy comparisons with varying outlier ratio and deficient number of samples for SVP [27] based andWNNM [11] based methods, LMaFit [40], Zheng et al . [48], Eriksson et al . [18], and our method. The experiments consist of small scale problems (O ∈ R30×7 with rank-2) in (a,c) and large scale problems (O ∈ R5000×20 with rank-3) in (b,d). The cases with wellsampled and under-sampled data on subspaces are shown at the top and bottom rows respectively. X-axis represents the percentage of outlier, and Y-axis represents the average error. LMaFit, Zheng et al . and Eriksson et al . are MF methods. MF methods also result in low accuracy under the case of deficient number of samples. Comparing (b) and (d), MF methods are prone to the data under-sampled on subspaces, because bilinear model enforcedly constrains the target rank and excessively attempts to match it.\n−1 0 1 10\n−8\n10 −6\n10 −4\n10 −2\n10 0\nTarget rank offset\nSVP LMaFit Zheng Ours\n(a) Well sampled.\n−1 0 1 10\n−8\n10 −6\n10 −4\n10 −2\n10 0\nTarget rank offset\nSVP LMaFit Zheng Ours\n(b) Under sampled.\nFig. 9: Effects when the target rank is incorrectly set. We set the input target rank N = Ntrue + toffset, where the truth rank Ntrue = 3. The lower value the better.\nto a rank-2 matrix. This situation often happens when the last basis is less supported by a few true samples. This is also called the unbalanced singular values case (e.g. σ(A) = [100, 10, 1e−1]). Our results shown in Fig. 8- (c,d) have smaller errors than results from LMaFit, Zheng et al. and Eriksson et al., even for the under-sampled cases."
    }, {
      "heading" : "4.1.5 Incorrect Setting of Target Rank",
      "text" : "Our method takes advantage of the target rank from the problem definition. When the target rank is set incorrectly, the question of the behavior of our method naturally arises. For the sake of completeness, we have experimented with incorrect target rank setting in Fig. 9.\nWe considered the situation where the rank is known, but ambiguous within some bound (e.g., the truth rank is 3, but ambiguous within rank-{2,3,4}). The data construction is similar to the experiments conducted in Sec. 4.1.4, i.e. well-sample and under-sampled data cases. The rank-3 matrices O ∈ R3000×100 are used for experiment. Fig. 9 shows that MF based methods are prone to incorrect target rank setting. Interestingly, for the data under-sampled on\n9 5 10 15 20 25 30 10 −8 10 −6 10 −4 10 −2 10 0\nIterations\nT er\nm in\nat io\nn C\nrit er\nio n\nRPCA,r=4 Ours, r=4 RPCA,r=3 Ours, r=3 RPCA,r=2 Ours, r=2\n(a) Stop criterion\n5 10 15 20 25 30 10\n−8\n10 −6\n10 −4\n10 −2\n10 0\nIterations\nR el\nat iv\ne E\nrr or\nRPCA,r=4 Ours, r=4 RPCA,r=3 Ours, r=3 RPCA,r=2 Ours, r=2\n(b) Relative Error\nmin(m,n).\nsubspaces, MF based methods with a target rank lower than the true rank show better performance than for the well-sampled data case. This is because the bilinear model enforcedly constrains the target rank within the bilinear matrix structure. Therefore, when the 3rd basis is weakly supported by samples, fitting with a rank-2 bilinear model only for the 1st and 2nd basis provides better precision than using a rank-3 bilinear model. This result is consistent with Fig. 8-(b,d)."
    }, {
      "heading" : "4.1.6 Convergence Behavior",
      "text" : "To examine the convergence behavior of both RPCA [31] and our method, we plot the evolution of the relative errors ‖AGT−Â‖F ‖AGT ‖F + ‖EGT−Ê‖F ‖EGT ‖F and termination criteria ‖O−A−E‖F ‖O‖F over the iterations in Fig. 10-(a) and (b), respectively. We randomly generate 5000 × 40 matrices for the rank-2, 3, 4 cases, and the average value over the trials is computed.\nWe use the MATLAB implementation of RPCA provided by Wright et al. [44]. We run our method until convergence, and we observe that it is terminated at similar moments with RPCA as shown in Fig. 10-(a). Also, our method takes the same amount of time as inexact ALM based RPCA [31]. Fig. 10-(b) also shows that our method provides higher accuracy than RPCA as well as a gradual convergence under the same termination criterion."
    }, {
      "heading" : "4.1.7 Lambda (λ) parameter",
      "text" : "We conduct all the experiments in this paper with the same λ parameter recommended by Candès et al. [9]. For completeness, we show in this section how the choice of λ can affect the solution of both RPCA and ours. Note that tuning the optimal λ to balance the nuclear norm and sparsity is not possible unless the ground truth solution is known as discussed by Chandrasekaran et al. [10]. Thus, the results provided here are only for reference. Fig. 11 shows normalized MSE when λ varies, where λ = L/ √ max(m,n). The results show that our method consistently produces less errors than RPCA under different settings of λ.\nFig. 12: Illustration of the observed intensity values for (a) saturation region, (b) moving object, and (c) consistent cases. Solid lines denote ideal relationship between intensity and exposure, and dots and dotted lines denote the observed intensities.\n(a) (b) (c) (d) (e)\nFig. 13: Comparison of the low-rank matrix and sparse error results between RPCA and ours on the Arch dataset [21]. (a) Three out of the five input multi–exposure image samples. Low-rank (b,d) and sparse error (c,e) results, respectively obtained by RPCA (b,c) and the proposed approach (d,e)."
    }, {
      "heading" : "4.2 Real-world Applications",
      "text" : ""
    }, {
      "heading" : "4.2.1 High Dynamic Range (HDR) Imaging",
      "text" : "We apply the proposed method for modeling a background scene and a ghost-free HDR composition. The input is a set of low dynamic range (LDR) images and the goal is to composite an HDR image using RPCA to reject outliers, such as moving objects and saturations, in the LDR images. We assume that the differently exposed images Ii are aligned and the camera response function (CRF) is calibrated (or linear). Then, the captured images can be represented as Ii = κRΔti, where R denotes the sensor irradiance, Δti is the exposure time of the i-th image, and κ is a positive scalar. We construct the observed intensity matrix O ∈ Rm×n = [vec(I1)| · · · |vec(In)] by stacking the vectorized input images, where m and n are the number of pixels and images respectively. Since the intensities of the input images are linearly dependent, the ideal solution of this problem is rank-1. However, in practice, rank(O) is higher than 1 due to moving objects, saturation or other artifacts (illustrated in Fig. 12). We apply RPCA (nuclear norm) and our method (PSSV) to each color channel independently, in order to separate artifacts and background scene.\nThe Arch and Sculpture Garden datasets from Gallo et al. [21] are used for evaluation. The estimated backgrounds as low-rank matrix and the sparse outliers from RPCA and our method are shown in Fig. 13. The example in Fig. 13-(a) consists of only 5 input images which is very limited. Ideally, the decomposed low-rank matrix A = [vec(A1)| · · · |vec(An)] consists of relative intensities of the\n10\nbackground scene from which moving objects or saturation artifacts should be removed (see Fig. 13-(b,d)). RPCA returns a low-rank matrix whose magnitude differs drastically from the input image, as shown in Fig. 13-(b). Moreover RPCA yields a dense non-zero entries in E, instead of being sparse, as shown in Fig. 13-(c). This situation is similar to the example in Fig. 2 where the minimum nuclear norm favors a solution with smaller variance of magnitudes. In contrast, our proposed method shows a correctly modeled background scene and successfully detects outlier regions, as shown in Fig. 13-(d,e). For displaying the sparse components in Fig. 13-(c,e), each color component (R,G,B) is set with (|ER|, |EG|, |EB |), where E{R,G,B} denotes sparse error matrix for each channel.\nAfter we estimate the low-rank matrix, we composite the HDR images using the standard method of Debevec et al. [15]. The final HDR results are shown in Fig. 14. Because the background modelling by RPCA is inaccurate, ghosting appears in their HDR results. In contrast, our results are ghost-free."
    }, {
      "heading" : "4.2.2 Motion Detection by Temporal Edge",
      "text" : "RPCA-based background modeling for surveillance purpose requires a large number of observations to estimate background and moving objects under global illumination changes. Such requirement is not suitable for online algorithm in surveillance. Using a few images as input, the moving region detection by RPCA could fail due to the limited number of observations. In this problem, we observe that edge images make moving object boundaries more sparse and they rarely overlap. We stack a few n edge images (obtained by Sobel operator) in video sequence as column vectors of a matrix O ∈ Rm×n = [vec(O1)| · · · |vec(On)]. Without moving objects, the edge pixels on the background texture are static, so the matrix O should be low-rank, essentially, rank-1. Since moving object regions are not consistent with background edges, the regions can be modeled as sparse outliers.\nFig. 15 shows the comparisons with RPCA and the proposed method. RPCA fails to decompose low–rank and sparse matrix in Fig. 15-(b) due to deficient observations where n = 5. On the other hand, our method successfully estimates moving object boundary, and the results are similar to the one obtained with many observations in Fig. 15- (e).\n11"
    }, {
      "heading" : "4.2.3 Outlier Rejection for Photometric Stereo",
      "text" : "Intensity observation is modeled in Lambertian photometric stereo as O = [vec(O1)| · · · |vec(On)] = N L4, where O ∈ Rm×n, N ∈ R3×m and L ∈ R3×n denote measured intensity, normal and light direction matrix, respectively, and m and n are the number of pixels and images. Hayakawa et al. [23] show that the intensity matrix lies in a subspace of rank 3, as illustrated in Fig. 16. However, this constraint is hardly satisfied in real situations due to shadow from self-occlusion, saturation and some object materials which do not exactly follow the Lambertian diffusion model. Considering the rank-3 constraint, the artifacts mentioned above can be regarded as sparse outliers and we get a lowrank structure as O = N L+E.\nThe robust photometric stereo with outlier rejection can be formulated as a RPCA problem as suggested by Wu et al. [45]. We compare our method with the standard least square (LS) method [42] and RPCA by Wu et al. [45]. Among them, Wu et al. and our method do not require light information (i.e. uncalibrated setting), while the LS method requires light calibration a priori. Thus, RPCA based outlier rejection is a more challenging problem than the robust regression given light information [26]. The LS based photometric stereo estimates the normals by minimizing ‖O−N L‖2F . We corrupt some input images by painted artifacts to mimic outliers. The corrupted inputs are included in 2 out of n = 5 inputs (Fig. 17 and Fig. 18-(top)), and 4 out of n = 10 inputs (Fig. 18-(bottom)). Outlier rejection results are shown in Fig. 17. We present qualitative comparison of normal recovery results in Fig. 17 and Fig. 18. Wu et al. return a planar surface normal when the rank of input matrix is lower than 3 due to the lack of observations (italic in Table 2). When more input images are available, RPCA begins to return detail preserved results, as shown in Fig. 18-(e). On the other hand, our method consistently provides robust results for both limited and sufficient observations, as shown in Fig. 18-(c,f).\nFor quantitative results, we use the Bunny dataset [26] generated using the Cook-Torrance reflectance model and consisting of 40 different lighting conditions. The average ratio of specular and shadow regions in Bunny are 8.4% and 24% respectively, which act as outliers. Table 3 shows quantitative results. We vary the number of images and add 5% of uniformly distributed corruption. Each value in Table 3 is averaged over 20 randomly selected test sets. Wu et al. [45] produce degenerated results, as the rank of the resulting matrix is lower than 3 due to the lack of supports from the observations. When more input images are available, RPCA returns more satisfying results, but still the accuracy is lower than LS method. In contrast,\n4. Note that the intensities only on the object region are used in the observation matrix O with the corresponding object mask.\nour method provides accurate results for both limited and sufficient observations."
    }, {
      "heading" : "4.2.4 Batch Image Alignment",
      "text" : "Given several images of an object of interest (e.g. face), the batch image alignment task aims to align them to a fixed canonical template [6], [38]. In this problem, we search for a transformation gi for each image Ii to make the images linearly correlated. We note g the set of transformations: g = {g1, . . . , gn} where n is the number of images and write O ◦ g = [vec(I1 ◦ gi)| · · · |vec(In ◦ gn)]. Contrary to the formulation of Peng et al. [38], we consider PSSV mathematically formulated as follows:\nargmin A,E,g\n‖A‖p=N + λ‖E‖1, s.t. O ◦ g = A+E. (24)\nWe applied our approach to the head dataset acquired under varying poses (see Fig. 20-(a)) [38]. For linearly correlated noise-free batch images, the rank is N = 1, when the transformations for exact image alignment are estimated. Our results of alignment, low-rank estimation and error sparsity are shown in Fig. 20-(e,f,g). Compared to the results obtained by RASL [38], our method can correctly detect the outliers (Fig. 20-(c) v.s. Fig. 20-(f)), even with only 3 input images.\nOur method can correctly detect the outliers and also robustly align the images even if the geometric model has more degrees of freedom than an affine homography model. Detailed comparisons in Fig. 22 show the average image obtained from the aligned image stack by each method. If well aligned, the average image should show seamless image without duplicated edges. Our results show fine average images due to more accurate homography estimation than RASL."
    }, {
      "heading" : "4.2.5 Image Recovery",
      "text" : "Images of natural scenes follow natural statistics [25]. As shown by Hu et al. [24], information of image scenes is\n12\ndominated by the top 20 singular values, which is lowrank. Hu et al. [24] proposed a matrix completion method with the truncated nuclear norm (TNN) as introduced in Sec. 2. We formulate the matrix completion as\nargmin A,B\n‖A‖p, s.t. A = B,PΩ(B) = PΩ(O), (25)\nwhere PΩ(·) is the orthogonal projection operator setting [PΩ(X)]i,j = [X]i,j for (i, j) ∈ Ω and 0 otherwise. Although the auxiliary variable B looks unnecessary, with the affine constraint (the projection operator) it makes the efficient PSVT operator applicable in ADMM algorithm. We set ρ = 1.05 in ADMM (see the supplementary material and Alg. 2), and for fairness, we follow the same setting as suggested by Hu et al., i.e. μ0 = 1e−3.\nFig. 23 shows the comparison with Hu et al. Since the proposed method has a similar objective function with Hu et al., PSNR of recovered images are similar (the max. difference is 0.134), but our method requires fewer iterations and runs 4 times faster. Because Hu et al. perform local approximation to solve PSSV, their algorithm requires outer\nloop to fix subspaces and inner loop to minimize nuclear norm with affine constraint. In contrast, our method directly minimizes PSSV, which is a key contribution of our work."
    }, {
      "heading" : "5 DISCUSSIONS AND CONCLUSION",
      "text" : "In this paper, we revisited the rank minimization method in RPCA for low-level vision problems. When the target rank is known, we show that, by modifying the objective function from the nuclear norm to PSSV, we can achieve a better control of the target rank of the low-rank solution, even when the number of observations is limited. The appealing advantage of our solution is that it can be easily utilized in existing algorithms, e.g. ADMM [31], and the efficient computation properties still hold. The generality and the effectiveness of our approach are supported through numerous and extensive experiments on both synthetic examples and several real-world applications which outperform the conventional nuclear norm objective function. We do not consider scalability issues of our method in this paper, but the recent approach suggested by Oh et al. [37] allow to speed-up the application of our method. An interesting direction of future work is the mathematical analysis of the properties of our partial sum objective function compared\n13\nto the nuclear norm solution. In the following, we discuss some open questions related to our paper. Sufficient number of samples versus minimum number of samples In our experimental analysis, we found that our solution is more robust than the nuclear norm solution when facing a limited number of samples. DefiningK as the (theoretical) minimum number of samples for processing, e.g. 2 images for HDR, 3 images for photometric stereo, our approach requires more than K samples for a robust model estimation and outlier rejection. We believe that the number of needed additional samples depends on the problem setting, e.g. the shape of feature space or the distribution of the samples. Target rank While our formulation implicitly encourages a target rank constraint in the resulting matrix, this constraint is hardly enforced. We discuss here two possible scenarios that can produce the resulting matrix having a rank different from the target rank. The first scenario is when a very limited number of samples are observed. In such case, PSVT can produce a deficient rank lower than the target rank when the span of the observed samples is less than the target rank, but this case is a fundamental limitation of under-sampling rather than a conceptual limitation of our approach. Another scenario is due to too much noise (especially for Gaussian noise that does not follow the sparsity property) in the observed samples which results in large singular values in the residual ranks. In this case, a solution to satisfy the rank constraint is to increase τ in Eq. (17). When τ is equal to infinity, our PSVT solution is close to the result using singular value projection [27]. However, the projection method enforcing target rank could produce an over-fitting solution due to the mentioned noise effects."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "We would like to thank reviewers and associate editor for their valuable comments, and thank Steve Seitz and Dan Goldman for the photometric stereo dataset. The work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIP) (No. 2010-0028680). In So Kweon is the corresponding author."
    } ],
    "references" : [ {
      "title" : "Nonconvex analysis",
      "author" : [ "A. Bagirov", "N. Karmitsa", "M. Makela" ],
      "venue" : "Introduction to Nonsmooth Optimization, pages 61–116. Springer International Publishing",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A singular value thresholding algorithm for matrix completion",
      "author" : [ "J.-F. Cai", "E.J. Candès", "Z. Shen" ],
      "venue" : "SIAM Journal on Optimization, 20(4):1956– 1982",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "An appropriate subdifferential for quasiconvex functions",
      "author" : [ "A. Daniilidis", "N. Hadjisavvas", "J.-E. Martinez-Legaz" ],
      "venue" : "SIAM Journal on Optimization, 12(2):407–420",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "The augmented Lagrange multiplier method for exact recovery of corrupted low-rank matrices",
      "author" : [ "Z. Lin", "M. Chen", "Y. Ma" ],
      "venue" : "Technical Report UILU-ENG-09-2215, UIUC",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Partial sum minimization of singular values in robust PCA: Algorithm and applications",
      "author" : [ "T.-H. Oh", "Y.-W. Tai", "J.-C. Bazin", "H. Kim", "I.S. Kweon" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "On the moreau–yosida regularization of the vector k-norm related functions",
      "author" : [ "B. Wu", "C. Ding", "D. Sun", "K.-C. Toh" ],
      "venue" : "SIAM Journal on Optimization, 24(2):766–794",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "1 INTRODUCTION VArious low-level vision applications, including High Dynamic Range (HDR) [35], [36], photometric stereo [3], [23], batch image alignment [38] and factorization-based structure from motion [5], [41], can be formulated as a low-rank matrix recovery problem.",
      "startOffset" : 120,
      "endOffset" : 123
    }, {
      "referenceID" : 4,
      "context" : "1 INTRODUCTION VArious low-level vision applications, including High Dynamic Range (HDR) [35], [36], photometric stereo [3], [23], batch image alignment [38] and factorization-based structure from motion [5], [41], can be formulated as a low-rank matrix recovery problem.",
      "startOffset" : 204,
      "endOffset" : 207
    }, {
      "referenceID" : 0,
      "context" : "[1] proved that convex approximation by nuclear norm can still achieve bounded and stable results even under small noise measurements.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "[7] revisited the relationship between nuclear norm regularization and bilinear MF model [2], and proposed a rank continuation heuristic to avoid local minima.",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 3,
      "context" : "The ADMM for non-convex problems can be considered as a local optimization method, which aims to converge to a point with better objective value [4].",
      "startOffset" : 145,
      "endOffset" : 148
    }, {
      "referenceID" : 0,
      "context" : "The color magnitude represents the success ratio [0,1].",
      "startOffset" : 49,
      "endOffset" : 54
    }, {
      "referenceID" : 0,
      "context" : "The color magnitude represents the success ratio [0,1].",
      "startOffset" : 49,
      "endOffset" : 54
    }, {
      "referenceID" : 0,
      "context" : "The color magnitude represents the success ratio [0,1].",
      "startOffset" : 49,
      "endOffset" : 54
    }, {
      "referenceID" : 0,
      "context" : "The weight vector used to span each column vector of AGT is randomly sampled from the uniform distribution U [0, 1].",
      "startOffset" : 109,
      "endOffset" : 115
    }, {
      "referenceID" : 0,
      "context" : "The selected entries are corrupted by random noise from U [0, 1].",
      "startOffset" : 58,
      "endOffset" : 64
    }, {
      "referenceID" : 5,
      "context" : "face), the batch image alignment task aims to align them to a fixed canonical template [6], [38].",
      "startOffset" : 87,
      "endOffset" : 90
    } ],
    "year" : 2015,
    "abstractText" : "Robust Principal Component Analysis (RPCA) via rank minimization is a powerful tool for recovering underlying low-rank structure of clean data corrupted with sparse noise/outliers. In many low-level vision problems, not only it is known that the underlying structure of clean data is low-rank, but the exact rank of clean data is also known. Yet, when applying conventional rank minimization for those problems, the objective function is formulated in a way that does not fully utilize a priori target rank information about the problems. This observation motivates us to investigate whether there is a better alternative solution when using rank minimization. In this paper, instead of minimizing the nuclear norm, we propose to minimize the partial sum of singular values, which implicitly encourages the target rank constraint. Our experimental analyses show that, when the number of samples is deficient, our approach leads to a higher success rate than conventional rank minimization, while the solutions obtained by the two approaches are almost identical when the number of samples is more than sufficient. We apply our approach to various low-level vision problems, e.g. high dynamic range imaging, motion edge detection, photometric stereo, image alignment and recovery, and show that our results outperform those obtained by the conventional nuclear norm rank minimization method.",
    "creator" : "LaTeX with hyperref package"
  }
}