{
  "name" : "1510.08660.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "RATM: Recurrent Attentive Tracking Model",
    "authors" : [ "Samira Ebrahimi Kahou", "Vincent Michalski", "Roland Memisevic" ],
    "emails" : [ "samira.ebrahimi-kahou@polymtl.ca", "vincent.michalski@umontreal.ca", "roland.memisevic@umontreal.ca" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Traditional work on object tracking uses different sampling methods to select and evaluate multiple window candidates based on similarity in feature space. [1]. In this work we introduce a tracking method, that predicts the position of an object at time t only based on prior information, i.e. without actually considering the content of the current frame.\nRecently, a fully differentiable attention mechanism, that can selectively read and write in images was introduced and used to build a model for sequential generation of images [2]. Interestingly, in the experiments of [2] on handwritten digits, the read mechanism learned to trace contours and the write mechanism generated digits in a continuous motion of the write window, hinting at the potential of this method for tracking applications. The proposed recurrent attentive tracking model (RATM) employs a modified version of that read mechanism. Previous work has investigated the application of attention mechanisms for tracking with neural networks [3]. In contrast to that work, we develop a fully-integrated neural framework, that can completely be trained by back-propagation."
    }, {
      "heading" : "2 Method",
      "text" : "The proposed model mainly consists of two components: an attention mechanism that extracts patches from the input images and a recurrent neural network (RNN), which predicts attention parameters, i.e. it predicts where to look in the next frame. Sections 2.1 and 2.2 describe the attention mechanism and the RNN, respectively."
    }, {
      "heading" : "2.1 Attention",
      "text" : "The attention mechanism introduced in [2] extracts glimpses from the input data by applying a set of 2D Gaussian window filters. Each filter response corresponds to one pixel of the glimpse. Exploiting the separability of 2D Gaussian windows, this attention mechanism constructs an N × N grid of two-dimensional Gaussian filters by sequentially applying a set of row filters and a set of column\nar X\niv :1\n51 0.\n08 66\n0v 1\n[ cs\n.L G\n] 2\n9 O\n(a) Layout of the Recurrent Attentive Tracking Model. The black arrows indicate that the hidden states affect the patch extraction in the next time step. (b) Top: A full image, Bottom: The selected attention patch, presented as network input.\nfilters. The grid has 4 parameters1, the grid center gX , gY , the isotropic standard deviation σ and the stride between grid points δ. For the filter at row i, column j in the attention patch, the mean locations µiX , µ j Y are computed as follows:\nµiX = gX + (i− N\n2 − 0.5) · δ, µjY = gY + (j −\nN\n2 − 0.5) · δ (1)\nThe parameters are dynamically generated as a linear transformation of a vector h:\n(g̃X , g̃Y , σ̃, δ̃) = Wh, (2)\nfollowed by normalization of the parameters:\ngX = g̃X + 1\n2 , gY =\ng̃Y + 1\n2 , δ = max(A,B)− 1 N − 1 · |δ̃|, σ = |σ̃|, (3)\nwhere A and B are the width and height of the input image.\nIn contrast to the original implementation in [2], we use the absolute value function to ensure positivity of δ and σ. The intuition behind this choice is the following: In our experiments we observed that the stride and standard deviation parameters tend to saturate at low values, causing the attention window to zoom in into a single pixel. This effectively inhibited gradient flow through neighboring pixels of the attention filters. By replacing the exponential function with the absolute value, we introduce a discontinuity at x = 0. Piecewise linear activation functions have been shown to benefit optimization [4] and the absolute value function is a convenient trade-off between the harsh zeroing of all negative inputs of the Rectified Linear Unit (ReLU) and the extreme saturation for highly negative inputs of the exponential function.\nThe filters which are used to extract a patch p from an image x are computed as follows:\nFX [i, a] = 1\nZX exp\n( − (a− µ i X) 2\n2σ2\n) , FY [j, b] = 1\nZY exp\n( − (b− µjY )2\n2σ2\n) (4)\nIn Equation 4, a and b correspond to column and row indices in the input image. The N ×N patch p is then extracted from the image x by separate application of both filters: p = FY xFTX .\nAn example of the patch extraction is shown in Figure 1b."
    }, {
      "heading" : "2.2 Recurrent Neural Network",
      "text" : "Advances in the optimization of RNNs enable their successful application in a wide range of tasks [5, 6, 7]. The most simple RNN consists of one input, one hidden and one output layer. In time-step\n1The original read mechanism from [2] also adds a scalar intensity parameter, that is multiplied to filter responses.\nt, the network, based on the input frame xt and the previous hidden state ht−1, computes the new hidden state\nht = σ(Winxt +Wrecht−1), (5)\nwhere σ is a non-linear activation function, Win is the input and Wrec is the recurrent weight matrix. While the application of the more sophisticated long-short-term memory [7] and gated recurrent units [6] is very common in recent work, we rely on a simpler variant of recurrent networks, called IRNN [8], in which Wrec is initialized with a scaled version of the identity matrix and σ in Equation 5 corresponds to the element-wise ReLU activation function [4]\nσ(x) = max(0, x). (6)\nIn our experiments we use the IRNN implementation of [9]."
    }, {
      "heading" : "2.3 Recurrent Attentive Tracking Model",
      "text" : "The task of tracking involves the mapping of a sequence of inputs X = (x1,x2, . . . ,xT ) to a sequence of locations Y = (y1,y2, . . . ,yT ). Given that for the prediction ŷt of an object’s location at time t, the trajectory (y1, . . . ,yt−1) usually contains highly relevant contextual information, it is suitable to choose a hidden state model which has the capacity of building a representation of this trajectory. The proposed RATM is a fully differentiable neural network, consisting of the described attention mechanism and an RNN. The hidden state vector ht of the RNN is mapped to window parameters according to Equations 2 and 3. The resulting image patch returned by the attention mechanism is then passed as input xt+1 to the RNN, which computes ht+1 according to Equation 5. A sketch of the model is shown in Figure 1a. Instead of passing the raw input patch to the RNN, a pre-trained model can be used to extract image features, e.g. a convolutional neural network (CNN). Note that the size of this feature model affects memory consumption and/or computation speed of the whole model, especially during training, because gradients have to be back-propagated through its instances in all time-steps. In Section 3, we explore various ways of generating a training signal for gradient descent training of RATM."
    }, {
      "heading" : "3 Experiments",
      "text" : "In this section we present some of the experiments with RATM and comment on qualitative results. Unless noted otherwise, the initial parameters of the attention mechanism are set such that the resulting first patch roughly covers the whole image."
    }, {
      "heading" : "3.1 Bouncing Balls",
      "text" : "For our initial experiment, we used the bouncing balls data set [10] with 32 frames. In the objective function, we minimized only the mean-squared error between the selected patch in the last frame and a target patch which is simply a cropped white ball image as the shape and color of the object is constant across the whole data set. The dynamics in this data set are very limited and even though the tracking looks accurate, it can be explained by the RNNs potential for memorizing the set of possible trajectories. For this reason, we experimented with more challenging data sets. Figure 1 shows the results of tracking a ball in a test sequence in the top row."
    }, {
      "heading" : "3.2 MNIST",
      "text" : "Single-Digit: To better understand the learning ability of RATM, we generated a new data set by moving one digit randomly drawn from MNIST [11]. We respected the same data split for training and testing as MNIST, i.e. digits were drawn from the training split to generate training sequences and testing split for testing. The generated videos have frame size 100×100 and show a digit moving in a random walk with momentum against a black background. This experiment is different from bouncing balls tracking since digits are from different classes which the model has to recognize. As an objective function we used the average classification error provided by a pre-trained shallow CNN, that classified the patches of each time-step. This CNN is trained on the MNIST data set and the model parameters remained fixed during RATM training. The model failed to learn only based on the classification term. To amend this, we initially tried to add a penalty on the distance between\ncorners of the ground truth bounding boxes and the corners of the attention grid. This again led to unsatisfactory results and we changed the localization cost to the mean-squared error between ground truth center coordinates and patch center. This modification helped the model to reliably detect and track the digits. The localization term helped in the early stages of training to guide RATM to track the digits, whereas the classification term encourages the model to properly zoom into the image and maximize classification accuracy. Training is done on sequences with only 10 frames. The classification and localization penalties were applied at every time-step. At test time, the CNN is switched off and we let the model track test sequences of 30 frames. Figure 1 shows one test sample in the middle row.\nMulti-Digit: It was interesting to see how robust RATM is in presence of another moving digit in the background. Thus, we generated new sequences by modifying the bouncing balls script [10]. The balls were replaced by randomly drawn MNIST digits. We also added a random walk with momentum to the motion vector. The cost function and settings are the same as in the single digit experiment, except that we set the initial bias of the attention mechanism such that the initial window is centered on the digit to be tracked. The model was trained on 10 frames and was able to focus on digits at least for 15 frames in test cases. Figure 1 shows tracking results on a test sequence in the bottom row."
    }, {
      "heading" : "4 Conclusion and Future Work",
      "text" : "We showed that the proposed method, RATM, is able to learn to track objects in experiments on artificial data. We experimented with RATM on the Caltech Pedestrian Dataset [12, 13], but the number of long annotated training sequences which contain the same subject is quite low for deep learning standards. Moreover, due to the high frame rate motion is very limited, so the model learned to predict almost a static window. The results were inconclusive and are not presented here. Currently we are experimenting with other data sets, not only containing pedestrians, and try to provide quantitative results as a baseline for comparison.\nAs a long-term project goal, we plan to explore multi-object tracking and to investigate the representation learned by the network. In particular, we will focus on the model’s ability to deal with occlusions and to compress object identity and motion trajectory in its hidden state."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors would like to thank the developers of Theano [14, 15] and Jörg Bornschein for helpful discussions. This work was supported by an NSERC Discovery Award and the German BMBF, project 01GQ0841. We also thank the Canadian Foundation for Innovation (CFI) for support under the Leaders program."
    } ],
    "references" : [ {
      "title" : "Visual tracking: An experimental survey",
      "author" : [ "Arnold W M Smeulders", "Dung M Chu", "Rita Cucchiara", "Simone Calderara", "Afshin Dehghan", "Mubarak Shah" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2014
    }, {
      "title" : "DRAW: A recurrent neural network for image generation",
      "author" : [ "Karol Gregor", "Ivo Danihelka", "Alex Graves", "Daan Wierstra" ],
      "venue" : "CoRR, abs/1502.04623,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2015
    }, {
      "title" : "Learning where to attend with deep architectures for image tracking",
      "author" : [ "Misha Denil", "Loris Bazzani", "Hugo Larochelle", "Nando de Freitas" ],
      "venue" : "CoRR, abs/1109.3737,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2011
    }, {
      "title" : "Rectified linear units improve restricted boltzmann machines",
      "author" : [ "Vinod Nair", "Geoffrey E Hinton" ],
      "venue" : "In Proceedings of the 27th International Conference on Machine Learning",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2010
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2014
    }, {
      "title" : "Learning phrase representations using rnn encoderdecoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart Van Merriënboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1406.1078,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2014
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1997
    }, {
      "title" : "A simple way to initialize recurrent networks of rectified linear units",
      "author" : [ "Quoc V Le", "Navdeep Jaitly", "Geoffrey E Hinton" ],
      "venue" : "arXiv preprint arXiv:1504.00941,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2015
    }, {
      "title" : "Recurrent neural networks for emotion recognition in video",
      "author" : [ "Samira Ebrahimi Kahou", "Vincent Michalski", "Kishore Konda", "Roland Memisevic", "Christopher Pal" ],
      "venue" : "In Proceedings of the 17th ACM on International Conference on Multimodal Interaction, ICMI ’15,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2015
    }, {
      "title" : "The recurrent temporal restricted boltzmann machine",
      "author" : [ "Ilya Sutskever", "Geoffrey E Hinton", "Graham W Taylor" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2009
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Yann Lecun", "Leon Bottou", "Yoshua Bengio", "Patrick Haffner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1998
    }, {
      "title" : "Pedestrian detection: A benchmark",
      "author" : [ "Piotr Dollár", "Christian Wojek", "Bernt Schiele", "Pietro Perona" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2009
    }, {
      "title" : "Pedestrian detection: An evaluation of the state of the art",
      "author" : [ "Piotr Dollár", "Christian Wojek", "Bernt Schiele", "Pietro Perona" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2012
    }, {
      "title" : "Theano: new features and speed improvements",
      "author" : [ "Frédéric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "David Warde-Farley", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1211.5590,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2012
    }, {
      "title" : "Theano: a cpu and gpu math expression compiler",
      "author" : [ "James Bergstra", "Olivier Breuleux", "Frédéric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio" ],
      "venue" : "In Proceedings of the Python for scientific computing conference (SciPy),",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "[1].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "Recently, a fully differentiable attention mechanism, that can selectively read and write in images was introduced and used to build a model for sequential generation of images [2].",
      "startOffset" : 177,
      "endOffset" : 180
    }, {
      "referenceID" : 1,
      "context" : "Interestingly, in the experiments of [2] on handwritten digits, the read mechanism learned to trace contours and the write mechanism generated digits in a continuous motion of the write window, hinting at the potential of this method for tracking applications.",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 2,
      "context" : "Previous work has investigated the application of attention mechanisms for tracking with neural networks [3].",
      "startOffset" : 105,
      "endOffset" : 108
    }, {
      "referenceID" : 1,
      "context" : "The attention mechanism introduced in [2] extracts glimpses from the input data by applying a set of 2D Gaussian window filters.",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 1,
      "context" : "In contrast to the original implementation in [2], we use the absolute value function to ensure positivity of δ and σ.",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 3,
      "context" : "Piecewise linear activation functions have been shown to benefit optimization [4] and the absolute value function is a convenient trade-off between the harsh zeroing of all negative inputs of the Rectified Linear Unit (ReLU) and the extreme saturation for highly negative inputs of the exponential function.",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 4,
      "context" : "Advances in the optimization of RNNs enable their successful application in a wide range of tasks [5, 6, 7].",
      "startOffset" : 98,
      "endOffset" : 107
    }, {
      "referenceID" : 5,
      "context" : "Advances in the optimization of RNNs enable their successful application in a wide range of tasks [5, 6, 7].",
      "startOffset" : 98,
      "endOffset" : 107
    }, {
      "referenceID" : 6,
      "context" : "Advances in the optimization of RNNs enable their successful application in a wide range of tasks [5, 6, 7].",
      "startOffset" : 98,
      "endOffset" : 107
    }, {
      "referenceID" : 1,
      "context" : "The original read mechanism from [2] also adds a scalar intensity parameter, that is multiplied to filter responses.",
      "startOffset" : 33,
      "endOffset" : 36
    }, {
      "referenceID" : 6,
      "context" : "While the application of the more sophisticated long-short-term memory [7] and gated recurrent units [6] is very common in recent work, we rely on a simpler variant of recurrent networks, called IRNN [8], in which Wrec is initialized with a scaled version of the identity matrix and σ in Equation 5 corresponds to the element-wise ReLU activation function [4]",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 5,
      "context" : "While the application of the more sophisticated long-short-term memory [7] and gated recurrent units [6] is very common in recent work, we rely on a simpler variant of recurrent networks, called IRNN [8], in which Wrec is initialized with a scaled version of the identity matrix and σ in Equation 5 corresponds to the element-wise ReLU activation function [4]",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 7,
      "context" : "While the application of the more sophisticated long-short-term memory [7] and gated recurrent units [6] is very common in recent work, we rely on a simpler variant of recurrent networks, called IRNN [8], in which Wrec is initialized with a scaled version of the identity matrix and σ in Equation 5 corresponds to the element-wise ReLU activation function [4]",
      "startOffset" : 200,
      "endOffset" : 203
    }, {
      "referenceID" : 3,
      "context" : "While the application of the more sophisticated long-short-term memory [7] and gated recurrent units [6] is very common in recent work, we rely on a simpler variant of recurrent networks, called IRNN [8], in which Wrec is initialized with a scaled version of the identity matrix and σ in Equation 5 corresponds to the element-wise ReLU activation function [4]",
      "startOffset" : 356,
      "endOffset" : 359
    }, {
      "referenceID" : 8,
      "context" : "In our experiments we use the IRNN implementation of [9].",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 9,
      "context" : "For our initial experiment, we used the bouncing balls data set [10] with 32 frames.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 10,
      "context" : "Single-Digit: To better understand the learning ability of RATM, we generated a new data set by moving one digit randomly drawn from MNIST [11].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 9,
      "context" : "Thus, we generated new sequences by modifying the bouncing balls script [10].",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 11,
      "context" : "We experimented with RATM on the Caltech Pedestrian Dataset [12, 13], but the number of long annotated training sequences which contain the same subject is quite low for deep learning standards.",
      "startOffset" : 60,
      "endOffset" : 68
    }, {
      "referenceID" : 12,
      "context" : "We experimented with RATM on the Caltech Pedestrian Dataset [12, 13], but the number of long annotated training sequences which contain the same subject is quite low for deep learning standards.",
      "startOffset" : 60,
      "endOffset" : 68
    }, {
      "referenceID" : 13,
      "context" : "The authors would like to thank the developers of Theano [14, 15] and Jörg Bornschein for helpful discussions.",
      "startOffset" : 57,
      "endOffset" : 65
    }, {
      "referenceID" : 14,
      "context" : "The authors would like to thank the developers of Theano [14, 15] and Jörg Bornschein for helpful discussions.",
      "startOffset" : 57,
      "endOffset" : 65
    } ],
    "year" : 2015,
    "abstractText" : "This work presents an attention mechanism-based neural network approach for tracking objects in video. A recurrent neural network is trained to predict the position of an object at time t + 1 given a series of selective glimpses into video frames at time steps 1 to t. The proposed recurrent attentive tracking model can be trained using simple gradient-based training. Various settings are explored in experiments on artificial data to justify design choices.",
    "creator" : "LaTeX with hyperref package"
  }
}