{
  "name" : "1705.06463.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Universal Dependencies Parsing for Colloquial Singaporean English",
    "authors" : [ "Hongmin Wang", "Yue Zhang", "GuangYong Leonard Chan", "Jie Yang", "Hai Leong Chieu" ],
    "emails" : [ "zhang}@sutd.edu.sg", "yang@mymail.sutd.edu.sg", "chaileon}@dso.org.sg" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Languages evolve temporally and geographically, both in vocabulary as well as in syntactic structures. When major languages such as English or French are adopted in another culture as the primary language, they often mix with existing languages or dialects in that culture and evolve into a stable language called a creole. Examples of creoles include the French-based Haitian Creole, and Colloquial Singaporean English (Singlish) (MianLian and Platt, 1993), an English-based creole. While the majority of the natural language processing (NLP) research attention has been focused on the major languages, little work has been done on adapting the components to creoles. One notable body of work originated from the featured\ntranslation task of the EMNLP 2011 Workshop on Statistical Machine Translation (WMT11) to translate Haitian Creole SMS messages sent during the 2010 Haitian earthquake. This work highlights the importance of NLP tools on creoles in crisis situations for emergency relief (Hu et al., 2011; Hewavitharana et al., 2011).\nSinglish is one of the major languages in Singapore, with borrowed vocabulary and grammars1 from a number of languages including Malay, Tamil, and Chinese dialects such as Hokkien, Cantonese and Teochew (Leimgruber, 2009, 2011), and it has been increasingly used in written forms on web media. Fluent English speakers unfamiliar with Singlish would find the creole hard to comprehend (Harada, 2009). Correspondingly, fundamental English NLP components such as POS taggers and dependency parsers perform poorly on such Singlish texts as shown in Table 2 and 4. For example, Seah et al. (2015) adapted the Socher et al. (2013) sentiment analysis engine to the Singlish vocabulary, but failed to adapt the parser. Since dependency parsers are important for tasks such as information extraction (Miwa and Bansal, 2016) and discourse parsing (Li et al., 2015), this hinders the development of such downstream applications for Singlish in written forms and thus makes it crucial to build a dependency parser that can perform well natively on Singlish.\nTo address this issue, we start with investigating the linguistic characteristics of Singlish and specifically the causes of difficulties for understanding Singlish with English syntax. We found that, despite the obvious attribute of inheriting a large portion of basic vocabularies and grammars from English, Singlish not only imports terms from regional languages and dialects, its lexical\n1We follow Leimgruber (2011) in using “grammar” to describe “syntactic constructions” and we do not differentiate the two expressions in this paper.\nar X\niv :1\n70 5.\n06 46\n3v 1\n[ cs\n.C L\n] 1\n8 M\nay 2\n01 7\nsemantics and syntax also deviate significantly from English (Leimgruber, 2009, 2011). We categorize the challenges and formalize their interpretation using Universal Dependencies (Nivre et al., 2016), which extends to the creation of a Singlish dependency treebank with 1,200 sentences.\nBased on the intricate relationship between Singlish and English, we build a Singlish parser by leveraging knowledge of English syntax as a basis. This overall approach is illustrated in Figure 1. In particular, we train a basic Singlish parser with the best off-the-shelf neural dependency parsing model using biaffine attention (Dozat and Manning, 2017), and improve it with knowledge transfer by adopting neural stacking (Chen et al., 2016; Zhang and Weiss, 2016) to integrate the English syntax. Since POS tags are important features for dependency parsing (Chen and Manning, 2014; Dyer et al., 2015), we train a POS tagger for Singlish following the same idea by integrating English POS knowledge using neural stacking.\nResults show that English syntax knowledge brings 51.50% and 25.01% relative error reduction on POS tagging and dependency parsing respectively, resulting in a Singlish dependency parser with 84.47% unlabeled attachment score (UAS) and 77.76% labeled attachment score (LAS).\nWe make our Singlish dependency treebank, the source code for training a dependency parser and the trained model for the parser with the best performance freely available online2.\n2https://github.com/wanghm92/Sing_Par"
    }, {
      "heading" : "2 Related Work",
      "text" : "Neural networks have led to significant advance in the performance for dependency parsing, including transition-based parsing (Chen and Manning, 2014; Zhou et al., 2015; Weiss et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015; Andor et al., 2016), and graph-based parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017). In particular, the biaffine attention method of Dozat and Manning (2017) uses deep bi-directional long short-term memory (bi-LSTM) networks for highorder non-linear feature extraction, producing the highest-performing graph-based English dependency parser. We adopt this model as the basis for our Singlish parser.\nOur work belongs to a line of work on transfer learning for parsing, which leverages English resources in Universal Dependencies to improve the parsing accuracies of low-resource languages (Hwa et al., 2005; Cohen and Smith, 2009; Ganchev et al., 2009). Seminal work employed statistical models. McDonald et al. (2011) investigated delexicalized transfer, where word-based features are removed from a statistical model for English, so that POS and dependency label knowledge can be utilized for training a model for lowresource language. Subsequent work considered syntactic similarities between languages for better feature transfer (Täckström et al., 2012; Naseem et al., 2012; Zhang and Barzilay, 2015).\nRecently, a line of work leverages neural network models for multi-lingual parsing (Guo et al., 2015; Duong et al., 2015; Ammar et al., 2016). The basic idea is to map the word embedding spaces between different languages into the same vector space, by using sentence-aligned bilingual data. This gives consistency in tokens, POS and dependency labels thanks to the availability of Universal Dependencies (Nivre et al., 2016). Our work is similar to these methods in using a neural network model for knowledge sharing between different languages. However, ours is different in the use of a neural stacking model, which respects the distributional differences between Singlish and English words. This empirically gives higher accuracies for Singlish.\nNeural stacking was previously used for cross-annotation (Chen et al., 2016) and crosstask (Zhang and Weiss, 2016) joint-modelling on monolingual treebanks. To the best of our knowledge, we are the first to employ it on cross-lingual\nfeature transfer from resource-rich languages to improve dependency parsing for low-resource languages. Besides these three dimensions in dealing with heterogeneous text data, another popular area of research is on the topic of domain adaption, which is commonly associated with crosslingual problems (Nivre et al., 2007). While this large strand of work is remotely related to ours, we do not describe them in details.\nUnsupervised rule-based approaches also offer an competitive alternative for cross-lingual dependency parsing (Naseem et al., 2010; Gillenwater et al., 2010; Gelling et al., 2012; Søgaard, 2012a,b; Martı́nez Alonso et al., 2017), and recently been benchmarked for the Universal Dependencies formalism by exploiting the linguistic constraints in the Universal Dependencies to improve the robustness against error propagation and domain adaption (Martı́nez Alonso et al., 2017). However, we choose a data-driven supervised approach given the relatively higher parsing accuracy owing to the availability of resourceful treebanks from the Universal Dependencies project."
    }, {
      "heading" : "3 Singlish Dependency Treebank",
      "text" : ""
    }, {
      "heading" : "3.1 Universal Dependencies for Singlish",
      "text" : "Since English is the major genesis of Singlish, we choose English as the source of lexical feature transfer to assist Singlish dependency parsing. Universal Dependencies provides a set of multilingual treebanks with cross-lingually consistent dependency-based lexicalist annotations, designed to aid development and evaluation for cross-lingual systems, such as multilingual parsers (Nivre et al., 2016). The current version of Universal Dependencies comprises not only major treebanks for 47 languages but also their siblings for domain-specific corpora and dialects. With the aligned initiatives for creating transfer-learning-friendly treebanks, we adopt the Universal Dependencies protocol for constructing the Singlish dependency treebank, both as a new resource for the low-resource languages and to facilitate knowledge transfer from English.\nOn top of the general Universal Dependencies guidelines, English-specific dependency relation definitions including additional subtypes are employed as the default standards for annotating the Singlish dependency treebank, unless augmented or redefined when necessary. The latest English\ncorpus in Universal Dependencies v1.43 collection is constructed from the English Web Treebank (Bies et al., 2012), comprising of web media texts, which potentially smooths the knowledge transfer to our target Singlish texts in similar domains. The statistics of this dataset, from which we obtain English syntactic knowledge, is shown in Table 1 and we refer to this corpus as UD-Eng. This corpus uses 47 dependency relations and we show below how to conform to the same standard while adapting to unique Singlish grammars."
    }, {
      "heading" : "3.2 Challenges and Solutions for Annotating Singlish",
      "text" : "The deviations of Singlish from English come from both the lexical and the grammatical levels (Leimgruber, 2009, 2011), which bring challenges for analysis on Singlish using English NLP tools. The former involves imported vocabularies from the first languages of the local people and the latter can be represented by a set of relatively localized features which collectively form 5 unique grammars of Singlish according to Leimgruber (2011). We find empirically that all these deviations can be accommodated by applying the existing English dependency relation definitions while ensuring consistency with the annotations in other non-English UD treebanks, which are explained with examples as follows.\nImported vocabulary: Singlish borrows a number of words and expressions from its nonEnglish origins (Leimgruber, 2009, 2011), such as “Kiasu”, which originates from Hokkien meaning “very anxious not to miss an opportunity”.4 These imported terms often constitute out-of-vocabulary (OOV) words with respect to a standard English treebank and result in difficulties for using English-trained tools on Singlish. All borrowed words are annotated based on their usages in Singlish, which mainly inherit the POS from their genesis languages. Table A4 in Appendix A\n3Only guidelines for Universal Dependencies v2 but not the English corpus is available when this work is completed.\n4Definition by the Oxford living Dictionaries for English.\nsummarizes all borrowed terms in our treebank. Topic-prominence: This type of sentences start with establishing its topic, which often serves as the default one that the rest of the sentence refers to, and they typically employ an object-subjectverb sentence structure (Leimgruber, 2009, 2011). In particular, three subtypes of topic-prominence are observed in the Singlish dependency treebank and their annotations are addressed as follows:\nFirst, topics framed as clausal arguments at the beginning of the sentence are labeled as “csubj” (clausal subject), as shown by “Drive this car” of (1) in Figure 2, which is consistent with the dependency relations in its Chinese translation.\nSecond, noun phrases used to modify the predicate with the absence of a preposition is regarded as a “nsubj” (nominal subject). Similarly, this is a common order of words used in Chinese and one example is the “SG” of (2) in Figure 2.\nThird, prepositional phrases moved in front are still treated as “nmod” (nominal modifier) of their\nintended heads, following the exact definition but as a Singlish-specific form of exemplification, as shown by the “Inside tent” of (3) in Figure 2.\nAlthough the “dislocated” (dislocated elements) relation in UD is also used for preposed elements, but it captures the ones “that do not fulfill the usual core grammatical relations of a sentence” and “not for a topic-marked noun that is also the subject of the sentence” (Nivre et al., 2016). In these three scenarios, the topic words or phrases are in relatively closer grammatical relations to the predicate, as subjects or modifiers.\nCopula deletion: Imported from the corresponding Chinese sentence structure, this copula verb is often optional and even deleted in Singlish, which is one of its diagnostic characteristics (Leimgruber, 2009, 2011). In UD-Eng standards, predicative “be” is the only verb used as a copula and it often depends on its complement to avoid copular head. This is explicitly designed in UD to promote parallelism for zero-copula phenomenon in languages such as Russian, Japanese, and Arabic. The deleted copula and its “cop” (copula) arcs are simply ignored, as shown by (4) in Figure 2.\nNP deletion: Noun-phrase (NP) deletion often results in null subjects or objects. It may be regarded as a branch of “Topic-prominence” but is a distinctive feature of Singlish with relatively high frequency of usage (Leimgruber, 2011). NP deletion is also common in pronoun-dropping languages such as Spanish and Italian, where the anaphora can be morphologically inferred. In one example, “Vorrei ora entrare brevemente nel merito.”5, from the Italian treebank in UD, “Vorrei” means “I would like to” and depends on the sentence root, “entrare”, with the “aux”(auxiliary) relation, where the subject “I” is absent but implicitly understood. Similarly, we do not recover such relations since the deleted NP imposes negligible alteration to the dependency tree, as exemplified by (5) in Figure 2.\nInversion: Inversion in Singlish involves either keeping the subject and verb in interrogative sentences in the same order as in statements, or tag questions in polar interrogatives (Leimgruber, 2011). The former also exists in non-English languages, such as Spanish and Italian, where the subject can prepose the verb in questions (La-\n5In English: (I) would now like to enter briefly on the merit (of the discussion).\nhousse and Lamiroy, 2012). This simply involves a change of word orders and thus requires no special treatments. On the other hand, tag questions should be carefully analyzed in two scenarios. One type is in the form of “isn’t it?” or “haven’t you?”, which are dependents of the sentence root with the “parataxis” relation.6 The other type is exemplified as “right?”, and its Singlish equivalent “tio boh?” (a transliteration from Hokkien) are labeled with the “discourse” (discourse element) relation with respect to the sentence root. See example (6) in Figure 2.\nDiscourse particles: Usage of clausal-final discourse particles, which originates from Hokkien and Cantonese, is one of the most typical feature of Singlish (Leimgruber, 2009, 2011; Lim, 2007). All discourse particles that appear in our treebank are summarized in Table A3 in Appendix A with the imported vocabulary:. These words express the tone of the sentence and thus have the “INTJ” (interjection) POS tag and depend on the root of the sentence or clause labeled with “discourse”, as is shown by the “leh” of (3) in Figure 2. The word “one” is a special instance of this type with the sole purpose being a tone marker in Singlish but not English, as shown by (7) in Figure 2."
    }, {
      "heading" : "3.3 Data Selection and Annotation",
      "text" : "Data Source: Singlish is used in written form mainly in social media and local Internet forums. After comparison, we chose the SG Talk Forum7 as our data source due to its relative abundance in Singlish contents. We crawled 84,459 posts using the Scrapy framework8 from pages dated up to 25th December 2016, retaining sentences of length between 5 and 50, which total 58,310. Sentences are reversely sorted according to the log likelihood of the sentence given by an English language model trained using the KenLM toolkit (Heafield et al., 2013)9 normalized by the sentence length, so that those most different from standard English can be chosen. Among the top 10,000 sentences, 1,977 sentences contain unique Singlish vocabularies defined by The\n6In UD: Relation between the main verb of a clause and other sentential elements, such as sentential parenthetical clause, or adjacent sentences without any explicit coordination or subordination.\n7http://sgTalk.com 8https://scrapy.org/ 9Trained using the afp eng and xin eng sources of English\nGigaword Fifth Edition (Gigaword).\nCoxford Singlish Dictionary10, A Dictionary of Singlish and Singapore English11, and the Singlish Vocabulary Wikipedia page12. The average normalized log likelihood of these 10,000 sentences is -5.81, and the same measure for all sentences in UD-Eng is -4.81. This means these sentences with Singlish contents are 10 times less probable expressed as standard English than the UD-Eng contents in the web domain. This contrast indicates the degree of lexical deviation of Singlish from English. We chose 1,200 sentences from the first 10,000. More than 70% of the selected sentences are observed to consist of the Singlish grammars and imported vocabularies described in section 3.2. Thus the evaluations on this treebank can reflect the performance of various POS taggers and parsers on Singlish in general.\nAnnotation: The chosen texts are divided by random selection into training, development, and testing sets according to the proportion of sentences in the training, development, and test division for UD-Eng, as summarized in Table 1. The sentences are tokenized using the NLTK Tokenizer,13 and then annotated using the Dependency Viewer.14 In total, all 17 UD-Eng POS tags and 41 out of the 47 UD-Eng dependency labels are present in the Singlish dependency treebank. Besides, 100 sentences are randomly selected and double annotated by one of the coauthors, and the inter-annotator agreement has a 97.76% accuracy on POS tagging and a 93.44% UAS and a 89.63% LAS for dependency parsing. A full summary of the numbers of occurrences of each POS tag and dependency label are included in Appendix A."
    }, {
      "heading" : "4 Part-of-Speech Tagging",
      "text" : "In order to obtain automatically predicted POS tags as features for a base English dependency parser, we train a POS tagger for UD-Eng using the baseline model of Chen et al. (2016), depicted in Figure 3. The bi-LSTM networks with a CRF layer (bi-LSTM-CRF) have shown state-of-the-art performance by globally optimizing the tag sequence (Huang et al., 2015; Chen et al., 2016).\n10http://72.5.72.93/html/lexec.php 11http://www.singlishdictionary.com 12https://en.wikipedia.org/wiki/\nSinglish_vocabulary 13http://www.nltk.org/api/nltk. tokenize.html 14http://nlp.nju.edu.cn/tanggc/tools/ DependencyViewer.exe\nBased on this English POS tagging model, we train a POS tagger for Singlish using the featurelevel neural stacking model of Chen et al. (2016). Both the English and Singlish models consist of an input layer, a feature layer, and an output layer."
    }, {
      "heading" : "4.1 Base Bi-LSTM-CRF POS Tagger",
      "text" : "Input Layer: Each token is represented as a vector by concatenating a word embedding from a lookup table with a weighted average of its character embeddings given by the attention model of Bahdanau et al. (2014). Following Chen et al. (2016), the input layer produces a dense representation for the current input token by concatenating its word vector and the ones for its surrounding context tokens in a window of finite size.\nFeature Layer: This layer employs a bi-LSTM network to encode the input into a sequence of hidden vectors that embody global contextual information. Following Chen et al. (2016), we adopt bi-LSTM with peephole connections (Graves and Schmidhuber, 2005).\nOutput layer: This is a CRF layer to predict the POS tags for the input words by maximizing the conditional probability of the sequence of tags given input sentence."
    }, {
      "heading" : "4.2 POS Tagger with Neural Stacking",
      "text" : "We adopt the deep integration neural stacking structure presented in Chen et al. (2016). As shown in Figure 4, the distributed vector representation for the target word at the input layer of the Singlish Tagger is augmented by concatenating the emission vector produced by the English Tagger with the original word and character-based embeddings, before applying the concatenation within a context window in section 4.1. During training, loss is back-propagated to all trainable parameters\nin both the Singlish Tagger and the pre-trained feature layer of the base English Tagger. At test time, the input sentence is fed to the integrated tagger model as a whole for inference."
    }, {
      "heading" : "4.3 Results",
      "text" : "We use the publicly available source code15 by Chen et al. (2016) to train a 1-layer biLSTM-CRF based POS tagger on UD-Eng, using 50-dimension pre-trained SENNA word embeddings (Collobert et al., 2011). We set the hidden layer size to 300, the initial learning rate for Adagrad (Duchi et al., 2011) to 0.01, the regularization parameter λ to 10−6, and the dropout rate to 15%. The tagger gives 94.84% accuracy on the UD-Eng test set after 24 epochs, chosen according to development tests, which is comparable to the stateof-the-art accuracy of 95.17% reported by Plank et al. (2016). We use these settings to perform 10- fold jackknifing of POS tagging on the UD-Eng training set, with an average accuracy of 95.60%.\nSimilarly, we trained a POS tagger using the Singlish dependency treebank alone with pretrained word embeddings on The Singapore Component of the International Corpus of English (ICE-SIN) (Nihilani, 1992; Ooi, 1997), which consists of both spoken and written texts. However, due to limited amount of training data, the\n15https://github.com/chenhongshen/ NNHetSeq\ntagging accuracy is not satisfactory even with a larger dropout rate to avoid over-fitting. In contrast, the neural stacking structure on top of the English base model trained on UD-Eng achieves a POS tagging accuracy of 89.50%16, which corresponds to a 51.50% relative error reduction over the baseline Singlish model, as shown in Table 2. We use this for 10-fold jackknifing on Singlish parsing training data, and tagging the Singlish development and test data."
    }, {
      "heading" : "5 Dependency Parsing",
      "text" : "We adopt the Dozat and Manning (2017) parser17 as our base model, as displayed in Figure 5, and apply neural stacking to achieve improvements over the baseline parser. Both the base and neural stacking models consist of an input layer, a feature layer, and an output layer."
    }, {
      "heading" : "5.1 Base Parser with Bi-affine Attentions",
      "text" : "Input Layer: This layer encodes the current input word by concatenating a pre-trained word embedding with a trainable word embedding and POS tag embedding from the respective lookup tables.\nFeature Layer: The two recurrent vectors produced by the multi-layer bi-LSTM network from each input vector are concatenated and mapped to multiple feature vectors in lower-dimension space by a set of parallel multilayer perceptron (MLP)\n16We empirically find that using ICE-SIN embeddings in neural stacking model performs better than using English SENNA embeddings. Similar findings are found for the parser, of which more details are given in section 6.\n17https://github.com/tdozat/Parser\nlayers. Following Dozat and Manning (2017), we adopt Cif-LSTM cells (Greff et al., 2016).\nOutput Layer: This layer applies biaffine transformation on the feature vectors to calculate the score of the directed arcs between every pair of words. The inferred trees for input sentence are formed by choosing the head with the highest score for each word and a cross-entropy loss is calculated to update the model parameters."
    }, {
      "heading" : "5.2 Parser with Neural Stacking",
      "text" : "Inspired by the idea of feature-level neural stacking (Chen et al., 2016; Zhang and Weiss, 2016), we concatenate the pre-trained word embedding, trainable word and tag embeddings, with the two recurrent state vectors at the last bi-LSTM layer of the English Tagger as the input vector for each target word. In order to further preserve syntactic knowledge retained by the English Tagger, the feature vectors from its MLP layer is added to the ones produced by the Singlish Parser, as illustrated in Figure 6, and the scoring tensor of the Singlish Parser is initialized with the one from the trained English Tagger. Loss is back-propagated by reversely traversing all forward paths to all trainable parameter for training and the whole model is used collectively for inference."
    }, {
      "heading" : "6 Experiments",
      "text" : ""
    }, {
      "heading" : "6.1 Experimental Settings",
      "text" : "We train an English parser on UD-Eng with the default model settings in Dozat and Manning (2017).\nIt achieves an UAS of 88.83% and a LAS of 85.20%, which are close to the state-of-the-art 85.90% LAS on UD-Eng reported by Ammar et al. (2016), and the main difference is caused by us not using fine-grained POS tags. We apply the same settings for a baseline Singlish parser. We attempt to choose a better configuration of the number of bi-LSTM layers and the hidden dimension based on the development set performance, but the default settings turn out to perform the best. Thus we stick to all default hyper-parameters in Dozat and Manning (2017) for training the Singlish parsers.\nWe experimented with different word embeddings, as with the raw text sources summarized in Table 3 and further described in section 6.2. When using the neural stacking model, we fix the model configuration for the base English parser model and choose the size of the hidden vector and the number of bi-LSTM layers stacked on top based on the performance on the development set. It turns out that a 1-layer bi-LSTM with 900 hidden dimension performs the best, where the bigger hidden layer accommodates the elongated input vector to the stacked bi-LSTM and the fewer number of recurrent layers avoids over-fitting on the small Singlish dependency treebank, given the deep bi-LSTM English parser network at the bottom. The evaluation of the neural stacking model is further described in section 6.3."
    }, {
      "heading" : "6.2 Investigating Distributed Lexical Characteristics",
      "text" : "In order to learn characteristics of distributed lexical semantics for Singlish, we compare performances of the Singlish dependency parser using several sets of pre-trained word embeddings: GloVe6B, large-scale English word embeddings18; ICE-SIN, Singlish word embeddings trained using GloVe (Pennington et al., 2014) on the ICE-SIN (Nihilani, 1992; Ooi, 1997) corpus; Giga100M, a small-scale English word embeddings trained using GloVe (Pennington et al., 2014) with the same settings on a comparable size of English data randomly selected from the English Gigaword Fifth Edition for a fair comparison with ICE-SIN embeddings.\nFirst, the English Giga100M embeddings marginally improve the Singlish parser from the baseline without pre-trained embeddings and also using the UD-Eng parser directly on Singlish, represented as “ENG-on-SIN” in Table 4. With much more English lexical semantics being fed to the Singlish parser using the English GloVe6B embeddings, further enhancement is achieved. Nevertheless, the Singlish ICE-SIN embeddings lead to even more improvement, with 13.78% relative error reduction, compared with 7.04% using the English Giga100M embeddings and 9.16% using the English GloVe6B embeddings, despite the huge difference in sizes in the latter case. This demonstrates the distributional differences between Singlish and English tokens, even though they share a large vocabulary. More detailed comparison is described in section 6.4."
    }, {
      "heading" : "6.3 Knowledge Transfer Using Neural Stacking",
      "text" : "We train a parser with neural stacking and Singlish ICE-SIN embeddings, which achieves the best performance among all the models, with a UAS of 84.47%, represented as “Stack-ICE-SIN” in Table 4, which corresponds to 25.01% relative error reduction compared to the baseline. This demonstrates that knowledge from English can be successfully incorporated to boost the Singlish parser. To further evaluate the effectiveness of the neural stacking model, we also trained a base model with the combination of UD-Eng and the Singlish tree-\n18Trained with Wikipedia 2014 the Gigaword. Downloadable from http://nlp.stanford.edu/data/ glove.6B.zip\nbank, represented as “ENG-plus-SIN” in Table 4, which is still outperformed by the neural stacking model. Besides, we performed a 5-cross-fold validation for the base parser with Singlish ICE-SIN embeddings and the parser using neural stacking, where half of the held-out fold is used as the development set. The average UAS and LAS across the 5 folds shown in Table 5 and the relative error reduction on average 23.61% suggest that the overall improvement from knowledge transfer using neural stacking remains consistent. This significant improvement is further explained in section 6.4."
    }, {
      "heading" : "6.4 Improvements over Grammar Types",
      "text" : "To analyze the sources of improvements for Singlish parsing using different model configurations, we conduct error analysis over 5 syntactic categories19, including 4 types of grammars mentioned in section 3.220, and 1 for all other cases, including sentences containing imported vocabularies but expressed in basic English syntax. The number of sentences and the results in each group of the test set are shown in Table 6.\nThe neural stacking model leads to the biggest improvement over all categories except for a tie UAS performance on “NP Deletion” cases, which explains the significant overall improvement.\nComparing the base model with ICE-SIN embeddings with the base parser trained on UD-Eng, which contain syntactic and semantic knowledge in Singlish and English, respectively, the former outperforms the latter on all 4 types of Singlish grammars but not for the remaining samples. This suggests that the base English parser mainly contributes to analyzing basic English syntax, while the base Singlish parser models unique Singlish grammars better.\nSimilar trends are also observed on the base model using the English Giga100M embeddings, but the overall performances are not as good as\n19Multiple labels are allowed for one sentence. 20The “Inversion” type of grammar is not analyzed since\nthere is only 1 such sentence in the test set.\nusing ICE-SIN embeddings, especially over basic English syntax where it undermines the performance to a greater extent. This suggests that only limited English distributed lexical semantic information can be integrated to help modelling Singlish syntactic knowledge due to the differences in distributed lexical semantics."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We have investigated dependency parsing for Singlish, an important English-based creole language, through annotations of a Singlish dependency treebank with 10,986 words and building an enhanced parser by leveraging on knowledge transferred from a 20-times-bigger English treebank of Universal Dependencies. We demonstrate the effectiveness of using neural stacking for feature transfer by boosting the Singlish dependency parsing performance to from UAS 79.29% to UAS 84.47%, with a 25.01% relative error reduction over the parser with all available Singlish resources. We release the annotated Singlish dependency treebank, the trained model and the source code for the parser with free public access. Possible future work include expanding the investigation to other regional languages such as Malay and Indonesian."
    }, {
      "heading" : "Acknowledgments",
      "text" : "Yue Zhang is the corresponding author. This research is supported by IGDSS1603031 from Temasek Laboratories@SUTD. We appreciate anonymous reviewers for their insightful comments, which helped to improve the paper, and Zhiyang Teng, Jiangming Liu, Yupeng Liu, and Enrico Santus for their constructive discussions."
    }, {
      "heading" : "A Statistics of Singlish Dependency Treebank",
      "text" : "POS Tags ADJ 782 INTJ 556 PUNCT 1604 ADP 490 NOUN 1779 SCONJ 126 ADV 941 NUM 153 SYM 11 AUX 429 PART 355 VERB 1704 CONJ 167 PRON 682 X 10 DET 387 PROPN 810\nTable A1: Statistics of POS tags\nDependency labels acl 37 dobj 612 acl:relcl 29 expl 10 advcl 194 iobj 15 advmod 859 list 10 appos 18 mwe 105 amod 423 name 117 aux 377 neg 261 auxpass 47 nmod 398 case 463 nmod:npmod 26 cc 167 nmod:poss 153 ccomp 138 nmod:tmod 81 compound 420 nsubj 1005 compound:prt 30 nsubjpass 34 conj 238 nummod 94 cop 152 mark 275 csubj 30 parataxis 241 det 304 punct 1607 det:predet 7 remnant 17 discourse 552 vocative 41 dislocated 2 xcomp 190\nTable A2: Statistics of dependency labels\nah aiyah ba hah / har / huh hiak hiak hiak hor huat la / lah lau leh loh / lor ma / mah wahlow / wah lau wa / wah ya ya walaneh / wah lan eh\nTable A3: List of discourse particles\nA-B act blur ah beng ah ne angpow arrowed ang ku kueh angmoh/ang moh ahpek / ah peks atas boh/bo boho jiak boh pian buay lin chu buen kuey C chai tow kway chao ah beng chap chye png char kway teow chee cheong fun / che cheong fen cheesepie cheong / chiong chiam / cham chiak liao bee / jiao liao bee chio ching chong chio bu / chiobu chui chop chop chow-angmoh chwee kueh D-F dey diam diam die kock standing die pain pain dun eat grass flip prata fried beehoon G gahmen / garment gam geylang gone case gong kia goreng pisang gui H-J hai si lang heng hiong hoot Hosay / ho say how lian jepun kia / jepun kias jialat / jia lak / jia lat K ka kaki kong kaki song kancheong kateks kautim kay kiang kayu kee chia kee siao kelong kena / kana kiam kiasu ki seow kkj kong si mi kopi kopi lui kopi-o kosong koyok ku ku bird L lagi lai liao laksa lao jio kong lao sai lau chwee nua liao / ler like dat / like that lim peh lobang M mahjong kaki makan masak masak mati mee mee pok mee rebus mee siam mee sua mei mei N-S nasi lemak pang sai piak sabo sai same same sia sianz / sian sia suay sibeh siew dai siew siew dai simi taisee soon kuey sotong suay / suey swee T tahan tak pakai te te kee tong tua tikopeh tio tio pian/dio pian talk cock / talk cock sing song U-Z umm zai up lorry / up one’s lorry xiao zhun / buay zhun\nTable A4: List of imported vocabularies"
    } ],
    "references" : [ {
      "title" : "Many languages, one parser",
      "author" : [ "Waleed Ammar", "George Mulcaire", "Miguel Ballesteros", "Chris Dyer", "Noah Smith." ],
      "venue" : "Transactions of the Association of Computational Linguistics 4:431–444. http://aclweb.org/anthology/Q16-1031.",
      "citeRegEx" : "Ammar et al\\.,? 2016",
      "shortCiteRegEx" : "Ammar et al\\.",
      "year" : 2016
    }, {
      "title" : "Globally normalized transition-based neural networks",
      "author" : [ "Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins." ],
      "venue" : "Proceedings of the ACL 2016. Association",
      "citeRegEx" : "Andor et al\\.,? 2016",
      "shortCiteRegEx" : "Andor et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "arXiv preprint abs/1409.0473. http://arxiv.org/abs/1409.0473.",
      "citeRegEx" : "Bahdanau et al\\.,? 2014",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Improved transition-based parsing by modeling characters instead of words with lstms",
      "author" : [ "Miguel Ballesteros", "Chris Dyer", "A. Noah Smith." ],
      "venue" : "Proceedings of the EMNLP 2015. Association for Computational Linguistics, pages 349–359.",
      "citeRegEx" : "Ballesteros et al\\.,? 2015",
      "shortCiteRegEx" : "Ballesteros et al\\.",
      "year" : 2015
    }, {
      "title" : "A fast and accurate dependency parser using neural networks",
      "author" : [ "Danqi Chen", "Christopher Manning." ],
      "venue" : "Proceedings of the EMNLP 2014. Association for Computational Linguistics, pages 740–750. https://doi.org/10.3115/v1/D14-1082.",
      "citeRegEx" : "Chen and Manning.,? 2014",
      "shortCiteRegEx" : "Chen and Manning.",
      "year" : 2014
    }, {
      "title" : "Neural network for heterogeneous annotations",
      "author" : [ "Hongshen Chen", "Yue Zhang", "Qun Liu." ],
      "venue" : "Proceedings of the EMNLP 2016. Association for Computational Linguistics, pages 731–741. http://aclweb.org/anthology/D16-1070.",
      "citeRegEx" : "Chen et al\\.,? 2016",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction",
      "author" : [ "Shay Cohen", "A. Noah Smith." ],
      "venue" : "Proceedings of the NAACL-HLT 2009. Association for Computational Linguistics, pages 74–82.",
      "citeRegEx" : "Cohen and Smith.,? 2009",
      "shortCiteRegEx" : "Cohen and Smith.",
      "year" : 2009
    }, {
      "title" : "Natural language processing (almost) from scratch",
      "author" : [ "Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa." ],
      "venue" : "Journal of Machine Learning Research 12:2493–2537.",
      "citeRegEx" : "Collobert et al\\.,? 2011",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2011
    }, {
      "title" : "Deep biaffine attention for neural dependency parsing",
      "author" : [ "Timothy Dozat", "Christopher D. Manning." ],
      "venue" : "International Conference on Learning Representations 2017. volume abs/1611.01734. http://arxiv.org/abs/1611.01734.",
      "citeRegEx" : "Dozat and Manning.,? 2017",
      "shortCiteRegEx" : "Dozat and Manning.",
      "year" : 2017
    }, {
      "title" : "The roles of singapore standard",
      "author" : [ ],
      "venue" : null,
      "citeRegEx" : "1244",
      "shortCiteRegEx" : "1244",
      "year" : 2009
    }, {
      "title" : "Scalable modified kneser-ney language model estimation",
      "author" : [ "Kenneth Heafield", "Ivan Pouzyrevsky", "H. Jonathan Clark", "Philipp Koehn." ],
      "venue" : "Proceedings of the ACL 2013 (Short Papers). Association for Computational Linguistics, pages 690–696.",
      "citeRegEx" : "Heafield et al\\.,? 2013",
      "shortCiteRegEx" : "Heafield et al\\.",
      "year" : 2013
    }, {
      "title" : "Proceedings of the Sixth Workshop on Statistical Machine Translation, Association for Computational Linguistics, chapter CMU Haitian Creole-English",
      "author" : [ "Sanjika Hewavitharana", "Nguyen Bach", "Qin Gao", "Vamshi Ambati", "Stephan Vogel" ],
      "venue" : null,
      "citeRegEx" : "Hewavitharana et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Hewavitharana et al\\.",
      "year" : 2011
    }, {
      "title" : "Bidirectional LSTM-CRF models for sequence tagging",
      "author" : [ "Zhiheng Huang", "Wei Xu", "Kai Yu." ],
      "venue" : "arXiv preprint abs/1508.01991. http://arxiv.org/abs/1508.01991.",
      "citeRegEx" : "Huang et al\\.,? 2015",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2015
    }, {
      "title" : "Bootstrapping parsers via syntactic projection across parallel texts",
      "author" : [ "Rebecca Hwa", "Philip Resnik", "Amy Weinberg", "Clara Cabezas", "Okan Kolak." ],
      "venue" : "Natural Language Engineering 11(3):311– 325. https://doi.org/10.1017/S1351324905003840.",
      "citeRegEx" : "Hwa et al\\.,? 2005",
      "shortCiteRegEx" : "Hwa et al\\.",
      "year" : 2005
    }, {
      "title" : "Simple and accurate dependency parsing using bidirectional lstm feature representations",
      "author" : [ "Eliyahu Kiperwasser", "Yoav Goldberg." ],
      "venue" : "Transactions of the Association of Computational Linguistics 4:313– 327. http://aclweb.org/anthology/Q16-1023.",
      "citeRegEx" : "Kiperwasser and Goldberg.,? 2016",
      "shortCiteRegEx" : "Kiperwasser and Goldberg.",
      "year" : 2016
    }, {
      "title" : "Word order in french, spanish and italian: A grammaticalization account",
      "author" : [ "Karen Lahousse", "Béatrice Lamiroy." ],
      "venue" : "Folia Linguistica 46(2):387–415.",
      "citeRegEx" : "Lahousse and Lamiroy.,? 2012",
      "shortCiteRegEx" : "Lahousse and Lamiroy.",
      "year" : 2012
    }, {
      "title" : "Modelling variation in Singapore English",
      "author" : [ "Jakob R.E. Leimgruber." ],
      "venue" : "Ph.D. thesis, Oxford University.",
      "citeRegEx" : "Leimgruber.,? 2009",
      "shortCiteRegEx" : "Leimgruber.",
      "year" : 2009
    }, {
      "title" : "Singapore english",
      "author" : [ "Jakob R.E. Leimgruber." ],
      "venue" : "Language and Linguistics Compass 5(1):47–62. https://doi.org/10.1111/j.1749-818X.2010.00262.x.",
      "citeRegEx" : "Leimgruber.,? 2011",
      "shortCiteRegEx" : "Leimgruber.",
      "year" : 2011
    }, {
      "title" : "When are tree structures necessary for deep learning of representations? In Proceedings of the EMNLP 2015",
      "author" : [ "Jiwei Li", "Thang Luong", "Dan Jurafsky", "Eduard Hovy." ],
      "venue" : "Association for Computational Linguistics, pages 2304–2314.",
      "citeRegEx" : "Li et al\\.,? 2015",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2015
    }, {
      "title" : "Mergers and acquisitions: on the ages and origins of singapore english particles",
      "author" : [ "Lisa Lim." ],
      "venue" : "World Englishes 26(4):446–473.",
      "citeRegEx" : "Lim.,? 2007",
      "shortCiteRegEx" : "Lim.",
      "year" : 2007
    }, {
      "title" : "Parsing universal dependencies without training",
      "author" : [ "Héctor Martı́nez Alonso", "Željko Agić", "Barbara Plank", "Anders Søgaard" ],
      "venue" : "In Proceedings of the EACL 2017. Association for Computational Linguistics,",
      "citeRegEx" : "Alonso et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Alonso et al\\.",
      "year" : 2017
    }, {
      "title" : "Multi-source transfer of delexicalized dependency parsers",
      "author" : [ "Ryan McDonald", "Slav Petrov", "Keith Hall." ],
      "venue" : "Proceedings of the EMNLP 2011. Association for Computational Linguistics, pages 62–72. http://aclweb.org/anthology/D11-1006.",
      "citeRegEx" : "McDonald et al\\.,? 2011",
      "shortCiteRegEx" : "McDonald et al\\.",
      "year" : 2011
    }, {
      "title" : "Dynamics of a contact continuum: Singaporean English",
      "author" : [ "Ho Mian-Lian", "John T. Platt." ],
      "venue" : "Oxford University Press, USA.",
      "citeRegEx" : "Mian.Lian and Platt.,? 1993",
      "shortCiteRegEx" : "Mian.Lian and Platt.",
      "year" : 1993
    }, {
      "title" : "End-to-end relation extraction using lstms on sequences and tree structures",
      "author" : [ "Makoto Miwa", "Mohit Bansal." ],
      "venue" : "Proceedings of the ACL 2016. Association for Computational Linguistics, pages 1105– 1116. https://doi.org/10.18653/v1/P16-1105.",
      "citeRegEx" : "Miwa and Bansal.,? 2016",
      "shortCiteRegEx" : "Miwa and Bansal.",
      "year" : 2016
    }, {
      "title" : "Selective sharing for multilingual dependency parsing",
      "author" : [ "Tahira Naseem", "Regina Barzilay", "Amir Globerson." ],
      "venue" : "Proceedings of the ACL 2012. Association for Computational Linguistics, pages 629–637. http://aclweb.org/anthology/P12-1066.",
      "citeRegEx" : "Naseem et al\\.,? 2012",
      "shortCiteRegEx" : "Naseem et al\\.",
      "year" : 2012
    }, {
      "title" : "Using universal linguistic knowledge to guide grammar induction",
      "author" : [ "Tahira Naseem", "Harr Chen", "Regina Barzilay", "Mark Johnson." ],
      "venue" : "Proceedings of the EMNLP 2010. Association for Computational Linguistics, Cambridge, MA, pages 1234–",
      "citeRegEx" : "Naseem et al\\.,? 2010",
      "shortCiteRegEx" : "Naseem et al\\.",
      "year" : 2010
    }, {
      "title" : "The international computerized corpus of english",
      "author" : [ "Paroo Nihilani." ],
      "venue" : "Words in a cultural context. Singapore: UniPress pages 84–88.",
      "citeRegEx" : "Nihilani.,? 1992",
      "shortCiteRegEx" : "Nihilani.",
      "year" : 1992
    }, {
      "title" : "Universal dependencies v1: A multilingual",
      "author" : [ "Joakim Nivre", "Marie-Catherine de Marneffe", "Filip Ginter", "Yoav Goldberg", "Jan Hajic", "Christopher D. Manning", "Ryan McDonald", "Slav Petrov", "Sampo Pyysalo", "Natalia Silveira", "Reut Tsarfaty", "Daniel Zeman" ],
      "venue" : null,
      "citeRegEx" : "Nivre et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Nivre et al\\.",
      "year" : 2016
    }, {
      "title" : "The CoNLL 2007 shared task on dependency parsing",
      "author" : [ "Joakim Nivre", "Johan Hall", "Sandra Kübler", "Ryan McDonald", "Jens Nilsson", "Sebastian Riedel", "Deniz Yuret." ],
      "venue" : "Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007. As-",
      "citeRegEx" : "Nivre et al\\.,? 2007",
      "shortCiteRegEx" : "Nivre et al\\.",
      "year" : 2007
    }, {
      "title" : "Analysing the Singapore ICE corpus for lexicographic evidence",
      "author" : [ "Vincent B Y Ooi." ],
      "venue" : "ENGLISH LANGUAGE & LITERATURE. http://scholarbank.nus.edu.sg/handle/10635/133118.",
      "citeRegEx" : "Ooi.,? 1997",
      "shortCiteRegEx" : "Ooi.",
      "year" : 1997
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher Manning." ],
      "venue" : "Proceedings of the EMNLP 2014. Association for Computational Linguistics, pages 1532–1543. https://doi.org/10.3115/v1/D14-1162.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss",
      "author" : [ "Barbara Plank", "Anders Søgaard", "Yoav Goldberg." ],
      "venue" : "Proceedings of the ACL 2016 (Short Papers). Associa-",
      "citeRegEx" : "Plank et al\\.,? 2016",
      "shortCiteRegEx" : "Plank et al\\.",
      "year" : 2016
    }, {
      "title" : "Troll detection by domain-adapting sentiment analysis",
      "author" : [ "Chun-Wei Seah", "Hai Leong Chieu", "Kian Ming Adam Chai", "Loo-Nin Teow", "Lee Wei Yeong." ],
      "venue" : "18th International Conference on Information Fusion (Fusion) 2015. IEEE, pages 792–799.",
      "citeRegEx" : "Seah et al\\.,? 2015",
      "shortCiteRegEx" : "Seah et al\\.",
      "year" : 2015
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "D. Christopher Manning", "Andrew Ng", "Christopher Potts." ],
      "venue" : "Proceedings of the EMNLP 2013. Asso-",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Two baselines for unsupervised dependency parsing",
      "author" : [ "Anders Søgaard." ],
      "venue" : "Proceedings of the NAACL-HLT Workshop on the Induction of Linguistic Structure. Association for Computational Linguistics, pages 81–83.",
      "citeRegEx" : "Søgaard.,? 2012a",
      "shortCiteRegEx" : "Søgaard.",
      "year" : 2012
    }, {
      "title" : "Unsupervised dependency parsing without training",
      "author" : [ "Anders Søgaard." ],
      "venue" : "Natural Language Engineering 18(2):187203. https://doi.org/10.1017/S1351324912000022.",
      "citeRegEx" : "Søgaard.,? 2012b",
      "shortCiteRegEx" : "Søgaard.",
      "year" : 2012
    }, {
      "title" : "Cross-lingual word clusters for direct transfer of linguistic structure",
      "author" : [ "Oscar Täckström", "Ryan McDonald", "Jakob Uszkoreit." ],
      "venue" : "Proceedings of the NAACL-HLT 2012. Association for Computational Linguistics, pages 477–487.",
      "citeRegEx" : "Täckström et al\\.,? 2012",
      "shortCiteRegEx" : "Täckström et al\\.",
      "year" : 2012
    }, {
      "title" : "Structured training for neural network transition-based parsing",
      "author" : [ "David Weiss", "Chris Alberti", "Michael Collins", "Slav Petrov." ],
      "venue" : "Proceedings of the ACL-IJCNLP 2015. Association for Computational Linguistics, pages 323–333.",
      "citeRegEx" : "Weiss et al\\.,? 2015",
      "shortCiteRegEx" : "Weiss et al\\.",
      "year" : 2015
    }, {
      "title" : "Hierarchical low-rank tensors for multilingual transfer parsing",
      "author" : [ "Yuan Zhang", "Regina Barzilay." ],
      "venue" : "Proceedings of the EMNLP 2015. Association for Computational Linguistics, pages 1857– 1867. https://doi.org/10.18653/v1/D15-1213.",
      "citeRegEx" : "Zhang and Barzilay.,? 2015",
      "shortCiteRegEx" : "Zhang and Barzilay.",
      "year" : 2015
    }, {
      "title" : "Stackpropagation: Improved representation learning for syntax",
      "author" : [ "Yuan Zhang", "David Weiss." ],
      "venue" : "Proceedings of the 54th ACL. Association for Computational Linguistics, pages 1557– 1566. https://doi.org/10.18653/v1/P16-1147.",
      "citeRegEx" : "Zhang and Weiss.,? 2016",
      "shortCiteRegEx" : "Zhang and Weiss.",
      "year" : 2016
    }, {
      "title" : "A neural probabilistic structuredprediction model for transition-based dependency parsing",
      "author" : [ "Hao Zhou", "Yue Zhang", "Shujian Huang", "Jiajun Chen." ],
      "venue" : "Proceedings of the ACL-IJCNLP 2015. Association for Computational Linguistics, pages",
      "citeRegEx" : "Zhou et al\\.,? 2015",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "This work highlights the importance of NLP tools on creoles in crisis situations for emergency relief (Hu et al., 2011; Hewavitharana et al., 2011).",
      "startOffset" : 102,
      "endOffset" : 147
    }, {
      "referenceID" : 23,
      "context" : "Since dependency parsers are important for tasks such as information extraction (Miwa and Bansal, 2016) and discourse parsing (Li et al.",
      "startOffset" : 80,
      "endOffset" : 103
    }, {
      "referenceID" : 18,
      "context" : "Since dependency parsers are important for tasks such as information extraction (Miwa and Bansal, 2016) and discourse parsing (Li et al., 2015), this hinders the development of such downstream applications for Singlish in written forms and thus makes it crucial to build a dependency parser that can perform well natively on Singlish.",
      "startOffset" : 126,
      "endOffset" : 143
    }, {
      "referenceID" : 16,
      "context" : "Singlish is one of the major languages in Singapore, with borrowed vocabulary and grammars1 from a number of languages including Malay, Tamil, and Chinese dialects such as Hokkien, Cantonese and Teochew (Leimgruber, 2009, 2011), and it has been increasingly used in written forms on web media. Fluent English speakers unfamiliar with Singlish would find the creole hard to comprehend (Harada, 2009). Correspondingly, fundamental English NLP components such as POS taggers and dependency parsers perform poorly on such Singlish texts as shown in Table 2 and 4. For example, Seah et al. (2015) adapted the Socher et al.",
      "startOffset" : 204,
      "endOffset" : 592
    }, {
      "referenceID" : 16,
      "context" : "Singlish is one of the major languages in Singapore, with borrowed vocabulary and grammars1 from a number of languages including Malay, Tamil, and Chinese dialects such as Hokkien, Cantonese and Teochew (Leimgruber, 2009, 2011), and it has been increasingly used in written forms on web media. Fluent English speakers unfamiliar with Singlish would find the creole hard to comprehend (Harada, 2009). Correspondingly, fundamental English NLP components such as POS taggers and dependency parsers perform poorly on such Singlish texts as shown in Table 2 and 4. For example, Seah et al. (2015) adapted the Socher et al. (2013) sentiment analysis engine to the Singlish vocabulary, but failed to adapt the parser.",
      "startOffset" : 204,
      "endOffset" : 625
    }, {
      "referenceID" : 16,
      "context" : "We follow Leimgruber (2011) in using “grammar” to describe “syntactic constructions” and we do not differentiate the two expressions in this paper.",
      "startOffset" : 10,
      "endOffset" : 28
    }, {
      "referenceID" : 27,
      "context" : "We categorize the challenges and formalize their interpretation using Universal Dependencies (Nivre et al., 2016), which extends to the creation of a Singlish dependency treebank with 1,200 sentences.",
      "startOffset" : 93,
      "endOffset" : 113
    }, {
      "referenceID" : 8,
      "context" : "In particular, we train a basic Singlish parser with the best off-the-shelf neural dependency parsing model using biaffine attention (Dozat and Manning, 2017), and improve it with knowledge transfer by adopting neural stacking (Chen et al.",
      "startOffset" : 133,
      "endOffset" : 158
    }, {
      "referenceID" : 5,
      "context" : "In particular, we train a basic Singlish parser with the best off-the-shelf neural dependency parsing model using biaffine attention (Dozat and Manning, 2017), and improve it with knowledge transfer by adopting neural stacking (Chen et al., 2016; Zhang and Weiss, 2016) to integrate the English syntax.",
      "startOffset" : 227,
      "endOffset" : 269
    }, {
      "referenceID" : 39,
      "context" : "In particular, we train a basic Singlish parser with the best off-the-shelf neural dependency parsing model using biaffine attention (Dozat and Manning, 2017), and improve it with knowledge transfer by adopting neural stacking (Chen et al., 2016; Zhang and Weiss, 2016) to integrate the English syntax.",
      "startOffset" : 227,
      "endOffset" : 269
    }, {
      "referenceID" : 4,
      "context" : "Since POS tags are important features for dependency parsing (Chen and Manning, 2014; Dyer et al., 2015), we train a POS tagger for Singlish following the same idea by integrating English POS knowledge using neural stacking.",
      "startOffset" : 61,
      "endOffset" : 104
    }, {
      "referenceID" : 4,
      "context" : "Neural networks have led to significant advance in the performance for dependency parsing, including transition-based parsing (Chen and Manning, 2014; Zhou et al., 2015; Weiss et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015; Andor et al., 2016), and graph-based parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017).",
      "startOffset" : 126,
      "endOffset" : 254
    }, {
      "referenceID" : 40,
      "context" : "Neural networks have led to significant advance in the performance for dependency parsing, including transition-based parsing (Chen and Manning, 2014; Zhou et al., 2015; Weiss et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015; Andor et al., 2016), and graph-based parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017).",
      "startOffset" : 126,
      "endOffset" : 254
    }, {
      "referenceID" : 37,
      "context" : "Neural networks have led to significant advance in the performance for dependency parsing, including transition-based parsing (Chen and Manning, 2014; Zhou et al., 2015; Weiss et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015; Andor et al., 2016), and graph-based parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017).",
      "startOffset" : 126,
      "endOffset" : 254
    }, {
      "referenceID" : 3,
      "context" : "Neural networks have led to significant advance in the performance for dependency parsing, including transition-based parsing (Chen and Manning, 2014; Zhou et al., 2015; Weiss et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015; Andor et al., 2016), and graph-based parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017).",
      "startOffset" : 126,
      "endOffset" : 254
    }, {
      "referenceID" : 1,
      "context" : "Neural networks have led to significant advance in the performance for dependency parsing, including transition-based parsing (Chen and Manning, 2014; Zhou et al., 2015; Weiss et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015; Andor et al., 2016), and graph-based parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017).",
      "startOffset" : 126,
      "endOffset" : 254
    }, {
      "referenceID" : 14,
      "context" : ", 2016), and graph-based parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017).",
      "startOffset" : 33,
      "endOffset" : 90
    }, {
      "referenceID" : 8,
      "context" : ", 2016), and graph-based parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017).",
      "startOffset" : 33,
      "endOffset" : 90
    }, {
      "referenceID" : 1,
      "context" : ", 2015; Andor et al., 2016), and graph-based parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017). In particular, the biaffine attention method of Dozat and Manning (2017) uses deep bi-directional long short-term memory (bi-LSTM) networks for highorder non-linear feature extraction, producing the highest-performing graph-based English dependency parser.",
      "startOffset" : 8,
      "endOffset" : 185
    }, {
      "referenceID" : 13,
      "context" : "Our work belongs to a line of work on transfer learning for parsing, which leverages English resources in Universal Dependencies to improve the parsing accuracies of low-resource languages (Hwa et al., 2005; Cohen and Smith, 2009; Ganchev et al., 2009).",
      "startOffset" : 189,
      "endOffset" : 252
    }, {
      "referenceID" : 6,
      "context" : "Our work belongs to a line of work on transfer learning for parsing, which leverages English resources in Universal Dependencies to improve the parsing accuracies of low-resource languages (Hwa et al., 2005; Cohen and Smith, 2009; Ganchev et al., 2009).",
      "startOffset" : 189,
      "endOffset" : 252
    }, {
      "referenceID" : 36,
      "context" : "Subsequent work considered syntactic similarities between languages for better feature transfer (Täckström et al., 2012; Naseem et al., 2012; Zhang and Barzilay, 2015).",
      "startOffset" : 96,
      "endOffset" : 167
    }, {
      "referenceID" : 24,
      "context" : "Subsequent work considered syntactic similarities between languages for better feature transfer (Täckström et al., 2012; Naseem et al., 2012; Zhang and Barzilay, 2015).",
      "startOffset" : 96,
      "endOffset" : 167
    }, {
      "referenceID" : 38,
      "context" : "Subsequent work considered syntactic similarities between languages for better feature transfer (Täckström et al., 2012; Naseem et al., 2012; Zhang and Barzilay, 2015).",
      "startOffset" : 96,
      "endOffset" : 167
    }, {
      "referenceID" : 6,
      "context" : ", 2005; Cohen and Smith, 2009; Ganchev et al., 2009). Seminal work employed statistical models. McDonald et al. (2011) investigated delexicalized transfer, where word-based features are removed from a statistical model for English, so that POS and dependency label knowledge can be utilized for training a model for lowresource language.",
      "startOffset" : 8,
      "endOffset" : 119
    }, {
      "referenceID" : 0,
      "context" : "Recently, a line of work leverages neural network models for multi-lingual parsing (Guo et al., 2015; Duong et al., 2015; Ammar et al., 2016).",
      "startOffset" : 83,
      "endOffset" : 141
    }, {
      "referenceID" : 27,
      "context" : "This gives consistency in tokens, POS and dependency labels thanks to the availability of Universal Dependencies (Nivre et al., 2016).",
      "startOffset" : 113,
      "endOffset" : 133
    }, {
      "referenceID" : 5,
      "context" : "Neural stacking was previously used for cross-annotation (Chen et al., 2016) and crosstask (Zhang and Weiss, 2016) joint-modelling on monolingual treebanks.",
      "startOffset" : 57,
      "endOffset" : 76
    }, {
      "referenceID" : 39,
      "context" : ", 2016) and crosstask (Zhang and Weiss, 2016) joint-modelling on monolingual treebanks.",
      "startOffset" : 22,
      "endOffset" : 45
    }, {
      "referenceID" : 28,
      "context" : "Besides these three dimensions in dealing with heterogeneous text data, another popular area of research is on the topic of domain adaption, which is commonly associated with crosslingual problems (Nivre et al., 2007).",
      "startOffset" : 197,
      "endOffset" : 217
    }, {
      "referenceID" : 25,
      "context" : "Unsupervised rule-based approaches also offer an competitive alternative for cross-lingual dependency parsing (Naseem et al., 2010; Gillenwater et al., 2010; Gelling et al., 2012; Søgaard, 2012a,b; Martı́nez Alonso et al., 2017), and recently been benchmarked for the Universal Dependencies formalism by exploiting the linguistic constraints in the Universal Dependencies to improve the robustness against error propagation and domain adaption (Martı́nez Alonso et al.",
      "startOffset" : 110,
      "endOffset" : 228
    }, {
      "referenceID" : 27,
      "context" : "Universal Dependencies provides a set of multilingual treebanks with cross-lingually consistent dependency-based lexicalist annotations, designed to aid development and evaluation for cross-lingual systems, such as multilingual parsers (Nivre et al., 2016).",
      "startOffset" : 236,
      "endOffset" : 256
    }, {
      "referenceID" : 16,
      "context" : "The deviations of Singlish from English come from both the lexical and the grammatical levels (Leimgruber, 2009, 2011), which bring challenges for analysis on Singlish using English NLP tools. The former involves imported vocabularies from the first languages of the local people and the latter can be represented by a set of relatively localized features which collectively form 5 unique grammars of Singlish according to Leimgruber (2011). We find empirically that all these deviations can be accommodated by applying the existing English dependency relation definitions while ensuring consistency with the annotations in other non-English UD treebanks, which are explained with examples as follows.",
      "startOffset" : 95,
      "endOffset" : 441
    }, {
      "referenceID" : 27,
      "context" : "Although the “dislocated” (dislocated elements) relation in UD is also used for preposed elements, but it captures the ones “that do not fulfill the usual core grammatical relations of a sentence” and “not for a topic-marked noun that is also the subject of the sentence” (Nivre et al., 2016).",
      "startOffset" : 272,
      "endOffset" : 292
    }, {
      "referenceID" : 17,
      "context" : "It may be regarded as a branch of “Topic-prominence” but is a distinctive feature of Singlish with relatively high frequency of usage (Leimgruber, 2011).",
      "startOffset" : 134,
      "endOffset" : 152
    }, {
      "referenceID" : 17,
      "context" : "Inversion: Inversion in Singlish involves either keeping the subject and verb in interrogative sentences in the same order as in statements, or tag questions in polar interrogatives (Leimgruber, 2011).",
      "startOffset" : 182,
      "endOffset" : 200
    }, {
      "referenceID" : 19,
      "context" : "Discourse particles: Usage of clausal-final discourse particles, which originates from Hokkien and Cantonese, is one of the most typical feature of Singlish (Leimgruber, 2009, 2011; Lim, 2007).",
      "startOffset" : 157,
      "endOffset" : 192
    }, {
      "referenceID" : 10,
      "context" : "Sentences are reversely sorted according to the log likelihood of the sentence given by an English language model trained using the KenLM toolkit (Heafield et al., 2013)9 normalized by the sentence length, so that those most different from standard English can be chosen.",
      "startOffset" : 146,
      "endOffset" : 169
    }, {
      "referenceID" : 12,
      "context" : "The bi-LSTM networks with a CRF layer (bi-LSTM-CRF) have shown state-of-the-art performance by globally optimizing the tag sequence (Huang et al., 2015; Chen et al., 2016).",
      "startOffset" : 132,
      "endOffset" : 171
    }, {
      "referenceID" : 5,
      "context" : "The bi-LSTM networks with a CRF layer (bi-LSTM-CRF) have shown state-of-the-art performance by globally optimizing the tag sequence (Huang et al., 2015; Chen et al., 2016).",
      "startOffset" : 132,
      "endOffset" : 171
    }, {
      "referenceID" : 5,
      "context" : "In order to obtain automatically predicted POS tags as features for a base English dependency parser, we train a POS tagger for UD-Eng using the baseline model of Chen et al. (2016), depicted in Figure 3.",
      "startOffset" : 163,
      "endOffset" : 182
    }, {
      "referenceID" : 5,
      "context" : "Based on this English POS tagging model, we train a POS tagger for Singlish using the featurelevel neural stacking model of Chen et al. (2016). Both the English and Singlish models consist of an input layer, a feature layer, and an output layer.",
      "startOffset" : 124,
      "endOffset" : 143
    }, {
      "referenceID" : 2,
      "context" : "Input Layer: Each token is represented as a vector by concatenating a word embedding from a lookup table with a weighted average of its character embeddings given by the attention model of Bahdanau et al. (2014). Following Chen et al.",
      "startOffset" : 189,
      "endOffset" : 212
    }, {
      "referenceID" : 2,
      "context" : "Input Layer: Each token is represented as a vector by concatenating a word embedding from a lookup table with a weighted average of its character embeddings given by the attention model of Bahdanau et al. (2014). Following Chen et al. (2016), the input layer produces a dense representation for the current input token by concatenating its word vector and the ones for its surrounding context tokens in a window of finite size.",
      "startOffset" : 189,
      "endOffset" : 242
    }, {
      "referenceID" : 5,
      "context" : "Following Chen et al. (2016), we adopt bi-LSTM with peephole connections (Graves and Schmidhuber, 2005).",
      "startOffset" : 10,
      "endOffset" : 29
    }, {
      "referenceID" : 5,
      "context" : "We adopt the deep integration neural stacking structure presented in Chen et al. (2016). As shown in Figure 4, the distributed vector representation for the target word at the input layer of the Singlish Tagger is augmented by concatenating the emission vector produced by the English Tagger with the original word and character-based embeddings, before applying the concatenation within a context window in section 4.",
      "startOffset" : 69,
      "endOffset" : 88
    }, {
      "referenceID" : 7,
      "context" : "(2016) to train a 1-layer biLSTM-CRF based POS tagger on UD-Eng, using 50-dimension pre-trained SENNA word embeddings (Collobert et al., 2011).",
      "startOffset" : 118,
      "endOffset" : 142
    }, {
      "referenceID" : 5,
      "context" : "We use the publicly available source code15 by Chen et al. (2016) to train a 1-layer biLSTM-CRF based POS tagger on UD-Eng, using 50-dimension pre-trained SENNA word embeddings (Collobert et al.",
      "startOffset" : 47,
      "endOffset" : 66
    }, {
      "referenceID" : 5,
      "context" : "We use the publicly available source code15 by Chen et al. (2016) to train a 1-layer biLSTM-CRF based POS tagger on UD-Eng, using 50-dimension pre-trained SENNA word embeddings (Collobert et al., 2011). We set the hidden layer size to 300, the initial learning rate for Adagrad (Duchi et al., 2011) to 0.01, the regularization parameter λ to 10−6, and the dropout rate to 15%. The tagger gives 94.84% accuracy on the UD-Eng test set after 24 epochs, chosen according to development tests, which is comparable to the stateof-the-art accuracy of 95.17% reported by Plank et al. (2016). We use these settings to perform 10fold jackknifing of POS tagging on the UD-Eng training set, with an average accuracy of 95.",
      "startOffset" : 47,
      "endOffset" : 583
    }, {
      "referenceID" : 26,
      "context" : "Similarly, we trained a POS tagger using the Singlish dependency treebank alone with pretrained word embeddings on The Singapore Component of the International Corpus of English (ICE-SIN) (Nihilani, 1992; Ooi, 1997), which consists of both spoken and written texts.",
      "startOffset" : 188,
      "endOffset" : 215
    }, {
      "referenceID" : 29,
      "context" : "Similarly, we trained a POS tagger using the Singlish dependency treebank alone with pretrained word embeddings on The Singapore Component of the International Corpus of English (ICE-SIN) (Nihilani, 1992; Ooi, 1997), which consists of both spoken and written texts.",
      "startOffset" : 188,
      "endOffset" : 215
    }, {
      "referenceID" : 8,
      "context" : "We adopt the Dozat and Manning (2017) parser17 as our base model, as displayed in Figure 5, and apply neural stacking to achieve improvements over the baseline parser.",
      "startOffset" : 13,
      "endOffset" : 38
    }, {
      "referenceID" : 8,
      "context" : "Following Dozat and Manning (2017), we adopt Cif-LSTM cells (Greff et al.",
      "startOffset" : 10,
      "endOffset" : 35
    }, {
      "referenceID" : 5,
      "context" : "Inspired by the idea of feature-level neural stacking (Chen et al., 2016; Zhang and Weiss, 2016), we concatenate the pre-trained word embedding, trainable word and tag embeddings, with the two recurrent state vectors at the last bi-LSTM layer of the English Tagger as the input vector for each target word.",
      "startOffset" : 54,
      "endOffset" : 96
    }, {
      "referenceID" : 39,
      "context" : "Inspired by the idea of feature-level neural stacking (Chen et al., 2016; Zhang and Weiss, 2016), we concatenate the pre-trained word embedding, trainable word and tag embeddings, with the two recurrent state vectors at the last bi-LSTM layer of the English Tagger as the input vector for each target word.",
      "startOffset" : 54,
      "endOffset" : 96
    }, {
      "referenceID" : 8,
      "context" : "We train an English parser on UD-Eng with the default model settings in Dozat and Manning (2017).",
      "startOffset" : 72,
      "endOffset" : 97
    }, {
      "referenceID" : 0,
      "context" : "90% LAS on UD-Eng reported by Ammar et al. (2016), and the main difference is caused by us not using fine-grained POS tags.",
      "startOffset" : 30,
      "endOffset" : 50
    }, {
      "referenceID" : 0,
      "context" : "90% LAS on UD-Eng reported by Ammar et al. (2016), and the main difference is caused by us not using fine-grained POS tags. We apply the same settings for a baseline Singlish parser. We attempt to choose a better configuration of the number of bi-LSTM layers and the hidden dimension based on the development set performance, but the default settings turn out to perform the best. Thus we stick to all default hyper-parameters in Dozat and Manning (2017) for training the Singlish parsers.",
      "startOffset" : 30,
      "endOffset" : 455
    }, {
      "referenceID" : 30,
      "context" : "In order to learn characteristics of distributed lexical semantics for Singlish, we compare performances of the Singlish dependency parser using several sets of pre-trained word embeddings: GloVe6B, large-scale English word embeddings18; ICE-SIN, Singlish word embeddings trained using GloVe (Pennington et al., 2014) on the ICE-SIN (Nihilani, 1992; Ooi, 1997) corpus; Giga100M, a small-scale English word embeddings trained using GloVe (Pennington et al.",
      "startOffset" : 292,
      "endOffset" : 317
    }, {
      "referenceID" : 26,
      "context" : ", 2014) on the ICE-SIN (Nihilani, 1992; Ooi, 1997) corpus; Giga100M, a small-scale English word embeddings trained using GloVe (Pennington et al.",
      "startOffset" : 23,
      "endOffset" : 50
    }, {
      "referenceID" : 29,
      "context" : ", 2014) on the ICE-SIN (Nihilani, 1992; Ooi, 1997) corpus; Giga100M, a small-scale English word embeddings trained using GloVe (Pennington et al.",
      "startOffset" : 23,
      "endOffset" : 50
    }, {
      "referenceID" : 30,
      "context" : ", 2014) on the ICE-SIN (Nihilani, 1992; Ooi, 1997) corpus; Giga100M, a small-scale English word embeddings trained using GloVe (Pennington et al., 2014) with the same settings on a comparable size of English data randomly selected from the English Gigaword Fifth Edition for a fair comparison with ICE-SIN embeddings.",
      "startOffset" : 127,
      "endOffset" : 152
    } ],
    "year" : 2017,
    "abstractText" : "Singlish can be interesting to the ACL community both linguistically as a major creole based on English, and computationally for information extraction and sentiment analysis of regional social media. We investigate dependency parsing of Singlish by constructing a dependency treebank under the Universal Dependencies scheme, and then training a neural network model by integrating English syntactic knowledge into a state-ofthe-art parser trained on the Singlish treebank. Results show that English knowledge can lead to 25% relative error reduction, resulting in a parser of 84.47% accuracies. To the best of our knowledge, we are the first to use neural stacking to improve cross-lingual dependency parsing on low-resource languages. We make both our annotation and parser available for further research.",
    "creator" : "LaTeX with hyperref package"
  }
}