{
  "name" : "1704.01155.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks",
    "authors" : [ "Weilin Xu", "David Evans", "Yanjun Qi" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Although deep neural networks (DNNs) have achieved great success in many computer vision tasks, recent studies have shown they are vulnerable to adversarial examples. Such examples, typically generated by adding small but purposeful distortions, can frequently fool DNN models. Previous studies to defend against adversarial examples mostly focused on refining the DNN models. They have either shown limited success or suffer from expensive computation. We propose a new strategy, feature squeezing, that can be used to harden DNN models by detecting adversarial examples. Feature squeezing reduces the search space available to an adversary by coalescing samples that correspond to many different feature vectors in the original space into a single sample. By comparing a DNN model’s prediction on the original input with that on the squeezed input, feature squeezing detects adversarial examples with high accuracy and few false positives. This paper explores two instances of feature squeezing: reducing the color bit depth of each pixel and smoothing using a spatial filter. These strategies are straightforward, inexpensive, and complementary to defensive methods that operate on the underlying model, such as adversarial training."
    }, {
      "heading" : "1 Introduction",
      "text" : "Deep Neural Networks (DNNs) perform exceptionally well on many artificial intelligence tasks, including security-sensitive applications like malware classification [15, 4] and face recognition [22]. Unlike when machine learning is used in other fields, security applications involve intelligent and adaptive adversaries responding to the deployed systems. Recent studies have shown that attackers can force deep learning models to mis-classify images by making imperceptible modifications to pixel values. The maliciously generated inputs are called “adversarial examples” [6, 24] and are\nnormally crafted by using an optimization procedure to search for small, but effective, artificial perturbations.\nThe goal of our work is to harden DNN systems against adversarial examples. Most of the previous work towards this goal has focused on modifying the DNN models themselves, including adversarial training and gradient masking (details in Section 2.3). In contrast, our work focuses on finding simple and low-cost defensive strategies that alter the input samples but leave the model unchanged. Our approach is driven by the intuition that the standard feature input space for image classification is unnecessarily large, and this vast input space provides extensive opportunities for an adversary to construct adversarial examples. Our strategy is to reduce the degrees of freedom available to an adversary by “squeezing” out unnecessary input features. The key to our approach is to compare the model’s prediction on the original sample with its prediction on the sample after squeezing, as depicted in Figure 1. If the original and squeezed examples produce substantially different outputs from the model, the input is likely to be adversarial. By measuring the disagreement among predictions and selecting a threshold value, our system outputs the correct prediction for legitimate examples and rejects adversarial inputs.\nFor image classification, there are many possible ways to squeeze features. In this work, we explore two simple methods: reducing the color depth of each pixel in an image, and using spatial smoothing to reduce the difference among individual pixels. We demonstrate that feature squeezing significantly enhances the robustness of a\nar X\niv :1\n70 4.\n01 15\n5v 1\n[ cs\n.C V\n] 4\nA pr\n2 01\nmodel by predicting correct labels of adversarial examples (Section 5), while preserving the accuracy on legitimate inputs (Section 4), thus enabling an accurate detector for adversarial examples (Section 6). A few other recent studies have proposed methods to detect adversarial examples through sample statistics, training a detector, or prediction inconsistency (Section 2.4). Feature squeezing appears to be both more accurate, and far less expensive, than previous methods.\nContributions. Our key contribution is introducing and evaluating feature squeezing as a technique for detecting adversarial examples. We study two instances of feature squeezing: reducing color bit depth (Section 3.1) and spatial smoothing (Section 3.2). In Section 4, we show that both techniques preserve high accuracy on legitimate examples (on both the grey-scaled MNIST and colored CIFAR-10 datasets) for state-of-the-art DNN architectures. Section 5 evaluates the effectiveness of two squeezing methods for defending against adversarial samples produced by both FGSM and JSMA attacks. The bit depth reduction increases the accuracy on FGSM-ε-0.3 adversarial examples from 10.74% to 94.44% on MNIST. When combined with adversarial training, it further increases the accuracy to 96.37%. These results enable feature squeezing to be used to detect adversarial examples, as we demonstrate in Section 6. We achieve 98.73% detection accuracy in our experiment when considering two adversary techniques, FGSM and JSMA (Section 6). Although our work is primarily empirical, Section 7 discusses why feature squeezing works and considers how adversaries may adapt to deployed classifiers that use feature squeezing."
    }, {
      "heading" : "2 Background",
      "text" : "This section provides a brief introduction to neural networks, methods for finding adversarial examples, and proposed defenses."
    }, {
      "heading" : "2.1 Neural Networks",
      "text" : "Deep Neural Networks (DNNs) can efficiently learn highly-accurate models from large corpora of training samples in many domains [11, 9, 15]. Convolutional Neural Networks (CNNs), first popularized by LeCun et al. [13], perform exceptionally well on image classification. A deep CNN can be written as a function g : X→Y , where X represents the input space and Y is the output space representing a categorical set. For a sample x ∈ X ,\ng(x) = fL( fL−1(. . .(( f1(x)))).\nEach fi represents a layer, which can be a classical feedforward linear layer, rectification layer, max-pooling\nlayer, or a convolutional layer that performs a sliding window operation across all positions in an input sample. The last output layer, fL, learns the mapping from a hidden space to the output space (class labels) through a softmax function.\nA training set contains Ntr labeled inputs in which ith input is denoted (xi,yi). When training a deep model, parameters related to each layer are randomly initialized, and input samples (xi,yi) are fed through the network. The output of this network is a prediction g(xi) associated with the ith sample. To train the DNN, the difference between prediction output, g(xi), and its true label, yi, is fed back into the network using a back-propagation algorithm to update DNN parameters.\nImplementation. We use two state-of-the-art CNN models for MNIST and CIFAR-10. The appendix provides details on the architecture and training parameters for both models. Our MNIST model (a three-layer CNN with dropout) achieves a test accuracy of 99.30%; our CIFAR-10 model (a 32-layer ResNet [25]) achieves 92.57% test accuracy. The prediction performance of both CNN models is competitive with state-of-the-art results [1]."
    }, {
      "heading" : "2.2 Generating Adversarial Examples",
      "text" : "An adversarial example is an input crafted by an adversary with the goal of producing an incorrect output from a target classifier. Since ground truth, at least for image classification tasks, is based on human perception which is hard to model or test, research in adversarial examples typically defines an adversarial example as a misclassified sample x′ generated by perturbing a correctly classified sample x by some limited amount. Adversarial examples can be targeted, in which case the adversary’s goal is for x′ to be classified as a particular class z, or untargeted, in which case the adversary’s goal is just for x′ to be classified as any class other than its correct class.\nMore formally, given x ∈ X and g(·), the goal of an adversary is to find an x′ ∈ X such that\ng(x) 6= g(x′)∧∆(x,x′)≤ ε (1) Here, ∆(x,x′) represents the difference between input x and x′. The strength of the adversary, ε , measures the permissible transformations. The distance metric ∆ and the strength threshold ε are meant to model how close the adversarial example needs to be to the original sample to “fool” a human observer.\nSeveral techniques have been proposed to find adversarial examples. Szegedy et al. [24] first observed that DNN models are vulnerable to adversarial perturbation and used the Limited-memory BFGS (L-BFGS) to find adversarial examples. Their study also found that adversarial perturbations generated from one DNN model can\nalso force other DNN models to produce incorrect outputs. Subsequent papers have explored other strategies to generate adversarial manipulations, including using gradient information [6], saliency maps [19], and evolutionary algorithms [16].\nImplementation. Our experiments use implementations of the FGSM and JSMA attacks provided by the Cleverhans library [17]. We describe these two methods in more detail next."
    }, {
      "heading" : "2.2.1 Fast Gradient Sign Method (L∞)",
      "text" : "Goodfellow et al. hypothesized that DNNs are vulnerable to adversarial perturbations because of their linear nature [6]. They proposed the fast gradient sign method (FGSM) for efficiently finding adversarial examples. To control the cost of attacking, FGSM assumes that the attack strength at every feature dimension is the same. This essentially measures the perturbation ∆(x,x′) using the L∞-norm. The strength of perturbation at every dimension is limited by the same constant parameter, ε , which is also used as the perturbation amount, assuming that the maximum permissible perturbation is most effective.\nAs an untargeted attack, the perturbation is calculated directly by using gradient vector of a loss function:\n∆(x,x′) = ε · sign(∇xJ(g(x),y)) (2)\nHere the loss function, J(·, ·), is the loss that have been used in training the specific DNN model, and y is the correct label for x. Equation (2) essentially increases the loss J(·, ·) by perturbing the input x based on a transformed gradient.\nThe adversarial examples in Figure 3 were generated using FGSM with ε ranging from 0.1 to 0.5. Setting ε = 0.0 means no perturbations are permitted; ε = 1.0 allows each pixel to be set to either the minimum or maximum value (with clipping to the range [0,1]). The choice of ε is arbitrary, but examples where ε > 0.3 are typically not considered valid adversarial examples [6] since such high ε values produce images that are obviously different from the original images.\n2.2.2 Jacobian-based Saliency Map Approach (L0) Papernot et al. [19] proposed the Jacobian-based saliency map approach (JSMA) to search for adversarial examples by only modifying a limited number of input pixels in an image. As a targeted attack, JSMA iteratively perturbs pixels in an input image that have high adversarial saliency scores. The adversarial saliency map is calculated from the Jacobian (gradient) matrix ∇xg(x) of the DNN model at the current input x. The (c, p)th component in Jacobian matrix ∇xg(x) describes the derivative of output class c with respect to feature pixel p. The adversarial saliency score of each pixel is calculated to reflect how this pixel will increase the output score of\nthe target class t versus changing the score of all other possible output classes. The process is repeated until misclassification (i.e., being predicted as the target class) is achieved, or it reaches the maximum number of perturbed pixels. Essentially, JSMA optimizes Equation (1) by measuring perturbation ∆(x,x′) through the L0-norm.\nThe images in the third row in Figure 6 were generated using JSMA configured to target the c+1 mod 10 label (e.g., making an adversarial example that looks like a 3 but is classified as a 4). We use the parameters configured by default in Cleverhans [17]."
    }, {
      "heading" : "2.3 Defensive Techniques",
      "text" : "Papernot et al. [20] provide a comprehensive summary of recent work on defending against adversarial samples, grouping work into two broad categories: adversarial training and gradient masking, which we discuss further below.\nA third approach is to modify feature sets, but it has not previously been applied to DNN models. Wang et al. proposed a theory that unnecessary features are the primary cause of a classifier’s vulnerability to adversarial examples [27]. Zhang et al. proposed an adversaryaware feature selection model that can improve classifier robustness against evasion attacks [29]. Our proposed feature squeezing method is broadly part of this theme.\nAdversarial Training. A straightforward solution is adversarial training, which introduces discovered adversarial examples and the corresponding ground truth labels to the training dataset [6, 24]. Ideally, the model will learn how to restore the ground truth from the adversarial perturbations and perform robustly on the future adversarial examples. This technique, however, suffers from the high cost to generate adversarial examples and (at least) doubles the training cost of DNN models due to its iterative re-training procedure. Its effectiveness also depends on having a technique for efficiently generating adversarial examples similar to the one used by the adversary, which may not be the case in practice. As pointed out by Papernot et al. [20], it is essential to include adversarial examples produced by all known attacks in adversarial training, since this defensive training is non-adaptive. But, it is computationally expensive to find adversarial inputs by most known techniques, and there is no way to be confident the adversary is limited to techniques that are known to the trainer.\nGradient Masking. These defenses seek to reduce the sensitivity of DNN models to small changes made to their sample inputs, by forcing the model to produce near-zero gradients. Gu et al. proposed adding a gradient penalty term in the objective function, which is defined as the summation of the layer-by-layer Frobenius norm\nof the Jacobian matrix [8]. Although the trained model behaves more robustly against adversaries, the penalty significantly reduces the capacity of the model and sacrifices accuracy on many tasks [20]. Papernot et al. introduced defensive distillation to harden DNN models [21]. A defensively distilled model is trained with the smoothed labels generated by a normally-trained DNN model. Then to hide model’s gradient information from an adversary, the distilled model replaces its last layer with a “harder” softmax function after training. Experimental results indicating that larger perturbations are required when using JSMA to evade distilled models. However, two subsequent studies showed that defensive distillation failed to mitigate a variant of JSMA with a division trick [2] and a black-box attack [18]. Papernot et al. concluded that methods designed to conceal gradient information are bound to have limited success because of the transferability of adversarial examples [20]."
    }, {
      "heading" : "2.4 Detecting Adversarial Examples",
      "text" : "A few recent studies [14, 7, 5] have focused on detecting adversarial examples. The strategies they explored can be considered into three groups: sample statistics, training a detector and prediction inconsistency. Our method is both far less expensive, and appears to be considerably more accurate, than the best previous methods (see Section 6.2 for a comparison).\nSample Statistics. Grosse et al. [7] propose a statistical test method for detecting adversarial examples using maximum mean discrepancy and energy distance as the statistical distance measures. Their experimental results show that the test method requires a large group of adversarial examples and legitimate samples and is not capable of detecting individual examples, therefore making it less useful in practice. Feinman et al. propose detecting adversarial examples using kernel density estimation [5], which measures the distance between an unknown input example and a group of legitimate examples in a manifold space (represented as features in some middle layers of a DNN). It is computationally expensive and can only detect adversarial examples lying far from the manifolds of the legitimate population. In our opinion, using sample statistics to differentiate between adversarial examples and legitimate inputs is unlikely to be effective due to the intrinsic deceptive nature of such examples. Experimental results from both Grosse et al. [7] and Feinman et al. [5] have found that strategies relying on sample statistics gave inferior detection performance compared to other strategies.\nTraining a Detector. Similar to adversarial training, adversarial examples can also be used to train a detector. Because of the large number of adversarial examples\nneeded, this method is expensive and prone to overfitting employed adversarial techniques. Metzen et al. proposed attaching a CNN-based detector as a branch off a middle layer of the original DNN [14]. The detector outputs two classes and uses adversarial examples (as one class) plus legitimate examples (as the other class) for training. The detector is trained while freezing the weights of the original DNN, so does not sacrifice classification accuracy on the legitimate inputs. Grosse et al. demonstrate a similar detection method (previously proposed by Nguyen et al. [16]) that adds a new “adversarial” class in the last layer of the DNN model [7]. The revised model is trained with both legitimate and adversarial inputs, reducing the accuracy on legitimate inputs due to the change to the model architecture.\nPrediction Inconsistency. The basic idea of prediction inconsistency is to measure the disagreement among several models in predicting an unknown input example, since one adversarial example may not fool every DNN model. Feinman et al. borrowed an idea from dropout [10] and designed a detection technique they called Bayesian neural network uncertainty [5] . In the original form, a dropout layer randomly drops some weights (by temporarily setting to zero) in each training iteration and uses all weights at the testing phase, which can be interpreted as training many different sub-models and averaging their predictions in test. For detecting adversarial examples, Feinman et al. propose using the “training” mode of dropout layers to generate many predictions of each input. They reported that the disagreement among the predictions of sub-models is rare on legitimate inputs but common on adversarial examples, thus can be employed for detection."
    }, {
      "heading" : "3 Feature Squeezing Methods",
      "text" : "Although the notion of feature squeezing is quite general, we focus on two simple squeezing methods: reducing the color depth of images (Section 3.1), and using smoothing to reduce the variation among pixels (Section 3.2). In later sections, we look at the impact of each squeezing method on classifier accuracy (Section 4) and robustness against adversarial inputs (Section 5)."
    }, {
      "heading" : "3.1 Color Depth",
      "text" : "A neural network, as a differentiable model, assumes that the input space is continuous. However, the modern computer architectures only support discrete representations as an approximation of continuous natural data. A standard digital image is represented by an array of pixels, each of which is usually represented as a number that represents a specific color.\nCommon image representations use color bit depths that lead to irrelevant features, so we hypothesise and that reducing bit depth can reduce adversarial opportunity without harming classifier accuracy. Two common representations, which we focus on here because of their use in our test datasets, are 8-bit grayscale and 24-bit color. A grayscale image provides 28 = 256 possible values for each pixel. An 8-bit value represents the intensity of a pixel where 0 is entirely black, 255 is totally white. Other numbers represent different shades of gray. The 8-bit scale can be extended to display color images with three separated channels: red, green and blue. This provides 24 bits of information for each pixel. The 224 ≈ 16 million possible values are capable of representing most of the natural colors, hence the name “True Color”."
    }, {
      "heading" : "3.1.1 Squeezing Color Bits",
      "text" : "While people usually prefer larger bit depth as it makes the displayed image closer to the natural image, large color depths are often not necessary for interpreting images (for example, people have no problems recognizing black-and-white images). We investigate the bit depth squeezing with two popular datasets for image classification: MNIST and CIFAR-10.\nMNIST. The MNIST dataset contains 70,000 images of hand-written digits (0 to 9). Of these, 60,000 images are used as training data and the remaining 10,000 images are used for testing. Each image is 28× 28 pixels, and each pixel is encoded as 8-bit grayscale.\nFigure 2 shows the first example for each class in the MNIST dataset, with the original 8-bit grayscale images in the top row and the 1-bit monochrome images below. The images in the bottom row appear nearly identical to the top row, and are generated by applying a binary filter with 0.5 as the cutoff to images of the top row. The processed images are still recognizable to humans, even though the feature space is only 1/128th the size of the original 8-bit grayscale space.\nFigure 3 hints at why reducing color depth can mitigate adversarial examples generated by the fast gradient sign method (FGSM) [6] (Section 2.2.1). FGSM controls the perturbation strength at every feature dimension by the same constant ε , which is chosen by the adversary (but inherently limited by the desire to find adversarial inputs that appear similar to the original image). The upper row shows five adversarial examples generated from an original input for different values of ε . The lower row shows those examples after reducing the bit depth of each pixel into binary. To a human eye, the binary-filtered images look more like the correct class; in our experiments, we find this is true for DNN classifiers also.\nIn Section 4.1 and Section 5.1, we report that the classification accuracy on MNIST is barely impacted by the binary filter, but its robustness against adversarial inputs\nis greatly improved.\nCIFAR-10. The CIFAR-10 dataset contains 60,000 images, each with 32×32 pixels encoded with 24-bit True Color. Figure 4 shows that we can reduce the original 8-bit (per RGB channel) images to fewer bits without significantly decreasing the image recognizability to humans. It is difficult to tell the difference between the original images with 8-bit per channel color and images using as few as 5 bits of color depth. Bit depths lower than 5 do introduce some human-observable loss, unlike what we observed in the MNIST dataset. This is because we lose much more information in the color image though we reduce the same bits per channel. For example, if we reduce the bit-depth-per-channel from 8 bits to 1 bit, the resulting grayscale space is 1/128 large as the original, while the resulting RGB space is only 2−(24−3) = 1/2,097,152 of the original size. Nevertheless, in Section 4.1 we find that a model trained on only the 1-bit-per-channel CIFAR-10 images still achieves surprisingly high accuracy (77.00%)."
    }, {
      "heading" : "3.1.2 Implementation",
      "text" : "We add a color depth reduction layer in front of the DNN model. We assume the image pixel values have been scaled in the range of [0,1]. Therefore, we design our bit depth reduction operation to keep the original value scale while squeezing out unnecessary precision. We use a simple nonlinear approach to reduce bit depth equally on each channel. Section 3.2 discusses some alternatives that may be worth exploring.\nThe reduction operation is implemented with the Python numpy library using the code shown in Figure 5. The input x is a pixel matrix in the size of width×height representing an 8-bit grayscale image scaled in the [0,1]\nrange (or a three-dimensional tensor of size width× height×3 representing a 24-bit True Color image). The parameter bit depth is the target bit depth which is an integer chosen from [1,7]. The function returns an image in the same size and value scale, but with its effective bit depth reduced to bit depth (even though the values are still represented using floating point).\nThough the general method in Figure 5 can reduce the bit depth to 1, the binary filter can be implemented more efficiently. It only takes one line of code in Tensorflow to binarize an [0,1] scaled input (using 0.5 as the cutoff):\nx bin = tf.nn.relu(tf.sign(x−0.5))"
    }, {
      "heading" : "3.2 Spatial Smoothing",
      "text" : "Spatial smoothing (also known as blur) is a group of techniques widely used in image processing for reducing image noise. Common methods include Gaussian smoothing and the median smoothing method we use.\nMedian smoothing is a non-linear noise reduction technique [28]. As we report in Section 5.2, it is particularly effective in mitigating adversarial examples generated by the L0 attack. The median filter runs a sliding window over each pixel of the image, where the center pixel is replaced by the median value of the neighboring pixels within the window. It does not actually reduce the number of pixels in the image, but spreads pixel values across nearby pixels. The median filter is essentially\nsqueezing features out of the sample by making adjacent pixels more similar.\nThe size of the window is a configurable parameter, ranging from 1 up to the image size. If it were set to the image size, it would (modulo edge effects) flatten the entire image to one color. A square shape window is often used in median filtering, though there are other design choices. Several padding methods can be employed for the pixels on the edge, since there are no real pixels to fill the window. We choose reflect padding [23], in which we mirror the image along with the edge for calculating the median value of a window when necessary.\nMedian smoothing is particularly effective at removing sparsely-occurring black and white pixels in an image (descriptively known as salt-and-pepper noise), whilst preserving edges of objects well.\nFigure 6 presents some examples from MNIST with median smoothing of a 3x3-window. It suggests why spatial smoothing can mitigate adversarial examples generated by the Jacobian-based saliency map approach (JSMA) [19] (Section 2.2.2). JSMA identifies the most influential pixels and modifies their values to a maximum or minimum. The first row shows the first image from each digit class in the MNIST dataset. The second row displays the results of applying a 3× 3 median filter to\nthose images. The third row shows generated adversarial examples, using the targeted JSMA attack, and the bottom row illustrates the result of spatially smoothing those adversarial examples. As with FGSM, both humans and machines see the correct image class after smoothing.\nImplementation. We use the median filter implemented in SciPy."
    }, {
      "heading" : "3.3 Other Squeezing Methods",
      "text" : "Our results in this paper are limited to these two simple squeezing methods, which are surprisingly effective on our test datasets. However, we believe many other squeezing methods are possible, and continued experimentation will be worthwhile to find the most effective squeezing methods.\nOne possible area to explore includes lossy compression techniques. Kurakin et al. explored the effectiveness of the JPEG format in mitigating the adversarial examples [12]. Their experiment shows that a very low JPEG quality (e.g. 10 out of 100) is able to destruct the adversarial perturbations generated by FGSM with ε=16 (at scale of [0,255]) for at most 30% of the successful adversarial examples. However, they didn’t evaluate the potential loss on the accuracy of legitimate inputs. .\nAnother possible direction is dimension reduction. For example, Turk and Pentland’s early work pointed out that many pixels are irrelevant features in the face recognition tasks, and the face images can be projected to a feature space named eigenfaces [26]. Even though eigenfaces doesn’t fit CNNs which exploit the spatial features of an image, the image restoration through eigenfaces may be a useful technique to mitigate adversarial perturbations in a face recogintion task."
    }, {
      "heading" : "4 Accuracy",
      "text" : "Before considering the impact of feature squeezing on adversarial inputs, we confirm our intuition that image features can be squeezed without harming classifier accuracy. We verify that the two feature squeezing methods, bit depth reduction and median smoothing, are capable of preserving the model accuracy on legitimate inputs. This supports our theory that many irrelevant features are present in images as used in typical image classification tasks."
    }, {
      "heading" : "4.1 Color Depth",
      "text" : "MNIST. We train an MNIST model using the original training data, and test its accuracy by adjusting the bit depth of the testing images. Table 3 shows the classification accuracy on the legitimate samples of the MNIST\ndataset. Reducing the bit depth of input images from 8 bits to 1 decreases the accuracy on the test samples from 99.30% to 99.24%. That is, over the 10,000 images, the most aggressive squeezing increases the total number of incorrect predictions by 6. The binary filter fixes 13 predictions (Table 1, and introduces 19 incorrect predictions (Table 2). For two of the images (included at the right most of Table 1), the DNN model makes different predictions on the original and binary-filtered images, but both of them are incorrect. 1\nCIFAR-10. Table 4 shows the accuracy of trained models on the CIFAR-10 dataset as we vary the bit depth. The vertical axis shows the bit depth of the filter used on the training images, so the first row (at bit depth 8) corresponds to the original model. Note that the most accurate model is not always found when the training and testing bit depths are the same—for example, the model trained on 6-bit depth images performs slightly better (92.60%) on the 8-bit (original) test images than the original model (92.50%).\nWe also trained models using images with multiple bit depths, shown at the bottom of Table 4. For example, the 1357 model was trained using four versions of each training image, filtered at bit-depth 1, 3, 5, and 7. Notably, this model outperforms the best single-filter trained model for depths 1–3, and is within .01 of the best classifier at all other testing depths (4–8). This is the classifier we use for the robustness experiments in Section 5.1."
    }, {
      "heading" : "4.2 Spatial Smoothing",
      "text" : "MNIST. Spatial smoothing has a larger impact on the legitimate accuracy than the bit depth reduction (the accuracy results are included in Figure 9a in Section 5.2). The original accuracy on legitimate inputs is preserved with the window size smaller than 5× 5, but drops off quickly for larger window sizes, and is below 0.5 for 8×8. We choose the window size as 3×3, which causes only a small accuracy drop from 99.30% to 99.10% (20 out of the 10,000 testing images that were classified correctly are misclassified after performing 3× 3 smoothing). We believe this is an acceptable loss considering the strengthened robustness against adversary (Section 5.2).\nCIFAR-10. In addition to square windows, we also try rectangular window shapes in median smoothing on CIFAR-10: K ×K, 1×K and K × 1, for values of K from 1 to 8. The three window shapes have different impacts on the accuracy, as shown in Figure 7. In general, K× 1 has the highest accuracy, followed by 1×K and K×K. This makes sense because the square shape\n1The “correct” class for each image is based on human judgment, as recorded in the MNIST dataset labels. For many of these examples, it is not abundantly obvious what digit the writer intended.\nhas the largest window size, resulting in more aggressive smoothing. The fact that K×1 outperforms 1×K in all cases implies that more information is contained in the vertical direction than horizontally, which is consistent with human vision."
    }, {
      "heading" : "5 Robustness",
      "text" : "The previous section demonstrated that images, as used in classification tasks, contain many irrelevant features that can be squeezed without reducing accuracy. This section evaluates the effectiveness of feature squeezing against adversarial examples.\nThreat model. We assume a powerful adversary who has full access to a target trained model, but no ability to influence that model. The adversary’s goal is to find inputs that are misclassified by the model. We focus on gradient-based, white-box evasion attacks against a trained and fixed DNN classifier, and consider both un-\ntargeted and targeted attacks. For now, we do not consider an adversary that adapts to our methods. Observant readers will notice that our squeezing methods provide new opportunities to adversaries, some of which we discuss in Section 7.2. For this reason, we do not advocate using feature squeezing by itself for hardening DNNs; instead, we advocate for the detection framework depicted in Figure 1 and analyze detection of adversarial\nBit Depth Legitimate ε=.1 ε=.2 ε=.3 eps 0.4 eps 0.5 8 0.993 0.6284 0.2152 0.1083 0.067 0.0472 7 0.993 0.6252 0.2196 0.1075 0.0668 0.0473 6 0.993 0.6696 0.2075 0.1068 0.0674 0.0474 5 0.993 0.6638 0.2247 0.1109 0.0691 0.0468 4 0.993 0.4501 0.2169 0.1203 0.0673 0.046 3 0.9928 0.4081 0.3544 0.1157 0.0628 0.0442 2 0.9926 0.987 0.1011 0.0955 0.0876 0.04 1 0.9924 0.9874 0.9711 0.9444 0.8954 0.5759\neps 1-Bit 8-Bit FGS, no Clipping 0 .992 .993 .993\n0.1 .987 .640 .368 0.2 .971 .215 .143 0.3 .944 .107 .085 0.4 .895 .067 .061 0.5 .576 .047 .049 0.6 .043 .042 .044 0.7 .043 .040 .039 0.8 .043 .038 .036 0.9 .043 .040 .035 1 .043 .043 .033\nexamples in Section 6."
    }, {
      "heading" : "5.1 Color Depth",
      "text" : "The resolution of a specific bit depth is defined as the shortest distance between two values on a specific scale (like [0,1]) that can be distinguished. For example, the resolution of 8-bit color depth is 1 in the [0,255] scale and 0.00390625 in the [0,1] scale. Reducing the bit depth lowers the resolution and diminishes the opportunity an adversary has to find effective perturbations. This should be effective in mitigating L2 and L∞ attacks, as the adversary’s goal is to produce small and imperceptible perturbations. As the resolution is reduced, such perturbations no longer have any impact.\nMNIST. We evaluate the binary filter as a way to mitigate the FGSM adversarial inputs because it is the most powerful one and it barely reduces the accuracy on the legitimate examples. Note that a straightforward gradient-based adversary would fail on attacking\na feature-squeezed model, as the gradient is zeroed by the filter layer. Instead, we assume the adversary can remove the filter layer to restore the unsqueezed model for use in generating the adversarial examples. This is equivalent to generating adversarial examples on the original trained model.\nWe compare the accuracy on the adversarial examples between the original classifier and the one with the binary filter, across 10 different ε values2 ranging from 0.1 to 1.0. Figure 8b shows that the binary filter is effective on the FGSM adversarial inputs, even up to large ε values. For ε = 0.3, the accuracy of the model on adversarial inputs increases from 10.74% (barely above random guessing on the original images, since there are 10 classes) to 94.44% (with the filtered images).\nThe cutoff value of the binary filter is 0.5 in the [0,1] scale, which means changing a pixel’s value by ±0.25\n2Recall that ε is the amount of the pixel value increased or decreased by the fast gradient sign method. So ε = 1 means the adversary might produce invalid pixel values. We always clip the resulting intensity value into the range of [0,1] to produce valid images.\nmedian_k Original Adversarial (JMSA) 1 .993 .014 2 .988 .700 3 .991 .976 4 .980 .953 5 .943 .906 6 .845 .791 7 .650 .616 8 .479 .454\nsquare_median_filter_size_kLegitimate 10KJSMA Adv. 100 1 .9257 .0100 2 .8592 .8400 3 .7812 .7500 4 .6483 .6900 5 .5283 .5300 6 .4256 .4600 7 .3654 .4700 8 .3248 .3700\nhas no affect at all for those pixels with original values falling into [0, .25) and (.75,1]. This explains why the binary filter works well even for large ε values. Although examples generated by FGSM with ε > 0.3 are not really adversarial examples due to the obvious manipulations on the image, the classifier with the binary filter is still able to correctly recognize most of the adversarial examples generated by ε = 0.4 (89.54%) and ε = 0.5 (57.59%), as reported in Figure 8b.\nCIFAR-10. Because the DNN model for CIFAR-10 is more sensitive to the FGSM adversary, adversarial examples at very low ε values can be found. Therefore, we represent the ε in the [0,255] value scale on the CIFAR-10 task. We choose ε from the integers in the\n[1,20] range, the end points of which are equivalent to 0.00390625 and 0.078125 in the [0,1] scale.\nFigures 8c and 8d present the results of 1-bit depth and 3-bit depth filters in mitigating an FGSM adversary for CIFAR-10. The 3-bit depth in testing slightly increases the accuracy on adversarial inputs overall, while almost perfectly preserving the accuracy on legitimate data. The more aggressive 1-bit depth filter is more robust against adversaries, but reduces the accuracy on legitimate inputs from 91.46% to 78.81%.\nThough these results show that our mitigation is not effective enough for CIFAR-10, other squeezing strategies may be more effective, including the spatial smoothing we report on in the next section."
    }, {
      "heading" : "5.2 Spatial Smoothing",
      "text" : "The adversarial perturbations produced by the JSMA (L0) attack are similar to salt-and-pepper noise, though it is introduced intentionally instead of randomly. Note that the adversarial strength of an L0 adversary limits the number of pixels that can be manipulated, so it is not surprising that maximizing the amount of change to each modified pixel is typically most useful to the adversary. Only the first 1,000 test images in MNIST and the first 100 test images in CIFAR-10 are selected for JSMA, since it is dramatically slower than FGSM.\nMNIST. We ran the JSMA implementation from Cleverhans on MNIST, using the default configuration that always increases the value of a selected pixel to the maximum. JSMA is extremely effective in generating adversarial examples but computationally expensive. Hence, we only generated JSMA adversarial examples for the first 1,000 of the 10,000 testing images in MNIST. Figure 9a shows that the success rate of the JSMA adversary\nis close to 100%, reflected by the near-zero accuracy on adversarial examples predicted by the original model (a 1×1 median filter changes nothing).\nThe intuition suggested by Figure 6 that spatial smoothing can effectively mitigate the adversarial examples is confirmed by the experiment with 1,000 samples. Figure 9a shows that spatial smoothing achieves the best accuracy on the L0 adversarial examples when the window size is 3× 3, increasing accuracy from 1.40% (adversary succeeds 98.60% of the time) to 97.60% (adversary succeeds 2.40% of the time). This comes at a cost of reducing the accuracy on legitimate inputs from 99.30% to 99.10%.\nCIFAR-10. We adapted the JSMA code in Cleverhans to attack the CIFAR-10 model that takes colored image as input. Because of the higher computational cost due to the larger model and the larger input space for CIFAR10, we only generated JSMA adversarial examples for the first 100 of the 10,000 images in the CIFAR-10 test set. Figure 9b shows the results using the K×K square window shape to enable comparisons with the MNIST result (though, as noted in Section 4.2, rectangular windows are more promising for CIFAR-10). Without mitigation, JSMA is effective on CIFAR-10 with nearly 100% success (this corresponds to the 1×1 filter size in the figure, where the accuracy on adversarial examples is 1.0%). Spatial smoothing is effective in reducing the salt-noise-like perturbations introduced by the L0 JSMA attack (configured as increasing the pixel values to maximum by default). The best results come with the 2× 2 window, where the accuracy on adversarial examples increases from 1.00% to 84.00% which the accuracy on legitimate data is decreased from 92.57% to 85.92%."
    }, {
      "heading" : "5.3 Combining with Adversarial Training",
      "text" : "Since our approach modifies inputs rather than the model, it is compatible with any defense technique that operates on the model. The most successful previous defense against adversarial examples is adversarial training (Section 2.3). To evaluate the effectiveness of composing our feature squeezing method with adversarial training, we combined it with the adversarial training code from Cleverhans [17]. The objective is to minimize the mean loss on the legitimate examples and the adversarial ones generated by FGSM on the fly with ε = 0.3. The model is trained in 100 epochs, which is the same as the original MNIST model in this work.\nFigure 10 shows that the bit depth reduction by itself significantly outperforms the adversarial training method on MNIST in face of the FGSM adversary, but that the composing both methods produces even better results. Used by itself, the binary filter feature squeezing method outperforms adversarial training for ε values ranging\neps Adv. Train 1-Bit Depth Combined 0 .9932 .9924 .9926\n0.1 .9794 .9874 .9887 0.2 .9540 .9711 .9784 0.3 .9205 .9444 .9637 0.4 .8855 .8954 .9444 0.5 .7827 .5759 .8065 0.6 .6507 .0432 .4991 0.7 .5921 .0432 .4991 0.8 .5530 .0432 .4991 0.9 .5224 .0432 .4991 1 .4991 .0432 .4991\nfrom 0.1 to 0.4. This is the best case for adversarial training since the adversarially-trained model is learning from the same exact adversarial method (retraining is done with FGSM examples generated at ε = 0.3) as the one used to produce the adversarial examples in the test. Nevertheless, our method still outperforms it, even at the same ε = 0.3 value: 94.44% accuracy on adversarial examples compared to 92.05%.\nFeature squeezing is far less expensive than adversarial training. It is almost cost-free, as we simply insert a binary filter into the pre-trained MNIST model. On the other hand, adversarial training is very expensive as it requires both generating adversarial examples and retraining the classifier for many epochs.3\nWhen its cost is not prohibitive, though, adversarial training is still beneficial since it can be combined with feature squeezing. Simply inserting a binary filter before the adversarially-trained model increases the robustness against an FGSM adversary. For example, the accuracy on adversarial inputs with ε = 0.3 is 96.37% for the combined model, which significantly outperforms both standalone approaches: 92.05% for adversarial training and 94.44% for the bit depth reduction."
    }, {
      "heading" : "6 Detecting Adversarial Inputs",
      "text" : "We consider a simple strategy for detecting adversarial inputs that builds upon our feature squeezing methods: to compare the model’s prediction on the original sample with its prediction on the sample after squeezing. The predictions of an legitimate example and its\n3We would like to test retraining with the JSMA adversary on CIFAR-10 also, but have not been able to do this experiment as the time to do adversarial training using JSMA on a larger model is prohibitively expensive.\n0 500\n1000\n1500\n2000\n2500\n0.0 0.5 1.0 1.5 2.0\nNon-adversarial (10,000)\n\uD835\uDF00 = .1 \uD835\uDF00 = .2\n\uD835\uDF00 = .3\n0\n50\n100\n150\n200\n250\n0.0 0.5 1.0 1.5 2.0\nFGSM\nNon-adversarial (1000)\nJSMA\n0\n50\n100\n150\n200\n250\n0.0 0.5 1.0 1.5 2.0\nJSMA\nNon-adversarial (1000) (a) FGSM examples.\n0\n500\n1000\n1500\n2000\n2500\n0.0 0.5 1.0 1.5 2.0\nNon-adversarial (10,000)\n\uD835\uDF00 = .1 \uD835\uDF00 = .2\n\uD835\uDF00 = .3\n0\n50\n100\n150\n200\n250\n0.0 0.5 1.0 1.5 2.0\nFGSM\nNon-adversarial (1000)\nJSMA\n0\n50\n100\n150\n200\n250\n0.0 0.5 1.0 1.5 2.0\nJSMA\nNon-adversarial (1000)\n(b) JSMA examples.\n0 500 1000 1500 2000 2500 0.0 0.5 1.0 1.5 2.0 Non-adversarial (10,000) \uD835\uDF00 = .1 \uD835\uDF00 = .2 \uD835\uDF00 = .3\n0\n50\n100\n150\n200\n250\n0.0 0.5 1.0 1.5 2.0\nFGSM\nNon-adversarial (1000)\nJSMA\n0\n50\n100\n150\n200\n250\n0.0 0.5 1.0 1.5 2.0\nJSMA\nNon-adversarial (1000)\n(c) FGSM & JSMA examples.\nFigure 11: Detecting adversarial examples generated by FGSM (or JSMA) with the bit depth reduction (or the median smoothing) technique on MNIST. The three histograms present the distribution of L1 score from each category of examples. We get 10K legitimate examples and 10K adversarial examples for each ε in subfigure (a); and we only get 1K examples of each category in subfigure (b) and (c) due to the time-consuming JSMA. The curves, fitted by 100 histogram bins, are colored differently based on the attack method and the ε value used in FGSM.\nsqueezed version should be the same. On the other hand, if the original and squeezed examples produce substantially different outputs from the model, the input is likely to be adversarial. Our experiments confirm this intuition for both squeezing methods and datasets."
    }, {
      "heading" : "6.1 Experimental Results",
      "text" : "Our experiments on detection use test images from MNIST and explore both the L∞ adversary FGSM and the L0 adversary JSMA.\nWe use L1 norm to measure the difference between the original prediction vector and the filtered prediction:\nscore(x,xsqueezed) = |g(x)−g(xsqueezed)|1 (3)\nHere g(x) is the output vector of a DNN model produced by the softmax layer, whose ith entry reflects the probability x is classified into the ith class. A higher score means there is a greater difference between the original prediction and the filtered prediction for an input x. As our accuracy results confirm (Section 4), such scores should be small on legitimate inputs as the prediction is unlikely to be changed by feature squeezing. On the other hand, the L1 score on adversarial inputs should be larger, as feature squeezing has demonstrated the strong performance recovering the ground truth from adversarial examples (Section 5). This means the filtered prediction will be very different from the prediction without filtering. Thus, we may find a threshold value that accurately distinguishes between legitimate and adversarial inputs.\nWe split each dataset into two groups in the following three experiments—the first half for training the detector and the remainder for validation. Each half contains the same number of legitimate examples and the same number of adversarial ones. We test every different value in the training set as thresholds in the training phase and choose the one that produces the best detection accuracy\non training examples. Next, we use the chosen threshold value on the second half of the dataset and report the accuracy of detection.\nDetecting FGSM Adversary. We simulate an attacker who chooses three different ε values from {0.1,0.2,0.3} in generating the adversarial examples from the 10,000 images in the MNIST test set. The accuracy of the original classifier on these three groups of adversarial examples is 64.03%, 21.52% and 10.74% respectively (Figure 8b in Section 5). This means the overall adversarial success rate is 67.90% (that is, out of the 30,000 adversarial examples, 20,371 are misclassified).\nFigure 11a displays a histogram (drawn as curves fitted on 100 bins) of the L1 scores on the 30,000 adversarial images as well as the original 10,000 legitimate ones. The legitimate examples are very different from the adversarial ones in terms of the distribution of L1 score (which extends above the range shown in the graph), supporting our assumption that the legitimate examples often have small L1 scores, while the adversarial ones often have larger L1.\nThere are only a few outliers. The first 10 bins (0 <= score < 0.18) contain 9,796 out of the 10,000 legitimate examples along with 17 adversarial examples, of which five are unsuccessful ones that are correctly predicted by the original DNN-MNIST model. We also find a few outliers from the legitimate examples. The legitimate example with the largest L1 score (1.5) is the third image to the right in Table 2. A few more legitimate examples from Table 1 and Table 2 also have larger L1 scores, but they are rare outliers that human viewers might already consider adversarial.\nWe try every different values in the training set as thresholds and select 0.3076 as the best with the training data. Using this threshold on the withheld half of the dataset results in a detection accuracy of 99.74% on the validation data, while only 22 out of 5000 legitimate\nsamples are wrongly detected as adversarial. The extremely high detection rate helps build a stronger defense. Even though the DNN model with the bit depth reduction filter is not able to recognize the ground truth of every adversarial example (as in Figure 8b, the accuracy on ε = 0.1, 0.2 and 0.3 are 98.74%, 97.11% and 94.44% respectively), the detector is able to recognize them as adversarial examples near perfectly. Only 30 out of 15,000 (0.2%) adversarial examples in validation are wrongly classified as legitimate inputs.\nAnother interesting observation is that the L1 score is capable of detecting even unsuccessful adversarial inputs. Of the 30,000 adversarial examples generated, 67.90% are misclassified. However, all but 30 of the 15,000 examples (the held out half) are correctly detected as adversarial in validation. This is because instead of measuring the difference between two predicted classes, we use the L1 norm of the difference between the two prediction vectors which provides a more sensitive and detailed measurement.\nDetecting JSMA Adversary. We use the 1,000 adversarial MNIST examples generated by JSMA in Section 5.2 as well as their corresponding legitimate seeds for the detection experiment, which is 2,000 examples in total . We generate the filtered variant of each input by running the 3× 3 median filter, as suggested in Figure 9a. Next we compare the predictions between the original prediction and the filtered one by calculating the L1 score.\nFigure 11b shows the distribution of the L1 scores from two groups. As with FGSM, the legitimate inputs get low L1 scores, and the adversarial ones get higher scores in general, although there are a few outliers. Using the same mechanism as for FGSM detection, we get detection accuracy of 99.20% on the validation data with a 98.80% true positive rate and 0.40% false positive rate by choosing 0.5503 as the threshold.\nJoint Detection. In deployment, the operator of a DNN classifier would not normally know which attack methods are used by an adversary. Consequently, a method that can detect adversarial inputs regardless of the attack type is strongly desired. As we have seen in the bit depth filtering and median smoothing experiments, different feature squeezing methods work on different adversar-\nial strategies so it is important that they can be composed (as depicted in Figure 1) to detect unknown adversarial techniques.\nWe conduct an experiment that detects a mixed group of adversarial examples. It comprises 1,000 legitimate images, 1,000 FGSM (ε = 0.3) images, and 1,000 JSMA images. We produce three input variants for each image: the original x, the binarized xbin, and the smoothed xsmo. We compare the predictions among the three and calculate the final score as the maximum of the three combinations:\nscorejoint =max(score(x,xbin),score(x,xsmo),score(xbin,xsmo))\nAs shown by the histogram in Figure 11c, legitimate examples are well separated from the mixed adversarial ones by this score. Indeed, the detection accuracy is as high as 98.73% in validation by choosing 0.3612 as the threshold, resulting in a true detection rate of 99.50% and the false positive rate of 1.40%."
    }, {
      "heading" : "6.2 Comparison with Other Methods",
      "text" : "We can only make rough comparisons between our results and the detection performance of other published methods due to the dramatically different experimental setups and ways of measuring detection rates. Two previous works conducted their experiments on the same MNIST dataset, including the integrated outlier class detection [7] and the best result from Feinman et al. [5], where a logistic regression model is trained to detect adversarial examples using both the kernel density estimation and the Baysian neural network uncertainty as features. Table 5 summarizes their experimental results and compares with ours.\nIt is important to note that the results in Table 5 are not directly comparable, due to differences in experimental setups. First, the base classifiers used by the three studies are slightly different about the DNN model architecture and the accuracy on legitimate inputs. Second, the datasets for evaluating detection performance vary between studies in the number of examples as well as in the ratio of different example groups.\nDespite those differences, Table 5 presents the experimental results using the same measures as Grosse et al. [7] and Feinman et al. [5]. When compared with\nFeinman et al. [5] using ROC-AUC scores (on the validation sets), our method has superior performance on both detecting FGSM (99.99% over 90.57%) and JSMA (99.67% over 98.13%). Regarding the recall on detecting adversarial examples, our method significantly outperforms the integrated outlier class detection by Grosse et al. on detecting JSMA (98.80% compared to 83.76%), although it preforms slightly worse on FGSM detection (but within 0.3%). The computational cost and risk of over-fitting are both much higher for Grosse et al.’s method than feature squeezing due to generating adversarial examples and retraining the classifier. Besides, as reported by Grosse et al. this strategy reduces the accuracy on legitimate inputs due to the change to the model architecture, whereas our approach is guaranteed to have no impact on accuracy since it works with the original model."
    }, {
      "heading" : "7 Discussion",
      "text" : "The effectiveness of feature squeezing seems surprising since it is so simple and inexpensive compared with other proposed defenses. In this section, we speculate on why it works and discuss how adversaries can adapt to feature squeezing."
    }, {
      "heading" : "7.1 Why Squeezing Works",
      "text" : "Developing a theory of adversarial examples remains an illusive goal, but our intuition is that the effectiveness of squeezing stems from how it reduces the search space of possible perturbations available to an adversary.\nBit Depth Reduction. The original model with 8-bit depth inputs is sensitive to the tiny perturbations even on the least significant bit. By essentially eliminating some of the lower bits, we shrink the feature space and force the adversary to produce larger perturbations. Since the features we effectively eliminate are not relevant for classification, this has little impact on the accuracy of legitimate samples. Bit depth reduction is effective in mitigating the adversarial examples generated by FGSM, but this leaves an open question of how well our method would work against more advanced adversaries. To date, researchers have focused on improving the attack method to reduce the amount of perturbation needed to generate adversarial examples. For example, Carlini et al. [3] improve the L∞ attack and claim that it is “so successful that we can change the classification of an image to any desired label by only flipping the lowest bit of each pixel”. Such an attack is easily defeated by our method, since all those perturbations have no impact on the squeezed features. By its nature, feature squeezing is more effective\nin mitigating the smaller adversarial perturbations.\nSpatial Smoothing. Section 5.2 demonstrated the effectiveness of spatial (median) smoothing on mitigating the L0 perturbations. This is not a surprising result, as the L0 perturbation is similar to salt and pepper noise in image processing, on which the median filter is particularly effective for noise reduction. The main concern of spatial smoothing is how to preserve the accuracy on legitimate data. Different window shapes seems promising from our preliminary results (Figure 7 of Section 4.2). In addition, the multiple channels of a colored image can be smoothed differently, as they might contain different amounts of information about a real image, and human vision perceives different colors differently. Further, we hypothesize that median smoothing may work better on higher resolution images, such as those in the ImageNet dataset, since the same window size of the filter will have less impact on recognizability of a larger image, while still disrupting adversarial perturbations."
    }, {
      "heading" : "7.2 Adversarial Adaptation",
      "text" : "Our threat model considers only static adversaries, who use state-of-the-art methods to find adversarial examples but do not adapt to attack our feature squeezing method directly. In practice, however, an adversary may take advantage of feature squeezing in attacking a DNN model. For example, when facing binary squeezing, an adversary can construct an image by setting all pixel intensity values to be near 0.5. This image is entirely gray to human eyes, but by setting pixel values to either 0.499 or 0.501 it can result in an arbitrary 1-bit filtered image after squeezing. Fortunately, it is possible to detect such an attack by comparing the difference between the original and the squeezed inputs, as the difference would exceed normal thresholds. As seen in Section 6, adversarial examples can be accurately detected based on the differences between the model outputs on the original and squeezed inputs. For deployments where it is too expensive to run the model on both inputs, another strategy could be to make the squeezing nondeterministic. Instead of performing a binary filter with a fixed split point at 0.5, the split point for each pixel would be offset by some small random amount to make the resulting output unpredictable to an adversary.\nWhen the detection deployment is used, to be successful an adversary needs to find an example where the original classifier produces the wrong output and the L1 score between the squeezed and original input is below the detection threshold. We believe this is a much harder problem that finding an adversarial example, as is supported by our experimental results. Introducing randomness in the squeezing method should also help in this case, since\nthe adversary’s search requires the knowledge on the exact squeezing operation."
    }, {
      "heading" : "8 Conclusion",
      "text" : "Adversarial examples lead deep learning models to generate incorrect classifications, raising concerns about using DNNs in security-sensitive tasks. Previous defensive methods have been computationally expensive, and proven vulnerable to small advances in adversarial techniques. In this paper, we propose feature squeezing, a new and general method for making DNN models robust against adversarial inputs, while preserving high accuracy on legitimate examples. Feature squeezing is lowcost and easy to implement, and can be composed with any defense strategy that operates on the model such as adversarial training. By eliminating unnecessary points in the feature space, feature squeezing reduces the degrees of freedom available to an adversary. It can be deployed in a simple framework to detect adversarial examples with high accuracy by comparing a model’s predictions on feature-squeezed inputs with its predictions on the original inputs.\nAvailability The source code of our feature squeezing methods, along with the data from all of our experiments, will soon be available at http://www.EvadeML.org. Please contact us for early access or to be notified when it is posted."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work was funded by grants from the National Science Foundation and Air Force Office of Scientific Research, and a gift from Google. We thank Nicolas Papernot and the other authors of cleverhans for making such a useful tool available to the research community."
    } ],
    "references" : [ {
      "title" : "Defensive Distillation is not Robust to Adversarial Examples",
      "author" : [ "Nicholas Carlini", "David Wagner" ],
      "venue" : "arXiv preprint arXiv:1607.04311,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2016
    }, {
      "title" : "Towards Evaluating the Robustness of Neural Networks",
      "author" : [ "Nicholas Carlini", "David Wagner" ],
      "venue" : "In IEEE Symposium on Security and Privacy (Oakland),",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2016
    }, {
      "title" : "Large-scale Malware Classification using Random Projections and Neural Networks",
      "author" : [ "George E Dahl", "Jack W Stokes", "Li Deng", "Dong Yu" ],
      "venue" : "In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2013
    }, {
      "title" : "Detecting Adversarial Samples from Artifacts",
      "author" : [ "Reuben Feinman", "Ryan R Curtin", "Saurabh Shintre", "Andrew B Gardner" ],
      "venue" : "arXiv preprint arXiv:1703.00410,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2017
    }, {
      "title" : "Explaining and Harnessing Adversarial Examples",
      "author" : [ "Ian J Goodfellow", "Jonathon Shlens", "Christian Szegedy" ],
      "venue" : "In International Conference on Learning Representations (ICLR),",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2015
    }, {
      "title" : "On the (Statistical) Detection of Adversarial Examples",
      "author" : [ "Kathrin Grosse", "Praveen Manoharan", "Nicolas Papernot", "Michael Backes", "Patrick McDaniel" ],
      "venue" : "arXiv preprint arXiv:1702.06280,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2017
    }, {
      "title" : "Towards Deep Neural Network Architectures Robust to Adversarial Examples",
      "author" : [ "Shixiang Gu", "Luca Rigazio" ],
      "venue" : "arXiv preprint arXiv:1412.5068,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2014
    }, {
      "title" : "DeepSpeech: Scaling up End-to-end Speech Recognition",
      "author" : [ "Awni Hannun", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Greg Diamos", "Erich Elsen", "Ryan Prenger", "Sanjeev Satheesh", "Shubho Sengupta", "Adam Coates", "others" ],
      "venue" : "arXiv preprint arXiv:1412.5567,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2014
    }, {
      "title" : "Improving Neural Networks by Preventing Co-adaptation of Feature Detectors",
      "author" : [ "Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov" ],
      "venue" : "arXiv preprint arXiv:1207.0580,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2012
    }, {
      "title" : "ImageNet Classification with Deep Convolutional Neural Networks",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2012
    }, {
      "title" : "Adversarial Examples in the Physical World",
      "author" : [ "Alexey Kurakin", "Ian Goodfellow", "Samy Bengio" ],
      "venue" : "In International Conference on Learning Representations (ICLR) Workshop,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2017
    }, {
      "title" : "Gradient-based Learning Applied to Document Recognition",
      "author" : [ "Yann LeCun", "Léon Bottou", "Yoshua Bengio", "Patrick Haffner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1998
    }, {
      "title" : "On Detecting Adversarial Perturbations",
      "author" : [ "Jan Hendrik Metzen", "Tim Genewein", "Volker Fischer", "Bastian Bischoff" ],
      "venue" : "arXiv preprint arXiv:1702.04267,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2017
    }, {
      "title" : "Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images",
      "author" : [ "Anh Nguyen", "Jason Yosinski", "Jeff Clune" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2015
    }, {
      "title" : "cleverhans v1.0.0: an adversarial machine learning library",
      "author" : [ "Nicolas Papernot", "Ian Goodfellow", "Ryan Sheatsley", "Reuben Feinman", "Patrick McDaniel" ],
      "venue" : "arXiv preprint arXiv:1610.00768,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2016
    }, {
      "title" : "Practical Black-Box Attacks against Deep Learning Systems using Adversarial Examples",
      "author" : [ "Nicolas Papernot", "Patrick McDaniel", "Ian Goodfellow", "Somesh Jha", "Z Berkay Celik", "Ananthram Swami" ],
      "venue" : "arXiv preprint arXiv:1602.02697,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2016
    }, {
      "title" : "The Limitations of Deep Learning in Adversarial Settings",
      "author" : [ "Nicolas Papernot", "Patrick McDaniel", "Somesh Jha", "Matt Fredrikson", "Z Berkay Celik", "Ananthram Swami" ],
      "venue" : "In IEEE European Symposium on Security and Privacy (EuroS&P),",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2016
    }, {
      "title" : "Towards the Science of Security and Privacy in Machine Learning",
      "author" : [ "Nicolas Papernot", "Patrick McDaniel", "Arunesh Sinha", "Michael Wellman" ],
      "venue" : "arXiv preprint arXiv:1611.03814,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2016
    }, {
      "title" : "Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks",
      "author" : [ "Nicolas Papernot", "Patrick McDaniel", "Xi Wu", "Somesh Jha", "Ananthram Swami" ],
      "venue" : "In IEEE Symposium on Security and Privacy (Oakland),",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2016
    }, {
      "title" : "Deep Face Recognition",
      "author" : [ "O.M. Parkhi", "A. Vedaldi", "A. Zisserman" ],
      "venue" : "In British Machine Vision Conference,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2015
    }, {
      "title" : "Intriguing Properties of Neural Networks",
      "author" : [ "Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus" ],
      "venue" : "In International Conference on Learning Representations (ICLR),",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2014
    }, {
      "title" : "Face Recognition using Eigenfaces",
      "author" : [ "Matthew A Turk", "Alex P Pentland" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 1991
    }, {
      "title" : "A Theoretical Framework for Robustness of (Deep) Classifiers Under Adversarial Noise",
      "author" : [ "Beilun Wang", "Ji Gao", "Yanjun Qi" ],
      "venue" : "In International Conference on Learning Representations (ICLR) Workshop,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2017
    }, {
      "title" : "Adversarial Feature Selection against Evasion Attacks",
      "author" : [ "Fei Zhang", "Patrick PK Chan", "Battista Biggio", "Daniel S. Yeung", "Fabio Roli" ],
      "venue" : "IEEE Transactions on Cybernetics,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "Deep Neural Networks (DNNs) perform exceptionally well on many artificial intelligence tasks, including security-sensitive applications like malware classification [15, 4] and face recognition [22].",
      "startOffset" : 164,
      "endOffset" : 171
    }, {
      "referenceID" : 19,
      "context" : "Deep Neural Networks (DNNs) perform exceptionally well on many artificial intelligence tasks, including security-sensitive applications like malware classification [15, 4] and face recognition [22].",
      "startOffset" : 193,
      "endOffset" : 197
    }, {
      "referenceID" : 4,
      "context" : "The maliciously generated inputs are called “adversarial examples” [6, 24] and are normally crafted by using an optimization procedure to search for small, but effective, artificial perturbations.",
      "startOffset" : 67,
      "endOffset" : 74
    }, {
      "referenceID" : 20,
      "context" : "The maliciously generated inputs are called “adversarial examples” [6, 24] and are normally crafted by using an optimization procedure to search for small, but effective, artificial perturbations.",
      "startOffset" : 67,
      "endOffset" : 74
    }, {
      "referenceID" : 9,
      "context" : "Deep Neural Networks (DNNs) can efficiently learn highly-accurate models from large corpora of training samples in many domains [11, 9, 15].",
      "startOffset" : 128,
      "endOffset" : 139
    }, {
      "referenceID" : 7,
      "context" : "Deep Neural Networks (DNNs) can efficiently learn highly-accurate models from large corpora of training samples in many domains [11, 9, 15].",
      "startOffset" : 128,
      "endOffset" : 139
    }, {
      "referenceID" : 11,
      "context" : "[13], perform exceptionally well on image classification.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "[24] first observed that DNN models are vulnerable to adversarial perturbation and used the Limited-memory BFGS (L-BFGS) to find adversarial examples.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 4,
      "context" : "Subsequent papers have explored other strategies to generate adversarial manipulations, including using gradient information [6], saliency maps [19], and evolutionary algorithms [16].",
      "startOffset" : 125,
      "endOffset" : 128
    }, {
      "referenceID" : 16,
      "context" : "Subsequent papers have explored other strategies to generate adversarial manipulations, including using gradient information [6], saliency maps [19], and evolutionary algorithms [16].",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 13,
      "context" : "Subsequent papers have explored other strategies to generate adversarial manipulations, including using gradient information [6], saliency maps [19], and evolutionary algorithms [16].",
      "startOffset" : 178,
      "endOffset" : 182
    }, {
      "referenceID" : 14,
      "context" : "Our experiments use implementations of the FGSM and JSMA attacks provided by the Cleverhans library [17].",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 4,
      "context" : "hypothesized that DNNs are vulnerable to adversarial perturbations because of their linear nature [6].",
      "startOffset" : 98,
      "endOffset" : 101
    }, {
      "referenceID" : 4,
      "context" : "3 are typically not considered valid adversarial examples [6] since such high ε values produce images that are obviously different from the original images.",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 16,
      "context" : "[19] proposed the Jacobian-based saliency map approach (JSMA) to search for adversarial examples by only modifying a limited number of input pixels in an image.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "We use the parameters configured by default in Cleverhans [17].",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 17,
      "context" : "[20] provide a comprehensive summary of recent work on defending against adversarial samples, grouping work into two broad categories: adversarial training and gradient masking, which we discuss further below.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 22,
      "context" : "proposed a theory that unnecessary features are the primary cause of a classifier’s vulnerability to adversarial examples [27].",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 23,
      "context" : "proposed an adversaryaware feature selection model that can improve classifier robustness against evasion attacks [29].",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 4,
      "context" : "A straightforward solution is adversarial training, which introduces discovered adversarial examples and the corresponding ground truth labels to the training dataset [6, 24].",
      "startOffset" : 167,
      "endOffset" : 174
    }, {
      "referenceID" : 20,
      "context" : "A straightforward solution is adversarial training, which introduces discovered adversarial examples and the corresponding ground truth labels to the training dataset [6, 24].",
      "startOffset" : 167,
      "endOffset" : 174
    }, {
      "referenceID" : 17,
      "context" : "[20], it is essential to include adversarial examples produced by all known attacks in adversarial training, since this defensive training is non-adaptive.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 6,
      "context" : "of the Jacobian matrix [8].",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 17,
      "context" : "Although the trained model behaves more robustly against adversaries, the penalty significantly reduces the capacity of the model and sacrifices accuracy on many tasks [20].",
      "startOffset" : 168,
      "endOffset" : 172
    }, {
      "referenceID" : 18,
      "context" : "introduced defensive distillation to harden DNN models [21].",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 0,
      "context" : "However, two subsequent studies showed that defensive distillation failed to mitigate a variant of JSMA with a division trick [2] and a black-box attack [18].",
      "startOffset" : 126,
      "endOffset" : 129
    }, {
      "referenceID" : 15,
      "context" : "However, two subsequent studies showed that defensive distillation failed to mitigate a variant of JSMA with a division trick [2] and a black-box attack [18].",
      "startOffset" : 153,
      "endOffset" : 157
    }, {
      "referenceID" : 17,
      "context" : "concluded that methods designed to conceal gradient information are bound to have limited success because of the transferability of adversarial examples [20].",
      "startOffset" : 153,
      "endOffset" : 157
    }, {
      "referenceID" : 12,
      "context" : "A few recent studies [14, 7, 5] have focused on detecting adversarial examples.",
      "startOffset" : 21,
      "endOffset" : 31
    }, {
      "referenceID" : 5,
      "context" : "A few recent studies [14, 7, 5] have focused on detecting adversarial examples.",
      "startOffset" : 21,
      "endOffset" : 31
    }, {
      "referenceID" : 3,
      "context" : "A few recent studies [14, 7, 5] have focused on detecting adversarial examples.",
      "startOffset" : 21,
      "endOffset" : 31
    }, {
      "referenceID" : 5,
      "context" : "[7] propose a statistical test method for detecting adversarial examples using maximum mean discrepancy and energy distance as the statistical distance measures.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "propose detecting adversarial examples using kernel density estimation [5], which measures the distance between an unknown input example and a group of legitimate examples in a manifold space (represented as features in some middle layers of a DNN).",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 5,
      "context" : "[7] and Feinman et al.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[5] have found that strategies relying on sample statistics gave inferior detection performance compared to other strategies.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 12,
      "context" : "proposed attaching a CNN-based detector as a branch off a middle layer of the original DNN [14].",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 13,
      "context" : "[16]) that adds a new “adversarial” class in the last layer of the DNN model [7].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 5,
      "context" : "[16]) that adds a new “adversarial” class in the last layer of the DNN model [7].",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 8,
      "context" : "borrowed an idea from dropout [10] and designed a detection technique they called Bayesian neural network uncertainty [5] .",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 3,
      "context" : "borrowed an idea from dropout [10] and designed a detection technique they called Bayesian neural network uncertainty [5] .",
      "startOffset" : 118,
      "endOffset" : 121
    }, {
      "referenceID" : 4,
      "context" : "Figure 3 hints at why reducing color depth can mitigate adversarial examples generated by the fast gradient sign method (FGSM) [6] (Section 2.",
      "startOffset" : 127,
      "endOffset" : 130
    }, {
      "referenceID" : 5,
      "context" : "The parameter bit depth is the target bit depth which is an integer chosen from [1,7].",
      "startOffset" : 80,
      "endOffset" : 85
    }, {
      "referenceID" : 16,
      "context" : "It suggests why spatial smoothing can mitigate adversarial examples generated by the Jacobian-based saliency map approach (JSMA) [19] (Section 2.",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 10,
      "context" : "explored the effectiveness of the JPEG format in mitigating the adversarial examples [12].",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 21,
      "context" : "For example, Turk and Pentland’s early work pointed out that many pixels are irrelevant features in the face recognition tasks, and the face images can be projected to a feature space named eigenfaces [26].",
      "startOffset" : 201,
      "endOffset" : 205
    }, {
      "referenceID" : 17,
      "context" : "We choose ε from the integers in the [1,20] range, the end points of which are equivalent to 0.",
      "startOffset" : 37,
      "endOffset" : 43
    }, {
      "referenceID" : 14,
      "context" : "To evaluate the effectiveness of composing our feature squeezing method with adversarial training, we combined it with the adversarial training code from Cleverhans [17].",
      "startOffset" : 165,
      "endOffset" : 169
    }, {
      "referenceID" : 5,
      "context" : "[7] 99.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[5] 90.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "Two previous works conducted their experiments on the same MNIST dataset, including the integrated outlier class detection [7] and the best result from Feinman et al.",
      "startOffset" : 123,
      "endOffset" : 126
    }, {
      "referenceID" : 3,
      "context" : "[5], where a logistic regression model is trained to detect adversarial examples using both the kernel density estimation and the Baysian neural network uncertainty as features.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[7] and Feinman et al.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[5].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[5] using ROC-AUC scores (on the validation sets), our method has superior performance on both detecting FGSM (99.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "[3] improve the L∞ attack and claim that it is “so successful that we can change the classification of an image to any desired label by only flipping the lowest bit of each pixel”.",
      "startOffset" : 0,
      "endOffset" : 3
    } ],
    "year" : 2017,
    "abstractText" : "Although deep neural networks (DNNs) have achieved great success in many computer vision tasks, recent studies have shown they are vulnerable to adversarial examples. Such examples, typically generated by adding small but purposeful distortions, can frequently fool DNN models. Previous studies to defend against adversarial examples mostly focused on refining the DNN models. They have either shown limited success or suffer from expensive computation. We propose a new strategy, feature squeezing, that can be used to harden DNN models by detecting adversarial examples. Feature squeezing reduces the search space available to an adversary by coalescing samples that correspond to many different feature vectors in the original space into a single sample. By comparing a DNN model’s prediction on the original input with that on the squeezed input, feature squeezing detects adversarial examples with high accuracy and few false positives. This paper explores two instances of feature squeezing: reducing the color bit depth of each pixel and smoothing using a spatial filter. These strategies are straightforward, inexpensive, and complementary to defensive methods that operate on the underlying model, such as adversarial training.",
    "creator" : "LaTeX with hyperref package"
  }
}