{
  "name" : "1405.1520.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "claspfolio 2: Advances in Algorithm Selection for Answer Set Programming",
    "authors" : [ "Holger Hoos", "Marius Lindauer", "Torsten Schaub" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n40 5.\n15 20\nv1 [\ncs .A\nI] 7"
    }, {
      "heading" : "1 Introduction",
      "text" : "Answer Set Programming (ASP; (Baral 2003)) has become a popular approach to declarative problem solving. This is mainly due its combination of a rich and simple modeling language with high performance solving technology. ASP decouples problem specifications from solving algorithms; however, modern ASP solvers are known to be sensitive to search configurations – a phenomenon that is common to advanced Boolean constraint processing techniques. To avoid the necessity of manual solver configuration, a substantial amount of research was thus devoted to automated algorithm configuration and selection approaches, as we detail in Section 2; in ASP, we find works by Gebser et al. (2011), Maratea et al. (2012), Silverthorn et al. (2012), Maratea et al. (2013) and Hoos et al. (2014), and in particular the two portfolio-based systems claspfolio (Gebser et al. 2011) and ME-ASP (Maratea et al. 2013). The idea of such portfoliobased systems is to train classifiers on features of benchmark instances in order to predict the putatively best solver from a given solver portfolio. The portfolio of solvers used in this approach may consist of distinct configurations of the same solver or contain different solvers.\nIn what follows, we describe the new portfolio-based ASP system claspfolio, whose earlier version 1.0 won first, second, and third places at various ASP competitions. Version 0.8 of claspfolio was briefly described in a short paper by Gebser et al. (2011) and is conceptually identical to the first stable release of version 1.0. The key design features of this prototype were (i) feature generation using a light-weight version of the ASP solver clasp, the original\nclaspre system, (ii) performance estimation of portfolio solvers via support vector regression, and (iii) a portfolio consisting of different clasp configurations only. In contrast to this rigid original design, the new version 2 of claspfolio provides a modular and open architecture (Section 3) that allows for integrating several different approaches and techniques. This includes (i) different feature generators, (ii) different approaches to solver selection, (iii) variable solver portfolios, as well as (iv) solver-schedule-based pre-solving techniques. The default setting of claspfolio 2 relies on an advanced version of claspre (Section 4), a light-weight version of clasp that produces statistics based on which numerous static and dynamic instance features are generated.\nThe flexible and open design of claspfolio 2 is a distinguishing factor even beyond ASP. As such, it provides a unique framework for comparing and combining existing approaches and techniques in a uniform setting. We take advantage of this and conduct an extensive experimental study comparing the influence of different options regarding (i), (ii), and (iii). In addition to gaining insights into the impact of the various approaches and techniques, we identify distinguished options showing substantial performance gains not only over clasp’s default configuration but moreover over manually tuned configurations of clasp. claspfolio 2 is 19-51% faster than the best known static clasp configuration and also 14-37% faster than claspfolio 1.0, as shown in Table 7 at the end of the paper. To facilitate reproducibility of our results and to promote the use of high-performance ASP solving technology, we have made claspfolio 2 publicly available as open-source software at http://potassco.sourceforge.net/#claspfolio."
    }, {
      "heading" : "2 Related Work",
      "text" : "Our work continues a long line of research that can be traced back to John Rice’s seminal work on algorithm selection (Rice 1976) on one side, and to work by Huberman et al. (1997) on parallel algorithm portfolios on the other side. Especially on SAT problems, automatic algorithm selectors have achieved impressive performance improvements in the last decade. SATzilla (Xu et al. 2008; 2007; 2009; 2011; 2012) predicted algorithm performance by means of ridge regression until 2009 and nowadays uses a pairwise voting scheme based on random forests; ISAC (Kadioglu et al. 2010) clusters instances in the instance feature space and uses a nearest neighbour approach on cluster centers for algorithm selection; 3S (Kadioglu et al. 2011; Malitsky et al. 2013) uses k-NN in the feature space and introduces pre-solving schedules computed by Integer Linear Programming and cost-sensitive clustering; SNAPP (Collautti et al. 2013) predicts algorithm performance based on instance features and chooses an algorithm based on the similarity of the predicted performances. All these systems are specialized on a single approach. They are highly efficient but do not provide a uniform setting, that is, different inputs and different performance metrics.\nApart from SAT, there exist several algorithm selectors for other problems. Following the original claspfolio of Gebser et al. (2011) approach, Maratea et al. (2012) presented ME-ASP, a multi-engine algorithm selector for ASP with an instance feature generator for syntactic features. Similarly, AQME (Pulina and Tacchella 2007) is a multi-engine selector for QSAT. CP-Hydra (O’Mahony et al. 2008) selects a set of CSP solvers based on case-based reasoning and schedules them heuristically. Stone Soup (Seipp et al. 2012; Helmert et al. 2011) uses greedy hill climbing to find algorithm schedules for planning problems. aspeed (Hoos et al. 2014) also computes algorithm schedules, but takes advantage of the modeling and solving capabilities of ASP to find timeout-minimal schedules.\nRelated to our work on a more general level, Hutter et al. (2012) gave an overview over run-\ntime prediction techniques, which is also used in some algorithm selection approaches, e.g., SATzilla09. A comparison of different machine learning algorithms for algorithm selection was presented by Kotthoff et al. (2012). Based on these results, Kotthoff (2013) introduced LLAMA, Leveraging Learning to Automatically Manage Algorithms, a flexible framework that provides functionality to train and assess the performance of different algorithm selection techniques."
    }, {
      "heading" : "3 Generalized Algorithm Selection Framework",
      "text" : "claspfolio 2’s new algorithm framework combines the flexibility of LLAMA with additional state-of-the-art techniques and produces an executable algorithm selection solver. As such, it provides a unique framework for comparing and combining existing approaches and techniques in a uniform setting. Furthermore, the new design of claspfolio 2 follows the idea of Level 4 of programming by optimisation (Hoos 2012): “The software-development process is centered on the idea of providing design choices and alternatives in all parts of a project that might benefit from them; design choices that cannot be justified convincingly are not made prematurely.”\nA further distinguishing feature of the claspfolio 2 framework is the efficient and deep integration of an algorithm scheduling system, viz. aspeed (Hoos et al. 2014), into an algorithm selection framework to compute a static pre-solving schedule. claspfolio 2 uses aspeed to determine the running times used within pre-solving schedules. Thereby, it considers the estimated quality of the algorithm selector to determine the running time of the complete pre-solving schedule. This also allows us to integrate the pre-solving strategies of SATzilla and 3S.\nThe general workflow underlying claspfolio 2 consists of collecting training data, learning\na prediction model and training a pre-solving schedule; the portfolio-based ASP solver thus obtained solves a given problem instance with the pre-solving schedule and a solver selected by the prediction model. In what follows, we describe how this workflow is implemented efficiently in claspfolio 2; see Figure 1.\n1. Resources. To train an algorithm selector, training instances and a portfolio of algorithms are required. Algorithm selection is based on the assumption that the given training instances are representative for the instances to be solved using the trained algorithm selection solver. In addition, a portfolio, i.e., a set of algorithms with complementary strengths (e.g., high-performance solvers used in a competition), provides the basis for algorithm selectors to efficiently solve a large variety of instances.\n2. Data Collection. An algorithm selection task is defined based on the performance of all algorithms on all training instances (Assess Performance), instance features for each instance (Compute Features) and the costs for feature computation define an algorithm selection task.claspfolio2 supports several feature generators, of which claspre is used by default.\n3. Training. The training phase of claspfolio 2 makes use of two distinct components: Prediction and Scheduling. Both components can also be used separately in claspfolio 2.\nThe Prediction component of claspfolio 2 involves feature pre-processing, e.g., feature normalization and feature selection, and performance pre-processing, e.g., performance score transformation and algorithm filtering1. Based on the preprocessed data, a scoring model is learned, which maps the feature vector for a given problem instance to scores for all algorithms such that algorithms expected to perform well on the given instances are assigned better scores.\nThe Scheduling component of claspfolio 2 computes a timeout-minimal pre-solving schedule using aspeed (Hoos et al. 2014), where each algorithm gets a (potentially zero) time slice of the overall runtime budget available for solving a given problem instance. If the prediction component is not used, the schedule consists only of the given algorithms. If the prediction component is used, cross validation is used to obtain an unbiased estimate of the performance (Performance Estimation) of the prediction component (Arrow I). The resulting performance estimate of the prediction component is used as an additional simulated algorithm in the schedule generation process. All components of the schedule except the simulated one form the pre-solving schedule used in claspfolio 2. If the prediction performs well, the pre-solving schedule may be empty because the pre-solving schedule cannot perform better than a perfect predictor, i.e., the selection of the best solver. In contrast, if prediction performs very poorly (e.g., as a result of non-informative instance features), the simulated algorithm may be assigned a time slice of zero seconds and the prediction component is de facto ignored in the solving step.\nLike SATzilla (Xu et al. 2008), claspfolio 2 allows to ignore instances solved by the presolving schedule (Arrow II) when learning the scoring model, such that the resulting model is focused on the harder instances not solved by the pre-solvers that are actually subject to algorithm selecting during the solving phase.\n4. Solving a (new) instance starts with the computation of its features. If feature computation fails, e.g., because it requires too much time, a backup solver is used to solve the instance. Oth-\n1 Algorithm filtering removes components of the portfolio given some strategy, e.g., algorithms with a marginal contribution on virtual best solver performance of 0 can be removed. In (Xu et al. 2008), this is called solver subset selection and in (Maratea et al. 2012), solver selection.\nerwise, the scoring model is used to score each algorithm of the portfolio based on the computed feature vector. If the algorithm with the best score is part of the pre-solving schedule, it is removed from the schedule, because running the same algorithm twice does not increase the solving probability (when using deterministic algorithms like clasp). Next, the pre-solving schedule is executed.2 If at the end of executing the pre-solving schedule, the instance has not been solved, the algorithm with the highest score is run for the remainder of the overall time budget.\n4 claspre: Instance Features for ASP\nThe entire concept of algorithm selection is based on instance features which characterize benchmark instances and allow for predicting the putatively best solver from a given portfolio. These instance features should be cheap-to-compute to save as much time as possible for the actual solving process, but should also provide sufficient information to distinguish between (classes of) instances for which different solvers or solver configurations work best.\nFor feature generation, claspfolio 2 uses claspre in its default configuration. claspre is a light-weight version of clasp (Gebser et al. 2011) that extracts instance features of ground ASP instances in smodels format (Syrjänen ), using clasp’s internal statistics. The features determined by claspre can be grouped into static and dynamic ones. The former are listed in\n2 Unlike this, SATzilla runs the pre-solving schedule first and then computes the instance features, because the feature computation can be costly in SAT and the pre-solving schedule can solve the instance without incurring this cost. However, this does not permit removal of the selected solver from the pre-solving schedule.\nTable 1 and include 38 properties, such as number of constraints. Beyond that, claspre performs a limited amount of search to collect dynamic information about solving characteristics. These dynamic features are computed after each restart of the search process, where restarts are performed after a fixed number of conflicts. Thereby, 25 dynamic features (Table 2) are extracted after each restart, such as the average number of conflict levels skipped while back-jumping.\nThe number of restarts performed is a parameter of claspre. More restarts lead to longer feature vectors that may contain more information. The number of restarts and number of conflicts between restarts determine the time used by claspre for feature computation We note that the pre-processing and search performed by claspre can actually solve a given ASP instance. The probability of this happening increases with the length of the search performed within claspre; however, at the same time, long runs of claspre reduce the time available for running solvers from the portfolio."
    }, {
      "heading" : "5 Empirical Performance Analysis",
      "text" : "As previously described, claspfolio 2’s modular and open architecture (Section 3) allows for integrating several different approaches and techniques, including (i) different feature generators, (ii) different approaches to solver selection, as well as (iii) variable solver portfolios. Taking advantages of this flexibility, we conducted an extensive experimental study to assess the efficacy of the various choices on large and representative sets of ASP instances.\nTraining data of claspfolio 2 is stored in the algorithm selection data format developed by the COSEAL Group,3 an international group of experts in the field of algorithm selection and configuration. Detailed experimental results and the source code of claspfolio 2 are available at http://www.cs.uni-potsdam.de/claspfolio. Our empirical analysis makes use of commonly used techniques from statistics and machine learning (see, e.g., (Bishop 2007))."
    }, {
      "heading" : "5.1 Setup",
      "text" : "All our experiments were performed on a computer cluster with dual Intel Xeon E5520 quadcore processors (2.26 GHz, 8192 KB cache) and 48 GB RAM per node, running Scientific Linux (2.6.18-308.4.1.el5). Each algorithm run was limited to a runtime cutoff of 600 CPU seconds and to a memory cutoff of 6 GB. Furthermore, we used permutation tests with 100 000 permutations and significance level α = 0.05 to our performance metrics, the (0/1) timeout scores, the PAR10 scores and the PAR1 scores,4 to asses the statistical significance of observed performance differences."
    }, {
      "heading" : "5.2 Instance Sets",
      "text" : "We used all instances submitted to the 2013 ASP Competition in the NP category that could be grounded with gringo (3.0.5) within 600 CPU seconds and 6 GB memory. The resulting instance set consists of 2214 instances from 17 problem classes; we call it Comp-13-Set. As an even more heterogeneous instance set, we used the ASP Potassco-Set introduced by Hoos et al. (2013); it consists of 2589 instances from 105 problem classes and includes instances\n3 https://code.google.com/p/coseal 4 PARX is the penalized average runtime penalizing timeouts by X times the runtime cutoff.\nfrom the ASP competitions organized in 2007 (SLparse track), 2009 (with the encodings of the Potassco group) and 2011 (decision NP-problems from the system track), as well as several instances from the ASP benchmark collection platform asparagus.5 All instances were grounded with gringo, and the grounding time was not counted towards solving the instances.\nEach instance set was randomly split into equally sized, disjoint training and test set; only the training sets were used in the process of building algorithm portfolios. The resulting claspfolio2 solvers were evaluated on the hold-out test sets. We also used the training instances to determine the best claspfolio 2 configuration (Subsection 5.3). To assess the performance of claspfolio 2 (Subsection 5.6), we used a 10-fold cross validation on the test set. Notice that we cannot use the training set for claspfolio 2 to obtain an unbiased learned model, because the algorithm portfolios have an optimistic performance estimation on the training set on which they were build."
    }, {
      "heading" : "5.3 Building Algorithm Portfolios",
      "text" : "In addition to a set of training instances, a portfolio (i.e., a set) of algorithms is required to construct a portfolio solver. claspfolio 2 can handle portfolios containing different solvers as well as different configurations of a given solver, all of which are viewed as individual ASP solvers. We investigated the following portfolios of ASP solvers:\n• Expert-portfolio of four clasp (2.1.3) configurations designed by Benjamin Kaufmann (configurations: frumpy (default), jumpy, handy and crafty) • SOTA-portfolio (Maratea et al. 2012): non-portfolio solvers participating in the 2013 ASP Competition6 and in addition, the well-established solvers cmodels and smodels; in detail: clasp (Gebser et al. 2011), cmodels (Giunchiglia et al. 2006), lp2bv (Nguyen et al. 2013), lp2mip (Liu et al. 2012), lp2sat (Janhunen 2006), smodels (Simons et al. 2002), and wasp (Alviano et al. 2013) • Hydra-like-portfolio (Xu et al. 2010; Xu et al. 2011) of clasp (2.1.3) configurations • ISAC-like-portfolio (Kadioglu et al. 2010) of clasp (2.1.3) configurations\nExpert-portfolio and SOTA-portfolio are portfolios manually constructed by experts. In contrast, Hydra and ISAC are automatic methods for constructing portfolios using algorithm configurators, e.g., ParamILS (Hutter et al. 2007), GGA (Ansótegui et al. 2009) or SMAC (Hutter et al. 2011). They generate a portfolio of configurations of a given solver by determining configurations that complement each other well on a given set of training instances, with the goal of optimizing the performance of the portfolio under the idealized assumption of perfect selection; this performance is also called the virtual best solver (vbs) or oracle performance of the portfolio.\nAn implementation of Hydra that can be applied to solvers for arbitrary problems has not yet been published by Xu et al.; therefore, we have implemented our own version of Hydra (in consultation with the authors), which we refer to as Hydra-like-portfolio in the following. Also, since the only published version of ISAC (2.0) does not include algorithm configuration, we reimplemented the part of ISAC responsible for portfolio generation, dubbed ISAC-like-portfolio. In contrast to the original ISAC, which performs g-means clustering, ISAC-like-portfolio uses k-means clustering, where the number of clusters is determined\n5 http://asparagus.cs.uni-potsdam.de 6 IDP3 was removed from the portfolio because it was strongly dominated by all other solvers.\nby using cross-validation to optimize the scoring function of the k-means procedure (following Hoos et al. (2013)).\nUsing this approach, ISAC-like-portfolio found 15 clusters for Comp-13-Set and 11 clusters for Potassco-Set, inducing 15 and 11 configuration tasks, respectively. To obtain a fair comparison, we allocated the same time budget to Hydra-like-portfolio and allowed it to perform 15 and 11 iterations, respectively (each consisting of one configuration task). The configuration process performed by SMAC (2.06.01; Hutter et al. 2011) on each cluster and in each Hydra iteration, respectively, was allocated 120 000 CPU seconds, i.e., 200 times the target algorithm cutoff time, and 10 independent repetitions, from which the result with the best PAR10 score on the given training set was selected. SMAC optimized PAR10.\nTable 3 shows the performance of the virtual best solvers (i.e., the performance of a perfect algorithm selector) for the different considered portfolios. Interestingly, the results differ qualitatively between two benchmark sets. While SOTA-portfolio performs better than Expert-portfolio on Comp-13-Set, Expert-portfolio is better on Potassco-Set. Furthermore, while for both sets, the automatic generation methods found better performing portfolios than the the manual selected methods, on the Comp-13-Set, ISAC-like-portfolio produced a better results than Hydra-like-portfolio, and the opposite holds for Potassco-Set. Furthermore, unlike conjectured by Maratea et al. (2012), a set of configurations of the same, highly parameterized solver (Expert-portfolio,ISAC-like-portfolioand Hydra-like-portfolio) generally did not yield worse performance than a mixed portfolio, such as SOTA-portfolio.\nWhile we gave Hydra the same time budget as ISAC to find portfolios, the components added by Hydra-like-portfolio in its final three iterations decreased the number of timeouts only by one on our training and test sets. Following Xu et al. (2010), Hydra would be terminated when the performance does not improve on the training set after an iteration. Hence, Hydra-like-portfolio not only produced a better portfolio on Potassco-Set than ISAC, but also does so using less configuration time than ISAC."
    }, {
      "heading" : "5.4 Feature Sets",
      "text" : "In addition to the claspre feature set presented in Section 4, we considered a set of ASP features introduced by Maratea et al. (2013) that is focussed on very efficiently computable syntactic features, such as number of variables. The published version of their feature generator supports only the ASPCore 1.0 (Calimeri et al. 2011) language of the 2011 ASP Competition.\nOur Comp-13-Set consists of instances of the 2013 ASP Competition in ASPCore 2.0, which introduced further language constructs. Therefore, we re-implemented this feature generator with the help of Maratea et al. to be compatible with ASPCore 2.0.7\nOne of the most established and investigated feature generators for SAT is provided as part of SATzilla (Xu et al. 2008). ASP instances can be translated to SAT with techniques by Janhunen (2006), using his tool lp2sat. We use a combination of lp2sat8 with the feature generator of SATzilla to generate a set of instance features for ASP instances; this is the first time, these features are studied in the context of ASP. Since the full set of SATzilla features is very expensive to compute and our SAT encodings can get quite large, we decided to only use the efficiently computable base features.\nTable 4 shows the runtime statistics for claspre with static features, claspre(s), claspre with static and dynamic features, claspre(s+d), with 4 restarts and 32 conflicts between the restarts, the (re-implemented) feature generator of ME-ASP and the combination of lp2sat and SATzilla’s feature generator on our full benchmark sets (training + test instances). claspre(s) is only slightly faster than claspre with additional dynamic features, since its search was limited to 128 conflicts. To solve typical ASP instances, searches well beyond 100000 conflicts are often required; nevertheless, claspre(s) solved 51 instances through pre-processing, and claspre(s+d) solved 123 instances on Comp-13-Set, 9 and 400 instances on Potassco-Set, respectively. The feature generation of ME-ASP was faster, but (unsurprisingly, considering the nature of these features) did not solve any instance. Because of the substantial overhead of generating translations from ASP to SAT, the combination of lp2sat and SATzilla’s feature generator turned out to be substantially slower than the other approaches and failed to compute the feature vectors of 1094 instances on Comp-13-Set and 377 instances on Potassco-Setwithin the given cutoff time."
    }, {
      "heading" : "5.5 Algorithm Selection Approaches",
      "text" : "As previously mentioned, claspfolio 2 was explicitly designed to easily integrate several stateof-the-art algorithm selection approaches. This not only permits us to optimize the performance of claspfolio 2, but also to compare the considered algorithm selection approaches within a\n7 The new feature generator is implement in Python, whereas the original generator was implemented in C++, which induced an overhead of a factor 2 in terms of running time on average on ASPCore 1.0 instances from the 2011 ASP Competition. 8 lp2sat was used as submitted at the 2013 ASP Competition.\ncontrolled environment. Although our re-implementations may not reproduce the original implementations in all details (something that would be difficult to achieve, considering that sources are not available for some published approaches), they provide the only freely available, opensource implementations of some of these systems and thus provide a basis for further analysis and improvements.9\nTable 5 gives an overview of the approaches available within claspfolio 2. These differ with respect to (i) the algorithm selection method, (ii) the feature normalization technique, (iii) the maximal number of pre-solvers used and (iv) the maximal running time allocated to the pre-solving schedule. In all cases, the pre-solving schedules were computed by aspeed, and hyperparameters of the machine learning techniques were set using grid search on training data."
    }, {
      "heading" : "5.6 Results",
      "text" : "We have assessed the performance of claspfolio 2 on all 112 combinations of our 4 feature sets, 4 portfolios and 7 algorithm selection approaches, using a cross validation on both test sets. To study the effect of each design choice, we collected statistics over the distribution of results by keeping one choice fixed and varying all remaining components; the results are shown in Table 6. The top part of the table shows results obtained for using each of the feature sets, in terms of average PAR10 performance, standard deviation in PAR10 performance and best PAR10 performance over all 28 combinations of portfolios and selection approaches. The subsequent parts of Table 6 show analogous results for different portfolios and selection approaches.\nOn average, the best feature set was claspre(s) (the static claspre features) on Comp-13-Set, followed by claspre(s+d) (the static + dynamic claspre features), the feature sets of ME-ASP and lp2sat. However, the best claspfolio2 configuration on Comp-13-Setused ME-ASP. The fact that claspre(s+d) gave worse results than claspre(s), although the former is superset of the latter, indicates that not all features were useful and that feature selection should be used to identify a subset of features with highest information content. On Potassco-Set, the best average performance and the best performance of any claspfolio 2 configuration was consistently obtained by using claspre(s+d). We believe that the additional dynamic features are necessary to distinguish between the larger number of different problem classes in Potassco-Set.\n9 As with Hydra and ISAC above, published and trainable, general-purpose implementations of 3S and ME-ASP are not available.\nThe results on the impact of the portfolio of algorithms used as a basis for algorithm selection confirm our assumption that the best potential performance, i.e., best VBS performance, is a good indicator of the actual performance achieved by a high-performance selection approach. On Comp-13-Set, ISAC-like-portfolio achieved the best performance, while on Potassco-Set, Hydra-like-portfolio yielded even better results. Furthermore, the portfolios obtained using the two automatic portfolio generation methods, ISAC and Hydra, yielded better results than the manually created ones, Expert-portfolio and SOTA-portfolio.\nAs shown in the lower part of Table 6, the SATzilla’11-like approach performed best on both benchmark sets, followed closely by 3S-like and ISAC-like. SATzilla’09-like and claspfolio-1.0-like showed similar, but weaker performance results, followed by the ME-ASP-like approach and the pure algorithm schedules of aspeed.\nOverall, the best combination both on the training and test sets of Comp-13-Set was the ME-ASP features, ISAC-like-portfolioand SATzilla’11-likeselection approach, and claspre(s+d) features, Hydra-like-portfolioand SATzilla’11-likeselection approach for Potassco-Set."
    }, {
      "heading" : "6 Conclusions and Future Work",
      "text" : "Our new, modular claspfolio 2 ASP solver architecture comprises a diverse set of portfoliobased algorithm selection techniques, including feature extractors, manually and automatically constructed base algorithm portfolios, algorithm selection mechanisms and solver-schedule-based pre-solving techniques. As seen from the high-level overview of empirical performance results in Table 7, on standard, diverse and heterogeneous sets of ASP benchmarks, claspfolio 2 is substantially more robust than the default configuration of clasp, the manual tuned configuration of clasp of the 2013 ASP Competition, and than all other assessed individual solvers; in fact, its performance in terms of PAR10-score lies only about 20% and 15% above that of the best known oracle on Potassco-Set and Comp-13-Set benchmark sets, respectively. The reimplementation of claspfolio 1.0 in claspfolio 2, which had a similar performance in preliminary experiments than the original implementation, achieves also about 14− 37% higher PAR10-score than claspfolio 2. While the best configuration of claspfolio 2 varies between these two benchmark sets, the performance differences are relatively minor: on Comp-13-Set, the best configuration of claspfolio 2 for Potassco-Set – which we also chose as the default configuration for claspfolio 2 – achieves a PAR10-score only about 2.1% lower than the best configuration for Comp-13-Set, and on Potassco-Set, its PAR10-score is about 9.6% higher. This configuration uses the claspre(s+d) feature set in combination with the Hydra-like-portfolio base algorithm portfolio construction approach and the SATzilla’11-like algorithm selection mechanism, but other feature sets, base algorithm portfolios and algorithm selection mechanisms also achieve very strong performance."
    }, {
      "heading" : "Acknowledgments",
      "text" : "T. Schaub and M. Lindauer were supported by the DFG projects under SCHA 550/8-3. H. Hoos was supported by an NSERC Discovery Grant."
    } ],
    "references" : [ {
      "title" : "WASP: A native asp solver based on constraint learning",
      "author" : [ "M. ALVIANO", "C. DODARO", "W. FABER", "N. LEONE", "F. RICCA" ],
      "venue" : "Proceedings of the Twelfth International Conference on Logic Programming and Nonmonotonic Reasoning (LPNMR’13), P. Cabalar and T. Son, Eds. Lecture Notes in Artificial Intelligence, vol. 8148. Springer-Verlag, 54–66.",
      "citeRegEx" : "ALVIANO et al\\.,? 2013",
      "shortCiteRegEx" : "ALVIANO et al\\.",
      "year" : 2013
    }, {
      "title" : "A gender-based genetic algorithm for the automatic configuration of algorithms",
      "author" : [ "C. ANSÓTEGUI", "M. SELLMANN", "K. TIERNEY" ],
      "venue" : "Proceedings of the Fifteenth International Conference on Principles and Practice of Constraint Programming (CP’09), I. Gent, Ed. Lecture Notes in Computer Science, vol. 5732. Springer-Verlag, 142–157.",
      "citeRegEx" : "ANSÓTEGUI et al\\.,? 2009",
      "shortCiteRegEx" : "ANSÓTEGUI et al\\.",
      "year" : 2009
    }, {
      "title" : "Knowledge Representation, Reasoning and Declarative Problem Solving",
      "author" : [ "C. BARAL" ],
      "venue" : "Cambridge University Press.",
      "citeRegEx" : "BARAL,? 2003",
      "shortCiteRegEx" : "BARAL",
      "year" : 2003
    }, {
      "title" : "Proceedings of the Thirteenth International Conference on Principles and Practice of Constraint Programming (CP’07)",
      "author" : [ "C. BESSIERE", "Ed." ],
      "venue" : "Lecture Notes in Computer Science, vol. 4741. SpringerVerlag.",
      "citeRegEx" : "BESSIERE and Ed.,? 2007",
      "shortCiteRegEx" : "BESSIERE and Ed.",
      "year" : 2007
    }, {
      "title" : "Pattern Recognition and Machine Learning (Information Science and Statistics), 1st ed",
      "author" : [ "C. BISHOP" ],
      "venue" : "2006. Corr. 2nd printing ed. Springer.",
      "citeRegEx" : "BISHOP,? 2007",
      "shortCiteRegEx" : "BISHOP",
      "year" : 2007
    }, {
      "title" : "Third ASP competition - file and language formats",
      "author" : [ "F. CALIMERI", "G. IANNI", "F. RICCA" ],
      "venue" : "Tech. rep., Università della Calabria.",
      "citeRegEx" : "CALIMERI et al\\.,? 2011",
      "shortCiteRegEx" : "CALIMERI et al\\.",
      "year" : 2011
    }, {
      "title" : "SNAPP: Solver-based nearest neighbor for algorithm portfolios",
      "author" : [ "M. COLLAUTTI", "Y. MALITSKY", "D. MEHTA", "B. O’SULLIVAN" ],
      "venue" : "In Proceedings of the Twenty-Fourth European Conference on Machine Learning (ECML’13),",
      "citeRegEx" : "COLLAUTTI et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "COLLAUTTI et al\\.",
      "year" : 2013
    }, {
      "title" : "Technical Communications of the Twenty-eighth International Conference on Logic Programming (ICLP’12)",
      "author" : [ "A. DOVIER", "V. SANTOS COSTA", "Eds." ],
      "venue" : "Vol. 17. Leibniz International Proceedings in Informatics (LIPIcs).",
      "citeRegEx" : "DOVIER et al\\.,? 2012",
      "shortCiteRegEx" : "DOVIER et al\\.",
      "year" : 2012
    }, {
      "title" : "Potassco: The Potsdam answer set solving collection",
      "author" : [ "M. GEBSER", "R. KAMINSKI", "B. KAUFMANN", "M. OSTROWSKI", "T. SCHAUB", "M. SCHNEIDER" ],
      "venue" : "AI Communications 24, 2, 107–124.",
      "citeRegEx" : "GEBSER et al\\.,? 2011",
      "shortCiteRegEx" : "GEBSER et al\\.",
      "year" : 2011
    }, {
      "title" : "A portfolio solver for answer set programming: Preliminary report",
      "author" : [ "M. GEBSER", "R. KAMINSKI", "B. KAUFMANN", "T. SCHAUB", "M. SCHNEIDER", "S. ZILLER" ],
      "venue" : "Proceedings of the Eleventh International Conference on Logic Programming and Nonmonotonic Reasoning (LPNMR’11), J. Delgrande and W. Faber, Eds. Lecture Notes in Artificial Intelligence, vol. 6645. Springer-Verlag, 352–357.",
      "citeRegEx" : "GEBSER et al\\.,? 2011",
      "shortCiteRegEx" : "GEBSER et al\\.",
      "year" : 2011
    }, {
      "title" : "Answer set programming based on propositional satisfiability",
      "author" : [ "E. GIUNCHIGLIA", "Y. LIERLER", "M. MARATEA" ],
      "venue" : "Journal of Automated Reasoning 36, 4, 345–377.",
      "citeRegEx" : "GIUNCHIGLIA et al\\.,? 2006",
      "shortCiteRegEx" : "GIUNCHIGLIA et al\\.",
      "year" : 2006
    }, {
      "title" : "Fast downward stone soup: A baseline for building planner portfolios",
      "author" : [ "M. HELMERT", "G. RÖGER", "E. KARPAS" ],
      "venue" : "ICAPS 2011 Workshop on Planning and Learning. 28–35.",
      "citeRegEx" : "HELMERT et al\\.,? 2011",
      "shortCiteRegEx" : "HELMERT et al\\.",
      "year" : 2011
    }, {
      "title" : "Programming by optimisation",
      "author" : [ "H. HOOS" ],
      "venue" : "Communications of the ACM 55, 70–80.",
      "citeRegEx" : "HOOS,? 2012",
      "shortCiteRegEx" : "HOOS",
      "year" : 2012
    }, {
      "title" : "aspeed: Solver scheduling via answer set programming",
      "author" : [ "H. HOOS", "R. KAMINSKI", "M. LINDAUER", "T. SCHAUB" ],
      "venue" : "Theory and Practice of Logic Programming First View, 1–26. Available at http://arxiv.org/abs/1401.1024 .",
      "citeRegEx" : "HOOS et al\\.,? 2014",
      "shortCiteRegEx" : "HOOS et al\\.",
      "year" : 2014
    }, {
      "title" : "Robust benchmark set selection for boolean constraint solvers",
      "author" : [ "H. HOOS", "B. KAUFMANN", "T. SCHAUB", "M. SCHNEIDER" ],
      "venue" : "See Pardalos and Nicosia (2013), 138–152.",
      "citeRegEx" : "HOOS et al\\.,? 2013",
      "shortCiteRegEx" : "HOOS et al\\.",
      "year" : 2013
    }, {
      "title" : "An economic approach to hard computational problems",
      "author" : [ "B. HUBERMAN", "R. LUKOSE", "T. HOGG" ],
      "venue" : "Science 275, 51–54.",
      "citeRegEx" : "HUBERMAN et al\\.,? 1997",
      "shortCiteRegEx" : "HUBERMAN et al\\.",
      "year" : 1997
    }, {
      "title" : "Sequential model-based optimization for general algorithm configuration",
      "author" : [ "F. HUTTER", "H. HOOS", "K. LEYTON-BROWN" ],
      "venue" : "Proceedings of the Fifth International Conference on Learning and Intelligent Optimization (LION’11). Lecture Notes in Computer Science, vol. 6683. Springer-Verlag, 507–523.",
      "citeRegEx" : "HUTTER et al\\.,? 2011",
      "shortCiteRegEx" : "HUTTER et al\\.",
      "year" : 2011
    }, {
      "title" : "Automatic algorithm configuration based on local search",
      "author" : [ "F. HUTTER", "H. HOOS", "T. STÜTZLE" ],
      "venue" : "Proceedings of the Twenty-second National Conference on Artificial Intelligence (AAAI’07). AAAI Press, 1152–1157.",
      "citeRegEx" : "HUTTER et al\\.,? 2007",
      "shortCiteRegEx" : "HUTTER et al\\.",
      "year" : 2007
    }, {
      "title" : "Algorithm runtime prediction: The state of the art",
      "author" : [ "F. HUTTER", "L. XU", "H.H. HOOS", "K. LEYTON-BROWN" ],
      "venue" : "Artificial Intelligence.",
      "citeRegEx" : "HUTTER et al\\.,? 2012",
      "shortCiteRegEx" : "HUTTER et al\\.",
      "year" : 2012
    }, {
      "title" : "Some (in)translatability results for normal logic programs and propositional theories",
      "author" : [ "T. JANHUNEN" ],
      "venue" : "Journal of Applied Non-Classical Logics 16, 1-2, 35–86.",
      "citeRegEx" : "JANHUNEN,? 2006",
      "shortCiteRegEx" : "JANHUNEN",
      "year" : 2006
    }, {
      "title" : "Algorithm selection and scheduling",
      "author" : [ "S. KADIOGLU", "Y. MALITSKY", "A. SABHARWAL", "H. SAMULOWITZ", "M. SELLMANN" ],
      "venue" : "Proceedings of the Seventeenth International Conference on Principles and Practice of Constraint Programming (CP’11), J. Lee, Ed. Lecture Notes in Computer Science, vol. 6876. Springer-Verlag, 454–469.",
      "citeRegEx" : "KADIOGLU et al\\.,? 2011",
      "shortCiteRegEx" : "KADIOGLU et al\\.",
      "year" : 2011
    }, {
      "title" : "ISAC – instance-specific algorithm configuration",
      "author" : [ "S. KADIOGLU", "Y. MALITSKY", "M. SELLMANN", "K. TIERNEY" ],
      "venue" : "Proceedings of the Nineteenth European Conference on Artificial Intelligence (ECAI’10), H. Coelho, R. Studer, and M. Wooldridge, Eds. IOS Press, 751–756.",
      "citeRegEx" : "KADIOGLU et al\\.,? 2010",
      "shortCiteRegEx" : "KADIOGLU et al\\.",
      "year" : 2010
    }, {
      "title" : "LLAMA: leveraging learning to automatically manage algorithms",
      "author" : [ "L. KOTTHOFF" ],
      "venue" : "Tech. rep., Cork Constraint Computation Centre. published at arXiv.",
      "citeRegEx" : "KOTTHOFF,? 2013",
      "shortCiteRegEx" : "KOTTHOFF",
      "year" : 2013
    }, {
      "title" : "An evaluation of machine learning in algorithm selection for search problems",
      "author" : [ "L. KOTTHOFF", "I.P. GENT", "I. MIGUEL" ],
      "venue" : "AI Communications 25, 3, 257–270.",
      "citeRegEx" : "KOTTHOFF et al\\.,? 2012",
      "shortCiteRegEx" : "KOTTHOFF et al\\.",
      "year" : 2012
    }, {
      "title" : "Answer set programming via mixed integer programming",
      "author" : [ "G. LIU", "T. JANHUNEN", "I. NIEMEL" ],
      "venue" : "Proceedings of the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning (KR’12), G. Brewka, T. Eiter, and S. McIlraith, Eds. AAAI Press, 32–42.",
      "citeRegEx" : "LIU et al\\.,? 2012",
      "shortCiteRegEx" : "LIU et al\\.",
      "year" : 2012
    }, {
      "title" : "Boosting sequential solver portfolios: Knowledge sharing and accuracy prediction",
      "author" : [ "Y. MALITSKY", "A. SABHARWAL", "H. SAMULOWITZ", "M. SELLMANN" ],
      "venue" : "See Pardalos and Nicosia (2013), 153– 167.",
      "citeRegEx" : "MALITSKY et al\\.,? 2013",
      "shortCiteRegEx" : "MALITSKY et al\\.",
      "year" : 2013
    }, {
      "title" : "Applying machine learning techniques to ASP solving",
      "author" : [ "M. MARATEA", "L. PULINA", "F. RICCA" ],
      "venue" : "See Dovier and Santos Costa (2012), 37–48.",
      "citeRegEx" : "MARATEA et al\\.,? 2012",
      "shortCiteRegEx" : "MARATEA et al\\.",
      "year" : 2012
    }, {
      "title" : "A multi-engine approach to answer-set programming",
      "author" : [ "M. MARATEA", "L. PULINA", "F. RICCA" ],
      "venue" : "Theory and Practice of Logic Programming First View, 1–28.",
      "citeRegEx" : "MARATEA et al\\.,? 2013",
      "shortCiteRegEx" : "MARATEA et al\\.",
      "year" : 2013
    }, {
      "title" : "Translating answer-set programs into bit-vector logic",
      "author" : [ "M. NGUYEN", "T. JANHUNEN", "I. NIEMELÄ" ],
      "venue" : "Proceedings of the Nineteenth International Conference on Applications of Declarative Programming and Knowledge Management (INAP’11) and the Twenty-fifth Workshop on Logic Programming (WLP’11), H. Tompits, S. Abreu, J. Oetsch, J. Pührer, D. Seipel, M. Umeda, and A. Wolf, Eds. Lecture Notes in Computer Science, vol. 7773. Springer-Verlag, 105–116.",
      "citeRegEx" : "NGUYEN et al\\.,? 2013",
      "shortCiteRegEx" : "NGUYEN et al\\.",
      "year" : 2013
    }, {
      "title" : "Using case-based reasoning in an algorithm portfolio for constraint solving",
      "author" : [ "E. O’MAHONY", "E. HEBRARD", "A. HOLLAND", "C. NUGENT", "B. O’SULLIVAN" ],
      "venue" : "Proceedings of the Nineteenth Irish Conference on Artificial Intelligence and Cognitive Science (AICS’08),",
      "citeRegEx" : "O.MAHONY et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "O.MAHONY et al\\.",
      "year" : 2008
    }, {
      "title" : "Proceedings of the Seventh International Conference on Learning and Intelligent Optimization (LION’13)",
      "author" : [ "P. PARDALOS", "G. NICOSIA", "Eds." ],
      "venue" : "Lecture Notes in Computer Science, vol. 7997. Springer-Verlag.",
      "citeRegEx" : "PARDALOS et al\\.,? 2013",
      "shortCiteRegEx" : "PARDALOS et al\\.",
      "year" : 2013
    }, {
      "title" : "A multi-engine solver for quantified boolean formulas",
      "author" : [ "L. PULINA", "A. TACCHELLA" ],
      "venue" : "See Bessiere (2007), 574–589.",
      "citeRegEx" : "PULINA and TACCHELLA,? 2007",
      "shortCiteRegEx" : "PULINA and TACCHELLA",
      "year" : 2007
    }, {
      "title" : "The algorithm selection problem",
      "author" : [ "J. RICE" ],
      "venue" : "Advances in Computers 15, 65–118.",
      "citeRegEx" : "RICE,? 1976",
      "shortCiteRegEx" : "RICE",
      "year" : 1976
    }, {
      "title" : "Learning portfolios of automatically tuned planners",
      "author" : [ "J. SEIPP", "M. BRAUN", "J. GARIMORT", "M. HELMERT" ],
      "venue" : "Proceedings of the Twenty-Second International Conference on Automated Planning and Scheduling (ICAPS’12), L. McCluskey, B. Williams, J. R. Silva, and B. Bonet, Eds. AAAI.",
      "citeRegEx" : "SEIPP et al\\.,? 2012",
      "shortCiteRegEx" : "SEIPP et al\\.",
      "year" : 2012
    }, {
      "title" : "Surviving solver sensitivity: An ASP practitioner’s guide",
      "author" : [ "B. SILVERTHORN", "Y. LIERLER", "M. SCHNEIDER" ],
      "venue" : "See Dovier and Santos Costa (2012), 164–175.",
      "citeRegEx" : "SILVERTHORN et al\\.,? 2012",
      "shortCiteRegEx" : "SILVERTHORN et al\\.",
      "year" : 2012
    }, {
      "title" : "Extending and implementing the stable model semantics",
      "author" : [ "P. SIMONS", "I. NIEMELÄ", "T. SOININEN" ],
      "venue" : "Artificial Intelligence 138, 1-2, 181–234.",
      "citeRegEx" : "SIMONS et al\\.,? 2002",
      "shortCiteRegEx" : "SIMONS et al\\.",
      "year" : 2002
    }, {
      "title" : "Hierarchical hardness models for SAT",
      "author" : [ "L. XU", "H. HOOS", "K. LEYTON-BROWN" ],
      "venue" : "See Bessiere (2007), 696–711.",
      "citeRegEx" : "XU et al\\.,? 2007",
      "shortCiteRegEx" : "XU et al\\.",
      "year" : 2007
    }, {
      "title" : "Hydra: Automatically configuring algorithms for portfolio-based selection",
      "author" : [ "L. XU", "H. HOOS", "K. LEYTON-BROWN" ],
      "venue" : "Proceedings of the Twenty-fourth National Conference on Artificial Intelligence (AAAI’10), M. Fox and D. Poole, Eds. AAAI Press, 210–216.",
      "citeRegEx" : "XU et al\\.,? 2010",
      "shortCiteRegEx" : "XU et al\\.",
      "year" : 2010
    }, {
      "title" : "SATzilla: Portfolio-based algorithm selection for SAT",
      "author" : [ "L. XU", "F. HUTTER", "H. HOOS", "K. LEYTON-BROWN" ],
      "venue" : "Journal of Artificial Intelligence Research 32, 565–606.",
      "citeRegEx" : "XU et al\\.,? 2008",
      "shortCiteRegEx" : "XU et al\\.",
      "year" : 2008
    }, {
      "title" : "SATzilla2009: An automatic algorithm portfolio for SAT",
      "author" : [ "L. XU", "F. HUTTER", "H. HOOS", "K. LEYTON-BROWN" ],
      "venue" : "SAT 2009 competitive events booklet: preliminary version, D. Le Berre, O. Roussel, L. Simon, V. Manquinho, J. Argelich, C. Li, F. Manyà, and J. Planes, Eds. 53–55. Available at http://www.cril.univ-artois.fr/SAT09/solvers/booklet.pdf .",
      "citeRegEx" : "XU et al\\.,? 2009",
      "shortCiteRegEx" : "XU et al\\.",
      "year" : 2009
    }, {
      "title" : "Hydra-MIP: Automated algorithm configuration and selection for mixed integer programming",
      "author" : [ "L. XU", "F. HUTTER", "H. HOOS", "K. LEYTON-BROWN" ],
      "venue" : "RCRA workshop on Experimental Evaluation of Algorithms for Solving Problems with Combinatorial Explosion at the International Joint Conference on Artificial Intelligence (IJCAI’11).",
      "citeRegEx" : "XU et al\\.,? 2011",
      "shortCiteRegEx" : "XU et al\\.",
      "year" : 2011
    }, {
      "title" : "Evaluating component solver contributions to portfolio-based algorithm selectors",
      "author" : [ "L. XU", "F. HUTTER", "H. HOOS", "K. LEYTON-BROWN" ],
      "venue" : "Proceedings of the Fifteenth International Conference on Theory and Applications of Satisfiability Testing (SAT’12), A. Cimatti and R. Sebastiani, Eds. Lecture Notes in Computer Science, vol. 7317. Springer-Verlag, 228–241.",
      "citeRegEx" : "XU et al\\.,? 2012",
      "shortCiteRegEx" : "XU et al\\.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "Answer Set Programming (ASP; (Baral 2003)) has become a popular approach to declarative problem solving.",
      "startOffset" : 29,
      "endOffset" : 41
    }, {
      "referenceID" : 8,
      "context" : "(2014), and in particular the two portfolio-based systems claspfolio (Gebser et al. 2011) and ME-ASP (Maratea et al.",
      "startOffset" : 69,
      "endOffset" : 89
    }, {
      "referenceID" : 27,
      "context" : "2011) and ME-ASP (Maratea et al. 2013).",
      "startOffset" : 17,
      "endOffset" : 38
    }, {
      "referenceID" : 2,
      "context" : "Answer Set Programming (ASP; (Baral 2003)) has become a popular approach to declarative problem solving. This is mainly due its combination of a rich and simple modeling language with high performance solving technology. ASP decouples problem specifications from solving algorithms; however, modern ASP solvers are known to be sensitive to search configurations – a phenomenon that is common to advanced Boolean constraint processing techniques. To avoid the necessity of manual solver configuration, a substantial amount of research was thus devoted to automated algorithm configuration and selection approaches, as we detail in Section 2; in ASP, we find works by Gebser et al. (2011), Maratea et al.",
      "startOffset" : 30,
      "endOffset" : 687
    }, {
      "referenceID" : 2,
      "context" : "Answer Set Programming (ASP; (Baral 2003)) has become a popular approach to declarative problem solving. This is mainly due its combination of a rich and simple modeling language with high performance solving technology. ASP decouples problem specifications from solving algorithms; however, modern ASP solvers are known to be sensitive to search configurations – a phenomenon that is common to advanced Boolean constraint processing techniques. To avoid the necessity of manual solver configuration, a substantial amount of research was thus devoted to automated algorithm configuration and selection approaches, as we detail in Section 2; in ASP, we find works by Gebser et al. (2011), Maratea et al. (2012), Silverthorn et al.",
      "startOffset" : 30,
      "endOffset" : 710
    }, {
      "referenceID" : 2,
      "context" : "Answer Set Programming (ASP; (Baral 2003)) has become a popular approach to declarative problem solving. This is mainly due its combination of a rich and simple modeling language with high performance solving technology. ASP decouples problem specifications from solving algorithms; however, modern ASP solvers are known to be sensitive to search configurations – a phenomenon that is common to advanced Boolean constraint processing techniques. To avoid the necessity of manual solver configuration, a substantial amount of research was thus devoted to automated algorithm configuration and selection approaches, as we detail in Section 2; in ASP, we find works by Gebser et al. (2011), Maratea et al. (2012), Silverthorn et al. (2012), Maratea et al.",
      "startOffset" : 30,
      "endOffset" : 737
    }, {
      "referenceID" : 2,
      "context" : "Answer Set Programming (ASP; (Baral 2003)) has become a popular approach to declarative problem solving. This is mainly due its combination of a rich and simple modeling language with high performance solving technology. ASP decouples problem specifications from solving algorithms; however, modern ASP solvers are known to be sensitive to search configurations – a phenomenon that is common to advanced Boolean constraint processing techniques. To avoid the necessity of manual solver configuration, a substantial amount of research was thus devoted to automated algorithm configuration and selection approaches, as we detail in Section 2; in ASP, we find works by Gebser et al. (2011), Maratea et al. (2012), Silverthorn et al. (2012), Maratea et al. (2013) and Hoos et al.",
      "startOffset" : 30,
      "endOffset" : 760
    }, {
      "referenceID" : 2,
      "context" : "Answer Set Programming (ASP; (Baral 2003)) has become a popular approach to declarative problem solving. This is mainly due its combination of a rich and simple modeling language with high performance solving technology. ASP decouples problem specifications from solving algorithms; however, modern ASP solvers are known to be sensitive to search configurations – a phenomenon that is common to advanced Boolean constraint processing techniques. To avoid the necessity of manual solver configuration, a substantial amount of research was thus devoted to automated algorithm configuration and selection approaches, as we detail in Section 2; in ASP, we find works by Gebser et al. (2011), Maratea et al. (2012), Silverthorn et al. (2012), Maratea et al. (2013) and Hoos et al. (2014), and in particular the two portfolio-based systems claspfolio (Gebser et al.",
      "startOffset" : 30,
      "endOffset" : 783
    }, {
      "referenceID" : 2,
      "context" : "Answer Set Programming (ASP; (Baral 2003)) has become a popular approach to declarative problem solving. This is mainly due its combination of a rich and simple modeling language with high performance solving technology. ASP decouples problem specifications from solving algorithms; however, modern ASP solvers are known to be sensitive to search configurations – a phenomenon that is common to advanced Boolean constraint processing techniques. To avoid the necessity of manual solver configuration, a substantial amount of research was thus devoted to automated algorithm configuration and selection approaches, as we detail in Section 2; in ASP, we find works by Gebser et al. (2011), Maratea et al. (2012), Silverthorn et al. (2012), Maratea et al. (2013) and Hoos et al. (2014), and in particular the two portfolio-based systems claspfolio (Gebser et al. 2011) and ME-ASP (Maratea et al. 2013). The idea of such portfoliobased systems is to train classifiers on features of benchmark instances in order to predict the putatively best solver from a given solver portfolio. The portfolio of solvers used in this approach may consist of distinct configurations of the same solver or contain different solvers. In what follows, we describe the new portfolio-based ASP system claspfolio, whose earlier version 1.0 won first, second, and third places at various ASP competitions. Version 0.8 of claspfolio was briefly described in a short paper by Gebser et al. (2011) and is conceptually identical to the first stable release of version 1.",
      "startOffset" : 30,
      "endOffset" : 1468
    }, {
      "referenceID" : 32,
      "context" : "Our work continues a long line of research that can be traced back to John Rice’s seminal work on algorithm selection (Rice 1976) on one side, and to work by Huberman et al.",
      "startOffset" : 118,
      "endOffset" : 129
    }, {
      "referenceID" : 38,
      "context" : "SATzilla (Xu et al. 2008; 2007; 2009; 2011; 2012) predicted algorithm performance by means of ridge regression until 2009 and nowadays uses a pairwise voting scheme based on random forests; ISAC (Kadioglu et al.",
      "startOffset" : 9,
      "endOffset" : 49
    }, {
      "referenceID" : 21,
      "context" : "2008; 2007; 2009; 2011; 2012) predicted algorithm performance by means of ridge regression until 2009 and nowadays uses a pairwise voting scheme based on random forests; ISAC (Kadioglu et al. 2010) clusters instances in the instance feature space and uses a nearest neighbour approach on cluster centers for algorithm selection; 3S (Kadioglu et al.",
      "startOffset" : 175,
      "endOffset" : 197
    }, {
      "referenceID" : 20,
      "context" : "2010) clusters instances in the instance feature space and uses a nearest neighbour approach on cluster centers for algorithm selection; 3S (Kadioglu et al. 2011; Malitsky et al. 2013) uses k-NN in the feature space and introduces pre-solving schedules computed by Integer Linear Programming and cost-sensitive clustering; SNAPP (Collautti et al.",
      "startOffset" : 140,
      "endOffset" : 184
    }, {
      "referenceID" : 25,
      "context" : "2010) clusters instances in the instance feature space and uses a nearest neighbour approach on cluster centers for algorithm selection; 3S (Kadioglu et al. 2011; Malitsky et al. 2013) uses k-NN in the feature space and introduces pre-solving schedules computed by Integer Linear Programming and cost-sensitive clustering; SNAPP (Collautti et al.",
      "startOffset" : 140,
      "endOffset" : 184
    }, {
      "referenceID" : 31,
      "context" : "Similarly, AQME (Pulina and Tacchella 2007) is a multi-engine selector for QSAT.",
      "startOffset" : 16,
      "endOffset" : 43
    }, {
      "referenceID" : 33,
      "context" : "Stone Soup (Seipp et al. 2012; Helmert et al. 2011) uses greedy hill climbing to find algorithm schedules for planning problems.",
      "startOffset" : 11,
      "endOffset" : 51
    }, {
      "referenceID" : 11,
      "context" : "Stone Soup (Seipp et al. 2012; Helmert et al. 2011) uses greedy hill climbing to find algorithm schedules for planning problems.",
      "startOffset" : 11,
      "endOffset" : 51
    }, {
      "referenceID" : 13,
      "context" : "aspeed (Hoos et al. 2014) also computes algorithm schedules, but takes advantage of the modeling and solving capabilities of ASP to find timeout-minimal schedules.",
      "startOffset" : 7,
      "endOffset" : 25
    }, {
      "referenceID" : 9,
      "context" : "Our work continues a long line of research that can be traced back to John Rice’s seminal work on algorithm selection (Rice 1976) on one side, and to work by Huberman et al. (1997) on parallel algorithm portfolios on the other side.",
      "startOffset" : 158,
      "endOffset" : 181
    }, {
      "referenceID" : 8,
      "context" : "Following the original claspfolio of Gebser et al. (2011) approach, Maratea et al.",
      "startOffset" : 37,
      "endOffset" : 58
    }, {
      "referenceID" : 8,
      "context" : "Following the original claspfolio of Gebser et al. (2011) approach, Maratea et al. (2012) presented ME-ASP, a multi-engine algorithm selector for ASP with an instance feature generator for syntactic features.",
      "startOffset" : 37,
      "endOffset" : 90
    }, {
      "referenceID" : 8,
      "context" : "Following the original claspfolio of Gebser et al. (2011) approach, Maratea et al. (2012) presented ME-ASP, a multi-engine algorithm selector for ASP with an instance feature generator for syntactic features. Similarly, AQME (Pulina and Tacchella 2007) is a multi-engine selector for QSAT. CP-Hydra (O’Mahony et al. 2008) selects a set of CSP solvers based on case-based reasoning and schedules them heuristically. Stone Soup (Seipp et al. 2012; Helmert et al. 2011) uses greedy hill climbing to find algorithm schedules for planning problems. aspeed (Hoos et al. 2014) also computes algorithm schedules, but takes advantage of the modeling and solving capabilities of ASP to find timeout-minimal schedules. Related to our work on a more general level, Hutter et al. (2012) gave an overview over run-",
      "startOffset" : 37,
      "endOffset" : 774
    }, {
      "referenceID" : 22,
      "context" : "A comparison of different machine learning algorithms for algorithm selection was presented by Kotthoff et al. (2012). Based on these results, Kotthoff (2013) introduced LLAMA, Leveraging Learning to Automatically Manage Algorithms, a flexible framework that provides functionality to train and assess the performance of different algorithm selection techniques.",
      "startOffset" : 95,
      "endOffset" : 118
    }, {
      "referenceID" : 22,
      "context" : "A comparison of different machine learning algorithms for algorithm selection was presented by Kotthoff et al. (2012). Based on these results, Kotthoff (2013) introduced LLAMA, Leveraging Learning to Automatically Manage Algorithms, a flexible framework that provides functionality to train and assess the performance of different algorithm selection techniques.",
      "startOffset" : 95,
      "endOffset" : 159
    }, {
      "referenceID" : 12,
      "context" : "Furthermore, the new design of claspfolio 2 follows the idea of Level 4 of programming by optimisation (Hoos 2012): “The software-development process is centered on the idea of providing design choices and alternatives in all parts of a project that might benefit from them; design choices that cannot be justified convincingly are not made prematurely.",
      "startOffset" : 103,
      "endOffset" : 114
    }, {
      "referenceID" : 13,
      "context" : "aspeed (Hoos et al. 2014), into an algorithm selection framework to compute a static pre-solving schedule.",
      "startOffset" : 7,
      "endOffset" : 25
    }, {
      "referenceID" : 13,
      "context" : "The Scheduling component of claspfolio 2 computes a timeout-minimal pre-solving schedule using aspeed (Hoos et al. 2014), where each algorithm gets a (potentially zero) time slice of the overall runtime budget available for solving a given problem instance.",
      "startOffset" : 102,
      "endOffset" : 120
    }, {
      "referenceID" : 38,
      "context" : "Like SATzilla (Xu et al. 2008), claspfolio 2 allows to ignore instances solved by the presolving schedule (Arrow II) when learning the scoring model, such that the resulting model is focused on the harder instances not solved by the pre-solvers that are actually subject to algorithm selecting during the solving phase.",
      "startOffset" : 14,
      "endOffset" : 30
    }, {
      "referenceID" : 38,
      "context" : "In (Xu et al. 2008), this is called solver subset selection and in (Maratea et al.",
      "startOffset" : 3,
      "endOffset" : 19
    }, {
      "referenceID" : 26,
      "context" : "2008), this is called solver subset selection and in (Maratea et al. 2012), solver selection.",
      "startOffset" : 53,
      "endOffset" : 74
    }, {
      "referenceID" : 8,
      "context" : "claspre is a light-weight version of clasp (Gebser et al. 2011) that extracts instance features of ground ASP instances in smodels format (Syrjänen ), using clasp’s internal statistics.",
      "startOffset" : 43,
      "endOffset" : 63
    }, {
      "referenceID" : 4,
      "context" : ", (Bishop 2007)).",
      "startOffset" : 2,
      "endOffset" : 15
    }, {
      "referenceID" : 12,
      "context" : "As an even more heterogeneous instance set, we used the ASP Potassco-Set introduced by Hoos et al. (2013); it consists of 2589 instances from 105 problem classes and includes instances",
      "startOffset" : 87,
      "endOffset" : 106
    }, {
      "referenceID" : 26,
      "context" : "3) configurations designed by Benjamin Kaufmann (configurations: frumpy (default), jumpy, handy and crafty) • SOTA-portfolio (Maratea et al. 2012): non-portfolio solvers participating in the 2013 ASP Competition6 and in addition, the well-established solvers cmodels and smodels; in detail: clasp (Gebser et al.",
      "startOffset" : 125,
      "endOffset" : 146
    }, {
      "referenceID" : 8,
      "context" : "2012): non-portfolio solvers participating in the 2013 ASP Competition6 and in addition, the well-established solvers cmodels and smodels; in detail: clasp (Gebser et al. 2011), cmodels (Giunchiglia et al.",
      "startOffset" : 156,
      "endOffset" : 176
    }, {
      "referenceID" : 10,
      "context" : "2011), cmodels (Giunchiglia et al. 2006), lp2bv (Nguyen et al.",
      "startOffset" : 15,
      "endOffset" : 40
    }, {
      "referenceID" : 28,
      "context" : "2006), lp2bv (Nguyen et al. 2013), lp2mip (Liu et al.",
      "startOffset" : 13,
      "endOffset" : 33
    }, {
      "referenceID" : 24,
      "context" : "2013), lp2mip (Liu et al. 2012), lp2sat (Janhunen 2006), smodels (Simons et al.",
      "startOffset" : 14,
      "endOffset" : 31
    }, {
      "referenceID" : 19,
      "context" : "2012), lp2sat (Janhunen 2006), smodels (Simons et al.",
      "startOffset" : 14,
      "endOffset" : 29
    }, {
      "referenceID" : 35,
      "context" : "2012), lp2sat (Janhunen 2006), smodels (Simons et al. 2002), and wasp (Alviano et al.",
      "startOffset" : 39,
      "endOffset" : 59
    }, {
      "referenceID" : 0,
      "context" : "2002), and wasp (Alviano et al. 2013) • Hydra-like-portfolio (Xu et al.",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 37,
      "context" : "2013) • Hydra-like-portfolio (Xu et al. 2010; Xu et al. 2011) of clasp (2.",
      "startOffset" : 29,
      "endOffset" : 61
    }, {
      "referenceID" : 40,
      "context" : "2013) • Hydra-like-portfolio (Xu et al. 2010; Xu et al. 2011) of clasp (2.",
      "startOffset" : 29,
      "endOffset" : 61
    }, {
      "referenceID" : 21,
      "context" : "3) configurations • ISAC-like-portfolio (Kadioglu et al. 2010) of clasp (2.",
      "startOffset" : 40,
      "endOffset" : 62
    }, {
      "referenceID" : 17,
      "context" : ", ParamILS (Hutter et al. 2007), GGA (Ansótegui et al.",
      "startOffset" : 11,
      "endOffset" : 31
    }, {
      "referenceID" : 16,
      "context" : "2009) or SMAC (Hutter et al. 2011).",
      "startOffset" : 14,
      "endOffset" : 34
    }, {
      "referenceID" : 16,
      "context" : "The configuration process performed by SMAC (2.06.01; Hutter et al. 2011) on each cluster and in each Hydra iteration, respectively, was allocated 120 000 CPU seconds, i.",
      "startOffset" : 44,
      "endOffset" : 73
    }, {
      "referenceID" : 12,
      "context" : "by using cross-validation to optimize the scoring function of the k-means procedure (following Hoos et al. (2013)).",
      "startOffset" : 95,
      "endOffset" : 114
    }, {
      "referenceID" : 12,
      "context" : "by using cross-validation to optimize the scoring function of the k-means procedure (following Hoos et al. (2013)). Using this approach, ISAC-like-portfolio found 15 clusters for Comp-13-Set and 11 clusters for Potassco-Set, inducing 15 and 11 configuration tasks, respectively. To obtain a fair comparison, we allocated the same time budget to Hydra-like-portfolio and allowed it to perform 15 and 11 iterations, respectively (each consisting of one configuration task). The configuration process performed by SMAC (2.06.01; Hutter et al. 2011) on each cluster and in each Hydra iteration, respectively, was allocated 120 000 CPU seconds, i.e., 200 times the target algorithm cutoff time, and 10 independent repetitions, from which the result with the best PAR10 score on the given training set was selected. SMAC optimized PAR10. Table 3 shows the performance of the virtual best solvers (i.e., the performance of a perfect algorithm selector) for the different considered portfolios. Interestingly, the results differ qualitatively between two benchmark sets. While SOTA-portfolio performs better than Expert-portfolio on Comp-13-Set, Expert-portfolio is better on Potassco-Set. Furthermore, while for both sets, the automatic generation methods found better performing portfolios than the the manual selected methods, on the Comp-13-Set, ISAC-like-portfolio produced a better results than Hydra-like-portfolio, and the opposite holds for Potassco-Set. Furthermore, unlike conjectured by Maratea et al. (2012), a set of configurations of the same, highly parameterized solver (Expert-portfolio,ISAC-like-portfolioand Hydra-like-portfolio) generally did not yield worse performance than a mixed portfolio, such as SOTA-portfolio.",
      "startOffset" : 95,
      "endOffset" : 1513
    }, {
      "referenceID" : 12,
      "context" : "by using cross-validation to optimize the scoring function of the k-means procedure (following Hoos et al. (2013)). Using this approach, ISAC-like-portfolio found 15 clusters for Comp-13-Set and 11 clusters for Potassco-Set, inducing 15 and 11 configuration tasks, respectively. To obtain a fair comparison, we allocated the same time budget to Hydra-like-portfolio and allowed it to perform 15 and 11 iterations, respectively (each consisting of one configuration task). The configuration process performed by SMAC (2.06.01; Hutter et al. 2011) on each cluster and in each Hydra iteration, respectively, was allocated 120 000 CPU seconds, i.e., 200 times the target algorithm cutoff time, and 10 independent repetitions, from which the result with the best PAR10 score on the given training set was selected. SMAC optimized PAR10. Table 3 shows the performance of the virtual best solvers (i.e., the performance of a perfect algorithm selector) for the different considered portfolios. Interestingly, the results differ qualitatively between two benchmark sets. While SOTA-portfolio performs better than Expert-portfolio on Comp-13-Set, Expert-portfolio is better on Potassco-Set. Furthermore, while for both sets, the automatic generation methods found better performing portfolios than the the manual selected methods, on the Comp-13-Set, ISAC-like-portfolio produced a better results than Hydra-like-portfolio, and the opposite holds for Potassco-Set. Furthermore, unlike conjectured by Maratea et al. (2012), a set of configurations of the same, highly parameterized solver (Expert-portfolio,ISAC-like-portfolioand Hydra-like-portfolio) generally did not yield worse performance than a mixed portfolio, such as SOTA-portfolio. While we gave Hydra the same time budget as ISAC to find portfolios, the components added by Hydra-like-portfolio in its final three iterations decreased the number of timeouts only by one on our training and test sets. Following Xu et al. (2010), Hydra would be terminated when the performance does not improve on the training set after an iteration.",
      "startOffset" : 95,
      "endOffset" : 1979
    }, {
      "referenceID" : 5,
      "context" : "0 (Calimeri et al. 2011) language of the 2011 ASP Competition.",
      "startOffset" : 2,
      "endOffset" : 24
    }, {
      "referenceID" : 25,
      "context" : "In addition to the claspre feature set presented in Section 4, we considered a set of ASP features introduced by Maratea et al. (2013) that is focussed on very efficiently computable syntactic features, such as number of variables.",
      "startOffset" : 113,
      "endOffset" : 135
    }, {
      "referenceID" : 38,
      "context" : "7 One of the most established and investigated feature generators for SAT is provided as part of SATzilla (Xu et al. 2008).",
      "startOffset" : 106,
      "endOffset" : 122
    }, {
      "referenceID" : 19,
      "context" : "ASP instances can be translated to SAT with techniques by Janhunen (2006), using his tool lp2sat.",
      "startOffset" : 58,
      "endOffset" : 74
    } ],
    "year" : 2014,
    "abstractText" : "Building on the award-winning, portfolio-based ASP solver claspfolio, we present claspfolio 2, a modular and open solver architecture that integrates several different portfolio-based algorithm selection approaches and techniques. The claspfolio 2 solver framework supports various feature generators, solver selection approaches, solver portfolios, as well as solver-schedule-based pre-solving techniques. The default configuration of claspfolio 2 relies on a light-weight version of the ASP solver clasp to generate static and dynamic instance features. The flexible open design of claspfolio 2 is a distinguishing factor even beyond ASP. As such, it provides a unique framework for comparing and combining existing portfolio-based algorithm selection approaches and techniques in a single, unified framework. Taking advantage of this, we conducted an extensive experimental study to assess the impact of different feature sets, selection approaches and base solver portfolios. In addition to gaining substantial insights into the utility of the various approaches and techniques, we identified a default configuration of claspfolio 2 that achieves substantial performance gains not only over clasp’s default configuration and the earlier version of claspfolio, but also over manually tuned configurations of clasp.",
    "creator" : "LaTeX with hyperref package"
  }
}