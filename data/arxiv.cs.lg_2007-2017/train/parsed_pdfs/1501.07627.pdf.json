{
  "name" : "1501.07627.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Representing Objects, Relations, and Sequences",
    "authors" : [ "Stephen I. Gallant", "Wendy Okaywe", "Pitney Bowes" ],
    "emails" : [ "sgallant@mmres.com,", "wokaywe@mmres.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Gallant and Okaywe: Representing Objects, Relations and Sequences 1\nVector Symbolic Architectures (VSAs) are high-dimensional vector representations of objects (eg., words, image parts), relations (eg., sentence structures), and sequences for use with machine learning algorithms. They consist of a vector addition operator for representing a collection of unordered objects, a Binding operator for associating groups of objects, and a methodology for encoding complex structures.\nWe first develop Constraints that machine learning imposes upon VSAs: for example, similar structures must be represented by similar vectors. The constraints suggest that current VSAs should represent phrases (“The smart Brazilian girl”) by binding sums of terms, in addition to simply binding the terms directly.\nWe show that matrix multiplication can be used as the binding operator for a VSA, and that matrix elements can be chosen at random. A consequence for living systems is that binding is mathematically possible without the need to specify, in advance, precise neuron-to-neuron connection properties for large numbers of synapses.\nA VSA that incorporates these ideas, MBAT (Matrix Binding of Additive Terms), is described that satisfies all Constraints.\nWith respect to machine learning, for some types of problems appropriate VSA representations permit us to prove learnability, rather than relying on simulations. We also propose dividing machine (and neural) learning and representation into three Stages, with differing roles for learning in each stage.\nFor neural modeling, we give “representational reasons” for nervous systems to have many recurrent connections, as well as for the importance of phrases in language processing.\nSizing simulations and analyses suggest that VSAs in general, and MBAT in particular, are ready for real-world applications.\nGallant and Okaywe: Representing Objects, Relations and Sequences 2"
    }, {
      "heading" : "1. Introduction",
      "text" : "Representation is an important topic in its own right.\nPerhaps the most successful representation ever invented (other than writing itself!) is the decimal representations of integers, a great advance over counting by simple hash marks. Decimal representations illustrate that a good representation can help us calculate more quickly, by orders of magnitude, and thereby enable computations that would otherwise be impossibly difficult. This is precisely our goal here. We want a representation of objects (for example, words), multiple relations of those objects (eg., assorted syntactic and semantic information), and sequences (eg., sentences), that is hospitable to machine learning. For another example in the computer vision domain, we are interested in representing image parts, relations among those parts, and sequences of (sub) image presentations (eg., from eye saccades).\nWe seek a representation that permits us to use standard machine learning techniques (eg., neural networks, perceptron learning, regression) to simultaneously learn mappings of objects, relations, and sequences. Moreover, we want to use standard algorithms “out of the box” on vector inputs, without the need for constructing a separate learning architecture for each task.\nThis would open the possibility of “higher order holistic modeling”, where the predicted output encodes objects simultaneously with their structure, and where the structure can be more complex than selection from a small set of options. For example, we want to be able to predict full parse trees in one shot, rather than word-for-word part-of-speech tags. Ultimately we would like to predict a translated or summarized sentence, or a transformed image representation.\nThese are longer term goals; a more immediate motivation for developing such representations is to facilitate the use of machine learning when starting from the outputs of Structured Classification approaches. For example, Collobert et al. [2011] produce a system that outputs structure information (part of speech, chunks, semantic roles) for each word in a sentence. We want to be able to cleanly incorporate these outputs into a fixed-length vector representing the entire sentence, for use with follow-on machine learning.\nTo address these goals, since the 1990s a number of investigators have worked on incorporating structure into high-dimensional, distributed vector representations. (A distributed vector represents objects or other information by patterns over the entire vector.) Following Levy & Gayler [2008], we’ll refer to these architectures as Vector Symbolic Architectures (VSAs).\nThe desire to use machine learning techniques places a number of Constraints on Representation. Inputs and outputs to standard machine learning algorithms are most conveniently expressed as fixed-length vectors, ie., vectors having a pre-specified number of components. Thus we cannot directly apply neural networks to sentences, because sentences have no fixed and bounded length (and also because they possess important structure that is not immediate from the string of letters in a sentence).\nGallant and Okaywe: Representing Objects, Relations and Sequences 3\nHere we will focus on developing a VSA that simultaneously represents multiple objects, multiple versions of relations among those objects, and sequences of such objects/relations using a single fixed-length vector, in a way that satisfies the representational constraints. We name the VSA we develop Matrix Binding of Additive Terms or MBAT.\nVector Symbolic Architectures\nTo help with basic intuition for VSAs, consider Table 1, where five terms (“smart”, “girl”, “saw”, “grey”, “elephant”) are shown with their corresponding vectors (V smart , Vgirl, etc).\nNotationally, we represent all matrices by M, and vectors by other capital letters, such as V,W. We will also follow the standard convention of representing the vector for a term (V smart ) by just the term (smart) where the context is clear.\nof two vectors, and the process for recognizing individual constituents from a sum using the dot product.\nTable 1 suggests that we have a way of recognizing individual constituents of a vector sum using dot products (vector inner products). This will be formalized below in Section 3.\nVector Symbolic Architectures trace their origins to Smolensky’s [1990] tensor product models, but avoid the exponential growth in vector size of those models. VSAs include Kanerva’s Binary Spatter Codes (BSC) [1994, 1997], Plate’s Holographic Reduced Representations (HRR) [1992, 2003], Rachkovskij and Kussul’s Context Dependent Thinning (CDT) [2001], and Gayler’s Multiply-Add-Permute coding (MAP) [1998].\nVector Symbolic Architectures can be characterized along five defining characteristics:\nGallant and Okaywe: Representing Objects, Relations and Sequences 4\n Components of vectors are either binary (BSC), sparse binary (CDT), “bi-polar” (+1/-1) (MAP objects), continuous (HRR and MAP sums), or complex (HRR).\n Addition of vectors (also referred to as “bundling”) represents collections of (simple or complex) objects, but without any structure among the summed terms. If objects\nrepresent words, their addition gives an unordered “bag of words.” Operators used for addition include normal vector addition as in Table 1 (HRR, MAP), and addition followed by conversion to binary components according to thresholds (BSC).\n Binding of vectors is used to group objects, and can also be used for ordering them. Binding operators include Exclusive-OR or parity (BSC) and component-wise\nmultiplication (MAP).\nA particularly important binding method is circular convolution (HRR). Letting D be vector dimensionality, the circular convolution of two vectors, V = X * Y, is defined by\nVj = ∑(k = 0, … D-1) Xk Yj-k\nwhere the subscript calculation is taken mod D. In other words, reverse the numbering of Y’s indices, and now each component of the result Vj is just the dot product of X and (reverse numbered) Y, where Y is first rotated j positions prior to taking the dot product.\nBinding is commutative with respect to its two operands, and VSAs typically include an inverse operation for recovering one operand if the other is known. Inverses can be mathematically exact inverses (BSC, MAP) or have mean-0 noise added to the result (HRR), in which case a “cleanup step” is required to find the unknown element. Cleanup consists of finding the closest resulting vector using dot products with all vectors, or making use of Auto-associative memories [Kohonen 1977, Anderson et al. 1977].\nFor CDT binding, sparse binary vectors (representing objects or sub-structures) are first OR’ed together forming vector V. Then V is AND’ed with the union of a fixed number of permutations of V to control the expected number of 1s in the final vector. A separate addition operator is not needed for CDT.\n Quoting applied to binding produces unique binding operators in order to differentiate among groups joined by binding. This typically involves a random permutation of vector\nelements to represent, for example, two different subject phrases in a single sentence. (“The smart girl and the grey elephant went for a walk.”)\n Complex Structure Methodology represents complex relations among objects, such as nested sub-clauses in a sentence. For VSAs, this consists of binding (and quoting) to get\nsub-objects bound together, and addition to represent unordered collections of bound subobjects. For example, let us suppose each of the roles actor, verb, and object have their own vectors, as well as objects Mary, loves, and pizza. Then using * to denote binding in VSAs, we might represent “Mary loves pizza” by the vector\n(actor * Mary) + (verb * loves) + (object * pizza).\nThis permits extraction of, say, the actor (Mary) by binding the final sum with the inverse of actor, and following with a “cleanup” step.\nGallant and Okaywe: Representing Objects, Relations and Sequences 5\nFor MBAT, we will be presenting a different, unary binding operator, and a different complex structure methodology that emphasizes additive “phrases”.\nOrganization\nThis paper is organized as follows. We first propose in Section 2 a collection of necessary Constraints for representing structured objects for use by standard machine learning algorithms. We then (Section 3) describe the MBAT architecture that encodes objects, structures and sequences into a single distributed vector. In Section 4, we examine the role (and advisability) of machine learning during three information processing stages: Preprocessing, Representation Generation, and Output Computation. We also see how for many tasks, with a suitable representation we can prove learnability for standard machine learning algorithms, rather than rely upon simulations.\nSection 5 looks at capacity, namely the required dimensionality for vectors. Both analytic estimates and simulation results are presented. Section 6 re-examines the Constraints with respect to the MBAT architecture. Section 7 reviews prior research. Section 8 (Discussion) revisits VSAs with respect to Complex Structure Methodology, and suggests applications for MBAT in computational linguistics, computer vision and modeling neural information processing. The Appendix develops estimates for required dimensionality of vectors."
    }, {
      "heading" : "2. Requirements for a Good Representation of Objects,",
      "text" : "Relations and Sequences\nWe want a representation of structured objects, such as sentences or images, that is directly suitable for machine learning, without the need for constructing special-case learning algorithms. There are several requirements that must be met:\nConstraint 1: Fixed Length Vector. Most standard machine learning approaches take inputs that are vectors of some pre-specified length. Thus if we want a way to simultaneously learn mappings of objects and structures, we need a way to represent many different objects, and structures of those objects, simultaneously, in a single vector with pre-specified length, eg., 1,000 components. (For simplicity and concreteness, we refer here to a 1,000-dimensional system. However, a practical system may require a different dimensionality, either larger for increased capacity or smaller for increased speed of computations. Section 5 and the Appendix explore dimensionality requirements.)\nConstraint 2: Distributed Representations. We need to represent hundreds of thousands of objects involved in an exponentially larger number of representations, so only one bit or vector component per object will not supply sufficient capacity. Therefore, we need to use a distributed representation for the vector, where information is stored in patterns, and where an individual component gives little, if any, information.\nTo take a specific example in natural language, we might represent a word as a 1,000 dimensional vector, whose components are randomly generated choices of -1 and +1 (as in Table 1). Then we can represent a sentence as the (single vector!) sum of the vectors for words in the sentence. We’ll examine disadvantages of this representation in Constraint 4, but the sum of vectors gives one way to represent a variable length sentence as a single, distributed, fixed-length vector.\nGallant and Okaywe: Representing Objects, Relations and Sequences 6\nIt is amusing to note that the usual computer representation of a sentence as a text string qualifies as a distributed representation! Any individual letter gives little or no information; only the larger pattern of letters gives information. Similarly, an image bit map is also a distributed representation. These representations have a minor problem in that they are not fixed length, but they also have a major “continuity” problem, as discussed below in Constraint 4.\nConstraint 3: A Complex Structure Methodology for Representing Objects and Structure Information Within the Distributed Vector. For many natural-language tasks, we clearly must take the syntactical structure into account. Here we encounter the “Binding Encoding Problem” in Cognitive Science and Artificial Intelligence surveyed by Treisman [1999]: for the word pair “smart girl”, we need to represent that “smart” refers to “girl”, and not some other word in the sentence. More generally, we need to be able to represent full parse information (or relations among image features) in a single distributed vector. This includes representing sub-clauses in sentences and representing parts of images with associated features (eg., color, location, motion).\nConversely, given a vector, we need to be able to recognize objects or structures encoded in the vector.\nConstraint 4: Map Similar Objects and Structures to Similar Representations. For learning algorithms to be able to generalize, it is necessary that similar objects and structures be represented by similar vectors. This is a continuity property for maps from objects and their structures to their representations.\nOn the representation side, vector similarity is readily defined by Euclidean distance between vectors. Two vectors are similar if (after normalization) they are close in Euclidean distance or, equivalently, if they have a significantly greater dot product than the dot product for two randomly chosen vectors.\nStarting with object similarity, we need to represent similar objects by similar vectors. For example, we want the vector for “smart” to be similar to the vector for “intelligent”.\nTurning to structure representations, we also need to represent similar structures by similar vectors. For example, we want all of the following to have similar vectors (to varying degrees):\n “The smart girl saw the gray elephant”\n “The gray elephant was seen by the smart girl”\n “The smart girl I just met saw the young gray elephant eating peanuts”\n(The second case might also be considered as having a different structure, but similar meaning.)\nFor images, we want to be able to replace similar image parts, or include additional features and structure information for an image, and have the new image vector similar to the original image.\nGallant and Okaywe: Representing Objects, Relations and Sequences 7\nThis Similarity Constraint is where character strings and bit maps fail as vector representations. For a string, if we add a space, or change to a similar word with a different number of letters, or switch active/passive voice, then the vector of letters changes drastically (as measured by vector Euclidean distance). Similarly, adding a row of pixels to an image can make a drastic difference in bit map vector similarity.\nConstraint 5: Sequences. We need to represent sequences of objects and relations. For example, we want to represent a group of sentences, as well as images derived from a sequence of eye saccades. This requirement for sequences is especially strong for spoken language, where even a single two-syllable word like “baby” does not hit our auditory system all at once, but rather as a sequence of sounds.\nThus, ultimately, we need to represent, over time, sequences of objects and relations: phonemes, words, sentences, images, or sensory inputs.\nConstraint 6: Efficient Encoding into the Representation. If we want to be able to encode, say, 100,000 sentences as 100,000 vectors, we need the mapping computation from each sentence to its representation vector to be roughly linear in the length of the sentence (or number of objects and relations for the sentence). Methods that require a machine learning pass over all 100,000 sentences to represent one of the sentences, or that require n 2 computation to represent n sets of objects and structures, would seem to be impractical. Similarly, we can’t practically use a representation method that, when presented with a new object, requires re-computation of the representations for all previously seen objects.\nConstraint 7: Neural Plausibility. Although not required for computational applications in language, vision, etc., we are nonetheless interested in representations that can serve as abstract models that capture important representational functionality in living systems.\nTo summarize, we have listed six “must have” Constraints, along with one final “nice to have” Constraint for representing objects, structures, and sequences so that we can use machine learning algorithms (and their extensive mathematical theory) “out of the box,” without constructing special case learning algorithms."
    }, {
      "heading" : "3. Representing Objects, Relations and Sequences",
      "text" : "Using a Single Distributed Vector\nWe now define MBAT, a Vector Symbolic Architecture, and show how it represents objects, relations and sequences by a single, distributed, fixed-length vector, while satisfying previously described Constraints.\nWe employ two vector operations: Addition (+) and Binding (#), as well as a Complex Structure Methodology of binding additive phrases, as described in the following Sub-Sections.\n3a. Vector Addition (+) and Additive Phrases The familiar vector addition operator is sufficient to encode an unordered set of vectors as a single vector of the same dimension as its constituent vectors. For example, in previous work we encoded a document as the sum of its constituent term vectors, and used this document vector for\nGallant and Okaywe: Representing Objects, Relations and Sequences 8\nInformation Retrieval purposes [Caid et al. 1995]. The key property of vector addition, illustrated in Table 1, is:\nProperty 1: Addition Preserves Recognition\nThis property is non-intuitive. For example, with scalars if we know that six positive and negative integers added together sum to 143, we cannot say whether one of those numbers was 17.\nBy contrast, as in Table 1, suppose we add together six 1,000 dimensional vectors with random +1/-1 components representing words,\nV Sum = V 1 + V 2 + … + V 6 .\nLet us denote the vector for the term “girl” by Vgirl.\nNow we can be highly certain whether Vgirl was one of the six. We simply compute the inner product (dot product)\nx = V girl • VSum = ∑ Vi girl Vi Sum\nand if x is near 1,000 the answer is “yes”, while if x is near 0 then the answer is “no”.\nProof: If Vgirl is one of the six vectors, say V1, then\nV girl • VSum = Vgirl • (Vgirl + V2 + … + V6)\n= V girl • Vgirl + Vgirl • (V2 + … + V6)\n= 1,000 + <mean 0 noise>\nSimilarly, if Vgirl is not one of the six vectors, then\nV girl • VSum = <mean 0 noise>\nThis completes the proof except for one small point: we have to verify that the standard deviation of the <mean 0 noise> term does not grow as fast as the vector dimension (here 1,000), or else the two dot products could become overwhelmed by noise, and indistinguishable for practical purposes. The Appendix shows that the standard deviation of the noise grows by the square root of the vector dimension, completing the proof. █\nThe Addition Property of high-dimensional vectors gets us part of the way to a good distributed representation for a collection of objects. For example, we can represent a sentence (or a document or a phrase) by a single (normalized) 1,000 dimensional vector consisting of the sum of the individual word vectors. Then we can compute the Euclidean distance between vectors to find, for example, documents with vectors most similar to a query vector. This was the approach for our previous document retrieval efforts. However, we still need to represent structure among objects.\nGallant and Okaywe: Representing Objects, Relations and Sequences 9\n3b. The Binding Operator (#)\nFor both language and vision, relying solely on vector addition is not sufficient. Due to the commutativity of vector addition, multiple phrases such as in “The smart girl saw the gray elephant” will have exactly the same vector sum as “The smart elephant saw the gray girl” or even “elephant girl gray saw smart the the”. In other words, vector addition gives us the “bag of words” used to create the sum, but no other structure information.\nHere we run into the classic “Binding Encoding Problem” in Cognitive Science and Artificial Intelligence, surveyed by Treisman [1999]. We need some way to bind “gray” to “elephant” and not to “girl” or to any other word, while retaining a distributed representation. More generally, we need the ability to represent a parse tree for a sentence, yet without abandoning distributed representations.\nPhrases\nIt is first helpful to formalize the definition of phrase with respect to representations. We define a phrase as a set of items that can have their order changed without making the representation unusable. Phrases loosely correspond to language phrases, such as noun clauses and prepositional phrases, or “chunks” in computational linguistics. For example, in “The smart Brazilian girl saw a gray elephant,” we can reorder the leading four-word noun phrase as in “Brazilian the girl smart saw a gray elephant,” and still understand the sentence, even though it becomes ungrammatical.\nSimilarly, for machine vision, an example of a phrase would be the vectors for an associated shape, X and Y-positions, color, and motion. Again, order is not critical.\nNeural Motivation\nTo motivate the binding operator we propose, consider the neural information processing schematic in Figure 1.\nGallant and Okaywe: Representing Objects, Relations and Sequences 10\nHere we have inputs from various sensory subsystems: vision, hearing, and touch. The current state of the system (“neuron state”) is modified by information from these inputs, as well as recurrent connections from itself.\nThe Figure illustrates the “Brain Binding Problem,” where we need the capability of linking together diverse sensory inputs (or different neural regions), with the current state of the system. Sequences also come into play here, as when we hear “baby” as two phonemes over two time periods, we need to sequentially bind the inputs to recognize and represent the term “baby” and its associations.\nFor the binding task, the main thing we have to work with are the recurrent connections at the top of the Figure. (Any Theory of Neural Information Processing that does not include a major role for such recurrent connections is missing a very big elephant in the Neural Physiology room!) Moreover, we cannot make too many organizational demands upon the recurrent connections, because any complex structure needs to be passed genetically and hooked up during a noisy, messy growth phase.\nSo, continuing with our motivational exploration for binding, what if we take the easiest possible genetic/growth structural organization, namely random. Can we have the recurrent connections compute a random map and have that be of any use for binding?\nGallant and Okaywe: Representing Objects, Relations and Sequences 11\nBinding Operator (#)\nReturning to the mathematics, let us now define the simplest version of a unary binding operator, #. (Below we will also define several alternatives.)\nLet M be a fixed square matrix of appropriate dimension for our vectors, eg., 1,000 by 1,000. We let components of M be randomly chosen values (eg., +1/-1).\nAs a point of notation, when raising a matrix to a power, we will always use parentheses, as in (M) 3 . This distinguishes from the designation of several different matrices, for example M Actor and M Object .\nNow if we have a sum of vectors, V 1 + V 2 + V 3 , i.e., a phrase, we can bind them as part of a structure description by:\n#( V 1 + V 2 + V 3 ) ≡ M (V1 + V2 + V3).\n(The “#” operator “pounds” vectors together.) Thus all terms in the vector of the form (M) 1 V are differentiated from terms of the form (M) 2 V, (M) 3 V, etc. We can think of (M) i V as transforming V into a unique “bind space” according to i.\nWith respect to complex structure methodology, in MBAT binding operates upon additive phrases, where the order of vectors in the phrase is not critical. Thus we bind\n#(actor + the + smart + Brazilian + girl)\n≡ M (actor + the + smart + Brazilian + girl).\nEach term in the phrase may itself be the result of a binding operation, which allows us to represent complex structure (for example, sub-clauses of a sentence).\nSome things to note:\n One of the vectors in the summed arguments can be the current state of the system, so letting\nV(n) be the current state at time n,\nwe have the next state given by\nV(n+1) = M (V(n)) + ∑ Vinputs ( 3.1)\nNote that the Binding operator in this formulation corresponds to the recurrent connections in Figure 1. Mi,j is the synapse between cell j and cell i. (Also, individual cells in the “Current State” do not need to differentiate whether inputs are coming from feed-forward sources or from recurrent connections.)\n This formula for computing the next state also gives a way to represent input sequences. Kanerva [2009] and Plate [2003] previously employed this technique for sequence\ncoding, using different binding operators.\nGallant and Okaywe: Representing Objects, Relations and Sequences 12\n Phrases that get bound together must be unambiguous with respect to order. Thus we can bind phrases like “the smart girl”, where order doesn’t really matter in understanding the\nphrase. However, we couldn’t bind in one step “the smart girl saw the grey elephant”, because we would run into the binding ambiguity of whether “smart” refers to “girl” or “elephant”. Several binding operations would be required, as in Figure 2.\n We can make good use of Tags, represented by (random) tag vectors added to phrases, to\nspecify additional syntactic and semantic information such as actor (ie., V actor), object, phraseHas3words, etc.\n Computing binding (matrix multiplication) involves more work than computing circular convolution in Holographic Reduced Representations if Fast Fourier Transforms are used\nfor HRRs [Plate 2003]. Also, Binding in MBAT requires us to make use of a different mathematical space, i.e., matrices vs. vectors-only in HRRs.\nNow we can see how to unambiguously represent “The smart girl saw the gray elephant” in Figure 2.\nGallant and Okaywe: Representing Objects, Relations and Sequences 13"
    }, {
      "heading" : "1. actor + the + smart + girl + phraseHas3words",
      "text" : ""
    }, {
      "heading" : "2. verb + saw + phraseHas1word",
      "text" : ""
    }, {
      "heading" : "3. object + the + gray + elephant + phraseHas3words",
      "text" : "the form of Tag vectors. This vector is added to the random recurrent connections from V(n), to produce the next state vector, V(n+1).\nThe resulting (single) vector, V, is formed from 13 object/Tag vectors:\nV = (M) 2 (actor + the + smart + girl + phraseHas3words) + M (verb + saw + phraseHas1word) + (object + the + gray + elephant + phraseHas3words).\nTags such as phraseHas3words and phraseHas1word, though perhaps not biologically realistic, greatly simplify the task of decoding the vector, i.e. producing the sentence encoded in the sum.\nGallant and Okaywe: Representing Objects, Relations and Sequences 14\nIf we need to speak the sentence, an approach to “decoding a vector” is to produce each phrase by first computing the number of words in the phrase, and then finding that many terms with the highest dot products.\nAs desired, “smart” is associated with “girl” in this sum of 13 vectors, because we have term\n(M) 2 Vsmart and (M)2 Vgirl, but elephant appears as Velephant.\nWe also have a recognition property for the binding operator.\nProperty 2: Binding (#) Preserves Recognition\nSuppose we are given V = #( V 1 + V 2 + V 3 ). Can we tell if V girl is among the bound operands?\nYes, we simply look at M Vgirl • V\n= M V girl • ( M V 1 + M V2 + M V3) (3.2)\nand the result follows similarly to Property 1 for vector sums.\n█\n3c. Complex Structure Methodology\nFigure 2 also illustrates the Complex Structure Methodology we employ in MBAT. Binding is a unary operator that operates upon phrases consisting of bundled (added) vectors. Each vector being summed may be an object (eg., word), or the result of another binding operation (eg., subclause). Thus “the smart Brazilian girl” is represented by\n#(actor + the + smart + Brazilian + girl).\nGiven a vector for a complex structure, we can check whether “girl” appears in any of the phrases at three outermost levels by taking a dot product with the single vector 1\n[(M)0 + (M)1 + (M)2] Vgirl . (3.3)\nThe dot product with V given by [ ((M)0 + (M)1 + (M)2) Vgirl] • V will be large positive only if Vgirl appears in one of the phrases, i.e. as Vgirl, (M)1 Vgirl, or (M)2 Vgirl.\nSimilarly, we can determine if “smart” and “girl” appear together in any phrase in V by checking if\nMAXi=0 2 [(M)i ( Vsmart + Vgirl) • V ] (3.4)\nis sufficiently positive. Note that if “girl” appears without “smart” in a phrase, then the value above is still positive, but half of the value than when both appear in the same phrase. Also note that we cannot replace the MAX operator by a sum, or we run into binding ambiguity issues. 2\nThus using additive vector phrases, (V 1 + V 2 + V 3 ), as operands for binding helps with subsequent recognition (and learning) of items. It also helps reduce computational demands compared to using only binding operations, because vector addition is cheaper than matrix multiplication.\n1 (M) 0 is the identity matrix. 2 It is not clear to what extent MAX should be considered neurally plausible.\nGallant and Okaywe: Representing Objects, Relations and Sequences 15\n3d. Variants of the Binding Operator\nAs with vector addition, vector binding has several important variations.\n We can define a collection of binding operators with structural significance, and give\neach phrase its own binding operator, such as M Actor and M Object\n. This makes all phrases at the same level. For example,\nV = M Actor (the + smart + girl + phraseHas3words) + M verb (saw + phraseHas1word) + M Object (the + gray + elephant + phraseHas3words).\n As a special case, we can also define “Two Input” binding operators. For example, if we\nwant a binary parse tree, we can define #( V 1 , V 2 ) to be M Left V 1 + M Right V 2 , where M Left and M Right\nare two different fixed matrices. Note that “Two Input #” is noncommutative:\n#( V 1 , V 2 ) ≠ #( V2, V1 )\nas required for specifying a binary tree.\n “Binary World”: A most interesting variation is to replace components of\n#( V 1 + V 2 + V 3 ) by “+1” if greater than or equal to 0, and “-1” if less than 0 (as in Kanerva’s Binary Spatter Codes). Restricting to +1/-1 components has the advantage of playing nicely with Auto-associative learning algorithms [Kohonen 1977, Anderson et al. 1977].\nIt is worth noting that we can preserve many of the benefits of continuous vector components (eg., for vector sums), while still restricting all vector components to +1/-1. We take a group of vector components computed from (approximately) the same connections and employ different thresholds, obtaining a binary representation for a continuous sum. For example, we can replace the first continuous component, V1, of an input by the group of binary components\nV1a ≡ +1 if (V1 ≥ 37); else -1\nV1b ≡ +1 if (V1 ≥ 5) ; else -1\nV1c ≡ +1 if (V1 ≥ 19) ; else -1\n… .\n Permutation Matrices: It is possible to use a permutation (random or not) for the binding matrix, as permutations are maximally sparse and easy to invert. However, an advantage\nof using matrices with many non-zero elements is that they can boost the representational dimensionality of isolated inputs. For example, suppose the goal is to learn ExclusiveOR (XOR) calculated on components 1 and 2 (and ignoring other components). A random permutation maps the two inputs to two different components but retains the same dimensionality, so that the probability of the resulting representation being linearly separable remains at 0. By contrast, in a “binary world” architecture with -1/+1 components, when a non-sparse random matrix is applied to inputs followed by a thresholding step, components 1 and 2 are spread non-linearly among many components. This increases the effective dimensionality of the representation [Gallant & Smith 1987], and makes the probability of linear separability (and easy learnability) greater than 0.\nGallant and Okaywe: Representing Objects, Relations and Sequences 16\nSuch increased representational ability is an advantage with working in Binary World, rather than using continuous vector components.\nAnother advantage of using a non-sparse binding matrix is that the representation decays more gracefully when noise is added to the matrix. Finally, in the nervous system, the majority of neurons synapse with many other neurons rather than a single neuron, making a permutation matrix appear much less neurally plausible.\n An important performance tuning issue for practical implementations is scaling the binding operator so that, for example, an (M) 2 V girl term does not dominate other terms.\nOne approach is to normalize the result of a binding operation so that the resulting vector\nhas the same length as a vector for a single term, √D. Alternatively, the normalization can make each M V i phrase component have length √D. Finally, we could just work in “Binary World,” in which case the problem goes away.\n3e. Multiple Simultaneous Representations\nAn important technique for reducing “brittleness” of the structure representation (such as parse information) is to simultaneously encode several structure descriptions (with different binding operators) in the vector by adding them. This increases robustness by having different structures “voting” in the final representation.\nAn example of multiple simultaneous representations is representing sentences as structureless additions of word vectors, plus binding of phrases, plus sequentially binding phrase components to fix their precise order. For example, with “The smart Brazilian girl …”, we might have\n(the + smart + Brazilian + girl) +\nM Actor (the + smart + Brazilian + girl) +\nM Actor_Ordered (the + M(smart + M(Brazilian + M(girl)))).\nWe may also specify different positive weights for each of the three groups, for example to increase the importance of the top “surface” group with no binding.\nMultiple simultaneous representations are helpful because we cannot know, a priori, which kind of phrase grouping will be critical for capturing the essence of what is to be learned in later stages.\nFor another example, if parser A results in sentence representation V A , and parser B produces V B , then the final representation for the sentence can be V A + V B .\nAs a third example, if we have two (or more) image feature extraction programs (perhaps operating at different scales in the image), each program’s outputs can be converted to a vector and then added together to get the final vector representation.\nTo summarize this Section, the two operators + and #, coupled with representing complex structure by applying # to additive phrases, permit us to represent objects, structures and sequences in MBAT. In Section 6, we check whether MBAT satisfies the representational constraints we have posed.\nGallant and Okaywe: Representing Objects, Relations and Sequences 17"
    }, {
      "heading" : "4. Learning and Representation: Three Stages",
      "text" : "For both computational and neural systems, we distinguish three Computational Stages: PreProcessing, Representation Generation, and Output Computation. This distinction is helpful, because learning plays a different role in each Stage.\nPre-Processing Stage\nThe Pre-Processing Stage occurs prior to actually generating a vector representation. Here is where vector representations for objects (words, images) are developed so that similar objects have similar vectors. Typically the mapping is the result of a preliminary learning phase to capture object similarities in vectors (as discussed in Section 6).\nAs an important example of a Pre-Processing Stage in living neural systems, there appears to be much feed-forward processing of features of various complexity (eg., line detectors, moving edge recognizers, etc). These computations can be genetically hard-wired and/or learned during development, but then do not need to be re-learned in the course of the following Representation Generation Stage.\nFor automated systems, the identification of phrases (or “chunks”) is a typical pre-processing operation that can be quite helpful for following Stages.\nAlthough the learning involved in the Pre-Processing Stage may be computationally intensive, it is done only once, and then can be used in an unlimited number of representation calculations. Thus it avoids violating the Efficient Encoding Constraint, because it is not a part of the Representation Generation Stage. The features resulting from this Stage serve as the inputs to the representational system.\nRepresentation Generation Stage\nAlthough the Pre-Processing Stage (and Output Computation Stage) can involve significant machine learning, there are reasons for the Representation Generating Stage to avoid machine learning.\n Internal learning means tailoring the representation for one set of applications, but this can make the representation less suited to a different set of problems.\n When representing inputs, a learning step might slow down processing so much as to make the resulting representation system impractical, thereby violating the Efficient\nEncoding Constraint.\nOn the other hand, we can envision a good case for learning some vector representations as part of a separate “long term memory” component, where we want to incrementally add a set of sequences to an existing set so that they may be recalled. Memory, recall, and novelty detection are important issues, but beyond the scope of this Representation paper.\nGallant and Okaywe: Representing Objects, Relations and Sequences 18\nOutput Computation Stage\nFinally the Output Computation Stage is clearly a place where learning is vital for mapping representations to desired outputs. Here is where we benefit from being able to use conventional fixed-length vectors as inputs.\nOne major benefit is that a lot of previous theory is immediately applicable. These include the Perceptron Convergence Theorem [Rosenblatt 1959, see also Minsky & Papert 1969], Perceptron Cycling Theorem [Minsky & Papert 1969, Block & Levin 1970], Cover’s theorem for the likelihood of a set of vectors to be separable [Cover 1965], and Vapnik-Chervonenkis generalization bounds [1971]. This body of theory permits us to prove learnability in many cases, as well as to set bounds on generalization.\nTo take a specific example, suppose we use random vector representations of words, and we have a collection of sentences with at most four phrases encoded as in Figure 2, and each sentence either contains the word “girl” or the word “elephant” (but not both). Then we can prove that perceptron learning will learn to distinguish the two cases, making a bounded number of errors in the process.\nProof: Consider\n[ ( (M) 0 + (M) 1 + (M) 2 + (M) 3 ) ( V girl – V elephant ) ] • V.\nExcluding noise with mean 0, if V has V girl in a phrase, the dot product will be positive. If, by contrast, V has V elephant in a phrase, then the computation will be negative. Therefore the vector in brackets is a perfect linear discriminant. Now we can apply the Perceptron Convergence Theorem [Rosenblatt 1959, see also Minsky & Papert 1969, Gallant 1993] to know that Perceptron Learning will find some error-free classifier while making a bounded number of wrong guesses. (A bound is derivable from the bracketed term.)\nThis proof illustrates a simple and general way of proving that these kinds of mappings are learnable using the MBAT representational framework. We merely show that at least one errorfree linear classifier exists, and then we can immediately conclude that perceptron learning will learn some error-free classifier (perhaps a different one) in finite time.\nNote that there is no need to decode (i.e., fully recover) vectors in the process of learning!\nReviewing the three Stages, the overall processing picture is:\n a (one time, done in the past) Pre-Processing Stage, which may involve significant machine learning, feature computations, novelty detection, etc., which (efficiently)\nproduces the inputs to:\n the Representation Generating Stage (our main focus), where there may be no learning, and followed by:\n an Output Computation Stage, which almost always involves machine learning for a particular task.\nGallant and Okaywe: Representing Objects, Relations and Sequences 19"
    }, {
      "heading" : "5. Capacity: Analytics and Simulation",
      "text" : "For practical systems, we need to know what dimension, D, is required for vectors to represent objects and relations, and how D scales with increased system sizes.\nThe Appendix derives analytic estimates when adding S random +1/-1 vectors (referred to as the bundled vectors) to form vector V, and where N other random vectors are present in the system. (A bundle can be the vectors in a phrase.) In particular, we derive bounds and estimates for the required dimension, D, so that at least 98% of the time each of the S bundled vectors has a higher dot product with V than each of the N other vectors. Said differently, we seek error-free separation performance at least 98% of the time.\nFor a “Small” system where S = 20 bundled vectors, and where there are 1,000 other random vectors, we derive that D = 899 dimensions guarantees error-free performance at least 98.4% of the time. An example of a “Small” system application would be finding whether a simple diagram (collection of shapes in various configurations) is among 20 designated examples.\nSimilarly, for a “Medium” system with S = 100 bundled vectors, and where there are 100,000 other random vectors, we derive an estimate for the required dimension D of 6,927 for error-free performance 98.2% of the time.\nFinally, for a “Large” system with S = 1,000 bundled vectors, and where there are 1,000,000 other random vectors, we derive an estimate for the required dimension D of 90,000 for error-free performance 99% of the time.\nThe approximation derived in the Appendix allow us to say how required dimension D scales as S and N increase. In summary, for a given error threshold:\n For fixed number of vectors S bundled together, as dimension D increases, the number of other vectors, N, we can distinguish from the bundled vectors (while keeping the same\nerror threshold) increases exponentially with D.\n For fixed number of other vectors, N, as dimension D increases, the number of vectors S we can bundle together while distinguishing bundled and random vectors (and while\nkeeping the error threshold) increases linearly with D.\nThus representing additional bundled vectors (S) is fairly “expensive” (required D is linear in S), while distinguishing the bundled vectors from N other Random vectors is fairly “cheap” (required D is logarithmic in N).\nIn addition to the analytic estimates in the Appendix, we also performed capacity simulations for “Small” and “Medium” sized problems. Here we investigated, for different values of vector dimension D, computing the sum of S vectors to form bundled vector V. We then found the S vectors with highest dot products with V among the S bundled vectors and the N additional random vectors. We computed:\n1. The fraction of bundled vectors that are in the S vectors having highest dot product with V.\n2. The fraction of trials that produce error-free performance: all bundled vectors have higher dot products with V than any of the Random vectors. (This is the same measure\nanalyzed in the Appendix.)\nGallant and Okaywe: Representing Objects, Relations and Sequences 20\nFigures 3 and 4 give the results. The “Small Sized” system required several hours computation time on a 2.4 GHz laptop, and the “Medium Sized” system required 24 hours simulation time.\nGallant and Okaywe: Representing Objects, Relations and Sequences 21\nFrom the simulations, we conclude:\n For a “Small” system (S=20; N=1,000), a lower estimate for required vector dimension D is only 350. This gives 90% (18 of 20) of the top vectors being bundled vectors.\n For the same “Small” system, an upper estimate for required vector dimension D is 900. This gives 98% probability of having error-free performance with highest dot products\nall being the 20 bundled vectors.\n Similarly, for a “Medium” system (S=100; N=100,000), we have lower and upper estimates for required dimension D of 2,500 and 7,000 respectively.\nIn the Appendix, we derive an approximation formula for p, the probability of error-free performance. Letting\nT (x) = one-sided tail probability in a normal distribution of a random variable being at least x standard deviations from the mean\nWe derive\np = 1 – NS T (√ (D/(2S-1)))\nwhere the approximation is valid when p is close to 1 (ie., p > 0.9).\nThe analytic estimates for required dimensionality for error-free performance in the Appendix are in close agreement with simulation results. Results are summarized in Table 2.\nGallant and Okaywe: Representing Objects, Relations and Sequences 22\nSome additional comments on capacity.\n It is possible to improve recognition ability for bundle vectors when the vectors added together are not random. For example, in natural-language applications we can use a\n“language model” [Brown et al. 1990], which gives statistics for a vector being included in the presence of other vectors in the bundle.\n When we use vectors such that similar objects have similar vectors, for example giving similar vectors to similar words, then this will decrease raw discrimination ability to the\nextent that vectors are more similar than random. However, this should help, rather than hurt, an implementation – that’s the reason we made the objects more similar in the first place!\n When machine learning follows representation in a practical system, this may require significantly less dimensionality than required for error-free discrimination, depending\nupon the specifics of the learning task and performance requirements.\nFor example, if there are S positive examples and N negative examples to be learned, we don’t need A • V > R • V for every case where A is a positive example and R is a negative example.\nInstead of using the sum of the positive examples, V, to discriminate, we have the liberty of finding any vector X that does a good job of making A • X > R • X for each pair A, R. If any X exists that works for all pairs, it is a linear discriminant which is easily learned by Perceptron Learning. Moreover, most practical modeling problems do not require perfect discriminants.\nFinally, it is worth estimating computational requirements. Generating a representation vector of dimension D involves a D × D matrix multiply for each binding operation, plus vector additions as needed, regardless of the number of objects in the system. For a “Medium” sized system with D = 2,000 to 7,000, generating a representation on a single processor computer is clearly practical, although each binding requires 8 billion to 343 billion multiplications and additions.\nGallant and Okaywe: Representing Objects, Relations and Sequences 23\n(Speedups are available using multiple processors and also by fast matrix multiplication techniques.)\nFor a “Large” system with D = 90,000, extensive parallelism would be required. In general, multiple processors divide the computation time by the number of processors for these computations. It is noteworthy that a living system with neurons summing synaptic inputs from many other neurons, and with neurons working in parallel, could conceivably compute binding operations of vector sums for the state update Equation 3.1 in a single time step!\nFollow-on learning in the Output Computation Stage may require less computation. For example, perceptron learning requires only vector dot products and vector additions for each iteration, avoiding the more expensive matrix multiplications. Of course, the number of iterations required is also important. (Worst-case bounds on required iterations for linearly separable problems grow linearly with the dimension and the number of training examples, and grow by the square of the length of the shortest integral solution vector [Gallant 1993].)\nWe conclude that vector dimension requirements and computations appear easily manageable for practical implementations."
    }, {
      "heading" : "6. Checking the Constraints",
      "text" : "We want to verify that by using distributed vectors and two operations on vectors, addition (+) and binding (#), plus MBAT’s approach for representing complex structure, we are able to satisfy the Constraints from Section 2.\nConstraints 1 (fixed-length vector) and 2 (distributed representation) are obviously satisfied. We have seen how the binding operator can represent structure (Constraint 3), including a practical solution to the binding problem, as well as sequences (Constraint 5). Computations are clearly linear in the number of objects, plus complexity (description size) of structures (Constraint 6).\nLet us consider Constraint 4 (similar objects/structures map to similar representations). Although “similarity” is not precisely defined, nevertheless we can verify our benchmark test cases.\nWith natural language, there are a number of ways to get similar vector representations for similar words, as surveyed in Section 7.\nNote that this (learned) preprocessing is a one-time computation that does not affect speed during the Representation Generation Stage.\nNow, suppose we encode “The smart girl saw the gray elephant” by the sum of vectors:\nV = M Actor (the + smart + girl + phraseHas3words) + M verb (saw + phraseHas1word) + M Object (the + gray + elephant + phraseHas3words).\nThen if we have object encodings such that similar objects have similar vectors, we can interchange similar objects and leave the resulting representation vector similar to the original. For example, if V intelligent is similar to V smart , then “The intelligent girl saw the gray elephant” will have a vector very similar to V.\nGallant and Okaywe: Representing Objects, Relations and Sequences 24\nTurning to structure similarity, we need add only one additional vector term, V Passive_Voice , to V in order to encode\n“The gray elephant was seen by the smart girl”.\nThus these two representation vectors are very similar as vectors.\nIn a like manner, we can encode\n“The smart girl I just met saw the young gray elephant eating peanuts”\nby adding to the original vector those additional vectors for the two new clauses, and adding young to the elephant clause. Once again, we arrive at a vector similar to the original one (i.e., significantly closer than two randomly chosen sentence vectors).\nThe last remaining Constraint, Neural Plausibility, is even less precise. However, we maintain this Constraint is satisfied by having a system with individual vector components (neurons), a notion of state consisting of which neurons are firing, and a way to represent objects and structural relationships in the overall state that does not make unreasonable demands on “wiring” of the system by genetics or during growth."
    }, {
      "heading" : "7. Prior Research",
      "text" : "Vector Symbolic Architectures were summarized in Section 1, and are further discussed in the next Section.\nA key early source for Distributed Representations is [Hinton’s 1984], as well as [Hinton 1986]. This paper presents characteristics, advantages and neural plausibility arguments. (These topics are reviewed in Gallant [1993].)\nIn Information Retrieval, the use of high dimensional vectors to represent terms (words) was pioneered by Salton & McGill [1983]. Deerwester et al. [1990] represented terms, documents and queries by starting with a document-by-term matrix, and then using Singular Value Decomposition to reduce dimensionality. This also achieves some measure of similarity of representation among similar terms. A later approach used random vectors and learned modifications of those vectors to represent similarity of meaning among words in the MatchPlus system [Caid et al., 1995]. The basic idea was to start with random vectors for terms, and make passes over a corpus while modifying vectors by adding in a fraction of surrounding vectors (and normalizing).\nIn the same spirit, other language systems make good use of a “language model” to statistically compute probabilities of a word given its immediate predecessors [Brown et al. 1990], or computing the probability of a word given its surrounding window [Okanohara & Tsujii 2007, Collobert &Weston 2008]. See also the Brown clustering algorithm [1992], phrase clustering [Lin & Wu, 2009] and [Huang & Yates 2009]. Self-Organizing maps present another possibility [Kohonen 1995, Hagenbuchner et al. 2009].\nHowever, all this Information Retrieval work is still within the bag of words limitations imposed by not having a binding operation.\nGallant and Okaywe: Representing Objects, Relations and Sequences 25\nMore recently, Jones and Mewhort [2007] looked at incorporating positional information with semantic vectors created by a similar approach to MatchPlus. They capture order information by looking at surrounding windows (up to distance 7) for each term in a corpus. They then take the (HRR) convolution of each window, while replacing the target term by a dummy.\nFor example, suppose we’re learning the order vector for King in “Rev. Martin Luther King, Jr. said …”. Then letting ɸ be a constant placeholder for the target term “King”, we would add HRRs for: “Luther * ɸ”, “ɸ * Jr”, “Luther * ɸ * Jr”, etc. The resulting order vector is then normalized and added to the semantic vector for King. The result is a vector that captures semantics as well as word order syntactic effects. Similar results were obtained by Sahlgren et al. [2008] by encoding order information with permutations; see also Recchia et al. [2010]. Such vectors should provide interesting starting codings for terms in language systems, including MBAT.\nWith respect to matrix multiplication bindings, Hinton’s “triple memory” system [1981] used random matrix connections in a subsidiary role while focusing on learning, rather than representation. Also, Plate’s book [2003, page 22] later mentions in passing exactly the “Two Input” version of the binding operator from Section 3, which he attributes to Hinton. Plate also lists matrix multiplication as an alternative binding possibility in Section 7.3, Table 26.\nIn a Computational Linguistics setting, Rudolph & Giesbrecht [2010] proposed using only matrices (rather than vectors) to represent objects, and examined matrix multiplication as a composition operation. Similar results were obtained by Sahlgren et al. [2008] by encoding order information with permutations; see also Recchia et al. [2010]. However, vector addition carried out by sparse matrices in D 2 dimensions rather than D dimensions is inefficient. There is also loss of the binding recognition property once we use a large number of different matrices for multiplication, rather than a small set of matrices for binding operators.\nMitchell & Lapata [2008], also in a Linguistics domain, mention the binary version of the # operator in passing, although most of their efforts focus on a bag-of-words semantic space model.\nTurning to sequences, the traditional approach to dealing with sequential inputs (eg., a sentence) is to use a sliding window. A related approach, Elman nets [1990], are three-layer neural networks that copy the hidden layer outputs as net inputs for the next cycle, thereby producing an additional “sliding window over hidden layer outputs.” Elman nets are therefore able to accumulate some state information over the entire sequence.\nAnother related sequence approach, Time Delay Neural Networks of Waibel et al. [1989], has several layers of groups of hidden nodes. The first node in each group sees (say) nodes 1-3 in the group (or input) immediately below, the second node of each group sees nodes 2-4 in the group below, etc. Thus we have a multi-stage fan-in from the input layer, with each stage reducing dimensionality while expanding global coverage.\nAll three of these approach typically employ Backpropagation (or variants) to adjust weights in response to training data. Therefore they are more approaches for learning algorithms, rather than approaches for representation. Although we could consider hidden layer activations as representations for new inputs after learning has ended, there is limited ability to recognize stored objects, and the only type of structure that is explicitly captured is sequentiality. Nevertheless, these techniques might prove useful in a Pre-Processing Stage prior to generating representations.\nGallant and Okaywe: Representing Objects, Relations and Sequences 26\nFor sequence representations that do not require learning, Kanerva [1988] represents sequences using pointer chains. Later, Plate [2003] employs trajectory association, where the idea is to bind powers of a vector to sequence information. For example if we want to represent the sequence A, B, C, we can take some fixed vector V and compute\nV*A + V*V*B + V*V*V*C.\nThere are additional variations involving helper terms for easier decoding.\nThere is also a body of research on learning with structural inputs, much of which involves using Backpropagation related algorithms to learn weights in a pre-defined network without directed loops [Frasconi et al. 1998]. Again, the focus is on learning, rather than representation. The Backpropagation computations (along with potentially large numbers of hidden units) make this approach impractical for generating general-purpose representations.\nAnother early work involving Reduced Representations is Pollack’s RAAM (Recursive Auto Associative Memory) architecture [1990] and later extensions, for example LRAAM (Labeling RAAM) by Sperduti, Starita & Goller [1995]. These approaches use Backpropagation learning (or variants) on a network of inputs, hidden units, and outputs that attempt to reproduce inputs. The hidden units, after learning, encode the reduced representations of the inputs. A drawback of these approaches is the need for learning over all inputs to achieve the representations of the inputs. For example, adding additional input cases requires re-learning the representation for all previous input patterns using Backpropagation (violating the Efficient Coding Constraint). Improvements in capacity and generalization were reported by Voegtlin & Dominey [2005]. Although these approaches are all too slow (non-linear) for the Representation Generation Stage, their abilities to capture generalization may present good synergy as part of the Pre-processing Stage.\nAnother important line of research for learning structures with generalization was Hinton’s family tree tasks [1986, 1990], followed by Linear Relational Embedding [Paccanaro & Hinton 2001a, 2001b; Paccanaro 2003]. As with RAAM architectures, generalization ability may prove useful in producing Pre-processing Stage inputs for MBAT or other approaches.\nSperduti [1997] proposed a “generalized recursive neuron” architecture for classification of structures. This complex neuron structure can be seen as generalizing some other ways of encoding structure, including LRAAM, but representation size grows with respect to the size of the structure being represented, as does computational requirements.\nAnother recent approach represents structures by dynamic sequences, but requires a Principal Component Analysis (PCA) for obtaining the representation. Sperduti [2007] reports a speed-up for PCA calculation; the result could play a role in Pre-processing Stage learning of inputs, but is still too computationally demanding for computing representations in the Representation Generation Stage.\nIt is worth noting that Sperduti et al. [1995] conduct simulations to show good performance for learning to discriminate presence of particular terms in the representation. By contrast, Section 4 proves learnability for a similar task, without the need for simulation.\nMore recently, Collobert & Weston [2008] show how a general neural network architecture can be simultaneously trained on multiple tasks (part-of-speech Tags, chunks, named entity Tags, semantic roles, semantically similar words) using Backpropagation. They encode sentences using\nGallant and Okaywe: Representing Objects, Relations and Sequences 27\nTime Delay Neural Networks of Waibel et al. The network learns its own vector representations for words during multi-task learning, so that different tasks in effect help each other for a shared portion of the vector representation for each task. However, each task ends up with its own vector representation for the non-shared part. Outputs consist of either a choice from a finite set (eg., part-of-speech Tags) or a single number (probability) on a word-by-word basis.\nIt is not apparent that their internal vector encodings can serve as representations for, say, sentences, because each learning task produces a different vector for the same sentence. Nor are the sets of outputs, produced for each word, easy to directly convert into a fixed-length vector for the sentence.\nHowever, there is an appealing natural synergy of Collobert & Weston’s system with the representation we examine, because outputs from their system can serve as structure information to be included in the representation vector. In particular, the “chunking” outputs can comprise phrases, which are ideal candidates for binding operations in constructing the vector for a sentence. Vector Symbolic Architectures in general, and MBAT in particular, give a natural way to leverage word-by-word information as inputs to learning algorithms by converting it to fixedlength, distributed vectors.\nIn a later work, Collobert et al. [2011] develop a unified neural network architecture for these linguistic tasks, where all tasks are trained using two somewhat complex architectures based upon Time Delay Neural Networks. As in previous work, their system outputs a set of tags for each word. Training time is one hour to three days, depending upon the task, and scoring of new input is very fast after training. Their impressive point is that, at least for producing various Tags for words in the domain of language, one of two neural network learning architectures can produce excellent results for a variety of tagging tasks. The architecture is highly tuned to producing word-by-word Tags from language input. Therefore, it seems very hard to adapt this approach to other tasks such as machine translation, where more general output structures must be produced, or to other domains such as image processing, without first converting outputs to fixed-length vectors.\nThese papers are example of a broader category, Structured Classification, where for each sequential input object we compute either one choice from a fixed and finite set of choices, or we compute a single scalar value. There is much other research in the Structured Classification paradigm, which we do not review here.\nRecently Socher et al. [2010, 2011a, 2011b] have shown how binding matrices can be learned using Backpropagation with complex loss functions. Socher’s Recursive Neural Networks (RNN) are binary trees with distributed representations, which are structurally identical to “Two Input” binding operators in Section 3. In particular, Socher’s matrix [2011a] for combining two vectors is equivalent to concatenating rows from M Left and M Right to form a single “double wide” matrix for applying to pairs of concatenated column vectors.\nThe RNN approach is applied to Penn Treebank 3 data to learn parsing and the Stanford background dataset 4 to obtain a new performance level for segmentation and annotation. They also report [2010] excellent results with the WSJ development dataset, and an unlabeled corpus of the English Wikipedia [2011b].\n3 http://www.cis.upenn.edu/~treebank/ 4 http://dags.stanford.edu/projects/scenedataset.html\nGallant and Okaywe: Representing Objects, Relations and Sequences 28\nSocher’s work demonstrates that the kind of Vector Symbolic Architectures we describe can be useful for practical problems.\nFinally, there is interesting work by Maass et al. [2002] on asynchronous modeling of noisy neuron behavior, including recurrent connections, in their “Liquid State Machine” model. They show ability to train output neurons to discriminate noisy input patterns in neurons with fully asynchronous firing times.\nThis modeling is at a granular neuron level, and is therefore more suited for neural modeling and less suited for large scale, practical systems. For example, simulations typically deal with distinguishing among a small number of input patterns, and there is no attempt at explicitly representing complex structure among objects.\nNevertheless, the “Liquid State Machine” models of Maass and colleagues share several characteristics with the kind of representational systems we examine, including:\n They are general representations, not tuned to any specific task.\n There is a specific task/output readout Stage that involves learning.\n State transitions are similar to Equation 3.1 in Section 3.\n The overall system does not need to converge to a stable state, as with most learning algorithms.\n The mathematical model can be used to hypothesize computational explanations for aspects of neural organization and processing.\nMaass et al. investigate “local” versus “long range” recurrent connections, giving computational explanations for the distributions. They find that less than complete connections work better than complete connections with their asynchronous, noisy, dynamic models. (This result may not apply to non-asynchronous systems.)"
    }, {
      "heading" : "8. Discussion",
      "text" : "We have shown that a desire to apply standard machine learning techniques (neural networks, perceptron learning, regression) to collections of objects, structures and sequences imposes a number of Constraints on the Representation to be used. Constraints include the necessity for using distributed, fixed-length vector representations, mappings of similar objects and structures into similar vector representation, and efficient generation of the representation.\nIn response we have developed MBAT, a neurally plausible Vector Symbolic Architecture that satisfies these constraints. MBAT uses vector Addition and several possible variants of a vector Binding operator, plus a Complex Structure Methodology that focuses upon additive terms (ie., phrases).\nMBAT as a Vector Symbolic Architecture\nViewed from the perspective of Vector Symbolic Architectures, MBAT can be characterized as follows:\n Vector components are either continuous, or are two-valued (eg., +1/-1).\nGallant and Okaywe: Representing Objects, Relations and Sequences 29\n Addition is vector addition, optionally followed by thresholding as in Binary Spatter Codes.\n Binding is a unary operator consisting of matrix multiplication. Either one matrix or a matrix chosen from a small set of matrices (M Actor , M Object , etc) is used for binding.\nComponents of matrices can be chosen at random, and can have +1/-1 entries. If vectors are restricted to having +1/-1 components, then matrix addition and multiplication are followed by a thresholding step. Another variation adds binding operands back into the result of matrix multiplication.\nA two-argument version of binding is available by multiplying each argument by one of two fixed matrices, and adding the results.\n Quoting is by repeated matrix multiplication, or by using different matrices from a small set of matrices.\nThis is similar to quoting by permutation, for example Gayler [2003], Kanerva [2009], and Plate [2003], except we need not restrict ourselves to permutation matrices. (See comments in Section 3 on permutations.)\n The Complex Structure Methodology applies (unary) binding to additive phrases, M(V+W+X). Each of the added terms may be the result of another binding operation.\nSeveral different representations can be simultaneously represented by adding their vectors.\nVector Symbolic Architectures and Complex Structure Methodology\nThe procedure for encoding complex structure deserves further comment with respect to VSAs in general, and Holographic Reduced Representations in particular.\nFor VSAs, Context Dependent Thinning will map similar structures to similar vectors, as required by Constraint 4. Other VSA methods can run into problems with this constraint: there is no vector similarity between V*W and V*W*X. For example, (smart * girl), and (smart * Brazilian * girl) have no similarity.\nAnother troublesome case for all VSAs is representing repeated objects whenever binding is used for phrases. We might like (tall * boy), (very * tall * boy), and (very * very * tall * boy) to each be different, yet with appropriate similarity between pairs of phrases. However HRRs give no relation between pairs of phrases, and BSC gives no difference between phrases 1 and 3 (unless various workarounds are employed).\nThe issue is whether to represent phrases by binding the terms directly, (V*W*X), or by binding their sum, #(V+W+X), as in MBAT.\nNote that in HRRs we can convert a two-argument * operator to a unary * operator by\n*(V+W+X) ≡ dummy * (V+W+X).\n(There are other ways to create unary operators from two-argument operators.)\nGallant and Okaywe: Representing Objects, Relations and Sequences 30\nWhen included in representations, such additive phrases permit HRRs to map similar phrases to similar vectors, thereby satisfying Constraint 4 in Section 2. We now have\n*(smart + Brazilian + girl)\nsimilar to\n*(Brazilian + girl),\nwhich was not the case with (smart*Brazilian*girl) and (Brazilian*girl). Computation is also reduced, because * requires more work than computing vector addition.\nMoreover, it is also much easier to recognize whether V girl is in *(smart + Brazilian + girl) than it is to recognize whether it is in (smart * Brazilian * girl), because we can take a single dot product, similar to equations 3.2 – 3.4.\nThus employing an additive phrase as the argument for a unary binding operator would appear beneficial for HRR representations (and also possibly Socher’s models). Even better, we can combine an additive phrase vector with an HRR binding (rather than replacing it) as in\n(V*W*X) + dummy*(V+W+X).\nThis is an example of multiple simultaneous representations.\nFinally, it can be seen that HRR binding of sums, as in dummy*(V+W+X), corresponds precisely to MBAT bindings, where the MBAT binding matrix is restricted to matrices having each row other than the first be a single rotation of the preceding row.\nJackendoff’s Challenges\nThese complex structure issues connect to Jackendoff’s Challenges to Cognitive Neuroscience [2002] and to Gayler’s response [2003]. Jackendoff issued four challenges for Cognitive Neuroscience, of which Challenge Two, “the problem of 2”, involves multiple instances of the same token in a sentence, for example “the little star” and “the big star”. We want to keep the two stars distinct, while maintaining a partial similarity (both are stars). M Actor (the + little + star) and M Object (the + big + star) are different, yet both are similar to (M Actor + M Object ) star, as desired.\nFurther, using additive phrases, we can encode the original example discussed by Jackendoff, “The little star’s beside a big star” by the vector\nM Actor (the + little + star) + M Verb ( ‘s ) + M Relation (beside + the + big + star)\nand similarly for HRRs using additive terms.\nApplications\nFor applications of MBAT, each modeling environment will require that we select the appropriate Preprocessing Stage details to take advantage of specific characteristics. For example with language, we need to select a method for making similar terms have similar vectors, decide which\nGallant and Okaywe: Representing Objects, Relations and Sequences 31\nchunking, tagger (etc.) software is available for specifying structure information, and decide which binding operator variation to employ.\nSimilarly, for machine vision, we need to see what are the available feature detectors and what higher-level structure information is available.\nHowever once these details are specified, we can create representations using MBAT, and then use standard machine learning algorithms directly “out of the box” to learn tasks of interest.\nWe believe that MBAT provides a practical playing field where machine learning can efficiently operate upon objects, their structures, and sequences all at once – as either inputs or outputs.\nLet us briefly look at possible applications.\n For Information Retrieval, representing structure (in addition to terms) may improve performance in various learning tasks, for example finding specific categories of articles\n(eg., “joint ventures where a specific agreement is announced”).\n In natural-language processing, MBAT gives a way to make good use of parse information keyed to sets of phrases as in\nV = M Actor (the + smart + girl + phraseHas3words) + M verb (saw + phraseHas1word) + M Object (the + gray + elephant + phraseHas3words).\nThus we have a direct approach, using existing Taggers, for learning machine translation from paired corpora with paired sentences. For example, we can work from a collection of English sentences with corresponding French translations. (Different dimension vectors can be used for the two languages.) We take the vectors for English and French translations, and then train a classifier to go from the D components of the French sentence vector to the first component of the English vector. Similarly for the other components of the English vector, resulting in D classifiers in total. The net result is a map from French words and parse structure to English words and parse structure. Whether this would work well, or whether it would not work at all, would need to be explored in an implementation.\nA potential difficulty with translation is that it may be challenging to construct an output module that goes from a vector to a corresponding string of terms. For this task, we need to recover the sentence, rather than recognize the components of a sentence encoded by a vector. Here it is likely that embedding Tags into phrases (eg., phraseHas3words) will help with output. Nevertheless, constructing a vector-to-sentence module is a critical – and likely difficult – task for translation (or summarization) when using vector representations.\nThere are other potential applications that share similarities with representing natural language, including representing genome sequences and chemical structures.\n Another application area is computer vision. Here a natural problem seems to be recognizing whether an image contains a specific object. Such tasks likely require\nmultiple representations of structures at different scales in the image. This suggests combining multiple feature detectors (working on different scale sizes), and employing different binding operators (M close_to , M above , etc) to end up with sums of terms such as:\nGallant and Okaywe: Representing Objects, Relations and Sequences 32\nMclose_to (location + shape_1 + shape_2) and\nM above (location + shape_1 + M shape_2), etc.\nAs with natural language, our representation only gives a research path to generating a practical system, but does not totally solve the problem. A creative implementation is still required.\n A final application is neural modeling. In particular, we want to capture the computational essence of neural information processing at a useful level of mathematical\nabstraction.\nOf course the brain does not have complete recurrent connections, where every neuron is connected to every other. (In other words, binding matrices contain many zero terms, which doesn’t fundamentally change the analysis.) Specialized brain sub-structures and many other details are also ignored in our abstract model.\nNevertheless, the MBAT computational architecture suggests a number of computational explanations for large-scale aspects of neural organization.\nThe most important example is that the binding operation suggests a plausible computational reason for the brain having so many recurrent connections. (A neuron has an estimated average of 7,000 synaptic connections to other neurons.)\nA second, and more subtle, computation-based explanation is for an aspect of neural organization currently taken completely for granted: the need for a separate memory mechanism. In other words, why not have a unified “whole brain” that simply remembers everything? The computational explanation is that, with the representation we have developed, objects/structures are expensive to store, because the number of required vector components rises linearly with the number of stored items. Also, recovery – as opposed to recognition – of objects is not directly given by the representation we have explored. Hence the need for a specialized memory functionality, separate from general binding, that efficiently stores large numbers of objects and that facilitates recovery of stored vectors.\nFinally, the MBAT architecture can motivate Cognitive Science hypotheses. For example, we can hypothesize that there are neural resources devoted to recognizing phrases in language at an early processing stage. This hypothesis is supported by the computational benefits we have seen, as well as by the help given for phrase recognition in both written and spoken language: punctuation, small sets of prepositions and conjunctions, typical word ordering for phrases, pauses in speaking, tone patterns, coordinated body language, etc. Recent Magnetoencephalography studies by Bemis & Pylkkänen [2011] give additional support. 5\nFor some applications, VSAs in general will need to be augmented by several very important missing pieces. These revolve around learning (including long term and short term memory), storage/recall of vectors, novelty recognition and filtering, focus of attention, and dealing with\n5 Localized neural response for minimal phrases (“red boat”) occurs not in traditional language areas, but instead first in areas associated with syntax, followed by areas associated with semantic processing.\nGallant and Okaywe: Representing Objects, Relations and Sequences 33\nlarge sequences or structured large objects (reading a book). Is there a good extension of MBAT that will properly accommodate these functions?\nWe leave such Representation Theory development, as well as construction of practical systems in natural language, computer vision, and other areas, to future work.\nAcknowledgments\nThanks to Tony Plate, Pentti Kanerva, Ron Rivest, Charles Elkan, Bill Woods, and Bob Cassels for useful discussions and suggestions on an early draft, and to the Referees for extremely helpful comments.\nGallant and Okaywe: Representing Objects, Relations and Sequences 34\nReferences\nAnderson, James A. (1973). A theory for the recognition of items from short memorized lists. Psychological Review 80(6):417-438.\nAnderson, James A. and Jack W. Silverstein and Stephen A. Ritz and Randall S. Jones. (1977) Distinctive features, categorical perception, and probability learning: some applications of a neural model, Psychological Review, vol 84, pg. 413—451\nBemis, D. K., & Pylkkänen, L. (2011). Simple Composition: A Magnetoencephalography investigation into the Comprehension of Minimal Linguistic Phrases. Journal of Neuroscience 31(8), 2801-2814.\nBlock, H. D., S. A. Levin (1970). On the boundedness of an iterative procedure for solving a system of linear inequalities. Proc. Amer. Math. Soc. 26 229–235.\nBrown, Peter F., John Cocke, Stephen A. Della Pietra, Vincent J. Della Pietra, Fredrick Jelinek, John D. Lafferty, Robert L. Mercer, and Paul S. Roossin (1990). A Statistical Approach To Machine Translation. Computational Linguistics Volume 16, Number 2, June 1990.\nBrown, Peter F., P. V. deSouza, R. L. Mercer, V. J. D. Pietra, and J C. Lai. (1992). Class-based n-gram models of natural language. Computational Linguistics, 18(4):467–479.\nCaid WR, Dumais ST and Gallant SI. (1995) Learned vector-space models for document retrieval. Information Processing and Management, Vol. 31, No. 3, pp. 419-429.\nCollobert, Ronan and Jason Weston. (2008). A unified architecture for natural language processing: deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning (ICML '08). ACM, New York, NY, USA, 160-167.\nCollobert, Ronan, Jason Weston, Leon Bottou, Michael Karlen, Koray Kavukcuoglu and Pavel Kuksa. (2011) Natural Language Processing (Almost) from Scratch. Journal of Machine Learning Research 12 (2011) 2461-2505.\nCover, T. M. (1965). Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition. IEEE Transactions on Electronic Computers, 14, 326-334.\nDeerwester, S., Dumais, S. T., Landauer, T. K., Furnas, G. W. and Harshman, R. A. (1990) Indexing by latent semantic analysis. Journal of the Society for Information Science, 41(6), 391-407.\nElman, J.L. (1990). \"Finding Structure in Time\". Cognitive Science 14 (2): 179–211.\nFrasconi, Paolo, Marco Gori, Alessandro Sperduti (1998). A general framework for adaptive processing of data structures. IEEE Transactions on Neural Networks (TNN) 9(5):768-786Frasconi, Paolo, Marco Gori,\nGallant, S. I. (1993) Neural Network Learning and Expert Systems. M.I.T. Press. (ISBN 0-262-07145-2).\nGallant, S. I. and Smith, D. (1987). \"Random Cells: An Idea Whose Time Has Come and Gone ... and Come Again?\" Proceedings of the IEEE International Conference on Neural Networks (San Diego 1987), vol. II, 671-678.\nGayler, R. W. (1998). Multiplicative binding, representation operators, and analogy [Abstract of poster]. In K. Holyoak, D. Gentner & B. Kokinov (Eds.), Advances in analogy research: Integration of theory and data from the cognitive, computational, and neural sciences. Sofia, Bulgaria: New Bulgarian University.\nGallant and Okaywe: Representing Objects, Relations and Sequences 35\nGayler, R.W. (2003). Vector Symbolic Architectures answer Jackendoff’s challenges for cognitive neuroscience. In Peter Slezak (Ed.), ICCS/ASCS International Conference on Cognitive Science (pp. 133- 138). Sydney, Australia: University of New South Wales.\nHagenbuchner , M., A. Sperduti, and A. C. Tsoi. 2009. Graph self-organizing maps for cyclic and unbounded graphs. Neurocomputing. 72, 7-9 (March 2009), 1419-1430.\nHinton, G. E. (1981). Implementing semantic networks in parallel hardware. In G. E. Hinton and J. A. Anderson, eds., Parallel Models of Associative Memory. Hillsdale, NJ: Erlbaum.\nHinton, G. E. (1984). Distributed Representations. Technical Report CMU-CS-84-157, Carnegie-Mellon University, Department of Computer Science. Revised version in Rumelhart, David E. and James L. McClelland (Eds). Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 1: Foundations. MIT Press, Cambridge, MA, USA. (1986)\nHinton, G. E. (1986). Learning distributed representations of concepts. In Proceedings of the Eighth Annual Conference of the Cognitive Science Society, pages 1–12. Erlbaum, NJ.\nHinton, G. E. (1990) Mapping Part-Whole Hierarchies into Connectionist Networks. Artificial Intelligence 46 (1990) 47-75.\nHuang, F. and A. Yates. (2009) Distributional representations for handling sparsity in supervised sequence labeling. In Meeting of the Association for Computational Linguistics (ACL), pages 495–503.\nJackendoff, R. (2002). Foundations of language: Brain, meaning, grammar, evolution. Oxford, UK: Oxford University Press.\nJones, M.N., Mewhort, D.J.K. (2007) Representing word meaning and order information in a composite holographic lexicon. Psychological Review 114\nKanerva, P. (1988) Sparse Distributed Memory. Cambridge, MA: MIT Press.\nKanerva, P. (1994). The binary spatter code for encoding concepts at many levels. In Marinaro, M., Morasso, P., eds.: ICANN ’94: Proceedings of International Conference on Artificial Neural Networks. Volume 1., London, Springer-Verlag 226–229\nKanerva, P. (1997). Fully distributed representation. In Real World Computing Symposium (RWC97).\nKanerva, Pentti. (2009) Hyperdimensional Computing: An Introduction to Computing in Distributed Representation with High-Dimensional Random Vectors. Cogn Comput (2009) 1:139–159.\nKelly, M.A. (2010). Advancing the theory and utility of holographic reduced representations. Master's thesis. School of Computing, Queens University Kingston, Ontario, Canada.\nKelly, M.A., Blostein, D., and Mewhort, D.J.K. (in press). Encoding structure in holographic reduced representations. Canadian Journal of Experimental Psychology.\nKohonen, T. (1977) Associative memory: A system-theoretical approach, Springer\nKohonen, T. (1995) Self-Organizing Maps, volume 30 of Springer Series in Information Sciences. Springer, Berlin, Heidelberg.\nLevy, S.D., and Gayler, R.W. (2008). Vector Symbolic Architectures: A new building material for Artificial General Intelligence. Proceedings of the First Conference on Artificial General Intelligence (AGI08). IOS Press.\nGallant and Okaywe: Representing Objects, Relations and Sequences 36\nLin, D. and X. Wu. (2009) Phrase clustering for discriminative learning. In Meeting of the Association for Computational Linguistics (ACL), pages 1030–1038.\nMaass, Wolfgang, Thomas Natschläger, Henry Markram. (2002) Real-Time Computing Without Stable States: A New Framework for Neural Computation Based on Perturbations. Neural Computation, November 2002, Vol. 14, No. 11, Pages 2531-2560\nMinsky, Marvin and Seymour Papert. (1969) Perceptrons: An Introduction to Computational Geometry, The MIT Press, Cambridge MA (Second edition in 1972.)\nMitchell, Jeff and Mirella Lapata. (2008) Vector-based Models of Semantic Composition. Proceedings of ACL-08: HLT, pages 236–244,Columbus, Ohio, USA, June 2008.\nOkanohara, D., and Tsujii, J. (2007). A discriminative language model with pseudo-negative samples. Proceedings of the 45 th Annual Meeting of the ACL, 73-80.\nPaccanaro, Alberto (2003). Learning Distributed Representations of High-Arity Relational Data with Nonlinear Relational Embedding. ICANN 2003:149-156\nPaccanaro, Alberto, Geoffrey E. Hinton. (2001a) Learning Hierarchical Structures with Linear Relational Embedding. NIPS 2001:857-864\nPaccanaro, Alberto, Geoffrey E. Hinton. (2001b): Learning Distributed Representations of Concepts Using Linear Relational Embedding. IEEE Trans. Knowl. Data Eng. (TKDE) 13(2):232-244 (2001)\nPlate, TA (1992) Holographic Recurrent Networks. In Giles, Hanson, and Cowens, Ed., Advances in Neural Information Processing Systems 5 (NIPS 1992). Morgan Kaufmann, San Mateo, Ca.\nPlate, TA (2003). Holographic Reduced Representation: Distributed representation of cognitive structure. Stanford: CSLI Publications\nPollack, J. (1990). Recursive distributed representations. Artificial Intelligence, 46, 77-105.\nRachkovskij, Dmitri A. and Ernst M. Kussul (2001). Binding and Normalization of Binary Sparse Distributed Representations by Context-Dependent Thinning Neural Computation 13:2, 411-452\nRecchia, G.L., Jones, M.N., Sahlgren, M. and Kanerva, P. (2010). Encoding sequential information in vector space models of semantics: Comparing holographic reduced representation and random permutation. In S. Ohisson & R. Catrambone (eds.), Proc. 32nd Annual Cognitive Science Society, pp.865-870. Austin, TX: Cognitive Science Society.\nRosenblatt, Frank (1959). Two theorems of statistical separability in the perceptron, Paper 1-3, Procs. Symposium on the Mechanization of Thought, National Physical Laboratory, Teddington, UK, November 1958, Vol I, H.M. Stationery Office, London, 1959.\nRudolph, S., and Giesbrecht, E. (2010). Compositional Matrix-Space Models of Language. Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (pp. 907-916). Association for Computational Linguistics.\nSahlgren, M., Holst, A., and Kanerva. P. (2008). Permutations as a means to encode order in word space. Proc. 30th Annual Conference of the Cognitive Science Society, pp.1300-1305. Austin, TX: Cognitive Science Society.\nSalton, G. and McGill, M.J. (1983) Introduction to Modern Information Retrieval. McGraw-Hill.\nGallant and Okaywe: Representing Objects, Relations and Sequences 37\nSmolensky, P. (1990) Tensor product variable binding and the representation of symbolic structures in connectionist systems. Artificial Intelligence 46 159–216\nSocher, Richard, Christopher D. Manning, Andrew Y. Ng. (2010) Learning Continuous Phrase Representations and Syntactic Parsing with Recursive Neural Networks. Deep Learning and Unsupervised Feature Learning Workshop, NIPS 2010.\nSocher, Richard, Cliff Lin, Andrew Y. Ng, and Christopher D. Manning. (2011a) Parsing Natural Scenes and Natural Language with Recursive Neural Networks. The 28th International Conference on Machine Learning, ICML 2011.\nSocher, Richard, Jeffrey Pennington, Eric Huang, Andrew Y. Ng, and Christopher D. Manning. (2011b) Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions. Conference on Empirical Methods in Natural Language Processing, EMNLP 2011.\nSperduti, A., Starita, A., and Goller, C. (1995). Learning distributed representations for the classification of terms. In Proceedings of the international joint conference on artificial intelligence.\nSperduti, Alessandro, Ro Sperduti, Antonina Starita (1997). Supervised neural networks for the classification of structures. IEEE Transactions on Neural Networks, 8 (3), 714-735;\nSperduti, Alessandro. (1997) A general framework for adaptive processing of data structures. Technical Report DSI-RT-15/97, Universita degli Studi di Firenze, Dipartimento di Sistemi e Informatica.\nSperduti, Alessandro. (2007) Efficient Computation of Recursive Principal Component Analysis for Structured Input. ECML 2007:335-346\nTreisman, Anne (1999) Solutions to the Binding Problem: Progress through Controversy and Convergence. Neuron, Vol. 24, 105–110, September, 1999.\nVapnik, V. N. and A. Ya. Chervonenkis (1971). On the Uniform Convergence of Relative Frequencies of Events to Their Probabilities. Theory Probab. Appl. 16, pp. 264-280.\nVoegtlin, Thomas and Peter F. Dominey. 2005. Linear recursive distributed representations. Neural Netw. 18, 7 (September 2005), 878-895.\nWaibel, A.; Hanazawa, T.; Hinton, G.; Shikano, K.; Lang, K.J. (1989). Phoneme recognition using time-delay neural networks. IEEE Transactions on Acoustics, Speech and Signal Processing, 37(3):328- 339.\nGallant and Okaywe: Representing Objects, Relations and Sequences 38"
    }, {
      "heading" : "Appendix: Capacity for Vector Sums",
      "text" : "It is possible to derive good approximation formulas for storage and recognition performance within a single distributed vector, as we show below.\nPrevious approximations appear in the Appendices in Plate’s book [2003] and Anderson [1973], both of which look at vector components drawn from normal distributions rather than +1/-1. Their findings for normally distributed components agree with Propositions 1-3 below.\nWe use the following notation, assumptions, and simplifications:\n D = dimension of vectors\n S = number of vectors bundled together to form vector V\n N = number of randomly generated vectors that we wish to distinguish from those used in the sum forming V\n T (x) = one-sided tail probability in a normal distribution of a random variable being at least x standard deviations from the mean\n Z = √ (D/(2S-1)) for fixed D and S\n All object vectors are randomly generated +1/-1 vectors.\n For simplicity, we do not include continuous vectors produced by binding operations (vectors formed by random matrix multiplications). We could, however, include such\nvectors if we’re thresholding matrix multiplication results to obtain +1/-1 vectors.\nThe first thing to note is that the dot product of two random vectors has mean 0 and variance D (and hence standard deviation √D), because it is the sum of D independent random variables, each with mean=0 and variance=1.\nSimilarly, when we add S vectors to form vector V, then for a random vector R, we have R • V\nalso has mean 0 and variance SD, giving standard deviation √(SD).\nLet A be a randomly chosen vector from the S vectors Added (bundled) to form V, and R a Random vector.\nWe’re interested in the probability of a mistaken recognition where R • V > A • V.\nAs in the proof of Property 1 for vector addition,\nR • V = 0 + <mean 0 noise from the dot product with a sum of S vectors>, and\nA • V = D + <mean 0 noise from the dot product with a sum of S-1 vectors>.\nGallant and Okaywe: Representing Objects, Relations and Sequences 39\nFor D large enough, the Central Limit Theorem of statistics guarantees that the first noise term will be closely approximated by a normal distribution with mean 0 and standard deviation √(SD), denoted N(0, √(SD)). Similarly, the second noise term is closely approximated by N(0, √((S1)D))\nSo the probability of a mistake with these two vectors is given by:\nthe probability of a random value selected from N(0, √(SD)) being greater than a random value selected from N(D, √((S-1)D).\nThis is equivalent to the probability of a random vector being negative when selected from N(D, √((2S-1)D), because the difference of two normal distributions, X-Y, is a normal distribution having mean equal to the difference of the two means, and variance equal to the sum of the two variances. [Many thanks to the Referee who pointed this out.]\nProof: From basic definitions, if Y is normally distributed then so is (-Y), with mean(-Y) = -mean(Y) and with Var(-Y) = Var(Y). The result now follows from well-known properties for the sum of two normal variables, applied to X and (-Y).\nThus, looking at standard deviations, an error occurs when a difference is in the tail probability at least D / √ ((2S-1)D) = √ (D/(2S-1)) standard deviations from the mean.\nHere it is convenient to introduce some simplifying notation. We define:\nZ = √ (D/(2S-1)) for fixed D and S.\nThus, for pre-specified D and S, we have Z corresponding to D as measured in standard deviations of the difference in noise terms.\nWe also adopt the notation T (x) = one-sided tail probability of a random variable being at least x standard deviations from the mean in a normal distribution.\nThus an estimate for the error with the pair of randomly chosen vectors, one from the bundled vectors and some other random vector is\n(*) T (Z) = T (√ (D/(2S-1))).\nNow we only need to note that the tail probabilities of a normal distribution decrease exponentially (ie., as e -x ) with the number of standard deviations to conclude:\nProposition 1: For a fixed number, S, of random +1/-1 vectors bundled together to get vector V, the probability of a random vector having greater dot product with V than a randomly selected vector in the sum decreases exponentially with vector dimension, D.\nproof: T (√ (D/(2S-1))) decreases exponentially as D increases.\nWe are really interested in the probability of all S of the vectors in the sum having greater dot product with V than any of N random vectors. This probability is given by\n[ 1 - T (Z) ]NS ≈ 1 – NS T (Z)\nGallant and Okaywe: Representing Objects, Relations and Sequences 40\nassuming NS T (Z) is close to 0, and dropping higher-order terms. Thus we have\nProposition 2: For a fixed number, S, of random +1/-1 vectors bundled together to get vector V, if we generate N additional random vectors, then the probability that each random vector has less dot product with V than each vector in the sum equals 1 minus an exponentially decreasing probability as vector dimension, D increases.\nIn other words, as D increases, the probability of less than error-free discrimination between S vectors in the bundle and N random vectors decreases exponentially.\nProposition 3: For a fixed number, N, of random +1/-1 vectors, the number of vectors, S, in a bundle that can be perfectly discriminated against with constant probability increases nearly linearly with vector dimension D. (More precisely, the required D increases linearly with S, plus second order terms.)\nIn summary, as D increases, the number of random vectors we can discriminate from a sum of S vectors increases exponentially, and the number of vectors we can have in the sum while maintaining discriminatory power increases slightly less than linearly with respect to D.\nIt is instructive to compute several of these bounds.\nFor a “Small” system where we sum S=20 vectors (ie., terms, Tags, image primitives, etc.) to form V, and we have N = 1,000 additional random vectors, we compute\nProbability of error = (20) (1,000) [T (√ (D/(2S-1)))].\nFor the term in brackets, the tail probability which is 4.8 standard deviations from the mean is 1/1,259,000 which gives a probability of error of about 1.6%.\nFor 4.8 standard deviations, we need D large enough to satisfy\n4.8 = Z = √ (D/(2S-1)), or D = 4.8 2 * 39 = 898.56.\nThus:\n 98.4% of the time, a vector dimension of D = 899 will give error-free discrimination between 20 vectors in the sum and 1,000 additional random vectors.\n Similarly, we can consider a “Medium” sized system with up to 100 vectors summed together, and with 100,000 other random vectors. Here a 5.9 standard deviation value for\nD is required for 1.8% probability of error, which works out to a bound on required vector dimension of D = 6,927. (Details omitted.)\n Finally, we can consider a “Large” sized system with up to 1,000 vectors summed together, and with 1,000,000 other random vectors. Here a 6.7 standard deviation value\nfor D is required for 1% probability of error, which works out to a bound on required vector dimension of D = 90,000.\nBy comparison, Plate gives simulation results [Figure 56] that for 99% error free performance in a small system with 14 bundled vectors and 1,000 additional random vectors, the required\nGallant and Okaywe: Representing Objects, Relations and Sequences 41\ndimension D is between 850 and 900, which is comparable to our simulations with +1/-1 components.\nHe also derives a bound on required dimensionality where vector components are normally distributed. Letting q be the probability of error, Plate derives:\nD < 8(S + 1) ln(N/q)\nprovided\nD > 2(S + 1) / π.\nThese bounds are consistent with Propositions 1-3.\nFor “Small”, “Medium”, and “Large” systems, Plate’s bound for 1% probability of error yields required dimensions of 2,000; 13,000; and 148,000 respectively. Thus these bounds are not quite as tight as those derived above, or possibly systems with continuous vector components require greater dimensions than systems with binary components."
    } ],
    "references" : [ {
      "title" : "A theory for the recognition of items from short memorized lists. Psychological Review 80(6):417-438",
      "author" : [ "Anderson", "James A" ],
      "venue" : "Psychological Review,",
      "citeRegEx" : "Anderson and A.,? \\Q1973\\E",
      "shortCiteRegEx" : "Anderson and A.",
      "year" : 1973
    }, {
      "title" : "Simple Composition: A Magnetoencephalography investigation into the Comprehension of Minimal Linguistic Phrases",
      "author" : [ "D.K. Bemis", "L. Pylkkänen" ],
      "venue" : "Journal of Neuroscience",
      "citeRegEx" : "Bemis and Pylkkänen,? \\Q2011\\E",
      "shortCiteRegEx" : "Bemis and Pylkkänen",
      "year" : 2011
    }, {
      "title" : "A Statistical Approach To Machine Translation",
      "author" : [ "Lafferty", "Robert L. Mercer", "Paul S. Roossin" ],
      "venue" : "Computational Linguistics Volume 16, Number",
      "citeRegEx" : "Lafferty et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "Lafferty et al\\.",
      "year" : 1990
    }, {
      "title" : "A unified architecture for natural language processing: deep neural networks with multitask learning",
      "author" : [ "Collobert", "Ronan", "Jason Weston" ],
      "venue" : "Information Processing and Management,",
      "citeRegEx" : "Collobert et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2008
    }, {
      "title" : "Natural Language Processing (Almost) from Scratch",
      "author" : [ "M. T" ],
      "venue" : "Journal of Machine Learning Research",
      "citeRegEx" : "T.,? \\Q2011\\E",
      "shortCiteRegEx" : "T.",
      "year" : 2011
    }, {
      "title" : "Finding Structure in Time",
      "author" : [ "J.L. Elman" ],
      "venue" : "Cognitive Science",
      "citeRegEx" : "Elman,? \\Q1990\\E",
      "shortCiteRegEx" : "Elman",
      "year" : 1990
    }, {
      "title" : "Random Cells: An Idea Whose Time Has Come and Gone ... and Come Again?",
      "author" : [ "S.I. Gallant", "D. Smith" ],
      "venue" : "Proceedings of the IEEE International Conference on Neural Networks (San Diego 1987),",
      "citeRegEx" : "Gallant and Smith,? \\Q1987\\E",
      "shortCiteRegEx" : "Gallant and Smith",
      "year" : 1987
    }, {
      "title" : "Vector Symbolic Architectures answer Jackendoff’s challenges for cognitive neuroscience. In Peter Slezak (Ed.), ICCS/ASCS International Conference on Cognitive Science (pp. 133138). Sydney, Australia: University of New South Wales",
      "author" : [ "R.W. Gayler" ],
      "venue" : null,
      "citeRegEx" : "Gayler,? \\Q2003\\E",
      "shortCiteRegEx" : "Gayler",
      "year" : 2003
    }, {
      "title" : "Implementing semantic networks",
      "author" : [ "G.E. Hinton" ],
      "venue" : "Parallel Models of Associative Memory. Hillsdale,",
      "citeRegEx" : "Hinton,? \\Q1981\\E",
      "shortCiteRegEx" : "Hinton",
      "year" : 1981
    }, {
      "title" : "Distributional representations for handling sparsity in supervised sequence labeling. In Meeting of the Association for Computational Linguistics (ACL), pages 495–503",
      "author" : [ "F. Huang", "A. Yates" ],
      "venue" : "Jackendoff, R",
      "citeRegEx" : "Huang and Yates.,? \\Q2009\\E",
      "shortCiteRegEx" : "Huang and Yates.",
      "year" : 2009
    }, {
      "title" : "Fully distributed representation",
      "author" : [ "P. Kanerva" ],
      "venue" : "In Real World Computing Symposium (RWC97). Kanerva, Pentti",
      "citeRegEx" : "Kanerva,? \\Q1997\\E",
      "shortCiteRegEx" : "Kanerva",
      "year" : 1997
    }, {
      "title" : "Encoding structure in holographic reduced representations",
      "author" : [ "M.A. Kelly", "D. Blostein", "D.J.K. Mewhort" ],
      "venue" : "Canadian Journal of Experimental Psychology. Kohonen, T",
      "citeRegEx" : "Kelly et al\\.,? \\Q1977\\E",
      "shortCiteRegEx" : "Kelly et al\\.",
      "year" : 1977
    }, {
      "title" : "Vector Symbolic Architectures: A new building material for Artificial General Intelligence",
      "author" : [ "S.D. Levy", "R.W. Gayler" ],
      "venue" : "Proceedings of the First Conference on Artificial General Intelligence",
      "citeRegEx" : "Levy and Gayler,? \\Q2008\\E",
      "shortCiteRegEx" : "Levy and Gayler",
      "year" : 2008
    }, {
      "title" : "Phrase clustering for discriminative learning. In Meeting of the Association for Computational Linguistics (ACL), pages 1030–1038",
      "author" : [ "D. Lin", "X. Wu" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Lin and Wu.,? \\Q2009\\E",
      "shortCiteRegEx" : "Lin and Wu.",
      "year" : 2009
    }, {
      "title" : "Perceptrons: An Introduction to Computational Geometry, The MIT Press, Cambridge MA (Second edition",
      "author" : [ "Minsky", "Marvin", "Seymour Papert" ],
      "venue" : "Proceedings of ACL-08: HLT, pages 236–244,Columbus, Ohio,",
      "citeRegEx" : "Minsky et al\\.,? \\Q1969\\E",
      "shortCiteRegEx" : "Minsky et al\\.",
      "year" : 1969
    }, {
      "title" : "Learning Distributed Representations of High-Arity Relational Data with Nonlinear Relational Embedding",
      "author" : [ "Paccanaro", "Alberto", "Geoffrey E. Hinton" ],
      "venue" : "Annual Meeting of the ACL,",
      "citeRegEx" : "Paccanaro et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Paccanaro et al\\.",
      "year" : 2003
    }, {
      "title" : "Learning Distributed Representations of Concepts Using Linear Relational Embedding",
      "author" : [ "Paccanaro", "Alberto", "Geoffrey E. Hinton" ],
      "venue" : "IEEE Trans. Knowl. Data Eng. (TKDE)",
      "citeRegEx" : "Paccanaro et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Paccanaro et al\\.",
      "year" : 2001
    }, {
      "title" : "Recursive distributed representations",
      "author" : [ ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Pollack,? \\Q1990\\E",
      "shortCiteRegEx" : "Pollack",
      "year" : 1990
    }, {
      "title" : "Compositional Matrix-Space Models of Language",
      "author" : [ "Vol I", "H.M. Stationery Office", "London", "S. 1959. Rudolph", "E. Giesbrecht" ],
      "venue" : "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (pp. 907-916)",
      "citeRegEx" : "1958 et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "1958 et al\\.",
      "year" : 2010
    }, {
      "title" : "Tensor product variable binding and the representation of symbolic structures in connectionist systems",
      "author" : [ "P. Smolensky" ],
      "venue" : "Artificial Intelligence",
      "citeRegEx" : "Smolensky,? \\Q1990\\E",
      "shortCiteRegEx" : "Smolensky",
      "year" : 1990
    }, {
      "title" : "Parsing Natural Scenes and Natural Language with Recursive Neural Networks",
      "author" : [ "Socher", "Richard", "Cliff Lin", "Andrew Y. Ng", "Christopher D. Manning" ],
      "venue" : "The 28th International Conference on Machine Learning,",
      "citeRegEx" : "Socher et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2011
    }, {
      "title" : "Learning distributed representations for the classification of terms",
      "author" : [ "A. Sperduti", "A. Starita", "C. Goller" ],
      "venue" : "Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Sperduti et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Sperduti et al\\.",
      "year" : 2011
    }, {
      "title" : "A general framework for adaptive processing of data structures",
      "author" : [ "Sperduti", "Alessandro" ],
      "venue" : "Technical Report DSI-RT-15/97, Universita degli Studi di Firenze, Dipartimento di Sistemi e Informatica",
      "citeRegEx" : "Sperduti and Alessandro.,? \\Q1997\\E",
      "shortCiteRegEx" : "Sperduti and Alessandro.",
      "year" : 1997
    }, {
      "title" : "On the Uniform Convergence of Relative Frequencies of Events to Their Probabilities",
      "author" : [ "V.N. Vapnik", "A. Ya" ],
      "venue" : "Neuron,",
      "citeRegEx" : "Vapnik and Ya.,? \\Q1999\\E",
      "shortCiteRegEx" : "Vapnik and Ya.",
      "year" : 1999
    }, {
      "title" : "Phoneme recognition using time-delay neural networks",
      "author" : [ "A. Waibel", "T. Hanazawa", "G. Hinton", "K. Shikano", "K.J. Lang" ],
      "venue" : "IEEE Transactions on Acoustics, Speech and Signal Processing,",
      "citeRegEx" : "Waibel et al\\.,? \\Q1989\\E",
      "shortCiteRegEx" : "Waibel et al\\.",
      "year" : 1989
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "and Okaywe, T. W. (2013). Representing Objects, Relations, and Sequences.",
      "startOffset" : 12,
      "endOffset" : 25
    }, {
      "referenceID" : 3,
      "context" : "For example, Collobert et al. [2011] produce a system that outputs structure information (part of speech, chunks, semantic roles) for each word in a sentence.",
      "startOffset" : 13,
      "endOffset" : 37
    }, {
      "referenceID" : 16,
      "context" : "Vector Symbolic Architectures trace their origins to Smolensky’s [1990] tensor product models, but avoid the exponential growth in vector size of those models.",
      "startOffset" : 53,
      "endOffset" : 72
    }, {
      "referenceID" : 4,
      "context" : "VSAs include Kanerva’s Binary Spatter Codes (BSC) [1994, 1997], Plate’s Holographic Reduced Representations (HRR) [1992, 2003], Rachkovskij and Kussul’s Context Dependent Thinning (CDT) [2001], and Gayler’s Multiply-Add-Permute coding (MAP) [1998].",
      "startOffset" : 171,
      "endOffset" : 193
    }, {
      "referenceID" : 4,
      "context" : "VSAs include Kanerva’s Binary Spatter Codes (BSC) [1994, 1997], Plate’s Holographic Reduced Representations (HRR) [1992, 2003], Rachkovskij and Kussul’s Context Dependent Thinning (CDT) [2001], and Gayler’s Multiply-Add-Permute coding (MAP) [1998].",
      "startOffset" : 171,
      "endOffset" : 248
    }, {
      "referenceID" : 4,
      "context" : "Here we encounter the “Binding Encoding Problem” in Cognitive Science and Artificial Intelligence surveyed by Treisman [1999]: for the word pair “smart girl”, we need to represent that “smart” refers to “girl”, and not some other word in the sentence.",
      "startOffset" : 110,
      "endOffset" : 126
    }, {
      "referenceID" : 4,
      "context" : "Due to the commutativity of vector addition, multiple phrases such as in “The smart girl saw the gray elephant” will have exactly the same vector sum as “The smart elephant saw the gray girl” or even “elephant girl gray saw smart the the”. In other words, vector addition gives us the “bag of words” used to create the sum, but no other structure information. Here we run into the classic “Binding Encoding Problem” in Cognitive Science and Artificial Intelligence, surveyed by Treisman [1999]. We need some way to bind “gray” to “elephant” and not to “girl” or to any other word, while retaining a distributed representation.",
      "startOffset" : 74,
      "endOffset" : 494
    }, {
      "referenceID" : 4,
      "context" : " This formula for computing the next state also gives a way to represent input sequences. Kanerva [2009] and Plate [2003] previously employed this technique for sequence coding, using different binding operators.",
      "startOffset" : 2,
      "endOffset" : 106
    }, {
      "referenceID" : 4,
      "context" : " This formula for computing the next state also gives a way to represent input sequences. Kanerva [2009] and Plate [2003] previously employed this technique for sequence coding, using different binding operators.",
      "startOffset" : 2,
      "endOffset" : 123
    }, {
      "referenceID" : 4,
      "context" : "These include the Perceptron Convergence Theorem [Rosenblatt 1959, see also Minsky & Papert 1969], Perceptron Cycling Theorem [Minsky & Papert 1969, Block & Levin 1970], Cover’s theorem for the likelihood of a set of vectors to be separable [Cover 1965], and Vapnik-Chervonenkis generalization bounds [1971]. This body of theory permits us to prove learnability in many cases, as well as to set bounds on generalization.",
      "startOffset" : 0,
      "endOffset" : 308
    }, {
      "referenceID" : 4,
      "context" : "This paper presents characteristics, advantages and neural plausibility arguments. (These topics are reviewed in Gallant [1993].) In Information Retrieval, the use of high dimensional vectors to represent terms (words) was pioneered by Salton & McGill [1983].",
      "startOffset" : 0,
      "endOffset" : 128
    }, {
      "referenceID" : 4,
      "context" : "This paper presents characteristics, advantages and neural plausibility arguments. (These topics are reviewed in Gallant [1993].) In Information Retrieval, the use of high dimensional vectors to represent terms (words) was pioneered by Salton & McGill [1983]. Deerwester et al.",
      "startOffset" : 0,
      "endOffset" : 259
    }, {
      "referenceID" : 4,
      "context" : "This paper presents characteristics, advantages and neural plausibility arguments. (These topics are reviewed in Gallant [1993].) In Information Retrieval, the use of high dimensional vectors to represent terms (words) was pioneered by Salton & McGill [1983]. Deerwester et al. [1990] represented terms, documents and queries by starting with a document-by-term matrix, and then using Singular Value Decomposition to reduce dimensionality.",
      "startOffset" : 0,
      "endOffset" : 285
    }, {
      "referenceID" : 4,
      "context" : "1990], or computing the probability of a word given its surrounding window [Okanohara & Tsujii 2007, Collobert &Weston 2008]. See also the Brown clustering algorithm [1992], phrase clustering [Lin & Wu, 2009] and [Huang & Yates 2009].",
      "startOffset" : 88,
      "endOffset" : 173
    }, {
      "referenceID" : 4,
      "context" : "Then letting ɸ be a constant placeholder for the target term “King”, we would add HRRs for: “Luther * ɸ”, “ɸ * Jr”, “Luther * ɸ * Jr”, etc. The resulting order vector is then normalized and added to the semantic vector for King. The result is a vector that captures semantics as well as word order syntactic effects. Similar results were obtained by Sahlgren et al. [2008] by encoding order information with permutations; see also Recchia et al.",
      "startOffset" : 0,
      "endOffset" : 373
    }, {
      "referenceID" : 4,
      "context" : "Then letting ɸ be a constant placeholder for the target term “King”, we would add HRRs for: “Luther * ɸ”, “ɸ * Jr”, “Luther * ɸ * Jr”, etc. The resulting order vector is then normalized and added to the semantic vector for King. The result is a vector that captures semantics as well as word order syntactic effects. Similar results were obtained by Sahlgren et al. [2008] by encoding order information with permutations; see also Recchia et al. [2010]. Such vectors should provide interesting starting codings for terms in language systems, including MBAT.",
      "startOffset" : 0,
      "endOffset" : 453
    }, {
      "referenceID" : 7,
      "context" : "With respect to matrix multiplication bindings, Hinton’s “triple memory” system [1981] used random matrix connections in a subsidiary role while focusing on learning, rather than representation.",
      "startOffset" : 48,
      "endOffset" : 87
    }, {
      "referenceID" : 4,
      "context" : "Also, Plate’s book [2003, page 22] later mentions in passing exactly the “Two Input” version of the binding operator from Section 3, which he attributes to Hinton. Plate also lists matrix multiplication as an alternative binding possibility in Section 7.3, Table 26. In a Computational Linguistics setting, Rudolph & Giesbrecht [2010] proposed using only matrices (rather than vectors) to represent objects, and examined matrix multiplication as a composition operation.",
      "startOffset" : 74,
      "endOffset" : 335
    }, {
      "referenceID" : 4,
      "context" : "Also, Plate’s book [2003, page 22] later mentions in passing exactly the “Two Input” version of the binding operator from Section 3, which he attributes to Hinton. Plate also lists matrix multiplication as an alternative binding possibility in Section 7.3, Table 26. In a Computational Linguistics setting, Rudolph & Giesbrecht [2010] proposed using only matrices (rather than vectors) to represent objects, and examined matrix multiplication as a composition operation. Similar results were obtained by Sahlgren et al. [2008] by encoding order information with permutations; see also Recchia et al.",
      "startOffset" : 74,
      "endOffset" : 527
    }, {
      "referenceID" : 4,
      "context" : "Also, Plate’s book [2003, page 22] later mentions in passing exactly the “Two Input” version of the binding operator from Section 3, which he attributes to Hinton. Plate also lists matrix multiplication as an alternative binding possibility in Section 7.3, Table 26. In a Computational Linguistics setting, Rudolph & Giesbrecht [2010] proposed using only matrices (rather than vectors) to represent objects, and examined matrix multiplication as a composition operation. Similar results were obtained by Sahlgren et al. [2008] by encoding order information with permutations; see also Recchia et al. [2010]. However, vector addition carried out by sparse matrices in D 2 dimensions rather than D dimensions is inefficient.",
      "startOffset" : 74,
      "endOffset" : 607
    }, {
      "referenceID" : 4,
      "context" : "Another related sequence approach, Time Delay Neural Networks of Waibel et al. [1989], has several layers of groups of hidden nodes.",
      "startOffset" : 35,
      "endOffset" : 86
    }, {
      "referenceID" : 10,
      "context" : "Gallant and Okaywe: Representing Objects, Relations and Sequences 26 For sequence representations that do not require learning, Kanerva [1988] represents sequences using pointer chains.",
      "startOffset" : 128,
      "endOffset" : 143
    }, {
      "referenceID" : 10,
      "context" : "Gallant and Okaywe: Representing Objects, Relations and Sequences 26 For sequence representations that do not require learning, Kanerva [1988] represents sequences using pointer chains. Later, Plate [2003] employs trajectory association, where the idea is to bind powers of a vector to sequence information.",
      "startOffset" : 128,
      "endOffset" : 206
    }, {
      "referenceID" : 16,
      "context" : "Another early work involving Reduced Representations is Pollack’s RAAM (Recursive Auto Associative Memory) architecture [1990] and later extensions, for example LRAAM (Labeling RAAM) by Sperduti, Starita & Goller [1995].",
      "startOffset" : 56,
      "endOffset" : 127
    }, {
      "referenceID" : 16,
      "context" : "Another early work involving Reduced Representations is Pollack’s RAAM (Recursive Auto Associative Memory) architecture [1990] and later extensions, for example LRAAM (Labeling RAAM) by Sperduti, Starita & Goller [1995]. These approaches use Backpropagation learning (or variants) on a network of inputs, hidden units, and outputs that attempt to reproduce inputs.",
      "startOffset" : 56,
      "endOffset" : 220
    }, {
      "referenceID" : 4,
      "context" : "These approaches use Backpropagation learning (or variants) on a network of inputs, hidden units, and outputs that attempt to reproduce inputs. The hidden units, after learning, encode the reduced representations of the inputs. A drawback of these approaches is the need for learning over all inputs to achieve the representations of the inputs. For example, adding additional input cases requires re-learning the representation for all previous input patterns using Backpropagation (violating the Efficient Coding Constraint). Improvements in capacity and generalization were reported by Voegtlin & Dominey [2005]. Although these approaches are all too slow (non-linear) for the Representation Generation Stage, their abilities to capture generalization may present good synergy as part of the Pre-processing Stage.",
      "startOffset" : 0,
      "endOffset" : 615
    }, {
      "referenceID" : 4,
      "context" : "As with RAAM architectures, generalization ability may prove useful in producing Pre-processing Stage inputs for MBAT or other approaches. Sperduti [1997] proposed a “generalized recursive neuron” architecture for classification of structures.",
      "startOffset" : 116,
      "endOffset" : 155
    }, {
      "referenceID" : 21,
      "context" : "It is worth noting that Sperduti et al. [1995] conduct simulations to show good performance for learning to discriminate presence of particular terms in the representation.",
      "startOffset" : 24,
      "endOffset" : 47
    }, {
      "referenceID" : 3,
      "context" : "In a later work, Collobert et al. [2011] develop a unified neural network architecture for these linguistic tasks, where all tasks are trained using two somewhat complex architectures based upon Time Delay Neural Networks.",
      "startOffset" : 17,
      "endOffset" : 41
    }, {
      "referenceID" : 4,
      "context" : "Socher’s Recursive Neural Networks (RNN) are binary trees with distributed representations, which are structurally identical to “Two Input” binding operators in Section 3. In particular, Socher’s matrix [2011a] for combining two vectors is equivalent to concatenating rows from M Left and M Right to form a single “double wide” matrix for applying to pairs of concatenated column vectors.",
      "startOffset" : 129,
      "endOffset" : 211
    }, {
      "referenceID" : 4,
      "context" : "Finally, the MBAT architecture can motivate Cognitive Science hypotheses. For example, we can hypothesize that there are neural resources devoted to recognizing phrases in language at an early processing stage. This hypothesis is supported by the computational benefits we have seen, as well as by the help given for phrase recognition in both written and spoken language: punctuation, small sets of prepositions and conjunctions, typical word ordering for phrases, pauses in speaking, tone patterns, coordinated body language, etc. Recent Magnetoencephalography studies by Bemis & Pylkkänen [2011] give additional support.",
      "startOffset" : 16,
      "endOffset" : 599
    } ],
    "year" : 2013,
    "abstractText" : "Vector Symbolic Architectures (VSAs) are high-dimensional vector representations of objects (eg., words, image parts), relations (eg., sentence structures), and sequences for use with machine learning algorithms. They consist of a vector addition operator for representing a collection of unordered objects, a Binding operator for associating groups of objects, and a methodology for encoding complex structures. We first develop Constraints that machine learning imposes upon VSAs: for example, similar structures must be represented by similar vectors. The constraints suggest that current VSAs should represent phrases (“The smart Brazilian girl”) by binding sums of terms, in addition to simply binding the terms directly. We show that matrix multiplication can be used as the binding operator for a VSA, and that matrix elements can be chosen at random. A consequence for living systems is that binding is mathematically possible without the need to specify, in advance, precise neuron-to-neuron connection properties for large numbers of synapses. A VSA that incorporates these ideas, MBAT (Matrix Binding of Additive Terms), is described that satisfies all Constraints. With respect to machine learning, for some types of problems appropriate VSA representations permit us to prove learnability, rather than relying on simulations. We also propose dividing machine (and neural) learning and representation into three Stages, with differing roles for learning in each stage. For neural modeling, we give “representational reasons” for nervous systems to have many recurrent connections, as well as for the importance of phrases in language processing. Sizing simulations and analyses suggest that VSAs in general, and MBAT in particular, are ready for real-world applications.",
    "creator" : "Microsoft® Word 2010"
  }
}