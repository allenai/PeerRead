{
  "name" : "1602.00133.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "SCOPE: Scalable Composite Optimization for Learning on Spark",
    "authors" : [ "Shen-Yi Zhao", "Ru Xiang", "Ying-Hao Shi", "Peng Gao", "Wu-Jun Li" ],
    "emails" : [ "zhaosy@lamda.nju.edu.cn,", "xiangr@lamda.nju.edu.cn,", "shiyh@lamda.nju.edu.cn,", "gaop@lamda.nju.edu.cn,", "liwujun@nju.edu.cn" ],
    "sections" : [ {
      "heading" : "Introduction",
      "text" : "Many machine learning models can be formulated as composite optimization problems which have the following form with finite sum of some functions: min\nw∈Rd P (w) =\n1 n ∑n i fi(w), where w is the parameter to learn (optimize), n is the number of training instances, and fi(w) is the loss function on the training instance i. For example, fi(w) = log(1 + e−yix\nT i w) + λ2 ‖w‖ 2 in logistic regression (LR), and fi(w) = max{0, 1 − yixTi w} + λ2 ‖w‖\n2 in support vector machine (SVM), where λ is the regularization hyperparameter and (xi, yi) is the training instance iwith xi ∈ Rd being the feature vector and yi ∈ {+1,−1} being the class label. Other cases like matrix factorization and deep neural networks can also be written as similar forms of composite optimization.\nDue to its efficiency and effectiveness, stochastic optimization (SO) has recently attracted much attention to solve the composite optimization problems in machine learning (Xiao 2009; Bottou 2010; Duchi, Hazan, and Singer 2011; Schmidt, Roux, and Bach 2013; Johnson\nCopyright c© 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nand Zhang 2013; Zhang, Mahdavi, and Jin 2013; ShalevShwartz and Zhang 2013; 2014; Lin, Lu, and Xiao 2014; Nitanda 2014). Existing SO methods can be divided into two categories. The first category is stochastic gradient descent (SGD) and its variants, such as stochastic average gradient (SAG) (Schmidt, Roux, and Bach 2013) and stochastic variance reduced gradient (SVRG) (Johnson and Zhang 2013), which try to perform optimization on the primal problem. The second category, such as stochastic dual coordinate ascent (SDCA) (Shalev-Shwartz and Zhang 2013), tries to perform optimization with the dual formulation. Many advanced SO methods, such as SVRG and SDCA, are more efficient than traditional batch learning methods in both theory and practice for large-scale learning problems.\nMost traditional SO methods are sequential which means that the optimization procedure is not parallelly performed. However, with the increase of data scale, traditional sequential SO methods may not be efficient enough to handle large-scale datasets. Furthermore, in this big data era, many large-scale datasets are distributively stored on a cluster of multiple machines. Traditional sequential SO methods cannot be directly used for these kinds of distributed datasets. To handle large-scale composite optimization problems, researchers have recently proposed several parallel SO (PSO) methods for multi-core systems and distributed SO (DSO) methods for clusters of multiple machines.\nPSO methods perform SO on a single machine with multicores (multi-threads) and a shared memory. Typically, synchronous strategies with locks will be much slower than asynchronous ones. Hence, recent progress of PSO mainly focuses on designing asynchronous or lock-free optimization strategies (Recht et al. 2011; Liu et al. 2014; Hsieh, Yu, and Dhillon 2015; J. Reddi et al. 2015; Zhao and Li 2016).\nDSO methods perform SO on clusters of multiple machines. DSO can be used to handle extremely large problems which are beyond the processing capability of one single machine. In many real applications especially industrial applications, the datasets are typically distributively stored on clusters. Hence, DSO has recently become a hot research topic. Many DSO methods have been proposed, including distributed SGD methods from primal formulation and distributed dual formulation. Representative distributed SGD methods include PSGD (Zinkevich et al. 2010), BAVGM (Zhang, Wainwright, and Duchi 2012)\nar X\niv :1\n60 2.\n00 13\n3v 5\n[ st\nat .M\nL ]\n1 1\nD ec\n2 01\nand Splash (Zhang and Jordan 2015). Representative distributed dual formulations include DisDCA (Yang 2013), CoCoA (Jaggi et al. 2014) and CoCoA+ (Ma et al. 2015). Many of these methods provide nice theoretical proof about convergence and promising empirical evaluations. However, most of these DSO methods might not be scalable enough.\nIn this paper, we propose a novel DSO method, called scalable composite optimization for learning (SCOPE), and implement it on the fault-tolerant distributed platform Spark (Zaharia et al. 2010). SCOPE is both computationefficient and communication-efficient. Empirical results on real datasets show that SCOPE can outperform other stateof-the-art distributed learning methods on Spark, including both batch learning methods and DSO methods, in terms of scalability.\nPlease note that some asynchronous methods or systems, such as Parameter Server (Li et al. 2014), Petuum (Xing et al. 2015) and the methods in (Zhang and Kwok 2014; Zhang, Zheng, and Kwok 2016), have also been proposed for distributed learning with promising performance. But these methods or systems cannot be easily implemented on Spark with the MapReduce programming model which is actually a bulk synchronous parallel (BSP) model. Hence, asynchronous methods are not the focus of this paper. We will leave the design of asynchronous version of SCOPE and the corresponding empirical comparison for future study."
    }, {
      "heading" : "SCOPE",
      "text" : ""
    }, {
      "heading" : "Framework of SCOPE",
      "text" : "SCOPE is based on a master-slave distributed framework, which is illustrated in Figure 1. More specifically, there is a master machine (called Master) and p (p ≥ 1) slave machines (called Workers) in the cluster. These Workers are called Worker 1, Worker 2, · · · , and Worker p, respectively."
    }, {
      "heading" : "Data Partition and Parameter Storage",
      "text" : "• For Workers: The whole dataset D is distributively stored\non all the Workers. More specifically,D is partitioned into p subsets, which are denoted as {D1,D2, · · · ,Dp} with D = ⋃p k=1Dk.Dk is stored on Worker k. The data stored on different Workers are different from each other, which means that if i 6= j, Di ∩ Dj = ∅.\n• For Master: The parameter w is stored on the Master and the Master always keeps the newest version of w.\nAlgorithm 1 Task of Master in SCOPE Initialization: p Workers, w0; for t = 0, 1, 2, . . . , T do\nSend wt to the Workers; Wait until it receives z1, z2, . . . , zp from the pWorkers; Compute the full gradient z = 1n ∑p k=1 zk, and then send z to each Worker; Wait until it receives ũ1, ũ2, . . . , ũp from the p Workers; Compute wt+1 = 1p ∑p k=1 ũk;\nend for\nDifferent Workers can not communicate with each other. This is similar to most existing distributed learning frameworks like MLlib (Meng et al. 2016), Splash, Parameter Server, and CoCoA and so on.\nOptimization Algorithm The whole optimization (learning) algorithm is completed cooperatively by the Master and Workers:\n• Task of Master: The operations completed by the Master are outlined in Algorithm 1. We can find that the Master has two main tasks. The first task is to compute the full gradient after all the local gradient sum {zk} have been received from all Workers, and then send the full gradient to all Workers. The second task is to update the parameter w after all the locally updated parameters {ũk} have been received, and then send the updated parameter to all Workers. It is easy to see that the computation load of the Master is lightweight.\n• Task of Workers: The operations completed by the Workers are outlined in Algorithm 2. We can find that each Worker has two main tasks. The first task is to compute the sum of the gradients on its local data (called local gradient sum), i.e., zk = ∑ i∈Dk ∇fi(w) for Worker k, and\nthen send the local gradient sum to the Master. The second task is to train w by only using the local data, after which the Worker will send the locally updated parameters, denoted as ũk for Worker k, to the Master and wait for the newest w from Master.\nHere, wt denotes the global parameter at the tth iteration and is stored on the Master. uk,m denotes the local parameter at the mth iteration on Worker k.\nSCOPE is inspired by SVRG (Johnson and Zhang 2013) which tries to utilize full gradient to speed up the convergence of stochastic optimization. However, the original SVRG in (Johnson and Zhang 2013) is sequential. To design a distributed SVRG method, one natural strategy is to adapt the mini-batch SVRG (Zhao et al. 2014) to distributed settings, which is a typical strategy in most distributed SGD frameworks like Parameter Server (Li et al. 2014) and Petuum (Xing et al. 2015). In appendix1, we briefly outline the sequential SVRG and the mini-batch based distributed SVRG (called DisSVRG). We can find that there\n1All the appendices and proofs of this paper can be found in the arXiv version of this paper (Zhao et al. 2016).\nAlgorithm 2 Task of Workers in SCOPE Initialization: initialize η and c > 0; For the Worker k: for t = 0, 1, 2, . . . , T do\nWait until it gets the newest parameter wt from the Master; Let uk,0 = wt, compute the local gradient sum zk =∑ i∈Dk ∇fi(wt), and then send zk to the Master; Wait until it gets the full gradient z from the Master; for m = 0 to M − 1 do\nRandomly pick up an instance with index ik,m from Dk; uk,m+1 = uk,m−η(∇fik,m(uk,m)−∇fik,m(wt)+ z + c(uk,m −wt));\nend for Send uk,M or 1M ∑M m=1 uk,m, which is called the locally updated parameter and denoted as ũk, to the Master;\nend for\nexist three major differences between SCOPE and SVRG (or DisSVRG).\nThe first difference is that in SCOPE each Worker locally performs stochastic optimization by only using its native data (refer to the update on uk,m+1 for each Worker k in Algorithm 2). On the contrary, SVRG or DisSVRG perform stochastic optimization on the Master (refer to the update on um+1) based on the whole dataset, which means that we need to randomly pick up an instance or a mini-batch from the whole datasetD in each iteration of stochastic optimization. The locally stochastic optimization in SCOPE can dramatically reduce the communication cost, compared with DisSVRG with mini-batch strategy.\nThe second difference is the update rule of wt+1 in the Master. There are no locally updated parameters in DisSVRG with mini-batch strategy, and hence the update rule of wt+1 in the Master for DisSVRG can not be written in the form of Algorithm 1, i.e., wt+1 = 1p ∑p k=1 ũk.\nThe third difference is the update rule for uk,m+1 in SCOPE and um+1 in SVRG or DisSVRG. Compared to SVRG, SCOPE has an extra term c(uk,m − wt) in Algorithm 2 to guarantee convergence, where c > 0 is a parameter related to the objective function. The strictly theoretical proof will be provided in the following section about convergence. Here, we just give some intuition about the extra term c(uk,m −wt). Since SCOPE puts no constraints about how to partition training data on different Workers, the data distributions on different Workers may be totally different from each other. That means the local gradient in each Worker can not necessarily approximate the full gradient. Hence, the term ∇fik,m(uk,m)−∇fik,m(wt) + z is a bias estimation of the full gradient. This is different from SVRG whose stochastic gradient is an unbias estimation of the full gradient. The bias estimation ∇fik,m(uk,m) − ∇fik,m(wt) + z in SCOPE may lead uk,m+1 to be far away from the optimal value w∗. To avoid this, we use the technique in the proximal stochastic gradient that adds an extra term c(uk,m −wt) to\nmake uk,m+1 not be far away from wt. If wt is close to w∗, uk,m+1 will also be close to w∗. So the extra term in SCOPE is reasonable for convergence guarantee. At the same time, it does not bring extra computation since the update rule in SCOPE can be rewritten as\nuk,m+1 =(1− cη)uk,m − η(∇fik,m(uk,m)−∇fik,m(wt) + ẑ),\nwhere ẑ = z − cwt can be pre-computed and fixed as a constant for different m.\nBesides the above mini-batch based strategy (DisSVRG) for distributed SVRG, there also exist some other distributed SVRG methods, including DSVRG (Lee et al. 2016), KroMagnon (Mania et al. 2015), SVRGfoR (Konecný, McMahan, and Ramage 2015) and the distributed SVRG in (De and Goldstein 2016). DSVRG needs communication between Workers, and hence it cannot be directly implemented on Spark. KroMagnon focuses on asynchronous strategy, which cannot be implemented on Spark either. SVRGfoR can be implemented on Spark, but it provides no theoretical results about the convergence. Furthermore, SVRGfoR is proposed for cases with unbalanced data partitions and sparse features. On the contrary, our SCOPE can be used for any kind of features with theoretical guarantee of convergence. Moreover, in our experiment, we find that our SCOPE can outperform SVRGfoR. The distributed SVRG in (De and Goldstein 2016) cannot be guaranteed to converge because it is similar to the version of SCOPE with c = 0.\nEASGD (Zhang, Choromanska, and LeCun 2015) also adopts a parameter like c to control the difference between the local update and global update. However, EASGD assumes that each worker has access to the entire dataset while SCOPE only requires that each worker has access to a subset. Local learning strategy is also adopted in other problems like probabilistic logic programs (Riguzzi et al. 2016)."
    }, {
      "heading" : "Communication Cost",
      "text" : "Traditional mini-batch based distributed SGD methods, such as DisSVRG in the appendix, need to transfer parameter w and stochastic gradients frequently between Workers and Master. For example, the number of communication times is O(TM) for DisSVRG. Other traditional mini-batch based distributed SGD methods have the same number of communication times. Typically, M = Θ(n). Hence, traditional mini-batch based methods have O(Tn) number of communication times, which may lead to high communication cost.\nMost training (computation) load of SCOPE comes from the inner loop of Algorithm 2, which is done at local Worker without any communication. It is easy to find that the number of communication times in SCOPE is O(T ), which is dramatically less than O(Tn) of traditional mini-batch based distributed SGD or distributed SVRG methods. In the following section, we will prove that SCOPE has a linear convergence rate in terms of the iteration number T . It means that to achieve an -optimal solution2, T = O(log 1 ).\n2ŵ is called an -optimal solution if E‖ŵ −w∗‖2 ≤ where w∗ is the optimal solution.\nHence, T is typically not large for many problems. For example, in most of our experiments, we can achieve convergent results with T ≤ 10. Hence, SCOPE is communicationefficient. SCOPE is a synchronous framework, which means that some waiting time is also needed for synchronization. Because the number of synchronization is also O(T ), and T is typically a small number. Hence, the waiting time is also small."
    }, {
      "heading" : "SCOPE on Spark",
      "text" : "One interesting thing is that the computing framework of SCOPE is quite suitable for the popular distributed platform Spark. The programming model underlying Spark is MapReduce, which is actually a BSP model. In SCOPE, the task of Workers that computes local gradient sum zk and the training procedure in the inner loop of Algorithm 2 can be seen as the Map process since both of them only use local data. The task of Master that computes the average for both full gradient z and wt+1 can be seen as the Reduce process.\nThe MapReduce programming model is essentially a synchronous model, which need some synchronization cost. Fortunately, the number of synchronization times is very small as stated above. Hence, both communication cost and waiting time are very small for SCOPE. In this paper, we implement our SCOPE on Spark since Spark has been widely adopted in industry for big data applications, and our SCOPE can be easily integrated into the data processing pipeline of those organizations using Spark."
    }, {
      "heading" : "Convergence of SCOPE",
      "text" : "In this section, we will prove the convergence of SCOPE when the objective functions are strongly convex. We only list some Lemmas and Theorems, the detailed proof of which can be found in the appendices (Zhao et al. 2016).\nFor convenience, we use w∗ to denote the optimal solution. ‖ · ‖ denotes the L2 norm ‖ · ‖2. We assume that n = pq, which means that each Worker has the same number of training instances and |D1| = |D2| = · · · = |Dp| = q. In practice, we can not necessarily guarantee that these |Dk|s are the same. However, it is easy to guarantee that ∀i, j, |(|Di| − |Dj |)| ≤ 1, which will not affect the performance.\nWe define p local functions as Fk(w) = 1q ∑ i∈Dk fi(w), where k = 1, 2, . . . , p. Then we have P (w) = 1 p ∑p k=1 Fk(w).\nTo prove the convergence of SCOPE, we first give two assumptions which have also been widely adopted by most existing stochastic optimization algorithms for convergence proof.\nAssumption 1 (Smooth Gradient). There exists a constant L > 0 such that ∀a,b ∈ Rd and i = 1, 2, . . . , n, we have ‖∇fi(a)−∇fi(b)‖ ≤ L‖a− b‖. Assumption 2 (Strongly Convex). For each local function Fk(·), there exists a constant µ > 0 such that ∀a,b ∈ Rd, we have Fk(a) ≥ Fk(b) +∇Fk(b)T (a−b) + µ2 ‖a−b‖ 2.\nPlease note that these assumptions are weaker than those in (Zhang and Jordan 2015; Ma et al. 2015; Jaggi et al.\n2014), since we do not need each fi(w) to be convex and we do not make any assumption about the Hessian matrices either. Lemma 1. Let γm = 1p ∑p k=1 E‖uk,m−w∗‖2. If c > L−µ, then we have γm+1 ≤ [1−η(2µ+ c)]γm+(cη+3L2η2)γ0. Let α = 1 − η(2µ + c), β = cη + 3L2η2. Given L and µ which are determined by the objective function, we can always guarantee 0 < α < 1, 0 < β < 1, and α + β < 1 by setting η < min{ 2µ3L2 , 1 2µ+c}. We have the following theorems: Theorem 1. If we take wt+1 = 1p ∑p k=1 uk,M , then we can get the following convergence result:\nE‖wt+1 −w∗‖2 ≤ (αM + β\n1− α )E‖wt −w∗‖2.\nWhen M > log 1−α−β 1−α\nα , αM + β1−α < 1, which means we can get a linear convergence rate if we take wt+1 = 1 p ∑p k=1 uk,M .\nTheorem 2. If we take wt+1 = 1p ∑p k=1 ũk with ũk =\n1 M ∑M m=1 uk,m, then we can get the following convergence result:\nE‖wt+1 −w∗‖2 ≤ ( 1\nM(1− α) +\nβ\n1− α )E‖wt −w∗‖2.\nWhen M > 11−α−β , 1 M(1−α) + β 1−α < 1, which means we can also get a linear convergence rate if we take wt+1 = 1 p ∑p k=1 ũk with ũk = 1 M ∑M m=1 uk,m.\nAccording to Theorem 1 and Theorem 2, we can find that SCOPE gets a linear convergence rate whenM is larger than some threshold. To achieve an -optimal solution, the computation complexity of each worker is O((np + M) log 1 ). In our experiment, we find that good performance can be achieved with M = np . Hence, SCOPE is computationefficient."
    }, {
      "heading" : "Impact of Parameter c",
      "text" : "In Algorithm 2, we need the parameter c to guarantee the convergence of SCOPE. Specifically, we need c > L − µ according to Lemma 1. Here, we discuss the necessity of c.\nWe first assume c = 0, and try to find whether Algorithm 2 will converge or not. It means that in the following derivation, we always assume c = 0.\nLet us define another local function:\nF (t) k (w) = Fk(w) + (z−∇Fk(wt)) T (w −w∗)\nand denote w∗k,t = arg min w F (t) k (w).\nLet vk,m = ∇fik,m(uk,m)−∇fik,m(wt)+z+c(uk,m− wt). When c = 0, vk,m = ∇fik,m(uk,m)−∇fik,m(wt) + z. Then, we have E[vk,m|uk,m] = ∇F (t)k (uk,m) and ∇F (t)k (wt) = z. Hence, we can find that each local Worker actually tries to optimize the local function F (t)k (w) with SVRG based on the local data Dk. It means that if we set\na relatively small η and a relatively large M , the uk,m will converge to w∗k,t.\nSince F (t)k (w) is strongly convex, we have ∇F (t)k (w∗k,t) = 0. Then, we can get\n∇Fk(w∗k,t)−∇Fk(w∗) = ∇Fk(wt)−∇Fk(w∗)− z. For the left-hand side, we have\n∇Fk(w∗k,t)−∇Fk(w∗) ≈ ∇2Fk(w∗)(w∗k,t −w∗). For the right-hand side, we have\n∇Fk(wt)−∇Fk(w∗)− z =∇Fk(wt)−∇Fk(w∗)− (z−∇P (w∗)) ≈∇2Fk(w∗)(wt −w∗)−∇2P (w∗)(wt −w∗).\nCombining the two approximations, we can get\nw∗k,t −w∗ ≈ (I−A−1k A)(wt −w ∗),\nwhere Ak = ∇2Fk(w∗) and A = ∇2P (w∗) are two Hessian matrices for the local function Fk(w∗) and the global function P (w∗), respectively. Assuming in each iteration we can always get the local optimal values for all local functions, we have\nwt+1 −w∗ ≈ (I− 1\np p∑ k=1 A−1k A)(wt −w ∗). (1)\nPlease note that all the above derivations assume that c = 0. From (1), we can find that Algorithm 2 will not necessarily converge if c = 0, and the convergence property is dependent on the Hessian matrices of the local functions.\nHere, we give a simple example for illustration. We set n = p = 2 and F1(w) = (w − 1)2, F2(w) = 100(w − 10)2. We set a small step-size η = 10−5 and a large M = 4000. The convergence results of SCOPE with different c are presented in Table 1."
    }, {
      "heading" : "Separating Data Uniformly",
      "text" : "If we separate data uniformly, which means that the local data distribution on each Worker is similar to the global data distribution, then we have Ak ≈ A and ‖I − 1 p ∑p i=1 A −1 k A‖ ≈ 0. From (1), we can find that c = 0 can make SCOPE converge for this special case."
    }, {
      "heading" : "Experiment",
      "text" : "We choose logistic regression (LR) with a L2norm regularization term to evaluate SCOPE and baselines. Hence, P (w) is defined as P (w) = 1 n ∑n i=1 [ log(1 + e−yix T i w) + λ2 ‖w‖ 2 ] . The code can be downloaded from https://github.com/LIBBLE/ LIBBLE-Spark/."
    }, {
      "heading" : "Dataset",
      "text" : "We use four datasets for evaluation. They are MNIST-8M, epsilon, KDD12 and Data-A. The first two datasets can be downloaded from the LibSVM website3. MNIST-8M contains 8,100,000 handwritten digits. We set the instances of digits 5 to 9 as positive, and set the instances of digits 0 to 4 as negative. KDD12 is the dataset of Track 1 for KDD Cup 2012, which can be downloaded from the KDD Cup website4. Data-A is a dataset from a data mining competition5. The information about these datasets is summarized in Table 2. All the data is normalized before training. The regularization hyper-parameter λ is set to 10−4 for the first three datasets which are relatively small, and is set to 10−6 for the largest dataset Data-A. Similar phenomenon can be observed for other λ, which is omitted due to space limitation. For all datasets, we set c = λ× 10−2."
    }, {
      "heading" : "Experimental Setting and Baseline",
      "text" : "Distributed Platform We have a Spark cluster of 33 machines (nodes) connected by 10GB Ethernet. Each machine has 12 Intel Xeon E5-2620 cores with 64GB memory. We construct two clusters, a small one and a large one, from the original 33 machines for our experiments. The small cluster contains 9 machines, one master and eight slaves. We use 2 cores for each slave. The large cluster contains 33 machines, 1 master and 32 slaves. We use 4 cores for each slave. In both clusters, each machine has access to 64GB memory on the corresponding machine and one core corresponds to one Worker. Hence, the small cluster has one Master and 16 Workers, and the large cluster has one Master and 128 Workers. The small cluster is for experiments on the three relatively small datasets including MNIST-8M, epsilon and KDD12. The large cluster is for experiments on the largest dataset Data-A. We use Spark1.5.2 for our experiment, and implement our SCOPE in Scala.\nBaseline Because the focus of this paper is to design distributed learning methods for Spark, we compare SCOPE with distributed learning baselines which can be implemented on Spark. More specifically, we adopt the following baselines for comparison:\n• MLlib6 (Meng et al. 2016): MLlib is an open source library for distributed machine learning on Spark. It is mainly based on two optimization methods: mini-batch based distributed SGD and distributed lbfgs. We find that\n3https://www.csie.ntu.edu.tw/∼cjlin/libsvmtools/datasets/ 4http://www.kddcup2012.org/ 5http://www.yiban.cn/project/2015ccf/comp detail.php?cid=231 6http://spark.apache.org/mllib/\nthe distributed SGD method is much slower than distributed lbfgs on Spark in our experiments. Hence, we only compare our method with distributed lbfgs for MLlib, which is a batch learning method.\n• LibLinear7 (Lin et al. 2014): LibLinear is a distributed Newton method, which is also a batch learning method.\n• Splash8 (Zhang and Jordan 2015): Splash is a distributed SGD method by using the local learning strategy to reduce communication cost (Zhang, Wainwright, and Duchi 2012), which is different from the mini-batch based distributed SGD method.\n• CoCoA9 (Jaggi et al. 2014): CoCoA is a distributed dual coordinate ascent method by using local learning strategy to reduce communication cost, which is formulated from the dual problem.\n• CoCoA+10 (Ma et al. 2015): CoCoA+ is an improved version of CoCoA. Different from CoCoA which adopts average to combine local updates for global parameters, CoCoA+ adopts adding to combine local updates.\nWe can find that the above baselines include state-of-theart distributed learning methods with different characteristics. All the authors of these methods have shared the source code of their methods to the public. We use the source code provided by the authors for our experiment. For all baselines, we try several parameter values to choose the best performance."
    }, {
      "heading" : "Efficiency Comparison with Baselines",
      "text" : "We compare SCOPE with other baselines on the four datasets. The result is shown in Figure 2. Each marked point on the curves denotes one update for w by the Master, which typically corresponds to an iteration in the outer-loop. For SCOPE, good convergence results can be got with number of updates (i.e., the T in Algorithm 1) less than five. We can find that Splash vibrates on some datasets since it introduces variance in the training process. On the contrary, SCOPE are stable, which means that SCOPE is a variance reduction method like SVRG. It is easy to see that SCOPE has a linear convergence rate, which also conforms to our theoretical analysis. Furthermore, SCOPE is much faster than all the other baselines.\nSCOPE can also outperform SVRGfoR (Konecný, McMahan, and Ramage 2015) and DisSVRG. Experimental comparison can be found in appendix (Zhao et al. 2016)."
    }, {
      "heading" : "Speedup",
      "text" : "We use dataset MNIST-8M for speedup evaluation of SCOPE. Two cores are used for each machine. We evaluate speedup by increasing the number of machines. The training process will stop when the gap between the objective function value and the optimal value is less than 10−10. The speedup is defined as follows: speedup =\n7https://www.csie.ntu.edu.tw/∼ cjlin/liblinear/ 8http://zhangyuc.github.io/splash 9https://github.com/gingsmith/cocoa\n10https://github.com/gingsmith/cocoa\ntime with 16 cores by SCOPE time with 2π cores where π is the number of machines and we choose π = 8, 16, 24, 32. The experiments are performed by 5 times and the average time is reported for the final speedup result.\nThe speedup result is shown in Figure 3, where we can find that SCOPE has a super-linear speedup. This might be reasonable due to the higher cache hit ratio with more machines (Yu et al. 2014). This speedup result is quite promising on our multi-machine settings since the communication cost is much larger than that of multi-thread setting. The good speedup of SCOPE can be explained by the fact that most training work can be locally completed by each Worker and SCOPE does not need much communication cost.\nSCOPE is based on the synchronous MapReduce framework of Spark. One shortcoming of synchronous framework is the synchronization cost, which includes both communication time and waiting time. We also do experiments to show the low synchronization cost of SCOPE, which can be found in the appendix (Zhao et al. 2016)."
    }, {
      "heading" : "Conclusion",
      "text" : "In this paper, we propose a novel DSO method, called SCOPE, for distributed machine learning on Spark. Theoretical analysis shows that SCOPE is convergent with linear convergence rate for strongly convex cases. Empirical results show that SCOPE can outperform other state-of-the-art distributed methods on Spark."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work is partially supported by the “DengFeng” project of Nanjing University."
    }, {
      "heading" : "Appendix",
      "text" : ""
    }, {
      "heading" : "SVRG and Mini-Batch based Distributed SVRG",
      "text" : "The sequential SVRG is outlined in Algorithm 3, which is the same as the original SVRG in (Johnson and Zhang 2013).\nAlgorithm 3 Sequential SVRG Initialization: initialize w0, η; for t = 0, 1, 2, ..., T do u0 = wt; Compute the full gradient z = 1n ∑n i=1∇fi(u0);\nfor m = 0 to M − 1 do Randomly pick up an im from {1, . . . , n}; um+1 = um − η(∇fim(um)−∇fim(u0) + z); end for Take wt+1 to be uM or the average of {um};\nend for\nThe mini-batch based distributed SVRG (called DisSVRG) is outlined in Algorithm 4 and Algorithm 5, with Algorithm 4 for the operations completed by the Master and Algorithm 5 for the operations completed by the Workers.\nAlgorithm 4 Task of Master in DisSVRG Initialization: p Workers, w0, η; for t = 0, 1, 2, . . . , T do u0 = wt; Send wt to the Workers; Wait until it receives z1, z2, . . . , zp from the pWorkers; Compute the full gradient z = 1n ∑p k=1 zk;\nfor m = 0 to M − 1 do Send um to all Workers; Wait until it receives |Sm,k|, {∇fSm,k(um)} and {∇fSm,k(wt)} from all the Workers, compute |Sm| =\n∑ k |Sm,k|;\nCompute∇fSm(um) = 1|Sm| ∑p k=1∇fSm,k(um);\nCompute∇fSm(u0) = 1|Sm| ∑p k=1∇fSm,k(wt); Update the parameter: um+1 = um − η(∇fSm(um)−∇fSm(u0) + z);\nend for Take wt+1 to be uM or the average of {um};\nend for"
    }, {
      "heading" : "Proof of Lemma 1",
      "text" : "We define the local stochastic gradient in Algorithm 2 as follows:\nvk,m = ∇fik,m(uk,m)−∇fik,m(wt) + z+ c(uk,m −wt).\nThen the update rule at local Workers can be rewritten as follows:\nuk,m+1 = uk,m − ηvk,m. (2)\nFirst, we give the expectation and variance property of vk,m in Lemma 2 and Lemma 3. Lemma 2. The conditional expectation of local stochastic gradient vk,m on uk,m is\nE[vk,m|uk,m] =∇Fk(uk,m)−∇Fk(wt) + z+ c(uk,m −wt).\nAlgorithm 5 Task of Workers in DisSVRG For the Worker k: for t = 0, 1, 2, . . . , T do\nWait until it gets the newest parameter wt from the Master; Compute the local gradient sum zk =∑ i∈Dk ∇fi(wt), and then send zk to the Master; for m = 0 to M − 1 do Wait until it gets the newest parameter um from the Master; Randomly pick up a mini-batch indices Sm,k from Dk; Compute∇fSm,k(um) = ∑ i∈Sm,k ∇fi(um);\nCompute∇fSm,k(wt) = ∑ i∈Sm,k ∇fi(wt); Send |Sm,k|, ∇fSm,k(um) and ∇fSm,k(wt) to the Master;\nend for end for"
    }, {
      "heading" : "Proof.",
      "text" : "E[vk,m|uk,m]\n= 1\nq ∑ i∈Dk [∇fi(uk,m)−∇fi(wt) + z+ c(uk,m −wt)]\n=∇Fk(uk,m)−∇Fk(wt) + z+ c(uk,m −wt)\nLemma 3. The variance of vk,m has the the following property:\nE[‖vk,m‖2|uk,m] ≤ 3(L2 + c2)‖uk,m −wt‖2 + 3L2‖wt −w∗‖2."
    }, {
      "heading" : "Proof.",
      "text" : "E[‖vk,m‖2|uk,m]\n= 1\nq ∑ i∈Dk ‖∇fi(uk,m)−∇fi(wt) + z+ c(uk,m −wt)‖2\n≤3 q ∑ i∈Dk [‖∇fi(uk,m)−∇fi(wt)‖2 + ‖z‖2\n+ c2‖uk,m −wt‖2]\n≤3 q ∑ i∈Dk [ L2‖uk,m −wt‖2 + ‖z‖2 + c2‖uk,m −wt‖2 ] ≤3 q ∑ i∈Dk [ (L2 + c2)‖uk,m −wt‖2 + L2‖wt −w∗‖2\n] =3(L2 + c2)‖uk,m −wt‖2 + 3L2‖wt −w∗‖2\nThe second inequality uses Assumption 1. The third inequality uses the fact that∇P (w∗) = 0.\nBased on Lemma 2 and Lemma 3, we prove Lemma 1 as follows:\nProof. According to (2), we have\n‖uk,m+1 −w∗‖2\n=‖uk,m −w∗ − ηvk,m‖2\n=‖uk,m −w∗‖2 − 2ηvTk,m(uk,m −w∗) + η2‖vk,m‖2\nWe take expectation on both sides of the above equality, and obtain\nE[‖uk,m+1 −w∗‖2|uk,m] = ‖uk,m −w∗‖2\n− 2η(∇Fk(uk,m)−∇Fk(wt) + z\n+ c(uk,m −wt))T (uk,m −w∗) + η2E[‖vk,m‖2|uk,m] (3)\nFor the second line of the right side of the above equality, we have\nE[vk,m|uk,m]T (uk,m −w∗)\n=∇Fk(uk,m)T (uk,m −w∗)\n+ (z−∇Fk(wt))T (uk,m −w∗)\n+ c(uk,m −wt)T (uk,m −w∗)\n≥Fk(uk,m)− Fk(w∗) + µ 2 ‖uk,m −w∗‖2\n+ zT (uk,m −wt)−∇Fk(wt)T (uk,m −wt)\n+ (z−∇Fk(wt))T (wt −w∗)\n+ c(uk,m −wt)T (uk,m −w∗)\n≥Fk(uk,m)− F (w∗) + µ 2 ‖uk,m −w∗‖2\n+ Fk(wt)− Fk(uk,m) + µ\n2 ‖uk,m −wt‖2\n+ zT (uk,m −wt) + (z−∇Fk(wt))T (wt −w∗)\n+ c(uk,m −wt)T (uk,m −w∗) =Fk(wt)− Fk(w∗)\n+ µ 2 ‖uk,m −w∗‖2 + µ 2 ‖uk,m −wt‖2 + zT (uk,m −wt) + (z−∇Fk(wt))T (wt −w∗)\n+ c(uk,m −wt)T (uk,m −w∗) =Fk(wt)− Fk(w∗)\n+ µ+ c 2 ‖uk,m −w∗‖2 + µ+ c 2 ‖uk,m −wt‖2 + zT (uk,m −wt) + (z−∇Fk(wt))T (wt −w∗) − c 2 {‖uk,m −w∗‖2 − 2(uk,m −wt)T (uk,m −w∗)\n+ ‖uk,m −wt‖2} =Fk(wt)− Fk(w∗)\n+ µ+ c 2 ‖uk,m −w∗‖2 + µ+ c 2 ‖uk,m −wt‖2 + zT (uk,m −wt) + (z−∇Fk(wt))T (wt −w∗) − c 2 ‖wt −w∗‖2\nBoth the first and second inequalities for the above derivation use Assumption 2.\nWe use σm = σ(u1,m,u2,m, . . . ,up,m) to denote the σalgebra. Then we can take a summation for (3) with k = 1 to\np, and obtain p∑\nk=1\nE[‖uk,m+1 −w∗‖2|σm]\n≤ p∑\nk=1\n‖uk,m −w∗‖2\n− 2η p∑\nk=1\n{Fk(wt)− Fk(w∗) + µ+ c 2 ‖uk,m −w∗‖2\n+ µ+ c\n2 ‖uk,m −wt‖2 + zT (uk,m −wt)\n− c 2 ‖wt −w∗‖2 + (z−∇Fk(wt))T (wt −w∗)}\n+ η2 p∑\nk=1\nE[‖vk,m‖2|σm]\n= p∑ k=1 ‖uk,m −w∗‖2\n− 2η p∑\nk=1\n{P (wt)− P (w∗) + µ+ c 2 ‖uk,m −w∗‖2\n+ µ+ c\n2 ‖uk,m −wt‖2 + zT (uk,m −wt)}\n+ cpη‖wt −w∗‖2 + η2 p∑\nk=1\nE[‖vk,m‖2|σm]\n≤ p∑\nk=1\n‖uk,m −w∗‖2\n− 2η p∑\nk=1\n{P (wt)− P (w∗) + µ+ c 2 ‖uk,m −w∗‖2\n+ µ+ c− L\n2 ‖uk,m −wt‖2 + P (uk,m)− P (wt)}\n+ cpη‖wt −w∗‖2 + η2 p∑\nk=1\nE[‖vk,m‖2|σm]\n= p∑ k=1 ‖uk,m −w∗‖2\n− 2η p∑\nk=1\n{P (uk,m)− P (w∗) + µ+ c 2 ‖uk,m −w∗‖2\n+ µ+ c− L\n2 ‖uk,m −wt‖2}\n+ cpη‖wt −w∗‖2 + η2 p∑\nk=1\nE[‖vk,m‖2|σm]\n≤ p∑\nk=1\n‖uk,m −w∗‖2 − 2η p∑\nk=1\n{2µ+ c 2 ‖uk,m\n−w∗‖2 + µ+ c− L 2 ‖uk,m −wt‖2}\n+ cpη‖wt −w∗‖2 + η2 p∑\nk=1\nE[‖vk,m‖2|σm] (4)\nIn the first equality, we use the definition of local function Fk(·) that P (w) = 1\np ∑p k=1∇Fk(w) which leads to ∑p k=1 Fk(wt) =∑p\nk=1 P (wt) and ∑p k=1(z−∇Fk(wt)) T (wt −w∗) = 0. In the first inequality, we use Assumption 1 which leads to P (uk,m) ≤\nP (wt) + z T (uk,m −wt) + L2 ‖uk,m −wt‖ 2. If we use γm = 1p ∑p k=1 E‖uk,m − w\n∗‖2, then according to Algorithm 2, it is easy to get that γ0 = E‖wt −w∗‖2. Moreover, according to (4) and Lemma 3, we can obtain\nγm+1 ≤ γm − η(2µ+ c)γm + (cη + 3L2η2)x0 + am\nwhere\nam = 3η2(L2 + c2)− η(µ+ c− L)\np\np∑ k=1 E‖uk,m −wt‖2\nIf c > L− µ, we can choose a small η such that am < 0. Then we get the result\nγm+1 ≤ [1− η(2µ+ c)]γm + (cη + 3L2η2)γ0"
    }, {
      "heading" : "Proof of Theorem 1",
      "text" : "Proof. According to Lemma 1, we have\nγm ≤ αγm−1 + βγ0\n≤ (αm + β 1− α )γ0\nSince we take wt+1 = 1p ∑p k=1 uk,M , then we have\nE‖wt+1 −w∗‖2 =E‖ 1\np p∑ k=1 uk,M −w∗‖2\n≤1 p p∑ k=1 E‖uk,M −w∗‖2\n=γM\n≤(αM + β 1− α )E‖wt −w ∗‖2"
    }, {
      "heading" : "Proof of Theorem 2",
      "text" : "Proof. According to Lemma 1, we have\nγm+1 + (1− α)γm ≤ γm + βγ0\nSumming m from 1 to M , we have\nγm+1 + (1− α) M∑\nm=1\nγm ≤ (1 +Mβ)γ0\nSince we take wt+1 = 1pM ∑M m=1 ∑p k=1 uk,m, then we have\nE‖wt+1 −w∗‖2 =E‖ 1\npM M∑ m=1 p∑ k=1 uk,M −w∗‖2\n≤ 1 pM M∑ m=1 p∑ k=1 ‖uk,M −w∗‖2\n≤( 1 M(1− α) + β 1− α )E‖wt −w ∗‖2"
    }, {
      "heading" : "Efficiency Comparison with DisSVRG",
      "text" : "In this section, we compare SCOPE with the mini-batch based distributed SVRG (called DisSVRG) in Algorithm 4 and Algorithm 5 using the dataset MNIST-8M. The result is shown in Figure 4. The x-axis is the CPU time, containing both computation and synchronization time, with the unit millisecond. The y-axis is the objective function value minus the optimal value in a log scale. In this paper, the optimal value is the minimal value got by running all the baselines and SCOPE for a large number of iterations. It is easy to see that DisSVRG is much slower than our SCOPE, which means that the traditional mini-batch based DSO strategy is not scalable due to huge communication cost."
    }, {
      "heading" : "Efficiency Comparison with SVRGfoR",
      "text" : "SVRGfoR (Konecný, McMahan, and Ramage 2015) is proposed for cases when the number of Workers is relatively large, and with unbalanced data partitions and sparse features. We use the KDD12 dataset with sparse features for evaluation. We construct a large cluster with 1600 Workers. Furthermore, we partition the data in an unbalanced way. The largest number of data points on one Worker is 423954, and the smallest number of data points on one Worker is 28. We tune several stepsizes for SVRGfoR to get the best performance. The experimental results are shown in Figure 5. We can find that SCOPE is much faster than SVRGfoR."
    }, {
      "heading" : "Synchronization Cost",
      "text" : "SCOPE is based on the synchronous MapReduce framework of Spark. One shortcoming of synchronous framework is that there exists synchronization cost besides the computation cost. The synchronization cost includes both communication time and waiting time. Fortunately, the synchronization cost of SCOPE is low because most computation is completed locally and only a small number of synchronization times is needed. Here, we use experiment to verify this.\nWe use the dataset MNIST-8M for evaluation. The result is shown in Figure 6. The x-axis is the number of cores, the y-axis is the CPU time (in millisecond) per iteration, which is computed as dividing the total time by the number of iterations T . Please note that the CPU time includes both computation time and synchronization time (cost). During the training process, if the Workers or Master are computing, we consider the time as computation time. In each synchronization step, we consider the time gap between the completion of the fastest Worker and the slowest Worker as waiting time. If there is communication between Workers and Master, we consider the time as communication time. From Figure 6, we can find that SCOPE does not take too much waiting time and communication time compared to the computation time. We can also find that with the increase of the number of Workers (cores), the synchronization time per iteration does not increase too much, which is promising for distributed learning on clusters with multiple machines.\nThe speedup in Figure 6 seems to be smaller than that in Figure 3. Both Figure 6 and Figure 3 are correct. Some Workers still perform computation during the waiting time. So there is a repeating part in the waiting time and computation time in Figure 6. Furthermore, the total number of iterations to achieve the same objective value may not be the same for different number of cores."
    } ],
    "references" : [ {
      "title" : "Large-scale machine learning with stochastic gradient descent",
      "author" : [ "L. Bottou" ],
      "venue" : "In International Conference on Computational Statistics",
      "citeRegEx" : "Bottou,? \\Q2010\\E",
      "shortCiteRegEx" : "Bottou",
      "year" : 2010
    }, {
      "title" : "Efficient distributed SGD with variance reduction",
      "author" : [ "S. De", "T. Goldstein" ],
      "venue" : "In IEEE International Conference on Data Mining",
      "citeRegEx" : "De and Goldstein,? \\Q2016\\E",
      "shortCiteRegEx" : "De and Goldstein",
      "year" : 2016
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "J.C. Duchi", "E. Hazan", "Y. Singer" ],
      "venue" : "Journal of Machine Learning Research 12:2121–2159",
      "citeRegEx" : "Duchi et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2011
    }, {
      "title" : "Passcode: Parallel asynchronous stochastic dual co-ordinate descent",
      "author" : [ "C.-J. Hsieh", "H.-F. Yu", "I.S. Dhillon" ],
      "venue" : "In International Conference on Machine Learning",
      "citeRegEx" : "Hsieh et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hsieh et al\\.",
      "year" : 2015
    }, {
      "title" : "On variance reduction in stochastic gradient descent and its asynchronous variants",
      "author" : [ "S.J. Reddi", "A. Hefny", "S. Sra", "B. Poczos", "A.J. Smola" ],
      "venue" : "In Neural Information Processing Systems",
      "citeRegEx" : "Reddi et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Reddi et al\\.",
      "year" : 2015
    }, {
      "title" : "Communication-efficient distributed dual coordinate ascent",
      "author" : [ "M. Jaggi", "V. Smith", "M. Takac", "J. Terhorst", "S. Krishnan", "T. Hofmann", "M.I. Jordan" ],
      "venue" : "In Neural Information Processing Systems",
      "citeRegEx" : "Jaggi et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Jaggi et al\\.",
      "year" : 2014
    }, {
      "title" : "Accelerating stochastic gradient descent using predictive variance reduction",
      "author" : [ "R. Johnson", "T. Zhang" ],
      "venue" : "In Neural Information Processing Systems",
      "citeRegEx" : "Johnson and Zhang,? \\Q2013\\E",
      "shortCiteRegEx" : "Johnson and Zhang",
      "year" : 2013
    }, {
      "title" : "Federated optimization: Distributed optimization beyond the datacenter",
      "author" : [ "J. Konecný", "B. McMahan", "D. Ramage" ],
      "venue" : null,
      "citeRegEx" : "Konecný et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Konecný et al\\.",
      "year" : 2015
    }, {
      "title" : "Distributed stochastic variance reduced gradient methods and a lower bound for communication complexity. arXiv:1507.07595v2",
      "author" : [ "J.D. Lee", "Q. Lin", "T. Ma", "T. Yang" ],
      "venue" : null,
      "citeRegEx" : "Lee et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2016
    }, {
      "title" : "Scaling distributed machine learning with the parameter server",
      "author" : [ "M. Li", "D.G. Andersen", "J.W. Park", "A.J. Smola", "A. Ahmed", "V. Josifovski", "J. Long", "E.J. Shekita", "B. Su" ],
      "venue" : "In USENIX Symposium on Operating Systems Design and Implementation",
      "citeRegEx" : "Li et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2014
    }, {
      "title" : "Large-scale logistic regression and linear support vector machines using spark",
      "author" : [ "C.-Y. Lin", "C.-H. Tsai", "C.-P. Lee", "C.-J. Lin" ],
      "venue" : "In IEEE International Conference on Big Data",
      "citeRegEx" : "Lin et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2014
    }, {
      "title" : "An accelerated proximal coordinate gradient method",
      "author" : [ "Q. Lin", "Z. Lu", "L. Xiao" ],
      "venue" : "In Neural Information Processing Systems",
      "citeRegEx" : "Lin et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2014
    }, {
      "title" : "An asynchronous parallel stochastic coordinate descent algorithm",
      "author" : [ "J. Liu", "S.J. Wright", "C. Ré", "V. Bittorf", "S. Sridhar" ],
      "venue" : "In International Conference on Machine Learning",
      "citeRegEx" : "Liu et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2014
    }, {
      "title" : "Adding vs. averaging in distributed primal-dual optimization",
      "author" : [ "C. Ma", "V. Smith", "M. Jaggi", "M.I. Jordan", "P. Richtárik", "M. Takác" ],
      "venue" : "In International Conference on Machine Learning",
      "citeRegEx" : "Ma et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2015
    }, {
      "title" : "Perturbed iterate analysis for asynchronous stochastic optimization. arXiv:1507.06970",
      "author" : [ "H. Mania", "X. Pan", "D.S. Papailiopoulos", "B. Recht", "K. Ramchandran", "M.I. Jordan" ],
      "venue" : null,
      "citeRegEx" : "Mania et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mania et al\\.",
      "year" : 2015
    }, {
      "title" : "Mllib: Machine learning in apache spark",
      "author" : [ "X. Meng", "J. Bradley", "B. Yavuz", "E. Sparks", "S. Venkataraman", "D. Liu", "J. Freeman", "D. Tsai", "M. Amde", "S. Owen", "D. Xin", "R. Xin", "M.J. Franklin", "R. Zadeh", "M. Zaharia", "A. Talwalkar" ],
      "venue" : "Journal of Machine Learning Research 17(34):1–7",
      "citeRegEx" : "Meng et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Meng et al\\.",
      "year" : 2016
    }, {
      "title" : "Stochastic proximal gradient descent with acceleration techniques",
      "author" : [ "A. Nitanda" ],
      "venue" : "In Neural Information Processing Systems",
      "citeRegEx" : "Nitanda,? \\Q2014\\E",
      "shortCiteRegEx" : "Nitanda",
      "year" : 2014
    }, {
      "title" : "Hogwild!: A lock-free approach to parallelizing stochastic gradient descent",
      "author" : [ "B. Recht", "C. Re", "S.J. Wright", "F. Niu" ],
      "venue" : "In Neural Information Processing Systems",
      "citeRegEx" : "Recht et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Recht et al\\.",
      "year" : 2011
    }, {
      "title" : "Scaling structure learning of probabilistic logic programs by mapreduce",
      "author" : [ "F. Riguzzi", "E. Bellodi", "R. Zese", "G. Cota", "E. Lamma" ],
      "venue" : "In European Conference on Artificial Intelligence",
      "citeRegEx" : "Riguzzi et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Riguzzi et al\\.",
      "year" : 2016
    }, {
      "title" : "Minimizing finite sums with the stochastic average gradient",
      "author" : [ "M.W. Schmidt", "N.L. Roux", "F.R. Bach" ],
      "venue" : "CoRR abs/1309.2388",
      "citeRegEx" : "Schmidt et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Schmidt et al\\.",
      "year" : 2013
    }, {
      "title" : "Stochastic dual coordinate ascent methods for regularized loss",
      "author" : [ "S. Shalev-Shwartz", "T. Zhang" ],
      "venue" : "Journal of Machine Learning Research",
      "citeRegEx" : "Shalev.Shwartz and Zhang,? \\Q2013\\E",
      "shortCiteRegEx" : "Shalev.Shwartz and Zhang",
      "year" : 2013
    }, {
      "title" : "Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization",
      "author" : [ "S. Shalev-Shwartz", "T. Zhang" ],
      "venue" : "In International Conference on Machine Learning",
      "citeRegEx" : "Shalev.Shwartz and Zhang,? \\Q2014\\E",
      "shortCiteRegEx" : "Shalev.Shwartz and Zhang",
      "year" : 2014
    }, {
      "title" : "Dual averaging method for regularized stochastic learning and online optimization",
      "author" : [ "L. Xiao" ],
      "venue" : "In Neural Information Processing Systems",
      "citeRegEx" : "Xiao,? \\Q2009\\E",
      "shortCiteRegEx" : "Xiao",
      "year" : 2009
    }, {
      "title" : "Petuum: A new platform for distributed machine learning on big data",
      "author" : [ "E.P. Xing", "Q. Ho", "W. Dai", "J.K. Kim", "J. Wei", "S. Lee", "X. Zheng", "P. Xie", "A. Kumar", "Y. Yu" ],
      "venue" : "In ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
      "citeRegEx" : "Xing et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Xing et al\\.",
      "year" : 2015
    }, {
      "title" : "Trading computation for communication: Distributed stochastic dual coordinate ascent",
      "author" : [ "T. Yang" ],
      "venue" : "In Neural Information Processing Systems",
      "citeRegEx" : "Yang,? \\Q2013\\E",
      "shortCiteRegEx" : "Yang",
      "year" : 2013
    }, {
      "title" : "Distributed stochastic ADMM for matrix factorization",
      "author" : [ "Z.-Q. Yu", "X.-J. Shi", "L. Yan", "W.-J. Li" ],
      "venue" : "In International Conference on Conference on Information and Knowledge Management",
      "citeRegEx" : "Yu et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2014
    }, {
      "title" : "Spark: Cluster computing with working sets",
      "author" : [ "M. Zaharia", "M. Chowdhury", "M.J. Franklin", "S. Shenker", "I. Stoica" ],
      "venue" : "In USENIX Workshop on Hot Topics in Cloud Computing",
      "citeRegEx" : "Zaharia et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Zaharia et al\\.",
      "year" : 2010
    }, {
      "title" : "Splash: User-friendly programming interface for parallelizing stochastic algorithms. CoRR abs/1506.07552",
      "author" : [ "Y. Zhang", "M.I. Jordan" ],
      "venue" : null,
      "citeRegEx" : "Zhang and Jordan,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhang and Jordan",
      "year" : 2015
    }, {
      "title" : "Asynchronous distributed ADMM for consensus optimization",
      "author" : [ "R. Zhang", "J.T. Kwok" ],
      "venue" : "In International Conference on Machine Learning",
      "citeRegEx" : "Zhang and Kwok,? \\Q2014\\E",
      "shortCiteRegEx" : "Zhang and Kwok",
      "year" : 2014
    }, {
      "title" : "Deep learning with elastic averaging SGD",
      "author" : [ "S. Zhang", "A. Choromanska", "Y. LeCun" ],
      "venue" : "In Neural Information Processing Systems",
      "citeRegEx" : "Zhang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2015
    }, {
      "title" : "Linear convergence with condition number independent access of full gradients",
      "author" : [ "L. Zhang", "M. Mahdavi", "R. Jin" ],
      "venue" : "In Neural Information Processing Systems",
      "citeRegEx" : "Zhang et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2013
    }, {
      "title" : "Communication-efficient algorithms for statistical optimization",
      "author" : [ "Y. Zhang", "M.J. Wainwright", "J.C. Duchi" ],
      "venue" : "In Neural Information Processing Systems",
      "citeRegEx" : "Zhang et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2012
    }, {
      "title" : "Asynchronous distributed semi-stochastic gradient optimization",
      "author" : [ "R. Zhang", "S. Zheng", "J.T. Kwok" ],
      "venue" : "In AAAI Conference on Artificial Intelligence",
      "citeRegEx" : "Zhang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2016
    }, {
      "title" : "Fast asynchronous parallel stochastic gradient descent: A lock-free approach with convergence guarantee",
      "author" : [ "Zhao", "S.-Y", "Li", "W.-J" ],
      "venue" : "In AAAI Conference on Artificial Intelligence",
      "citeRegEx" : "Zhao et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2016
    }, {
      "title" : "Accelerated mini-batch randomized block coordinate descent method",
      "author" : [ "T. Zhao", "M. Yu", "Y. Wang", "R. Arora", "H. Liu" ],
      "venue" : "In Neural Information Processing Systems",
      "citeRegEx" : "Zhao et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2014
    }, {
      "title" : "SCOPE: scalable composite optimization for learning on Spark",
      "author" : [ "S.-Y. Zhao", "R. Xiang", "Y.-H. Shi", "P. Gao", "W.-J. Li" ],
      "venue" : "CoRR abs/1602.00133",
      "citeRegEx" : "Zhao et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2016
    }, {
      "title" : "Parallelized stochastic gradient descent",
      "author" : [ "M. Zinkevich", "M. Weimer", "L. Li", "A.J. Smola" ],
      "venue" : "In Neural Information Processing Systems",
      "citeRegEx" : "Zinkevich et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Zinkevich et al\\.",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "The first category is stochastic gradient descent (SGD) and its variants, such as stochastic average gradient (SAG) (Schmidt, Roux, and Bach 2013) and stochastic variance reduced gradient (SVRG) (Johnson and Zhang 2013), which try to perform optimization on the primal problem.",
      "startOffset" : 195,
      "endOffset" : 219
    }, {
      "referenceID" : 20,
      "context" : "The second category, such as stochastic dual coordinate ascent (SDCA) (Shalev-Shwartz and Zhang 2013), tries to perform optimization with the dual formulation.",
      "startOffset" : 70,
      "endOffset" : 101
    }, {
      "referenceID" : 17,
      "context" : "Hence, recent progress of PSO mainly focuses on designing asynchronous or lock-free optimization strategies (Recht et al. 2011; Liu et al. 2014; Hsieh, Yu, and Dhillon 2015; J. Reddi et al. 2015; Zhao and Li 2016).",
      "startOffset" : 108,
      "endOffset" : 213
    }, {
      "referenceID" : 12,
      "context" : "Hence, recent progress of PSO mainly focuses on designing asynchronous or lock-free optimization strategies (Recht et al. 2011; Liu et al. 2014; Hsieh, Yu, and Dhillon 2015; J. Reddi et al. 2015; Zhao and Li 2016).",
      "startOffset" : 108,
      "endOffset" : 213
    }, {
      "referenceID" : 36,
      "context" : "Representative distributed SGD methods include PSGD (Zinkevich et al. 2010), BAVGM (Zhang, Wainwright, and Duchi 2012) ar X iv :1 60 2.",
      "startOffset" : 52,
      "endOffset" : 75
    }, {
      "referenceID" : 27,
      "context" : "and Splash (Zhang and Jordan 2015).",
      "startOffset" : 11,
      "endOffset" : 34
    }, {
      "referenceID" : 24,
      "context" : "Representative distributed dual formulations include DisDCA (Yang 2013), CoCoA (Jaggi et al.",
      "startOffset" : 60,
      "endOffset" : 71
    }, {
      "referenceID" : 5,
      "context" : "Representative distributed dual formulations include DisDCA (Yang 2013), CoCoA (Jaggi et al. 2014) and CoCoA+ (Ma et al.",
      "startOffset" : 79,
      "endOffset" : 98
    }, {
      "referenceID" : 13,
      "context" : "2014) and CoCoA+ (Ma et al. 2015).",
      "startOffset" : 17,
      "endOffset" : 33
    }, {
      "referenceID" : 26,
      "context" : "In this paper, we propose a novel DSO method, called scalable composite optimization for learning (SCOPE), and implement it on the fault-tolerant distributed platform Spark (Zaharia et al. 2010).",
      "startOffset" : 173,
      "endOffset" : 194
    }, {
      "referenceID" : 9,
      "context" : "Please note that some asynchronous methods or systems, such as Parameter Server (Li et al. 2014), Petuum (Xing et al.",
      "startOffset" : 80,
      "endOffset" : 96
    }, {
      "referenceID" : 23,
      "context" : "2014), Petuum (Xing et al. 2015) and the methods in (Zhang and Kwok 2014; Zhang, Zheng, and Kwok 2016), have also been proposed for distributed learning with promising performance.",
      "startOffset" : 14,
      "endOffset" : 32
    }, {
      "referenceID" : 28,
      "context" : "2015) and the methods in (Zhang and Kwok 2014; Zhang, Zheng, and Kwok 2016), have also been proposed for distributed learning with promising performance.",
      "startOffset" : 25,
      "endOffset" : 75
    }, {
      "referenceID" : 15,
      "context" : "This is similar to most existing distributed learning frameworks like MLlib (Meng et al. 2016), Splash, Parameter Server, and CoCoA and so on.",
      "startOffset" : 76,
      "endOffset" : 94
    }, {
      "referenceID" : 6,
      "context" : "SCOPE is inspired by SVRG (Johnson and Zhang 2013) which tries to utilize full gradient to speed up the convergence of stochastic optimization.",
      "startOffset" : 26,
      "endOffset" : 50
    }, {
      "referenceID" : 6,
      "context" : "However, the original SVRG in (Johnson and Zhang 2013) is sequential.",
      "startOffset" : 30,
      "endOffset" : 54
    }, {
      "referenceID" : 34,
      "context" : "To design a distributed SVRG method, one natural strategy is to adapt the mini-batch SVRG (Zhao et al. 2014) to distributed settings, which is a typical strategy in most distributed SGD frameworks like Parameter Server (Li et al.",
      "startOffset" : 90,
      "endOffset" : 108
    }, {
      "referenceID" : 9,
      "context" : "2014) to distributed settings, which is a typical strategy in most distributed SGD frameworks like Parameter Server (Li et al. 2014) and Petuum (Xing et al.",
      "startOffset" : 116,
      "endOffset" : 132
    }, {
      "referenceID" : 23,
      "context" : "2014) and Petuum (Xing et al. 2015).",
      "startOffset" : 17,
      "endOffset" : 35
    }, {
      "referenceID" : 33,
      "context" : "All the appendices and proofs of this paper can be found in the arXiv version of this paper (Zhao et al. 2016).",
      "startOffset" : 92,
      "endOffset" : 110
    }, {
      "referenceID" : 8,
      "context" : "Besides the above mini-batch based strategy (DisSVRG) for distributed SVRG, there also exist some other distributed SVRG methods, including DSVRG (Lee et al. 2016), KroMagnon (Mania et al.",
      "startOffset" : 146,
      "endOffset" : 163
    }, {
      "referenceID" : 14,
      "context" : "2016), KroMagnon (Mania et al. 2015), SVRGfoR (Konecný, McMahan, and Ramage 2015) and the distributed SVRG in (De and Goldstein 2016).",
      "startOffset" : 17,
      "endOffset" : 36
    }, {
      "referenceID" : 1,
      "context" : "2015), SVRGfoR (Konecný, McMahan, and Ramage 2015) and the distributed SVRG in (De and Goldstein 2016).",
      "startOffset" : 79,
      "endOffset" : 102
    }, {
      "referenceID" : 1,
      "context" : "The distributed SVRG in (De and Goldstein 2016) cannot be guaranteed to converge because it is similar to the version of SCOPE with c = 0.",
      "startOffset" : 24,
      "endOffset" : 47
    }, {
      "referenceID" : 18,
      "context" : "Local learning strategy is also adopted in other problems like probabilistic logic programs (Riguzzi et al. 2016).",
      "startOffset" : 92,
      "endOffset" : 113
    }, {
      "referenceID" : 33,
      "context" : "We only list some Lemmas and Theorems, the detailed proof of which can be found in the appendices (Zhao et al. 2016).",
      "startOffset" : 98,
      "endOffset" : 116
    }, {
      "referenceID" : 27,
      "context" : "Please note that these assumptions are weaker than those in (Zhang and Jordan 2015; Ma et al. 2015; Jaggi et al. 2014), since we do not need each fi(w) to be convex and we do not make any assumption about the Hessian matrices either.",
      "startOffset" : 60,
      "endOffset" : 118
    }, {
      "referenceID" : 13,
      "context" : "Please note that these assumptions are weaker than those in (Zhang and Jordan 2015; Ma et al. 2015; Jaggi et al. 2014), since we do not need each fi(w) to be convex and we do not make any assumption about the Hessian matrices either.",
      "startOffset" : 60,
      "endOffset" : 118
    }, {
      "referenceID" : 5,
      "context" : "Please note that these assumptions are weaker than those in (Zhang and Jordan 2015; Ma et al. 2015; Jaggi et al. 2014), since we do not need each fi(w) to be convex and we do not make any assumption about the Hessian matrices either.",
      "startOffset" : 60,
      "endOffset" : 118
    }, {
      "referenceID" : 15,
      "context" : "More specifically, we adopt the following baselines for comparison: • MLlib6 (Meng et al. 2016): MLlib is an open source library for distributed machine learning on Spark.",
      "startOffset" : 77,
      "endOffset" : 95
    }, {
      "referenceID" : 10,
      "context" : "• LibLinear7 (Lin et al. 2014): LibLinear is a distributed Newton method, which is also a batch learning method.",
      "startOffset" : 13,
      "endOffset" : 30
    }, {
      "referenceID" : 27,
      "context" : "• Splash8 (Zhang and Jordan 2015): Splash is a distributed SGD method by using the local learning strategy to reduce communication cost (Zhang, Wainwright, and Duchi 2012), which is different from the mini-batch based distributed SGD method.",
      "startOffset" : 10,
      "endOffset" : 33
    }, {
      "referenceID" : 5,
      "context" : "• CoCoA9 (Jaggi et al. 2014): CoCoA is a distributed dual coordinate ascent method by using local learning strategy to reduce communication cost, which is formulated from the dual problem.",
      "startOffset" : 9,
      "endOffset" : 28
    }, {
      "referenceID" : 13,
      "context" : "• CoCoA+10 (Ma et al. 2015): CoCoA+ is an improved version of CoCoA.",
      "startOffset" : 11,
      "endOffset" : 27
    }, {
      "referenceID" : 33,
      "context" : "Experimental comparison can be found in appendix (Zhao et al. 2016).",
      "startOffset" : 49,
      "endOffset" : 67
    }, {
      "referenceID" : 25,
      "context" : "This might be reasonable due to the higher cache hit ratio with more machines (Yu et al. 2014).",
      "startOffset" : 78,
      "endOffset" : 94
    }, {
      "referenceID" : 33,
      "context" : "We also do experiments to show the low synchronization cost of SCOPE, which can be found in the appendix (Zhao et al. 2016).",
      "startOffset" : 105,
      "endOffset" : 123
    } ],
    "year" : 2016,
    "abstractText" : "Many machine learning models, such as logistic regression (LR) and support vector machine (SVM), can be formulated as composite optimization problems. Recently, many distributed stochastic optimization (DSO) methods have been proposed to solve the large-scale composite optimization problems, which have shown better performance than traditional batch methods. However, most of these DSO methods might not be scalable enough. In this paper, we propose a novel DSO method, called scalable composite optimization for learning (SCOPE), and implement it on the fault-tolerant distributed platform Spark. SCOPE is both computationefficient and communication-efficient. Theoretical analysis shows that SCOPE is convergent with linear convergence rate when the objective function is strongly convex. Furthermore, empirical results on real datasets show that SCOPE can outperform other state-of-the-art distributed learning methods on Spark, including both batch learning methods and DSO methods.",
    "creator" : "TeX"
  }
}