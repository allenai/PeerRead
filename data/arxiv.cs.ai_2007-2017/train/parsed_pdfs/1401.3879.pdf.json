{
  "name" : "1401.3879.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Soft Constraints of Difference and Equality",
    "authors" : [ "Emmanuel Hebrard", "Barry O’Sullivan", "Igor Razgon" ],
    "emails" : [ "hebrard@laas.fr", "dmarx@cs.bme.hu", "b.osullivan@cs.ucc.ie", "ir45@mcs.le.ac.uk" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Constraints for reasoning about equality and difference within assignments to a set of variables are ubiquitous in constraint programming. In many settings, one needs to enforce a given degree of diversity or similarity in a solution. For example, in a university timetabling problem we will want to ensure that all courses taken by a particular student are held at different times. Similarly, in meeting scheduling we will want to ensure that the participants of the same meeting are scheduled to meet at the same time and in the same place. Sometimes, when the problem is over-constrained, we might wish to maximise the extent to which these constraints are satisfied. Consider again our timetabling example: we might wish to\nc©2011 AI Access Foundation. All rights reserved.\nmaximise the number of courses that are scheduled at different times when a student’s preferences cannot all be met.\nIn a constraint programming setting requirements on the diversity and similarity amongst variables can be specified using global constraints. One of the most commonly used global constraints is the AllDifferent (Régin, 1994), which enforces that all variables take pairwise different values. A soft version of the AllDifferent constraint, named SoftAllDiff, has been proposed by Petit, Régin, and Bessiere (2001). They proposed two cost metrics for measuring the degree of satisfaction of the constraint, which are to be minimised or maximised: graph- and variable-based cost. These two cost metrics are generic and widely used (e.g., van Hoeve, 2004). The former counts the number of equalities, whilst the latter counts the number of variables to change in order to satisfy the corresponding hard constraint. When we wish to enforce that a set of variables take equal values, we can use the AllEqual, or its soft variant for the graph-based cost, the SoftAllEqual constraint (Hebrard, O’Sullivan, & Razgon, 2008), or its soft variant for the variable-based cost, the AtMostNValue constraint (Beldiceanu, 2001).\nWhen considering these two constraints (AllDifferent and AllEqual), these two costs (graph-based and variable-based) and objectives (minimisation and maximisation) we can define eight algorithmic problems related to constraints of difference and equality. In fact, because the graph-based costs of AllDifferent and AllEqual are dual, only six distinct problems are thus defined. The structure of this class of constraints is illustrated in Figure 1. For each one, we give the complexity of the best known algorithm for achieving ac and bc. Three of these problems were studied in the past: minimising the cost of SoftAllDiff variable (Petit et al., 2001) and graph-based cost (van Hoeve, 2004) is polynomial whilst maximising the variable-based cost of SoftAllDiff is NP-hard (Bessiere, Hebrard, Hnich, Kiziltan, & Walsh, 2006) for ac and polynomial (Beldiceanu, 2001) for bc. A fourth one, maximising the variable-based cost of the SoftAllEqual constraint, can directly be mapped to a known problem: the Global Cardinality constraint. In this paper,1 we introduce two efficient algorithms for achieving, respectively, Arc consistency (ac) and Bounds consistency (bc) on the fifth case, minimising the variable-based cost for SoftAllEqual. Moreover, the computational complexity of the last remaining case, maximising the graph-based cost for SoftAllDiff (or, equivalently, minimising the graph-based cost for SoftAllEqual) was still unknown. Informally, this problem is to maximise the number of pairs of variables assigned to a common value. It turns out to be a challenging and interesting problem, in that it is hard but yet can be addressed in several ways. In particular, we show that:\n• Finding a solution with at least k pairs of equal variables is NP-complete, hence achieving ac on the corresponding constraint is NP-hard.\n• When domains are contiguous, it can be solved in a polynomial number of steps through dynamic programming, hence achieving bc on the corresponding constraint is polynomial.\n• There exists a linear approximation by a factor of 12 for the general case.\n1. Part of the material presented in this paper is based on two conference publications (Hebrard et al., 2008; Hebrard, Marx, O’Sullivan, & Razgon, 2009).\n• If no value appears in the domains of more than two distinct variables, then the problem can be solved by a general matching, thus defining another tractable class.\n• There exists a fixed parameter tractable algorithm for this problem for a parameter k equal to the number of values that appear in more than two distinct domains.\nMoreover, we show that the constraint defined by setting a lower bound on the graphbased cost of SoftAllEqual can be used to efficiently find a set of similar solutions to a set of problems, for instance to promote stability or regularity. Similarly, the dual constraint (SoftAllDiff) can be used to find a set of diverse solutions, for instance to sample a set of configurations. Notice that these two applications have motivated, in part, our choice of cost metrics.\nThe remainder of this paper is organised as follows. In Section 2 we introduce the necessary technical background. A complete taxonomy of constraints of equality and difference, based on results by other authors as well as original material is presented in Section 3. Then, in the following sections, we present the new results allowing us to close the gaps in this taxonomy. First, in Section 4 we present two efficient algorithm for achieving ac and bc when minimising the variable-based cost of SoftAllEqual. Second, in Section 5 we give a proof of NP-hardness for the problem of achieving ac when maximising the graph-based cost of SoftAllDiff. Third, in Section 6 we present a polynomial algorithm to achieve bc on the same constraint. Finally, in the remaining sections, we explore the algorithmic properties of this preference cost. In Section 7, we show that a natural greedy algorithm approximates the maximum number of equalities within a factor of 12 , and that its complexity can be brought down to linear time. Next, in Section 8, we identify a polynomial class for this constraint. Then, in Section 9, we identify a parameter based on this class and show that the SoftAllEqualG constraint is fixed-parameter tractable with respect to this parameter. Finally, in Section 10, we show how the results obtained in this paper can be applied to sample solutions or, conversely, to promote stability. In particular, we describe two constructions using SoftAllDiffminG and SoftAllEqual min G respectively. Concluding remarks are made in Section 11."
    }, {
      "heading" : "2. Background",
      "text" : "In this section we present the necessary background required by the reader and introduce the notation we use throughout the paper."
    }, {
      "heading" : "2.1 Constraint Satisfaction",
      "text" : "A constraint satisfaction problem (CSP) is a triplet P = (X ,D, C) where X is a set of variables, D is a mapping of variables to finite sets of values and C is a set of constraints that specify allowed combinations of values for subsets of variables. Without loss of generality, we assume D(X) ⊂ Z for all X ∈ X , and we denote by min(X) and max(X) the minimum and maximum values in D(X), respectively. An assignment of a set of variables X is a set of pairs S such that |X | = |S| and for each X ∈ X , there exists (X, v) ∈ S with v ∈ D(X). A constraint C ∈ C is arc consistent (ac) iff, when a variable in the scope of C is assigned any value, there exists an assignment to the other variables in C such that C is satisfied. This satisfying assignment is called a domain support for the value. Similarly, we call a\nrange support an assignment satisfying C, but where values, instead of being taken from the domain of each variable (v ∈ D(X)), can be any integer between the minimum and maximum of this domain following the natural order on Z (v ∈ [min(X), . . . ,max(X)]) . A constraint C ∈ C is range consistent (rc) iff every value of every variable in the scope of C has a range support. A constraint C ∈ C is bounds consistent (bc) iff for every variable X in the scope of C, min(X) and max(X) have a range support. Given a CSP P = (X ,D, C), we shall use the following notation throughout the paper: n shall denote the number of variables, i.e., n = |X |; m shall denote the number of distinct unary assignments, i.e., m = ∑ X∈X |D(X)|; Λ shall denote the total set of values, i.e., Λ = ⋃ X∈X D(X); finally, λ shall denote the total number of distinct values, i.e., λ = |Λ|."
    }, {
      "heading" : "2.2 Soft Global Constraints",
      "text" : "Adding a cost variable to a constraint to represent its degree of violation is now common practice in constraint programming. This model was introduced by Petit, Régin, and Bessiere (2000). It offers the advantage of unifying hard and soft constraints since arc consistency, along with other types of consistencies, can be applied to such constraints with no extra effort. As a consequence, classical constraint solvers can model over-constrained problems in this way without modification. This approach was applied to a number of other constraints, for instance by van Hoeve, Pesant, and Rousseau (2006). Several cost metrics have been explored for the AllDifferent constraint, as well as several others (e.g., Beldiceanu & Petit, 2004). It is important, if one uses such a unifying model, that the cost metric chosen can be evaluated in polynomial time given a complete assignment of the variables that are constrained. This is the case for the two metrics considered in this paper for the constraints AllDifferent and AllEqual.\nThe variable-based cost counts how many variables need to change in order to obtain a valid assignment for the hard constraint. It can be viewed as the smallest Hamming distance with respect to a satisfying assignment. The graph-based cost counts how many times a component of a decomposition of the constraint is violated. Typically these components correspond to edges of a decomposition graph, e.g. for an AllDifferent constraint, the decomposition graph is a clique and an edge is violated if and only if both variables connected by this edge share the same value. The following example, still for the AllDifferent constraint, shows two solutions involving four variables X1, . . . , X4 each with domain {a, b}:\nS1 = {(X1, a), (X2, b), (X3, a), (X4, b)}.\nS2 = {(X1, a), (X2, b), (X3, b), (X4, b)}.\nIn both solutions, at least two variables must change (e.g., X3 and X4) to obtain a valid solution. Therefore, the variable-based cost is 2 for S1 and S2. However, in S1 only two edges are violated, (X1, X3) and (X2, X4), whilst in S2, three edges are violated, (X2, X3), (X2, X4) and (X3, X4). Thus, the graph-based cost of S1 is 2 whereas it is 3 for S2."
    }, {
      "heading" : "2.3 Parameterised Complexity",
      "text" : "We shall use the notion of parameterised complexity in Section 9. We refer the reader to Niedermeier’s (2006) book for a comprehensive introduction. Given a problem A, a\nparameterised version of A is obtained by specifying a parameter of this problem and getting as additional input a non-negative integer k which restricts the value of this parameter. The resulting parameterised problem 〈A, k〉 is fixed-parameter tractable (FPT) with respect to k if it can be solved in time f(k) ∗ nO(1), where f(k) is a function depending only on k. When the size of the problem is significantly larger than the parameter k, a fixed-parameter algorithm essentially has polynomial behaviour. For instance if f(k) = 2k then, as long as k is bounded by log n, the problem can be solved in polynomial time."
    }, {
      "heading" : "3. Taxonomy",
      "text" : "In this section we introduce a taxonomy of soft constraints based on AllDifferent and AllEqual. We consider the eight algorithmic problems related to constraints of difference and equality defined by combining these two constraints, two costs (graph-based and variable-based), and two objectives (minimisation and maximisation). In fact, because the graph-based costs of AllDifferent and AllEqual are dual, only six different problems are defined. Observe that we consider only costs defined through inequalities, rather than equalities. There are several reasons for doing so. First, reasoning about the lower bound or the upper bound of the cost variable can yield two extremely different problems, and hence different algorithmic solutions. For instance, we shall see that in some cases the problem is tractable in one direction, and NP-hard in the other direction. When reasoning about cost equality, one will often separate the inference procedures relative to the lower bound, upper bound, and intermediate values. Reasoning about lower and upper bounds is sufficient to model an equality although it might hinder domain filtering when intermediate values for the cost are forbidden. We thus cover equalities in a restricted way, albeit arguably reasonable in practice. Indeed, when dealing with costs and objectives, reasoning about inequalities and bounds is more useful in practice than imposing (dis)equalities.\nWe close the last remaining cases: the complexity of achieving ac and bc SoftAllEqualminV in Section 4, that of achieving ac on SoftAllEqualminG in Section 5 and that of achieving bc on SoftAllEqualminG in Section 6. Based on these results, Figure 1 can now be completed (fourth and fifth columns).\nThe next six paragraphs correspond to the six columns of Figure 1, that is, to the twelve elements of the taxonomy. For each of them, we briefly outline the current state of the art, using the following assignment as a recurring example to illustrate the various costs:\nS3 = {(X1, a), (X2, a), (X3, a), (X4, a), (X5, b), (X6, b), (X7, c)}.\n3.1 SoftAllDiff: Variable-based cost, Minimisation\nDefinition 1 (SoftAllDiffminV )\nSoftAllDiffminV ({X1, . . . , Xn}, N)⇔ N ≥ n− |{v | ∃Xi = v}|.\nHere the cost to minimise is the number of variables that need to be changed in order to obtain a solution satisfying an AllDifferent constraint. For instance, the cost of S3 is 4 since three of the four variables assigned to a as well as one of the variables assigned to b must change. This objective function was first studied by Petit et al. (2001), and an algorithm for achieving ac in O(n √ m) was introduced. To the best of our knowledge, no\nalgorithm with better time complexity for the special case of bounds consistency has been proposed for this constraint. Notice however that Mehlhorn and Thiel’s (2000) algorithm achieves bc on the AllDifferent constraint with an O(n log n) time complexity. The question of whether this algorithm could be adapted to achieve bc on SoftAllDiffminV remains open.\n3.2 SoftAllDiff: Variable-based cost, Maximisation\nDefinition 2 (SoftAllDiffmaxV )\nSoftAllDiffmaxV ({X1, . . . , Xn}, N)⇔ N ≤ n− |{v | ∃Xi = v}|.\nHere the same cost is to be maximised. In other words, we want to minimise the number of distinct values assigned to the given set of variables, since the complement of this number to n is exactly the number of variables to modify in order to obtain a solution satisfying an AllDifferent constraint. For instance, the cost of S3 is 4 and the number of distinct values is 7 − 4 = 3. This constraint was studied under the name AtMostNValue. An algorithm in O(n log n) to achieve bc was proposed by Beldiceanu (2001), and a proof that achieving ac is NP-hard was given by Bessiere et al. (2006).\n3.3 SoftAllDiff: Graph-based cost, Minimisation & SoftAllEqual: Graph-based cost, Maximisation\nDefinition 3 (SoftAllDiffminG ≡ SoftAllEqualmaxG )\nSoftAllDiffminG ({X1, . . . , Xn}, N)⇔ N ≥ |{{i, j} | Xi = Xj & i < j}|.\nHere the cost to minimise is the number of violated constraints when decomposing AllDifferent into a clique of binary NotEqual constraints. For instance, the cost of S3 is 7 since four variables share the value a (six violations) and two share the value b (one violation). Clearly, it is equivalent to maximising the number of violated binary Equal constraints in a decomposition of a global AllEqual. Indeed, these two costs are complementary to ( n 2 ) of each other (on S3: 7 + 14 = 21). An algorithm in O(nm) for achieving ac on this constraint was introduced by van Hoeve (2004). Again, to our knowledge there is no algorithm improving this complexity for the special case of bc.\n3.4 SoftAllEqual: Graph-based cost, Minimisation & SoftAllDiff: Graph-based cost Maximisation\nDefinition 4 (SoftAllEqualminG ≡ SoftAllDiffmaxG )\nSoftAllEqualminG ({X1, . . . , Xn}, N)⇔ N ≥ |{{i, j} | Xi 6= Xj & i < j}|.\nHere we consider the same two complementary costs, however we aim at optimising in the opposite way. In Section 5 we show that achieving ac on this constraint is NP-hard and, in Section 6 we show that, when domains are contiguous intervals, computing the optimal cost can be done in O(min(nλ2, n3)). As a consequence, bc can be achieved in polynomial time.\n3.5 SoftAllEqual: Variable-based cost, Minimisation\nDefinition 5 (SoftAllEqualminV )\nSoftAllEqualminV ({X1, . . . , Xn}, N)⇔ N ≥ n−max v∈Λ (|{i | Xi = v}|).\nHere the cost to minimise is the number of variables that need to be changed in order to obtain a solution satisfying an AllEqual constraint. For instance, the cost of S3 is 3 since four variables already share the same value. This is equivalent to maximising the number of variables sharing a given value. Therefore this bound can be computed trivially by counting the occurrences of every value in the domains. However, pruning the domains according to this bound without degrading the time complexity is not as trivial. In Section 4, we introduce two filtering algorithms, achieving ac and rc in the same complexity as that of counting values.\n3.6 SoftAllEqual: Variable-based cost, Maximisation\nDefinition 6 (SoftAllEqualmaxV )\nSoftAllEqualmaxV ({X1, . . . , Xn}, N)⇔ N ≤ n−max v∈Λ (|{i | Xi = v}|).\nHere the same cost has to be maximised. In other words we want to minimise the maximum cardinality of each value. For instance, the cost of S3 is 3, that is, the complement to n of the maximum cardinality of a value (3 = 7 − 4). This is exactly equivalent to applying a Global Cardinality constraint (considering only the upper bounds on the cardinalities). Two algorithms, for achieving ac and bc on this constraint and running in O( √ nm) and O(n log n) respectively, was introduced by Quimper et al. (2004).\n4. The Complexity of Arc and Bounds Consistency on SoftAllEqualminV\nHere we show how to achieve ac, rc and bc on the SoftAllEqualminV constraints (see Definition 5). This constraint is satisfied if and only if n minus the cardinality of any set of variables assigned to a single value is less than or equal to the value of the cost variable N . In other words, it is satisfied if there are at least k variables sharing a value, where k = n − max(N). Therefore, for simplicity sake, we shall consider the following equivalent formulation, where N is a lower bound on the complement to n of the same cost (N ′ = n−N):\nN ′ ≤ max v∈Λ (|{i | Xi = v}|).\nWe shall see that to filter the domain of N ′ and the Xi’s we need to compute two properties:\n1. An upper bound k∗ on the number of occurrences amongst all values.\n2. The set of values that can actually appear k∗ times.\nComputing the set of values that appear in the largest possible number of variable domains can be performed trivially in O(m), by counting the number of occurrences of every value, i.e., the number of variables whose domain contains v.\nHowever, if domains are discrete intervals defined by lower and upper bounds, it can be done even more efficiently. Given two integers a and b, a ≤ b, we say that the set of all integers x, a ≤ x ≤ b, is an interval and denote it by [a, b]. In the rest of this section we shall assume that the overall set of values values Λ = ⋃ X∈X D(X) is the interval [1, λ].\nDefinition 7 (Occurrence function and derivative) Given a constraint network P = (X ,D, C), the occurrence function occ is the mapping from values in Λ to N defined as follows:\nocc(v) = |{X | X ∈ X & v ∈ doms(X)}|.\nThe “ derivative” of occ, δocc, maps each value v ∈ Λ to the difference between the value of occ(v − 1) and occ(v):\nδocc(0) = 0, δocc(v) = occ(v)− occ(v − 1).\nWe give an example of the occurrence function for a set of variables with interval domains in Figure 2.\nAlgorithm 1 computes occ−1, that is, the inverse of the occurrence function, which maps every element in the interval [1, n] to the set of values appearing that many times. It runs\nAlgorithm 1: Computing the inverse occurrence function. Data: A set of variables: X Result: occ−1 : [1, n] 7→ 2Λ\nδocc(v)← ∅; 1 foreach X ∈ X do\nδocc(min(X))← δocc(min(X)) + 1; δocc(max(X) + 1)← δocc(max(X) + 1)− 1;\n2 ∀x ∈ [1, n], occ−1(x)← ∅; x← 0; pop first element (v, a) of δocc; repeat\npop first element (w, b) of δocc; x← x+ δocc(a); occ−1(x)← occ−1(x) ∪ [a, b− 1]; a← b;\nuntil δocc = ∅;\nin O(n log n) worst-case time complexity if we assume it is easy to extract both an upper bound (k∗ ≥ N ′) and the set of values that can appear k∗ times from occ−1.\nThe idea behind this algorithm, which we shall reuse throughout this paper, is that when domains are given as discrete intervals one can compute the non-null values of the derivative δocc of the occurrence function occ in O(n log n) time. The procedure is closely related to the concept of sweep algorithms (Beldiceanu & Carlsson, 2001) used, for instance, to implement filtering algorithms for the Cumulative constraint. Instead of scanning the entire horizon, one can jump from an event to the next, assuming that nothing changes between two events. As in the case of the Cumulative constraint, events here correspond to start and end points of the domains. In fact, it is possible to compute the same lower bound, with the same complexity, by using Petit, Régin, and Bessiere’s (2002) Range-based Max-CSP Algorithm (RMA)2 on a reformulation as a Max-CSP. Given a set of variables X , we add an extra variable Z whose domain is the union of all domains in X : D(Z) = Λ = ⋃ X∈X D(X). Then\n2. We thank the anonymous reviewer who made this observation.\nwe link it to other variables in X through binary equality constraints:\n∀X ∈ X , Z = X.\nThere is a one-to-one mapping between the solutions of this Max-CSP and the satisfying assignments of a SoftAllEqualminV constraint on (X , N), where the value of N corresponds to the number of violated constraints in the Max-CSP. The lower bound on the number of violations computed by RMA and the lower bound k∗ on N computed in Algorithm 1 are, therefore, the same. Moreover the procedures are essentially equivalent, i.e., modulo the modelling step. Algorithm 1 can be seen as a particular case of RMA: the same ordered set of intervals is computed, and subsequently associated with a violation cost. However, we use our formalism, since the notion of occurrence function and its derivative is important and used throughout the paper.\nWe first define a simple data structure that we shall use to compute and represent the function δocc. A specific data structure is required since indexing the image of δocc(v) by the value v would add a factor of λ to the (space and therefore time) complexity. The non-zero values of δocc are stored as a list of pairs whose first element is a value v ∈ [1, . . . , λ] and second element stands for δocc(v). The list is maintained in increasing order of the pair’s first element. Given an ordered list δocc = [(v1, o1), . . . , (vk, ok)], the assignment operation δocc(vi)← oi can therefore been done in O(log |δocc|) steps as follows:\n1. The rank r of the pair (vj , oj) such that vj is minimum and vj ≥ vi is computed through a dichotomic search.\n2. If vi = vj , the pair (vj , oj) is removed.\n3. The pair (vi, oi) is inserted at rank r.\nMoreover, one can access the element with minimum (resp. maximum) first element in constant time since it is first (resp. last) in the list. Finally, the value of δocc(vi) is oi if there exists a pair (vj , oj) in the list, and 0 otherwise. Computing this value can also be done in logarithmic time.\nThe derivative δocc(v) is computed in Loop 1 of Algorithm 1 using the assignment operator defined above. Observe that if D(X) = [a, b], then X contributes only to two values of δocc: it increases δocc(a) by 1 and decreases δocc(b + 1) by 1. For every value w such that there is no X with min(X) = w or max(X) + 1 = w, δocc(w) is null. In other words, we can define δocc(v) for any value v, as follows:\nδocc(v) = (|{i | min(Xi) = v}| − |{i | max(Xi) = v − 1}|).\nTherefore, by going through every variable X ∈ X , we can compute the non-null values of δocc in time O(n log n) using the simple list structure described above.\nThen, starting from Line 2, we compute occ−1 by going through the non-zero values v of the derivative, i.e. such that δocc(v) 6= 0, in increasing order of v. Recall that we use an ordered list, so this is trivially done in linear time. By definition, the occurrence function is constant on the interval defined by two such successive values. Since the number of non-zero values of δocc is bounded by O(n), the overall worst-case time complexity is in O(n log n). We use Figure 3 (a,c & d) to illustrate an execution of Algorithm 1. First, six variables and\ntheir domains are represented in Figure 3(a). Then, in Figures 3(c) and 3(d) we show the derivative and the inverse, respectively, of the occurrence function.\nAlternatively, when λ < n log n, it is possible to compute occ−1 in O(n+λ) by replacing the data structure used to store δocc by a simple array, indexed by values in [1, λ]. Accessing and updating a value of δocc can thus be done in constant time.\nNow we show how to prune the variables in X with respect to this bound without degrading the time complexity. According to the method used we can, therefore, achieve ac or rc in a worst-case time complexity of O(m) or O(min(n+ λ, n log n), respectively.\nTheorem 1 Enforcing ac (resp. rc) on SoftAllEqualminV can be achieved in in O(m) steps (resp. O(min(n+ λ, n log n)).\nProof. We suppose, without loss of generality, that the current lower bound on N ′ is k. We first compute the inverse occurrence function either by counting values, or considering interval domains using Algorithm 1. From this we can define the set of values with highest number of occurrences. Let this number of occurrences be k∗, and the corresponding set of values be V (i.e. occ−1(k∗) = V ). Then there are three cases to consider:\n1. First, if every value appears in strictly fewer than k domains (k∗ < k) then the constraint is violated.\n2. Second, if at least one value v appears in the domains of at least k + 1 variables (k∗ > k), then we can build a support for every value w ∈ D(X). Let v ∈ V , we assign all the variables in X \\X with v when possible. The resulting assignment has at least k occurrences of v, hence it is consistent. Consequently, since k∗ > k, every value is consistent.\n3. Otherwise, if neither of the two cases above hold, we know that no value appears in more than k domains, and that at least one appears k times. Recall that V denotes the set of such values. In this case, the pair (X, v) is inconsistent if and only if v 6∈ V & V ⊂ D(X). We first suppose that this condition does not hold and show that we can build a support. If v ∈ V then clearly we can assign every possible variable to v and achieve a cost of k. If V 6⊂ D(X), then we consider w such that w ∈ V and w 6∈ D(X). By assigning every variable with w when possible we achieve a cost of k no matter what value is assigned to X.\nNow we suppose that v 6∈ V & V ⊂ D(X) holds and show that (X, v) does not have an ac support. Indeed, once X is assigned to v the domains are such that no value appears in k domains or more, since every value in V has now one fewer occurrence, hence we are back to Case 1.\nComputing the set V of values satisfying the condition above can be done easily once the inverse occurrence function has been computed. On the one hand, if this function occ−1 has been computed by counting every value in every domain, then the supports used in the proofs are all domain supports, hence ac is achieved. On the other hand, if domains are approximated by their bounds and Algorithm 1 is used instead, the supports are all range supports, hence rc is achieved. In Case 3, the domain can be pruned down to the set V of values whose number of occurrences is k, as illustrated in Figure 3 (b). 2\nCorollary 1 Enforcing bc on SoftAllEqualminV can be achieved in O(min(n+λ, n log n) steps.\nProof. This is a direct implication of Theorem 1. 2\nThe proof of Theorem 1 yields a domain filtering procedure. Algorithm 2 achieves either ac or rc depending on the version of Algorithm 1 used in Line 1 to compute the inverse occurrence function. The later function occ−1 is then used in Line 2, 3 and 4 to, respectively, catch a global inconsistency, prune the upper bound of N ′ and prune the domains of the variables in X .\nFigure 3(b) illustrates the pruning that one can achieve on X provided that the lower bound on N ′ is equal to 4. Dashed lines represent inconsistent intervals. The set V of values used in Line 4 of Algorithm 2 is occ−1(4) = {[15, 40] ∪ [70, 70]}.\nAlgorithm 2: Propagation of SoftAllEqualminV ({X1, . . . , Xn}, N ′). 1 occ−1 ← Algorithm 1; ub← n; while occ−1(ub) = ∅ do\nub← ub− 1; 2 if min(N ′) > ub then fail;\nelse 3 max(N ′)← ub;\nif min(N ′) = max(N ′) then V ← occ−1(min(N ′));\n4 foreach X ∈ X do if V ⊂ D(X) then D(X)← V ;\n5. The Complexity of Arc Consistency on SoftAllEqualminG\nHere we show that achieving ac on SoftAllEqualminG is NP-hard. In order to achieve ac we need to compute an arc consistent lower bound on the cost variable N constrained as follows:\nN ≤ |{{i, j} | Xi 6= Xj & i < j}|.\nIn other words, we want to find an assignment of the variables in X minimising the number of pairwise disequalities, or maximising the number of pairwise equalities. We consider the corresponding decision problem (SoftAllEqualminG -decision), and show that it is NP-hard through a reduction from 3dMatching (Garey & Johnson, 1979).\nDefinition 8 (SoftAllEqualminG -decision) Data: An integer N , a set X of variables. Question: Does there exist a mapping s : X 7→ Λ such that ∀X ∈ X , s[X] ∈ D(X) and |{{i, j} | s[Xi] = s[Xj ] & i 6= j}| ≥ N?\nDefinition 9 (3dMatching) Data: An integer K, three disjoint sets X,Y, Z, and T ⊆ X × Y × Z. Question: Does there exist M ⊆ T such that |M | ≥ K and ∀m1,m2 ∈M, ∀i ∈ {1, 2, 3}, m1[i] 6= m2[i]?\nTheorem 2 (The Complexity of SoftAllEqualminG ) Finding a satisfying assignment for the SoftAllEqualminG constraint is NP-complete even if no value appears in more than three domains.\nProof. The problem SoftAllEqualminG -decision is clearly in NP: checking the number of equalities in an assignment can be done in O(n2) time.\nWe use a reduction from 3dMatching to show completeness. Let P = (X,Y, Z, T,K) be an instance of 3dMatching, where: K is an integer; X,Y, Z are three disjoint sets such that X ∪ Y ∪ Z = {x1, . . . , xn}; and T = {t1, . . . , tm} is a set of triplets over X × Y × Z. We build an instance I of SoftAllEqualminG as follows:\n1. Let n = |X|+ |Y |+ |Z|, we build n variables {X1, . . . , Xn}.\n2. For each tl = 〈xi, xj , xk〉 ∈ T , we have l ∈ D(Xi), l ∈ D(Xj) and l ∈ D(Xk).\n3. For each pair (i, j) such that 1 ≤ i < j ≤ n, we put the value (|T |+ (i− 1) ∗ n+ j) in both D(Xi) and D(Xj).\nWe show there exists a matching of P of size K if and only if there exists a solution of I with b3K+n2 c equalities. We refer to “a matching of P” and to a “solution of I” as “a matching” and “a solution” throughout this proof, respectively. ⇒: We show that if there exists a matching of cardinality K then there exists a solution with at least b3K+n2 c equalities. Let M be a matching of cardinality K. We build a solution as follows. For all tl = 〈xi, xj , xk〉 ∈ M we assign Xi, Xj and Xk to l (item 2 above). Observe that there remain exactly n− 3K unassigned variables after this process. We pick an arbitrary pair of unassigned variables and assign them with their common value (item 3 above), until at most one variable is left (if one variable is left we assign it to an arbitrary value). Therefore, the solution obtained in this way has exactly b3K+n2 c equalities, 3K from the variables corresponding to the matching and bn−3K2 c for the remaining variables. ⇐: We show that if the cardinality of the maximal matching is K, then there is no solution with more than b3K+n2 c equalities. Let S be a solution. Furthermore, let L be the number of values appearing three times in S. Observe that this set of values corresponds to a matching. Indeed, a value l appears in three domains D(Xi),D(Xj) and D(Xk) if and only if there exists a triplet tl = 〈xi, xj , xk〉 ∈ T (item 2 above). Since a variable can only be assigned to a single value, the values appearing three times in a solution form a matching. Moreover, since no value appears in more than three domains, all other values can appear at most twice. Hence the number of equalities in S is less than or equal to b3L+n2 c, where L is the size of a matching. It follows that if there is no matching of cardinality greater than K, there is no solution with more than b3K+n2 c equalities. 2\nCohen, Cooper, Jeavons, and Krokhin (2004) showed that the language of soft binary equality constraints is NP-complete, for as few as three distinct values. On the one hand, Theorem 2 applies to a more specific class of problems where the constraint network formed by the soft binary constraints is a clique. On the other hand, the proof requires an unbounded number of values, these two results are therefore incomparable. However, we shall see in Section 9 that this problem is fixed parameter tractable with respect to the number of values, hence polynomial when it is bounded.\n6. The Complexity of Bounds Consistency on SoftAllEqualminG\nIn this section we introduce an efficient algorithm that, assuming the domains are discrete intervals, computes the maximum possible pairs of equal values in an assignment. We therefore need to solve the optimisation version of the problem defined in the previous section (Definition 8):\nDefinition 10 (SoftAllEqualminG -optimisation) Data: A set X of variables. Question: What is the maximum integer K such that there exists a mapping s : X 7→ Λ satisfying ∀X ∈ X , s[X] ∈ D(X) and |{{i, j} | s[Xi] = s[Xj ] & i 6= j}| = K?\nThe algorithm we introduce allows us to close the last remaining open complexity question in Figure 1: bc on the SoftAllEqualminG constraint. We then improve it by reducing the time complexity thanks to a preprocessing step.\nWe use the same terminology as in Section 4, and refer to the set of all integers x such that a ≤ x ≤ b as the interval [a, b]. Let X be the set of variables of the considered CSP and assume that the domains of all the variables of X are sub-intervals of [1, λ]. We denote by ME(X ) the set of all assignments P to the variables of X such that the number of pairs of equal values of P is the maximum possible. The subset of X containing all the variables whose domains are subsets of [a, b] is denoted by Xa,b. The subset of Xa,b including all the variables containing the given value c in their domains is denoted by Xa,b,c. Finally the number of pairs of equal values in an element of ME(Xa,b) is denoted by Ca,b(X ) or just Ca,b if the considered set of variables is clear from the context. For notational convenience, if b < a, then we set Xa,b = ∅ and Ca,b = 0. The value C1,λ(X ) is the number of equal pairs of values in an element of ME(X ).\nTheorem 3 C1,λ(X ) can be computed in O((n+ λ)λ2) steps.\nProof. The problem is solved by a dynamic programming approach: for every a, b such that 1 ≤ a ≤ b ≤ λ, we compute Ca,b. The main observation that makes it possible to use dynamic programming is the following: in every P ∈ME(Xa,b) there is a value c (a ≤ c ≤ b) such that every variable X ∈ Xa,b,c is assigned value c. To see this, let value c be a value that is assigned by P to a maximum number of variables. Suppose that there is a variable X with c ∈ D(X) that is assigned by P to a different value, say c′. Suppose that c and c′ appear on x and y variables, respectively. By changing the value of X from c′ to c, we increase the number of equalities by x − (y − 1) ≥ 1 (since x ≥ y), contradicting the optimality of P .\nNotice that Xa,b \\ Xa,b,c is the disjoint union of Xa,c−1 and Xc+1,b (if c − 1 < a or c + 1 > b, then the corresponding set is empty). These two sets are independent in the sense that there is no value that can appear on variables from both sets. Thus it can be assumed that P ∈ ME(Xa,b) restricted to Xa,c−1 and Xc+1,b are elements of ME(Xa,c−1) and ME(Xc+1,b), respectively. Taking into consideration all possible values c, we get\nCa,b = max c,a≤c≤b\n(( |Xa,b,c|\n2\n) + Ca,c−1 + Cc+1,b ) . (1)\nIn the first step of Algorithm 3, we compute |Xa,b,c| for all values of a, b, c. For each triple a, b, c, it is easy to compute |Xa,b,c| in time O(n), hence all these values can be computed in time O(nλ3). However, the running time can be reduced to O((n+ λ)λ2) by using the same idea as in Algorithm 1. For each pair a, b, we compute the number of occurrences of each value c by first computing a derivative δa,b. More precisely, we define δa,b(c) = |Xa,b,c|− |Xa,b,c−1| and compute δa,b(c) for every a < c ≤ b (Algorithm 3, Line 1-2). Thus by going through all the variables, we can compute the δa,b(c) values for a fixed a, b and for all a ≤ c ≤ b in time O(n) and we can also compute |Xa,b,a| in the same time bound. Now it is possible to compute the values |Xa,b,c|, a < c ≤ b in time O(λ) by using the equality |Xa,b,c| = |Xa,b,c−1|+ δa,b(c) iteratively (Algorithm 3, Line 3).\nIn the second step of the algorithm, we compute all the values Ca,b. We compute these values in increasing order of b− a. If a = b, then Ca,b = (|Xa,a,a|\n2\n) . Otherwise, values Ca,c−1\nand Cc+1,b are already available for every a ≤ c ≤ b, hence Ca,b can be determined in time O(λ) using Eq. (1) (Algorithm 3, Line 4). Thus all the values Ca,b can be computed in time\nAlgorithm 3: Computing the maximum number of equalities. Data: A set of variables: X Result: C1,λ(X ) ∀ 1 ≤ a, b, c ≤ λ, δa,b(c)← |Xa,b,c| ← Ca,b ← 0; foreach k ∈ [0, λ− 1] do\nforeach a ∈ [1, λ− k] do b← a+ k; foreach X ∈ Xa,b do\n1 δa,b(min(X))← δa,b(min(X)) + 1; 2 δa,b(max(X) + 1)← δa,b(max(X) + 1)− 1; foreach c ∈ [a, b] do 3 |Xa,b,c| ← |Xa,b,c−1|+ δa,b(c); 4 Ca,b ← max(Ca,b, ( (|Xa,b,c| 2 ) + Ca,c−1 + Cc+1,b));\nreturn C1,λ;\nO(λ3), including C1,λ, which is the value of the optimum solution of the problem. Using standard techniques (storing for each Ca,b a value c that minimises (1)), a third step of the algorithm can actually produce a variable assignment that obtains the maximum value. 2\nAlgorithm 3 computes the largest number of equalities one can achieve by assigning a set of variables with interval domains. It can therefore be used to find an optimal solution to either SoftAllDiffmaxG or SoftAllEqual min G . Notice that for the latter one needs\nto take the complement to ( n 2 ) in order to get the value of the violation cost. Clearly, it follows that achieving range or bounds consistency on these two constraints can be done\nin polynomial time, since Algorithm 3 can be used as an oracle for testing the existence of a range support. We give an example of the execution of Algorithm 3 in Figure 4. A set of ten variables, from X1 to X10 are represented. Then we give the table Ca,b for all pairs a, b ∈ [1, λ].\nThe complexity can be further reduced if λ n. Here again, we will use the occurrence function, albeit in a slightly different way. The intuition is that some values and intervals of values are dominated by other. When the occurrence function is monotonically increasing, it means that we are moving toward dominating values (they can be taken by a larger set of variables), and conversely, a monotonic decrease denotes dominated values. Notice that since we are considering discrete values, some variations may not be apparent in the occurrence function. For instance, consider two variables X and Y with respective domains [a, b] and [b + 1, c] such that a ≤ b ≤ c. The occurrence function for these two variables is constant on [a, c]. However, for our purpose, we need to distinguish between “true” monotonicity and that induced by the discrete nature of the problem. We therefore consider some rational values when defining the occurrence function. In the example above, by introducing an extra point b + 12 to the occurrence function, we can now capture the fact that in fact it is not monotonic on [a, c].\nLet X be a set of variables with interval domains in [1, λ]. Consider the occurrence function occ : Q 7→ [0..n], where Q ⊂ Q is a set of values of the form a/2 for some a ∈ N, such that min(Q) = 1 and max(Q) = λ. Intuitively, the value of occ(a) is the number of variables whose domain interval encloses the value a, more formally:\n∀a ∈ Q, occ(a) = |{X | X ∈ X ,min(X) ≤ a ≤ max(X)}|.\nSuch a function, along with the corresponding set of intervals, is depicted in Figure 5. A crest of the function occ is an interval [a, b] ⊆ Q such that for some c ∈ [a, b], occ is monotonically increasing on [a, c] and monotonically decreasing on [c, b]. For instance, on the set intervals represented in Figure 5, [1, 15] is a crest since it is monotonically increasing on [1, 12] and monotonically decreasing on [12, 15].\nLet I be a partition of [1, λ] into a set of intervals such that every element of I is a crest. For instance, I = {[1, 15], [16, 20], [21, 29], [30, 42]} is such a partition for the set of intervals shown in Figure 5. We shall map each element of I to an integer corresponding to its rank in the natural order. We denote by RI(X ) the reduction of X by the partition I. The reduction has as many variables as X (equation 2 below) but the domains are replaced with the set of intervals in I that overlap with the corresponding variable in X (equation 3 below). Observe that the domains remain intervals after the reduction.\nRI(X ) = {X ′1, . . . , X ′|X |}. (2)\n∀X ′i ∈ RI(X ), D(X ′i) = {I | I ∈ I & D(Xi) ∩ I 6= ∅}. (3)\nFor instance, the set of intervals depicted in Figure 5 can be reduced to the set shown in Figure 4, where each element in I is mapped to an integer in [1, 4].\nTheorem 4 If I is a partition of [1, λ] such that every element of I is a crest of occ, then ME(X ) = ME(RI(X )).\nProof. First, we show that for any optimal solution s ∈ME(X ), we can produce a solution s′ ∈ME(RI(X )) that has at least as many equalities as s. Indeed, for any value a, consider every variable X assigned to this value, that is, such that s[X] = a. Let I ∈ I be the crest containing a, by definition we have I ∈ D(X ′). Therefore we can assign all these variables to the same value I.\nNow we show the opposite, that is, given a solution to the reduced problem, one can build a solution to the original problem with at least as many equalities. The key observation is that, for a given crest [a, b], all intervals overlapping with [a, b] have a common value. Indeed, suppose that this is not the case, that is, there exists [c1, d1] and [c2, d2] both overlapping with [a, b] and such that d1 < c2. Then occ(d1) > occ(d1 + 1 2) and similarly occ(c2− 12) < occ(c2). However, since a ≤ d1 < c2 ≤ b, [a, b] would not satisfy the conditions for being a crest, hence a contradiction. Therefore, for a given crest I, and for every variable X ′ such that s′[X ′] = I, we can assign X to this common value, hence obtaining as many equalities. 2\nWe show that this transformation can be achieved in O(n log n) steps. We once again use the derivative of the occurrence function (δocc), however, defined on Q rather than [1, λ]:\nδocc(v)← (|{i | min(Xi) = v}| − |{i | max(Xi) = v − 1\n2 }|).\nMoreover, we can compute it in O(n log n) steps as shown in Algorithm 4. We first compute the non-null values of δocc by looping through each variable X ∈ X (Line 1). We use the\nsame data structure as for Algorithm 1, hence the complexity of this step isO(n log n). Next, we create the partition into crests by going through the derivative once and identifying the inflection points. The variable polarity (Line 3) is used to keep track of the evolution of the function occ. The decreasing phases are denoted by polarity = neg whilst the increasing phases correspond to polarity = pos. We know that a value v is the end of a crest interval when the variable polarity switches from neg to pos. Clearly, the number of elements in δocc is bounded by 2n. Recall that the list data structure is sorted. Therefore, going through the values δocc(v) in increasing order of v can be done in linear time, hence the overall O(n log n) worst-case time complexity.\nAlgorithm 4: Computing a partition into crests. Data: A set of variables: X Result: I δocc ← ∅;\n1 foreach X ∈ X do δocc(min(X))← δocc(min(X)) + 1; δocc(max(X) +\n1 2 )← δocc(max(X) + 12 )− 1;\nI ← ∅; min← max← 1;\n2 while δocc 6= ∅ do 3 polarity ← pos;\nk = 1; repeat\npick and remove the first element (a, k) of δocc; max← round(a)− 1; if polarity = pos & k < 0 then polarity ← neg;\nuntil polarity = pos or k < 0 ; add [min,max] to I; min← max+ 1;\nreturn I\nTherefore, we can replace every crest by a single value at the preprocessing stage and then run Algorithm 3. Moreover, observe that the number of crests is bounded by n, since each needs at least one interval to start and one interval to end. Thus we obtain the following theorem, where n stands for the number of variables, λ for the number of distinct values, and m for the sum of all domain sizes.\nTheorem 5 Enforcing rc on SoftAllEqualminG can be achieved in O(min(λ2, n2)nm) steps.\nProof. If λ ≤ n then one can achieve range consistency by iteratively calling Algorithm 3 after assigning each of the O(m) unit assignments ((X, v) ∀X ∈ X , v ∈ D(X)). The resulting complexity is O(nλ2)m (see Theorem 3, the term λ3 is absorbed by nλ2 due to λ ≤ n).\nOtherwise, if λ > n, the same procedure is used, but after applying the reformulation described in Algorithm 4. The complexity of the Algorithm 4 is O(n log n), and since after the reformulation we have λ = O(n), the resulting complexity is O(n3m). 2"
    }, {
      "heading" : "7. Approximation Algorithm",
      "text" : "We have completed the taxonomy of soft global constraints introduced in Section 3. However, in this section and in the rest of the paper we refine our analysis of the problem of maximising the number of pairs of variables sharing a value, that is, SoftAllEqualminG - optimisation (Definition 10).\nGiven a solution s over a set of variable X , we denote by obj(s) the number of equalities in X . obj(s) = |{{i, j} | s[Xi] = s[X[j] & i 6= j}|. Furthermore, we shall denote as s∗ and obj(s∗) an optimal solution and the number of equalities in this solution, respectively. We first study a natural greedy algorithm for approximating the maximum number of equalities in a set of variables (Algorithm 5). This algorithm picks the value that occurs in the largest number of domains, and assigns as many variables as possible to this value (this can be achieved in O(m)). Then it recursively repeats the process on the resulting sub-problem until all variables are assigned (at most O(n) times). We show that, surprisingly, this straightforward algorithm approximates the maximum number of equalities with a factor of 12 in the worst case. Moreover, it can be implemented to run in O(m) amortised time. We use the following data structures3:\n• var : Λ 7→ 2X maps every value v to the set of variables whose domains contain v.\n• occ : Λ 7→ N maps every value v to the number of variables whose domains contain v.\n• val : N 7→ 2Λ maps every integer i ∈ [0..n] to the set of values appearing in exactly i domains.\nThese data structures are initialised in Lines 1, 2 and 3 of Algorithm 5, respectively. Then, Algorithm 6 recursively chooses the value with largest number of occurrences (Line 2), makes the corresponding assignments (Line 7) while updating the current state of the data structures (Loop 3).\nAlgorithm 5: Computing a lower bound on the maximum number of equalities. Data: A set of variables: X Result: An integer E such that obj(s∗)/2 ≤ E ≤ obj(s∗)\n1 var(v)← ∅, ∀v ∈ ⋃ X∈X D(X);\nforeach X ∈ X do foreach v ∈ D(X) do\nadd X to var(v); 2 occ(v)← |var(v)|, ∀v ∈ ⋃ X∈X D(X); 3 val(k)← ∅, ∀k ∈ [0..n]; foreach v ∈ ⋃ X∈X D(X) do\nadd v to val(|var(v)|); return AssignAndRecurse(var, val, occ, n);\nTheorem 6 (Algorithm Correctness) Algorithm 5 approximates the optimal satisfying assignment of the SoftAllEqualG constraint within a factor of 1 2 and - provided that the data-structure for representing domains respects some assumptions - runs in O(m).\n3. We describe these structures at a lower level in the subsequent proof of complexity.\nAlgorithm 6: procedure AssignAndRecurse of Algorithm 5.\nData: A mapping: var : Λ 7→ 2X , A mapping: val : N 7→ 2Λ, A mapping: occ : Λ 7→ [0..n], An integer: k\n1 while val(k) = ∅ do k ← k − 1; if k ≤ 1 then\nreturn 0;\nelse 2 pick and remove any v ∈ val(k); 3 foreach X ∈ var(v) do if v ∈ D(X) then foreach w 6= v ∈ D(X) do 4 remove w from val(occ(w)); 5 occ(w)← occ(w)− 1; 6 add w to val(occ(w));\n7 assign X with v;\nreturn k(k−1) 2 +AssignAndRecurse(var, val, k);\nProof. We first prove the correctness of the approximation ratio, the soundness of the algorithm and then the complexity of the algorithm.\nApproximation Factor. We proceed using induction on the number of distinct values λ in the current subproblem involving all unassigned variables. Let s be the solution computed by Algorithm 5 and let s∗ be an optimal solution. We denote as P (λ) the proposition “If there are no more than λ values in the union of the domains of X , then obj(s) ≥ obj(s∗)/2”. P (1) implies that every unassigned variable can be assigned to a unique value v. Algorithm 6 therefore chooses this value and assigns all variables to it. In this case obj(s) = obj(s∗).\nNow we suppose that P (λ) holds and we show that P (λ+ 1) also holds. Let the set of variables X of the problem be such that | ⋃ X∈X D(X)| = λ+ 1 and let v be the first value chosen by Algorithm 6. We partition the variables into two subset Xv and X̄v depending on the presence of the value v in their domains.\n• Xv = {X ∈ X | v ∈ D(X)} is the set of variables whose domains contain v.\n• X̄v = X \\ Xv is the complementary set of variables which do not contain the value v.\nUsing these notations, we will partition the equalities into two subsets in order to count them. The first subset of equalities are those involving at least one variable in Xv, the second subset are those restricted to variables in X̄v.\nWe first compute a bound on the number of equalities that one can achieve on X . Let k = |Xv|, let s∗v be an optimal solution on X̄v and let obj(s∗v) be the number of equalities in s∗v. For each variable X ∈ Xv, given any value w in D(X), there are no more than k variables in X containing w. Indeed, v was chosen for maximising this criterion and belongs to the domains of exactly k variables. Therefore, there are at most k(k− 1) equalities that involve at least a variable in Xv, since each one can be involved in at most k− 1 equalities, and there are k of them. Consequently, on the set of variables X , one can achieve at most k(k − 1) + obj(s∗v) equalities.\nOn the other hand, Algorithm 6 assigns every variable in Xv to v and therefore produces k(k − 1)/2 equalities involving at least one variable in Xv. Moreover, observe that since v does not belong to any domain in X̄v, the number of distinct values in X̄v is at most λ.\nThe induction hypothesis P (λ) can therefore be used, hence we know that the number of equalities achieved by Algorithm 5 on the subset X̄v is at least obj(s∗v)/2. Consequently, on the set of variables X , Algorithm 5 achieves at least k(k − 1)/2 + obj(s∗v)/2 equalities.\nSince the lower bound on the number of equalities achieved by the greedy algorithm is half of the upper bound computed above, we can conclude that if P (λ + 1) holds, then P (λ+ 1) also holds.\nCorrectness. Here we show that the mappings occ and val are correctly updated in a call to Algorithm 6. The domain of a variable X changes only when it is assigned to a value v in Line 7. In that case, the occurrence of every value w ∈ D(X) such that w 6= v is decreased by one when assigning X to v. Indeed, for every such value w, occ(w) is decremented and w is removed from val(occ(w) + 1) and added to val(occ(w)).\nComplexity. Now we show that Algorithm 5 runs in O(m) steps under the following assumptions:\n• The values are consecutive and taken from the set {1, . . . , λ}.\n• Assigning a variable to a value can be done in constant time.\n• Checking membership of a value in a variable’s domain can be done in constant time.\nNotice that if the first assumption does not hold, one can rename values. However, it would require a further O(λ log λ) time complexity to sort them, as well as O(nλ) to create a new set of domains.\nFor every 0 ≤ k ≤ n, we use a doubly linked list to represent val(k). Moreover we use a single array index with λ+ 1 elements to store the current position of every value v in the list it appears in (observe that each value appears in exactly one list). To add a value v in val(k) we simply append it at the tail of the list and set its index to the previous length. To remove a value v from val(k), we delete the element at position index[v] in val(k). The total space complexity for this data-structure is therefore O(λ). For each value v, the set of variables var(k) is implemented as a simple list, hence a O(m) space complexity. The mapping occ(v) is represented as an array with one element per value, hence a O(λ) space complexity.\nInitialising all three mappings is done in linear time since each addition requires only constant time. This step can therefore be achieved in O(m) steps. In Line 1 of Algorithm 6, k can be decremented at most n times in total, hence Line 2 is executed at most n times in total.\nObserve that no value is chosen more than once in Line 2. Moreover, the total space complexity of var is O(m). Therefore, the total number of steps in Loop 3 is O(m).\nLast, observe that no pair variable/value (X,w) will be explored more than once in Lines 4, 5 and 6. Indeed, since X is assigned to v in Line 7, it will never pass the condition in Line 3 since subsequent chosen values will not be equal to v. The overall time complexity is thus in O(m).\n2\nTheorem 7 (Tightness of the Approximation Ratio) The approximation factor of 12 for Algorithm 5 is tight.\nProof. Let {X1, . . . , X4} be a set of four variables with domains as follows:\nX1 ∈ {a}; X2 ∈ {b}; X3 ∈ {a, c}; X4 ∈ {b, c}.\nEvery value appears in exactly two domains, hence Algorithm 5 can choose any value. We suppose that the value c is chosen first. At this point no other value can contribute to an equality, hence Algorithm 5 returns 1. However, it is possible to achieve two equalities with the following solution: X1 = a, X3 = a, X2 = b, X4 = b. 2"
    }, {
      "heading" : "8. Tractable Class",
      "text" : "In this section we explore further the connection between the SoftAllEqualminG constraint and vertex matching. We showed earlier that the general case was linked to 3dMatching. We now show that the particular case where no value appears in more than two domains solving the SoftAllEqualG constraint is equivalent to the vertex matching problem on general graphs, and therefore can be solved by a polynomial time algorithm. We shall then use this tractable class to show that SoftAllEqualG is NP-hard only if an unbounded number of values appear in more than two domains.\nDefinition 11 (The VertexMatching Problem) Data: An integer K, an undirected graph G = (V,E). Question: Does there exist M ⊆ E such that |M | ≥ K and ∀e1, e2 ∈ M , e1 and e2 do not share a vertex.\nTheorem 8 (Tractable Class of SoftAllEqualminG ) If all triplets of variables X,Y, Z ∈ X are such that D(X)∩D(Y )∩D(Z) = ∅ then finding an optimal satisfying assignment to SoftAllEqualminG is in P .\nProof. In order to solve this problem, we build a graph GX = (V,E) with a vertex xi for each variable Xi ∈ X , that is, V = {xi | Xi ∈ X}. Then for each pair {i, j} such that D(Xi) ∩ D(Xj) 6= ∅, we create an undirected edge {i, j}; let E = {{i, j} | i 6= j & D(Xi) ∩ D(Xj) 6= ∅}.\nWe first show that if there exists a matching of cardinality K, then there exists a solution with at least K equalities. Let M be a matching of cardinality K of GX , for each edge e = (i, j) ∈ M we assign Xi and Xj to any value v ∈ D(Xi) ∩ D(Xj) (by construction, we know that there exists such a value). Observe that no variable is considered twice since it would mean that two edges of the matching have a common vertex. The obtained solution therefore has at least |M | equalities.\nNow we show that if there exists a solution S with K equalities, then there exists a matching of cardinality K. Let S be a solution, and let M = {{i, j} | S[Xi] = S[Xj ]}. Observe that M is a matching of GX . Indeed, suppose that two edges sharing a vertex (say {i, j}, {j, k}) are both in M . It follows that S[Xi] = S[Xj ] = S[Xk], however this is in contradiction with the hypothesis. We can therefore compute a solution S maximising the number of equalities by computing a maximal matching in GX . 2\nThis tractable class can be generalised by restricting the number of occurrences of values in the domains of variables. The notion of heavy values is key to this result.\nDefinition 12 (Heavy Value) A heavy value is a value that occurs more than twice in the domains of the variables of the problem.\nTheorem 9 (Tractable Class with Heavy Values) If the domain D(Xi) of each variable Xi contains at most one heavy value then finding an optimal satisfying assignment of SoftAllEqualminG is in P .\nProof. Consider a two stage algorithm. In the first stage, we explore every heavy value w and assign w to every variable whose domain contains it. Notice that no variable will be assigned twice. In the second stage, the CSP created by the domains of unassigned variables consists of only values having at most two occurrences, so we solve this CSP by transforming it to the matching problem as suggested in the proof of Theorem 8.\nWe show that there exists an optimal solution where each variable that can be assigned to a heavy value is assigned to this value. Let s∗ be an optimal solution and w be a heavy value over a set T of variables of cardinality t. We suppose that only z < t of them are assigned to w in s∗. Consider the solution s′ obtained by assigning all these t variables to w: we add exactly t(t − 1)/2 − z(z − 1)/2 equalities. However, we potentially remove t−z equalities since values other than w do not appear more than twice. We therefore have obj(s′)−obj(s∗) ≥ t2−3t−z2 +3z, which is non-negative for t ≥ 3 and z < t. By iteratively applying this transformation, we obtain an optimal solution where each variable that can be assigned to a heavy value is assigned to this value. The first stage of the algorithm is thus correct. The second stage is correct by Theorem 8. 2"
    }, {
      "heading" : "9. Parameterised Complexity",
      "text" : "We further advance our analysis of the complexity of the SoftAllEqualminG constraint by introducing a fixed-parameter tractable (FPT) algorithm with respect to the number of values. This result is important because it shows that the complexity of propagating this constraint grows only polynomially in the number of variables. It may therefore be possible to achieve ac at a reasonable computational cost even for a very large set of variables, provided that the total number of distinct values is relatively small.\nWe first show that the SoftAllEqualminG -optimisation problem is FPT with respect to the number of values λ. We use the tractable class introduced in the previous section to generalise this result, showing that the problem is FPT with respect to the number of heavy values occurring in domains containing two or more heavy values. We begin with a definition.\nDefinition 13 (Solution from a Total Order) A solution s≺ is induced by a total order ≺ over the values if and only if\ns[X] = v ⇒ ∀w ≺ v, w 6∈ D(X).\nWe now prove the following key lemma.\nLemma 1 There exists a total order ≺ over the set of values, such that the solution s≺ induced by ≺ is optimal.\nProof. Let s∗ be an optimal solution, v be a value, and occ(s∗, v) be the number of variables assigned to v in s∗. Moreover, let ≺occ be a total order such that values are ranked by decreasing number of occurrences (occ(s∗, v)) and ties are broken arbitrarily. We show that ≺occ induces s∗.\nConsider, without loss of generality, a pair of values v, w such that v ≺occ w. By definition we have occ(s∗, v) ≥ occ(s∗, w). We suppose that the hypothesis is falsified and show that this leads to a contradiction. Suppose that there exists a variable X such that {v, w} ⊆ D(X) and s∗[X] = w (that is, ≺occ does not induce s∗). The objective value of the solution s′ such that s′[X] = v and s′[Y ] = s∗[Y ] ∀y 6= x is given by: obj(s′) = obj(s∗) + occ(s∗, v)− (occ(s∗, w)− 1). Therefore, obj(s′) > obj(s∗). However, s∗ is optimal, hence this is a contradiction. 2\nAn interesting consequence of Lemma 1 is that searching over the space of total orders on values is enough to compute an optimal solution. Moreover, the fixed-parameter tractability of the SoftAllEqualminG constraint follows easily from the same lemma.\nTheorem 10 (FPT – number of values) Finding an optimal satisfying assignment of the SoftAllEqualminG constraint is fixed-parameter tractable with respect to λ, the number of values in the domains of the constrained variables.\nProof. Explore all possible λ! permutations of values. For each permutation create a solution induced by this permutation. Compute the cost of this solution. Return the solution having the highest cost. According to Lemma 1, this solution is optimal. Creating an induced solution can be done by selecting for each domain the first value in the order. Clearly, this can be done in O(m). Computing the cost of the given solution can be done by computing the number of occurrences occ(w) and then summing up occ(w) ∗ (occ(w)− 1)/2 for all values w. Clearly, this can be done in O(m) as well. Hence the theorem follows. 2\nWe can also derive the following corollary from Lemma 1:\nCorollary 2 The number of optimal solutions of the CSP with the SoftAllEqualG is at most λ!.\nProof. According to Lemma 1, each optimal solution is induced by an order over the values of the given problem. Clearly each order induces exactly one solution. Thus the number of optimal solution does not exceed the number of total orders which is at most λ!. 2\nCorollary 2 shows that the number of optimal solutions of the considered problem does not depend on the number of variables and they all can be explored by considering all possible orders of values. We believe this fact is interesting from the practical point of view because in essence it means that even enumerating all optimal solutions is scalable with respect to the number of variables. Moreover, we can show that SoftAllEqualminG is fixed-parameter tractable with respect to the number of conflicting values, defined as follows.\nDefinition 14 (Conflicting Value) A value w of a given CSP is a conflicting value if and only if it is a heavy value and there is a domain D(X) that contains w and another heavy value.\nTheorem 11 (FPT – number of conflicting values) Let k be the number of conflicting values of a CSP comprising only one SoftAllEqualG constraint. Then the CSP can be solved in time O(k! √ nλ), hence SoftAllEqualminG is fixed-parameter tractable with respect to k.\nProof. Consider all the permutations of the conflicting values. For each permutation perform the following two steps. In the first step for each variable X where there are two or more conflicting values, remove all the conflicting values except the one which is the first in the order among the conflicting values of D(X) according to the given permutation. In the second stage we obtain a problem where each domain contains exactly one heavy value. Solve this problem polynomially by the algorithm provided in the proof of Theorem 9.\nLet s be the solution obtained by this algorithm. We show that this solution is optimal. Let p∗ be a permutation of all the values of the considered CSP so that the solution s∗ induced by p∗ has the highest possible cost. By Lemma 1, s∗ is an optimal solution. Let p1 be the permutation of the conflicting values which is induced by p∗ and let s1 be the solution obtained by the algorithm above with respect to p1. By definition of s, obj(s) ≥ obj(s1). We show that obj(s1) ≥ obj(s∗) from which the optimality of s immediately follows.\nObserve that there is no X such that s∗[X] = w and w was removed from D(X) in the first stage of the above algorithm where the permutation p1 is considered. Indeed, w can only be removed from D(X) if it is preceded in p1 by a value v ∈ D(X). It follows that w is also preceded in p∗ by v and consequently s∗(X) 6= w. Thus s∗ is a solution of the CSP obtained as a result of the first stage. However s1 is an optimal solution of that CSP by Theorem 9 and, consequently, obj(s1) ≥ obj(s∗) as required.\nRegarding the runtime, observe that the execution of the algorithm consists of k! running an algorithm for finding the largest bipartite matching of the given graph. This graph has n vertices (corresponding to the variables). Moreover, each edge is associated with a value and no two edges are associated with the same value (because when the matching applies each value has at most two occurrences). It follows that the graph has at most λ edges. According to Micali and Vazirani (1980), the largest matching can be found in O( √ nλ), hence the upper bound. 2 This result shows that the complexity of propagating the SoftAllEqualminG constraint comes primarily from the number of (conflicting) values, whereas other factors, such as the number of variables, have little impact. Notice that detecting conflicting values can be done in linear time (O(m)), by first counting occurrences of every value, then flagging any value with at least two occurrences as “heavy” and finally flagging heavy values as “conflicting” in every domain containing at least two of them.\nObserve, moreover, the “exponential” part of this algorithm is based on the exploration of all possible orders over the given set of conflicting values. In fact the ordering relation between two values matters only if these values belong to a domain of the same variable. In other words consider a graph H on values of the given CSP instance. Two values a and b are connected by an edge if and only if they belong to the domain of the same variable. Instead of considering all possible orders over the given set of values we may consider all possible ways of transforming the given graph into an acyclic digraph. The upper bound on the number of possible transformations is 2E(H) where E(H) is the number of edges of\nH. For sparse graphs such a bound is much more optimistic that k!. For example, if the average degree of a vertex is 4 then the number of considered partial orders is 22k = 4k."
    }, {
      "heading" : "10. Finding a Set of Similar or Diverse Solutions",
      "text" : "Problems of similarity and diversity have a wide range of applications. Finding several diverse solutions can be used to sample the solution space, for instance for product recommendation (Shimazu, 2001), case-based reasoning (Smyth & McClave, 2001; Aha & Watson, 2001) or constraint elicitation (Bessière, Coletta, Koriche, & O’Sullivan, 2005; Gama, Camacho, Brazdil, Jorge, & Torgo, 2005).\nConversely, similarity is important for problems with a periodic aspect. For instance, a schedule or timetable may need to be computed on a weekly basis, but the constraints might change slightly from week to week. In this type of problems the regularity of the solutions, that is, the similarity between each week’s solution, is a very valuable property (Groër, Golden, & Wasil, 2009).\nFinally, finding similar solutions to a set of variants of a problem can be useful to find solutions that are robust to uncertainty. Suppose, for example, that we are to solve a Travelling Salesman Problem (TSP), however, the costs associated with a set of k − 1 links between pairs of cities are uncertain or variable over time. We would like to find an optimal, or near-optimal, route such that when the cost of traversing a link changes, a limited amount of re-routing is sufficient to obtain another near-optimal solution. For that purpose, one can build a similar structure as that pictured in Figure 6 by duplicating the TSP once per uncertain link, the last being the original formulation. In each duplicate, the cost of the corresponding link is then set to some expected upper bound. If we minimise the distance between solutions, we obtain a solution with good properties of robustness: if the cost associated with the ith link increases, the solution of the ith duplicate is a valid alternative avoiding this link (if it degrades the solution quality too much) whilst requiring a small amount of re-routing.\nWe therefore want to find a set of k solutions — either pairwise similar or different — to a set of k problems, distinct or not. A heuristic method was introduced to solve the problem of finding k solutions of a constraint network, such that the minimum (resp. maximum) distance between all pairs of solutions is maximum (resp. minimum) by Hebrard, Hnich, O’Sullivan, and Walsh (2005). Since reasoning on the maximum minimum distance is NPhard (Frances & Litman, 1997), it was proposed to use the sum of the Hamming distances instead. In this section, we first formally define the notion of Hamming distance between variables and between solutions. Next, we show that the constraints studied in this paper can help achieve ac and rc in polynomial time for respectively maximising and minimising the sum of pairwise distances between solutions to a set of problem instances."
    }, {
      "heading" : "10.1 Hamming Distance:",
      "text" : "The Hamming distance between the instantiation of two variables X and Y is defined as follows:\n∆h(X,Y ) = { 1 iff X 6= Y 0 otherwise\nWhereas the Hamming distance between two solutions si and sj (over the sets of variables {Xi1, . . . , Xin} and {X j 1 , . . . , X j n}, respectively) is defined as:\n∆h(si, sj) = ∑\n1≤`≤n ∆h(X\ni `, X j ` )\nGiven a problem P with n variables {X1, . . . , Xn}, we duplicate P k times, with identical constraints if we seek a set of diverse solutions to P , or altered constraints to model the expected scenarios if we seek for a set of similar solutions for some variations of P (see Figure 6).\nThen the objective to maximise or minimise is the sum of the pairwise distances between the (sub-)solutions of the duplicated problems:∑\n1≤i<j≤k ∆h(si, sj) (4)"
    }, {
      "heading" : "10.2 Constraint Formulation:",
      "text" : "The first approaches to this problem relied on heuristic methods (Hebrard et al., 2005; Hentenryck, Coffrin, & Gutkovich, 2009), It was also shown that when the problem P allows it, knowledge compilation methods could efficiently solve this problem (Hadzic, Holland, & O’Sullivan, 2009).\nHere we show that one can achieve arc or bound consistency for maximising this objective function. Whilst arc consistency is NP-hard for minimisation, bounds consistency can be achieved in polynomial time both for minimisation and maximisation. First, we decompose the objective function described previously (Equation 4) using the SoftAllEqualminG or SoftAllEqualmaxG constraints for optimising, respectively, solution similarity or diversity. Then we shall see that achieving ac (resp. bc) on this decomposition is equivalent to achieving ac (resp. bc) on the global constraint defined by bounding the objective.\nRemember that each row in Figure 6 represents a duplicate of the original set of variables {X1, . . . , Xn}. The objective function is defined as the sum of the Hamming distances between every pair of rows. However, consider now Figure 6 by vertical slices. Each column corresponds to the set of duplicates {Xji | 1 ≤ j ≤ k} of an original original variable Xi. One can compute the contribution of this set of variables to the sum of Hamming distances between pairs of rows as the number of pairwise disequalities in the set: |{{j, k} | Xji 6= Xki & j < k}|. Notice that this is precisely the definition of the cost to minimise (resp. maximise) in SoftAllEqualminG (resp. SoftAllEqual max G ).\nTherefore we can model the objective function as the constraint networks shown in Figure 7, respectively for minimisation and maximisation. Notice that, to simplify the model, we use the following, equivalent formulation for SoftAllEqualmaxG , rather than Definition 3:\nSecond, notice that the constraint networks depicted in Figure 7 are such that no two constraints share more than one variable, and there is no Berge-cycle (Berge, 1970) in the constraint hypergraph, that is, a sequence C1, X1, C2, . . . , Xk, Ck+1 such that:\n• X1, . . . , Xk are distinct variables,\n• C1, . . . , Ck+1 are distinct constraints,\n• k ≥ 2 and C1 = Ck+1,\n• Xi is in the scope of Ci and Ci+1.\nIndeed, the SoftAllEqual constraints do not share any variable, and the overlap with the sum constraint is limited to a single variable with each SoftAllEqual. The constraint hypergraph is therefore Berge-acyclic, and in such constraint networks it was shown that propagating ac is sufficient to filter all globally inconsistent values (Janssen & Vilarem, 1988; Jégou, 1991).\nTherefore, when every constraint in this network is ac (resp. rc), the network is globally arc consistent (resp. globally range consistent). We can view these two constraint networks as two global constraints, respectively CN div and CN sim, over the set variables {Xji | 1 ≤ i ≤ n, 1 ≤ j ≤ k} and a variable N to represent the objective:\nCN div({Xji | 1 ≤ i ≤ n, 1 ≤ j ≤ k}, N)⇔ N ≤ ∑\n1≤i≤n Ni & ∀1 ≤ i ≤ n SoftAllEqualmaxG (X1i , . . . , Xki , Ni)\nCN sim({Xji | 1 ≤ i ≤ n, 1 ≤ j ≤ k}, N)⇔ N ≥ ∑\n1≤i≤n Ni & ∀1 ≤ i ≤ n SoftAllEqualminG (X1i , . . . , Xki , Ni)\nTheorems 12 and 13 (where m = ∑\n1≤i≤n |D(X1i )| denotes the sum of the domain sizes in one copy of the problem) follow from, respectively, (van Hoeve, 2004) and Theorem 5:\nTheorem 12 Enforcing ac on CN div({Xji | 1 ≤ i ≤ n, 1 ≤ j ≤ k}, N) can be achieved in O(k2m) steps.\nProof. Since the constraint network equivalent to CN div is Berge-acyclic, we know that it is ac iff every constraint in the decomposition is ac. Moreover, we describe a filtering algorithm that requires only a bounded number of calls to the propagator of each constraint in the decomposition.\nWe assume that no variable’s domain is completely wiped out during the process. If it was the case, the process would be interrupted earlier (as soon as an inconsistency is detected while achieving ac on a component).\nWe introduce some terminology:\n• property (1) denotes the fact that for all 1 ≤ i ≤ n the domains of the variables in Xji are consistent with the upper bounds of Ni,\n• property (2) denotes the fact that for all 1 ≤ i ≤ n the domains of the variables in Xji are consistent with the lower bounds of Ni,\n• property (3) denotes the fact that the sum constraint (N ≤ ∑\n1≤i≤nNi) is bc (or equivalently ac).\nFirst, the domain of some variable Xji for 1 ≤ i ≤ n and 1 ≤ j ≤ k might have changed, as well as the lower bound of N . A change on the upper bound of N either results on an immediate failure, or bears no consequences.\n1. For every i ∈ [1..n], we update the upper bound of the variable Ni by calling the procedure proposed by van Hoeve (2004) to find the maximum possible number of disequalities. Hence property (1) holds.\n2. We achieve bc (equivalent to ac in this case) on the sum constraint. Notice that only the upper bound of N and the lower bounds of Ni for some 1 ≤ i ≤ n will be updated, therefore property (1) and (3) hold.\n3. For every i ∈ [1..n], we prune the domains of the variables in {Xji | 1 ≤ j ≤ k} by calling the filtering procedure proposed by van Hoeve (2004). Since this domain reduction will not trigger any further changes in the bounds of Ni, we know property (1), (2) and (3) hold, hence CN div is ac.\nThe first phase requires O( ∑\n1≤i≤n k 2|D(X1i )|), that is O(k2m) steps. The second phase\nrequires O(n) steps. Finally, the third phase, like the first, requires O(k2m) steps. Hence an overall O(k2m) time complexity.\nTheorem 13 Enforcing rc on CN sim({Xji | 1 ≤ i ≤ n, 1 ≤ j ≤ k}, N) can be achieved in O(k4m) steps.\nProof. This proof is very similar to that of Theorem 12, if we swap upper and lower bounds, and if we use the procedure described in Section 6 for phase (1) and (3).\nThe first phase requires O(k3n) steps. The second phase requires O(n) steps. Finally, the third phase requires O( ∑ 1≤i≤n k\n4|D(X1i )|), that is O(k4m) steps. Hence an overall O(k4m) time complexity."
    }, {
      "heading" : "11. Conclusion",
      "text" : "In many applications we are concerned with stating constraints on the similarity and diversity amongst assignments to variables. To formulate such problems we can use soft variants of the well known AllDifferent and AllEqual constraints. In this paper we considered the global constraints AllDifferent and AllEqual, and their optimisation variants, SoftAllDiff and SoftAllEqual, respectively. Furthermore, we considered two cost functions, based either on the Hamming distance to a satisfying assignment or on the number of violations on the decomposition graph. We have shown that the constraint ensuring an upper bound on the Hamming distance with a solution satisfying the AllEqual constraint can be propagated efficiently, both for arc and bounds consistency. Then we have shown that, on the one hand, deciding the existence of an assignment minimising the number of violation in the decomposition graph of the AllEqual constraint is NP-complete, hence propagating arc consistency on the constraint ensuring this property is NP-hard. On the other hand, propagating bounds consistency on the same constraint can be done in polynomial time. Moreover, we have shown that this problem is fixed parameter tractable in the number of distinct values of the problem. This work complements nicely some earlier results of Cohen et al. (2004) showing that the language of soft binary equality constraints was NP-complete, for as few as three distinct values in the domains. In this paper we have shown that the problem remains NP-complete even if the graph of soft binary equality constraints forms a clique, however, becomes polynomial if the number of values is bounded.\nThis paper therefore provides a comprehensive complexity analysis of achieving ac and bc on an important class of soft constraints of difference and equality. Interestingly, this taxonomy shows that enforcing equality is harder than enforcing difference."
    }, {
      "heading" : "Acknowledgments",
      "text" : "Hebrard, O’Sullivan and Razgon are supported by Science Foundation Ireland (Grant Number 05/IN/I886). Marx is supported in part by the ERC Advanced grant DMMCA, the Alexander von Humboldt Foundation, and the Hungarian National Research Fund (Grant Number OTKA 67651)."
    } ],
    "references" : [ {
      "title" : "Case-Based Reasoning Research and Development, 4th International Conference on Case-Based Reasoning",
      "author" : [ "D.W. Aha", "I. Watson" ],
      "venue" : "ICCBR",
      "citeRegEx" : "Aha and Watson,? \\Q2001\\E",
      "shortCiteRegEx" : "Aha and Watson",
      "year" : 2001
    }, {
      "title" : "Sweep as a Generic Pruning Technique Applied to the Non-Overlapping Rectangles Constraint",
      "author" : [ "N. Beldiceanu", "M. Carlsson" ],
      "venue" : "Proceedings of the 7th International Conference on Principles and Practice of Constraint Programming (CP01),",
      "citeRegEx" : "Beldiceanu and Carlsson,? \\Q2001\\E",
      "shortCiteRegEx" : "Beldiceanu and Carlsson",
      "year" : 2001
    }, {
      "title" : "Cost evaluation of soft global constraints",
      "author" : [ "N. Beldiceanu", "T. Petit" ],
      "venue" : "Proceedings of the 6th International Conference on Integration of AI and OR Techniques in Constraint Programming for Combinatorial Optimization Problems (CPAIOR-04),",
      "citeRegEx" : "Beldiceanu and Petit,? \\Q2004\\E",
      "shortCiteRegEx" : "Beldiceanu and Petit",
      "year" : 2004
    }, {
      "title" : "Pruning for the Minimum Constraint Family and for the Number of Distinct Values Constraint Family",
      "author" : [ "N. Beldiceanu" ],
      "venue" : "Proceedings of the 7th International Conference on Principles and Practice of Constraint Programming (CP01),",
      "citeRegEx" : "Beldiceanu,? \\Q2001\\E",
      "shortCiteRegEx" : "Beldiceanu",
      "year" : 2001
    }, {
      "title" : "Graphs and Hypergraphs. Dunod",
      "author" : [ "C. Berge" ],
      "venue" : null,
      "citeRegEx" : "Berge,? \\Q1970\\E",
      "shortCiteRegEx" : "Berge",
      "year" : 1970
    }, {
      "title" : "A SAT-Based Version Space Algorithm for Acquiring Constraint Satisfaction Problems",
      "author" : [ "C. Bessière", "R. Coletta", "F. Koriche", "B. O’Sullivan" ],
      "venue" : "In Gama et al. (Gama et al.,",
      "citeRegEx" : "Bessière et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Bessière et al\\.",
      "year" : 2005
    }, {
      "title" : "Filtering algorithms for the nvalue",
      "author" : [ "C. Bessiere", "E. Hebrard", "B. Hnich", "Z. Kiziltan", "T. Walsh" ],
      "venue" : "constraint. Constraints,",
      "citeRegEx" : "Bessiere et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Bessiere et al\\.",
      "year" : 2006
    }, {
      "title" : "A maximal tractable class of soft constraints",
      "author" : [ "D. Cohen", "M. Cooper", "P. Jeavons", "A. Krokhin" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Cohen et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Cohen et al\\.",
      "year" : 2004
    }, {
      "title" : "Computers and Intractability: A Guide to the Theory of NP-completeness. W.H",
      "author" : [ "M.R. Garey", "D.S. Johnson" ],
      "venue" : null,
      "citeRegEx" : "Garey and Johnson,? \\Q1979\\E",
      "shortCiteRegEx" : "Garey and Johnson",
      "year" : 1979
    }, {
      "title" : "The Consistent Vehicle Routing Problem",
      "author" : [ "C. Groër", "B. Golden", "E. Wasil" ],
      "venue" : "Manufacturing & Service Operations Management,",
      "citeRegEx" : "Groër et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Groër et al\\.",
      "year" : 2009
    }, {
      "title" : "Reasoning about Optimal Collections of Solutions",
      "author" : [ "T. Hadzic", "A. Holland", "B. O’Sullivan" ],
      "venue" : "Proceedings of the 15th International Conference on Principles and Practice of Constraint Programming (CP-09),",
      "citeRegEx" : "Hadzic et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Hadzic et al\\.",
      "year" : 2009
    }, {
      "title" : "Finding Diverse and Similar Solutions in Constraint Programming",
      "author" : [ "E. Hebrard", "B. Hnich", "B. O’Sullivan", "T. Walsh" ],
      "venue" : "Proceedings of the 20th National Conference on Artificial Intelligence and the Seventeenth Conference on Innovative Applications of Artificial Intelligence (AAAI-05 / IAAI-05),",
      "citeRegEx" : "Hebrard et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Hebrard et al\\.",
      "year" : 2005
    }, {
      "title" : "Constraints of Difference and Equality: A Complete Taxonomic Characterisation",
      "author" : [ "E. Hebrard", "D. Marx", "B. O’Sullivan", "I. Razgon" ],
      "venue" : "Proceedings of the 15th International Conference on Principles and Practice of Constraint Programming",
      "citeRegEx" : "Hebrard et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Hebrard et al\\.",
      "year" : 2009
    }, {
      "title" : "A Soft Constraint of Equality: Complexity and approximability",
      "author" : [ "E. Hebrard", "B. O’Sullivan", "I. Razgon" ],
      "venue" : "Proceedings of the 14th International Conference on Principles and Practice of Constraint Programming (CP-08),",
      "citeRegEx" : "Hebrard et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Hebrard et al\\.",
      "year" : 2008
    }, {
      "title" : "Constraint-based local search for the automatic generation of architectural tests",
      "author" : [ "P.V. Hentenryck", "C. Coffrin", "B. Gutkovich" ],
      "venue" : "Proceedings of the 15th International Conference on Principles and Practice of Constraint Programming (CP09),",
      "citeRegEx" : "Hentenryck et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Hentenryck et al\\.",
      "year" : 2009
    }, {
      "title" : "Problèmes de satisfaction de contraintes: techniques de résolution et application à la synthèse de peptides",
      "author" : [ "P. Janssen", "Vilarem", "M.-C" ],
      "venue" : "C.R.I.M. Research Report.",
      "citeRegEx" : "Janssen et al\\.,? \\Q1988\\E",
      "shortCiteRegEx" : "Janssen et al\\.",
      "year" : 1988
    }, {
      "title" : "Contribution à létude des problèmes de satisfaction de contraintes: algorithmes de propagation et de résolution",
      "author" : [ "P. Jégou" ],
      "venue" : "Propagation de contraintes dans les réseaux dynamiques. Ph.D. thesis",
      "citeRegEx" : "Jégou,? \\Q1991\\E",
      "shortCiteRegEx" : "Jégou",
      "year" : 1991
    }, {
      "title" : "Faster Algorithms for Bound-Consistency of the Sortedness and the Alldifferent Constraint",
      "author" : [ "K. Mehlhorn", "S. Thiel" ],
      "venue" : "Proceedings of the 6th International Conference on Principles and Practice of Constraint Programming (CP-00),",
      "citeRegEx" : "Mehlhorn and Thiel,? \\Q2000\\E",
      "shortCiteRegEx" : "Mehlhorn and Thiel",
      "year" : 2000
    }, {
      "title" : "Invitation to Fixed-Parameter Algorithms",
      "author" : [ "R. Niedermeier" ],
      "venue" : null,
      "citeRegEx" : "Niedermeier,? \\Q2006\\E",
      "shortCiteRegEx" : "Niedermeier",
      "year" : 2006
    }, {
      "title" : "Meta-constraints on Violations for over Constrained Problems",
      "author" : [ "T. Petit", "Régin", "J.-C", "C. Bessiere" ],
      "venue" : "In 12th IEEE International Conference on Tools with Artificial Intelligence",
      "citeRegEx" : "Petit et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Petit et al\\.",
      "year" : 2000
    }, {
      "title" : "Specific Filtering Algorithms for OverConstrained Problems",
      "author" : [ "T. Petit", "Régin", "J.-C", "C. Bessiere" ],
      "venue" : "Proceedings of the 7th International Conference on Principles and Practice of Constraint Programming (CP-01),",
      "citeRegEx" : "Petit et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Petit et al\\.",
      "year" : 2001
    }, {
      "title" : "Range-Based Algorithm for Max-CSP",
      "author" : [ "T. Petit", "Régin", "J.-C", "C. Bessiere" ],
      "venue" : "Proceedings of the 8th International Conference on Principles and Practice of Constraint Programming (CP-02),",
      "citeRegEx" : "Petit et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Petit et al\\.",
      "year" : 2002
    }, {
      "title" : "Improved Algorithms for the Global Cardinality Constraint",
      "author" : [ "Quimper", "C.-G", "A. Lopez-Ortiz", "P. van Beek", "A. Golynski" ],
      "venue" : "Proceedings of the 10th International Conference on Principles and Practice of Constraint Programming (CP-",
      "citeRegEx" : "Quimper et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Quimper et al\\.",
      "year" : 2004
    }, {
      "title" : "A Filtering Algorithm for Constraints of Difference in CSPs",
      "author" : [ "Régin", "J.-C" ],
      "venue" : "Proceedings of the 12th National Conference on Artificial Intelligence",
      "citeRegEx" : "Régin and J..C.,? \\Q1994\\E",
      "shortCiteRegEx" : "Régin and J..C.",
      "year" : 1994
    }, {
      "title" : "Expertclerk: Navigating Shoppers Buying Process with the Combination of Asking and Proposing",
      "author" : [ "H. Shimazu" ],
      "venue" : "Proceedings of the 17th International Joint Conference on Artificial Intelligence",
      "citeRegEx" : "Shimazu,? \\Q2001\\E",
      "shortCiteRegEx" : "Shimazu",
      "year" : 2001
    }, {
      "title" : "Similarity vs. diversity",
      "author" : [ "B. Smyth", "P. McClave" ],
      "venue" : null,
      "citeRegEx" : "Smyth and McClave,? \\Q2001\\E",
      "shortCiteRegEx" : "Smyth and McClave",
      "year" : 2001
    }, {
      "title" : "A hyper-arc consistency algorithm for the soft alldifferent constraint",
      "author" : [ "van Hoeve", "W.-J" ],
      "venue" : "Proceedings of the 10th International Conference on Principles and Practice of Constraint Programming (CP-04),",
      "citeRegEx" : "Hoeve and W..J.,? \\Q2004\\E",
      "shortCiteRegEx" : "Hoeve and W..J.",
      "year" : 2004
    }, {
      "title" : "On Global Warming: Flow-Based Soft Global Constraints",
      "author" : [ "van Hoeve", "W.-J", "G. Pesant", "Rousseau", "L.-M" ],
      "venue" : "Journal of Heuristics,",
      "citeRegEx" : "Hoeve et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Hoeve et al\\.",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "When we wish to enforce that a set of variables take equal values, we can use the AllEqual, or its soft variant for the graph-based cost, the SoftAllEqual constraint (Hebrard, O’Sullivan, & Razgon, 2008), or its soft variant for the variable-based cost, the AtMostNValue constraint (Beldiceanu, 2001).",
      "startOffset" : 282,
      "endOffset" : 300
    }, {
      "referenceID" : 20,
      "context" : "Three of these problems were studied in the past: minimising the cost of SoftAllDiff variable (Petit et al., 2001) and graph-based cost (van Hoeve, 2004) is polynomial whilst maximising the variable-based cost of SoftAllDiff is NP-hard (Bessiere, Hebrard, Hnich, Kiziltan, & Walsh, 2006) for ac and polynomial (Beldiceanu, 2001) for bc.",
      "startOffset" : 94,
      "endOffset" : 114
    }, {
      "referenceID" : 3,
      "context" : ", 2001) and graph-based cost (van Hoeve, 2004) is polynomial whilst maximising the variable-based cost of SoftAllDiff is NP-hard (Bessiere, Hebrard, Hnich, Kiziltan, & Walsh, 2006) for ac and polynomial (Beldiceanu, 2001) for bc.",
      "startOffset" : 203,
      "endOffset" : 221
    }, {
      "referenceID" : 13,
      "context" : "Part of the material presented in this paper is based on two conference publications (Hebrard et al., 2008; Hebrard, Marx, O’Sullivan, & Razgon, 2009).",
      "startOffset" : 85,
      "endOffset" : 150
    }, {
      "referenceID" : 18,
      "context" : "We refer the reader to Niedermeier’s (2006) book for a comprehensive introduction.",
      "startOffset" : 23,
      "endOffset" : 44
    }, {
      "referenceID" : 19,
      "context" : "This objective function was first studied by Petit et al. (2001), and an algorithm for achieving ac in O(n √ m) was introduced.",
      "startOffset" : 45,
      "endOffset" : 65
    }, {
      "referenceID" : 20,
      "context" : "References: [1] (Petit et al., 2001), [2] (Bessiere et al.",
      "startOffset" : 16,
      "endOffset" : 36
    }, {
      "referenceID" : 6,
      "context" : ", 2001), [2] (Bessiere et al., 2006), [3] (Beldiceanu, 2001), [4] (van Hoeve, 2004), [5] (Hebrard et al.",
      "startOffset" : 13,
      "endOffset" : 36
    }, {
      "referenceID" : 3,
      "context" : ", 2006), [3] (Beldiceanu, 2001), [4] (van Hoeve, 2004), [5] (Hebrard et al.",
      "startOffset" : 13,
      "endOffset" : 31
    }, {
      "referenceID" : 13,
      "context" : ", 2006), [3] (Beldiceanu, 2001), [4] (van Hoeve, 2004), [5] (Hebrard et al., 2008), [6] (Hebrard et al.",
      "startOffset" : 60,
      "endOffset" : 82
    }, {
      "referenceID" : 12,
      "context" : ", 2008), [6] (Hebrard et al., 2009), [7] (present paper), [8] (Quimper et al.",
      "startOffset" : 13,
      "endOffset" : 35
    }, {
      "referenceID" : 22,
      "context" : ", 2009), [7] (present paper), [8] (Quimper et al., 2004).",
      "startOffset" : 34,
      "endOffset" : 56
    }, {
      "referenceID" : 17,
      "context" : "Notice however that Mehlhorn and Thiel’s (2000) algorithm achieves bc on the AllDifferent constraint with an O(n log n) time complexity.",
      "startOffset" : 20,
      "endOffset" : 48
    }, {
      "referenceID" : 3,
      "context" : "An algorithm in O(n log n) to achieve bc was proposed by Beldiceanu (2001), and a proof that achieving ac is NP-hard was given by Bessiere et al.",
      "startOffset" : 57,
      "endOffset" : 75
    }, {
      "referenceID" : 3,
      "context" : "An algorithm in O(n log n) to achieve bc was proposed by Beldiceanu (2001), and a proof that achieving ac is NP-hard was given by Bessiere et al. (2006).",
      "startOffset" : 57,
      "endOffset" : 153
    }, {
      "referenceID" : 22,
      "context" : "Two algorithms, for achieving ac and bc on this constraint and running in O( √ nm) and O(n log n) respectively, was introduced by Quimper et al. (2004).",
      "startOffset" : 130,
      "endOffset" : 152
    }, {
      "referenceID" : 3,
      "context" : "The procedure is closely related to the concept of sweep algorithms (Beldiceanu & Carlsson, 2001) used, for instance, to implement filtering algorithms for the Cumulative constraint. Instead of scanning the entire horizon, one can jump from an event to the next, assuming that nothing changes between two events. As in the case of the Cumulative constraint, events here correspond to start and end points of the domains. In fact, it is possible to compute the same lower bound, with the same complexity, by using Petit, Régin, and Bessiere’s (2002) Range-based Max-CSP Algorithm (RMA)2 on a reformulation as a Max-CSP.",
      "startOffset" : 69,
      "endOffset" : 549
    }, {
      "referenceID" : 24,
      "context" : "Finding several diverse solutions can be used to sample the solution space, for instance for product recommendation (Shimazu, 2001), case-based reasoning (Smyth & McClave, 2001; Aha & Watson, 2001) or constraint elicitation (Bessière, Coletta, Koriche, & O’Sullivan, 2005; Gama, Camacho, Brazdil, Jorge, & Torgo, 2005).",
      "startOffset" : 116,
      "endOffset" : 131
    }, {
      "referenceID" : 24,
      "context" : "Finding several diverse solutions can be used to sample the solution space, for instance for product recommendation (Shimazu, 2001), case-based reasoning (Smyth & McClave, 2001; Aha & Watson, 2001) or constraint elicitation (Bessière, Coletta, Koriche, & O’Sullivan, 2005; Gama, Camacho, Brazdil, Jorge, & Torgo, 2005). Conversely, similarity is important for problems with a periodic aspect. For instance, a schedule or timetable may need to be computed on a weekly basis, but the constraints might change slightly from week to week. In this type of problems the regularity of the solutions, that is, the similarity between each week’s solution, is a very valuable property (Groër, Golden, & Wasil, 2009). Finally, finding similar solutions to a set of variants of a problem can be useful to find solutions that are robust to uncertainty. Suppose, for example, that we are to solve a Travelling Salesman Problem (TSP), however, the costs associated with a set of k − 1 links between pairs of cities are uncertain or variable over time. We would like to find an optimal, or near-optimal, route such that when the cost of traversing a link changes, a limited amount of re-routing is sufficient to obtain another near-optimal solution. For that purpose, one can build a similar structure as that pictured in Figure 6 by duplicating the TSP once per uncertain link, the last being the original formulation. In each duplicate, the cost of the corresponding link is then set to some expected upper bound. If we minimise the distance between solutions, we obtain a solution with good properties of robustness: if the cost associated with the ith link increases, the solution of the ith duplicate is a valid alternative avoiding this link (if it degrades the solution quality too much) whilst requiring a small amount of re-routing. We therefore want to find a set of k solutions — either pairwise similar or different — to a set of k problems, distinct or not. A heuristic method was introduced to solve the problem of finding k solutions of a constraint network, such that the minimum (resp. maximum) distance between all pairs of solutions is maximum (resp. minimum) by Hebrard, Hnich, O’Sullivan, and Walsh (2005). Since reasoning on the maximum minimum distance is NPhard (Frances & Litman, 1997), it was proposed to use the sum of the Hamming distances instead.",
      "startOffset" : 117,
      "endOffset" : 2211
    }, {
      "referenceID" : 11,
      "context" : "The first approaches to this problem relied on heuristic methods (Hebrard et al., 2005; Hentenryck, Coffrin, & Gutkovich, 2009), It was also shown that when the problem P allows it, knowledge compilation methods could efficiently solve this problem (Hadzic, Holland, & O’Sullivan, 2009).",
      "startOffset" : 65,
      "endOffset" : 127
    }, {
      "referenceID" : 4,
      "context" : "Second, notice that the constraint networks depicted in Figure 7 are such that no two constraints share more than one variable, and there is no Berge-cycle (Berge, 1970) in the constraint hypergraph, that is, a sequence C1, X1, C2, .",
      "startOffset" : 156,
      "endOffset" : 169
    }, {
      "referenceID" : 16,
      "context" : "The constraint hypergraph is therefore Berge-acyclic, and in such constraint networks it was shown that propagating ac is sufficient to filter all globally inconsistent values (Janssen & Vilarem, 1988; Jégou, 1991).",
      "startOffset" : 176,
      "endOffset" : 214
    }, {
      "referenceID" : 7,
      "context" : "This work complements nicely some earlier results of Cohen et al. (2004) showing that the language of soft binary equality constraints was NP-complete, for as few as three distinct values in the domains.",
      "startOffset" : 53,
      "endOffset" : 73
    } ],
    "year" : 2011,
    "abstractText" : "In many combinatorial problems one may need to model the diversity or similarity of sets of assignments. For example, one may wish to maximise or minimise the number of distinct values in a solution. To formulate problems of this type we can use soft variants of the well known AllDifferent and AllEqual constraints. We present a taxonomy of six soft global constraints, generated by combining the two latter ones and the two standard cost functions, which are either maximised or minimised. We characterise the complexity of achieving arc and bounds consistency on these constraints, resolving those cases for which NP-hardness was neither proven nor disproven. In particular, we explore in depth the constraint ensuring that at least k pairs of variables have a common value. We show that achieving arc consistency is NP-hard, however bounds consistency can be achieved in polynomial time through dynamic programming. Moreover, we show that the maximum number of pairs of equal variables can be approximated by a factor of 12 with a linear time greedy algorithm. Finally, we provide a fixed parameter tractable algorithm with respect to the number of values appearing in more than two distinct domains. Interestingly, this taxonomy shows that enforcing equality is harder than enforcing difference.",
    "creator" : "TeX"
  }
}