{
  "name" : "1606.02877.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Understanding User Instructions by Utilizing Open Knowledge for Service Robots",
    "authors" : [ "Dongcai Lu", "Feng Wu", "Xiaoping Chen" ],
    "emails" : [ "RoboCup@Home" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms—Service Robots, Human-Robot Interaction, Natural Language Understanding and Task Planning.\nI. INTRODUCTION\nNOWADAYS, service robots can do more and more workin our daily life, such as moving around in a house, fetching drink or medicine for elderly people, or preparing food for a family. They are smart and can do many complex tasks autonomously. Nevertheless, when robots encounter user requests or tasks in an open-ended form (e.g., through dialogs in natural language), they often fail to response properly, not only due to possible language processing failures but also the challenges of task planning with incomplete knowledge. For example, as illustrated in Figure 1, a daily instruction “clean up toys” is challenging for a robot to process if the action “clean up” is under-specified and “have a headache” is also nontrivial for a robot to offer help to people without grounding the helping verb (i.e., knowing how to help). These are common tasks in domestic scenarios and therefore it is desirable for service robots to be able to complete such tasks given user instructions in natural language.\nTypically, user instructions are action-directed in the sense that the fundamental purpose of an instruction is to specify what users want a robot to do for them. This indicates a connection between robot understanding (i.e., knowing what the users said) and acting (i.e., doing what the users asked). In other words, understanding an instruction means that the robot is able to generate a plan (i.e., sequence of actions) for the tasks specified in the instruction [1], [2], [3], [4], [5], [6]. Therefore, it is crucial for the robot to have the knowledge about the tasks and actions in order to do planning. However,\nsome knowledge may be missing in the instruction (e.g., “have a headache” does not directly indicate that the robot should given the user an aspirin). Consequently, the robot does not know how to act when such instructions are presented.\nFortunately, there is more and more common knowledge available in open resources, such as the Open Mind Indoor Common Sense (OMICS) database [7], wikihow1, WordNet, and many other digital dictionaries. In these dictionaries, actions are often hierarchical where a high-level action is composed of several lower-level actions. Similarly, user instructions are often specified hierarchically in which an action is referred by an action verb or verb phrase. For instance, “clean up a house” may indicate a series of subtasks such as “clean the table”, “clean the floor”, etc. Therefore, commonsense knowledge about hierarchical relations between tasks and subtasks is useful for instruction understanding.\nIn our previous studies [8], we found that a user instruction representing a high-level task can usually be reduced into a sequence of low-level subtasks, using hierarchical knowledge in open resources. Furthermore, we observed that this reduction procedure often ends up at so-called primitive tasks (i.e., lowlevel subtasks expressed in common verbs [9]). For instance, in OMICS, “serve a drink from fridge” is reduced into a sequence of low-level subtasks expressed in common verbs, such as “go to fridge”, “open the fridge door”, and “take the drink”, where ‘go’, ‘open’, and ‘take’ are common verbs. Ideally, if all of the primitive tasks in the reduction can be directly mapped into robot’s actions, the robot can simply complete the task by executing those actions.\nHowever, it is generally nontrivial to map primitive tasks\n1http://www.wikihow.com\nar X\niv :1\n60 6.\n02 87\n7v 1\n[ cs\n.R O\n] 9\nJ un\n2 01\n6\n2 to robot’s actions. One of the key challenges is that there is little knowledge about common verbs in most open resources and furthermore how they can be executed by robot with its actions. To avoid this challenge, most of the existing approaches [3], [4], [10] manually create a small set of handcoded robot actions for primitive tasks though their scalability (i.e., only work for small problems) and generality (i.e., only work for specific domains) are limited. To build a generalpurpose system for handling large-scale user instructions, we directly tackle this challenge and consider the follow problems: 1) how to define semantics of meanings of common verbs, match and recover such semantics in user instructions and 2) how to handle a large number of instructions and generate plans in realtime using open knowledge resources.\nTo address these problems, we propose a novel system for service robots to 1) process user instructions based on semantic roles of common verbs defined in semantic dictionaries, and 2) then generate plans for the corresponding tasks of user instructions. The semantic roles suggest possible entities in the knowledge representation that may be missing from or omitted in natural instructions. In more detail, we introduce a heuristic method to match and recover missing semantic roles from the context of user instructions. Then, we use a planner based on Answer Set Programming (ASP) [11] to exploit definitions of common verbs in terms of semantic roles and generate a plan for the task specified in the user instruction. By putting them together, we built a general-purpose system for service robots that can handle large-scale user instructions using open commonsense knowledge.\nTo evaluate our approach, we conducted a corpus-based experiment on two test sets with 11885 user tasks and 467 user desires collected from OMICS. We also developed a prototype system and ran a case study on a service robot in two typical domestic scenarios. Our experimental results show substantial improvement in performance on user instruction understanding. It is worth pointing out that the proposed system has been successfully deployed in our KeJia2 robot, which participates annually RoboCup@Home3 competition and won the first place once and the second place twice in the pass three years. During the benchmark tests of the RoboCup@Home competitions, our system is used by our robot for understanding the instructions in English given by referees and completing the corresponding tasks. This confirms the usefulness of our system in practice.\nThe remainder of this article is organized as follows. Section II introduces our problem and Section III presents an overview of our system. Then, Section IV proposes our main algorithms, followed by Sections V and VI describing the two key techniques used in our algorithms. Next, Section VII reports our experimental results. Finally, Section VIII briefly reviews the related work and Section IX concludes."
    }, {
      "heading" : "II. PROBLEM STATEMENT",
      "text" : "We aim to building a general-purpose system so that the robot can understand user instructions and provide service for the\n2http://ai.ustc.edu.cn/en/robocup/atHome/index.php 3http://www.robocup.org/robocup-home/\nFig. 2. System architecture.\nuser. To this end, we must solve the problem of generating a sequence of primitive actions, which can be directly executed by a robot, given user instructions in natural language. For example, when a user says: “please serve a meal for me”, the robot will take the meal, put it on a plate, and place the plate on a table; when a user says: “I am thirsty”, the robot will take a drink from the fridge and deliver it to the user. To achieve this, our system must be able to extract a task from a user instruction in natural language (i.e., knowing what the user said) and generate a executable plan for the task (i.e., doing what the user asked). In other words, natural language understanding and task planning must be combined systematically in order to solve our problem.\nIn the next section, we give an overview of our system for instruction understanding and task planning that is built by integrating different modules."
    }, {
      "heading" : "III. SYSTEM OVERVIEW",
      "text" : "The overall architecture of our system is shown in Figure 2. As we can see, the human-robot dialog system transcribes spoken utterances into text sentences and manages the dialog with users. Each sentence in the dialog is then transferred to the processing module, which generates a sequence of primitive actions for the task expressed in natural language. After that, a sequence of commands corresponding to each primitive action is computed by the Motion Planning module. Finally, the commands are executed by the Robot Control module.\nHere, we focus on the Processing module that takes a text sentence as its input and outputs a sequence of primitive actions that are executable by the robot. The main components of our Processing module are described in detail as follows."
    }, {
      "heading" : "A. Open Knowledge",
      "text" : "As shown in Figure 2, we use open knowledge both for Natural Language Processing (NLP) and task planning. The Open Knowledge considered in our system includes OMICS, FrameNet, and Re-FrameNet as introduced below.\nOMICS [7] is an extensive collection of knowledge for indoor service robots gathered from internet users. Currently, it contains 48 tables capturing different sorts of knowledge,\n3 among which the Help and Tasks/Steps tables are most useful for our system. Each tuple of the Help table maps a user desire to a task that may meet the desire (e.g., 〈 “feel thirsty”, “by offering drink” 〉). Each tuple of the Tasks/Steps table decomposes a task into several steps (e.g., 〈 “serve a drink”, 0. “get a glass”, 1. “get a bottle”, 2. “fill class from bottle”, 3. “give class to person” 〉). Given this, OMICS offers useful knowledge about hierarchism of naturalistic instructions, where a high-level user request (e.g., “serve a drink”) can be reduced to lower-level tasks (e.g., “get a glass”, · · · ). Another feature of OMICS is that elements of any tuple in an OMICS table are semantically related according to a predefined template. This facilitates the semantic interpretation of the OMICS tuples.\nFrameNet4 is a digital dictionary providing rich semantic information for action verbs. It groups action verbs into Frames and specifies word definitions in terms of semantic roles called Frame Elements (FEs) for each Frame [12]. Although the connections between an action verb and its semantic roles are useful for resolving under-specification of naturalistic instructions, this knowledge cannot be directly used by robots since it is not formalized in FrameNet. To overcome this difficulty, we developed Re-FrameNet — a formalized version of FrameNet by rewriting part of FrameNet knowledge in a formal meta-language.\nSpecifically, in Re-FrameNet5, a Frame of FrameNet is formalized as a meta-task and re-defined by a set of precondition, postcondition, invariant, and/or steps over semantic roles of the meta-task. In the definition, FEs (i.e., semantic roles) such as Theme, Source, and Goal of the Frame are taken as meta-variables. Therefore, the definition of a metatask specifies the common semantic structure of action verbs in the corresponding Frame. For example, the meta-task putPlacing is defined as:\n( define ( meta-task put-Placing ( :parameters ?Agent ?Theme ?Source ?Goal)) ( :precondition ...) ( :postcondition ...) ( :invariant ...) )\nwhere all action verbs in Frame Placing (e.g., lay, heap, deposit) share the same definition. When a robot tries to plan with put-Placing as its action verb (verb sense) for an instruction, our NLP components will try to extract appropriate entities for every semantic roles specified in the definition of meta-task put-Placing (See Section V for more detail).\nIt is worth noting that common verbs are normally not explained in the aforementioned open resources because most of them belong to the so-called General Service List (GSL) — a list of roughly 2000 most frequent English words [9]. The GSL is taken as the defining vocabulary of dictionaries such as the Longman Dictionary of Contemporary English, based on the notion that words should be defined using “terms less abstruse than the word that is to be explained” [13]. As a result, there are few definitions of the GSL verbs in OMICS or other digital dictionaries.\n4https://framenet.icsi.berkeley.edu/fndrupal/ 5http://ai.ustc.edu.cn/en/research/reframenet.php"
    }, {
      "heading" : "B. NLP Module",
      "text" : "This module maps user instruction in natural language I to the OMICS tables, which contains tuple 〈task, steps〉 for taskoriented instructions or tuple 〈desire, task〉 for desire-oriented instructions (See Section IV for more detail). The output is a logical form L to the Planning module, containing a framesemantic representation as:\n(meta-task take-Taking ( : parameters food fridge) )\nSpecifically, interpreting I to L is done in three steps: 1) dependency parsing that analyzes the dependencies of each word in a sentence, 2) frame-semantic parsing that identifies the verb’s frame, and 3) semantic matching and recovering that fills the semantic roles for a given frame. In Section V, each step will be described in detail."
    }, {
      "heading" : "C. Planning Module",
      "text" : "The Planning module takes the logical form of user instruction L, online knowledge base (e.g., Re-FrameNet, WordNet, FrameNet), domain knowledge, and robot’s skills as the inputs. The output of the Planning module is a high-level plan for the motion planning module.\nWe employ both global and local planners in the Planning module. The global planner searches through the whole knowledge of task decomposition in OMICS to generate a plan. However, most of tasks is OMICS cannot be decomposed into robot’s primitive actions because many steps in OMICS are referred by common verbs, for which OMICS does not contain decomposition knowledge. For example, verbs such as take, place, put, get, and turn frequently occur in task steps but there is no knowledge in OMICS about how to execute them by the robot. Therefore, a local planner based on ASP is used for planning based on merely the instruction itself.\nNote that the local planner is incapable of generating a plan for under-specification terms in an instruction. Therefore, common verbs referred by the instruction must be specified first in order to generate a plan. Fortunately, semantic dictionaries such as FrameNet provide rich knowledge about common verbs. In Re-FrameNet, we reorganize the definition of an action verb by a set of precondition, postcondition, and invariant over semantic roles of the action (a.k.a., the functional definition of action). Given this, a planner based on ASP can plan actions for the instruction using the formalized functional definition of an action. Section VI will give more detail about our planning method."
    }, {
      "heading" : "D. Skills and Action Model",
      "text" : "For a robot, we define an Action Model to specify its skills. Specifically, an Action Model consists of several primitive actions. Each primitive action a is defined by a set of precondition, postcondition and invariant, similar to the definition of a common verb in Re-FrameNet. In other words, they specify conditions under which a can be executed, conditions that hold when a finishes, and conditions that must be satisfied during the execution of a respectively. Indeed, a primitive action is the formal specification of a robot skill. As we will show later sections, the Action Model is useful for our system to generate a plan that is executable by the robot.\n4 Algorithm 1 SolveTask(task t, ActionModel AM) 1: gSeen := ∅ /* prevent infinite recursive loop when exploratory\nsearching itself */ 2: initiate worldmodel and plans 3: if t ∈ gSeen then 4: return null 5: end if 6: gSeen = gSeen ∪ t 7: subTasks = FindSubTasks(t) /* find subtasks of task t from the\nTasks/Steps table in OMICS */ 8: for each task s in subTasks do 9: if GeneratePlans(s, AM) = null then\n10: FoundEqualTask = False 11: while there is a new t ′ from the Tasks/Steps table semantically equivalent to s do 12: if SolveTask( t ′ ,AM ) 6= null then 13: FoundEqualTask = True 14: plans.append(SolveTask(t ′ , AM)) 15: wordmodel = simulator(wordmodel, plans) 16: break 17: end if 18: end while 19: if FoundEqualTask = False then 20: return null 21: end if 22: else 23: plans.append(GeneratePlans(s,AM)) 24: wordmodel = simulator(wordmodel, plans) 25: end if 26: end for/*successfully planned*/ 27: return plans /* all steps have been solved. */"
    }, {
      "heading" : "E. Learning Module",
      "text" : "In this module, methods such as log linear, Conditional Random Field (CRF), Learn from Demonstration (LfD) are used to learn robot’s low-level skills. Intuitively, the more skills a robot possesses, the more capable it is. For example, unless a robot knows how to pour water to a cup, it cannot finish the high-level task such as “make a coffee” (with the task-step tuple 〈 “make a coffee”, 0. “put hot water in a cup”, 1. “pour the coffee” 〉). In this paper, we assume that our robot has all necessary low-level skills to complete a task specified by user instructions though most of the skills must be learned one by one in practice. The learning methods for robot skills are interesting but beyond the scope of this article.\nAfter introducing our system as a whole, we describe our main algorithms for instruction understanding next."
    }, {
      "heading" : "IV. UNDERSTANDING USER INSTRUCTIONS",
      "text" : "There are two types of user instructions that we consider in this article: 1) task-oriented instruction (e.g., “serve a meal”) and 2) desire-oriented instruction (e.g., “I am thirsty”). In OMICS, a task-oriented instruction is represented as tuple 〈t, s〉, where s = 〈s1, s2, · · · , sn〉 is a sequence of the n steps to complete the task t. For example, given task t = “serve a meal”, a sequence of steps may be s = 〈s1 : “take the meal”, s2 : “put it on a plate”, s3 : “place the plate on a table”〉. Similarly, a desire-oriented instruction is represented as tuple 〈d, t〉, where t is the task corresponding to the user desire d. For instance, given user desire d = “I am thirsty”, the task for a robot may be t = “serve a drink”. Indeed, in most of the domestic\nAlgorithm 2 SolveHelp(desire t, ActionModel AM ) 1: AllHelps := FindHelpsMaptoDesire(desire t)\n/* find all help tasks mapped to desire t*/ 2: for each help task s in AllHelps do 3: if GeneratePlans(s, AM ) = null then 4: for task gs in Tasks/Steps Table do 5: if gs semantically equivalent to s then 6: return SolveTask(gs,AM) 7: end if 8: end for 9: else\n10: return GeneratePlans(s, AM ) 11: end if 12: end for 13: return null\nAlgorithm 3 GeneratePlans(task t, ActionModel AM ) 1: /* generate a plan for low-level task t*/ 2: sem := SemanticMatchAndRecover(t) 3: if sem.frame = NULL then 4: return null 5: end if 6: if sem.frame ∈ AM then 7: return sem.frame(sem.parameters) 8: else 9: gRFN := FindRFNBySem(sem.frame) /* find the definition of sem.frame in Re-FrameNet */ 10: Res = solver(gRFN, sem,AM ) /* compute a plan by inputting rules of gRFN, sem and AM. */ 11: if Res 6= NULL then 12: return Res 13: else 14: return null 15: end if 16: end if\nscenarios, a user instruction is usually either task-oriented or desire-oriented. Now, we turn to our algorithms for generating a plan for these two types of user instructions respectively.\nAlgorithm 1 is used to process task-oriented instructions by utilizing the Tasks/Steps table in OMICS. The input is a naturally expressed task t and the robot’s action model AM and the output is a sequence of primitive actions plans. Specifically, it first finds all subtasks of task t from the Tasks/Steps table of OMICS. Then, it tries to generate a plan (i.e., a sequence of primitive actions) for each subtask. If a plan is successfully generated, the plan is added to the plan list plans and the simulator advances to the next subtask. Otherwise, it searches the Tasks/Steps table of OMICS again for all Semantically Equivalent (SE) tasks6 of that subtask until one of the SE tasks is successfully planned. If there is no SE task or none of the SE tasks can be successfully planned, a null is returned to indicate the failure of task planning. After all subtasks are successfully planned, plans are returned and executed by the robot.\nAlgorithm 2 is used to process desire-oriented instructions by utilizing the Help table in OMICS. Similarly, the input is a desire and an action model and the output is a plan. Specifically, it first finds a list of help tasks offering the\n6For example, the tasks of “give someone an object” and “take an object to someone” are semantically equivalent.\ncorresponding help when given a desire. Then, it tries to plan for each of the help tasks by checking whether the help task can be successfully planned with a sequence of primitive actions. If so, the resulting plan is returned. Otherwise, it\nsearches the Tasks/Steps table in OMICS for a SE task of the help task and calls Algorithm 1 to generate the plan.\nNotice that both Algorithms 1 and 2 depend on Algorithm 3 to generate a plan for a low-level task t. In Algorithm 3, it first performs semantic role matching and recovering for task t and outputs a frame and its roles. If no verb frame is identified, the process terminates with null as no plan can be generated. If the frame is a primitive action, this frame plus its roles are returned. Otherwise, the frame is evoked by common verbs. In this case, it first finds the definition of sem.frame in Re-FrameNet and translate it to a set of rules. After that, it computes a plan based on the rules of gRFN , the frame sem, and the action model AM .\nNow, the key procedures in Algorithm 3 are: 1) how to do semantic role matching and recovering given a task expressed in natural language; 2) how to compute a plan given a set of rules, a frame, and an action model. The details about the two procedures are described in Sections V and VI respectively."
    }, {
      "heading" : "V. SEMANTIC MATCHING AND RECOVERING",
      "text" : "We propose a three-phase procedure to translate a user instruction expressed in natural language into the internal representation, which can be handled by our planner. Firstly, a probabilistic syntactic parser is used to retrieve the dependencies of the instruction. Secondly, the frame of sentence’s verb is identified by frame-semantic parsing. Here, without loss of generality, we assume that each instruction represents just a single task (verb). Thirdly, the semantic roles of the frame are recovered and filled as much as possible with the matched entities appeared in the instruction or its sentential context, represented as a meta-task in Re-FrameNet. More details about our three-phase procedure is described below."
    }, {
      "heading" : "A. Dependency Parsing",
      "text" : "We use the Stanford parser [14] in the first phase, which produces the Stanford-typed dependencies between words in a sentence. These dependencies indicate the grammatical relations between words in terms of the name of relation, governor, and dependence [15]. Figure 3 illustrates the parsing of a sentence “take food out of refrigerator”. The edge of the type dobj denotes that the noun “food” is the direct object of the verb “take”. The verb “take” also governs the noun “refrigerator” via the typed dependency prep out of. Since the typed dependency between a verb and a noun reveals their semantic-role relation, the syntactic structure of an instruction is used for our semantic role matching and recovering."
    }, {
      "heading" : "B. Frame Semantic Parsing",
      "text" : "Given that a verb varies in different senses, an instruction may represent different meanings and therefore can be mapped to different frames in FrameNet. For instance, the verb “take” can represent the Frame Bring or Removing under different\ncontexts. The Stanford parser does not disambiguate verb senses. Therefore, we propose a Frame Semantic Parsing method to map a verb to a unique Frame. Specifically, we define a frame identification model and train the model with sets of data from FrameNet and OMICS as below.\n1) Model: Given a sentence x = 〈x1, . . . , xn〉 with frameevoking verb v, we seek the most likely Frame f∗ in the frame identification stage. Let F be the set of candidate Frames for v, L the set of verbs found in the FrameNet annotations, and Lf ⊆ L the subset of verbs annotated by evoking the Frame f . The frame identification can be formalized by the following prediction rule:\nf∗ = arg max f∈F ∑ l∈Lf p(f, l|v,x)\nFor f ∈ F and l ∈ Lf , a conditional log-linear model is used to model the probability p(f, l|v,x; θ):\np(f, l|v,x; θ) = exp[θ · Φ(f, l, v,x)]∑ f ′∈F ∑ l′∈Lf′ exp[θ · Φ(f ′, l′, v,x)]\nwhere θ · Φ(f, l, v,x) is the inner product ∑M i=1 θi × Φi(f, l, v,x) and θ is the parameter vector over the feature function Φ with M dimension.\nGenerally, the feature function allows for a variety of (possibly overlapping) features. A feature Φi may relate a frame f to a verb v, representing a lexical-semantic relationship.\n2) Data: Our training and test sets come from FrameNet lexicon and OMICS. The FrameNet lexicon is a taxonomy of manually identified general-purpose Frames in English. Listed in the lexicon with each Frame are several lemmas (with part of speech) that can denote the Frame or some aspect of it — these are often called Lexical Units (LUs). Table I shows some examples of our training and test sets.\n3) Training: Given the training subset of the data in the form 〈xj , vj , f j , sj〉Nj=1 where N is the number of sentences, we discriminatively train the frame identification model by maximizing the following log-likelihood function:\nmax θ N∑ j=1 log ∑ l∈Ljf p(f j , l|vj ,x).\nSpecifically, we optimize it using a distributed version of gradient ascent algorithm with initial value ~θ as:\nfor k = 0..D − 1 for i = 1..M\nθi = θi + α ∂ ∑N j=1 log ∑ l∈Ljf p(f j , l|vj ,x)\n∂θ\n6\nwhere D is a parameter that controls the number of passes over the training data, M is the number of features, and N is the total size of our training set.\nNote that the computational complexity of the algorithm above is O(D ×M × N). When the number of features is large, it will be costly to train our model sequentially. In order to update the parameter of a feature f faster, we consider Nf training examples that contains only f instead of N . Hence, the computational complexity becomes O(D×M×Nf ), where Nf is usually much smaller than N ."
    }, {
      "heading" : "C. Roles Matching and Recovering",
      "text" : "After the Frame for the meta-task achieved from Re-FrameNet is identified, the semantic roles of the meta-task must be filled with the corresponding entities (expressed by nouns) in the sentence or from its sentential context. In Figure 4, given steps s = 〈s1, ..., sn〉 and Frames of each step f = 〈f1, ..., fn〉, we match and recover missing semantic roles of each Frame r = 〈r1, ..., rn〉, where ri = 〈ri1, ..., riki〉.\nTake the flow of instructions 〈 step 1: “go to fridge”; step 2: “open the fridge door”; step 3: “take the beer”; step 4: “close the fridge door” 〉 for example. The third instruction (i.e., step 3) is identified as the meta-task take-Taking, whose semantic roles in Re-FrameNet include Agent, Theme, and Source. However, this instruction only explicitly specifies the role Theme (the beer), while the others are missing from it. Note that the semantic role Source can be recovered and matched with the entity fridge in the sentential context of this instruction. Therefore, the challenge of our third phase lies in the recovering of missing semantic roles.\nTo address this challenge, we borrow ideas from the “last objects” method [16] and propose the following method:\nIn general, we divide semantic matching and recovering into two cases. The first case is for zero sentential distance, i.e., recovering semantic roles based on the instruction itself. Table II shows some heuristic rules for this case, each assigning a noun of the designated dependence type to a semantic role of a meta-task. For example, according to the first rule in Table II, beverage is assigned to the semantic role Theme of the metatask put-Placing. Similarly, fridge is assigned to Goal of the same meta-task according to the second rule. After matching, the single instruction “put beverage in the fridge” is interpreted as an instantiated meta-task of put-Placing as follow:\n( define ( meta-task put-Placing ( :parameters robot beverage null fridge)) ... )\nIn the case where a semantic role of a sentence cannot be identified within the sentence, semantic matching is conducted\n7Some of unspecified roles should be identified by grounding [17], [6], [18], [19], which is beyond the scope of this article.\n7\nbased on a taxonomical hierarchy, which specifies what sorts of entities can be taken as values by a semantic role. For example, the Theme role of meta-task put-Placing should take a holdable object for the robot. Table III shows a part of the hierarchy about meta-task take-Taking. Moreover, the hierarchy needs to be extended by class-subclass relationships, as exemplified in Table IV. Consider the example sentence “take the beer” in Figure 2. The entities appeared in the context are fridge and fridge-door. In our taxonomical hierarchy, fridgedoor is an instance of door which is neither supportable nor containable. Therefore, only fridge can be a value of the Source role of take-Taking. In the case of multiple candidates for a semantic role, the nearest entity will be selected. The high-level part of our hierarchy is similar to that of AfNet [18]. This is beneficial to integrating grounding mechanism into our prototype system."
    }, {
      "heading" : "VI. TASK PLANNING WITH ASP",
      "text" : "Given the meta-task semantic representation of a sentence, we generate an action sequence using OMICS and functional definition knowledge of common verbs (e.g., Re-FrameNet). In our previous work, we proposed the OK-planner [8] based on ASP. In this approach, all types of knowledge are converted into ASP and then an ASP solver is applied to generate an action sequence. However, this work does not consider common verbs for handling complex tasks.\nIn this article, we built our planner upon our previous work but additionally consider the following challenges: 1) how to define the functional knowledge of primitive actions in Action Model and 2) how to convert Re-FrameNet definition of common verbs into ASP."
    }, {
      "heading" : "A. Planning with Action Model",
      "text" : "As aforementioned, we specify robot skills in our system by an action model, i.e., a set of primitive actions that are executable for the robot. Table V shown some basic definition of the primitive actions for a typical service robot though different types of robots may have different action model. Formally, each primitive action a is defined as a pair 〈pre(a), eff(a)〉, where pre(a) and eff(a) are the preconditions and effects of a respectively. For instance, moveto(obj) is a primitive action that tells the robot to move close to the specified object obj. The pre and eff of moveto(obj) show whether the robot is near the specified obj before and after the moveto action respectively.\nGiven any initial state s0 and a possible plan a1, . . ., an, an action model determines a predicted trajectory τ∗ = 〈s0, a1, s1, . . . , an, sn〉 through inference for all the states s1, . . ., sn along with the execution of the action sequence during planning. For instance, given an instruction “get food from fridge”, we need to generate a plan for the robot as:\nNote that the semantic representation of a user instruction can be easily converted into a ASP form [8]. All we have to do is to fill sufficient knowledge for the ASP planner. Using our Re-FrameNet definition, an action verb is reorganized by a set of precondition, postcondition, and invariant over semantic roles of the action. Therefore, the remaining problem for our approach is how to convert the functional definitions of common verbs into ASP."
    }, {
      "heading" : "B. Conversion of Functional Knowledge",
      "text" : "Let α be a common verb (word sense). The set of linguistic variables of α’s frame is denoted by Θ(α). The set of properties and relations over Θ(α) occur in the functional definitions of verbs belonging to α’s Frame is denoted by Σ(α). Given a task taskα based on the common verb α as:\n(:meta-task α (:parameters (p1 X ) · · · (ph X )))\nwhere X1, . . . , Xh ∈ Θ(α) and p1, . . . , ph are predicates over a set X of variables, each constraint of the common verb α can be converted to a set of ASP rules w.r.t. the task taskα as:\n1. A precondition\n(:precond α (conj (disj l1 · · · ln) · · · (disj l′1 · · · l′m)))\nis converted to the following ASP rules:\n← process(taskα, t, t′), not true(l1, t), . . . , not true(ln, t), t < t′, p1(X ), . . . , ph(X ) · · · ← process(taskα, t, t′), not true(l′1, t), . . . , not true(l′n, t), t < t′, p1(X ), . . . , ph(X )\n2. A postcondition\n(:postcond α (conj (disj l1 · · · ln) · · · (disj l′1 · · · l′m)))\n8\nis converted to the following ASP rules:\n← process(taskα, t, t′), not true(l1, t′), . . . , not true(ln, t′), t < t′, p1(X ), . . . , ph(X ) · · · ← process(taskα, t, t′), not true(l′1, t′), . . . , not true(l′n, t′), t < t′, p1(X ), . . . , ph(X )\n3. An invariant\n(:invariant α (conj (disj l1 · · · ln) · · · (disj l′1 · · · l′m)))\nis converted to the following ASP rules:\n← process(taskα, t, t′), not true(l1, t′′), . . . , not true(ln, t′′), t < t′, t <= t′′, t′′ <= t′, p1(X ), . . . , ph(X ) · · · ← process(taskα, t, t′), not true(l′1, t′′), . . . , not true(l′n, t′′), t < t′, t <= t′′, t′′ <= t′, p1(X ), . . . , ph(X )\n4. An invariant\n(disj (:invariant α (conj (disj l1 · · · ln) · · · (disj l′1 · · · l′m))) (:invariant α (conj (disj l∗1 · · · l∗n) · · · (disj l′∗1 · · · l′∗m))))\nis converted to the following ASP rules:\nf ← process(taskα, t, t′), not true(l1, t′′), . . . , not true(ln, t′′), t < t′, t <= t′′, t′′ <= t′, p1(X ), . . . , ph(X ) · · · f ← process(taskα, t, t′), not true(l′1, t′′), . . . , not true(l′n, t′′), t < t′, t <= t′′, t′′ <= t′, p1(X ), . . . , ph(X ) f∗ ← process(taskα, t, t′), not true(l∗1 , t′′), . . . , not true(l∗n, t′′), t < t′, t <= t′′, t′′ <= t′, p1(X ), . . . , ph(X ) · · · f∗ ← process(taskα, t, t′), not true(l′∗1 , t′′), . . . , not true(l′∗n , t′′), t < t′, t <= t′′, t′′ <= t′, p1(X ), . . . , ph(X )\n← f, f∗\nAfter all pieces of knowledge have been converted into the ASP rules, an ASP solver iclingo [20] — a combination of Gringo and clasp for incremental grounding and solving — is used to incrementally ground the ASP rules above and search for answer sets, from which a plan can be computed [8]."
    }, {
      "heading" : "VII. EXPERIMENTS",
      "text" : "We empirically evaluate our system with three experiments. The first experiment was devised to investigate the performance of our SMR (i.e., Semantic Matching and Recovering) method. The second experiment aimed to testing the performance of the whole system when different open knowledge bases were used. We also analyzed the main factors that may affect the performance. Finally, we demonstrate that how our approach can be deployed in our KeJia robot to solve instruction understanding problems in two domestic scenarios. Additionally, we also present our long-term effort on applying the proposed technique in the RoboCup@Home competitions."
    }, {
      "heading" : "A. Experiments with SMR",
      "text" : "To test our SMR method, we collect 191,740 examples annotated with frame-semantic structures for the frame identification model from FrameNet lexicon and 470 examples from OMICS. Then, we parse each sentence by the Stanford parser. Finally, we only select those examples whose LU is a verb or a verb phrase. As a result, the training data contains 70,149 examples and the test data contains 18,183 examples from FrameNet and 630 examples from OMICS. In our experiments, the frame identification model instantiates 76,289 binary features.\nTable VI shows the results on each part of translation of hierarchal instructions. The performance is evaluated by Precise (P), Recall (R), F1 (F) defined as: Precise = TP/(TP+FP ), Recall = TP/T , F1 = 2 ∗ Precise ∗ Recall/(Precise + Recall), where TP stands for the number of the sentences parsed correctly, FP is the number of the sentences parsed wrongly, and T is the total length of the dataset.\nAs we can see from the results, syntactic results have a very high precise and F1 value, which benefits to the metatask identification phase. However, it does not disambiguate the meaning of a verb (e.g., the verb “get” has two meanings: “Getting: get the food” and “Motion: get to the room”). The meta-task identification, which obtains a F1 value of 80 over the FrameNet data and 71.07 over the OMICS data. Moreover, the overall performance of the whole translation system maintains a quite high precise and relatively low recall due to the data sparseness and one meta-task assumption."
    }, {
      "heading" : "B. Experiments on OMICS",
      "text" : "The experiments on OMICS were divided into two tests. Test 1 was conducted on 11,885 user tasks from the Tasks/Steps table and Test 2 on 467 user desires from the Help table.\nTest 1 consisted of four rounds. In the first round, only the definitions of the 11,885 tasks from the Tasks/Steps table and a small action model AM representing the basic perception and manipulation skills of a robot were used. Specifically, AM contained only 6 primitive actions: move, find, pick up, put down, open, and close. Synonymy knowledge from FrameNet was used into the second to fourth rounds of Test 1. In the third and fourth rounds, rewritten knowledge from Re-FrameNet was considered with our SMR technique. However, in the third round, missing roles were not recovered from the context.\nTable VII shows the experimental results of Test 1. The second row shows the numbers of tasks that were successfully planned by the global planner with tasks/steps in the four rounds. The third row shows the total numbers of tasks that were successfully planned in the four rounds. The fourth\n9\nrow shows the percentages of successfully planned tasks with respect to the total number of tested tasks. Since there are no ground truth data for OMICS, we randomly drew 80 and 100 samples from the last two rounds respectively and verified them manually. It turned out that 51 and 64 samples among them were correct. As shown in the fifth row of Table VII, the correctness percent decreased when Re-FrameNet was used; but the number of correctly planned tasks still increased remarkable. Moreover, we can see that the overall performance improved when semantic roles of common verbs was used, much better than the state-of-art solution [8].\nAs shown in Figure 5, the number of the successfully planned tasks gradually increased when more frames were added to the algorithm. It also shows that some frames cannot be mapped into robots’ action (i.e., Mass motion and Waiting). The main reason is the limit of robots’ primitive actions.\nTable IX reports the main types of failures that we observed in Test 1. Specifically, the Parsed Failure occurred in 3027 tasks because the semantic matching and recovering procedure failed to retrieve any frame from Re-FrameNet (RFN) for a task. The RFN Failure occurred in 4394 tasks due to the fact that Re-FrameNet contains only 43 frames, in which 7421 tasks cannot be used to generate a plan by the robot. A Global Planning Failure occurs when a task/step t cannot be planned and none of the following conditions hold: t is a primitive action, semantically equivalent to meta-task in Re-FrameNet or another task in the Tasks/Steps table. In total, there were 3527 tasks failed in this category. A Local Planning Failure occurs when the solver (in Algorithm 3) is launched but fails to generate any plan. Further study reveals that these two sorts of planning failures are mainly due to lack of knowledge/skills.\nTest 2 was conducted on 467 user desires from the Help table of OMICS. The experimental results are shown in Table VIII. As we can see, the success rates were higher than the corresponding rounds of Test 1. In particular, the success rate is as high as 81% in the last round. This is because a desire can be met by various tasks, which can be different\nfrom one another. Therefore, knowledge used in the rounds of Test 2 was much richer than that in Test 1.\nNotice that the overall performance increased about 5 times in Test 1 and 50% in Test 2 when semantic roles of common verbs and Re-FrameNet was used. There are two main reasons for this improvement. Firstly, rewritten knowledge of common verbs in Re-FrameNet fills knowledge gaps caused by lack of definitions of these verbs in OMICS . Without the knowledge, 761(=935-174) tasks would not have been successfully planned in the last two rounds of Test 1. Secondly, our SMR mechanism contributed significantly to the improvement. Without it, 179(=935-756) out of these 761 tasks would not have been successfully planned. In other words, Re-FrameNet and SMR made about 76% and 24% contributions to the improvement of success rate in task planning respectively."
    }, {
      "heading" : "C. Case Study on KeJia Robot",
      "text" : "We conducted a case study of our system with the KeJia robot. As shown in Figures 6 and 7, our KeJia robot is based on a two-wheels driving chassis of 62cm×53cm×32cm and the equipped sensors include a laser range finder, a 1394 camera, and a Kinect. A lifting system is mounted on the chassis attached with the robot’s upper body. Assembled with the upper body is a 6 DOF arm. It is able to reach objects over 83cm far from mounting point and the maximum payload is about 500g. The robot’s power is supplied by a 20Ah battery that guarantees the continuous running of at least 1 hour. The computational resources consist of a laptop and an onboard PC. Our system is built upon existing modules including motion control for the mobile base and arm, navigation, recognition and localization.\nIn our case study, we first tried two typical scenarios where the robot can benefit from the proposed techniques. Then, we introduce our long-term effort on developing generalpurpose systems for user instruction understanding in the annual RoboCup@Home competitions.\n1) Scenario 1: As shown in Figure 6, a toy and a toy box were placed on the floor. Our KeJia robot was asked by a user to “clean up toys”. Note that, with only this instruction, the robot is unable to complete the task because the action “clean up” is unspecified. In our system, the robot first extracted the subtasks of the task “clean up toys” based on the knowledge in OMICS. By doing so, a tuple of 〈task. “clean up toys”: step 1. “pick up toys from floor”; step 2. “put toys in toybox”. 〉 was generated. Then, our SMR method matched and recovered\n10\nsemantic roles of each step in the tuple as:\n( define ( task clean up (toys) ( :subtasks pick up-Pick up\n( :parameters toys floor)) ( :subtasks put-Placing\n( :parameters toys floor toybox))))\nAfter that, our planner sequentially processed each subtask. In this phase, since the action pick up is a primitive action, the subtask pick up can be directly executed by our robot. For the second subtask, we tried to generate a plan given the definition of the meta-task put-Placing as:\n( define ( meta-task put-Placing ( :parameters ?Agent ?Theme ?Source ?Goal)) ( :precondition (at Theme Source)) ( :precondition (conj(portable Theme)(object Theme))) ( :postcondition (at Theme Goal))\nIn this scenario, the plan generated by the planner for this task is shown in Figures 6(c) and 6(d). At this point, the task “clean up toys” is solved by our system and finally the entire plan is executed by the robot to complete the task.\n2) Scenario 2: As shown in Figure 7, a user spoke to the robot that he “have a headache”. This was identified as a user desire. Similar to the previous scenario, our system first extracted a series of help tasks for the user desire such as “with pain medication”, “give them an aspirin”, etc. Then, our SMR method matched and recovered semantic roles of each help task. In this scenario, our planner failed to plan for the task “with pain medication” but successfully recovered the Source elements and generated a plan for the task “give them an aspirin”. A list of actions for the plan of this task are illustrated in Figure 7.\nA video demon for the two scenarios above with our KeJia robot is given at: https://youtu.be/A4GBXHG0l74\n3) RoboCup@Home: This is an international annual competition for domestic service robots and is part of the RoboCup event. In this competition, a set of benchmark tests are proposed to evaluate the robots’ abilities and performance in a realistic non-standardized home environment setting. The most related benchmark test to this article is the General Purpose Service Robot (GPSR) test, which requires a robot to solve tasks upon request in natural language randomly generated by the referees during the competition.\nIn the RoboCup@Home competitions of the past three years, our team — WrightEagle (WE) [21] got the 1st place once and 2nd place twice. Table X shows the total scores of the top 5 teams in the benchmark tests (without the final stage). It can be seen from the results that our team (i.e., WE) performed very well in the competitions. Particularly, in the GPSR tests, the performance of our system was competitive comparing to other top teams as shown in Table XI.\nAlthough there are generally many factors contributing to the success in the RoboCup@Home competitions, our robot did benefit substantially from the proposed system as described in this article to process user instructions and generate plans. The competitions motivated us to develop a general-purpose system for understanding user instructions in natural language and also provide a good testbed for such systems.\n11"
    }, {
      "heading" : "VIII. RELATED WORK",
      "text" : "To date, many approaches on instruction understanding and task planning for service robots have been proposed in the literature. For instance, several integrated systems [2], [16], [22] for natural language understanding have been introduced to enable robots to complete tasks given instructions in natural language. However, they all assume that instructions are definitely specified for the domains and do not consider semantic disambiguation of verbs and their roles. Work have been proposed to manually create environment-driven instructions for grounding user instructions in natural language to robots’ actions [10], [23]. However, these methods cannot scale to large number of tasks because each task need to be manually specified in an environment, and are not suitable for different types of robots (e.g., robots with different arm configurations).\nTo improve generality and scalability, researchers have tried to exploit online knowledge and learn large-scare knowledge representations to build a general-purpose system for instruction understanding. For example, Lemaignan et al. [24], [25] have tried to understand and reason about knowledge around an action model using online knowledge for robots. It is worth pointing out that we previously proposed an integrated system [8] for our KeJia robot consisting of multi-mode NLP, integrated decision-making, and open knowledge searching.\nThe approaches that are most related to ours are the ones using OMICS for robots to complete household tasks. The first attempt to utilize OMICS to accomplish a household task is [26], which proposed a generative model based on the Markov chain techniques. Later on, [27], [28], [29] presented a system called KNOWROB for processing knowledge in order to achieve more flexible and general behavior. Most recently, we proposed a formal description of knowledge gaps between user instructions and local knowledge in robotic system for instruction understanding [30], [8], [31], [32]. However, in these efforts using OMICS for robot task planning with user instructions, common verbs are normally not defined in the knowledge base, which limits their performance on utilizing existing open knowledge. Thus, our work is proposed to address the weakness of state-of-the-art methods."
    }, {
      "heading" : "IX. CONCLUSIONS",
      "text" : "This article proposed a general-purpose system for service robot handling large-scale user instructions in natural language. The key problem that we addressed is how to map primitive tasks into robot actions using semantic roles of common verbs provided by semantic dictionaries — a common resource of open knowledge in linguistics. To solve this problem, we proposed a novel approach for semantic matching\nand recovering. Furthermore, we utilized semantic roles of common verbs defined in semantic dictionaries for handling underspecification of naturalistic language instructions in task planning. Empirical evaluation and analysis were made and show good performance with two test sets consisting of 11885 user tasks and 467 user desires collected from OMICS. Moreover, we developed a prototype system deployed on our KeJia robot and demonstrated our techniques with two typical scenarios. Notably, our system has been used in the RoboCup@Home competitions and shown good performance in the benchmark tests over the past three years.\nHere, we conclude with the following findings:\n1) Overall performance of our system can be improved when Re-FrameNet was used. As shown by our experimental results, both the knowledge in Re-FrameNet and the SMR technique contributed to the improvement, indicating that rewritten knowledge of common verbs and recovering semantic roles from context are useful for naturalistic instruction understanding and planning. 2) The computational efficiency of our system can be improved using the hierarchism of user instructions and knowledge. As shown by our case study, instruction understanding and task planning can be done for our robot in realtime, given that task decomposition knowledge such as OMICS was used for efficient global planning and costly local planning was limited only to small number of low-level tasks defined in Re-FrameNet.\nIn the future, we plan to develop techniques to learn extra knowledge unavailable from user input, such as knowledge about robot manipulation, action configurations in finer degrees other than semantic role, and most importantly grounding. Moreover, we will investigate methods to automatically generate a large set of Re-FrameNet for robot tasks."
    } ],
    "references" : [ {
      "title" : "Developing High-level Cognitive Functions for Service Robots",
      "author" : [ "X. Chen", "J. Ji", "J. Jiang", "G. Jin", "F. Wang", "J. Xie" ],
      "venue" : "Proceedings of 9th International Conference on Autonomous Agents and Multi-agent Systems, 2010.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "What to do and how to do it: Translating natural language directives into temporal and dynamic logic representation for goal management and action execution",
      "author" : [ "J. Dzifcak", "M. Scheutz", "C. Baral", "P. Schermerhorn" ],
      "venue" : "IEEE International Conference on Robotics and Automation. ICRA, 2009, pp. 4163–4168.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Toward understanding natural language directions",
      "author" : [ "T. Kollar", "S. Tellex", "D. Roy", "N. Roy" ],
      "venue" : "5th ACM/IEEE International Conference on Human-Robot Interaction, 2010.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Everything robots always wanted to know about housework (but were afraid to ask)",
      "author" : [ "D. Nyga", "M. Beetz" ],
      "venue" : "IEEE/RSJ International Conference on Intelligent Robots and Systems, 2012.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Robobrain: Large-scale knowledge engine for robots",
      "author" : [ "A. Saxena", "A. Jain", "O. Sener", "A. Jami", "D.K. Misra", "H.S. Koppula" ],
      "venue" : "International Symposium of Robotics Research, 2014.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Understanding natural language commands for robotic navigation and mobile manipulation",
      "author" : [ "S. Tellex", "T. Kollar", "S. Dickerson", "M. Walter", "A. Banerjee", "S. Teller", "N. Roy" ],
      "venue" : "Proceedings of National Conference on Articial Intelligence, 2011.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Common sense data acquisition for indoor mobile robots",
      "author" : [ "R. Gupta", "M. Kochenderfer" ],
      "venue" : "Proceedings of the 19th National Conference on Artificial Intelligence, San Jose, California, USA, 2004, pp. 605–610.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Handling open knowledge for service robots",
      "author" : [ "X. Chen", "J. Ji", "Z. Sui", "J. Xie" ],
      "venue" : "Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence, 2013.  12",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A general service list of English words: with semantic frequencies and a supplementary word-list for the writing of popular science and technology",
      "author" : [ "M.P. West" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1953
    }, {
      "title" : "Tell me dave: Contextsensitive grounding of natural language to manipulation instructions",
      "author" : [ "D. Misra", "J. Sung", "K. Lee", "A. Saxena" ],
      "venue" : "The International Journal of Robotics Research, 2014.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "The stable model semantics for logic programming",
      "author" : [ "M. Gelfond", "V. Lifschitz" ],
      "venue" : "Proceedings of the 5th International Conference on Logic Programming. ICLP-88, 1988, pp. 1070–1080.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "The berkeley framenet project",
      "author" : [ "C.F. Baker", "C.J. Fillmore", "J.B. Lowe" ],
      "venue" : "Proceedings of the 17th international conference on Computational linguistics. Association for Computational Linguistics, 1998, pp. 86–90.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Dictionaries for learners of english",
      "author" : [ "P. Bogaards" ],
      "venue" : "International Journal of Lexicography, vol. 9, no. 4, pp. 277–320, 1996.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Generating Typed Dependency Parses from Phrase Structure Parses",
      "author" : [ "M.-C. de Marneffe", "B. Maccartney", "C.D. Manning" ],
      "venue" : "Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC-06). Genoa, Italy: ELRA/ELDA Paris, 2006, pp. 449–454.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "The Stanford typed dependencies representation",
      "author" : [ "M.-C. de Marneffe", "C.D. Manning" ],
      "venue" : "Proceedings of the COLING 2008 Workshop on Cross-framework and Cross-domain Parser Evaluation, no. ii. Manchester, UK: ACL, 2008, pp. 1–8.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Robust spoken instruction understanding for HRI",
      "author" : [ "R. Cantrell", "M. Scheutz", "P. Schermerhorn", "X. Wu" ],
      "venue" : "Proceedings of the 5th ACM/IEEE International Conference on Robot Interaction, 2010.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Learning environmental knowledge from task-based human-robot dialog",
      "author" : [ "T. Kollar", "V. Perera", "D. Nardi", "M. Veloso" ],
      "venue" : "Proc. of the IEEE International Conference on Robotics and Automation, 2013.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "AfRob: The Affordance Network Ontology for Robots",
      "author" : [ "K.M. Varadarajan", "M. Vincze" ],
      "venue" : "IEEE/RSJ International Conference on Intelligent Robots and Systems, 2012.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Grounding Natural Language References to Unvisited and Hypothetical Locations",
      "author" : [ "T. Williams", "R. Cantrell", "G. Briggs", "P. Schermerhorn", "M. Scheutz" ],
      "venue" : "Proceedings of the Twenty-Seventh AAAI Conference on Artificial Intelligence, Bellevue, Washington, USA, 2013.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Engineering an incremental asp solver",
      "author" : [ "M. Gebser", "R. Kaminski", "B. Kaufmann", "M. Ostrowski", "T. Schaub", "S. Thiele" ],
      "venue" : "Logic Programming. Springer, 2008, pp. 190–205.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Towards a principled solution to simulated robot soccer",
      "author" : [ "A. Bai", "F. Wu", "X. Chen" ],
      "venue" : "Proceedings of the Robot Soccer World Cup XVI Symposium (RoboCup), Mexico City, Mexico, 2012, pp. 141–153.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Temporal-logic-based reactive mission and motion planning",
      "author" : [ "H. Kress-Gazit", "G.E. Fainekos", "G.J. Pappas" ],
      "venue" : "IEEE Transactions on Robotics, vol. 25, no. 6, pp. 1370–1381, 2009.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Learning spatialsemantic representations from natural language descriptions and scene classifications",
      "author" : [ "S. Hemachandra", "M. Walter", "S. Tellex", "S. Teller" ],
      "venue" : "2014 IEEE International Conference on Robotics and Automation (ICRA), 2014, pp. 2623–2630.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Grounding the interaction: knowledge management for interactive robots",
      "author" : [ "S. Lemaignan" ],
      "venue" : "KI-K unstliche Intelligenz, pp. 1–3, 2012.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Grounding the interaction: Anchoring situated discourse in everyday human-robot interaction",
      "author" : [ "S. Lemaignan", "R. Ros", "E. Sisbot", "R. Alami", "M. Beetz" ],
      "venue" : "International Journal of Social Robotics, vol. 4, no. 2, pp. 181–199, 2012.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Building plans for household tasks from distributed knowledge",
      "author" : [ "C. Shah", "R. Gupta" ],
      "venue" : "Proceedings of the 19th International Joint Conference on Artificial Intelligence (IJCAI 2005) Workshop on Modeling Natural Action Selection. Citeseer, 2005.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Knowrob-mapknowledge-linked semantic object maps",
      "author" : [ "M. Tenorth", "L. Kunze", "D. Jain", "M. Beetz" ],
      "venue" : "Humanoid Robots (Humanoids), 2010 10th IEEE-RAS International Conference on. IEEE, 2010, pp. 430–435.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Putting peoples common sense into knowledge bases of household robots",
      "author" : [ "L. Kunze", "M. Tenorth", "M. Beetz" ],
      "venue" : "KI 2010: Advances in Artificial Intelligence. Springer, 2010, pp. 151–159.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Knowrob: A knowledge processing infrastructure for cognition-enabled robots",
      "author" : [ "M. Tenorth", "M. Beetz" ],
      "venue" : "The International Journal of Robotics Research, vol. 32, no. 5, pp. 566–590, 2013.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Toward open knowledge enabling for human-robot interaction",
      "author" : [ "X. Chen", "J. Xie", "J. Ji", "Z. Sui" ],
      "venue" : "Journal of Human-Robot Interaction, vol. 1, no. 2, pp. 100–117, 2012.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Understanding instructions on large scale for human-robot interaction",
      "author" : [ "J. Xie", "X. Chen" ],
      "venue" : "Proceedings of the 2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent  Agent Technologies (IAT)-Volume 03. IEEE Computer Society, 2014, pp. 175–182.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Multi-mode natural language processing for human-robot interaction",
      "author" : [ "J. Xie", "X. Chen", "J. Ji" ],
      "venue" : "Web Intelligence, vol. 13, no. 4. IOS Press, 2015, pp. 267–278.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : ", sequence of actions) for the tasks specified in the instruction [1], [2], [3], [4], [5], [6].",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 1,
      "context" : ", sequence of actions) for the tasks specified in the instruction [1], [2], [3], [4], [5], [6].",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 2,
      "context" : ", sequence of actions) for the tasks specified in the instruction [1], [2], [3], [4], [5], [6].",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 3,
      "context" : ", sequence of actions) for the tasks specified in the instruction [1], [2], [3], [4], [5], [6].",
      "startOffset" : 81,
      "endOffset" : 84
    }, {
      "referenceID" : 4,
      "context" : ", sequence of actions) for the tasks specified in the instruction [1], [2], [3], [4], [5], [6].",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 5,
      "context" : ", sequence of actions) for the tasks specified in the instruction [1], [2], [3], [4], [5], [6].",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 6,
      "context" : "Fortunately, there is more and more common knowledge available in open resources, such as the Open Mind Indoor Common Sense (OMICS) database [7], wikihow1, WordNet, and many other digital dictionaries.",
      "startOffset" : 141,
      "endOffset" : 144
    }, {
      "referenceID" : 7,
      "context" : "In our previous studies [8], we found that a user instruction representing a high-level task can usually be reduced into a sequence of low-level subtasks, using hierarchical knowledge in open resources.",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 8,
      "context" : "level subtasks expressed in common verbs [9]).",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 2,
      "context" : "To avoid this challenge, most of the existing approaches [3], [4], [10] manually create a small set of handcoded robot actions for primitive tasks though their scalability (i.",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 3,
      "context" : "To avoid this challenge, most of the existing approaches [3], [4], [10] manually create a small set of handcoded robot actions for primitive tasks though their scalability (i.",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 9,
      "context" : "To avoid this challenge, most of the existing approaches [3], [4], [10] manually create a small set of handcoded robot actions for primitive tasks though their scalability (i.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 10,
      "context" : "Then, we use a planner based on Answer Set Programming (ASP) [11] to exploit definitions of common verbs in terms of semantic roles and generate a plan for the task specified in the user instruction.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 6,
      "context" : "OMICS [7] is an extensive collection of knowledge for indoor service robots gathered from internet users.",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 11,
      "context" : "It groups action verbs into Frames and specifies word definitions in terms of semantic roles called Frame Elements (FEs) for each Frame [12].",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 8,
      "context" : "a list of roughly 2000 most frequent English words [9].",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 12,
      "context" : "The GSL is taken as the defining vocabulary of dictionaries such as the Longman Dictionary of Contemporary English, based on the notion that words should be defined using “terms less abstruse than the word that is to be explained” [13].",
      "startOffset" : 231,
      "endOffset" : 235
    }, {
      "referenceID" : 13,
      "context" : "We use the Stanford parser [14] in the first phase, which produces the Stanford-typed dependencies between words in a sentence.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 14,
      "context" : "These dependencies indicate the grammatical relations between words in terms of the name of relation, governor, and dependence [15].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 15,
      "context" : "To address this challenge, we borrow ideas from the “last objects” method [16] and propose the following method: TABLE III PART OF HIERARCHY FOR take-taking.",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 16,
      "context" : "7Some of unspecified roles should be identified by grounding [17], [6], [18], [19], which is beyond the scope of this article.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 5,
      "context" : "7Some of unspecified roles should be identified by grounding [17], [6], [18], [19], which is beyond the scope of this article.",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 17,
      "context" : "7Some of unspecified roles should be identified by grounding [17], [6], [18], [19], which is beyond the scope of this article.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 18,
      "context" : "7Some of unspecified roles should be identified by grounding [17], [6], [18], [19], which is beyond the scope of this article.",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 17,
      "context" : "The high-level part of our hierarchy is similar to that of AfNet [18].",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 7,
      "context" : "In our previous work, we proposed the OK-planner [8] based on ASP.",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 7,
      "context" : "Note that the semantic representation of a user instruction can be easily converted into a ASP form [8].",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 19,
      "context" : "After all pieces of knowledge have been converted into the ASP rules, an ASP solver iclingo [20] — a combination of Gringo and clasp for incremental grounding and solving — is used to incrementally ground the ASP rules above and search",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 7,
      "context" : "for answer sets, from which a plan can be computed [8].",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 7,
      "context" : "Moreover, we can see that the overall performance improved when semantic roles of common verbs was used, much better than the state-of-art solution [8].",
      "startOffset" : 148,
      "endOffset" : 151
    }, {
      "referenceID" : 20,
      "context" : "In the RoboCup@Home competitions of the past three years, our team — WrightEagle (WE) [21] got the 1st place once and 2nd place twice.",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 1,
      "context" : "For instance, several integrated systems [2], [16], [22] for natural language understanding have been introduced to enable robots to complete tasks given instructions in natural language.",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 15,
      "context" : "For instance, several integrated systems [2], [16], [22] for natural language understanding have been introduced to enable robots to complete tasks given instructions in natural language.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 21,
      "context" : "For instance, several integrated systems [2], [16], [22] for natural language understanding have been introduced to enable robots to complete tasks given instructions in natural language.",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 9,
      "context" : "Work have been proposed to manually create environment-driven instructions for grounding user instructions in natural language to robots’ actions [10], [23].",
      "startOffset" : 146,
      "endOffset" : 150
    }, {
      "referenceID" : 22,
      "context" : "Work have been proposed to manually create environment-driven instructions for grounding user instructions in natural language to robots’ actions [10], [23].",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 23,
      "context" : "[24], [25] have tried to understand and reason about knowledge around an action model using online knowledge for robots.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "[24], [25] have tried to understand and reason about knowledge around an action model using online knowledge for robots.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 7,
      "context" : "It is worth pointing out that we previously proposed an integrated system [8] for our KeJia robot consisting of multi-mode NLP, integrated decision-making, and open knowledge searching.",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 25,
      "context" : "The first attempt to utilize OMICS to accomplish a household task is [26], which proposed a generative model based on the Markov chain techniques.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 26,
      "context" : "Later on, [27], [28], [29] presented a system called KNOWROB for processing knowledge in order to achieve more flexible and general behavior.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 27,
      "context" : "Later on, [27], [28], [29] presented a system called KNOWROB for processing knowledge in order to achieve more flexible and general behavior.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 28,
      "context" : "Later on, [27], [28], [29] presented a system called KNOWROB for processing knowledge in order to achieve more flexible and general behavior.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 29,
      "context" : "Most recently, we proposed a formal description of knowledge gaps between user instructions and local knowledge in robotic system for instruction understanding [30], [8], [31], [32].",
      "startOffset" : 160,
      "endOffset" : 164
    }, {
      "referenceID" : 7,
      "context" : "Most recently, we proposed a formal description of knowledge gaps between user instructions and local knowledge in robotic system for instruction understanding [30], [8], [31], [32].",
      "startOffset" : 166,
      "endOffset" : 169
    }, {
      "referenceID" : 30,
      "context" : "Most recently, we proposed a formal description of knowledge gaps between user instructions and local knowledge in robotic system for instruction understanding [30], [8], [31], [32].",
      "startOffset" : 171,
      "endOffset" : 175
    }, {
      "referenceID" : 31,
      "context" : "Most recently, we proposed a formal description of knowledge gaps between user instructions and local knowledge in robotic system for instruction understanding [30], [8], [31], [32].",
      "startOffset" : 177,
      "endOffset" : 181
    } ],
    "year" : 2016,
    "abstractText" : "Understanding user instructions in natural language is an active research topic in AI and robotics. Typically, natural user instructions are high-level and can be reduced into low-level tasks expressed in common verbs (e.g., ‘take’, ‘get’, ‘put’). For robots understanding such instructions, one of the key challenges is to process high-level user instructions and achieve the specified tasks with robots’ primitive actions. To address this, we propose novel algorithms by utilizing semantic roles of common verbs defined in semantic dictionaries and integrating multiple open knowledge to generate task plans. Specifically, we present a new method for matching and recovering semantics of user instructions and a novel task planner that exploits functional knowledge of robot’s action model. To verify and evaluate our approach, we implemented a prototype system using knowledge from several open resources. Experiments on our system confirmed the correctness and efficiency of our algorithms. Notably, our system has been deployed in the KeJia robot, which participated the annual RoboCup@Home competitions in the past three years and achieved encouragingly high scores in the benchmark tests.",
    "creator" : "LaTeX with hyperref package"
  }
}