{
  "name" : "1505.06449.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Efficient Elastic Net Regularization for Sparse Linear Models",
    "authors" : [ "Zachary C. Lipton", "Charles Elkan" ],
    "emails" : [ "elkan}@cs.ucsd.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 5.\n06 44\n9v 1\n[ cs\n.L G"
    }, {
      "heading" : "1 Introduction",
      "text" : "Machine learning practitioners are often tasked with learning over large datasets. An abundance of affordable storage, network bandwidth, and the proliferation of datagenerating devices, have combined to saddle many organizations with big data problems. Document auto-tagging problems frequently contain millions of documents, hundreds of thousands of features, and thousands of labels. When the number of labels and features is large, even the weight matrix of a dense linear model may not fit in main memory on a single workstation. However, for many applications features are sparse and good regularized models can be made compact. Given such sparsity, it is desirable train machine learning models using algorithms that require time that scales only with the number of non-zero features.\nOnline algorithms, such as stochastic gradient descent, are widely used to train high-dimensional models on large-scale data. In these methods, each example is processed one at a time, updating the model on the fly. When a dataset is sparse and the\nobjective function is not regularized, this sparsity can be exploited by updating only the weights corresponding to non-zero features. However, to prevent overfitting models to high-dimensional data, it is often necessary to apply a regularization penalty, imposing a prior belief that the model parameters should be sparse and small in magnitude. Problematically, widely-used regularizers such as ℓ1 (lasso), ℓ22 (ridge), and elastic net (ℓ1 + ℓ22) destroy the sparsity of the gradient, seemingly requiring an update for each nonzero weight for each example.\nIn this paper, we develop upon a method for lazily updating weights, first described by [2], [10], and [6]. As each example is processed, we update only those weights corresponding to non-zero features. The model is brought current as needed by lazily applying closed-form constant-time updates whenever a feature becomes non-zero. For sparse data sets, the algorithm runs in time independent of the nominal dimensionality, scaling linearly with the number of non-zero features per example.\nTo date, only the closed form updates for the ℓ1, ℓ∞, and the rarely used ℓ2 norm have been described. We extend this work by showing the proper closed form updates for the popular ℓ22 and elastic net regularized models. When the learning rate varies (typically decreasing as a function of time), we show that the proper elastic net update can be computed with a dynamic programming algorithm, requiring only one constanttime subproblem computation per update. 1\nAdditionally, we implement the system, showing that on a widely studied dataset containing the abstracts of millions of articles from biomedical literature, we can train a logistic regression classifier with elastic net regularization 612 times faster than an otherwise identical model with dense updates."
    }, {
      "heading" : "2 Background and Definitions",
      "text" : "We consider a data matrix X ∈ Rn×d where each row xi corresponds to one of n examples and each column, indexed by j corresponds to one of d features. We desire a linear model, parametrized by a vector w ∈ Rd, which minimizes a convex objective function F (w), expressible as ∑n\ni=1 Fi(w), where F returns the loss with respect to the entire corpus X and Fi returns the loss calculated on example xi.\nIn many datasets, the vast majority of entries xij are zero-valued. The-bag-ofwords representation of text is one such case. We say that such datasets are sparse. When features correspond to counts or binary values, as in bag-of-words, we sometimes say say that a zero-valued feature xij is absent. We use p to refer to the average number of nonzero features per example. Naturally, when a dataset is sparse, we prefer algorithms which take time O(p) to those O(d).\n1Our dynamic programming algorithms for constant-time updates with varying learning rates add time O(1) per stochastic gradient update and have space complexity O(T ), where T is the total number of stochastic gradient updates. If this space complexity is too great, that problem can be solved by alotting a space budget and bringing all weights current when the budget is filled. As this cost is amortized across many iterations, it adds negligibly to the total running time."
    }, {
      "heading" : "2.1 Regularization",
      "text" : "To prevent overfitting, regularization restricts the freedom of a model’s parameters, penalizing their distance from some prior belief. Most widely used regularizers penalize large weights with an objective function of the form\nF (w) = L(w) +R(w) (1)\nMany commonly used regularizers R(w) are of the form λ||w|| where λ determines the strength of regularization and ℓ0, ℓ1, ℓ22, and ℓ∞ are common choices for the norm. The ℓ1 regularizer is popular owing to its tendency to produce sparse models. In this paper, we focus on elastic net, a linear combination of ℓ1 and ℓ22 regularization that has been shown to produce comparably sparse models to ℓ1 while often achieving superior accuracy [12]."
    }, {
      "heading" : "2.2 Stochastic Gradient Descent",
      "text" : "Gradient descent is a common strategy to find the optimal parametersw. Any gradient descent method requires an initial learning rate η. Then to minimize F (w), a number of steps T , indexed by t, are taken in the direction of the negative gradient:\nw(t+1) := w(t) − η\nn ∑\ni=1\n∇Fi(w).\nAn appropriately attenuated learning rate ensures both that the optimal parameters will eventually be reached and that the algorithm will yield a value within a distance ǫ of the optimal value for any small value ǫ [2] .\nTraditional (or ”batch”) gradient descent requires a pass through the entire dataset for each update. Stochastic gradient descent (SGD) circumvents this problem by updating the model once after visiting each example. With SGD, one determines a number of epochs E indexed by e. Within each epoch, examples are randomly selected one at a time (or in ”mini-batches”). For simplicity of notation, w.l.o.g., we will assume the examples are selected one at a time. The gradient is calculated with respect to that example, and the model is updated according to the rule wt+1 := wt − η∇w(t)Fi. Because the examples are chosen randomly, the expected value of this noisy gradient is identical to the true value of the gradient taken with respect to the entire corpus.\nGiven a continuously differentiable convex objective function F (w), stochastic gradient descent is known to converge for learning rates η that satisfy ∑\nt ηt = ∞ and ∑\nt η 2 t < ∞ [1]. Learning rates ηi ∝ 1 t and ηi ∝ 1√t both satisfy these properties. 2 For many objective functions, such as linear or logistic regression without regularization, the noisy gradient ∇wFi is sparse when the input is sparse. In these cases, one needs only to update the weights corresponding to non-zero features in the current example xi. This runs in time O(p), where p ≪ d is the average number of nonzero features in an example.\n2 Some common objective functions as those with ℓ1 regularization, are not differentiable when weights are equal to zero. However, forward backward splitting (FoBoS) [10], offers a principled approach to this problem.\nRegularization, however, can ruin the sparsity of the gradient. Consider an objective function of form (Equation 1), where ||w|| for some norm ||·||. In these cases, even when the feature xij = 0, the gradient ∇wjFi is nonzero owing to the regularization penalty. A simple and correct solution is to update weights when either the weight or feature is nonzero, but to ignore weights when both are zero. Given feature sparsity and persistent model sparsity throughout training, ignoring all weights wj = 0 corresponding to features xij = 0 provides a substantial benefit. But such an algorithm would still scale with the size of the model, which may be far larger than p. Our algorithm scales in time complexity O(p)."
    }, {
      "heading" : "2.3 Forward Backward Splitting",
      "text" : "Proximal algorithms are an approach to optimization in which each update consists of solving a convex optimization problem [8]. Forward Backward Splitting (FoBoS) [10] uses proximal algorithms to provide a principled approach to online optimization with non-smooth regularizers. We first step in the direction of the negative gradient of the differentiable loss function. We then update the weight by solving a convex optimization problem, which simultaneously penalizes distance from the new parameters and minimizes the regularization term. Duchi and Singer established the convergence of the Forward Backward method in [10].\nFirst an standard unregularized stochastic gradient step is applied\nw(t+ 1 2 ) = w(t) − η(t)∇wLi (2)\nThen a convex optimization is solved which applies the regularization penalty. For elastic net the problem to be solved is\nw(t+1) = argminw\n(\n1 2 ||w −w(t+ 1 2 )||22 + η tλ1||w||1 + ηtλ2 2 ||w||22\n)\n(3)\nNote that when ∇wjL = 0, w (t+ 12 ) j = w (t) j . The problems corresponding to ℓ1 or ℓ 2 2 separately can easily be derived by setting the corresponding λ to 0."
    }, {
      "heading" : "3 Lazy Updates",
      "text" : "We extend the method for lazy updates introduced in [2], [6], and [10] to the case of ℓ2 and elastic net regularization. The essence of the algorithm for sparse lazy updates is as follows (Algorithm 1). We maintain an array ψ ∈ Rd in which each ψj stores the index of the last iteration at which each feature was nonzero and a counter k, which stores the index of the current iteration. When processing each example xi, we iterate through the nonzero features xij , comparing the current iteration index k to ψj . For each feature with nonzero weight wj , we lazily apply the k−ψj successive updates in constant time, bringing those weights current. Using the updated weights, we compute the prediction ŷ(k) with the current parameters w(k). We then compute the gradient and update the weights. When training is complete, we pass once through all nonzero weights to apply the lazy updates to bring the model current. Provided that we can\napply all lazy updates in O(1) time, our algorithm processes each example in O(p) time regardless of the dimension d.\nAlgorithm 1 Lazy Updates\nRequire: ψ ∈ Rd\nfor t ∈ 1, ..., T do Sample xi randomly from the training set for j s.t. xij 6= 0 do\nwj ← Lazy(wj, t, ψj) ψj ← t\nend for w ← w − η(t)∇wFi\nend for\nTo use the algorithm with a chosen regularizer, it remains only to demonstrate the existence of constant time updates. In the following subsections, we derive the closed form updates for ℓ1, ℓ22 and elastic net regularization, starting with the simple case when the learning rate η is fixed during each epoch, and extending to the more complicated case when the learning rate is decreased between iterations as a function of time. These results hold for schedule of weight decrease that depend on time, but cannot be directly applied to Adagrad, an algorithm for which each weight has a separate learning rate which is decreased with the inverse of the accumulated sum of squared gradients with respect to that weight."
    }, {
      "heading" : "4 Prior Work",
      "text" : "Over the last several years, a large body of work has advanced the field of online learning. Notable contributions include ways of adaptively attenuating the learning rate separately for each parameter such as AdaGrad [3] and AdaDelta [11], using small batches to reduce the variance of the noisy gradient ( [7]), and variance reduction methods such as Stochastic Average Gradient (SAG) [9], and Stochastic Variance Reduced Gradient (SVRG) [5].\nIn 2008 Carpenter described an idea for performing lazy updates for stochastic gradient descent [2]. In his method, they maintain a vector ψ ∈ Nd, where each ψi stores the index of the last epoch in which each weight was last regularized. They then perform periodic batch updates. However, as they acknowledge, the system they describe results in updates that do not produce the same result as applying an update after each time step.\nLangford et al. concurrently developed a similar approach for lazily updating ℓ1 regularized linear models [6]. They restrict their attention to ℓ1 models. Additionally, they only describe the closed form update when the learning rate η is constant, although they do suggest that an update can be derived when ηi decays as i grows large. We extend this work, deriving the closed form updates for ℓ22, and elastic net regularization. Our algorithms work for both fixed and varying learning rates.\nIn 2009, Duchi and Singer describe a method they call Forward Backward Splitting (FoBoS) for online optimization of a regularized loss function [4]. Here, after each gradient step to minimize the loss function, the weight is regularized by solving a convex optimization problem. They derive regularization updates for ℓ1, ℓ22, ℓ 2 2, and ℓ∞ norms. Further, they share the insight of applying lazy updates, when training on sparse high dimensional data. Their lazy updates hold for all norms ℓq for q ∈ 1, 2,∞, However they do not hold for the commonly used ℓ22 norm. Consequently it also does not hold for mixed norms involving the ℓ22 norm such as the widely used elastic net (ℓ1 + ℓ 2 2)."
    }, {
      "heading" : "5 Closed Form Lazy Updates for SGD",
      "text" : "In this section, we will derive constant-time stochastic gradient updates when processing examples from sparse datasets. Using these, the lazy update algorithm can train linear models with time complexity O(p). For brevity, we will describe the more general case where the learning rate is varied. When the learning rate is constant the algorithm can be easily modified to have O(1) space complexity."
    }, {
      "heading" : "5.1 Lazy SGD Update ℓ1 Regularization with Attenuated Learning Rate",
      "text" : "The closed form update for ℓ1 regularized models was derived by Singer and Duchi [10]. The constant time lazy update can be applied with:\nw (k) j = sgn(w (ψj) j )\n[\n|w (ψj) j | − λ1 (S(k − 1)− S(ψj − 1))\n]\n+ . (4)\nwhere S(t) is a function that returns the partial sum ∑t τ=0 η (τ).\nThe sum ∑t+n−1 τ=t η (τ) can be computed in constant time, using a dynamic programming scheme. On each iteration t, we compute S(t) in constant time given its predecessor as S(t) = η(t)+S(t− 1). The base case for this recursion is S(0) = η(0). We then cache this value in an array for subsequent constant time lookup.\nWhen the learning rate decays with 1/t, the terms η(τ) follow the harmonic series, and we do not require dynamic programming to compute the lazy update for ℓ1regularization. Each partial sum of the harmonic series is a harmonic number H(t) = ∑t\ni=1 1 t . Clearly ∑t+n−1 τ=t η (τ) = η(0) (H(t+ n)−H(t)), where Hτ is the τth harmonic number. While there is no instantly computable analytic expression to calculate the τth harmonic number, there exist good approximations.\nThe O(T ) space complexity of this algorithm may seem problematic. However, this easily dealt with by bringing all weights current after each epoch. The cost to do this is amortized across all iterations and is thus negligible."
    }, {
      "heading" : "5.2 Lazy SGD Update for ℓ22 Regularization with Attenuated Learning Rate",
      "text" : "We now consider the case of ℓ22 regression with attenuated learning rate. For a given examplexi, if a feature xij = 0 and the learning rate is varying, the stochastic gradient update rule for an ℓ22 regularized objective is\nw (t+1) j = w (t) j − η (t)λ2 · w (t) j\n(5)\nConsidering successive updates, the decreasing learning rate prevents a collecting lazy updates as terms in a geometric series as we could when if learning rate was fixed. However, we can employ a dynamic programming strategy, similar to the system we demonstrated for ℓ1 with attenuated learning rate.\nLemma 1. For SGD with ℓ22 regularization, the constant time lazy update to bring a weight current from iteration ψj to k is given by\nw (k) j = w (ψj) j\nP (k − 1)\nP (ψj − 1) (6)\nwhere P(t) is the partial product ∏t τ=0(1− η (τ)λ2).\nProof. Rewriting the above expressions for lazy updates:\nw (t+1) j = w (t) j (1− η (t)λ2)\nw (t+n) j = w (t) j (1− η (t)λ2)(1 − η (t+1)λ2) · ... · (1 − η (t+n−1)λ2) (7)\nThe products P (t) = ∏t τ=0(1 − η (τ)λ2) can be cached on each iteration in constant time using the recursive relation\nP (t) = (1 − η(t)λ2)P (t− 1)\nThe base case is P (0) = a0 = (1− η0λ2). Given cached values P (0), ..., P (t+ n), it is then easy to calculate the exact lazy update in constant time:\nw (t+n) j = w (t) j\nP (t+ n− 1)\nP (t− 1) . (8)\nOur claim follows.\nAs in the case of ℓ22 regularization with fixed learning rate, we needn’t worry that the regularization update will flip the sign of our weightwj , owing to ∀t ≥ 0, P (t) > 0."
    }, {
      "heading" : "5.3 Lazy SGD Update for Elastic Net Regularization with Attenuated Learning Rate",
      "text" : "Finally, we derive the constant time lazy update for SGD with elastic net regularization. recalling that a model regularized by elastic net has an objective function of the form\nF (w) = L(w) + λ1||w||1 + λ2 2 ||w||22.\nWhen a feature xj = 0, the SGD update rule is\nw (t+1) j = sgn(w (t) j )\n[\n|w (t) j | − η (t)λ1 − η (t)λ2|w (t) j |\n]\n+\n= sgn(w (t) j )\n[\n(1 − η(t)λ2)|w (t) j | − η (t)λ1\n]\n+\n(9)\nTheorem 1. To bring a weight wψjj current to w k j using update (Equation 9), the constant time update is\nw (k) j = sgn(w ψj j )\n[\n|wj | ψj\nP (k − 1)\nP (ψj − 1) − P (k − 1) · (B(k − 1)−B(ψj − 1))\n]\n+ (10)\nProof. The attenuated learning rate prevents us from working out a simple expansion like ??. Instead, we can write the following inductive expressions for a few consecutive terms in the sequence\nw (t+1) j = sgn(w (t) j )\n[\n(1− η(t)λ2)|w (t) j | − η (t)λ1\n]\n+ (11)\nSubstituting aτ = (1 − η(τ)λ2) and bτ = −η(τ)λ1 gives\nw(t+1) = sgn(w (t) j )\n[\nat|w (t)|+ bt\n]\n+\n...\nw(t+n) = sgn(w (t) j ) [ a(t+n−1)(...a(t+1) ( atw t − bt ) − b(t+1)...)− b(t+n−1) ]\n+\n= sgn(w (t) j )\n[\n|w (t) j |\nt+n−1 ∏\nτ=t\naτ +\nt+n−2 ∑\nτ=t\nbi\n(\nt+n−2 ∏\nq=τ\naq\n)\n+ b(t+n−1)\n]\n+\n(12)\nThe leftmost term, ∏t+n−1 τ=t aτ can be calculated in constant time as P (t+n−1)/P (t− 1) using cached values from the dynamic programming scheme discussed in the previous section.\nTo cache the remaining terms, we group the center and right-most terms and apply the following simplifications\nt+n−2 ∑\nτ=t\nbi\n(\nt+n−2 ∏\nq=τ\naq\n)\n+ bt+n−1\n= bt P (t+ n− 2)\nP (t− 1) + bt+1\nP (t+ n− 2)\nP (t) + ...+ bt+n−1\nP (t+ n− 2)\nP (t+ n− 2)\n= −λ1P (t+ n− 2)\n(\nη(t)\nP (t− 1) +\nη(t+1)\nP (t) + ...+\nη(t+n−1)\nP (t+ n− 2)\n) (13)\nWe now add a new layer to our dynamic programming formulation. In addition to pre-calculating all values P (t) as we go, we define a partial sum over inverses of partial products\nB(t) = t ∑\nτ=0\nη(τ)\nP (τ − 1) .\nGiven that P (t − 1) can be accessed in O(1) at time t, B(t) can now be cached in constant time. The dynamic programming here depends upon the simple recurrence relation\nB(t) = B(t− 1) + η(t)\nP (t− 1)\nwith the base case B(−1) = 0 Thus B(0) = 0 + η(0)/1 = η(0). Thus, for SGD elastic net with heuristic clipping and attenuated learning rate, the update rule to lazily apply any number n of successive regularization updates in constant time to weight wj is:\nw(t+n) = sgn(w (t) j )\n[\n|w (t) j |\nP (t+ n− 1)\nP (t− 1) − λ1P (t+ n− 1) (B(t+ n− 1)−B(t− 1))\n]\n+\n(14)"
    }, {
      "heading" : "6 Lazy Updates for Forward Backward Splitting",
      "text" : "Recall the optimization problems for ℓ1, ℓ22 and elastic net regularization (Section 2.3). They also described the lazy update for ℓ1 regularization which is the same as the truncated gradient update in (Equation 4). Thus we turn our attention to FoBoS updates for ℓ22 and elastic net regularization."
    }, {
      "heading" : "6.1 Lazy FoBoS Update for ℓ22 Regularization",
      "text" : "For ℓ22 regularization, to apply the regularization update we solve the problem from Equation 3 with λ1 set to 0. Solving forw∗ gives the simple updatew (t+1) j = w (t) j /(1+ η(t)λ2) whenever xij = 0. Note that this differs from the standard stochastic gradient descent step.\nWe can store the values P ′(t) = ∏t τ=0 1 1+ηtλ2 . Thus constant time lazy update for\nFoBoS with ℓ22 regularization to bring a weight current at time k from time ψj is\nwkj = w (ψj) j\nP ′(k − 1)\nP ′(ψj − 1) (15)\nwhere P ′(t) = (1 + η(t)λ2)1 · P ′(t− 1) with base case P ′(−1) = 1."
    }, {
      "heading" : "6.2 Lazy FoboS Update for Elastic Net Regularization",
      "text" : "Finally, in the case of elastic net regularization via forward backward splitting, we solve the convex optimization problem from Equation 3. This problem also comes apart and can be optimized for each wj separately. Setting the derivative wrt wj yields the following optimal solution:\nw (t+1) j = sgn(w (t) j )\n[\n|w (t) j | − η (t)λ1\nηtλ2 + 1\n]\n+\nTheorem 2. The exact constant time lazy update for FoBoS with elastic net regularization and attenuated learning rate to bring a weight current at time k from time ψj is\nw (k) j = sgn(w (ψj) j )\n[\n|w (ψj) j |\nP ′(k − 1)\nP ′(ψj − 1) − P ′(k − 1) · λ1 (B ′(k − 1)−B′(ψj − 1))\n]\n+ (16)\nwhere B′(t) = B′(t− 1) + η (t)\nP (t−1) with base cases B ′(0) = η(0) and B′(−1) = 0.\nProof. We substitute at = (η(t)λ2 + 1)−1 and bt = −η(t)λ1. Note that neither at nor bt depends upon wj . Consider successive updates:\nwt+1j = sgn(w t j) [ at(|w (t) j |+ bt) ]\n+\nw (t+n) j = sgn(w t j)\n\n|wtj |\nt+n−1 ∏\nβ=t\naβ +\nt+n−1 ∑\nτ=t\n(\nbτ\nt+n−1 ∏\nα=τ\naα\n)\n\n\n+\n. (17)\nIn the first term, P ′(t+n−1) P ′(t−1) can be substituted for ∏t+n−1 β=t aβ . The second term can be expanded\nt+n−1 ∑\nτ=t\n(\nbτ\nt+n−1 ∏\nα=τ\naα\n)\n=\nt+n−1 ∑\nτ=t\n(\nbτ P (t+ n− 1)\nP (τ − 1)\n)\n= −P (t+ n− 1) · λ1\nt+n−1 ∑\nτ=t\n(\nη(τ)\nP (τ − 1)\n)\n(18)\nUsing the dynamic programming approach, for each iteration t, we can calculate\nB′(t) = B′(t− 1) + η(t)\nP (t− 1)\nwith the base cases B′(0) = η(0) and B′(−1) = 0. Thus\nw (t+n) j = sgn(w t j)\n[\n|wtj | P ′(t+ n− 1)\nP ′(t− 1) − P ′(t+ n− 1) · λ1 (B ′(t+ n− 1)−B′(t− 1))\n]\n+ (19)"
    }, {
      "heading" : "7 Experiments",
      "text" : "We do not need justify the usefulness of logistic regression with elastic net regularization. However, to confirm the correctness and speed of our dynamic programming solution, we tested the system on a bag-of-words representation of abstracts from biomedical articles indexed in Medline. Our dataset contained 1, 000, 000 examples, 260,941 features and 88.54 nonzero features per document\nWe prototyped the system in Python. Sparse datasets are represented as lists of dictionaries, where keys correspond to feature indices and values correspond to word counts. We implemented both standard and lazy FoBoS for logistic regression regularized with elastic net. We confirmed on a synthetic dataset that the standard FoBoS updates and lazy updates output identical weights up to 4 significant figures. To make a fair comparison, both systems exploit sparsity when calculating predictions.\nLazy update FoBos performed 612.2 times faster than FoBoS with dense regularization updates. It is important to consider that a pure speedup proportional to the number of zeros to non-zeros in the training data would be 2947.1528×. Our additional calculations provide a constant factor slowdown. But overall, when considerably less than one quarter of all features have nonzero values, our algorithm provides a tremendous speedup. While our dynamic programming strategy consumes space linear in the number of iterations, it does not present a significant time penalty. Further, when training one epoch, storing two floating point numbers per example is a modest use of space compared to the data itself.\nFoBos Elastic Net w/ Lazy Updates FoBos Elastic Net w/ Dense Updates 1893 Examples / Second 3.086 Examples / Second"
    }, {
      "heading" : "8 Discussion",
      "text" : "Many interesting datasets are high-dimensional, and many high-dimensional datasets are sparse. To be fast, algorithms should scale with the number of non-zeros per example, not the nominal dimensionality. We have demonstrated an algorithm for fast learning on linear models with ℓ22 and elastic net regularization, verifying its correctness analytically and performance empirically."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This research was conducted with generous support from the Division of Biomedical Informatics at the University of California, San Diego, which has funded the first author via a training grant from the National Library of Medicine.Galen Andrew began evaluating lazy updates for multilabel classification with Charles Elkan in the summer of 2014. His notes provided an insightful starting point for this research. Sharad Vikram provided invaluable help in checking the derivations of closed form updates."
    } ],
    "references" : [ {
      "title" : "Stochastic gradient descent tricks",
      "author" : [ "Léon Bottou" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2012
    }, {
      "title" : "Lazy sparse stochastic gradient descent for regularized multinomial logistic regression",
      "author" : [ "Bob Carpenter" ],
      "venue" : "Alias-i, Inc., Tech. Rep, pages",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2008
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "John Duchi", "Elad Hazan", "Yoram Singer" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2011
    }, {
      "title" : "Efficient projections onto the l 1-ball for learning in high dimensions",
      "author" : [ "John Duchi", "Shai Shalev-Shwartz", "Yoram Singer", "Tushar Chandra" ],
      "venue" : "In Proceedings of the 25th international conference on Machine learning,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2008
    }, {
      "title" : "Accelerating stochastic gradient descent using predictive variance reduction",
      "author" : [ "Rie Johnson", "Tong Zhang" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2013
    }, {
      "title" : "Sparse online learning via truncated gradient",
      "author" : [ "John Langford", "Lihong Li", "Tong Zhang" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2009
    }, {
      "title" : "Efficient mini-batch training for stochastic optimization",
      "author" : [ "Mu Li", "Tong Zhang", "Yuqiang Chen", "Alexander J Smola" ],
      "venue" : "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2014
    }, {
      "title" : "Minimizing finite sums with the stochastic average gradient",
      "author" : [ "Mark Schmidt", "Nicolas Le Roux", "Francis Bach" ],
      "venue" : "arXiv preprint arXiv:1309.2388,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2013
    }, {
      "title" : "Efficient learning using forward-backward splitting",
      "author" : [ "Yoram Singer", "John C Duchi" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2009
    }, {
      "title" : "Adadelta: an adaptive learning rate method",
      "author" : [ "Matthew D Zeiler" ],
      "venue" : "arXiv preprint arXiv:1212.5701,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2012
    }, {
      "title" : "Regularization and variable selection via the elastic net",
      "author" : [ "Hui Zou", "Trevor Hastie" ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "In this paper, we develop upon a method for lazily updating weights, first described by [2], [10], and [6].",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 8,
      "context" : "In this paper, we develop upon a method for lazily updating weights, first described by [2], [10], and [6].",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 5,
      "context" : "In this paper, we develop upon a method for lazily updating weights, first described by [2], [10], and [6].",
      "startOffset" : 103,
      "endOffset" : 106
    }, {
      "referenceID" : 10,
      "context" : "In this paper, we focus on elastic net, a linear combination of l1 and l2 regularization that has been shown to produce comparably sparse models to l1 while often achieving superior accuracy [12].",
      "startOffset" : 191,
      "endOffset" : 195
    }, {
      "referenceID" : 1,
      "context" : "An appropriately attenuated learning rate ensures both that the optimal parameters will eventually be reached and that the algorithm will yield a value within a distance ǫ of the optimal value for any small value ǫ [2] .",
      "startOffset" : 215,
      "endOffset" : 218
    }, {
      "referenceID" : 0,
      "context" : "t ηt = ∞ and ∑ t η 2 t < ∞ [1].",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 8,
      "context" : "However, forward backward splitting (FoBoS) [10], offers a principled approach to this problem.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 8,
      "context" : "Forward Backward Splitting (FoBoS) [10] uses proximal algorithms to provide a principled approach to online optimization with non-smooth regularizers.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 8,
      "context" : "Duchi and Singer established the convergence of the Forward Backward method in [10].",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 1,
      "context" : "3 Lazy Updates We extend the method for lazy updates introduced in [2], [6], and [10] to the case of l2 and elastic net regularization.",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 5,
      "context" : "3 Lazy Updates We extend the method for lazy updates introduced in [2], [6], and [10] to the case of l2 and elastic net regularization.",
      "startOffset" : 72,
      "endOffset" : 75
    }, {
      "referenceID" : 8,
      "context" : "3 Lazy Updates We extend the method for lazy updates introduced in [2], [6], and [10] to the case of l2 and elastic net regularization.",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 2,
      "context" : "Notable contributions include ways of adaptively attenuating the learning rate separately for each parameter such as AdaGrad [3] and AdaDelta [11], using small batches to reduce the variance of the noisy gradient ( [7]), and variance reduction methods such as Stochastic Average Gradient (SAG) [9], and Stochastic Variance Reduced Gradient (SVRG) [5].",
      "startOffset" : 125,
      "endOffset" : 128
    }, {
      "referenceID" : 9,
      "context" : "Notable contributions include ways of adaptively attenuating the learning rate separately for each parameter such as AdaGrad [3] and AdaDelta [11], using small batches to reduce the variance of the noisy gradient ( [7]), and variance reduction methods such as Stochastic Average Gradient (SAG) [9], and Stochastic Variance Reduced Gradient (SVRG) [5].",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 6,
      "context" : "Notable contributions include ways of adaptively attenuating the learning rate separately for each parameter such as AdaGrad [3] and AdaDelta [11], using small batches to reduce the variance of the noisy gradient ( [7]), and variance reduction methods such as Stochastic Average Gradient (SAG) [9], and Stochastic Variance Reduced Gradient (SVRG) [5].",
      "startOffset" : 215,
      "endOffset" : 218
    }, {
      "referenceID" : 7,
      "context" : "Notable contributions include ways of adaptively attenuating the learning rate separately for each parameter such as AdaGrad [3] and AdaDelta [11], using small batches to reduce the variance of the noisy gradient ( [7]), and variance reduction methods such as Stochastic Average Gradient (SAG) [9], and Stochastic Variance Reduced Gradient (SVRG) [5].",
      "startOffset" : 294,
      "endOffset" : 297
    }, {
      "referenceID" : 4,
      "context" : "Notable contributions include ways of adaptively attenuating the learning rate separately for each parameter such as AdaGrad [3] and AdaDelta [11], using small batches to reduce the variance of the noisy gradient ( [7]), and variance reduction methods such as Stochastic Average Gradient (SAG) [9], and Stochastic Variance Reduced Gradient (SVRG) [5].",
      "startOffset" : 347,
      "endOffset" : 350
    }, {
      "referenceID" : 1,
      "context" : "In 2008 Carpenter described an idea for performing lazy updates for stochastic gradient descent [2].",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 5,
      "context" : "concurrently developed a similar approach for lazily updating l1 regularized linear models [6].",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 3,
      "context" : "In 2009, Duchi and Singer describe a method they call Forward Backward Splitting (FoBoS) for online optimization of a regularized loss function [4].",
      "startOffset" : 144,
      "endOffset" : 147
    }, {
      "referenceID" : 8,
      "context" : "1 Lazy SGD Update l1 Regularization with Attenuated Learning Rate The closed form update for l1 regularized models was derived by Singer and Duchi [10].",
      "startOffset" : 147,
      "endOffset" : 151
    } ],
    "year" : 2017,
    "abstractText" : "We extend previous work on efficiently training linear models by applying stochastic updates to non-zero features only, lazily bringing weights current as needed. To date, only the closed form updates for the l1, l∞, and the rarely used l2 norm have been described. We extend this work by showing the proper closed form updates for the popular l22 and elastic net regularized models. We show a dynamic programming algorithm to calculate the proper elastic net update with only one constant-time subproblem computation per update. Our algorithm handles both fixed and decreasing learning rates and we derive the result for both stochastic gradient descent (SGD) and forward backward splitting (FoBoS). We empirically validate the algorithm, showing that on a bag-of-words dataset with 260, 941 features and 88 nonzero features on average per example, our method trains a logistic regression classifier with elastic net regularization 612 times faster than an otherwise identical implementation with dense updates.",
    "creator" : "LaTeX with hyperref package"
  }
}