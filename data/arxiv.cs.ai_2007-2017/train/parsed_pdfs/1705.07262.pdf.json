{
  "name" : "1705.07262.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Batch Reinforcement Learning on the Industrial Benchmark: First Experiences",
    "authors" : [ "Daniel Hein", "Steffen Udluft", "Michel Tokic", "Alexander Hentschel", "Thomas A. Runkler", "Volkmar Sterzing" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 5.\n07 26\n2v 2\n[ cs\n.L G\n] 2\nThe experimental results of PSO-P on IB are compared to results of closed-form control policies derived from the modelbased Recurrent Control Neural Network (RCNN) and the model-free Neural Fitted Q-Iteration (NFQ).\nExperiments show that PSO-P is not only of interest for academic benchmarks, but also for real-world industrial applications, since it also yielded the best performing policy in our IB setting. Compared to other well established RL techniques, PSOP produced outstanding results in performance and robustness, requiring only a relatively low amount of effort in finding adequate parameters or making complex design decisions.\nI. INTRODUCTION\nThe process of controlling industrial plants, or parts of such, involves a variety of challenging aspects that reinforcement learning (RL) [1] algorithms need to tackle. For coping adequately with the complexity of real-world systems, important challenges that need to be considered are: continuous state and action spaces, high-dimensional and only partially observable state spaces, stochasticity induced from heteroscedastic sensor noise and latent variables, delayed effects, multi-criterial reward components, and non-stationarity in the optimal steerings, i.e. the optimal policy will not approach a fixed operation point.\nHere, we consider applications where on-line learning, like the classical temporal-difference learning approach [2], is prohibited for safety reasons, since it requires exploration on\nthe plant’s dynamics. In contrast, batch RL algorithms generate a policy based on existing data, which is deployed to the plant after training. In this setting, either the value function or the system dynamics are trained by historic operational plant data, which is a set of four-tuples (observation, action, reward, next observation) called batch in the following. Research from the past two decades suggests that the family of batch RL algorithms [3]–[6], meet the requirements of real-world systems, especially when involving neural networks modeling either the state/action value function [7]–[11], or the system dynamics [12]–[14]. Moreover, batch RL algorithms are data efficient [7], [15], because the batch data is utilized repeatedly during the training phase.\nIn the following we investigate different RL approaches on the Industrial Benchmark (IB) [16] that comes with challenges being vital in industrial settings as described above. We report on results for applying Particle Swarm Optimization Policy (PSO-P) [17], which is a powerful algorithm for RL with continuous actions that achieves remarkable results out of the box. The actions to perform are derived from rollouts on a system model, which simulates the IB’s transition dynamics. This model, a recurrent neural network, was trained on a batch of transitions sampled from applying random actions to the IB. In real-world applications, however, transitions are usually available in form of historic operational data, that might have been produced by a constant default controller.\nWe compare the performance of PSO-P on IB with two other RL approaches, that utilize the batch in different ways. First, the Recurrent Control Neural Network (RCNN) [15], which is a model-based RL algorithm for continuous actions, that uses the system model during training of the controller. Second, Neural Fitted Q-Iteration (NFQ) [7], a model-free RL algorithm for discrete actions, where the controller is learned via iteratively applying Watkins’ Q-learning algorithm [18] on the batch data. Here, the system model is used to select the\nbest policy after the NFQ training has finished. Policy selection is necessary, because the performance of NFQ policies can decrease significantly over iterations. This is a well-known phenomenon in the context of neuro-dynamic programming [19].\nII. INDUSTRIAL BENCHMARK\nThe Industrial Benchmark1 (IB) [16] aims at being realistic in the sense that it includes a variety of aspects that we found to be vital in industrial applications. It is not designed to be an approximation of any specific real-world system, but to pose a comparable hardness and complexity found in many industrial applications. State and action space are continuous, the state space is high-dimensional and only partially observable. The actions consist of three continuous components and affect three steerings. Moreover, the IB includes stochastic and delayed effects. The optimization task is multi-criterial in the sense that there are two reward components that show opposite dependencies on the actions. The dynamical behavior is heteroscedastic with state-dependent observation noise and statedependent probability distributions, based on latent variables. Furthermore, it depends on an external driver that cannot be influenced by the actions. The IB is designed such that the optimal policy will not approach a fixed operation point in the three steerings. Any specific choice is driven by our experience with industrial challenges.\nAt any time step t the RL agent can influence the environment (IB) via actions at that are three dimensional vectors in [−1, 1]3. Each action can be interpreted as three proposed changes to three observable state variables called current steerings. Those current steerings are: velocity v, gain g, and shift h. Each of those is limited to [0, 100], yielding\nat = (∆v,∆g,∆h), (1)\nvt+1 = max(0,min(100, v + dv∆v)), (2)\ngt+1 = max(0,min(100, g + dg∆g)), (3)\nht+1 = max(0,min(100, h+ dh∆h)), (4)\nwith scaling factors dv = 1, dg = 10, and dh = 5.75. After applying the action at, the environment transitions to the next time step t + 1, yielding the internal state st+1. State st and successor state st+1 are the Markovian states of the environment. In many industrial applications, an operatordefined load pt is applied to the system. Depending on load pt and the control values at, the system shows fatigue ft and consumes resources such as power, fuel, etc., represented by consumption ct. Both, pt and at, are external drivers for the IB. In response, the IB generates values for ct+1 and ft+1, which are part of the internal state st+1. The reward is solely determined by st+1:\nrt = −ct+1 − 3ft+1. (5)\nIn the real world tasks that motivated the IB, the reward function has always been known explicitly. That is why we\n1Source code available at: http://github.com/siemens/industrialbenchmark\nassume that here the reward function is also known and consumption and fatigue are observable. However, except for the values of the steerings, the remaining part of the Markov state remains unobservable. This yields an observation vector ot ⊂ st consisting of:\n1) the current steerings: velocity vt, gain gt, and shift ht, 2) the external driver: set point pt, 3) and the reward relevant variables: consumption ct and\nfatigue ft.\nOne of the IB’s features is the possibility to freeze its stochasticity. On the one hand, for data generation, online RL experiments, and policy evaluation, stochasticity makes the benchmark realistic and challenging. On the other hand, there are some settings where freezing the randomness in the stochastic effects might be useful. This is realized by remembering the applied seed of the IB-internal pseudo-random number generator (RNG). For instance, in the experiments presented in Section IV, we searched for the true maximum reward given a certain Markov state and its current RNG seed, as the upper bound for the RL technique performance evaluation. This has been done by applying PSO-P directly on the IB system dynamics, provided with full knowledge about the future, encoded in the RNG seed."
    }, {
      "heading" : "III. PSO-P",
      "text" : "In the Particle Swarm Optimization Policy (PSO-P) framework [17], solving an RL problem is reformulated as an optimization problem. RL is an area of machine learning, where the Markov decision problem has to be solved by learning from observed state transitions (st, at) → (st+1, rt), with st and st+1 representing the Markovian states, at the applied action, and rt the real-valued reward, at discrete time steps t and t+1. The goal is to find a policy maximizing the expected cumulative reward R = ∑∞ k=0 γ krk, called return, where 0 ≤ γ ≤ 1 is the so-called discount factor [1]. Since the true underlying Markovian state st is not observable in the IB, it is approximated by a sufficient amount of historic observations, i.e. the information contained in st is approximated by (ot−H , . . . , ot) with horizon H . Given a system model m(ot−H , . . . , ot, at) = (ot+1, rt) (see Section IV), trained by supervised learning methods on previous observations, finding the best action at, for a given an observation horizon with respect to the return R, is described as\nR(ot−H , . . . , ot,x) =\nT−1∑\nk=0\nγkrt+k,with (6)\n(ot+k+1, rt+k) = m(ot+k−H , . . . , ot+k, at+k).\nThe discount factor γ is chosen such that at the end of the time horizon T , the last reward accounted for is weighted by q ∈ [0, 1], computed by γ = q1/(T−1). Particle swarm optimization [20] (PSO) is then searching for the optimal action sequence x̂ = (at, at+1, . . . , at+T−1) satisfying\nx̂ ∈ argmax x∈AT R(ot,x). (7)\nAnalogues to receding horizon control (RHC), only the first element of x̂ is returned. This yields an RL policy at = π(ot), which conducts an optimization for every new observation. This might be computationally expensive, but it does not rely on a predefined closed-form policy structure, which very often is a hard to asses a priori assumption for common RL algorithms on novel problems."
    }, {
      "heading" : "IV. EXPERIMENTS",
      "text" : "We compare the performance of PSO-P on IB with the well established RCNN and NFQ methods. For all of the applied RL techniques, we required an adequate system model m simulating IB trajectory rollouts:\n• RCNN: The RCNN is trained on m to calculate the\nrespective gradients for the policy’s weight update step.\n• NFQ: Despite NFQ being considered model-free RL, it\nis still very useful to evaluate the policy’s performance after each Q-iteration step on m, since performance drops during the training are very likely to occur when applying NFQ on off-policy batch data. Therefore, in our experiments the policy with the best performance according to model m is saved and returned as the final NFQ training result. • PSO-P: The policy represented by PSO-P utilizes m\nduring runtime. PSO-generated trajectories are rated by performing rollouts on m in every policy query.\nIn the following experiments the system model m predicts consumption c and fatigue f for each step of the rollout, and the reward is computed according to Eq. (5).\nTo generate an adequate training data set D, we initialized the IB 10 times for each set point p = {10, 20, . . . , 100} and produced random trajectories of lengths 1 000, resulting in |D| = 100 000.\nThe system model m was chosen to consist of two recurrent neural networks (RNN) mc and mf , to predict consumption c and fatigue f , respectively. Both models are unfolded a sufficient number of steps into the past (Hc = 30 time steps for mc, Hf = 10 time steps for mf ) and 50 time steps into the future. In each time step they take the observable variables of the past and present as input. Whereas, in the future branch of the RNNs only the steerings (velocity, gain, and shift) are used as input. The topology of these RNNs is described in [21] as ”Markov Decision Process Extraction Network”.\nBoth models have been trained with the RPROP learning algorithm [22] on the data set D, with 70% training and 30% validation data for early stopping. The training process was repeated 8 times and the networks with the lowest validation error were chosen. In our experiments, we could validate that the training process of these RNNs is stable and the results depend little on the chosen learning algorithm.\nFig. 1 depicts the squared error of the two selected RNNs with respect to the true IB values and describes the ’no self input’ and ’self input’ design of both RNNs."
    }, {
      "heading" : "A. RCNN",
      "text" : "The Recurrent Control Neural Network (RCNN) [15] consists of two parts. One is a system model m which is trained to predict the return by a rollout of length T . The second part is a policy network, that computes for each step of the rollout an action to be fed in the system model. This policy network takes as input the internal state of the system model m, which has the Markov property approximately [21].\nThe policy network has been trained with the same data set D, as the system model m. It uses the 30+30 neurons of the internal states of the consumption and the fatigue networks, mc and mf , as input, followed by two hidden layers with 12 and 6 neurons, respectively, and three output neurons to encode the changes in the three steering variables, velocity, gain, and shift. The hidden layers use hyperbolic tangent as activation function, the output layer uses the sine function. All\nthese configuration parameters have been chosen with almost no tuning. Some tuning has been necessary to configure the training process of the policy network, though. Neither for RPROP, nor Vario-Eta [23], nor momentum-backprop stable training behaviors have been observed. The best results have been observed with online-backprop with small mini-batches and small learning rates η. We used random learning rates between η = 10−4 and η = 10−6, uniformly chosen in the logarithm of η, and a batch size of one.\nOne note concerning the possibility to assess the quality of a trained policy without executing it on the ”real system” [24], here the IB: if the validation error of a system model is lower on average for a rollout of sufficient length, it is the better system model (Fig. 1). If the selected system model estimates a higher return over the rollout of sufficient length for a given policy, that policy is likely to perform better, when executed on the IB (Fig. 2)."
    }, {
      "heading" : "B. NFQ",
      "text" : "The policies of Neural Fitted Q-iteration (NFQ) [7] were trained using a [9-20-1]-layered feed-forward MLP, with 9 neurons on the input layer for the observation ot and action at, and 20 neurons on the hidden layer. The output layer comprises one neuron for the associated Q-value Q(ot, at). All neurons of the neural network utilize a logistic activation function. Since NFQ is an algorithm for discrete actions, we discretized the three delta steerings towards a setting of either −1, 0, or 1 to each steering, which in total yields |A| = 27 different actions.\nThe weights of the networks were trained using non-batch stochastic gradient descent with a manually tuned constant learning rate of η = 0.1. This setting produced better results than using RPROP, as suggested by Riedmiller [7], because weights trained with RPROP tended to be unstable during learning on our dataset. Before starting the training, the 100 000 training samples from D were permuted. Furthermore, we divided D into a set of training data (90%) and validation data (10%) for early-stopping. The input data is presented to the neural network using a Z-score transformation. The Qvalues of the output layer were scaled into the interval of the activation function as proposed by Gabel et al. [25].\nThe overall training and evaluation procedure is as follows. After creating an MLP with random initial weights from the interval [−0.1, 0.1], NFQ is performed over 200 iterations. Each row of training data is presented to the neural network for a maximum of 300 training epochs, in case the error on the validation set does not start to raise within 10 epochs. During the experiments we observed that the performance of NFQ policies, once successfully learned, can degrade over time. This is consistent with findings in [3], [19], [26]. Therefore, we utilized a policy selection process in each experiment: we evaluated the Q-function after each NFQ iteration on the system model m and saved the policy with the best performance according to m. Subsequently, only this policy is retained and its performance is evaluated on the IB over 10 initial states (set points p = {10, 20, . . . , 100})."
    }, {
      "heading" : "C. PSO-P",
      "text" : "In the PSO-P setup we applied a PSO search on m with 100 particles searching for 100 iterations until the best trajectory found so far was returned. The planning time horizon was set to T = 50, which yielded γ = 0.251/49 = 0.9721 as discount factor. The particles were arranged in the so called star topology, i.e. each particle connected with every single other particle in the swarm [27].\nThe calculation of the particles’ fitnesses could be computed in parallel on 96 CPUs 2, resulting in an overall computation time of less than 8 seconds to compute at = π(ot). While today the computation time might still be too long for several real-world industrial applications, in the future the increase in CPU speed and/or parallelization, as well as computation on GPU clusters might make PSO-P computational tractable for more and more applications."
    }, {
      "heading" : "V. DISCUSSION",
      "text" : "All of the three applied RL techniques were able to produce decent results on the IB. The average rewards per step are given in tables in Appendix A.\nFig. 3 condenses the results of 30 RL policies and highlights the superior performance of PSO-P. This RL technique produces significantly more robust results than NFQ and RCNN. Note that all of the techniques have been trained/evaluated on the same system model m.\n2Intel Xeon CPU E7-4850 v2 @ 2.30GHz\nThe NFQ results are of the lowest performance in our experiments. This can be partially explained by the fact that NFQ applied only discrete actions, which is some limitation given that the IB is designed to work on continuous action spaces. Nevertheless, a second NFQ-inherent problem which has been revealed, is its highly unstable training behavior in off-policy, batch-based trainings settings. The training process itself gives no answer to the question when to stop the training. One might think that it might be a good plan to perform the training as long as computationally feasible, and use the result from the last iteration. In our experiments, this procedure would have only created bad policies (see Fig. 4). Significantly better results were achieved by evaluating the resulting NFQ policy of each iteration with the system model m. The policy which produced the highest approximated average reward was then declared the best NFQ policy of each experiment. Fig. 5 gives a detailed explanation on some properties of the best performing NFQ policy.\nThe RCNN experiments produced better results, compared to NFQ. RCNN policies operate in the continuous IB action space and yield compact closed-form policies. Even though all of the trained policies yielded similar training errors, their real performance evaluated on the IB differs quite a lot. This property implies that RCNN is rather sensitive towards prediction errors of the system modelm. Fig. 6 gives a detailed explanation on some properties of the best performing RCNN policy.\nIn our experiments PSO-P has demonstrated high reward performance and outstanding robustness, that have been observed before [17]. For 8 out of 10 set point values for p, PSO-P yielded the best RL policy on average. Moreover, the performances of all experiments were very close to each other,\nwhich implies a high robustness against different initial PSO conditions, like particle positions and velocities. Fig. 7 gives a detailed explanation on some properties of the best performing PSO-P result."
    }, {
      "heading" : "VI. CONCLUSION",
      "text" : "In this paper, we have compared the new RL approach PSOP with two standard RL techniques, RCNN and NFQ, on a recently introduced industrial benchmark. This benchmark has been designed to imitate realistic behavior and aspects which can be found in real-world industrial applications.\nThe experiments show important steps of the off-policy, batch based method stack necessary for applying RL in industrial facilities. Starting with limited exploration data, an RNN system model is trained and tested. Such a model is crucial, because applying random policies on the real system is usually prohibited in real-world applications. Despite NFQ being classified as a model-free RL technique, our experiments show that it still requires a precise system model for policy selection. The same model has been used for either training a closed-form neural network policy (RCNN), policy selection (NFQ), and exploiting the model for finding optimal actions (PSO-P).\nNFQ, with its inherent limitation to discrete actions, and its tendency for instability during the training process, produced the worst performing policies. Although higher performance could be achieved, for example, by increasing the discrete\naction space and approximating the Markov state by concatenating historic observations.\nRCNN, with its ability to apply continuous actions and an inherent policy performance measure, computed closed-form policies of good performance. Possible improvements are, for example, different network topologies, bigger neural networks, and more advanced neural learning algorithms.\nPSO-P demonstrated the best performance with unmatched robustness. Out of the box, by only setting few, easy to determine parameters, it produced the best results for almost\nevery set point. The biggest disadvantage of this technique lies in the computational effort required for the determination of the next action. In our experiments the next action has been computed in less than 8 seconds, which is still too long for many industrial applications. Improvements increasing computational power and speed might lower this value, until\nit becomes feasible for more and more applications.\nIn summary, first experiences have been made with the IB, which indeed contains many realistic objectives, issues, and features. Experiments have shown that the benchmark could be solved by three completely different RL techniques in a realistic off-policy, batch-based setting."
    }, {
      "heading" : "ACKNOWLEDGMENT",
      "text" : "The project this report is based on was supported with funds from the German Federal Ministry of Education and Research under project number 01IB15001. The sole responsibility for the report’s contents lies with the authors."
    }, {
      "heading" : "APPENDIX A RESULT TABLES",
      "text" : "Tables I, II, and III contain the average per step rewards for each of the experiments. The maximum achievable average reward is given in brackets in the first column. These values have been computed by applying PSO-P on the real IB system dynamics under preservation of the initial seed of the pseudo-random number generator, i.e. the optimizer searched for the best actions given a fixed and infinitely replicable future. As a result, a very accurate estimate of the maximum achievable average reward for each initial Markov state has been evaluated."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "The Particle Swarm Optimization Policy (PSO-P)<lb>has been recently introduced and proven to produce remarkable<lb>results on interacting with academic reinforcement learning<lb>benchmarks in an off-policy, batch-based setting. To further<lb>investigate the properties and feasibility on real-world applica-<lb>tions, this paper investigates PSO-P on the so-called Industrial<lb>Benchmark (IB), a novel reinforcement learning (RL) benchmark<lb>that aims at being realistic by including a variety of aspects found<lb>in industrial applications, like continuous state and action spaces,<lb>a high dimensional, partially observable state space, delayed<lb>effects, and complex stochasticity.<lb>The experimental results of PSO-P on IB are compared to<lb>results of closed-form control policies derived from the model-<lb>based Recurrent Control Neural Network (RCNN) and the<lb>model-free Neural Fitted Q-Iteration (NFQ).<lb>Experiments show that PSO-P is not only of interest for<lb>academic benchmarks, but also for real-world industrial appli-<lb>cations, since it also yielded the best performing policy in our IB<lb>setting. Compared to other well established RL techniques, PSO-<lb>P produced outstanding results in performance and robustness,<lb>requiring only a relatively low amount of effort in finding<lb>adequate parameters or making complex design decisions.",
    "creator" : "LaTeX with hyperref package"
  }
}