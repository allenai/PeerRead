{
  "name" : "1301.3868.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Making Sensitivity Analysis Computationally Efficient",
    "authors" : [ "Uffe Kj::erulff", "Linda C. van der Gaag" ],
    "emails" : [ "uk@cs.auc.dk", "linda@cs." ],
    "sections" : [ {
      "heading" : null,
      "text" : "To investigate the robustness of the output probabilities of a Bayesian network, a sensi tivity analysis can be performed. A one-way sensitivity analysis establishes, for each of the probability parameters of a network, a func tion expressing a posterior marginal proba bility of interest in terms of the parameter. Current methods for computing the coeffi cients in such a function rely on a large num ber of network evaluations. In this paper, we present a method that requires just a single outward propagation in a junction tree for es tablishing the coefficients in the functions for all possible parameters; in addition, an in ward propagation is required for processing evidence. Conversely, the method requires a single outward propagation for computing the coefficients in the functions expressing all possible posterior marginals in terms of a sin gle parameter. We extend these results to an n-way sensitivity analysis in which sets of pa rameters are studied.\n1 INTRODUCTION\nThe robustness of the output probabilities of a Bayesian network can be investigated by performing a sensitivity analysis of the network. For mathemat ical models in general, sensitivity analysis serves to identify the effects of the inaccuracies in a model's pa rameters on its output (Morgan & Henrion 1990). For a Bayesian network, more specifically, performing a sensitivity analysis yields insight in the relation be tween the probability parameters of the network and its posterior marginals. The simplest type of sensi tivity analysis is a one-way analysis in which a single parameter is studied; a more general n-way analysis serves to investigate the joint effects of inaccuracies in\na set of parameters.\nIn the brute-force approach to performing a one-way sensitivity analysis of a Bayesian network, each proba bility parameter of the network is varied systematically and the effect on the output probabilities of the net work is investigated. Performing a sensitivity analysis in this way requires thousands of network evaluations, or (full) propagations, and is, therefore, much too time consuming to be of any practical use.\nLaskey (1995) has been the first to address the compu tational complexity of sensitivity analysis of Bayesian networks. She has introduced a method for computing the partial derivative of a posterior marginal proba bility with respect to a parameter under study. Her method thus yields a first-order approximation of the effect of varying a single probability parameter on a posterior marginal. Compared to the brute-force ap proach, her method requires considerably less compu tational effort. The method, however, provides insight only in the effect of small variations of parameters; when larger variations are considered, the quality of the approximation may rapidly break down.\nThe relation between a posterior marginal probability of interest and a parameter under study can be ex pressed through a simple mathematical function. The function expressing the posterior marginal is a quo tient of two linear functions in the parameter, as has been shown by Castillo et al. (1996). Building upon this property, it suffices to establish the coefficients in this function to determine the effect of parameter variation. Castillo et al. (1997) and Coupe & van der Gaag (1998) have designed methods to this end. These methods require a single network evaluation for each coefficient to be established. Although these methods currently are the most efficient available, they rely on a large number of network evaluations and, as a con sequence, are infeasible for large realistic networks.\nIn this paper, we present a new method for sensitivity analysis of Bayesian networks. Our method, like the\n318 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\ntwo methods mentioned above, exploits the property that a posterior marginal probability relates by a sim ple mathematical function to a parameter under study. It requires just a single outward propagation in a junc tion tree, however, to compute the coefficients in the functions for all possible parameters; in addition, it re quires an inward propagation for processing evidence. Conversely, the method requires a single outward prop agation for establishing the coefficients in the functions expressing all possible posterior marginals in terms of a single parameter. Our method can be readily ex tended to an n-way sensitivity analysis in which sets of parameters are varied.\nIn addition to a sensitivity analysis, an uncertainty analysis can be performed for investigating the robust ness of the output probabilities of a Bayesian network. In an uncertainty analysis, all parameters are varied si multaneously through sampling; it therefore provides little insight into the effects of variation of specific pa rameters. Experiments with uncertainty analysis have led to the suggestion that Bayesian networks are in sensitive to inaccuracies in their parameters (Pradhan et al. 1996, Henrion et al. 1996). In these experiments, however, a measure of model robustness was obtained by assuming a lognormal distribution for each parame ter and averaging over the probability of the true diag nosis for various diagnostic situations in a medical ap plication. Rather than in the average of the probabili ties of the true diagnosis, however, it is in the variation of these probabilities that inaccuracies in parameters are reflected. From these experimental results, there fore, no decisive conclusions can be drawn as to the sensitivity of Bayesian networks. In fact, Coupe et al. (1999) have reported high sensitivities in an emprical study in the medical domain, involving real patient data. We feel that these and emerging similar expe riences warrant further investigation into sensitivity analysis of Bayesian networks.\nThe paper is organised as follows. Section 2 reviews the important basic property that a posterior marginal probability can be expressed as a quotient of two linear functions in a parameter under study. In Section 3, we briefly describe currently available methods for sensi tivity analysis that build upon this property. In Sec tion 4, we present our method for computing the co efficients in the functions for all possible parameters, using just one propagation in a junction tree. In Sec tion 5, we describe a similar method for computing the coefficients in the sensitivity functions relating all possible posterior marginals to a single probability pa rameter. These methods are generalised to an n-way sensitivity analysis in Section 6. The paper ends with some concluding remarks in Section 7.\n2 THE BASIC P ROPERTY\nSensitivity analysis of a Bayesian network basically amounts to establishing, for each of the network's pa rameters, a function expressing an output probabil ity in terms of the parameter under study. For out put probabilities, we shall consider posterior marginal probabilities of the form y = p( a I e), where a is a value of a variable A and e denotes the evidence avail able. Each of the network's parameters is of the form x = p(bi l1r), where b; is a value of a variable B and 1r is an arbitrary combination of values of the set of parents II= pa(B) of B. We will write p(ale)(x) to denote the function expressing the posterior marginal p( a I e) in terms of the parameter x. In the sequel, we will assume that in a sensitivity anal ysis, upon varying a parameter x = p(b; l1r), each of the other probabilities p( bj l1r) is co-varied accordingly, by scaling by the ratio between the probability masses left. More formally, let the variable B have for its do main dom(B) = {b1, ... , bm}, m 2: 1. Note that the parameters p(bj l1r), j =f. i , are functions of x. We now assume for these functions that\nwith p(b; l1r) < 1.\nif j = i otherwise, (1)\nWith the assumption of co-variation as outlined above, the function y(x) yielded by a sensitivity analysis is a quotient of two linear functions in x. The following theorem reviews this important property; the associ ated proof provides the basis for the algorithms pre sented in Sections 4 and 5. Theorem 1 Let p be the probability function defined by a Bayesian network over a set of variables V. Let y = p( a I e) and x = p(b; l1r) be as indicated above. Then,\ny= p(a, e)(x) p( e) (x) ax+ f3 \"(X+ 8' (2)\nwhere a, (3, \"(, and 8 are constants with respect to x. Proof: The joint probability p(a, e) can be expressed in terms of x as\np(a,e}(x) � (�:{V)) (x),\nwhere Lv:a, ... ,dp(V) denotes summation over the variables V\\ {A, . . . , D} with A, ... , DE V fixed at values a, ... , d, respectively. The sum Lv:a,e p(V) in the above equation can be split into n + 1 separate sums, such that the first sum\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 319\nincludes only terms with the value b1 for B and the state 1r for II, the second sum includes only terms with the value b2 for B and II in state 1r, and so on, and the last sum includes the remaining terms. So,\n\" l:v:a,e,b;,11' p(V) + \" p(V). L 1-p(b·l7r) L j#i ' V:a,e,n'f11'\nFor the probability p( e) we derive a similar expression by summing, in the above derivation, over all values of the variable A instead of keeping it fixed at a. From the resulting expressions p(a, e)(x) and p(e)(x), it is readily seen that the output y = p(a I e) can be written as a quotient of two functions that are linear in x. D\nFrom Theorem 1 we have that the function that ex presses a posterior marginal probability y in terms of a single parameter x is characterised by at most three coefficients. The theorem is easily extended to\nn parameters. The function then includes the prod ucts of all possible combinations of parameters, termed monomials, in both its numerator and its denomina tor. The numerator as well as the denominator are characterised by 2n coefficients, many of which may be zero.\n3 CURRENT METHODS\nThe most efficient methods for sensitivity analysis of Bayesian networks currently available exploit the basic property reviewed in the previous section. We briefly review these methods.\nNot all parameters in a Bayesian network can influ ence a posterior marginal probability of interest. The subset of parameters (possibly) influencing the poste rior marginal is dependent upon the evidence e. The set of relevant parameters is easily identified using a variation of the algorithm described by Geiger et a!.\n(1990), as described by Castillo et a!. (1997). After having identified the set of relevant parameters, the sensitivity analysis can be restricted to this set.\nBuilding upon the set of n relevant parameters, x1, . . . , Xn, the algorithm of Castillo et a!. (1997) iden tifies sets of monomials for which the coefficients will be zero in the linear function p(a, e)(x1, . . . , Xn) · For the resulting m monomials, the algorithm constructs a system of m independent equations of the form yi = p(a, e)(xi, . . . , x�), where, for each j, xj denote arbitrary values for parameter Xj. The corresponding values yi, i = 1, ... , m, are obtained through m net work evaluations. The coefficients in the function are now determined by solving the set of equations thus obtained. Coupe & van der Gaag independently de scribed a similar method, also based on the idea of solving a system of independent equations. They fur ther argue that in a one-way sensitivity analysis three network evaluations suffice per relevant parameter.\nThe methods reviewed above have a computational complexity that is considerably less than the brute force approach of systematic variation of parameters. However, the methods can still be quite time consum ing: for a network of realistic size, it can easily require several hundreds of network evaluations to perform a one-way sensitivity analysis. An more general n-way sensitivity analysis to study the joint effect of simulta neous variation of n parameters can in fact be so time consuming that it is infeasible in practice.\n4 ANALYSIS OF ONE OUTPUT WRT. ALL PARAMETERS\nThe new methods for sensitivity analysis presented in this paper have been tailored to Bayesian networks in their junction-tree representation. The methods basi cally perform a single or a few outward propagations in a junction tree and, as a result, are much less time consuming than the methods reviewed in the previous section.\nIn this section, we present our method for computing the coefficients in the sensitivity functions expressing a posterior marginal of interest y = p( a I e) in terms of all possible probability parameters x. Recall that these functions are of the form presented in Theorem 1. Our method now builds on the idea that, in a junction tree, the expressions for p( a, e) and p( e) in terms of x can be derived from the potential of a clique containing both the variable and the parents to which the parameter x pertains. The following theorem details the coefficients to be computed.\nTheorem 2 Let p be the probability function defined by a Bayesian network and let T be a junction tree\n320 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\nfor the network. Let y = p(aie) and x = p(b; l1r) be as before. Suppose that, in T, an inward propagation has been performed towards a clique containing the variable of interest A; suppose that subsequently an outward propagation from this clique has been per formed with the value a for A. Now, let K be a clique in T containing both the variable B and its par ents II = pa(B); let ¢x = p(K, a, e) be the potential of clique K after the abovementioned propagations. Then, p(a, e)(x) = ax+ (3 with\nLK:b;,,. ¢x \"\"'LK:b;,,. ¢x a= p(b;l1r) - �1-p(b;l7r)' J -r- 1 (3)\n(3 = L LK:b;,,. ¢x + L ¢x.\n(4) #i 1-p(b; 17r) K:ll-=f-11\"\nProof: The property follows directly from the proof of Theorem 1 by observing that p( a, e) = 2:: K ¢ K. D\nBuilding upon similar observations, we have the fol lowing corollary.\nCorollary 1 Let p be the probability function defined by a Bayesian netwerk and let T be a junction tree for the network. Let x = p(b; l1r) and K be as before. Suppose that the evidence e has been processed in T by an inward and subsequent outward propagation. Let ¢'K = p(K, e) be the potential of clique K after the propagation. Then, p(e)(x) = \"(X+ 6 with\n6 = \"\"' LK:b;,,. ¢'K\n+ \"\"' ¢'K. � 1 -p(b ·i7r) � j-=f-i 1 K:0-=/-1r\n(5)\n(6)\nTheorem 2 and Corollary 1 provide the basis for our method for computing the coefficients in the func tions expressing the posterior marginal of interest y = p(a I e) in terms of all possible parameters x. The method is composed of the following steps:\n1. Enter the evidence e into the junction tree and perform an inward and an outward propagation using an arbitrary root clique.\n2. Compute the coefficients 'Y and 6, using the equa tions (5) and (6) from Corollary 1, for all relevant parameters, locally per clique.\n3. Perform an outward propagation from a clique containing the variable of interest A, with the ad ditional evidence A = a.\n4. Compute the coefficients a and (3, using the equa tions (3) and (6) from Theorem 2, for all relevant parameters, locally per clique.\nWe would like to note that our method requires just one inward and two outward propagations to estab lish all sensitivity functions for a specific posterior marginal, whereas the methods reviewed in the pre vious section require three inward and outward prop agations per parameter.\nThe method described above outlines the basic idea. The method, however, may be easier to implement in the alternative form based upon Theorems 3 and 4.\nTheorem 3 Let the junction tree T be as before. Also, let y = p(aie) and x = p(b;l1r) be as before and let K be a clique in T including both B and II = pa( B). Now, let x1 be the initially specified value for x and let x2 denote an arbitrary other value for x. Suppose that, in T, an inward propagation has been performed towards a clique containing the vari able of interest A; suppose that subsequently an out ward propagation has been performed with the value a for A. Now, let ¢x = p(K, a, e) be the resulting clique potential for clique K. Let\ny1 = p(a, e)(x1) = L ¢x, K\n2 2 \"\"' p'(Bi1r)\ny =p(a,e)(x )= � ¢Kp(B i 7r) '\n(7)\n(8)\nwhere p( B l1r) and p' ( B l1r) denote parameter vectors with x = x1 and x = x2, respectively. Then, y = ax+ (3 with\n(9)\nProof: Since both variable B and its parents are in cluded in clique K, we can obtain from the parameter vector\np(Bi1r) = (q1 (x1 ), ... , Qi-1 (x1 ), x1, Qi+l (x1 ), ... , Qn(x1 ))\nthe parameter vector\np'(Bi1r) (q� (x2), ... 'q�-1 (x2), x2, q�+l (x2), ... , q�(x2))\nwhere q and q' are parameters co-varying according to equation (1), by multiplication of the potential ¢x by p'(B l1r)jp(B l1r). From Theorem 1, we have that y = ax + (3. The expressions for a and (3 now follow from simple mathematical manipulation. D\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 321\nTheorem 4 Let the junction tree T be as before. Also, let y = p(afe) and x = p(bif1r) be as before. Suppose that, in T, an inward propagation has been performed towards a clique including the variable of interest A. Then, p(e)(x) = \"(X + 8 with\n\"(= eta + a�a and 8 = f3a + f3�a1 (10) where aa, f3a and a�a, f3�a are as in equation (9), ob tained from outward propagations with the evidence A = a and A -::/:- a, respectively.\nProof: We begin by observing that p(e)(x) = p(a,e)(x) + p(•a,e)(x). By entering the evidence A = a in a clique H containing the variable A and propagating outwards, we obtain the potential ¢K = p(K, a, e) for clique K. From this potential, Y1. = p(a, e) = LK cPK is readily computed, as de scribed in equations (7) from Theorem 3. Similarly, by entering the evidence that A does not have the value a (that is, by multiplying the clique potential for H with a vector over dom(A), in which the en try corresponding to state a is zero and all other en tries equal 1) and propagating outwards, we obtain the potential ¢� = p(K, •a, e). From this poten tial, Y�a = p(•a,e) = LK ¢� is readily computed. Using equation (8) from Theorem 3, we get y� and Y�a· Now, using equation (9), we find eta, a�a, f3a, and f3�a· Inserting these coefficients into the expres sion p( e) (x) = p(a, e) (x) + p( •a, e) (x) yields the result stated in the theorem. D\nOur alternative method, building upon Theorems 3 and 4, is composed of the following steps:\n1. Enter the evidence e into the junction tree and perform an inward propagation towards a clique H containing the variable of interest A.\n2. Perform an outward propagation from H with the additional evidence A = a.\n3. Compute y;,_ and y�, using the equations (7) and (8) from Theorem 3.\n4. Compute the coefficients a = a a and f3 = f3a, using (9), for all relevant parameters, locally per clique.\n5. Retract the evidence A = a without retracting the evidence e.\n6. Perform an outward propagation from H with the additional evidence A -::/:- a.\n7. Compute Y�a and Y�a' using the equations (7) and (8) from Theorem 3.\n8. Compute a�a and f3�a, using (9).\n9. Compute the coefficients 'Y and 8, using equation (10) from Theorem 4.\nTo allow for retracting the evidence A = a in Step 5 of our method without retracting e, fast-retraction prop agation (Cowell & Dawid 1992) is used in Step 2.\nComparing the computational costs of the two al ternative methods, we note that they both require one inward and two outward propagations. Consider ing the first method, we observe that Steps 2 and 4 are equally costly. The computation of the coeffi cients a and 'Y costs 2 ·Jdom(K)J/m operations, where m = fdom(pa(B))J; the computation of j3 and 8 costs 2fdom(K) I arithmetic operations. Thus, the addi tional cost of the first method is roughly in the or der of 2 to 3 times Jdom(K)f. The additional costly steps in the second method are Steps 3 and 7, both costing approximately 3 ·fdom(K)J operations. Thus, the additional cost of the second method is roughly 6 · fdom(K)f arithmetic operations. This rough com parison of the computational costs of the two methods only addresses the number of arithmetic operations in volved. The first method, however, has a much larger overhead in terms of computing indices in performing the various summations. Thus, depending on the im plementation, the two methods might very well have comparable performance.\n5 ANALYSIS OF ALL OUTPUTS WRT. ONE PARAMETER\nHaving identified a parameter x to which an output probability of a Bayesian network is particularly sensi tive, one might be interested in establishing sensitivity functions for all possible posterior marginals in terms of this parameter. Such an analysis amounts to com puting the coefficients in these functions. Note that while the method described in Section 4 provides for evaluating the overall robustness of a Bayesian net work, the method described in this section is provides for getting insight in the spread of influence from sep arate parameters.\nThe method of Castillo et a!. ( 1 997) and the method of Coupe & van der Gaag (1998) can be exploited for es tablishing the coefficients in the sensitivity functions for all possible output probabilities, requiring three propagations per posterior marginal. Building upon the ideas put forward in the previous section, how ever, a more efficient method is obtained. Theorem 5 provides the basis for our method.\nTheorem 5 Let the junction tree T be as before. Also, let y = p(afe) and x = p(bl1r) be as be fore. Suppose that, in T, an inward propagation has\n322 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\nbeen performed towards a clique containing the vari able B to which the parameter x pertains. Then, p(e)(x) =\"(X+ J with\n'Y = O:a + O:�a and J = f3a + f3�a, (11) where O:a,f3a and a.�a,f3�a are as in equation (9), ob tained from two outward propagations with two dis tinct values for x.\nProof: Let K be a clique containing both the variable B and its set of parents II = pa(B). Let x1 and x2 denote two different values of the parameter x. From the outward propagation using x1 from clique K, we obtain the probability vector p(A,e)(x1) = (y�,Y�a) through marginalization of the clique potential for a clique H containing A. Similarly, from the outward propagation with x2, we find the vector p'(A, e)(x2) = (y�, Y�a)· From\n=\nwe get the result stated in the theorem. 0\nTheorem 5 provides the basis for our method for com puting the coefficients in the functions expressing all possible output probabilities y = p( a I e) in a single pa rameter x = p(blrr). The method is composed of the following steps:\n1. Enter the evidence e into the junction tree and perform an inward and an outward propagation using an arbitrary root clique.\n2. Compute the probability vector p(A, e) (y�, Y�a) through marginalization of the clique po tential for a clique H containing A, for all vari ables of interest.\n3. Change the value of parameter x and perform an outward propagation from a clique containing both the variable B and its parents.\n4. Compute the probability vector p'(A, e) = (y�, Y�a) through marginalization of the potential for clique H, for all variables of interest.\n5. Compute a.a, a.�a, f3a, and f3�a, using the equation (9) from Theorem 3.\n6. Compute 'Y and J, using the equation (11).\nWe would like to note that our method requires just one inward and two outward propagations to estab lish all sensitivity functions for a specific probability parameter.\n6 n-WAY SENSITIVITY ANALYSIS\nSo far, we have addressed one-way sensitivity analy ses only, in which the effects of separate parameters are studied. In this section, we turn our attention to more general n-way analyses in which the effects of simultaneous variation of n parameters are studied.\nOne can regard n-way sensitivity analysis as involving analyses of joint effects for all subsets of size n or less of all, say m, relevant parameters. Using the method of Castillo et al. (1997), this would involve L:::�=l ( 7) separate analyses and I::�= I ri ( 7) probability propa gations to compute the 2n coefficients, assuming r-ary variables.\nProvided the n parameters all belong to the same clique, Theorem 6 below states that we only need one propagation to compute the 2n coefficients, but I::�= I ( 7) local computations involving marginaliza tions of clique potentials.\nIn essence, Theorem 1 states that the mathematical expression for a probability, p(e)(x), of a vector of in stantiations, e, as a function of a probability parame ter, x, takes the form of a linear function of x. The orem 6 generalizes this statement to the case with n parameters, x1, . . . , Xn, and states that the resulting function is a multilinear function in x1, ... , Xn.\nTo simplify the exposition, we shall assume that the parameters are independent; that is, for each pair of parameters, x; = p(bx; lrrx;) and Xj = p(bx; lrrx;), with the associated variables Bx;, IIx;, Bx;, and IIx;, it holds true that rrx; # rrx;, Bx; (j. IIx;, and Bx; (j. IIx;. To generalize the theorem to cover the case of dependent parameters is fairly straightforward.\nTheorem 6 Let p be the probability function for a Bayesian network, where evidence e has been prop agated in a junction tree, T, for the network. Let X = { x1, . . . , Xn} be a set of parameters of the net work, where, for each i = 1, . . . ,n,\nX;= p(bx; lrrx;),\nwith the associated variables, Bx; and IIx;, being members of a clique, C, in T. Then\np(e)(X) = L 'Yx(Z) II z + L 1/Jc, z�x zEZ C:ll#1r\nwhere II = {IIx,, ... , IIxn }, rr = (rrx,, ... , rrxn ), and\nwhere\nfx(Y)\n'Yx(Z) = ( -1)1X\\ZI L !x(Y), y�z\n( -1)1X\\YI\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 323\nwhere¢>c = p(C,e), W = X\\Y = {w1, • • • ,wk}, k = !W I , by= (byp···,byiY1), b� = (b:n,, ... ,b:n1w1), and Px, denotes the initial value of Xi.\nProof: Using the same procedure as in the proof of Theorem 1 we get\np(e)(X)\n(� ¢>c) (X) ( F � c,o�, �·�. /c + c�/c) (X)\n= L · · · LP(b�1 !1l\"x,)(xr) · · · p(b�n !1l\"xn )(xn) b' b' zt Zn\nL c·b' b' ¢>c . zl , ... , :l!n ,1f L p(b�, 17l\"x,) ... p(b�n 11l\"xJ + C:0¥71\" ¢>c.\nThe multiple sum can be grouped into sums over the subsets Z � X such that b� = bz for each z E Z:\np(e)(X) L:: li z L:: ...\np(b�, !1ru,)(ur) · . ·p(b�1u111l\"uiui)(uiUI)\nL c:bz,b;_,,11\" ¢>c\nflzEZ p(bz !1r z) fluEU p(b� i1r u) +\nL ¢>c, (12) C:07\"'7r\nwhere U = X\\ Z = {u1, ... ,uiUI}. Now, expand ing the terms p(b� 17ru)(u), u E U, using (1), an easy calculation yields\np(b�, l1r u,) ( ur) · · · p(b�1u1 l1r u1u1) ( uiUI)\nII ( p(b� 17ru) p(b� 17ru) ) = U 1 -p(bul1ru) -U 1-p(bul1ru) uE\n= II 1 �(b�� 1l\"u; (L (-1)ISI II x). (13) uEU p( u I u) sr::;u xES\nInserting (13) in (12) and rearranging terms yields\np(e)(X) = L IT z L ( -1)ISI IT x Z<;;X zEZ S<;;U xES\nIf, instead of summing over subsets S � X \\ Z, we sum over the subsets of Z and takes care that the signs of the terms are preserved, we get the desired result. D\nNote that, since the computation of /X (X) ranges over all subsets of X, rx(Z) can be computed, for each Z C X, as a sum over a subset of the terms involved in the computation of 'Yx(X)\n.\nThe result presented in Theorem 6 is limited in the sense that all parameters under investigation must belong to the same clique in the junction tree. We shall now present a more general method which uti lizes the results obtained by lower-order analyses. Let X = { x1, • • . , Xn } be the n parameters under investi gation and write\np(e)(X) = L r(Z) II z + J. (15) Z<;;X zEZ\nThrough one-way analysis involving the parameter x we obtain the constants ax and f3x in\np(e)(x) = axx + f3x·\nThe 2n constants in (15) are related to ax and f3x in the way that ax equals the sum of all coefficients, r(Z), for which x E Z, and f3x equals the sum of the remaining coefficients. That is,\nand\nax= L r(Z) Z<;;X:xEZ\nf3x = L r(Z) + J. Z<;;X:x{lZ\n(16)\n(17)\nThus, for each one-way analysis of a parameter Xi, i = 1, ... , n, we obtain 2 equations of the form (16) and (17). In addition, we obtain the equation\np(e) = L r(Z) II Zo + J, (18) Z<;;X zEZ\nwhere z0 denote the value specified for parameter z in the Bayesian network. This gives us a total of 2n + 1 equations. However, we need (at least) 2n equations to compute the 2n coefficients.\nNow, if for each parameter, z E Z, we assign a new value and perform a full propagation, then we obtain an additional 2n + 1 equations. Thus, we can obtain at least 2n equations by performing l2n2: 1J full propagations with different parameter values, in addition to the initial propagation performed for the\n324 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\none-way analyses. For example, to perform 4-way analyses, a total of two full propagations is sufficient.\nThis result can be generalized very easily, since each m-way analysis gives rise to 2m equations of the form (16) and (17). Thus, to perform n-way analyses, where\nn > m, we need at most\nadditional propagations, as there are ( ;:, ) relevant m way analyses. So, for example, if we have performed 2-way analyses and want to perform 5-way analyses, no further propagations are needed.\n7 CONC LUDING REMARKS\nWe have presented methods for sensitivity analysis of Bayesian networks which are significantly more ef ficient than current methods. In the case of one way analysis, the number of probability propagations of current methods grows linearly in the number of relevant parameters, whereas the methods presented above only requires one inward and one or two out ward propagations, no matter the number of relevant parameters.\nTo substantiate the importance of this difference, we have investigated three real-world networks to get an idea of the typical number of relevant parameters in a realistic scenario. All three networks are from the medical domain: a subnetwork of Munin (Andreassen et a!. 1989) containing 1003 variables, a network mod elling the pathophysiology of ventricular septal defect (Coupe et al. 1999) containing 38 variables, and a net work related to disorders in the oesophagus containing 70 variables. The investigation were conducted using real patient data involving, respectively, 15, 5, and 3 patients. The average number of relevant parameters were found to be 16313, 738, and 992, respectively. (Since no censoring of parameters representing func tional relationships were performed on parameters for the Munin network, the figure 16313 is probably some what overestimated.)\nEfficient methods for sensitivity analysis play an im portant role in both the knowledge acquisition and the validation phases for manually constructed Bayesian network models.\nCoupe et al. (1999) reports on an empirical study using sensitivity analysis to focus attention on the most in fluential parameters in the knowledge acquisition pro cess, thereby considerably reducing the time required to acquire the parameter values.\nThe validation phase involves two aspects: fine-tuning and robustness analysis. The fine-tuning aspect in-\nvolves adjustment of the parameter values to make the network respond correctly to a number of test cases. A gradient descent approach is useful for that purpose ( cf. neural network type training), where the gradi ent of a posterior marginal with respect to a subset of parameters can easily be computed through a minor modification of the algorithms for sensitivity analy sis. Based on the work described in the present paper, Jensen (1999) has suggested a method for gradient de scent training of Bayesian networks.\nOnce a network has been fine-tuned, and thus responds correctly on a selection of test cases, the robustness of the network may be investigated. This involves, in essence, determining lower and upper bounds for parameter values for which the output of the network still agrees with the test cases. A parameter value close to one of the bounds indicate a possible lack of robustness. Given analytic expressions for the outputs in terms of the parameters, derived by e.g. methods described in the present paper, these bounds are easily determined.\nThe time complexity of n-way sensitivity analysis may be fairly high for large n, even with the methods pre sented in this paper. Also, our method assumes that the variables and the parents associated with then pa rameters reside in the same clique in a junction tree. The method of Coupe et al. (2000) for n-way sensitiv ity analysis is based on propagation of tables of coeffi cients in a junction tree, and, therefore, has a (poten tially very much) larger space requirement. However, their method is general in the sense that it does not put any restrictions on the location of the parameters.\nAcknowledgements During the initial phase of the work, the authors received valuable comments from Finn V. Jensen. Also, he suggested the basis for the general method for n-way analysis, described at the end of Section 6.\nThe research has been partly funded by the Danish National Centre for IT research, Project no. 87.2.\nReferences\nAndreassen, S., Jensen, F. V., Andersen, S. K., Falck, B., Kj<Brulff, U., Woldbye, M., S0rensen, A. R., Rosenfalck, A. & Jensen, F. (1989). MUNIN - an expert EMG assistant, in J. E. Desmedt (ed.), Computer-Aided Electromyography and Ex pert Systems, Elsevier Science Publishers B. V. (North-Holland), Amsterdam, chapter 21.\nCastillo, E., Gutierrez, J. M. & Hadi, A. S. (1996). A new method for efficient symbolic propagation in discrete Bayesian networks, Networks 28: 31-43.\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 325\nCastillo, E., Gutierrez, J. M. & Hadi, A. S. (1997). Sensitivity analysis in discrete Bayesian networks, IEEE Transactions on Systems, Man, and Cyber netics 27( 4): 412-423.\nCoupe, V., Jensen, F. V., Kj<Erulff, U. & van der Gaag, L. C. (2000). Efficient n-way sensitivity analysis of Bayesian networks, Technical report, Depart ment of Computer Science, University of Utrecht, The Netherlands.\nCoupe, V. M. H., Peek, N., Ottenkamp, J. & Habbema, J. D. F. (1999). Using sensitivity anal ysis for efficient quantification of a belief network, Artificial Intelligence in Medicine 17: 223-247.\nCoupe, V. M. H. & van der Gaag, L. C. (1998). Prac ticable sensitivity analysis of Bayesian belief net works, in M. Huskova, P. Lachout & J. A. Visek (eds) , Proceedings of the Joint Session of the 6th Prague Symposium of Asymptotic Statistics and the 13th Prague Conference on Information The ory, Statistical Decision Functions and Random Processes, Union of Czech Mathematicians and Physicists, pp. 81-86.\nCowell, R. G. & Dawid, A. P. (1992). Fast retrac tion of evidence in a probabilistic expert system, Statistics and Computing 2: 37-40.\nGeiger, D., Verma, T. & Pearl, J. (1990). Identify ing independence in Bayesian networks, Networks 20(5): 507-534. Special Issue on Influence Dia grams.\nHenrion, M., Pradhan, M., del Favero, B., Huang, K., Provan, G. & O'Rorke, P. (1996). Why is diag nosis using belief networks insensitive to impreci sion in probabilities?, Proceedings of the Twelfth Conference on Uncertainty in Artificial Intelli gence, Morgan Kaufmann Publishers, San Fran cisco, California, pp. 307-314.\nJensen, F. V. (1999). Gradient descent training of Bayesian networks, Proceedings of the Fifth Eu ropean Conference on Symbolic and Quantitative Approaches to Reasoning and Uncertainty, Lon don, England.\nLaskey, K. B. (1995). Sensitivity analysis for prob ability assessments in Bayesian networks, IEEE Transactions on Systems, Man, and Cybernetics 25: 901-909.\nMorgan, M. & Henrion, M. (1990). Uncertainty, a Guide to Dealing with Uncertainty in Quantita tive Risk and Policy Analysis, Cambridge Uni versity Press, Cambridge.\nPradhan, M., Henrion, M., Provan, G., Favero, B. D. & Huang, K. (1996). The sensitivity of belief net works to imprecise probabilities: an experimental investigation, Artificial Intelligence 85: 363-397."
    } ],
    "references" : [ {
      "title" : "Computer-Aided Electromyography and Ex­",
      "author" : [ "S. Andreassen", "F.V. Jensen", "S.K. Andersen", "B. Falck", "U. Kj<Brulff", "M. Woldbye", "A.R. S0rensen", "A. Rosenfalck", "F. Jensen" ],
      "venue" : "MUNIN - an expert EMG assistant,",
      "citeRegEx" : "Andreassen et al\\.,? \\Q1989\\E",
      "shortCiteRegEx" : "Andreassen et al\\.",
      "year" : 1989
    }, {
      "title" : "A new method for efficient symbolic propagation in discrete Bayesian networks, Networks",
      "author" : [ "E. Castillo", "J.M. Gutierrez", "A.S. Hadi" ],
      "venue" : null,
      "citeRegEx" : "Castillo et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Castillo et al\\.",
      "year" : 1996
    }, {
      "title" : "Sensitivity analysis in discrete Bayesian networks",
      "author" : [ "E. Castillo", "J.M. Gutierrez", "A.S. Hadi" ],
      "venue" : "IEEE Transactions on Systems,",
      "citeRegEx" : "Castillo et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Castillo et al\\.",
      "year" : 1997
    }, {
      "title" : "Efficient n-way sensitivity analysis of Bayesian networks, Technical report, Depart­ ment of Computer Science, University of Utrecht, The Netherlands",
      "author" : [ "V. Coupe", "F.V. Jensen", "U. Kj<Erulff", "L.C. van der Gaag" ],
      "venue" : null,
      "citeRegEx" : "Coupe et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Coupe et al\\.",
      "year" : 2000
    }, {
      "title" : "Using sensitivity anal­ ysis for efficient quantification of a belief network",
      "author" : [ "V.M.H. Coupe", "N. Peek", "J. Ottenkamp", "J.D.F. Habbema" ],
      "venue" : "Artificial Intelligence in Medicine",
      "citeRegEx" : "Coupe et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Coupe et al\\.",
      "year" : 1999
    }, {
      "title" : "Prac­ ticable sensitivity analysis of Bayesian belief net­",
      "author" : [ "V.M.H. Coupe", "L.C. van der Gaag" ],
      "venue" : "Proceedings of the Joint Session of the 6th",
      "citeRegEx" : "Coupe and Gaag,? \\Q1998\\E",
      "shortCiteRegEx" : "Coupe and Gaag",
      "year" : 1998
    }, {
      "title" : "Fast retrac­ tion of evidence in a probabilistic expert system, Statistics and Computing",
      "author" : [ "R.G. Cowell", "A.P. Dawid" ],
      "venue" : null,
      "citeRegEx" : "Cowell and Dawid,? \\Q1992\\E",
      "shortCiteRegEx" : "Cowell and Dawid",
      "year" : 1992
    }, {
      "title" : "Identify­ ing independence in Bayesian networks, Networks",
      "author" : [ "D. Geiger", "T. Verma", "J. Pearl" ],
      "venue" : "Special Issue on Influence Dia­ grams",
      "citeRegEx" : "Geiger et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "Geiger et al\\.",
      "year" : 1990
    }, {
      "title" : "Why is diag­ nosis using belief networks insensitive to impreci­ sion in probabilities",
      "author" : [ "M. Henrion", "M. Pradhan", "B. del Favero", "K. Huang", "G. Provan", "P. O'Rorke" ],
      "venue" : "Proceedings of the Twelfth",
      "citeRegEx" : "Henrion et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Henrion et al\\.",
      "year" : 1996
    }, {
      "title" : "Gradient descent training of Bayesian networks, Proceedings of the Fifth Eu­",
      "author" : [ "F.V. Jensen" ],
      "venue" : null,
      "citeRegEx" : "Jensen,? \\Q1999\\E",
      "shortCiteRegEx" : "Jensen",
      "year" : 1999
    }, {
      "title" : "Sensitivity analysis for prob­ ability assessments",
      "author" : [ "K.B. Laskey" ],
      "venue" : null,
      "citeRegEx" : "Laskey,? \\Q1995\\E",
      "shortCiteRegEx" : "Laskey",
      "year" : 1995
    }, {
      "title" : "The sensitivity of belief net­ works to imprecise probabilities: an experimental investigation",
      "author" : [ "M. Pradhan", "M. Henrion", "G. Provan", "B.D. Favero", "K. Huang" ],
      "venue" : "Artificial Intelligence",
      "citeRegEx" : "Pradhan et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Pradhan et al\\.",
      "year" : 1996
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "The function expressing the posterior marginal is a quo­ tient of two linear functions in the parameter, as has been shown by Castillo et al. (1996). Building upon this property, it suffices to establish the coefficients in this function to determine the effect of parameter variation.",
      "startOffset" : 126,
      "endOffset" : 149
    }, {
      "referenceID" : 1,
      "context" : "The function expressing the posterior marginal is a quo­ tient of two linear functions in the parameter, as has been shown by Castillo et al. (1996). Building upon this property, it suffices to establish the coefficients in this function to determine the effect of parameter variation. Castillo et al. (1997) and Coupe & van der Gaag (1998) have designed methods to this end.",
      "startOffset" : 126,
      "endOffset" : 309
    }, {
      "referenceID" : 1,
      "context" : "The function expressing the posterior marginal is a quo­ tient of two linear functions in the parameter, as has been shown by Castillo et al. (1996). Building upon this property, it suffices to establish the coefficients in this function to determine the effect of parameter variation. Castillo et al. (1997) and Coupe & van der Gaag (1998) have designed methods to this end.",
      "startOffset" : 126,
      "endOffset" : 341
    }, {
      "referenceID" : 3,
      "context" : "In fact, Coupe et al. (1999) have reported high sensitivities in an emprical study in the medical domain, involving real patient data.",
      "startOffset" : 9,
      "endOffset" : 29
    }, {
      "referenceID" : 1,
      "context" : "Using the method of Castillo et al. (1997), this would involve L:::�=l ( 7) separate analyses and I::�= I ri ( 7) probability propa­ gations to compute the 2n coefficients, assuming r-ary variables.",
      "startOffset" : 20,
      "endOffset" : 43
    }, {
      "referenceID" : 4,
      "context" : "1989) containing 1003 variables, a network mod­ elling the pathophysiology of ventricular septal defect (Coupe et al. 1999) containing 38 variables, and a net­ work related to disorders in the oesophagus containing 70 variables.",
      "startOffset" : 104,
      "endOffset" : 123
    }, {
      "referenceID" : 9,
      "context" : "Based on the work described in the present paper, Jensen (1999) has suggested a method for gradient de­ scent training of Bayesian networks.",
      "startOffset" : 50,
      "endOffset" : 64
    }, {
      "referenceID" : 3,
      "context" : "The method of Coupe et al. (2000) for n-way sensitiv­ ity analysis is based on propagation of tables of coeffi­ cients in a junction tree, and, therefore, has a (poten­ tially very much) larger space requirement.",
      "startOffset" : 14,
      "endOffset" : 34
    } ],
    "year" : 2011,
    "abstractText" : "To investigate the robustness of the output probabilities of a Bayesian network, a sensi­ tivity analysis can be performed. A one-way sensitivity analysis establishes, for each of the probability parameters of a network, a func­ tion expressing a posterior marginal proba­ bility of interest in terms of the parameter. Current methods for computing the coeffi­ cients in such a function rely on a large num­ ber of network evaluations. In this paper, we present a method that requires just a single outward propagation in a junction tree for es­ tablishing the coefficients in the functions for all possible parameters; in addition, an in­ ward propagation is required for processing evidence. Conversely, the method requires a single outward propagation for computing the coefficients in the functions expressing all possible posterior marginals in terms of a sin­ gle parameter. We extend these results to an n-way sensitivity analysis in which sets of pa­ rameters are studied.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}