{
  "name" : "1511.06437.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A CONVNET FOR NON-MAXIMUM SUPPRESSION",
    "authors" : [ "Jan Hosang", "Rodrigo Benenson" ],
    "emails" : [ "schiele}@mpi-inf.mpg.de" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Non-maximum suppression (NMS) is used in virtually all state-of-the-art object detection pipelines. While essential object detection ingredients such as features, classifiers, and proposal methods have been extensively researched surprisingly little work has aimed to systematically address NMS. The de-facto standard for NMS is based on greedy clustering with a fixed distance threshold, which forces to trade-off recall versus precision. We propose a convnet designed to perform NMS of a given set of detections. We report experiments on a synthetic setup, and results on crowded pedestrian detection scenes. Our approach overcomes the intrinsic limitations of greedy NMS, obtaining better recall and precision."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "The bulk of current object detection pipelines are based on three steps: 1) propose a set of windows (either via sliding window, or object proposals), 2) score each window via a properly trained classifier, 3) remove overlapping detections (non-maximum suppression). The popular DPM (Felzenszwalb et al., 2010) and R-CNN family (Girshick et al., 2014; Girshick, 2015; Ren et al., 2015) follow this approach. Both object proposals (Hosang et al., 2015) and detection classifiers (Russakovsky et al., 2015) have received enormous attention, while non-maximum suppression (NMS) has been seldom addressed. The de-facto standard for NMS consists of greedily merging the higher scoring windows with lower scoring windows if they overlap enough (e.g. intersection-over-union IoU > 0.5), which we call GreedyNMS in the following.\nGreedyNMS is popular because it is conceptually simple, fast, and for most tasks results in satisfactory detection quality. Despite its popularity, GreedyNMS has important shortcomings. As illustrated in figure 1 (and also shown experimentally in section 4) GreedyNMS trades off precision vs. recall. If the IoU threshold is too large (too strict) then not enough surrounding detections are suppressed, high scoring false positives are introduced and precision suffers; if the IoU threshold is too low (too loose) then multiple true positives are merged together and the recall suffers (rightmost case in figure 1). For any IoU threshold, GreedyNMS is sacrificing precision or recall. One can do better than this by leveraging the full signal of the score map (statistics of the surrounding detections) rather than blindly applying a fixed policy everywhere in the image.\nCurrent object detectors are becoming surprisingly effective on both general (e.g. Pascal VOC, MS COCO) and specific objects detection (e.g pedestrians, faces). The oracle analyses for “perfect NMS” from Hosang et al. (2015, table 5) and Parikh & Zitnick (2011, figure 12) both indicate that NMS accounts for almost a quarter of the remaining detection problem.\nGreedyNMS is, strictly speaking, not a “non-maximum suppression” method since it focuses on removing overlapping detections, ignoring if a detection is a local maximum or not. To improve detections, we would like to prune false positives (and keep true positives). Instead of doing hard pruning decisions, we design our network to make soft decisions by re-scoring (re-ranking) the input detection windows. Our re-scoring is final, and no post-processing is done afterwards, thus the resulting score maps must be very “peaky”. We call our proposed network “Tyrolean network”1, abbreviated Tnet.\n1Tyrolean because “it likes to see peaky score maps”.\nar X\niv :1\n51 1.\n06 43\n7v 3\n[ cs\n.C V\n] 8\nJ an\n2 01\n6\nContribution We are the first to show that a convnet can be trained and used to overcome the limitations of GreedyNMS. Our experiments demonstrate that, across different occlusion levels, the Tyrolean network (Tnet) performs strictly better than GreedyNMS at any IoU threshold.\nAs an interesting scenario for NMS, we report results over crowded pedestrian scenes. Our Tnet is focused on the NMS task, it does not directly access image content (thus does not act as “second detector”), does not use external training data, and provides better results than auto-context (Tu & Bai, 2010)."
    }, {
      "heading" : "1.1 RELATED WORK",
      "text" : "Despite being a core ingredient of detection, non-maximum suppression has received little attention compared to feature design, classifier design, and object proposals.\nClustering detections The decade old greedy NMS (GreedyNMS) is used in popular detectors such as V&J (Viola & Jones, 2004), DPM (Felzenszwalb et al., 2010), and is still used in the state-ofthe-art R-CNN detector family (Girshick et al., 2014; Girshick, 2015; Ren et al., 2015). Alternatives such as mean-shift clustering (Dalal & Triggs, 2005; Wojek et al., 2008), agglomerative clustering (Bourdev et al., 2010), and heuristic variants (Sermanet et al., 2014) have been considered, but they have yet to show consistent gains. Recently Tang et al. (2015); Rothe et al. (2015) proposed principled clustering approaches that provide globally optimal solutions, however the results reached are on par, but do not surpass, GreedyNMS.\nLinking detections to pixels The Hough voting framework enables more sophisticated reasoning amongst conflicting detections by linking the detections to local image evidence (Leibe et al., 2008; Barinova et al., 2012; Wohlhart et al., 2012). Hough voting itself, however, provides low detection accuracy. Yao et al. (2012) and Dai et al. (2015) refine detections by linking them with semantic labelling; while Yan et al. (2015) side-steps NMS all-together by defining the detection task directly as a labelling problem. These approaches arguably propose a sound formulation of the detection problem, however they rely on semantic labelling/image segmentation. Our system can operate directly on bounding box detections.\nCo-occurrence To better handle dense crowds or common object pairs, it has been proposed to use specialized 2-object detectors (Sadeghi & Farhadi, 2011; Tang et al., 2012; Ouyang & Wang, 2013), which then require a more careful NMS strategy to merge single-object with double-object detections. Similarly Rodriguez et al. (2011) proposed to adapt the NMS threshold using crowd density estimation. Our approach is directly learning based (no hand-crafted 2-objects or density estimators), and does not use additional image information. Desai et al. (2011) considered improving the NMS procedure by considering the spatial arrangement between detections of different classes. The feature for spatial arrangement are fully hand-crafted, while we let a convnet learn the spatial arrangement patterns. We focus here on the single class case (albeit our approach could be extended to handle multiple classes).\nSpeed Neubeck & Van Gool (2006) discuss how to improve NMS speed, without aiming to improve quality. Here we aim at improving quality, while still having practical speeds.\nAuto-context uses the local (Tu & Bai, 2010; Chen et al., 2013) or global information (Vezhnevets & Ferrari, 2015) on the image to re-score detections. Albeit such approaches do improve detection quality, they still require a final NMS processing step. Our convnet does re-score detections, but at the same time outputs a score map that does not require further processing. We provide experiments (section 4) that show improved performance over auto-context.\nConvnets and NMS Recently a few works have linked convnets and NMS. Detection convnets are commonly trained unaware of the NMS post-processing step; Wan et al. (2015) connected NMS with the loss to train the detection convnet, making the training truly end-to-end. The used NMS is greedy and with fixed parameters. Stewart & Andriluka (2015) propose to use an LSTM to decide how many detections should be considered in a local region. The detections amongst the regions are then merged via traditional NMS. In contrast our convnet runs in a fully convolutional mode and requires no post-processing. Both Wan et al. (2015) and Stewart & Andriluka (2015) served as inspiration for the design of our training loss. To the best of our knowledge our Tnet is the first network explicitly designed to replace the final NMS stage. This work focuses on NMS itself, our proposed network is independent of the detections source (it can be from a convnet or not). We report experiment applied over DPM (Felzenszwalb et al., 2010) detections."
    }, {
      "heading" : "2 BASE TYROLEAN NETWORK",
      "text" : "The main intuition behind our proposed Tyrolean network (Tnet) is that the score map of a detector together with a map that represents the overlap between neighbouring hypotheses contains valuable information to perform better NMS than GreedyNMS (also see figure 1). Thus, our network is a traditional convnet but with access to two slightly unusual inputs (described below), namely score map information and IoU maps. Figure 2 shows the overall network. The parameter range (number of layers, number of units per layer) is inspired by AlexNet (Krizhevsky et al., 2012) and VGG (Simonyan & Zisserman, 2015). In our base Tnet the first stage applies 512 11×11 filters over each input layer, and 512 1 × 1 filters are applied on layers 2, 3, and 4. ReLU non-linearities are used after each layer but the last one.\nThe network is trained and tested in a fully convolutional fashion. It uses the same information as GreedyNMS, and does not access the image pixels directly. The required training data are only a set of object detections (before NMS), and the ground truth bounding boxes of the dataset.\nInput grid As preprocessing all detections in an image are mapped into a 2d grid (based on their centre location). If more than one detection falls into the same cell, we keep only the highest scoring detection. Each cell in the grid is associated with a detection bounding box and score. We use\ncells of 4 × 4 pixels, thus an input image of size W × H will be mapped to input layers of size w × h = W/4 × H/4. Since the cells are small, mapping detections to the input grid has minimal impact on the NMS procedure. In preliminary experiments we validated that: a) we can at least recover the performance of GreedyNMS (applying GreedyNMS over the input grid provides the same results as directly applying GreedyNMS), b) the detection recall stays the same (after mapping to the input grid the overall recall is essentially identical to the raw detections). The current incarnation of Tnet can handle mild changes in scale amongst neighbouring detections. We report experiments with detections over a 3× scale range (most boxes have [50, 150] pixel height). The overall approach can handle a wider range of scale differences with simple modifications (making the grid 3d, i.e. x-y-scale), which is left for future work.\nIoU layer In order to reason about neighbouring detection boxes (or segments) we feed Tnet with IoU values. For each location we consider a 11×11 = 121 neighbourhood, thus the input IoU layer has w× h× 121 values. Together the cell size and neighbourhood size should provide the Tnet with sufficient information about surroundings of a detection, where this choice depends on the object sizes in the image and the expected object density and thus are application dependent.\nScore maps layer To reason about the detection confidence, we feed Tnet with the raw detection score map (once mapped to the input grid). The NMS task involves ranking operations which are not easily computed by linear and relu (max(·, 0)) operators. To ease the task we also feed the Tnet with score maps resulting from GreedyNMS at multiple IoU thresholds. All score maps are stacked as a multi-channel input image and feed into the network. S(τ) denotes a score map resulting from applying GreedyNMS with IoU≥ τ , S(τ1, τ2) denotes a two channels map (S(τ1) and S(τ2) stacked). Note that S(1) returns the raw detection score map. Our base Tnet uses S(1, 0.3) which has dimensionality w × h × 2 (see figure 2). The convolutional filters applied over the score maps input have the same size as the IoU layer neighbourhood (11× 11 cells). Tnet is then responsible for interpreting the multiple score maps and the IoU layer, and make the best local decision. Our Tnet operates in a fully feed-forward convolutional manner. Each location is visited only once, and the decision is final. In other words, for each location the Tnet has to decide if a particular detection score corresponds to a correct detection or will be suppressed by a neighbouring detection in a single feed-forward path.\nParameter rules of thumb Figure 2 indicates the base parameters used. Preliminary experiments indicated that removing some of the top layers has a clear negative impact on the network performance, while the width of these layers is not that important (512, 1 024, 2 048 filters in layers 2,3, and 4 shows a slow increase in quality). Having a high enough resolution in the input grid is critical, while keeping a small enough number of convolutions over the inputs allows to keep the number of model parameters under control. During training data augmentation is necessary to avoid overfitting. The training procedure is discussed in more detail in 2.1, while experimental results for some parameters variants are reported in section 4.\nInput variants In the experiments in the next sections we consider different input variants. The IoU layer values can be computed over bounding boxes (regressed by the sliding window detector), or over estimated instance segments (Pinheiro et al., 2015). Similarly, for the score maps we consider different numbers of GreedyNMS thresholds, which changes the dimensionality of the input score map layer. In all cases we expect the Tnet to improve over a fixed threshold GreedyNMS by discovering patterns in the detector score maps and IoU arrangements that enable to do adaptive NMS decisions."
    }, {
      "heading" : "2.1 TRAINING PROCEDURE",
      "text" : "Typically detectors are trained as classifiers on a set of positive and negative windows, determined by the IoU between detection and object annotation. When doing so the spatial relation between detector outputs and the annotations is being neglected and little work has addressed this.The DPM (Felzenszwalb et al., 2010; Pepik et al., 2015) includes structured output learning to ensure the detection score falls off linearly with the overlap between detector window and annotation. Wan et al. (2015) explicitly include a fixed NMS procedure into the network loss during training so the detector is tuned towards the NMS at test time. We adopt from Stewart & Andriluka (2015) the idea of computing the loss by matching detections to annotations, but instead of regressing bounding\nboxes at every location we predict new detection scores that are high for matched detections and low everywhere else. In contrast to the conventional wisdom of training the detector to have a smooth score decrease around positive instances, we declare a detection right next to a true positive to be a negative training sample as long as it is not matched to an annotation. We do this because our network must itself perform NMS.\nTraining loss Our goal is to reduce the score of all detections that belong to the same person, except exactly one of them. To that end, we match every annotation to all detections that overlap at least 0.5 IoU and choose the maximum scoring detection among them as the one positive training sample. All other detections are negative training examples. This yields a label yp for every location p in the input grid (see previous section). Since background detections are much more frequent than true positives, it is necessary to weight the loss terms to balance the two. We use the weighted logistic loss\nL(x) = 1∑\np∈G wyp ∑ p∈G wyp log ( 1 + e−ypf(xp) ) (1)\nwhere xp is the feature descriptor at position p, f(xp) is the output of the network at position p. The weights wyp are chosen so that both classes have the same weight either per frame or globally on the entire dataset (denoted by wf and wg respectively). Since we have a one-to-one correspondence between input grid cells and labels it is straight forward to train a fully convolutional network to minimize this loss.\nRelaxed loss It is impossible for the network to recover from certain mistakes that are already present in the input detections. For example, false positives on the background might be impossible to tell apart from true positives since the network does not have access to the image and only sees detection scores and overlaps between detections. On the other hand detections of distinct objects with high overlap can be hard to detect since the detections can assign low scores to barely visible objects. It proved beneficial to assign lower weight to these cases, which we call the relaxed loss. We declare negative samples to be hard if the corresponding detections are not suppressed by a 0.3 NMS and true positives to be hard if they are suppressed by a 0.3 NMS on the annotations with the matched detection scores. The weight of hard examples is decreased by a factor of r. Our base Tnet uses r = 1 (non-relaxed) with weighting strategy wf , and section 4 reports results for other r values and wg .\nTraining parameters The model is trained from scratch, randomly initialized with MSRA (He et al., 2015), and optimized with Adam (Kingma & Ba, 2015). We use a learning rate of 10−4, a weight decay of 5 · 10−5, a momentum of 0.9, and gradient clipping at 1 000. The model is trained for 100 000 iterations with one image per iteration. All experiments are implemented with the Caffe framework (Jia et al., 2014).\nAs pointed out in Mathias et al. (2014) the threshold for GreedyNMS requires to be carefully selected on the validation set of each task, the commonly used default IoU > 0.5 can severely underperform. Other NMS approaches such as (Tang et al., 2015; Rothe et al., 2015) also require training data to be adjusted. When maximizing performance in cluttered scenes is important, training a Tnet is thus not a particularly heavy burden. Currently, training our base Tnet on un-optimized CPU and GPU code takes a day."
    }, {
      "heading" : "3 CONTROLLED SETUP EXPERIMENTS",
      "text" : "NMS is usually the last stage of an entire detection pipeline. Therefore, in an initial set of experiments, we want to understand the problem independent of a specific detector and abstract away the particularities of a given dataset."
    }, {
      "heading" : "3.1 OMNIST DATASET",
      "text" : "If all objects appeared alone in the images, NMS would be trivial. The core issue for NMS is deciding if two local maxima in the detection score map correspond to only one object or to multiple ones. To investigate this core aspect we create the oMNIST (“overlapping MNIST”) toy dataset.\nThis data does not aim at being particularly realistic, but rather to enable a detailed analysis of the NMS problem.\nEach image is composed of one or two MNIST digits. To emphasise the occlusion cases, we sample 1/5 single digits, and 4/5 double digit cases. The digits are off-centre and when two digits are present they overlap with bounding box IoU ∈ [0.2, 0.6]. We also mimic a detector by generating synthetic score maps. Each ground truth digit location generates a perturbed bump with random magnitude in the range [1, 9], random x-y scaling, rotation, a small translation, and additive Gaussian noise. Albeit noisy, the detector is “ideal” since its detection score remains high despite strong occlusions. Figure 3 shows examples of the generated score maps and corresponding images. By design GreedyNMS will have difficulties handling such cases (at any IoU threshold).\nOther than score maps our convnet uses IoU information between neighbouring detections (like GreedyNMS). In our experiments we consider using the perfect segmentation masks for IoU (ideal case), noisy segmentation masks, and the sliding window bounding boxes.\nWe generate a training/test split of 100k/10k images, kept fix amongst different experiments."
    }, {
      "heading" : "3.2 RESULTS",
      "text" : "Results are summarised in table 1 and figure 5. We summarise the curves via AR; the average recall on the precision range [0.5, 1.0]. The evaluation is done using the standard Pascal VOC protocol, with IoU > 0.5 (Everingham et al., 2015).\nGreedyNMS As can be easily seen in figure 5 varying the IoU thresholds for GreedyNMS trades off precision and recall as discussed in section 1. The best AR that can be obtained with GreedyNMS is 60.2% for IoU > 0.3. Example score maps for this method can be found in figure 4.\nUpper bound As an upper bound for any method relying on score map information we can calculate the overlap between neighbouring hypotheses based on perfect segmentation masks available in this toy scenario. In that case even a simple strategy such as\nGreedyNMS can be used and based on our idealized but noisy detection score maps this results in 90.0% AR. In section 4 we report experiments using segmentation masks estimated from the image content that result in inferior performance however.\nBase Tnet Using the same information as GreedyNMS with bounding boxes, our base Tnet reaches better performance for the entire recall range (see figure 5 and table 1, S(1, 0.3) indicates the score maps from GreedyNMS with IoU > 0.3 and≥ 1, i.e. the raw score map). In this configuration Tnet obtains 79.5% AR clearly superior to GreedyNMS. This shows that, at least in a controlled setup,\na convnet can indeed exploit the available information to overcome the limitations of the popular GreedyNMS method.\nInstead of picking a specific IoU threshold to feed Tnet, we consider IoU&S(1, 0→ 0.6), which includes S(1, 0.6, 0.4, 0.3, 0.2, 0.0). As seen in figure 5, not selecting a specific threshold results in the best performance of 86.0% AR. As soon as some ranking signal is provided (via GreedyNMS results), our Tnet is able to learn how to exploit best the information available. Qualitative results are presented in figure 4.\nTable 1 reports results for a few degraded cases. If we remove GreedyNMS S(0.3) and only provide the raw score map (IoU&S(1, 0.3)) performance decreases somewhat.\nAuto-context Importantly we show that IoU&S(1) improves over S(1) only. (S(1) is the information exploited by auto-context methods, see §1.1). This shows that the convnet is learning to do more than simple auto-context. The detection improves not only by noticing patterns on the score map, but also on how the detection boxes overlap."
    }, {
      "heading" : "4 PERSON DETECTION EXPERIMENTS",
      "text" : "After the proof of concept for a controlled setup, we move to a realistic pedestrian detection setup. We are particularly interested in datasets that show diverse amounts of occlusion (and thus NMS is non-trivial). We decided for the PETS dataset, which exhibits diverse levels of occlusion and provides a reasonable volume of training and test data. Additionally we test the generalization of the trained model on the ParkingLot dataset.\nPETS We use 8 of the PETS sequences (Ferryman & Ellis, 2010), ∼200 frames each, that we split in 5 for training (S1L1-1, S1L1-2, S1L2-1, S1L2-2, S2L1, and S3MF1), 1 for validation (S2L3) and 1 for testing (S2L2). The different videos show diverse densities of crowds. As shown in figure 6 more than 40/50/25% of the train/val/test data has an IoU > 0.3 with another ground truth box. Since detectors tend to have non-zero detection scores in most areas of the image, the training volume is proportional to number of pixels not the number of images. Thus we can adequately train our Tnet with only a few hundred frames. PETS has been previously used to study person detection (Tang et al., 2013), tracking (Milan et al., 2014), and crowd density estimation (Subburaman et al., 2012). Standard pedestrian datasets such as Caltech (Dollár et al., 2012) or KITTI Geiger et al. (2012) average less than two pedestrian per frame, making close-by detections a rare occurrence.\nDue to its size and challenging occlusion statistics we consider PETS a suitable dataset to explore NMS. Figure 11 shows example frames.\nd\nParkingLot We use the first ParkingLot (Shu et al., 2012) sequence to evaluate the generalization capabilities of the model. We use an improved set annotations, provided every third frame (250 frames in total) and rectify the mistakes from the original annotations. Compared to PETS the sequence has similar overlap statistics than the PETS test set (see figure 6), but presents different background and motion patterns. Figure 12 shows examples from the dataset.\nPerson detector In this work we take the detector as a given. For our experiments we use the baseline DPM detector from (Tang et al., 2013). We are not aware of a detector (convnet or not) providing better results on PETSlike sequences (we considered some of the top detectors in (Dollár et al., 2012)). Importantly, for our exploration the detector quality is not important per-se. As discussed\nin section 3.1 GreedyNMS suffers from intrinsic issues, even when providing an idealized detector. In fact Tnet benefits from better detectors, since there will be more signal in the score maps, it becomes easier to do NMS. We thus consider our DPM detector a fair detection input. We use the DPM detections after bounding box regression, but before any NMS processing.\nPerson segments In section 4.1 we report results using segments estimated from the image content. We use our re-implementation of DeepMask (Pinheiro et al., 2015), trained on the Coco dataset (Lin et al., 2014). DeepMask is a network specifically designed for objects segmentation which provides competitive performance. Our re-implementation obtains results of comparable quality as the original; example results on PETS are provided in the appendix section A. We use DeepMask as a realistic example of what can be expected from modern techniques for instance segmentation."
    }, {
      "heading" : "4.1 PETS RESULTS",
      "text" : "Our PETS results are presented in table 2, figure 7 (validation set), figures 8, 8 (test set). Qualitative results are shown in figure 11.\nGreedyNMS on boxes Just like in the oMNIST case, the GreedyNMS curves in figure 7 have a recall versus precision trade-off. For PETS we pick IoU > 0.3 as a reference threshold which provides a reasonable balance.\nGreedyNMS on segments As discussed in section 3, GreedyNMS should behave best when the detection overlap is based on the visible area of the object. We compute DeepMask segmentations on the DPM detection, feed these in GreedyNMS, and select the best IoU threshold for the validation set. As seen in table 2 results are slightly under-performing the bounding boxes cases. Albeit many segments are rather accurate (see appendix section A), segments tend to be accurate on the easy cases for the detector, and drop in performance when heavier occlusion is present. Albeit in theory having good segmentations should make GreedyNMS quite better, in practice they do not. At the end, the segments hurt more than they help for GreedyNMS.\nAuto-context The entry S (1) in table 2 shows the case where only the raw detection score map is feed to Tnet (same nomenclature as section 3.2). Since performance is lower than other variants (e.g. IoU&S (1)), this shows that our approach is exploiting available information better than just doing auto-context over DPM detections.\nTnet Both in validation and test set (figures 7 and 8) our trained network with IoU&S (1, 0.3) input provides a clear improvement over vanilla GreedyNMS. Just like in the oMNIST case, the network is able to leverage patterns in the detector output to do better NMS than the de-facto standard GreedyNMS method. Table 2 report the results for a few additional variants. IoU&S(1, 0→ 0.6) shows that it is not necessary to select a specific IoU threshold for the input score map layer. Given an assortment (S(1, 0.6, 0.4, 0.3, 0.2, 0.0)) the network will learn to leverage the information available. Using the relaxed loss described in 2.1 helps further improve the results. Amongst the parameters tried on the validation set, r = 0.3 provides the largest improvement. Lower r values decrease performance, while higher r values converge towards the default r = 1 performance (base Tnet). Weighting classes equally on the entire dataset (wg strategy) gives a mild improvement from 57.9% to 58.0% AR compared to the default per frame weighting wf .\nStrong Tnet We combine the best ingredients identified on the validation set into one strong model. We use IoU&S(1, 0→0.6), relaxed loss with r = 0.3, and global weighting wg . Figure 8 shows that we further improve over the base Tnet from 59.5% to 71.8% AR on the PETS test set. The gap between base Tnet and GreedyNMS is smaller on the test set than on validation, because test set has lighter occlusions (see figure 6). Still our strong Tnet provides a consistent improvement over GreedyNMS. Figure 9 provides a more detailed view of the results from figure 8. It compares our strong Tnet result versus the upper envelope of GreedyNMS over all thresholds ([0, 1]), when evaluated over different\nsubsets of the test set. Each subset corresponds to ground truth bounding boxes with other boxes overlapping more than a given IoU level (see figure 6). For all ranges, our strong Tnet improves over GreedyNMS. This shows that our network does not fit to a particular range of occlusions, but learns to handle all of them with comparable effectiveness. At test time Tnet is reasonably fast, taking ∼200 milliseconds per frame (all included).\n4.2 PARKINGLOT RESULTS\nTo verify that our Tnet can generalize beyond PETS, we run the same DPM detector as on the PETS experiment over the ParkingLot sequence and do NMS using the networks trained on PETS training set only. Results in figure 10 show that Tnet improves from 80.3% to 83.3% AR over the best GreedyNMS threshold of IoU > 0.3. Even though Tnet was not trained on this sequence we see a similar result as on the PETS dataset. Not only does our Strong Tnet improve over the best GreedyNMS result, but it improves over the upper envelope of all GreedyNMS thresholds. See qualitative results in figure 12."
    }, {
      "heading" : "5 CONCLUSION",
      "text" : "We have discussed in detail the limitations of GreedyNMS and presented experimental examples showing its recall versus precision trade-off. For the sake of speed and simplicity GreedyNMS disregards most of the information available in the detector response. Our proposed Tyrolean network (Tnet) mines the patterns in the score map values and bounding box arrangements to surpass the performance of GreedyNMS. On the person detection task, our final results show that our approach provides, compared to any GreedyNMS threshold, both high recall and improved precision. These results confirm that Tnet overcomes the intrinsic limitations of GreedyNMS, while keeping practical test time speeds.\nAlbeit the proposed architecture results in good results for the scenario covered, there is certainly room for further probing the parameter space of convnets for NMS and exploring other applications domains (e.g. NMS for boundaries estimation, or other detection datasets). Explicitly handling detection scales, considering multi-class problems, or back-propagating into the detector itself are future directions of interest.\nCurrent detection pipelines start with a convnet and end with a hard-coded NMS procedure. For detection problems where occlusions are present, we reckon that significant improvements can be obtained by ending with a Tnet."
    }, {
      "heading" : "ACKNOWLEDGEMENTS",
      "text" : "We thank Siyu Tang and Anton Milan for providing the pre-trained DPM model and additional PETS ground-truth annotations, respectively."
    }, {
      "heading" : "A DEEPMASK RESULTS ON PETS",
      "text" : "To obtain segmentation masks on PETS, we train our reimplementation of DeepMask (Pinheiro et al., 2015) for all classes on the COCO training set. Our implementation is based on the FastRCNN network Girshick (2015). To generate instance segmentations on PETS, we upscale the image by a factor of 2× and predict segments on detections. Figure 13 shows mask predictions for annotations on the PETS test set. It works well in low occlusion cases (left and middle column), however, under heavy occlusion it makes mistakes by collapsing the segment or merging the occluding and the occluded person (see right-most column)."
    } ],
    "references" : [ {
      "title" : "On detection of multiple object instances using hough transforms",
      "author" : [ "Barinova", "Olga", "Lempitsky", "Victor", "Kholi", "Pushmeet" ],
      "venue" : null,
      "citeRegEx" : "Barinova et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Barinova et al\\.",
      "year" : 2012
    }, {
      "title" : "Detecting people using mutually consistent poselet activations",
      "author" : [ "Bourdev", "Lubomir", "Maji", "Subhransu", "Brox", "Thomas", "Malik", "Jitendra" ],
      "venue" : "In ECCV,",
      "citeRegEx" : "Bourdev et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Bourdev et al\\.",
      "year" : 2010
    }, {
      "title" : "Detection evolution with multi-order contextual co-occurrence",
      "author" : [ "Chen", "Guang", "Ding", "Yuanyuan", "Xiao", "Jing", "Han", "Tony X" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Chen et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2013
    }, {
      "title" : "Convolutional feature masking for joint object and stuff segmentation",
      "author" : [ "Dai", "Jifeng", "He", "Kaiming", "Sun", "Jian" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Dai et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Dai et al\\.",
      "year" : 2015
    }, {
      "title" : "Histograms of oriented gradients for human detection",
      "author" : [ "N. Dalal", "B. Triggs" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Dalal and Triggs,? \\Q2005\\E",
      "shortCiteRegEx" : "Dalal and Triggs",
      "year" : 2005
    }, {
      "title" : "Discriminative models for multi-class object layout",
      "author" : [ "C. Desai", "D. Ramanan", "C. Fowlkes" ],
      "venue" : null,
      "citeRegEx" : "Desai et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Desai et al\\.",
      "year" : 2011
    }, {
      "title" : "Pedestrian detection: An evaluation of the state of the art",
      "author" : [ "Dollár", "Piotr", "Wojek", "Christian", "Schiele", "Bernt", "Perona", "Pietro" ],
      "venue" : null,
      "citeRegEx" : "Dollár et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Dollár et al\\.",
      "year" : 2012
    }, {
      "title" : "The pascal visual object classes challenge: A retrospective",
      "author" : [ "M. Everingham", "S.M.A. Eslami", "L. Van Gool", "C.K.I. Williams", "J. Winn", "A. Zisserman" ],
      "venue" : null,
      "citeRegEx" : "Everingham et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Everingham et al\\.",
      "year" : 2015
    }, {
      "title" : "Object detection with discriminatively trained part-based models",
      "author" : [ "P. Felzenszwalb", "R. Girshick", "D. McAllester", "D. Ramanan" ],
      "venue" : null,
      "citeRegEx" : "Felzenszwalb et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Felzenszwalb et al\\.",
      "year" : 2010
    }, {
      "title" : "Pets2010: Dataset and challenge",
      "author" : [ "J Ferryman", "Ellis", "Anna" ],
      "venue" : "In AVSS,",
      "citeRegEx" : "Ferryman et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Ferryman et al\\.",
      "year" : 2010
    }, {
      "title" : "Are we ready for autonomous driving? the kitti vision benchmark suite",
      "author" : [ "Geiger", "Andreas", "Lenz", "Philip", "Urtasun", "Raquel" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Geiger et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Geiger et al\\.",
      "year" : 2012
    }, {
      "title" : "Fast R-CNN",
      "author" : [ "R. Girshick" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "Girshick,? \\Q2015\\E",
      "shortCiteRegEx" : "Girshick",
      "year" : 2015
    }, {
      "title" : "Rich feature hierarchies for accurate object detection and semantic segmentation",
      "author" : [ "R. Girshick", "J. Donahue", "T. Darrell", "J. Malik" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Girshick et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Girshick et al\\.",
      "year" : 2014
    }, {
      "title" : "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
      "author" : [ "He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "What makes for effective detection proposals",
      "author" : [ "Hosang", "Jan", "Benenson", "Rodrigo", "Dollár", "Piotr", "Schiele", "Bernt" ],
      "venue" : null,
      "citeRegEx" : "Hosang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hosang et al\\.",
      "year" : 2015
    }, {
      "title" : "Caffe: Convolutional architecture for fast feature embedding",
      "author" : [ "Jia", "Yangqing", "Shelhamer", "Evan", "Donahue", "Jeff", "Karayev", "Sergey", "Long", "Jonathan", "Girshick", "Ross", "Guadarrama", "Sergio", "Darrell", "Trevor" ],
      "venue" : null,
      "citeRegEx" : "Jia et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Jia et al\\.",
      "year" : 2014
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "D.P. Kingma", "J. Ba" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Kingma and Ba,? \\Q2015\\E",
      "shortCiteRegEx" : "Kingma and Ba",
      "year" : 2015
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Robust object detection with interleaved categorization and segmentation",
      "author" : [ "Leibe", "Bastian", "Leonardis", "Aleš", "Schiele", "Bernt" ],
      "venue" : null,
      "citeRegEx" : "Leibe et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Leibe et al\\.",
      "year" : 2008
    }, {
      "title" : "Microsoft coco: Common objects in context",
      "author" : [ "Lin", "Tsung-Yi", "Maire", "Michael", "Belongie", "Serge", "Hays", "James", "Perona", "Pietro", "Ramanan", "Deva", "Dollár", "Piotr", "Zitnick", "C Lawrence" ],
      "venue" : "In ECCV,",
      "citeRegEx" : "Lin et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2014
    }, {
      "title" : "Face detection without bells and whistles",
      "author" : [ "M. Mathias", "R. Benenson", "M. Pedersoli", "L. Van Gool" ],
      "venue" : "In ECCV,",
      "citeRegEx" : "Mathias et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mathias et al\\.",
      "year" : 2014
    }, {
      "title" : "Continuous energy minimization for multitarget tracking",
      "author" : [ "A. Milan", "S. Roth", "K. Schindler" ],
      "venue" : null,
      "citeRegEx" : "Milan et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Milan et al\\.",
      "year" : 2014
    }, {
      "title" : "Efficient non-maximum suppression",
      "author" : [ "Neubeck", "Alexander", "Van Gool", "Luc" ],
      "venue" : "In ICPR,",
      "citeRegEx" : "Neubeck et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Neubeck et al\\.",
      "year" : 2006
    }, {
      "title" : "Single-pedestrian detection aided by multi-pedestrian detection",
      "author" : [ "W. Ouyang", "X. Wang" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Ouyang and Wang,? \\Q2013\\E",
      "shortCiteRegEx" : "Ouyang and Wang",
      "year" : 2013
    }, {
      "title" : "Human-debugging of machines",
      "author" : [ "Parikh", "Devi", "C. Zitnick" ],
      "venue" : "In NIPS WCSSWC,",
      "citeRegEx" : "Parikh et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Parikh et al\\.",
      "year" : 2011
    }, {
      "title" : "Multi-view and 3d deformable part models",
      "author" : [ "Pepik", "Bojan", "Stark", "Michael", "Gehler", "Peter", "Schiele", "Bernt" ],
      "venue" : null,
      "citeRegEx" : "Pepik et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Pepik et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning to segment object candidates",
      "author" : [ "Pinheiro", "Pedro O", "Collobert", "Ronan", "Dollar", "Piotr" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Pinheiro et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Pinheiro et al\\.",
      "year" : 2015
    }, {
      "title" : "Faster R-CNN: Towards real-time object detection with region proposal networks",
      "author" : [ "S. Ren", "K. He", "R. Girshick", "J. Sun" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Ren et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2015
    }, {
      "title" : "Density-aware person detection and tracking in crowds",
      "author" : [ "Rodriguez", "Mikel", "Laptev", "Ivan", "Sivic", "Josef", "Audibert", "Jean-Yves" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "Rodriguez et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Rodriguez et al\\.",
      "year" : 2011
    }, {
      "title" : "Non-maximum suppression for object detection by passing messages between windows",
      "author" : [ "Rothe", "Rasmus", "Guillaumin", "Matthieu", "Van Gool", "Luc" ],
      "venue" : "In ACCV,",
      "citeRegEx" : "Rothe et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Rothe et al\\.",
      "year" : 2015
    }, {
      "title" : "Recognition using visual phrases",
      "author" : [ "M.A. Sadeghi", "A. Farhadi" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Sadeghi and Farhadi,? \\Q2011\\E",
      "shortCiteRegEx" : "Sadeghi and Farhadi",
      "year" : 2011
    }, {
      "title" : "Overfeat: Integrated recognition, localization and detection using convolutional networks",
      "author" : [ "Sermanet", "Pierre", "Eigen", "David", "Zhang", "Xiang", "Mathieu", "Michael", "Fergus", "Rob", "LeCun", "Yann" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Sermanet et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sermanet et al\\.",
      "year" : 2014
    }, {
      "title" : "Part-based multiple-person tracking with partial occlusion handling",
      "author" : [ "Shu", "Guang", "Dehghan", "Afshin", "Oreifej", "Omar", "Hand", "Emily", "Shah", "Mubarak" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Shu et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Shu et al\\.",
      "year" : 2012
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "K. Simonyan", "A. Zisserman" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Simonyan and Zisserman,? \\Q2015\\E",
      "shortCiteRegEx" : "Simonyan and Zisserman",
      "year" : 2015
    }, {
      "title" : "End-to-end people detection in crowded scenes",
      "author" : [ "Stewart", "Russell", "Andriluka", "Mykhaylo" ],
      "venue" : null,
      "citeRegEx" : "Stewart et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Stewart et al\\.",
      "year" : 2015
    }, {
      "title" : "Counting people in the crowd using a generic head detector",
      "author" : [ "Subburaman", "Venkatesh Bala", "Descamps", "Adrien", "Carincotte", "Cyril" ],
      "venue" : "In AVSS,",
      "citeRegEx" : "Subburaman et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Subburaman et al\\.",
      "year" : 2012
    }, {
      "title" : "Detection and tracking of occluded people",
      "author" : [ "S. Tang", "M. Andriluka", "B. Schiele" ],
      "venue" : "In BMVC,",
      "citeRegEx" : "Tang et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2012
    }, {
      "title" : "Learning people detectors for tracking in crowded scenes",
      "author" : [ "Tang", "Siyu", "Andriluka", "Mykhaylo", "Milan", "Anton", "Schindler", "Kaspar", "Roth", "Stefan", "Schiele", "Bernt" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "Tang et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2013
    }, {
      "title" : "Subgraph decomposition for multi-target tracking",
      "author" : [ "Tang", "Siyu", "Andres", "Bjoern", "Andriluka", "Mykhaylo", "Schiele", "Bernt" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Tang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2015
    }, {
      "title" : "Auto-context and its application to high-level vision tasks and 3d brain image segmentation",
      "author" : [ "Tu", "Zhuowen", "Bai", "Xiang" ],
      "venue" : null,
      "citeRegEx" : "Tu et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Tu et al\\.",
      "year" : 2010
    }, {
      "title" : "Object localization in imagenet by looking out of the window",
      "author" : [ "Vezhnevets", "Alexander", "Ferrari", "Vittorio" ],
      "venue" : "In BMVC,",
      "citeRegEx" : "Vezhnevets et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Vezhnevets et al\\.",
      "year" : 2015
    }, {
      "title" : "Robust real-time face detection",
      "author" : [ "P. Viola", "M. Jones" ],
      "venue" : "In IJCV,",
      "citeRegEx" : "Viola and Jones,? \\Q2004\\E",
      "shortCiteRegEx" : "Viola and Jones",
      "year" : 2004
    }, {
      "title" : "End-to-end integration of a convolutional network, deformable parts model and non-maximum suppression",
      "author" : [ "Wan", "Li", "Eigen", "David", "Fergus", "Rob" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Wan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Wan et al\\.",
      "year" : 2015
    }, {
      "title" : "Detecting partially occluded objects with an implicit shape model random field",
      "author" : [ "Wohlhart", "Paul", "Donoser", "Michael", "Roth", "Peter M", "Bischof", "Horst" ],
      "venue" : "In ACCV,",
      "citeRegEx" : "Wohlhart et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Wohlhart et al\\.",
      "year" : 2012
    }, {
      "title" : "Sliding-windows for rapid object class localization: A parallel technique",
      "author" : [ "Wojek", "Christian", "Dorkó", "Gyuri", "Schulz", "André", "Schiele", "Bernt" ],
      "venue" : "In DAGM,",
      "citeRegEx" : "Wojek et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Wojek et al\\.",
      "year" : 2008
    }, {
      "title" : "Object detection by labeling superpixels",
      "author" : [ "Yan", "Junjie", "Yu", "Yinan", "Zhu", "Xiangyu", "Lei", "Zhen", "Li", "Stan Z" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Yan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Yan et al\\.",
      "year" : 2015
    }, {
      "title" : "Describing the scene as a whole: Joint object detection, scene classification and semantic segmentation",
      "author" : [ "J. Yao", "S. Fidler", "R. Urtasun" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Yao et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Yao et al\\.",
      "year" : 2012
    }, {
      "title" : "To generate instance segmentations on PETS, we upscale the image by a factor of 2× and predict segments on detections",
      "author" : [ "RCNN network Girshick" ],
      "venue" : "Figure 13 shows mask predictions for annotations on the PETS test set. It works well in low occlusion cases (left and middle column), however, under heavy occlusion it makes mistakes by collapsing the segment or merging the occluding and the occluded person (see right-most column).",
      "citeRegEx" : "Girshick,? 2015",
      "shortCiteRegEx" : "Girshick",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "The popular DPM (Felzenszwalb et al., 2010) and R-CNN family (Girshick et al.",
      "startOffset" : 16,
      "endOffset" : 43
    }, {
      "referenceID" : 12,
      "context" : ", 2010) and R-CNN family (Girshick et al., 2014; Girshick, 2015; Ren et al., 2015) follow this approach.",
      "startOffset" : 25,
      "endOffset" : 82
    }, {
      "referenceID" : 11,
      "context" : ", 2010) and R-CNN family (Girshick et al., 2014; Girshick, 2015; Ren et al., 2015) follow this approach.",
      "startOffset" : 25,
      "endOffset" : 82
    }, {
      "referenceID" : 27,
      "context" : ", 2010) and R-CNN family (Girshick et al., 2014; Girshick, 2015; Ren et al., 2015) follow this approach.",
      "startOffset" : 25,
      "endOffset" : 82
    }, {
      "referenceID" : 14,
      "context" : "Both object proposals (Hosang et al., 2015) and detection classifiers (Russakovsky et al.",
      "startOffset" : 22,
      "endOffset" : 43
    }, {
      "referenceID" : 8,
      "context" : "Clustering detections The decade old greedy NMS (GreedyNMS) is used in popular detectors such as V&J (Viola & Jones, 2004), DPM (Felzenszwalb et al., 2010), and is still used in the state-ofthe-art R-CNN detector family (Girshick et al.",
      "startOffset" : 128,
      "endOffset" : 155
    }, {
      "referenceID" : 12,
      "context" : ", 2010), and is still used in the state-ofthe-art R-CNN detector family (Girshick et al., 2014; Girshick, 2015; Ren et al., 2015).",
      "startOffset" : 72,
      "endOffset" : 129
    }, {
      "referenceID" : 11,
      "context" : ", 2010), and is still used in the state-ofthe-art R-CNN detector family (Girshick et al., 2014; Girshick, 2015; Ren et al., 2015).",
      "startOffset" : 72,
      "endOffset" : 129
    }, {
      "referenceID" : 27,
      "context" : ", 2010), and is still used in the state-ofthe-art R-CNN detector family (Girshick et al., 2014; Girshick, 2015; Ren et al., 2015).",
      "startOffset" : 72,
      "endOffset" : 129
    }, {
      "referenceID" : 44,
      "context" : "Alternatives such as mean-shift clustering (Dalal & Triggs, 2005; Wojek et al., 2008), agglomerative clustering (Bourdev et al.",
      "startOffset" : 43,
      "endOffset" : 85
    }, {
      "referenceID" : 1,
      "context" : ", 2008), agglomerative clustering (Bourdev et al., 2010), and heuristic variants (Sermanet et al.",
      "startOffset" : 34,
      "endOffset" : 56
    }, {
      "referenceID" : 31,
      "context" : ", 2010), and heuristic variants (Sermanet et al., 2014) have been considered, but they have yet to show consistent gains.",
      "startOffset" : 32,
      "endOffset" : 55
    }, {
      "referenceID" : 1,
      "context" : ", 2008), agglomerative clustering (Bourdev et al., 2010), and heuristic variants (Sermanet et al., 2014) have been considered, but they have yet to show consistent gains. Recently Tang et al. (2015); Rothe et al.",
      "startOffset" : 35,
      "endOffset" : 199
    }, {
      "referenceID" : 1,
      "context" : ", 2008), agglomerative clustering (Bourdev et al., 2010), and heuristic variants (Sermanet et al., 2014) have been considered, but they have yet to show consistent gains. Recently Tang et al. (2015); Rothe et al. (2015) proposed principled clustering approaches that provide globally optimal solutions, however the results reached are on par, but do not surpass, GreedyNMS.",
      "startOffset" : 35,
      "endOffset" : 220
    }, {
      "referenceID" : 18,
      "context" : "Linking detections to pixels The Hough voting framework enables more sophisticated reasoning amongst conflicting detections by linking the detections to local image evidence (Leibe et al., 2008; Barinova et al., 2012; Wohlhart et al., 2012).",
      "startOffset" : 174,
      "endOffset" : 240
    }, {
      "referenceID" : 0,
      "context" : "Linking detections to pixels The Hough voting framework enables more sophisticated reasoning amongst conflicting detections by linking the detections to local image evidence (Leibe et al., 2008; Barinova et al., 2012; Wohlhart et al., 2012).",
      "startOffset" : 174,
      "endOffset" : 240
    }, {
      "referenceID" : 43,
      "context" : "Linking detections to pixels The Hough voting framework enables more sophisticated reasoning amongst conflicting detections by linking the detections to local image evidence (Leibe et al., 2008; Barinova et al., 2012; Wohlhart et al., 2012).",
      "startOffset" : 174,
      "endOffset" : 240
    }, {
      "referenceID" : 0,
      "context" : ", 2008; Barinova et al., 2012; Wohlhart et al., 2012). Hough voting itself, however, provides low detection accuracy. Yao et al. (2012) and Dai et al.",
      "startOffset" : 8,
      "endOffset" : 136
    }, {
      "referenceID" : 0,
      "context" : ", 2008; Barinova et al., 2012; Wohlhart et al., 2012). Hough voting itself, however, provides low detection accuracy. Yao et al. (2012) and Dai et al. (2015) refine detections by linking them with semantic labelling; while Yan et al.",
      "startOffset" : 8,
      "endOffset" : 158
    }, {
      "referenceID" : 0,
      "context" : ", 2008; Barinova et al., 2012; Wohlhart et al., 2012). Hough voting itself, however, provides low detection accuracy. Yao et al. (2012) and Dai et al. (2015) refine detections by linking them with semantic labelling; while Yan et al. (2015) side-steps NMS all-together by defining the detection task directly as a labelling problem.",
      "startOffset" : 8,
      "endOffset" : 241
    }, {
      "referenceID" : 36,
      "context" : "Co-occurrence To better handle dense crowds or common object pairs, it has been proposed to use specialized 2-object detectors (Sadeghi & Farhadi, 2011; Tang et al., 2012; Ouyang & Wang, 2013), which then require a more careful NMS strategy to merge single-object with double-object detections.",
      "startOffset" : 127,
      "endOffset" : 192
    }, {
      "referenceID" : 27,
      "context" : "Similarly Rodriguez et al. (2011) proposed to adapt the NMS threshold using crowd density estimation.",
      "startOffset" : 10,
      "endOffset" : 34
    }, {
      "referenceID" : 5,
      "context" : "Desai et al. (2011) considered improving the NMS procedure by considering the spatial arrangement between detections of different classes.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 2,
      "context" : "Auto-context uses the local (Tu & Bai, 2010; Chen et al., 2013) or global information (Vezhnevets & Ferrari, 2015) on the image to re-score detections.",
      "startOffset" : 28,
      "endOffset" : 63
    }, {
      "referenceID" : 8,
      "context" : "We report experiment applied over DPM (Felzenszwalb et al., 2010) detections.",
      "startOffset" : 38,
      "endOffset" : 65
    }, {
      "referenceID" : 41,
      "context" : "Detection convnets are commonly trained unaware of the NMS post-processing step; Wan et al. (2015) connected NMS with the loss to train the detection convnet, making the training truly end-to-end.",
      "startOffset" : 81,
      "endOffset" : 99
    }, {
      "referenceID" : 41,
      "context" : "Detection convnets are commonly trained unaware of the NMS post-processing step; Wan et al. (2015) connected NMS with the loss to train the detection convnet, making the training truly end-to-end. The used NMS is greedy and with fixed parameters. Stewart & Andriluka (2015) propose to use an LSTM to decide how many detections should be considered in a local region.",
      "startOffset" : 81,
      "endOffset" : 274
    }, {
      "referenceID" : 41,
      "context" : "Detection convnets are commonly trained unaware of the NMS post-processing step; Wan et al. (2015) connected NMS with the loss to train the detection convnet, making the training truly end-to-end. The used NMS is greedy and with fixed parameters. Stewart & Andriluka (2015) propose to use an LSTM to decide how many detections should be considered in a local region. The detections amongst the regions are then merged via traditional NMS. In contrast our convnet runs in a fully convolutional mode and requires no post-processing. Both Wan et al. (2015) and Stewart & Andriluka (2015) served as inspiration for the design of our training loss.",
      "startOffset" : 81,
      "endOffset" : 554
    }, {
      "referenceID" : 41,
      "context" : "Detection convnets are commonly trained unaware of the NMS post-processing step; Wan et al. (2015) connected NMS with the loss to train the detection convnet, making the training truly end-to-end. The used NMS is greedy and with fixed parameters. Stewart & Andriluka (2015) propose to use an LSTM to decide how many detections should be considered in a local region. The detections amongst the regions are then merged via traditional NMS. In contrast our convnet runs in a fully convolutional mode and requires no post-processing. Both Wan et al. (2015) and Stewart & Andriluka (2015) served as inspiration for the design of our training loss.",
      "startOffset" : 81,
      "endOffset" : 585
    }, {
      "referenceID" : 17,
      "context" : "The parameter range (number of layers, number of units per layer) is inspired by AlexNet (Krizhevsky et al., 2012) and VGG (Simonyan & Zisserman, 2015).",
      "startOffset" : 89,
      "endOffset" : 114
    }, {
      "referenceID" : 26,
      "context" : "The IoU layer values can be computed over bounding boxes (regressed by the sliding window detector), or over estimated instance segments (Pinheiro et al., 2015).",
      "startOffset" : 137,
      "endOffset" : 160
    }, {
      "referenceID" : 8,
      "context" : "The DPM (Felzenszwalb et al., 2010; Pepik et al., 2015) includes structured output learning to ensure the detection score falls off linearly with the overlap between detector window and annotation.",
      "startOffset" : 8,
      "endOffset" : 55
    }, {
      "referenceID" : 25,
      "context" : "The DPM (Felzenszwalb et al., 2010; Pepik et al., 2015) includes structured output learning to ensure the detection score falls off linearly with the overlap between detector window and annotation.",
      "startOffset" : 8,
      "endOffset" : 55
    }, {
      "referenceID" : 8,
      "context" : "The DPM (Felzenszwalb et al., 2010; Pepik et al., 2015) includes structured output learning to ensure the detection score falls off linearly with the overlap between detector window and annotation. Wan et al. (2015) explicitly include a fixed NMS procedure into the network loss during training so the detector is tuned towards the NMS at test time.",
      "startOffset" : 9,
      "endOffset" : 216
    }, {
      "referenceID" : 8,
      "context" : "The DPM (Felzenszwalb et al., 2010; Pepik et al., 2015) includes structured output learning to ensure the detection score falls off linearly with the overlap between detector window and annotation. Wan et al. (2015) explicitly include a fixed NMS procedure into the network loss during training so the detector is tuned towards the NMS at test time. We adopt from Stewart & Andriluka (2015) the idea of computing the loss by matching detections to annotations, but instead of regressing bounding",
      "startOffset" : 9,
      "endOffset" : 391
    }, {
      "referenceID" : 13,
      "context" : "Training parameters The model is trained from scratch, randomly initialized with MSRA (He et al., 2015), and optimized with Adam (Kingma & Ba, 2015).",
      "startOffset" : 86,
      "endOffset" : 103
    }, {
      "referenceID" : 15,
      "context" : "All experiments are implemented with the Caffe framework (Jia et al., 2014).",
      "startOffset" : 57,
      "endOffset" : 75
    }, {
      "referenceID" : 38,
      "context" : "Other NMS approaches such as (Tang et al., 2015; Rothe et al., 2015) also require training data to be adjusted.",
      "startOffset" : 29,
      "endOffset" : 68
    }, {
      "referenceID" : 29,
      "context" : "Other NMS approaches such as (Tang et al., 2015; Rothe et al., 2015) also require training data to be adjusted.",
      "startOffset" : 29,
      "endOffset" : 68
    }, {
      "referenceID" : 13,
      "context" : "Training parameters The model is trained from scratch, randomly initialized with MSRA (He et al., 2015), and optimized with Adam (Kingma & Ba, 2015). We use a learning rate of 10−4, a weight decay of 5 · 10−5, a momentum of 0.9, and gradient clipping at 1 000. The model is trained for 100 000 iterations with one image per iteration. All experiments are implemented with the Caffe framework (Jia et al., 2014). As pointed out in Mathias et al. (2014) the threshold for GreedyNMS requires to be carefully selected on the validation set of each task, the commonly used default IoU > 0.",
      "startOffset" : 87,
      "endOffset" : 452
    }, {
      "referenceID" : 7,
      "context" : "5 (Everingham et al., 2015).",
      "startOffset" : 2,
      "endOffset" : 27
    }, {
      "referenceID" : 37,
      "context" : "PETS has been previously used to study person detection (Tang et al., 2013), tracking (Milan et al.",
      "startOffset" : 56,
      "endOffset" : 75
    }, {
      "referenceID" : 21,
      "context" : ", 2013), tracking (Milan et al., 2014), and crowd density estimation (Subburaman et al.",
      "startOffset" : 18,
      "endOffset" : 38
    }, {
      "referenceID" : 35,
      "context" : ", 2014), and crowd density estimation (Subburaman et al., 2012).",
      "startOffset" : 38,
      "endOffset" : 63
    }, {
      "referenceID" : 6,
      "context" : "Standard pedestrian datasets such as Caltech (Dollár et al., 2012) or KITTI Geiger et al.",
      "startOffset" : 45,
      "endOffset" : 66
    }, {
      "referenceID" : 6,
      "context" : "Standard pedestrian datasets such as Caltech (Dollár et al., 2012) or KITTI Geiger et al. (2012) average less than two pedestrian per frame, making close-by detections a rare occurrence.",
      "startOffset" : 46,
      "endOffset" : 97
    }, {
      "referenceID" : 32,
      "context" : "ParkingLot We use the first ParkingLot (Shu et al., 2012) sequence to evaluate the generalization capabilities of the model.",
      "startOffset" : 39,
      "endOffset" : 57
    }, {
      "referenceID" : 37,
      "context" : "For our experiments we use the baseline DPM detector from (Tang et al., 2013).",
      "startOffset" : 58,
      "endOffset" : 77
    }, {
      "referenceID" : 6,
      "context" : "We are not aware of a detector (convnet or not) providing better results on PETSlike sequences (we considered some of the top detectors in (Dollár et al., 2012)).",
      "startOffset" : 139,
      "endOffset" : 160
    }, {
      "referenceID" : 26,
      "context" : "We use our re-implementation of DeepMask (Pinheiro et al., 2015), trained on the Coco dataset (Lin et al.",
      "startOffset" : 41,
      "endOffset" : 64
    }, {
      "referenceID" : 19,
      "context" : ", 2015), trained on the Coco dataset (Lin et al., 2014).",
      "startOffset" : 37,
      "endOffset" : 55
    } ],
    "year" : 2016,
    "abstractText" : "Non-maximum suppression (NMS) is used in virtually all state-of-the-art object detection pipelines. While essential object detection ingredients such as features, classifiers, and proposal methods have been extensively researched surprisingly little work has aimed to systematically address NMS. The de-facto standard for NMS is based on greedy clustering with a fixed distance threshold, which forces to trade-off recall versus precision. We propose a convnet designed to perform NMS of a given set of detections. We report experiments on a synthetic setup, and results on crowded pedestrian detection scenes. Our approach overcomes the intrinsic limitations of greedy NMS, obtaining better recall and precision.",
    "creator" : "LaTeX with hyperref package"
  }
}