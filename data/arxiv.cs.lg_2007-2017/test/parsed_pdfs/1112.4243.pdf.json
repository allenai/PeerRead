{
  "name" : "1112.4243.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Online Learning for Classification of Low-rank Representation Features and Its Applications in Audio Segment Classification",
    "authors" : [ "Ziqiang Shi", "Jiqing Han", "Tieran Zheng", "Shiwen Deng" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n11 2.\n42 43\nv1 [\ncs .L\nG ]\n1 9"
    }, {
      "heading" : "1 Introduction",
      "text" : "Audio feature extraction and classification methods have been studied by many researchers over the years [1, 2, 3, 4]. In general, audio classification can be performed in two steps, which involves reducing the audio sound to a small set of parameters using various feature extraction techniques and classifying or categorizing over these parameters. Feature commonly exploited for audio classification can be roughly classified into time domain features, transformation domain features, time-transformation domain features or their combinations [4, 5]. Many of those features are common to audio signal processing and speech recognition and have many successful performances in various applications. However almost all these features are based on short time duration and in vector form (it is easy to handle but sometimes not proper), although it is believed that long time duration (seconds) help a lot in decision making. In this work we will build robust features on a long time duration in matrix form which is the most natural way using long time audio information.\nIn order to map or smooth the audio segment into a robust matrix space, we introduce the trace norm regularization technique to audio signal processing. The trace norm regularization\n1\nis a principled approach to learn low-rank matrices through convex optimization problems [7]. These similar problems arise in many machine learning tasks such as matrix completion [8], multi-task learning [9], robust principle component antilysis (robust PCA) [10, 11], and matrix classification [12]. In this paper, robust PCA is used to extract matrix representation features for audio segments. Unlike traditional frame based vector features, these matrix features are extracted based on sequences of audio frames. It is believed that in a short duration the signals are contributed by a few factors. Thus it is natural to approximate the frame sequence by lowrank features using robust PCA which assumes that the observed matrices are combinations of some low-rank matrices and some corruption noise matrices.\nHaving extracted descriptive features, various machine learning methods are used to provide a final classification of the audio events such as rule-based approaches, Gaussian mixture models, support vector machines, Bayesian networks, and etc. [4, 5, 6]. In most previous work, these two steps for audio classification are always separate and independent. In this work, we can learn the classifiers in solving similar optimization problems using trace norm regularization. After extraction of the robust low-rank matrix feature, the regularization framework based matrix classification approach proposed by Tomioka and Aihara in [12] is used to predict the label.\nThe problem of matrix classification (MC) with spectral regularization was first proposed by Tomioka and Aihara in [12]. The goal of the problem is to infer the weight matrix and bias under low trace norm constraints and low deviation of the empirical statistics from their predictions. The trace norm was use to measure the complexity of the weight matrix of the linear classifier for matrix classifications. This kind of inference task belongs to the more general problem of learning low-rank matrix through convex optimization. For the matrix rank minimization is NP-hard in general due to the combinatorial nature of the rank function, a commonly-used convex relaxation of the rank function is the trace norm (nuclear norm) [7], defined as the sum of the singular values of the matrix.\nRecent related researches are not focused on matrix classification directly, but rather on general trace norm minimization problem [13, 14, 15]. These general algorithm can be adapted to matrix classification suitably. In these methods, most are iterative batch procedures [13, 14, 15], accessing the whole training set at each iteration in order to minimize a weighted sum of a cost function and the trace norm. This kind of learning procedure cannot deal with huge size training set for the data probably cannot be loaded into memory simultaneously. Furthermore it cannot be started until the training data are prepared, hence cannot effectively deal with training data appear in sequence, such as audio and video processing.\nTo address these problems, we propose an online approach that processes the training samples, one at a time, or in mini-batches to learn the weight matrix and the bias for matrix classification. We transform the general batch-mode accelerated proximal gradient (APG) [13, 14] method for trace norm minimization to the online learning framework. In this online learning framework, a slight improvement over the exact APG leads an inexact APG (IAPG) method, which needs less computation in one iteration than using exact APG. In addition, as a special case of general convex optimization problem, we derived the closed-form of the Lipschitz constant, hence the step size estimation [13, 14] of the general APG method was omitted in our approach.\nOur main contributions in this work can be summarized as follows:\n1. To our best knowledge, we are the first to introduce low-rank constraints in audio and speech signal processing, and the results show that these constrains make the systems more robust to noise, especially to large corruptions.\n2\n2. We propose online learning algorithms to learn the trace norm minimization based matrix classifier, which make the approaches work in real applications.\nThe paper is organized as follows: Section 2 presents the extraction of matrix representation feature. Section 3 presents the matrix classification problem solving via the general APG method and the proposed audio event detection with matrix classification. The proposed online methods with exact and inexact APG for weight and bias learning are introduced in Section 5.2. Section 5 is devoted to experimental results to demonstrate the characteristics and merits of the proposed algorithm. Finally we give some concluding remarks in Section 6."
    }, {
      "heading" : "2 Low-Rank Matrix Representation Features",
      "text" : "Over the past decades, a lot work has been done on audio and speech features for audio and speech processing [2, 3, 5]. Due to convenience and the short-time stationary assumption, these features are mainly in vector form based on frames, although it is believed that features based on longer duration help a lot in decision making. In order to build long term features, the consecutive frame signals are made together as rows, then the audio segments become matrices. Generally, it is assumed and believed that the consecutive frame signals are influenced by a few factors, thus these matrices are combinations of low-rank components and noise. Hence it is natural to approximate these matrices by low-rank matrices. In this work, transformations of these approximate low-rank matrices are used as features.\nGiven an observed data matrixD ∈ Rm×n, wherem is the number of frames and n represents the number of samples in a frame, it is assumed that it can be decomposed as\nD = A+ E, (1)\nwhere A is the low-rank component and E is the error or noise matrix. The purpose here is to recover the low-rank component without knowing the rank of it. For this problem, PCA is a suitable approach that it can find the low-dimensional approximating subspace by forming a lowrank approximation to the data matrix [16]. However, it breaks down under large corruption, even if that corruption affects only a very few of the observation which is often encountered in practice [11]. To solve this problem, the following convex optimization formulation is proposed\nmin A,E∈Rm×n\n‖A‖∗ + λ‖E‖1, subject to D = A+ E, (2)\nwhere ‖ · ‖∗ denotes the trace norm of a matrix which is defined as the sum of the singular values, ‖ · ‖1 denotes the sum of the absolute values of matrix elements, and λ is a positive regularization parameter. This optimization is refereed to as robust PCA in [10] for its ability to exactly recover underlying low-rank structure in data even in the presence of large errors or outliers. In order to solve Equation (2), several algorithms have been proposed, among which the augmented Lagrange multiplier method is the most efficient and accurate at present [11]. In our work, this robust PCA method is employed for the low-rank matrix extraction.\nIn order to apply the augmented Lagrange multiplier (ALM) to the robust PCA problem, Lin et. al. [11] identify the problem as\nX = (A,E), f(X) = ‖A‖∗ + λ‖E‖1, and h(X) = D −A− E, (3)\n3\nand the Lagrangian function becomes\nL(A,E, Y, µ) . = ‖A‖∗ + λ‖E‖1+ < Y,D −A− E > +\nµ 2 ‖D −A− E‖2F . (4)\nTwo ALM algorithms to solve the above formulation are proposed in [11]. Considering a balance between processing speed and accuracy, the robust PCA via the inexact ALM method is chosen in our work. Thus the matrix representation feature extraction process based on this approach is summarized in Algorithm 2. In Algorithm 2, J(D) is defined as the larger one of ‖D‖2 and λ−1‖D‖∞, where ‖ · ‖∞ is the maximum absolute value of the matrix elements. The Sε[·] is the soft-thresholding operator introduced in [11].\nFig. 1 shows the recovered low-rank matrices via applying robust PCA to the matrix form of a typical laugh sound effect audio segment with or without corruptions. In which, the regularization parameter is fixed as 1. It can be seen that robust PCA extracted matrices are robust to large errors and Gaussian noise. Ideally, these above recovered low-rank matrices can be used as features directly. But in order to balance the speed and performance, in this work the we transform the recovered low-rank matrices into MFCCs (mel-frequency cepstral coefficients) matrices. All rows in the low-rank matrices are transformed into MFCCs independently. Fig. 2 shows the spectrograms of the signal in Fig. 1 respectively. It seems that the spectrograms of the low-rank components vary not much compare to the spectrograms of the corrupted signals.\nRecovering of Low-rank Component from Audio Segments via RPCA. Input: D ∈ Rm×n (matrix form of the audio segment). Initialize: D ∈ Rm×n, Y0 = D/J(D), E0 = 0, µ0 > 0, ρ > 1, k = 0. 1: while not converged do 2: // Lines 3-4 solve Ak+1 = argmin\nA L(A,Ek, Yk, µk).\n3: (U, S, V ) = svd(D − Ek + µ−1k Yk). 4: Ak = USµ−1\nk\n[S]V T .\n5: // Line 6 solves Ek+1 = argmin E L(Ak+1, E, Yk, µk). 6: Ek+1 = Sλµ−1 k\n[D −Ak+1 + µ−1k Yk]. 7: Yk+1 = Yk + µk(D −Ak+1 − Ek+1). 8: Update µk to µk+1. 9: k ← k + 1. 10: end while Output: W ← Wk."
    }, {
      "heading" : "3 Low Rank Matrix Classification",
      "text" : ""
    }, {
      "heading" : "3.1 Notation and Problem Statement",
      "text" : "Having extracted robust matrix representation features, the linear matrix classification approach based on trace norm regularization framework proposed in [12] is used to classify them. The motivation for trace norm regularization framework is two fold: a) trace norm considers the interactive information among the frames in the matrix while the simple approach that treat the matrix as a long vector would lose the information; b) trace norm is a suitable quantity\n4\n5\n6\nthat measures the complexity of the linear classifier. Generally, the problem for trace norm regularization based matrix classification is formulated as\nmin W,b\nFs(W, b) = fs(W, b) + λ ‖W‖∗ (5)\nwhere W ∈ Rm×n is the unknown weight matrix, b ∈ R is the bias, ‖·‖ ∗ denotes the trace norm defined as the sum of the singular values, and λ is the regularization parameter. fs(W, b) = s ∑\ni=1\nℓ(yi,Tr(W TXi) + b) is the empirical cost function induced by some convex smooth loss\nfunction ℓ(·, ·), where Tr(·) denotes the trace, the subscript of fs(W, b) indicates the number of training samples or time of training procedure which is apparent from context, and (Xi, yi) ∈ Rm×n ×R is the ith sample. In this work, the standard squared loss function is used. Hence the empirical cost function becomes fs(W, b) = s ∑\ni=1\n(yi − Tr(WTXi)− b)2."
    }, {
      "heading" : "3.2 APG Method for Matrix Classification",
      "text" : "Recently Toh and Yun [13], Ji and Ye [14], and Liu et al. [15] independently proposed similar algorithms that converge as O( 1\nk2 ) for problem (5) by using APG, where k is the iteration counter.\nThe precondition of using APG algorithm is that the loss function should be smooth, convex, and the gradient should satisfy Lipschitz condition. Since fs(W, b) in this work is a composition of smooth convex function with an affine mapping, hence it is convex and smooth [18]. For Lipschitz continuous, it is shown in Theorem 1 that the gradient of fs(W, b), denoted as\n∇W fs(W, b) = −2 s ∑\ni=1\n(yi − Tr(WTXi)− b)Xi, (6)\nis Lipschitz continuous. Thus the APG method can be used to solve matrix classification problem. In order to solve the unconstrained convex optimization problem (5), APG approximate fs(W, b) locally as a quadratic function with bias fixed and solve\nWk+1 = arg min W∈Rm×n Q(W,Zk) = fs(Zk, b) + tk 2 ‖W − Zk‖2F\n+ < ∇W fs(Zk, b),W − Zk > +λ ‖W‖∗ , (7)\nwhich is assumed to be easy, to update the solution W . Based on the the work of Nesterov [19, 20], Toh and Yun [13], Ji and Ye [14], and Liu et al. [15] showed that setting Zk = Wk + tk−1−1\ntk (Wk −Wk−1) for a sequence tk satisfying t2k+1− tk+1 ≤ t2k results in a convergence rate of O( 1 k2 ). Due to Lemma 1, the estimation of step size tk in general APG [13, 14, 15] is omitted, for we have explicit Lipschitz constant. The APG approach for batch-mode weight matrix learning is described in Algorithm 3.2. The Sε[·] in Algorithm 3.2 is the soft-thresholding operator introduced in [11]:\nSε[x] .=\n\n\n x− ε, if x > ε, x+ ε, if x < −ε, 0, otherwise\n(8)\nwhere x ∈ R and ε > 0. For vectors and matrices, this operator is extended by applying element-wise.\n7\nBatch-Mode Weight Matrix Learning via APG Initialize W0 = Z1 ∈ Rm×n, α1 = 1, L = 2mn s ∑\ni=1 ‖Xi‖2F , λ. 1: while not converged do 2: (U, S, V ) = svd(Zk − 1L(−2 ∑s i=1 (yi − Tr(ZTk Xi)− b)Xi)). 3: Wk = US λ L [S]V T . 4: αk+1 = 1+ √ 1+4α2 k\n2 .\n5: Zk+1 = Wk + αk−1 αk+1 (Wk −Wk−1).\n6: bk = 1\ns\ns ∑\ni=1 (yi − Tr(WTk Xi)). 7: k ← k + 1. 8: end while Output: W ← Wk.\nThe general APG [13, 14, 15] algorithms only provide the methods for learning weight matrices, do not give out the bias updating rules. In order to update the bias b, fixes the weight matrix Wk and solve the following problem\nbk = min b\ns ∑\ni=1\n(yi − Tr(WTk Xi)− b)2 + λ ‖Wk‖∗ , (9)\nwhich results in the bias updating rule\nbk = 1\ns\ns ∑\ni=1\n(yi − Tr(WTk Xi)). (10)\nThis results in the line 6 of Algorithm 3.2. For the stopping criteria of the iterations, we take the following relative error conditions:\n‖Wk+1 −Wk‖F /‖Wk‖F < ε1 and |bk+1 − bk|/|bk| < ε2. (11)\nAfter the weight matrix W and bias b are found, the observed MFCCs matrix Xi can be classified via\nŷi = Tr(W TXi) + b. (12)"
    }, {
      "heading" : "3.3 Determination of Lipschitz Constant",
      "text" : "As a special case of general convex optimization problem, we derived the closed-form of the Lipschitz constant, hence the step size estimation [13, 14] of the general APG method was omitted in all our approach. The determination of the Lipschitz constant is shown in the following theorem.\nTheorem 1. ∇W fs(·, b) is Lipschitz continuous with constant L = 2mn s ∑\ni=1\n‖Xi‖2F , i.e., ∀U, V ∈\nR m×n, ‖∇W fs(U, b)−∇W fs(V, b)‖F ≤ L ‖U − V ‖F , (13) where ‖·‖F denotes the Frobenius norm.\n8\nProof. Applying Equation (6) with U, V to the right of Equation (13), we obtain\n‖∇W fs(U, b)−∇W fs(V, b)‖F =‖ − 2 ∑s\ni=1 (yi − Tr(UTXi)− b)Xi\n+ 2 ∑s\ni=1 (yi − Tr(V TXi)− b)Xi‖F\n=2 ∥ ∥\n∥\n∑s\ni=1 (Tr(UTXi)l − Tr(V TXi))Xi\n∥ ∥ ∥\nF\n≤2 ∑s\ni=1\n∣ ∣Tr((UT − V T )Xi) ∣ ∣ ‖Xi‖F ≤2mn ∑s\ni=1\n∥ ∥UT − V T ∥ ∥ F ‖Xi‖2F\n=(2mn ∑s\ni=1 ‖Xi‖2F )\n∥ ∥UT − V T ∥ ∥\nF ,\nwhere in the last inequality, the easily verified fact that Tr(ATB) ≤ ‖A‖ 1 ‖B‖ 1 ≤ mn ‖A‖F ‖B‖F for ∀A,B ∈ Rm×n is used. Here ‖·‖ 1 denotes the ℓ1 norm which is the sum of the absolute values of the matrix elements. Thus the lemma is proofed, that is to say ∇W fs(·, b) is Lipschitz continuous with constant L = 2mn ∑s\ni=1 ‖Xi‖ 2 F .\nThe APG based batch-mode weight learning method is effective for small training set, but with large training sets, this classical optimization technique may become impractical in terms of memory requirements. Furthermore, this method cannot efficiently deal with dynamic training data of time sequences, such as audio and video processing. To tackle the insufficiency, we propose an online learning framework in the following section."
    }, {
      "heading" : "4 Online Learning for Matrix Classification",
      "text" : ""
    }, {
      "heading" : "4.1 Online Learning with APG",
      "text" : "We present in this section the basic components of our online learning algorithm for matrix classification, as well as a few minor variants which speed up our implementation in practice.\nOur procedure is summarized in Algorithm 4.1. The ⊗ operator in step 6 of the algorithm denotes the Kronecker product. Given two matrices A ∈ Rm1×n1 and B ∈ Rm2×n2 , A ⊗ B denotes the Kronecker product between A and B, defined as the matrix in Rm1m2×n1n2 , defined by blocks of sizes m2 × n2 equal to A[i, j]B. GridTr(Zk,t, Bt) in step 13 denotes an operator with input Zk,t ∈ Rm×n and Bt ∈ Rmm×nn, result in Rm×n with the (i, j)th element defined as the trace of the product between ZTk,t and the (i, j)th R\nm×n block of Bt. Assuming the training set composed of i.i.d. samples of a distribution p(X, y), its inner loop draws one training sample (Xt, yt) at a time. This sample is first used to update the “past” information At−1, Bt−1, ct−1, and Dt−1. Then the Algorithm 3.2 is applied to update the weight matrix with the warm startWt−1 obtained at the previous iteration. Since Ft(W, bt−1) is relative close to Ft−1(W, bt−1) for large values of t, so are Wt and Wt−1, under suitable assumptions, which makes it efficient to use Wt−1 as warm restart for computing Wt.\n9\nOnline MC Learning Based on APG. Initialize W0 ∈ Rm×n, b0 ∈ R, L0 = 0, λ ∈ R. 1: A0 ∈ Rm×n ← 0, B0 ∈ Rmm×nn ← 0, c0 ∈ R ← 0, D0 ∈ Rm×n ← 0(reset the “past” information). 2: for t = 1 to T do 3: Draw training sample (Xt, yt) from p(X, y). 4: // Line 5-9 update “past” information. 5: At ← At−1 + ytXt; 6: Bt ← Bt−1 +Xt ⊗Xt; 7: ct ← ct−1 + yt; 8: Dt ← Dt−1 +Xt; 9: Lt ← Lt−1 + 2mn ‖Xt‖2F . 10: // Line 11-19 update Wt and bt using Algorithm 3.2, with Wt−1 and bt−1 as warm restart. 11: W0,t = Z1,t = Wt−1 ∈ Rm×n, b0,t = bt−1, α1 = 1, k = 1. 12: while not converged do 13: (U, S, V ) = svd(Zk,t − 1Lt (−2At + 2GridTr(Zk,t, Bt) + 2bk−1,tDt). 14: Wk,t = US λ\nLt\n[S]V T .\n15: αk+1 = 1+\n√ 1+4α2\nk\n2 .\n16: Zk+1,t = Wk,t + αk−1 αk+1 (Wk,t −Wk−1,t). 17: bk,t = 1 t (ct − Tr(WTk,tDt) 18: k ← k + 1. 19: end while 20: Wt ← Wk,t, bt ← bk,t. 21: end for Output: W ← WT , b ← bT .\n10"
    }, {
      "heading" : "4.2 Online Learning with inexact APG",
      "text" : "Algorithm 4.1 calls APG to update the weight matrix for each coming sample by solving the sub-problem with fixed bias b\nWt = min W\nt ∑\ni=1\n(yi − Tr(WTXi)− bt−1)2 + λ ‖W‖∗ (14)\nexactly which cause computational load for large scale training set. Fortunately, due to the closeness of consecutive weight matrix, we do not have to solve the sub-problem exactly. Rather, updating Wt−1 once when solving this sub-problem is sufficient in practice. This leads to an online MC learning method based on inexact APG, described in Algorithm 4.2.\nOnline MC Learning with Inexact APG. Initialize W0 ∈ Rm×n, b0 ∈ R, L0 = 0, λ ∈ R. 1: A0 ∈ Rm×n ← 0, B0 ∈ Rmm×nn ← 0, c0 ∈ R ← 0, D0 ∈ Rm×n ← 0 (reset the “past” information). 2: for t = 1 to T do 3: Draw training sample (Xt, yt) from p(X, y). 4: // Line 5-9 update “past” information. 5: At ← At−1 + ytXt; 6: Bt ← Bt−1 +Xt ⊗Xt. 7: ct ← ct−1 + yt; 8: Dt ← Dt−1 +Xt. 9: Lt ← Lt−1 + 2mn ‖Xt‖2F . 10: // Line 11-16 compute Wt using inexact APG, with Wt−1 as warm restart. 11: W0,t = Wt−1 ∈ Rm×n. 12: (U, S, V ) = svd(W0,t − 1Lt (−2At + 2GridTr(W0,t, Bt) + 2bt−1Dt). 13: W1,t = US λ\nLt\n[S]V T .\n14: (U, S, V ) = svd(W1,t − 1Lt (−2At + 2GridTr(W1,t, Bt) + 2bt−1Dt). 15: W2,t = US λ\nLt\n[S]V T .\n16: Wt ← W2,t. 17: // Line 18 updates the bias bt. 18: bt = 1 t (ct − Tr(WTt Dt) 19: end for Output: W ← WT , b ← bT ."
    }, {
      "heading" : "4.3 Online Learning with Mini-batch",
      "text" : "In some conditions, use the classical heuristic in gradient descent algorithm, we may also improve the convergence speed of our algorithm by drawing µ > 1 training samples at each iteration instead of a single one. Let us denote by (Xt,1, yt,1), ..., (Xt,µ, yt,µ) the samples drawn at iteration\n11\nt. We can now replace lines 5 and 9 of Algorithm 4.1 and 4.2 by\nAt ← At−1 + µ ∑\ni=1\nyt,iXt,i,\nBt ← Bt−1 + µ ∑\ni=1\nXt,i ⊗Xt,i,\nct ← ct−1 + µ ∑\ni=1\nyt,i,\nDt ← Dt−1 + µ ∑\ni=1\nXt,i,\nLt ← Lt−1 + µ ∑\ni=1\n2mn ‖Xt,i‖2F .\n(15)\nBut in real applications, this batch method may not improve the convergence speed on the whole since the batch past information computation (Equation (15)) would occupy much of the time. The updating of Bt needs to do Kronecher product which spend much of the computing resource. If the computation cost of Equation (15) can be ignored or largely decreased, for example by parallel computing, the batch method would increase the convergence speed by a factor of µ."
    }, {
      "heading" : "5 Experimental Validation",
      "text" : ""
    }, {
      "heading" : "5.1 Dataset",
      "text" : "Experiments are conducted on a collected database. We downloaded about 20hours videos from Youku [21], with different programs and different languages. The start and end position of all the applause and laugh of the audio-tracks are manually labeled. The database includes 800 segments of each sound effect. Each segment is about 3-8s long and totally about 1hour data for each sound effect. All the audio recordings were converted to monaural wave format at a sampling frequency of 8kHz and quantized 16bits. Furthermore, the audio signals have been normalized, so that they have zero mean amplitude with unit variance in order to remove any factors related to the recording conditions."
    }, {
      "heading" : "5.2 Online Learning",
      "text" : "In this section, we conduct detailed experiments to demonstrate the characteristics and merits of the online learning for matrix classification problem. Five algorithms are compared: the traditional batch algorithm with exact APG algorithm (APG); the online learning algorithm with exact APG (OL APG); the online learning algorithm with inexact APG (OL IAPG); the online learning algorithm with exact APG and update Equation (15) (OL APG Batch); the online learning algorithm with inexact APG and update Equation (15) (OL IAPG Batch). All algorithms are run in Matlab on a personal computer with an Intel 3.40GHz dual-core central processing unit (CPU) and 2GB memory.\nFor this experiment, audio streams were windowed into a sequence of short-term frames (20 ms long) with non overlap. 13 dimensional MFCCs including energy are extracted, and adjacent 50 frames (one second) of MFCCs form the MFCCs matrix feature. The goal is to classify the matrices according to their labels. Two learning tasks are used to evaluate the performance of the online learning method, which are laugh/non-laugh segment classifier learning\n12\nand applause/non-applause segment classifier learning. For OL APG and OL APG Batch algorithms, the parameters in the stopping criteria (11) are set ε1 = 10 −8 and ε2 = 10 −8 or smaller, which are determined by empirical evidence that larger values would make the algorithm diverge. The regularization constant λ is anchored by the large explicit fixed step size L and the matrices involved, this can be seen from λ\nL in the line 3 in Algorithm 3.2, which means that in\npractice the parameter λ should be set adaptably with the step size L in the online process. But due to this variation of λ, the comparisons between the algorithms would not bring into effect. Hence in this work we use λ = 1 throughout.\nFig. 3 compares the five online algorithms. The proposed online algorithm draws samples from the entire training set. We use a logarithmic scale for the computation time. Fig. 3a shows the values of the target functions as functions of time. It can be seen that the online learning methods without batch or with small batch past information updating converge faster than the methods with large batch past information updating and reason for this has been explained in the last paragraph of Section 5.2. After online methods and batch methods converge, the two methods result in almost equal performance. Fig. 3(b)(d) shows the classification rates for different algorithms respectively. In accordance with the values of the target functions, the classification accuracies of online methods without or with small batch updating become stable quickly than that of methods with batch updating. Although the inexact algorithms process samples much fast with less resources than exact ones, they converge slowly."
    }, {
      "heading" : "5.3 Robustness",
      "text" : "This section is to assess the effectiveness of robust PCA extracted low-rank matrix features. Original features (MFCCs Matrix), corrupted with 0dB and -5dB white Gaussian noise (WGN SNR=5dB, 0dB, -5dB) and 10%, 30%, 50% random large errors (LE 10%, 30%, 50%), and parallelism robust PCA extracted features (rPCA) are compared. In the comparisons, the parameters in the stopping criteria (11) are set ε1 = 10 −6 and ε2 = 10 −6, which are determined\nby the same method as in Section 5.2. The regularization constant λ is set 1/ √ 50 which is a classical normalization factor according to [22]. The classification accuracy of the one second audio segments is used to evaluate the performance of the methods. Fig. 4 shows the performances of the methods with different matrix features under different noise conditions as the functions of the training time used in Algorithm 3.2. It can be seen that the original MFCCs matrix feature is not robust to noises, especially random large errors. If 10% of the elements of the MFCCs matrix feature are corrupted with random large errors, then generally there would be a decrease of 25% in audio segments classification accuracy, while for robust PCA extracted low-rank features, the decrease are 5% in average. For WGN, the robust PCA features also perform better than original features, although not so sharp as in the situation of large errors. The experiments show that the low-rank components are more robust to noises and errors than the original features.\nWe also compare our method with the state-of-the-art SVM classifier with long vector feature (650 dimension) obtained by vectorizing the matrix. The results are summarized in Table 1 and Table 2 for applause/non-applause and laugh/non-laugh classification respectively. The results show that the SVM become useless under 5dB wight noise and 10% large corruptions, while our methods still works. But for the low-rank component, the SVM performs better on some situations for which is due to the robustness of the features.\n13"
    }, {
      "heading" : "6 Conclusions",
      "text" : "In this work, we present a novel framework based on trace norm minimization for audio segment classification. The novel method unified feature extraction and pattern classification into the same framework. In this framework, robust PCA extracted low-rank component of original signal is more robust to corrupted noise and errors, especially to random large errors. We also introduced online learning algorithms for matrices classification tasks. We obtain the closed-form updating rules of the weight matrix and the bias. We derive the explicit form of the Lipschitz constant, which saves the computation burden in searching step size. Experiments show that even the percent of the original feature elements corrupted with random large errors is up to 50%, the performance of the robust PCA extracted features almost have no decrease. In future work, we plan to test this robust feature in other audio or speech processing related applications and extend robust PCA, even trace norm minimization related methods from matrices to the more general multi-way arrays (tensors). Some work related to learning methods are also worth considering, such that the alternating between minimization with respect to weight matrix and bias may results in fluctuation of target value (even in batch mode), thus optimization algorithm that minimization jointly on weight matrix and bias are required; for multi-classification problems with more classes, some hierarchy methods may be introduced to improve the classification accuracy."
    } ],
    "references" : [ {
      "title" : "Content analysis for audio classification and segmentation",
      "author" : [ "L. Lu" ],
      "venue" : "IEEE Transactions on Speech and Audio processing, vol. 10, no. 7, pp. 504-516",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Highlight sound effects detection in audio stream",
      "author" : [ "R. Cui", "L. Lu", "H.J. Zhung", "L.H. Cai" ],
      "venue" : "Proceedings of IEEE International Conference on Multimedia and Expo, pp. 37-40",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Audio based event detection for multimedia surveillance",
      "author" : [ "K.A. Pradeep", "C.M. Namunu", "S.K. Mohan" ],
      "venue" : "Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Audio signal feature extraction and classification using local discriminant bases",
      "author" : [ "K. Umapathy", "S. Krishnan", "R.K. Rao" ],
      "venue" : "IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, no. 1, pp. 1236-1246",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Feature analysis and selection for acoustic event detection",
      "author" : [ "X. Zhuang", "X. Zhou", "T.S. Huang", "M. Hasegawa-Johnson" ],
      "venue" : "Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 17-20",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Content-based audio classification and retrieval by support vector machines",
      "author" : [ "G. Guo", "S.Z. Li" ],
      "venue" : "IEEE Transactions on Neural Networks vol. 14, no. 1, pp. 209-215",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "A rank minimization heuristic with application to minimum order system approximation",
      "author" : [ "M. Fazel", "H. Hindi", "S.P. Boyd" ],
      "venue" : "Proceedings of the American Control Conference, pp. 4734-4739",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Maximum-margin matrix factorization",
      "author" : [ "N. Srebro", "J.D.M. Rennie", "T.S. Jaakkola" ],
      "venue" : "Proceedings of Advances in Neural Information Processing Systems, pp. 1329-1336",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Convex multi-task feature learning",
      "author" : [ "A. Argyriou", "T. Evgeniou", "M. Pontil" ],
      "venue" : "Machine Learning, vol. 73, no. 3, pp. 243-272",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Robust principal component analysis: Exact recovery of corrupted low-rank matrices via convex optimization",
      "author" : [ "J. Wright", "A. Ganesh", "S. Rao", "Y. Peng", "Y. Ma" ],
      "venue" : "Proceedings of Advances in Neural Information Processing Systems",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "The augmented lagrange multiplier method for exact recovery of corrupted low-rank matrices",
      "author" : [ "Z. Lin", "M. Chen", "L. Wu", "Y. Ma" ],
      "venue" : "arXiv:1009.5055",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Classifying matrices with a spectral regularization",
      "author" : [ "R. Tomioka", "K. Aihara" ],
      "venue" : "24th International Conference on Machine Learning, pp. 895-902",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "An accelerated proximal gradient algorithm for nuclear norm regularized least squares problems",
      "author" : [ "K. Toh", "S. Yun" ],
      "venue" : "Pacific J. Optim., vol. 6, pp. 615-640",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "An accelerated gradient method for trace norm minimization",
      "author" : [ "S. Ji", "J. Ye" ],
      "venue" : "26th International Conference on Machine Learning, pp. 457-464",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "An implementable proximal point algorithmic framework for nuclear norm minimization",
      "author" : [ "Y.J. Liu", "D. Sun", "K.C. Toh" ],
      "venue" : "Mathematical Programming, pp. 1-38",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Principal Component Analysis",
      "author" : [ "I.T. Jolliffe" ],
      "venue" : "Springer Series in Statistics, Berlin: Springer",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1986
    }, {
      "title" : "Exact matrix completion via convex optimization",
      "author" : [ "E.J. Candes", "B. Recht" ],
      "venue" : "Technical Report, UCLA Computational and Applied Math",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Convex optimization",
      "author" : [ "S.P. Boyd", "L. Vandenberghe" ],
      "venue" : "Cambridge Univ Pr",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "A method of solving a convex programming problem with convergence rate O( 1 k2  )",
      "author" : [ "Y. Nesterov" ],
      "venue" : "Soviet Mathematics Doklady, vol. 27, no. 2, pp. 372-376",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 1983
    }, {
      "title" : "Smooth minimization of non-smooth functions",
      "author" : [ "Y. Nesterov" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "1 Introduction Audio feature extraction and classification methods have been studied by many researchers over the years [1, 2, 3, 4].",
      "startOffset" : 120,
      "endOffset" : 132
    }, {
      "referenceID" : 1,
      "context" : "1 Introduction Audio feature extraction and classification methods have been studied by many researchers over the years [1, 2, 3, 4].",
      "startOffset" : 120,
      "endOffset" : 132
    }, {
      "referenceID" : 2,
      "context" : "1 Introduction Audio feature extraction and classification methods have been studied by many researchers over the years [1, 2, 3, 4].",
      "startOffset" : 120,
      "endOffset" : 132
    }, {
      "referenceID" : 3,
      "context" : "1 Introduction Audio feature extraction and classification methods have been studied by many researchers over the years [1, 2, 3, 4].",
      "startOffset" : 120,
      "endOffset" : 132
    }, {
      "referenceID" : 3,
      "context" : "Feature commonly exploited for audio classification can be roughly classified into time domain features, transformation domain features, time-transformation domain features or their combinations [4, 5].",
      "startOffset" : 195,
      "endOffset" : 201
    }, {
      "referenceID" : 4,
      "context" : "Feature commonly exploited for audio classification can be roughly classified into time domain features, transformation domain features, time-transformation domain features or their combinations [4, 5].",
      "startOffset" : 195,
      "endOffset" : 201
    }, {
      "referenceID" : 6,
      "context" : "is a principled approach to learn low-rank matrices through convex optimization problems [7].",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 7,
      "context" : "These similar problems arise in many machine learning tasks such as matrix completion [8], multi-task learning [9], robust principle component antilysis (robust PCA) [10, 11], and matrix classification [12].",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 8,
      "context" : "These similar problems arise in many machine learning tasks such as matrix completion [8], multi-task learning [9], robust principle component antilysis (robust PCA) [10, 11], and matrix classification [12].",
      "startOffset" : 111,
      "endOffset" : 114
    }, {
      "referenceID" : 9,
      "context" : "These similar problems arise in many machine learning tasks such as matrix completion [8], multi-task learning [9], robust principle component antilysis (robust PCA) [10, 11], and matrix classification [12].",
      "startOffset" : 166,
      "endOffset" : 174
    }, {
      "referenceID" : 10,
      "context" : "These similar problems arise in many machine learning tasks such as matrix completion [8], multi-task learning [9], robust principle component antilysis (robust PCA) [10, 11], and matrix classification [12].",
      "startOffset" : 166,
      "endOffset" : 174
    }, {
      "referenceID" : 11,
      "context" : "These similar problems arise in many machine learning tasks such as matrix completion [8], multi-task learning [9], robust principle component antilysis (robust PCA) [10, 11], and matrix classification [12].",
      "startOffset" : 202,
      "endOffset" : 206
    }, {
      "referenceID" : 3,
      "context" : "[4, 5, 6].",
      "startOffset" : 0,
      "endOffset" : 9
    }, {
      "referenceID" : 4,
      "context" : "[4, 5, 6].",
      "startOffset" : 0,
      "endOffset" : 9
    }, {
      "referenceID" : 5,
      "context" : "[4, 5, 6].",
      "startOffset" : 0,
      "endOffset" : 9
    }, {
      "referenceID" : 11,
      "context" : "After extraction of the robust low-rank matrix feature, the regularization framework based matrix classification approach proposed by Tomioka and Aihara in [12] is used to predict the label.",
      "startOffset" : 156,
      "endOffset" : 160
    }, {
      "referenceID" : 11,
      "context" : "The problem of matrix classification (MC) with spectral regularization was first proposed by Tomioka and Aihara in [12].",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 6,
      "context" : "For the matrix rank minimization is NP-hard in general due to the combinatorial nature of the rank function, a commonly-used convex relaxation of the rank function is the trace norm (nuclear norm) [7], defined as the sum of the singular values of the matrix.",
      "startOffset" : 197,
      "endOffset" : 200
    }, {
      "referenceID" : 12,
      "context" : "Recent related researches are not focused on matrix classification directly, but rather on general trace norm minimization problem [13, 14, 15].",
      "startOffset" : 131,
      "endOffset" : 143
    }, {
      "referenceID" : 13,
      "context" : "Recent related researches are not focused on matrix classification directly, but rather on general trace norm minimization problem [13, 14, 15].",
      "startOffset" : 131,
      "endOffset" : 143
    }, {
      "referenceID" : 14,
      "context" : "Recent related researches are not focused on matrix classification directly, but rather on general trace norm minimization problem [13, 14, 15].",
      "startOffset" : 131,
      "endOffset" : 143
    }, {
      "referenceID" : 12,
      "context" : "In these methods, most are iterative batch procedures [13, 14, 15], accessing the whole training set at each iteration in order to minimize a weighted sum of a cost function and the trace norm.",
      "startOffset" : 54,
      "endOffset" : 66
    }, {
      "referenceID" : 13,
      "context" : "In these methods, most are iterative batch procedures [13, 14, 15], accessing the whole training set at each iteration in order to minimize a weighted sum of a cost function and the trace norm.",
      "startOffset" : 54,
      "endOffset" : 66
    }, {
      "referenceID" : 14,
      "context" : "In these methods, most are iterative batch procedures [13, 14, 15], accessing the whole training set at each iteration in order to minimize a weighted sum of a cost function and the trace norm.",
      "startOffset" : 54,
      "endOffset" : 66
    }, {
      "referenceID" : 12,
      "context" : "We transform the general batch-mode accelerated proximal gradient (APG) [13, 14] method for trace norm minimization to the online learning framework.",
      "startOffset" : 72,
      "endOffset" : 80
    }, {
      "referenceID" : 13,
      "context" : "We transform the general batch-mode accelerated proximal gradient (APG) [13, 14] method for trace norm minimization to the online learning framework.",
      "startOffset" : 72,
      "endOffset" : 80
    }, {
      "referenceID" : 12,
      "context" : "In addition, as a special case of general convex optimization problem, we derived the closed-form of the Lipschitz constant, hence the step size estimation [13, 14] of the general APG method was omitted in our approach.",
      "startOffset" : 156,
      "endOffset" : 164
    }, {
      "referenceID" : 13,
      "context" : "In addition, as a special case of general convex optimization problem, we derived the closed-form of the Lipschitz constant, hence the step size estimation [13, 14] of the general APG method was omitted in our approach.",
      "startOffset" : 156,
      "endOffset" : 164
    }, {
      "referenceID" : 1,
      "context" : "2 Low-Rank Matrix Representation Features Over the past decades, a lot work has been done on audio and speech features for audio and speech processing [2, 3, 5].",
      "startOffset" : 151,
      "endOffset" : 160
    }, {
      "referenceID" : 2,
      "context" : "2 Low-Rank Matrix Representation Features Over the past decades, a lot work has been done on audio and speech features for audio and speech processing [2, 3, 5].",
      "startOffset" : 151,
      "endOffset" : 160
    }, {
      "referenceID" : 4,
      "context" : "2 Low-Rank Matrix Representation Features Over the past decades, a lot work has been done on audio and speech features for audio and speech processing [2, 3, 5].",
      "startOffset" : 151,
      "endOffset" : 160
    }, {
      "referenceID" : 15,
      "context" : "For this problem, PCA is a suitable approach that it can find the low-dimensional approximating subspace by forming a lowrank approximation to the data matrix [16].",
      "startOffset" : 159,
      "endOffset" : 163
    }, {
      "referenceID" : 10,
      "context" : "However, it breaks down under large corruption, even if that corruption affects only a very few of the observation which is often encountered in practice [11].",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 9,
      "context" : "This optimization is refereed to as robust PCA in [10] for its ability to exactly recover underlying low-rank structure in data even in the presence of large errors or outliers.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 10,
      "context" : "In order to solve Equation (2), several algorithms have been proposed, among which the augmented Lagrange multiplier method is the most efficient and accurate at present [11].",
      "startOffset" : 170,
      "endOffset" : 174
    }, {
      "referenceID" : 10,
      "context" : "[11] identify the problem as",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "Two ALM algorithms to solve the above formulation are proposed in [11].",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 10,
      "context" : "The Sε[·] is the soft-thresholding operator introduced in [11].",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 11,
      "context" : "1 Notation and Problem Statement Having extracted robust matrix representation features, the linear matrix classification approach based on trace norm regularization framework proposed in [12] is used to classify them.",
      "startOffset" : 188,
      "endOffset" : 192
    }, {
      "referenceID" : 12,
      "context" : "2 APG Method for Matrix Classification Recently Toh and Yun [13], Ji and Ye [14], and Liu et al.",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 13,
      "context" : "2 APG Method for Matrix Classification Recently Toh and Yun [13], Ji and Ye [14], and Liu et al.",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 14,
      "context" : "[15] independently proposed similar algorithms that converge as O( 1 k2 ) for problem (5) by using APG, where k is the iteration counter.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "Since fs(W, b) in this work is a composition of smooth convex function with an affine mapping, hence it is convex and smooth [18].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 18,
      "context" : "Based on the the work of Nesterov [19, 20], Toh and Yun [13], Ji and Ye [14], and Liu et al.",
      "startOffset" : 34,
      "endOffset" : 42
    }, {
      "referenceID" : 19,
      "context" : "Based on the the work of Nesterov [19, 20], Toh and Yun [13], Ji and Ye [14], and Liu et al.",
      "startOffset" : 34,
      "endOffset" : 42
    }, {
      "referenceID" : 12,
      "context" : "Based on the the work of Nesterov [19, 20], Toh and Yun [13], Ji and Ye [14], and Liu et al.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 13,
      "context" : "Based on the the work of Nesterov [19, 20], Toh and Yun [13], Ji and Ye [14], and Liu et al.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 14,
      "context" : "[15] showed that setting Zk = Wk + tk−1−1 tk (Wk −Wk−1) for a sequence tk satisfying tk+1− tk+1 ≤ t2k results in a convergence rate of O( 1 k2 ).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "Due to Lemma 1, the estimation of step size tk in general APG [13, 14, 15] is omitted, for we have explicit Lipschitz constant.",
      "startOffset" : 62,
      "endOffset" : 74
    }, {
      "referenceID" : 13,
      "context" : "Due to Lemma 1, the estimation of step size tk in general APG [13, 14, 15] is omitted, for we have explicit Lipschitz constant.",
      "startOffset" : 62,
      "endOffset" : 74
    }, {
      "referenceID" : 14,
      "context" : "Due to Lemma 1, the estimation of step size tk in general APG [13, 14, 15] is omitted, for we have explicit Lipschitz constant.",
      "startOffset" : 62,
      "endOffset" : 74
    }, {
      "referenceID" : 10,
      "context" : "2 is the soft-thresholding operator introduced in [11]: Sε[x] .",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 12,
      "context" : "The general APG [13, 14, 15] algorithms only provide the methods for learning weight matrices, do not give out the bias updating rules.",
      "startOffset" : 16,
      "endOffset" : 28
    }, {
      "referenceID" : 13,
      "context" : "The general APG [13, 14, 15] algorithms only provide the methods for learning weight matrices, do not give out the bias updating rules.",
      "startOffset" : 16,
      "endOffset" : 28
    }, {
      "referenceID" : 14,
      "context" : "The general APG [13, 14, 15] algorithms only provide the methods for learning weight matrices, do not give out the bias updating rules.",
      "startOffset" : 16,
      "endOffset" : 28
    }, {
      "referenceID" : 12,
      "context" : "3 Determination of Lipschitz Constant As a special case of general convex optimization problem, we derived the closed-form of the Lipschitz constant, hence the step size estimation [13, 14] of the general APG method was omitted in all our approach.",
      "startOffset" : 181,
      "endOffset" : 189
    }, {
      "referenceID" : 13,
      "context" : "3 Determination of Lipschitz Constant As a special case of general convex optimization problem, we derived the closed-form of the Lipschitz constant, hence the step size estimation [13, 14] of the general APG method was omitted in all our approach.",
      "startOffset" : 181,
      "endOffset" : 189
    } ],
    "year" : 2011,
    "abstractText" : "In this paper, a novel framework based on trace norm minimization for audio segment is proposed. In this framework, both the feature extraction and classification are obtained by solving corresponding convex optimization problem with trace norm regularization. For feature extraction, robust principle component analysis (robust PCA) via minimization a combination of the nuclear norm and the l1-norm is used to extract low-rank features which are robust to white noise and gross corruption for audio segments. These low-rank features are fed to a linear classifier where the weight and bias are learned by solving similar trace norm constrained problems. For this classifier, most methods find the weight and bias in batch-mode learning, which makes them inefficient for large-scale problems. In this paper, we propose an online framework using accelerated proximal gradient method. This framework has a main advantage in memory cost. In addition, as a result of the regularization formulation of matrix classification, the Lipschitz constant was given explicitly, and hence the step size estimation of general proximal gradient method was omitted in our approach. Experiments on real data sets for laugh/non-laugh and applause/non-applause classification indicate that this novel framework is effective and noise robust.",
    "creator" : "LaTeX with hyperref package"
  }
}