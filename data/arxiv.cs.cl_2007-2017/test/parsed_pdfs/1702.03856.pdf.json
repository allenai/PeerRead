{
  "name" : "1702.03856.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Towards speech-to-text translation without speech recognition",
    "authors" : [ "Sameer Bansal", "Herman Kamper", "Adam Lopez", "Sharon Goldwater" ],
    "emails" : [ "alopez}@inf.ed.ac.uk,", "kamperh@gmail.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Typical speech-to-text translation systems pipeline automatic speech recognition (ASR) and machine translation (MT) (Waibel and Fugen, 2008). But high-quality ASR requires hundreds of hours of transcribed audio, while high-quality MT requires millions of words of parallel text—resources available for only a tiny fraction of the world’s estimated 7,000 languages (Besacier et al., 2014). Nevertheless, there are important low-resource settings in which even limited speech translation would be of immense value: documentation of endangered languages, which often have no writing system (Besacier et al., 2006; Martin et al., 2015); and crisis response, for which text applications have proven useful (Munro, 2010), but only help literate populations. In these settings, target translations may be available. For example, ad hoc translations may be\ncollected in support of relief operations. Can we do anything at all with this data?\nIn this exploratory study, we present a speechto-text translation system that learns directly from source audio and target text pairs, and does not require intermediate ASR or MT. Our work complements several lines of related recent work. For example, Duong et al. (2016) and Anastasopoulos et al. (2016) presented models that align audio to translated text, but neither used these models to try to translate new utterances (in fact, the latter model cannot make such predictions). Berard et al. (2016) did develop a direct speech to translation system, but presented results only on a corpus of synthetic audio with a small number of speakers. Finally, Adams et al. (2016a; 2016b) targeted the same low-resource speech-to-translation task, but instead of working with audio, they started from word or phoneme lattices. In principle these could be produced in an unsupervised or minimallysupervised way, but in practice they used supervised ASR/phone recognition. Additionally, their evaluation focused on phone error rate rather than translation. In contrast to these approaches, our method can make translation predictions for audio input not seen during training, and we evaluate it on real multi-speaker speech data.\nOur simple system (§2) builds on unsupervised speech processing (Versteegh et al., 2015; Lee et al., 2015; Kamper et al., 2016b), and in particular on unsupervised term discovery (UTD), which creates hard clusters of repeated word-like units in raw speech (Park and Glass, 2008; Jansen and Van Durme, 2011). The clusters do not account for all of the audio, but we can use them to simulate a partial, noisy transcription, or pseudotext, which we pair with translations to learn a bag-ofwords translation model. We test our system on the CALLHOME Spanish-English speech translation corpus (Post et al., 2013), a noisy multi-speaker corpus of telephone calls in a variety of Spanish diar X\niv :1\n70 2.\n03 85\n6v 1\n[ cs\n.C L\n] 1\n3 Fe\nb 20\n17\nalects (§3). Using the Spanish speech as the source and English text translations as the target, we identify several challenges in the use of UTD, including low coverage of audio and difficulty in crossspeaker clustering (§4). Despite these difficulties, we demonstrate that the system learns to translate some content words (§5)."
    }, {
      "heading" : "2 From unsupervised term discovery to direct speech-to-text translation",
      "text" : "For UTD we use the Zero Resource Toolkit (ZRTools; Jansen and Van Durme, 2011).1 ZRTools uses dynamic time warping (DTW) to discover pairs of acoustically similar audio segments, and then uses graph clustering on overlapping pairs to form a hard clustering of the discovered segments. Replacing each discovered segment with its unique cluster label, or pseudoterm, gives us a partial, noisy transcription, or pseudotext (Fig. 1).\nIn creating a translation model from this data, we face a difficulty that does not arise in the parallel texts that are normally used to train translation models: the pseudotext does not represent all of the source words, since the discovered segments do not cover the full audio (Fig. 1). Hence we must not assume that our MT model can completely recover the translation of a test sentence. In these conditions, the language modeling and ordering assumptions of most MT models are unwarranted, so we instead use a simple bag-of-words translation model based only on co-occurrence: IBM Model 1 (Brown et al., 1993) with a Dirichlet prior over translation distributions, as learned by fast align (Dyer et al., 2013).2 In particular, for each pseudoterm, we learn a translation distribution over possible target words. To translate a pseudoterm in test data, we simply return its highest-probability translation (or translations, as discussed in §5).\nThis setup implies that in order to translate, we must apply UTD on both the training and test audio. Using additional (not only training) audio in UTD increases the likelihood of discovering more clusters. We therefore generate pseudotext for the combined audio, train the MT model on the pseudotext of the training audio, and apply it to the pseudotext of the test data. This is fair since the UTD has access to only the audio.3\n1https://github.com/arenjansen/ZRTools 2We disable diagonal preference to simulate Model 1. 3This is the simplest approach for our proof-of-concept sys-"
    }, {
      "heading" : "3 Dataset",
      "text" : "Although we did not have access to a low-resource dataset, there is a corpus of noisy multi-speaker speech that simulates many of the conditions we expect to find in our motivating applications: the CALLHOME Spanish–English speech translation dataset (LDC2014T23; Post el al., 2013).4 We ran UTD over all 104 telephone calls, which pair 11 hours of audio with Spanish transcripts and their crowdsourced English translations. The transcripts contain 168,195 Spanish word tokens (10,674 types), and the translations contain 159,777 English word tokens (6,723 types). Though our system does not require Spanish transcripts, we use them to evaluate UTD and to simulate a perfect UTD system, called the oracle.\nFor MT training, we use the pseudotext and translations of 50 calls, and we filter out stopwords in the\ntem. In a more realistic setup, we could use the training audio to construct a consensus representation of each pseudoterm (Petitjean et al., 2011; Anastasopoulos et al., 2016), then use DTW to identify its occurrences in test data to translate.\n4We did not use the Fisher portion of the corpus.\ntranslations with NLTK (Bird et al., 2009).5 Since UTD is better at matching patterns from the same speaker (§4.2), we created two types of 90/10% train/test split: at the call level and at the utterance level. For the latter, 90% of the utterances are randomly chosen for the training set (independent of which call they occur in), and the rest go in the test set. Hence at the utterance level, but not the call level, some speakers are included in both training and test data. Although the utterance-level split is optimistic, it allows us to investigate how multiple speakers affect system performance. In either case, the oracle has about 38k Spanish tokens to train on."
    }, {
      "heading" : "4 Analysis of challenges from UTD",
      "text" : "Our system relies on the pseudotext produced by ZRTools (the only freely available UTD system we are aware of), which presents several challenges for MT. We used the default ZRTools parameters, and it might be possible to tune them to our task, but we leave this to future work."
    }, {
      "heading" : "4.1 Assigning wrong words to a cluster",
      "text" : "Since UTD is unsupervised, the discovered clusters are noisy. Fig. 1 shows an example of an incorrect match between the acoustically similar “qué tal vas con” and “te trabajo y” in utterances B and C, leading to a common assignment to c2. Such inconsistencies in turn affect the translation distribution conditioned on c2.\nMany of these errors are due to cross-speaker matches, which are known to be more challenging for UTD (Carlin et al., 2011; Kamper et al., 2015; Bansal et al., 2017). Most matches in our corpus are across calls, yet these are also the least accurate (Table 1). Within-utterance matches, which are always from the same speaker, are the most reliable, but make up the smallest proportion of the discovered pairs. Within-call matches fall in between. Overall, average cluster purity is only 34%, meaning that 66% of discovered patterns do not match the most frequent type in their cluster."
    }, {
      "heading" : "4.2 Splitting words across different clusters",
      "text" : "Although most UTD matches are across speakers, recall of cross-speaker matches is lower than for same-speaker matches. As a result, the same word from different speakers often appears in multiple clusters, preventing the model from learning good translations. ZRTools discovers 15,089 clusters in\n5http://www.nltk.org/\nour data, though there are only 10,674 word types. Only 1,614 of the clusters map one-to-one to a unique word type, while a many-to-one mapping of the rest covers only 1,819 gold types (leaving 7,241 gold types with no corresponding cluster).\nFragmentation of words across clusters renders pseudoterms impossible to translate when they appear only in test and not in training. Table 2 shows that these pseudotext out-of-vocabulary (OOV) words are frequent, especially in the call-level split. This reflects differences in acoustic patterns of different speakers, but also in their vocabulary — even the oracle OOV rate is higher in the call-level split."
    }, {
      "heading" : "4.3 UTD is sparse, giving low coverage",
      "text" : "UTD is most reliable on long and frequentlyrepeated patterns, so many spoken words are not represented in the pseudotext, as in Fig. 1. We found that the patterns discovered by ZRTools match only 28% of the audio. This low coverage reduces training data size, affects alignment quality, and adversely affects translation, which is only possible when pseudoterms are present. For almost half the utterances, UTD fails to produce any pseudoterm at all."
    }, {
      "heading" : "5 Speech translation experiments",
      "text" : "We evaluate our system by comparing its output to the English translations on the test data. Since it translates only a handful of words in each sentence, BLEU, which measures accuracy of word sequences, is an inappropriate measure of accuracy.6 Instead we compute precision and recall over\n6BLEU scores for supervised speech translation systems trained on our data can be found in Kumar et al. (2014).\nthe content words in the translation. We allow the system to guess K words per test pseudoterm, so for each utterance, we compute the number of correct predictions as corr@K = |pred@K ∩ gold|, where pred@K is the multiset of words predicted using K predictions per pseudoterm and gold is the multiset of content words in the reference translation. For utterances where the reference translation has no content words, we use stop words. The utterance-level scores are then used to compute corpus-level Precision@K and Recall@K.\nTable 4 and Fig. 2 show that even the oracle has mediocre precision and recall, indicating the difficulties of training an MT system using only bag-of-content-words on a relatively small corpus. Splitting the data by utterance works somewhat better, since training and test share more vocabulary.\nTable 4 and Fig. 2 also show a large gap between the oracle and our system. This is not surprising given the problems with the UTD output discussed in Section 4. In fact, it is encouraging given the small number of discovered terms and the low cluster purity that our system can still correctly translate some words (Table 3). These results are a positive proof of concept, showing that it is possible to discover and translate keywords from audio data even with no ASR or MT system. Nevertheless, UTD quality is clearly a limitation, especially\nfor the more realistic by-call data split."
    }, {
      "heading" : "6 Conclusions and future work",
      "text" : "Our results show that it is possible to build a speech translation system using only source-language audio paired with target-language text, which may be useful in many situations where no other speech technology is available. Our analysis also points to several possible improvements. Poor cross-speaker matches and low audio coverage prevent our system from achieving a high recall, suggesting the of use speech features that are effective in multi-\nspeaker settings (Kamper et al., 2015; Kamper et al., 2016a) and speaker normalization (Zeghidour et al., 2016). Finally, Bansal et al. (2017) recently showed that UTD can be improved using the translations themselves as a source of information, which suggests joint learning as an attractive area for future work.\nOn the other hand, poor precision is most likely due to the simplicity of our MT model, and designing a model whose assumptions match our data conditions is an important direction for future work, which may combine our approach with insight from recent, quite different audio-to-translation models (Duong et al., 2016; Anastasopoulos et al., 2016; Adams et al., 2016a; Adams et al., 2016b; Berard et al., 2016). Parameter-sharing using word and acoustic embeddings would allow us to make predictions for OOV pseudoterms by using the nearest in-vocabulary pseudoterm instead."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank David Chiang and Antonios Anastasopoulos for sharing alignments of the CALLHOME speech and transcripts; Aren Jansen for assistance with ZRTools; and Marco Damonte, Federico Fancellu, Sorcha Gilroy, Ida Szubert, Nikolay Bogoychev, Naomi Saphra, Joana Ribeiro and Clara Vania for comments on previous drafts. This work was supported in part by a James S McDonnell Foundation Scholar Award and a Google faculty research award."
    } ],
    "references" : [ {
      "title" : "Learning a translation model from word lattices",
      "author" : [ "Adams et al.2016a] Oliver Adams", "Graham Neubig", "Trevor Cohn", "Steven Bird" ],
      "venue" : "In Proc. Interspeech",
      "citeRegEx" : "Adams et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Adams et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning a lexicon and translation model from phoneme lattices",
      "author" : [ "Adams et al.2016b] Oliver Adams", "Graham Neubig", "Trevor Cohn", "Steven Bird", "Quoc Truong Do", "Satoshi Nakamura" ],
      "venue" : "In Proc. EMNLP",
      "citeRegEx" : "Adams et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Adams et al\\.",
      "year" : 2016
    }, {
      "title" : "An unsupervised probability model for speech-to-translation alignment of low-resource languages",
      "author" : [ "David Chiang", "Long Duong" ],
      "venue" : "In Proc. EMNLP",
      "citeRegEx" : "Anastasopoulos et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Anastasopoulos et al\\.",
      "year" : 2016
    }, {
      "title" : "Weakly supervised spoken term discovery using cross-lingual side information",
      "author" : [ "Bansal et al.2017] Sameer Bansal", "Herman Kamper", "Sharon Goldwater", "Adam Lopez" ],
      "venue" : "In Proc. ICASSP",
      "citeRegEx" : "Bansal et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Bansal et al\\.",
      "year" : 2017
    }, {
      "title" : "Listen and translate: A proof of concept for end-to-end speech-to-text translation",
      "author" : [ "Olivier Pietquin", "Christophe Servan", "Laurent Besacier" ],
      "venue" : "In NIPS Workshop on End-to-end Learning for Speech and Audio Process-",
      "citeRegEx" : "Berard et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Berard et al\\.",
      "year" : 2016
    }, {
      "title" : "Towards speech translation of non written languages",
      "author" : [ "Bowen Zhou", "Yuqing Gao" ],
      "venue" : "In Proc. SLT",
      "citeRegEx" : "Besacier et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Besacier et al\\.",
      "year" : 2006
    }, {
      "title" : "Automatic speech recognition for under-resourced languages: A survey",
      "author" : [ "Etienne Barnard", "Alexey Karpov", "Tanja Schultz" ],
      "venue" : "Speech Communication,",
      "citeRegEx" : "Besacier et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Besacier et al\\.",
      "year" : 2014
    }, {
      "title" : "Natural language processing with Python. O’Reilly Media",
      "author" : [ "Bird et al.2009] Steven Bird", "Ewan Klein", "Edward Loper" ],
      "venue" : null,
      "citeRegEx" : "Bird et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bird et al\\.",
      "year" : 2009
    }, {
      "title" : "The mathematics of statistical machine translation: Parameter estimation",
      "author" : [ "Brown et al.1993] Peter F Brown", "Vincent J Della Pietra", "Stephen A Della Pietra", "Robert L Mercer" ],
      "venue" : null,
      "citeRegEx" : "Brown et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Brown et al\\.",
      "year" : 1993
    }, {
      "title" : "Rapid evaluation of speech representations for spoken term discovery",
      "author" : [ "Samuel Thomas", "Aren Jansen", "Hynek Hermansky" ],
      "venue" : "In Proc. Interspeech",
      "citeRegEx" : "Carlin et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Carlin et al\\.",
      "year" : 2011
    }, {
      "title" : "An attentional model for speech translation without transcription",
      "author" : [ "Duong et al.2016] Long Duong", "Antonios Anastasopoulos", "David Chiang", "Steven Bird", "Trevor Cohn" ],
      "venue" : "In Proc. NAACL HLT",
      "citeRegEx" : "Duong et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Duong et al\\.",
      "year" : 2016
    }, {
      "title" : "A simple, fast, and effective reparameterization of IBM model 2",
      "author" : [ "Dyer et al.2013] Chris Dyer", "Victor Chahuneau", "Noah A Smith" ],
      "venue" : "In Proc. ACL",
      "citeRegEx" : "Dyer et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Dyer et al\\.",
      "year" : 2013
    }, {
      "title" : "Efficient spoken term discovery using randomized algorithms",
      "author" : [ "Jansen", "Van Durme2011] Aren Jansen", "Benjamin Van Durme" ],
      "venue" : "In Proc. ASRU",
      "citeRegEx" : "Jansen et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Jansen et al\\.",
      "year" : 2011
    }, {
      "title" : "Unsupervised neural network based feature extraction using weak top-down constraints",
      "author" : [ "Kamper et al.2015] Herman Kamper", "Micha Elsner", "Aren Jansen", "Sharon Goldwater" ],
      "venue" : "In Proc. ICASSP",
      "citeRegEx" : "Kamper et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kamper et al\\.",
      "year" : 2015
    }, {
      "title" : "2016a. A segmental framework for fully-unsupervised largevocabulary speech recognition",
      "author" : [ "Aren Jansen", "Sharon Goldwater" ],
      "venue" : "arXiv preprint arXiv:1606.06950",
      "citeRegEx" : "Kamper et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kamper et al\\.",
      "year" : 2016
    }, {
      "title" : "Unsupervised word segmentation and lexicon discovery using acoustic word embeddings",
      "author" : [ "Aren Jansen", "Sharon Goldwater" ],
      "venue" : null,
      "citeRegEx" : "Kamper et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kamper et al\\.",
      "year" : 2016
    }, {
      "title" : "Some insights from translating conversational telephone speech",
      "author" : [ "Kumar et al.2014] Gaurav Kumar", "Matt Post", "Daniel Povey", "Sanjeev Khudanpur" ],
      "venue" : "In Proc. ICASSP",
      "citeRegEx" : "Kumar et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kumar et al\\.",
      "year" : 2014
    }, {
      "title" : "Unsupervised lexicon discovery from acoustic input",
      "author" : [ "Lee et al.2015] Chia-ying Lee", "T O’Donnell", "James Glass" ],
      "venue" : null,
      "citeRegEx" : "Lee et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2015
    }, {
      "title" : "Utterance classification in speech-tospeech translation for zero-resource languages in the hospital administration",
      "author" : [ "Martin et al.2015] Lara J Martin", "Andrew Wilkinson", "Sai Sumanth Miryala", "Vivian Robison", "Alan W Black" ],
      "venue" : null,
      "citeRegEx" : "Martin et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Martin et al\\.",
      "year" : 2015
    }, {
      "title" : "Crowdsourced translation for emergency response in Haiti: the global collaboration of local knowledge",
      "author" : [ "Robert Munro" ],
      "venue" : "In AMTA Workshop on Collaborative Crowdsourcing for Translation",
      "citeRegEx" : "Munro.,? \\Q2010\\E",
      "shortCiteRegEx" : "Munro.",
      "year" : 2010
    }, {
      "title" : "Unsupervised pattern discovery in speech",
      "author" : [ "Park", "Glass2008] Alex S Park", "James Glass" ],
      "venue" : "IEEE Trans. Audio, Speech, Language Process.,",
      "citeRegEx" : "Park et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Park et al\\.",
      "year" : 2008
    }, {
      "title" : "A global averaging method for dynamic time warping, with applications to clustering",
      "author" : [ "Alain Ketterlin", "Pierre Gançarski" ],
      "venue" : "Pattern Recognition,",
      "citeRegEx" : "Petitjean et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Petitjean et al\\.",
      "year" : 2011
    }, {
      "title" : "Improved speechto-text translation with the Fisher and Callhome Spanish–English speech translation corpus",
      "author" : [ "Post et al.2013] Matt Post", "Gaurav Kumar", "Adam Lopez", "Damianos Karakos", "Chris Callison-Burch", "Sanjeev Khudanpur" ],
      "venue" : null,
      "citeRegEx" : "Post et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Post et al\\.",
      "year" : 2013
    }, {
      "title" : "Spoken language translation",
      "author" : [ "Waibel", "Fugen2008] Alex Waibel", "Christian Fugen" ],
      "venue" : "IEEE Signal Processing Magazine,",
      "citeRegEx" : "Waibel et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Waibel et al\\.",
      "year" : 2008
    }, {
      "title" : "Joint learning of speaker and phonetic similarities with Siamese networks",
      "author" : [ "Gabriel Synnaeve", "Nicolas Usunier", "Emmanuel Dupoux" ],
      "venue" : "In Proc. Interspeech",
      "citeRegEx" : "Zeghidour et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zeghidour et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "But high-quality ASR requires hundreds of hours of transcribed audio, while high-quality MT requires millions of words of parallel text—resources available for only a tiny fraction of the world’s estimated 7,000 languages (Besacier et al., 2014).",
      "startOffset" : 222,
      "endOffset" : 245
    }, {
      "referenceID" : 5,
      "context" : "Nevertheless, there are important low-resource settings in which even limited speech translation would be of immense value: documentation of endangered languages, which often have no writing system (Besacier et al., 2006; Martin et al., 2015); and crisis response, for which text applications have proven useful (Munro, 2010), but only help literate populations.",
      "startOffset" : 198,
      "endOffset" : 242
    }, {
      "referenceID" : 18,
      "context" : "Nevertheless, there are important low-resource settings in which even limited speech translation would be of immense value: documentation of endangered languages, which often have no writing system (Besacier et al., 2006; Martin et al., 2015); and crisis response, for which text applications have proven useful (Munro, 2010), but only help literate populations.",
      "startOffset" : 198,
      "endOffset" : 242
    }, {
      "referenceID" : 19,
      "context" : ", 2015); and crisis response, for which text applications have proven useful (Munro, 2010), but only help literate populations.",
      "startOffset" : 77,
      "endOffset" : 90
    }, {
      "referenceID" : 6,
      "context" : "For example, Duong et al. (2016) and Anastasopoulos et al.",
      "startOffset" : 13,
      "endOffset" : 33
    }, {
      "referenceID" : 0,
      "context" : "(2016) and Anastasopoulos et al. (2016) presented models that align audio to translated text, but neither used these models to try to translate new utterances (in fact, the latter model cannot make such predictions).",
      "startOffset" : 11,
      "endOffset" : 40
    }, {
      "referenceID" : 0,
      "context" : "(2016) and Anastasopoulos et al. (2016) presented models that align audio to translated text, but neither used these models to try to translate new utterances (in fact, the latter model cannot make such predictions). Berard et al. (2016) did develop a direct speech to translation system, but presented results only on a corpus of synthetic audio with a small number of speakers.",
      "startOffset" : 11,
      "endOffset" : 238
    }, {
      "referenceID" : 17,
      "context" : "Our simple system (§2) builds on unsupervised speech processing (Versteegh et al., 2015; Lee et al., 2015; Kamper et al., 2016b), and in particular on unsupervised term discovery (UTD), which creates hard clusters of repeated word-like units in raw speech (Park and Glass, 2008; Jansen and Van Durme, 2011).",
      "startOffset" : 64,
      "endOffset" : 128
    }, {
      "referenceID" : 22,
      "context" : "We test our system on the CALLHOME Spanish-English speech translation corpus (Post et al., 2013), a noisy multi-speaker corpus of telephone calls in a variety of Spanish diar X iv :1 70 2.",
      "startOffset" : 77,
      "endOffset" : 96
    }, {
      "referenceID" : 8,
      "context" : "In these conditions, the language modeling and ordering assumptions of most MT models are unwarranted, so we instead use a simple bag-of-words translation model based only on co-occurrence: IBM Model 1 (Brown et al., 1993) with a Dirichlet prior over translation distributions, as learned by fast align (Dyer et al.",
      "startOffset" : 202,
      "endOffset" : 222
    }, {
      "referenceID" : 11,
      "context" : ", 1993) with a Dirichlet prior over translation distributions, as learned by fast align (Dyer et al., 2013).",
      "startOffset" : 88,
      "endOffset" : 107
    }, {
      "referenceID" : 21,
      "context" : "In a more realistic setup, we could use the training audio to construct a consensus representation of each pseudoterm (Petitjean et al., 2011; Anastasopoulos et al., 2016), then use DTW to identify its occurrences in test data to translate.",
      "startOffset" : 118,
      "endOffset" : 171
    }, {
      "referenceID" : 2,
      "context" : "In a more realistic setup, we could use the training audio to construct a consensus representation of each pseudoterm (Petitjean et al., 2011; Anastasopoulos et al., 2016), then use DTW to identify its occurrences in test data to translate.",
      "startOffset" : 118,
      "endOffset" : 171
    }, {
      "referenceID" : 7,
      "context" : "translations with NLTK (Bird et al., 2009).",
      "startOffset" : 23,
      "endOffset" : 42
    }, {
      "referenceID" : 9,
      "context" : "Many of these errors are due to cross-speaker matches, which are known to be more challenging for UTD (Carlin et al., 2011; Kamper et al., 2015; Bansal et al., 2017).",
      "startOffset" : 102,
      "endOffset" : 165
    }, {
      "referenceID" : 13,
      "context" : "Many of these errors are due to cross-speaker matches, which are known to be more challenging for UTD (Carlin et al., 2011; Kamper et al., 2015; Bansal et al., 2017).",
      "startOffset" : 102,
      "endOffset" : 165
    }, {
      "referenceID" : 3,
      "context" : "Many of these errors are due to cross-speaker matches, which are known to be more challenging for UTD (Carlin et al., 2011; Kamper et al., 2015; Bansal et al., 2017).",
      "startOffset" : 102,
      "endOffset" : 165
    }, {
      "referenceID" : 16,
      "context" : "BLEU scores for supervised speech translation systems trained on our data can be found in Kumar et al. (2014).",
      "startOffset" : 90,
      "endOffset" : 110
    }, {
      "referenceID" : 13,
      "context" : "speaker settings (Kamper et al., 2015; Kamper et al., 2016a) and speaker normalization (Zeghidour et al.",
      "startOffset" : 17,
      "endOffset" : 60
    }, {
      "referenceID" : 24,
      "context" : ", 2016a) and speaker normalization (Zeghidour et al., 2016).",
      "startOffset" : 35,
      "endOffset" : 59
    }, {
      "referenceID" : 3,
      "context" : "Finally, Bansal et al. (2017) recently showed that UTD can be improved using the translations themselves as a source of information, which suggests joint learning as an attractive area for future work.",
      "startOffset" : 9,
      "endOffset" : 30
    }, {
      "referenceID" : 10,
      "context" : "On the other hand, poor precision is most likely due to the simplicity of our MT model, and designing a model whose assumptions match our data conditions is an important direction for future work, which may combine our approach with insight from recent, quite different audio-to-translation models (Duong et al., 2016; Anastasopoulos et al., 2016; Adams et al., 2016a; Adams et al., 2016b; Berard et al., 2016).",
      "startOffset" : 298,
      "endOffset" : 410
    }, {
      "referenceID" : 2,
      "context" : "On the other hand, poor precision is most likely due to the simplicity of our MT model, and designing a model whose assumptions match our data conditions is an important direction for future work, which may combine our approach with insight from recent, quite different audio-to-translation models (Duong et al., 2016; Anastasopoulos et al., 2016; Adams et al., 2016a; Adams et al., 2016b; Berard et al., 2016).",
      "startOffset" : 298,
      "endOffset" : 410
    }, {
      "referenceID" : 4,
      "context" : "On the other hand, poor precision is most likely due to the simplicity of our MT model, and designing a model whose assumptions match our data conditions is an important direction for future work, which may combine our approach with insight from recent, quite different audio-to-translation models (Duong et al., 2016; Anastasopoulos et al., 2016; Adams et al., 2016a; Adams et al., 2016b; Berard et al., 2016).",
      "startOffset" : 298,
      "endOffset" : 410
    } ],
    "year" : 2017,
    "abstractText" : "We explore the problem of translating speech to text in low-resource scenarios where neither automatic speech recognition (ASR) nor machine translation (MT) are available, but we have training data in the form of audio paired with text translations. We present the first system for this problem applied to a realistic multi-speaker dataset, the CALLHOME Spanish-English speech translation corpus. Our approach uses unsupervised term discovery (UTD) to cluster repeated patterns in the audio, creating a pseudotext, which we pair with translations to create a parallel text and train a simple bag-of-words MT model. We identify the challenges faced by the system, finding that the difficulty of cross-speaker UTD results in low recall, but that our system is still able to correctly translate some content words in test data.",
    "creator" : "LaTeX with hyperref package"
  }
}