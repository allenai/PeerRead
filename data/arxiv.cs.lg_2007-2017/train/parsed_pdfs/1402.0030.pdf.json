{
  "name" : "1402.0030.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Neural Variational Inference and Learning in Belief Networks",
    "authors" : [ "Andriy Mnih", "Karol Gregor" ],
    "emails" : [ "ANDRIY@DEEPMIND.COM", "KAROL@DEEPMIND.COM" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Compared to powerful globally-normalized latent variable models, such as deep belief networks (Hinton et al., 2006) and deep Boltzmann machines (Salakhutdinov & Hinton, 2009a), which can now be trained on fairly large datasets, their purely directed counterparts have been left behind due to the lack of efficient learning algorithms. This is unfortunate, because their modularity and ability to generate observations efficiently make them better suited for integration into larger systems.\nTraining highly expressive directed latent variable models on large datasets is a challenging problem due to the difficulties posed by inference. Although the generality\nof Markov Chain Monte Carlo (MCMC) methods makes them straightforward to apply to models of this type (Neal, 1992), they tend to suffer from slow mixing and are usually too computationally expensive to be practical in all but the simplest models. Such methods are also difficult to scale to large datasets because they need to store the current state of the latent variables for all the training observations between parameter updates.\nVariational methods (Jordan et al., 1999) provide an optimization-based alternative to the sampling-based Monte Carlo methods, and tend to be more efficient. They involve approximating the exact posterior using a distribution from a more tractable family, often a fully factored one, by maximizing a variational lower bound on the loglikelihood w.r.t. the parameters of the distribution. For a small class of models, using such variational posteriors allows the expectations that specify the parameter updates to be computed analytically. However, for highly expressive models such as the ones we are interested in, these expectations are intractable even with the simplest variational posteriors. This difficulty is usually dealt with by lower bounding the intractable expectations with tractable one by introducing more variational parameters, as was done for sigmoid belief nets by Saul et al. (1996). However, this technique increases the gap between the bound being optimized and the log-likelihood, potentially resulting in a poorer fit to the data. In general, variational methods tend to be more model-dependent than sampling-based methods, often requiring non-trivial model-specific derivations.\nWe propose a new approach to training directed graphical models that combines the advantages of the samplingbased and variational methods. Its central idea is using a feedforward network to implement efficient exact sampling from the variational posterior for the given observation. We train this inference network jointly with the model by maximizing the variational lower bound on the log-likelihood, estimating all the required gradients using samples from the inference network. Although naive estimate of the gradient for the inference network parameters is unusable due to its high variance, we make the approach practical by applying several straightforward and general variance reduction\nar X\niv :1\n40 2.\n00 30\nv1 [\ncs .L\nG ]\n3 1\nJa n\n20 14\ntechniques. Due to our use of stochastic feedforward networks for performing inference we call our approach Neural Variational Inference and Learning (NVIL).\nCompared to MCMC methods, where many iterations over the latent variables are required to generate a sample from the exact posterior and successive samples tend to be highly correlated, NVIL does not suffer from mixing issues as each forward pass through the inference network generates an independent exact sample from the variational posterior. In addition to being much faster than MCMC, our approach has the additional advantage of not needing to store the latent variables for each observation and thus is not only more memory efficient but also applicable to the pure online learning setting, where each training case is seen once before being discarded.\nIn contrast to other work on scaling up variational inference (Kingma & Welling, 2013; Ranganath et al., 2013; Rezende et al., 2014), NVIL can handle both discrete and continuous latent variables, as well as variational posteriors with complex dependency structures. Moreover, the variance reduction methods we employ are simple and modelindependent, unlike the more sophisticated model-specific control variates of Paisley et al. (2012).\nThough the idea of training an inference model by following the gradient of the variational bound has been considered before, it was dismissed as infeasible (Dayan & Hinton, 1996). Our primary contribution is to show how to reduce the variance of the naive gradient estimator to make it practical without narrowing its range of applicability. We also show that the resulting method trains sigmoid belief networks better than the wake-sleep algorithm (Hinton et al., 1995b), which is the only algorithm capable of training the same range of models we are aware of. Finally, we demonstrate the effectiveness and scalability of NVIL by using it to achieve state-of-the-art results on the Reuters RCV1 document dataset."
    }, {
      "heading" : "2. Neural variational inference and learning",
      "text" : ""
    }, {
      "heading" : "2.1. Variational objective",
      "text" : "Suppose we are interested in training a latent variable model Pθ(x, h) with parameters θ. We assume that exact inference in the model is intractable and thus maximum likelihood learning is not an option. For simplicity, we will also assume that all the latent variables in the model are discrete, though essentially the same approach applies if some or all of the variables are continuous.\nWe will train the model by maximizing a variational lower bound on the marginal log-likelihood. Following the standard variational inference approach (Jordan et al., 1999), given an observation x, we introduce a distribution\nQφ(h|x) with parameters φ, which will serve as an approximation to its exact posterior Pθ(h|x). The variational posterior Q will have a simpler form than the exact posterior and thus will be easier to work with.\nThe contribution of x to the log-likelihood can then be lower-bounded as follows (Jordan et al., 1999):\nlogPθ(x) = log ∑ h Pθ(x, h)\n≥ ∑ h Qφ(h|x) log Pθ(x, h) Qφ(h|x)\n= EQ[logPθ(x, h)− logQφ(h|x)] (1) = L(x, θ, φ).\nBy rewriting the bound as\nL(x, θ, φ) = logPθ(x)−KL(Qφ(h|x), Pθ(h|x)), (2)\nwe see that its tightness is determined by the KullbackLeibler (KL) divergence between the variational distribution and the exact posterior. Maximizing the bound with respect to the parameters φ of the variational distribution makes the distribution a better approximation to the posterior (w.r.t. the KL-divergence) and tightens the bound.\nIn contrast to most applications of variational inference where the variational posterior for each observation is defined using its own set of variational parameters, our approach does not use any local variational parameters. Instead, we use a flexible feedforward model to compute the variational distribution from the observation. We call the model mapping x to Qφ(h|x) the inference network. The architecture of the inference network is constrained only by the requirement that Qφ(h|x) it defines has to be efficient to evaluate and sample from. Using samples from the inference network we will be able to compute gradient estimates for the model and inference network parameters for a large class of highly expressive architectures, without having to deal with architecture-specific approximations.\nGiven a training set D, consisting of observations x1, ..., xD, we train the model by (locally) maximizing L(D, θ, φ) = ∑ i L(xi, θ, φ) using gradient ascent w.r.t. to the model and inference network parameters. To ensure scalability to large datasets, we will perform stochastic optimization by estimating gradients on small minibatches of randomly sampled training cases."
    }, {
      "heading" : "2.2. Parameter gradients",
      "text" : "The gradient of the variational bound for a single observation x w.r.t. to the model parameters is straightforward to derive and has the form\n∇θL(x) = EQ [∇θ logPθ(x, h)] , (3)\nwhere we left θ and φ off the list of the arguments of L to simplify the notation. The corresponding gradient w.r.t. to the inference network parameters is somewhat more involved:\n∇φL(x) = EQ[(logPθ(x, h)− logQφ(h|x)) ×∇φ logQφ(h|x)], (4)\nWe give its derivation in the appendix.\nAs both gradients involve expectations which are intractable in all but a handful of special cases, we will estimate them with Monte Carlo integration, using samples from the inference network. Having generated n samples h(1), ..., h(n) from Qφ(h|x), we compute\n∇θL(x) ≈ 1\nn n∑ i=1 ∇θ logPθ(x, h(i)) (5)\nand\n∇φL(x) ≈ 1\nn n∑ i=1 (logPθ(x, h (i))− logQφ(h(i)|x))\n×∇φ logQφ(h(i)|x). (6)\nThe above gradient estimators are unbiased and thus can be used to perform stochastic maximization of the variational objective using a suitable learning rate annealing schedule. The speed of convergence of this procedure, however, depends heavily on the variance of the estimators used, as we will see in Section 4.2.\nThe model gradient estimator (5) is well-behaved and does not pose a problem. The variance of the inference network gradient estimator (6), however, can be very high due to the scaling of the gradient inside the expectation by a potentially large term. As a result, learning variational parameters with updates based on this estimator can be unacceptably slow. In fact, it is widely believed that learning variational parameters using gradient estimators of the form (6) is infeasible (Hinton & Zemel, 1994; Dayan & Hinton, 1996; Kingma & Welling, 2013). In the next section we will show how to make this approach practical by applying variance reduction techniques."
    }, {
      "heading" : "2.3. Variance reduction techniques",
      "text" : "Though gradient estimates computed using Eq. 6 are usually too noisy to be useful in practice, it is easy to reduce their variance to a manageable level with the following model-independent techniques."
    }, {
      "heading" : "2.3.1. CENTERING THE LEARNING SIGNAL",
      "text" : "Inspecting Eq. 4, we see that we are using\nlφ(x, h) = logPθ(x, h)− logQφ(h|x) (7)\nas the learning signal for the inference network parameters, and thus are effectively fitting logQφ(h|x) to logPθ(x, h). This might seem surprising, given that we want the inference network Qφ(h|x) to approximate the posterior distribution Pθ(x|h), as opposed to the joint distribution Pθ(x, h). It turns out however that using the joint instead of the posterior distribution in Eq. 4 does not affect the value of the expectation. To see that we start by noting that\nEQ[∇φ logQφ(h|x)] = EQ [ ∇φQφ(h|x) Qφ(h|x) ] = ∇φEQ[1] = 0. (8)\nTherefore we can subtract any c that does not depend on h from the learning signal in Eq. 4 without affecting the value of the expectation:\nEQ[(lφ(x, h)− c)∇φ logQφ(h|x)] = EQ[lφ(x, h)∇φ logQφ(h|x)]− cEQ[∇φ logQφ(h|x)] = EQ[lφ(x, h)∇φ logQφ(h|x)]. (9)\nAnd as logPθ(x, h) = logPθ(h|x) + logPθ(x) and logPθ(x) does not depend on h, using Pθ(h|x) in Eq. 4 in place of does not affect the expectation.\nThis equivalence allows us to compute the learning signal efficiently, without having to evaluate the intractable Pθ(h|x) term. The price we pay for this tractability is the much higher variance of the estimates computed using Eq. 6. Fortunately, Eq. 9 suggests that we can reduce the variance by subtracting a carefully chosen c from the learning signal. The simplest option is to make c a parameter and adapt it as learning progresses. However, c will not be able capture the systematic differences in the learning signal for different observations x, which arise in part due to the presence of the logPθ(x) term. Thus we can reduce the gradient variance further by subtracting an observation-dependent term Cψ(x) to minimize those differences. Doing this does not affect the expected value of the gradient estimator because Cψ(x) does not depend on the latent variables. Borrowing a name from the reinforcement learning literature we will refer to c and Cψ(x) as baselines. We will elaborate on this connection in Section 3.4.\nWe implement the input-dependent baseline Cψ(x) using a neural network and train it to minimize the expected square of the centered learning signalEQ[(lφ(x, h)−Cψ(x)−c)2]. Though this approach to fitting the baseline does not result in the maximal variance reduction, it is simpler and in our experience works as well as the optimal approach of Weaver & Tao (2001) which requires taking into account the magnitude of the gradient of the inference network parameters. We also experimented with per-parameter baselines but found that they did not improve on the global ones.\nFinally, we note that incorporating baselines into the learning signal can be seen as using simple control variates. In contrast to the more elaborate control variates (e.g. of Paisley et al. (2012)), baselines do not depend on the form of the model or of the variational distribution and thus are easier to use."
    }, {
      "heading" : "2.3.2. VARIANCE NORMALIZATION",
      "text" : "Even after centering, using lφ(x, h) as the learning signal is non-trivial as its average magnitude can change dramatically, and not necessarily monotonically, as training progresses. This variability makes training an inference network using a fixed learning rate difficult. We address this issue by dividing the centered learning signal by a running estimate of its standard deviation. This normalization ensures that the signal is approximately unit variance, and can be seen as a simple and efficient way of adapting the learning rate. To ensure that we stop learning when the magnitude of the signal approaches zero, we apply variance normalization only when the estimate of the standard deviation is greater than 1."
    }, {
      "heading" : "2.3.3. LOCAL LEARNING SIGNALS",
      "text" : "So far we made no assumptions about the structure of the model or the inference network. However, by taking advantage of their conditional independence properties we can train the inference network using simpler and less noisy local learning signals instead of the monolithic global learning signal lφ(x, h). Our approach to deriving a local signal for a set of parameters involves removing all the terms from the global signal that do not affect the value of the resulting gradient estimator.\nWe will derive the layer-specific learning signals for the common case of both the model and the inference network having n layers of latent variables. The model and the variational posterior distributions then naturally factor as\nPθ(x, h) =Pθ(x|h1) ∏n−1\ni=1 Pθ(h\ni|hi+1)Pθ(hn), (10)\nQφ(h|x) =Qφ1(h1|x) ∏n−1\ni=1 Qφi+1(h\ni+1|hi), (11)\nwhere hi denotes the latent variables in the ith layer and φi the parameters of the variational distribution for that layer. We will also use hi:j to denote the latent variables in layers i through j.\nTo learn the parameters of the the variational distribution for layer i , we need to compute the following gradient:\n∇φiL(x) = EQ(h|x)[lφ(x, h)∇φi logQφi(hi|hi−1)].\nUsing the law of iterated expectation we can rewrite the\nexpectation w.r.t. Q(h|x) as\n∇φiL(x) = EQ(h1:i−1|x)[ EQ(hi:n|hi−1)[lφ(x, h)∇φi logQφi(hi|hi−1)]|hi−1]],\nwhere we also used the fact that under the variational posterior, hi:n is independent of h1:i−2 and x, given hi−1. As a consequence of Eq. 9, when computing the expectation w.r.t. Q(hi:n|hi−1), all the terms in the learning signal that do not depend on hi:n can be safely dropped without affecting the result. This gives us the following local learning signal for layer i:\nliφ(x, h) = logPθ(h i−1:n)− logQφ(hi:n|hi−1). (12)\nTo get the signal for the first hidden layer we simply use x in place of h0, in which case we simply recover the global learning signal. For hidden layers i > 1, however, the local signal involves fewer terms than lφ(x, h) and thus can be expected to be less noisy. As we do not assume any within-layer structure, Eq. 12 applies to models and inference network whether or notQφ(hi|hi−1) and Pθ(hi|hi+1) are factorial.\nSince local signals can be significantly different from each other, we use separate baselines and variance estimates for each signal. For layers i > 1, the input-dependent baseline Cψ(x) is replaced by Ciψi(h i−1).\nIn some cases, further simplification of the learning signal is possible, yielding a different signal per latent variable. We leave exploring this as future work."
    }, {
      "heading" : "3. Related work",
      "text" : ""
    }, {
      "heading" : "3.1. Feedforward approximations to inference",
      "text" : "The idea of training an approximate inference network by optimizing a variational lower bound is not new. It goes back at least to Hinton & Zemel (1994), who derived the variational objective from the Minimum Description Length (MDL) perspective and used it to train linear autoencoders. Their probabilistic encoder and decoder correspond to our inference network and model respectively. However, they computed the gradients analytically, which was possible due to the simplicity of their model, and dismissed the sampling-based approach as infeasible due to noise.\nRecently a method for training nonlinear models with continuous latent variables, called Stochastic Gradient Variational Bayes (SGVB), has been proposed by Kingma & Welling (2013) and Rezende et al. (2014). Like NVIL, it involves using feedforward models to perform approximate inference and trains them by optimizing a sampling-based estimate of the variational bound on the log-likelihood. However, SGVB is considerably less general than NVIL,\nbecause it uses a gradient estimator obtained by taking advantage of special properties of real-valued random variables and thus is not applicable to models with discrete random variables. Moreover, unlike NVIL, SGVB method cannot handle inference networks with nonlinear dependencies between latent variables. The ideas of the two methods are complementary however, and NVIL is likely to benefit from the SGVB-style treatment of continuousvalued variables, while SGVB might converge faster using the variance reduction techniques we proposed.\nGregor et al. (2013) have recently proposed a related algorithm for training sigmoid belief network like models based on the MDL framework. They also use a feedforward model to perform approximate inference, but concentrate on the case of a deterministic inference network and can handle only binary latent variables. The inference network is trained by backpropagating through binary thresholding units, ignoring the thresholding nonlinearities, to approximately minimize the coding cost of the joint latentvisible configurations. This approach can be seen as approximately maximizing a looser variational lower bound than (2) due to the absence of the entropy term.\nAn inference network for efficient generation of samples from the approximate posterior can also be seen as a probabilistic generalization of the approximate feedforward inference methods developed for sparse coding models in the last few years (Kavukcuoglu et al., 2008; Bradley & Bagnell, 2008; Gregor & LeCun, 2010)."
    }, {
      "heading" : "3.2. Sampling-based variational inference",
      "text" : "Like NVIL, Black Box Variational Inference (BBVI, Ranganath et al., 2013) learns the variational parameters of the posterior by optimizing the variational bound using sampling-based gradient estimates, which makes it applicable to a large range of models. However, unlike NVIL, BBVI follows the traditional approach of learning a separate set of variational parameters for each observation and does not use an inference network. Moreover, BBVI uses a fully-factorized mean field approximation to the posterior, which limits its power."
    }, {
      "heading" : "3.3. The wake-sleep algorithm",
      "text" : "NVIL shares many similarities with the wake-sleep algorithm (Hinton et al., 1995a), which enjoys the same scalability and applicability to a wide range of models. This algorithm was introduced for training Helmholtz machines (Dayan et al., 1995), which are multi-layer belief networks augmented with recognition networks. These recognition networks are used for approximate inference and are directly analogous to NVIL inference networks. Wake-sleep alternates between updating the model parameters in the wake phase and the recognition network parameters in the\nsleep phase. The model parameter update is based on the samples generated from the recognition network on the training data and is identical to the NVIL one (Eq. 5). However, in contrast to NVIL, the recognition network parameters are learned from samples generated by the model. In other words, the recognition network is trained to recover the hidden causes corresponding to the samples from the model distribution by following the gradient\n∇φL(x) = EPθ(x,h) [∇φ logQφ(h|x)] . (13)\nUnfortunately, this update does not optimize the same objective as the model parameter update, which means that the wake-sleep algorithm does not optimize a well-defined objective function and is not guaranteed to converge. This is the algorithm’s main weakness, compared to NVIL, which optimizes a variational lower bound on the loglikelihood.\nThe wake-sleep gradient for recognition network parameters does have the advantage of being much easier to estimate than the corresponding gradient of the variational bound. In fact, the idea of training the recognition networks using the gradient of the bound was mentioned in (Hinton & Zemel, 1994) and (Dayan & Hinton, 1996) but not seriously considered due concerns about the high variance of the estimates. In Section 4.2 we show that while the naive estimator of the gradient given in Eq. 6 does exhibit high variance, the variance reduction techniques from Section 2.3 improve it dramatically and make it practical."
    }, {
      "heading" : "3.4. REINFORCE",
      "text" : "Using the gradient (4) to train the inference network can be seen as an instance of the REINFORCE algorithm (Williams, 1992) from reinforcement learning (RL), which adapts the parameters of a stochastic model to maximize the external reward signal which depends on the model’s output. Given a model Pθ(x) and a reward signal r(x), REINFORCE updates the model parameters using the rule\n∆θ ∝ EP [(r(x)− b)∇θ logPθ(x)]. (14)\nWe can view NVIL as an application of REINFORCE on the per-training-case basis, with the inference network corresponding to the stochastic model, latent state h to the output, and the learning signal lφ(x, h) to the reward. The term b in Eq. 14, called a baseline in the RL literature, is a hyperparameter that can be adapted to reduce the variance of the parameter update. Thus it serves the same function as c and Cψ(x) that we subtract from the learning signal to center it in Section 2.3.1. The considerable body of work on baselines and other variance reduction methods done in the RL community (e.g. Greensmith et al., 2004) is likely to contain additional techniques relevant for training inference networks."
    }, {
      "heading" : "4. Experimental results",
      "text" : "We performed two sets of experiments, with the first set intended to evaluate the effectiveness of our variance reduction techniques and to compare NVIL’s performance to that of the wake-sleep algorithm. In the second set of experiments, we demonstrate NVIL’s ability to handle larger real-world datasets by using it to train generative models of documents."
    }, {
      "heading" : "4.1. Experimental protocol",
      "text" : "We trained all models using stochastic gradient ascent using minibatches of 20 observations sampled randomly from the training data. The gradient estimates were computed using a single sample from the inference network. For each dataset, we created a validation set by removing a random subset of 100 observations from the training set. The only form of regularization we used was early stopping based on the validation bound, implemented by keeping track of the parameter configuration with the best validation score seen so far. We implemented each input-dependent baseline using a neural network with a single hidden layer of 100 tanh units.\nWe used fixed learning rates because we found them to produce superior results to the annealing schedules we experimented with. The learning rates we report were selected based on the validation set performance in preliminary experiments with smaller models. We always make the learning rate for inference network five times smaller than for the model (which is the one we report), as we found this to improve performance. We used inference networks with layered structure given by Eq. 11, without dependencies within each layer except in the experiment with autoregressive inference networks. All multi-layer inference networks were trained using layer-specific learning signals from Section 2.3.3.\nAs the models we train are intractable, we cannot compute the exact log-likelihoods for them. Instead we report the estimates of the variational bound (2) computed using 10 samples from the inference network, which we found to be sufficient to get the accurate bound estimates. We expect this approach to underestimate the log-likelihood considerably, but leave finding more direct and thus less pessimistic evaluation methods as future work."
    }, {
      "heading" : "4.2. Modelling images of digits",
      "text" : "Our first set of experiments was performed on the binarized version of the MNIST dataset, which has become the standard benchmark for evaluating generative models of binary data. The dataset consists of 70,000 28 × 28 binary images of handwritten digits, partitioned into a 60,000-image training set and 10,000-image test set. We used the bina-\nrization of Salakhutdinov & Murray (2008), which makes our scores directly comparable to those in the literature.\nWe used 3 × 10−4 as the learning rate for training models with NVIL on this dataset. Centering the input vectors by subtracting the mean vector was essential for making the inference networks and input-dependent baselines work well.\nTo demonstrate the importance of variance reduction techniques, we trained two SBNs using a range of variance control settings. The first SBN had a single layer of 200 latent variables, while the second one had two layers of 200 variables each. Figure 1 shows the estimate of the variational objective on the validation set plotted against the number of parameter updates. For both models, it is clear that using all three techniques – the input-dependent and inputindependent baselines along with variance normalization – is essential for best performance. However, of the three techniques, the input-dependent baseline appears to be the least important. Comparing the plots for the two models suggests that variance reduction becomes more important for larger models, with the gap between the best combination and the others (excluding the very worst one) widening. For both models, learning with all three variance reduction techniques disabled makes barely any progress and is clearly infeasible.\nWe found that disabling layer-specific learning signals had little effect on the performance of the resulting model. The difference was about 0.4 nats for an SBN with two or three layers of latent variables.\nWe next compared NVIL to the wake-sleep algorithm, which is its closest competitor in terms of scalability and breadth of applicability, by training a range of models us-\ning both algorithms. Wake-sleep training used a learning rate of 1 × 10−4, as we found this algorithm to be more sensitive to the choice of the learning rate than NVIL, performing considerably better with lower learning rates. The results, along with some baselines from the literature, are shown in Table 1. We report only the means of the bound estimates as their standard deviations were all very small, none exceeding 0.1 nat. We can see that models trained with NVIL have considerably better bounds on the loglikelihood, compared to their wake-sleep counterparts, with the difference ranging from 3.4 to 8.6 nats. Additional layers make SBNs perform better, independently of the training method. Interestingly, single-layer fDARN (Gregor et al., 2013) models, which have autoregressive connections between the latent variables, perform better than any of the SBN models trained using the same algorithm. Comparing to results from the literature, we see that all the SBN and fDARN models we trained perform much better than a mixture of 500 factorial Bernoulli distributions (MoB) but not as well as the deterministic Neural Autoregressive Distribution Estimator (NADE) (Larochelle & Murray, 2011). The NVIL-trained fDARN models with 200 and 500 latent variables also outperform the fDARN (as well as the more expressive DARN) model with 400 latent variables from (Gregor et al., 2013), which were trained using an MDLbased algorithm. The fDARN and multi-layer SBN models trained using NVIL also outperform a 500-hidden-unit RBM trained with 3-step contrastive divergence (CD), but not the one trained with 25-step CD (Salakhutdinov & Murray, 2008). However, both sampling and CD-25 training in an RBM is considerably more expensive than sampling or NVIL training for any of our models.\nThe sampling-based approach to computing gradients al-\nlows NVIL to handle variational posteriors with complex dependencies. To demonstrate this ability, we retrained several of the SBN models using inference networks with autoregressive connections within each layer. These networks can capture the dependencies between variables within layers and thus are considerably more expressive than the ones with factorial layers. Results in Table 2 indicate that using inference networks with autoregressive connections produces better models, with the single-layer models exhibiting large gains."
    }, {
      "heading" : "4.3. Document modelling",
      "text" : "We also applied NVIL to the more practical task of document modelling. The goal is to train a generative model of documents which are represented as vectors of word counts, also known as bags of words. We trained two simple models on the 20 Newsgroups and Reuters Corpus Volume I (RCV1-v2) datasets, which have been used to evaluate similar models in (Salakhutdinov & Hinton, 2009b;\nLarochelle & Lauly, 2012). 20 Newsgroups is a fairly small dataset of Usenet newsgroup posts, consisting of about 11K training and 7.5K test documents. RCV1 is a much larger dataset of Reuters newswire articles, with about 794.4K training and 10K test documents. We use the standard preprocessed versions of the datasets from Salakhutdinov & Hinton (2009b), which have vocabularies of 2K and 10K words respectively.\nWe experimented with two simple document models, based on the SBN and DARN architectures. Both models had a single layer of latent variables and a multinomial visible layer and can be seen as directed counterparts of the Replicated Softmax model (Salakhutdinov & Hinton, 2009b). We used the same training procedure as on MNIST with the exception of the learning rates which were 3× 10−5 on 20 Newsgroups and 10−3 on RCV1.\nThe established evaluation metric for such models is the perplexity per word, computed as exp ( − 1N ∑ n 1 Ln logP (xn) )\n, where N is the number of documents, Ln is the length of document n, and P (xn) the probability of the document under the model. As we cannot compute logP (xn), we use the variational lower bound in its place and thus report an upper bound on perplexity.\nThe results for our models, along with ones for the Replicated Softmax and DocNADE models from (Salakhutdinov & Hinton, 2009b) and (Larochelle & Lauly, 2012) respectively, are shown in Table 3. We can see that the SBN and fDARN models with 50 latent variables perform well, producing better scores than LDA and Replicated Softmax on both datasets. Their performance is also competitive with that of DocNADE on 20 Newsgroups. The score of 724 for fDARN with 50 latent variables on RCV1 is already better than DocNADE’s 742, the best published result on that dataset. fDARN with 200 hidden units, however, performs even better, setting a new record with 598."
    }, {
      "heading" : "5. Discussion and future work",
      "text" : "We developed, NVIL, a new training method for intractable directed latent variable models which is general and easy to apply to new models. We showed that NVIL consistently outperforms the wake-sleep algorithm at training sigmoidbelief-network-like models. Finally, we demonstrated the potential of our approach by achieving state-of-the-art results on a sizable dataset of documents (Reuters RCV1).\nAs the emphasis of this paper is on the training method, we applied it to some of the simplest possible model and inference network architectures, which was sufficient to obtain promising results. We believe that considerable performance gains can be made by using more expressive architectures, such as those with nonlinearities between layers of stochastic variables. Applying NVIL to models with continuous latent variables is another promising direction since binary latent variables are not always appropriate.\nWe expect NVIL to be also applicable to training conditional latent variable models for modelling the distribution of observations given some context, which would require making the inference network take both the context and the observation as input. This would make it an alternative to the importance-sampling training method of Tang & Salakhutdinov (2013) for conditional models with structured high-dimensional outputs.\nWe hope that the generality and flexibility of our approach will make it easier to apply powerful directed latent variable models to real-world problems.\nAppendix Derivation of the inference network gradient\n∇φL(x) =∇φEQ[logPθ(x, h)− logQφ(h|x)] =∇φ ∑ h Qφ(h|x) logPθ(x, h)−\n∇φ ∑ h Qφ(h|x) logQφ(h|x)\n= ∑ h\nlogPθ(x, h)∇φQφ(h|x)−∑ h (logQφ(h|x) + 1)∇φQφ(h|x)\n= ∑ h (logPθ(x, h)− logQφ(h|x))∇φQφ(h|x),\nwhere we used the fact that ∑ h∇φQφ(h|x) =\n∇φ ∑ hQφ(h|x) = ∇φ1 = 0. Using the identity\n∇φQφ(h|x) = Qφ(h|x)∇φ logQφ(h|x), then gives\n∇φL(x) = ∑ h (logPθ(x, h)− logQφ(h|x))\n×Qφ(h|x)∇φ logQφ(h|x) =EQ [(logPθ(x, h)− logQφ(h|x))∇φ logQφ(h|x)]"
    }, {
      "heading" : "ACKNOWLEDGEMENTS",
      "text" : "We thank Koray Kavukcuoglu, Volodymyr Mnih, and Nicolas Heess for their helpful comments. We thank Ruslan Salakhutdinov for providing us with the preprocessed document datasets."
    } ],
    "references" : [ {
      "title" : "Differential sparse coding",
      "author" : [ "Bradley", "David M", "Bagnell", "J Andrew" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Bradley et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Bradley et al\\.",
      "year" : 2008
    }, {
      "title" : "Varieties of helmholtz machine",
      "author" : [ "Dayan", "Peter", "Hinton", "Geoffrey E" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "Dayan et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Dayan et al\\.",
      "year" : 1996
    }, {
      "title" : "The helmholtz machine",
      "author" : [ "Dayan", "Peter", "Hinton", "Geoffrey E", "Neal", "Radford M", "Zemel", "Richard S" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Dayan et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Dayan et al\\.",
      "year" : 1995
    }, {
      "title" : "Variance reduction techniques for gradient estimates in reinforcement learning",
      "author" : [ "Greensmith", "Evan", "Bartlett", "Peter L", "Baxter", "Jonathan" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Greensmith et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Greensmith et al\\.",
      "year" : 2004
    }, {
      "title" : "Learning fast approximations of sparse coding",
      "author" : [ "Gregor", "Karol", "LeCun", "Yann" ],
      "venue" : "In Proc. International Conference on Machine learning (ICML’10),",
      "citeRegEx" : "Gregor et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Gregor et al\\.",
      "year" : 2010
    }, {
      "title" : "Deep autoregressive networks",
      "author" : [ "Gregor", "Karol", "Mnih", "Andriy", "Wierstra", "Daan" ],
      "venue" : "arXiv preprint arXiv:1310.8499,",
      "citeRegEx" : "Gregor et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Gregor et al\\.",
      "year" : 2013
    }, {
      "title" : "Autoencoders, minimum description length, and Helmholtz free energy",
      "author" : [ "Hinton", "Geoffrey E", "Zemel", "Richard S" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Hinton et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 1994
    }, {
      "title" : "The \"wake-sleep\" algorithm for unsupervised neural networks",
      "author" : [ "Hinton", "Geoffrey E", "Dayan", "Peter", "Frey", "Brendan J", "Neal", "Radford M" ],
      "venue" : null,
      "citeRegEx" : "Hinton et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 1995
    }, {
      "title" : "The\" wake-sleep\" algorithm for unsupervised neural networks",
      "author" : [ "Hinton", "Geoffrey E", "Dayan", "Peter", "Frey", "Brendan J", "Neal", "Radford M" ],
      "venue" : "Science, pp. 1158–1158,",
      "citeRegEx" : "Hinton et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 1995
    }, {
      "title" : "A fast learning algorithm for deep belief nets",
      "author" : [ "Hinton", "Geoffrey E", "Osindero", "Simon", "Teh", "Yee Whye" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Hinton et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2006
    }, {
      "title" : "An introduction to variational methods for graphical models",
      "author" : [ "Jordan", "Michael I", "Ghahramani", "Zoubin", "Jaakkola", "Tommi S", "Saul", "Lawrence K" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Jordan et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Jordan et al\\.",
      "year" : 1999
    }, {
      "title" : "Fast inference in sparse coding algorithms with applications to object recognition",
      "author" : [ "Kavukcuoglu", "Koray", "Ranzato", "Marc’Aurelio", "LeCun", "Yann" ],
      "venue" : "Technical report, Courant Institute,",
      "citeRegEx" : "Kavukcuoglu et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Kavukcuoglu et al\\.",
      "year" : 2008
    }, {
      "title" : "Auto-encoding variational bayes",
      "author" : [ "Kingma", "Diederik P", "Welling", "Max" ],
      "venue" : "arXiv preprint arXiv:1312.6114,",
      "citeRegEx" : "Kingma et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2013
    }, {
      "title" : "A neural autoregressive topic model",
      "author" : [ "Larochelle", "Hugo", "Lauly", "Stanislas" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Larochelle et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Larochelle et al\\.",
      "year" : 2012
    }, {
      "title" : "The neural autoregressive distribution estimator",
      "author" : [ "Larochelle", "Hugo", "Murray", "Iain" ],
      "venue" : "JMLR: W&CP,",
      "citeRegEx" : "Larochelle et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Larochelle et al\\.",
      "year" : 2011
    }, {
      "title" : "Connectionist learning of belief networks",
      "author" : [ "Neal", "Radford M" ],
      "venue" : "Artificial intelligence,",
      "citeRegEx" : "Neal and M.,? \\Q1992\\E",
      "shortCiteRegEx" : "Neal and M.",
      "year" : 1992
    }, {
      "title" : "Variational bayesian inference with stochastic search",
      "author" : [ "Paisley", "John William", "Blei", "David M", "Jordan", "Michael I" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Paisley et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Paisley et al\\.",
      "year" : 2012
    }, {
      "title" : "Stochastic back-propagation and variational inference in deep latent gaussian models",
      "author" : [ "Rezende", "Danilo Jimenez", "Mohamed", "Shakir", "Wierstra", "Daan" ],
      "venue" : "arXiv preprint arXiv:1401.4082,",
      "citeRegEx" : "Rezende et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Rezende et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep boltzmann machines",
      "author" : [ "Salakhutdinov", "Ruslan", "Hinton", "Geoffrey E" ],
      "venue" : "In International Conference on Artificial Intelligence and Statistics, pp",
      "citeRegEx" : "Salakhutdinov et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Salakhutdinov et al\\.",
      "year" : 2009
    }, {
      "title" : "Replicated softmax: an undirected topic model",
      "author" : [ "Salakhutdinov", "Ruslan", "Hinton", "Geoffrey E" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Salakhutdinov et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Salakhutdinov et al\\.",
      "year" : 2009
    }, {
      "title" : "On the quantitative analysis of Deep Belief Networks",
      "author" : [ "Salakhutdinov", "Ruslan", "Murray", "Iain" ],
      "venue" : "In Proceedings of the 25th Annual International Conference on Machine Learning",
      "citeRegEx" : "Salakhutdinov et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Salakhutdinov et al\\.",
      "year" : 2008
    }, {
      "title" : "Mean field theory for sigmoid belief networks",
      "author" : [ "Saul", "Lawrence K", "Jaakkola", "Tommi", "Jordan", "Michael I" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Saul et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Saul et al\\.",
      "year" : 1996
    }, {
      "title" : "Learning stochastic feedforward neural networks",
      "author" : [ "Tang", "Yichuan", "Salakhutdinov", "Ruslan" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Tang et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2013
    }, {
      "title" : "The optimal reward baseline for gradient-based reinforcement learning",
      "author" : [ "Weaver", "Lex", "Tao", "Nigel" ],
      "venue" : "Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Weaver et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Weaver et al\\.",
      "year" : 2001
    }, {
      "title" : "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
      "author" : [ "Williams", "Ronald J" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "Williams and J.,? \\Q1992\\E",
      "shortCiteRegEx" : "Williams and J.",
      "year" : 1992
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "Compared to powerful globally-normalized latent variable models, such as deep belief networks (Hinton et al., 2006) and deep Boltzmann machines (Salakhutdinov & Hinton, 2009a), which can now be trained on fairly large datasets, their purely directed counterparts have been left behind due to the lack of efficient learning algorithms.",
      "startOffset" : 94,
      "endOffset" : 115
    }, {
      "referenceID" : 10,
      "context" : "Variational methods (Jordan et al., 1999) provide an optimization-based alternative to the sampling-based Monte Carlo methods, and tend to be more efficient.",
      "startOffset" : 20,
      "endOffset" : 41
    }, {
      "referenceID" : 10,
      "context" : "Variational methods (Jordan et al., 1999) provide an optimization-based alternative to the sampling-based Monte Carlo methods, and tend to be more efficient. They involve approximating the exact posterior using a distribution from a more tractable family, often a fully factored one, by maximizing a variational lower bound on the loglikelihood w.r.t. the parameters of the distribution. For a small class of models, using such variational posteriors allows the expectations that specify the parameter updates to be computed analytically. However, for highly expressive models such as the ones we are interested in, these expectations are intractable even with the simplest variational posteriors. This difficulty is usually dealt with by lower bounding the intractable expectations with tractable one by introducing more variational parameters, as was done for sigmoid belief nets by Saul et al. (1996). However, this technique increases the gap between the bound being optimized and the log-likelihood, potentially resulting in a poorer fit to the data.",
      "startOffset" : 21,
      "endOffset" : 904
    }, {
      "referenceID" : 17,
      "context" : "In contrast to other work on scaling up variational inference (Kingma & Welling, 2013; Ranganath et al., 2013; Rezende et al., 2014), NVIL can handle both discrete and continuous latent variables, as well as variational posteriors with complex dependency structures.",
      "startOffset" : 62,
      "endOffset" : 132
    }, {
      "referenceID" : 16,
      "context" : "Moreover, the variance reduction methods we employ are simple and modelindependent, unlike the more sophisticated model-specific control variates of Paisley et al. (2012).",
      "startOffset" : 149,
      "endOffset" : 171
    }, {
      "referenceID" : 10,
      "context" : "Following the standard variational inference approach (Jordan et al., 1999), given an observation x, we introduce a distribution Qφ(h|x) with parameters φ, which will serve as an approximation to its exact posterior Pθ(h|x).",
      "startOffset" : 54,
      "endOffset" : 75
    }, {
      "referenceID" : 10,
      "context" : "The contribution of x to the log-likelihood can then be lower-bounded as follows (Jordan et al., 1999):",
      "startOffset" : 81,
      "endOffset" : 102
    }, {
      "referenceID" : 16,
      "context" : "of Paisley et al. (2012)), baselines do not depend on the form of the model or of the variational distribution and thus are easier to use.",
      "startOffset" : 3,
      "endOffset" : 25
    }, {
      "referenceID" : 17,
      "context" : "Recently a method for training nonlinear models with continuous latent variables, called Stochastic Gradient Variational Bayes (SGVB), has been proposed by Kingma & Welling (2013) and Rezende et al. (2014). Like NVIL, it involves using feedforward models to perform approximate inference and trains them by optimizing a sampling-based estimate of the variational bound on the log-likelihood.",
      "startOffset" : 184,
      "endOffset" : 206
    }, {
      "referenceID" : 11,
      "context" : "An inference network for efficient generation of samples from the approximate posterior can also be seen as a probabilistic generalization of the approximate feedforward inference methods developed for sparse coding models in the last few years (Kavukcuoglu et al., 2008; Bradley & Bagnell, 2008; Gregor & LeCun, 2010).",
      "startOffset" : 245,
      "endOffset" : 318
    }, {
      "referenceID" : 2,
      "context" : "This algorithm was introduced for training Helmholtz machines (Dayan et al., 1995), which are multi-layer belief networks augmented with recognition networks.",
      "startOffset" : 62,
      "endOffset" : 82
    }, {
      "referenceID" : 5,
      "context" : "Interestingly, single-layer fDARN (Gregor et al., 2013) models, which have autoregressive connections between the latent variables, perform better than any of the SBN models trained using the same algorithm.",
      "startOffset" : 34,
      "endOffset" : 55
    }, {
      "referenceID" : 5,
      "context" : "The NVIL-trained fDARN models with 200 and 500 latent variables also outperform the fDARN (as well as the more expressive DARN) model with 400 latent variables from (Gregor et al., 2013), which were trained using an MDLbased algorithm.",
      "startOffset" : 165,
      "endOffset" : 186
    } ],
    "year" : 2017,
    "abstractText" : "Highly expressive directed latent variable models, such as sigmoid belief networks, are difficult to train on large datasets because exact inference in them is intractable and none of the approximate inference methods that have been applied to them scale well. We propose a fast non-iterative approximate inference method that uses a feedforward network to implement efficient exact sampling from the variational posterior. The model and this inference network are trained jointly by maximizing a variational lower bound on the log-likelihood. Although the naive estimator the inference model gradient is too high-variance to be useful, we make it practical by applying several straightforward modelindependent variance reduction techniques. Applying our approach to training sigmoid belief networks and deep autoregressive networks, we show that it outperforms the wake-sleep algorithm on MNIST and achieves state-of-the-art results on the Reuters RCV1 document dataset.",
    "creator" : "LaTeX with hyperref package"
  }
}