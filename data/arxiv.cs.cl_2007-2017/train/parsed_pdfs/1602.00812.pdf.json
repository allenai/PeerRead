{
  "name" : "1602.00812.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "The Grail theorem prover: Type theory for syntax and semantics",
    "authors" : [ "Richard Moot" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "The Grail theorem prover:\nType theory for syntax and semantics\nRichard Moot\nLaBRI (CNRS), Bordeaux University"
    }, {
      "heading" : "1 Introduction",
      "text" : "This chapter will describe a series of tools for developing and testing typelogical grammars. The Grail family of theorem provers have been designed to work with a variety of modern type-logical frameworks, including multimodal type-logical grammars (Moortgat 2011), NLcl (Barker & Shan 2014), the Displacement calculus (Morrill, Valent́ın & Fadda 2011) and hybrid type-logical grammars (Kubota & Levine 2012).\nThe tools give a transparent way of implementing grammars and testing their consequences, providing a natural deduction proof in the specific type-logical grammar for each of the readings of a sentence. None of this replaces careful reflection by the grammar writer, of course, but in many cases, computational testing of hand-written grammars will reveal surprises, showing unintended consequences of our grammar and such unintended proofs (or unintended absences of proofs) help us improve the grammar. Computational tools also help us speed up grammar development, for example by allowing us to compare several alternative solutions to a problem and investigate their differences.\nThis paper will describe the underlying formalism of the theorem provers, as it is visible during an interactive proof trace, and present the general strategy followed by the theorem provers. The presentation in this paper will be somewhat informal, referring the reader elsewhere for full proofs.\nThe rest of this paper is structured as follows. Section 2 will present a general introduction to type-logical grammars and illustrate its basic concepts using the Lambek calculus, ending the section with some problems at the syntaxsemantics interface for the Lambek calculus. Section 3 will look at recent developments in type-logical grammars and how they solve some of the problems at the syntax-semantics interface. Section 4 will look at two general frameworks for automated theorem proving for type-logical grammars, describing the internal representation of partial proofs and giving a high-level overview of the proof search mechanism.\nar X\niv :1\n60 2.\n00 81\n2v 1\n[ cs\n.C L\n] 2\nF eb\n2 01\n6"
    }, {
      "heading" : "2 Type-logical grammars",
      "text" : "Type-logical grammars are a family of grammar formalisms built on a foundation of logic and type theory. Type-logical grammars originated when Lambek (1958) introduced his Syntactic Calculus (called the Lambek calculus, L, by later authors). Though Lambek built on the work of Ajdukiewicz (1935), BarHillel (1953) and others, Lambek’s main innovation was to cast the calculus as a logic, giving a sequent calculus and showing decidability by means of cut elimination. This combination of linguistic and computational applications has proved very influential.\nIn its general form, a type-logical grammar consists of following components:\n1. a logic, which fulfills the role of “Universal Grammar” mainstream linguistics1,\n2. a homomorphism from this logic to intuitionistic (linear) logic, this mapping is the syntax-semantics interface, with intuitionistic linear logic — also called the Lambek-van Benthem calculus, LP (van Benthem 1995) — fulfilling the role of “deep structure”.\n3. a lexicon, which is a mapping from words of natural language to sets of formulas in our logic,\n4. a set of goal formulas, which specifies the formulas corresponding to different types of sentences in our grammar.2\nA sentence w1, . . . , wn is grammatical iff the statement A1, . . . , An ` C is derivable in our logic, for some Ai ∈ lex(wi), for some goal formula C. In other words, we use the lexicon to map words to formulas and then ask the logic whether the resulting sequence of formulas is a theorem. Parsing in a type-logical grammar is quite literally a form of theorem proving, a very pure realisation of the slogan “parsing as deduction”.\nOne of the attractive aspects of type-logical grammars is their simple and transparent syntax-semantics interface. Though there is a varierty of logics used for the syntax of type-logical grammars (I will discuss the Lambek calculus in Section 2.1 and two generalisations of it in Sections 3.1 and 3.2), there is a large consensus over the syntax-semantics interface. Figure 1 gives a picture of the standard architecture of type-logical grammars.\nThe “bridge” between syntax and semantics in the figure is the CurryHoward isomorphism between linear lambda terms and proofs in multiplicative intuitionistic linear logic.\n1This is rather different from Montague’s use of the term “Universal Grammar” (Montague 1970). In Montague’s sense, the different components of a type-logical grammar together would be an instantiation of Universal Grammar.\n2Many authors use a single designated goal formula, typically s, as is standard in formal language theory. I prefer this slightly more general setup, since it allows us to distinguish between, for example, declarative sentences, imperatives, yes/no questions, wh questions, etc., both syntactically and semantically.\nTheorem proving occurs in two places of the picture: first when parsing a sentence in a given type-logical grammar and also at the end when we use the resulting semantics for inferences. I will have little to say about this second type of theorem proving (Chatzikyriakidis 2015, Mineshima, Mart́ınez-Gómez, Miyao & Bekki 2015, provide some investigations into this question, in a way which seems compatible with the syntax-semantics interface pursued here, though developing a full integrated system combining these systems with the current work would be an interesting research project); theorem proving for parsing will be discussed in Section 4.\nThe lexicon plays the role of translating words to syntactic formulas but also specifies the semantic term which is used to compute the semantics later. The lexicon of a categorial grammar is “semantically informed”. The desired semantics of a sentence allows us to reverse-engineer the formula and lexical lambda-term which produce it.\nMany current semantic theories do not provide a semantic formula directly, but first provide a proto-semantics on which further computations are performed to produce the final semantics (eg. for anaphora resolution, presuppositions projection etc.). In the current context this means at least some inference is necessary to determine semantic and pragmatic well-formedness."
    }, {
      "heading" : "2.1 The Lambek calculus",
      "text" : "To make things more concrete, I will start by presenting the Lambek calculus (Lambek 1958). Lambek introduced his calculus as a way to “obtain an effective rule (or algorithm) for distinguishing sentences from nonsentences”, which would\nbe applicable both to formal and to (at least fragments of) natural languages (Lambek 1958, p. 154). The simplest formulas used in the Lambek calculus are atomic formulas, which normally include s for sentence, n for common noun, np for noun phrase. We then inductively define the set of formulas of the Lambek calculus by saying that, they include the atomic formulas, and that, if A and B are formulas (atomic or not), then A/B, A •B and B\\A are also formulas.\nThe intended meaning of a formula A/B — called A over B — is that it is looking for an expression of syntactic type B to its right to produce an expression of syntactic type A. An example would be a word like “the” which is assigned the formula np/n in the lexicon, indicating that it is looking for a common noun (like “student”) to its right to form a noun phrase, meaning “the student” would be assigned syntactic type np. Similarly, the intended meaning of a formula B\\A — called B under A — is that it is looking for an expression of syntactic type B to its left to produce an expression of type A. This means an intransitive verb like “slept”, when assigned the formula np\\s in the lexicon, combines with a noun phrase to its left to form a sentence s. We therefore predict that “the student slept” is a sentence, given the earlier assignment of np to “the student”. Finally, a formula A • B denotes the concatenation of an expression of type A to an expression of type B.\nBasic statements of the Lambek calculus are of the form A1, . . . , An ` C (with n ≥ 1), indicating a claim that the sequence of formulas A1, . . . , An is of type C; the sequent comma ‘,’ is implicitly associative and non-commutative. Table 1 shows the natural deduction rules for the Lambek calculus. Γ, ∆, etc. denote non-empty sequences for formulas.\nA simple Lambek calculus lexicon is shown in Table 2. I have adopted the standard convention in type-logical grammars of not using set notation for the lexicon, but instead listing multiple lexical entries for a word separately. This corresponds to treating lex as a non-deterministic function rather than as a set-valued function.\nProper names, such as “Alyssa” and “Emory” are assigned the category np. Common nouns, such as “student” and “exam” are assigned the category n. Adjectives, such as “difficult” or “erratic” are not assigned a basic syntactic category but rather the category n/n, indicating they are looking for a common noun to their right to form a new common noun, so we predict that both “diffi-\ncult exam” and “exam” can be assigned category n. For more complex entries, “someone” is looking to its right for a verb phrase to produce a sentence, where np\\s is the Lambek calculus equivalent of verb phrase, whereas “whom” is first looking to its right for a sentence which is itself missing a noun phrase to its right and then to its left for a noun.\nGiven the lexicon of Table 2, we can already derive some fairly complex sentences, such as the following, and, as we will see in the next section, obtain the correct semantics.\n(1) Every student aced some exam.\n(2) The student who slept during the exam loves Alyssa.\nOne of the two derivations of Sentence (1) is shown in Figure 2. To improve readability, the figure uses a “sugared” notation: instead of writing the lexical hypothesis corresponding to “exam” as n ` n, we have written it as exam ` n. The withdrawn np’s corresponding to the object and the subject are given a labels p0 and q0 respectively; the introduction rules are coindexed with the withdrawn hypotheses, even though this information can be inferred from the rule instantiation.\nWe can always uniquely reconstruct the antecedent from the labels. For example, the sugared statement “p0 aced q0 ` s” in the proof corresponds to np, (np\\s)/np, np ` s.\nAlthough it is easy to verify that the proof of Figure 2 has correctly applied the rules of the Lambek calculus, finding such a proof from scratch may look a bit complicated (the key steps at the beginning of the proof involve introducing two np hypotheses and then deriving s/np to allow the object quantifier to take narrow scope). We will defer the question “given a statement Γ ` C, how do we\ndecide whether or not it is derivable?” to Section 4 but will first discuss how this proof corresponds to the following logical formula.\n∀x.[student(x)⇒ ∃y.[exam(y) ∧ ace(x, y)]]"
    }, {
      "heading" : "2.2 The syntax-semantics interface",
      "text" : "For the Lambek calculus, specifying the homomorphism to multiplicative intuitionistic linear logic is easy: we replace the two implications ‘\\’ and ‘/’ by the linear implication ‘(’ and the product ‘•’ by the tensor ‘⊗’. In a statement Γ ` C, Γ is now a multiset of formulas instead of a sequence. In other words, the sequent comma ‘,’ is now associative, commutative instead of associative noncommutative. For the proof of Figure 2 of the previous section, this mapping gives the proof shown in Figure 3.\nWe have kept the order of the premisses of the rules as they were in Figure 2 to allow for an easier comparison. This deep structure still uses the same atomic formulas as the Lambek calculus, it just forgets about the order of the formulas and therefore can no longer distinguish between the leftward looking implication ‘\\’ and the rightward looking implication ‘/’.\nTo obtain a semantics in the tradition of Montague (1974), we use the following mapping from syntactic types to semantic types, using Montague’s atomic types e (for entity) and t (for truth value).\nnp∗ = e\nn∗ = e→ t s∗ = t\n(A( B)∗ = A∗ → B∗\nApplying this mapping to the deep structure proof of Figure 3 produces the intuitionistic proof and the corresponding (linear) lambda term as shown in Figure 4\nThe computed term corresponds to the derivational semantics of the proof. To obtain the complete meaning, we need to substituting, for each of z0, . . . , z4, the meaning assigned in the lexicon.\nFor example “every” has syntactic type ((s/np)\\s)/n and semantic type (e → t) → (e → t) → t. The corresponding lexical lambda term of this type is λP e→t.λQe→t.(∀(λxe.((⇒ (P x))(Qx)))), with ‘∀’ a constant of type (e → t)→ t and ‘⇒’ a constant of type t→ (t→ t). In the more familiar Montague formulation, this lexical term corresponds to λP e→t.λQe→t.∀x.[(P x)⇒ (Qx)], where we can see the formula in higher-order logic we are constructing more clearly. Although the derivational semantics is a linear lambda term, the lexical term assigned to “every” is not, since the variable x has two bound occurrences.\nThe formula assigned to “some” has the same semantic type but a different term λP e→t.λQe→t.(∃(λxe.((∧(P x))(Qx)))).\nThe other words are simple, “exam” is assigned exame→t, “student” is assigned studente→t, and “aced” is assigned acee→(e→t).\nSo to compute the meaning, we start with the derivational semantics, repeated below.\n((z0 z1) (λx.((z3 z4)λy.((z2 y)x))))\nThen we substitute the lexical meanings, for z0, . . . , z4.\nz0 := λP e→t.λQe→t.(∀(λxe.((⇒ (P x))(Qx)))) z1 := student e→t z2 := ace e→(e→t) z3 := λP e→t.λQe→t.(∃(λxe.((∧(P x))(Qx)))) z4 := exam e→t\nThis produces the following lambda term.\n((λP e→t.λQe→t.(∀(λxe.((⇒ (P x))(Qx)))) studente→t) (λx.((λP e→t.λQe→t.(∃(λxe.((∧(P x))(Qx)))) exame→t)\nλy.((acee→(e→t) y)x))))\nFinally, when we normalize this lambda term, we obtain the following semantics for this sentence.\n(∀(λxe.((⇒ (studente→t)x))(∃(λye.((∧(exame→t y))(((acee→(e→t) y)x)))))\nThis lambda term represents the more readable higher-order logic formula3.\n∀x.[student(x)⇒ ∃y.[exam(y) ∧ ace(x, y)]]\nProofs in the Lambek calculus, and in type-logical grammars are subsets of the proofs in intuitionistic (linear) logic and these proofs are compatible with formal semantics in the tradition initiated by Montague (1974).\nThe example in this section presented the calculation of the semantics of a simple example in “slow motion”: many authors assign a lambda term directly to a proof in their type-logical grammar, leaving the translation to intuitionistic linear logic implicit.\nGiven a semantic analysis without a corresponding syntactic proof, we can try to reverse engineer the syntactic proof. For example, suppose we want to assign the reflexive “himself” the lambda term λR(e→e→t)λxe.((Rx)x), that is, a term of type (e → e → t) → e → t. Then, using some syntactic reasoning to eliminate implausible candidates like (np ( n) ( n, the only reasonable deep structure formula is (np ( np ( s) ( (np ( s) and, reasoning a bit further about which of the implications is left and right, we quickly end up with the quite reasonable (though far from perfect) Lambek calculus formula ((np\\s)/np)\\(np\\s).\n3We have used the standard convention in Montague grammar of writing (p x) as p(x) and ((p y)x) as p(x, y), for a predicate symbol p."
    }, {
      "heading" : "2.3 Going further",
      "text" : "Though the Lambek calculus is a beautiful and simple logic and though it gives a reasonable account of many interesting phenomena on the syntax-semantics interface, the Lambek calculus has a number of problems, which I will discuss briefly below. The driving force of research in type-logical grammars since the eighties has been to find solutions to these problems and some of these solutions will be the main theme of the next section.\nFormal language theory The Lambek calculus generates only context-free languages (Pentus 1997). There is a rather large consensus that natural languages are best described by a class of languages at least slightly large than the context-free languages. Classical examples of phenomena better analysed using so-call mildly context-sensitive language include verb clusters in Dutch and Swiss (Huijbregts 1984, Shieber 1985).\nThe Syntax-Semantics Interface Though our example grammar correctly predicted two readings for Sentence (1) above, our treatment of quantifiers doesn’t scale well. For example, if we want to predict two readings for the following sentence\n(3) A student aced every exam.\nthen we need to add an additional lexical entry both for “a” and for “every”; this is easily done, but we end up with two lexical formulas for both words. However, this would still not be enough. For example, the following sententece is also grammatical.\n(4) Alyssa gave every student a difficult exam.\n(5) Alyssa believes a student committed perjury.\nIn Sentence (4), “every student” does not occur in a peripheral position, and though it is possible to add a more complex formula with the correct behaviour, we would need yet another formula for Sentence (5). Sentence (5) is generally considered to have two readings: a de dicto reading, where Alyssa doesn’t have a specific student in mind (she could conclude this, for example, when two students make contradictory statements under oath, this reading can be felicitously followed by “but she doesn’t know which”), and a de re reading where Alyssa believes a specific student perjured. The Lambek calculus cannot generate this second reading without adding yet another formula for “a”.\nIt seems we are on the wrong track when we need to add a new lexical entry for each different context in which a quantifier phrase occurs. Ideally, we would like a single formula for “every” and “a” which applied in all these different cases.\nAnother way to see this is that we want to keep the deep structure formula n ( ((np ( s) ( s) and that we need to replace the Lambek calculus by\nanother logic such that the correct deep structures for the desired readings of Sentences like (4) and (5) are produced.\nLexical Semantics The grammar above also overgenerates in several ways.\n1. “ace” implies a (very positive) form of evaluation with respect to the object. “aced the exam” is good, whereas “aced Emory”, outside of the context of a tennis match is bad. “aced logic” can only mean something like “aced the exam for the logic course”.\n2. “during” and similar temporal adverbs imply its argument is a temporal interval: “during the exam” is good, but “during the student” is bad, and “during logic” can only mean something like “during the contextually understood logic lecture”\nIn the literature on semantics, there has been an influential movement towards a richer ontology of types (compared to the “flat” Montagovian picture presented above) but also towards a richer set of operations for combining terms of specific types, notably allowing type coercions (Pustejovsky 1995, Asher 2011). So an “exam” can be “difficult” (it subject matter, or informational content) but also “take a long time” (the event of taking the exam). The theory of semantics outlined in the previous section needs to be extended if we want to take these and other observations into account."
    }, {
      "heading" : "3 Modern type-logical grammars",
      "text" : "We ended the last section with some problems with using the Lambek calculus as a theory of the syntax-semantics interface. The problems are of two different kinds.\n1. The problems of the syntax-semantic interface, and, in a sense, also those of formal language theory are problems where the deep structure is correct but our syntactic calculus cannot produce an analysis mapping to the desired deep structure. We will present two solutions to these problems in Sections 3.1 and 3.2.\n2. The problems of lexical semantics, on the other hand require a more sophisticated type system than Montague’s simply typed type-logical with basic types e and t and mechanisms like coercions which allow us to conditionally “repair” certain type mismatches in this system. We will discuss a solution to this problem in Section 3.3."
    }, {
      "heading" : "3.1 Multimodal grammars",
      "text" : "Multimodal type-logical grammars (Moortgat 2011) take the non-associative Lambek calculus as its base, but allow multiple families of connectives.\nLogical Rules\n∆ ` A •i B Γ[A ◦i B] ` C Γ[∆] ` C [•iE] Γ ` A ∆ ` B Γ ◦i ∆ ` A •i B [•iI]\nΓ ` A/iB ∆ ` B Γ ◦i ∆ ` A [/iE] Γ ◦i B ` A Γ ` A/iB [/iI]\nΓ ` B ∆ ` B\\iA Γ ◦i ∆ ` A [\\iE] B ◦i Γ ` A Γ ` B\\iA [\\iI]\nStructural Rules\nFor the basic statements Γ ` C of the Lambek calculus, we ask the question whether we can derive formula C, the succedent, from a sequence of formulas Γ, the antecedent. In the multimodal Lambek calculus, the basic objects are labeled binary trees4. The labels come from a separate set of indices or modes I. Multimodal formulas are then of the form A/iB, A •i B and A\\iB, and antecedent terms are of the form Γ ◦i ∆, with i an index from I (we have omitted the outer brackets for the rules, but the operator ◦i is non-associative). Sequents are still written as Γ ` C, but Γ is now a binary branching, labeled tree with formulas as its leaves.\nGiven a set of words w1, . . . , wn and a goal formula C, the question is now: is there a labeled tree Γ with formulas A1, . . . , An as its yield, such that Γ ` C is deriable and Ai ∈ lex(wi) for all i (the implementation of Section 4.1 will automatically compute such a Γ).\nThe rules of multimodal type-logical grammars are shown in Table 3. In the rules, Γ[∆] denotes an antecedent tree Γ with distinguished subtree ∆ — the subtree notation is a non-associative version of the Lambek calculus antecedent Γ,∆,Γ′, where ∆ is a subsequence instead of a subtree as it is in Γ[∆].\nEach logical connective with mode i uses a structural connective ◦i in its rule. For the /E, •I and \\E rules, reading from premisses to conclusions, we build structure. For the /I, •E and \\I rules we remove a structural connective with the same mode as the logical connective. The natural deduction rules use explicit antecedents, although, for convenience, we will again use conindexation between the introduction rules for the implications ‘/’ and ‘\\’ and its withdrawn\n4We can also allow unary branches (and, more generally n-ary branches) and the corresponding logical connectives. The unary connectives ♦ and are widely used, but, since they will only play a marginal role in what follows, I will not present them to keep the current presentation simple. However, they form an essential part of the analysis of many phenomena and are consequently available in the implementation.\npremiss (and similarly for the •E rule and its two premisses). The main advantage of adding modes to the logic is that modes allow us to control the application of structural rules lexically. This gives us fine-grained control over the structural rules in our logic.\nFor example, the base logic is non-associative. Without structural rules, the sequent a/b, b/c ` a/c, which is derivable in the Lambek calculus is not derivable in its multimodal incarnation a/ab, b/ac ` a/ac. The proof attempt below, with the failed rule application marked by the ‘E’ symbol, shows us that the elimination rules and the introduction rule for this sequent do not match up correctly.\na/ab ` a/ab b/ac ` b/ac c ` c b/ac ◦a c ` b [/E]\na/ab ◦a (b/ac ◦a c) ` a [/E] (a/ab ◦a b/ac) ◦a c ` a E\na/ab ◦a b/ac ` a/ac [/I]\nThis is where the structural rules, shown at the bottom of Table 3 come in. The general form, read from top to bottom, states that we take a structure Γ containing a distinguished subtree Ξ which itself has n subtrees ∆1, . . . ,∆n, and we replace this subtree Ξ with a subtree Ξ′ which has the same number of subtrees, though not necessarily in the same order (π is a permutation on the leaves). In brief, we replace a subtree Ξ by another subtree Ξ′ and possibly rearranges the leaves (subtrees) of Ξ, without deleting or copying any subtrees. Examples of structural rules are the following.\nΓ[∆1 ◦a (∆2 ◦a ∆3)] ` C Γ[(∆1 ◦a ∆2) ◦a ∆3] ` C Ass Γ[(∆1 ◦1 ∆3) ◦0 ∆2] ` C Γ[(∆1 ◦0 ∆2) ◦1 ∆3] ` C MC\nThe first structural rule is one of the structural rules for associativity. It is the simplest rule which will make the proof attempt above valid (with Γ[] the empty context, ∆1 = a/ab, ∆2 = b/ac and ∆3 = c). This structural rule keeps the order of the ∆i the same.\nThe rule above on the right is slightly more complicated. There, the order of ∆2 and ∆3 is swapped as is the relative position of modes 0 and 1. Rules like this are called “mixed commutativity”, they permit controlled access to permutation. One way to see this rule, seen from top to bottom, is that is “moves out” a ∆3 constituent which is on the right branch of mode 1. Rules of this kind are part of the solution to phenomena like Dutch verb clusters (Moortgat & Oehrle 1994).\nMany modern type-logical grammars, such as the Displacement calculus and NLcl can be seen as multimodal grammars (Valent́ın 2014, Barker & Shan 2014)."
    }, {
      "heading" : "3.2 First-order linear logic",
      "text" : "We have seen that multimodal type-logical grammars generalize the Lambek calculus by offering the possibility of fine-tuned controlled over the application\nof structural rules. In this section, I will introduce a second way of extending the Lambek calculus.\nMany parsing algorithms use pairs of integers to represent the start and end position of substrings of the input string. For example, we can represent the sentence\n(6) Alyssa believes someone committed perjury.\nas follows (this is a slightly simplified version of Sentence (5) from Section 2.3); we have treated “committed perjury” as a single word.\n0 1 2 3 4 Alyssa believes someone committed perjury\nThe basic idea of first-order linear logic as a type-logical grammar is that we can code strings as pairs (or, more generally, tuples) of integers representing string positions. So for deciding the grammaticality of a sequence of words w1, . . . , wn ` C, with a goal formula C, we now give a parametric translation from ‖Ai‖i−1,i for each lexical entry wi and ‖C‖0,n for the conclusion formula.\nGiven these string positions, we can assign the noun phrase “Alyssa” the formula np(0, 1), that is a noun phrase from position 0 to position 1. The verb “believes”, which occurs above between position 1 and 2, can then be assigned the complex formula ∀x2.[s(2, x2)( ∀x1.[np(x1, 1)( s(x1, x2)]], meaning that it first selects a sentence to its right (that is, starting at its right edge, position 2 and ending anywhere) and then a noun phrase to its left (that is, starting anywhere and ending at its left edge, position 1) to produce a sentence from the left position of the noun phrase argument to the right position of the sentence argument.\nWe can systematize this translation, following Moot & Piazza (2001), and obtain the following translation from Lambek calculus formulas to first-order linear logic formulas.\n‖p‖x,y = p(x, y) ‖A/B‖x,y = ∀z.‖B‖y,z ( ‖A‖x,z ‖B\\A‖y,z = ∀x.‖B‖x,y ( ‖A‖x,z\n‖A •B‖x,z = ∃y.‖A‖x,y ⊗ ‖B‖y,z\nGiven this translation, the lexical entry for “believes” discussed above is simply the translation of the Lambek calculus formula (np\\s)/s, with position pair 1, 2, to first-order linear logic. Doing the same for “committed perjury” with formula np\\s and positions 3, 4 gives ∀z.[np(z, 3) ( s(z, 4)]. For “someone” we would simply translate the Lambek calculus formula s/(np\\s), but we can do better than that: when we translate “someone” as ∀y1.∀y2.[(np(2, 3) ( s(y1, y2))( s(y1, y2)], we improve upon the Lambek calculus analysis.\nAs we noted in Section 2.3, the Lambek calculus cannot generate the “de re” reading, where the existential quantifier has wide scope. Figure 5 shows how the simple first-order linear logic analysis does derive this reading.\nBesides the Lambek calculus, first-order linear logic has many other modern type-logical grammars as fragments. Examples include lambda grammars (Oehrle 1994), the Displacement calculus (Morrill et al. 2011) and hybrid typelogical grammars (Kubota & Levine 2012). We can see first-order linear logic as a sort of “machine language” underlying these different formalisms, with each formalism introducing its own set of abbreviations convenient for the grammar writer. Seeing first-order linear logic as an underlying language allows us to compare the analyses proposed for different formalisms and find, in spite of\ndifferent starting points, a lot of convergence. In addition, as discussed in Section 4.2, we can use first-order linear logic as a uniform proof strategy for these formalisms.\nSyntax-Semantics Interface As usual, we obtain the deep structure of a syntactic derivation by defining a homomorphism from the syntactic proof to a proof in multiplicative intuitionistic linear logic. For first-order linear logic, the natural mapping simply forgets all first-order quantifiers and replaces all atomic predicates p(x1, . . . , xn) by propositions p. Since the first-order variables have, so far, only been used to encode string positions, such a forgetful mapping makes sense.\nHowever, other solutions are possible. When we add semantically meaningful terms to first-order linear logic, the Curry-Howard isomorphism for the firstorder quantifiers will give us dependent types and this provides a natural connection to the work using dependent types for formal semantics (Ranta 1991, Pogodalla & Pompigne 2012, Luo 2012b, Luo 2015)."
    }, {
      "heading" : "3.3 The Montagovian Generative Lexicon",
      "text" : "In the previous sections, we have discussed two general solutions to the problems of the syntax-semantics interface of the Lambek calculus. Both solutions proposed a more flexible syntactic logic. In this section, we will discuss a different type of added flexibility, namely in the syntax-semantics interface itself.\nThe basic motivating examples for a more flexible composition have been amply debated in the literature (Pustejovsky 1995, Asher 2011). Our solution is essentially the one proposed by Bassac, Mery & Retoré (2010), called the Montagovian Generative Lexicon. I will only give a brief presentation of this framework. More details can be found in Chapter XYZ. Note to edi-\ntors: add link to appropriate chapter Like many other solutions, the first step consists of splitting Montague’s type e for entities into several types: physical objects, locations, informational objects, eventualities, etc. Although there are different opinions with respect to the correct granularity of types (Pustejovsky 1995, Asher 2011, Luo 2012a), nothing much hinges on this for the present discussion.\nThe second key element is the move to the second-order lambda calculus, system F (Girard, Lafont & Regnier 1995), which allows abstraction over types as well as over terms. In our Lambek calculus, the determiner “the” was assigned the formula np/n and the type of its lexical semantics was therefore (e→ t)→ e, which we implement using the ι operators of type (e→ t)→ e, which, roughly speaking, selects a contextually salient entity from (a characteristic function of) a set. When we replace the single type e by several different types, we want to avoid listing several separate syntactically identical by semantically different entries for “the” in the lexicon, and therefore assign it a polymorphic term Λα.ι(α→t)→α of type Πα.((α → t) → α), quantifying over all types α. Though this looks problematic, the problem is resolved once we realize that only certain function words (quantifiers, conjunctions like “and”) are assigned polymorphic terms and that we simply use universal instantiation to obtain the value of the\nquantifier variable. So if “student” is a noun of type human, that is of type h → t, then “the student” will be of type h, instantiating α to h. Formally, we use β reduction as follows (this is substitution of types instead of terms, substituting type h for α).\n((Λα.ι(α→t)→α){h} studenth→t) =β (ι student)h\nThe final component of the Montagovian Generative Lexicon is a set of lexically specified, optional transformations. In case of a type mismatch, an optional transformation can “repair” the term.\nAs an example from Moot & Retoré (2011) and Mery, Moot & Retoré (2013), one of the classic puzzles in semantics are plurals and collective and distributive readings. For example, verbs like “meet” have collective readings, the apply to groups of individuals collectively, so we have the following contrast, where collectives like committees and plurals like students can meet, but not singular or distributively quantified noun phrases. The contrast with verbs like “sneeze”, which force a distributive reading is clear.\n(7) The committee met.\n(8) All/the students met\n(9) *A/each/the student met.\n(10) All/the students sneezed.\n(11) A/each/the student sneezed.\nIn the Montagovian Generative lexicon, we can models these fact as follows. First, we assign the plural morphology “-s” the semantics ΛαλPα→tλQα→t.|Q| > 1∧∀xα.Q(x)⇒ P (x), then “students” is assigned the following term λQh→t.|Q| > 1 ∧ ∀xhQ(x)⇒ student(x), that is the sets of cardinality greater than one such that all its members are students. Unlike “student” which was assigned a term of type h→ t, roughly a propery of humans, the plural “students” is assigned a term of type (h→ t)→ t, roughly a property of sets of humans. Consequently, the contrast between “the student” and “the students” is that the first is of type h (a human) and the second of type h→ t (a set of humans) as indicated below.\nphrase syntactic type lambda-term the student np (ιstudent)h the students np (ι(λQh→t.|Q| > 1 ∧ ∀xhQ(x)⇒ student(x)))h→t\nTherefore, the meaning of “the students” is the contextually determined set of humans, from the sets of more than one human such that all of them are students.\nThen we distinguish the verbs “meet” and “sneeze” as follows, with the simpler verb “sneeze” simply selecting for a human subject and the collective verb “meet” selecting for a set of humans (of cardinality greater than one) as its subject.\nword syntactic type lambda-term met np\\s λPh→t.|P | > 1 ∧meet(P )\n# ΛαλR(α→t)→tλS(α→t)→t∀Pα→t.S(P )⇒ R(P ) met# np\\s λR(h→t)→t∀Ph→t.R(P )⇒ |P | > 1 ∧meet(P ) sneezed np\\s λxh.sneeze(x) * ΛαλPα→tλQα→t∀xα.Q(x)⇒ P (x) sneezed∗ np\\s λPh→t.∀xh.P (x)⇒ sneeze(x)\nGiven these basic lexical entries, we already correctly predict that “the student met” is ill-formed semantically (there is an unresolvable type mismatch) but “the students met” and “the student sneezed” are given the correct semantics.\nThe interesting case is “the students sneezed” which has as its only reading that each student sneezed individually. Given that “the students” is of type h → t and that “sneezed” requires an argument of type h, there is a type mismatch when we apply the two terms. However, “sneeze” has the optional distributivity operator ‘*’, which when we apply it to the lexical semantics for “sneeze” produces the term λPh→t.∀xh.P (x)⇒ sneeze(x), which combines with “the students” to produce the reading.\n∀xh.(ι(λQh→t.|Q| > 1 ∧ ∀yhQ(y)⇒ student(y))x)⇒ sneeze(x)\nIn other words, all of the members of the contextually determined set of more than human which are all students, sneeze.\nThe basic idea for the Montagovian Generative Lexicon is that lexical entries specify optional transformations which can repair certain sorts of type mismatches in the syntax-semantics interface. This adaptability allows the framework to solve many semantic puzzles.\nThough a proof-of-concept application of these ideas exists, more robust and scalable applications, as well as efforts incorporate these ideas into wide-coverage semantics, are ongoing research."
    }, {
      "heading" : "4 Theorem proving",
      "text" : "When looking at the rules and examples for the different logics, the reader may have wondered: how do we actually find proofs for type-logical grammars? This question becomes especially urgent once our grammars become more complex and the consequences of our lexical entries, given our logic, become hard to oversee. Though pen and paper generally suffice to show that a given sentence is derivable for the desired reading, it is generally much more laborious to show that a given sentence is underivable or that it has only the desired readings. This is where automated theorem provers are useful: they allow more extensive and intensive testing of your grammars, producing results more quickly and with less errors.\nThough the natural deduction calculi we have seen so far can be used for automated theorem proving (Carpenter 1994, Moot & Retoré 2012), and though\nLambek (1958) already gave a sequent calculus decision procedure, both logics have important drawbacks for proof search.\nNatural deduction proofs have a 1-1 correspondence between proofs and readings, though this is somewhat complicated to enforce for a logic with the •E rule (and the related ♦E rule). For the sequent calculus, the product rule is just like the other rules, but sequent calculus suffers from the so-called “spurious ambiguity” problem, which means that it generates many more proofs than readings.\nFortunately, there are proof systems which combine the good aspects of these calculi and eliminate their drawbacks. Proof nets are a graphical representation of proofs first introduced for linear logic (Girard 1987). Proof nets suffer neither from spurious ambiguity nor from complications for the product rules.\nProof nets are usually defined as a subset of a larger class, called proof structures. Proof structures are “candidate proofs”: part of the search space of a naive proof search procedure which need not correspond to actual proofs. Proof nets are those proof structures which correspond to sequent proofs. Perhaps surprisingly, we can distinguish proof nets from other proof structures by looking only at graph-theoretical properties of these structures.\nProof search for type-logical grammars using proof nets uses the following general procedure.\n1. For each of the words in the input sentence, find one of the formulas assigned to it in the lexicon,\n2. Unfold the formulas to produce a partial proof structure\n3. Enumerate all proof structures for the given formulas by identifying nodes in the module.\n4. Check if the resulting proof structure is a proof net according to the correctness condition."
    }, {
      "heading" : "4.1 Multimodal proof nets",
      "text" : "Table 5 presents the links for multimodal proof nets. The top row list the links corresponding to the elimination rules of natural deduction, the bottom row those corresponding to the introduction rules. There are two types of links: tensor links, with an open center, and par links, with a filled center. Par links have a single arrow pointing to the main formula of the link (the complex formula containing the principal connective). The top and bottom row are up-down symmetric with tensor and par reversed. The tensor links correspond to the logical rules which build structure when we read them from top to bottom, the par links to those rules which remove structure.\nThe formulas written above the central node of a link are its premisses, whereas the formulas written below it are its conclusions. Left-to-right order of the premisses as well as the conclusions is important.\nA proof structure is a set of formula occurrences and a set of links such that:\n1. each formula is at most once the premiss of a link,\n2. each formula is at most once the conclusion of a link.\nA formula which is not the premiss of any link is a conclusion of the proof structure. A formula which is not the conclusion of any link is a hypothesis of the proof structure. We say a proof structure with hypotheses Γ and conclusions ∆ is a proof structure of Γ ` ∆ (we are overloading of the ‘`’ symbol here, though this use should always be clear from the context; note that ∆ can contain multiple formulas).\nAfter the first step of lexical lookup we have a sequent Γ ` C, and we can enumerate its proof structures as follows: unfold the formulas in Γ, C until we reach the atomic subformulas (this is step 2 of the general procedure), then identify atomic subformulas (step 3 of the general procedure, we turn to the last step, checking correctness below). This identification step can, by the conditions on proof structures only identify hypotheses with conclusions and must leave all formulas of Γ, including atomic formulas, as hypotheses and C as a conclusion.\nFigure 6 shows the lexical unfolding of the sequent a/ab, b/ac ` a/ac. It is already a proof structure, though a proof structure of a, a/ab, b, b/ac, c ` a, a/ac, b, c (to the reader familiar with the proof nets of linear logic: some other presentations of proof nets use more restricted definitions of proof structures where a “partial proof structure” such as shown in the figure is called a module).\nTo turn the proof structure in a proof structure of a/ab, b/ac ` a/ac, we identify the atomic formulas. In this case, there is only a single way to do this, since a, b and c all occur once as a hypothesis and once as a conclusion, though in general there may be many possible matchings. Figure 7 shows, on the left, the proof structure after identifying the a abd b formulas. Since left and right (linear order), up and down (premiss, conclusion) have meaning in the graph, connecting the c formulas is less obvious: c is a conclusion of the /I link and must therefore be below it, but a premiss of the /E link and must therefore be above it. This is hard to achieve in the figure shown on the left. Though a possible solution would be to draw the figure on a cylinder, where “going up” from the topmost c we arrive at the bottom one, for ease of type-setting and reading the figure, I have chosen the representation shown in Figure 7 on the right. The curved line goes up from the c premiss of the /E link and arrives from below at the /I link, as desired. One way so see this strange curved connection is as a graphical representation of the coindexation of a premiss with a rule in the natural deduction rule for the implication.\nFigure 7 therefore shows, on the right, a proof structure for a/ab, b/ac ` a/ac. However, is it also a proof net, that is, does it correspond to a proof? In a multimodal logic, the answer depends on the available structural rules. For example, if no structural rules are applicable to mode a then a/ab, b/ac ` a/ac is underivable, but if mode a is associative, then it is derivable.\nWe decide whether a proof structure is a proof net based only on properties of the graph. As a first step, we erase all formula information from the internal nodes of the graph; for administrative reasons, we still need to be able to identify which of the hypotheses and conclusion of the structure correspond to which\nformula occurrence5. All relevant information for correctness is present in this graph, which we call an abstract proof structure.\nWe talked about how the curved line in proof structures (and abstract proof structure) corresponds to the coindexation of discharged hypotheses with rule names for the implication introduction rules. However, the introduction rules for multimodal type-logical grammars actually do more than just discharge a hypothesis, they also check whether the discharged hypothesis is the immediate left (for \\I) or right (for /I) daughter of the root node, that is, that the withdrawn hypothesis A occurs as A ◦i Γ (for \\I and mode i) or Γ ◦i A (for /I and mode i). The par links in the (abstract) proof structure represent a sort of “promise” that will produce the required structure. We check whether it is satisfied by means of contractions on the abstract proof structure.\nThe multimodal constraction are shown in Table 6. All portrayed configurations contract to a single vertex: we erase the two internal vertices and the paired links and we identify the two external vertices, keeping all connections of the external vertices to the rest of the abstract proof structure as they were: the vertex which is the result of the contraction will be a conclusion of the same link as the top external vertex (or a hypothesis of the abstract proof structure in case it wasn’t) and it will be a premiss of the same link as the bottom external\n5We make a slight simplification here. A single vertex abstract proof structure can have both a hypothesis and a conclusion without these two formulas necessarily being identical, eg. for sequents like (a/b) • b ` a. Such a sequent would correspond to the abstract proof structure (a/b)•b · a . So, formally, both the hypotheses and the conclusions of an abstract proof structure are assigned a formula and when a node is both a hypothesis and a conclusion it can be assigned two different formulas. In order not to make the notation of abstract proof structure more complex, we will stay with the simpler notation. Moot & Puite (2002) present the full details.\nvertex (or a conclusion of the abstract proof structure in case it wasn’t). The contraction for /I checks if the withdrawn hypothesis is the right daughter of a tensor link with the same mode information i, and symmetrically for the \\I contraction. The •E contraction contracts two hypotheses occurring as sister nodes.\nAll contractions are instantiations of the same pattern: a tensor link and a par link are connected, respecting left-right and up-down at both vertices except the one with the arrow.\nTo get a better feel for the contractions, we will start with its simplest instances. When we do pattern matching on the contraction for /I, we see that it corresponds to the following patterns, depending on our choice for the tensor link (the par link is always /I).\nC/iB ` C/iB A ` (A •i B)/iB A ` C/i(A\\iC)\nA proof structure is a proof net iff it contracts to a tree containing only tensor links using the contractions of Table 6 and any structural rewrites, discussed below — Moot & Puite (2002) present full proofs. In other words, we need to contract all par links in the proof structure according to their contraction, each contraction ensuring the correct application of the rule after which it is named. The abstract proof structure on the right of Figure 8 does not contract, since there is no substructure corresponding to the /I contraction: for a valid contraction, a par link is connected to both “tentacles” of a single tensor link, and the two tentacles without arrow are connected to different tensor links. This is correct, since a/ab, b/ac ` a/ac is underivable in a logic without structural rules for a.\nHowever, we have seen that this statement becomes derivable once we add associativity of a and it is easily verified to be a theorem of the Lambek calculus.\nHow can we add a modally controlled version of associativity to the proof net calculus? We can add such a rule by adding a rewrite from a tensor tree to another tensor tree with the same set of leaves. The rewrite for associativity is shown in Figure 9. To apply a structural rewrite, we replace the tree on the left hand side of the arrow by the one on the right hand side, reattaching the leaves and the root to the rest of the proof net.\nJust like the structural rules, a structural rewrite always has the same leaves on both sides of the arrow — neither copying nor deletion is allowed6, though we can reorder the leaves in any way (the associativity rule doesn’t reorder the leaves).\nFigure 10 shows how the contractions and the structural rewrites work together to derive a/ab, b/ac ` a/ac.\nWe start with a structural rewrite, which rebrackets the pair of tensor links.\n6From the point of view of linear logic, we stay within the purely multiplicative fragment, which is simplest proof-theoretically.\nThe two hypotheses are now the premisses of the same link, and this also produces a contractible structure for the /I link. Hence, we have shown the proof structure to be a proof net.\nthe 1\nn 1\nexam 4\n2\nwas 5\nn\n1\n8 Goal\n2\n3\na 1\n2\n6\n3\n2\n3\nIn the Grail theorem prover, the representation of abstract proof structures looks as shown in Figure 11 (this is an automatically produced subgraph close to the graph on the left of Figure 10, though with a nonassociative mode n and therefore not derivable). This graph is used during user interaction. The graphs are drawn using GraphViz, an external graph drawing program which does not guarantee respecting our desires for left, right and top/bottom, so tentacles are labeled 1, 2 and 3 (for left, right and top/bottom respectively) to allow us to make these distinctions regardless of the visual representation. Vertices are given unique identifiers for user interaction, for example to allow specifying which pair of atoms should be identified or which par link should be contracted.\nAlthough the structural rules give the grammar writer a great deal of flexibility, such flexibility complicates proof search. As discussed at the beginning of Section 4, theorem proving using proof nets is a four step process, which in the current situation looks as follows: 1) lexical lookup, 2) unfolding, 3) identification of atoms, 4) graph rewriting. In the current case, both the graph rewriting and the identification of atoms are complicated7 and since we can interleave the atom connections and the graph rewriting it is not a priori clear which strategy is optimal for which set of structural rules. The current implementation does graph rewriting only once all atoms have been connected.\nThe Grail theorem prover implements some strategies for early failure. Since all proofs in multimodal type-logical grammars are a subset of the proofs in multiplicative linear logic, we can reject (partial) proof structures which are invalid in multiplicative linear logic, a condition which is both powerful and easy to check.\nAs a compromise between efficiency and flexibility, Grail allows the grammar writer to specify a first-order approximation of her structural rules. Unlike the test for validity in multiplicative linear logic which is valid for any set of\n7Lexical ambiguity is a major problem for automatically extracted wide-coverage grammars as well, though standard statistical methods can help alleviate this problem (Moot 2010).\nstructural rules, specifying such a first-order approximation is valid only when there is a guarantee that all derivable sequents in the multimodal grammar are a subset of their approximations derivable in first-order linear logic. Errors made here can be rather subtle and hard to detect. It is recommended to use such methods to improve parsing speed only when a grammar has been sufficiently tested and where it is possible to verify whether no valid readings are excluded, or, ideally, to prove that the subset relation holds between the multimodal logic and its first-order approximation.\nThe next section will discuss first-order proof nets in their own right. Though these proof nets have been used as an underlying mechanism in Grail for a long time, we have seen in Section 3.2 that many modern type-logical grammars are formulated in a way which permits a direct implementation without an explicit set of structural rules.\nAs to the proof search strategy used by Grail, it is an instance of the “dancing links” algortihm (Knuth 2000): when connecting atomic formulas, we always link a formula which has the least possibilities and we rewrite the abstract proof structures only once a fully linked proof structure has been produced. Though the parser is not extremely fast, evaluation both on randomly generated statements and on multimodal statements extracted from corpora show that the resulting algorithm performs more than well enough (Moot 2008)."
    }, {
      "heading" : "4.2 First-order proof nets",
      "text" : "Proof nets for first-order linear logic (Girard 1991) are a simple extension of the proof nets for standard, multiplicative linear logic (Danos & Regnier 1989). Compared to the multimodal proof nets of the previous section, all logical links have the main formula of the link as their conclusion but there is now a notion of polarity, corresponding to whether or not the formula occurs on the left hand side of the turnstile (negative polarity) or on the right hand side (positive polarity).\nWe unfold a sequent A1, . . . , An ` C by using the negative unfolding for each of the Ai and the positive unfolding for C. The links for first-order proof nets are shown in Table 7.\nContrary to multiplicative proof nets, where a tensor link was drawn with an open central node and a par link with a filled central node, here par links are drawn as a connected pair of dotted lines and tensor links as a pair of solid lines.\nAs before, premisses are drawn above the link and conclusions are drawn below it. With the exception of the cut and axiom links, the order of the premisses and the conclusions is important. We assume without loss of generality that every quantifier link uses a distinct eigenvariable.\nA set of formula occurrences connected by links is a proof structure if every formula is at most once the premiss of a link and if every formula is exactly once the conclusion of a link. Those formulas which are not the premiss of any link are the conclusions of the proof structure — note the difference with multimodal proof nets: a proof structure has conclusions but no hypotheses\nand, as a consequence, each formula in the proof net must be the conclusion of exactly one (instead of at most one) link.\nFor polarized proof nets, unfolding the formulas according to the links of Table 7 no longer produces a proof structure, since the atomic formulas after unfolding are not the conclusions of any link. Such “partial proof structures” are called a modules. To turn a module into a proof structure, we connect atomic formulas of opposite polarity by axiom links until we obtain a complete matching of the atomic formulas, that is until every atomic formula is the conclusion of an axiom link.\nThe negative ∀ and the positive ∃ link, are defined using substitution of an arbitrary term t for the eigenvariable of the link. In actual proof search, we use unification of these variables when the axiom links are performed.\nAs usual, not all proof structures are proof nets. However, since the logical rules for the quantifiers make essential use of the notion of “free occurrence of a variable”, this should be reflected in out correctness condition. Girard (1991) uses a notion of switching for proof structures which extends the switchings of Danos & Regnier (1989).\nA switching is, for each of the binary par links a choice of its left or right premiss and for each of the unary par links with eigenvariable x a choice of one of the formulas in the structure with a free occurrence of x or of the premiss of the rule.\nGiven a switching, a correction graph replaces a binary par link by a connection from the conclusion of the link to the premiss chosen by the switching, and it replace a unary par link by a link from the conclusion to the formula chosen by the switching.\nFinally, a proof structure is a proof net when all its correction graphs are both acyclic and connected (Girard 1991).\nAs an example, look at the proof structure of a( ∃x.b(x) ` ∃y.[a( b(y)] shown in Figure 12. This statement is not derivable in first-order linear logic (nor in intuitionistic logic). Consider therefore the switching connecting the binary par link to its left premiss a and the link for x to the formula a( b(x) (it has a free occurrence of x, so this like is a valid switching).\nThis switching produces the correction graph shown in Figure 13. It contains a cycle, drawn with bold edges, and is therefore not a proof structure (in addition, the b axiom is disconnected from the rest of the structure, giving a second reason for rejecting the proof structure).\nContractions Though switchings conditions for proof nets are simple and elegant, they don’t lend themselves to naive application: already for the example proof structure of Figure 12 there are six possible switchings to consider and, as the reader can verify, only the switching shown in Figure 13 is cyclic (and disconnected). In general, it is often the case that all switchings but one are acyclic and connected, as it is here.\nThough there are efficient ways of testing acyclicity and connectedness for multiplicative proof nets (Guerrini 1999, Murawski & Ong 2000) and it seems these can be adapted to the first-order case (though some care needs to be taken when we allow complex terms), the theorem prover for first-order linear logic uses a extension of the contraction criterion of Danos (1990).\nGiven a proof structure we erase all formulas from the vertices and keep only a set of the free variables at this vertex. We then use the contractions of Table 8 to contract the edges of the graph. The resulting vertex of each contraction has\nthe union of the free variables of the two vertices of the redex (we remove the eigenvariable x of a ∀ contraction, which is labeled u). A proof structure is a proof net iff it contracts to a single vertex using the contractions of Table 8.\nTo give an example of the contractions, Figure 14 shows the contractions for the underivable proof structure of Figure 12. The initial structure, which simply takes the proof structure of Figure 12 and replaces the formulas by the corresponding set of free variables, is shown on the left. Contracting the five solid edges using the c contraction produces the structure shown in the figure on the right.\nNo further contractions apply: the two connected dotted links from the binary par link do not end in the same vertex, so the par contraction p cannot apply. In addition, the universal contraction u cannot apply either, since it requires all vertices with its eigenvariable x to occur at the node from which the arrow is leaving and there is another occurrence of x at the bottom node of the structure. We have therefore shown that this is not a proof net.\nSince there are no structural rewrites, the contractions for first-order linear logic are easier to apply than those for multimodal type-logical grammars: it is rather easy to show confluence for the contractions (the presence of structural rules, but also the unary versions of the multimodal contractions, means confluence is not guaranteed for multimodal proof nets). We already implicitly\nused confluence when we argued that the proof structure in Figure 14 was not a proof net. The theorem prover uses a maximally contracted representation of the proof structure to represent the current state of proof search and this means less overhead and more opportunities for early failure during proof search.\nLike before, the theorem proving uses four steps, which look as follows in the first-order case: 1) lexical lookup, 2) unfolding, 3) axiom links with unification, 4) graph contraction. Unlike the multimodal proof nets of the previous section, the graph contractions are now confluent and can be performed efficiently (the linear time solutions for the multiplicative case may be adaptable, but a naive implementation already has an O(n2) worst-case performance). After lexical lookup, theorem proving for first-order linear logic unfolds the formulas as before, but uses a greedy contraction strategy. This maximally contracted partial proof net constrains further axiom links: for example, a vertex containing a free variable x cannot be linked to the conclusion of the edge of its eigenvariable (the vertex to which the arrow of the edge with variable x points) or to one of its descendants, since such a structure would fail to satisfy the condition that the two vertices of a ∀ link for the u contraction of Figure 8 are distinct. Another easily verified constraint is that two atomic formulas can only be connected by an axiom link if these formulas unify8. Like for multimodal proof nets, the firstorder linear logic theorem prover chooses an axiom link for one of the atoms with the fewest possibilities."
    }, {
      "heading" : "4.3 Tools",
      "text" : "Table 9 lists the different theorem provers which are available. Grail 0 (Moot, Schrijen, Verhoog & Moortgat 2015) and Grail 3 (Moot 2015a) use the multimodal proof net calculus of Section 4.1, whereas LinearOne (Moot 2015c) uses the first-order proof nets of Section 4.2. GrailLight (Moot 2015b) is a specialpurpose chart parser, intended for use with an automatically extracted French\n8As discussed in Section 4.1, the multimodal theorem prover allows the grammar writer to specify first-order approximations of specific formulas. So underneath the surface of Grail there is some first-order reasoning going on as well.\ngrammar for wide-coverage parsing and semantics (Moot 2010, Moot 2012). All provers are provided under the GNU Lesser General Public License — this means, notably, there is no warranty, though I am committed to making all software as useful as possible; so contact me for any comments, feature requests or bug reports. All theorem provers can be downloaded from the author’s GitHub site.\nhttps://github.com/RichardMoot/\nThe columns of table Table 9 indicate whether the theorem provers provide natural deduction output, graph output (of the partial proof nets), whether there is an interactive mode for proof search, whether the implementation is complete and whether the grammar can specify its own set of structural rules; “NA” means the question doesn’t apply to the given system (GrailLight doesn’t use a graphs to represent proofs and first-order linear logic does not have a grammar-specific set of structural rules). The table should help you select the most adequate tool for your purposes.\nLinearOne provides natural deduction output not only for first-order linear logic, but also for the Displacement calculus, hybrid type-logical grammars and lambda grammars. That is, the grammar writer can write a grammar in any of these formalisms, LinearOne will do proof search of the translation of this grammar in first-order linear logic and then translate any resulting proofs back to the source language.\nThe syntactic example proofs in this chapter have been automatically generated using these tools and the corresponding grammars files, as well as many other example grammars, are included in the repository."
    } ],
    "references" : [ {
      "title" : "Die syntaktische Konnexität’, Studies in Philosophy",
      "author" : [ "K. Ajdukiewicz" ],
      "venue" : null,
      "citeRegEx" : "Ajdukiewicz,? \\Q1935\\E",
      "shortCiteRegEx" : "Ajdukiewicz",
      "year" : 1935
    }, {
      "title" : "A quasi-arithmetical notation for syntactic description",
      "author" : [ "Y. Press. Bar-Hillel" ],
      "venue" : null,
      "citeRegEx" : "Bar.Hillel,? \\Q1953\\E",
      "shortCiteRegEx" : "Bar.Hillel",
      "year" : 1953
    }, {
      "title" : "Continuations and Natural Language, Oxford Studies in Theoretical Linguistics",
      "author" : [ "C. Barker", "C. Shan" ],
      "venue" : null,
      "citeRegEx" : "Barker and Shan,? \\Q2014\\E",
      "shortCiteRegEx" : "Barker and Shan",
      "year" : 2014
    }, {
      "title" : "Towards a type-theoretical account of lexical semantics",
      "author" : [ "C. Bassac", "B. Mery", "C. Retoré" ],
      "venue" : "Journal of Logic, Language and Information",
      "citeRegEx" : "Bassac et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Bassac et al\\.",
      "year" : 2010
    }, {
      "title" : "A natural deduction theorem prover for type-theoretic categorial grammars",
      "author" : [ "B. Carpenter" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Carpenter,? \\Q1994\\E",
      "shortCiteRegEx" : "Carpenter",
      "year" : 1994
    }, {
      "title" : "Natural language reasoning using Coq: Interaction and automation, in ‘Proceedings of Traitement Automatique des Langues Naturelles (TALN 2015)",
      "author" : [ "S. Chatzikyriakidis" ],
      "venue" : null,
      "citeRegEx" : "Chatzikyriakidis,? \\Q2015\\E",
      "shortCiteRegEx" : "Chatzikyriakidis",
      "year" : 2015
    }, {
      "title" : "The structure of multiplicatives",
      "author" : [ "V. Danos", "L. Regnier" ],
      "venue" : "Archive for Mathematical Logic",
      "citeRegEx" : "Danos and Regnier,? \\Q1989\\E",
      "shortCiteRegEx" : "Danos and Regnier",
      "year" : 1989
    }, {
      "title" : "Linear logic",
      "author" : [ "Girard", "J.-Y" ],
      "venue" : "Theoretical Computer Science",
      "citeRegEx" : "Girard and J..Y.,? \\Q1987\\E",
      "shortCiteRegEx" : "Girard and J..Y.",
      "year" : 1987
    }, {
      "title" : "Quantifiers in linear logic II",
      "author" : [ "Girard", "J.-Y" ],
      "venue" : "Proceedings of the conference with the same name, Viareggio,",
      "citeRegEx" : "Girard and J..Y.,? \\Q1991\\E",
      "shortCiteRegEx" : "Girard and J..Y.",
      "year" : 1991
    }, {
      "title" : "Correctness of multiplicative proof nets is linear, in ‘Fourteenth",
      "author" : [ "S. Guerrini" ],
      "venue" : "Annual IEEE Symposium on Logic in Computer Science’,",
      "citeRegEx" : "Guerrini,? \\Q1999\\E",
      "shortCiteRegEx" : "Guerrini",
      "year" : 1999
    }, {
      "title" : "The weak inadequacy of context-free phrase structure grammars",
      "author" : [ "R. Huijbregts" ],
      "venue" : null,
      "citeRegEx" : "Huijbregts,? \\Q1984\\E",
      "shortCiteRegEx" : "Huijbregts",
      "year" : 1984
    }, {
      "title" : "Dancing links’, arXiv preprint cs/0011047",
      "author" : [ "D.E. Knuth" ],
      "venue" : null,
      "citeRegEx" : "Knuth,? \\Q2000\\E",
      "shortCiteRegEx" : "Knuth",
      "year" : 2000
    }, {
      "title" : "Gapping as like-category coordination",
      "author" : [ "Y. Kubota", "R. Levine" ],
      "venue" : "‘Logical Aspects of Computational Linguistics’,",
      "citeRegEx" : "Kubota and Levine,? \\Q2012\\E",
      "shortCiteRegEx" : "Kubota and Levine",
      "year" : 2012
    }, {
      "title" : "The mathematics of sentence structure",
      "author" : [ "J. Lambek" ],
      "venue" : "American Mathematical Monthly",
      "citeRegEx" : "Lambek,? \\Q1958\\E",
      "shortCiteRegEx" : "Lambek",
      "year" : 1958
    }, {
      "title" : "Common nouns as types, in ‘Logical aspects of computational linguistics (LACL2012)",
      "author" : [ "Z. Luo" ],
      "venue" : null,
      "citeRegEx" : "Luo,? \\Q2012\\E",
      "shortCiteRegEx" : "Luo",
      "year" : 2012
    }, {
      "title" : "Formal semantics in modern type theories with coercive subtyping",
      "author" : [ "Z. Luo" ],
      "venue" : "Linguistics and Philosophy",
      "citeRegEx" : "Luo,? \\Q2012\\E",
      "shortCiteRegEx" : "Luo",
      "year" : 2012
    }, {
      "title" : "A Lambek calculus with dependent types, in ‘Types for Proofs and Programs (TYPES 2015)’, Tallinn",
      "author" : [ "Z. Luo" ],
      "venue" : null,
      "citeRegEx" : "Luo,? \\Q2015\\E",
      "shortCiteRegEx" : "Luo",
      "year" : 2015
    }, {
      "title" : "Plurals: individuals and sets in a richly typed semantics, in ‘The Tenth International Workshop of Logic and Engineering of Natural Language Semantics 10 (LENLS10)",
      "author" : [ "B. Mery", "R. Moot", "C. Retoré" ],
      "venue" : null,
      "citeRegEx" : "Mery et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mery et al\\.",
      "year" : 2013
    }, {
      "title" : "Higherorder logical inference with compositional semantics, in ‘Proceedings of Empirical Method for Natural Language Processing (EMNLP 2015)",
      "author" : [ "K. Mineshima", "P. Mart́ınez-Gómez", "Y. Miyao", "D. Bekki" ],
      "venue" : null,
      "citeRegEx" : "Mineshima et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mineshima et al\\.",
      "year" : 2015
    }, {
      "title" : "The proper treatment of quantification in ordinary English, in R. Thomason, ed., ‘Formal Philosophy",
      "author" : [ "R. Montague" ],
      "venue" : null,
      "citeRegEx" : "Montague,? \\Q1974\\E",
      "shortCiteRegEx" : "Montague",
      "year" : 1974
    }, {
      "title" : "Categorial type logics",
      "author" : [ "M. Moortgat" ],
      "venue" : "North-Holland Elsevier, Amsterdam,",
      "citeRegEx" : "Moortgat,? \\Q2011\\E",
      "shortCiteRegEx" : "Moortgat",
      "year" : 2011
    }, {
      "title" : "Adjacency, dependency and order, in ‘Proceedings 9th Amsterdam Colloquium",
      "author" : [ "M. Moortgat", "R.T. Oehrle" ],
      "venue" : null,
      "citeRegEx" : "Moortgat and Oehrle,? \\Q1994\\E",
      "shortCiteRegEx" : "Moortgat and Oehrle",
      "year" : 1994
    }, {
      "title" : "Filtering axiom links for proof nets",
      "author" : [ "R. Moot" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Moot,? \\Q2008\\E",
      "shortCiteRegEx" : "Moot",
      "year" : 2008
    }, {
      "title" : "Wide-coverage French syntax and semantics using Grail, in ‘Proceedings of Traitement Automatique des Langues Naturelles (TALN)",
      "author" : [ "R. Moot" ],
      "venue" : null,
      "citeRegEx" : "Moot,? \\Q2010\\E",
      "shortCiteRegEx" : "Moot",
      "year" : 2010
    }, {
      "title" : "Wide-coverage semantics for spatio-temporal reasoning",
      "author" : [ "R. Moot" ],
      "venue" : "Traitement Automatique des Languages",
      "citeRegEx" : "Moot,? \\Q2012\\E",
      "shortCiteRegEx" : "Moot",
      "year" : 2012
    }, {
      "title" : "Grail’, http://www.labri.fr/perso/moot/grail3.html. Mature and flexible parser for multimodal grammars",
      "author" : [ "R. Moot" ],
      "venue" : null,
      "citeRegEx" : "Moot,? \\Q2015\\E",
      "shortCiteRegEx" : "Moot",
      "year" : 2015
    }, {
      "title" : "Grail light’, https://github.com/RichardMoot/GrailLight. Fast, lightweight version of the Grail parser",
      "author" : [ "R. Moot" ],
      "venue" : null,
      "citeRegEx" : "Moot,? \\Q2015\\E",
      "shortCiteRegEx" : "Moot",
      "year" : 2015
    }, {
      "title" : "Linear one: A theorem prover for first-order linear logic’, https://github.com/RichardMoot/LinearOne",
      "author" : [ "R. Moot" ],
      "venue" : null,
      "citeRegEx" : "Moot,? \\Q2015\\E",
      "shortCiteRegEx" : "Moot",
      "year" : 2015
    }, {
      "title" : "Linguistic applications of first order multiplicative linear logic",
      "author" : [ "R. Moot", "M. Piazza" ],
      "venue" : "Journal of Logic, Language and Information",
      "citeRegEx" : "Moot and Piazza,? \\Q2001\\E",
      "shortCiteRegEx" : "Moot and Piazza",
      "year" : 2001
    }, {
      "title" : "Proof nets for the multimodal Lambek calculus",
      "author" : [ "R. Moot", "Q. Puite" ],
      "venue" : "Studia Logica",
      "citeRegEx" : "Moot and Puite,? \\Q2002\\E",
      "shortCiteRegEx" : "Moot and Puite",
      "year" : 2002
    }, {
      "title" : "Second order lambda calculus for meaning assembly: on the logical syntax of plurals, in ‘Computing Natural Reasoning (COCONAT)",
      "author" : [ "R. Moot", "C. Retoré" ],
      "venue" : null,
      "citeRegEx" : "Moot and Retoré,? \\Q2011\\E",
      "shortCiteRegEx" : "Moot and Retoré",
      "year" : 2011
    }, {
      "title" : "The Logic of Categorial Grammars: A Deductive Account of Natural Language Syntax and Semantics, number 6850 in ‘Lecture Notes in Artificial Intelligence",
      "author" : [ "R. Moot", "C. Retoré" ],
      "venue" : null,
      "citeRegEx" : "Moot and Retoré,? \\Q2012\\E",
      "shortCiteRegEx" : "Moot and Retoré",
      "year" : 2012
    }, {
      "title" : "Grail0: A theorem prover for multimodal categorial grammars’, https://github.com/RichardMoot/Grail0",
      "author" : [ "R. Moot", "X. Schrijen", "G.J. Verhoog", "M. Moortgat" ],
      "venue" : null,
      "citeRegEx" : "Moot et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Moot et al\\.",
      "year" : 2015
    }, {
      "title" : "The displacement calculus",
      "author" : [ "G. Morrill", "O. Valent́ın", "M. Fadda" ],
      "venue" : "Journal of Logic, Language and Information",
      "citeRegEx" : "Morrill et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Morrill et al\\.",
      "year" : 2011
    }, {
      "title" : "Dominator trees and fast verification of proof nets, in ‘Logic in Computer Science",
      "author" : [ "A.S. Murawski", "Ong", "C.-H. L" ],
      "venue" : null,
      "citeRegEx" : "Murawski et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Murawski et al\\.",
      "year" : 2000
    }, {
      "title" : "Term-labeled categorial type systems",
      "author" : [ "R.T. Oehrle" ],
      "venue" : "Linguistics & Philosophy",
      "citeRegEx" : "Oehrle,? \\Q1994\\E",
      "shortCiteRegEx" : "Oehrle",
      "year" : 1994
    }, {
      "title" : "Product-free Lambek calculus and context-free grammars",
      "author" : [ "M. Pentus" ],
      "venue" : "Journal of Symbolic Logic",
      "citeRegEx" : "Pentus,? \\Q1997\\E",
      "shortCiteRegEx" : "Pentus",
      "year" : 1997
    }, {
      "title" : "Controlling extraction in abstract categorial grammars",
      "author" : [ "S. Pogodalla", "F. Pompigne" ],
      "venue" : "‘Proceedings of Formal Grammar 2010–2011’,",
      "citeRegEx" : "Pogodalla and Pompigne,? \\Q2012\\E",
      "shortCiteRegEx" : "Pogodalla and Pompigne",
      "year" : 2012
    }, {
      "title" : "The generative lexicon",
      "author" : [ "J. Pustejovsky" ],
      "venue" : null,
      "citeRegEx" : "Pustejovsky,? \\Q1995\\E",
      "shortCiteRegEx" : "Pustejovsky",
      "year" : 1995
    }, {
      "title" : "Intuitionistic categorial grammar",
      "author" : [ "A. Ranta" ],
      "venue" : "Linguistics and Philosophy",
      "citeRegEx" : "Ranta,? \\Q1991\\E",
      "shortCiteRegEx" : "Ranta",
      "year" : 1991
    }, {
      "title" : "Evidence against the context-freeness of natural language",
      "author" : [ "S. Shieber" ],
      "venue" : "Linguistics & Philosophy",
      "citeRegEx" : "Shieber,? \\Q1985\\E",
      "shortCiteRegEx" : "Shieber",
      "year" : 1985
    }, {
      "title" : "The hidden structural rules of the discontinuous Lambek calculus",
      "author" : [ "O. Valent́ın" ],
      "venue" : "Artificial Intelligence’,",
      "citeRegEx" : "Valent́ın,? \\Q2014\\E",
      "shortCiteRegEx" : "Valent́ın",
      "year" : 2014
    }, {
      "title" : "Language in Action: Categories, Lambdas and Dynamic Logic",
      "author" : [ "J. van Benthem" ],
      "venue" : null,
      "citeRegEx" : "Benthem,? \\Q1995\\E",
      "shortCiteRegEx" : "Benthem",
      "year" : 1995
    } ],
    "referenceMentions" : [ {
      "referenceID" : 20,
      "context" : "The Grail family of theorem provers have been designed to work with a variety of modern type-logical frameworks, including multimodal type-logical grammars (Moortgat 2011), NLcl (Barker & Shan 2014), the Displacement calculus (Morrill, Valent́ın & Fadda 2011) and hybrid type-logical grammars (Kubota & Levine 2012).",
      "startOffset" : 156,
      "endOffset" : 171
    }, {
      "referenceID" : 12,
      "context" : "Type-logical grammars originated when Lambek (1958) introduced his Syntactic Calculus (called the Lambek calculus, L, by later authors).",
      "startOffset" : 38,
      "endOffset" : 52
    }, {
      "referenceID" : 0,
      "context" : "Though Lambek built on the work of Ajdukiewicz (1935), BarHillel (1953) and others, Lambek’s main innovation was to cast the calculus as a logic, giving a sequent calculus and showing decidability by means of cut elimination.",
      "startOffset" : 35,
      "endOffset" : 54
    }, {
      "referenceID" : 0,
      "context" : "Though Lambek built on the work of Ajdukiewicz (1935), BarHillel (1953) and others, Lambek’s main innovation was to cast the calculus as a logic, giving a sequent calculus and showing decidability by means of cut elimination.",
      "startOffset" : 35,
      "endOffset" : 72
    }, {
      "referenceID" : 13,
      "context" : "To make things more concrete, I will start by presenting the Lambek calculus (Lambek 1958).",
      "startOffset" : 77,
      "endOffset" : 90
    }, {
      "referenceID" : 13,
      "context" : "For the Lambek calculus, specifying the homomorphism to multiplicative intuitionistic linear logic is easy: we replace the two implications ‘\\’ and ‘/’ by the linear implication ‘(’ and the product ‘•’ by the tensor ‘⊗’. In a statement Γ ` C, Γ is now a multiset of formulas instead of a sequence. In other words, the sequent comma ‘,’ is now associative, commutative instead of associative noncommutative. For the proof of Figure 2 of the previous section, this mapping gives the proof shown in Figure 3. We have kept the order of the premisses of the rules as they were in Figure 2 to allow for an easier comparison. This deep structure still uses the same atomic formulas as the Lambek calculus, it just forgets about the order of the formulas and therefore can no longer distinguish between the leftward looking implication ‘\\’ and the rightward looking implication ‘/’. To obtain a semantics in the tradition of Montague (1974), we use the following mapping from syntactic types to semantic types, using Montague’s atomic types e (for entity) and t (for truth value).",
      "startOffset" : 8,
      "endOffset" : 933
    }, {
      "referenceID" : 13,
      "context" : "Proofs in the Lambek calculus, and in type-logical grammars are subsets of the proofs in intuitionistic (linear) logic and these proofs are compatible with formal semantics in the tradition initiated by Montague (1974). The example in this section presented the calculation of the semantics of a simple example in “slow motion”: many authors assign a lambda term directly to a proof in their type-logical grammar, leaving the translation to intuitionistic linear logic implicit.",
      "startOffset" : 14,
      "endOffset" : 219
    }, {
      "referenceID" : 36,
      "context" : "Formal language theory The Lambek calculus generates only context-free languages (Pentus 1997).",
      "startOffset" : 81,
      "endOffset" : 94
    }, {
      "referenceID" : 20,
      "context" : "Multimodal type-logical grammars (Moortgat 2011) take the non-associative Lambek calculus as its base, but allow multiple families of connectives.",
      "startOffset" : 33,
      "endOffset" : 48
    }, {
      "referenceID" : 21,
      "context" : "We can systematize this translation, following Moot & Piazza (2001), and obtain the following translation from Lambek calculus formulas to first-order linear logic formulas.",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 35,
      "context" : "Examples include lambda grammars (Oehrle 1994), the Displacement calculus (Morrill et al.",
      "startOffset" : 33,
      "endOffset" : 46
    }, {
      "referenceID" : 33,
      "context" : "Examples include lambda grammars (Oehrle 1994), the Displacement calculus (Morrill et al. 2011) and hybrid typelogical grammars (Kubota & Levine 2012).",
      "startOffset" : 74,
      "endOffset" : 95
    }, {
      "referenceID" : 13,
      "context" : "In the previous sections, we have discussed two general solutions to the problems of the syntax-semantics interface of the Lambek calculus. Both solutions proposed a more flexible syntactic logic. In this section, we will discuss a different type of added flexibility, namely in the syntax-semantics interface itself. The basic motivating examples for a more flexible composition have been amply debated in the literature (Pustejovsky 1995, Asher 2011). Our solution is essentially the one proposed by Bassac, Mery & Retoré (2010), called the Montagovian Generative Lexicon.",
      "startOffset" : 123,
      "endOffset" : 531
    }, {
      "referenceID" : 22,
      "context" : "As an example from Moot & Retoré (2011) and Mery, Moot & Retoré (2013), one of the classic puzzles in semantics are plurals and collective and distributive readings.",
      "startOffset" : 19,
      "endOffset" : 40
    }, {
      "referenceID" : 22,
      "context" : "As an example from Moot & Retoré (2011) and Mery, Moot & Retoré (2013), one of the classic puzzles in semantics are plurals and collective and distributive readings.",
      "startOffset" : 19,
      "endOffset" : 71
    }, {
      "referenceID" : 22,
      "context" : "Moot & Puite (2002) present the full details.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 21,
      "context" : "A proof structure is a proof net iff it contracts to a tree containing only tensor links using the contractions of Table 6 and any structural rewrites, discussed below — Moot & Puite (2002) present full proofs.",
      "startOffset" : 170,
      "endOffset" : 190
    }, {
      "referenceID" : 23,
      "context" : "7Lexical ambiguity is a major problem for automatically extracted wide-coverage grammars as well, though standard statistical methods can help alleviate this problem (Moot 2010).",
      "startOffset" : 166,
      "endOffset" : 177
    }, {
      "referenceID" : 11,
      "context" : "As to the proof search strategy used by Grail, it is an instance of the “dancing links” algortihm (Knuth 2000): when connecting atomic formulas, we always link a formula which has the least possibilities and we rewrite the abstract proof structures only once a fully linked proof structure has been produced.",
      "startOffset" : 98,
      "endOffset" : 110
    }, {
      "referenceID" : 22,
      "context" : "Though the parser is not extremely fast, evaluation both on randomly generated statements and on multimodal statements extracted from corpora show that the resulting algorithm performs more than well enough (Moot 2008).",
      "startOffset" : 207,
      "endOffset" : 218
    }, {
      "referenceID" : 9,
      "context" : "Though there are efficient ways of testing acyclicity and connectedness for multiplicative proof nets (Guerrini 1999, Murawski & Ong 2000) and it seems these can be adapted to the first-order case (though some care needs to be taken when we allow complex terms), the theorem prover for first-order linear logic uses a extension of the contraction criterion of Danos (1990). Given a proof structure we erase all formulas from the vertices and keep only a set of the free variables at this vertex.",
      "startOffset" : 103,
      "endOffset" : 373
    } ],
    "year" : 2017,
    "abstractText" : "This chapter will describe a series of tools for developing and testing typelogical grammars. The Grail family of theorem provers have been designed to work with a variety of modern type-logical frameworks, including multimodal type-logical grammars (Moortgat 2011), NLcl (Barker & Shan 2014), the Displacement calculus (Morrill, Valent́ın & Fadda 2011) and hybrid type-logical grammars (Kubota & Levine 2012). The tools give a transparent way of implementing grammars and testing their consequences, providing a natural deduction proof in the specific type-logical grammar for each of the readings of a sentence. None of this replaces careful reflection by the grammar writer, of course, but in many cases, computational testing of hand-written grammars will reveal surprises, showing unintended consequences of our grammar and such unintended proofs (or unintended absences of proofs) help us improve the grammar. Computational tools also help us speed up grammar development, for example by allowing us to compare several alternative solutions to a problem and investigate their differences. This paper will describe the underlying formalism of the theorem provers, as it is visible during an interactive proof trace, and present the general strategy followed by the theorem provers. The presentation in this paper will be somewhat informal, referring the reader elsewhere for full proofs. The rest of this paper is structured as follows. Section 2 will present a general introduction to type-logical grammars and illustrate its basic concepts using the Lambek calculus, ending the section with some problems at the syntaxsemantics interface for the Lambek calculus. Section 3 will look at recent developments in type-logical grammars and how they solve some of the problems at the syntax-semantics interface. Section 4 will look at two general frameworks for automated theorem proving for type-logical grammars, describing the internal representation of partial proofs and giving a high-level overview of the proof search mechanism.",
    "creator" : "LaTeX with hyperref package"
  }
}