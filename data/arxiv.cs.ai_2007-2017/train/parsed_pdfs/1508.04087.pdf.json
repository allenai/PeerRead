{
  "name" : "1508.04087.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "The SP theory of intelligence: its distinctive features and advantages",
    "authors" : [ "J Gerard Wolff" ],
    "emails" : [ "jgw@cognitionresearch.org;" ],
    "sections" : [ {
      "heading" : null,
      "text" : "∗Dr Gerry Wolff, BA (Cantab), PhD (Wales), CEng, MBCS (CITP); CognitionResearch.org, Menai Bridge, UK; jgw@cognitionresearch.org; +44 (0) 1248 712962; +44 (0) 7746 290775; Skype: gerry.wolff; Web: www.cognitionresearch.org.\nar X\niv :1\n50 8.\n04 08\n7v 1\n[ cs\n.A I]\n1 7\nA ug\ndeliver attractive short-term benefits, a major strength of SP system is that it can provide a firm foundation for the long-term development of many aspects of AI and, at the same time, it may deliver some benefits and applications on relatively short timescales. It is envisaged that a high-parallel, open-source version of the SP machine will be created, hosted on an existing high-performance computer and derived from the existing SP computer model. This will be a means for researchers everywhere to explore what can be done with the system, and to create new versions of it.\nKeywords: intelligence, information compression, multiple alignment, perception, cognition, neural networks, deep learning, unsupervised learning, reasoning, mathematics."
    }, {
      "heading" : "1 Introduction",
      "text" : "The SP theory of intelligence, described in outline in Appendix A, is a unique attempt to simplify and integrate observations and concepts across artificial intelligence, mainstream computing, mathematics, and human perception and cognition.\nThe main aim of this paper is to highlight distinctive features of the SP theory and its apparent advantages compared with some AI-related alternatives.\nThe next section provides a broad-brush view of distinctive features of the SP theory, and its strengths.\nThe main aim in sections that follow is to highlight apparent advantages of the SP concepts compared with some alternatives, acknowledging intellectual debts of the SP system, and its weaknesses. Given the vast literature in AI and related fields, and the wide scope of the SP theory, the treatment is necessarily selective."
    }, {
      "heading" : "2 Overview of distinctive features and strengths",
      "text" : "of the SP theory\nThis section is an expanded and revised version of [59, Section II-G], summarising the main distinctive features and strengths of the SP system."
    }, {
      "heading" : "2.1 Simplification and integration of observations and",
      "text" : "concepts\nAs noted above, the SP theory aims to simplify and integrate observations and concepts across a broad canvass. Although the theory is not complete (Section A.9), there is now much evidence that the attempt is proving successful—that the SP theory, in accordance with Occam’s razor (Appendix B), combines relative simplicity with descriptive and explanatory power across a wide range of observations and concepts (Appendix A.7) and a wide range of potential benefits and applications (Appendix A.8).\nCombining relative simplicity with descriptive and explanatory power is perhaps the most distinctive feature of the SP theory, and a major strength of the theory."
    }, {
      "heading" : "2.2 Simplification and integration in computing sys-",
      "text" : "tems\nThe provision of one simple format for knowledge (Appendix A.3), and one framework for the processing of knowledge (Appendix A.4), promotes an overall simplification of computing systems, including both hardware and software [62, Section 5].\nThose two things also promote seamless integration of diverse kinds of knowledge and diverse aspects of intelligence [62, Section 7], an integration that appears to be necessary if we are to achieve human-like versatility and adaptability in AI [59, Section IV-A]."
    }, {
      "heading" : "2.3 The SP theory is a theory of computing",
      "text" : "Most other AI-related research is founded on the idea that computing may be understood in terms of the Universal Turing Machine [48] or equivalent models such as Lamda Calculus [3] or Post’s Canonical System [38].1 By contrast, the SP theory is itself a theory of computing [55, Chapter 4]. What is distinctive about the SP theory as a theory of computing is that it provides much of the human-like intelligence that is missing from earlier models.\n1An apparent exception is the concept of a “neural Turing machine” [10]."
    }, {
      "heading" : "2.4 Information compression via the matching and uni-",
      "text" : "fication of patterns\nIn trying to cut through complexities, the SP research programme focuses on a simple, ‘primitive’ idea: that information compression may be understood as a search for patterns that match each other, with the merging or ‘unification’ of patterns that are the same (‘ICMUP’ meaning “information compression via the matching and unifying patterns”. See Appendix A.4.1). The potential advantage of this approach is that it can help us avoid old tramlines, and open doors to new ways of thinking."
    }, {
      "heading" : "2.5 Multiple alignment",
      "text" : "More specifically, ICMUP provides the basis for a concept of multiple alignment, borrowed and adapted from that concept in bioinformatics (Appendix A.4.2). Developing this idea as a framework for the simplification and integration of concepts across a broad canvass has been a major undertaking. Multiple alignment is a distinctive and powerful idea in the SP research programme."
    }, {
      "heading" : "2.6 Transparency in the representation and processing",
      "text" : "of knowledge\nBy contrast with sub-symbolic approaches to artificial intelligence (artificial neural networks, deep learning, and related approaches), and notwithstanding objections to symbolic AI,2 knowledge in the SP system is transparent and open to inspection, and likewise for the processing of knowledge."
    }, {
      "heading" : "2.7 The DONSVIC principle",
      "text" : "A related point is the expectation, confirmed by evidence to date, that unsupervised learning in the SP system will conform to the “DONSVIC” principle—The Discovery of Natural Structures Via Information Compression [57, Section 5.2]. By contrast with sub-symbolic approaches to artificial intelligence, it is anticipated that, normally, structures created via unsupervised learning in the SP system will be comprehensible by people.\n2See, for example, “Hubert Dreyfus’s views on artificial intelligence”, Wikipedia, bit.ly/1hGHVm8, retrieved 2014-08-19."
    }, {
      "heading" : "2.8 Mathematics",
      "text" : "By contrast with other approaches to artificial intelligence, mainstream computing, or human perception and cognition, the SP theory has quite a lot to say about the nature of mathematics. In brief, it appears that several aspects of mathematics may be understood in terms of ICMUP and, potentially, in terms of multiple alignment [55, Chapter 10], [61]. Although logic has received less attention in the SP programme of research, it seems likely that similar principles will apply there [55, Chapter 10]."
    }, {
      "heading" : "2.9 Perception and cognition",
      "text" : "The SP theory draws extensively on research on human and animal perception and cognition, and neuroscience. In particular, an important part of its inspiration is research developing computer models of the learning of natural language (summarised in [53]).3"
    }, {
      "heading" : "2.10 SP-neural",
      "text" : "The SP theory includes proposals—SP-neural—for how abstract concepts in the theory may be realised in terms of neurons and neural processes. The SP-neural proposals (Appendix A.6) are significantly different from artificial neural networks as commonly conceived in computer science, and arguably more plausible in terms of neuroscience."
    }, {
      "heading" : "3 Minimum length encoding, algorithmic in-",
      "text" : "formation theory, and Kolmogorov complex-\nity\nThis section and the ones that follow consider how the SP theory relates to a selection of AI-related topics, emphasising distinctive features of the theory and its apparent advantages compared with alternatives, but also acknowledging weaknesses in the SP system as it is now, and where it has drawn inspiration from earlier work.\nAs mentioned in Appendix A.4, information compression in the SP theory may be seen as an example of the principle of minimum length encoding (MLE) [44, 51, 39]. Also, information compression and MLE are closely\n3See also “Language learning” in www.cognitionresearch.org.\nrelated to algorithmic information theory (AIT), and Kolmogorov complexity (KC) [28].\nAmongst these inter-related areas of study, distinctive features of the SP theory are:\n• Most research on information compression, MLE, AIT, and KC, assumes that ‘computing’ is defined by the Universal Turing Machine. By contrast, the SP theory is itself a theory of computing (Section 2.3).\n• By contrast with most research in these areas, there is a central role in the SP theory for ICMUP (Section 2.4) and, more specifically, the concept of multiple alignment (Section 2.5)."
    }, {
      "heading" : "4 Deep learning in neural networks",
      "text" : "This section, about deep learning (DL) in artificial neural networks (ANNs), draws mainly on a review by Schmidhuber [42], who has achievements and long experience in the field.\nWithout in any way wishing to diminish the undoubted successes of DL in ANNs (both of which, together, will be referred to as NNs for short), the aim here is to highlight potential advantages of the SP system. This may seem unduly presumptious since the SP system, unlike some NNs, has not won any competitions and has not been adopted or promoted by any company or incorporated in any products. But for reasons given in the subsections that follow, it appears that the SP system is built on firmer foundations than the current generation of NNs, and its long-term prospects are better. As noted in Appendix A.9, there is also potential for applications on relatively short timescales.\nThe great variety of NNs makes it difficult to say things that are true of all of them. The subsections that follow attempt to say things that are at least true of the majority."
    }, {
      "heading" : "4.1 Adaptability of structures",
      "text" : "There is a superficial resemblance between NNs and multiple alignments (especially if the latter are realised as SP-neural, as outlined in Appendix A.6) because they both have layers or levels and they both have connections between the levels. But NNs are not multiple alignments and provide much less scope for adaptation:\n• Standardly, the number of layers of an NN and the size of each layer are pre-defined, whereas the number of rows in a multiple alignment, and their sizes, depend entirely on the incoming and stored information from which it is built.4\n• Normally, there is just one set of layers in an NN with a structure that is fixed, although its behaviour may be changed via changes in the strengths of links within the structure. By contrast, the SP system works by building what is normally a great diversity multiple alignments, each one created by drawing patterns from a pool of New patterns and what may be a very large pool of Old patterns.\n• Standardly, any given layer in an NN connects only with the layer immediately above (if any) and immediately below (if any). By contrast, any given row in a multiple alignment may have connections with any other row, depending on what the multiple alignment represents.\n• Learning in the SP system is quite different from learning in an NN. Instead of varying the strengths of links between neurons, the SP system learns by creating Old patterns, which may be derived directly from New patterns or, more commonly, from multiple alignments containing New and Old patterns."
    }, {
      "heading" : "4.2 Biological validity",
      "text" : "It is generally recognised that NNs are only loosely related to biological systems. For example: “In modern software implementations of artificial neural networks, the approach inspired by biology has been largely abandoned for a more practical approach based on statistics and signal processing.”5\nAlthough there are still big gaps in our knowledge of neural structures in the brain and their functions, it appears that the organisation and workings of the SP system is better supported by available evidence:\n• Models of language learning. As noted in Section 2.9, the SP programme of research derives largely from earlier research developing computer models of language learning in children. That research is underpinned by much relevant empirical evidence.\n4But in the ‘Group Method of Data Handling’ (GMDH), the number of layers and the number of neurons in each layer depend on the problem being solved [42, Section 5.3]. However, it is evident from a review of research in this area [19] that NNs of this type are quite different from multiple alignments in the SP system, and they appear to have much less versatility and adaptability.\n5“Artificial neural network”, Wikipedia, bit.ly/1I5qib7, retrieved 2015-07-16.\n• Cell assemblies. In SP-neural (Appendix A.6, [55, Chapter 11]), abstract concepts in the SP theory map quite neatly into structures that are quite similar to Donald Hebb’s [13] concept of a “cell assembly”, itself derived from neurophysiological evidence.\n• Grandmother cells. The organisation of the SP system, including SP neural, implies the existence of single neurons or small bunches of neurons, any one of which may represent a specific concept such as one’s grandmother. And, notwithstanding some scepticism, there is evidence for the existence of such neurons or clusters of neurons [12].6 By contrast, the organisation and workings of NNs suggests that the neural representation of a concept such as one’s grandmother would normally be ‘distributed’ across many widely-dispersed neurons and many connections amongst neurons. There is more on this issue in Section 4.10.1.\nAlthough the ‘simple’ and ‘complex’ types of neuron discovered by Hubel and Wiesel [16] appear to have provided some inspiration for DL concepts [42, Section 5.2], they may also be seen to provide empirical support for the SP system, especially SP-neural (Appendix A.6)."
    }, {
      "heading" : "4.3 Learning paradigms",
      "text" : "Supervised learning, unsupervised learning, reinforcement learning—three forms of learning with NNs—are, in that connection, normally treated as alternatives with equal status.7.\nBy contrast, in the SP perspective, unsupervised learning is seen as a foundation for all other forms of learning [59, Sections V-A.1 and V-A.2]. The main reasons are, in brief, that:\n• Much learning occurs without the benefit of labelled examples, help from a teacher, or carrots and sticks.8\n6It is sometimes suggested that the concept of a grandmother cell or cells is implausible because death of the cell or cells would mean that one could no longer recognise one’s grandmother. But that is exactly the kind of thing that can happen with people who have suffered a stroke or are suffering from dementia.\n7Although Schmidhuber acknowledges that unsupervised learning may facilitate supervised learning and reinforcement learning [42, Section 4.2]\n8With regard to the last point, it is clear that motivations have an influence on learning—we tend to learn things best if they interest us and if we give them attention. But, contrary to the central dogma of Skinnerian learning theory, it is unlikely that motivations are fundamental in learning.\n• Extraction of redundancy from data, which is central in the SP theory, may be seen to operate not only in unsupervised learning but also in supervised learning—where there is redundancy in the associations between labels and corresponding examples—and in reinforcement learning—where there is redundancy in the associations between actions and corresponding rewards or punishments.\nOverall, the SP system promises to provide a unifying framework for learning, potentially more satisfactory than when different forms of learning are treated separately."
    }, {
      "heading" : "4.4 Learning from a single occurrence or experience",
      "text" : "Most NNs incorporate some variant of the idea, proposed by Donald Hebb [13] and known as “Hebbian learning”, that neurons that repeatedly fire at about the same time will tend to become connected, or for existing connections between them to be strengthened.\nThis mechanism for learning leads to gradual changes in the behaviour of NNs, in keeping with the observation that it normally takes time to learn things like how to talk, or how to win competitions in golf. That correspondence between the workings of NNs and a familiar feature of how we learn may strengthen the belief that NNs are psychologically valid.\nBut this feature of NNs seems to conflict with the undoubted fact that we can and often do learn things from a single occurrence or experience. Getting burned once will teach us to be careful with fire. We may retain memories for many years of significant events in our lives that occurred only once. And we may recognise a face that we have seen only briefly, we may recognise music that we have heard only once before, and likewise for films. It is true that we may rehearse things mentally but often it seems that, with little or no rehearsal, we remember things that have been seen or heard only once.\nBecause the slow strengthening of links between neurons seems not to account for our ability to remember things after a single exposure, Hebb adopted a “reverberatory” theory for this kind of memory [13, p. 62]. But, as Milner has pointed out [32], it is difficult to understand how this kind of mechanism could explain our ability to assimilate a previously-unseen telephone number: for each digit in the number, its pre-established cell assembly may reverberate; but this does not explain memory for the sequence of digits in the number. We may add that it is unclear how the proposed mechanism would encode a phone number in which one or more of the digits is repeated.\nThe SP theory provides an explanation both for learning from a single experience and for the fact that some kinds of learning are slow:\n• Learning in the SP system starts by assimilating New information directly, followed by a possible encoding of the information in terms of any existing Old patterns, and the creation of newly-minted Old patterns via information compression [55, Sections 3.9.2 and 9.2.2], [57, Section 5.1]. The taking in of New information, with or without its encoding in terms of existing Old patterns, means that the system can learn from a single exposure to a pattern or event. That remains true even if shortcomings in memory cause some New information to be lost.\n• Learning something like a natural language is much more complicated than remembering one’s first day in school or when one had a ride on a camel. With the learning of complex knowledge or skills, the main challenge is heuristic search through the vast abstract space of possible knowledge structures to find one or two that are reasonably good. That process takes time."
    }, {
      "heading" : "4.5 Computational resources, speed of learning, and",
      "text" : "volumes of data\nIn addition to apparent problems with learning from a single experience, there seem to be related issues with NNs concerning the computational resources they require for learning, their speed of learning, and the volumes of data they require. For example:\n• One news report9 describes how an NN with “16,000 computer processors” and “one billion connections” was exposed to “10 million randomly selected YouTube video thumbnails”, “over the course of three days”. Then, “after being presented with a list of 20,000 different items”, it began to recognize pictures of cats.\n• Another news report10 refers to “... billions or even hundreds of billions of connections that have to be processed for every image” and “Training such a large network requires quadrillions of floating point operations ....”.\n• “... the new millennium brought a DL breakthrough in form of cheap, multiprocessor graphics cards or GPUs. ... GPUs excel at the fast ma-\n9“Google’s artificial brain learns to find cat videos”, Wired, 2012-06-26, wrd.cm/18YaV5I.\n10“Growing pains for deep learning”, Communications of the ACM, Vol. 58, No. 7, July 2015, pp. 14–16.\ntrix and vector multiplications required ... for NN training, where they can speed up learning by a factor of 50 and more.” [42, Section 4.5].\nAlthough it is true that it takes time for a person to learn his or her native language or languages (Section 4.4) and the human brain contains billions of neurons,11 the current generation of NNs appears to overlook what can be achieved with ICMUP (Section 2.4 and Appendix A.4.1), with small amounts of data and quite modest computational resources:\n• In accordance with the theory developed by Marr and Poggio [30] in which ICMUP may be seen to play a central role, a computer program developed by Grimson could discover the hidden image in a randomdot stereogram ([20], [58, Section 5.1]) with performance on a late1970s computer that “coincides well with that of human subjects” [11, Section 5]. Although Grimson does not give run times, it looks as if his program finds the hidden image in a random-dot stereogram about as fast as people—normally in less than a minute.\n• With run times of only a few minutes on a PC, the SP computer model, founded on ICMUP, demonstrates unsupervised learning of plausible generative grammars for the syntax of English-like artificial languages (Section 8). Similar results have been obtained with earlier models of language learning [53], also founded on ICMUP.\nIt is true that what is being attempted with many NNs is relatively ambitious, but biological neurons are very much slower than electronic ones. On balance, there appear to be problems with NNs relating to the computational efficiency of their learning and the volumes of data they require to obtain useful results."
    }, {
      "heading" : "4.6 Recognition of images and speech",
      "text" : "With some qualification (Appendices 4.7 and 4.8), NNs do well in tasks such as the recognition of images (eg, [63]) or speech (eg, [5]).\nBut the SP system has strengths in pattern recognition ([55, Chapter 6], [57, Section 9]). These include:\n11According to one estimate [14], the average human brain contains about 86 billion neurons.\n• That, via multiple alignment, it can recognise patterns at multiple levels of abstraction, with the integration of class-inclusion relations and part-whole relations;\n• It can model “family resemblance” or polythetic categories, meaning that recognition does not depend on the presence absence of any particular feature or combination of features;\n• Recognition is robust in the face of errors of omission, commission or substitution;\n• For any given identification, or any related inference, the SP system may calculate associated probabilities;\n• And the system provides a model for the way in which context may influence recognition.\nWith regard to the last point, any kind of knowledge may influence recognition (as with recognition of things by people), something that flows from how the SP system provides for seamless integration of diverse kinds of knowledge and diverse kinds of processing [62, Section 7].\nIn addition to the strengths just mentioned, in computer vision [58] there is potential in the SP system in:\n• The recognition of objects;\n• Scene analysis;\n• The learning of visual entities and classes of entity and the piecing together of coherent concepts from fragments;\n• The creation of 3D models of objects and of their surroundings;\n• Providing an account of how we may see things that are not objectively present in an image;\n• Providing insights into the phenomena of lightness constancy, colour constancy, and ambiguities in visual perception;\n• And the integration of vision with other senses and other aspects of intelligence.\nWith vision and speech, the SP computer model, as it is now, has some shortcomings but it appears that these problems are soluble (Appendix A.9): it is not good at the extraction or recognition of low-level perceptual features, such as—in the case of vision—edges, angles, colours, luminances or textures, or—in the case of speech—phonemes, formant ratios or formant transitions . How these problems may be overcome in the case of vision is discussed in [55, Section 13.2.1] and [58, Section 3].\nIn summary, the SP computer model does not yet rival commercial applications of NNs in the recognition of images or speech, but it has relevant strengths and potential."
    }, {
      "heading" : "4.7 Deep neural networks are easily fooled",
      "text" : "A recent report describes how “We can cause [a deep neural network] to misclassify an image by applying a certain hardly perceptible perturbation.” [47, Abstract]. For example, the NN may correctly recognise a picture of a car but may fail to recognise another slightly different picture of a car which, to a person, looks almost identical (ibid., Figure 6).\nAnother report [37] describes how at least one kind of deep neural network can be fooled quite easily into assigning an image with near certainty to a recognisable class of objects such as ‘guitar’ or ‘penguin’, when people judge the given image to be something like white noise on a TV screen or an abstract pattern containing nothing that resembles a guitar or a penguin or any other object.\nOf course, these kinds of failures are a potential problem in any kind of application where recognition needs to be reliable. And without a good theory for how NNs work (Section 4.11), they may be difficult to weed out.\nWith regard to the errors described in [47], there is already evidence that the SP computer model would not make that kind of mistake. It can recognise words containing errors of omission, commission and substitution [55, Section 6.2.1], and likewise for diseases in medical diagnosis viewed as pattern recognition [54, Section 3.6] and in the parsing of natural language [57, Section 4.2.2].\nNo attempt has been made to test experimentally whether or not the SP computer model is prone to the kinds of errors described in [37], but a knowledge of how it works suggests that it would not be."
    }, {
      "heading" : "4.8 Under-generalisation and over-generalisation",
      "text" : "An issue with any learning system is its ability to generalise from the data (I) that is the basis of its learning without under-generalisation (‘overfitting’) or\nover-generalisation (‘underfitting’). If, for example, the system has learned the concept ‘horse’, it should, in its later recognition of horses, not be too closely constrained to recognise only horses that are identical to or very similar to those in I (under-generalisation) and, at the same time, it should not make such mistakes as assigning cows, sheep or dogs to the category ‘horse’ (over-generalisation)."
    }, {
      "heading" : "4.8.1 Under-generalisation in NNs",
      "text" : "It is widely recognised that NNs may suffer from overfitting, and various solutions have been proposed. For example, Srivastava and colleagues [46] suggest that some neurons in an NN, together with their connections, may be randomly dropped from the NN during training, to prevent them coadapting too much; while Zeng and colleagues [65] suggest that, in a multistage classifier, unsupervised pre-training and specially-designed stage-wise supervised training can help to avoid overfitting; and Wiesler and colleagues [52] say that they have found that a “factorized structure” can be effective against overfitting."
    }, {
      "heading" : "4.8.2 Over-generalisation in NNs",
      "text" : "The problem of underfitting in NNs has also drawn attention. For example, Dauphin and Bengio [6] show how underfitting may arise from the failure of some big neural networks to take full advantage of their computational capacity, and they make suggestions for overcoming the problem; and Ganin and Lempitsky [9] describe how a “two-stage architecture” can help overcome problems of underfitting."
    }, {
      "heading" : "4.8.3 Almost certainly, information compression solves both problems",
      "text" : "Without attempting a detailed comparison with alternative accounts of overfitting and underfitting in NNs, the suggestion here is that, in the SP theory, information compression provides a simple, elegant solution for both problems.\nSection 8.0.4 outlines how, in connection with how children learn their first language or languages, information compression can explain how they learn to generalise correctly beyond the language that they have heard and, at the same time, to eliminate the over-generalisations that are prominent in the early stages.\nIt seems likely that the same principles would apply to the learning of grammars for the recognition of images or speech, thus solving two problems— the problems of overfitting and underfitting—with one over-arching principle.\nThe SP system may also help to solve the problem overfitting in the way that it can recognise patterns via multiple alignment in the face of errors of omission, commission, and substitution (Section 4.6)."
    }, {
      "heading" : "4.9 Information compression",
      "text" : "Schmidhuber’s review [42] contains a short section (4.4) about “Occams razor: compression and minimum description length (MDL)”, and it mentions information compression in some other sections. Although he suggests (in Section 5.10) that “much of machine learning is essentially about compression”, the overall thrust of the review is that information compression is merely one of several “recurring themes” in deep learning, without any great significance.\nBy contrast, information compression is fundamental in the SP theory, running through it like Blackpool in a stick of rock, in its foundations (Appendix A.1), in the matching and unification of patterns (Appendix A.4.1), in the building of multiple alignments (Appendix A.4.2), and in unsupervised learning (Appendix A.4.3).\nIn view of evidence for the importance of information compression in intelligence, computing, and mathematics (Appendix A.1), the peripheral status of information compression in the design and operation of NNs, weakens them in comparison with the SP system."
    }, {
      "heading" : "4.10 Transparency in the representation and process-",
      "text" : "ing of knowledge\nA problem with NNs is that there is considerably uncertainty about how they represent knowledge and how they process it:\n“... we actually understand surprisingly little of why certain models work and others don’t. ... One of the challenges of neural networks is understanding what exactly goes on at each layer.” [33].\n“... no one knows how neural networks come up with their answers. ... A programmer need adjust only the number of nodes and layers to optimise how it captures relevant features in the data. However, since it’s impossible to tell exactly how a neural\nnetwork does what it does, this tweaking is a matter of trial and error.”12\nWith regard to the first quote, it is true that, as described in the blog, NNs can be made to reveal some of their knowledge. But, while many of the resulting images have artistic appeal, they are not transparent representations of knowledge, and it’s not clear how they are learned or how they function in such tasks as recognition.\nBy contrast with these uncertainties:\n• In the SP system, all kinds of knowledge, including those detailed in Section 10, are represent transparently as SP patterns.\n• In the SP computer model, an audit trail can be provided for all processing, including the building of multiple alignments and the creation of grammars.\n• In the SP system, it is anticipated that unsupervised learning will conform to the DONSVIC principle (Section 2.7; [57, Section 5.2]), and this is confirmed by evidence to date. To the extent that this remains true, structures created via unsupervised learning in the SP system will be transparent and comprehensible by people."
    }, {
      "heading" : "4.10.1 Distributed or localist encodings",
      "text" : "There is further uncertainty about whether knowledge in an NN is represented and processed in a ‘distributed’ or ‘localist’ scheme. The dominant view is that, in neural networks, knowledge of a concept such as ‘grandmother’ is encoded in neurons that are widely-distributed, with links between them. In support of that ‘subsymbolic’ view, “there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis”, suggesting that “it is the entire space of activations, rather than the individual units, that contains the bulk of the semantic information” [47, Abstract].\nBut some researchers suggest that “it is possible to train neurons to be selective for high-level concepts ... In our experiments, we obtained neurons that function as detectors for faces, human bodies, and cat faces by training on random frames of YouTube videos.” [23, Conclusion]. This research appears to support the ‘localist’ or ‘symbolic’ view that a concept such as\n12“The rapid rise of neural networks and why they’ll rule our world”, New Scientist, 2015-07-08, bit.ly/1IkbbuC.\n‘grandmother’ may be represented by a single neuron or, perhaps, a small cluster of neurons (cf. Section []).\nAlthough the SP theory may turn out to be wrong empirically, it at least clear on this issue. As noted in Section 4.2, it is very much in the localist camp. In SP-neural, it is envisaged that knowledge in the form of SP patterns is recorded on the cortex very much like writing on a sheet of paper (Appendix A.6)."
    }, {
      "heading" : "4.10.2 Class-inclusion hierarchies and Part-whole hierarchies",
      "text" : "With regard to hierarchical structures, there seems to be some uncertainty about whether NNs discover:\n• Class-inclusion hierarchies:\n“Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction.” [24, Abstract].\n“Hidden layers: these learn more abstract representations as you head up.”13).\n• Or part-whole hierarchies:\n“... the first layer maybe looks for edges or corners. Intermediate layers interpret the basic features to look for overall shapes or components, like a door or a leaf. The final few layers assemble those into complete interpretations ... such as entire buildings or trees.” [33].\nand, either way, the representation is obscure. While it is clear that a human face is part of a human body (Section 4.10.1), it seems reasonable to assume that concepts such as ‘horse’, ‘cow’, and ‘sheep’, would be most fully encoded in the highest layer of an NN, in either a distributed or localist scheme (Section 4.10.1). If that is accepted, the question arises how the NN would encode knowledge of something like an agricultural exhibition which is likely to contain representations of all three of the concepts mentioned, and with multiple instances for each one of them. Likewise, we may ask how such concepts may be encoded as examples of abstractions such as ‘mammal’ or ‘vertebrate’. It is not clear how concepts\n13Slide 3 in “Deep learning for NLP (without magic)”, slide show by R. Socher and C. Manning, dated 2013, retrieved 2015-07-25, stanford.io/1bmBsKK.\nthat are most fully encoded in the top layer of an NN could ever be part of something that is larger or more abstract.\nBy contrast with these uncertainties with NNs, the SP system can represent class-inclusion hierarchies and part-whole hierarchies, with a clear distinction between the two (eg, the class-inclusion hierarchy in [55, Figure 6.7] and the part-whole hierarchy in [55, Figure 6.8]). At the same time it provides for seamless integration of both kinds of hierarchy, as illustrated in [57,\nFigure 16]."
    }, {
      "heading" : "4.10.3 Iteration and recursion",
      "text" : "Related to issues just discussed are questions about how NNs may encode repeated instances of this or that category (such as the many instances of horses, cows, and sheep that one would expect to see at an agricultural exhibition—Section 4.10.2), and how NNs may encode the kinds of recursive structures that are prominent in natural language, such as This is the horse and the hound and the horn, That belonged to the farmer sowing his corn, That kept the rooster that crowed in the morn, ....\nBy contrast with uncertainties about how NNs may handle such structures, the SP theory provides crisp answers:\n• Any given SP pattern may appear one, two, or more times within any one multiple alignment. An example is how the pattern representing a noun phrase (‘NP’) appears twice in the multiple alignment shown in [57, Figure 4], once in row 7 and once in row 2. In the interpretation of any given body of data, there should be no difficulty arising from the occurrence of multiple instances of any given category.\n• Multiple alignments can accommodate recursive structures, as illustrated in [55, Figure 4.2] and [58, Figure 4].\n• Recursion may also be accommodated in SP-neural, as illustrated in [55, Figure 11.10]."
    }, {
      "heading" : "4.11 Theoretical foundations",
      "text" : "Problems of transparency in the representation and processing of knowledge (Section 4.10) and failures in recognition (Section 4.7) appear to be symptoms of deeper problems with NNs: theoretical foundations that are weak or absent.\nThat there are problems in that area is also suggested by the many different versions of NNs: ‘time-delay’ neural networks, ‘gradient-based deep\nlearners with alternating convolutional and down-sampling layers’, ‘weightsharing feed-forward’,‘nonlinear auto-regressive with exogenous inputs recurrent’, ‘max-pooling convolutional’, ‘multi-column GPU max-pooling convolutional’, ‘bi-directional long short-term memory recurrent’, and more [42].\nIt is true that the creation and testing of many versions of the SP computer model have been important in the development of the SP theory [57, Section 2.5]. But now the model—the first version of the SP machine—is relatively stable, with ICMUP and multiple alignment centre stage. Compared with NNs as they have been developed so far, the SP system is underpinned by coherent and powerful theoretical foundations."
    }, {
      "heading" : "4.12 Symbolic AI",
      "text" : "“As regards knowledge representation in the brain, one of the key challenges is to understand how neural activations, which are widely distributed and sub-symbolic, give rise to behavior that is symbolic, such as language and logical reasoning.” [7, Section 2].\n“[Deep learning techniques] have no obvious ways of performing logical inferences ...”14\nThis section briefly reviews aspects of AI which seem to pose problems for NNs, and where the SP system is relatively strong. In broad terms, these seem to be areas where the ‘symbolic’ tradition in AI has proved to be relatively successful."
    }, {
      "heading" : "4.12.1 The processing of natural language",
      "text" : "An assumption in the SP programme of research is that, in the quest for human-like capabilities with natural language, we should aim to create a system with human-like knowledge of the syntactic and semantic structures of language. This contrasts with the suggestion by Zhang and LeCun [66, Section 1] that “... text understanding can be handled by a deep learning system without artificially embedding knowledge about words, phrases, sentences or any other syntactic or semantic structures associated with a language.”\nOf course, people may gain a superficial understanding of natural language from a few scattered clues and, in some situations, that may be all that is required. But for a thorough understanding of, for example, a legal, philosophical or scientific argument, it seems unlikely that we can by-pass the need for a good knowledge of syntactic and semantic structures.\n14“Is ‘deep learning’ a revolution in artificial intelligence?”, The New Yorker, Gary Marcus, 2012-11-25, nyr.kr/1Be7S22.\nIt seems that NNs are not well suited to the processing of natural language:\n• Zhang and LeCun’s claim (ibid.) that their ‘ConvNet’ NN may be “... able to learn the hierarchical representations of words, phrases and sentences in order to understand text.” has not yet been demonstrated.\n• In addressing issues in this area, Socher and colleagues [43] introduced the concept of ‘Compositional Vector Grammar’, but their results are not as clear or straightforward as one might wish, something which two of them (Socher and Manning) acknowledge in a slide show:\n“Concern: problem with model interpretability. No discrete categories or words, everything is a continuous vector. We’d like have symbolic features like NP, VP, etc. and see why their combination makes sense.”15\nNotwithstanding some qualifying remarks, their concern remains valid.\nBy contrast, the SP system has several strengths in the analysis and production of natural language (Section 7), and it has clear potential for the learning of natural language, including segmental structures, classes of structure, and abstract patterns (Section 8)."
    }, {
      "heading" : "4.12.2 Reasoning and other ‘symbolic’ aspects of AI",
      "text" : "Some years ago, there was quite a lively interest in how NNs might do reasoning (eg, [27]) but that interest seems to have subsided, probably because NNs really are not well suited to much more than the relatively simple kinds of inference that, in pattern recognition, correct errors of omission, commission, or substitution.\nBy contrast, the SP system demonstrates several kinds of reasoning, with clear potential for further development (Section 9).\nSimilar things may be said about areas which appear to be problematic for NNs and where symbolic AI and the SP system are relatively strong, such as planning [55, Chapter 8], problem solving (ibid.), and grammatical inference ([55, Chapter 9], [57, Section 5]).\n15Slide 201 in “Deep learning for NLP (without magic)”, slide show by R. Socher and C. Manning, dated 2013, retrieved 2015-07-27, stanford.io/1bmBsKK."
    }, {
      "heading" : "4.12.3 Discussion",
      "text" : "As a general rule, it seems that the SP system, compared with NNs, is relatively strong in areas of AI where the ‘symbolic’ approach has been successful. There seem to be three main reasons for this:\n• Unlike NNs, there is transparency in both the representation and processing of knowledge (Section 2.6).\n• Within the multiple alignment framework, it is possible to model concepts from mainstream computing and the symbolic tradition of AI such as ‘variable’, ‘value’, and ‘type’ (Section 9).\n• The multiple alignment framework is much more adaptable than the deep learning framework (Section 4.1)."
    }, {
      "heading" : "5 Universal search",
      "text" : "Some ideas, that may be grouped together loosely under the heading ‘universal search’, seem, at first sight, to offer comprehensive solutions to problems in AI and beyond.\nSolomonoff [45] has argued that the great majority of problems in science and mathematics may be seen as either ‘machine inversion’ problems or ‘time limited optimization’ problems, and that both kinds of problem can be solved by inductive inference using the principle of minimum length encoding. In ‘Levin search’ [25, 26], which aims to solve inversion problems, all possible programs on a universal Turing machine are interleaved, sharing computation time equally among them, until one of the executed programs manages to solve the given problem. Ideas of this kind have been developed by Hutter (eg, [17]), Schmidhuber (eg, [41]), and others.16\nIt would be premature to say that this kind of approach to AI is better than the SP approach, or vice versa. From the perspective of the SP research programme, the main sources of concern are:\n• The apparent difficulty of translating the abstract concepts of universal search into working models that exhibit aspects of intelligence or are potentially useful.\n• The apparent difficulty of squeezing the subtlety and complexity of human intelligence into the procrustean bed of ‘well defined problems’ [17]—something that appears to be a prerequisite for universal search.\n16For a useful overview, see “Universal search” by Matteo Gagliolo [8].\n• With problems in AI, it is rarely possible to guarantee solutions that are theoretically ideal—a focus of interest in research on universal search. Normally, via heuristics search, we should aim for solutions that are “reasonably good” and not necessarily perfect (Appendix A.4.4).\n• In terms of the trade-off between simplicity and power (Section A.4.5), it appears that theories in the area of universal search are running the risk of being too simple and over-general, and correspondingly weak in terms of descriptive or explanatory power. By contrast, the SP system provides mechanisms for finding good full and partial matches between patterns (Appendix A.4.1), for building multiple alignments (Appendix A.4.2), and for creating grammars (Appendix A.4.3), and it has plenty to say about a range of observations and concepts in AI and beyond."
    }, {
      "heading" : "6 Some models for AI",
      "text" : "This section considers briefly some of the systems that have been proposed as models for AI, or aspects of AI: Bayesian networks, support vector machines, hidden Markov models, Kalman filters, self-organising maps, Petri nets, and cellular automata.\nThe suggestion here, which is admittedly a rather sweeping generalisation, is that, while these models are admirably simple, and while they may have applications in particular areas, they lack the descriptive and explanatory range of the SP system. In terms of the quest for a wide-ranging theory (Sections B and 2.1), their scope is too narrow.\nIn addition to its relatively wide scope, the SP system, compared with a Bayesian network, has two main advantages:\n• Simplicity in the representation of statistical knowledge. Each node in a Bayesian network contains a table of conditional probabilities for all possible combinations of inputs and these tables can be quite large. By contrast, the SP framework only requires a single measure of frequency for each pattern. Absolute or conditional probabilities can be derived from that frequency measure, as required.\n• Creation of ontologies. Bayes’ theorem assumes that the categories that are to be related to each other via conditional probabilities are already ‘given’. By contrast, the SP system provides an account of how a knowledge of categories and entities may be developed via unsupervised learning (Appendix A.4.3).\nSome of the power of the SP system can be seen in how, with appropriate data, it provides an alternative to Bayesian networks in modelling such phenomena as ‘explaining away’ ([57, Section 10.2], [55, Section 7.8])."
    }, {
      "heading" : "7 Analysis and production of natural language",
      "text" : "This section considers, with varying emphasis, the strengths and potential of the SP system in the parsing, understanding, and production of natural language, in comparison with symbolic approaches to these topics. Comparisons with neural network approaches are considered in Section 4.\nThe analysis and production of natural language is a relatively mature and successful area of symbolic AI. The SP computer model is not an immediate rival to existing systems but its long-term potential is considerable:\n• It can model the parsing of natural language directly and transparently, as illustrated in Figure 2;\n• In parsing, it can accommodate syntactic ambiguities, the resolution of ambiguities via the provision of appropriate context, and recursive structures in syntax;\n• Parsing is robust against errors of omission, commission, or substitution;\n• One mechanism may achieve both the parsing and production of natural language, without any modification;\n• The system provides for the representation and processing of several different non-syntactic or ‘semantic’ forms of knowledge;\n• Preliminary tests show how the system may integrate syntax with semantics, both in the interpretation and the production of language.\nThese things are described most fully in [55, Chapter 5] and more briefly and selectively in [57, Section 8]. Perhaps the main strengths of the SP system in this area are:\n• It provides a simple, direct means of encoding discontinuous dependencies in syntax [55, Section 5.4].\n• The use of one simple format for all kinds of knowledge and one framework for all kinds of processing (Section 2.2) is likely to facilitate:\n– The seamless integration of syntax with semantics. Although syntax and semantics-like structures may be integrated in systems like Prolog, such systems have other shortcomings compared with the SP system (Section 9).\n– The seamless integration of natural language processing with other aspects of intelligence.\n– Modelling the way in which non-syntactic (semantic) context may influence the interpretation of language, as is the case with people.\n• The system’s potential for unsupervised learning of syntactic-semantic knowledge (Section 8) promises automatic learning of language, reducing the work required to create hand-crafted grammars and, potentially, achieving greater accuracy in the modelling of natural language."
    }, {
      "heading" : "8 Learning natural language",
      "text" : "Although grammatical inference has been the subject of research for many years, automatic learning of the syntax of a natural language remains a major challenge. Even more difficult is automatic learning of the kinds of syntactic-semantic structures that are needed for such things as interpreting the meaning of natural language, the production of speech or writing from meanings, and, when it is done at a high standard, translation from one language to another.\nAs mentioned in Appendix A, the SP programme of research grew out earlier research developing computer models of language learning, but has required a radical reorganisation of earlier models to meet the goals of the SP programme. Now, the SP computer model demonstrates unsupervised learning of plausible generative grammars for the syntax of English-like artificial languages, including the learning of segmental structures, classes of structure, and abstract patterns [55, Chapter 9], in accordance with the DONSVIC principle (Section 2.7). It appears that, with some further work outlined in Sections 8.1 and 8.2, the potential is considerable.\n8.0.4 Learning without over-generalisation or under-generalisation\nAs noted in Appendix A.4.5, unsupervised learning via compression of a body of information, I, produces a ‘grammar’ and an ‘encoding’ of I in terms of the grammar, and there is a trade-off in size between the two. And, normally, the greatest overall compression of I is achieved with a grammar that is neither too big nor too small.\nIt appears that this solves a long-standing problem in understanding how children learn their first language or languages: to explain how children learn to generalise correctly beyond the language they have heard and to weed out over-generalisations (such as “hitted” or “gooses”)—and this in the face of evidence that they can do these things without correction by a ‘teacher’, or anything equivalent. It appears that, for a given I, a learning process that aims to minimise the overall size of the grammar and the encoding is likely to yield a grammar that generalises correctly beyond I but does not overgeneralise. These things are discussed briefly in [57, Section 5.3] and more fully in [53]."
    }, {
      "heading" : "8.1 Learning the syntax of natural language",
      "text" : "As it stands, the SP computer model cannot learn a plausible grammar for any natural language, probably because of two weaknesses mentioned in Appendix A.9: its inability to learn intermediate levels of abstraction or discontinuous dependencies in data. But it appears that those two problems are soluble, and it seems likely that, with their solution, the SP model would become a powerful tool for the unsupervised learning of realistic grammars for the syntax of natural language, at least for language in textual form."
    }, {
      "heading" : "8.2 Learning semantic structures and the integration",
      "text" : "of syntax with semantics\nThe use of one simple format for the representation of knowledge (Appendix A.3) and one versatile framework for processing knowledge (Appendix A.4) means versatility in the representation and processing of diverse kinds of knowledge (Appendix A.7). More specifically, it is likely to mean:\n• Versatility in the learning of diverse kinds of knowledge, including the meanings or ‘semantics’ of natural language.\n• Potential for the learning of integrated syntactic-semantic structures— the kinds of structures that may serve in the interpretation of language [55, Figure 5.18], the production of language from meanings [55, Figure 5.18], and translation from one language to another [60, Section IIIA.4]."
    }, {
      "heading" : "9 Exact and inexact forms of reasoning",
      "text" : "From the General Problem Solver [36], through Prolog [4], to such systems as Description Logics [15], concepts derived from classical logic have been prominent in AI and related fields such as the semantic web.\nAlthough the all-or-nothing certainties of classical logic can be useful, it has been recognised for some time that much of human thinking and reasoning revolves around judgements that may have varying levels of certainty. This has led to several proposals for systems that, in one way or another, combine exact or ‘logical’ reasoning with inexact, ‘fuzzy’ or ‘probabilistic’ kinds of reasoning (eg, [64, 22]). Some recent developments are described in [40].\nCompared with these strands of research, distinctive features and apparent advantages of the SP system include:\n• The SP system is fundamentally probabilistic but the exactness of reasoning may be varied. Because information compression is central in the workings of the SP system (Appendix A.4) and because of the intimate relation between information compression and concepts of prediction and probability [28], the SP system is fundamentally probabilistic. This chimes with research into the fundamentals of mathematics:\n“I have recently been able to take a further step along the path laid out by Gödel and Turing. By translating a particular computer program into an algebraic equation of a type that was familiar even to the ancient Greeks, I have shown that there is randomness in the branch of pure mathematics known as number theory. My work indicates that—to borrow Einstein’s metaphor—God sometimes plays dice with whole numbers.” [2, p. 80].\nFor any given inference reached via any of the kinds of reasoning described below, the SP system may calculate associated probabilities [55, Section 3.7], [57, Section 4.4].\nAlthough the SP system is fundamentally probabilistic, users may control the exactness of the reasoning that it delivers. As in everyday life, we may gain confidence in an uncertain inference or decision by gathering more evidence. And, as in everyday life, we may choose to concentrate on probabilities that are close to 0 or 1, and to ignore the rest.\n• Modelling concepts from logic and mathematics. Although it may not be obvious at first sight, the multiple alignment framework can model several of the concepts that are familiar in logic and mathematics, including: ‘variable’, ‘value’ and ‘type’ [55, Section 10.3.5.1]; ‘function’ with ‘arguments’ or ‘parameters’ [55, Section 10.3.3.2]; and ‘sets’ and operations on sets [55, Section 10.4.1]. There is more about these kinds of things in [62, Section 6.6].\n• Versatility in reasoning. Despite the essential simplicity of the multiple alignment framework, the SP computer model demonstrates several kinds of reasoning, including both exact and probabilistic kinds of reasoning, with clear potential for further development:\n– One-step ‘deductive’ reasoning;\n– Abductive reasoning;\n– Reasoning with probabilistic decision networks and decision trees;\n– Reasoning with ‘rules’;\n– Nonmonotonic reasoning and reasoning with default values;\n– Reasoning in Bayesian networks, including “explaining away”;\n– Causal diagnosis;\n– Reasoning which is not supported by evidence;\n– And inheritance of attributes in an object-oriented class hierarchy or heterarchy.\nThe foregoing are described quite fully in [55, Section 6.4, Chapters 7 and 10] and more selectively in [57, Section 10]. There is also potential for spatial reasoning [59, Section IV-F.1] and what-if reasoning [59, Section IV-F.2].\n• Integration of diverse forms of reasoning. An attraction of the SP system is that there can be seamless integration of two or more kinds of reasoning, in any combination (Section 2.2).\n• Integration with other aspects of intelligence. Another attraction of the SP system is that there can be seamless integration of reasoning with other aspects of intelligence (ibid.)."
    }, {
      "heading" : "10 Representation and processing of diverse",
      "text" : "forms of knowledge\nA problem with AI and, more generally, computing as they have developed to date is that knowledge may be represented with a large number of different formalisms and often, for each one, there is a large number of different formats. This complexity is compounded by the fact that, normally, each formalism and each format has its own mode of processing. Until recently, this complexity has been easy to ignore. But with the advent of big data, it has become a major problem, a problem that the SP system may help to solve (Section 12.1).\nAs described in outline in [60, Section III-B], and in more detail in [55, 57], the SP system can represent a variety of kinds of knowledge including:\n• The syntax of natural language;\n• Class hierarchies, part-whole hierarchies, and their integration;17\n• Trees and networks, including Bayesian networks;\n• Entity-relationship structures;\n• Relational knowledge (tuples);\n• If-then rules, associations, and other knowledge in support of reasoning;\n• Patterns and images;\n• Structures in three dimensions;\n• And sequential and parallel procedures.\nSince information compression is at the heart of how knowledge is represented in the SP system, and since the multiple alignment framework appears to be a very general means of compressing information, there is reason to believe that any kind of knowledge may be represented in the system. The same arguments apply, mutatis mutandis, to the processing of knowledge in the SP system.\n17The way the SP system represents and processes such structures is arguably simpler and more elegant than in systems like the “Web Ontology Language” (OWL), developed for the semantic web. By contrast with OWL, the SP system has no need for constructs like ‘Class’, ‘subClassOf’, ‘Property’ and ‘subPropertyOf’, since their equivalents are emergent properties of the multiple alignment framework."
    }, {
      "heading" : "11 IBM’s Watson",
      "text" : "As is now well known, a team of researchers at IBM developed a computing system, called Watson, that, in 2011, beat the best human players at the TV quiz game Jeopardy!18\nOf course this is a major achievement, with potential benefits in terms of ideas and, perhaps, applications. But doubt has been expressed about its significance for AI:\n“... systems that seem to have mastered complex language tasks, such as IBM’s Jeopardy! winner Watson, do it by being superspecialized to a particular format. ‘It’s cute as a demonstration, but not work that would really translate to any other situation,’ [says Yann LeCun].”19\nand there are possible concerns about how it was and is being developed:\n• Dave Ferrucci, leader of the team that developed Watson, has been quoted as saying:\n“Did we sit down when we built Watson and try to model human cognition? Absolutely not. We just tried to create a machine that could win at Jeopardy!”20\n• The original Watson was created by combining natural language understanding with statistical analysis of very large amounts of text. IBM has now added capabilities in translation, speech-to-text, and text-tospeech, and they plan to add capabilities in deep learning,21 with large numbers of medical images as data for learning.22\nIn short, Watson was originally developed, and is continuing to be developed, as a kluge: “a clumsy or inelegant—yet surprisingly effective—solution to a problem.” [29, p. 2].\nDoes this matter? There are several possible answers to this question. On the plus side (it’s a good thing to do):\n18See, for example, “Watson (computer)”, Wikipedia, retrieved 2015-08-12, bit.ly/1DwVKiC.\n19“Teaching machines to understand us”, MIT Technology Review,, 2015-08-06, bit.ly/1KT1820.\n20“The man who would teach machines to think”, The Atlantic, November 2013, theatln.tc/1fi9AFv.\n21“IBM pushes deep learning with a Watson upgrade”, MIT Technology Review, 2015-07-09, bit.ly/1Nq0bMg.\n22“Why IBM just bought billions of medical images for Watson to look at”, MIT Technology Review, 2015-08-11, bit.ly/1P6lcvQ.\n• It is probably possible to create a system that does useful things and earns money.\n• Combining technologies may help to overcome weaknesses in individual technologies and it may help to overcome fragmentation in AI (“If deep learning can be combined with other AI techniques effectively, that could produce more rounded, useful systems.”).23\n• Since the human mind is a kluge [29], it should not matter if AI systems are the same—but see Appendix B.4.\nBut on the minus side:\n• Creating a kluge may yield short-term gains but is unlikely to be satisfactory on longer timescales [62, Sections 2, 6, and 7].\n• Creating a kluge may be a distraction from the long-term goal of developing ‘cognitive computing’, as described by Kelly and Hamm, both of IBM:\n“The creation of this new era of [cognitive] computing is a monumental endeavor ... no company can take on this challenge alone. So we look to our clients, university researchers, government policy makers, industry partners, and entrepreneurs—indeed the entire tech industry—to take this journey with us.” [21, Preface].\nOn balance, a two-pronged strategy is probably best: take advantage of short-term gains that may accrue from developing Watson as a kluge and, at the same time, develop cleaner and more elegant solutions to problems in AI. The SP system is a good candidate for inclusion in the second strand of research."
    }, {
      "heading" : "12 Big data and autonomous robots",
      "text" : "Potential benefits and applications for the SP theory are summarised in Appendix A.8. This section gives a bit more detail about two areas of potential application: big data and autonomous robots.\n23“IBM pushes deep learning with a Watson upgrade”, MIT Technology Review, 2015-07-09, bit.ly/1Nq0bMg."
    }, {
      "heading" : "12.1 Big data",
      "text" : "The paper “Big data and the SP theory of intelligence” [60] describes how the SP theory may help to solve nine problems with big data:\n• Helping to overcome the problem of variety in big data. The SP system may serve as a universal framework for the representation and processing of knowledge (UFK), helping to tame the great variety of formalisms and formats for data, each with its own mode of processing (Section 10).\n• Learning and discovery. In accordance with the DONSVIC principle [57, Section 5.2], the system has strengths in the unsupervised learning or discovery of ‘natural’ structures in data, with potential for further development.\n• Interpretation of data. The SP system has strengths in areas such as pattern recognition, information retrieval, parsing and production of natural language, translation from one representation to another, several kinds of reasoning, planning and problem solving.\n• Velocity: analysis of streaming data. The SP system lends itself to an incremental style, assimilating information as it is received, much as people do.\n• Volume: making big data smaller. Reducing the size of big data via lossless compression can yield direct benefits in the storage, management, and transmission of data, and indirect benefits in several of the other areas discussed in [60].\n• Supercharging the transmission of data. In addition to economies in the transmission of data via simple reductions in volume, there is potential for additional and very substantial economies in the transmission of data by judicious separation of ‘encoding’ and ‘grammar’.\n• Computational and energy efficiency. There is potential for large gains in the computational efficiency of computers, with corresponding savings in the use of energy in computing, and for corresponding reductions in the size and weight of computers.\n• Veracity: managing errors and uncertainties in data. The SP system can identify possible errors or uncertainties in data, suggest possible corrections or interpolations, and calculate associated probabilities.\n• Visualisation. Knowledge structures created by the system, and inferential processes in the system, are all transparent and open to inspection. They lend themselves to display with static and moving images.\nConsidering these proposed solutions collectively, and in several cases individually, it appears that there are no alternatives that can rival the potential of what is described in [60]."
    }, {
      "heading" : "12.2 Autonomous robots",
      "text" : "The paper “Autonomous robots and the SP theory of intelligence” [59] describes how the SP theory may help in the design of the information-processing ‘brains’ of autonomous robots:\n• Computational and energy efficiency. This is a revised version of the discussion in [60].\n• Towards human-like versatility in intelligence. The strengths of the SP system in diverse areas, summarised in Appendix A.7, can help in the development of human-like versatility in autonomous robots.\n• Towards human-like adaptability in intelligence. It appears that unsupervised learning in the SP framework has potential as a key to human-like adaptability in intelligence, both directly and as a basis for other kinds of learning.\nThis approach to the development of intelligence in autonomous robots is quite different from others, and arguably more promising."
    }, {
      "heading" : "13 Conclusion",
      "text" : "Preceding sections of this paper have aimed to highlight distinctive features of the SP theory of intelligence and its apparent advantages compared with some AI-related alternatives.\nSection 2 summarises distinctive features and advantages of the SP system:\n• Simplification and integration of observations and concepts;\n• Simplification and integration of structures and processes in computing systems;\n• The SP theory is itself a theory of computing;\n• The theory has a central role for information compression via the matching and unification of patterns;\n• More specifically, all processing is done via a concept of multiple alignment, borrowed and adapted from bioinformatics. This is perhaps the most distinctive feature of the theory;\n• Transparency in the representation and processing of knowledge;\n• The discovery of ‘natural’ structures via information compression (DONSVIC);\n• Interpretation of aspects of mathematics in terms of the SP theory;\n• Interpretation of phenomena in human perception and cognition;\n• Realisation of abstract concepts in terms of neurons and their interconnections (SP-neural).\nIn sections that follow, distinctive features and advantages of the SP system have been highlighted in comparison with alternatives:\n• The concept of minimum length encoding and related concepts;\n• Deep learning in neural networks;\n• Concepts of universal search;\n• Bayesian networks and other models for AI;\n• The analysis and production of natural language;\n• The learning of natural language;\n• Exact and inexact forms of reasoning;\n• Representation and processing of diverse forms of knowledge;\n• IBM’s Watson;\n• Solving problems associated with big data, and in the development of intelligence in autonomous robots.\nThe main conclusion of the paper is that, while some alternatives to the SP system may deliver attractive short-term benefits, a major strength of SP system is that it can provide a firm foundation for the long-term development of many aspects of AI and, at the same time, it may deliver some benefits and applications on relatively short timescales.\nIt is envisaged that a high-parallel, open-source version of the SP machine will be created, hosted on an existing high-performance computer, and derived from the existing SP computer model. This will be a means for researchers everywhere to explore what can be done with the system, and to create new versions of it.\nAppendices"
    }, {
      "heading" : "A Outline of the SP theory",
      "text" : "An noted in the introduction, the SP theory is a unique attempt to simplify and integrate observations and concepts across artificial intelligence, mainstream computing, mathematics, and human perception and cognition.\nThe theory and its applications are described most fully in [55] and more briefly in [57]. Other aspects of the theory are described in other papers referenced elsewhere in this paper.\nThe theory is conceived as an abstract brain-like system that, in an ‘input’ perspective, may receive New information via its senses, and compress some or all of it to create Old information, as illustrated schematically in Figure 1. In the theory, information compression is the mechanism both for the learning and organisation of knowledge and for pattern recognition, reasoning, problem solving, and more.\nThe SP theory originates in part from an earlier programme of research on grammatical inference and the unsupervised learning of natural language, with information compression as a unifying theme (summarised in [53]). However, meeting the goals of the SP research programme has meant a radical reorganisation of the system, with the development of a concept of multiple alignment (Appendix A.4.2) as a framework for the simplification and integration of diverse structures and functions [59, Section V-A.4].\nThe subsections that follow outline the main elements of the theory.\nA.1 Foundations\nThe SP theory is founded on a range of observations suggesting the fundamental importance of information compression in natural and artificial\nintelligence, in computing, and in mathematics ([61], [55, Chapter 2]).\nA.2 The SP computer model and the SP machine\nThe SP theory is realised in the form of a computer model, SP71, which may be regarded as a version of the SP machine.\nAn outline of the organisation and workings of the SP computer model works may be found in [55, Section 3.9], with more detail, including pseudocode, in [55, Sections 3.10 and 9.2].24 Fully commented source code for the SP71 computer model may be downloaded via a link near the bottom of www.cognitionresearch.org/sp.htm, and via “Ancillary files” under www.arxiv.org/abs/1306.3888.\nA.3 Patterns and symbols\nIn the SP system, knowledge is represented with arrays of atomic symbols in one or two dimensions called patterns. The SP71 model works with onedimensional patterns but it is envisaged that the system will be generalised to work with patterns in two dimensions [57, Section 3.3].\nAn ‘atomic symbol’ in the SP system is simply a mark that can be matched with any other symbol to determine whether it is the same or different: no other result is permitted.\n24These sources describe SP70, a slightly earlier version of the model than SP71 but quite similar to it. The description of SP70 includes a description, in [55, Sections 3.9.1 and 3.10], of a subset of the SP70 model called SP61.\nPatterns in two dimensions are likely to have a role in the processing of images ([55, Chapter 13], [58]) and also in the processing of sensory or motor streams of information that occur in parallel [59, Sections IV-A.4, IV-H, V-G to V=I, and Appendix C].\nIn themselves, SP patterns are not particularly expressive. But within the multiple alignment framework (Appendix A.4.2), they support the representation and processing of a wide variety of kinds of knowledge (Appendix A.7). It appears that the system has potential as a universal framework for the representation and processing of knowledge (UFK) [60, Section III].\nA.4 Information compression\nIn the SP system, all kinds of processing is done by compression of information. This is essentially the principle of minimum length encoding (MLE) [44, 51, 39]25 but with qualifications described in Section 3.\nThe default assumption in the SP theory is that compression of information is always lossless, meaning that all non-redundant information is retained. In particular applications, there may be a case for discarding nonredundant information (see, for example, [60, Section X-B]) but any such discard is reversible.\nThe name “SP” is short for Simplicity and Power, because compression of any given body of information, I, may be seen as a process of reducing “redundancy” of information in I and thus increasing its “simplicity”, whilst retaining as much as possible of its non-redundant descriptive and explanatory “power”. As noted in Section B, it is no accident that the same two concepts are prominent in Occam’s Razor as a touchstone of success for scientific theories.\nIn the SP system, information compression is achieved via the matching and unification of patterns, or parts thereof (‘ICMUP’, [61]). More specifically, it is achieved via the building of multiple alignments and via the unsupervised learning of grammars. These three things are described briefly in the following three subsections.\nA.4.1 Information compression via the matching and unification of patterns\nThe basis for information compression in the SP system is a process of searching for patterns that match each other with a process of merging or ‘unifying’\n25MLE is an umbrella term for “minimum message length” encoding (MML), “minimum description length” encoding (MDL), and similar concepts.\npatterns that are the same: “information compression via the matching and unification of patterns” or “ICMUP” [61].\nAt the heart of the SP computer model is a method for finding good full and partial matches between sequences with advantages compared with classical methods [55, Appendix A].26\nThe emphasis on ICMUP is motivated partly by evidence of the importance of such processes in human perception and cognition, and partly by its potential to cut through much complexity and to achieve a new perspective on artificial intelligence, mainstream computing, and mathematics [61].\nBecause one goal of the SP theory is to develop a new perspective on mathematics (Appendix A), and because ICMUP has been adopted as a foundation for the SP theory, the theory minimises the use of mathematics [61, Section 2.1].\nA.4.2 Information compression via the building of multiple alignments\nThe process for finding good full and partial matches between patterns is the foundation for processes that build multiple alignments like the ones shown in Figure 2. This concept is similar to multiple alignment in bioinformatics but with important differences [55, Section 3.4]. It is a powerful and distinctive feature of the SP system.\nIn Figure 2, the SP pattern in row 0 is a sentence to be parsed, while each of rows 1 to 8 contains an SP pattern representing a grammatical form (where ‘grammatical form’ includes words). This example shows the best multiple alignment created by the SP computer model when the New pattern is processed in conjunction with a set of pre-existing Old patterns like those shown in rows 1 to 8.\nHere, the ‘best’ multiple alignment is the one in which the New pattern may be encoded most economically in terms of the Old patterns—and this means a multiple alignment in which there is a relatively large number of symbols that match each other from row to row, aligned in columns. The way in which an encoding is derived from a multiple alignment is explained in [55, Section 3.5] and [57, Section 4.1]. Like all other kinds of knowledge in the SP system, encodings derived from multiple alignments are recorded using SP patterns (Appendix A.3).\n26The main advantages are [55, Section 3.10.3.1]: 1) That it can match arbitrarily long sequences without excessive demands on memory; 2) For any two sequences, it can find a set of alternative matches (each with a measure of how good it is) instead of a single ‘best’ match; 3) The ‘depth’ or thoroughness of the searching can be controlled by parameters.\nThe overall effect of this multiple alignment is to analyse the sentence into its grammatical parts and sub-parts, an analysis that is, in its essentials, the same as a conventional parsing.\nA.4.3 Information compression via unsupervised learning\nAs outlined in [55, Section 3.9.2] and [57, Section 5.1], and described more fully in [55, Chapter 9], the SP system may, without assistance from a “teacher” or anything equivalent, derive one or more plausible grammars from a body of New patterns, with information compression as a guiding principle. In that process, multiple alignment has a central role as a source of SP patterns for possible inclusion in any grammar [55, Section 9,2,5], [57, Section 5.1.1].\nAlthough the current model has some shortcomings (Appendix A.9, [57, Section 3.3]), it appears that these problems are soluble.\nA.4.4 Heuristic search\nLike most problems in artificial intelligence, each of the afore-mentioned problems—finding good full and partial matches between patterns, finding or constructing good multiple alignments, and inferring one or more good grammars from a body of data—is normally too complex to be solved by exhaustive search.\nWith intractable problems like these, it is often assumed that the goal is to find theoretically ideal solutions. But with these and most other AI problems, “The best is the enemy of the good”. By scaling back one’s ambitions and searching for “reasonably good” solutions, it is often possible to find solutions that are useful, and without undue computational demands.\nAs with other AI applications, and as with the building of multiple alignments in bioinformatics, the SP71 model uses heuristic techniques—‘hill climbing’ or ‘descent’—in all three cases mentioned above [55, Appendix A; Sections 3.9 and 3.10; Chapter 9]. This means searching for solutions in stages, with a pruning of the search tree at every stage, guided by measures of compression, and with backtracking to increase the chances of success. With these kinds of techniques, acceptably good approximate solutions can normally be found without excessive computational demands and with “big O” values that are within acceptable limits.\nA.4.5 Grammars and encodings, simplicity and power\nIn unsupervised learning in the SP system, compression of a body of information, I, produces two distinct results: a grammar and an encoding of I in terms of the grammar, both of them expressed as SP patterns. The two together represent a lossless compression of I.\nThe term ‘grammar’ has been adopted because the SP programme of research derives largely from earlier research on models of language learning and grammatical inference27Summarised in [53].) but, because of the versatility of SP patterns in the multiple alignment framework (Appendix A.3), the term is applied, in this research, to any kind of knowledge, not just natural language.\nOften but not invariably, there is a trade-off between the size of the grammar and the size of the encoding: as a general rule, small grammars yield large encodings and vice versa. Normally, the greatest overall compression of I is obtained with grammars that are not at the extremes of size (small or big), and likewise for encodings. It appears that this means learning that avoids both over-generalisation and under-generalisation (Sections 8.0.4 and 4.8).\nFrom the trade-off we can see that there is a direct relationship between the concepts of ‘grammar’ and ‘encoding’ on the one hand, and the aforementioned concepts of ‘simplicity’ and ‘power’ on the other: for a given I, there is simplicity in any grammar when the grammar is small, and the grammar has power when the encoding is small. Any reasonably thorough\n27(\ncompression of I is likely to yield a good balance between the two.28\nA.5 Information compression, prediction, and probabilities\nOwing to the close connection between information compression and concepts of prediction and probability [28], the SP system is fundamentally probabilistic. Each SP pattern has an associated frequency of occurrence and probabilities may be calculated for each multiple alignment and for any inference that may be drawn from any given multiple alignment.\nAlthough the SP system is fundamentally probabilistic: it can be constrained to answer only those kinds of questions where probabilities are close to 0 or 1; and, via the use of error-reducing redundancy, it can deliver decisions with high levels of confidence. Contrary to what one may suppose, there is no conflict between the use of error-reducing redundancy and the notion that “computing” may be understood as information compression—the two things are independent, as described in [55, Section 2.3.7].\nA.6 SP-neural\nPart of the SP theory is the idea, described most fully in [55, Chapter 11], that the abstract concepts of symbol and pattern in the SP theory may be realised more concretely in the brain with collections of neurons in the cerebral cortex.\nThe neural equivalent of an SP pattern is called a pattern assembly. The word “assembly” has been adopted in this term because the concept is quite similar to Donald Hebb’s [13] concept of a cell assembly. The main difference is that the concept of pattern assembly is unambiguously explicit in proposing that the sharing of structure between two or more pattern assemblies is achieved by means of ‘references’ from one structure to another, as described and discussed in [55, Section 11.4.1]). Also, learning in the SP system is quite different from the kind of “Hebbian” learning that is popular in artificial neural networks (see Sections 4.4 and 4.5).\nFigure 3 shows schematically how pattern assemblies may be represented and inter-connected with neurons. Here, each pattern assembly, such as ‘< NP < D > < N > >’, is represented by the sequence of atomic symbols of the corresponding SP pattern. Each atomic symbol, such as ‘<’ or ‘NP’, would\n28Here, the qualification, ‘reasonably thorough’ is quite important. Compression algorithms like the popular LZ algorithm is ‘quick and dirty’—it is designed for speed on low-powered computers and, for that reason, will normally miss quite large amounts of redundancy.\nbe represented in the pattern assembly by one neuron or a small group of inter-connected neurons.29 Apart from the inter-connections amongst pattern assemblies, the cortex in SP-neural is somewhat like a sheet of paper on which knowledge may be written in the form of neurons.\nIt is envisaged that any pattern assembly may be ‘recognised’ if it receives more excitatory inputs than rival pattern assemblies, perhaps via a winnertakes-all mechanism [55, Section 11.3.4]. And, once recognised, any pattern\n29Not shown in the figure are lateral connections within each pattern assembly and inhibitory connections elsewhere, as outlined in [55, Sections 11.3.3 and 11.3.4].\nassembly may itself be a source of excitatory signals leading to the recognition of higher-level pattern assemblies.\nA.7 Empirical and conceptual support for the SP theory\nAs noted in Appendix B.2, the SP theory has non-trivial things to say about a wide range of observations and concepts in artificial intelligence, mainstream computing, mathematics, and human perception and cognition. These things are described most fully in [55], more briefly in [57], and in extended summaries in [59, Sections IV and V]. In a bare-bones summary, the main strengths of the SP system are in:\n• Natural language processing [55, Chapter 5], [57, Section 8].\n• Pattern recognition and vision [55, Chapter 6], [57, Section 9], [58].\n• Information storage and retrieval [55, Chapter 6], [57, Section 11], [56].\n• The representation and processing of diverse kinds of knowledge ([57, Section 7], [60, Section III-B] and, more generally, [55, Chapters 5 to 10]).\n• Benefits accruing from the seamless integration of diverse kinds of knowledge and diverse aspects of intelligence [62, Sections 2, 5, and 7].\n• Several kinds of reasoning [55, Chapter 7], [57, Section 10].\n• Planning and problem solving [55, Chapter 8], [57, Section 12].\n• Unsupervised learning [55, Chapter 9], [57, Section 5], [59, Section V].\n• Implications for our understanding of human perception and cognition, including neural processing [55, Chapters 11 and 12], [58].\n• Implications for our understanding of the nature of mathematics [55, Chapter 10], [61].\nThere is more detail about some of these capabilities in the body of the paper.\nA.8 Potential benefits and applications\nIn summary, potential benefits and applications of the SP system include:\n• Helping to solve nine problems associated with big data [60].\n• Computer vision and pattern recognition that models several aspects of natural vision [58], [57, Section 9].\n• The development of versatility and adaptability in autonomous robots, with potential for gains in computational efficiency [59].\n• The system may be developed as a versatile database management system, with intelligence [56].\n• The system may serve as a repository for medical knowledge and as an aid for medical diagnosis [54].\n• There are several other potential benefits and applications in: simplification of computing systems, including software; unsupervised learning; the processing of natural language; software engineering; information compression; the semantic web; bioinformatics; the detection of computer viruses; data fusion; new kinds of computer; the development of scientific theories; and the seamless integration of diverse kinds of knowledge and processing [62].\nAs describe in Section A.9, next, some potential applications may be developed on relatively short timescales.\nTwo of the potential areas of application—big data and autonomous robots—are outlined in more detail in Section 12.\nA.9 Development of the SP system\nLike most scientific theories, the SP system is not complete [57, Section 3.3]. As it is now, the main shortcomings in the SP computer model are: the process for finding good full and partial matches between one-dimensional patterns needs to be generalised to patterns in two dimensions; a better understanding is needed of how the system may be applied to the discovery and recognition of low-level features in speech and images; in unsupervised learning, the model does not learn intermediate levels of abstraction or discontinuous dependencies in data; and a better understanding is needed of how the system may be applied in the representation and processing of numbers. It appears that none of these problems are showstoppers===that all of them are soluble.\nSince there are many more avenues to be explored than could be tackled by any one research group (Appendices A.7 and A.8), it is envisaged that the SP computer model will be the basis for the creation of a high-parallel, opensource version of the SP machine, hosted on an existing high-performance computer [60, Section XII]. This will be a means for researchers everywhere to explore what can be done with the system, and to create new versions of it.\nSome potential applications of the SP system may be developed on relatively short timescales using existing high-performance computers or even ordinary computers. These include the SP system as an intelligent database [56], and applications in such areas as medical diagnosis [54], pattern recognition ([55, Chapter 6], [57, Section 9]), information compression [62, Section 6.7], highly-economical transmission of information [60, Section VIII], bioinformatics [62, Section 6.10.2], and natural language processing [62, Section 6.2]."
    }, {
      "heading" : "B Occam’s Razor: simplicity and power",
      "text" : "One of the most widely accepted principles in science—Occam’s Razor—is that a good theory should combine conceptual simplicity with explanatory or descriptive power. Albert Einstein expressed it thus: “A theory is more impressive the greater the simplicity of its premises, the more different things it relates, and the more expanded its area of application.”30\nIn these terms, Alan Turing’s concept of a ‘Universal Turing Machine’, and equivalent models such as Post’s [38] ‘Canonical System’, are undoubtedly good models of ‘computing’. But notwithstanding Turing’s vision that computers might become intelligent [49], the concept of a Universal Turing Machine, does not tell us how!31 Several decades of research have yielded some useful insights and some impressive applications but I believe it is fair to say that AI has been and is suffering from an excess of narrow subfields and, with some honourable exceptions, insufficient attention to the need to simplify and integrate observations and concepts across different areas.32 Hence the SP programme of research.\nIt is no accident that ‘simplicity’ and ‘power’ are also prominent in the SP theory:\n30Quoted in [18, p. 512] 31But Turing began to address that problem in a report about “unorganised\nmachines” [50]. 32Honourable exceptions include research aiming to develop “unified theories of cognition” and “artificial general intelligence”.\n• The two terms, together, are equivalent to ‘information compression’ (Appendix A.4);\n• Cosmologist John Barrow has written that “Science is, at root, just the search for compression in the world” [1, p. 247];\n• And information compression is central in the SP theory (Section A.4).\nB.1 Motivation\nPart of the motivation for developing the SP theory has been to try to overcome the problem of narrow focus and over-specialisation, identified by other authors:\n• Neisser [34] writes of the need to avoid ‘microtheories’ in psychology.\n• In a similar vein, Newell, in his famous essay “You can’t play 20 questions with nature and win” [35], urges researchers to develop theories with wide scope (pp. 284–289) dealing with “a genuine slab of human behaviour” (p. 303).\n• Kelly and Hamm, both of IBM, write that “Today, as scientists labor to create machine technologies to augment our senses, there’s a strong tendency to view each sensory field in isolation as specialists focus only on a single sensory capability. Experts in each sense don’t read journals devoted to the others senses, and they don’t attend one another’s conferences. Even within IBM, our specialists in different sensing technologies don’t interact much.” [21, location 1004].\n• And McCorduck writes “The goals once articulated with debonair intellectual verve by AI pioneers appeared unreachable ... Subfields broke off—vision, robotics, natural language processing, machine learning, decision theory—to pursue singular goals in solitary splendor, without reference to other kinds of intelligent behaviour.” [31, p. 417]. Later, she writes of “the rough shattering of AI into subfields ... and these with their own sub-subfields—that would hardly have anything to say to each other for years to come.” (ibid., p. 424). She adds: “Worse, for a variety of reasons, not all of them scientific, each subfield soon began settling for smaller, more modest, and measurable advances, while the grand vision held by AI’s founding fathers, a general machine intelligence, seemed to contract into a negligible, probably impossible dream.”\nB.2 Evaluation of the SP theory in terms of simplicity and power\nAlthough, in comparing one theory with another, we must rely on relatively informal assessments of simplicity and power, the SP theory, in those terms, appears to do well [62, Section 4]:\n• The key concept of multiple alignment, with associated processes, are, in the SP computer model, expressed in an ‘exec’ file that requires less than 500 KB of storage space.\n• The theory has non-trivial things to say about a wide range of observations and concepts in artificial intelligence, mainstream computing, mathematics, and human perception and cognition (Appendix A.7) and it has many potential benefits and applications (Appendix A.8).\nIt appears that the SP theory avoids what are perhaps the two most common pitfalls in the development of scientific theories: it is not an over-simple and over-general, catch-all theory, that explains everything and nothing; and it is not an over-specific theory that merely re-describes the data that it is meant to explain. In short, there appears to be a good balance between simplicity and power as outlined in Appendix A.4.5.\nB.3 If it works, who cares?\nSome people may argue that these concerns are misplaced—that researchers should concentrate on creating things that work and not worry about the development of good theory. It is true that a suck-it-and-see approach can produce useful results and may indeed be helpful in the development of theory. But no theory or bad theory is almost always a handicap: imagine the difficulties of space travel using Ptolemy’s epicycles as a guide, or the impoverishment of biology without an understanding of DNA.\nB.4 Since the human mind is a kluge, why worry about good theory for AI?\nGary Marcus has argued persuasively [29] that, as a result of the way biological evolution builds on what comes to hand, the human mind is, in many respects, a kluge, without the coherence or elegance of a well-designed piece of engineering. Since human perception and cognition is a source of inspiration and a touchstone of success for AI, some people may conclude that we\nneed not worry about good theory for AI. Here are some reasons to think otherwise:\n• Although it is clear that the human mind has many shortcomings, it also has extraordinary versatility and adaptability. This is still a major challenge for AI, a challenge which appears to demand the development of good theory.\n• Much progress in science has depended on a willingness to look for simplicity within the apparent complexity of the world, witness Newton’s laws of motion."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "The main aim of this paper is to highlight distinctive features of the SP theory of intelligence and its apparent advantages compared with some AI-related alternatives. The theory is outlined in an appendix, with pointers to where fuller information may be found. The distinctive features and advantages are summarised: simplification and integration of observations and concepts; simplification and integration of structures and processes in computing systems; the SP theory is itself a theory of computing; a central role for information compression via the matching and unification of patterns, and for multiple alignment; transparency in the representation and processing of knowledge; the discovery of ‘natural’ structures via information compression (DONSVIC); interpretation of aspects of mathematics; interpretation of phenomena in human perception and cognition; realisation of abstract concepts in terms of neurons and their inter-connections (SP-neural). Distinctive features and advantages of the SP system are highlighted in comparison with alternatives: the concept of minimum length encoding and related concepts; deep learning in neural networks; concepts of universal search; Bayesian networks and other models for AI; the analysis and production of natural language; the learning of natural language; exact and inexact forms of reasoning; representation and processing of diverse forms of knowledge; IBM’s Watson; solving problems associated with big data, and in the development of intelligence in autonomous robots. The main conclusion of the paper is that, while some alternatives to the SP system may ∗Dr Gerry Wolff, BA (Cantab), PhD (Wales), CEng, MBCS (CITP); CognitionResearch.org, Menai Bridge, UK; jgw@cognitionresearch.org; +44 (0) 1248 712962; +44 (0) 7746 290775; Skype: gerry.wolff; Web: www.cognitionresearch.org. 1 ar X iv :1 50 8. 04 08 7v 1 [ cs .A I] 1 7 A ug 2 01 5 deliver attractive short-term benefits, a major strength of SP system is that it can provide a firm foundation for the long-term development of many aspects of AI and, at the same time, it may deliver some benefits and applications on relatively short timescales. It is envisaged that a high-parallel, open-source version of the SP machine will be created, hosted on an existing high-performance computer and derived from the existing SP computer model. This will be a means for researchers everywhere to explore what can be done with the system, and to create new versions of it.",
    "creator" : "LaTeX with hyperref package"
  }
}