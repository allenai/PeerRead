{
  "name" : "1601.06823.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Survey on the attention based RNN model and its applications in computer vision",
    "authors" : [ "Feng Wang" ],
    "emails" : [ "f.wang-6@student.tudelft.nl", "D.M.J.Tax@tudelft.nl" ],
    "sections" : [ {
      "heading" : "1 Introduction 3",
      "text" : "1.1 What does ”attention” mean? . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.2 What are the applications of the attention based model in computer vision? 4 1.3 What is the attention based RNN model? . . . . . . . . . . . . . . . . . . . 5\n1.3.1 Recurrent neural network (RNN) . . . . . . . . . . . . . . . . . . . . 5 1.3.2 The RNN for sequence to sequence problem . . . . . . . . . . . . . . 5 1.3.3 Attention based RNN model . . . . . . . . . . . . . . . . . . . . . . 8\n1.4 What are the attention mechanisms introduced in this survey? . . . . . . . 9 1.5 What are the advantages of using attention based RNN model? . . . . . . . 10 1.6 Organization of this survey . . . . . . . . . . . . . . . . . . . . . . . . . . . 11"
    }, {
      "heading" : "2 The attention based RNN model 11",
      "text" : "2.1 Neural network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 2.2 A general RNN unit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 2.3 The model for sequence to sequence problem . . . . . . . . . . . . . . . . . 13\n2.3.1 Encoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 2.3.2 Decoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 2.3.3 Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n2.4 Attention based RNN model . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.4.1 Item-wise soft attention . . . . . . . . . . . . . . . . . . . . . . . . . 15 2.4.2 Item-wise hard attention . . . . . . . . . . . . . . . . . . . . . . . . . 16 2.4.3 Location-wise hard attention . . . . . . . . . . . . . . . . . . . . . . 16 2.4.4 Location-wise soft attention . . . . . . . . . . . . . . . . . . . . . . . 19\nar X\niv :1\n60 1.\n06 82\n3v 1\n[ cs\n2.5 Learning of the attention based RNN models . . . . . . . . . . . . . . . . . 23 2.5.1 Soft attention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 2.5.2 Hard attention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 2.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26"
    }, {
      "heading" : "3 Applications of the attention based RNN model in computer vision 27",
      "text" : "3.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 3.2 Image classification and object detection . . . . . . . . . . . . . . . . . . . . 27 3.3 Image caption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31"
    }, {
      "heading" : "4 Conclusion and future research 34",
      "text" : "References 37\nAbstract\nThe recurrent neural networks (RNN) can be used to solve the sequence to sequence problem, where both the input and the output have sequential structures. Usually there are some implicit relations between the structures. However, it is hard for the common RNN model to fully explore the relations between the sequences. In this survey, we introduce some attention based RNN models which can focus on different parts of the input for each output item, in order to explore and take advantage of the implicit relations between the input and the output items. The different attention mechanisms are described in detail. We then introduce some applications in computer vision which apply the attention based RNN models. The superiority of the attention based RNN model is shown by the experimental results. At last some future research directions are given."
    }, {
      "heading" : "1. Introduction",
      "text" : "In this section, we discuss the nature of attention based recurrent neural network (RNN) model. What does ”attention” mean? What are the applications of the attention based model in computer vision area? What is the attention based RNN model? What are the attention mechanisms introduced in this survey? What are the advantages of using attention based RNN model? These are the questions we would like to answer in this section."
    }, {
      "heading" : "1.1 What does ”attention” mean?",
      "text" : "In psychology, limited by the processing bottlenecks, humans tend to selectively concentrate on a part of the information, and at the same time ignore other perceivable information. The above mechanism is usually called attention [3]. For example, in human visual processing, although human’s eye has the ability to receive a large visual field, usually only a small part is fixated on. The reason is different areas of retina have different magnitude of processing ability, which is usually referred as acuity. And only a small area of the retina, fovea, has the greatest acuity. To allocate the limited visual processing resources, one needs to firstly choose a particular part of the visual field, and then focuses on it. For example, when humans are reading, usually the words to be read at the particular moment are attended and processed. As a result, there are two main aspects of attention:\n• Decide which part of the input needs to be focused on.\n• Allocate the limited processing resources to the important part.\nThe definition of attention introduced in psychology is very abstract and intuitive, so the idea is borrowed and widely used in computer science area, although some techniques do not contain exactly the word ”attention”. For example, a CPU will only load the data needed for computing instead of loading all data available, which can be seen as a näıve application of the attention. Furthermore, to make the data loading more effective, the multilevel storage design in computer architecture is employed, i.e., the allocation of computing resources is aided by the structure of multi caches, main memory, and storage device as shown in Figure 1. In a big picture, all of them make up an attention model."
    }, {
      "heading" : "1.2 What are the applications of the attention based model in computer vision?",
      "text" : "As mentioned above, the attention mechanism plays an important role in human visual processing. Some researchers also bring the attention into computer vision area. As in the human perception, a computer vision system should also focus on the important part of the input image, instead of giving all pixels the same weights. A simple and widely used method is extracting local image features from the input. Usually the local image features can be points, lines, corners, or small image patches, and then some measurements are taken from the patches and converted into descriptors. Obviously, the distribution of local features’ positions in a natural image is not uniform, which makes the system give different weights to different parts of the input image, and satisfies the definition of attention.\nSaliency detection is another typical example directly motivated by human perception. As mentioned above, humans have ability to detect salient objects/regions rapidly and then attend on by moving the line of sight. This capability is also studied by the computer vision community, and many saliency object/region detection algorithms are proposed, e.g. [2, 24, 35]. However, most of the saliency detection methods only use the low level image features, e.g., contrast, edge, intensity, etc., which makes them bottom-up, stimulus-driven, and fixed. So they cannot capture the task specific semantic information. Furthermore, the detections of different saliency objects/regions in an image are individually and independent, which is obviously unlike the humans, where the next attended region of an image usually is influenced by the previous perceptions.\nThe sliding window paradigm [11, 52] is another model which matches the essence of attention. It is common that the interested object only occupies a small region of the\ninput image, and sometimes there are some limitations in the computational capabilities, so the sliding window is widely used in many computer vision tasks, e.g., object detection, object tracking, etc. However, since the size, shape and location of a window can be assigned arbitrarily, in theory there are infinite possible windows for an input image. Lots of work is devoted to reducing the number of windows to be evaluated, and some of them make notable speedups compared to the näıve method. But most of the window reducing algorithms are specifically designed for object detection or object tracking tasks, and the lack of universality makes them difficult to be applied in other tasks. Besides, most of the sliding window methods do not have the ability to take full advantage of the past processed data in future prediction."
    }, {
      "heading" : "1.3 What is the attention based RNN model?",
      "text" : "Note that the attention is just a mechanism, or a methodology, so there is no strict definition in mathematics what it is. For example, the local image features, saliency detection, sliding window methods introduced above, or some recently proposed object detection methods like [19], all employ the attention mechanism in different mathematical forms. On the other hand, as illustrated later, the RNN is a specific type of neural network with a specific mathematical description. The attention based RNN models in this survey refers in particular to the RNN models which are designed for sequence to sequence problems with the attention mechanism. Besides, all the systems in this survey are end-to-end trainable, where the parameters for both the attention module and the common RNN model should be learned simultaneously. Here we will give a general explanation of what the RNN is and what the attention based RNN model is, and the mathematical details are left to be described later."
    }, {
      "heading" : "1.3.1 Recurrent neural network (RNN)",
      "text" : "Recently, the convolutional neural network (CNN) is very popular in the computer vision community. But the biggest problem for the CNN is that it only accepts a fixed length input vector and gives a fixed-length output vector. So it cannot handle the data with rich structures, especially the sequences. That is the reason why RNN is interesting, which can not only operate over sequences of input vectors, but also generate sequences of output vectors. Figure 2 gives an abstract structure of a RNN unit.\nThe blue rectangles are the input vectors in a sequence, and the yellow rectangles are the output vectors in a sequence. By holding a hidden state (green rectangles in Figure 2), the RNN is able to process the sequence data. The dashed arrow indicates sometimes the output vectors do not have to be generated. The details of the structure of the neural network, the recurrent neural network, how the hidden state works, and how the parameters are learned will be described later."
    }, {
      "heading" : "1.3.2 The RNN for sequence to sequence problem",
      "text" : ""
    }, {
      "heading" : "1.3.2.1. Sequence to sequence problem",
      "text" : "As mentioned above, the RNN unit holds a hidden state which empowers it to process sequential data with variable sizes. In this survey, we focus on the RNN models for sequence to sequence problem. The sequence to sequence problem is defined as to map the input sequence to the output sequence, which is a very general definition. Here the input sequence\ndoes not refer strictly to a sequence of items, otherwise for different sequence to sequence problems, it could be in different forms. For example:\n• For the machine translation task, one usually needs to translate some sentences in the source language to the target language. In this case, the input sequence is a natural language sentence, where each item in the sequence is the word in the sentence. The order of the items in the sequence is critical.\n• In the video classification task, a video clip usually should be assigned a label. In this case, the input sequence is a list of frames of the video, where each frame is an image. And the order is critical.\n• The object detection task requires the model to detect some objects in an input image. In this case, the input sequence is only an image which can be seen as a list of objects. But obviously, it is not trivial to extract those objects from the image without additional data or information. So for the object detection task, the input of the model is just a single feature map (an image).\nSimilarly, the output sequence also does not always contain explicit items, but in this survey, we only focus on the problems where the output sequence has explicit items, although sometimes the number of items in the sequence is one. Besides, no matter the input is a feature map or it contains some explicit items, we always use ”input sequence” to represent them. And the term ”item” is used to describe the item in the sequence no matter it is contained by the sequence explicitly or implicitly."
    }, {
      "heading" : "1.3.2.2. The structure of the RNN model for sequence to sequence problem",
      "text" : "Under the condition that the lengths of the input and output sequences can vary, it is not possible for a common RNN to directly construct the corresponding relationships between the items in the input and output sequences without any additional information. As a\nresult, the RNN model needs to firstly learn and absorb all (or a part of) useful information from all of the input items, and then make the predictions. So it is natural to divide the whole system into two parts: the encoder, and the decoder [10, 47]. The encoder encodes all (or a part of) the information of the input data to an intermediate code, and the decoder employs this code in generating the output sequence. In some sense, all neural networks can be cut into two parts in any position, the first half is called the encoder, and the second half is treated as the decoder, so here the encoder and decoder are both neural networks.\nFigure 3 gives a visual illustration of a RNN model for sequence to sequence problem. The blue rectangles represent the input items, the dark green rectangles are the hidden states of the encoder, the shallow green rectangles represent the hidden states of the decoder, and the yellow rectangles are the output items. Although there are three input items and three output items in Figure 3, actually the number of input and output items can be arbitrary number and do not have to be the same. And the rectangle surrounded by the red box in Figure 3 serves as the ”middleman” between the input and the output which is the intermediate code generated by the encoder. In Figure 3 both the encoder and the decoder are recurrent networks, while with different types of the input sequence mentioned above, the encoder does not have to be recurrent all the time. For example, when the input is a feature map, a neural network without recurrence usually is applied as the encoder.\nWhen dealing with a supervised learning problem, during training, usually the ground truth output sequence corresponds to an input sequence is available. With the predicted output sequence, one can calculate the log-likelihood between the items which have the same indices in the predicted and the ground truth output sequences. The sum of the\nlog-likelihood of all items usually is set as the objective function of the supervised learning problem. And then usually the gradient decent/ascent is used to optimize the objective in order to learn the value of the parameters of the model."
    }, {
      "heading" : "1.3.3 Attention based RNN model",
      "text" : "For sequence to sequence problems, usually there are some corresponding relations between the items in the output and input sequences. However for a common RNN model introduced above, the length of the intermediate code is fixed, which prevents the model giving different weights to different items in an input sequence explicitly, so all items in the input sequence has the same importance no matter which output item is attempted to be predicted. This inspires the researchers to add the attention mechanism into the common RNN model.\nBesides, the encoding-decoding process can also be regarded as a compression process: firstly the input sequence is compressed into an intermediate code, and then the decoder decompresses the code to get the output. The biggest problem is that no matter what kind of input the model gets, it always compresses the information into a fixed length code, and then the decoder uses this code to decompress all items of a sequence. From an information theory perspective, this is not a good design since the length of the compressed code should have a linear relationship to the amount of information the inputs contain. As in the real file compression, the length of the compressed file is proportional to the amount of information the source files contain. There are some straightforward solutions to solve the problem mentioned above:\n1. Build an extremely non-linear and complicated encoder model such that it can store large amount of information into the intermediate code.\n2. Just use a long enough code.\n3. Stay with the fixed length code, but let the encoder only encode a part of the input items which are needed for the current decoding.\nBecause the input sequence can be infinite long (or contain infinite amount of information), the first solution does not solve the problem essentially. Needless to say, it is even a more difficult task to estimate the amount of information some data contains. Based on the same reason, solution 2 is not a good choice either. In some extreme cases, even if the amount of information is finite, the code should still be large enough to store all possible input-output pairs, which is obviously not practical.\nSolution 3 also cannot solve the problem when the amount of the input information needed for the current decoding is infinite. But usually a specific item in the output sequence only corresponds to a part of the input items. In this case, solution 3 is more practical and can prevent the problem in some degrees compared to other solutions if the needed input items can be ”located” correctly. Furthermore, it essentially has the same insights as the multilevel storage design pattern as shown in Figure 1. The RNN model which has the addressing ability is called the attention based RNN model.\nWith the attention mechanism, the RNN model is able to assign different weights to different parts of items in the input sequence, and consequently, the inherent corresponding\nrelations between items in input and output sequences can be captured and exploited explicitly. Usually, the attention module is an additional neural network which is connected to the original RNN model whose details will be introduced later."
    }, {
      "heading" : "1.4 What are the attention mechanisms introduced in this survey?",
      "text" : "In this survey, we introduce four types of attention mechanisms, which are: item-wise soft attention, item-wise hard attention, location-wise hard attention, and location-wise soft attention. Here we only give a brief and high-level illustration of them, and leave the mathematical descriptions in the next chapter.\nFor the item-wise and the location-wise, as mentioned above, the forms of the input sequence are different for different tasks. The item-wise mechanism requires that the input is a sequence containing explicit items, or one has to add an additional pre-processing step to generate a sequence of items from the input. However the location-wise mechanism is designed for the input which is a single feature map. The term ”location” is used because in most cases this kind of input is an image, and when treating it as a sequence of objects, all the objects can be pointed by their locations.\nAs described above, the item-wise method works on ”item-level”. After being fed into the encoder, each item in the input sequence has an individual code, and all of them make up a code set. During decoding, at each step the item-wise soft attention just calculates a weight for each code in the code set, and then makes a linear combination of them. The combined code is treated as the input of the decoder to make the current prediction. The only difference lies in the item-wise hard attention is that it instead makes a hard decision and stochastically picks some (usually one) codes from the code set according to their weights as the intermediate code fed into the decoder.\nThe location-wise attention mechanism directly operates on an entire feature map. At each decoding step, the location-wise hard attention mechanism discretely picks a sub-region from the input feature map and feeds it to the encoder to generate the intermediate code. And the location of the sub-region to be picked is calculated by the attention module. The location-wise soft attention still accepts the entire feature map as input at each step while otherwise makes a transformation on it in order to highlight the interesting parts instead of discretely picking a sub-region.\nAs mentioned above, usually the attention mechanism is implemented as an additional neural network connected to the raw RNN. The whole model should still be end-to-end, where both the raw RNN and the attention module are learned simultaneously. When it comes to soft and hard, for the soft attention, the attention module is differentiable with respect to the inputs, so the whole system can still be updated by gradient ascent/decent. However, since the hard attention mechanism makes hard decisions and gives discrete selections of the intermediate code, the whole system is not differentiable with respect to its inputs anymore. Then some techniques from the reinforcement learning are used to solve learning problem.\nIn summary, the attention mechanisms are designed to help the model select better intermediate codes for the decoder. Figure 4 gives a visual illustration of the four attention mechanisms which are about to be introduced in this survey."
    }, {
      "heading" : "1.5 What are the advantages of using attention based RNN model?",
      "text" : "Here we give some general advantages of the attention based RNN model. The detailed comparison between the attention based RNN model and the common RNN model will be illustrated later. Advantages:\n• First of all, as its name indicates, the attention based RNN model is able to learn to assign weights to different parts of the input instead of treating all input items equally, which can provide the inherent relations between the items the in input and output sequence. This is not only a way to boost the performance of some tasks, but it is also a powerful tool of visualization compared to the common RNN model.\n• The hard attention model does not need to process all items in the input sequence, instead, sometimes only processes the interested ones, so it is very useful for some tasks with only partially observable environments, like game playing, which usually cannot be handled by the common RNN model.\n• Not limited in computer vision area, the attention based RNN model is also suitable for all kinds of sequence related problems. For example:\n– Applications in natural language processing (NLP), e.g., machine translation [6, 37], machine comprehension [22], sentence summarization [43], word representation [33, 34].\n– Bioinformatics [46].\n– Speech recognition [8, 36].\n– Game play [38].\n– Robotics."
    }, {
      "heading" : "1.6 Organization of this survey",
      "text" : "In Section 2, we describe the general mathematical form of the attention based RNN model, especially four types of the attention mechanisms: item-wise soft attention, item-wise hard attention, location-wise hard attention, and location-wise soft attention. Next, we introduce some typical applications of the attention based RNN model in computer vision area in Section 3. At last, we summarize the insights of the attention based RNN model, and discuss some potential challenges and future research directions."
    }, {
      "heading" : "2. The attention based RNN model",
      "text" : "One can have a general idea of what the attention based RNN is from the last section. In this section, we firstly give a short introduction of the neural network, followed by an abstract mathematical description of the RNN. And then four attention mechanisms on RNN model are analyzed in detail."
    }, {
      "heading" : "2.1 Neural network",
      "text" : "A neural network is a data processing and computational model consists of multiple neurons which connect to each other. The basic component of all neural network is the neuron whose name indicates its inspiration from the human neuron cell. Ignoring the biological implication, a neuron in the neural network accepts one or more inputs, reweighs the inputs and sums them, and finally gives one output by passing the sum through an activation function. Let the input to the neuron be i =< i1, i2, . . . , in >, the corresponding weights be w =< w1, w2, . . . , wn >, and the output be o. A visual example of a neuron is given in Figure 5, where f is the activation function. And we have:\no = f ( n∑ k=1 (wkik) + w0 ) (1)\nwhere w0 is the bias parameter. If we add i0 = 1 into i and rewrite the previous equation in vector form, then\no = f ( n∑ k=0 (wkik) ) = f ( wT i ) (2)\nIn summary, a neuron just applies an activation function on the linear transformation of the input parameterized by the weights. Usually the activation functions are designed to be non-linear in order to import non-linearity into the model. There are many different forms of activation functions, i.e., sigmoid, tanh, ReLU [40], PReLU [21].\nA neural network is usually constructed by the connections of many neurons, and the most popular structure is layer by layer connection as shown in Figure 6. In Figure 6, each circle represents a neuron, and each arrow represents a connection between neurons. The outputs of all neurons in a layer in Figure 6 are connected to the next layer, which can be categorized as fully-connected layer. In summary, if we treat the neural network as a black box and see it from outside, a simple mathematical form can be obtained:\no = φW (i) (3)\nwhere i is the input, o is the output, and φW is a particular model consists of many neurons parameterized by W . One property of the model φW is that it is usually differentiable with respect to W and i. For more details about the neural network, one can refer to [44].\nThe convolutional neural network (CNN) is a specific type of neural network with structures designed for image inputs [30], which usually consists of multiple convolutional layers followed by a few fully-connected layers. The convolutional layer can take a region of a 2D\nfeature map as input, and this is the reason why it is suitable for image data. Recently the CNN is one of the most popular model as candidate for image related problems. A deep CNN model trained on some large image classification dataset (e.g., ImageNet [12]) has good generalization power/ability and can be easily transferred to other tasks. For more details about CNN, one can refer to [44]."
    }, {
      "heading" : "2.2 A general RNN unit",
      "text" : "As its name shows, the RNN is also a type of neural network, which therefore consists of a certain number of neurons as all kinds of neural network. The ”recurrent” in the RNN indicates that it performs the same operations for each item in the input sequence, and at the same time, keeps a memory of the processed items for future operations. Let the input sequence be I = (i1, i2, . . . , iT ). At each step t, the operation performed by a general RNN unit can be described by: [\nôt ht\n] = φW (it,ht−1) (4)\nwhere ôt is the predicted output vector at time t, and ht is the hidden state at time t. φW is a neural network parameterized by W which takes the t-th input item (it), and the previous hidden state ht−1 as inputs. Here it is clear that the hidden state h performs as a memory to store the previous processed information. From Equation (4), we can see the definition of a RNN unit is quite general, because there is no specific requirements for the structure of the network φW . People have already proposed hundreds or even thousands of different structures which are out of the scope of this paper. Now two widely used structures are LSTM (long-short term memory) [23] and GRU (gated recurrent units) [9]."
    }, {
      "heading" : "2.3 The model for sequence to sequence problem",
      "text" : "As shown in Figure 3, the model for sequence to sequence problem can be generally divided into two parts: the encoder, and the decoder, where both of them are neural networks. Let the input sequence be X, and the output sequence be Y . As mentioned above, in some tasks the input sequence does not consists of explicit items, for which we use X to represent the input sequence, and otherwise X = (x1,x2, . . . ,xT ). Besides, in this survey, only the output sequence containing explicit items is considered, i.e., Y = (y1, y2, . . . , yT ′) is the output sequence. The lengths of the input and the output sequences do not have to be the same."
    }, {
      "heading" : "2.3.1 Encoder",
      "text" : "As mentioned above, the core task of an encoder is to encode all (or a part of) the input items to an intermediate code to be decoded by the decoder. In the common RNN for sequence to sequence problem, the model does not try to figure out the corresponding relations between the items in the input and output sequences, and usually all items in the input sequence are compressed into a single intermediate code. So for the encoder,\nc = φWenc (X) (5)\nwhere φWenc represents the encoder neural network parameterized by Wenc, c is the intermediate code to be used by the decoder. Here the encoder can be any neural network, but its type usually depends on the input data. For example, when the input X is treated as a single feature map, usually a neural network without recurrence is used as the encoder, like a CNN for the image input. But it is also common to set encoder as a recurrent network when the input is a sequence of data, e.g., a video clip, a natural language sentence, a human speech clip."
    }, {
      "heading" : "2.3.2 Decoder",
      "text" : "If the length of the output sequence is larger than 1, in most cases the decoder is recurrent, because the decoder at least needs to have the knowledge what it has already predicted to prevent the repeated prediction. Especially in the RNN model with the attention mechanism which will be introduced later, the weights of the input items are assigned with the guidance of the past predictions, so in this case, the decoder should be able to store some history information. As a result, in this survey we only consider the cases where the decoder is an RNN.\nAs mentioned above, in a common RNN, the decoder accepts the intermediate code c as input, and at each step j generates the predicted output ŷj , and the hidden state hj by[\nŷj hj\n] = φWdec (c, ŷj−1,hj−1) (6)\nwhere φWdec represents the decoder recurrent neural network parameterized by Wdec, which accepts the code c, the previous hidden state hj−1, and the previous predicted output ŷj−1 as input. After T\n′ steps of decoding, a sequence of predicted outputs Ŷ = (ŷ1, ŷ2, . . . , ŷT ′) is obtained."
    }, {
      "heading" : "2.3.3 Learning",
      "text" : "Like many other supervised learning problems, the supervised sequence to sequence problem is also optimized by maximizing the log-likelihood. With the input sequence X, the ground truth output sequence Y , and the predicted output sequence Ŷ , the log-likelihood for the j-th item yj in the output sequence Y is\nLj (X,Y, θ) = log p (yj |X,y1, y2, . . . , yj−1, θ) = log p (yj |ŷj , θ) (7)\nwhere θ = [Wenc,Wdec] represents all learnable parameters in the model, and p() calculates the likelihood of yj based on ŷj . For example, in a classification problem where yj indicates the index of the label, the p (yj |ŷj , θ) = softmax(ŷj)yj . The objective function is then set as the sum of the log-likelihood:\nL(X,Y, θ) = T ′∑ j=1 Lj(X,Y, θ) = T ′∑ j=1 log p (yj |X,y1, y2, . . . , yj−1, θ) = T ′∑ j=1 log p (yj |ŷj , θ) (8)\nFor the common encoder-decoder system, both the encoder and decoder network are differentiable with respect to their parameters, Wenc, and Wdec, respectively, so the objective\nfunction is differentiable with respect to θ. As a result, one can update the parameters θ by gradient ascent to maximize the sum of the log-likelihood."
    }, {
      "heading" : "2.4 Attention based RNN model",
      "text" : "In this section, we give a comprehensive mathematical description of the four attention mechanisms. As mentioned above, essentially the attention module helps the encoder calculate a better intermediate code c for the decoder. So in this section, the decoder part is identical as shown in Section 2.3.2, but the encoder network φWenc may not always be the same as in Equation (5). In order to make the illustration clear, we still keep the term φWenc to represent the encoder."
    }, {
      "heading" : "2.4.1 Item-wise soft attention",
      "text" : "The item-wise soft attention requires the input sequence X contains some explicit items x1,x2, . . . ,xT . Instead of extracting only one code c from the input X, the encoder of the item-wise soft attention RNN model extracts a set of codes C from X:\nC = {c1, c2, . . . , cT ′′} (9) where the size of C does not have to be the same as X, which means one can use multiple input items to calculate a single code or extract multiple codes from one input item. For simplicity, we just set T ′′ = T . So\nct = φWenc (xt) for t ∈ (1, 2, . . . , T ) (10) where φWenc is the encoder neural network parameterized by Wenc. Note here that the encoder is different from the encoder in Equation (5), because the encoder in the item-wise soft attention model only takes one item of the sequence as input.\nDuring the decoding, the intermediate code c fed into the decoder is calculated by the attention module, which accepts all individual codes C, and the decoder’s previous hidden state hj−1 as input. At the decoding step j, the intermediate code is:\nc = cj = φWatt (C,hj−1) (11) 1\nwhere φWatt represents the attention module parameterized by Watt. Actually, as long as the attention module φWatt is differentiable with respect to Watt, it satisfies the requirements of the soft attention. Here we only introduce the first proposed item-wise soft attention model in [6] for natural language processing. In this item-wise soft attention model, at decoding step j, for each input code ct, a weight αtj is calculated by:\nejt = fatt (ct,hj−1) (12)\nand\nαjt = exp(ejt)∑T t=1 exp(ejt)\n(13)\n1. For simplicity, in the following sections, we just use c to represents the intermediate code fed into the decoder, and ignore the superscript j.\nwhere fatt usually is also a neural network within the attention module and calculates the unnormalized weight ejt. Here the normalized weight αtj is explained as the probability that how the code ct is relevant to the output yj , or the importance should be assigned to the t-th input item when making the j-th prediction. Note here that Equation (13) is a simple softmax function transforming the scores ejt to the scale from 0 to 1, which makes the αjt can be interpreted as a probability.\nSince now we have the probabilities, then the code c is just calculated by taken the expectation of all cts with their probabilities αjts:\nc = φWatt (C,hj−1)\n= E(ct)\n= T∑ t=1 αjtct\n(14)\nFigure 7 gives a visual illustration of how the item-wise soft attention works at decoding step j, where the purple ellipse represents the Equation (12) and Equation (13), and the encoder network is not recurrent."
    }, {
      "heading" : "2.4.2 Item-wise hard attention",
      "text" : "The item-wise hard attention is very similar to the item-wise soft attention. It still needs to calculate the weights for each code as shown from Equation (9) to Equation (13). As mentioned above, the αjt can be interpreted as the probability that how the code ct is relevant to the output yj , so instead of a linear combination of all codes in C, the itemwise hard attention stochastically picks one code based on their probabilities. In detail, an indicator lj is generated from a categorical distribution at decoding step j to indicate which code should be picked:\nlj ∼ C ( T, {αjt}Tt=1 ) (15)\nwhere C() is a categorical distribution parameterized by the probabilities of the codes ({αjt}Tt=1). And lj works as an index in this case:\nc = clj (16)\nWhen the size of C is 2, the above categorical distribution in Equation (15) turns into a Bernoulli distribution:\nlj = B (T, αj1) (17)"
    }, {
      "heading" : "2.4.3 Location-wise hard attention",
      "text" : "As mentioned above, for some types of input X, like an image, it is not trivial to directly extract items from them. So the location-wise hard attention model is developed which accepts the whole feature map X as input, stochastically picks a sub-region from it, and uses this sub-region to calculate the intermediate code at each decoding step. This kind of location-wise hard attention is analyzed in many previous works [13, 28], while here we only\nfocus on two recent proposed mechanisms in [39] and [4], because the attention models in those works are more general and can be trained end-to-end.\nSince the location-wise hard attention model only picks a sub-region of the input X at the decoding step, it makes sense to estimate that the picked region may not correspond to the output item to be predicted. There are two potential solutions for this problem:\n1. Pick only one input vector at each decoding step.\n2. Make a few glimpses for each prediction, which means at each decoding step, pick some sub-regions, process them and update the model one by one, where the subsequent region (glimpse) is acquired based on the previous processed regions (glimpses). And the last prediction is treated as the ”real” prediction in this decoding step. The number of glimpses M either can be an arbitrary number (a hyperparameter) or automatically be decided by the model.\nWith a large enough training dataset, it is reasonable to estimate that method 1 can also find the optimal while the convergence may take longer time because of the limitation mentioned above. In testing, obviously method 1 is faster than method 2, but there may be a sacrifice on the performance. For method 2, it may have better performance in testing, while it also needs longer calculating time. Till now there are not so many works done on how to select an appropriate number of glimpses which leaves it an open problem. However, it is clear that method 1 is just an extreme case of method 2, and in the following sections, we treat the hard attention models as taking M glimpses at each step of prediction.\nTo keep the notation consistent, here we still use term l as the indicator generated by the attention model to show which sub-region should be extracted, i.e., in the case of locationwise hard attention, the lmj indicates the location of the center of the sub-region is about to be picked. In detail, at the m-th glimpse in the decoding step j, the attention module accepts the input X, the previous hidden states of the decoder (hm−1j ), and calculates l m j by:\nlmj ∼ N ( fatt ( X,hm−1j ) , s )\n(18)\nwhere h0j = hj−1 and N () is a normal distribution with a mean fatt ( X,hm−1j ) and a standard deviation s. And fatt usually is a neural network similar to the fatt in Equation (12). Then the attention module picks the sub-region centered at location lmj :\nXmout = Xlmj (19)\nwhere Xlmj indicates a sub-region of X centered at l m j , and X m out represents the output sub-region at glimpse m. Note here that the shape, size of the sub-region, and the standard deviation s in Equation (18) are all hyperparameters. One can also let the attention network generate these parameters as the generation of lmj in Equation (18) [59]. When X m out is generated, it is fed into the encoder in calculating the code cm for the decoder, i.e.,\ncm = φWenc (X m out) (20)\nand then we have [ ŷmj hmj ] = φWdec ( cm, ŷm−1j ,h m−1 j ) (21)\nAfter M glimpses:\nc = cM (22)\nhj = h M j (23)\nŷj = ŷ M j (24)\nwhere hj is the j-th hidden state of the decoder, and ŷj is the j-th prediction."
    }, {
      "heading" : "2.4.4 Location-wise soft attention",
      "text" : "As mentioned above, the location-wise soft attention also accepts a feature map X as input, and at each decoding step, a transformed version of the input feature map is generated to calculate the intermediate code. It is firstly proposed in [25] called the spatial transformation network (STN). Instead of the RNN model, the STN in [25] is initially applied on a CNN model, but it can be easily transferred to a RNN model with tiny modifications. To make the explanations more clear, we use Xin to represent the input of the STN (where usually X = Xin), and Xout to represent the output of the STN. Figure 8 gives a visual example of how the STN is embedded into the whole framework, and how it works at decoding step j, in which the purple ellipse is the STN module. From Figure 8, we can see that after being processed by the STN, the Xin is transformed to Xout, and it is used to calculate the intermediate code c which is then fed to the decoder. The details of how the STN works will be illustrated in the following part of this section.\nLet the shape of Xin be Uin × Vin × Qin and the shape of Xout be Uout × Vout × Qout, where U , V , Q represent the height, width, and number of channels respectively, so when\nXin is a color image, its shape should be Uin× Vin× 3. The STN works identically on each channel to keep the consistency between the channels, so the number of channels of the input and output are the same (Q = Qin = Qout). A typical STN network consists of three parts shown in Figure 9: localization network, grid generator, and the sampler.\nAt each decoding time j, the localization network φWloc takes the input Xin, the previous hidden state of the decoder hj−1 as input, and generates the transformation parameter Aj as output:\nAj = φWloc(Xin,hj−1) (25) 2\nThen the transformation τ parameterized by Aj is applied on a mesh grid G generated by the grid generator to produce a feature map S which indicates how to select pixels3 from Xin and map them to Xout. In detail, the grid G consists of pixels of the output feature map Xout, i.e.,\nG = {Gi} = {( xXout1,1 , y Xout 1,1 ) , ( xXout1,2 , y Xout 1,2 ) , . . . , ( xXoutUout,Vout , y Xout Uout,Vout )} (26)\nwhere x and y represent the coordinates of the pixel, and ( x (Xout) 1,1 , y (Xout) 1,1 ) is the pixel\nof Xout which locates at coordinate (1, 1). And we have\nSi = τAj (Gi) (27)\nS = {Si} = {( xS1,1, y S 1,1 ) , ( xS1,2, y S 1,2 ) , . . . , ( xSUout,Vout , y S Uout,Vout )} (28)\nwhere each Si shows the coordinates in the input feature map that defines a sampling position. Figure 10 gives a visual example illustrating how the transformation and the grid\n2. Since the original STN in [25] is applied on a CNN, there is no hidden state. As a result, in [25] the location network only takes Xin as input, and Equation (25) changes to Aj = φWloc(Xin). 3. Here the pixel means the element of the feature map Xin, which does not have to be an image.\ngenerator work. In Figure 10, the grid G consists of the red dots in the right part of the figure. Then a transformation (the dashed green lines in the figure) is applied on G to generate S, which is the red (and blue) dots in the left part of the figure. Then S indicates the positions to perform the sampling.\nActually, τ can be any transformations, for example, the affine transformation, the plane projective transformation, or the thin plate spline. In order to make the descriptions above more clear, we assume τ is a 2D affine transformation, so Aj is a matrix consists of 6 elements:\nAj = [ a1,1 a1,2 a1,3 a2,1 a2,2 a2,3 ] (29)\nThen Equation (27) can be rewritten as:\nSi = ( xSi ySi ) = τAj (Gi) = Aj  xXoutiyXouti 1  = [a1,1 a1,2 a1,3 a2,1 a2,2 a2,3 ] xXoutiyXouti 1  (30)\nThe last step is the sampling which is operated by the sampler. Because S is calculated by a transformation, Si does not always correspond exactly to the pixels in Xin, hence a sampling kernel is applied to map the pixels in Xin to Xout:\nXqout,i = Uin∑ u Vin∑ v Xqin,u,vk ( xSi − v ) k ( ySi − u ) ∀i ∈ [1, 2, . . . , UoutVout] ∀q ∈ [1, 2, . . . , Q]\n(31) where k can be any kernel as long as it is partial differentiable with respect to both xSi and ySi , for example, one of the most widely used sampling kernel for images is the bilinear interpolation:\nXqout,i = Uin∑ u Vin∑ v Xqin,u,v max ( 0, 1− ∣∣xSi − v∣∣)max (0, 1− ∣∣ySi − u∣∣) ∀i ∈ [1, 2, . . . , UoutVout] ∀q ∈ [1, 2, . . . , Q]\n(32)\nwhen the coordinates of Xin and Xout are normalized, i.e., ( xXin1,1 , y Xin 1,1 ) = (−1,−1) and(\nxXinUin,Vin , y Xin Uin,Vin\n) = (+1,+1). A visual example of the bilinear interpolation is shown in\nFigure 11, where the blue dot represents a sampling position, and its value is calculated by a weighted sum of its surrounding pixels as shown in Equation (32). In Figure 11, the surrounding pixels of the sampling position are represented by four red intersection symbols.\nNow if we review the whole process described above, the STN:\n1. Accepts a feature map, and the previous hidden state of the decoder as input.\n2. Generates parameters for a pre-defined transformation.\n3. Generates the sampling grid and calculates the corresponding sampling positions in the input feature map based on the transformation.\n4. Calculates the output pixel values by a pre-defined sampling kernel.\nTherefore, for an input feature map Xin, an output feature map Xout is generated which has the ability to only focus on some interesting parts in Xin. For instance, the affine transformation defined in Equation (30) allows translation, rotation, scale, skew, and cropping, which is enough for most of the image related tasks. The whole process introduced above, i.e., feature map transformation, grid generation, and sampling, is inspired by the standard texture mapping in computer graphics.\nAt last the Xout is fed into the encoder to generate the intermediate code c:\nc = φWenc(Xout) (33)"
    }, {
      "heading" : "2.5 Learning of the attention based RNN models",
      "text" : "As mentioned in the introduction, the difference between the soft and hard attention mechanisms in optimization is that the soft attention module is differentiable with respect to its parameters so the standard gradient ascent/decent can be used for optimization. However, for the hard attention, the model is non-differentiable and the techniques from the reinforcement learning are applied for optimization."
    }, {
      "heading" : "2.5.1 Soft attention",
      "text" : "For the item-wise soft attention mechanism, when fatt in Equation (12) is differentiable with respect to its inputs, it is clear that the whole attention model is differentiable with respect to Watt, as well as the whole RNN model is differentiable with respect to θ = [Wenc,Wdec,Watt]. As a result, the same sum of log-likelihood in Equation (8) is used as the objective function for the learning, and the gradient ascent is used to maximize the objective.\nThe location-wise soft attention module (STN) is also differentiable with respect to its parameters if the location network φWloc , the transformation τ and the sampling kernel k are carefully selected to ensure their gradients with respect to their inputs can be defined. For example, when using a differentiable location network φWloc , the affine transformation (Equation (29), Equation (30)), and the bilinear interpolation (Equation (32)) as the sampling kernel, we have:\n∂Xqout,i ∂Xqin,u,v = Uin∑ u Vin∑ v max ( 0, 1− ∣∣xSi − v∣∣)max (0, 1− ∣∣ySi − u∣∣) (34) ∂Xqout,i\n∂xSi = Uin∑ u Vin∑ v Xqin,u,v max ( 0, 1− ∣∣ySi − u∣∣)  0 if ∣∣v − xSi ∣∣ ≥ 1\n1 if v ≥ xSi −1 if v < xSi\n(35)\n∂Xqout,i\n∂ySi = Uin∑ u Vin∑ v Xqin,u,v max ( 0, 1− ∣∣xSi − v∣∣)  0 if ∣∣u− ySi ∣∣ ≥ 1\n1 if u ≥ ySi −1 if u < ySi\n(36)\n∂xSi ∂a1,1 = xXouti (37)\n∂xSi ∂a1,2 = yXouti (38)\n∂xSi ∂a1,3 = 1 (39)\n∂ySi ∂a2,1 = xXouti (40)\n∂ySi ∂a2,2 = yXouti (41)\n∂ySi ∂a2,3 = 1 (42)\nNow the gradients of Wloc ( ∂Aj ∂Wloc\n) can be easily derived from Equation (25). As a result, the entire STN is differentiable with respect to its parameters. As in the item-wise soft attention, the objective function is the sum of the log-likelihood, and the gradient ascent is used for optimization."
    }, {
      "heading" : "2.5.2 Hard attention",
      "text" : "As shown in Section 2.4.2 and Section 2.4.3, the hard attention mechanism stochastically picks an item or a sub-region from the input. In this case, the gradients with respect to the picked item/region are zero because they are discrete. The zero gradients cannot be used to maximize the sum of the log-likelihood by the standard gradient ascent. In general, this is a problem of training a neural network with discrete values/units, and here we just introduced the method applied in [39, 4, 57] which employs the techniques from the reinforcement learning. The learning methods for the item-wise and location-wise hard attention are essentially the same, but the item-wise hard attention implicitly sets the number of glimpse as 1. In this section, we just make the number of glimpses as M .\nInstead of the raw log-likelihood in Equation (7), a new objective L′j is defined which is a variational lower bound on the log-likelihood log p (y i|X,y1, y2, . . . , y i−1, θ) 4 in Equation (7) parameterized by θ (θ = [Wenc,Wdec,Watt]). And we have\n4. For notational simplicity, we ignore the y1, y2, . . . , yi−1, θ in the log-likelihood later, so log p (yi|X,y1, y2, . . . , yi−1, θ) = log p (yi|X)\nlog-likelihood = Lj = log p(yj |X) > log ∑ lj p(lj |X)p(yj |lj , X)\n= ∑ lj p(lj |X) log p(yj |lj , X)\n= ∑ lj p(lj |xl) log p(yj |lj ,xl) = L′j\n(43)\nThen the derivative of L′j is:\n∂L′j ∂θ = ∑ lj ( p(lj |xl) ∂ log p(yj |lj ,xl) ∂θ + log p(yj |lj ,xl) ∂p(lj |xl) ∂θ )\n= ∑ lj p(lj |xl) ( ∂ log p(yj |lj ,xl) ∂θ + log p(yj |lj ,xl) ∂ log p(lj |xl) ∂θ ) (44) As shown above, lj is generated from a distribution (Equation (15), Equation (17), or\nEquation (18)), which indicates that p(lj |xl) and ∂ log p(lj |xl)\n∂θ can be estimated by a Monte Carlo sampling as demonstrated in [56]:\n∂L′j ∂θ ≈ 1 M M∑ m=1\n( ∂ log p(yj |lmj ,xl)\n∂θ + log p(yj |lmj ,xl) ∂ log p(lmj |xl) ∂θ\n) (45)\nThe whole learning process described above for the hard attention is equivalent to the REINFORCE learning rule in [56], and from the reinforcement learning perspective, after each step, the model can get a reward from the environment. In Equation (45), the log p(yj |lmj ,xl) is used as the reward Rj . But in the reinforcement learning, the reward can be assigned to an arbitrary value. Depending on the tasks to be solved, here are some widely used schemes:\n• Set the reward to be exactly Rj = log p(yj |lmj ,xl).\n• Set the reward to be a real value proportional to log p(yj |lmj ,xl), which means Rj = β log p(yj |lmj ,xl), and β is a hyperparameter.\n• Set the reward as a zero/one discrete value:\nRj =\n{ 1 yj = argmax\nyj log p(yj |lmj ,xl)\n0 otherwise. (46)\nThen Equation (45) can be written as:\n∂L′j ∂θ ≈ 1 M M∑ m=1\n( ∂ log p(yj |lmj ,xl)\n∂θ +Rj ∂ log p(lmj |xl) ∂θ\n) (47)\nNow as shown in [39], although Equation (47) is an unbiased estimation of the real gradient of Equation (44), it may have high variance because of the unbounded Rj . As a result, usually a variance reduction item b is added to the equation:\n∂L′j ∂θ ≈ 1 M M∑ m=1\n( ∂ log p(yj |lmj ,xl)\n∂θ + (Rj − bj) ∂ log p(lmj |xl) ∂θ\n) (48)\nwhere b can be calculated in different ways which are discussed in [4, 5, 39, 53, 57]. A common approach is to set bj = E(R). By using the variance reduction variable b, the training becomes faster and more robust. At last, a hyperparameter λ can be added to balance the two gradient components:\n∂L′j ∂θ ≈ 1 M M∑ m=1\n( ∂ log p(yj |lmj ,xl)\n∂θ + λ(Rj − bj) ∂ log p(lmj |xl) ∂θ\n) (49)\nWith the gradients in Equation (49), now the hard attention based RNN model can also be trained by gradient ascent. For the whole sequence Y , we have the new objective function L′(Y,X, ):\nL′(Y,X, θ) = T ′∑ j=1 ∑ lj p ( lj |xl ) log p ( yj |lj ,xl )\n= T ′∑ j=1 ∑ lj p (lj |X) log p (yj |lj , X) 6 T ′∑ j=1 log ∑ lj p(yj |X) = T ′∑ j=1 log p(yj |X)\n= T ′∑ j=1 log p (yj |X,y1, y2, . . . , yj−1, θ) = sum of the log-likelihood\n(50)"
    }, {
      "heading" : "2.6 Summary",
      "text" : "In this section, we firstly give a brief introduction of the neural network, the CNN, and the RNN. Then we discuss the common RNN model for sequence to sequence problem. And the detailed descriptions of four types of attention mechanisms are given: item-wise soft attention, item-wise hard attention, location-wise hard attention, and location-wise soft attention, followed by how to optimize the attention based RNN models."
    }, {
      "heading" : "3. Applications of the attention based RNN model in computer vision",
      "text" : "In this section, we firstly give a brief overview of the differences when one uses different attention based RNN models. And then two applications in computer vision which apply the attention based RNN model are introduced."
    }, {
      "heading" : "3.1 Overview",
      "text" : "• As mentioned above, the item-wise attention model requires that the input sequence consists of explicit items, which is not trivial for some types of input, like an image input. A simple solution is to manually extract some patches from the input images and treat them as a sequence of item. For the extraction method, one can either extract some fixed size patches from some pre-defined locations, or apply other object proposal methods, like [49].\n• The location-wise attention model can directly accept a feature map as input which avoids the ”items extraction” step mentioned above. And if compared to human perception, the location-wise attention is more appealing. Because when human being sees an image, he will not manually divide the image into some patches and calculate the weights for them before recognizing the objects in the image. Instead, he will directly focus on an object and its surroundings. That is exactly what the locationwise attention does.\n• The item-wise attention model has to calculate the codes for all items in the input sequence, which is less efficient than the location-wise attention model.\n• The soft attention is differentiable with respect to its inputs, so the model can be trained by optimizing the sum of the log-likelihood using gradient ascent. However, the hard attention is non-differentiable, usually the techniques from the reinforcement learning are applied to maximize a variational lower bound of the log-likelihood, and it makes sense to estimate there will be a sacrifice on the performance.\n• The STN keeps both the advantages of the location-wise attention and the soft attention, where it can directly accept a feature map as input and optimize the raw sum of log-likelihood. However, it is just recently proposed, and originally applied on CNN. Currently, there are not enough applications of the STN based RNN model."
    }, {
      "heading" : "3.2 Image classification and object detection",
      "text" : "Image classification is one of the most classical and fundamental applications in computer vision area, where each image has a (some) label(s), and the model needs to learn how to predict the label given a new input image. Now the most popular and successful model for image classification task is the CNN. While as mentioned above, the CNN takes one fixed size vector as input, which has some disadvantages:\n• When the input image is larger than the input size accepted by the CNN, either the image needs to be rescaled to a smaller size to meet the requirements of the model, or the image needs to be cut into some subparts to be processed one by one. If the image\nis rescaled smaller, there are some sacrifices on the details of the image, which may harm the classification performance. On the other hand, if the image is cut into some patches, the amount of computation will scale linearly with the size of the image.\n• The CNN has a degree of spatial transformation invariance build-in, while when the image is noisy or the object indicated by the label only occupies a small region of the input image, a CNN model may not still keep satisfied performance.\nAs a result, [39] and [4] propose the RNN based image classification models with the location-wise hard attention mechanism. As explained above, the location-wise hard attention based model stochastically picks a sub-region from the input image to generate the intermediate code. So the models in [39] and [4] can not only make prediction of the image label, but also localize the position of the object. This means the attention based RNN model integrates the image classification and object detection into a single end-to-end trainable model, which is another advantage compared to the CNN based object detection model. If one wants to apply a convolution network to solve the object detection problem, he has to use a separate model to propose the potential locations of the objects, like [49], which is expensive.\nIn this section, we will briefly introduce and analyze the models proposed in [39], [4], and their extensions. A brief structure of the RNN model in [39] is shown in Figure 12. In this model, enc is the encoder neural network taking a patch x of the input image to generate the intermediate code, where x is cut from the raw image based on a location value l generated by the attention network. In detail, l is generated by Equation (18), and x is extracted by Equation (19). Then the decoder accepts the intermediate code as input to make the potential prediction. Here in Figure 12 the decoder and the attention network are put in the same network r(1).\nThe experiments in [39] compare the performances of the proposed model with some nonrecurrent neural networks, and the results show the superiority of the attention based RNN model to the non-recurrent neural networks with similar number of parameters, especially on the datasets with noise. For the details of the experiments, one can refer to Section 4.1 in [39].\nHowever, the original model shown in Figure 12 is very simple, where enc is a two-layer neural network, and r(1) is a three-layer neural network. Besides, the first glimpse (l1 in Figure 12) is assigned manually. And all the experiments are only conducted on some toy datasets. There is no evidence to prove the generalization power of the model in Figure 12 to some real-world problems. [4] proposes an extended version of model as shown in Figure 13 by firstly making the network deeper, and secondly using a context vector to obtain a better first glimpse.\nThe encoder (enc) in [4] is deeper, i.e., it consists of three convolutional layers and one fully connected layer. Another big difference between the model in Figure 13 and the model in Figure 12 is that the model in Figure 13 adds an independent layer r(2) as the attention network in order to make the first glimpse as accurate as possible, i.e., the model extracts a context vector from the whole image (Icoarse in the figure), and feeds it to the attention model to generate the first potential location l1. The same context vector of the whole image is not fed into the decoder network r(1), because [4] observes that if so, the predicted\nlabel are highly influenced by the information of the whole image. Besides, both r(1) and r(2) are recurrent, while in Figure 12, only one hidden state exists.\nBoth two models introduced above are evaluated on a dataset called ”MNIST pairs” which is generated from the MNIST dataset. MNIST [29] is a handwritten digits dataset, which contains 70000 28×28 binary images (Figure 14a). The MNIST pairs dataset randomly puts two digits into a 100×100 image with some additional noise in the background [39] (Figure 14b). The results are shown in Table 1. It is clear that the performance of [4] is better, and the context vector indeed makes some improvements by suggesting a more accurate first glimpse.\nThe model in [4] are also tested on multi-digit street view house number (SVHN) [41] sequence recognition task, and the results show that with the same level of error rates, the number of parameters to be learned, and the training time of the attention based RNN model are much less than the state-of-the-art CNN methods.\nHowever nowadays both MNIST and SVHN are toy datasets with constraints environments, i.e., both of them are about digits classification and detection. Besides, their sizes are relatively small compared to other popular image classification/detection datasets, like the ImageNet. [45] further extends the model in [4] with only a few modifications and applies it to a real-world image classification task: Stanford Dogs fine-grained categorization task [27], in which the images have larger clutter, occlusion, and variations in pose. The biggest change made in [45] is that a part of the GoogLeNet [48] is used as the encoder, which is a pre-trained CNN on ImageNet dataset. The performance shown in Table 2 indicates that the attention based RNN model indeed can be applied to real-world datasets while still keep a competitive performance."
    }, {
      "heading" : "3.3 Image caption",
      "text" : "Image caption is a very challenging problem: by given an input image, the system needs to generate a natural language sentence to describe the content of the image. The classical way of solving this problem is by dividing the problem into some sub-problems, like object detection, objects-words alignment, sentence generation by the template, etc., and solving them independently. This process will obviously make some sacrifices on the performance. With the development and success of the recurrent network in machine translation area, the image caption problem can also be treated as a machine translation problem, i.e., the image can also be seen as a language, and the system just translate it to another language (natural language, like English). The RNN based image caption system is recently proposed in [51] (Neural Image Caption, NIC) which perfectly fits our encoder-decoder RNN framework without the attention mechanism.\nThe general structure of the NIC is shown in Figure 15. With an input image, the same trick mentioned above is applied: a pre-trained convolutional network is used as the encoder, and the output of a fully-connected layer is the intermediate code. Then it is followed by a recurrent network serving as the decoder to generate the natural language caption, and in NIC, the LSTM unit is used. Compared to the pervious methods which decompose the image caption problem into some sub-problems, the NIC is end-to-end, and is directly trained on the raw data, so the whole system is much simpler and can keep all information of the input data. The experimental results show the superiority of the NIC compared to the traditional methods.\nHowever, one problem for the NIC is that only a single image representation (the intermediate code) is obtained by the encoder from the whole input image, which is counterintuitive, i.e., when human beings describe an image by natural language, usually some objects or some salient areas are focused on one by one. So it is natural to import the ability of ”focusing” by applying the attention based RNN model.\n[57] adds the item-wise attention mechanisms into the NIC system. The encoder is still a pre-trained CNN, while instead of the fully-connected layer, the output of a convolutional layer is used to calculate the code set C in Equation (9). In detail, at each decoding step the whole image is fed into the pre-trained CNN, and the output of the CNN is a 2D feature map. Then some pre-defined 14×14 sub-regions of the feature map are extracted as the items in C. [57] implements both the item-wise soft and the item-wise hard attention\nmechanisms. For the item-wise soft attention, at each decoding step a weight for each code in C is calculated by Equation (12) and Equation (13), and then an expectation of all codes in C (Equation (14)) is used as the intermediate code. For the item-wise hard attention, the same weights for all codes in C are calculated and the index of the code to be picked is generated by Equation (15).\nBy using the attention mechanism, [57] reports improvements compared to the common RNN models without attention mechanism including NIC. However, both NIC and [57] have participated in the MS COCO Captioning Challenge 20155, and the competition gives opposite results. MS COCO Captioning Challenge 2015 is an image caption competition running on one of the biggest image caption datasets: Microsoft COCO [32]. Microsoft COCO contains 164k images in total where each image is labeled by at least 5 natural language captions. The performance is evaluated by human beings as well as some commonly used evaluation metrics (BLEU [42], Meteor [14], ROUGE-L [31] and CIDEr-D [50]). There\n5. MS COCO Captioning Challenge 2015, http://mscoco.org/dataset/#captions-challenge2015\nare 17 teams participating the competition in total, and here we list the rankings of some top teams in Table 3 including [57].\nExcept for the team ”Human” and the system proposed by [57], all other teams shown in Table 3 use models without the attention mechanism. We can notice that the attention based model performs worse than NIC on both the human evaluations and the automatic evaluation metrics, which suggests that there are still many spaces for improvements. In addition, by comparing the human and the automatic evaluation metrics, the attention based RNN model performs worse in the automatic evaluation metrics than in the human evaluations. This may suggest that, on one side, the current automatic evaluation metrics are still not perfect, and on the other side, the attention based RNN model can generate captions which are more ”natural” for human beings.\nIn addition to the performance, another big advantage of the attention based RNN model is that it makes the visualization easier and much more intuitive as mentioned in Section 1.5. Figure 16 gives a visual example showing where the model attends in the decoding process. It is clear the attention mechanism indeed works, for example, when generating the word ”people”, the model focuses on the people, and when generating ”water”, the lake is attended.\nThe model in [57] uses fixed-size feature maps (14×14) to construct the code set C, and each feature map corresponds to a fixed-size patch of the input image. [26] claims that this design may harm the performance because some ”meaningful scenes” may only occupy a small part of the image patch or cannot be cover by a single patch, where the meaningful scene indicates the objects/scenes correspond to the word is about to be predicted. [26] makes an improvement by firstly extract many object proposals [49] from an input image, which potentially contain the meaningful scenes, as the input sequence. And then each proposal is used to calculate the code in C. However, [26] neither directly compares its performance to [57], nor attends the MS COCO Captioning Challenge 2015. So here we cannot directly measure if and how the performance will be improved by using the object proposals to construct the input sequence. When analyzing in theory, we doubt if the improvements can really be obtained, because the currently popular object proposal methods only extract regions which probably contain some ”objects”. But a verb or an adjective in the ground truth image caption may not correspond to any explicit objects. Still, the problem proposed in [26] is very interesting and cannot be ignored: when the input of a RNN model is an\nimage, it is not trivial to obtain a sequence of items from it. One straightforward solution is to use the location-wise attention models like the location-wise hard attention or the STN."
    }, {
      "heading" : "4. Conclusion and future research",
      "text" : "In this survey, we discuss the attention based RNN model and its applications in some computer vision tasks. Attention is an intuitive methodology by giving different weights to different parts of the input which is widely used in the computer vision area. Recently with the development of the neural network, the attention mechanism is also embedded to the RNN model to further enhance its ability to solve sequence to sequence problem. We illustrate how the common RNN works, and gives detailed descriptions of four different attention mechanisms, and their pros and cons are also analyzed.\n• The item-wise attention based model requires that the input sequence contains explicit items. For some types of the input, like an image input, an additional step is needed to extract items from the input. Besides, the item-wise attention based RNN model needs to feed all items in the input sequence to the encoder, so on one side, it results in a slow training process, and on the other side, the testing efficiency is also not improved compared to the RNN without the attention module.\n• The location-wise attention based model allows the input to be an entire feature map. At each time, the location-wise attention based model only focuses on a part of the input feature map, which gives efficiency improvements in both training and testing compared to the item-wise attention model.\n• The soft attention module is differentiable with respect to its inputs, so the standard gradient ascent/decent can be used for optimization.\n• The hard attention module is non-differentiable, and the objective is optimized by some techniques from the reinforcement learning.\nAt last, some applications in computer vision which apply the attention based RNN models are introduced. The first application is image classification and object detection which applies the location-wise hard attention mechanism. The second one is the image caption which uses both the item-wise soft and hard attention. The experimental results demonstrate that\n• Considering the performance, the attention based RNN model is better than, or at least comparable to, the model without the attention mechanism.\n• The attention based RNN model makes the visualization more intuitive.\nThe location-wise soft attention model, i.e., the STN, is not introduced in the applications, because it is firstly proposed on the CNN and currently there are not enough applications of the STN based RNN model.\nSince the attention based RNN model is a new topic, it has not been well addressed yet. As a result, there are many problems which either need theoretical analyses or practical solutions. Here we list a few of them:\n• A large majority of the current attention based RNN models are only applied on some small datasets. How to use the attention based RNN model to larger datasets, like the ImageNet dataset?\n• Currently, most of the experiments are performed to compare the RNN models with/without the attention mechanism. More comprehensive comparisons of different attention mechanisms for different tasks are needed.\n• In many applications the input sequences contain explicit items naturally, which makes them fit the item-wise attention model. For example, most of the applications in NLP treat the natural sentence as the input, where each item is a word in the sentence. Is it possible to convert these types of inputs to a feature map and apply the location-wise attention model?\n• The current hard attention mechanism uses techniques from the reinforcement learning in optimization by maximizing an approximation of the sum of log-likelihood, which makes a sacrifice on the performance. Can the learning method be improved? [5] gives some insights but it is still an open question.\n• How to extend the four attention mechanisms introduced in this survey? For example, an intuitive way of extension is to put multiple attention modules in parallel instead of only embedding one into the RNN model. Another direction is to further generalize an RNN and make it have a similar structure as the modern computer architecture [54, 20], e.g. a multiple cache system shown in Figure 1, where the attention module serves as a ”scheduler” or an addressing module.\nIn summary, the attention based RNN model is a newly proposed model, and there are still many interesting questions remaining to be answered."
    } ],
    "references" : [ {
      "title" : "Salient region detection and segmentation",
      "author" : [ "Radhakrishna Achanta", "Francisco Estrada", "Patricia Wils", "Sabine Süsstrunk" ],
      "venue" : "In Computer Vision Systems,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2008
    }, {
      "title" : "Cognitive psychology and its implications",
      "author" : [ "John R Anderson" ],
      "venue" : "WH Freeman/Times Books/Henry Holt & Co,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1990
    }, {
      "title" : "Multiple object recognition with visual attention",
      "author" : [ "Jimmy Ba", "Volodymyr Mnih", "Koray Kavukcuoglu" ],
      "venue" : "In Proceedings of the International Conference on Learning Representations (ICLR),",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2015
    }, {
      "title" : "Learning wake-sleep recurrent attention models",
      "author" : [ "Jimmy Ba", "Ruslan R Salakhutdinov", "Roger B Grosse", "Brendan J Frey" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2015
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1409.0473,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2014
    }, {
      "title" : "Symbiotic segmentation and part localization for fine-grained categorization",
      "author" : [ "Yuning Chai", "Victor S. Lempitsky", "Andrew Zisserman" ],
      "venue" : "In IEEE International Conference on Computer Vision,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2013
    }, {
      "title" : "Listen, attend and spell",
      "author" : [ "William Chan", "Navdeep Jaitly", "Quoc V Le", "Oriol Vinyals" ],
      "venue" : "arXiv preprint arXiv:1508.01211,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2015
    }, {
      "title" : "On the properties of neural machine translation: Encoder–decoder approaches. Syntax, Semantics and Structure in Statistical Translation, page",
      "author" : [ "Kyunghyun Cho", "Bart van Merriënboer", "Dzmitry Bahdanau", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2014
    }, {
      "title" : "Describing multimedia content using attention-based encoder-decoder networks",
      "author" : [ "Kyunghyun Cho", "Aaron Courville", "Yoshua Bengio" ],
      "venue" : "Multimedia, IEEE Transactions on,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2015
    }, {
      "title" : "Histograms of oriented gradients for human detection",
      "author" : [ "Navneet Dalal", "Bill Triggs" ],
      "venue" : "In Computer Vision and Pattern Recognition,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2005
    }, {
      "title" : "Imagenet: A large-scale hierarchical image database",
      "author" : [ "Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Fei-Fei Li" ],
      "venue" : "IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2009
    }, {
      "title" : "Learning where to attend with deep architectures for image tracking",
      "author" : [ "Misha Denil", "Loris Bazzani", "Hugo Larochelle", "Nando de Freitas" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2012
    }, {
      "title" : "Meteor universal: Language specific translation evaluation for any target language",
      "author" : [ "Michael Denkowski", "Alon Lavie" ],
      "venue" : "In Proceedings of the EACL 2014 Workshop on Statistical Machine Translation,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2014
    }, {
      "title" : "Language models for image captioning: The quirks and what works",
      "author" : [ "Jacob Devlin", "Hao Cheng", "Hao Fang", "Saurabh Gupta", "Li Deng", "Xiaodong He", "Geoffrey Zweig", "Margaret Mitchell" ],
      "venue" : "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2015
    }, {
      "title" : "Long-term recurrent convolutional networks for visual recognition and description",
      "author" : [ "Jeff Donahue", "Lisa Anne Hendricks", "Sergio Guadarrama", "Marcus Rohrbach", "Subhashini Venugopalan", "Trevor Darrell", "Kate Saenko" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2015
    }, {
      "title" : "From captions to visual concepts and back",
      "author" : [ "Hao Fang", "Saurabh Gupta", "Forrest N. Iandola", "Rupesh K. Srivastava", "Li Deng", "Piotr Dollár", "Jianfeng Gao", "Xiaodong He", "Margaret Mitchell", "John C. Platt", "C. Lawrence Zitnick", "Geoffrey Zweig" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2015
    }, {
      "title" : "Fine-grained categorization by alignments",
      "author" : [ "Efstratios Gavves", "Basura Fernando", "Cees GM Snoek", "Arnold WM Smeulders", "Tinne Tuytelaars" ],
      "venue" : "In Computer Vision (ICCV),",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2013
    }, {
      "title" : "An active search strategy for efficient object class detection",
      "author" : [ "Abel Gonzalez-Garcia", "Alexander Vezhnevets", "Vittorio Ferrari" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2015
    }, {
      "title" : "Neural turing machines",
      "author" : [ "Alex Graves", "Greg Wayne", "Ivo Danihelka" ],
      "venue" : "arXiv preprint arXiv:1410.5401,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2014
    }, {
      "title" : "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "arXiv preprint arXiv:1502.01852,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2015
    }, {
      "title" : "Teaching machines to read and comprehend",
      "author" : [ "Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2015
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1997
    }, {
      "title" : "A model of saliency-based visual attention for rapid scene analysis",
      "author" : [ "Laurent Itti", "Christof Koch", "Ernst Niebur" ],
      "venue" : "IEEE Transactions on Pattern Analysis & Machine Intelligence,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 1998
    }, {
      "title" : "Spatial transformer networks",
      "author" : [ "Max Jaderberg", "Karen Simonyan", "Andrew Zisserman" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2015
    }, {
      "title" : "Aligning where to see and what to tell: image caption with region-based attention and scene factorization",
      "author" : [ "Junqi Jin", "Kun Fu", "Runpeng Cui", "Fei Sha", "Changshui Zhang" ],
      "venue" : "arXiv preprint arXiv:1506.06272,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2015
    }, {
      "title" : "Novel dataset for fine-grained image categorization: Stanford dogs",
      "author" : [ "Aditya Khosla", "Nityananda Jayadevaprakash", "Bangpeng Yao", "Fei-Fei Li" ],
      "venue" : "In Proc. CVPR Workshop on Fine-Grained Visual Categorization (FGVC),",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2011
    }, {
      "title" : "Learning to combine foveal glimpses with a third-order boltzmann machine",
      "author" : [ "Hugo Larochelle", "Geoffrey E Hinton" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2010
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Yann LeCun", "Léon Bottou", "Yoshua Bengio", "Patrick Haffner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 1998
    }, {
      "title" : "Rouge: A package for automatic evaluation of summaries. In Text summarization branches out",
      "author" : [ "Chin-Yew Lin" ],
      "venue" : "Proceedings of the ACL-04 workshop,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2004
    }, {
      "title" : "Microsoft COCO: common objects in context",
      "author" : [ "Tsung-Yi Lin", "Michael Maire", "Serge J. Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Dollár", "C. Lawrence Zitnick" ],
      "venue" : "In Computer Vision - ECCV 2014 - 13th European Conference,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2014
    }, {
      "title" : "Two/too simple adaptations of word2vec for syntax problems",
      "author" : [ "Wang Ling", "Chris Dyer", "Alan W. Black", "Isabel Trancoso" ],
      "venue" : "In NAACL HLT",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2015
    }, {
      "title" : "Not all contexts are created equal: Better word representations with variable attention",
      "author" : [ "Wang Ling", "Yulia Tsvetkov", "Silvio Amir", "Ramon Fermandez", "Chris Dyer", "Alan W. Black", "Isabel Trancoso", "Chu-Cheng Lin" ],
      "venue" : "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2015
    }, {
      "title" : "Learning to detect a salient object",
      "author" : [ "Tie Liu", "Zejian Yuan", "Jian Sun", "Jingdong Wang", "Nanning Zheng", "Xiaoou Tang", "Heung-Yeung Shum" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2011
    }, {
      "title" : "Listen, attend, and walk: Neural mapping of navigational instructions to action sequences",
      "author" : [ "Hongyuan Mei", "Mohit Bansal", "Matthew R. Walter" ],
      "venue" : "In Proceedings of AAAI,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2016
    }, {
      "title" : "Encoding source language with convolutional neural network for machine translation",
      "author" : [ "Fandong Meng", "Zhengdong Lu", "Mingxuan Wang", "Hang Li", "Wenbin Jiang", "Qun Liu" ],
      "venue" : "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2015
    }, {
      "title" : "Playing atari with deep reinforcement learning",
      "author" : [ "Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin Riedmiller" ],
      "venue" : "In NIPS Deep Learning Workshop",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2013
    }, {
      "title" : "Recurrent models of visual attention",
      "author" : [ "Volodymyr Mnih", "Nicolas Heess", "Alex Graves", "Koray Kavukcuoglu" ],
      "venue" : "In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2014
    }, {
      "title" : "Rectified linear units improve restricted boltzmann machines",
      "author" : [ "Vinod Nair", "Geoffrey E. Hinton" ],
      "venue" : "In Proceedings of the 27th International Conference on Machine Learning",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2010
    }, {
      "title" : "Reading digits in natural images with unsupervised feature learning",
      "author" : [ "Yuval Netzer", "Tao Wang", "Adam Coates", "Alessandro Bissacco", "Bo Wu", "Andrew Y Ng" ],
      "venue" : "In NIPS workshop on deep learning and unsupervised feature learning,",
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 2011
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu" ],
      "venue" : "In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 2002
    }, {
      "title" : "A neural attention model for abstractive sentence summarization",
      "author" : [ "Alexander M. Rush", "Sumit Chopra", "Jason Weston" ],
      "venue" : "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 2015
    }, {
      "title" : "Deep learning in neural networks: An overview",
      "author" : [ "Jürgen Schmidhuber" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 2015
    }, {
      "title" : "Attention for fine-grained categorization",
      "author" : [ "Pierre Sermanet", "Andrea Frome", "Esteban Real" ],
      "venue" : "arXiv preprint arXiv:1412.7054,",
      "citeRegEx" : "45",
      "shortCiteRegEx" : "45",
      "year" : 2014
    }, {
      "title" : "Convolutional LSTM networks for subcellular localization of proteins",
      "author" : [ "Søren Kaae Sønderby", "Casper Kaae Sønderby", "Henrik Nielsen", "Ole Winther" ],
      "venue" : "In Algorithms for Computational Biology - Second International Conference, AlCoB 2015, Mexico City,",
      "citeRegEx" : "46",
      "shortCiteRegEx" : "46",
      "year" : 2015
    }, {
      "title" : "Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le" ],
      "venue" : "Annual Conference on Neural Information Processing Systems",
      "citeRegEx" : "47",
      "shortCiteRegEx" : "47",
      "year" : 2014
    }, {
      "title" : "Going deeper with convolutions",
      "author" : [ "Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "48",
      "shortCiteRegEx" : "48",
      "year" : 2015
    }, {
      "title" : "Selective search for object recognition",
      "author" : [ "Jasper R.R. Uijlings", "Koen E.A. van de Sande", "Theo Gevers", "Arnold W.M. Smeulders" ],
      "venue" : "International Journal of Computer Vision,",
      "citeRegEx" : "49",
      "shortCiteRegEx" : "49",
      "year" : 2013
    }, {
      "title" : "Cider: Consensusbased image description evaluation",
      "author" : [ "Ramakrishna Vedantam", "C. Lawrence Zitnick", "Devi Parikh" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "50",
      "shortCiteRegEx" : "50",
      "year" : 2015
    }, {
      "title" : "Show and tell: A neural image caption generator",
      "author" : [ "Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "51",
      "shortCiteRegEx" : "51",
      "year" : 2015
    }, {
      "title" : "Rapid object detection using a boosted cascade of simple features",
      "author" : [ "Paul Viola", "Michael Jones" ],
      "venue" : "In Computer Vision and Pattern Recognition,",
      "citeRegEx" : "52",
      "shortCiteRegEx" : "52",
      "year" : 2001
    }, {
      "title" : "The optimal reward baseline for gradientbased reinforcement learning",
      "author" : [ "Lex Weaver", "Nigel Tao" ],
      "venue" : "Proceedings of the 17th Conference in Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "53",
      "shortCiteRegEx" : "53",
      "year" : 2001
    }, {
      "title" : "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
      "author" : [ "Ronald J. Williams" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "56",
      "shortCiteRegEx" : "56",
      "year" : 1992
    }, {
      "title" : "Show, attend and tell: Neural image caption generation with visual attention",
      "author" : [ "Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C. Courville", "Ruslan Salakhutdinov", "Richard S. Zemel", "Yoshua Bengio" ],
      "venue" : "In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015,",
      "citeRegEx" : "57",
      "shortCiteRegEx" : "57",
      "year" : 2015
    }, {
      "title" : "Unsupervised template learning for fine-grained object recognition",
      "author" : [ "Shulin Yang", "Liefeng Bo", "Jue Wang", "Linda G. Shapiro" ],
      "venue" : "In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems",
      "citeRegEx" : "58",
      "shortCiteRegEx" : "58",
      "year" : 2012
    }, {
      "title" : "Attentionnet: Aggregating weak directions for accurate object detection",
      "author" : [ "Donggeun Yoo", "Sunggyun Park", "Joon-Young Lee", "Anthony S Paek", "In So Kweon" ],
      "venue" : "In Proceedings of the IEEE International Conference on Computer Vision,",
      "citeRegEx" : "59",
      "shortCiteRegEx" : "59",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "The above mechanism is usually called attention [3].",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 0,
      "context" : "[2, 24, 35].",
      "startOffset" : 0,
      "endOffset" : 11
    }, {
      "referenceID" : 22,
      "context" : "[2, 24, 35].",
      "startOffset" : 0,
      "endOffset" : 11
    }, {
      "referenceID" : 32,
      "context" : "[2, 24, 35].",
      "startOffset" : 0,
      "endOffset" : 11
    }, {
      "referenceID" : 9,
      "context" : "The sliding window paradigm [11, 52] is another model which matches the essence of attention.",
      "startOffset" : 28,
      "endOffset" : 36
    }, {
      "referenceID" : 49,
      "context" : "The sliding window paradigm [11, 52] is another model which matches the essence of attention.",
      "startOffset" : 28,
      "endOffset" : 36
    }, {
      "referenceID" : 17,
      "context" : "For example, the local image features, saliency detection, sliding window methods introduced above, or some recently proposed object detection methods like [19], all employ the attention mechanism in different mathematical forms.",
      "startOffset" : 156,
      "endOffset" : 160
    }, {
      "referenceID" : 8,
      "context" : "So it is natural to divide the whole system into two parts: the encoder, and the decoder [10, 47].",
      "startOffset" : 89,
      "endOffset" : 97
    }, {
      "referenceID" : 44,
      "context" : "So it is natural to divide the whole system into two parts: the encoder, and the decoder [10, 47].",
      "startOffset" : 89,
      "endOffset" : 97
    }, {
      "referenceID" : 4,
      "context" : ", machine translation [6, 37], machine comprehension [22], sentence summarization [43], word representation [33, 34].",
      "startOffset" : 22,
      "endOffset" : 29
    }, {
      "referenceID" : 34,
      "context" : ", machine translation [6, 37], machine comprehension [22], sentence summarization [43], word representation [33, 34].",
      "startOffset" : 22,
      "endOffset" : 29
    }, {
      "referenceID" : 20,
      "context" : ", machine translation [6, 37], machine comprehension [22], sentence summarization [43], word representation [33, 34].",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 40,
      "context" : ", machine translation [6, 37], machine comprehension [22], sentence summarization [43], word representation [33, 34].",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 30,
      "context" : ", machine translation [6, 37], machine comprehension [22], sentence summarization [43], word representation [33, 34].",
      "startOffset" : 108,
      "endOffset" : 116
    }, {
      "referenceID" : 31,
      "context" : ", machine translation [6, 37], machine comprehension [22], sentence summarization [43], word representation [33, 34].",
      "startOffset" : 108,
      "endOffset" : 116
    }, {
      "referenceID" : 43,
      "context" : "– Bioinformatics [46].",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 6,
      "context" : "– Speech recognition [8, 36].",
      "startOffset" : 21,
      "endOffset" : 28
    }, {
      "referenceID" : 33,
      "context" : "– Speech recognition [8, 36].",
      "startOffset" : 21,
      "endOffset" : 28
    }, {
      "referenceID" : 35,
      "context" : "– Game play [38].",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 37,
      "context" : ", sigmoid, tanh, ReLU [40], PReLU [21].",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 19,
      "context" : ", sigmoid, tanh, ReLU [40], PReLU [21].",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 41,
      "context" : "For more details about the neural network, one can refer to [44].",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 27,
      "context" : "The convolutional neural network (CNN) is a specific type of neural network with structures designed for image inputs [30], which usually consists of multiple convolutional layers followed by a few fully-connected layers.",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 10,
      "context" : ", ImageNet [12]) has good generalization power/ability and can be easily transferred to other tasks.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 41,
      "context" : "For more details about CNN, one can refer to [44].",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 21,
      "context" : "Now two widely used structures are LSTM (long-short term memory) [23] and GRU (gated recurrent units) [9].",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 7,
      "context" : "Now two widely used structures are LSTM (long-short term memory) [23] and GRU (gated recurrent units) [9].",
      "startOffset" : 102,
      "endOffset" : 105
    }, {
      "referenceID" : 4,
      "context" : "Here we only introduce the first proposed item-wise soft attention model in [6] for natural language processing.",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 11,
      "context" : "This kind of location-wise hard attention is analyzed in many previous works [13, 28], while here we only",
      "startOffset" : 77,
      "endOffset" : 85
    }, {
      "referenceID" : 26,
      "context" : "This kind of location-wise hard attention is analyzed in many previous works [13, 28], while here we only",
      "startOffset" : 77,
      "endOffset" : 85
    }, {
      "referenceID" : 36,
      "context" : "focus on two recent proposed mechanisms in [39] and [4], because the attention models in those works are more general and can be trained end-to-end.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 2,
      "context" : "focus on two recent proposed mechanisms in [39] and [4], because the attention models in those works are more general and can be trained end-to-end.",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 54,
      "context" : "One can also let the attention network generate these parameters as the generation of lm j in Equation (18) [59].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 23,
      "context" : "It is firstly proposed in [25] called the spatial transformation network (STN).",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 23,
      "context" : "Instead of the RNN model, the STN in [25] is initially applied on a CNN model, but it can be easily transferred to a RNN model with tiny modifications.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 23,
      "context" : "The figure is taken from [25].",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 23,
      "context" : "Since the original STN in [25] is applied on a CNN, there is no hidden state.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 23,
      "context" : "As a result, in [25] the location network only takes Xin as input, and Equation (25) changes to Aj = φWloc(Xin).",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 36,
      "context" : "In general, this is a problem of training a neural network with discrete values/units, and here we just introduced the method applied in [39, 4, 57] which employs the techniques from the reinforcement learning.",
      "startOffset" : 137,
      "endOffset" : 148
    }, {
      "referenceID" : 2,
      "context" : "In general, this is a problem of training a neural network with discrete values/units, and here we just introduced the method applied in [39, 4, 57] which employs the techniques from the reinforcement learning.",
      "startOffset" : 137,
      "endOffset" : 148
    }, {
      "referenceID" : 52,
      "context" : "In general, this is a problem of training a neural network with discrete values/units, and here we just introduced the method applied in [39, 4, 57] which employs the techniques from the reinforcement learning.",
      "startOffset" : 137,
      "endOffset" : 148
    }, {
      "referenceID" : 51,
      "context" : "As shown above, lj is generated from a distribution (Equation (15), Equation (17), or Equation (18)), which indicates that p(lj |xl) and ∂ log p(lj |x) ∂θ can be estimated by a Monte Carlo sampling as demonstrated in [56]: ∂Lj ∂θ ≈ 1 M M ∑",
      "startOffset" : 217,
      "endOffset" : 221
    }, {
      "referenceID" : 51,
      "context" : "The whole learning process described above for the hard attention is equivalent to the REINFORCE learning rule in [56], and from the reinforcement learning perspective, after each step, the model can get a reward from the environment.",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 36,
      "context" : "Now as shown in [39], although Equation (47) is an unbiased estimation of the real gradient of Equation (44), it may have high variance because of the unbounded Rj .",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 2,
      "context" : "where b can be calculated in different ways which are discussed in [4, 5, 39, 53, 57].",
      "startOffset" : 67,
      "endOffset" : 85
    }, {
      "referenceID" : 3,
      "context" : "where b can be calculated in different ways which are discussed in [4, 5, 39, 53, 57].",
      "startOffset" : 67,
      "endOffset" : 85
    }, {
      "referenceID" : 36,
      "context" : "where b can be calculated in different ways which are discussed in [4, 5, 39, 53, 57].",
      "startOffset" : 67,
      "endOffset" : 85
    }, {
      "referenceID" : 50,
      "context" : "where b can be calculated in different ways which are discussed in [4, 5, 39, 53, 57].",
      "startOffset" : 67,
      "endOffset" : 85
    }, {
      "referenceID" : 52,
      "context" : "where b can be calculated in different ways which are discussed in [4, 5, 39, 53, 57].",
      "startOffset" : 67,
      "endOffset" : 85
    }, {
      "referenceID" : 46,
      "context" : "For the extraction method, one can either extract some fixed size patches from some pre-defined locations, or apply other object proposal methods, like [49].",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 36,
      "context" : "As a result, [39] and [4] propose the RNN based image classification models with the location-wise hard attention mechanism.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 2,
      "context" : "As a result, [39] and [4] propose the RNN based image classification models with the location-wise hard attention mechanism.",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 36,
      "context" : "So the models in [39] and [4] can not only make prediction of the image label, but also localize the position of the object.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 2,
      "context" : "So the models in [39] and [4] can not only make prediction of the image label, but also localize the position of the object.",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 46,
      "context" : "If one wants to apply a convolution network to solve the object detection problem, he has to use a separate model to propose the potential locations of the objects, like [49], which is expensive.",
      "startOffset" : 170,
      "endOffset" : 174
    }, {
      "referenceID" : 36,
      "context" : "In this section, we will briefly introduce and analyze the models proposed in [39], [4], and their extensions.",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 2,
      "context" : "In this section, we will briefly introduce and analyze the models proposed in [39], [4], and their extensions.",
      "startOffset" : 84,
      "endOffset" : 87
    }, {
      "referenceID" : 36,
      "context" : "A brief structure of the RNN model in [39] is shown in Figure 12.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 36,
      "context" : "The experiments in [39] compare the performances of the proposed model with some nonrecurrent neural networks, and the results show the superiority of the attention based RNN model to the non-recurrent neural networks with similar number of parameters, especially on the datasets with noise.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 36,
      "context" : "1 in [39].",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 2,
      "context" : "[4] proposes an extended version of model as shown in Figure 13 by firstly making the network deeper, and secondly using a context vector to obtain a better first glimpse.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "The encoder (enc) in [4] is deeper, i.",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 2,
      "context" : "The same context vector of the whole image is not fed into the decoder network r(1), because [4] observes that if so, the predicted",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 36,
      "context" : "Figure 12: The model proposed in [39].",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 2,
      "context" : "The figure is taken from [4], and some modifications are made to fit the model in [39].",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 36,
      "context" : "The figure is taken from [4], and some modifications are made to fit the model in [39].",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 36,
      "context" : "Model Test error [39] 9% [4] without the context vector 7% [4] 5%",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 2,
      "context" : "Model Test error [39] 9% [4] without the context vector 7% [4] 5%",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 2,
      "context" : "Model Test error [39] 9% [4] without the context vector 7% [4] 5%",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 36,
      "context" : "The MNIST pairs dataset randomly puts two digits into a 100×100 image with some additional noise in the background [39] (Figure 14b).",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 2,
      "context" : "It is clear that the performance of [4] is better, and the context vector indeed makes some improvements by suggesting a more accurate first glimpse.",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 2,
      "context" : "The model in [4] are also tested on multi-digit street view house number (SVHN) [41] sequence recognition task, and the results show that with the same level of error rates, the number of parameters to be learned, and the training time of the attention based RNN model are much less than the state-of-the-art CNN methods.",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 38,
      "context" : "The model in [4] are also tested on multi-digit street view house number (SVHN) [41] sequence recognition task, and the results show that with the same level of error rates, the number of parameters to be learned, and the training time of the attention based RNN model are much less than the state-of-the-art CNN methods.",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 2,
      "context" : "Figure 13: The model proposed in [4].",
      "startOffset" : 33,
      "endOffset" : 36
    }, {
      "referenceID" : 2,
      "context" : "The figure is taken from [4], and some modifications are made.",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 36,
      "context" : "(b) Two examples taken from the MNIST pairs dataset [39].",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 42,
      "context" : "[45] further extends the model in [4] with only a few modifications and applies it to a real-world image classification task: Stanford Dogs fine-grained categorization task [27], in which the images have larger clutter, occlusion, and variations in pose.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 2,
      "context" : "[45] further extends the model in [4] with only a few modifications and applies it to a real-world image classification task: Stanford Dogs fine-grained categorization task [27], in which the images have larger clutter, occlusion, and variations in pose.",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 25,
      "context" : "[45] further extends the model in [4] with only a few modifications and applies it to a real-world image classification task: Stanford Dogs fine-grained categorization task [27], in which the images have larger clutter, occlusion, and variations in pose.",
      "startOffset" : 173,
      "endOffset" : 177
    }, {
      "referenceID" : 42,
      "context" : "The biggest change made in [45] is that a part of the GoogLeNet [48] is used as the encoder, which is a pre-trained CNN on ImageNet dataset.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 45,
      "context" : "The biggest change made in [45] is that a part of the GoogLeNet [48] is used as the encoder, which is a pre-trained CNN on ImageNet dataset.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 5,
      "context" : "Model Mean accuracy (MA) [7] [58]* 0.",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 53,
      "context" : "Model Mean accuracy (MA) [7] [58]* 0.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 5,
      "context" : "38 [7]* 0.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 16,
      "context" : "46 [18]* 0.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 42,
      "context" : "50 GoogLeNet 96×96 (the encoder in [45]) 0.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 42,
      "context" : "42 RNN with location-wise hard attention [45] 0.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 48,
      "context" : "The RNN based image caption system is recently proposed in [51] (Neural Image Caption, NIC) which perfectly fits our encoder-decoder RNN framework without the attention mechanism.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 52,
      "context" : "[57] adds the item-wise attention mechanisms into the NIC system.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 52,
      "context" : "[57] implements both the item-wise soft and the item-wise hard attention",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 48,
      "context" : "The figure is taken from [51].",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 52,
      "context" : "By using the attention mechanism, [57] reports improvements compared to the common RNN models without attention mechanism including NIC.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 52,
      "context" : "However, both NIC and [57] have participated in the MS COCO Captioning Challenge 20155, and the competition gives opposite results.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 29,
      "context" : "MS COCO Captioning Challenge 2015 is an image caption competition running on one of the biggest image caption datasets: Microsoft COCO [32].",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 39,
      "context" : "The performance is evaluated by human beings as well as some commonly used evaluation metrics (BLEU [42], Meteor [14], ROUGE-L [31] and CIDEr-D [50]).",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 12,
      "context" : "The performance is evaluated by human beings as well as some commonly used evaluation metrics (BLEU [42], Meteor [14], ROUGE-L [31] and CIDEr-D [50]).",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 28,
      "context" : "The performance is evaluated by human beings as well as some commonly used evaluation metrics (BLEU [42], Meteor [14], ROUGE-L [31] and CIDEr-D [50]).",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 47,
      "context" : "The performance is evaluated by human beings as well as some commonly used evaluation metrics (BLEU [42], Meteor [14], ROUGE-L [31] and CIDEr-D [50]).",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 15,
      "context" : "Model Human Other evaluation metrics M1 M2 BLEU-1 BLEU-2 BLEU-3 BLEU-4 Meteor ROUGE-L CIDEr-D Human 1 1 6 12 12 13 3 11 6 Google NIC 2 3 2 2 2 2 1 1 1 MSR [17] 3 2 5 5 5 5 4 5 4 [57] 4 5 9 9 9 9 6 8 9 MSR Captivator [15] 5 4 1 1 1 1 2 2 2 Berkeley LRCN [16] 6 6 10 7 8 8 7 6 8",
      "startOffset" : 155,
      "endOffset" : 159
    }, {
      "referenceID" : 52,
      "context" : "Model Human Other evaluation metrics M1 M2 BLEU-1 BLEU-2 BLEU-3 BLEU-4 Meteor ROUGE-L CIDEr-D Human 1 1 6 12 12 13 3 11 6 Google NIC 2 3 2 2 2 2 1 1 1 MSR [17] 3 2 5 5 5 5 4 5 4 [57] 4 5 9 9 9 9 6 8 9 MSR Captivator [15] 5 4 1 1 1 1 2 2 2 Berkeley LRCN [16] 6 6 10 7 8 8 7 6 8",
      "startOffset" : 178,
      "endOffset" : 182
    }, {
      "referenceID" : 13,
      "context" : "Model Human Other evaluation metrics M1 M2 BLEU-1 BLEU-2 BLEU-3 BLEU-4 Meteor ROUGE-L CIDEr-D Human 1 1 6 12 12 13 3 11 6 Google NIC 2 3 2 2 2 2 1 1 1 MSR [17] 3 2 5 5 5 5 4 5 4 [57] 4 5 9 9 9 9 6 8 9 MSR Captivator [15] 5 4 1 1 1 1 2 2 2 Berkeley LRCN [16] 6 6 10 7 8 8 7 6 8",
      "startOffset" : 216,
      "endOffset" : 220
    }, {
      "referenceID" : 14,
      "context" : "Model Human Other evaluation metrics M1 M2 BLEU-1 BLEU-2 BLEU-3 BLEU-4 Meteor ROUGE-L CIDEr-D Human 1 1 6 12 12 13 3 11 6 Google NIC 2 3 2 2 2 2 1 1 1 MSR [17] 3 2 5 5 5 5 4 5 4 [57] 4 5 9 9 9 9 6 8 9 MSR Captivator [15] 5 4 1 1 1 1 2 2 2 Berkeley LRCN [16] 6 6 10 7 8 8 7 6 8",
      "startOffset" : 253,
      "endOffset" : 257
    }, {
      "referenceID" : 52,
      "context" : "are 17 teams participating the competition in total, and here we list the rankings of some top teams in Table 3 including [57].",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 52,
      "context" : "Except for the team ”Human” and the system proposed by [57], all other teams shown in Table 3 use models without the attention mechanism.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 52,
      "context" : "The model in [57] uses fixed-size feature maps (14×14) to construct the code set C, and each feature map corresponds to a fixed-size patch of the input image.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 24,
      "context" : "[26] claims that this design may harm the performance because some ”meaningful scenes” may only occupy a small part of the image patch or cannot be cover by a single patch, where the meaningful scene indicates the objects/scenes correspond to the word is about to be predicted.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "[26] makes an improvement by firstly extract many object proposals [49] from an input image, which potentially contain the meaningful scenes, as the input sequence.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 46,
      "context" : "[26] makes an improvement by firstly extract many object proposals [49] from an input image, which potentially contain the meaningful scenes, as the input sequence.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 24,
      "context" : "However, [26] neither directly compares its performance to [57], nor attends the MS COCO Captioning Challenge 2015.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 52,
      "context" : "However, [26] neither directly compares its performance to [57], nor attends the MS COCO Captioning Challenge 2015.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 24,
      "context" : "Still, the problem proposed in [26] is very interesting and cannot be ignored: when the input of a RNN model is an",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 52,
      "context" : "The figure is taken from [57].",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 3,
      "context" : "Can the learning method be improved? [5] gives some insights but it is still an open question.",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 18,
      "context" : "Another direction is to further generalize an RNN and make it have a similar structure as the modern computer architecture [54, 20], e.",
      "startOffset" : 123,
      "endOffset" : 131
    } ],
    "year" : 2016,
    "abstractText" : "3",
    "creator" : "LaTeX with hyperref package"
  }
}