{
  "name" : "1505.06292.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Low-Rank Matrix Recovery from Row-and-Column Affine Measurements",
    "authors" : [ "Avishai Wagner" ],
    "emails" : [ "avishai.wagner@mail.huji.ac.il", "or.zuk@mail.huji.ac.il" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 5.\n06 29\n2v 1\n[ cs\nWe propose and study a row-and-column affine measurement scheme for lowrank matrix recovery. Each measurement is a linear combination of elements in one row or one column of a matrix X . This setting arises naturally in applications from different domains. However, current algorithms developed for standard matrix recovery problems do not perform well in our case, hence the need for developing new algorithms and theory for our problem. We propose a simple algorithm for the problem based on Singular Value Decomposition (SV D) and least-squares (LS), which we term SVLS . We prove that (a simplified version of) our algorithm can recover X exactly with the minimum possible number of measurements in the noiseless case. In the general noisy case, we prove performance guarantees on the reconstruction accuracy under the Frobenius norm. In simulations, our row-andcolumn design and SVLS algorithm show improved speed, and comparable and in some cases better accuracy compared to standard measurements designs and algorithms. Our theoretical and experimental results suggest that the proposed rowand-column affine measurements scheme, together with our recovery algorithm, may provide a powerful framework for affine matrix reconstruction.\nKeywords: low-rank matrix recovery, row and column measurements, matrix completion, singular value decomposition"
    }, {
      "heading" : "1 Introduction",
      "text" : "In the low-rank affine matrix recovery problem, an unknown matrix X ∈ Rn1×n2 with rank(X) = r is measured indirectly via an affine transformation A : Rn1×n2 → Rd\nand possibly with additive (typically Gaussian) noise z ∈ Rd. Our goal is to recover X from the vector of noisy measurements b = A(X) + z. The problem has found numerous applications throughout science and engineering, in different fields such as collaborative filtering [19], face recognition [1], quantum state tomography [14] and computational biology [9]. The problem has been studied mathematically quite extensively in the last few years. Most attention thus far has been given to two particular ensembles of random transformations A: (i) the Matrix Completion (MC) setting, in which each element of A(X) is a single entry of the matrix where the subset of the observed measurements is sampled uniformly at random [4, 6, 8, 17, 18, 21] (ii) Gaussian-Ensemble (GE) affine-matrix-recovery, in which each element of A(X) is a weighted sum of all elements of X with i.i.d. Gaussian weights [5, 22]. Remarkably, although the recovery problem is in general NP-hard, when r ≪ min(n1, n2) and under certain conditions on the matrix X or the measurement operator A, one can recover X from d ≪ n1n2 measurements with high probability and using efficient algorithms [6, 8, 21, 22]. However, it is desirable to study the problem with other affine transformations A beyond the two ensembles mentioned above for the following reasons: (i) In some applications we cannot control the measurements operator A, and different models for the measurements may be needed to allow a realistic analysis of the problem (ii) When we can control and design the measurement operator A, other measurement operators may outperform the two ensembles mentioned above with respect to optimizing different resources such as the number of measurements required, computation time and storage. The main goal of this paper is to present and study a different set of affine transformations, which we term row-and-column affine measurements. This setting may arise naturally in many applications, since it is often natural and possibly cheap to measure a single row or column of a matrix, or a linear combination of a few such rows and columns. For example, (i) In collaborative filtering, we may wish to recover a users-items preference matrix and have access to only a subset of the users, but can observe their preference scores for all items (ii) When recovering a protein-RNA interactions matrix in molecular biology, a single experiment may simultaneously measure the interactions of a specific protein with all RNA molecules [10].\nIn general, we can represent any affine transformation A in matrix representation A(X) = Avec(X), where vec(X) is a column vector obtained by stacking all columns of X on top of each other. In our row and column framework the measurement operator A is represented differently using two matrices A(R), A(C) which multiply X as a matrix (rather than multiplying the vector vec(X)) from left and right, respectively. We focus on two ensembles of A(R), A(C): (i) Matrix Completion from single Columns and Rows (RCMC). Here we observe single matrix entries in similar to standard matrix\ncompletion case, however the measured entries are not scattered randomly along the matrix, but instead we sample a few rows and a few columns, and measure all entries in these rows and columns. This ensemble is implemented by setting the rows (columns) of A(R) (A(C)) as random vectors from the standard basis of Rn1 (Rn2). (ii) Gaussian Row-and-Column (GRC) measurements. Here each set of measurements is a weighted linear combination of the matrix’s rows (or columns) with the weights taken as i.i.d. Gaussians. This ensemble is implemented by setting the entries of A(R), A(C) as i.i.d. Gaussian random variables.\nThe measurement operators A in our RCMC and GRC models do not satisfy standard requirements which hold for GE and MC. It is thus not surprising that algorithms such as nuclear norm minimization [6, 22], which succeed for the GE and MC models, fail in our case, and different algorithms and theory are required. However, the specific algebraic structure provided by the row-and-column measurements, allows us to derive efficient and simple algorithms, and to analyze their performance. In addition, we provide extensive simulation results, which demonstrate the improved accuracy and speed of our approach over existing measurement designs and algorithms. All of our algorithms and simulations are implemented in a Matlab software package available at https://github.com/avishaiwa/SVLS."
    }, {
      "heading" : "1.1 Prior Work",
      "text" : "Before giving a detailed derivation and analysis of our design and algorithms, we give an overview of existing designs and their properties. We concentrate on two properties: (i) storage required in order to represent the measurement operator, and (ii) measurement sparsity, defined as the sum over all measurements of the number of matrix entries participating in each measurement, that is S(A) = ||vec(A)||0. The latter property may be related to measurement costs, as well as to computational time.\nIn the Gaussian Ensemble model, the entries of the matrix A in the matrix representationA(X) = Avec(X) are i.i.d. Gaussian random variables, Aij ∼ N(0, 1). For this ensemble, one can recover uniquely a low rank matrix X with O(r(n1+n2)) noiseless measurements using nuclear norm minimization [5, 22] or other methods such as Singular Value Projection (SVP) [16], which is optimal up to constants. Recovery in this model is robust to noise, with only a small increase in number of measurements. The main disadvantage of this model is that the design requires O(dn1n2) storage space for A, which could be problematic for large matrices. Another possible disadvantage of this method is that measurements are dense - each measurement represents a linear combination of all O(n1n2) matrix entries, and the overall measurement sparsity of A(X) is also O(dn1n2), which could be problematic for large n1, n2.\nIn the standard matrix completion problem [6] we can recover X with high probability from single entries chosen uniformly at random using nuclear norm minimization [2, 8, 20, 21, 26] or using other methods such as SVD and gradient descent [17, 18]. This model has the lowest storage requirements (O(d)) and measurement sparsity (O(d)). However, recovery guarantees for this model are weaker: setting n = max(n1, n2), it is shown that Θ(nrlog(n)) measurements are required to recover a rank r matrix of size n1 × n2 [8]. In addition, unique recovery from this number of measurements requires additional incoherence conditions on the matrix X , and recovery of matrices which fail to satisfy such conditions (e.g. matrices with a few spikes) may require a much larger number of measurements.\nRecently a new design of rank one projections was proposed [3], where each measurement is of the form αTXβ and such that α ∈ Rn1 , β ∈ Rn2 have i.i.d standard Gaussian entries. It was proven that nuclear norm minimization can recover X with high probability in this design from O(n1r + n2r) measurements. This is the first model deviating from MC and GE we are aware of. This model is different from our row-and-column model, as each measurement is obtained by multiplying X from both sides, whereas in our model each measurement is obtained by multiplying X from either left or right. Moreover, in our model the measurements are not chosen independently from each other but come in groups of size n1 or n2 (corresponding to rows or columns A(R), A(C)). An advantage of rank one projection is that it leads to a significance reduction in measurement storage needed for A with overall O(dn1 +dn2) storage space. However, each measurement is still dense and involve all matrix elements, hence measurement sparsity is O(dn1n2). In contrast, our GRC model requires only O(d) storage for A, and every measurement depends only on O(n) elements, leading to a reduced overall time for all measurements O(dn1 + dn2). For RCMC, we need only O(dlog(n)n ) storage for A, and measurement sparsity is O(d)."
    }, {
      "heading" : "2 Preliminaries and Notations",
      "text" : "We denote by Rn1×n2 the space of matrices of size n1 × n2, by On1×n2 the space of matrices of size n1 × n2 with orthonormal columns, and by M(r)n1×n2 the space of matrices of size n1 × n2 and rank 6 r. We denote n = max(n1, n2).\nWe denote by || · ||F the matrix Frobenius norm, by || · ||∗ the nuclear norm, and by || · ||2 the spectral norm. For a vector, || · || denotes the standard l2 norm.\nFor X ∈ Rn1×n2we denote by span(X) the subspace of Rn1 spanned by the columns of X and define PX to be the orthogonal projection into span(X).\nFor a matrix X we denote by Xi• the i-th row, by X•j the j-th column and by Xij the (i, j) element. For two sets of indices I, J , we denote by XIJ the sub-matrix\nobtained by taking the rows with indices in I and columns with indices in J of X . We denote by [k] the set of indices 1, .., k. We denote by vec(X) the (column) vector obtained by stacking all the columns of X on top of each other.\nWe use the notation X i.i.d.∼ G to denote a random matrix X with i.i.d. entries Xij ∼ G. For a rank-r matrix X ∈ M(r)n1×n2 let X = UΣV T be the Singular Value Decomposition (SVD) where U ∈ On1×r, V ∈ Or×n2 and Σ = diag(σ1(X), ..., σr(X)) with σ1(X) ≥ σ2(X) ≥ .. ≥ σr(X) > 0 the (non-zero) singular values of X (we omit the zero singular values and their corresponding vectors from the decomposition). For a general matrix X ∈ Rn1×n2 we denote by X(r) the top-r singular value decomposition of X , X(r) = U•[r]Σ[r][r]V T•[r].\nOur model assumes two affine transformations applied to X , representing rows and columns, B(C,0) = XA(C) and B(R,0) = A(R)X, achieved by multiplications with two matrices A(R) ∈ Rk(R)×n1 and A(C) ∈ Rn2×k(C) , respectively. We obtain noisy observations of these transformations, B(R), B(C) obtained by applying additive noise:\nA(R)X + Z(R) = B(R) ; XA(C) + Z(C) = B(C) (1)\nwhere the total number of measurements is d = k(R)n1 + n2k(C), and Z(R) ∈ Rn1×k(R) ,Z\n(C) ∈ Rk(C)×n2 are two zero-mean noise matrices. Our goal is to recover X from the observed measurements B(C) and B(R). To achieve this goal, we define the squared loss function\nF(X) = ||A(R)X −B(R)||2F + ||XA(C) −B(C)||2F (2)\nand solve the least squares problem:\nMinimizeF(X) s.t. X ∈ M(r)n1×n2 . (3)\nIf Z(R), Z(C) i.i.d.∼ N(0, τ2) , minimizing the loss function in eq. (2) is equivalent to maximizing the log-likelihood of the data, giving a statistical motivation for the above score. Problem (3) is non-convex due to the non-convex rank constraint rank(X) ≤ r. Our problem is a specialization of the general affine matrix recovery problem [22], in which a matrix is measured using a general affine transformation A with b = A(X) + z. We consider next and throughout the paper two specific random ensembles of measurement matrices:\n1. Row and Column Matrix Completion (RCMC): In this ensemble each row of A(R) and each column of A(C) is a vector of the standard basis ej for some j -\nthus each measurement B(R)ij or B (C) ij is obtained from a single entry of X . We define a row-inclusion probability p(R) and column inclusion probability p(C)\nsuch that each row (column) of X will be measured with probability p(R) (p(C)). More precisely, we define r1, .., rn1 i.i.d. Bernoulli variables, P (ri = 1) = p (R), and include ei as a row in A(R) if and only if ri = 1. Similarly, we define c1...cn2 i.i.d. Bernoulli variables, P (ci = 1) = p(C), and include ei as a column in A(C) if and only if ci = 1. The expected number of observed rows (columns) is k(R) = n1p (R) (k(C) = n2p(C)). The model is very similar to the possibly more natural model of picking k(R) distinct rows and k(C) distinct columns at random for fixed k(R), k(C), but allows for easier analysis.\n2. Gaussian Rows and Columns (GRC): In this ensembleA(R), A(C) i.i.d.∼ N(0, 1). Each observation B(R)ij or B (C) ij is obtained by a weighted sum of a single row\nor column of X , with i.i.d. Gaussian weights."
    }, {
      "heading" : "2.1 Comparison to Standard Designs",
      "text" : "Our proposed rows-and-columns design differs from standard designs appearing in the literature. It is instructive to compare our GRC ensemble to the Gaussian Ensemble (GE) [5], with the matrix representation A(X) = Avec(X) where A ∈ Rd×n1n2 and A\ni.i.d.∼ N(0, 1). For the latter, the following r-Restricted Isometry Property (RIP) can be used:\nDefinition 1. (r-RIP) Let A : Rn1×n2 → Rd be a linear map. For every integer r with 1 ≤ r ≤ min(n1, n2), define the r-Restricted Isometry Constant to be the smallest number ǫr such that\n(1− ǫr)||X ||F ≤ ||A(X)|| ≤ (1 + ǫr)||X ||F (4)\nholds for all matrices X of rank at most r.\nThe GE model satisfies the r-RIP condition for d = O(rn) with high probability [22]. Based on this property it is known that nuclear norm minimization [5, 22] and other algorithms such as SVP [16] can recover X with high probability. Unlike GE, in our GRC model A(X) doesn’t satisfy the r-RIP, and nuclear norm minimization fails. Instead, A(R), A(C) preserve matrix Frobenius norm in high probability - a weaker property which holds for any low-rank matrix. (see Lemma 7 in the Appendix).\nWe next compare RCMC to the standard Matrix Completion model [6], in which single entries are chosen at random to be observed. Unlike GE, for MC incoherence conditions on X are required in order to guarantee unique recovery of X [6] :\nDefinition 2. (Incoherence). Let U be a subspace of Rn of dimension r, and PU be the orthogonal projection on U . Then the coherence of U (with respect to the standard\nbasis {ei}) is defined as µ(U)≡n\nr maxi||PU (ei)||2. (5)\nWe say that a matrix X ∈ Rn1×n2 is µ-incoherent if for the SV D X = UΣV T we have max(µ(U), µ(V )) ≤ µ.\nWhen X is µ-incoherent, and when known entries are sampled uniformly at random from X , several algorithms [2, 16, 17] succeed to recover X with high probability. In particular, nuclear norm minimization has gained popularity as a solver for the standard MC problem because it provides recovery guarantees and a convenient representation as a convex optimization problem with availability of many iterative solvers for the problem. However, nuclear norm minimization fails for the RCMC design, even when the matrix X is incoherent, as shown by the next example:\nExample 1. Take X ∈ Rn×n for n3 ∈ N with Xij = 1, ∀(i, j) ∈ [n] × [n]. Thus ||X ||∗ = n. Take k(R) = k(C) = n3 . Set all unknown entries to 0.5, giving a matrix X0 of rank 2 with σ1(X0) = ( √ 2+1)n 3 , σ2(X0) = ( √ 2−1)n 3 . Therefore ||X0||∗ = n √ 2 3 < ||X ||∗ and nuclear norm minimization fails to recover the correct X .\nIn Section 3 we present our SVLS algorithm, which does not rely on nuclear-norm minimization. In Section 4 we show that SVLS successfully approximates X for the GRC ensemble."
    }, {
      "heading" : "3 Algorithms for Recovery of X",
      "text" : "In this section we give an efficient algorithm which we call SVLS (Singular Value Least Squares). SVLS is very easy to implement - for simplicity, we start with Algorithm 1 for the noiseless case and then present Algorithm 2 (SVLS) which is applicable for the general (noisy) case."
    }, {
      "heading" : "3.1 Noiseless Case",
      "text" : "In the noiseless case we reduce the optimization problem (3) to solving a system of linear equations [6], and provide Algorithm 1, which often leads to a closed-form estimator. We then give conditions under which with high probability, the closed-form solution is unique and is equal to the true matrix X . If rank(A(R)Û) = r one can write the resulting estimator X̂ in closed-form as follows:\nX̂ = ÛY = Û [ÛTA(R) T A(R)Û ]−1ÛTA(R) T B(R) (6)\nAlgorithm 1 does not treat the row and column measurements symmetrically. We can apply the same algorithm, but changing the role of rows and columns. The resulting\nAlgorithm 1 Input: A(R), A(C), B(R), B(C) and rank r\n1. Compute a basis (of size r) to the column space of B(C) using Gaussian elimi-\nnation, represented as the columns of a matrix Û ∈ Rn1×r.\n2. Solve the linear system B(R)•j = A (R)ÛY•j for each j = 1, .., n2 and write the\nsolutions as a matrix Y = Y•1...Y•n2 .\n3. Output X̂ = ÛY\nclosed form solution is then:\nX̂ = B(C)A(C)V̂ [V̂ TA(C)A(C) T V̂ ]−1V̂ T (7)\nfor an orthogonal matrix V̂ representing a basis for the rows of X . Since the algorithm uses Gaussian elimination steps for solving systems of linear equations, it is crucial that we have exact noiseless measurements. Next, we modify the algorithm to work also for noisy measurements."
    }, {
      "heading" : "3.2 General (Noisy) Case",
      "text" : "In the noisy case we seek a matrix X minimizing the loss F in eq. (2). The minimization problem is non-convex and there is no known algorithm with optimality guarantees. We propose Algorithm 2 (SVLS), which empirically returns a matrix estimator X̂ with a low value of the loss F . In addition, we prove in Section 4 recovery guarantees on the performance of SVLS.\nAlgorithm 2 SVLS Input: A(R), A(C), B(R), B(C) and rank r\n1. Compute Û , the r largest left singular vectors of B(C), (Û is a basis for the\ncolumns space of B(C)(r) ).\n2. Find the least-squares solution\nŶ = argminY ‖ B(R) −A(R)ÛY ||F . (8)\nIf rank(A(R)Û) = r we can write Ŷ in closed form as before:\nŶ = [ÛTA(R) T A(R)Û ]−1ÛTA(R) T B(R). (9)\n3. Compute the estimate X̂(R) = Û Ŷ .\n4. Repeat steps 1-3, replacing the roles of columns and rows to get an estimate\nX̂(C).\n5. Set X̂ = argminX̂(R),X̂(C)F(X), for the loss F(X) given in eq. (2)."
    }, {
      "heading" : "3.2.1 Gradient Descent",
      "text" : "The estimator X̂ returned by SVLS may not minimize the loss function F in eq. (2). We therefore perform an additional gradient descent stage starting from X̂ to achieve an estimator with lower loss (while still possibly only a local minimum since the problem is non-convex). SVLS can be thus viewed as a fast method for providing a desirable starting point for local-search algorithms. The details of the gradient descent are given in the Appendix, Section 7.2."
    }, {
      "heading" : "3.3 Estimation of Unknown Rank",
      "text" : "In real life problems, one doesn’t know the true rank of a matrix and should estimate it from data. Our rows-and-columns sampling design is particularly suitable for rank estimation since rank(B(C,0)) = rank(B(R,0)) = rank(X) with high probability when enough rows and columns are sampled. In the noiseless case we can estimate rank(X) by r̂=rank(B(C,0)) or rank(B(R,0)).\nFor the noisy case we estimate rank(X) from B(C), B(R). We use the popular\nelbow method to estimate rank(B(C)) in the following way\nr̂(C) = argmaxi∈[k(C)−1]\n(\nσi(B (C))\nσi+1(B(C))\n)\n(10)\nWe compute similarly r̂(R) from B(R) and take the average as our rank estimator, r̂ = round ( r̂(C)+r̂(C)\n2\n)\n. We demonstrate the performance of our rank estimation\nusing simulations in the Appendix, Section 7.7.\nModern methods for rank estimation from singular values [13] can be similarly applied to B(R), B(C) and may yield more accurate rank estimates. After we estimate the rank, we can plug-in r̂ as the rank parameter in the SVLS algorithm and recover X ."
    }, {
      "heading" : "3.4 Low Rank Approximation",
      "text" : "In the low rank matrix approximation problem, the goal is to approximate a (possibly full rank) matrix X by the closest (in Frobenius norm) rank-r matrix X(r). By the Eckart-Young Theorem [12], this problem has a closed-form solution which is the truncated SV D of X . SV D is a powerful tool in affine matrix recovery and different algorithms such as SVT, OptSpace , SVP and others apply SVD. In [15] the authors try to find a low rank approximation to X using measurements XA(C) = B(C) and A(R)X = B(R). For large n1, n2 they give a single-pass algorithm which computes X(r) using only B(C) and B(R). We bring their algorithm in the Appendix, Section 7.6. The main difference between the above formulation and our problem in eq. (3) is the rank estimation. In [15] it is assumed that k(R) = k(C) = k and one estimates X(k) instead of a rank-r matrix which can lead to poor performance if r ≪ k. We adjusted the algorithm presented in [15] to our problem and give a new estimator which is a combination of SVLS and [15]’s method, replacing X̂(R) and X̂(C) in steps 3,4 of SVLS by:\nX̂ (R) P = X̂ (R)V̂ V̂ T , X̂ (C) P = ÛÛ T X̂(C). (11)\nHere V̂ is the r largest right singular vectors of B(R) and Û is the r largest left singular vectors of B(C). We call this new estimator SV LSP . Simulations show almost identical and in some cases slightly better performance of this modified algorithm compared to SVLS (see Appendix, Section 7.6). This modified estimator is however difficult to analyze rigorously, and therefore we present throughout the paper our results for the SVLS estimator."
    }, {
      "heading" : "4 Performance Guarantees",
      "text" : "In this section we give guarantees on the accuracy of the estimator X̂ returned by SVLS . Our guarantees are probabilistic, with respect to randomizing the design matrices\nA(R), A(C). For the noiseless case we give conditions which are close to optimal for exact recovery."
    }, {
      "heading" : "4.1 Noiseless Case",
      "text" : "A rank r matrix of size n1 × n2 has r(n1 + n2 − r) degrees of freedom, and can therefore not be uniquely recovered by fewer measurements. Setting k(R) = k(C) = r gives precisely this minimal number of measurements. We next show that this number suffices, with probability 1, to guarantee accurate recovery of X in the GRC model. In the RCMC model the number of measurements is increased by a logarithmic factor in n and we need an additional incoherence assumption on X in order to guarantee accurate recovery with high probability. We first present two Lemmas which will be useful. Their proofs are given in the Appendix, Section 7.1.\nLemma 1. Let X1, X2 ∈ M(r)n1×n2 and A(R) ∈ Rk(R)×n1 , A(C) ∈ Rn2×k(C) such that rank(A(R)X1) = rank(X1A(C)) = r. If A(R)X1 = A(R)X2 and X1A(C) = X2A (C) then X1 = X2.\nLemma 2. Let X ∈ M(r)n1×n2 and A(R) ∈ Rk(R)×n1 , A(C) ∈ Rn2×k(C) such that rank(A(R)X) = rank(XA(C)) = r. For Algorithm 1 with inputsA(R), A(C), B(R,0), B(C,0) and r the output X̂ satisfies\nA(R)X = A(R)X̂, XA(C) = X̂A(C) (12)"
    }, {
      "heading" : "4.1.1 Exact Recovery for GRC",
      "text" : "For the noiseless case, we can recover X with the minimal number of measurements, as shown in Theorem 1 (proof given in the Appendix, Section 7.1):\nTheorem 1. Let X̂ be the output of Algorithm 1 in the GRC model with Z(C), Z(R) = 0 and k(R), k(C) ≥ r. Then P (X̂ = X) = 1."
    }, {
      "heading" : "4.1.2 Exact Recovery for RCMC",
      "text" : "In the RCMC model, rows and columns of X are sampled with replacement. Since the same row can be sampled over and over, we cannot guarantee uniqueness of solution, as was the case for the GRC model, but rather wish to prove that exact recovery of X is possible with high probability. We assume the Bernoulli rows and columns model as described in Section 2 and assume for simplicity that k(R) = k(C) = k.\nTheorem 2. Let X = UΣV T be the SV D of X ∈ Rn1×n2 , and max(µ(U), µ(V )) < µ. Take A(R) and A(C) as in the RCMC model without noise and probabilities p(R) =\nk n1 and p(C) = kn2 . Let β > 1 such that CR √ βlog(n)rµ k < 1 where CR is uniform constant and let X̂ be the output of Algorithm 1. Then P ( X̂ = X )\n> 1 − 6min(n1, n2) −β .\nThe proof of Theorem 2 is in the Appendix, Section 7.3.\nRemark 1. Both row and column measurements are need in order to guarantee unique recovery. If, for example, we observe only rows then even with n − 1 observed rows and rank r = 1 we can only determine the unobserved row up to a constant, and thus cannot recover X uniquely."
    }, {
      "heading" : "4.2 General (Noisy) Case",
      "text" : "In the noisy case we cannot guarantee exact recovery of X , and our goal is to minimize the error ||X − X̂ ||F for X̂ the output of SVLS. Here, we give bounds on the error for the GRC model. For simplicity, we show the result for k(R) = k(C) = k.\nWe focus on the high dimensional case k ≤ n, where the number of measurements is low. In this case our bound is similar to the bound of the Gaussian Ensemble (GE). In [5] it is shown for GE that ||X − X̂||F < CG √ nrτ2\nd holds with high probability for\nsome constant CG. We next give an analogous result for our GRC model (proof in the Appendix, Section 7.4).\nTheorem 3. Let A(R) and A(C) with k ≥ max(4r, 40) be as in the GRC model with noise matrices Z(R), Z(C). Let X̂ be the output of SVLS. Then there exist constants c, c(R), c(C) such that with probability > 1− 5e−ck:\n||X − X̂||F ≤ √ r\nk\n[ c(C)||Z(C)||2 + c(R)||Z(R)||2 ] . (13)\nTheorem 3 applies for any Z(C) and Z(R). If k ≤ n and Z(R), Z(C) i.i.d.∼ N(0, τ2), then from eq. (35) we get max(||Z(R)||2, ||Z(C)||2) ≤ 4τ √ n with probability 1 − e−2n. We therefore get the next Corollary for i.i.d. additive Gaussian noise:\nCorollary 1. Let A(R), A(C) as in the GRC with n ≥ k ≥ max(4r, 40), model and Z(R), Z(C) i.i.d.∼ N(0, τ2). Then there exist constants c, CGRC such that:\nP ( ||X − X̂||F ≤ CGRC √ τ2nr\nk\n)\n> 1− 5e−ck − e−2n. (14)"
    }, {
      "heading" : "5 Simulations Results",
      "text" : "We studied the performance of our algorithm using simulations. We measured the reconstruction accuracy using the Relative Root-Mean-Squared-Error (RRMSE), defined as\nRRMSE ≡ RRMSE(X, X̂) ≡ ||X − X̂ ||F /||X ||F . (15)\nFor simplicity, we concentrated on square matrices with n1 = n2 = n and used an equal number of row and column measurements, k(R) = k(C) = k . In all simulations we sampled a random rank-r matrix X = UV T with U, V ∈ Rn×r , U, V\ni.i.d.∼ N(0, σ2). In all simulations we assumed that rank(X) is unknown and estimated using the\nelbow method in eq. (10)."
    }, {
      "heading" : "5.1 Row-Column Matrix Completion (RCMC)",
      "text" : "In the noiseless case we compared our design to standard MC. We compared the reconstruction rate (probability of exact recovery of X as function of the number of measurements d) for the RCMC design with SVLS to the reconstruction rate for the standard MC design with the OptSpace [18] and SVT[2] algorithms. To allow for numerical errors, for each simulation yieldingX and X̂ we defined recovery as successful if their RRMSE was lower than 10−3, and for each value of d recorded the percentage of simulations for which recovery was successful. In Figure 1 we show results for n = 150, r = 3 and σ = 1. SVLS recovers X with probability 1 with the optimal number of measurements d = r(2n − r) = 894 yielding dn2 ≈ 0.04 while MC with OptSpace and SVT need roughly 3-fold and 8-fold more measurements, respectively, to guarantee exact recovery.\nThe improvement in accuracy is not due to our design or our algorithm alone, but due to their combination. We compared our method to OptSpace and SVT for RCMC. We sampled a matrix X with n = 100, r = 3, σ = 1 and noise level τ2 = 0.252, and varied the number of row and column measurements k. Figure 2 shows that while the performance of SVLS is very stable even for small k, the performance of OptSpace varies, with multiple instances achieving poor accuracy, and SVT which minimizes the nuclear norm achieves poor accuracy for all problem instances.\nRemark 2. The OptSpace algorithm has a trimming step which delete dense columns. We omitted this step in the RCMC model since it would delete all the known columns and rows and it’s not stable for this type of measurements, but it still get better result than SVT.\nNext, we compared our RCMC to standard MC. We sampled X as before with U, V ∈ R1000×r with standard Gaussian distribution, different rank and different noise ratio. The observations were corrupted by additive Gaussian noise Z with relative noise level NR ≡ ||Z||F /||X ||F . Results, displayed in Table 1, show that SVLS is significantly faster than the other two algorithms. It is also more accurate than MC for small number of measurements, and comparable to MC for large number of measurements.\nFinally, we checked for RCMC and MC our performance only on unobserved entries, to examine if RRMSE is optimistic due to overfitting to observed entries. Results, shown in the Appendix, Section 7.8, indicate than no overfitting is observed."
    }, {
      "heading" : "5.2 Gaussian Rows and Columns (GRC)",
      "text" : "We tested the performance of the GRC model with A(R), A(C) i.i.d.∼ N(0, 1n ) and with X = UV T where U, V i.i.d.∼ N(0, 1√\nr ). We compare our results to the Gaussian\nEnsemble model (GE) where for each n, A(X) was normalized to allow a fair comparison. In Figure 3 we take n = 100 and r = 2, and change the number of measurements d = 2nk (where A(R) ∈ Rk×n and A(C) ∈ Rn×k). We added Gaussian noise Z(R), Z(C) with different noise levels τ . For all noise levels, the performance of GRC was better than the performance of GE. The RRMSE error decays at a rate of √ k. For GE we used the APGL algorithm [26] for nuclear norm minimization.\nIn the next tests we ran SVLS for measurements with different noise levels. We take n = 1000 and k = 100 with different rank level every entry in Z(C), Z(R) i.i.d.∼ N(0, τ2) and different values of τ . Results are shown in Figure 4. The change in the relative error RRMSE is linear in τ while the rate depends on r.\nWe next examined the behaviour of theRRMSE when n → ∞ and when n, k, r → ∞ together, while the ratios kn and dr are kept constant. Results (shown in the Appendix, Section 7.5) indicate that when properly scaled, the RRMSE error is not sensitive to the value of n and other parameters, in agreement with Theorem 3."
    }, {
      "heading" : "6 Discussion",
      "text" : "We introduced a new measurements ensemble for low rank matrix recovery where every measurements is an affine combination of a row or column of X . We focused on two models: matrix completion from single columns and rows (RCMC) and matrix recovery from Gaussian combination of columns and rows (GRC). We proposed a fast algorithm for this ensemble. For the RCMC model we proved that in the noiseless case our method recovers X with high probability and simulation results show that the\nRCMC model outperforms the standard approach for matrix completion in both speed and accuracy for models with small noise.\nFor the GRC model we proved that our method recoversX with the optimal number of measurements in the noiseless case and gave an upper bounds on the error for the noisy case. For RCMC, our simulations show that the RCMC design may achieve comparable or favorable results, compared to the standard MC design, especially for low noise level. Proving recovery guarantees for this RCMC model is an interesting future challenge.\nOur proposed measurement scheme is not restricted to recovery of low-rank matrices. One can employ this measurement scheme and recover X by minimizing other matrix norms. This direction can lead to new algorithms that may improve matrix recovery for real datasets."
    }, {
      "heading" : "7 Appendix",
      "text" : ""
    }, {
      "heading" : "7.1 Proofs for Noiseless GRC Case",
      "text" : "Proof of Lemma 1\nProof. First, rank(X2A(C)) = rank(X1A(C)) = r and similarly rank(A(R)X2) = rank(A(R)X1) = r. Since span(X1A(C)), span(X2A(C)) are subspaces of span(X1), span(X2) respectively, and dim(span(X2)) ≤ r we get span(X2) = span(X2A(C)) = span(X1A\n(C)) = span(X1), and we define U ∈ On1×r a basis for this subspace. For X1, X2 there are Y1, Y2 ∈ Rr×n2 such that X1 = UY1, X2 = UY2. Therefore A(R)UY1 = A(R)UY2. Since rank(A(R)UY1) = r and U ∈ On1×r we get rank(A(R)U) = r, hence the matrix UTA(R) T A(R)U is invertible, which gives Y1 = Y2, and therefore X1 = UY1 = UY2 = X2.\nProof of Lemma 2\nProof. span(XA(C)) ⊆ span(X) and rank(XA(C)) = rank(X) = r, therefore span(XA(C)) = span(X) and Û from stage 1 in Algorithm 1 is a basis for span(X). We can write X = ÛY for some matrix Y ∈ Rr×n2 . Since rank(A(R)ÛY ) = rank(Û) = r, we have rank(A(R)Û) = r. Thus eq. (6) gives X̂ in closed form and we get:\nA(R)X̂ = A(R)Û [ÛTA(R) T A(R)Û ]−1ÛTA(R) T B(R,0) =\nA(R)Û [ÛTA(R) T A(R)Û ]−1ÛTA(R) T A(R)ÛY =\nA(R)ÛY = A(R)X. (16)\nX̂A(C) = Û [ÛTA(R) T A(R)Û ]−1ÛTA(R) T A(R)XA(C) =\nÛ [ÛTA(R) T A(R)Û ]−1ÛTA(R) T A(R)ÛY A(C) =\nÛY A(C) = XA(C). (17)\nLemma 3. Let V ∈ On×r and A(C) ∈ Rn×k be a random matrix A(C) i.i.d.∼ N(0, σ2). Then V TA(C) i.i.d.∼ N(0, σ2).\nProof. For any two matrices A ∈ Rn1×n2 andB ∈ Rm1×m2 we define their Kronecker product as a matrix in Rn1m1×n2m2 :\nA⊗B =\n\n      \na11B a12B . . a1n2B\n. . . . .\n. . . . .\n. . . . .\nan11B an12B . . an1n2B\n\n      \n(18)\nNow, we have vec(V TA(C)) = (In ⊗ V T )vec(A(C)) and since vec(A(C)) ∼ N(0, σIn) the vector (In ⊗ V T )vec(A(C)) is also a multivariate Gaussian vector with zero mean and covariance matrix:\nCOV ( V TA(C) ) = COV ( (In ⊗ V T )vec(A(C)) ) =\n(In ⊗ V T )COV ( vec(A(C)) ) (In ⊗ V T )T =\nσ2(In ⊗ V T )(In ⊗ V T )T = σ2Ir ⊗ In = σ2Inr. (19)\nProof of Theorem 1\nFor the GRC model, Lemmas 1,2 and 3 can be used to prove exact recovery of X with the minimal possible number of measurements:\nProof. Let UΣV T be the SVD of X . From Lemma 3 the elements of the matrix V TA(C) have a continuous Gaussian distribution and since the measure of low rank matrices is zero and k(C) ≥ r we get that P (rank(V TA(C)) = r) = 1. Since B(C) = UΣV TA(C) we get P (rank(B(C)) = rank(UΣV TA(C)) = r) = 1. In the same way P (rank(B(R)) = r) = 1. Combining Lemma 2 with Lemma 1 give us the required result."
    }, {
      "heading" : "7.2 Gradient Descent",
      "text" : "The gradient descent stage is performed directly in the space of rank r matrices, using the decomposition X̂=WS where W ∈ Rn1×r and S ∈ Rr×n2 and computing the gradient of the loss as a function of W and S,\nL(W,S) = F(WS) = ||A(R)WS −B(R)||2F + ||WSA(C) −B(C)||2F . (20)\nWe want to minimize eq. (20) but the loss L isn’t convex and therefore gradient descent may fail to converge to a global optimum. We propose X̂ (the output of SVLS\n) as a starting point which may be close enough to enable gradient descent to converge to the global optimum, and in addition may accelerate convergence.\nThe gradient of L is (using the chain rule)\n∂L ∂W = 2 [ A(R) T (A(R)WS −B(R))ST + (WSA(C) −B(C))A(C)T ST ]\n∂L ∂S = 2 [ WTA(R) T (A(R)WS −B(R)) +WT (WSA(C) −B(C))A(C)T ] (21)"
    }, {
      "heading" : "7.3 Proofs for Noiseless RCMC Case",
      "text" : "We prove that if U ∈ On1×r is orthonormal then with high probability we have p−1||UTA(R)TA(R)U − pIr||2 < 1. Because U is orthonormal, this is equivalent to\np−1||UUTA(R)TA(R)UUT − pUUT ||2 < 1 ⇔ p−1||PUPA(R)T PU − pPU ||2 < 1 (22)\nwhere PU = UUT , PA(R)T = A (R)T A(R) and p(R) = p. We generalize Theorem\n4.1 from [6].\nLemma 4. Suppose A(R) as in the RCMC model with inclusion probability p, and U ∈ On1×r with µ(U) = n1r maxi||PU (ei)||2 = µ. Then there is a numerical constant CR such that for all β > 1, if CR √\nβlog(n1)rµ pn1 < 1 then:\nP\n(\np−1||PUPA(R)T PU − pPU ||2 < CR\n√\nβlog(n1)rµ\npn1\n)\n> 1− 3n−β1 (23)\nThe proof of Lemma 4 builds upon (yet generalizes) the proof of Theorem 4.1 from [6]. We next present a few lemmas which are required for the proof of Lemma 4. We start with a lemma from [7].\nLemma 5. If yi is a family of vectors in Rd and ri is a sequence of i.i.d. Bernoulli random variables with P (ri = 1) = p, then\nE ( p−1||Σi(ri − p)yi ⊗ yi|| ) < C\n√\nlog(d)\np maxi||yi|| (24)\nfor some numerical constant C provided that the right hand side is less than 1.\nWe next use a result from large deviations theory [25]:\nTheorem 4. Let Y1...Yn be a sequence of independent random variables taking values in a Banach space and define\nZ = supf∈F\nn ∑\ni=1\nf(Yi) (25)\nwhere F is a real countable set of functions such that if f ∈ F then −f ∈ F . Assume that |f | ≤ B and E(f(Yi)) = 0 for every f ∈ F and i ∈ [n]. Then there exists a constant C such that for every t ≥ 0\nP ( |Z − E(Z)| ≥ t ) ≤ 3exp ( −t CB log(1 + t σ +Br ) )\n(26)\nwhere σ = supf∈F ∑n i=1 E(f 2(Yi)).\nTheorem 4 is used in the proof of the next lemma which is taken from Theorem 4.2\nin [6]. We bring here the lemma and proof in our notations for convenience.\nLemma 6. Let U ∈ On×r with incoherence constant µ. Let ri be i.i.d. Bernoulli random variables with P (ri = 1) = p and let Yi = p−1(ri − p)PU (ei) ⊗ PU (ei) for i = 1, .., n. Let Y =\n∑n i=1 Yi and Z = ||Y ||2. Suppose E(Z) ≤ 1. Then for every\nλ > 0 we have\nP ( |Z−E(Z)| ≥ λ √ µrlog(n)\npn\n) ≤ 3exp ( −γmin(λ2log(n), λ √ pnlog(n)\nµr ) ) (27)\nfor some positive constant γ.\nProof. We know that Z = ||Y ||2 = supf1,f2〈f1, Y f2〉 = supf1,f2 ∑n i=1〈f1,Yif2〉, where the supremum is taken over a countable set of unit vectors f1, f2 ∈ FV . Let F be the set of all functions f such that f(Y ) = 〈f1, Y f2〉 for some unit vectors f1, f2 ∈ FV . For every f ∈ F and i ∈ [n] we have E(f(Yi)) = 0. From the incoherence of U we conclude that\n|f(Yi)| = p−1|ri − p| × |〈f1, PU (ei)〉| × |〈PU (ei), f2〉| ≤ p−1||PU (ei)||2 ≤ p−1 r\nn µ. (28)\nIn addition\nE(f2(Yi)) = p −1(1− p)〈f1, PU (ei)〉2〈PU (ei), f2〉2 ≤\np−1||PU (ei)||2|〈PU (ei), f2〉2| ≤ p−1 r\nn µ|〈PU (ei), f2〉|2. (29)\nSince ∑n i=1 |〈PU (ei), f2〉|2 = ∑n i=1 |〈ei, PU (f2)〉|2 = ||PU (f2)||2 ≤ 1, we get ∑n\ni=1 E(f 2(Yi)) ≤ p−1 rnµ.\nWe can take B = 2p−1 rnµ and t = λ √ µrlog(n) pn and from Theorem 4:\nP (|Z − E(Z)| ≥ t) ≤ 3exp ( −t KB log(1 + t 2 ) ) ≤ 3exp (−tlog(2) KB min(1, t 2 ) )\n(30)\nwhere the last inequality is due to the fact that for every u > 0 we have log(1 + u) ≥ log(2)min(1, u). Taking γ = −log(2)/K finishes our proof.\nWe are now ready to prove Lemma 4\nProof. (Lemma 4) Represent any vector w ∈ Rn1 in the standard basis as w = ∑n1\ni=1〈w, ei〉ei. Therefore PU (w) = ∑n1 i=1〈PU (w), ei〉ei = ∑n1 i=1〈w,PU (ei)〉ei. Recall the ri Bernoulli variables which determine if ei is included as a row of A(R) as in Section 2 and define Yi and Z as in Lemma 6. We get\nPA(R)T PU (w) =\nn1 ∑\ni=1\nri〈w,PU (ei)〉ei =⇒\nPUPA(R)T PU (w) =\nn1 ∑\ni=1\nri〈w,PU (ei)〉PU (ei) (31)\nIn other words the matrix PUPA(R)T PU is given by\nPUPA(R)T PU =\nn1 ∑\ni=1\nriPU (ei)⊗ PU (ei) (32)\nU is µ−incoherent, thus maxi∈[n1]||PU (ei)|| ≤ √ rµ n1 , hence from Lemma 5 we have for p large enough:\nE(p−1||PUPA(R)T PU − pPU ||2) < C √ log(n1)rµ\npn1 ≤ 1. (33)\nFor β > 1 which satisfy the lemma’s requirement, take λ = √\nβ γ where γ as in\nTheorem 4. We get that if p > µlog(n1)rβn1γ then from Lemma 6 with probability of at least 1− 3n−β1 we have Z ≤ C √ log(n1)rµ pn1 + 1√γ √ log(n1)rµβ pn1\n. Taking CR = C + 1√γ finishes our proof.\nProof of Theorem 2\nProof. From Lemma 4 and using a union bound we have that with probability > 1 − 6min(n1, n2) −β , p(R) −1||p(R)Ir − UTA(R) T A(R)U ||2 < 1 and p(C) −1||p(C)Ir −\nV TA(C)A(C) T V ||2 < 1. Since the singular values of p(R)Ir − UTA(R) T A(R)U are |p(R) − σi(UTA(R) T A(R)U)| for 1 ≤ i ≤ r, we have\np(R) − σr(UTA(R) T A(R)U) ≤ σ1(p(R)Ir − UTA(R) T A(R)U) < p(R)\n⇒ 0 < σr(UTA(R) T A(R)U) (34)\nand similarly for V TA(C)A(C) T V . Therefore rank(A(R)U) = rank(V TA(C)) = r and rank(A(R)X) = rank(XA(C)) = r with probability > 1 − 6min(n1, n2)−β . From Lemma 2 we get A(R)X = A(R)X̂ XA(C) = X̂A(C) and from Lemma 1 we get X = X̂ with probability > 1− 6min(n1, n2)−β ."
    }, {
      "heading" : "7.4 Proofs for Noisy GRC Case",
      "text" : "The proof of Theorem 3 is using strong concentration results on the largest and smallest singular values of n× k matrix with i.i.d Gaussian entries:\nTheorem 5. [24] Let A ∈ Rn×k be a random matrix A i.i.d.∼ N(0, 1n ). Then, its largest and smallest singular values obey:\nP (\nσ1(A) > 1 + √ k√ n + t ) ≤e−nt2/2\nP ( σk(A) ≤ 1− √ k√ n − t ) ≤e−nt2/2. (35)\nCorollary 2. Let A ∈ Rn×k be a random matrix A i.i.d.∼ N(0, 1) where n ≥ 4k, and let A† be the Moore-Penrose pseudoinverse of A. Then\nP\n(\n||A†||2 ≤ 6√ n\n)\n> 1− e−n/18 (36)\nProof. Since A† is the pseudoinverse of A, ||A†||2= 1σk(A) and from Theorem 5 we get σk(A) ≥ √ n− √ k− t√n with probability ≥ 1− ent2/2 (notice the scaling by √n of the entries of A compared to Theorem 5). Therefore, if we take n ≥ 4k and t = 13 we get\nP\n(\n||A†||2 ≤ 6√ n\n)\n= P\n( σk(A) ≥ √ n\n6\n)\n≥ 1− e−n/18. (37)\nWe also use the following lemma from [23]:\nLemma 7. Let Q to be a finite set of vectors in Rn, let δ ∈ (0, 1) and k be an integer such that\nǫ ≡ √ 6log(2|Q|/δ) k ≤ 3. (38)\nLet A ∈ Rk×n be a random matrix with A i.i.d.∼ N(0, 1k ). Then,\nP\n(\nmaxx∈Q\n∣ ∣ ∣ ∣ ||Ax||2 ||x||2 − 1 ∣ ∣ ∣ ∣ ≤ ǫ ) > 1− δ. (39)\nLemma 7 is a direct result of the Johnson-Lindenstrauss lemma [11] applied to each vector in Q and using the union bound. Representing the vectors in Q as a matrix, Lemma 7 shows that A(R), A(C) preserve matrix Frobenius norm with high probability - a weaker property than the RIP which holds for any low-rank matrix.\nTo prove Theorem 3, we first represent ||X−X̂||F as a sum three parts (Lemma 8), then give probabilistic upper bounds to each of the parts and finally use union bound. We define A(R) Û = A(R)Û and A(C) V T = V TA(C). From Lemma 3 A(R) Û , A (C) V T i.i.d.∼ N(0, 1), hence rank(A(R) Û ) = rank(A (C) V T ) = r with probability 1. We assume w.l.o.g that X̂ = X̂(R) (see SVLS description). Therefore, from eq. (9) we have X̂ = Û(A (R)T\nÛ A\n(R) Û )−1A\n(R)T Û B(R).\nWe denote by A(R) Û\n† = (A (R)T\nÛ A\n(R) Û )−1A\n(R)T Û and A(C)V T\n† = A (C)T\nV T (A (C) V T A\n(C)T V T ) −1\nthe Moore-Penrose pseudoinverse of A(R) Û and A(C)V T , respectively. We next prove the following lemma\nLemma 8. Let A(R) and A(C) be as in the GRC model and Z(R), Z(C) be noise matrices. Let X̂ be the output of SVLS. Then:\n||X − X̂||F ≤ I+ II+ III\nwhere:\nI ≡ ||(B(C,0) −B(C)(r) )A (C) V T † ||F (40)\nII ≡||ÛA(R) Û † A(R)(B(C,0) −B(C)(r) )A (C) V T † ||F (41)\nIII ≡||ÛA(R) Û\n† Z(R)||F . (42)\nProof. We represent ||X − X̂||F as follows\n||X − X̂||F =\n||X − Û(A(R) T\nÛ A\n(R) Û )−1A\n(R)T Û (A(R)X + Z(R))||F =\n||X − ÛA(R) Û\n† A(R)X − ÛA(R)\nÛ\n† Z(R)||F ≤\n||X − ÛA(R) Û\n† A(R)X ||F + III (43)\nwhere we have used the triangle inequality. We next use the following equality\nXA(C)A (C)\nV T\n† V T = UΣV TA(C)A (C)\nV T\n† V T = UΣV T = X (44)\nto obtain:\n||X − ÛA(R) Û\n† A(R)X ||F =\n||(In − ÛA(R)Û † A(R))X ||F =\n||(In − ÛA(R)Û † A(R))XA(C)A (C) V T † V T ||F =\n||(In − ÛA(R)Û † A(R))B(C,0)A (C) V T † ||F (45)\nwhere the last equality is true because V is orthogonal.\nSince Û is a basis for span(B(C)(r) ) there exists a matrix Y such that ÛY = B (C) (r)\nand we get:\n(In − ÛA(R)Û † A(R))B (C) (r) = B (C) (r) − ÛA (R) Û † A(R)ÛY = B (C) (r) − ÛY = 0. (46)\nTherefore\n||(In − ÛA(R)Û † A(R))B(C,0)A (C) V T † ||F =\n||(In − ÛA(R)Û † A(R))(B(C,0) −B(C)(r) )A (C) V T † ||F ≤\n||(B(C,0) −B(C)(r) )A (C) V T † ||F + ||ÛA(R)Û † A(R)(B(C,0) −B(C)(r) )A (C) V T † ||F = I+ II\n(47)\nCombining eq. (43) and eq. (47) gives the required result.\nWe next bound each of the three parts in the formula of Lemma 8. We use the\nfollowing claim:\nClaim 1. ||B(C,0) −B(C)(r) ||2 ≤ 2||Z(C)||2\nProof. We know that ||B(C) − B(C)(r) ||2 ≤ ||B(C) − B(C,0)||2 since rank(B (C) (r) ) = rank(B(C,0)) = r with probability 1, and by definition B(C)(r) is the closest rank-r matrix to B(C) in Frobenius norm. Therefore from the triangle inequality\n||(B(C,0) −B(C)(r) )||2 ≤ ||B (C) −B(C)(r) ||2 + ||B (C) −B(C,0)||2 ≤\n2||B(C,0) −B(C)||2 = 2||Z(C)||2. (48)\nNow we are ready to prove Theorem 3. The proof uses the following inequalities\nfor matrix norms for any two matrices A,B:\n||AB||2 ≤ ||A||2||B||2 ||AB||F ≤ ||A||F ||B||2\nrank(A) 6 r ⇒ ||A||F ≤ √ r||A||2. (49)\nProof. (Theorem 3) We prove (probabilistic) upper bounds on the three terms appearing in Lemma 8.\n1. We have\nrank\n(\n(B(C,0) −B(C)(r) )A (C) V T\n† )\n6 rank\n(\nA (C) V T\n† )\n6 r. (50)\nTherefore\nI = ||(B(C,0) −B(C)(r) )A (C) V T † ||F ≤ √ r||(B(C,0) −B(C)(r) )||2||A (C) V T † ||2 (51)\nSince A(C) V T\ni.i.d.∼ N(0, 1), from Corollary 2 we get P (\n||A(C) V T † ||2 ≤ 6√k )\n≥ 1− e−k/18 for k ≥ 4r, hence with probability ≥ 1− e−k/18,\nI ≤ 6 √ r\nk ||(B(C,0) −B(C)(r) )||2. (52)\nFrom Claim 1 and eq. (40) we get a bound on I for some absolute constants C1, c1:\nP ( I ≤ C1 √ r\nk ||Z(C)||2\n)\n> 1− e−c1k. (53)\n2. Û is orthogonal and can be omitted from II without changing the norm. Apply-\ning the second inequality in eq. (49) twice, we get the inequality:\nII =||ÛA(R) Û † A(R)(B(C,0) −B(C)(r) )A (C) V T † ||F ≤\n||A(R) Û † ||2||A(R)(B(C,0) − B(C)(r) )||F ||A (C) V T † ||2. (54)\nFrom Corollary 2 we know that for k > 4r we have ||A(R) Û † ||2 ≤ 6√k and ||A(C)V T † ||2 ≤ 6√k , each with probability > 1− e −k/18. Therefore,\nP ( II ≤ 36 k ||A(R)(B(C,0) −B(C)(r) )||F ) > 1− 2e−k/18. (55)\nA(R) andB(C,0)−B(C)(r) are independent and rank(B(C,0)−B (C) (r) ) ≤ 2r. Therefore we can apply Lemma 7 with k such that k6 > log(2k) + k 18 (this holds for k ≥ 40) to get with probability > 1− 2e−k/18:\nII ≤ 36 k ||A(R)(B(C,0) −B(C)(r) )||F ≤\n36 √ 2k\nk ||(B(C,0) −B(C)(r) )||F ≤ 36\n√\n4 r\nk ||(B(C,0) −B(C)(r) )||2. (56)\nFrom eq. (55) and (56) together with Claim 1 we have constants C2 and c2 such that,\nP ( II ≤ C2||Z(C)||2 ) > 1− 3e−c2k. (57)\n3. rank(A(R) Û\n† ) ≤ r and from Corollary 2 we get P (\n||A(R) Û † ||2 ≤ 6√k )\n> 1 − e−k/18 for k > 4r. Therefore, with probability > 1− e−k/18:\nIII =||ÛA(R) Û † Z(R)||F = ||A(R)Û † Z(R)||F ≤\n√ r||A(R)\nÛ\n† Z(R)||2 ≤ √ r||A(R)\nÛ\n† ||2||Z(R)||2 ≤ 6 √ r√ k ||Z(R)||2. (58)\nHence we have constants C3 and c3 such that, > 1− e−c3k.\nP ( III ≤ C3||Z(R)||2 ) > 1− e−c3k. (59)\nCombining equations (53,57,59) with Lemma 8 and taking the union bound while\nsetting c(C) = C1 + C2, c(R) = C3 with c = min(c1, c2, c3) concludes our proof."
    }, {
      "heading" : "7.5 Simulations for Large Values of n",
      "text" : "We varied n between 10 and 1000, with results averaged over 100 different matrices of rank 3 at each point, and tried to recover them using k = 20 row and column measurements. Measurement matrices were A(R), A(C) i.i.d.∼ 1n to allow similar norms for each measurement vector for different values of n. Recovery performance was insensitive to n. if we take A(R), A(C) i.i.d.∼ N(0, 1) instead of N(0, 1n ), the scaling of our results is in agreement with Theorem 3.\nNext, we take n, k, r → ∞ while the ratios nk = 5 and kr = 4 are kept constant, and compute the relative error for different noise level. Again, the relative error converges rapidly to constant, independent of n, k, r ."
    }, {
      "heading" : "7.6 Low Rank matrix Approximation",
      "text" : "We bring here the one pass algorithm to approximate X from [15] for the convenience of the reader. The output of this algorithm isn’t low rank if k > r. This algorithm is different from SV LSP and its purpose is to approximate a (possibly full rank) matrix by low rank matrix. We adjusted Algorithm 3 to our purpose with some changes. First, we estimate the rank of X using the elbow method from Section 3.3 and instead of calculating the QR decomposition of B(C) and B(R) T we find their r̂ largest singular vectors. Furthermore, we repeat part two in algorithm 3 while replacing the roles of columns and rows as in SVLS . This variation gives our modified algorithm SV LSP as described in Section 3.4.\nWe compared our SVLS to SV LSP which is presented in Section 3.4. We took X ∈ M(10)1000×1000 and σ = 1. We tried to recover X in the GRC model with k = 12 for 100 different matrices. For each matrix, we compared the RRMSE obtained for the outputs of SVLS and SV LSP . The RRMSE for SV LSP was lower than the RRMSE for SVLS in most cases but the differences were very small and negligible.\nAlgorithm 3 Input: A(R), A(C), B(R), B(C)\n1. compute Q(C)R(C) the QR decomposition of B(C), and Q(R)R(R) the QR de-\ncomposition of B(R) T\n2. Find the least-squares solution Y = argminC ||Q(C)B(C) −CQ(R) T B(R) T ||F .\n3. Return the estimate X̂ = Q(C)Y Q(R) T ."
    }, {
      "heading" : "7.7 Rank Estimation",
      "text" : "We test the elbow method for estimating the rank of X (see eq. (10)). We take a matrix X of size 400× 400 and different ranks. We add Gaussian noise with σ = 0.25 while the measurements are sampled as in the RCMC model. For each number of measurements we sampled 100 matrices and took the average estimated rank. We\ncompute the estimator r̂ for different values of d, the number of measurements. We compare our method to the rank estimation which appears in OptSpace [17] for the standard MC problem. Our simulation results, shown in Figure 8, indicate that the RCMC model with the elbow method is a much better design for rank estimation of X ."
    }, {
      "heading" : "7.8 Test Error",
      "text" : "In matrix completion with MC and RCMC ensembles the RRMSE loss function measures the loss on both the observed and unobserved entries. This loss may be too optimistic when considering our prediction error only on unobserved entries. Thus, instead of including all measurements in calculation of the RRMSE we compute a different measure of prediction error, given by the RRMSE only on the unobserved entries. For each single-entry measurements operator A define E(A) the set of measured entries and Ē it’s complement, i.e. the set of unmeasured entries (i, j) ∈ [n1]× [n2]. We define XĒ to be a matrix such that XĒij = Xij if (i, j) ∈ Ē and 0 otherwise. Instead of RRMSE(X, X̂) we now calculate RRMSE(XĒ, X̂Ē). This quantity measures\nour reconstruction only on the unseen matrix entries Xij , and is thus not influenced by overfitting. In Table 2 we performed exactly the same simulation as in Table 1 but with RRMSE(XĒ, X̂Ē). The results of OptSpace , SVT and SVLS stay similar to the results in Table 1 and our RRMSE loss function does not show overfitting."
    } ],
    "references" : [ {
      "title" : "Lambertian reflectance and linear subspaces",
      "author" : [ "Ronen Basri", "David W Jacobs" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2003
    }, {
      "title" : "A singular value thresholding algorithm for matrix completion",
      "author" : [ "Jian-Feng Cai", "Emmanuel J Candès", "Zuowei Shen" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1956
    }, {
      "title" : "ROP: Matrix recovery via rank-one projections",
      "author" : [ "Tony T Cai", "Anru Zhang" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2015
    }, {
      "title" : "Matrix completion with noise",
      "author" : [ "Emmanuel J Candès", "Yaniv Plan" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2010
    }, {
      "title" : "Tight oracle inequalities for low-rank matrix recovery from a minimal number of noisy random measurements",
      "author" : [ "Emmanuel J Candès", "Yaniv Plan" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2011
    }, {
      "title" : "Exact matrix completion via convex optimization",
      "author" : [ "Emmanuel J Candès", "Benjamin Recht" ],
      "venue" : "Foundations of Computational Mathematics,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2009
    }, {
      "title" : "Sparsity and incoherence in compressive sampling",
      "author" : [ "Emmanuel J Candès", "Justin Romberg" ],
      "venue" : "Inverse Problems,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2007
    }, {
      "title" : "The power of convex relaxation: Near-optimal matrix completion",
      "author" : [ "Emmanuel J Candès", "Terence Tao" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2010
    }, {
      "title" : "Genotype imputation via matrix completion",
      "author" : [ "Eric C Chi", "Hua Zhou", "Gary K Chen", "Diego Ortega Del Vecchyo", "Kenneth Lange" ],
      "venue" : "Genome Research,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2013
    }, {
      "title" : "Genomic maps of long noncoding RNA occupancy reveal principles of RNAchromatin interactions",
      "author" : [ "Ci Chu", "Kun Qu", "Franklin L Zhong", "Steven E Artandi", "Howard Y Chang" ],
      "venue" : "Molecular cell,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2011
    }, {
      "title" : "An elementary proof of a theorem of Johnson and Lindenstrauss",
      "author" : [ "Sanjoy Dasgupta", "Anupam Gupta" ],
      "venue" : "Random Structures & Algorithms,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2003
    }, {
      "title" : "The approximation of one matrix by another of lower rank",
      "author" : [ "Carl Eckart", "Gale Young" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1936
    }, {
      "title" : "The optimal hard threshold for singular values",
      "author" : [ "Matan Gavish", "David L Donoho" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2014
    }, {
      "title" : "Quantum state tomography via compressed sensing",
      "author" : [ "David Gross", "Yi-Kai Liu", "Steven T Flammia", "Stephen Becker", "Jens Eisert" ],
      "venue" : "Physical Review Letters,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2010
    }, {
      "title" : "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions",
      "author" : [ "Nathan Halko", "Per-Gunnar Martinsson", "Joel A Tropp" ],
      "venue" : "SIAM Review,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2011
    }, {
      "title" : "Guaranteed rank minimization via singular value projection",
      "author" : [ "Prateek Jain", "Raghu Meka", "Inderjit S Dhillon" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2010
    }, {
      "title" : "Matrix completion from noisy entries",
      "author" : [ "Raghunandan H Keshavan", "Andrea Montanari", "Sewoong Oh" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2009
    }, {
      "title" : "Matrix completion from a few entries",
      "author" : [ "Raghunandan H Keshavan", "Andrea Montanari", "Sewoong Oh" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2010
    }, {
      "title" : "Matrix factorization techniques for recommender systems",
      "author" : [ "Yehuda Koren", "Robert Bell", "Chris Volinsky" ],
      "venue" : null,
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2009
    }, {
      "title" : "Fixed point and Bregman iterative methods for matrix rank minimization",
      "author" : [ "Shiqian Ma", "Donald Goldfarb", "Lifeng Chen" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2011
    }, {
      "title" : "A simpler approach to matrix completion",
      "author" : [ "Benjamin Recht" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2011
    }, {
      "title" : "Guaranteed minimumrank solutions of linear matrix equations via nuclear norm minimization",
      "author" : [ "Benjamin Recht", "Maryam Fazel", "Pablo A Parrilo" ],
      "venue" : "SIAM Review,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2010
    }, {
      "title" : "Understanding Machine Learning: From Theory to Algorithms",
      "author" : [ "Shai Shalev-Shwartz", "Shai Ben-David" ],
      "venue" : null,
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2014
    }, {
      "title" : "Condition numbers of random matrices",
      "author" : [ "Stanislaw J Szarek" ],
      "venue" : "Journal of Complexity,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 1991
    }, {
      "title" : "New concentration inequalities in product spaces",
      "author" : [ "Michel Talagrand" ],
      "venue" : "Inventiones Mathematicae,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 1996
    }, {
      "title" : "An accelerated proximal gradient algorithm for nuclear norm regularized linear least squares problems",
      "author" : [ "Kim-Chuan Toh", "Sangwoon Yun" ],
      "venue" : "Pacific Journal of Optimization,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 18,
      "context" : "The problem has found numerous applications throughout science and engineering, in different fields such as collaborative filtering [19], face recognition [1], quantum state tomography [14] and computational biology [9].",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 0,
      "context" : "The problem has found numerous applications throughout science and engineering, in different fields such as collaborative filtering [19], face recognition [1], quantum state tomography [14] and computational biology [9].",
      "startOffset" : 155,
      "endOffset" : 158
    }, {
      "referenceID" : 13,
      "context" : "The problem has found numerous applications throughout science and engineering, in different fields such as collaborative filtering [19], face recognition [1], quantum state tomography [14] and computational biology [9].",
      "startOffset" : 185,
      "endOffset" : 189
    }, {
      "referenceID" : 8,
      "context" : "The problem has found numerous applications throughout science and engineering, in different fields such as collaborative filtering [19], face recognition [1], quantum state tomography [14] and computational biology [9].",
      "startOffset" : 216,
      "endOffset" : 219
    }, {
      "referenceID" : 3,
      "context" : "Most attention thus far has been given to two particular ensembles of random transformations A: (i) the Matrix Completion (MC) setting, in which each element of A(X) is a single entry of the matrix where the subset of the observed measurements is sampled uniformly at random [4, 6, 8, 17, 18, 21] (ii) Gaussian-Ensemble (GE) affine-matrix-recovery, in which each element of A(X) is a weighted sum of all elements of X with i.",
      "startOffset" : 275,
      "endOffset" : 296
    }, {
      "referenceID" : 5,
      "context" : "Most attention thus far has been given to two particular ensembles of random transformations A: (i) the Matrix Completion (MC) setting, in which each element of A(X) is a single entry of the matrix where the subset of the observed measurements is sampled uniformly at random [4, 6, 8, 17, 18, 21] (ii) Gaussian-Ensemble (GE) affine-matrix-recovery, in which each element of A(X) is a weighted sum of all elements of X with i.",
      "startOffset" : 275,
      "endOffset" : 296
    }, {
      "referenceID" : 7,
      "context" : "Most attention thus far has been given to two particular ensembles of random transformations A: (i) the Matrix Completion (MC) setting, in which each element of A(X) is a single entry of the matrix where the subset of the observed measurements is sampled uniformly at random [4, 6, 8, 17, 18, 21] (ii) Gaussian-Ensemble (GE) affine-matrix-recovery, in which each element of A(X) is a weighted sum of all elements of X with i.",
      "startOffset" : 275,
      "endOffset" : 296
    }, {
      "referenceID" : 16,
      "context" : "Most attention thus far has been given to two particular ensembles of random transformations A: (i) the Matrix Completion (MC) setting, in which each element of A(X) is a single entry of the matrix where the subset of the observed measurements is sampled uniformly at random [4, 6, 8, 17, 18, 21] (ii) Gaussian-Ensemble (GE) affine-matrix-recovery, in which each element of A(X) is a weighted sum of all elements of X with i.",
      "startOffset" : 275,
      "endOffset" : 296
    }, {
      "referenceID" : 17,
      "context" : "Most attention thus far has been given to two particular ensembles of random transformations A: (i) the Matrix Completion (MC) setting, in which each element of A(X) is a single entry of the matrix where the subset of the observed measurements is sampled uniformly at random [4, 6, 8, 17, 18, 21] (ii) Gaussian-Ensemble (GE) affine-matrix-recovery, in which each element of A(X) is a weighted sum of all elements of X with i.",
      "startOffset" : 275,
      "endOffset" : 296
    }, {
      "referenceID" : 20,
      "context" : "Most attention thus far has been given to two particular ensembles of random transformations A: (i) the Matrix Completion (MC) setting, in which each element of A(X) is a single entry of the matrix where the subset of the observed measurements is sampled uniformly at random [4, 6, 8, 17, 18, 21] (ii) Gaussian-Ensemble (GE) affine-matrix-recovery, in which each element of A(X) is a weighted sum of all elements of X with i.",
      "startOffset" : 275,
      "endOffset" : 296
    }, {
      "referenceID" : 4,
      "context" : "Gaussian weights [5, 22].",
      "startOffset" : 17,
      "endOffset" : 24
    }, {
      "referenceID" : 21,
      "context" : "Gaussian weights [5, 22].",
      "startOffset" : 17,
      "endOffset" : 24
    }, {
      "referenceID" : 5,
      "context" : "Remarkably, although the recovery problem is in general NP-hard, when r ≪ min(n1, n2) and under certain conditions on the matrix X or the measurement operator A, one can recover X from d ≪ n1n2 measurements with high probability and using efficient algorithms [6, 8, 21, 22].",
      "startOffset" : 260,
      "endOffset" : 274
    }, {
      "referenceID" : 7,
      "context" : "Remarkably, although the recovery problem is in general NP-hard, when r ≪ min(n1, n2) and under certain conditions on the matrix X or the measurement operator A, one can recover X from d ≪ n1n2 measurements with high probability and using efficient algorithms [6, 8, 21, 22].",
      "startOffset" : 260,
      "endOffset" : 274
    }, {
      "referenceID" : 20,
      "context" : "Remarkably, although the recovery problem is in general NP-hard, when r ≪ min(n1, n2) and under certain conditions on the matrix X or the measurement operator A, one can recover X from d ≪ n1n2 measurements with high probability and using efficient algorithms [6, 8, 21, 22].",
      "startOffset" : 260,
      "endOffset" : 274
    }, {
      "referenceID" : 21,
      "context" : "Remarkably, although the recovery problem is in general NP-hard, when r ≪ min(n1, n2) and under certain conditions on the matrix X or the measurement operator A, one can recover X from d ≪ n1n2 measurements with high probability and using efficient algorithms [6, 8, 21, 22].",
      "startOffset" : 260,
      "endOffset" : 274
    }, {
      "referenceID" : 9,
      "context" : "For example, (i) In collaborative filtering, we may wish to recover a users-items preference matrix and have access to only a subset of the users, but can observe their preference scores for all items (ii) When recovering a protein-RNA interactions matrix in molecular biology, a single experiment may simultaneously measure the interactions of a specific protein with all RNA molecules [10].",
      "startOffset" : 387,
      "endOffset" : 391
    }, {
      "referenceID" : 5,
      "context" : "It is thus not surprising that algorithms such as nuclear norm minimization [6, 22], which succeed for the GE and MC models, fail in our case, and different algorithms and theory are required.",
      "startOffset" : 76,
      "endOffset" : 83
    }, {
      "referenceID" : 21,
      "context" : "It is thus not surprising that algorithms such as nuclear norm minimization [6, 22], which succeed for the GE and MC models, fail in our case, and different algorithms and theory are required.",
      "startOffset" : 76,
      "endOffset" : 83
    }, {
      "referenceID" : 4,
      "context" : "For this ensemble, one can recover uniquely a low rank matrix X with O(r(n1+n2)) noiseless measurements using nuclear norm minimization [5, 22] or other methods such as Singular Value Projection (SVP) [16], which is optimal up to constants.",
      "startOffset" : 136,
      "endOffset" : 143
    }, {
      "referenceID" : 21,
      "context" : "For this ensemble, one can recover uniquely a low rank matrix X with O(r(n1+n2)) noiseless measurements using nuclear norm minimization [5, 22] or other methods such as Singular Value Projection (SVP) [16], which is optimal up to constants.",
      "startOffset" : 136,
      "endOffset" : 143
    }, {
      "referenceID" : 15,
      "context" : "For this ensemble, one can recover uniquely a low rank matrix X with O(r(n1+n2)) noiseless measurements using nuclear norm minimization [5, 22] or other methods such as Singular Value Projection (SVP) [16], which is optimal up to constants.",
      "startOffset" : 201,
      "endOffset" : 205
    }, {
      "referenceID" : 5,
      "context" : "In the standard matrix completion problem [6] we can recover X with high probability from single entries chosen uniformly at random using nuclear norm minimization [2, 8, 20, 21, 26] or using other methods such as SVD and gradient descent [17, 18].",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 1,
      "context" : "In the standard matrix completion problem [6] we can recover X with high probability from single entries chosen uniformly at random using nuclear norm minimization [2, 8, 20, 21, 26] or using other methods such as SVD and gradient descent [17, 18].",
      "startOffset" : 164,
      "endOffset" : 182
    }, {
      "referenceID" : 7,
      "context" : "In the standard matrix completion problem [6] we can recover X with high probability from single entries chosen uniformly at random using nuclear norm minimization [2, 8, 20, 21, 26] or using other methods such as SVD and gradient descent [17, 18].",
      "startOffset" : 164,
      "endOffset" : 182
    }, {
      "referenceID" : 19,
      "context" : "In the standard matrix completion problem [6] we can recover X with high probability from single entries chosen uniformly at random using nuclear norm minimization [2, 8, 20, 21, 26] or using other methods such as SVD and gradient descent [17, 18].",
      "startOffset" : 164,
      "endOffset" : 182
    }, {
      "referenceID" : 20,
      "context" : "In the standard matrix completion problem [6] we can recover X with high probability from single entries chosen uniformly at random using nuclear norm minimization [2, 8, 20, 21, 26] or using other methods such as SVD and gradient descent [17, 18].",
      "startOffset" : 164,
      "endOffset" : 182
    }, {
      "referenceID" : 25,
      "context" : "In the standard matrix completion problem [6] we can recover X with high probability from single entries chosen uniformly at random using nuclear norm minimization [2, 8, 20, 21, 26] or using other methods such as SVD and gradient descent [17, 18].",
      "startOffset" : 164,
      "endOffset" : 182
    }, {
      "referenceID" : 16,
      "context" : "In the standard matrix completion problem [6] we can recover X with high probability from single entries chosen uniformly at random using nuclear norm minimization [2, 8, 20, 21, 26] or using other methods such as SVD and gradient descent [17, 18].",
      "startOffset" : 239,
      "endOffset" : 247
    }, {
      "referenceID" : 17,
      "context" : "In the standard matrix completion problem [6] we can recover X with high probability from single entries chosen uniformly at random using nuclear norm minimization [2, 8, 20, 21, 26] or using other methods such as SVD and gradient descent [17, 18].",
      "startOffset" : 239,
      "endOffset" : 247
    }, {
      "referenceID" : 7,
      "context" : "However, recovery guarantees for this model are weaker: setting n = max(n1, n2), it is shown that Θ(nrlog(n)) measurements are required to recover a rank r matrix of size n1 × n2 [8].",
      "startOffset" : 179,
      "endOffset" : 182
    }, {
      "referenceID" : 2,
      "context" : "Recently a new design of rank one projections was proposed [3], where each measurement is of the form αXβ and such that α ∈ R1 , β ∈ R2 have i.",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 21,
      "context" : "Our problem is a specialization of the general affine matrix recovery problem [22], in which a matrix is measured using a general affine transformation A with b = A(X) + z.",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 4,
      "context" : "It is instructive to compare our GRC ensemble to the Gaussian Ensemble (GE) [5], with the matrix representation A(X) = Avec(X) where A ∈ Rd×n1n2 and A i.",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 21,
      "context" : "The GE model satisfies the r-RIP condition for d = O(rn) with high probability [22].",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 4,
      "context" : "Based on this property it is known that nuclear norm minimization [5, 22] and other algorithms such as SVP [16] can recover X with high probability.",
      "startOffset" : 66,
      "endOffset" : 73
    }, {
      "referenceID" : 21,
      "context" : "Based on this property it is known that nuclear norm minimization [5, 22] and other algorithms such as SVP [16] can recover X with high probability.",
      "startOffset" : 66,
      "endOffset" : 73
    }, {
      "referenceID" : 15,
      "context" : "Based on this property it is known that nuclear norm minimization [5, 22] and other algorithms such as SVP [16] can recover X with high probability.",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 5,
      "context" : "We next compare RCMC to the standard Matrix Completion model [6], in which single entries are chosen at random to be observed.",
      "startOffset" : 61,
      "endOffset" : 64
    }, {
      "referenceID" : 5,
      "context" : "Unlike GE, for MC incoherence conditions on X are required in order to guarantee unique recovery of X [6] : Definition 2.",
      "startOffset" : 102,
      "endOffset" : 105
    }, {
      "referenceID" : 1,
      "context" : "When X is μ-incoherent, and when known entries are sampled uniformly at random from X , several algorithms [2, 16, 17] succeed to recover X with high probability.",
      "startOffset" : 107,
      "endOffset" : 118
    }, {
      "referenceID" : 15,
      "context" : "When X is μ-incoherent, and when known entries are sampled uniformly at random from X , several algorithms [2, 16, 17] succeed to recover X with high probability.",
      "startOffset" : 107,
      "endOffset" : 118
    }, {
      "referenceID" : 16,
      "context" : "When X is μ-incoherent, and when known entries are sampled uniformly at random from X , several algorithms [2, 16, 17] succeed to recover X with high probability.",
      "startOffset" : 107,
      "endOffset" : 118
    }, {
      "referenceID" : 5,
      "context" : "1 Noiseless Case In the noiseless case we reduce the optimization problem (3) to solving a system of linear equations [6], and provide Algorithm 1, which often leads to a closed-form estimator.",
      "startOffset" : 118,
      "endOffset" : 121
    }, {
      "referenceID" : 12,
      "context" : "Modern methods for rank estimation from singular values [13] can be similarly applied to B, B and may yield more accurate rank estimates.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 11,
      "context" : "By the Eckart-Young Theorem [12], this problem has a closed-form solution which is the truncated SV D of X .",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 14,
      "context" : "In [15] the authors try to find a low rank approximation to X using measurements XA = B and AX = B.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 14,
      "context" : "In [15] it is assumed that k = k = k and one estimates X(k) instead of a rank-r matrix which can lead to poor performance if r ≪ k.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 14,
      "context" : "We adjusted the algorithm presented in [15] to our problem and give a new estimator which is a combination of SVLS and [15]’s method, replacing X̂ and X̂ in steps 3,4 of SVLS by:",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 14,
      "context" : "We adjusted the algorithm presented in [15] to our problem and give a new estimator which is a combination of SVLS and [15]’s method, replacing X̂ and X̂ in steps 3,4 of SVLS by:",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 4,
      "context" : "In [5] it is shown for GE that ||X − X̂||F < CG √",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 17,
      "context" : "We compared the reconstruction rate (probability of exact recovery of X as function of the number of measurements d) for the RCMC design with SVLS to the reconstruction rate for the standard MC design with the OptSpace [18] and SVT[2] algorithms.",
      "startOffset" : 219,
      "endOffset" : 223
    }, {
      "referenceID" : 1,
      "context" : "We compared the reconstruction rate (probability of exact recovery of X as function of the number of measurements d) for the RCMC design with SVLS to the reconstruction rate for the standard MC design with the OptSpace [18] and SVT[2] algorithms.",
      "startOffset" : 231,
      "endOffset" : 234
    }, {
      "referenceID" : 25,
      "context" : "For GE we used the APGL algorithm [26] for nuclear norm minimization.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 0,
      "context" : "[1] Ronen Basri and David W Jacobs.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "[2] Jian-Feng Cai, Emmanuel J Candès, and Zuowei Shen.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[3] Tony T Cai and Anru Zhang.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[4] Emmanuel J Candès and Yaniv Plan.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[5] Emmanuel J Candès and Yaniv Plan.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[6] Emmanuel J Candès and Benjamin Recht.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "[7] Emmanuel J Candès and Justin Romberg.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "[8] Emmanuel J Candès and Terence Tao.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "[9] Eric C Chi, Hua Zhou, Gary K Chen, Diego Ortega Del Vecchyo, and Kenneth Lange.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "[10] Ci Chu, Kun Qu, Franklin L Zhong, Steven E Artandi, and Howard Y Chang.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[11] Sanjoy Dasgupta and Anupam Gupta.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[12] Carl Eckart and Gale Young.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "[13] Matan Gavish and David L Donoho.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "[14] David Gross, Yi-Kai Liu, Steven T Flammia, Stephen Becker, and Jens Eisert.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "[15] Nathan Halko, Per-Gunnar Martinsson, and Joel A Tropp.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "[16] Prateek Jain, Raghu Meka, and Inderjit S Dhillon.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "[17] Raghunandan H Keshavan, Andrea Montanari, and Sewoong Oh.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "[18] Raghunandan H Keshavan, Andrea Montanari, and Sewoong Oh.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "[19] Yehuda Koren, Robert Bell, and Chris Volinsky.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "[20] Shiqian Ma, Donald Goldfarb, and Lifeng Chen.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "[21] Benjamin Recht.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "[22] Benjamin Recht, Maryam Fazel, and Pablo A Parrilo.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 22,
      "context" : "[23] Shai Shalev-Shwartz and Shai Ben-David.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "[24] Stanislaw J Szarek.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "[25] Michel Talagrand.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 25,
      "context" : "[26] Kim-Chuan Toh and Sangwoon Yun.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 5,
      "context" : "1 from [6].",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 5,
      "context" : "1 from [6].",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 6,
      "context" : "We start with a lemma from [7].",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 24,
      "context" : "We next use a result from large deviations theory [25]:",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 5,
      "context" : "2 in [6].",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 23,
      "context" : "[24] Let A ∈ Rn×k be a random matrix A i.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 22,
      "context" : "We also use the following lemma from [23]:",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 10,
      "context" : "Lemma 7 is a direct result of the Johnson-Lindenstrauss lemma [11] applied to each vector in Q and using the union bound.",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 14,
      "context" : "6 Low Rank matrix Approximation We bring here the one pass algorithm to approximate X from [15] for the convenience of the reader.",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 16,
      "context" : "We compare our method to the rank estimation which appears in OptSpace [17] for the standard MC problem.",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 16,
      "context" : "(10) in the main text, and for the MC model we used the method described in [17].",
      "startOffset" : 76,
      "endOffset" : 80
    } ],
    "year" : 2015,
    "abstractText" : "We propose and study a row-and-column affine measurement scheme for lowrank matrix recovery. Each measurement is a linear combination of elements in one row or one column of a matrix X . This setting arises naturally in applications from different domains. However, current algorithms developed for standard matrix recovery problems do not perform well in our case, hence the need for developing new algorithms and theory for our problem. We propose a simple algorithm for the problem based on Singular Value Decomposition (SV D) and least-squares (LS), which we term SVLS . We prove that (a simplified version of) our algorithm can recover X exactly with the minimum possible number of measurements in the noiseless case. In the general noisy case, we prove performance guarantees on the reconstruction accuracy under the Frobenius norm. In simulations, our row-andcolumn design and SVLS algorithm show improved speed, and comparable and in some cases better accuracy compared to standard measurements designs and algorithms. Our theoretical and experimental results suggest that the proposed rowand-column affine measurements scheme, together with our recovery algorithm, may provide a powerful framework for affine matrix reconstruction.",
    "creator" : "LaTeX with hyperref package"
  }
}