{"conference": "ICLR 2017 conference submission", "title": "Density estimation using Real NVP", "abstract": "Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.", "reviews": [{"is_meta_review": true, "comments": "Building on earlier work on a model called NICE, this paper presents an approach to constructing deep feed-forward generative models. The model is evaluated on several datasets. While it does not achieve state-of-the-art performance, it advances an interesting class of models. The paper is mostly well written and clear.\n\nGiven that inference and generation are both efficient and exact, and given that this represents a main advantage over other models, it would be great if the authors could provide some motivating example applications where this is needed/would be useful.\n\nThe authors claim that \u201cunlike both variational autoencoders and GANs, our technique is able to learn a semantically meaningful latent space which is as high dimensional as the input space.\u201d Where is the evidence for this claim? I didn\u2019t see any analysis of the semantic meaningfulness of the latent space learned by real NVP. Stronger evidence that the learned representations are actually useful for downstream tasks would be nice.\n\nI still think the author\u2019s intuitions around the \u201cfixed reconstruction cost of L2\u201d are very vague. The factorial Gaussian assumption itself does not limit the generative model, it merely smoothes an otherwise arbitrary distribution, and to a degree which can be arbitrarily small, p(x) = \\int p(z) N(x | f(z), \\sigma^2) dz. How a lose lower bound plays into this is not clear from the paper.", "IS_META_REVIEW": true}, {"TITLE": "ICLR committee final decision", "OTHER_KEYS": "ICLR 2017 pcs", "comments": "This paper clearly lays out the recipe for a family of invertible density models, explores a few extensions, and demonstrates the overall power of the approach. The work is building closely on previous approaches, so it suffers a bit in terms of originality. However, I expect this overall approach to become a standard tool for model-building.\n \n The main weakness of this paper is that it did not explore the computational tradeoffs of this approach against related methods. However, the paper already had a lot of content.", "IS_META_REVIEW": false, "DATE": "06 Feb 2017", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "TITLE": "Interesting and important work", "is_meta_review": false, "comments": "This paper presents a clever way of training a generative model which allows for exact inference, sampling and log likelihood evaluation. The main idea here is to make the Jacobian that comes when using the change of variables formula (from data to latents) triangular - this makes the determinant easy to calculate and hence learning possible. \nThe paper nicely presents this core idea and a way to achieve this - by choosing special \"routings\" between the latents and data such that part of the transformation is identity and part some complex function of the input (a deep net, for example) the resulting Jacobian has a tractable structure. This routing can be cascaded to achieve even more complex transformation. \n\nOn the experimental side, the model is trained on several datasets and the results are quite convincing, both in sample quality and quantitive measures. \nI would be very happy to see if this model is useful with other types of tasks and if the resulting latent representation help with classification or inference such as image restoration.\n\nIn summary - the paper is nicely written, results are quite good and the model is interesting - I'm happy to recommend acceptance.\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Interesting ideas, great empirical work, overall good contribution", "is_meta_review": false, "comments": "This paper proposes a new generative model that uses real-valued non-volume preserving transformations in order to achieve efficient and exact inference and sampling of data points.\nThe authors use the change-of-variable technique to obtain a model distribution of the data from a simple prior distribution on a latent variable. By carefully designing the bijective function used in the change-of-variable technique, they obtain a Jacobian that is triangular and allows for efficient computation.\n\nGenerative models with tractable inference and efficient sampling are an active research area and this paper definitely contributes to this field.\n\nWhile not achieving state-of-the-art, they are not far behind. This doesn't change the fact that the proposed method is innovative and worth exploring as it tries to bridge the gap between auto-regressive models, variational autoencoders and generative adversarial networks.\n\nThe authors clearly mention the difference and similarities with other types of generative models that are being actively researched.\nCompared to autoregressive models, the proposed approach offers fast sampling.\nCompared to generative adversarial networks, Real NVP offers a tractable log-likelihood evaluation.\nCompared to variational autoencoders, the inference is exact.\nCompared to deep Boltzmann machines, the learning of the proposed method is tractable.\nIt is clear that Real NVP goal is to bridge the gap between existing and popular generative models.\n\nThe paper presents a lot of interesting experiments showing the capabilities of the proposed technique. Making the code available online will certainly contribute to the field. Is there any intention of releasing the code?\n\nTypo: (Section 3.7) We also \"use apply\" batch normalization", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "No Title", "is_meta_review": false, "comments": "Building on earlier work on a model called NICE, this paper presents an approach to constructing deep feed-forward generative models. The model is evaluated on several datasets. While it does not achieve state-of-the-art performance, it advances an interesting class of models. The paper is mostly well written and clear.\n\nGiven that inference and generation are both efficient and exact, and given that this represents a main advantage over other models, it would be great if the authors could provide some motivating example applications where this is needed/would be useful.\n\nThe authors claim that \u201cunlike both variational autoencoders and GANs, our technique is able to learn a semantically meaningful latent space which is as high dimensional as the input space.\u201d Where is the evidence for this claim? I didn\u2019t see any analysis of the semantic meaningfulness of the latent space learned by real NVP. Stronger evidence that the learned representations are actually useful for downstream tasks would be nice.\n\nI still think the author\u2019s intuitions around the \u201cfixed reconstruction cost of L2\u201d are very vague. The factorial Gaussian assumption itself does not limit the generative model, it merely smoothes an otherwise arbitrary distribution, and to a degree which can be arbitrarily small, p(x) = \\int p(z) N(x | f(z), \\sigma^2) dz. How a lose lower bound plays into this is not clear from the paper.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Likelihood vs L2 reconstruction", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "is_meta_review": false}, {"TITLE": "Latent space interpolation, batch norm with moving average and close train/valid results", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_META_REVIEW": false, "DATE": "01 Dec 2016", "is_meta_review": false}, {"TITLE": "Equation 18 - missing derivatives of batch statistics?", "OTHER_KEYS": "(anonymous)", "comments": "In equations 17-18 the quantities $\\mu$ and $\\sigma$ depend on the current batch $x$. Shouldn't their derivatives with respect to $x$ be part of the Jacobian?", "IS_META_REVIEW": false, "DATE": "22 Nov 2016", "is_meta_review": false}], "SCORE": 7, "authors": "Laurent Dinh, Jascha Sohl-Dickstein, Samy Bengio", "KEYWORDS": "Efficient invertible neural networks for density estimation and generation", "accepted": true, "id": ""}
