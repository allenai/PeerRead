{"conference": "ICLR 2017 conference submission", "title": "A Convolutional Encoder Model for Neural Machine Translation", "abstract": "The prevalent approach to neural machine translation relies on bi-directional LSTMs to encode the source sentence. In this paper we present a faster and simpler architecture based on a succession of convolutional layers.  This allows to encode the entire source sentence simultaneously compared to recurrent networks for which computation is constrained by temporal dependencies. On WMT'16 English-Romanian translation we achieve competitive accuracy to the state-of-the-art and we outperform several recently published results on the WMT'15 English-German task.  Our models obtain almost the same accuracy as a very deep LSTM setup on WMT'14 English-French translation. Our convolutional encoder speeds up CPU decoding by more than two times at the same or higher accuracy as a strong bi-directional LSTM baseline.", "reviews": [{"is_meta_review": true, "comments": "This paper is the first (I believe) to establish a simple yet important result that Convnets for NMT encoders can be competitive to RNNs. The authors present a convincing set of results over many translation tasks and compare with very competitive baselines. I also appreciate the detailed report on training and generation speed. I find it's very interesting when position embeddings turn out to be hugely important (beside residual connections); unfortunately, there is little analysis to shed more lights on this aspect and perhaps compare other ways of capturing positions (a wild guess might be to use embeddings that represent some form of relative positions). The only concern I have (similar to the other reviewer) is that this paper perhaps fits better in an NLP conference.\n\nOne minor comment: it's slight strange that this well-executed paper doesn't have a single figure on the proposed architecture :) It will also be even better to draw a figure for the biLSTM architecture as well (it does take some effort to understand the last paragraph in Section 2, especially the part on having a linear layer to compute z).", "IS_META_REVIEW": true}, {"TITLE": "ICLR committee final decision", "OTHER_KEYS": "ICLR 2017 pcs", "comments": "This work demonstrates architectural choices to make conv nets work for NMT. In general the reviewers liked the work and were convinced by the results but found the main contributions to be \"incremental\". \n \n Pros:\n - Clarity: The work was clearly presented, and besides for minor comments (diagrams) the reviewers understood the work\n - Quality: The experimental results were thorough, \"very extensive and leaves no doubt that the proposed approach works well\".\n \n Mixed:\n - Novelty: There is appreciation that the work is novel. However as the work is somewhat \"application-specific\" the reviewers felt the technical contribution was not an overwhelming contribution.\n - Impact: While some speed ups were shown, not all reviewers were convinced that the benefit was sufficient, or \"main speed-up factor(s)\" were. \n \n This work is clearly worthwhile, but the reviews place it slightly below the top papers in this area.", "IS_META_REVIEW": false, "DATE": "06 Feb 2017", "is_meta_review": false}, {"TITLE": "Authors: Please post a rebuttal", "OTHER_KEYS": "ICLR 2017 conference", "comments": "Authors, It would be great to have a rebuttal for this paper as reviewers will be discussing over the next week. Thanks.", "IS_META_REVIEW": false, "DATE": "07 Jan 2017", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "TITLE": "Final Review: good paper applying CNN to translation to match bi-LSTM baseline", "is_meta_review": false, "comments": "The system described works comparably to bi-directional LSTM baseline for NMT, and CNN's are naturally parallelizable.\n\nKey ideas include the use of two stacked CNN's (one for each of encoding and decoding) for translation, with res connections and position embeddings. The use of CNN's for translation has been attempted previously (as described by the authors), but presumably it is the authors' combination of various architectural choices (attention, position embeddings, etc) that make the present system competitive with RNN's, whereas earlier attempts were not. They describe system's sensitivity to some of these choices (e.g. experiments to choose appropriate number of layers in each of the CNN's).\n\nThe experimental results are well reported in detail.\n\nOne or two figures would definitely be required to help clarify the architecture.\n\nThis paper is less about new ways of learning representations than about the combination of choices made (over the set of existing techniques) in order to get the good results that they do on the reported NMT tasks. In this respect, while I am fairly confident that the paper represents good work in machine learning, I am not quite as confident about its fit for this particular conference.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "19 Dec 2016 (modified: 25 Jan 2017)", "REVIEWER_CONFIDENCE": 3}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "A well-executed NLP paper", "is_meta_review": false, "comments": "This paper is the first (I believe) to establish a simple yet important result that Convnets for NMT encoders can be competitive to RNNs. The authors present a convincing set of results over many translation tasks and compare with very competitive baselines. I also appreciate the detailed report on training and generation speed. I find it's very interesting when position embeddings turn out to be hugely important (beside residual connections); unfortunately, there is little analysis to shed more lights on this aspect and perhaps compare other ways of capturing positions (a wild guess might be to use embeddings that represent some form of relative positions). The only concern I have (similar to the other reviewer) is that this paper perhaps fits better in an NLP conference.\n\nOne minor comment: it's slight strange that this well-executed paper doesn't have a single figure on the proposed architecture :) It will also be even better to draw a figure for the biLSTM architecture as well (it does take some effort to understand the last paragraph in Section 2, especially the part on having a linear layer to compute z).", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Good paper, but very incremental", "is_meta_review": false, "comments": "The paper reports a very clear and easy to understand result that a convolutional network can be used instead of the recurrent encoder for neural machine translation. \n\nApart from the known architectural elements, such as convolution, pooling, residual connections, position embeddings, the paper features one unexpected architectural twist: two stacks of convolutions, one for computing alignment and another for computing the representations.\nThe empirical evidence that this was necessary is provided, however the question of *why* it is necessary remains open. \n\nThe experimental evaluation is very extensive and leaves no doubt that the proposed approach works well. The convnet-based model was faster at evaluation, but it is not very clear what is the main speed-up factor. It\u2019s however hard to argue against the fact that the speed advantage of convnets is likely to increase if a more parallel implementation is considered. \n\nMy main concern is whether or not the paper is appropriate for ICLR, because the contribution is quite incremental and rather application-specific. ACL, EMNLP and other NLP conferences would be a better venue, I think. ", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "A few questions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "IS_META_REVIEW": false, "DATE": "27 Nov 2016", "is_meta_review": false}, {"TITLE": "Small typo", "OTHER_KEYS": "Ozan Caglayan", "comments": "Hi,\n\nThe conditional input is denoted by both c_i and c_t in sections 2 and 3.1 respectively. c_i is reused in section 3.2\n", "IS_META_REVIEW": false, "DATE": "17 Nov 2016", "is_meta_review": false}, {"TITLE": "multi-bleu.perl", "OTHER_KEYS": "Rico Sennrich", "comments": "I want to draw attention to the fact that you compare tokenized BLEU results (with multi-bleu.perl) and detokenized BLEU results (with mteval-v13a.pl). The two should *never* be mixed in the same table, as the tokenization will have a big effect on results. Even when comparing systems that all have tokenized BLEU, and all use the Moses tokenizer, using different parameters (such as the \"-a\" option for aggressive hyphen splitting) will skew the results.\n\nDetokenized BLEU is standard for WMT, and reported by Sennrich et al. (2016a,b).\n\nI re-ran BLEU on our EN-RO system for comparison:\n\ndetokenized BLEU, mteval-v13a.pl: 28.1 BLEU\ntokenized BLEU, multi-bleu.perl: 29.4 BLEU\n\nyour reported result (multi-bleu.perl): 28.5", "IS_META_REVIEW": false, "DATE": "16 Nov 2016", "is_meta_review": false}], "SCORE": 6, "authors": "Jonas Gehring, Michael Auli, David Grangier, Yann N. Dauphin", "KEYWORDS": "Investigate encoder models for translation and demonstrate that convolutions can outperform LSTMs as encoders.", "accepted": false, "id": ""}
