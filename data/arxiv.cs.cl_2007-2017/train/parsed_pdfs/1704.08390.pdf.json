{
  "name" : "1704.08390.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "yanxx418@d.umn.edu", "tpederse@d.umn.edu", "@midnight" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 4.\n08 39\n0v 1\n[ cs\n.C L\n] 2\n7 A\npr 2\n01 7\nThis paper describes the Duluth system that participated in SemEval-2017 Task 6 #HashtagWars: Learning a Sense of Humor. The system participated in Subtasks A and B using N-gram language models, ranking highly in the task evaluation. This paper discusses the results of our system in the development and evaluation stages and from two post-evaluation runs."
    }, {
      "heading" : "1 Introduction",
      "text" : "Humor is an expression of human uniqueness and intelligence and has drawn attention in diverse areas such as linguistics, psychology, philosophy and computer science. Computational humor draws from all of these fields and is a relatively new area of study. There is some history of systems that are able to generate humor (e.g., (Stock and Strapparava, 2003), (Özbal and Strapparava, 2012)). However, humor detection remains a less explored and challenging problem (e.g., (Mihalcea and Strapparava, 2006), (Zhang and Liu, 2014), (Shahaf et al., 2015), (Miller and Gurevych, 2015)).\nSemEval-2017 Task 6 (Potash et al., 2017) also focuses on humor detection by asking participants to develop systems that learn a sense of humor from the Comedy Central TV show, @midnight with Chris Hardwick. Our system ranks tweets according to how funny they are by training N-gram language models on two different corpora. One consisting of funny tweets provided by the task organizers, and the other on a freely available research corpus of news data. The funny tweet data is made up of tweets that are intended to be humorous responses to a hashtag given by host Chris Hardwick during the program."
    }, {
      "heading" : "2 Background",
      "text" : "Training Language Models (LMs) is a straightforward way to collect a set of rules by utilizing the fact that words do not appear in an arbitrary order; we in fact can gain useful information about a word by knowing the company it keeps (Firth, 1968). A statistical language model estimates the probability of a sequence of words or an upcoming word. An N-gram is a contiguous sequence of N words: a unigram is a single word, a bigram is a two-word sequence, and a trigram is a three-word sequence. For example, in the tweet\ntears in Ramen #SingleLifeIn3Words\n“tears”, “in”, “Ramen” and “#SingleLifeIn3Words” are unigrams; “tears in”, “in Ramen” and “Ramen #SingleLifeIn3Words” are bigrams and “tears in Ramen” and “in Ramen #SingleLifeIn3Words” are trigrams.\nAn N-gram model can predict the next word from a sequence of N-1 previous words. A trigram Language Model (LM) predicts the conditional probability of the next word using the following approximation:\nP (wn|w n−1 1 ) ≈ P (wn|wn−2, wn−1) (1)\nThe assumption that the probability of a word depends only on a small number of previous words is called a Markov assumption (Markov, 2006). Given this assumption the probability of a sentence can be estimated as follows:\nP (wn1 ) ≈ n∏\nk=1\nP (wk|wk−2, wk−1) (2)\nIn a study on how phrasing affects memorability, (Danescu-Niculescu-Mizil et al., 2012) take a language model approach to measure the distinctiveness of memorable movie quotes. They do this\nby evaluating a quote with respect to a “common language” model built from the newswire sections of the Brown corpus (Kucera and Francis, 1967). They find that movie quotes which are less like “common language” are more distinctive and therefore more memorable. The intuition behind our approach is that humor should in some way be memorable or distinct, and so tweets that diverge from a “common language” model would be expected to be funnier.\nIn order to evaluate how funny a tweet is, we train language models on two datasets: the tweet data and the news data. Tweets that are more probable according to the tweet data language model are ranked as being funnier. However, tweets that have a lower probability according to the news language model are considered the funnier since they are the least like the (unfunny) news corpus. We relied on both bigrams and trigrams when training our models.\nWe use KenLM (Heafield et al., 2013) as our language modeling tool. Language models are estimated using modified Kneser-Ney smoothing without pruning. KenLM also implements a backoff technique so if an N-gram is not found, KenLM applies the lower order N-gram’s probability along with its back-off weights."
    }, {
      "heading" : "3 Method",
      "text" : "Our system1 estimated tweet probability using Ngram LMs. Specifically, it solved the comparison (Subtask A) and semi-ranking (Subtask B) subtasks in four steps:\n1. Corpus preparation and pre-processing: Col-\nlected all training data into a single file. Preprocessing included filtering and tokenization.\n2. Language model training: Built N-gram lan-\nguage models using KenLM.\n3. Tweet scoring: Computed log probability for\neach tweet based on trained N-gram language model.\n4. Tweet prediction: Based on the log probabil-\nity scores.\n• Subtask A – Given two tweets, compare and predict which one is funnier.\n1https://xinru1414.github.io/HumorDetectionSemEval2017-Task6/\n• Subtask B – Given a set of tweets associated with one hashtag, rank tweets from\nthe funniest to the least funny."
    }, {
      "heading" : "3.1 Corpus Preparation and Pre-processing",
      "text" : "The tweet data was provided by the task organizers. It consists of 106 hashtag files made up of about 21,000 tokens. The hashtag files were further divided into a development set trial dir of 6 hashtags and a training set of 100 hashtags train dir. We also obtained 6.2 GB of English news data with about two million tokens from the News Commentary Corpus and the News Crawl Corpus from 2008, 2010 and 20112. Each tweet and each sentence from the news data is found on a single line in their respective files."
    }, {
      "heading" : "3.1.1 Preparation",
      "text" : "During the development of our system we trained our language models solely on the 100 hashtag files from train dir and then evaluated our performance on the 6 hashtag files found in trial dir. That data was formatted such that each tweet was found on a single line."
    }, {
      "heading" : "3.1.2 Pre-processing",
      "text" : "Pre-processing consists of two steps: filtering and tokenization. The filtering step was only for the tweet training corpus. We experimented with various filtering and tokenziation combinations during the development stage to determine the best setting.\n• Filtering removes the following elements from the tweets: URLs, tokens starting with\nthe “@” symbol (Twitter user names), and tokens starting with the “#” symbol (Hashtags).\n• Tokenization: Text in all training data was split on white space and punctuation"
    }, {
      "heading" : "3.2 Language Model Training",
      "text" : "Once we had the corpora ready, we used the KenLMToolkit to train the N-gram language models on each corpus. We trained using both bigrams and trigrams on the tweet and news data. Our language models accounted for unknown words and were built both with and without considering sentence or tweet boundaries.\n2http://www.statmt.org/wmt11/featured-translationtask.html"
    }, {
      "heading" : "3.3 Tweet Scoring",
      "text" : "After training the N-gram language models, the next step was scoring. For each hashtag file that needed to be evaluated, the logarithm of the probability was assigned to each tweet in the hashtag file based on the trained language model. The larger the probability, the more likely that tweet was according to the language model. Table 1 shows an example of two scored tweets from hashtag file Bad Job In 5 Words.tsv based on the tweet data trigram language model. Note that KenLM reports the log of the probability of the N-grams rather than the actual probabilities so the value closer to 0 (-19) has the higher probability and is associated with the tweet judged to be funnier."
    }, {
      "heading" : "3.4 Tweet Prediction",
      "text" : "The system sorts all the tweets for each hashtag and orders them based on their log probability score, where the funniest tweet should be listed first. If the scores are based on the tweet language model then they are sorted in ascending order since the log probability value closest to 0 indicates the tweet that is most like the (funny) tweets model. However, if the log probability scores are based on the news data then they are sorted in descending order since the largest value will have the smallest probability associated with it and is therefore least like the (unfunny) news model.\nFor Subtask A, the system goes through the sorted list of tweets in a hashtag file and compares each pair of tweets. For each pair, if the first tweet was funnier than the second, the system would output the tweet ids for the pair followed by a “1”. If the second tweet is funnier it outputs the tweet ids followed by a “0”. For Subtask B, the system outputs all the tweet ids for a hashtag file starting from the funniest."
    }, {
      "heading" : "4 Experiments and Results",
      "text" : "In this section we present the results from our development stage (Table 2), the evaluation stage (Table 3), and two post-evaluation results (Table 3). Since we implemented both bigram and trigam language models during the development stage but only results from trigram language models were submitted to the task, we evaluated bigram language models in the post-evaluation stage. Note that the accuracy and distance measurements listed in Table 2 and Table 3 are defined by the task organizers (Potash et al., 2017).\nTable 2 shows results from the development stage. These results show that for the tweet data the best setting is to keep the # and @, omit sentence boundaries, be case sensitive, and ignore tokenization. While using these settings the trigram language model performed better on Subtask B (.887) and the bigram language model performed better on Subtask A (.548). We decided to rely on trigram language models for the task evaluation since the advantage of bigrams on Subtask A was very slight (.548 versus .543). For the news data, we found that the best setting was to perform tokenization, omit sentence boundaries, and to be case sensitive. Given that trigrams performed most effectively in the development stage, we decided to use those during the evaluation.\nTable 3 shows the results of our system during the task evaluation. We submitted two runs, one with a trigram language model trained on the tweet data, and another with a trigram language model trained on the news data. In addition, after the evaluation was concluded we also decided to run the bigram language models as well. Contrary to what we observed in the development data, the bigram language model actually performed somewhat better than the trigram language model. In addition, and also contrary to what we observed with the development data, the news data proved generally more effective in the post–evaluation runs than the tweet data."
    }, {
      "heading" : "5 Discussion and Future Work",
      "text" : "We relied on bigram and trigram language models because tweets are short and concise, and often only consist of just a few words.\nThe performance of our system was not consistent when comparing the development to the evaluation results. During development language models trained on the tweet data performed better. However during the evaluation and postevaluation stage, language models trained on the news data were significantly more effective. We also observed that bigram language models performed slightly better than trigram models on the evaluation data. This suggests that going forward we should also consider both the use of unigram and character–level language models.\nThese results suggest that there are only slight differences between bigram and trigram models, and that the type and quantity of corpora used to train the models is what really determines the re-\nsults.\nThe task description paper (Potash et al., 2017) reported system by system results for each hashtag. We were surprised to find that our performance on the hashtag file #BreakUpIn5Words in the evaluation stage was significantly better than any other system on both Subtask A (with accuracy of 0.913) and Subtask B (with distance score of 0.636). While we still do not fully understand the cause of these results, there is clearly something about the language used in this hashtag that is distinct from the other hashtags, and is somehow better represented or captured by a language model. Reaching a better understanding of this result is a high priority for future work.\nThe tweet data was significantly smaller than the news data, and so certainly we believe that this\nwas a factor in the performance during the evaluation stage, where the models built from the news data were significantly more effective. Going forward we plan to collect more tweet data, particularly those that participate in #HashtagWars. We also intend to do some experiments where we cut the amount of news data and then build models to see how those compare.\nWhile our language models performed well, there is some evidence that neural network models can outperform standard back-off N-gram models (Mikolov et al., 2011). We would like to experiment with deep learning methods such as recurrent neural networks, since these networks are capable of forming short term memory and may be better suited for dealing with sequence data."
    } ],
    "references" : [ {
      "title" : "You had me at hello: How phrasing affects memorability",
      "author" : [ "Cristian Danescu-Niculescu-Mizil", "Justin Cheng", "Jon Kleinberg", "Lillian Lee." ],
      "venue" : "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers",
      "citeRegEx" : "Danescu.Niculescu.Mizil et al\\.,? 2012",
      "shortCiteRegEx" : "Danescu.Niculescu.Mizil et al\\.",
      "year" : 2012
    }, {
      "title" : "A synopsis of linguistic theory 19301955",
      "author" : [ "J. Firth." ],
      "venue" : "F. Palmer, editor, Selected Papers of J. R. Firth, Longman.",
      "citeRegEx" : "Firth.,? 1968",
      "shortCiteRegEx" : "Firth.",
      "year" : 1968
    }, {
      "title" : "Scalable modified Kneser-Ney languagemodel estimation",
      "author" : [ "Kenneth Heafield", "Ivan Pouzyrevsky", "Jonathan H. Clark", "Philipp Koehn." ],
      "venue" : "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics. Sofia, Bulgaria,",
      "citeRegEx" : "Heafield et al\\.,? 2013",
      "shortCiteRegEx" : "Heafield et al\\.",
      "year" : 2013
    }, {
      "title" : "Computational Analysis of Present-day American English",
      "author" : [ "Henry Kucera", "W. Nelson Francis." ],
      "venue" : "Brown University Press, Providence, RI, USA.",
      "citeRegEx" : "Kucera and Francis.,? 1967",
      "shortCiteRegEx" : "Kucera and Francis.",
      "year" : 1967
    }, {
      "title" : "An example of statistical investigation of the text Eugene Onegin concerning the connection of samples in chains",
      "author" : [ "A.A. Markov." ],
      "venue" : "Science in Context 19(4):591–600.",
      "citeRegEx" : "Markov.,? 2006",
      "shortCiteRegEx" : "Markov.",
      "year" : 2006
    }, {
      "title" : "Learning to laugh (automatically): Computational models for humor recognition",
      "author" : [ "Rada Mihalcea", "Carlo Strapparava." ],
      "venue" : "Computational Intelligence 22(2):126–142.",
      "citeRegEx" : "Mihalcea and Strapparava.,? 2006",
      "shortCiteRegEx" : "Mihalcea and Strapparava.",
      "year" : 2006
    }, {
      "title" : "Extensions of recurrent neural network language model",
      "author" : [ "Tomáš Mikolov", "Stefan Kombrink", "Lukáš Burget", "Jan Černockỳ", "Sanjeev Khudanpur." ],
      "venue" : "Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on.",
      "citeRegEx" : "Mikolov et al\\.,? 2011",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2011
    }, {
      "title" : "Automatic disambiguation of English puns",
      "author" : [ "Tristan Miller", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing",
      "citeRegEx" : "Miller and Gurevych.,? 2015",
      "shortCiteRegEx" : "Miller and Gurevych.",
      "year" : 2015
    }, {
      "title" : "A computational approach to the automation of creative naming",
      "author" : [ "Gözde Özbal", "Carlo Strapparava." ],
      "venue" : "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. Association for Com-",
      "citeRegEx" : "Özbal and Strapparava.,? 2012",
      "shortCiteRegEx" : "Özbal and Strapparava.",
      "year" : 2012
    }, {
      "title" : "SemEval-2017 Task 6: #HashtagWars: learning a sense of humor",
      "author" : [ "Peter Potash", "Alexey Romanov", "Anna Rumshisky." ],
      "venue" : "Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017). Vancouver, BC.",
      "citeRegEx" : "Potash et al\\.,? 2017",
      "shortCiteRegEx" : "Potash et al\\.",
      "year" : 2017
    }, {
      "title" : "Inside jokes: Identifying humorous cartoon",
      "author" : [ "Dafna Shahaf", "Eric Horvitz", "Robert Mankoff" ],
      "venue" : null,
      "citeRegEx" : "Shahaf et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Shahaf et al\\.",
      "year" : 2015
    }, {
      "title" : "Getting serious about the development of computational humor",
      "author" : [ "Oliviero Stock", "Carlo Strapparava." ],
      "venue" : "Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence. Acapulco, pages 59–64.",
      "citeRegEx" : "Stock and Strapparava.,? 2003",
      "shortCiteRegEx" : "Stock and Strapparava.",
      "year" : 2003
    }, {
      "title" : "Recognizing humor on Twitter",
      "author" : [ "Renxian Zhang", "Naishi Liu." ],
      "venue" : "Proceedings of the 23rd ACM International Conference on Conference on Information and KnowledgeManagement. ACM, New York, NY, USA, CIKM ’14, pages 889–898.",
      "citeRegEx" : "Zhang and Liu.,? 2014",
      "shortCiteRegEx" : "Zhang and Liu.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : ", (Stock and Strapparava, 2003), (Özbal and Strapparava, 2012)).",
      "startOffset" : 2,
      "endOffset" : 31
    }, {
      "referenceID" : 8,
      "context" : ", (Stock and Strapparava, 2003), (Özbal and Strapparava, 2012)).",
      "startOffset" : 33,
      "endOffset" : 62
    }, {
      "referenceID" : 5,
      "context" : ", (Mihalcea and Strapparava, 2006), (Zhang and Liu, 2014), (Shahaf et al.",
      "startOffset" : 2,
      "endOffset" : 34
    }, {
      "referenceID" : 12,
      "context" : ", (Mihalcea and Strapparava, 2006), (Zhang and Liu, 2014), (Shahaf et al.",
      "startOffset" : 36,
      "endOffset" : 57
    }, {
      "referenceID" : 10,
      "context" : ", (Mihalcea and Strapparava, 2006), (Zhang and Liu, 2014), (Shahaf et al., 2015), (Miller and Gurevych, 2015)).",
      "startOffset" : 59,
      "endOffset" : 80
    }, {
      "referenceID" : 7,
      "context" : ", 2015), (Miller and Gurevych, 2015)).",
      "startOffset" : 9,
      "endOffset" : 36
    }, {
      "referenceID" : 9,
      "context" : "SemEval-2017 Task 6 (Potash et al., 2017) also focuses on humor detection by asking participants to develop systems that learn a sense of humor from the Comedy Central TV show, @midnight with Chris Hardwick.",
      "startOffset" : 20,
      "endOffset" : 41
    }, {
      "referenceID" : 1,
      "context" : "Training Language Models (LMs) is a straightforward way to collect a set of rules by utilizing the fact that words do not appear in an arbitrary order; we in fact can gain useful information about a word by knowing the company it keeps (Firth, 1968).",
      "startOffset" : 236,
      "endOffset" : 249
    }, {
      "referenceID" : 4,
      "context" : "The assumption that the probability of a word depends only on a small number of previous words is called a Markov assumption (Markov, 2006).",
      "startOffset" : 125,
      "endOffset" : 139
    }, {
      "referenceID" : 0,
      "context" : "In a study on how phrasing affects memorability, (Danescu-Niculescu-Mizil et al., 2012) take a language model approach to measure the distinctiveness of memorable movie quotes.",
      "startOffset" : 49,
      "endOffset" : 87
    }, {
      "referenceID" : 3,
      "context" : "by evaluating a quote with respect to a “common language” model built from the newswire sections of the Brown corpus (Kucera and Francis, 1967).",
      "startOffset" : 117,
      "endOffset" : 143
    }, {
      "referenceID" : 2,
      "context" : "We use KenLM (Heafield et al., 2013) as our language modeling tool.",
      "startOffset" : 13,
      "endOffset" : 36
    }, {
      "referenceID" : 9,
      "context" : "Note that the accuracy and distance measurements listed in Table 2 and Table 3 are defined by the task organizers (Potash et al., 2017).",
      "startOffset" : 114,
      "endOffset" : 135
    }, {
      "referenceID" : 9,
      "context" : "The task description paper (Potash et al., 2017) reported system by system results for each hashtag.",
      "startOffset" : 27,
      "endOffset" : 48
    }, {
      "referenceID" : 6,
      "context" : "While our language models performed well, there is some evidence that neural network models can outperform standard back-off N-gram models (Mikolov et al., 2011).",
      "startOffset" : 139,
      "endOffset" : 161
    } ],
    "year" : 2017,
    "abstractText" : "This paper describes the Duluth system that participated in SemEval-2017 Task 6 #HashtagWars: Learning a Sense of Humor. The system participated in Subtasks A and B using N-gram language models, ranking highly in the task evaluation. This paper discusses the results of our system in the development and evaluation stages and from two post-evaluation runs.",
    "creator" : "LaTeX with hyperref package"
  }
}