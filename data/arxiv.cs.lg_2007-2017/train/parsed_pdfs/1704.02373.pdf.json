{
  "name" : "1704.02373.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "akc@es.aau.dk,", "zt@es.aau.dk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 4.\n02 37\n3v 1\n[ cs\n.S D\n] 6\nA pr\n2 01\n7\nbased unsupervised bottleneck (BN) feature extraction method for speech signals with an application to speaker verification. The method exploits the temporal structure of a speech signal and more specifically, it trains deep neural networks (DNNs) to discriminate temporal events obtained by uniformly segmenting the signal without using any label information, in contrast to conventional DNN based BN feature extraction methods that train DNNs using labeled data to discriminate speakers or passphrases or phones or a combination of them. We consider different strategies for TCL and its combination with transfer learning. Experimental results on the RSR2015 database show that the TCL method is superior to the conventional speaker and pass-phrase discriminant BN feature and Mel-frequency cepstral coefficients (MFCCs) feature for text-dependent speaker verification. The unsupervised TCL method further has the advantage of being able to leverage the huge amount of unlabeled data that are often available in real life. Index Terms: Unsupervised time-contrastive learning, DNNs, Bottleneck feature, Speaker verification"
    }, {
      "heading" : "1. Introduction",
      "text" : "Speaker verification is the task of either accepting or rejecting a person by his/her voice. It is broadly divided into two sub-categories: text-dependent (TD) and text-independent (TI) speaker verification (SV). In TD-SV, users are constrained to speak the same pass-phrase during both enrollment and test phases, whereas in TI-SV, speakers are free to speak any sentences (of text) during training and test phases. Since TD-SV relies on the same sentence, i.e. a matched phonetic content, during both training and test phrases, it outperforms the TI-SV counterpart for short speech utterances.\nSpeech is a quasi stationary signal and hence short-time based ceptral feature representations are mostly used in speaker [1] and speech recognition [2] systems. Recently, the deep neural network (DNN) concept [3] has gained great interest in the field of automatic speech and speaker recognition as DNNs are capable of modeling highly nonlinear structure of patterns in data. In the context of speaker verification, DNNs are found to be used either for extracting posteriori statistics [4, 5, 6] with respect to the pre-defined phonetic classes for i-vector extraction or for discriminative feature extraction, where a multi-layer DNN is trained with objective to discriminate speakers, passphrases, phonetic classes, phones or a combination of them. In case of i-vector extraction, a speech utterance is aligned against\nThe paper reflects some results from the OCTAVE Project (#647850), funded by the Research European Agency (REA) of the European Commission, in its framework programme Horizon 2020. The views expressed in this paper are those of the authors and do not engage any official position of the European Commission.\na DNN based automatic speech recognition (ASR) for sufficient statistics with respect to a pre-defined phonetic classes (called senones) to incorporate the phonetic knowledge in i-vectors. In case of feature extraction, the outputs of the DNN hidden layers are used to vectorize characterization of speech data, which are either directly used for speaker characterization called d-vector [7] analogous to the i-vector concept or projected onto a low dimensional space called bottleneck (BN) feature [8, 9, 10, 11] for speaker recognition. It has been demonstrated in [9, 10, 11] that DNN based systems either perform better than or provide complementary information to conventional short-time cepstral feature based speaker recognition systems.\nIn conventional BN feature extraction methods, DNNs are generally optimized to discriminate speakers or pass-phrases or phones or a combination of them in the training data. It is observed in [9] that the performances of TD-SV systems using BN features trained on discriminating speaker+pass-phrases or phonetic or speaker+phonetic are similar to each other. However, their performances are better than that of a cepstral feature. All of these DNN BN feature extraction methods exploit the labeling/supervision information of data. The success of these systems highly depend on obtaining good labeled data. They can be categorized as supervised or semi-supervised approaches. Unsupervised learning methods, although being challenging to deploy, are more attractive as they are able to take advantage of huge amount of unlabeled data available in real-life.\nInspired by the recently proposed time contrastive learning (TCL) algorithm for classification of EEG/MPEG data in [12], in this paper, we explore the TCL concept for speech signals aiming at finding out whether it is applicable for speech and what are the effective strategies. We present an unsupervised bottleneck feature extraction method for speech, which focuses on discriminating the temporal events across the speech signal with no need for any speaker or pass-phrase or phonetic labels in contrast to the conventional DNN based BN feature extraction approaches that discriminate speakers/passphrases/phones/both during training using labeled data. The main idea is to exploit the temporal non-stationary structure in the speech signal and discriminate the temporal events in an unsupervised manner. Specifically, speech utterances are first uniformly segmented into, e.g. N segments, and data-points (frames) within a particular segment are then considered belonging to a single class. N segments therefore constitute N classes. Next, a DNN is trained to discriminate the data of the different segments. Finally, the output of the DNN hidden layer is projected onto a low dimensional space to get BN features for speaker verification. Since we do not consider any speaker/pass-phrase/phonetic label/alignment information during the segmentation of speech utterances in the TCL approach, we call it an unsupervised DNN BN feature. Furthermore, We consider different strategies for TCL and its combination with transfer learning.\nWe compare the performance of the unsupervised TCL based BN feature with those of cepstral and conventional BN features in text-dependent speaker verification on RSR2015 consisting of short utterances. We show that the proposed BN feature gives better performance for TD-SV than cepstral and conventional BN features. Gaussian mixture model - universal background model (GMM-UBM) [13] technique is used for speaker verification, since it is well established that a GMM based classifier [14, 15] gives better performance in SV using short utterances than the i-vector [16]."
    }, {
      "heading" : "2. Conventional DNN bottleneck features",
      "text" : "Among conventional DNN bottleneck features, the BN feature in [9] is based on DNNs trained to optimize two cross-entropy based objective functions simultaneously: one for discriminating speakers and the other for discriminating pass-phrases. The output of the last DNN hidden layer is connected to two types of cross entropy nodes: one predicting speakers and the other predicting pass-phrases. The equally weighted combination of these two criteria is used as a final criterion and DNN multitask learning procedure is followed [17]. The output of a DNN hidden layer is used frame-level features called deep features. Bottleneck features are then obtained by projecting the high dimensional deep feature vectors onto a lower dimensional space. Fig.1 illustrates the conventional DNN based BN feature extraction method. It is observed in [9] that the performance of text-dependent speaker verification using BN features extracted based on the discrimination of both speaker and pass-phrases is similar to that of features based on discrimination of either speakers or speaker+phone. Among them, augmentation of a cepstral feature with the speaker+pass-phrase discriminant BN feature yields the lowest error rates. In this work, we consider speaker+pass-phrase discrimination and different hidden layers of the DNN for the conventional bottleneck feature extraction."
    }, {
      "heading" : "3. Time-contrastive learning",
      "text" : "In the TCL concept [12], multivariate time series data X are first divided into a number of uniform segments (say N ), and then all data-points within a particular segment are assigned to one class label as follows:\n(x1, xM ) ︸ ︷︷ ︸\nclass 1\n, . . . , (xiM+1, xiM+M ) ︸ ︷︷ ︸\n. . .\n, . . . , (x(N−1)M+1, xNM ) ︸ ︷︷ ︸\nclass N\nwhere i and M indicate the segment index and the number of data points within a segment for a particular data series, respectively. Finally, a DNN is trained to classify the data across the segments. The output of the last hidden layer is used as a feature. The method is evaluated on brain imaging data, specifically magnetoencephalography (MEG) signals, to classify the different states of brain and the task involves a classification of few number of classes (only four classes) [12]. Due to the significant difference between speech and MEG signals in nature, this work explores the potential of the TCL concept for speech feature extraction and its application to speaker verification as an example application."
    }, {
      "heading" : "4. Unsupervised DNN features",
      "text" : "Inspired by the TCL algorithm [12], we explore the concept on speech to generate bottleneck feature for speaker verification. Since the patterns varying across the speech signal highly depend on the contents of spoken words, we consider two TCL training approaches and their combinations with transfer learning."
    }, {
      "heading" : "4.1. TCL learning",
      "text" : "Similarly to the TCL algorithm [12], speech utterances are uniformly segmented into a number of pre-defined segments (say N ) regardless of speakers and contents, and data within a particular segment are assigned one class label distinct from the other segments. A DNN is then trained by pooling together all training utterances of different text contents and different speakers, to optimize the cross-entropy objective function for classifying the data across the different segments, i.e. discriminating the temporal events of the speech signal in unsupervised manner. Fig.2 illustrates the method.\nThe outputs of the DNN, i.e. deep features, are projected onto a low dimensional space to get the BN feature for speaker verification. The number of output nodes in the last layer of\nDNN is equal to the number of segments obtained by segmenting data into disjoint parts.\nUtterances for speaker verification are often of different lengths primarily due to different text contents. In this work, the data set for the TCL training is constructed by forming a set of utterances of equal lengths by truncating the utterances longer than a predefined length and discarding a minority number of utterances shorter than the predefined length. Each utterance in the constructed data set are then uniformly segmented to segments of 6 frames each.\nWe also consider a variant of the above mentioned TCL training method, in which the parameters of the DNN are updated by pooling only the utterances of the same text content together at a time. We call it TCL-seq. The number of output nodes is kept the same as TCL for all different types of text utterances. The motivation of this to see the impact of using homogeneous data for TCL training at each time, as compared to randomly mixing the training data in the aforementioned TCL method."
    }, {
      "heading" : "4.2. TCL transfer learning (TCL-tr)",
      "text" : "In order to reduce the intra class variability within a segment of the same class during DNN training, in contrast to the TCL approach above, the TCL transfer learning method pools utterances of the same text together at a time and updates the DNN parameters as follows:\nStep 1: Group the DNN training utterances based on their text\ncontents, i.e. g1, g2, . . . regardless of speakers. Although this implementation groups utterances with same text together to leverage the transfer learning, the DNN training is still based on the unsupervised TCL concept.\nStep 2: Read all utterances belonging to group g1, segment the\nutterances uniformly into a predefined number of segments n1 regardless of lengths and train an initial DNN to discriminate the data among the different segments. The number of output nodes in the last layer of DNN is n1. Fig. 3 illustrates the segmentation of data and definition of classes in unsupervised manner for a general case, namely group gi with ni segments.\nStep 3: Perform the DNN training for the utterances in group gj with a predefined number of uniform disjoint segments\nnj by using the model obtained by the previous step as an initial model (only modifying the number of output DNN nodes as per number of segments) as illustrated in Fig.4. Repeat this step until all of data groups are used for training.\nStep 4: Repeat training based on the transfer learning concept\nFinal DNN can be thought as representing the text independent model space as the model is trained using different contents of speech data in sequential manner. We consider different strategies to define the value of n (i.e. number of disjoint uniform segments) for different group of speech utterances. This gives several different features as follows:\nTCL transfer learning with variable number of segments (TCL-tr-Var): DNN training data are grouped based on the passphrases and the number of segments for a group is calculated based on the number of phones available in the pass-phrase.\nTCL transfer learning with variable number of segments + spkr+pass-phrase (TCL-tr-Var+spkr+phrase): This feature is similar to the TCL-tr-Var. Only difference is that additional speaker+pass-phrase discrimination training (as done in conventional DNNs) is performed, on top of the DNN model obtained through the TCL-tr-Var, by pooling all training data together. The number of output nodes in the last layer of DNN is equal to the number of speakers+pass-phrases. The motivation behind this is to see whether adding speaker+pass-phrases discrimination further improves the performance of the SV system as compared to TCL-tr-Var."
    }, {
      "heading" : "5. Experiment",
      "text" : "Experiments are conducted on male speakers of the RSR2015 database (evaluation set of part1 i.e. 3sess-pwd eval m task) [14] as per protocol. According to the protocol, there are 1708 speaker models to be trained. Three recording sessions are used to train each particular pass-phrase-wise target speaker model. The utterances are of very short duration on an average of 2-3s per speech signal. Test trials are divided into three types of nontarget: target wrong, imposter correct and imposter wrong for system performance evaluation. Table 1 shows the number of trails available for the system evaluation on RSR2015.\nDNN setup and training: All DNNs are 7 layer feedforward networks and are trained using the same learning rate and and number of epoch. Each hidden layer consists of 1024 sigmoid units. The DNNs are trained using a 627 dimensional feature vector based on MFCC (57 dimensional) features with a context window of 11 frames (i.e. 5 frames left, current frame, 5 frames right). A cross-entropy objective function is utilized to discriminate the classes in the output layer of the DNN. Training data include 97 (50 male, 47 female) non-target speakers (disjoint from evaluation) from the development set of the RSR2015 database. Each speaker has a recording of 30 pass-phrases over 9 sessions. It gives approximately 26, 132 utterances for training the DNNs. In the conventional DNN method, the number\nof nodes in the output layer is equal to the number of speakers + pass-phrases in the training data, resulting in 127 nodes (97 speakers, 30 pass-phrases). The proposed DNN system also uses the same training data. In the TCL learning, the training utterance length is limited to 120 frames by discarding a minority number of utterances with length smaller than 120 frames and truncating utterances with length larger than 120 frames, so all training utterances are of 120 frames. The utterances are then segmented to 20 disjoint segments, considering that majority of the utterances have about 20 phones (varying from 14 to 30 phones). CNTK toolkit [17] is used for implementing the DNN and bottleneck feature extraction.\nBottleneck feature: Outputs from the second (L2) and fourth (L4) hidden layers are used as bottleneck features for this study as in [9] since they show the best performance. It gives 1024 dimensional deep features which are then projected onto a 57 dimensional vector space to align the dimension to the MFCC feature for a fair comparison. The deep features are normalized to zero mean and unit variance at utterance level before projecting onto the lower dimensional vector space using principle component analysis (PCA).\nGender-dependent GMM-UBM (512 mixtures, having diagonal covariance matrix) is trained using non-target speakers (438 male) data (4380 utterances) from TIMIT database [18]. The UBM training data are also used for training PCA. Speaker models are derived from the GMM-UBM with maximum a posteriori (MAP) adaptation using their respective training data. In test phase, test utterance X = {x1, x2, . . . , xT } is scored against the target specific model (obtained in training) λr and GMM-UBM λubm. Finally, log likelihood ratio (LLR) value is calculated using the scores between the two models LLR(X) = 1 T ∑T t=1{log p(xt|λr) − log p(xt|λubm)}. Three iterations (with value of relevance factor 10.0) are used in MAP. Only Gaussian mean vectors of the GMM-UBM are adapted.\nFor spectral analysis, 57 dimensional MFCCs (with RASTA [19] filtering) consisting of static C1-C19 cepstra, with ∆ and ∆∆ coefficients are extracted from speech signals using 10 ms frame shift and a 20 ms Hamming window. An energy based Voice Activity Detection (VAD) is applied to remove the less energized frames. Then, the energized feature vector are normalized to zero mean and unit variance at utterance level. System performance is evaluated in terms of equal error rate (EER) and minimum detection cost function (MinDCF) [20]."
    }, {
      "heading" : "6. Results and discussion",
      "text" : "We compare the performance of the TCL based unsupervised BN features with baseline cepstral and DNN based bottleneck features in Table 2, presented in term of EER (%)/MinDCF\n(×100) (2008 SRE cost function). From Table 2, it can be seen that DNN BN features using fourth layer (L4) show lower error rates as compared to their counterparts based on L2. All BN features (L4) give lower average error rates and MinDCF values (across different types of non-targets) as compared to cepstral (MFCC) feature as in [9].\nFor L2, most of the TCL BN features obtain slightly lower error rates as compared to the conventional DNN BN feature feature. In case of L4, the TCL BN features (TCL, TCL-seq) obtain better/very close TD-SV performance as compared to the conventional BN feature.\nTCL and TCL-seq features show slightly lower error rates than the TCL-tr-Var feature. This indicates it is inefficient to initiate and train a new output layer for each set of utterances of the same sentence, i.e. the transfer learning. This can be database dependent and deserves further investigation. Furthermore, TCL shows better SV performance as compared to the TCL-seq. Incorporating speaker+pass-phrase discrimination in the TCL-tr-Var system further reduces the SV error rates as compared to that without.\nOverall, the TCL unsupervised DNN BN feature is effective for text-dependent speaker verification. Moreover, the TCL method does not use any speaker/pass-phrase/phonetic specific label information and is able to take advantage of huge amount of unlabeled data available, which represents a huge advantage."
    }, {
      "heading" : "7. Conclusion",
      "text" : "In this paper, we explored an unsupervised time-contrastive learning (TCL) method for training DNN based BN features for speaker verification, in which DNNs are trained to discriminate the temporal events across a speech signal. This is realized by uniformly segmenting the speech signal into a number of segments and assigning the same label to all speech frames in one segment but different labels to different segments. DNNs are then trained to discriminate data across the different time segments without any speaker or phonetic label in contrast to the conventional DNN BN feature extraction approaches which focus on discriminating speaker/pass-phrase/phone/both information in training using the labeled data. The output layer of the DNN is used for BN feature extraction. We consider different strategies for training the DNNs and yields different BN features. Experimental results on the RSR2015 database show that the TCL BN features outperform the conventional speaker and pass-phrase BN and cepstral features for TD-SV. This work confirms the feasibility of the TCL method for speech feature extraction. Besides its good performance, the TCL approach, as an unsupervised one, further has the big advantage of no need for labeled data."
    }, {
      "heading" : "8. References",
      "text" : "[1] T. Kinnunen and H. Li, “An Overview of Text-independent Speaker Recognition: from Features to Supervectors,” Speech Communication, vol. 52, pp. 12–40, 2010.\n[2] Z.-H. Tan and B. Lindber, “Low-Complexity Variable Frame Rate Analysis for Speech Recognition and Voice Activity Detection,” IEEE Journal of Selected Topics in Signal Processing, vol. 4, pp. 798–807, 2010.\n[3] G. Hinton et al., “Deep Neural Networks for Acoustic Modeling in Speech Recognition,” in IEEE Signal Process. Mag., 2012, pp. 82–97.\n[4] M. McLaren, Y. Lei, and L. Ferrer, “Advances in Deep Neural Network Approaches to Speaker Recognition,” in Proc. of IEEE Int. Conf. Acoust. Speech Signal Processing (ICASSP), 2015.\n[5] P. Kenny, V. Gupta, T. Stafylakis, P. Ouellet, and J. Alam, “Deep Neural Networks for Extracting Baum-welch Statistics for Speaker Recognition,” in Proc. of Odyssey Speaker and Language Recognition Workshop, 2014, pp. 293–298.\n[6] E. Dahl, D. Yu, L. Deng, , and A. Acero, “Context-dependent Pre-trained Deep Neural Networks for Large Vocabulary Speech Recognition,” IEEE Transactions on Audio, Speech, and Language Processing, vol. 20, pp. 30–42, 2012.\n[7] E. Variani, X. Lei, E. McDermott, I. Lopez-Moreno, and J. Gonzalez-Dominguez, “Deep Neural Networks for Small Footprint Text-dependent Speaker Verification,” in Proc. of IEEE Int. Conf. Acoust. Speech Signal Processing (ICASSP), 2014, pp. 4080–4084.\n[8] T. Fu, Y. Qian, Y. Liu, and Kai Yu, “Tandem Deep Features for Text-dependent Speaker Verification,” in Proc. of Interspeech, 2014, pp. 1327–1331.\n[9] Y. Liu, Y. Qian, N. Chen, T. Fu, Y. Zhang, and K. Yu, “Deep Feature for Text-dependent Speaker Verification,” Speech Communication, vol. 73, pp. 1–13, 2015.\n[10] C.-T. Do, C. Barras, V.-B. Le, and A. K. Sarkar, “Augmenting Short-term Cepstral Features with Long-term Discriminative Features for Speaker Verification of Telephone Data,” in Proc. of Interspeech, 2013, pp. 2484–2488.\n[11] S. Ghalehjegh and R. Rose, “Deep Bottleneck Features for ivector based Text-independent Speaker Verification,” in Proc. of IEEE Workshop on Automatic Speech Recognition and Under-\nstanding (ASRU), 2015, pp. 555–560.\n[12] A. Hyvarinen and H. Morioka, “Unsupervised Feature Extraction by Time-Contrastive Learning and Nonlinear ICA,” in in Proc. of Neural Information Processing systems (NIPS), 2016.\n[13] D. A. Reynolds, T. F. Quatieri, and R. B. Dunn, “Speaker Verification using Adapted Gaussian Mixture Models,” Digital Signal Processing, vol. 10, pp. 19–41, 2000.\n[14] A. Larcher, K. A. Lee, B. Ma, and H. Li, “Text-dependent Speaker Verification: Classifiers, Databases and RSR2015,” Speech Communication, vol. 60, pp. 56–77, 2014.\n[15] H. Delgado, M. Todisco, M. Sahidullah, A. K. Sarkar, N. Evans, T. Kinnunen, and Z.-H. Tan, “Further Optimisations of Constant Q Cepstral Processing for Integrated Utterance and Textdependent Speaker Verification,” in in Proc. of Spoken Language Technology Workshop (SLT), 2016.\n[16] N. Dehak, P. Kenny, R. Dehak, P. Ouellet, and P. Dumouchel, “Front-End Factor Analysis for Speaker Verification,” IEEE Trans. on Audio, Speech and Language Processing, vol. 19, pp. 788–798, 2011.\n[17] A. Agarwal et al., “An Introduction to Computational Networks and the Computational Network Toolkit,” 2016.\n[18] J. S. Garofolo, L. F. Lamel, W. M. Fisher, J. G. Fiscus, D. S. Pallett, N. L. Dahlgren, and V. Zue, “TIMIT Acoustic-Phonetic Continuous Speech Corpus LDC93S1,” 1993, Web Download. Philadelphia: Linguistic Data Consortium.\n[19] H. Hermanksy and N. Morgan, “Rasta Processing of Speech,” IEEE Trans. on Speech and Audio Processing, vol. 2, pp. 578– 589, 1994.\n[20] A. Martin, G. Doddington, T. Kamm, M. Ordowskiand, and M. Przybocki, “The Det Curve in Assessment of Detection Task Performance,” in Proc. of Eur. Conf. Speech Commun. and Tech. (Eurospeech), 1997, pp. 1895–1898.\nInput\n21 3\n1 2 3\nutterance 1 length 1\nlength 2\nclass 1 class 2 class 3 class 1\nclass 2\nclass 3\nInput layer\nOutput layer PCA\nBottleneck feature\nDeep feature\nutterance 2\nwn\nn1\nSegments n1\nn1\nclass n1\nclass n1\nw1"
    } ],
    "references" : [ {
      "title" : "An Overview of Text-independent Speaker Recognition: from Features to Supervectors",
      "author" : [ "T. Kinnunen", "H. Li" ],
      "venue" : "Speech Communication, vol. 52, pp. 12–40, 2010.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Low-Complexity Variable Frame Rate Analysis for Speech Recognition and Voice Activity Detection",
      "author" : [ "Z.-H. Tan", "B. Lindber" ],
      "venue" : "IEEE Journal of Selected Topics in Signal Processing, vol. 4, pp. 798–807, 2010.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Deep Neural Networks for Acoustic Modeling in Speech Recognition",
      "author" : [ "G. Hinton" ],
      "venue" : "IEEE Signal Process. Mag., 2012, pp. 82–97.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Advances in Deep Neural Network Approaches to Speaker Recognition",
      "author" : [ "M. McLaren", "Y. Lei", "L. Ferrer" ],
      "venue" : "Proc. of IEEE Int. Conf. Acoust. Speech Signal Processing (ICASSP), 2015.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Deep Neural Networks for Extracting Baum-welch Statistics for Speaker Recognition",
      "author" : [ "P. Kenny", "V. Gupta", "T. Stafylakis", "P. Ouellet", "J. Alam" ],
      "venue" : "Proc. of Odyssey Speaker and Language Recognition Workshop, 2014, pp. 293–298.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Context-dependent Pre-trained Deep Neural Networks for Large Vocabulary Speech Recognition",
      "author" : [ "E. Dahl", "D. Yu", "L. Deng", "A. Acero" ],
      "venue" : "IEEE Transactions on Audio, Speech, and Language Processing, vol. 20, pp. 30–42, 2012.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Deep Neural Networks for Small Footprint Text-dependent Speaker Verification",
      "author" : [ "E. Variani", "X. Lei", "E. McDermott", "I. Lopez-Moreno", "J. Gonzalez-Dominguez" ],
      "venue" : "Proc. of IEEE Int. Conf. Acoust. Speech Signal Processing (ICASSP), 2014, pp. 4080–4084.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Tandem Deep Features for Text-dependent Speaker Verification",
      "author" : [ "T. Fu", "Y. Qian", "Y. Liu", "Kai Yu" ],
      "venue" : "Proc. of Interspeech, 2014, pp. 1327–1331.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Deep Feature for Text-dependent Speaker Verification",
      "author" : [ "Y. Liu", "Y. Qian", "N. Chen", "T. Fu", "Y. Zhang", "K. Yu" ],
      "venue" : "Speech Communication, vol. 73, pp. 1–13, 2015.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Augmenting Short-term Cepstral Features with Long-term Discriminative Features for Speaker Verification of Telephone Data",
      "author" : [ "C.-T. Do", "C. Barras", "V.-B. Le", "A.K. Sarkar" ],
      "venue" : "Proc. of Interspeech, 2013, pp. 2484–2488.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Deep Bottleneck Features for ivector based Text-independent Speaker Verification",
      "author" : [ "S. Ghalehjegh", "R. Rose" ],
      "venue" : "Proc. of IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2015, pp. 555–560.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Unsupervised Feature Extraction by Time-Contrastive Learning and Nonlinear ICA",
      "author" : [ "A. Hyvarinen", "H. Morioka" ],
      "venue" : "in Proc. of Neural Information Processing systems (NIPS), 2016.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Speaker Verification using Adapted Gaussian Mixture Models",
      "author" : [ "D.A. Reynolds", "T.F. Quatieri", "R.B. Dunn" ],
      "venue" : "Digital Signal Processing, vol. 10, pp. 19–41, 2000.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Text-dependent Speaker Verification: Classifiers, Databases and RSR2015",
      "author" : [ "A. Larcher", "K.A. Lee", "B. Ma", "H. Li" ],
      "venue" : "Speech Communication, vol. 60, pp. 56–77, 2014.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Further Optimisations of Constant Q Cepstral Processing for Integrated Utterance and Textdependent Speaker Verification",
      "author" : [ "H. Delgado", "M. Todisco", "M. Sahidullah", "A.K. Sarkar", "N. Evans", "T. Kinnunen", "Z.-H. Tan" ],
      "venue" : "in Proc. of Spoken Language Technology Workshop (SLT), 2016.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Front-End Factor Analysis for Speaker Verification",
      "author" : [ "N. Dehak", "P. Kenny", "R. Dehak", "P. Ouellet", "P. Dumouchel" ],
      "venue" : "IEEE Trans. on Audio, Speech and Language Processing, vol. 19, pp. 788–798, 2011.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "An Introduction to Computational Networks and the Computational Network Toolkit",
      "author" : [ "A. Agarwal" ],
      "venue" : "2016.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "TIMIT Acoustic-Phonetic Continuous Speech Corpus LDC93S1",
      "author" : [ "J.S. Garofolo", "L.F. Lamel", "W.M. Fisher", "J.G. Fiscus", "D.S. Pallett", "N.L. Dahlgren", "V. Zue" ],
      "venue" : "1993, Web Download. Philadelphia: Linguistic Data Consortium.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Rasta Processing of Speech",
      "author" : [ "H. Hermanksy", "N. Morgan" ],
      "venue" : "IEEE Trans. on Speech and Audio Processing, vol. 2, pp. 578– 589, 1994.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "The Det Curve in Assessment of Detection Task Performance",
      "author" : [ "A. Martin", "G. Doddington", "T. Kamm", "M. Ordowskiand", "M. Przybocki" ],
      "venue" : "Proc. of Eur. Conf. Speech Commun. and Tech. (Eurospeech), 1997, pp. 1895–1898.  Input  2  1  3 1 2 3  utterance 1  length 1 length 2 class 1 class 2 class 3 class 1 class 2 class 3 Input layer Output layer PCA Bottleneck feature  Deep feature  utterance 2 wn  n1  Segments n1 n1 class n1 class n1  w1",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 1997
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Speech is a quasi stationary signal and hence short-time based ceptral feature representations are mostly used in speaker [1] and speech recognition [2] systems.",
      "startOffset" : 122,
      "endOffset" : 125
    }, {
      "referenceID" : 1,
      "context" : "Speech is a quasi stationary signal and hence short-time based ceptral feature representations are mostly used in speaker [1] and speech recognition [2] systems.",
      "startOffset" : 149,
      "endOffset" : 152
    }, {
      "referenceID" : 2,
      "context" : "Recently, the deep neural network (DNN) concept [3] has gained great interest in the field of automatic speech and speaker recognition as DNNs are capable of modeling highly nonlinear structure of patterns in data.",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 3,
      "context" : "In the context of speaker verification, DNNs are found to be used either for extracting posteriori statistics [4, 5, 6] with respect to the pre-defined phonetic classes for i-vector extraction or for discriminative feature extraction, where a multi-layer DNN is trained with objective to discriminate speakers, passphrases, phonetic classes, phones or a combination of them.",
      "startOffset" : 110,
      "endOffset" : 119
    }, {
      "referenceID" : 4,
      "context" : "In the context of speaker verification, DNNs are found to be used either for extracting posteriori statistics [4, 5, 6] with respect to the pre-defined phonetic classes for i-vector extraction or for discriminative feature extraction, where a multi-layer DNN is trained with objective to discriminate speakers, passphrases, phonetic classes, phones or a combination of them.",
      "startOffset" : 110,
      "endOffset" : 119
    }, {
      "referenceID" : 5,
      "context" : "In the context of speaker verification, DNNs are found to be used either for extracting posteriori statistics [4, 5, 6] with respect to the pre-defined phonetic classes for i-vector extraction or for discriminative feature extraction, where a multi-layer DNN is trained with objective to discriminate speakers, passphrases, phonetic classes, phones or a combination of them.",
      "startOffset" : 110,
      "endOffset" : 119
    }, {
      "referenceID" : 6,
      "context" : "In case of feature extraction, the outputs of the DNN hidden layers are used to vectorize characterization of speech data, which are either directly used for speaker characterization called d-vector [7] analogous to the i-vector concept or projected onto a low dimensional space called bottleneck (BN) feature [8, 9, 10, 11] for speaker recognition.",
      "startOffset" : 199,
      "endOffset" : 202
    }, {
      "referenceID" : 7,
      "context" : "In case of feature extraction, the outputs of the DNN hidden layers are used to vectorize characterization of speech data, which are either directly used for speaker characterization called d-vector [7] analogous to the i-vector concept or projected onto a low dimensional space called bottleneck (BN) feature [8, 9, 10, 11] for speaker recognition.",
      "startOffset" : 310,
      "endOffset" : 324
    }, {
      "referenceID" : 8,
      "context" : "In case of feature extraction, the outputs of the DNN hidden layers are used to vectorize characterization of speech data, which are either directly used for speaker characterization called d-vector [7] analogous to the i-vector concept or projected onto a low dimensional space called bottleneck (BN) feature [8, 9, 10, 11] for speaker recognition.",
      "startOffset" : 310,
      "endOffset" : 324
    }, {
      "referenceID" : 9,
      "context" : "In case of feature extraction, the outputs of the DNN hidden layers are used to vectorize characterization of speech data, which are either directly used for speaker characterization called d-vector [7] analogous to the i-vector concept or projected onto a low dimensional space called bottleneck (BN) feature [8, 9, 10, 11] for speaker recognition.",
      "startOffset" : 310,
      "endOffset" : 324
    }, {
      "referenceID" : 10,
      "context" : "In case of feature extraction, the outputs of the DNN hidden layers are used to vectorize characterization of speech data, which are either directly used for speaker characterization called d-vector [7] analogous to the i-vector concept or projected onto a low dimensional space called bottleneck (BN) feature [8, 9, 10, 11] for speaker recognition.",
      "startOffset" : 310,
      "endOffset" : 324
    }, {
      "referenceID" : 8,
      "context" : "It has been demonstrated in [9, 10, 11] that DNN based systems either perform better than or provide complementary information to conventional short-time cepstral feature based speaker recognition systems.",
      "startOffset" : 28,
      "endOffset" : 39
    }, {
      "referenceID" : 9,
      "context" : "It has been demonstrated in [9, 10, 11] that DNN based systems either perform better than or provide complementary information to conventional short-time cepstral feature based speaker recognition systems.",
      "startOffset" : 28,
      "endOffset" : 39
    }, {
      "referenceID" : 10,
      "context" : "It has been demonstrated in [9, 10, 11] that DNN based systems either perform better than or provide complementary information to conventional short-time cepstral feature based speaker recognition systems.",
      "startOffset" : 28,
      "endOffset" : 39
    }, {
      "referenceID" : 8,
      "context" : "It is observed in [9] that the performances of TD-SV systems using BN features trained on discriminating speaker+pass-phrases or phonetic or speaker+phonetic are similar to each other.",
      "startOffset" : 18,
      "endOffset" : 21
    }, {
      "referenceID" : 11,
      "context" : "Inspired by the recently proposed time contrastive learning (TCL) algorithm for classification of EEG/MPEG data in [12], in this paper, we explore the TCL concept for speech signals aiming at finding out whether it is applicable for speech and what are the effective strategies.",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 12,
      "context" : "Gaussian mixture model - universal background model (GMM-UBM) [13] technique is used for speaker verification, since it is well established that a GMM based classifier [14, 15] gives better performance in SV using short utterances than the i-vector [16].",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 13,
      "context" : "Gaussian mixture model - universal background model (GMM-UBM) [13] technique is used for speaker verification, since it is well established that a GMM based classifier [14, 15] gives better performance in SV using short utterances than the i-vector [16].",
      "startOffset" : 168,
      "endOffset" : 176
    }, {
      "referenceID" : 14,
      "context" : "Gaussian mixture model - universal background model (GMM-UBM) [13] technique is used for speaker verification, since it is well established that a GMM based classifier [14, 15] gives better performance in SV using short utterances than the i-vector [16].",
      "startOffset" : 168,
      "endOffset" : 176
    }, {
      "referenceID" : 15,
      "context" : "Gaussian mixture model - universal background model (GMM-UBM) [13] technique is used for speaker verification, since it is well established that a GMM based classifier [14, 15] gives better performance in SV using short utterances than the i-vector [16].",
      "startOffset" : 249,
      "endOffset" : 253
    }, {
      "referenceID" : 8,
      "context" : "Among conventional DNN bottleneck features, the BN feature in [9] is based on DNNs trained to optimize two cross-entropy based objective functions simultaneously: one for discriminating speakers and the other for discriminating pass-phrases.",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 16,
      "context" : "The equally weighted combination of these two criteria is used as a final criterion and DNN multitask learning procedure is followed [17].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 8,
      "context" : "It is observed in [9] that the performance of text-dependent speaker verification using BN features extracted based on the discrimination of both speaker and pass-phrases is similar to that of features based on discrimination of either speakers or speaker+phone.",
      "startOffset" : 18,
      "endOffset" : 21
    }, {
      "referenceID" : 11,
      "context" : "In the TCL concept [12], multivariate time series data X are first divided into a number of uniform segments (say N ), and then all data-points within a particular segment are assigned to one class label as follows:",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 11,
      "context" : "The method is evaluated on brain imaging data, specifically magnetoencephalography (MEG) signals, to classify the different states of brain and the task involves a classification of few number of classes (only four classes) [12].",
      "startOffset" : 224,
      "endOffset" : 228
    }, {
      "referenceID" : 11,
      "context" : "Inspired by the TCL algorithm [12], we explore the concept on speech to generate bottleneck feature for speaker verification.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 11,
      "context" : "Similarly to the TCL algorithm [12], speech utterances are uniformly segmented into a number of pre-defined segments (say N ) regardless of speakers and contents, and data within a particular segment are assigned one class label distinct from the other segments.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 13,
      "context" : "3sess-pwd eval m task) [14] as per protocol.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 16,
      "context" : "CNTK toolkit [17] is used for implementing the DNN and bottleneck feature extraction.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 8,
      "context" : "Bottleneck feature: Outputs from the second (L2) and fourth (L4) hidden layers are used as bottleneck features for this study as in [9] since they show the best performance.",
      "startOffset" : 132,
      "endOffset" : 135
    }, {
      "referenceID" : 17,
      "context" : "Gender-dependent GMM-UBM (512 mixtures, having diagonal covariance matrix) is trained using non-target speakers (438 male) data (4380 utterances) from TIMIT database [18].",
      "startOffset" : 166,
      "endOffset" : 170
    }, {
      "referenceID" : 18,
      "context" : "For spectral analysis, 57 dimensional MFCCs (with RASTA [19] filtering) consisting of static C1-C19 cepstra, with ∆ and ∆∆ coefficients are extracted from speech signals using 10 ms frame shift and a 20 ms Hamming window.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 19,
      "context" : "System performance is evaluated in terms of equal error rate (EER) and minimum detection cost function (MinDCF) [20].",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 8,
      "context" : "All BN features (L4) give lower average error rates and MinDCF values (across different types of non-targets) as compared to cepstral (MFCC) feature as in [9].",
      "startOffset" : 155,
      "endOffset" : 158
    } ],
    "year" : 2017,
    "abstractText" : "In this paper, we present a time-contrastive learning (TCL) based unsupervised bottleneck (BN) feature extraction method for speech signals with an application to speaker verification. The method exploits the temporal structure of a speech signal and more specifically, it trains deep neural networks (DNNs) to discriminate temporal events obtained by uniformly segmenting the signal without using any label information, in contrast to conventional DNN based BN feature extraction methods that train DNNs using labeled data to discriminate speakers or passphrases or phones or a combination of them. We consider different strategies for TCL and its combination with transfer learning. Experimental results on the RSR2015 database show that the TCL method is superior to the conventional speaker and pass-phrase discriminant BN feature and Mel-frequency cepstral coefficients (MFCCs) feature for text-dependent speaker verification. The unsupervised TCL method further has the advantage of being able to leverage the huge amount of unlabeled data that are often available in real life.",
    "creator" : "LaTeX with hyperref package"
  }
}