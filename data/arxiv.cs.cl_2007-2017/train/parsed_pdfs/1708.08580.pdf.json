{
  "name" : "1708.08580.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Generating Sentence Planning Variations for Story Telling",
    "authors" : [ "Stephanie M. Lukin", "Lena I. Reed", "Marilyn A. Walker" ],
    "emails" : [ "slukin@ucsc.edu", "lireed@ucsc.edu", "mawalker@ucsc.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Recently there has been an explosion in applications for natural language and dialogue interaction ranging from direction-giving and tourist information to interactive story systems (Dethlefs et al., 2014; Walker et al., 2011; Hu et al., 2015). While this is due in part to progress in statistical natural language understanding, many applications require the system to actually respond in a meaningful way. Yet the natural language generation (NLG) component of many interactive dialogue systems remains largely handcrafted. This\nlimitation greatly restricts the range of applications; it also means that it is impossible to take advantage of recent work in expressive and statistical language generation that can dynamically and automatically produce a large number of variations of given content (Rieser and Lemon, 2011; Paiva and Evans, 2004; Langkilde, 1998; Rowe et al., 2008; Mairesse and Walker, 2011). Such variations are important for expressive purposes, we well as for user adaptation and personalization (Zukerman and Litman, 2001; Wang et al., 2005; McQuiggan et al., 2008). We propose that a solution to this problem lies in new methods for developing language generation resources.\nFirst we describe the ES-TRANSLATOR (or EST), a computational language generator that has previously been applied only to fables, e.g. the fable in Table 3 (Rishes et al., 2013). We quantitatively evaluate the domain independence of the EST by applying it to social media narratives, such as the Startled Squirrel story in Table 1. We then present a parameterized general-purpose framework built on the EST pipeline, EST 2.0, that can generate many different tellings of the same story, by utilizing sentence planning and point of view parameters. Automatically generated story variations are shown in Table 2 and Table 4.\nWe hypothesize many potential uses for our ap-\nar X\niv :1\n70 8.\n08 58\n0v 1\n[ cs\n.C L\n] 2\n9 A\nug 2\n01 7\nproach to repurposing and retelling existing stories. First, such stories are created daily in the thousands and cover any topic imaginable. They are natural and personal, and may be funny, sad, heart-warming or serious. There are many potential applications: virtual companions, educational storytelling, or to share troubles in therapeutic settings (Bickmore, 2003; Pennebaker and Seagal, 1999; Gratch et al., 2012).\nPrevious research on NLG of linguistic style shows that dialogue systems are more effective if they can generate stylistic linguistic variations based on the user’s emotional state, personality, style, confidence, or other factors (André et al., 2000; Piwek, 2003; McQuiggan et al., 2008; Porayska-Pomsta and Mellish, 2004; Forbes-Riley and Litman, 2011; Wang et al., 2005; Dethlefs et al., 2014). Other work focuses on variation in journalistic writing or instruction manuals, where stylistic variations as well as journalistic slant or connotations have been explored (Hovy, 1988; Green and DiMarco, 1993; Paris and Scott, 1994; Power et al., 2003; Inkpen and Hirst, 2004). Previous iterations of the EST simply presented a sequence of events (Rishes et al., 2013). This work implements parameterized variation of linguistic style in the context of weblogs in order to introduce discourse structure into our generated stories.\nOur approach differs from previous work on NLG for narrative because we emphasize (1) domain-independent methods; and (2) generating a large range of variation, both narratological and stylistic. (Lukin and Walker, 2015)’s work on the EST is the first to generate dialogue within stories, to have the ability to vary direct vs. indirect speech, and to generate dialogue utterances using different stylistic models for character voices. Previous work can generate narratological variations, but is domain dependent (Callaway and Lester, 2002; Montfort, 2007).\nSec. 2 describes our corpus of stories and the ar-\nchitecture of our story generation framework, EST 2.0.1 Sec. 3 describes experiments testing the coverage and correctness of EST 2.0. Sec. 4 describes experiments testing user perceptions of different linguistic variations in storytelling. Our contributions are:\n• We produce SIG representations of 100 personal narratives from a weblog corpus, using the story annotation tool Scheherezade (Elson and McKeown, 2009; Elson, 2012); • We compare EST 2.0 to EST and show how we have not only made improvements to the translation algorithm, but can extend and compare to personal narratives. • We implement a parameterized variation of linguistic style in order to introduce discourse structure into our generated narratives. • We carry out experiments to gather user perceptions of different sentence planning choices that can be made with complex sentences in stories.\nWe sum up and discuss future work in Sec. 5."
    }, {
      "heading" : "2 Story Generation Framework",
      "text" : "Fig. 1 illustrates our overall architecture, which uses NLG modules to separate the process of planning What to say (content planning and selection,\n1The corpus is available from https://nlds.soe. ucsc.edu/personabank.\nfabula) from decisions about How to say it (sentence planning and realization, discourse). We build on three existing tools from previous work: the SCHEHEREZADE story annotation tool, the PERSONAGE generator, and the ES-TRANSLATOR (EST) (Elson, 2012; Mairesse and Walker, 2011; Rishes et al., 2013). The EST uses the STORY INTENTION GRAPH (SIG) representation produced by SCHEHEREZADE and its theoretical grounding as a basis for the content for generation. The EST bridges the narrative representation of the SIG to the representation required by PERSONAGE by generating the text plans and the deep syntactic structures that PERSONAGE requires. Thus any story or content represented as a SIG can be retold using PERSONAGE. See Fig. 1.\nThere are several advantages to using the SIG as the representation for a content pool:\n• Elson’s DRAMABANK provides stories encoded as SIGs including 36 Aesop’s Fables, such as The Fox and the Crow in Table 3.\n• The SIG framework includes an annotation tool called SCHEHERAZADE that supports representing any narrative as a SIG.\n• SCHEHEREZADE comes with a realizer that regenerates stories from the SIG: this realizer provides alternative story realizations that we can compare to the EST 2.0 output.\nWe currently have 100 personal narratives annotated with the SIG representation on topics such as travel, storms, gardening, funerals, going to the doctor, camping, and snorkeling, selected from a corpus of a million stories (Gordon and Swanson, 2009). We use the stories in Tables 1 and 3 in this paper to explain our framework.\nFig. 2 shows the SIG for The Startled Squirrel story in Table 1. To create a SIG, SCHEHERAZADE annotators: (1) identify key entities; (2) model events and statives as propositions and arrange them in a timeline; and (3) model the annotator’s understanding of the overarching goals, plans and beliefs of the story’s agents. SCHEHERAZADE allows users to annotate a story along several dimensions, starting with the surface form of the story (first column in Table 2) and then proceeding to deeper representations. The first dimension (second column in Table 2) is called the “timeline layer”, in which the story is encoded as predicate-argument structures (propositions) that are temporally ordered on a timeline. SCHEHERAZADE adapts information about predicate-argument structures from the VerbNet lexical database (Kipper et al., 2006) and uses\nWordNet (Fellbaum, 1998) as its noun and adjectives taxonomy. The arcs of the story graph are labeled with discourse relations, such as attempts to cause, or temporal order (see Chapter 4 of (Elson, 2012).)\nThe EST applies a model of syntax to the SIG which translates from the semantic representation of the SIG to the syntactic formalism of Deep Syntactic Structures (DSYNTS) required by the PERSONAGE generator (Lavoie and Rambow, 1997; Melčuk, 1988; Mairesse and Walker, 2011). Fig. 1 provides a high level view of the architecture of EST. The full translation methodology is described in (Rishes et al., 2013).\nDSYNTS are a flexible dependency tree representation of an utterance that gives us access to the underlying linguistic structure of a sentence that goes beyond surface string manipulation. The nodes of the DSYNTS syntactic trees are labeled with lexemes and the arcs of the tree are labeled with syntactic relations. The DSYNTS formalism distinguishes between arguments and modifiers and between different types of arguments\n(subject, direct and indirect object etc). Lexicalized nodes also contain a range of grammatical features used in generation. RealPro handles morphology, agreement and function words to produce an output string.\nThis paper utilizes the ability of the EST 2.0 and the flexibility of DSYNTS to produce direct speech that varies the character voice as illustrated in Table 4 (Lukin and Walker, 2015). By simply modifying the person parameter in the DSYNTS, we can change the sentence to be realized in the first person. For example, to produce the variations in Table 4, we use both first person, and direct speech, as well as linguistic styles from PERSONAGE: a neutral voice for the narrator, a shy voice for the crow, and a laid-back voice for the fox (Lukin and Walker, 2015). We fully utilize this variation when we retell personal narratives in EST 2.0.\nThis paper and introduces support for new discourse relations, such as aggregating clauses related by the contingency discourse relation (one of many listed in the Penn Discourse Tree Bank (PDTB) (Prasad et al., 2008)). In SIG encoding, contingency clauses are always expressed with the “in order to” relation (Table 6, 1). To support linguistic variation, we introduce “de-aggregation” onto these aggregating clauses in order to have the flexibility to rephrase, restructure, or ignore clauses as indicated by our parameterized sentence planner. We identify candidate story points in the SIG that contain a contingency relation (annotated in the Timeline layer) and deliberately break apart\nthis hard relationship to create nucleus and satellite DSYNTS that represents the entire sentence (Table 6, 2) (Mann and Thompson, 1988). We create a text plan (Table 6, 3) to allow the sentence planner to reconstruct this content in various ways. Table 5 shows sentence planning variations for the contingency relation for both fables and personal narratives (soSN, becauseNS, becauseSN, NS, N), the output of EST 1.0, the original sentence (original), and the SCHEHERAZADE realization (Sch) which provides an additional baseline. The Sch variant is the original “in order to” contingency relationship produced by the SIG annotation. The becauseNS operation presents the nucleus first, followed by a because, and then the satellite. We can also treat the nucleus and satellite as two different sentences (NS) or completely leave off the satellite (N). We believe the N variant is useful if the satellite can be easily inferred from the prior context.\nThe richness of the discourse information present in the SIG and our ability to de-aggregate and aggregate will enable us to implement other discourse relations in future work."
    }, {
      "heading" : "3 Personal Narrative Evaluation",
      "text" : "After annotating our 100 stories with the SCHEHERAZADE annotation tool, we ran them through the EST, and examined the output. We discovered several bugs arising from variation in the blogs that are not present in the Fables, and fixed them. In previous work on the EST, the machine translation metrics Levenshtein’s distance and BLEU score were used to compare\nTable 6: 1: original unbroken DSYNTS; 2) deaggregated DSYNTS; 3) contingency text plan\n1: ORIGINAL <dsynts id=\"5_6\"> <dsyntnode class=\"verb\" lexeme=\"organize\"\nmode=\"\" mood=\"ind\" rel=\"II\" tense=\"past\"> <dsyntnode article=\"def\" class=\"common_noun\"\nlexeme=\"bird\" number=\"pl\" person=\"\" rel=\"I\"/> <dsyntnode article=\"def\" class=\"common_noun\"\nlexeme=\"bird\" number=\"pl\" person=\"\" rel=\"II\"/> <dsyntnode class=\"preposition\" lexeme=\"on\"\nrel=\"ATTR\"> <dsyntnode article=\"def\" class=\"common_noun\"\nlexeme=\"railing\" number=\"sg\" person=\"\" rel=\"II\"> <dsyntnode article=\"no-art\" class=\"common_noun\" lexeme=\"deck\" number=\"sg\" person=\"\" rel=\"I\"/>\n</dsyntnode> </dsyntnode> <dsyntnode class=\"preposition\" lexeme=\"in_order\"\nrel=\"ATTR\"> <dsyntnode class=\"verb\" extrapo=\"+\" lexeme=\"wait\"\nmode=\"inf-to\" mood=\"inf-to\" rel=\"II\" tense=\"inf-to\"> <dsyntnode article=\"def\" class=\"common_noun\" lexeme=\"bird\" number=\"pl\" person=\"\" rel=\"I\"/>\n</dsyntnode> </dsyntnode>\n</dsyntnode> </dsynts>\n2: DEAGGREGATION <dsynts id=\"5\">\n<dsyntnode class=\"verb\" lexeme=\"organize\" mood=\"ind\" rel=\"II\" tense=\"past\">\n<dsyntnode article=\"def\" class=\"common_noun\" lexeme=\"bird\" number=\"pl\" person=\"\" rel=\"I\"/> <dsyntnode article=\"def\" class=\"common_noun\" lexeme=\"bird\" number=\"pl\" person=\"\" rel=\"II\"/> <dsyntnode class=\"preposition\" lexeme=\"on\" rel=\"ATTR\">\n<dsyntnode article=\"def\" class=\"common_noun\" l lexeme=\"railing\" number=\"sg\" person=\"\" rel=\"II\"> <dsyntnode article=\"no-art\" class=\"common_noun\" lexeme=\"deck\" number=\"sg\" person=\"\" rel=\"I\"/> </dsyntnode>\n</dsyntnode> </dsyntnode>\n</dsynts>\n<dsynts id=\"6\"> <dsyntnode class=\"verb\" lexeme=\"want\"\nmood=\"ind\" rel=\"II\" tense=\"past\"> <dsyntnode article=\"def\" class=\"common_noun\"\nlexeme=\"bird\" number=\"pl\" person=\"\" r <dsyntnode class=\"verb\" extrapo=\"+\" lexeme=\"wait\" mode=\"inf-to\" mood=\"inf-to\" rel=\"II\" tense=\"inf-to\"/>\n</dsyntnode> </dsynts>\n3: AGGREGATION TEXT PLAN <speechplan voice=\"Narrator\"> <rstplan> <relation name=\"contingency_cause\">\n<proposition id=\"1\" ns=\"nucleus\"/> <proposition id=\"2\" ns=\"satellite\"/>\n</relation> </rstplan> <proposition dialogue_act=\"5\" id=\"1\"/> <proposition dialogue_act=\"6\" id=\"2\"/>\n</speechplan>\nthe original Aesop’s Fables to their generated EST and SCHEHERAZADE reproductions (denoted EST and Sch) (Rishes et al., 2013). These metrics are not ideal for evaluating story quality, especially when generating stylistic variations of the original story. However they allow us to automatically test some aspects of system coverage, so we repeat this evaluation on the blog dataset.\nTable 7 presents BLEU and Levenshtein scores for the original 36 Fables and all 100 blog stories, compared to both Sch and EST 1.0. Levenshtein\ndistance computes the minimum edit distance between two strings, so we compare the entire original story to a generated version. A lower score indicates a closer comparison. BLEU score computes the overlap between two strings taking word order into consideration: a higher BLEU score indicates a closer match between candidate strings. Thus Table 7 provides quantitative evidence that the style of the original blogs is very different from Aesop’s Fables. Neither the EST output nor the Sch output comes close to representing the original textual style (Blogs Original-Sch and OriginalEST).\nHowever we find that EST compares favorably to Sch on the blogs with a relatively low Levenshtein score, and higher BLEU score (Blogs Sch-EST) than the original Fables evaluation (Fables Sch-EST). This indicates that even though the blogs have a diversity of language and style, our translation comes close to the Sch baseline."
    }, {
      "heading" : "4 Experimental Design and Results",
      "text" : "We conduct two experiments on Mechanical Turk to test variations generated with the deaggregation and point of view parameters. We compare the variations amongst themselves and to the original sentence in a story. We are also interested in identifying differences among individual stories.\nIn the first experiment, we show an excerpt from the original story telling and indicate to the participants that “any of the following sentences could come next in the story”. We then list all variations of the following sentence with the “in order to” contingency relationship (examples from the Startled Squirrel labeled EST 2.0 in Table 5).\nOur aim is to elicit rating of the variations in terms of correctness and goodness of fit within the story context (1 is best, 5 is worst), and to rank the sentences by personal preference (in experiment 1 we showed 7 variations where 1 is best, 7 is worst; in experiment 2 we showed 3 variations where 1 is best, 3 is worst). We also show\nthe original blog sentence and the EST 1.0 output before de-aggregation and sentence planning. We emphasize that the readers should read each variation in the context of the entire story and encourage them to reread the story with each new sentence to understand this context.\nIn the second experiment, we compare the original sentence with our best realization, and the realization produced by SCHEHEREZADE (Sch). We expect that SCHEHEREZADE will score more poorly in this instance because it cannot change point of view from third person to first person, even though its output is more fluent than EST 2.0 for many cases."
    }, {
      "heading" : "4.1 Results Experiment 1",
      "text" : "We had 7 participants analyze each of the 16 story segments. All participants were native English speakers. Table 8 shows the means and standard deviations for correctness and preference rankings in the first experiment. We find that averaged across all stories, there is a clear order for correctness and preference: original, soSN, becauseNS, becauseSN, NS, EST, N.\nWe performed an ANOVA on preference and found that story has no significant effect on the results (F(1, 15) = 0.18, p = 1.00), indicating that all stories are well-formed and there are no outliers in the story selection. On the other hand, realization does have a significant effect on preference (F(1, 6) = 33.74, p = 0.00). This supports our hypothesis that the realizations are distinct from each other and there are preferences amongst them.\nFig. 3 shows the average correctness and preference for all stories. Paired t-tests show that there is a significant difference in reported correctness between orig and soSN (p < 0.05), but no difference between soSN and becauseNS (p = 0.133), or becauseSN (p = 0.08). There is a difference between soSN and NS (p < 0.005), as well as between the two different because operations and NS (p < 0.05). There are no other significant differences.\nThe are larger differences on the preference metric. Paired t-tests show that there is a significant difference between orig and soSN (p < 0.0001) and soSN and becauseNS (p < 0.05). There is no difference in preference between becauseNS and becauseSN (p = 0.31). However there is a significant difference between soSN and becauseSN (p < 0.005) and becauseNS and NS (p < 0.0001). Finally, there is significant difference between becauseSN and NS (p < 0.005) and NS and EST (p < 0.005). There is no difference between EST and N (p = 0.375), but there is a difference between NS and N (p < 0.05).\nThese results indicate that the original sentence, as expected, is the most correct and preferred. Qualitative feedback on the original sentence included: “The one I ranked first makes a more interesting story. Most of the others would be sufficient, but boring.”; “The sentence I ranked first makes more sense in the context of the story. The others tell you similar info, but do not really fit.”. Some participants ranked soSN as their preferred variant (although the difference was never statistically significant): “The one I rated the best sounded really natural.”\nAlthough we observe an overall ranking trend, there are some differences by story for NS and N. Most of the time, these two are ranked the lowest. Some subjects observe: “#1 [orig] & #2 [soSN] had a lot of detail. #7 [N] did not explain what the person wanted to see” (a044 in Table 10); “The sentence I rated the worst [N] didn’t explain why the person wanted to cook them, but it would have been an okay sentence.” (a060 in Table 10); “I ranked the lower number [N] because they either did not contain the full thought of the subject or they added details that are to be assumed.” (a044 in Table 10); “They were all fairly good sentences. The one I ranked worst [N] just left out why they decided to use facebook.” (a042 in Table 10).\nHowever, there is some support for NS and N. We also find that there is a significant interaction between story and realization (F(2, 89) = 1.70, p = 0.00), thus subjects’ preference of the realization are based on the story they are reading. One subject commented: “#1 [orig] was the most descriptive about what family the person is looking for. I did like the way #3 [NS] was two sentences. It seemed to put a different emphasis on finding family” (a042 in Table 10). Another thought that the explanatory utterance altered the tone of the story: “The parent and the children in the story\nwere having a good time. It doesn’t make sense that parent would want to do something to annoy them [the satellite utterance]” (a060 in Table 10). This person preferred leaving off the satellite and ranked N as the highest preference.\nWe examined these interactions between story and preference ranking for NS and N. This may be depend on either context or on the SIG annotations. For example, in one story (protest in Table 10) our best realization soSN, produces: “The protesters wanted to block the street, so the person said for the protesters to protest in the street in order to block it.” and N produces “The person said for the protesters to protest in the street in order to block it.”. One subject, who ranked N second only to original, observed: “Since the police were coming there with tear gas, it appears the protesters had already shut things down. There is no need to tell them to block the street.” Another subject who ranked N as second preference similarly observed “Frankly using the word protesters and protest too many times made it seem like a word puzzle or riddle. The meaning was lost in too many variations of the word ‘protest.’ If the wording was awkward, I tried to assign it toward the ‘worst’ end of the scale. If it seemed to flow more naturally, as a story would, I tried to assign it toward the ‘best’ end.”\nAlthough the means in this story seem very distinct (Table 8), there is only a significant difference between orig and N (p < 0.005) and N and EST (p < 0.05). Table 8 also includes the means for story a042 (Table 10) where NS is ranked highest for preference. Despite this, the only significant difference between NS is with EST 1.0 (p < 0.05)."
    }, {
      "heading" : "4.2 Results Experiment 2",
      "text" : "Experiment 2 compares our best realization to the SCHEHERAZADE realizer, exploiting the ability of EST 2.0 to change the point of view. Seven participants analyzed each of the 16 story segments. All participants were native English speakers.\nTable 9 shows the means for correctness and preference rankings. Figure 4 shows a histogram of average correctness and preference by realization for all stories. There is a clear order for correctness and preference: original, soSN, Sch, with significant differences between all pairs of realizations (p < 0.0001).\nHowever, in six of the 19 stories, there is no significant difference between Sch and soSN. Three of them do not contain “I” or “the narrator” in the realization sentence. Many of the subjects comment that the realization with “the narrator” does not follow the style of the story: “The second [Sch] uses that awful ‘narrator.”’ (a001 in Table 10); “Forget the narrator sentence. From here on out it’s always the worst!” (a001 in Table 10). We hypothesize that in the three sentences without “the narrator”, Sch can be properly evaluated without the “narrator” bias. In fact, in these situations, Sch was rated higher than soSN: “I chose\nthe sentences in order of best explanatory detail” (Startled Squirrel in Table 5).\nCompare the soSN realization in the protest story in Table 10 “The leaders wanted to talk, so they met near the workplace.” with Sch “The group of leaders was meeting in order to talk about running a group of countries and near a workplace.” Sch has so much more detail than soSN. While the EST has massively improved and overall is preferred to Sch, some semantic components are lost in the translation process."
    }, {
      "heading" : "5 Discussion and Conclusions",
      "text" : "To our knowledge, this is the first time that sentence planning variations for story telling have been implemented in a framework where the discourse (telling) is completely independent of the fabula (content) of the story (Lonneker, 2005). We also show for the first time that the SCHEHEREZADE annotation tool can be applied to informal narratives such as personal narratives from weblogs, and the resulting SIG representations work with existing tools for translating from the SIG to a retelling of a story.\nWe present a parameterized sentence planner for story generation, that provides aggregation operations and variations in point of view. The technical aspects of de-aggregation and aggregation builds on previous work in NLG and our earlier work on SPaRKy (Cahill et al., 2001; Scott and de Souza, 1990; Paris and Scott, 1994; Nakatsu and White, 2010; Howcroft et al., 2013; Walker et al., 2007; Stent and Molina, 2009). However we are not aware of previous NLG applications needing to first de-aggregate the content, before applying aggregation operations.\nOur experiments show that, as expected, readers almost always prefer the original sentence over automatically produced variations, but that the soSN variant is preferred. We examine two specific stories where preferences vary from the overall trend: these stories suggest future possible experiments where we might vary more aspects of the story context and audience. We also compare our best variation to what SCHEHERAZADE produces. Despite the fact that the SCHEHERAZADE realizer was targeted at the SIG, our best variant is most often ranked as a preferred choice.\nIn future work, we aim to explore interactions between a number of our novel narratological parameters. We expect to do this both with a rule-based approach, as well as by building on recent work on statistical models for expressive generation (Rieser and Lemon, 2011; Paiva and\nEvans, 2004; Langkilde, 1998; Rowe et al., 2008; Mairesse and Walker, 2011). This should allow us to train a narrative generator to achieve particular narrative effects, such as engagement or empathy with particular characters. We will also expand the discourse relations that EST 2.0 can handle.\nAcknowledgements. This research was supported by Nuance Foundation Grant SC-14-74, NSF Grants IIS-HCC-1115742 and IIS-1002921.\nAppendix. Table 10 provides additional examples of the output of the EST 2.0 system, illustrating particular user preferences and system strengths and weaknesses."
    } ],
    "references" : [ {
      "title" : "The automated design of believable dialogues for animated presentation teams",
      "author" : [ "André et al.2000] E. André", "T. Rist", "S. van Mulken", "M. Klesen", "S. Baldes" ],
      "venue" : "Embodied conversational agents,",
      "citeRegEx" : "André et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "André et al\\.",
      "year" : 2000
    }, {
      "title" : "Relational agents: Effecting change through human-computer relationships",
      "author" : [ "T.W. Bickmore" ],
      "venue" : "Ph.D. thesis,",
      "citeRegEx" : "Bickmore.,? \\Q2003\\E",
      "shortCiteRegEx" : "Bickmore.",
      "year" : 2003
    }, {
      "title" : "From rags to riches: exploiting the potential of a flexible generation architecture",
      "author" : [ "Cahill et al.2001] L. Cahill", "J. Carroll", "R. Evans", "D. Paiva", "R. Power", "D. Scott", "K. van Deemter" ],
      "venue" : null,
      "citeRegEx" : "Cahill et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Cahill et al\\.",
      "year" : 2001
    }, {
      "title" : "Clusterbased prediction of user ratings for stylistic surface realisation",
      "author" : [ "Dethlefs et al.2014] N. Dethlefs", "H. Cuayáhuitl", "H. Hastie", "V. Rieser", "O. Lemon" ],
      "venue" : null,
      "citeRegEx" : "Dethlefs et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Dethlefs et al\\.",
      "year" : 2014
    }, {
      "title" : "A tool for deep semantic encoding of narrative texts",
      "author" : [ "Elson", "McKeown2009] D.K. Elson", "K.R. McKeown" ],
      "venue" : "In Proc. of the ACL-IJCNLP 2009 Software Demonstrations,",
      "citeRegEx" : "Elson et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Elson et al\\.",
      "year" : 2009
    }, {
      "title" : "Modeling Narrative Discourse",
      "author" : [ "D.K. Elson" ],
      "venue" : "Ph.D. thesis,",
      "citeRegEx" : "Elson.,? \\Q2012\\E",
      "shortCiteRegEx" : "Elson.",
      "year" : 2012
    }, {
      "title" : "WordNet: An Electronic Lexical Database",
      "author" : [ "C. Fellbaum" ],
      "venue" : null,
      "citeRegEx" : "Fellbaum.,? \\Q1998\\E",
      "shortCiteRegEx" : "Fellbaum.",
      "year" : 1998
    }, {
      "title" : "Designing and evaluating a wizarded uncertainty-adaptive spoken dialogue tutoring system",
      "author" : [ "Forbes-Riley", "Litman2011] K. Forbes-Riley", "D. Litman" ],
      "venue" : "Computer Speech & Language,",
      "citeRegEx" : "Forbes.Riley et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Forbes.Riley et al\\.",
      "year" : 2011
    }, {
      "title" : "Identifying personal stories in millions of weblog entries",
      "author" : [ "Gordon", "Swanson2009] A. Gordon", "R. Swanson" ],
      "venue" : "In Third Int. Conf. on Weblogs and Social Media,",
      "citeRegEx" : "Gordon et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Gordon et al\\.",
      "year" : 2009
    }, {
      "title" : "User-state sensing for virtual health agents and telehealth applications",
      "author" : [ "J. Gratch", "L.P. Morency", "S. Scherer", "G. Stratou", "J. Boberg", "S. Koenig", "T. Adamson", "A. Rizzo" ],
      "venue" : "Studies in health technology and informatics,",
      "citeRegEx" : "Gratch et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Gratch et al\\.",
      "year" : 2012
    }, {
      "title" : "Planning coherent multisentential text",
      "author" : [ "E.H. Hovy" ],
      "venue" : "In Proc. 26th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Hovy.,? \\Q1988\\E",
      "shortCiteRegEx" : "Hovy.",
      "year" : 1988
    }, {
      "title" : "Enhancing the expression of contrast in the SPARKY restaurant corpus",
      "author" : [ "C. Nakatsu", "M. White" ],
      "venue" : "ENLG",
      "citeRegEx" : "Howcroft et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Howcroft et al\\.",
      "year" : 2013
    }, {
      "title" : "Storytelling agents with personality and adaptivity",
      "author" : [ "Hu et al.2015] Z. Hu", "M. Walker", "M. Neff", "J.E. Fox Tree" ],
      "venue" : null,
      "citeRegEx" : "Hu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2015
    }, {
      "title" : "Near-synonym choice in natural language generation",
      "author" : [ "Inkpen", "Hirst2004] D.Z. Inkpen", "G. Hirst" ],
      "venue" : "In Recent Advances in Natural Language Processing III",
      "citeRegEx" : "Inkpen et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Inkpen et al\\.",
      "year" : 2004
    }, {
      "title" : "Extending verbnet with novel verb classes",
      "author" : [ "Kipper et al.2006] K. Kipper", "A. Korhonen", "N. Ryant", "M. Palmer" ],
      "venue" : "In Proc. of the 6th Int. Conf. on Language Resources and Evaluation (LREC",
      "citeRegEx" : "Kipper et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Kipper et al\\.",
      "year" : 2006
    }, {
      "title" : "Forest-based statistical sentence generation",
      "author" : [ "I. Langkilde" ],
      "venue" : "Proc. of the 1st Meeting of the North American Chapter of the ACL (ANLP-NAACL",
      "citeRegEx" : "Langkilde.,? \\Q1998\\E",
      "shortCiteRegEx" : "Langkilde.",
      "year" : 1998
    }, {
      "title" : "A fast and portable realizer for text generation systems",
      "author" : [ "Lavoie", "Rambow1997] B. Lavoie", "O. Rambow" ],
      "venue" : "In Proc. of the Third Conf. on Applied Natural Language Processing,",
      "citeRegEx" : "Lavoie et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Lavoie et al\\.",
      "year" : 1997
    }, {
      "title" : "Narratological knowledge for natural language generation",
      "author" : [ "B. Lonneker" ],
      "venue" : "In Proc. of the 10th European Workshop on Natural Language Generation (ENLG",
      "citeRegEx" : "Lonneker.,? \\Q2005\\E",
      "shortCiteRegEx" : "Lonneker.",
      "year" : 2005
    }, {
      "title" : "Narrative variations in a virtual storyteller",
      "author" : [ "Lukin", "Walker2015] S. Lukin", "M. Walker" ],
      "venue" : "Intelligent Virtual Agents",
      "citeRegEx" : "Lukin et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lukin et al\\.",
      "year" : 2015
    }, {
      "title" : "Controlling user perceptions of linguistic style: Trainable generation of personality traits",
      "author" : [ "Mairesse", "Walker2011] F. Mairesse", "M.A. Walker" ],
      "venue" : null,
      "citeRegEx" : "Mairesse et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Mairesse et al\\.",
      "year" : 2011
    }, {
      "title" : "Rhetorical structure theory: Toward a functional theory of text organization. Text-Interdisciplinary Journal for the Study of Discourse",
      "author" : [ "Mann", "S.A. Thompson1988] W.C. Mann" ],
      "venue" : null,
      "citeRegEx" : "Mann and Mann,? \\Q1988\\E",
      "shortCiteRegEx" : "Mann and Mann",
      "year" : 1988
    }, {
      "title" : "Modeling self-efficacy in intelligent tutoring systems: An inductive approach",
      "author" : [ "B.W. Mott", "J.C. Lester" ],
      "venue" : "User Modeling and User-Adapted Interaction,",
      "citeRegEx" : "McQuiggan et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "McQuiggan et al\\.",
      "year" : 2008
    }, {
      "title" : "Generating narrative variation in interactive fiction",
      "author" : [ "N. Montfort" ],
      "venue" : "Ph.D. thesis,",
      "citeRegEx" : "Montfort.,? \\Q2007\\E",
      "shortCiteRegEx" : "Montfort.",
      "year" : 2007
    }, {
      "title" : "Generating with Discourse Combinatory Categorial Grammar",
      "author" : [ "Nakatsu", "White2010] C. Nakatsu", "M. White" ],
      "venue" : "In Linguistic Issues in Language Technology,",
      "citeRegEx" : "Nakatsu et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Nakatsu et al\\.",
      "year" : 2010
    }, {
      "title" : "A framework for stylistically controlled generation",
      "author" : [ "Paiva", "Evans2004] D.S. Paiva", "R. Evans" ],
      "venue" : "In Natural Language Generation, Third Int. Conf., INLG 2004,",
      "citeRegEx" : "Paiva et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Paiva et al\\.",
      "year" : 2004
    }, {
      "title" : "Stylistic variation in multilingual instructions",
      "author" : [ "Paris", "Scott1994] C. Paris", "D. Scott" ],
      "venue" : "In The 7th Int. Conf. on Natural Language Generation",
      "citeRegEx" : "Paris et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Paris et al\\.",
      "year" : 1994
    }, {
      "title" : "A flexible pragmaticsdriven language generator for animated agents",
      "author" : [ "P. Piwek" ],
      "venue" : "In Proc. of Annual Meeting of the European Chapter of the Association for Computational Linguistics (EACL)",
      "citeRegEx" : "Piwek.,? \\Q2003\\E",
      "shortCiteRegEx" : "Piwek.",
      "year" : 2003
    }, {
      "title" : "Modelling politeness in natural language generation",
      "author" : [ "Porayska-Pomsta", "Mellish2004] K. PorayskaPomsta", "C. Mellish" ],
      "venue" : "In Proc. of the 3rd Conf. on INLG,",
      "citeRegEx" : "Porayska.Pomsta et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Porayska.Pomsta et al\\.",
      "year" : 2004
    }, {
      "title" : "Generating texts with style",
      "author" : [ "Power et al.2003] R. Power", "D. Scott", "N. BouayadAgha" ],
      "venue" : "In Proc. of the 4th Int. Conf. on Intelligent Text Processing and Computational Linguistics",
      "citeRegEx" : "Power et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Power et al\\.",
      "year" : 2003
    }, {
      "title" : "Reinforcement learning for adaptive dialogue systems: a data-driven methodology for dialogue management and natural language",
      "author" : [ "Rieser", "Lemon2011] V. Rieser", "O. Lemon" ],
      "venue" : null,
      "citeRegEx" : "Rieser et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Rieser et al\\.",
      "year" : 2011
    }, {
      "title" : "Generating different story tellings from semantic representations of narrative",
      "author" : [ "Rishes et al.2013] E. Rishes", "S.M. Lukin", "D.K. Elson", "M.A. Walker" ],
      "venue" : "In Interactive Storytelling,",
      "citeRegEx" : "Rishes et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Rishes et al\\.",
      "year" : 2013
    }, {
      "title" : "Archetype-Driven Character Dialogue Generation for Interactive Narrative",
      "author" : [ "Rowe et al.2008] J. Rowe", "E. Ha", "J. Lester" ],
      "venue" : "In Intelligent Virtual Agents,",
      "citeRegEx" : "Rowe et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Rowe et al\\.",
      "year" : 2008
    }, {
      "title" : "Getting the message across in RSTbased text generation",
      "author" : [ "Scott", "de Souza1990] D.R. Scott", "C.S. de Souza" ],
      "venue" : "Current Research in Natural Language Generation",
      "citeRegEx" : "Scott et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "Scott et al\\.",
      "year" : 1990
    }, {
      "title" : "Evaluating automatic extraction of rules for sentence plan construction In The 10th Annual Meeting of the Special Interest Group on Discourse and Dialogue",
      "author" : [ "Stent", "Molina 2009] A. Stent", "M. Molina" ],
      "venue" : null,
      "citeRegEx" : "Stent et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Stent et al\\.",
      "year" : 2009
    }, {
      "title" : "Individual and Domain Adaptation in Sentence Planning for Dialogue",
      "author" : [ "Walker et al.2007] M.A. Walker", "A. Stent", "F. Mairesse", "R. Prasad" ],
      "venue" : "Journal of Artificial Intelligence Research (JAIR)",
      "citeRegEx" : "Walker et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Walker et al\\.",
      "year" : 2007
    }, {
      "title" : "Perceived or not perceived: Film character models for expressive nlg",
      "author" : [ "Walker et al.2011] M.A. Walker", "R. Grant", "J. Sawyer", "G.I. Lin", "N. Wardrip-Fruin", "M. Buell" ],
      "venue" : "In Int. Conf. on Interactive Digital Storytelling,",
      "citeRegEx" : "Walker et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Walker et al\\.",
      "year" : 2011
    }, {
      "title" : "The politeness effect: Pedagogical agents and learning gains",
      "author" : [ "N. Wang", "W. Lewis Johnson", "R.E. Mayer", "P. Rizzo", "E. Shaw", "H. Collins" ],
      "venue" : "Frontiers in Artificial Intelligence and Applications,",
      "citeRegEx" : "Wang et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2005
    }, {
      "title" : "Natural language processing and user modeling: Synergies and limitations",
      "author" : [ "Zukerman", "Litman2001] I. Zukerman", "D. Litman" ],
      "venue" : "User Modeling and User-Adapted Interaction,",
      "citeRegEx" : "Zukerman et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Zukerman et al\\.",
      "year" : 2001
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Recently there has been an explosion in applications for natural language and dialogue interaction ranging from direction-giving and tourist information to interactive story systems (Dethlefs et al., 2014; Walker et al., 2011; Hu et al., 2015).",
      "startOffset" : 182,
      "endOffset" : 243
    }, {
      "referenceID" : 35,
      "context" : "Recently there has been an explosion in applications for natural language and dialogue interaction ranging from direction-giving and tourist information to interactive story systems (Dethlefs et al., 2014; Walker et al., 2011; Hu et al., 2015).",
      "startOffset" : 182,
      "endOffset" : 243
    }, {
      "referenceID" : 12,
      "context" : "Recently there has been an explosion in applications for natural language and dialogue interaction ranging from direction-giving and tourist information to interactive story systems (Dethlefs et al., 2014; Walker et al., 2011; Hu et al., 2015).",
      "startOffset" : 182,
      "endOffset" : 243
    }, {
      "referenceID" : 36,
      "context" : "Such variations are important for expressive purposes, we well as for user adaptation and personalization (Zukerman and Litman, 2001; Wang et al., 2005; McQuiggan et al., 2008).",
      "startOffset" : 106,
      "endOffset" : 176
    }, {
      "referenceID" : 21,
      "context" : "Such variations are important for expressive purposes, we well as for user adaptation and personalization (Zukerman and Litman, 2001; Wang et al., 2005; McQuiggan et al., 2008).",
      "startOffset" : 106,
      "endOffset" : 176
    }, {
      "referenceID" : 30,
      "context" : "the fable in Table 3 (Rishes et al., 2013).",
      "startOffset" : 21,
      "endOffset" : 42
    }, {
      "referenceID" : 1,
      "context" : "storytelling, or to share troubles in therapeutic settings (Bickmore, 2003; Pennebaker and Seagal, 1999; Gratch et al., 2012).",
      "startOffset" : 59,
      "endOffset" : 125
    }, {
      "referenceID" : 9,
      "context" : "storytelling, or to share troubles in therapeutic settings (Bickmore, 2003; Pennebaker and Seagal, 1999; Gratch et al., 2012).",
      "startOffset" : 59,
      "endOffset" : 125
    }, {
      "referenceID" : 10,
      "context" : "Other work focuses on variation in journalistic writing or instruction manuals, where stylistic variations as well as journalistic slant or connotations have been explored (Hovy, 1988; Green and DiMarco, 1993; Paris and Scott, 1994; Power et al., 2003; Inkpen and Hirst, 2004).",
      "startOffset" : 172,
      "endOffset" : 276
    }, {
      "referenceID" : 28,
      "context" : "Other work focuses on variation in journalistic writing or instruction manuals, where stylistic variations as well as journalistic slant or connotations have been explored (Hovy, 1988; Green and DiMarco, 1993; Paris and Scott, 1994; Power et al., 2003; Inkpen and Hirst, 2004).",
      "startOffset" : 172,
      "endOffset" : 276
    }, {
      "referenceID" : 30,
      "context" : "Previous iterations of the EST simply presented a sequence of events (Rishes et al., 2013).",
      "startOffset" : 69,
      "endOffset" : 90
    }, {
      "referenceID" : 22,
      "context" : "Previous work can generate narratological variations, but is domain dependent (Callaway and Lester, 2002; Montfort, 2007).",
      "startOffset" : 78,
      "endOffset" : 121
    }, {
      "referenceID" : 5,
      "context" : "the story annotation tool Scheherezade (Elson and McKeown, 2009; Elson, 2012); • We compare EST 2.",
      "startOffset" : 39,
      "endOffset" : 77
    }, {
      "referenceID" : 5,
      "context" : "(EST) (Elson, 2012; Mairesse and Walker, 2011; Rishes et al., 2013).",
      "startOffset" : 6,
      "endOffset" : 67
    }, {
      "referenceID" : 30,
      "context" : "(EST) (Elson, 2012; Mairesse and Walker, 2011; Rishes et al., 2013).",
      "startOffset" : 6,
      "endOffset" : 67
    }, {
      "referenceID" : 14,
      "context" : "SCHEHERAZADE adapts information about predicate-argument structures from the VerbNet lexical database (Kipper et al., 2006) and uses EST 2.",
      "startOffset" : 102,
      "endOffset" : 123
    }, {
      "referenceID" : 6,
      "context" : "WordNet (Fellbaum, 1998) as its noun and adjectives taxonomy.",
      "startOffset" : 8,
      "endOffset" : 24
    }, {
      "referenceID" : 5,
      "context" : "labeled with discourse relations, such as attempts to cause, or temporal order (see Chapter 4 of (Elson, 2012).",
      "startOffset" : 97,
      "endOffset" : 110
    }, {
      "referenceID" : 30,
      "context" : "The full translation methodology is described in (Rishes et al., 2013).",
      "startOffset" : 49,
      "endOffset" : 70
    }, {
      "referenceID" : 30,
      "context" : "the original Aesop’s Fables to their generated EST and SCHEHERAZADE reproductions (denoted EST and Sch) (Rishes et al., 2013).",
      "startOffset" : 104,
      "endOffset" : 125
    }, {
      "referenceID" : 17,
      "context" : "the discourse (telling) is completely independent of the fabula (content) of the story (Lonneker, 2005).",
      "startOffset" : 87,
      "endOffset" : 103
    } ],
    "year" : 2017,
    "abstractText" : "There has been a recent explosion in applications for dialogue interaction ranging from direction-giving and tourist information to interactive story systems. Yet the natural language generation (NLG) component for many of these systems remains largely handcrafted. This limitation greatly restricts the range of applications; it also means that it is impossible to take advantage of recent work in expressive and statistical language generation that can dynamically and automatically produce a large number of variations of given content. We propose that a solution to this problem lies in new methods for developing language generation resources. We describe the ES-TRANSLATOR, a computational language generator that has previously been applied only to fables, and quantitatively evaluate the domain independence of the EST by applying it to personal narratives from weblogs. We then take advantage of recent work on language generation to create a parameterized sentence planner for story generation that provides aggregation operations, variations in discourse and in point of view. Finally, we present a user evaluation of different personal narrative retellings.",
    "creator" : "LaTeX with hyperref package"
  }
}