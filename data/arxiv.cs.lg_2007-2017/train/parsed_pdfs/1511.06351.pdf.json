{
  "name" : "1511.06351.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "LEARNING REPRESENTATIONS USING COMPLEX-VALUED NETS",
    "authors" : [ "Andy M. Sarroff", "Victor Shepardson", "Michael A. Casey" ],
    "emails" : [ "sarroff@cs.dartmouth.edu" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "There are many types of data for which complex-valued representations are natural and appropriate. For example wind measurements may use complex-valued data to represent joint measurements of magnitude and direction (Goh et al., 2006). Direction of arrival is naturally modeled in ultrawideband communications using complex values (Terabayashi et al., 2014). It is also common to work with certain real-valued data, such as audio and EEG recordings, by first transforming them to complex-valued data in the frequency domain via a complex basis, as with the Fourier transform. Motivations behind complex-valued nets (CVNNs) are that they could be used with such real-tocomplex transformed data, or that they may be used for learning complex-valued representations as alternatives to Fourier and related transforms.\nWhilst research into CVNNs has developed in parallel with real-valued networks, there has been relatively little focus on CVNNs in deep learning and complex-valued representation learning. Most research targets highly-specific signal-processing domains such as communications and adaptive array processing. Several factors contribute to the slow adoption of CVNNs in applications outside of these domains: first, they are difficult to train because complex-valued activation functions cannot be simultaneously bounded and complex-differentiable; second, there are few if any known methods for regularization and hyper-parameter optimization specifically developed for CVNNs. Despite such obstacles, research on CVNNs is growing steadily, with new theoretical results (Zimmermann et al., 2011; Sorber et al., 2012; Hirose & Yoshida, 2012) appearing on the heels of comprehensive treatments in recent texts (Hirose, 2006; Mandic & Goh, 2009; Hirose, 2013).\nResearch on complex-valued activation functions and calculation of their derivatives for application to CVNNs is generally split between those composed exclusively from holomorphic activation functions and those composed exclusively from non-holomorphic activation functions. Holomorphic functions are complex differentiable at every point in a neighborhood of their domain. Nonholomorphic functions are not complex differentiable, but may be differentiable with respect to their real and imaginary parts. Each has advantages: Holomorphic activation functions may be more successful at jointly modeling phase and amplitude, however they are unbounded and therefore non-\nar X\niv :1\n51 1.\n06 35\n1v 1\n[ cs\n.L G\n] 1\n9 N\nov 2\n01 5\nholomorphic activation functions may be preferred at times. For example if one were to build a complex-valued Long-Short Term Memory network, the only suitable gating function in the complex domain would necessarily be non-holomorphic.\nIn this paper, we follow a more general framework (Amin et al., 2011; Amin & Murase, 2013) for building CVNNs, both deep and temporal, that allows for activation functions that are composed from combinations of both holomorphic and non-holomorphic functions. We do this by utilizing the mathematical conveniences of the Wirtinger derivative, which simplifies many of the computations that are required for gradient descent for complex-valued functions.\nThe remainder of this paper is organized as follows. In Section 2 we cover the background for CVNNs. Section 3 describes the Wirtinger derivative and how it is applied to back-propagation for gradient descent with complex-valued activation functions. We present experiments that illustrate the utility of the methods in Section 4 and we provide concluding remarks in Section 5."
    }, {
      "heading" : "2 BACKGROUND",
      "text" : "Complex numbers extend the concept of one-dimensional real numbers to two dimensions by expressing an ordered pair (x, y) ∈ R as a point z ∈ C in the complex plane, where z = x + iy and i = √ −1. Numbers in the complex domain provide a natural means for jointly expressing magnitude, |z|, and phase or direction, arg(z). Suppose we wish to learn a function f : Cm → Cn by optimizing the squared error\nL(z) = |z|2 = zz , (1) where (·) denotes the complex conjugate operator. Note that zz = (x+ iy)(x− iy) = x2 + y2 ∈ R and therefore the objective function is real-valued even though z is complex-valued.\nReal-valued functions of complex variables are non-holomorphic and therefore their complex derivative is undefined. However if we denote L(z) = u(x, y) + iv(x, y) with u : R→ R and v : R→ R and u and v are real-analytic (u and v are differentiable) functions then it is possible to find a stationary point in the objective function. Stated more simply, we may perform gradient descent with a real-valued cost function of complex variables even though the function does not have a complex derivative.\nIn this paper we apply the Wirtinger derivative (Wirtinger, 1927) to compute the gradient (Brandwood, 1983). Doing so allows us to perform differentiation on functions that are not complexanalytic but are real-analytic. It also provides a means for easily composing a combination of holomorphic and non-holomorphic functions within the computational graph of a neural network. Finally, by taking advantage of basic properties of the Wirtinger derivative, we perform gradient descent using two Jacobians per computational node.\nDue to space limitations the following summary is necessarily brief. A great overview of the core mechanics of complex-valued nets and the Wirtinger derivative is found in Mandic & Goh (2009). This and other literature are built on the theory developed in Brandwood (1983) and van den Bos (1994) for optimization of complex-valued nets using respectively first- and second-order derivatives with Wirtinger calculus. For a deeper discussion of Wirtinger calculus and optimization techniques we refer the reader to Kreutz-Delgado (2009); Li & Adali (2008). Finally Amin et al. (2011); Amin & Murase (2013) advocate a framework for composing holomorphic and non-holomorphic functions in complex-valued nets."
    }, {
      "heading" : "2.1 COMPLEX-VALUED AND REAL-VALUED NETS",
      "text" : "The components of a complex-valued number can be represented as a bivariate real number, so it is natural to ask why a complex-valued representation may be preferred. A multiplication of values in the real domain yields scaling. A multiplication of complex values yields scaling and rotation. Hence if we wish to model magnitude and phase jointly, it may be more natural to do so by using a complex representation.\nThere are cases when we may wish to model real-valued processes in the complex domain. For instance one cannot determine the instantaneous frequency or amplitude of a real-valued periodic\nwaveform from a single sample. Applying the Hilbert transform yields a complex-valued waveform with the same positive frequency components. However we suggest that complex-valued networks may also learn important relationships on instantaneous frequency and amplitude."
    }, {
      "heading" : "2.2 ACTIVATION FUNCTIONS",
      "text" : "Activation functions that are bounded and differentiable are generally desirable for training neural networks. (The rectified linear unit is a notable exception for boundedness.) Due to Liouville’s theorem, the only entire (holomorphic over the entire complex domain) function that is bounded is a constant. Thus we must choose between boundedness and differentiability for complex nets.\nSplit-complex activation functions operate on the real and imaginary or phase and magnitude components independently and merge the outputs together. Such functions are not holomorphic. However it is easy to define a bounded split-complex activation function, for example Geourgiou and Koutsougeras’ magnitude squashing activation function (Georgiou & Koutsougeras, 1992). It is suggested by Mandic & Goh (2009) that the split phase-magnitude and real-imaginary approaches are appropriate when we can assume rotational or cartesian symmetry of the data, respectively.\nAlternatively we may choose to use fully complex activation functions that are bounded almost everywhere. Certain elementary transcendental functions have been identified which provide squashing-type nonlinear discrimination with well defined first-order derivatives (Kim & Adalı, 2003). These functions have singularities, but with proper treatment of weights or using other regularization mechanisms singularities may be avoided. Figure 1 shows magnitude and phase surface plots for a complex tanh activation function and Geourgiou and Koutsougeras’ activation function. The tanh activation has regularly spaced singularities along the imaginary axis beyond the limits of the plots."
    }, {
      "heading" : "3 WIRTINGER FRAMEWORK FOR GRADIENT DESCENT",
      "text" : "This section outlines the routine for optimizing an arbitrary complex-valued neural network using the Wirtinger derivative and gradient descent. The network has a real-valued objective function of complex variables. It may have any combination of holomorphic and non-holomorphic activation functions. Wirtinger calculus (also known as CR Calculus in some texts) facilitates defining a computational graph that can be modularized as in many popular deep learning libraries, thereby allowing the construction of deep or temporal networks having many layers.\nIn the following subsection, the core concepts of Wirtinger calculus are reviewed. The following subsection describes the framework for building a computational graph and performing gradient descent."
    }, {
      "heading" : "3.1 WIRTINGER DERIVATIVES",
      "text" : "Define z ∈ C and x, y ∈ R with f(z) = g(x, y) = u(x, y) + iv(x, y). We extend the definition of f to include the complex conjugate of its input variable so that\nf(z) = f(z, z) = g(x, y) = u(x, y) + iv(x, y)\nz = x+ iy\nz = x− iy (2) Using this definition, the R-derivative and R-derivative of f are defined as:\n∂f\n∂z ∣∣∣∣ z is constant and ∂f ∂z ∣∣∣∣ z is constant\n(3)\nWe note that the R-derivative and R-derivative are formalisms, as z cannot be independent of z. However we treat one as constant when computing the derivative of other, applying the normal rules of calculus. Using these definitions, Brandwood (1983) shows that\n∂f ∂z = 1 2\n( ∂f\n∂x − i∂f ∂y\n) and ∂f\n∂z =\n1\n2\n( ∂f\n∂x + i\n∂f\n∂y\n) (4)\nWe note that the R-derivative is equal to zero for any holomorphic function. Recall the CauchyRiemann equations which state that for the complex derivative of f(z) = g(x, y) = u(x, y) + iv(x, y) to exist, the following identities must hold:\n∂u ∂x = ∂v ∂y and ∂v ∂x = −∂u ∂y (5)\nIf we expand the right had side of the R-derivative and substitute the Cauchy-Riemann equations the R-derivative vanishes. Thus an equivalent (and intuitive) statement about a holomorphic function is that it does not depend on the conjugate of its input. As an example, consider the loss function in Eq. (1). It is real-valued and therefore non-holomorphic and it clearly depends on the conjugate of its input variable, having R- and R-derivatives of z and z, respectively.\nIt is further shown by Brandwood that if f : C → R is a real-valued function, either ∂f∂z = 0 or ∂f ∂z = 0 is a necessary and sufficient condition for f to have a stationary point. By extension if f : CN → R is a real-valued function of a complex vector z = (z1, z2, . . . , zN )T and we define the cogradient and conjugate cogradient\n∇z = (∂/∂z1, ∂/∂z2, . . . , ∂zN )T (6) ∇z = (∂/∂z1, ∂/∂z2, . . . , ∂zN )T (7)\nthen ∇zf = 0 or ∇zf = 0 are necessary and sufficient to determine a stationary point. Finally, Brandwood uses Schwarz’s inequality to show that the maximum rate of change of f is in the direction of the conjugate cogradient∇zf . Using these definitions, we can perform gradient descent with the conjugate cogradient operator."
    }, {
      "heading" : "3.2 THE COMPUTATIONAL GRAPH",
      "text" : "We wish to perform gradient descent on a computational graph having a real-valued cost function and an arbitrary composition of holomorphic and non-holomorphic functions. Performing backpropagation on such a graph can be unwieldy if we choose to repeatedly switch between complex and real-valued representations of the graph. If we remain in the complex domain for all computations and use Wirtinger calculus, it is easier to build a modular framework that is useful for deep networks.\nConsider a complex-valued function,\nF (z, z) = [f1(z, z), f2(z, z), . . . , fM (z, z)] T with (8)\nz = [z1, z2, . . . , zN ] T and (9)\nz = [z1, z2, . . . , zN ] T . (10)\nWe define the Jacobian matrices,\nJF , ∂F (z, z)\n∂z (11)\nJcF , ∂F (z, z)\n∂z (12)\nA deep neural network is constructed from a composition of several non-scalar functions. Suppose we have a composition of functions (F ◦G◦H)(z, z), with F being a real-valued (non-holomorphic cost function), G being a non-holomorphic complex-valued function, and H being a holomorphic function. We would like to compute the gradient of F with respect to z. Figure 2 shows the dependency graph for back-propagating the gradient. The top part of the figure shows the Jacobian matrices for each stage of back-propagation. The naive method requires keeping track of four dependencies for every function in the graph.\nWe need keep track of only two partial derivatives for each function, as shown in the bottom part of Figure 2 (Amin et al., 2011; Li & Adali, 2008). Keeping in mind that F is a real-valued function of complex variables and that H is holomorphic s.t. ∂H∂z = 0, we apply the chain rule to the Jacobian matrices:\nJcF ◦G = JFJ c G + J c FJG\n= 2< (JF ) ( JcG + JG ) (13)\nJcF ◦G◦H = J c F ◦GJH (14)\nMore generally, given arbitrary functions F and G in the computational graph, we compose their Jacobians in the following way (Kreutz-Delgado, 2009):\nJF ◦G = JFJG + J c FJ c G\nJcF ◦G = JFJ c G + J c FJG"
    }, {
      "heading" : "4 EXPERIMENTS",
      "text" : "The Discrete Fourier Transform of N regularly-sampled points on a waveform yields complex coefficients ofN orthogonal complex sinusoids. However the Fourier representation may not be the best\ntransform for a given learning task. Deep networks are regularly trained to learn data representations that are more suitable than hand-picked features. In this experiment, we generate real and complexvalued waveforms having wide-band spectral components at multiple phases and magnitudes. We train recurrent complex- and real-valued models to predict the N -th frame of a waveform given the first N − 1 frames. In the following subsections we detail the data, models, and results."
    }, {
      "heading" : "4.1 DATA",
      "text" : "We generated four synthetic datasets having wide-band frequency spectra with random phases: Sawtooth-Like, Sawtooth-Like (Analytic), Inharmonic, and Inharmonic (Analytic). Each dataset had unique training, validation, and testing partitions. The training sets consisted of 10,000 observations split into 10 batches. The validation and test sets each had 1 batch of 1,000 observations.\nDatasets were generated as described below. Each observation (waveform) has 1024 samples with a Nyquist frequency denoted Ω. The waveform was split into four non-overlapping rectangularwindowed frames of 256 samples. The first three frames were used as input to the model and the remaining frame is reserved as ground truth for inference."
    }, {
      "heading" : "4.1.1 SAWTOOTH-LIKE",
      "text" : "Each waveform has a fundamental frequency drawn uniformly from the range [0,Ω) There are harmonics n = (2, . . . , N) above the fundamental frequency, with all harmonic frequencies being less than Ω and each harmonic having an amplitude of 1/n. All sinusoidal components have a random phase drawn from a uniform distribution [0, 1). Each real-valued waveform is made complex by adding a zero-valued imaginary component.\nWe refer to these waveforms as “Sawtooth-Like” because they have the same spectral components of a a band-limited sawtooth waveform. However since the phases of the spectral components are scrambled, the time-amplitude waveforms do not look like sawtooth waveforms. Each frame of an observation has a number of waveform periods in the range of [0, 128]. The expected number of periods per frame is 64."
    }, {
      "heading" : "4.1.2 SAWTOOTH-LIKE (ANALYTIC)",
      "text" : "Waveforms were generated as above, but with the following modification. For each frequency component with frequency ω and phase φ, a sinusoidal component is added to the imaginary axes having frequency ω and phase φ− π/2. An analytic signal encodes instantaneous magnitude and phase. In cases where a real-valued network was trained with this dataset, the real and imaginary parts of the data were split and hence there were twice the number of inputs and outputs as other experiments."
    }, {
      "heading" : "4.2 INHARMONIC",
      "text" : "Inharmonic waveforms were generated with five spectral components, each having a frequency drawn from a uniform distribution in the range [0,Ω), a phase drawn from a uniform distribution in the range [0, 1), and an amplitude of 1/5. Hence the phases of the individual components are random but not drawn from the full available range of [0, 2π). These waveforms are unlikely to exhibit periodicity."
    }, {
      "heading" : "4.3 INHARMONIC (ANALYTIC)",
      "text" : "Analytic waveforms were generated as above using the same methodology as for the Sawtooth-Like (Analytic) dataset."
    }, {
      "heading" : "4.4 MODELS",
      "text" : "We trained real- and complex-valued neural networks having one hidden recurrent layer of size 256. The input and output layers had 256 units each, with the exception of real-valued networks trained on the analytic datasets; these had 512 inputs and outputs. All models had weights and biases. Hence models had either 197,376 or 328,704 trainable parameters. Models were trained with a tanh activation function on the hidden layer and linear activation on the output layer."
    }, {
      "heading" : "4.5 TRAINING",
      "text" : "Training was performed for exactly 1000 epochs using mini-batch stochastic gradient descent with a momentum of 0.9 and the mean squared cost function. We employed a learning rate with power scheduling decay (Senior et al., 2013).\nWe and other authors have found that complex-valued networks are extremely sensitive to initial conditions and learning rates (Zimmermann et al., 2011). In order to facilitate finding a good setting of hyperparameters, we performed hyperparameter optimization using Spearmint (Snoek et al., 2012) for the following parameters: initial weight scaling, learning rate, and learning rate decay half life. For each dataset, 100 real- and complex-valued models were trained with unique hyperparameter settings and initial weights. The final model was chosen using the best performance on the validation set."
    }, {
      "heading" : "4.6 RESULTS",
      "text" : ""
    }, {
      "heading" : "4.6.1 OVERALL COMPARISON",
      "text" : "Each dataset was trained, validated, and tested on a complex- and real-valued network. We had hoped that complex-valued networks would outperform real-valued nets. In most cases, the final error between complex and real nets was comparable. However in all experiments, the real-valued networks had a lower final test error. Table 1 shows that both real and complex valued networks perform best on the Sawtooth-Like dataset. We were not surprised by this result. Considering that this dataset consists of only harmonically related spectral components, we presume that this dataset is easier to learn than the Inharmonic datasets.\nWe were surprised that both real and complex-valued networks had difficulty learning the Analytic datasets. These datasets encode instantaneous frequency and phase, and we therefore expected that they would work well with the complex valued network. It is possible that the fully complex tanh activation function is inappropriate for this dataset since instantaneous frequency does not change between inputs and outputs. In future work we will examine the performance of other activation functions on this dataset."
    }, {
      "heading" : "4.6.2 OPTIMIZATION",
      "text" : "The left pane of Figure 3 shows the sorted error across hyperparameter settings employed with the Sawtooth-Like dataset. We find it notable that most settings perform relatively poorly. There were only a few settings for both types of networks that achieved optimal performance. This figure underscores how sensitivities both types of networks are to hyperparameter settings.\nThe right pane shows the validation error across epochs for the Sawtooth-Like dataset. Note the discontinuity in the error curve for the complex-valued net. The complex valued nets are quite difficult to train and can easily approach regions of instability. We believe this is due to the singularities of the tanh function."
    }, {
      "heading" : "4.6.3 FILTERS",
      "text" : "We examined the input-to-hidden weights of the models. We found that despite the worse performance of complex-valued networks, they learned filters that are easily relatable to the datasets. Figure 4 shows the magnitude frequency responses of the first three input-to-hidden weights for the Sawtooth-Like (Analytic) (left) and Inharmonic (right) datasets. Observe that the frequency re-\nsponse of the complex model for the Sawtooth-Like dataset exhibits harmonically spaced peaks in the spectrum. The filters from the real-valued network are much noisier and it is difficult to discern any harmonic spacing. The filters of the complex model trained on the Inharmonic dataset also show high selectivity for a few spectral peaks, whereas, the filters learned by the real-valued model show selectivity but to a more limited degree."
    }, {
      "heading" : "5 CONCLUSIONS",
      "text" : "Despite potentially widespread applicability to machine learning tasks, the deep learning and representational learning communities have not fully embraced complex-valued nets. We argue that the mathematical conveniences of Wirtinger calculus offer a means for building a modular library for training complex-valued nets. Towards this end, we composed several synthetic datasets and compared the performance of complex- and real-valued nets. We found that complex-valued nets performed about as well as, but not better than, real-valued counterparts. We highlighted the fact that training complex-valued nets brings different challenges, including difficulties of boundedness and singularities in the activation functions. Finally we showed that despite poorer performance, complex-valued nets learn filter representations that are adapted to the domain of the data.\nIt is obvious that there are many challenges to successfully training complex-valued nets. We must find good methods for avoiding the singularities in holomorphic cost functions. There is no complex equivalent to the rectified linear unit. The models are extremely sensitive to initial conditions of the weights and to the learning rate. We will continue to explore these topics in future work. Our experiments were conducted on GPUs using a modified branch of the Chainer deep learning framework.1. As we continue to investigate complex-valued networks, we intend to develop our framework further and release it to the community.\n1http://docs.chainer.org/en/stable/index.html"
    } ],
    "references" : [ {
      "title" : "Learning algorithms in complex-valued neural networks using wirtinger calculus. In Complex-Valued Neural Networks: Advances and Applications, pp. 75–102",
      "author" : [ "Amin", "Md. Faijul", "Murase", "Kazuyuki" ],
      "venue" : null,
      "citeRegEx" : "Amin et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Amin et al\\.",
      "year" : 2013
    }, {
      "title" : "Wirtinger calculus based gradient descent and levenberg-marquardt learning algorithms in complex-valued neural networks",
      "author" : [ "Amin", "Md.Faijul", "MuhammadIlias", "A.Y.H. Al-Nuaimi", "Murase", "Kazuyuki" ],
      "venue" : "Neural Information Processing,",
      "citeRegEx" : "Amin et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Amin et al\\.",
      "year" : 2011
    }, {
      "title" : "A complex gradient operator and its application in adaptive array theory",
      "author" : [ "D.H. Brandwood" ],
      "venue" : "Communications, Radar and Signal Processing, IEE Proceedings F,",
      "citeRegEx" : "Brandwood,? \\Q1983\\E",
      "shortCiteRegEx" : "Brandwood",
      "year" : 1983
    }, {
      "title" : "Complex domain backpropagation",
      "author" : [ "G.M. Georgiou", "C. Koutsougeras" ],
      "venue" : "Circuits and Systems II: Analog and Digital Signal Processing, IEEE Transactions on,",
      "citeRegEx" : "Georgiou and Koutsougeras,? \\Q1992\\E",
      "shortCiteRegEx" : "Georgiou and Koutsougeras",
      "year" : 1992
    }, {
      "title" : "Complex-valued forecasting of wind profile",
      "author" : [ "S.L. Goh", "M. Chen", "D.H. Popović", "K. Aihara", "D. Obradovic", "D.P. Mandic" ],
      "venue" : "Renewable Energy, 31(11):1733–1750,",
      "citeRegEx" : "Goh et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Goh et al\\.",
      "year" : 2006
    }, {
      "title" : "Complex-Valued Neural Networks",
      "author" : [ "A. Hirose" ],
      "venue" : "Studies in computational intelligence. SpringerVerlag,",
      "citeRegEx" : "Hirose,? \\Q2006\\E",
      "shortCiteRegEx" : "Hirose",
      "year" : 2006
    }, {
      "title" : "Complex-Valued Neural Networks: Advances and Applications",
      "author" : [ "A. Hirose" ],
      "venue" : null,
      "citeRegEx" : "Hirose,? \\Q2013\\E",
      "shortCiteRegEx" : "Hirose",
      "year" : 2013
    }, {
      "title" : "Generalization characteristics of complex-valued feedforward neural networks in relation to signal coherence",
      "author" : [ "A. Hirose", "S. Yoshida" ],
      "venue" : "Neural Networks and Learning Systems, IEEE Transactions on,",
      "citeRegEx" : "Hirose and Yoshida,? \\Q2012\\E",
      "shortCiteRegEx" : "Hirose and Yoshida",
      "year" : 2012
    }, {
      "title" : "Approximation by fully complex multilayer perceptrons",
      "author" : [ "T Kim", "T. Adalı" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Kim and Adalı,? \\Q2003\\E",
      "shortCiteRegEx" : "Kim and Adalı",
      "year" : 2003
    }, {
      "title" : "The Complex Gradient Operator and the CR-Calculus",
      "author" : [ "K. Kreutz-Delgado" ],
      "venue" : "ArXiv e-prints,",
      "citeRegEx" : "Kreutz.Delgado,? \\Q2009\\E",
      "shortCiteRegEx" : "Kreutz.Delgado",
      "year" : 2009
    }, {
      "title" : "Complex-valued adaptive signal processing using nonlinear functions",
      "author" : [ "Li", "Hualiang", "Adali", "Tülay" ],
      "venue" : "EURASIP J. Adv. Signal Process,",
      "citeRegEx" : "Li et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2008
    }, {
      "title" : "Complex valued nonlinear adaptive filters: noncircularity, widely linear and neural models",
      "author" : [ "Mandic", "Danilo P", "Goh", "Vanessa Su Lee" ],
      "venue" : null,
      "citeRegEx" : "Mandic et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Mandic et al\\.",
      "year" : 2009
    }, {
      "title" : "An empirical study of learning rates in deep neural networks for speech recognition",
      "author" : [ "A. Senior", "G. Heigold", "M. Ranzato", "Yang", "Ke" ],
      "venue" : "In Acoustics, Speech and Signal Processing (ICASSP),",
      "citeRegEx" : "Senior et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Senior et al\\.",
      "year" : 2013
    }, {
      "title" : "Practical bayesian optimization of machine learning algorithms",
      "author" : [ "Snoek", "Jasper", "Larochelle", "Hugo", "Adams", "Ryan P" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Snoek et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Snoek et al\\.",
      "year" : 2012
    }, {
      "title" : "Unconstrained optimization of real functions in complex variables",
      "author" : [ "Sorber", "Laurent", "Barel", "Marc Van", "Lathauwer", "Lieven De" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "Sorber et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Sorber et al\\.",
      "year" : 2012
    }, {
      "title" : "Ultrawideband direction-of-arrival estimation using complex-valued spatiotemporal neural networks",
      "author" : [ "K. Terabayashi", "R. Natsuaki", "A. Hirose" ],
      "venue" : "Neural Networks and Learning Systems, IEEE Transactions on,",
      "citeRegEx" : "Terabayashi et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Terabayashi et al\\.",
      "year" : 2014
    }, {
      "title" : "Complex gradient and hessian",
      "author" : [ "A. van den Bos" ],
      "venue" : "IEE Proceedings - Vision, Image and Signal Processing,",
      "citeRegEx" : "Bos,? \\Q1994\\E",
      "shortCiteRegEx" : "Bos",
      "year" : 1994
    }, {
      "title" : "Zur formalen Theorie der Funktionen von mehr komplexen Ver anderlichen",
      "author" : [ "W. Wirtinger" ],
      "venue" : "Mathematische Annalen,",
      "citeRegEx" : "Wirtinger,? \\Q1927\\E",
      "shortCiteRegEx" : "Wirtinger",
      "year" : 1927
    }, {
      "title" : "Comparison of the complex valued and real valued neural networks trained with gradient descent and random search algorithms",
      "author" : [ "Zimmermann", "Hans-Georg", "Minin", "Alexey", "Kusherbaeva", "Victoria" ],
      "venue" : "In European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning (ESANN),",
      "citeRegEx" : "Zimmermann et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Zimmermann et al\\.",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "For example wind measurements may use complex-valued data to represent joint measurements of magnitude and direction (Goh et al., 2006).",
      "startOffset" : 117,
      "endOffset" : 135
    }, {
      "referenceID" : 15,
      "context" : "Direction of arrival is naturally modeled in ultrawideband communications using complex values (Terabayashi et al., 2014).",
      "startOffset" : 95,
      "endOffset" : 121
    }, {
      "referenceID" : 18,
      "context" : "Despite such obstacles, research on CVNNs is growing steadily, with new theoretical results (Zimmermann et al., 2011; Sorber et al., 2012; Hirose & Yoshida, 2012) appearing on the heels of comprehensive treatments in recent texts (Hirose, 2006; Mandic & Goh, 2009; Hirose, 2013).",
      "startOffset" : 92,
      "endOffset" : 162
    }, {
      "referenceID" : 14,
      "context" : "Despite such obstacles, research on CVNNs is growing steadily, with new theoretical results (Zimmermann et al., 2011; Sorber et al., 2012; Hirose & Yoshida, 2012) appearing on the heels of comprehensive treatments in recent texts (Hirose, 2006; Mandic & Goh, 2009; Hirose, 2013).",
      "startOffset" : 92,
      "endOffset" : 162
    }, {
      "referenceID" : 5,
      "context" : ", 2012; Hirose & Yoshida, 2012) appearing on the heels of comprehensive treatments in recent texts (Hirose, 2006; Mandic & Goh, 2009; Hirose, 2013).",
      "startOffset" : 99,
      "endOffset" : 147
    }, {
      "referenceID" : 6,
      "context" : ", 2012; Hirose & Yoshida, 2012) appearing on the heels of comprehensive treatments in recent texts (Hirose, 2006; Mandic & Goh, 2009; Hirose, 2013).",
      "startOffset" : 99,
      "endOffset" : 147
    }, {
      "referenceID" : 1,
      "context" : "In this paper, we follow a more general framework (Amin et al., 2011; Amin & Murase, 2013) for building CVNNs, both deep and temporal, that allows for activation functions that are composed from combinations of both holomorphic and non-holomorphic functions.",
      "startOffset" : 50,
      "endOffset" : 90
    }, {
      "referenceID" : 17,
      "context" : "In this paper we apply the Wirtinger derivative (Wirtinger, 1927) to compute the gradient (Brandwood, 1983).",
      "startOffset" : 48,
      "endOffset" : 65
    }, {
      "referenceID" : 2,
      "context" : "In this paper we apply the Wirtinger derivative (Wirtinger, 1927) to compute the gradient (Brandwood, 1983).",
      "startOffset" : 90,
      "endOffset" : 107
    }, {
      "referenceID" : 0,
      "context" : "In this paper we apply the Wirtinger derivative (Wirtinger, 1927) to compute the gradient (Brandwood, 1983). Doing so allows us to perform differentiation on functions that are not complexanalytic but are real-analytic. It also provides a means for easily composing a combination of holomorphic and non-holomorphic functions within the computational graph of a neural network. Finally, by taking advantage of basic properties of the Wirtinger derivative, we perform gradient descent using two Jacobians per computational node. Due to space limitations the following summary is necessarily brief. A great overview of the core mechanics of complex-valued nets and the Wirtinger derivative is found in Mandic & Goh (2009). This and other literature are built on the theory developed in Brandwood (1983) and van den Bos (1994) for optimization of complex-valued nets using respectively first- and second-order derivatives with Wirtinger calculus.",
      "startOffset" : 91,
      "endOffset" : 719
    }, {
      "referenceID" : 0,
      "context" : "In this paper we apply the Wirtinger derivative (Wirtinger, 1927) to compute the gradient (Brandwood, 1983). Doing so allows us to perform differentiation on functions that are not complexanalytic but are real-analytic. It also provides a means for easily composing a combination of holomorphic and non-holomorphic functions within the computational graph of a neural network. Finally, by taking advantage of basic properties of the Wirtinger derivative, we perform gradient descent using two Jacobians per computational node. Due to space limitations the following summary is necessarily brief. A great overview of the core mechanics of complex-valued nets and the Wirtinger derivative is found in Mandic & Goh (2009). This and other literature are built on the theory developed in Brandwood (1983) and van den Bos (1994) for optimization of complex-valued nets using respectively first- and second-order derivatives with Wirtinger calculus.",
      "startOffset" : 91,
      "endOffset" : 800
    }, {
      "referenceID" : 0,
      "context" : "In this paper we apply the Wirtinger derivative (Wirtinger, 1927) to compute the gradient (Brandwood, 1983). Doing so allows us to perform differentiation on functions that are not complexanalytic but are real-analytic. It also provides a means for easily composing a combination of holomorphic and non-holomorphic functions within the computational graph of a neural network. Finally, by taking advantage of basic properties of the Wirtinger derivative, we perform gradient descent using two Jacobians per computational node. Due to space limitations the following summary is necessarily brief. A great overview of the core mechanics of complex-valued nets and the Wirtinger derivative is found in Mandic & Goh (2009). This and other literature are built on the theory developed in Brandwood (1983) and van den Bos (1994) for optimization of complex-valued nets using respectively first- and second-order derivatives with Wirtinger calculus.",
      "startOffset" : 91,
      "endOffset" : 823
    }, {
      "referenceID" : 0,
      "context" : "In this paper we apply the Wirtinger derivative (Wirtinger, 1927) to compute the gradient (Brandwood, 1983). Doing so allows us to perform differentiation on functions that are not complexanalytic but are real-analytic. It also provides a means for easily composing a combination of holomorphic and non-holomorphic functions within the computational graph of a neural network. Finally, by taking advantage of basic properties of the Wirtinger derivative, we perform gradient descent using two Jacobians per computational node. Due to space limitations the following summary is necessarily brief. A great overview of the core mechanics of complex-valued nets and the Wirtinger derivative is found in Mandic & Goh (2009). This and other literature are built on the theory developed in Brandwood (1983) and van den Bos (1994) for optimization of complex-valued nets using respectively first- and second-order derivatives with Wirtinger calculus. For a deeper discussion of Wirtinger calculus and optimization techniques we refer the reader to Kreutz-Delgado (2009); Li & Adali (2008).",
      "startOffset" : 91,
      "endOffset" : 1062
    }, {
      "referenceID" : 0,
      "context" : "In this paper we apply the Wirtinger derivative (Wirtinger, 1927) to compute the gradient (Brandwood, 1983). Doing so allows us to perform differentiation on functions that are not complexanalytic but are real-analytic. It also provides a means for easily composing a combination of holomorphic and non-holomorphic functions within the computational graph of a neural network. Finally, by taking advantage of basic properties of the Wirtinger derivative, we perform gradient descent using two Jacobians per computational node. Due to space limitations the following summary is necessarily brief. A great overview of the core mechanics of complex-valued nets and the Wirtinger derivative is found in Mandic & Goh (2009). This and other literature are built on the theory developed in Brandwood (1983) and van den Bos (1994) for optimization of complex-valued nets using respectively first- and second-order derivatives with Wirtinger calculus. For a deeper discussion of Wirtinger calculus and optimization techniques we refer the reader to Kreutz-Delgado (2009); Li & Adali (2008). Finally Amin et al.",
      "startOffset" : 91,
      "endOffset" : 1081
    }, {
      "referenceID" : 0,
      "context" : "Finally Amin et al. (2011); Amin & Murase (2013) advocate a framework for composing holomorphic and non-holomorphic functions in complex-valued nets.",
      "startOffset" : 8,
      "endOffset" : 27
    }, {
      "referenceID" : 0,
      "context" : "Finally Amin et al. (2011); Amin & Murase (2013) advocate a framework for composing holomorphic and non-holomorphic functions in complex-valued nets.",
      "startOffset" : 8,
      "endOffset" : 49
    }, {
      "referenceID" : 2,
      "context" : "Using these definitions, Brandwood (1983) shows that",
      "startOffset" : 25,
      "endOffset" : 42
    }, {
      "referenceID" : 1,
      "context" : "We need keep track of only two partial derivatives for each function, as shown in the bottom part of Figure 2 (Amin et al., 2011; Li & Adali, 2008).",
      "startOffset" : 110,
      "endOffset" : 147
    }, {
      "referenceID" : 9,
      "context" : "More generally, given arbitrary functions F and G in the computational graph, we compose their Jacobians in the following way (Kreutz-Delgado, 2009): JF ◦G = JFJG + J c FJ c G JF ◦G = JFJ c G + J c FJG",
      "startOffset" : 126,
      "endOffset" : 148
    }, {
      "referenceID" : 12,
      "context" : "We employed a learning rate with power scheduling decay (Senior et al., 2013).",
      "startOffset" : 56,
      "endOffset" : 77
    }, {
      "referenceID" : 18,
      "context" : "We and other authors have found that complex-valued networks are extremely sensitive to initial conditions and learning rates (Zimmermann et al., 2011).",
      "startOffset" : 126,
      "endOffset" : 151
    }, {
      "referenceID" : 13,
      "context" : "In order to facilitate finding a good setting of hyperparameters, we performed hyperparameter optimization using Spearmint (Snoek et al., 2012) for the following parameters: initial weight scaling, learning rate, and learning rate decay half life.",
      "startOffset" : 123,
      "endOffset" : 143
    } ],
    "year" : 2015,
    "abstractText" : "Complex-valued neural networks (CVNNs) are an emerging field of research in neural networks due to their potential representational properties for audio, image, and physiological signals. It is common in signal processing to transform sequences of real values to the complex domain via a set of complex basis functions, such as the Fourier transform. We show how CVNNs can be used to learn complex representations of real valued time-series data. We present methods and results using a framework that can compose holomorphic and non-holomorphic functions in a multi-layer network using a theoretical result called the Wirtinger derivative. We test our methods on a representation learning task for real-valued signals, recurrent complex-valued networks and their real-valued counterparts. Our results show that recurrent complex-valued networks can perform as well as their realvalued counterparts while learning filters that are representative of the domain of the data.",
    "creator" : "LaTeX with hyperref package"
  }
}