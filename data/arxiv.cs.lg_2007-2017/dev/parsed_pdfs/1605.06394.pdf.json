{
  "name" : "1605.06394.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Bayesian Hyperparameter Optimization for Ensemble Learning",
    "authors" : [ "Julien-Charles Lévesque", "Christian Gagné", "Robert Sabourin" ],
    "emails" : [ "julien-charles.levesque.1@ulaval.ca" ],
    "sections" : [ {
      "heading" : null,
      "text" : "In this paper, we bridge the gap between hyperparameter optimization and ensemble learning by performing Bayesian optimization of an ensemble with regards to its hyperparameters. Our method consists in building a fixed-size ensemble, optimizing the configuration of one classifier of the ensemble at each iteration of the hyperparameter optimization algorithm, taking into consideration the interaction with the other models when evaluating potential performances. We also consider the case where the ensemble is to be reconstructed at the end of the hyperparameter optimization phase, through a greedy selection over the pool of models generated during the optimization. We study the performance of our proposed method on three different hyperparameter spaces, showing that our approach is better than both the best single model and a greedy ensemble construction over the models produced by a standard Bayesian optimization."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "For a long time, the tuning of hyperparameters for learning algorithms was solved by simple exhaustive methods such as grid search guided by cross-validation error. Grid search does work in practice, but it suffers from serious drawbacks such as a search space complexity that grows exponentially with the number of hyperparameters tuned. Recently, other strategies such as sequential model-based parameter optimization (Hutter et al., 2011), random search (Bergstra and Bengio, 2012), and Bayesian optimization (Snoek et al., 2012) have been shown to be better alternatives to grid search for non-trivial search spaces.\nWhile hyperparameter optimization focuses on the performance of a single model, it is generally accepted that en-\n∗julien-charles.levesque.1@ulaval.ca\nsembles can perform better than single classifiers, one of many striking examples being the winning entry of the Netflix challenge (Bell and Koren, 2007). More recent machine learning competitions such as Kaggle competitions are also often won by ensemble methods (Sun and Pfahringer, 2011). Given these previous results, it is logical to combine Bayesian hyperparameter optimization techniques with ensemble methods to further push generalization accuracy. Feurer et al. (2015a) performed post-hoc ensemble generation by reusing the product of a completed hyperparameter optimization, winning phase 1 of the ChaLearn AutoML challenge (Guyon et al., 2015). Lastly, Snoek et al. (2015) also constructed post-hoc ensembles of neural networks for image captioning.\nThese two lines of previous work make for a compelling argument to directly apply Bayesian optimization of hyperparameters for ensemble learning. Rather than trying to model the whole space of ensembles, which is likely hard and inefficient to optimize, we pose a performance model of the ensemble at hand when adding a new classifier with some given hyperparameters. This is achieved by reusing models previously assessed during the optimization, evaluating performance change induced by adding them one at a time to the ensemble. This allows us to compute observations of the true ensemble loss with regards to the hyperparameter values. These observations are used to condition a Bayesian optimization prior, creating mean and variance estimates over the hyperparameter space which will be used to optimize the configuration of a new classifier to add to the ensemble. Finally, we consider different possibilities to maintain and build the ensemble as the optimization progresses, and settle on a round-robin optimization of the classifiers in the ensemble. This ensemble optimization procedure comes at a very small additional cost compared with a regular Bayesian optimization of hyperparameter yet yields better generalization accuracy for the same number of trained models.\nWe evaluate our proposed approach on a benchmark of medium datasets for two different hyperparameter spaces, one consisting solely of SVM algorithms with different ker-\nar X\niv :1\n60 5.\n06 39\n4v 1\n[ cs\n.L G\n] 2\n0 M\nay 2\nnel types, and one larger space with various families of learning algorithms. In both search spaces, our approach is shown to outperform regular Bayesian optimization as well as post-hoc ensemble generation from pools of classifiers obtained by classical Bayesian optimization of hyperparameters. We also evaluate our approach on a search space of convolutional neural networks trained on the CIFAR-10 dataset. The proposed approach is also able to provide better performance in this case.\nThe paper is structured as follows: Section 2 presents the problem of Bayesian hyperparameter optimization and highlights some related work. Section 3 presents the main contributions of this paper, which can be summarized as a methodology for Bayesian optimization of ensembles through hyperparameter tuning. Finally, Section 4 presents the experiments and an analysis of the results."
    }, {
      "heading" : "2 HYPERPARAMETER OPTIMIZATION",
      "text" : "The behavior of a learning algorithm A is often tunable with regards to a set of external parameters, called hyperparameters γ = {γ1, γ2, . . . } ∈ Γ, which are not learned during training. The hyperparameter selection problem is one stage of a bi-level optimization problem, where the first objective is the tuning of the model’s parameters θ and the second objective is the performance with regards to the hyperparameters γ.\nThe procedure requires two datasets, one for training and one for hyperparameter optimization (also called validation), namely XT and XV , each assumed to be sampled i.i.d. from an underlying distribution D. The objective function to minimize for hyperparameter optimization takes the form of the empirical generalization error on XV :\nf(γ) = L(hγ |XV ) + (1)\nL(hγ |XV ) = 1\n|XV | |XV |∑ i=1 l0−1(hγ(xi), yi), (2)\nwhere is some noise on the observation of the generalization error, l0−1 is the zero-one loss function, and the model hγ is obtained by running the training algorithm with hyperparameters γ, hγ = A(XT , γ). Other loss functions could be applied, but unless otherwise specified, the loss function will be the zero-one loss.\nIn order to solve this problem, Bayesian optimization consists in posing a probabilistic regression model of the generalization error of trained models with respect to their hyperparameters γ, and exploiting this model to select new hyperparameters to explore. At each iteration, a model of f(γ) is conditioned on the set of previously observed hyperparameter values and associated losses {γi, L(hγi |XV )}ti=1. Selection of the next hyperparameters to evaluate is performed by maximizing an ac-\nquisition function a(γ|f(γ)), a criterion balancing exploration and exploitation given mean and variance estimates obtained from the model of f(γ). Among the model families for f(γ), two interesting choices are Gaussian Processes (Rasmussen and Williams, 2006; Snoek et al., 2012) and Random Forests (Hutter et al., 2011), both providing information about the mean and variance of the fitted distribution over the whole search space.\nA typical hyperparameter optimization is executed iteratively, subsequently generating a model of f(γ) from observations, selecting hyperparameter tuples γ to evaluate, training a classifier hγ with the given training data, evaluating it on the validation data, and looping until the maximum number of iterations or time budget is spent.\nRecent advances in hyperparameter optimization have primarily focused on making optimization faster, more accurate and applicable to a wider set of applications. In order to speed up convergence, Feurer et al. (2015b) have shown that hyperparameter optimization can be warm started with meta features about datasets. Touching on both speed and optimality, the design of better acquisition functions has seen a lot of interest, and predictive entropy search was shown to be less greedy than expected improvement in locating the optima of objective functions (HernándezLobato et al., 2014). New applications have also emerged, one notable example being the optimization of hyperparameters for anytime algorithms with freeze-and-thaw Bayesian optimization (Swersky et al., 2014)."
    }, {
      "heading" : "2.1 HYPERPARAMETER OPTIMIZATION AND ENSEMBLES",
      "text" : "The idea of generating ensembles with hyperparameter optimization has already received some attention. Bergstra and Cox (2013) applied hyperparameter optimization in a multi-stage approach akin to boosting in order to generate better representations of images. Lacoste et al. (2014b) proposed the Sequential Model-based Ensemble Optimization (SMBEO) method to optimize ensembles based on bootstrapping the validation datasets to simulate multiple independent hyperparameter optimization processes and combined the results with the agnostic Bayesian combination method.\nThe process of hyperparameter optimization generates many trained models, and is usually concluded by selecting a model according to the hold-out (or cross-validation) generalization error γ∗ = arg minγ L(hγ |XV ). This single model selection at the end of the optimization is the equivalent of a point estimate, and it can result in overfitting. One strategy to limit this overfitting in the selection of a final model is to select multiple models instead of one, reducing the risk of overfitting and thus increasing the generalization performance.\nA simple strategy to build an ensemble from a hyperparameter optimization is to keep the trained models as they are generated for evaluation instead of discarding them (Feurer et al., 2015a). This effectively generates a pool of classifiers to combine at the end of the optimization, a process which is called post-hoc ensemble generation. Forward greedy selection has been shown to perform well in the context of pruning a pool of classifiers (Caruana et al., 2004). At each iteration, given a pool of trained classifiers H to select from, a new classifier is added to the ensemble, selected according to the minimum ensemble generalization error. At the first iteration, the classifier added is simply the single best classifier. At step t, given the ensemble E = {he1 , he2 , . . . , het−1}, the next classifier is chosen to minimize the empirical error on the validation dataset when added to E:\nht = arg min h∈H\nL(E ∪ {h}|XV ) (3)\nL(E ∪ {h}|XV ) = |XV |∑ i=0 l0−1 ( g(xi, E ∪ {h}), yi ) , (4)\nwhere g(xi, E) is a function combining the predictions of the classifiers in E on sample xi. In this case, the combination rule is majority voting, as it is less prone to overfitting (Caruana et al., 2004; Feurer et al., 2015a). Other possible combination rules include weighted voting, stacking (Kuncheva, 2004) and agnostic Bayesian combination (Lacoste et al., 2014a), to name only a few. Such an approach can be shown to perform better than the single best classifier produced by the hyperparameter optimization, due in part to a reduction of the classifiers’ variance through combination."
    }, {
      "heading" : "3 ENSEMBLE OPTIMIZATION",
      "text" : "In this work, we aim at directly optimizing an ensemble of classifiers through Bayesian hyperparameter optimization. The strategies discussed in the previous section mostly aimed at reusing the product of a completed hyperparameter optimization after the fact. The goal is to make an online selection of hyperparameters that could be more interesting for an ensemble, but which do not necessarily maximize the objective function of Equation 1 on their own. Directly posing a model on the space of all possible ensembles of a given size f(E) = f(γ1, . . . , γm) would result in a very hard and inefficient optimization problem, effectively duplicating the training of many models.\nIn order to palliate this, we propose a more focused approach. We define the objective function to be the performance of a given ensemble E when it is augmented with a new classifier trained with hyperparameters γ, or hγ . In other words, the objective function is the empirical error provided by adding a model hγ to the ensemble E:\nf(γ|E) = L(E ∪A(γ,XT )|XV ), (5)\nagain using the empirical loss on a hold-out validation set XV . Contrarily to the post-hoc ensemble generation, a probabilistic model is fit on the performance that a model trained with given hyperparameters would provide to the ensemble. In order to do this, two things are required: 1) an already established ensemble, and 2) a pool of trained classifiers available to compute observations of Equation 5 to condition our model on. Given the history of trained models so far H = {h1, . . . , ht} and an ensemble defined by a selection of classifiers within the history E = {he1 , . . . , hem}, the observations used to model f(γ) are obtained by reusing the trained classifiers in H , keeping E constant. Consequently, the objective function models the true ensemble error. Given a zero-one loss function and an empty ensemble E = ∅, Equation 5 falls back to a classical hyperparameter optimization problem, and the objective function will be minimized by the best hyperparameters for a single model γ∗.\nThe power of the suggested framework is illustrated with an example shown in Figure 1. This figure presents one iteration of ensemble optimization given 20 trained SVMs in a one-dimensional hyperparameter space, where the single hyperparameter is the width of the RBF kernel (σ ∈ [10−5, 105]). The dataset used is the Pima Indian Diabetes dataset available from UCI (Frank and Asuncion, 2010), with separate training and validation splits. The current ensemble E consists of two models selected by forward greedy selection shown by black circles. Ensemble evaluation and member selection strategies will be discussed further; for now let us assume a fixed ensemble. The red Xs represent the generalization error of single models and the red curve represents a Gaussian Process prior conditioned on those observations, in other words, a model of\nf(γ). The blue crosses represent the generalization error of the ensemble E when the corresponding classifiers are added to it, and the blue curve is again a Gaussian Process prior conditioned on the ensemble observations, or, more generally speaking, a model of f(γ|E). For both Gaussian Processes, the variance estimates are represented by shaded areas. The next step would be to apply an acquisition function with the ensemble mean and variance estimates to select the next hyperparameters to evaluate.\nFigure 1 shows that the objective function of an ensemble and a single classifier can be different. It can also be observed in this case that the generalization error of the ensemble is lower than that of a single model, hence the interest in optimizing ensembles directly."
    }, {
      "heading" : "3.1 ALTERNATE FORMULATIONS",
      "text" : "In order to be able to generalize over the space of hyperparameters, it is crucial to have an ensemble which does not contain all the classifiers inH , because if it did there would be no information added in the computation of Equation 5. A different problem formulation could be derived which compares classifiers with the whole pool of trained models, which would take the form f(γ|H) = q(hγ |H,XV ), where q(·) is a metric of performance for a classifier with regards to the pool. For example, a diversity inducing metric such as pairwise disagreement (Kuncheva, 2004) could be used, but this would lead to degenerate pools of classifiers, as diversity is easily increased by trivial and degenerate classifiers (voting all for one class or the other).\nMulti-objective optimization approaches have been considered for the maximization of both diversity and accuracy, a problem typically solved with genetic algorithms (Tsymbal et al., 2005). However, this problem formulation does not guarantee a better performing ensemble – only a more diverse pool of classifiers – with the hope that it will lead to better generalization performance. Directly optimizing diversity in classifier ensembles is questionable, and the evidence thus far is mixed (Didaci et al., 2013; Kuncheva, 2003).\nLastly, an inverse problem could be posed, measuring the difference in the generalization error by removing classifiers from the history one by one, and optimizing this difference. One problem with such a model is that it would be vulnerable to redundancy – very good hyperparameters present in multiple copies in the history would be falsely marked as having no impact on the generalization error.\nFor the reasons stated above, the best solution appears to be the use of a fixed ensemble which is maintained and updated as the optimization progresses. Thus it is possible to build an accurate Bayesian model of how well an ensemble would perform if we added a model trained with hyperparameters γ. This means that we need to store the trained\nAlgorithm 1 Ensemble Optimization Procedure Input: XT ,XV , B,m,A,Γ, L Output: H , history of models; E, the final ensemble\n1: H,G,E ← ∅ 2: for i ∈ 1, . . . , B do 3: j ← i mod m 4: E ← E \\ {hj} 5: Li ← {L(E ∪ h|XV )}h∈H 6: f(γ|E)← BO(G,Li) // Fit model 7: γi ← arg maxγ∈Γ a(γ|f(γ|E)) // Next hypers 8: hi ← A(XT , γi) // Train model 9: G← G ∪ {γi}\n10: H ← H ∪ {hi} 11: hj ← arg minh∈H L(E ∪ {h}) // New model at j 12: E ← E ∪ {hj} // Update ensemble 13: end for\nclassifiers in a database (or store their predictions on the validation and testing splits) to permit ensemble construction and evaluation in the subsequent iterations."
    }, {
      "heading" : "3.2 ENSEMBLE UPDATE",
      "text" : "The problem defined above is straightforward as long as the ensemble is given beforehand. In this section we will tackle the problem of building and updating the ensemble as the optimization progresses. To make things simpler, an ensemble size m will be fixed beforehand – this number can be fine-tuned at the end of the optimization. Each iteration of the optimization procedure will contain two steps: first the evaluation as described in Section 3, maintaining a fixed ensemble, and then an ensemble update step. Since members of the ensemble should be changed as the optimization moves forward and better models are found, a round-robin strategy will be used for the ensemble construction. The ensemble E will in fact consist of m fixed positions, and at every iteration i, the classifier at position j = (i mod m) will be removed from the ensemble before finding hyperparameters which minimize Equation 5 – effectively optimizing the classifier at this position for the given iteration. At the end of an iteration the ensemble is updated again greedily, selecting the new best classifier (it could be the same classifier or a better one). The whole procedure is described in Algorithm 1 and in Figure 2.\nIn addition, it is expected that some classifiers will specialize given the fixed state of a large part of the ensemble for each iteration. For instance, when replacing an individually strong classifier, another strong classifier will most likely be required. Figure 3 shows an example of optimization on a one-dimensional hyperparameter space run for 50 iterations, where an ensemble of five classifiers was optimized. The ensemble is represented by the five diamonds and its generalization error is shown by the dotted and dashed line at the bottom of the figure. Then, each of the five members\ni is independently removed and a Gaussian Process model is fit on the performance of an ensemble given the remaining models in the pool – this corresponds to the five colored lines of Figure 3. We can see from this figure that the hyperparameters which minimize the ensemble error are different for each slot in the ensemble, illustrating our concept."
    }, {
      "heading" : "3.3 COMPUTATIONAL COMPLEXITY",
      "text" : "The computational overhead of the proposed method comes mainly from the evaluation of the empirical error of ensembles. It is very small with regards to the cost of running most learning algorithms (which is usually quadratic or worse in the number of samples), and also with the cost of conditioning the probabilistic model on the observations (which is cubic in the number of iterations). The computation of the empirical error of ensembles takes place in step 5 in Algorithm 1. Given an ensemble of sizem, a validation dataset of size n, and a history of trained classifiers of size t, the complexity of this step is O(t(mn+ n)) = O(tmn) since it requires one pass over all models in the history, and for each of those the combination of the classifiers through majority voting (mn) and the computation of the empirical error n."
    }, {
      "heading" : "3.4 LOSS FUNCTION",
      "text" : "The objective function defined in Equation 5 contains a loss function, which up until now referred to the empirical loss of the ensemble, or the zero-one loss. However, the zeroone loss contains a strong discontinuity and can result in optimization procedures failing due to the majority voting combination. For instance, if all classifiers of the ensemble\nare wrong on some instances, replacing one of those poor classifiers with a better one will not make the ensemble correctly classify those instances, resulting in the same performance with regards to the objective function, even though this classifier would be a good choice for the ensemble.\nThe performance of ensembles will be considered with regards to their classification margin, which will let us derive a more suitable loss function (Schapire and Freund, 2012). Given an ensemble of classifiers E outputting label predictions on a binary problem Y ∈ {−1, 1}, the normalized margin for a sample {x, y} is defined as follows:\nM(E, x, y) = 1 |E| ∑ h∈E yh(x). (6)\nThe normalized margin M ∈ [−1, 1] takes the value 1 when all classifiers of the ensemble correctly classify the sample x, −1 when all the classifiers are wrong, and somewhere in between otherwise. In the case of multi-class problems, predictions of classifiers can be brought back to a binary domain by attributing 1 for a correct classification and −1 for a misclassification. The margin becomes:\nMmc(E, x, y) = 1 |E| ∑ h∈E [1− 2l0−1 (h(x), y)]. (7)\nWe will now derive some loss functions from the margin. The margin itself could be the objective, since it is desirable that the margin of the ensemble be high. It must be rescaled to really become a loss function, giving a margin-based loss function:\nlM (E, x, y) = 1−M(E, x, y)\n2 . (8)\nThis loss function should not be used to optimize an ensemble because it is directly maximized only by the accuracy of individual classifiers. In other words, given a validation dataset XV and a set of classifiers H to evaluate, the classifier minimizing Equation 8 is always the classifier with the lowest empirical error on its own, without regards to the ensemble performance. Therefore, the loss function must not give the same weight to all classifiers without regards to ensemble performance, while also being smooth. A loss function which achieves this is the margin-based loss function taken to the power of two:\nlM2(E, x, y) = (1−M(E, x, y))2\n4 . (9)\nThis places a higher emphasis on samples misclassified by the ensemble, and decreases the importance of samples as the margin grows closer to 1. Since it meets the required properties, the squared margin loss function will be used as the ensemble loss function in this work.\nFigure 4 shows an example of the two loss functions discussed in this section, the zero-one loss and the squared margin loss, applied on the same ensemble. Both losses have different scales, with the empirical error scale on the left and the squared margin scale on the right of the figure. We can see that these two loss functions lead to different optimal hyperparameters given the same ensemble. In other words, the hyperparameters minimizing the objective function according to the models of f(γ|E) are different with the two loss functions.\nConsidering that a loss function other than the empirical error will be used during the optimization, it will probably be beneficial to rebuild an ensemble from scratch using the final pool of classifiers trained once the optimization is over. Hence, a post-hoc ensemble generation will be performed after the hyperparameter optimization."
    }, {
      "heading" : "4 EXPERIMENTS",
      "text" : "We showcase the performance of our ensemble optimization approach on three different problems. The first two problems consist of different algorithms and hyperparameter spaces evaluated on the same benchmark of medium datasets available from the UCI repository, presented in Table 1. For every repetition, a different hold-out testing partition was sampled with 33% of the total dataset size (unless the dataset had a pre-specified testing split, in which case it was used for all repetitions). The remaining instances of the dataset were used to complete a 5-fold crossvalidation procedure. Final models are retrained on all the data available for training and validation.\nIn every case, the prior on the objective function is a Gaussian Process (GP) with Matérn-52 kernel using automatic relevance determination1. The noise, amplitude, and length-scale parameters are obtained through slice sampling (Snoek et al., 2012). The slice sampling of GP hyperparameters for ensemble optimization must be reinitialized at every iteration given that different ensembles E can change the properties of the optimized function drastically. The acquisition function is the Expected Improvement over the best solution found so far. The compared methods and their abbreviated names are the following:\n• Classical Bayesian optimization (BO-best). It returns a single model selected with argmin on validation performance (Snoek et al., 2012). • Post-hoc ensemble constructed from the pool of classifiers with Bayesian optimization (BO-post). The posthoc ensemble is initiated by picking the three best classifiers from the pool before proceeding with forward\n1Code from http://github.com/JasperSnoek/spearmint.\ngreedy selection – this form of warm starting is recommended in (Caruana et al., 2004) to reduce overfitting. • The proposed ensemble optimization method using the squared margin loss function (EO). • Post-hoc ensemble constructed from the pool of classifiers generated by ensemble optimization (EO-post). The same post-hoc procedure as with BO-post is executed.\nThe hyperparameter spaces can contain continuous, discrete, and categorical parameters (e.g., base classifier or kernel choice). In the case of categorical and discrete parameters, they are represented using a continuous parameter which is later discretized. This does not deal with the fact that hyperparameter spaces of different classifiers are disjoint and should not be modeled jointly, but since all the compared methods are using this same technique, the comparison is fair."
    }, {
      "heading" : "4.1 SVM SEARCH SPACE",
      "text" : "The models used in this benchmark are SVM models, and the parameterization includes the choice of the kernel along with the various hyperparameters needed per kernel. The hyperparameter space Γ optimized can be described as follows:\n• One hyperparameter for the kernel choice: linear, RBF, polynomial, or sigmoid; • Configurable error costC ∈ [10−5, 105] (for all kernels); • RBF and sigmoid kernels both have a kernel width pa-\nrameter γRBF ∈ [10−5, 105]; • Polynomial kernel has a degree parameter d ∈ [1, 10]; • Sigmoid and polynomial kernels both have an intercept\nparameter, c ∈ [10−2, 102].\nAll compared approaches optimized the same search space. Each method is given a budget of B = 200 iterations, or 200 hyperparameter tuples tested, to optimize hyperparameters with a 5-fold cross-validation procedure. The ensemble selection stage exploits this cross-validation procedure, considering the next classifier which reduces the most the generalization error over all the cross-validation folds. Selected hyperparameters are retrained on the whole training and validation data, and combined directly on the testing split to generate the generalization error values presented in this section. The ensemble optimization method is run with an ensemble size m = 12. This ensemble size was selected empirically and may not be optimal. Future work could investigate strategies to dynamically size the ensemble as the optimization progresses, with no fixed limit.\nThe generalization error on the test split for the selected methods is presented in Table 2, averaged over 10 repetitions. The last column shows the ranks of each method averaged over all datasets, where the best rank is 1 and the worst rank is 4.\nA Wilcoxon signed-rank test is used to measure the statistical significance of the results. The Wilcoxon signed-rank test is a strong statistical test for comparing methods across multiple datasets, which is an nonparametric version of the Student’s t-test that does not assume normal distributions and is less sensitive to outliers (Demšar, 2006). The input for the Wilcoxon test is the generalization error of a method i on each dataset d, averaged across the R repetitions:\nei = { 1\nR R∑ r=1 ei,d,r}d, (10)\nwhere ei,d,r is the generalization error produced by method i on dataset d at repetition r. The Wilcoxon test is then computed for all pairs of methods (ei, ej). The results of this procedure are shown in Table 3.\nFrom Table 2 we can see that it is beneficial to build an ensemble from the output of a classical hyperparameter optimization, as seen by the lower rank of BO-post with regards to BO-best. However, the performance improvement is not shown to be significant according to the Wilcoxon test. Both the ensemble optimization methods seem to outperform classical Bayesian optimization strategies in terms of rankings. The Wilcoxon test shows that EO and EO-post both performed significantly better than BO-best and BOpost. It should be noted that there is no significant difference between EO and EO-post, highlighting that there was not a significant gain from the post-hoc ensemble construction. Caruana et al., 2004 presented some strategies to reduce overfitting in the forward greedy procedure – such as bagging from the pool of models – which could be considered in order to achieve more with the same pool, although this is left for future work.\nAnother test which can be used to assess the performance of the evaluated methods is the Friedman test with posthoc tests on classifier ranks averaged across datasets. A Friedman test with the four methods presented in this section finds a significant difference between them with a pvalue of 5.5× 10−4. The Friedman test is then usually followed by a post-hoc test to measure whether the difference in ranks is above a critical difference level, such as the Nemenyi test (Demšar, 2006). Figure 5 shows the results of\nTable 2: Generalization error on SVM hyperparameter space, averaged over 10 repetitions, 5-fold cross-validation. Last column shows the rank of methods averaged over all datasets.\n1234\nBO-best BO-post EO-post EO\nFigure 5: Methods by rank and significant differences according to a post-hoc Nemenyi test with significance level at p = 0.05 for the SVM hyperparameter space.\nsuch a test, with methods linked by bold lines being found not significantly different by the test for a significance level of p = 0.05. The Nemenyi post-hoc test gives a more visual insight as to what is going on, but it is more sensitive to the pool of tested methods – the outcome of the test can change if new methods are inserted in the experiments. According to this test, EO and EO-post are both significantly different from BO-best, meaning that ensemble optimization is significantly better than the single best classifier returned by Bayesian optimization."
    }, {
      "heading" : "4.2 SCIKIT-LEARN MODELS SEARCH SPACE",
      "text" : "The same methods as in the previous section were repeated for a different search space consisting of multiple base algorithms, all available from scikit-learn2. The models and their hyperparameters are K nearest neighbors with the number of neighbors n neighbors in [1, 30]; RBF SVM with the penalty C logarithmically in [10−5, 105] and the width of the RBF kernel γRBF logarithmically in [10−5, 105]; linear SVM with the penalty C logarithmically scaled in [10−5, 105]; decision tree with the maximal depth max depth in [1, 10], the minimum number of examples in a node to split min samples split in [2, 100], and the minimum number of training examples in a leaf min samples leaf in [2, 100]; random forest with the number of trees n estimators in [1, 30], the maximal depth max depth in [1, 10], the minimum number of examples in a node to split min samples split in [2, 100], and the minimum number of training examples in a leaf min samples leaf in [2, 100]; AdaBoost with the number of weak learners n estimators in [1, 30]; Gaussian Naive Bayes (GNB) and Linear Discriminant Analysis (LDA) both without any hyperparameters; and\n2Available at http://scikit-learn.org/.\nQuadratic Discriminant Analysis (QDA) with the regularization reg param logarithmically in [10−3, 103].\nThe set of hyperparameter optimization strategies is the same as in the previous section, and the maximum number of iterations B is set to 100. Tables 4 and 5 show the same metrics and Wilcoxon pairwise tests as introduced in Section 4.1.\nConclusions are similar for this hyperparameter space, whereas the generation of a post-hoc ensemble is again shown to be beneficial to the generalization accuracy for both BO and EO. In this case the post-hoc ensemble for BO is found significantly better than the single best classifier according to the Wilcoxon test. The overall best method is EO, which significantly outperforms all other methods, including EO-post, as seen in Table 5. For some datasets, the ensemble optimization procedure achieves a large drop in the generalization error, see for example datasets letter (ltr), musk-2 (msk) and semeion (sem) in Table 4.\nA Friedman test on this suite of experiments is again run and found significant with a p-value of 1.5× 10−5. Figure 6 shows the results of the Nemenyi post-hoc test, with\nTable 4: Generalization error on the scikit-learn hyperparameter space, averaged over 10 repetitions, 5-fold crossvalidation. Last column shows the rank of methods averaged over all datasets.\nadlt bnk car ches ltr mgic msk p-blk pim sem spam s-gc s-im s-sh s-pl thy tita wine Ranks\nBO-best 14.70 11.04 4.77 25.73 4.72 12.06 2.13 3.41 23.25 8.82 5.80 23.15 3.72 0.12 26.91 1.19 22.74 35.96 3.36 BO-post 14.62 10.53 4.80 25.73 4.72 11.99 2.18 3.26 23.38 8.82 5.53 23.15 3.64 0.11 26.16 1.20 22.74 35.71 3.11 EO 14.35 10.30 0.81 19.39 2.81 12.38 0.28 2.80 24.09 4.37 5.02 22.75 2.84 0.08 22.69 1.21 20.66 35.19 1.67 EO-post 14.32 10.39 1.01 20.47 2.76 12.48 0.28 2.94 23.38 4.37 5.24 22.90 2.99 0.07 23.50 1.10 21.20 35.47 1.86\nmethods linked by bold lines being found not significantly different by the test for a significance level of p = 0.05. According to this test, EO-post and EO are significantly different from both BO and BO-best, meaning that ensemble optimization approaches significantly outperform both Bayesian optimization baselines."
    }, {
      "heading" : "4.3 CONVOLUTIONAL NEURAL NETWORKS",
      "text" : "Lastly, we evaluated the performance of our approach when fine-tuning the parameters of a convolutional neural network for the CIFAR-10 dataset. In order to have a reproducible baseline, the cuda-convet implementation was used with the reference model files given which achieves 18% generalization error on the testing dataset3. One batch of the training data was set aside for validation (batches 1-4 used for training, 5 for validation, and 6 for testing). Performance of the baseline configuration on the given training batches was around 22.4% ± 0.9 for 250 epochs of training. The parameters optimized were the same as in (Snoek et al., 2012), namely the learning rates and weight decays for the convolution and softmax layers, and the parameters of the local response normalization layer (size, power and scale). The number of training epochs was kept fixed at 250.\nWe computed 10 repetitions of a standard Bayesian optimization and our proposed ensemble optimization with ensemble size m = 7, both with a budget of B = 100 hyperparameter tuples to evaluate. Figure 7 shows the performance of ensembles generated from both pools of classifiers with a post-hoc ensemble generation. In order to limit overfitting, the first three models of each ensemble were selected directly based on accuracy, as suggested in (Caruana et al., 2004). In both cases, the ensemble size benefits the generalization accuracy, although the classifiers generated by the ensemble optimization procedure do perform slightly better. The difference in generalization error between BO-post and EO-post at the last iteration is found significant by a Wilcoxon test with a p-value of 0.005. Further work should investigate strategies to use the remaining validation data once the models are chosen, to further improve generalization accuracy.\n3Code available at https://code.google.com/ archive/p/cuda-convnet/ and network configuration file used is layers-18pct.cfg.\n0 10 20 30 40 50\nforward greedy iterations\n0.14\n0.16\n0.18\n0.20\n0.22\n0.24\ner ro\nr ra\nte\nBO-post-val BO-post-test EO-post-val EO-post-test\nFigure 7: Generalization errors of a post-hoc ensemble with classical Bayesian optimization (BO-post) and a posthoc ensemble generated from our ensemble optimization approach (EO-post) on the CIFAR-10 dataset with regards to the number of classifiers in the final ensemble. Results averaged over 10 repetitions."
    }, {
      "heading" : "5 CONCLUSION",
      "text" : "In this work, we presented a methodology to achieve Bayesian optimization of ensembles through hyperparameter tuning. We tackle the various challenges posed by ensemble optimization in this context, and the result is an optimization strategy that is able to exploit trained models and generate better ensembles of classifiers at the computational cost of a regular hyperparameter optimization.\nWe showcase the performance of our approach on three different problem suites, and in all cases show a significant difference in generalization accuracy between our approach and post-hoc ensembles built on top of a classical hyperparameter optimization, according to Wilcoxon signed-rank tests. This is a strong validation of our method, especially considering that it involves little extra computation."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This research benefitted from the computing resources provided by Calcul Québec, Compute Canada and Nvidia. We would also like to thank Annette Schwerdtfeger for proofreading this paper."
    } ],
    "references" : [ {
      "title" : "Lessons from the Netflix prize challenge",
      "author" : [ "Bell", "Robert M.", "Yehuda Koren" ],
      "venue" : "In: ACM SIGKDD Explorations Newsletter 9.2, pp. 75–79. Bergstra, James and Yoshua Bengio (2012). “Random Search for Hyper-Parameter Optimization”. In: Journal of",
      "citeRegEx" : "Bell et al\\.,? 2007",
      "shortCiteRegEx" : "Bell et al\\.",
      "year" : 2007
    }, {
      "title" : "Hyperparameter Optimization and Boosting for Classifying Facial Expressions: How good can a “ Null ",
      "author" : [ "Bergstra", "James", "David D. Cox" ],
      "venue" : "Machine Learning Research",
      "citeRegEx" : "Bergstra et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bergstra et al\\.",
      "year" : 2013
    }, {
      "title" : "Ensemble Selection from Libraries of Models",
      "author" : [ "Learning. Caruana", "Rich", "Alexandru Niculescu-Mizil", "Geoff Crew", "Alex Ksikes" ],
      "venue" : "In: Proceedings of the 21st International Conference on Machine Learning, p. 9.",
      "citeRegEx" : "Caruana et al\\.,? 2004",
      "shortCiteRegEx" : "Caruana et al\\.",
      "year" : 2004
    }, {
      "title" : "Statistical Comparisons of Classifiers over Multiple Data Sets",
      "author" : [ "Demšar", "Janez" ],
      "venue" : "In: The Journal of Machine Learning Research 7, pp. 1–30. Didaci, Luca, Giorgio Fumera, and Fabio Roli (2013). “Diversity in Classifier Ensembles : Fertile Concept or Dead",
      "citeRegEx" : "Demšar and Janez,? 2006",
      "shortCiteRegEx" : "Demšar and Janez",
      "year" : 2006
    }, {
      "title" : "Efficient and Robust Automated Machine Learning",
      "author" : [ "Feurer", "Matthias", "Aaron Klein", "Katharina Eggensperger", "Jost Tobias Springenberg", "Manuel Blum", "Frank Hutter" ],
      "venue" : "Multiple Classifier Systems,",
      "citeRegEx" : "Feurer et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Feurer et al\\.",
      "year" : 2015
    }, {
      "title" : "Initializing Bayesian Hyperparameter Optimization via Meta-Learning",
      "author" : [ "tems. Feurer", "Matthias", "Jost Tobias Springenberg", "Frank Hutter" ],
      "venue" : "Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence",
      "citeRegEx" : "Feurer et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Feurer et al\\.",
      "year" : 2015
    }, {
      "title" : "UCI Machine Learning Repository",
      "author" : [ "A. Frank", "A. Asuncion" ],
      "venue" : "URL: https://archive.ics.uci. edu/ml/datasets.html. Guyon, Isabelle, Kristin Bennett, Gavin Cawley, Hugo Jair Escalante, Sergio Escalera, Tin Kam Ho, Núria Macià,",
      "citeRegEx" : "Frank and Asuncion,? 2010",
      "shortCiteRegEx" : "Frank and Asuncion",
      "year" : 2010
    }, {
      "title" : "Design of the 2015 ChaLearn AutoML Challenge",
      "author" : [ "Bisakha Ray", "Mehreen Saeed", "Alexander Statnikov", "Evelyne Viegas" ],
      "venue" : "In: 2015 International Joint Conference on Neural Networks (IJCNN). ChaLearn. Hernández-Lobato, José Miguel, Matthew W. Hoffman,",
      "citeRegEx" : "Ray et al\\.,? 2015",
      "shortCiteRegEx" : "Ray et al\\.",
      "year" : 2015
    }, {
      "title" : "Predictive Entropy Search for Efficient Global Optimization of Black-box Functions",
      "author" : [ "Zoubin Ghahramani" ],
      "venue" : "Advances in Neural Information Processing",
      "citeRegEx" : "Ghahramani,? \\Q2014\\E",
      "shortCiteRegEx" : "Ghahramani",
      "year" : 2014
    }, {
      "title" : "Agnostic Bayesian Learning of Ensembles",
      "author" : [ "Lacoste", "Alexandre", "Hugo Larochelle", "Mario Marchand", "François Laviolette" ],
      "venue" : "In: Proceedings of the 31st International Conference on Machine Learning 32. Lacoste, Alexandre, Hugo Larochelle, Mario Marchand,",
      "citeRegEx" : "Lacoste et al\\.,? 2014a",
      "shortCiteRegEx" : "Lacoste et al\\.",
      "year" : 2014
    }, {
      "title" : "Boosting: Foundations and Algorithms",
      "author" : [ "Press. Schapire", "Robert E.", "Yoav Freund" ],
      "venue" : "MIT Press. Snoek, Jasper, Hugo Larochelle, and Ryan P. Adams (2012). “Practical Bayesian Optimization of Machine",
      "citeRegEx" : "Schapire et al\\.,? 2012",
      "shortCiteRegEx" : "Schapire et al\\.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "Feurer et al. (2015a) performed post-hoc ensemble generation by reusing the product of a completed hyperparameter optimization, winning phase 1 of the ChaLearn AutoML challenge (Guyon et al.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 4,
      "context" : "Feurer et al. (2015a) performed post-hoc ensemble generation by reusing the product of a completed hyperparameter optimization, winning phase 1 of the ChaLearn AutoML challenge (Guyon et al., 2015). Lastly, Snoek et al. (2015) also constructed post-hoc ensembles of neural networks for image captioning.",
      "startOffset" : 0,
      "endOffset" : 227
    }, {
      "referenceID" : 4,
      "context" : "In order to speed up convergence, Feurer et al. (2015b) have shown that hyperparameter optimization can be warm started with meta features about datasets.",
      "startOffset" : 34,
      "endOffset" : 56
    }, {
      "referenceID" : 9,
      "context" : "Lacoste et al. (2014b) proposed the Sequential Model-based Ensemble Optimization (SMBEO) method to optimize ensembles based on",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 2,
      "context" : "Forward greedy selection has been shown to perform well in the context of pruning a pool of classifiers (Caruana et al., 2004).",
      "startOffset" : 104,
      "endOffset" : 126
    }, {
      "referenceID" : 2,
      "context" : "In this case, the combination rule is majority voting, as it is less prone to overfitting (Caruana et al., 2004; Feurer et al., 2015a).",
      "startOffset" : 90,
      "endOffset" : 134
    }, {
      "referenceID" : 9,
      "context" : "Other possible combination rules include weighted voting, stacking (Kuncheva, 2004) and agnostic Bayesian combination (Lacoste et al., 2014a), to name only a few.",
      "startOffset" : 118,
      "endOffset" : 141
    }, {
      "referenceID" : 6,
      "context" : "The dataset used is the Pima Indian Diabetes dataset available from UCI (Frank and Asuncion, 2010), with separate training and validation splits.",
      "startOffset" : 72,
      "endOffset" : 98
    }, {
      "referenceID" : 2,
      "context" : "greedy selection – this form of warm starting is recommended in (Caruana et al., 2004) to reduce overfitting.",
      "startOffset" : 64,
      "endOffset" : 86
    } ],
    "year" : 2016,
    "abstractText" : "In this paper, we bridge the gap between hyperparameter optimization and ensemble learning by performing Bayesian optimization of an ensemble with regards to its hyperparameters. Our method consists in building a fixed-size ensemble, optimizing the configuration of one classifier of the ensemble at each iteration of the hyperparameter optimization algorithm, taking into consideration the interaction with the other models when evaluating potential performances. We also consider the case where the ensemble is to be reconstructed at the end of the hyperparameter optimization phase, through a greedy selection over the pool of models generated during the optimization. We study the performance of our proposed method on three different hyperparameter spaces, showing that our approach is better than both the best single model and a greedy ensemble construction over the models produced by a standard Bayesian optimization.",
    "creator" : "LaTeX with hyperref package"
  }
}