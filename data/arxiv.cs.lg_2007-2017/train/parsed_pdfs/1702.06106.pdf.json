{
  "name" : "1702.06106.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "An Attention-Based Deep Net for Learning to Rank",
    "authors" : [ "Baiyang Wang", "Diego Klabjan" ],
    "emails" : [ "BAIYANG@U.NORTHWESTERN.EDU", "D-KLABJAN@NORTHWESTERN.EDU" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Learning to rank applies supervised or semi-supervised machine learning to construct ranking models for information retrieval problems. In learning to rank, a query is given and a number of search results are to be ranked by their relevant importance given the query. Many problems in information retrieval can be formulated or partially solved by learning to rank. In learning to rank, there are typically three approaches: the pointwise, pairwise, and listwise approaches (Liu, 2011). The pointwise approach assigns an importance score to each pair of query and search result. The pairwise approach discerns which search result is more relevant for a certain query and a pair of search results. The listwise approach outputs the ranks for all search results given a specific query, therefore being the most general.\nFor learning to rank, neural networks are known to enjoy a success. Generally in such models, neural networks are\nIn submission.\napplied to model the ranking probabilities with the features of queries and search results as the input. For instance, the RankNet (Burges et al., 2005) applies a neural network to calculate a probability for any search result being more relevant compared to another. Each pair of query and search result is combined into a feature vector, which is the input of the neural network, and a ranking priority score is the output. Another approach learns the matching mechanism between the query and the search result, which is particularly suitable for image retrieval. Usually the mechanism is represented by a similarity matrix which outputs a bilinear form as the ranking priority score; for instance, such a structure is applied in (Severyn & Moschitti, 2015).\nWe postulate that it could be beneficial to apply multiple embeddings of the queries and search results to a learning to rank model. It has already been observed that for training images, applying a committee of convolutional neural nets improves digit and character recognition (Ciresan et al., 2011; Meier et al., 2011). From such an approach, the randomness of the architecture of a single neural network can be effectively reduced. For training text data, combining different techniques such as tf-idf, latent Dirichlet allocation (LDA) (Blei et al., 2003), or word2vec (Mikolov et al., 2013), has also been explored by Das et al. (2015). This is due to the fact that it is relatively hard to judge different models a priori. However, we have seen no literature on designing a mechanism to incorporate different embeddings for ranking. We hypothesize that applying multiple embeddings to a ranking neural network can improve the accuracy not only in terms of “averaging out” the error, but it can also provide a more robust solution compared to applying a single embedding.\nFor learning to rank, we propose the application of the attention mechanism (Bahdanau et al., 2015; Cho et al., 2015), which is demonstrated to be successful in focusing on different aspects of the input so that it can incorporate distinct features. It incorporates different embeddings with weights changing over time, derived from a recurrent neural network (RNN) structure. Thus, it can help us better summarize information from the query and search re-\nar X\niv :1\n70 2.\n06 10\n6v 2\n[ cs\n.L G\n] 1\n5 M\nay 2\n01 7\nsults. We also apply a decoder mechanism to rank all the search results, which provides a flexible list-wise ranking approach that can be applied to both image retrieval and text querying. Our model has the following contributions: (1) it applies the attention mechanism to listwise learning to rank problems, which we think is novel in the learning to rank literature; (2) it takes different embeddings of queries and search results into account, incorporating them with the attention mechanism; (3) double attention mechanisms are applied to both queries and search results.\nSection 2 reviews the RankNet, similarity matching, and the attention mechanism in details. Section 3 constructs the attention-based deep net for ranking, and discusses how to calibrate the model. Section 4 demonstrates the performance of our model on image retrieval and text querying data sets. Section 5 discusses about potential future research and concludes the paper."
    }, {
      "heading" : "2. Literature Review",
      "text" : "To begin with, for the RankNet, each pair of query and search result is turned into a feature vector. For two feature vectors x0 ∈ Rd0 and x′0 ∈ Rd0 sharing the same query, we define x0 ≺ x′0 if the search result associated with x0 is ranked before that with x′0, and vice versa. For x0,{\nx1 = f(W0x0 + b0) ∈ Rd1 , x2 = f(W1x1 + b1) ∈ Rd2 = R,\n(1)\nand similarly for x′0. Here Wl is a dl+1× dl weight matrix, and bl ∈ Rdl+1 is a bias for l = 0, 1. Function f is an element-wise nonlinear activation function; for instance, it can be the sigmoid function σ(u) = eu/(1 + eu). Then for the RankNet, the ranking probability is defined as follows,\nP (x0 ≺ x′0) = ex2−x ′ 2/(1 + ex2−x ′ 2). (2)\nTherefore the ranking priority of two search results can be determined with a two-layer neural network structure, offering a pairwise approach. A deeper application of the RankNet can be found in (Song et al., 2014), where a five-layer RankNet is proposed, and each data example is weighed differently for each user in order to adapt to personalized search. A global model is first trained with the training data, and then a different regularized model is adapted for each user with a validation data set.\nA number of models similar to the RankNet has been proposed. For instance, the LambdaRank (Burges et al., 2006) speeds up the RankNet by altering the cost function according to the change in NDCG caused by swapping search results. the FRank (Tsai et al., 2007) applies the fidelity loss on the ranking probabilities from the RankNet. The ListNet (Cao et al., 2007) modifies the RankNet with permutation\nprobabilities, providing a listwise approach. More models can be found in the summary of (Liu, 2011).\nHowever, we are different from the above models not only because we integrate different embeddings with the attention mechanism, but also because we learn the matching mechanism between a query and search results with a similarity matrix. There are a number of papers applying this structure. For instance, Severyn & Moschitti (2015) applied a text convolutional neural net together with such a structure for text querying. For image querying, Wan et al. (2014) applied deep convolutional neural nets together with the OASIS algorithm (Chechik et al., 2009) for similarity learning. Still, our approach is different from them in that we apply the attention mechanism, and develop an approach allowing both image and text queries.\nWe explain the idea of similarity matching as follows. We take a triplet (q, r, r′) into account, where q denotes an embedding, i.e. vectorized feature representation of a query, and (r, r′) denotes the embeddings of two search results. A similarity function is defined as follows,\nSW (q, r) = q TWr, (3)\nand apparently r ≺ r′ if and only if SW (q, r) > SW (q, r′).\nNote that we may create multiple deep convolutional nets so that we obtain multiple embeddings for the queries and search results. Therefore, it is a question how to incorporate them together. The attention mechanism weighs the embeddings with different sets of weights for each state t, which are derived with a recurrent neural network (RNN) from t = 1 to t = T . Therefore, for each state t, the different embeddings can be “attended” differently by the attention mechanism, thus making the model more flexible. This model has been successfully applied to various problems. For instance, Bahdanau et al. (2015) applied it to neural machine translation with a bidirectional recurrent neural network. Cho et al. (2015) further applied it to image caption and video description generation with convolutional neural nets. Vinyals et al. (2015) applied it for solving combinatorial problems with the sequence-to-sequence paradigm.\nNote that in our scenario, the ranking process, i.e. sorting the search results from the most related one to the least related one for a query, can be modeled by different “states.” Thus, the attention mechanism helps incorporating different embeddings along with the ranking process, therefore providing a listwise approach. Below we explain our model in more details."
    }, {
      "heading" : "3. Model and Algorithm",
      "text" : ""
    }, {
      "heading" : "3.1. Introduction to the Model",
      "text" : "Both queries and search results can be embedded with neural networks. Given an input vector x0 representing a query or a search result, we denote the l-th layer in a neural net as xl ∈ Rdl , l = 0, 1, . . . , L. We have the following relation\nxl+1 = f(Wlxl + bl), l = 0, 1, . . . , L− 1. (4)\nwhere Wl is a dl+1 × dl weight matrix, bl ∈ Rdl+1 is the bias, and f is a nonlinear activation function. If the goal is classification with C categories, then\n(P (y = 1), . . . , P (y = C)) = softmax(WLxL + bL), (5)\nwhere y is a class indicator, and softmax(u) = (eu1/∑d i=1 e ui , . . . , eud/ ∑d i=1 e ui) for u ∈ Rd.\nFrom training this model, we may take the softmax probabilities as the embedding, and create different embeddings with different neural network structures. For images, convolutional neural nets (CNNs) (LeCun et al., 1998) are more suitable, in which each node only takes information from neighborhoods of the previous layer. Pooling over each neighborhood is also performed for each layer of a convolutional neural net.\nWith different networks, we can obtain different embeddings c1, . . . , cM . In the attention mechanism below, we generate the weights αt with an RNN structure, and summarize ct in a decoder series zt, etm = fATT (zt−1, c m, αt−1), m = 1, . . . ,M, αt = softmax(et), ct = ∑M m=1 αtmc m,\nzt = φθ(zt−1, ct).\n(6)\nHere fATT and φθ are chosen as tanh layers in our experiments. Note that the attention weight αt at state t depends on the previous attention weight αt−1, the embeddings, and the previous decoder state zt−1, and the decoder series zt sums up information of ct up to state t.\nAs aforementioned, given multiple embeddings, the ranking process can be viewed as applying different attention weights to the embeddings and generating the decoder series zt, offering a listwise approach. However, since there are features for both queries and search results, we consider them as separately, and apply double attention mechanisms to each of them. Our full model is described below."
    }, {
      "heading" : "3.2. Model Construction",
      "text" : "For each query Q, we represent M different embeddings of the query as q = (q1, . . . , qM ), where each qm is multidimensional. For the search results R1, . . . , RT associated\nwithQ, which are to be ranked, we can map each result into N different embeddings with a same structure, and obtain rt = (rt1, . . . , rtN ) as the embeddings ofRt, t = 1, . . . , T . Each rtn is also multi-dimensional. All these embeddings can either correspond to raw data or they can be the output of a parametric model, i.e. the output of a CNN in the case of images. In the latter case, they are trained jointly with the rest of the model.\nBelow we represent queries or search results in their embeddings. We assume that given the query q, the results r1, . . . rT are retrieved in the order r̃1, . . . , r̃T in equation (12) below. Note that {r1, . . . , rT } = {r̃1, . . . , r̃T }.\nWe observe that both the query q and the results r1, . . . , rT can be “attended” with different weights. To implement the attention-based mechanism, we assign different weights for different components of for each different t = 1, . . . , T . Specifically, we assign the attention weights α1, . . . , αT for the query, and β1, . . . , βT for the search results. To achieve this, we need to assign the pre-softmax weights et and ft, t = 1, . . . , T . Therefore we first let etm = eATT (zt−1, qm, g(r1, . . . , rT ), αt−1, βt−1), ftn = fATT (zt−1, q, hn(r1, . . . , rT ), αt−1, βt−1),\nm = 1, . . . ,M, n = 1, . . . , N, t = 1, . . . , T. (7) Note that there are two different attention functions. While they both consider the previous weights αt−1 and βt−1, the pre-softmax weight etm considers the m-th component of the query qm and all information from the search results, and ftn considers the opposite. In our experiments, g takes an average while h averages over the n-th embedding of each result. Here zt−1 is the value of the decoder at state t − 1. Letting et = (et1, . . . , etM ), ft = (ft1, . . . , ftN ), we impose the softmax functions for t = 1, . . . , T ,\nαt = softmax(et), βt = softmax(ft). (8)\nThe attention weights αt and βt can then be assigned to create the context vectors ct for the query and d̄t for the search results, which are defined as follows,\nct = M∑ m=1 αtmqm, d̄t = N∑ n=1 βtnhn(r1, . . . , rT ). (9)\nWith the context vectors ct and d̄t depending on the state t, we can finally output the decoder zt,\nzt = φθ(zt−1, ct, d̄t), t = 1, . . . , T. (10)\nEquations (8-11) are carried out iteratively from t = 1 to t = T . The hidden states z1, . . . , zT , together with the context vectors c1, . . . , cT , are used to rank results r1, . . . , rT . For each rt, we define a context vector dt,t′ which can be\ndirectly compared against the context vector ct for queries,\ndt,t′ = N∑ n=1 βtnrt′n, t = 1, . . . , T. (11)\nThis context vector dt,t′ , unlike d̄t, is not only specific for each state t, but also specific for each result rt′ , t′ = 1, . . . , T . Now suppose in terms of ranking that r̃1, . . . , r̃t−1 have already been selected. For choosing r̃t, we apply the softmax function for the similarity scores st,t′ between the query q and each result rt′ .\nThus we have P (r̃t = rt′ |r̃1, . . . , r̃t−1) ∝ softmax(st,rt′ ), st,rt′ = d T t,t′Wct + d T t,t′V zt,\nrt′ ∈ {r1, . . . , rT }\\{r̃1, . . . , r̃t−1}. (12)\nTherefore r̃1, . . . , r̃T can be retrieved in a listwise fashion. Equations (7-12) complete our model, which is shown in Figure 1 in the case of image retrieval."
    }, {
      "heading" : "3.3. Model Calibration",
      "text" : "To calibrate the parameters of this model, we apply the stochastic gradient descent algorithm which trains a minibatch, a subsample of the training data at each iteration. To rank search results for the testing data, we apply the beam search algorithm (Reddy, 1977), which keeps a number of paths, i.e. sequences of r̃1, . . . , r̃t at state t, of the highest log-likelihood. Other paths are trimmed at each state t, and finally the path with the highest log-likelihood is chosen.\nAlternatively, we may consider the hinge loss function to replace the softmax function. Comparing a potentially chosen result r̃t against any other unranked result rt′ , with rt′ ∈ {r1, . . . , rT }\\{r̃1, . . . , r̃t−1}, we apply the following hinge loss function, similar to Chechik et al. (2009)\nL(r̃t, rt′) = max{0, 1− st,r̃t + st,rt′}. (13)\nWhen training the model, for a single query q and all related search results, we apply the stochastic gradient descent algorithm to minimize the following\nT∑ t=1 ∑ rt′ L(r̃t, rt′). (14)"
    }, {
      "heading" : "4. Data Studies",
      "text" : ""
    }, {
      "heading" : "4.1. The MNIST Data Set",
      "text" : "The MNIST data set contains handwritings of 0-9 digits. There are 50,000 training, 10,000 validation, and 10,000 testing images. We take each image among them as a query. For each query image, there are 30 randomly selected search results with 5 images of the same digit and 25 images of different digits. The order is imposed so that images of the same digit are considered as related and images of different digits not related.\nFor this data set, we pretrain 5 convolutional neural networks based on different L2 regularization rates for all layers and Dropout (Srivastava et al., 2014) regularization for fully connected (FC) layers with respect to the values shown in Table 1. We vary the regularization rate because it can be expensive to determine a most suitable regularization rate in practice, and therefore applying the attention mechanism to different regularization rates can reduce the impact of improper regularization rates, automatically finding a better model.\nThe pretrained models are standard digit classification. The softmax layers of the five models as embeddings are plugged into our full model (where the CNNs are trained together with the rest). We initialize the similarity matrix W with the identity matrix and the remaining parameters with zeros. Our model is trained given such embeddings with a batch size of 100, a learning rate of 0.001, and 20 epochs. We choose the best epoch according to the validation data set.\nWe compare our algorithm against OASIS (Chechik et al., 2009), in the setting of image retrieval problems (Wan et al., 2014; Wang et al., 2014; Wu et al., 2013). We set the batch size and maximum learning rate for OASIS to be the same. To compare the results, we apply MAP and NDCGp (Liu, 2011) as the criteria, and calculate the standard deviations averaged over five runs for each model. The error rates, which are one minus MAPs and NDCGs, are shown in Table 2. The lowest error rates are in bold. The top row\nshows the average value while the bottom one (sd.) exhibits the standard deviation for 5 randomized runs.\nIn Table 2, “OASIS-1” or “OASIS-5” denote, respectively, OASIS trained with the first set of embeddings, or the average of all five sets of embeddings. We name our model as an “Attention-based Ranking Network” (AttRN). “AttRNSM” or “AttRN-HL” denote our model with softmax or hinge loss ranking functions.\nFrom Table 2, we observe that AttRN-HL outperforms other methods, while OASIS-5 and AttRN-SM achieve similar performance. Moreover, OASIS-5 clearly outperforms OASIS-1 demonstrating the benefit of applying multiple embeddings.\nIt is also of interest how sensitive AttRN-HL is against the number of embeddings. In Figure 2, we show the error rates of AttRN-HL with different numbers of embeddings. Here we apply regularization parameters (1, 0.8, 1e-4) and (1, 0.8, 2e-4) to the new embeddings. From Figure 2, the error rates steadily decrease as the number of embeddings increases, which is consistent with the intuition that adding more information to the model should yield better results. We also observe that the error rates tend to stabilize as the number of embeddings increases.\nHowever, while applying 7 embeddings slightly lowers the error rates as shown in Figure 2, the training time is roughly 4 times longer than for 5 embeddings. Therefore we consider applying 7 embeddings not to be beneficial, and consider the current practice to be representative of AttRN.\nTable 3 shows the differences in error rates by changing the pooling functions g and hn in (7) from “mean” to ”max.” We observe that the changes are very small, which means that our model is quite robust to such changes.\nTable 4 shows the changes in weights of the convolutional neural networks. The L2 norms sum over the weights of each layer and are averaged over 5 randomized runs. They increase after training our model, possibly because it requires the weights to be of higher magnitude to adjust for the ranking mechanism.\nNext we display three good cases of retrieval together with three poorer ones from AttRN-HL in terms of the actual images, which are shown in Figure 3. We observe that the poorer cases of retrieval are either the result of blurring query images or blurring search results."
    }, {
      "heading" : "4.2. The CIFAR-10 Data Set",
      "text" : "The CIFAR-10 data set contains images of 10 different classes. There are 50,000 training and 10,000 testing images. Again we took each image among them as a query, and randomly created 30 search results with 5 images of\nthe same class and 25 images of different classes. The order is imposed in the same way as MNIST and based on different classes. We use 5 convolutional neural networks in the same way as MNIST based on regularization rates shown in Table 5.\nWe apply a batch size of 50, a learning rate of 0.0005 and a fixed number of 20 epochs to train AttRN. The error rates and the standard deviations are shown in Table 6. AttRNHL again achieves the best performance while AttRN-SM proves unsuitable for this data set.\nTable 7 also shows that replacing the pooling functions from “mean” to ”max” again yields very small changes in error rates, which means that our model is quite robust.\nAgain we display three good cases of retrieval and three poorer ones from AttRN-HL in terms of the actual images, shown in Figure 4. We observe that the images considered as related tend to be in the same superclass as the query, but may be in a different class due to the variation within each class of images."
    }, {
      "heading" : "4.3. The 20 Newsgroups Data Set",
      "text" : "The 20 Newsgroups data set contains online news documents with 20 different topics. There are 11,293 training and 7,528 testing documents. The topics can be categorized into 7 superclasses: religion, computer, “for sale,” cars, sports, science, and politics. We consider each document as a query, and randomly choose 5 documents of the\nsame topic, 5 documents of different topics but the same superclass, and 20 documents of different superclasses as the search results for each query.\nWe impose the order on different documents so that: (1) documents with the same topic are considered to be related, and otherwise not; (2) documents with the same superclass are considered to be related, and otherwise not. We train our model with the first type of order, and calculate error rates for both orders.\nWe pretrain the documents with three different word2vectype (Mikolov et al., 2013) models: CBOW, skip-gram, and GLOVE (Pennington et al., 2014). We note that all these techniques apply neighboring words to predict word occurrences with a neural network based on different assumptions. Therefore, applying the attention mechanism to incorporate the three models can weaken the underlying distributional assumptions and result in a more flexible word embedding structure.\nWe apply text8 as the training corpus for CBOW and skipgram, and download the pretrained weights for GLOVE from Wikipedia 2014 and Gigaword 5. Each vector representing a word of dimension 300. For each document, we represent it as a tf-idf vector (Aggarwal & Zhai, 2012) and apply the word2vec weights to transform it into a 300- dimensional vector. For this instance, the embeddings are fixed and not modified during the training process of our model. We also impose a softmax layer corresponding to the 20 classes on top of word2vec and use the resulting final weights as pretrained weights to AttRN. We compare our model against RankNet, which is commonly applied to information retrieval problems (Burges et al., 2005; Song et al., 2014; Wang et al., 2013). For RankNet, the feature vector for each query-document pair is obtained by con-\ncatenating the embeddings of both the query and the result documents.\nFor AttRN, we apply a batch size of 100 and a learning rate of 0.0001. A fixed number of 20 epochs are applied to AttRN-HL, while 50 for AttRN-SM, since it takes longer for the error rates of the later to stabilize. For RankNet (RN), we choose an intermediate layer of 100 nodes, a learning rate of 0.1, and a fixed number of 200 epochs, which are the best combination we can obtain after tuning. Table 8 considers any two documents of the same topic to be related, while Table 9 considers any two documents of the same superclass to be related.\nTables 8 and 9 show that AttRN-SM is the most suitable for 20 Newsgroups while AttRN-HL achieves the second best, and is relatively close. We also observe that MAP for superclasses is too strict for this data set, because all documents of the same superclass are taken into account. NDCG scores are more realistic because a threshold is given and less related search results are down-weighted.\nTo conclude, AttRN-HL has the most robust performance across all three data sets, and is thus the recommended choice."
    }, {
      "heading" : "5. Conclusion",
      "text" : "In this paper, we proposed a new neural network for learning-to-rank problems which applies the attention mechanism to incorporate different embeddings of queries and search results, and ranks the search results with a listwise approach. Data experiments show that our model yields improvements over state-of-the-art techniques. For\nthe future, it would be of interest to consider improving the RNN structure in the attention mechanism, and tailoring the embedding part of the neural network to this problem."
    } ],
    "references" : [ {
      "title" : "Mining text data",
      "author" : [ "C. Aggarwal", "Zhai", "C. (eds" ],
      "venue" : null,
      "citeRegEx" : "Aggarwal et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Aggarwal et al\\.",
      "year" : 2012
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "D. Bahdanau", "K. Cho", "Y. Bengio" ],
      "venue" : "In International Conference on Learning Representations,",
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Latent dirichlet allocation",
      "author" : [ "D. Blei", "A. Ng", "M. Jordan" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Blei et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Blei et al\\.",
      "year" : 2003
    }, {
      "title" : "Learning to rank using gradient descent",
      "author" : [ "C. Burges", "T. Shaked", "E. Renshaw", "A. Lazier", "M. Deeds", "N. Hamilton", "G. Hullender" ],
      "venue" : "In Proceedings of the 22th International Conference on Machine Learning (ICML",
      "citeRegEx" : "Burges et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Burges et al\\.",
      "year" : 2005
    }, {
      "title" : "Learning to rank with nonsmooth cost functions",
      "author" : [ "C. Burges", "R. Regno", "Q. Le" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Burges et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Burges et al\\.",
      "year" : 2006
    }, {
      "title" : "Learning to rank: from pairwise approach to listwise approach",
      "author" : [ "Z. Cao", "T. Qin", "T. Liu", "M. Tsai", "H. Li" ],
      "venue" : "In Proceedings of the 24th International Conference on Machine Learning (ICML",
      "citeRegEx" : "Cao et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2007
    }, {
      "title" : "An online algorithm for large scale image similarity learning",
      "author" : [ "G. Chechik", "V. Sharma", "U. Shalit", "S. Bengio" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Chechik et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Chechik et al\\.",
      "year" : 2009
    }, {
      "title" : "Describing multimedia content using attention-based encoder-decoder networks",
      "author" : [ "K. Cho", "A. Courville", "Y. Bengio" ],
      "venue" : "IEEE Transactions on Multimedia,",
      "citeRegEx" : "Cho et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2015
    }, {
      "title" : "Convolutional neural network committees for handwritten character classification",
      "author" : [ "Ciresan", "Dan Claudiu", "Meier", "Ueli", "Gambardella", "Luca Maria", "Schmidhuber", "Jürgen" ],
      "venue" : "In ICDAR,",
      "citeRegEx" : "Ciresan et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Ciresan et al\\.",
      "year" : 2011
    }, {
      "title" : "Gaussian lda for topic models with word embeddings",
      "author" : [ "Das", "Rajarshi", "Zaheer", "Manzil", "Dyer", "Chris" ],
      "venue" : "In The 2015 Conference of the Association for Computational Linguistics",
      "citeRegEx" : "Das et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Das et al\\.",
      "year" : 2015
    }, {
      "title" : "Gradientbased learning applied to document recognition",
      "author" : [ "Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "LeCun et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1998
    }, {
      "title" : "Learning to rank for information retrieval",
      "author" : [ "T. Liu" ],
      "venue" : null,
      "citeRegEx" : "Liu,? \\Q2011\\E",
      "shortCiteRegEx" : "Liu",
      "year" : 2011
    }, {
      "title" : "Better digit recognition with a committee of simple neural nets",
      "author" : [ "Meier", "Ueli", "Ciresan", "Dan Claudiu", "Gambardella", "Luca Maria", "Schmidhuber", "Jürgen" ],
      "venue" : "In ICDAR,",
      "citeRegEx" : "Meier et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Meier et al\\.",
      "year" : 2011
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "J. Pennington", "R. Socher", "C. Manning" ],
      "venue" : "In Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Pennington et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Speech understanding systems: summary of results of the five-year research effort at carnegie-mellon university",
      "author" : [ "R. Reddy" ],
      "venue" : "Technical Report,",
      "citeRegEx" : "Reddy,? \\Q1977\\E",
      "shortCiteRegEx" : "Reddy",
      "year" : 1977
    }, {
      "title" : "Learning to rank short text pairs with convolutional deep neural networks",
      "author" : [ "A. Severyn", "A. Moschitti" ],
      "venue" : "In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information",
      "citeRegEx" : "Severyn and Moschitti,? \\Q2015\\E",
      "shortCiteRegEx" : "Severyn and Moschitti",
      "year" : 2015
    }, {
      "title" : "Adapting deep ranknet for personalized search",
      "author" : [ "Y. Song", "H. Wang", "X. He" ],
      "venue" : "In The 7th ACM WSDM Conference,",
      "citeRegEx" : "Song et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2014
    }, {
      "title" : "Dropout: A simple way to prevent neural networks from overfitting",
      "author" : [ "N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Srivastava et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2014
    }, {
      "title" : "Frank: a ranking method with fidelity loss",
      "author" : [ "M. Tsai", "T. Liu", "T. Qin", "H. Chen", "W. Ma" ],
      "venue" : "In The 30th Annual International ACM SIGIR Conference,",
      "citeRegEx" : "Tsai et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Tsai et al\\.",
      "year" : 2007
    }, {
      "title" : "Deep learning for content-based image retrieval: a comprehensive study",
      "author" : [ "J. Wan", "D. Wang", "S. Hoi", "P. Wu", "J. Zhu", "Y. Zhang", "J. Li" ],
      "venue" : "In Proceedings of the 22nd ACM international conference on Multimedia,",
      "citeRegEx" : "Wan et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Wan et al\\.",
      "year" : 2014
    }, {
      "title" : "Personalized ranking model adaptation for web search",
      "author" : [ "H. Wang", "X. He", "Chang", "M.-W", "Y. Song", "R. White", "W. Chu" ],
      "venue" : "In The 36th Annual ACM SIGIR Conference (SIGIR’2013),",
      "citeRegEx" : "Wang et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2013
    }, {
      "title" : "Learning fine-grained image similarity with deep ranking",
      "author" : [ "J. Wang", "Y. Song", "T. Leung", "C. Rosenberg", "J. Philbin", "B. Chen", "Y. Wu" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Wang et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2014
    }, {
      "title" : "Online multimodal deep similarity learning with application to image retrieval",
      "author" : [ "P. Wu", "S. Hoi", "H. Xia", "P. Zhao", "D. Wang", "C. Miao" ],
      "venue" : "In Proceedings of the 21st ACM international conference on Multimedia, Barcelona,",
      "citeRegEx" : "Wu et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "In learning to rank, there are typically three approaches: the pointwise, pairwise, and listwise approaches (Liu, 2011).",
      "startOffset" : 108,
      "endOffset" : 119
    }, {
      "referenceID" : 3,
      "context" : "For instance, the RankNet (Burges et al., 2005) applies a neural network to calculate a probability for any search result being more relevant compared to another.",
      "startOffset" : 26,
      "endOffset" : 47
    }, {
      "referenceID" : 8,
      "context" : "It has already been observed that for training images, applying a committee of convolutional neural nets improves digit and character recognition (Ciresan et al., 2011; Meier et al., 2011).",
      "startOffset" : 146,
      "endOffset" : 188
    }, {
      "referenceID" : 12,
      "context" : "It has already been observed that for training images, applying a committee of convolutional neural nets improves digit and character recognition (Ciresan et al., 2011; Meier et al., 2011).",
      "startOffset" : 146,
      "endOffset" : 188
    }, {
      "referenceID" : 2,
      "context" : "For training text data, combining different techniques such as tf-idf, latent Dirichlet allocation (LDA) (Blei et al., 2003), or word2vec (Mikolov et al.",
      "startOffset" : 105,
      "endOffset" : 124
    }, {
      "referenceID" : 13,
      "context" : ", 2003), or word2vec (Mikolov et al., 2013), has also been explored by Das et al.",
      "startOffset" : 21,
      "endOffset" : 43
    }, {
      "referenceID" : 2,
      "context" : "For training text data, combining different techniques such as tf-idf, latent Dirichlet allocation (LDA) (Blei et al., 2003), or word2vec (Mikolov et al., 2013), has also been explored by Das et al. (2015). This is due to the fact that it is relatively hard to judge different models a priori.",
      "startOffset" : 106,
      "endOffset" : 206
    }, {
      "referenceID" : 1,
      "context" : "For learning to rank, we propose the application of the attention mechanism (Bahdanau et al., 2015; Cho et al., 2015), which is demonstrated to be successful in focusing on different aspects of the input so that it can incorporate distinct features.",
      "startOffset" : 76,
      "endOffset" : 117
    }, {
      "referenceID" : 7,
      "context" : "For learning to rank, we propose the application of the attention mechanism (Bahdanau et al., 2015; Cho et al., 2015), which is demonstrated to be successful in focusing on different aspects of the input so that it can incorporate distinct features.",
      "startOffset" : 76,
      "endOffset" : 117
    }, {
      "referenceID" : 17,
      "context" : "A deeper application of the RankNet can be found in (Song et al., 2014), where a five-layer RankNet is proposed, and each data example is weighed differently for each user in order to adapt to personalized search.",
      "startOffset" : 52,
      "endOffset" : 71
    }, {
      "referenceID" : 4,
      "context" : "For instance, the LambdaRank (Burges et al., 2006) speeds up the RankNet by altering the cost function according to the change in NDCG caused by swapping search results.",
      "startOffset" : 29,
      "endOffset" : 50
    }, {
      "referenceID" : 19,
      "context" : "the FRank (Tsai et al., 2007) applies the fidelity loss on the ranking probabilities from the RankNet.",
      "startOffset" : 10,
      "endOffset" : 29
    }, {
      "referenceID" : 5,
      "context" : "The ListNet (Cao et al., 2007) modifies the RankNet with permutation probabilities, providing a listwise approach.",
      "startOffset" : 12,
      "endOffset" : 30
    }, {
      "referenceID" : 11,
      "context" : "More models can be found in the summary of (Liu, 2011).",
      "startOffset" : 43,
      "endOffset" : 54
    }, {
      "referenceID" : 6,
      "context" : "(2014) applied deep convolutional neural nets together with the OASIS algorithm (Chechik et al., 2009) for similarity learning.",
      "startOffset" : 80,
      "endOffset" : 102
    }, {
      "referenceID" : 19,
      "context" : "For image querying, Wan et al. (2014) applied deep convolutional neural nets together with the OASIS algorithm (Chechik et al.",
      "startOffset" : 20,
      "endOffset" : 38
    }, {
      "referenceID" : 1,
      "context" : "For instance, Bahdanau et al. (2015) applied it to neural machine translation with a bidirectional recurrent neural network.",
      "startOffset" : 14,
      "endOffset" : 37
    }, {
      "referenceID" : 1,
      "context" : "For instance, Bahdanau et al. (2015) applied it to neural machine translation with a bidirectional recurrent neural network. Cho et al. (2015) further applied it to image caption and video description generation with convolutional neural nets.",
      "startOffset" : 14,
      "endOffset" : 143
    }, {
      "referenceID" : 1,
      "context" : "For instance, Bahdanau et al. (2015) applied it to neural machine translation with a bidirectional recurrent neural network. Cho et al. (2015) further applied it to image caption and video description generation with convolutional neural nets. Vinyals et al. (2015) applied it for solving combinatorial problems with the sequence-to-sequence paradigm.",
      "startOffset" : 14,
      "endOffset" : 266
    }, {
      "referenceID" : 10,
      "context" : "For images, convolutional neural nets (CNNs) (LeCun et al., 1998) are more suitable, in which each node only takes information from neighborhoods of the previous layer.",
      "startOffset" : 45,
      "endOffset" : 65
    }, {
      "referenceID" : 15,
      "context" : "To rank search results for the testing data, we apply the beam search algorithm (Reddy, 1977), which keeps a number of paths, i.",
      "startOffset" : 80,
      "endOffset" : 93
    }, {
      "referenceID" : 6,
      "context" : ", r̃t−1}, we apply the following hinge loss function, similar to Chechik et al. (2009)",
      "startOffset" : 65,
      "endOffset" : 87
    }, {
      "referenceID" : 18,
      "context" : "For this data set, we pretrain 5 convolutional neural networks based on different L regularization rates for all layers and Dropout (Srivastava et al., 2014) regularization for fully connected (FC) layers with respect to the values shown in Table 1.",
      "startOffset" : 132,
      "endOffset" : 157
    }, {
      "referenceID" : 6,
      "context" : "We compare our algorithm against OASIS (Chechik et al., 2009), in the setting of image retrieval problems (Wan et al.",
      "startOffset" : 39,
      "endOffset" : 61
    }, {
      "referenceID" : 20,
      "context" : ", 2009), in the setting of image retrieval problems (Wan et al., 2014; Wang et al., 2014; Wu et al., 2013).",
      "startOffset" : 52,
      "endOffset" : 106
    }, {
      "referenceID" : 22,
      "context" : ", 2009), in the setting of image retrieval problems (Wan et al., 2014; Wang et al., 2014; Wu et al., 2013).",
      "startOffset" : 52,
      "endOffset" : 106
    }, {
      "referenceID" : 23,
      "context" : ", 2009), in the setting of image retrieval problems (Wan et al., 2014; Wang et al., 2014; Wu et al., 2013).",
      "startOffset" : 52,
      "endOffset" : 106
    }, {
      "referenceID" : 11,
      "context" : "To compare the results, we apply MAP and NDCGp (Liu, 2011) as the criteria, and calculate the standard deviations averaged over five runs for each model.",
      "startOffset" : 47,
      "endOffset" : 58
    }, {
      "referenceID" : 13,
      "context" : "We pretrain the documents with three different word2vectype (Mikolov et al., 2013) models: CBOW, skip-gram, and GLOVE (Pennington et al.",
      "startOffset" : 60,
      "endOffset" : 82
    }, {
      "referenceID" : 14,
      "context" : ", 2013) models: CBOW, skip-gram, and GLOVE (Pennington et al., 2014).",
      "startOffset" : 43,
      "endOffset" : 68
    }, {
      "referenceID" : 3,
      "context" : "We compare our model against RankNet, which is commonly applied to information retrieval problems (Burges et al., 2005; Song et al., 2014; Wang et al., 2013).",
      "startOffset" : 98,
      "endOffset" : 157
    }, {
      "referenceID" : 17,
      "context" : "We compare our model against RankNet, which is commonly applied to information retrieval problems (Burges et al., 2005; Song et al., 2014; Wang et al., 2013).",
      "startOffset" : 98,
      "endOffset" : 157
    }, {
      "referenceID" : 21,
      "context" : "We compare our model against RankNet, which is commonly applied to information retrieval problems (Burges et al., 2005; Song et al., 2014; Wang et al., 2013).",
      "startOffset" : 98,
      "endOffset" : 157
    } ],
    "year" : 2017,
    "abstractText" : "In information retrieval, learning to rank constructs a machine-based ranking model which given a query, sorts the search results by their degree of relevance or importance to the query. Neural networks have been successfully applied to this problem, and in this paper, we propose an attention-based deep neural network which better incorporates different embeddings of the queries and search results with an attention-based mechanism. This model also applies a decoder mechanism to learn the ranks of the search results in a listwise fashion. The embeddings are trained with convolutional neural networks or the word2vec model. We demonstrate the performance of this model with image retrieval and text querying data sets.",
    "creator" : "LaTeX with hyperref package"
  }
}