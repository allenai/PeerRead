{
  "name" : "1303.5756.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "From Relational Databases to Belief Networks",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 INTRODUCTION\nIt turns out that Relational Database (RD) and Be lief Network (BN) are very closely related to each other in many aspects. Spiegelhalter [1986] investi gated some of these relationships. He also discussed the issue about \"using data to learn about quantita tive assessments\" so that the conditional probabilities can be revised by data obtained after a BN has been built. Lauritzen, Spiegelhalter [1988], Herskovitz and Copper [1990] used a method based on Maximum En tropy (ME) principle [Shore and Johnson, Jan 1980] to obtain a consistent distribution from empirical data. Spiegelhalter [1986] and Wen [1990b] discussed decom· position of the networks to reduce the computational amount required by probabilistic reasoning.\nIn this paper, the relationship between RD and BN is investigated and a method to construct BN from statistical RD [Wen, 1990a] is proposed based on the principles of Nearest Neighborhood (NN) [Duda and Hart, 1973] and Occam's Razor (OR) [Blumer et al., 1987]. Most of the contemporary databases are rela tional, this makes the research in construction of BN from RD interesting and important. A comparison be tween our method and others is also given.\n2 RELATIONAL DATABASES\nAccording to the relational database theory [Ullman, 1982], we have the following basic definitions in RD:\nA relation r is a subset of the Cartesian product of do mains D1, . .. , Dk. A domain Di is a set of values taken by an attribute Ai. The members of a relation are tu ples. The value of a tuple ton attribute A is written as t[A]. The set of attribute for a relation r is the relation scheme R. Let X C R and t E r. We write t[X] for the partial tuple oft restricted to X. A collection of relation schemes is a relational database scheme. The current values of the relations corresponding to the database scheme are the relational database.\nLet r and s are relations on relational schemes R and S, respectively, A E R, a E D, and X C R. We will discuss the following operators on relations.\n1. Select operator: \"Select from r with A equal to a\" yields a relation CTA=a( r) = { t E rlt[A] = a}. 2. Project operator: The projection of r onto X is a relation 1rx ( r) = {t[X]It E r} 3. Join operator: The join of r and s is a relation r M s = { ti t [R] E r /\\ t[S] E s} To avoid redundancy and potential inconsistency, RD are often organized in normal forms according to the dependencies existing among the attribute subsets.\nFunctional dependency (FD): Let X, Y C R. X functionally determines Y, written X ---> Y, if\nVr on R, t, s E r 1\\ t[X] = s[X] = t[Y] = s[Y].\nMultivalued dependency (MD): X multideter mines Y, written X ___.___. Y, if Vr on R, t, s E r 1\\ t[X] = s[X] implies that 3u, v E r such that ( 1) u[X] = v[X] = t[X] = s[X], (2) u[Y] = t[Y], and u[R - X - Y] = s[R- X- Y], and (3) v[Y] = s [Y] and v[R- X - Y] = t[R- X - Yj.\nJoin dependency (JD) over R1, ... ,Rn, written C><J (Rl> ... , Rn), is satisfied by a relation rover R1 U . .. URn, if and only if 7rR, ( r) C><J • • • C><J 7rRJ r) = r\nLet F be a set of FD's on R(AJ, ... ,An) and p+ the closure of F. K C R is a key of R if K---> A1, ... ,An E F+/\\ )3X C K, X ___. A1, ... ,An E F+.\nTo determine keys and calculate F+, a set of inference rules, which is both complete and sound, called Arm strong's axioms has been developed [Ullman, 1982]. According to these rules R(A1, • . . , An) can be decom posed into a collection of subsets p(R1, . . • , Rk) such that R = R1U, . . . U Rk. For a decomposition p of R the following properties are always desirable:\nLossless Join (LJ): SupposeD is a set of dependen cies in R. p has a lossless join w.r.t. D if\\lr on R, r = 7rR, (r) M ... M 7rR,.(r). With this property any rela tion can be recovered from its projections.\nDependency Preservation (DP): This requires that D is preserved by the projection 7rR.(D) of D onto R;'s, where 7rR;(D) = { < X, Y > I < X, Y >E D+}, XY � R;, XY = XU Y and < X, Y > represents either X -> Y or X ->-> Y. p preserves D if\nk\nV < X, Y >E D, < X, Y >E jj+, D = U 7rR;(D). i=l\nThe normal forms we are going to discuss include:\nFourth Normal Form ( 4NF): R is in 4th normal form if \\IX->-> Y ED,\nY # (/J /\\ Y � X/\\ XY # R ===> 3 a key K, K � X.\nAcyclic Databases: The relations of the database\nThe model [Gallant, 1988] consists of 6 symptoms, 2 diseases, and 3 possible treatments in Table 1. The three columns in Table 1 forms a simple database. All variables are binary variables taking values 1 and -1,\nFrom Relational Databases to Belief Networks 407\nexcept variables u3 and u5 which can take values 1, 0, and -1. Here 1 means that the corresponding propo sition is true, -1 means false, and 0 means unknown. Suppose that there is a set F of FD on R(u1, .. . ,uu):\nu1, u2, U3 ue, u7, us Ug1 U10 � U7. u3, u4,us � ug. � Ug. u3, U71 Ug --) UlQ. ---+ un.\nAccording to Algorithm 2.1, we have a decomposition of R with a lossless join and preservation of dependen cies in F which contains the following relation schemes: R1 = {u1, u2,ua,u7}. R3 = {u6, u7, u8, ug}. Rs = {ug, u10, u11}. R2 = { u3, u4, us, us}. R4 = {u3, u7, us, u1o}. R6 = { u1, u2, u3, u4, us, us}.\nExample 2 [Cooper, 1984]: Metastatic cancer (A) is a possible cause of a brain tumor (C) and is also\n408 Wen\nan explanation for increased total serum calcium ( B). In turn, either of these could explain a patient falling into a coma (D). Severe headache (E) is also possibly associated with a brain tumor.\nSuppose we have the statistical information in Table 3 from a sample database. Each entry of the table gives the number of occurrences of the records in the database which contain various combinations of the attributes A, B, C, D, and E. This can be obtained by\na projection of the universal relation of the database on the set ABC DE without eliminating the duplicates. Obviously, the following MD hold on ABCDE:\nA__,__, B, A__,__, C, B, C __,__, D, C __,__, E.\nThus, we can use Algorithm 2.1 to decompose ABCDE into 4NF subrelations AB, AC, BCD, CE. Each rela tion preserves the corresponding dependency and they have a lossless join equal to ABCDE. The statistical information in su brelations is shown in Table 4.\n3 BELIEF NETWORKS\nIn this section, we introduce some concepts of BN based on the theory of discrete Markov random fields\n3.1 PROB ABI LIS TIC D EP END ENCY\nData extracted from a RD are called relational data. Although statistical relational data may satisfy MD, it refers to only FD and probabilistic dependency, a special kind of MD.\nDefinition 3.1: The frequency of an attribute sub set X of relation scheme R in a relation r on R is\nFx(r) = {Fx=x(r) = lux�fl1;\\l:z: E Dx} where Dx is the domain of X, lrl is the cardinality of r.\nDefinition 3.2: The conditional frequency of X in r given Y = y, Y C R is FxiY=Y=\nDefinition 3.3: Let X, Y C R, and Z = R- XY. r satisfies the probabilistic dependency (P D) X >-> Y if :z:yz E r ==>\nFxYZ=xyz(r) · Fx=x(r) = FxY=xy(r) · Fxz=u(r). According to the law of large numbers, it is reasonable to assume limlrl-= Fx(r) P(X), limlrl-= FxiY=y(r) = P(XIY = y), and if X ,_, Y then P(Y IX Z) = P(Y IX), ie. Y and Z are condition ally independent given X. It is easy to prove\nTheorem 3.1:\n1. X__, Y ==>X>-> Y with P(YIX) E {0, 1}, and\n2. X >-> Y ==> X __,__, Y.\nIn Example 2, we can easily check that the PD A >--> B, A>-> C, B, C >-> D, and C >-> E hold.\n3.2 B ASIC D EFINITIONS OF B ELI EF N ETWORKS\nConsider a probability space R = {Aili = 1, . . . m}, which corresponds to a relation scheme R, with JJ,f = a1 X . . . x am possible states S = {sj li = 1, ... , M} and a probability distribution p = {P(sj)lj = 1, ... ,M}. Each variable Ai in the space can take ai values. Sup pose according to the dependencies, we have the fol lowing constraint set CS on the distribution p of R:\nConditional constraints ( CCS ) :\nJ.Lk = P(A<oiXu, ... ,<p,.)\ncorresponding to the PD Xu, ... ,kp, >-> AkO, where k = 1, . . . , nand Xu, ... ,kp, ={Au, .. . , Akp,} C R.\nMarginal constraints ( M C S):\nZlk' = P(Xk'l, ... ,k'p�, ), where k' = 1, ... n'.\nUniversal constraint (UCS):\nP(Ao, . . . ,Am-d = 1. Ao, ... ,A...,_l\nIn Example 1, it is easy to extract the CCS in Table 5 from the sample database in Table 2. In Table 5, u stands for u = 1, u for u = -1 and u for u = 0. Note that all these conditional probabilities are accu rate and equal to 1, because all of the dependencies\nP(u71u,,u,,u,) =1, P( u1lu,, u,, u,) =1 P(u7lu,,u,,u,) =1, P(u,ju,,u,,u3) =1 P(u71u,,u,,u,) =1, P(u71u,,u,,u,) =1 P(u,ju,, u,, u,) =1,\nP(uslu,,u,,u.) =1, P(uslu,,u,,u.) =1 P( uslu,, u4, u•) =1, P( uslu,, u., u6) =1 P( uslu,, u,, u•) =1, P( uslu,, u., \"•) =1 P( uslu,, u,, uo) =1,\nP(uoluo,u7,us) =1, P(uolus,u7,us) =1 P(uolus,u7,us) =1, P(uoluo,u7,us) =1 P( uolus, u1, us) =1, P(uolus, u1, us) =1 P( uolus, u7, us) =1,\nP(u,olu,, u1, us) =1, P(u10lu,,u1,us) =1 P(u,olu,, u1, us) =1, P(u,olu,, u1, us) =1 P(u10lu,, u1, us) =1, P(u10lu,, u1, us) =1 P( u10 lu,, U7, us) =1,\nP(uuluo,u,o) =1, P(uuluo,ulO) =1 P(uuluo,u,o) =1, P( uuluo, u,o) =1\nare FD. It may not be the case for PD, ie. the con ditional probabilities may not be accurately estimated nor they necessarily equal to 1. Note also that the above set of conditional probabilities is not complete, eg. P(urlul, u2 , u3) is not specified.\nAccording to data dependencies and CCS, we may con struct a directed graph, or BN as follows\nDefinition 3.4: A Belief Network (BN) is a di rected graph G =< V, E >, such that\n1. The node set is V = R\nDefinition 3.5:\nA neighbor system cr in G is a set of sets { cr A; lA; E R, cr A; <;; R}, such that\n1. A; '/. cr A;, 2. Aj E crA; <===> 3 CCS J-!k = P(Ako!Xkl, ... ,kp) E\nC S, A;, Aj E Xko, ... ,kp\nThe neighbors of a set X C R in G is the set cr X = {A; E R -Xl3Aj E X, A; E crAj} ·\nThe neighborhood network of a BN G =< V, E > is Go =< V,Eo >, where Eo = {(A;,Ai)IA; E crAj}·\nA set C <;; R is called a clique if Aj E cr A; whenever A;, Aj E C and i # j. A clique MC is called maximal\nIn order to handle the combinatorial explosion of the number of states in BN, a decomposition may be de sired. The concept of neighbor Gibbs field [Wen, 1989] provides a valid factorization of the joint distribution for Markov random fields to localize the computation of the joint ME/MCE distribution of the whole BN within each of the maximal cliques of the neighbor hood network. This suggests that the network should be decomposed into a hypergraph with the cliques of the neighborhood network as its hyperedges. To keep consistency among the distributions of the cliques, the results obtained in each clique need to be propagated to other cliques through their intersections. Conse quently, it is desired to organize the decomposed result as an acyclic hypergraph [Beeri et al., 1983] to guar antee the termination of the propagation and to avoid other possible anomalies during the propagation.\n410 Wen\nThe decomposition techniques proposed in [Spiegelhal ter, 1986; Wen, 1991] are described briefly as follows:\n1. Construct a neighborhood network a a = < V, Ea > for BN a =< V, E >.\n2. Find a fill-in [Tarjan and Yannakakis, 1984] F of a a, such that Da, the MCC of a,=< V, F u Ea >\n• has the minimum IF I, for Spiegelhalter's method,\n• has the minimum total number of states of all cliques in a, for our method [Wen, 1991].\nDa is the decomposition wanted and corresponds to an acyclic hypergraph < V, Da > .\nUnfortunately, it has been shown that the problem of optimum belief network decomposition is NP hard under all of the above optimum criteria [Yannakakis, 1981; Wen, 1991]. Therefore, we proposed an al gorithm to obtain the optimum belief decomposition based on simulated annealing [Wen, 1991].\nIn Example 1, it is easy to verify by Graham reduction [Beeri et al., 1983] that the neighborhood network in Fig. 2 has already been an acyclic hypergraph. The decomposition is shown in Fig. 3. There are 6 sub-\nFigure 3: Decomposition of the BN for Example 2\nnetwroks in the decomposition: MC1 = {u9, u10, u11}, MC2 = {u7,us,ug,u10} MC3 = {u6,u7,us, ug}, MC4 = {u3, u7,us, u10}, MC5 = {u3,u4,u5, u8}, and 1 1C6 = {u1,u2,u3,u7}. The total number of states here is 124, comparing 29 x 32 = 4608 states in the original BN.\nNote that the decomposition corresponding to an acyclic database scheme [Beeri et a/., 1983] in Table 6, each relation scheme corresponds to a maximal clique in the decomposition. These relation schemes also satisfy JD and thus have lossless join and running\nintersection property [Beeri et a/., 1983]. In Fig. 3, the intersections between cliques have been shown by bold edges or shaded triangles.\nIn [Wen, 1989], we have proven\nTheorem 3.2: Belief updatin& the whole BN with Jeffrey's rule [Wen, 1990bj is equivalent to be lief updating the clique in the acyclic decomposi tion which contains the corresponding constraint set, and Jeffrey belief propagation to all the other cliques through the running intersections.\n3.4 THE MAIN OPERATIONS ON BN\nThere are three main operations on BN:\nBelief extracting - Extracting a specified marginal distribution of a distribution. This corresponds to the projection operation in RD.\nUpdating - Given a new marginal on a subspace and a prior on the whole space, calculate a plausible posterior of the whole space matching with the given marginal. Bayes or Minimum Cross Entropy (MCE) posterior, particularly the posterior obtained by Jef frey's updating [Wen, 1990b], are considered as plau sible. This corresponds to selection operation for RD.\nBelief propagation- After updating the marginal of a subspace, propagate the changes to the whole space through the running intersections. This operation cor responds to the join operation in RD.\n4 INITIAL DISTRIBUTIONS\nFor the initial distribution of a decomposed network, two requirements should be satisfied.\n1. The distribution should reflect the data in the sample database as faithfully as possible.\n2. The distribution should predicate unseen cases as accurately as possible.\n4.1 RECA LL\nThere may be many distributions satisfying the fist re quirement if the specification is incomplete. The most trivial one can be constructed as follows:\nIn Table 2, use 0 and 1 to replace of -1 and 1 for binary variables and use 00, 01 and 10 to replace of -1, 0 and 1 for u3 and us, and convert each row in each subrelation into hexadecimal, then we obtain \"Index\" in Table 7. Suppose all these examples are equally important, or\nhave the same probability, we obtain \"Probabilities\" in Table 7.\nThus, Table 7 gives all the non-zero probabilities in a distribution satisfying all of the training examples. Us ing this simple distribution, we can perform all recall like reasoning. Suppose we are given\nu1 = 1: a patient has swollen feet, u3 = 1: the patient suffers from hair loss, u6 = -1: the patient is not allergic to placibin.\nThis corresponds to a constraint set {P(ul) 1, P(u3) = 1, P(u6) = 0}. By Jeffrey's updating and belief propagation, we obtain a posterior in Table 8. This implies\n(1) U2 = U7 = Ug = Uu = 1, (2) Us = 0, ( 3) u4 = u8 = u1o = -1.\nComparing with the result in [Gallant, Oct 1987]:\nFrom Relational Databases to Belief Networks 411\n(1) U7 = Ug = U11 = 1, (2) U1Q = -1,\nwe can see that our method do reasoning in all direc tions while Gallants can only do reasoning bottom-up.\nThis method is simple but has some disadvantages:\n1. There are still \"unseen\" cases that cannot be han dled by this method.\n2. Not all MCS extracted from an RD are always consistent.\nTo overcome the second difficulty, we should extract a set of conditional probabilities, or Local Characteris tics (LC), instead of marginal ones, because any distri bution is completely determined by its LC's and a set of non-redundant LC's can be always made consistent.\n4.2 PREDICTION OR GENERA LIZATION\nIt is obviously more difficult to predicate unseen cases than to just recall the cases encountered before.\nHaving constructed a BN, we can use one of the follow ing methods to learn a set of LC's from an incomplete training database:\nFrequency Method: A set of LC's can be learned in the same way as that extracting marginal probabilities described in the previous subsection. The disadvan tage is that it has no generalization ability at all.\nME/MCE methods assign a uniform conditional distribution to the unseen cases. A special case of ME/MCE methods is so called Dirichlet distribution [Herskovitz and Cooper, 1990] which uses the following formula\nP(x- III _ )_G (X = x, llx =7rx)+1\n- X X - 7fX - --����--�--�-- G(llx = 1rx) + Vx\nwhere X is a variable in the underlying BN, x is one of the Vx values can be taken by X, IIx is the set of parents of X in the BN, 7f x is a particular instantia tion of IIx, and C( iP) is the number of cases/tuples in the database that match the instantiated set of vari ables <P. When the case/tuple does not occur in the database, the above conditional probability becomes P(X = xiiix = 7fX ) = ix and thus is a uniform one.\nThe NN/OR Method [Duda and Hart, 1973; Blumer et al., 1987]: For this method, the conditional probability assigned to an unseen case depends on its\n412 Wen\nneighbor conditional distributions. That is, if the un seen case has many neighbors who have high probabil ities to occur then it is assigned a relatively high con ditional probability, otherwise, a low or even zero con ditional probability. When there are neighbors having different probabilities, the NN /OR method prefers the choice making the final result simplest.\n4.3 THE NN/OR LEARNING\nIt has been shown that the decomposed relations pre serve all dependencies existing in the original relation. For Example 1, this means that learning can be per formed within each relation (see Table 2). For exam ple, in relation { u1, u2, u3, u7 }, conditional distribu tion P(u1lu1, u2, ua) can be learned as follows:\n1. Use frequency method to obtain the conditional probabilities for the cases occurring in the training database. For relation (u1,u2,ua,u7) in Table 2. The result is\nP(u1lu1,u2,ua) = 0 P(u1lu1,u2,ua) = 0 P(u1lu1, u2, ua) = 0 P(u1lu1, u2,ua) P(u1lu1, u2, ua) P(u7lu1, u2, ua) =1 = 1 = 1\n2. Draw a Karnough-like map for u1, u2, u3 and fill the probability values learned in step 1 into the corre sponding entries of the map (see Fig.4 a).\nFigure 4: The results of learning by NN/OR method\n3. For the unseen cases, entry (-1,1,-1) is filled with value 0 because all its nearest neighbors have entry values 0, and entry (1,-1,1) is filled with 1, similarly.\nIn some cases, the entries in the nearest neighborhood have different values. For example, P(u8lu3, u4, us) (see Fig. 4 b) has two nearest neighbors, ( -1,1, 1) and (1,1,-1), with values 1 and another one, (-1,-1,-1) with value 0. In this case, the Occam's Razor principle [Blumer et al., 1987] can be used to choose the value for the unseen cases. The principle says\nAmong the hypotheses consistent or compat-\nible with the given data set, choose the sim plest one.\nThere are many proposed measures of simplicity, the most common ones are as follows\n1. Kolmogorov complexity, 2. Minimum description length, and 3. Logic formula complexity, combinational com-\nplexity, and time complexity [Pearl, 1978].\nWe adopt logic formula complexity which depends on the number of connectives in the logic formula. Trying to assign 0 and 1 to entry (u3,u4, us), respectively, we find that logic formula u4 + u3us corresponding to P(u7lua, u4, us)= 1 is simpler than uau4+u3u5 +u4u5 corresponding to P( u7lu3, u4, u5) = 0. Therefore, we choose P(u7lua, u4, us)= 1.\nThe logical expressions of the 4 possible assignments for unseen cases in Fig. 4 a. are give in Table 9. Obviously, the one obtained by NN /OR method is the simplest and has the shortest code length.\nFig. 5 gives the results when some statistical informa tion in Table 4 c is missing.\nIT>(DIB,C c c IPCDJB,C c c B 0.05 r o.al B 10.05 '9/!ll:\\1 B II 0.8 19:��[] B 10.8 0.8 I a. P(DIB,C) m1ssmg (0.05- BC+0.8-(B+C)) b. P(DIB,C) missing (0.05-.8+0.8· B)\nIT>(DIB,C c c IP(DJB,C c B r 0.05l ro.8 l B 1 m!l:': B -u�:o� lo.sl B 1 o.8\nc. P(DJB,O) missing d. P(Dili, c) missing (0.05-C+0.8-C) (0.8)\nc 0.8 l o.8 I\nFigure 5: Learning P(DIB, C) in Example 2\nNote that we are using these examples to describe the method proposed here, we are not saying that any required learning accuracy can be guaranteed by the given sample sets. For more detail about learning from statistical relational data, see [Wen, 1990a].\n5 CONCLUSIONS\nThe relationship between RD and BN is investigated. Correspondences are discovered between many con-\ncepts and operations of RD and BN. A method to construct BN automatically from RD is proposed. It has been shown [Wen, 1990a] that the distributions of discrete Markov fields, eg. BN, are Probably Approx imately Correctly (PAC) learnable [Haussler, 1990] when the sizes of the biggest neighborhoods of the vari ables in the fields are fixed. Hence, our method, is ef ficient in these circumstances. A comparison between our method and other methods shows that\n1. Our method can fulfill the task of recall perfectly just as some other methods.\n2. Our method has more plausible result than that of other methods when generalization or predic tion is needed. For example, frequency method does not have any prediction capability while ME/MCE methods can not handle the case of functional dependency properly (always assign values � to all binary unseen cases).\nAcknowledgement\nThanks to A. Jennings and H. Liu for discussions and comments. The permission of the Executive General Manager, TRL, to publish this paper is gratefully ac knowledged.\nReferences\n[Beeri et al., 1983] Catriel Beeri, Ronald Fagin, David Maier, and Mihalis Yannakakis. On the desirability of acyclic database schemes. Journal of the ACM, 30(3):479-513, 1983.\n[Blumer et al., 1987] A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. Occam's ra zor. Information Processing Letters, 24:377-380, Apr. 1987.\n[Cooper, 1984] G. F. Cooper. NESTOR: a computer based medical diagnostic aid that integrates causal and probabilistic knowledge. Report HPP-84-48, Stanford University, 1984.\n[Duda and Hart, 1973] R. 0. Duda and P. E. Hart. Pattern Classification and Scene Analysis. John Wi ley and Sons, New York, 1973.\n[Gallant, 1988] S. I. Gallant. Connectionist expert systems. Communications of the ACM, 31(2):152- 169, 1988.\n[Gallant, Oct 1987] S. I. Gallant. Bayesian assessment of a connectionist model for fault detection. Techni cal Report TR MU-CCS-87-25, College of Computer Science, Northeastern University, Oct. 1987.\n[Haussler, 1990] D. Haussler. Probably approximately correct learning. In Proc. AAAI-90, volume 2, pages 1101-1108, Boston, MA, July 1990. American Asso ciation for Artificial Intelligence, AAAI Press J The MIT Press.\nFrom Relational Databases to Belief Networks 413\n[Herskovitz and Cooper, 1990] E. Herskovitz and G. F. Cooper. Kutato: An entropy-driven sys tem from construction of probabilistic expert sys tems from databases. In M. Henrion and Bonis sane, editors, Proc. 6th International Conf. on Un certainty in AI, Cambridge, MA., July 1990. North Holland.\n[Lauritzen and Spiegelhalter, 1988] S. L. Lauritzen and D. J. Spiegelhalter. Local computations with probabilities on graphical structures and their ap plication to expert systems. J. R. Statist. Soc. B, 50(2), 1988.\n[Pearl, 1978] J. Pearl. On the connection between the complexity and credibility of inferred models. Int. J. General Systems, 4:255-264, 1978.\n[Shore and Johnson, Jan 1980] J. Shore and R. W. Johnson. Axiomatic derivation of the principle of maximum entropy and the principle of minimum cross entropy. IEEE Trans. Infor. Theory, IT26(1):26-37, Jan 1980.\n[Spiegelhalter, 1986] D. J. Spiegelhalter. Probabilistic reasoning in predictive expert systems. in Uncer tainty in artificial intelligence, (ed) L. N. Kanal and J. F. Lemmer, North-Holland, Amsterdam, New York, Oxford, Tokyo, 1986.\n[Tarjan and Yannakakis, 1984] R. E. Tarjan and M. Yannakakis. Simple linear-time algorithms to chordality of graphs, test acyclicity of hypergraphs, and selectively reduce acyclic hypergraphs. SIAM J. Compt., 13(3):566-579, 1984.\n[Ullman, 1982] J. D. Ullman. Principles of Database Systems. Computer Science Press, Rockville, Mary land, 1982.\n[Wen, 1989] W. X. Wen. Markov and Gibbs Fields and MCE Reasoning in General Belief Networks. Tech nical Report 89/13, Computer Science, The Univer sity of Melbourne, July 1989.\n[Wen, 1990a] W. X. Wen. Learning from statistical relational data. Tech. report, AI Systems, Telecom Research Labs., 770 Blackburn Rd. Clayton, Vic. 3168, Australia., 11 1990.\n[Wen, 1990b] W. X. Wen. MCE Reasoning in Recur sive Causal Networks. Uncertainty in Artificial In telligence 4, ( ed) T. Levitt, R. Shachter, J. Lemmer, and L. Kanal, North Holland, 1990.\n[Wen, 1991] W. X. Wen. Optimal decomposition of belief networks. In Max Herion, Piero Bonissone, J. F. Kana!, and J. F. Lemmer, editors, UNcertainty in Artificial Intelligence 6. North Holland, 1991.\n[Yannakakis, 1981] M. Yannakakis. minimum fill-in is NP-complete. Meth., 2:77-79, 1981. Computing the SIAM J. Alg."
    } ],
    "references" : [ {
      "title" : "Journal of the ACM",
      "author" : [ "Catriel Beeri", "Ronald Fagin", "David Maier", "Mihalis Yannakakis. On the desirability of acyclic database schemes" ],
      "venue" : "30(3):479-513,",
      "citeRegEx" : "Beeri et al.. 1983",
      "shortCiteRegEx" : null,
      "year" : 1983
    }, {
      "title" : "Occam's ra­ zor",
      "author" : [ "A. Blumer", "A. Ehrenfeucht", "D. Haussler", "M.K. Warmuth" ],
      "venue" : "Information Processing Letters, 24:377-380, Apr.",
      "citeRegEx" : "Blumer et al.. 1987",
      "shortCiteRegEx" : null,
      "year" : 1987
    }, {
      "title" : "NESTOR: a computer­ based medical diagnostic aid that integrates causal and probabilistic knowledge",
      "author" : [ "G.F. Cooper" ],
      "venue" : "Report HPP-84-48, Stanford University",
      "citeRegEx" : "Cooper. 1984",
      "shortCiteRegEx" : null,
      "year" : 1984
    }, {
      "title" : "Pattern Classification and Scene Analysis",
      "author" : [ "Duda", "Hart", "1973] R. 0. Duda", "P.E. Hart" ],
      "venue" : null,
      "citeRegEx" : "Duda et al\\.,? \\Q1973\\E",
      "shortCiteRegEx" : "Duda et al\\.",
      "year" : 1973
    }, {
      "title" : "Connectionist expert systems",
      "author" : [ "S.I. Gallant" ],
      "venue" : "Communications of the ACM, 31(2):152169",
      "citeRegEx" : "Gallant. 1988",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "Techni­ cal Report TR MU-CCS-87-25",
      "author" : [ "S.I. Gallant. Bayesian assessment of a connectionist model for fault detection" ],
      "venue" : "College of Computer Science, Northeastern University, Oct.",
      "citeRegEx" : "Gallant. Oct 1987",
      "shortCiteRegEx" : null,
      "year" : 1987
    }, {
      "title" : "AAAI-90",
      "author" : [ "D. Haussler. Probably approximately correct learning. In Proc" ],
      "venue" : "volume 2, pages 1101-1108, Boston, MA, July",
      "citeRegEx" : "Haussler. 1990",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "Local computations with probabilities on graphical structures and their ap­ plication to expert systems",
      "author" : [ "S.L. Lauritzen", "D.J. Spiegelhalter" ],
      "venue" : "J. R. Statist. Soc. B, 50(2)",
      "citeRegEx" : "Lauritzen and Spiegelhalter. 1988",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "On the connection between the complexity and credibility of inferred models",
      "author" : [ "J. Pearl" ],
      "venue" : "Int. J. General Systems, 4:255-264",
      "citeRegEx" : "Pearl. 1978",
      "shortCiteRegEx" : null,
      "year" : 1978
    }, {
      "title" : "IT26(1):26-37",
      "author" : [ "J. Shore", "R.W. Johnson. Axiomatic derivation of the principle of maximum entropy", "the principle of minimum cross entropy. IEEE Trans. Infor. Theory" ],
      "venue" : "Jan",
      "citeRegEx" : "Shore and Johnson. Jan 1980",
      "shortCiteRegEx" : null,
      "year" : 1980
    }, {
      "title" : "Probabilistic reasoning in predictive expert systems",
      "author" : [ "D.J. Spiegelhalter" ],
      "venue" : "Uncer­ tainty in artificial intelligence, (ed) L. N. Kanal and J. F. Lemmer, North-Holland, Amsterdam, New York, Oxford, Tokyo",
      "citeRegEx" : "Spiegelhalter. 1986",
      "shortCiteRegEx" : null,
      "year" : 1986
    }, {
      "title" : "Simple linear-time algorithms to chordality of graphs",
      "author" : [ "R.E. Tarjan", "M. Yannakakis" ],
      "venue" : "test acyclicity of hypergraphs, and selectively reduce acyclic hypergraphs. SIAM J. Compt., 13(3):566-579",
      "citeRegEx" : "Tarjan and Yannakakis. 1984",
      "shortCiteRegEx" : null,
      "year" : 1984
    }, {
      "title" : "Principles of Database Systems",
      "author" : [ "J.D. Ullman" ],
      "venue" : "Computer Science Press, Rockville, Mary­ land",
      "citeRegEx" : "Ullman. 1982",
      "shortCiteRegEx" : null,
      "year" : 1982
    }, {
      "title" : "Tech­ nical Report 89/13",
      "author" : [ "W.X. Wen. Markov", "Gibbs Fields", "MCE Reasoning in General Belief Networks" ],
      "venue" : "Computer Science, The Univer­ sity of Melbourne, July",
      "citeRegEx" : "Wen. 1989",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "770 Blackburn Rd",
      "author" : [ "W.X. Wen. Learning from statistical relational data. Tech. report", "AI Systems", "Telecom Research Labs." ],
      "venue" : "Clayton, Vic. 3168, Australia., 11",
      "citeRegEx" : "Wen. 1990a",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "MCE Reasoning in Recur­ sive Causal Networks. Uncertainty in Artificial In­",
      "author" : [ "Wen", "1990b] W.X. Wen" ],
      "venue" : null,
      "citeRegEx" : "Wen and Wen.,? \\Q1990\\E",
      "shortCiteRegEx" : "Wen and Wen.",
      "year" : 1990
    }, {
      "title" : "Optimal decomposition of belief networks",
      "author" : [ "W.X. Wen" ],
      "venue" : "Max Herion, Piero Bonissone, J. F. Kana!, and J. F. Lemmer, editors, UNcertainty in Artificial Intelligence 6. North Holland",
      "citeRegEx" : "Wen. 1991",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "minimum fill-in is NP-complete",
      "author" : [ "M. Yannakakis" ],
      "venue" : "Meth., 2:77-79",
      "citeRegEx" : "Yannakakis. 1981",
      "shortCiteRegEx" : null,
      "year" : 1981
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "Lauritzen, Spiegelhalter [1988], Herskovitz and Copper [1990] used a method based on Maximum En­ tropy (ME) principle [Shore and Johnson, Jan 1980] to obtain a consistent distribution from empirical data.",
      "startOffset" : 118,
      "endOffset" : 147
    }, {
      "referenceID" : 14,
      "context" : "In this paper, the relationship between RD and BN is investigated and a method to construct BN from statistical RD [ Wen, 1990a] is proposed based on the principles of Nearest Neighborhood (NN) [ Duda and Hart, 1973] and Occam's Razor (OR) [Blumer et al.",
      "startOffset" : 115,
      "endOffset" : 128
    }, {
      "referenceID" : 1,
      "context" : "In this paper, the relationship between RD and BN is investigated and a method to construct BN from statistical RD [ Wen, 1990a] is proposed based on the principles of Nearest Neighborhood (NN) [ Duda and Hart, 1973] and Occam's Razor (OR) [Blumer et al., 1987].",
      "startOffset" : 240,
      "endOffset" : 261
    }, {
      "referenceID" : 12,
      "context" : "According to the relational database theory [Ullman, 1982], we have the following basic definitions in RD: A relation r is a subset of the Cartesian product of do­ mains D1, .",
      "startOffset" : 44,
      "endOffset" : 58
    }, {
      "referenceID" : 12,
      "context" : "To determine keys and calculate F+, a set of inference rules, which is both complete and sound, called Arm­ strong's axioms has been developed [Ullman, 1982].",
      "startOffset" : 143,
      "endOffset" : 157
    }, {
      "referenceID" : 0,
      "context" : "Acyclic Databases: The relations of the database form an acyclic hypergraph [Beeri et al., 1983].",
      "startOffset" : 76,
      "endOffset" : 96
    }, {
      "referenceID" : 12,
      "context" : "The following algorithms [Ullman, 1982] decomposes R into a 4NF decomposition with LJ and DP.",
      "startOffset" : 25,
      "endOffset" : 39
    }, {
      "referenceID" : 4,
      "context" : "The model [Gallant, 1988] consists of 6 symptoms, 2 diseases, and 3 possible treatments in Table 1.",
      "startOffset" : 10,
      "endOffset" : 25
    }, {
      "referenceID" : 2,
      "context" : "Example 2 [Cooper, 1984]: Metastatic cancer (A) is a possible cause of a brain tumor (C) and is also",
      "startOffset" : 10,
      "endOffset" : 24
    }, {
      "referenceID" : 13,
      "context" : "The concept of neighbor Gibbs field [Wen, 1989] provides a valid factorization of the joint distribution for Markov random fields to localize the computation of the joint ME/MCE distribution of the whole BN within each of the maximal cliques of the neighbor­ hood network.",
      "startOffset" : 36,
      "endOffset" : 47
    }, {
      "referenceID" : 0,
      "context" : "Conse­ quently, it is desired to organize the decomposed result as an acyclic hypergraph [Beeri et al., 1983] to guar­ antee the termination of the propagation and to avoid other possible anomalies during the propagation.",
      "startOffset" : 89,
      "endOffset" : 109
    }, {
      "referenceID" : 16,
      "context" : "The decomposition techniques proposed in [Spiegelhal­ ter, 1986; Wen, 1991] are described briefly as follows:",
      "startOffset" : 41,
      "endOffset" : 75
    }, {
      "referenceID" : 11,
      "context" : "Find a fill-in [Tarjan and Yannakakis, 1984] F of a a, such that Da, the MCC of a,=< V, F u Ea >",
      "startOffset" : 15,
      "endOffset" : 44
    }, {
      "referenceID" : 16,
      "context" : "• has the minimum IF I, for Spiegelhalter's method, • has the minimum total number of states of all cliques in a, for our method [Wen, 1991].",
      "startOffset" : 129,
      "endOffset" : 140
    }, {
      "referenceID" : 17,
      "context" : "Unfortunately, it has been shown that the problem of optimum belief network decomposition is NP hard under all of the above optimum criteria [Yannakakis, 1981; Wen, 1991].",
      "startOffset" : 141,
      "endOffset" : 170
    }, {
      "referenceID" : 16,
      "context" : "Unfortunately, it has been shown that the problem of optimum belief network decomposition is NP hard under all of the above optimum criteria [Yannakakis, 1981; Wen, 1991].",
      "startOffset" : 141,
      "endOffset" : 170
    }, {
      "referenceID" : 16,
      "context" : "Therefore, we proposed an al­ gorithm to obtain the optimum belief decomposition based on simulated annealing [Wen, 1991].",
      "startOffset" : 110,
      "endOffset" : 121
    }, {
      "referenceID" : 0,
      "context" : "In Example 1, it is easy to verify by Graham reduction [Beeri et al., 1983] that the neighborhood network in Fig.",
      "startOffset" : 55,
      "endOffset" : 75
    }, {
      "referenceID" : 13,
      "context" : "In [Wen, 1989], we have proven",
      "startOffset" : 3,
      "endOffset" : 14
    }, {
      "referenceID" : 5,
      "context" : "Comparing with the result in [Gallant, Oct 1987]: From Relational Databases to Belief Networks 411",
      "startOffset" : 29,
      "endOffset" : 48
    }, {
      "referenceID" : 1,
      "context" : "The NN/OR Method [Duda and Hart, 1973; Blumer et al., 1987]: For this method, the conditional probability assigned to an unseen case depends on its",
      "startOffset" : 17,
      "endOffset" : 59
    }, {
      "referenceID" : 1,
      "context" : "In this case, the Occam's Razor principle [Blumer et al., 1987] can be used to choose the value for the unseen cases.",
      "startOffset" : 42,
      "endOffset" : 63
    }, {
      "referenceID" : 8,
      "context" : "Logic formula complexity, combinational complexity, and time complexity [Pearl, 1978].",
      "startOffset" : 72,
      "endOffset" : 85
    }, {
      "referenceID" : 14,
      "context" : "For more detail about learning from statistical relational data, see [Wen, 1990a].",
      "startOffset" : 69,
      "endOffset" : 81
    }, {
      "referenceID" : 14,
      "context" : "It has been shown [Wen, 1990a] that the distributions of discrete Markov fields, eg.",
      "startOffset" : 18,
      "endOffset" : 30
    }, {
      "referenceID" : 6,
      "context" : "BN, are Probably Approx­ imately Correctly (PAC) learnable [Haussler, 1990] when the sizes of the biggest neighborhoods of the vari­ ables in the fields are fixed.",
      "startOffset" : 59,
      "endOffset" : 75
    } ],
    "year" : 2011,
    "abstractText" : "The relationship between belief networks and relational databases is examined. Based on this analysis, a method to construct belief networks automatically from statistical rela­ tional data is proposed. A comparison be­ tween our method and other methods shows that our method has several advantages when generalization or prediction is deeded.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}