{
  "name" : "1301.3898.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Probabilities of Causation: Bounds and Identification",
    "authors" : [ "Jin Tian" ],
    "emails" : [ "@cs." ],
    "sections" : [ {
      "heading" : null,
      "text" : "This paper deals with the problem of esti mating the probability that one event was a cause of another in a given scenario. Us ing structural-semantical definitions of the probabilities of necessary or sufficient cau sation (or both), we show how to optimally bound these quantities from data obtained in experimental and observational studies, making minimal assumptions concerning the data-generating process. In particular, we strengthen the results of Pearl (1999) by weakening the data-generation assumptions and deriving theoretically sharp bounds on the probabilities of causation. These results delineate precisely how empirical data can be used both in settling questions of attribution and in solving attribution-related problems of decision making.\n1 Introduction\nAssessing the likelihood that one event was the cause of another guides much of what we understand about (and how we act in) the world. For example, few of us would take aspirin to combat headache if it were not for our conviction that, with high proba bility, it was aspirin that \"actually caused\" relief in previous headache episodes. [Pearl, 1999] gave coun terfactual definitions for the probabilities of neces sary or sufficient causation (or both) based on struc tural model semantics, which defines counterfactuals as quantities derived from modifiable sets of func tions [Galles and Pearl, 1997, Galles and Pearl, 1998, Halpern, 1998, Pearl, 2000, chapter 7]. The central aim of this paper is to estimate proba bilities of causation from frequency data, as obtained in experimental and observational statistical studies. In general, such probabilities are non-identifiable, that\nis, non-estimable from frequency data alone. One factor that hinders identifiability is confounding - the cause and the effect may both be influenced by a third factor. Moreover, even in the absence of confounding, probabilities of causation are sensi tive to the data-generating process, namely, the func tional relationships that connect causes and effects [Robins and Greenland, 1989, Balke and Pearl, 1994]. Nonetheless, useful information in the form of bounds on the probabilities of causation can be extracted from empirical data without actually knowing the data generating process. We show that these bounds im prove when data from observational and experimental studies are combined. Additionally, under certain as sumptions about the data-generating process (such as exogeneity and monotonicity), the bounds may col lapse to point estimates, which means that the prob abilities of causation are identifiable - they can be ex pressed in terms of probabilities of observed quantities. These estimates often appear in the literature as mea sures of attribution, and our analysis thus explicates the assumptions that must be ascertained before those measures can legitimately be interpreted as probabili ties of causation.\nThe analysis of this paper extends the results reported in [Pearl, 1999] [Pearl, 2000, pp. 283-308]. Pearl de rived bounds and identification conditions under cer tain assumptions of exogeneity and monotonicity, and this paper narrows his bounds and weakens his as sumptions. In particular, we show that for most of Pearl's results, the assumption of strong exogeneity can be replaced by weak exogeneity (to be defined in Section 3.3). Additionally, we show that the point estimates that Pearl obtained under the assumption of monotonicity (Definition 6) constitute valid lower bounds when monotonicity is not assumed. Finally, we prove that the bounds derived by Pearl, as well as those provided in this paper are sharp, that is, they cannot be improved without strengthening the assumptions. We illustrate the use of our results in the context of legal disputes (Section 4) and personal\n590 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\ndecision making (Section 5) .\n2 Probabilities of Causation: Definitions\nIn this section, we present the definitions for the three aspects of causation as defined in [Pearl, 1999]. We use the language of counterfactuals in its structural model semantics, as given in Balke and Pearl (1995), Galles and Pearl (1997, 1998), and Halpern (1998). We use Yx = y to denote the counterfactual sentence \"Variable Y would have the value y, had X been x.\" The structural model interpretation of this sentence reads: \"Deleting the equation for X from the model and setting the value of X to a constant x will yield a solution in which variable Y will take on the value y.\" One property that the counterfactual relationships sat isfy is the consistency condition [Robins, 1987]:\n(X= x) :::} (Yx = Y) (1)\nstating that if we intervene and set the experimental conditions X = x equal to those prevailing before the intervention, we should not expect any change in the response variable Y. This property will be used in sev eral derivations of this section and Section 3. For de tailed exposition of the structural account and its ap plications see [Pearl, 2000, chapter 7]. For notational simplicity, we limit the discussion to binary variables; extension to multi-valued variables are straightforward (see Pearl 2000, p. 286, footnote 5).\nDefinition 1 (Probability of necessity (PN)) Let X and Y be two binary variables in a causal model M, let x and y stand for the propositions X = true and Y = true, respectively, and x' and y' for their complements. The probability of necessity is defined as the expression"
    }, {
      "heading" : "PN = P(Yx' =f alse I X= true, Y =true)",
      "text" : "P(y��lx,y) (2)\nIn other words, PN stands for the probability that event y would not have occurred in the absence of event x, y�,, given that x and y did in fact occur. Note that lower case letters (e.g., x, y) stand for propo sitions (or events). Note also the abbreviations Yx for Yx = true and y� for Yx = false. Readers accustomed to writing \"A > B\" for the counterfactual \"B if it were � A\" can translate Eq. (2) to read PN = P(x' > y'lx,y). PN has applications in epidemiology, legal reasoning, and artificial intelligence (AI). Epidemiologists have\nlong been concerned with estimating the probability that a certain case of disease is attributable to a par ticular exposure, which is normally interpreted coun terfactually as \"the probability that disease would not have occurred in the absence of exposure, given that disease and exposure did in fact occur.\" This counter factual notion is also used frequently in lawsuits, where legal responsibility is at the center of contention (see Section 4).\nDefinition 2 (Probability of sufficiency (PS))\n� PS = P(YxiY',x') (3)\nPS finds applications in policy analysis, AI, and psy chology. A policy maker may well be interested in the dangers that a certain exposure may present to the healthy population [Khoury et al., 1989]. Counterfac tually, this notion is expressed as the \"probability that a healthy unexposed individual would have gotten the disease had he/she been exposed.\" In psychology, PS serves as the basis for Cheng's (1997) causal power the ory [Glymour, 1998], which attempts to explain how humans judge causal strength among events. In AI, PS plays a major role in the generation of explana tions [Pearl, 2000, pp. 221-223].\nDefinition 3 (Probability of necessity and sufficiency (PNS))\n(4)\nPNS stands for the probability that y would respond to x both ways, and therefore measures both the suf ficiency and necessity of x to produce y. Although none of these quantities is sufficient for de termining the others, they are not entirely indepen dent, as shown in the following lemma.\nLemma 1 The probabilities of causation satisfy the following relationship [Pearl, 1999] :\nPNS = P(x,y)PN + P(x',y')PS (5)\nSince all the causal measures defined above invoke conditionalization on y, and since y is presumed af fected by x, the antecedent of the counterfactual Yx, we know that none of these quantities is identifiable from knowledge of frequency data alone, even under condition of no confounding. However, useful infor mation in the form of bounds may be derived for\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 591\nthese quantities from frequency data, especially when knowledge about causal effects P(yx) and P(Yx') is also available1. Moreover, under some general assump tions about the data-generating process, these quanti ties may even be identified.\n3 Bounds and Conditions of Identification\nIn this section we will assume that experimental data will be summarized in the form of the causal effects P(yx) and P(Yx' ), and nonexperimental data will be summarized in the form of the joint probability func tion: Pxy = {P(x, y), P(x', y), P(x,y'), P(x', y')}.\n3.1 Linear programming formulation\nSince every causal model induces a joint probability distribution on the four binary variables: X, Y, Yx and Yx', specifying the sixteen parameters of this dis tribution would suffice for computing the PN, PS, and PNS. Moreover, since Y is a deterministic function of the other three variables, the problem is fully specified by the following set of eight parameters:\nPlll = P(yx,Yx', x)\nPllo = P(yx, Yx', x')\nP101= P(yx, y�,, x)\nPlOO = P(yx, y�,, x')\nPoll= P(y�, Yx', x)\nPow= P(y�, Yx', x')\nPOOl= P(y�, y�,, x)\nPooo = P(y�, y�,, x')\n= P(x, Y,Yx')\n= P(x', y, Yx)\n= P(x, y, y�,)\n= P(x',y', yx)\n= P(x,y', Yx')\n= P(x', y, y�)\n= P(x, y', y�,) = P(x', y', y�)\nwhere we have used the consistency condition Eq. (1). These parameters are further constrained by the prob abilistic equality\n1 1 1 I: I: L:Pijk = 1 i=O j=O k=O\nPijk :2:0 for i, j, k E {0, 1} (6)\nIn addition, the nonexperimental probabilities Pxy impose the constraints:\nPlll + P101 Poll+ Po01 Pno +Pow\nP(x,y)\nP(x, y')\nP(x', y)\n(7)\n1The causal effects P(yx) and P(Yx') can be estimated reliably from controlled experimental studies, and from certain observational (i.e., nonexperimental) studies which permit the control of confounding through adjustment of covariates [Pearl, 1995].\nand the causal effects, P(yx) and P(Yx' ), impose the constraints:\nPlll + Pno + P101 + Pwo P111 + Pno +Poll +Pow\nThe quantities we wish to bound are:\nPNS\nPN\nPS\nPlOl + PlOO P101 / P(x, y)\nPioo/ P(x', y')\n(8)\n(9)\n(10)\n(11)\nOptimizing the functions in (9)-(11), subject to equal ity constraints, defines a linear programming (LP) problem that lends itself to closed-form solution. Balke (1995, Appendix B) describes a computer program that takes symbolic descriptions of LP problems and returns symbolic expressions for the desired bounds. The program works by systematically enumerating the vertices of the constraint polygon of the dual prob lem. The bounds reported in this paper were produced (or tested) using Balke's program, and will be stated here without proofs; their correctness can be verified by manually enumerating the vertices as described in [Balke, 1995, Appendix B]. These bounds are guaran teed to be sharp because the optimization is global.\n3.2 Bounds with no assumptions\n3.2.1 Given nonexperimental data\nGiven Pxy, constraints (6) and (7) induce the follow ing upper bound on PNS:\n0:::; PNS:::; P(x,y) + P(x', y'). (12)\nHowever, PN and PS are not constrained by Pxy.\nThese constraints also induce bounds on the causal effects P(yx) and P(Yx' ) :\nP(x, y) P(x',y) ::=; P(yx) ::=; :::; P(Yx') :::;\n3.2.2 Given causal effects\n1- P(x, y')\n1- P(x', y') (13)\nGiven constraints (6) and (8), the bounds induced on PNS are:\nmax[O, P(yx)- P(Yx')] :::; PNS:::; min[J-'(Yx), P(y�,) ] (14)\nwith no constraints on PN and PS.\n3.2.3 Given both nonexperimental data and causal effects\nGiven the constraints (6), (7) and (8), the following bounds are induced on the three probabilities of cau-\n592 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\nsation:\n0 P(yx) - P(Yx') P(y) - P(Yx') P(yx)- P(y) } �PNS (15) { P(yx) }\nPNS P(y�,) < min P(x, y) + P(x', y') P(yx)- P(Yx') + P(x, y') + P(x', y)\nmax { max { 0 P(y)-P(Yz') P(x,y) } :::; PN:::; min { (16) P(y',)-1P(x',y') } P(x,y) (17)\nP(yz l !P(x,y) } p x',y')\n(18) Thus we see that some information about PN and PS can be extracted without making any assumptions about the data-generating process. Furthermore, com bined data from both experimental and nonexperimen tal studies yield information that neither study alone can provide.\n3.3 Bounds under exogeneity (no confounding)\nDefinition 4 (Exogeneity) A variable X is said to be exogenous for Y in model Miff\nP(yx) = P(yjx) and P(Yx') = P(yjx'). (19)\nIn words, the way Y would potentially respond to ex perimental conditions x or x' is independent of the ac tual value of X.\nEq. (19) is also known as \"no-confounding\" [Robins and Greenland, 1989], \"as if randomized,\" or \"weak ignorability\" [Rosenbaum and Rubin, 1983]. Combining Eq. (19) with the constraints of (6)-(8), the linear programming optimization (Section 3.1) yields the following results:\nTheorem 1 Under condition of exogeneity , the three probabilities of causation are bounded as follows:\nunder the condition of no-confounding the lower bound for PN can be expressed as\nPN > 1- 1 � 1- -1- (23) - P(yjx)/P(yjx') RR\nwhere RR � P(yjx)/ P(yjx') is called relative risk in epidemiology. Courts have often used the condi tion RR > 2 as a criterion for legal responsibility [Bailey et al., 1994]. Eq. (23) shows that this practice represents a conservative interpretation of the \"more probable than not\" standard (assuming no confound ing); PN must indeed be higher than 0.5 if RR exceeds 2.\n3.3.1 Bounds under strong exogeneity\nThe condition of exogeneity, as defined in Eq. (19) is testable by comparing experimental and nonexperi mental data. A stronger version of exogeneity can be defined as the joint independence {Yx, Yx'} llX which was called \"strong ignorability\" by Rosenbaum and Rubin (1983). Though untestable, such joint indepen dence is implied when we assert the absence of factors that simultaneously affect exposure and outcome.\nDefinition 5 (Strong Exogeneity) A variable X is said to be strongly exogenous for Y in model M iff {Yx, Yx'} llX, that is,\nP(yx, Yx' jx) = P(yx, Yx') P(Yx, y�, jx) = P(yx, y�,) P(y�, Yx'lx) P(y�, Yx') (24) P(y�, y�, jx) P(y�, y�,)\nRemarkably, the added constraints introduced by strong exogeneity do not alter the bounds of Eqs. (20) (22). They do, however, strengthen Lemma 1:\nTheorem 2 If strong exogeneity holds, the probabili ties PN, PS, and PNS are constrained by the bounds of Eqs. {20}-{22), and, moreover, PN, PS, and PNS are related to each other as follows [Pearl, 1999]:\nPN\nPS\nPNS P(yjx)\nPNS P(y'lx')\n(25)\n(26)\nmax(O, P(yjx)- P(yjx')]:::; PNS:::; min(P(yjx), P(y'jx')] (20)\nmax(O, P(ylx)- P(ylx')] < PN < min[P(ylx),P(y'lx')] 21 P(ylx) - - P(ylx) ( ) 3.4 Identifiability under monotonicity\nmax(O, P(ylx)- P(ylx')] < PS < min (P(ylx), P(y'lx')] (22) D fi •t• 6 (M t · 't )\nP( 'I ') - - P( 'I ' ) e m wn ono omc� y\nY x Y x A variable Y is said to be monotonic relative to vari-\n[Pearl, 1999] derived Eqs. (20)-(22) under a stronger condition of exogeneity (see Definition 5). We see that\nable X in a causal model M iff\nY� 1\\ Yx' = false (27)\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 593\nMonotonicity expresses the assumption that a change from X = false to X = true cannot, under any cir cumstance make Y change from true to false. In epi demiology, this assumption is often expressed as \"no prevention,\" that is, no individual in the population can be helped by exposure to the risk factor.\nIn the linear programming formulation of Section 3.1, monotonicity narrows the feasible space to the mani fold:\nPon 0\nP01o 0\n3.4.1 Given nonexperimental data\n(28)\nUnder the constraints (6), (7), and (28), we find the same bounds for PNS as the ones obtained under no assumptions (Eq. (12)) . Moreover, there are still no constraints on PN and PS. Thus, with nonexperimen tal data alone, the monotonicity assumption does not provide new information.\nHowever, the monotonicity assumption induces sharper bounds on the causal effects P(yx) and P(Yx' ):\nP(y) P(x', y) :S P(yx) :S :S P(Yx•) :S 1- P(x, y') P(y) (29)\nCompared with Eq. (13), the lower bound for P(Yx) and the upper bound for P(Yx') are tightened. The importance of Eq. (29) lies in providing a simple nec essary test for the commonly made assumption of \"no-prevention.\" These inequalities are sharp, in the sense that every combination of experimental and non experimental data that satisfy these inequalities can be generated from some causal model in which Y is monotonic in X. Alternatively, if the no-prevention as sumption is theoretically unassailable, the inequalities of Eq. (29) can be used for testing the compatibility of the experimental and non-experimental data, namely, whether subjects used in clinical trials were sampled from the same target population, characterized by the joint distribution Pxy.\n3.4.2 Given causal effects\nConstraints (6), (8), and (28) induce no constraints on PN and PS, while the value of PNS is fully determined:\nThat is, under the assumption of monotonicity, PNS can be determined by experimental data alone, al though the joint event Yx 1\\ y�. can never be observed.\n3.4.3 Given both nonexperimental data and causal effects\nUnder the constraints (6)-(8) and (28) , the values of PN, PS, and PNS are all determined precisely.\nTheorem 3 If Y is monotonic relative to X, then PNS, PN, and PS are given by\nPNS P(yx, Y�·) = P(yx) - P(Yx') (30)\nPN P(y�. lx, y) = P(y) - P(Yx')\n(31) P(x, y)\nPS P(yx lx', y') = P(yx) - P(y)\n(32) P(x', y')\nEqs. (30)-(32) are applicable to situations where, in addition to observational probabilities, we also have information about the causal effects P(yx) and P(Yx' ). Such information may be obtained either directly, through separate experimental studies, or indirectly, from observational studies in which certain identifying assumptions are deemed plausible (e.g., assumptions that permits identification through adjustment of co variates) [Pearl, 1995].\n3.5 Identifiability under monotonicity and exogeneity\nUnder the assumption of monotonicity, if we further assume exogeneity, then P(yx) and P(Yx') are identi fied through Eq. (19), and from theorem 3 we conclude that PNS, PN, and PS are all identifiable.\nTheorem 4 (Identifiability under exogeneity and monotonicity) If X is exogenous and Y is monotonic relative to X, then the probabilities PN, PS, and PNS are all identi fiable, and are given by\nPNS\nPN\nPS\nP(yjx)- P(yjx') (33) P(y)- P(yjx') = P(yjx)- P(yjx') (34)\nP(x, y) P(yjx)\nP(yjx)- P(y) P(yjx)- P(yjx') (3S) P(x', y') P(y'ix')\nThese expressions are to be recognized as familiar mea sures of attribution that often appear in the literature. The r.h.s. of (33) is called \"risk-difference\" in epi demiology, and is also misnamed \"attributable risk\" [Hennekens and Buring, 1987, p. 87]. The probabil ity of necessity, PN, is given by the excess-risk-ratio (ERR)\nPN = P(ylx)- P(ylx') = 1 __ 1 (36) P(ylx) RR\n594 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\noften misnamed as the attributable fraction, attributable-rate percent, attributed fraction for the ex posed [Kelsey et al., 1996, p. 38], or attributable pro portion [Cole, 1997]. The reason we consider these la bels to be misnomers is that ERR invokes purely sta tistical relationships, hence it cannot in itself serve to measure attribution, unless fortified with some causal assumptions. Exogeneity and monotonicity are the causal assumptions that endow ERR with attribu tional interpretation, and these assumptions are rarely made explicit in the literature on attribution.\nThe expression for PS is likewise quite revealing\nPS = [P(yix) - P(yix')]/[1- P(yix')], (37)\nas it coincides with what epidemiologists call the \"rela tive difference\" [Shep, 1958], which is used to measure the susceptibility of a population to a risk factor x. It also coincides with what Cheng calls \"causal power\" (1997), namely, the effect of x on y after suppressing \"all other causes of y.\" See Pearl (1999) for additional discussions of these expressions.\nTo appreciate the difference between Eqs. (31) and (36) we can rewrite Eq. (31) as\nPN P(yix)P(x) + P(yix')P(x')- P(Yx') P(yix)P(x) P(yix)- P(yix')\n+ P(yix')- P(Yx' h8) P(yix) P(x, y)\nThe first term on the r.h.s. of (38) is the familiar ERR as in (36), and represents the value of PN un der exogeneity. The second term represents the cor rection needed to account for X's non-exogeneity, i.e. P(Yx') =I P(ylx'). We will call the r.h.s. of (38) by corrected excess-risk-ratio (CERR).\nFrom Eqs. (33)-(35) we see that the three notions of causation satisfy the simple relationships given by Eqs. (25) and (26) which we obtained under the strong exogeneity condition. In fact, we have the following theorem.\nTheorem 5 Monotonicity (27} and exogeneity (19} together imply strong exogeneity (24).\n3.6 Summary of results\nTable 1 lists the best estimate of PN under various assumptions and various types of data-the stronger the assumptions, the more informative the estimates. We see that the excess-risk-ratio (ERR), which epi demiologists commonly identify with the probability of causation, is a valid measure of PN only when two assumptions can be ascertained: exogeneity (i.e., no confounding) and monotonicity (i.e., no preven-\ntion). When monotonicity does not hold, ERR pro vides merely a lower bound for PN, as shown in Eq. (21). (The upper bound is usually unity.) In the presence of confounding, ERR must be corrected by the additive term [P(yix') - P(Yx' )]/ P(x, y), as stated in (38). In other words, when confounding bias (of the causal effect) is positive, PN is higher than ERR by the amount of this additive term. Clearly, owing to the division by P(x, y), the PN bias can be many times higher than the causal effect bias P(ylx') - P(Yx' ) . However, confounding results only from association between exposure and other factors that affect the outcome; one need not be concerned with associations between such factors and suscepti bility to exposure, as is often assumed in the literature (Khoury et al., 1989, Glymour, 1998]. The last two rows in Table 1 correspond to no as sumptions about exogeneity, and they yield vacuous bounds for PN when data come from either experi mental or observational study. In contrast, informa tive bounds (17) or point estimates (38) are obtained when data from experimental and observational stud ies are combined. Concrete use of such combination will be illustrated in Section 4.\n4 Example 1: Legal Responsibility\nA lawsuit is filed against the manufacturer of drug x, charging that the drug is likely to have caused the death of Mr. A, who took the drug to relieve symptom S associated with disease D. The manufacturer claims that experimental data on patients with symptom S show conclusively that drug x may cause only minor increase in death rates. The plaintiff argues, however, that the experimental study is of little relevance to this case, because it repre sents the effect of the drug on all patients, not on patients like Mr. A who actually died while using drug x. Moreover, argues the plaintiff, Mr. A is unique in that he used the drug on his own voli-\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 595\ntion, unlike subjects in the experimental study who took the drug to comply with experimental protocols. To support this argument, the plaintiff furnishes non experimental data indicating that most patients who chose drug x would have been alive if it were not for the drug. The manufacturer counter-argues by stat ing that: (1) counterfactual speculations regarding whether patients would or would not have died are purely metaphysical and should be avoided, and (2) nonexperimental data should be dismissed a priori, on the ground that such data may be highly biased; for example, incurable terminal patients might be more inclined to use drug x if it provides them greater symp tomatic relief. The court must now decide, based on both the experimental and non-experimental studies, what the probability is that drug x was in fact the cause of Mr. A's death.\nThe (hypothetical) data associated with the two stud ies are shown in Table 2. The experimental data pro vide the estimates\nP(yx) P(Yx') P(y�,)\n= 16/1000\n= 14/1000\n= 1- P(Yx')\n= 0.016\n= 0.014\n= 0.986\nThe non-experimental data provide the estimates\nP(y) P(x, y)\nP(x', y')\n=30/2000\n= 2/2000\n= 972/2000\n= 0.015\n= 0.001\n= 0.486\nSince both the experimental and nonexperimental data are available, we can obtain bounds on all three prob abilities of causation through Eqs. (15)-(18) without making any assumptions about the underlying mech anisms. The data in Table 2 imply the following nu merical results:\n0.002 ::; P N S ::; 0.016 1.0 ::; P N ::; 1.0\n0.002 ::; p s ::; 0.031\n(39)\n(40)\n( 41)\nThese figures show that although surviving patients who didn't take drug x have only less than 3.1% chance\nto die had they taken the drug, there is 100% assurance (barring sample errors) that those who took the drug and died would have survived had they not taken the drug. Thus the plaintiff was correct; drug x was in fact responsible for the death of Mr. A.\nIf we assume that drug x can only cause, but never prevent, death, Theorem 3 is applicable and Eqs. (30) (32) yield\nPNS PN PS\n0.002\n1.0\n0.002\n(42)\n( 43)\n(44)\nThus, we conclude that drug x was responsible for the death of Mr. A, with or without the no-prevention as sumption.\nNote that a straightforward use of the experimental excess-risk-ratio would yield a much lower (and incor rect) result:\nP(yx)- P(Yx') = 0.016-0.014 = 0_125 (45) P(yx) 0.016\nEvidently, what the experimental study does not re veal is that, given a choice, terminal patients stay away from drug x. Indeed, if there were any terminal pa tients who would choose x (given the choice), then the control group (x') would have included some such pa tients (due to randomization) and so the proportion of deaths among the control group P(Yx') would have been higher than P( x', y), the population proportion of terminal patients avoiding x. However, the equality P(Yx') = P(y, x') tells us that no such patients were present in the control group, hence (by randomization) no such patients exist in the population at large and therefore none of the patients who freely chose drug x was a terminal case; all were susceptible to x. The numbers in Table 2 were obviously contrived to show the usefulness of the bounds in Eqs. (15)-(18). Nevertheless, it is instructive to note that a combi nation of experimental and non-experimental studies may unravel what experimental studies alone will not reveal.\n5 Example 2: Personal Decision Making\nConsider the case of Mr. B, who is one of the surviving patients in the observational study of Table 2. Mr. B wonders how safe it would be for him to take drug x, given that he has refrained thus far from taking the drug and that he managed to survive the disease. His argument for switching to the drug rests on the observation that only 2 out of 1000 drug users died in\n596 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\nthe observational study, which he considers a rather small risk to take, given the effectiveness of the drug as a pain killer.\nConventional wisdom instructs us to warn Mr. B against consulting a nonexperimental study in matters of decisions, since such studies are marred with un controlled factors, which tend to bias effect estimates. Specifically, the death rate of 0.002 among drug users may be indicative of low tolerance to discomfort, or of membership in a medically-informed socio-economic group. Such factors do not apply to Mr. B, who did not use the drug in the past (be it by choice, instinct or ig norance), and who is now considering switching to the drug by rational deliberation. Conventional wisdom urges us to refer Mr. B to the randomized experimen tal study of Table 2, from which the death rate under controlled administration of the drug was evaluated to be P(yx) = 0.016, eight times higher than 0.002.\nWhat would his risk of death be, if Mr. B decides to start taking the drug? 0.2 percent or 1.6 percent?\nThe answer is that neither number is correct. Mr. B cannot be treated as a random patient in either study, because his history of not using the drug and his sur vival thus far puts him in a unique category of patients, for which the effect of the drug was not studied. 2 These two attributes provide extra evidence about Mr. B's sensitivity to the drug. This became clear already in Example 1, where we discovered definite re lationships among these attributes - for some obscure reasons, terminal patients chose not to use the drug.\nTo properly account for this additional evidence, the risk should be measured through the counterfactual expression P S = P(yx jx1, y1); the probability that a patient who survived with no drug would have died had he/she taken the drug. The appropriate bound for this probability is given in Eq. (41):\n0.002 ::; p s ::; 0.031\nThus, Mr. B's risk of death (upon switching to drug usage) can be as high as 3.1 percent; more than 15 times his intuitive estimate of 0.2 percent, and almost twice the naive estimate obtained from the experimen tal study.\nHowever, if the drug can safely be assumed to have no death-preventing effects, then monotonicity applies, and the appropriate bound is given by Eq. (44), PS = 0.002, which coincides with Mr. B's intuition.\n2The appropriate experimental design for measuring the risk of interest is to conduct a randomized clinical trial on patients in the category of Mr. B, that is, to subject a random sample of non-users to a period of drug treatment and measure their rate of survival.\n6 Conclusion\nThis paper shows how useful information about proba bilities of causation can be obtained from experimental and observational studies, with weak or no assump tions about the data-generating process. We have shown that, in general, bounds for the probabilities of causation can be obtained from combined experi mental and nonexperimental data. These bounds were proven to be sharp and, therefore, they represent the ultimate information that can be extracted from sta tistical methods. We clarify the two basic assumptions - exogeneity and monotonicity - that must be ascer tained before statistical measures such as excess-risk ratio could represent attributional quantities such as probability of causation.\nOne application of this analysis lies in the automatic generation of verbal explanations, where the distinc tion between necessary and sufficient causes has impor tant ramifications. As can be seen from the definitions and examples discussed in this paper, necessary cau sation is a concept tailored to a specific event under consideration (singular causation), whereas sufficient causation is based on the general tendency of certain event types to produce other event types. Adequate explanations should respect both aspects. Clearly, some balance must be made between the necessary and the sufficient components of causal explanation, and the present paper illuminates this balance by for mally explicating the basic relationships between the two components. In Pearl (2000, chapter 10) it is fur ther shown that PN and PS are too crude for cap turing probabilities of causation in multi-stage scenar ios, and that the structure of the intermediate pro cess leading from cause to effect must enter the defi nitions of causation and explanation. Such consider ations will be the subject of future investigation (See [Halpern and Pearl, 2000]).\nAnother important application of probabilities of cau sation is found in decision making problems. As was pointed out in Pearl (2000, pp. 217-219) and illustrated in Section 5, the counterfactual \"y would have been true if x were true\" can often be translated into a con ditional action claim \"given that currently x and y are false, y will be true if we do x.\" The evaluation of such conditional predictions, and the probabilities of such predictions, are commonplace in decision mak ing situations, where actions are brought into focus by certain eventualities that demand remedial correction. In troubleshooting, for example, we observe undesir able effects Y = y that are potentially caused by other conditions X = x and we wish to predict whether an action that brings about a change in X would rem edy the situation. The information provided by the\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 597\nevidence y and x is extremely valuable, and it must be processed before we can predict the effect of any action3. Thus, the expressions developed in this pa per constitute bounds on the effectiveness of pending policies, when full knowledge of the state of affairs is not available, yet the pre-action states of the decision variable (X) and the outcome variable (Y) are known. For these bounds to be valid in policy making, the data generating model must be time-invariant, that is, all probabilities associated with the model should rep resent epistemic uncertainty about static, albeit un known boundary conditions U = u. The constancy of U is well justified in the control and diagnosis of phys ical systems, where U represents fixed, but unknown physical characteristics of devices or subsystems. The constancy approximation is also justified in the health sciences where patients' genetic attributes and physi cal characteristics can be assumed relatively constant between observation and treatment. For instance, if a patient in the example of Section 5 wishes to assess the risk of switching from no drug to drug, it is reasonable to assume that this patient's susceptibility to the drug remains constant through the interim period of anal ysis. Therefore, the risk associated with this patient's decision will be well represented by the counterfactual expression PS = P(Yxlx',y'), and should be assessed by the bounds in Eq. (41).\nThe constancy assumption is less justified in economic systems, where agents are bombarded by rapidly fluc tuating streams of external forces ( \"shocks\" in econo metric terminology) and inter-agents stimuli. These forces and stimuli may vary substantially during the policy making interval and they require, therefore, de tailed time-dependent analysis. The canonical viola tion of the constancy assumption occurs, of course, in quantum mechanical systems, where the indeter minism is \"intrinsic\" and memory-less, and where the existence of a deterministic relationship between the boundary conditions and measured quantities is no longer a good approximation. A method of incorpo rating such intrinsic indeterminism into counterfactual analysis is outlined in Pearl (2000, p. 220).\nAcknowledgements\nWe thank the referees for making useful suggestions on the first draft. Full version of this paper will ap pear in the Annals of Mathematics and AI, 2000. This research was supported in parts by grants from NSF, ONR and AFOSR and by a Microsoft Fellowship to the first author.\n3Such processing have been applied indeed to the evaluation of economic poli cies [Balke and Pearl, 1995] and to repair-test strategies in troubleshooting [Breese and Heckerman, 1996]\nReferences\n[Bailey et al., 1994] L. A. Bailey, L. Gordis, and M. Green. Reference guide on epidemiology. Reference Manual on Scientific Evidence, 1994. Federal Judicial Center. Available online at http:/ jwww.fjc.gov /EVIDENCE/sciencejsc_ev _sec.html.\n[Balke and Pearl, 1994] A. Balke and J. Pearl. Proba bilistic evaluation of counterfactual queries. In Pro ceedings of the Twelfth National Conference on Arti ficial Intelligence, volume Volume I, pages 230-237. MIT Press, Menlo Park, CA, 1994.\n[Balke and Pearl, 1995] A. Balke and J. Pearl. Coun terfactuals and policy analysis in structural models. In P. Besnard and S. Hanks, editors, Uncertainty in Artificial Intelligence 11, pages 11-18. Morgan Kaufmann, San Francisco, 1995.\n[Balke, 1995] A. Balke. Probabilistic Counterfactuals: Semantics, Computation, and Applications. PhD thesis, Computer Science Department, University of California, Los Angeles, CA, November 1995.\n[Breese and Beckerman, 1996] J.S. Breese and D. Beckerman. Decision-theoretic troubleshoot ing: A framework for repair and experiment. In E. Horvitz and F. Jensen, editors, Proceedings of the Twelfth Conference on Uncertainty in Artificial Intelligence, pages 124-132. Morgan Kaufmann, San Francisco, CA, 1996.\n[Cheng, 1997] P.W. Cheng. From covariation to cau sation: A causal power theory. Psychological Re view, 104(2):367-405, 1997.\n[Cole, 1997] P. Cole. Causality in epidemiology, health policy, and law. Journal of Marketing Research, 27:10279-10285, 1997.\n[Galles and Pearl, 1997] D. Galles and J. Pearl. Ax ioms of causal relevance. Artificial Intelligence, 97(1-2):9-43, 1997.\n[Galles and Pearl, 1998] D. Galles and J. Pearl. An axiomatic characterization of causal counterfactu als. Foundations of Science, 3(1):151-182, 1998.\n[Glymour, 1998] C. Glymour. Psychological and nor mative theories of causal power and the probabilities of causes. In G.F. Cooper and S. Moral, editors, Un certainty in Artificial Intelligence, pages 166-172. Morgan Kaufmann, San Francisco, CA, 1998.\n[Halpern and Pearl, 2000] J. Y. Halpern and J. Pearl. Causes and explanations: A structural-model ap proach. Technical Report R-266, Cognitive System Laboratory, Department of Computer Science, Uni versity of California, Los Angeles, March, 2000.\n598 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\n[Halpern, 1998] J.Y. Halpern. Axiomatizing causal reasoning. In G.F. Cooper and S. Moral, editors, Uncertainty in Artificial Intelligence, pages 202- 210. Morgan Kaufmann, San Francisco, CA, 1998.\n[Hennekens and Buring, 1987] C.H. Hennekens and J.E. Buring. Epidemiology in Medicine. Brown, Lit tle, Boston, 1987.\n[Kelsey et al., 1996] J.L. Kelsey, A.S. Whittemore, A.S. Evans, and W.D. Thompson. Methods in Ob servational Epidemiology. Oxford University Press, New York, 1996.\n[Khoury et al., 1989] M.J. Khoury, W.D. F landers, S. Greenland, and M.J. Adams. On the mea surement of susceptibility in epidemiologic studies. American Journal of Epidemiology, 129(1):183-190, 1989.\n[Pearl, 1995] J. Pearl. Causal diagrams for experi mental research. Biometrika, 82:669-710, December 1995.\n[Pearl, 1999] J. Pearl. Probabilities of causation: three counterfactual interpretations and their iden tification. Synthese, 121(1-2):93-149, November 1999.\n[Pearl, 2000] J. Pearl. Causality: Models, Reasoning, and Inference. Cambridge University Press, NY, 2000.\n[Robins and Greenland, 1989] J .M. Robins and S. Greenland. The probability of causation under a stochastic model for individual risk. Biometrics, 45:1125-1138, 1989.\n[Robins, 1987] J.M. Robins. A graphical approach to the identification and estimation of causal pa rameters in mortality studies with sustained expo sure periods. Journal of Chronic Diseases, 40(Suppl 2):139S-161S, 1987.\n[Rosenbaum and Rubin, 1983] P. Rosenbaum and D. Rubin. The central role of propensity score in observational studies for causal effects. Biometrica, 70:41-55, 1983.\n[Shep, 1958] M.C. Shep. Shall we count the living or the dead? New England Journal of Medicine, 259:1210-1214, 1958."
    } ],
    "references" : [ {
      "title" : "Reference guide on epidemiology",
      "author" : [ "L.A. Bailey", "L. Gordis", "M. Green" ],
      "venue" : "Reference Manual on Scientific Evidence,",
      "citeRegEx" : "Bailey et al.. 1994",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Proba­ bilistic evaluation of counterfactual queries",
      "author" : [ "A. Balke", "J. Pearl" ],
      "venue" : "Pro­ ceedings of the Twelfth National Conference on Arti­ ficial Intelligence, volume Volume I, pages 230-237. MIT Press, Menlo Park, CA",
      "citeRegEx" : "Balke and Pearl. 1994",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Coun­ terfactuals and policy analysis in structural models",
      "author" : [ "A. Balke", "J. Pearl" ],
      "venue" : "P. Besnard and S. Hanks, editors, Uncertainty in Artificial Intelligence 11, pages 11-18. Morgan Kaufmann, San Francisco",
      "citeRegEx" : "Balke and Pearl. 1995",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Probabilistic Counterfactuals: Semantics",
      "author" : [ "A. Balke" ],
      "venue" : "Computation, and Applications. PhD thesis, Computer Science Department, University of California, Los Angeles, CA, November",
      "citeRegEx" : "Balke. 1995",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "From covariation to cau­ sation: A causal power theory",
      "author" : [ "P.W. Cheng" ],
      "venue" : "Psychological Re­ view, 104(2):367-405",
      "citeRegEx" : "Cheng. 1997",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Causality in epidemiology",
      "author" : [ "P. Cole" ],
      "venue" : "health policy, and law. Journal of Marketing Research, 27:10279-10285",
      "citeRegEx" : "Cole. 1997",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Ax­ ioms of causal relevance",
      "author" : [ "D. Galles", "J. Pearl" ],
      "venue" : "Artificial Intelligence, 97(1-2):9-43",
      "citeRegEx" : "Galles and Pearl. 1997",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "An axiomatic characterization of causal counterfactu­ als",
      "author" : [ "D. Galles", "J. Pearl" ],
      "venue" : "Foundations of Science, 3(1):151-182",
      "citeRegEx" : "Galles and Pearl. 1998",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Psychological and nor­ mative theories of causal power and the probabilities of causes",
      "author" : [ "C. Glymour" ],
      "venue" : "G.F. Cooper and S. Moral, editors, Un­ certainty in Artificial Intelligence, pages 166-172. Morgan Kaufmann, San Francisco, CA",
      "citeRegEx" : "Glymour. 1998",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Causes and explanations: A structural-model ap­ proach",
      "author" : [ "J.Y. Halpern", "J. Pearl" ],
      "venue" : "Technical Report R-266, Cognitive System Laboratory, Department of Computer Science, Uni­ versity of California, Los Angeles, March",
      "citeRegEx" : "Halpern and Pearl. 2000",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Axiomatizing causal reasoning",
      "author" : [ "J.Y. Halpern" ],
      "venue" : "G.F. Cooper and S. Moral, editors, Uncertainty in Artificial Intelligence, pages 202210. Morgan Kaufmann, San Francisco, CA",
      "citeRegEx" : "Halpern. 1998",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Epidemiology in Medicine",
      "author" : [ "C.H. Hennekens", "J.E. Buring" ],
      "venue" : "Brown, Lit­ tle, Boston",
      "citeRegEx" : "Hennekens and Buring. 1987",
      "shortCiteRegEx" : null,
      "year" : 1987
    }, {
      "title" : "Methods in Ob­ servational Epidemiology",
      "author" : [ "J.L. Kelsey", "A.S. Whittemore", "A.S. Evans", "W.D. Thompson" ],
      "venue" : "Oxford University Press, New York",
      "citeRegEx" : "Kelsey et al.. 1996",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "On the mea­ surement of susceptibility in epidemiologic studies",
      "author" : [ "M.J. Khoury", "W.D. F landers", "S. Greenland", "M.J. Adams" ],
      "venue" : "American Journal of Epidemiology, 129(1):183-190,",
      "citeRegEx" : "Khoury et al.. 1989",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Causal diagrams for experi­ mental research",
      "author" : [ "J. Pearl" ],
      "venue" : "Biometrika, 82:669-710, December",
      "citeRegEx" : "Pearl. 1995",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Probabilities of causation: three counterfactual interpretations and their iden­ tification",
      "author" : [ "J. Pearl" ],
      "venue" : "Synthese, 121(1-2):93-149, November",
      "citeRegEx" : "Pearl. 1999",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Causality: Models",
      "author" : [ "J. Pearl" ],
      "venue" : "Reasoning, and Inference. Cambridge University Press, NY",
      "citeRegEx" : "Pearl. 2000",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Biometrics",
      "author" : [ "J .M. Robins", "S. Greenland. The probability of causation under a stochastic model for individual risk" ],
      "venue" : "45:1125-1138,",
      "citeRegEx" : "Robins and Greenland. 1989",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "A graphical approach to the identification and estimation of causal pa­ rameters in mortality studies with sustained expo­ sure periods",
      "author" : [ "J.M. Robins" ],
      "venue" : "Journal of Chronic Diseases, 40(Suppl 2):139S-161S",
      "citeRegEx" : "Robins. 1987",
      "shortCiteRegEx" : null,
      "year" : 1987
    }, {
      "title" : "The central role of propensity score in observational studies for causal effects",
      "author" : [ "P. Rosenbaum", "D. Rubin" ],
      "venue" : "Biometrica, 70:41-55",
      "citeRegEx" : "Rosenbaum and Rubin. 1983",
      "shortCiteRegEx" : null,
      "year" : 1983
    }, {
      "title" : "Shall we count the living or the dead? New England Journal of Medicine",
      "author" : [ "M.C. Shep" ],
      "venue" : "259:1210-1214",
      "citeRegEx" : "Shep. 1958",
      "shortCiteRegEx" : null,
      "year" : 1958
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "[Pearl, 1999] gave coun­ terfactual definitions for the probabilities of neces­ sary or sufficient causation (or both) based on struc­ tural model semantics, which defines counterfactuals as quantities derived from modifiable sets of func­ tions [Galles and Pearl, 1997, Galles and Pearl, 1998, Halpern, 1998, Pearl, 2000, chapter 7].",
      "startOffset" : 0,
      "endOffset" : 13
    }, {
      "referenceID" : 15,
      "context" : "The analysis of this paper extends the results reported in [Pearl, 1999] [Pearl, 2000, pp.",
      "startOffset" : 59,
      "endOffset" : 72
    }, {
      "referenceID" : 15,
      "context" : "In this section, we present the definitions for the three aspects of causation as defined in [Pearl, 1999].",
      "startOffset" : 93,
      "endOffset" : 106
    }, {
      "referenceID" : 18,
      "context" : "One property that the counterfactual relationships sat­ isfy is the consistency condition [Robins, 1987]:",
      "startOffset" : 90,
      "endOffset" : 104
    }, {
      "referenceID" : 13,
      "context" : "A policy maker may well be interested in the dangers that a certain exposure may present to the healthy population [Khoury et al., 1989].",
      "startOffset" : 115,
      "endOffset" : 136
    }, {
      "referenceID" : 8,
      "context" : "\" In psychology, PS serves as the basis for Cheng's (1997) causal power the­ ory [Glymour, 1998], which attempts to explain how humans judge causal strength among events.",
      "startOffset" : 81,
      "endOffset" : 96
    }, {
      "referenceID" : 15,
      "context" : "Lemma 1 The probabilities of causation satisfy the following relationship [Pearl, 1999] :",
      "startOffset" : 74,
      "endOffset" : 87
    }, {
      "referenceID" : 14,
      "context" : ", nonexperimental) studies which permit the control of confounding through adjustment of covariates [Pearl, 1995].",
      "startOffset" : 100,
      "endOffset" : 113
    }, {
      "referenceID" : 17,
      "context" : "(19) is also known as \"no-confounding\" [Robins and Greenland, 1989], \"as if randomized,\" or \"weak ignorability\" [Rosenbaum and Rubin, 1983].",
      "startOffset" : 39,
      "endOffset" : 67
    }, {
      "referenceID" : 19,
      "context" : "(19) is also known as \"no-confounding\" [Robins and Greenland, 1989], \"as if randomized,\" or \"weak ignorability\" [Rosenbaum and Rubin, 1983].",
      "startOffset" : 112,
      "endOffset" : 139
    }, {
      "referenceID" : 0,
      "context" : "Courts have often used the condi­ tion RR > 2 as a criterion for legal responsibility [Bailey et al., 1994].",
      "startOffset" : 86,
      "endOffset" : 107
    }, {
      "referenceID" : 15,
      "context" : "{20}-{22), and, moreover, PN, PS, and PNS are related to each other as follows [Pearl, 1999]:",
      "startOffset" : 79,
      "endOffset" : 92
    }, {
      "referenceID" : 15,
      "context" : "[Pearl, 1999] derived Eqs.",
      "startOffset" : 0,
      "endOffset" : 13
    }, {
      "referenceID" : 14,
      "context" : ", assumptions that permits identification through adjustment of co­ variates) [Pearl, 1995].",
      "startOffset" : 78,
      "endOffset" : 91
    }, {
      "referenceID" : 5,
      "context" : "38], or attributable pro­ portion [Cole, 1997].",
      "startOffset" : 34,
      "endOffset" : 46
    }, {
      "referenceID" : 20,
      "context" : "as it coincides with what epidemiologists call the \"rela­ tive difference\" [Shep, 1958], which is used to measure the susceptibility of a population to a risk factor x.",
      "startOffset" : 75,
      "endOffset" : 87
    }, {
      "referenceID" : 9,
      "context" : "Such consider­ ations will be the subject of future investigation (See [Halpern and Pearl, 2000]).",
      "startOffset" : 71,
      "endOffset" : 96
    } ],
    "year" : 2011,
    "abstractText" : "This paper deals with the problem of esti­ mating the probability that one event was a cause of another in a given scenario. Us­ ing structural-semantical definitions of the probabilities of necessary or sufficient cau­ sation (or both), we show how to optimally bound these quantities from data obtained in experimental and observational studies, making minimal assumptions concerning the data-generating process. In particular, we strengthen the results of Pearl (1999) by weakening the data-generation assumptions and deriving theoretically sharp bounds on the probabilities of causation. These results delineate precisely how empirical data can be used both in settling questions of attribution and in solving attribution-related problems of decision making.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}