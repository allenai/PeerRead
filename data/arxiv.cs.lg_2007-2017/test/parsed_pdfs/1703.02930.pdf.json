{
  "name" : "1703.02930.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Nearly-tight VC-dimension bounds for piecewise linear neural networks",
    "authors" : [ "Nick Harvey", "Chris Liaw", "Abbas Mehrabian" ],
    "emails" : [ "nickhar@cs.ubc.ca", "cvliaw@cs.ubc.ca", "abbasmehrabian@gmail.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Deep neural networks underlie many of the recent breakthroughs of applied machine learning, particularly in image and speech recognition. These successes motivate a renewed study of these networks’ theoretical properties.\nClassification is one of the learning tasks in which deep neural networks have been particularly successful, e.g., for image recognition. A natural foundational question that arises is: what are the theoretical limits on the classification power of these networks? The established way to formalize this question is by considering VCdimension, as it is well known that this asymptotically determines the sample complexity of PAC learning with such classifiers [3].\nIn this paper, we prove nearly-tight bounds on the VC-dimension of deep neural networks in which the non-linear activation function is a piecewise linear function with a constant number of pieces. For simplicity we will henceforth refer to such networks as “piecewise linear networks”. The most common activation function used in practice is, by far, the rectified linear unit, also known as ReLU [7, 8]. The ReLU function is defined as σ(x) = max{0, x}, so it is clearly piecewise linear. ∗University of British Columbia. Email: nickhar@cs.ubc.ca †University of British Columbia. Email: cvliaw@cs.ubc.ca ‡University of British Columbia. Email: abbasmehrabian@gmail.com\nar X\niv :1\n70 3.\n02 93\n0v 1\n[ cs\n.L G\n] 8\nM ar\nIt is particularly interesting to consider how the VC-dimension is affected by the various attributes of the network: the number W of parameters (i.e., weights and biases), the number U of non-linear units (i.e., nodes), and the number L of layers. Among all networks with the same size (number of weights), is it true that those with more layers have more classification power (i.e., larger VC-dimension)?\nSuch a statement is indeed true, and previously known, thereby providing some justification for the advantages of deep neural networks. However, a tight characterization of how depth affects VC-dimension was unknown prior to this work.\nOur results. Our first main result is a new VC-dimension lower bound that holds even for the restricted family of ReLU networks.\nTheorem 1.1 (Main lower bound). There exists a universal constant C such that the following holds. Given any W,L with W > CL > C2, there exists a ReLU network with ≤ L layers and ≤W parameters with VC-dimension ≥WL log(W/L)/C.\nThe proof appears in Section 3. Prior to our work, the best known lower bounds were Ω(WL) [2, Theorem 2] and Ω(W logW ) [10, Theorem 1]; we strictly improve both bounds to Ω(WL log(W/L)).\nOur proof of Theorem 1.1 uses the “bit extraction” technique, which was also used in [2] to give an Ω(WL) lower bound. We refine this technique to gain the additional logarithmic factor that appears in Theorem 1.1.\nUnfortunately there is a barrier to refining this technique any further. Our next theorem shows the hardness of computing the mod function, implying that the bit extraction technique cannot yield a stronger lower bound than Theorem 1.1. Further discussion of this connection may be found in Remark 3.\nTheorem 1.2. Assume there exists a piecewise linear network with W parameters and L layers that computes a function f : R → R, with the property that |f(x) − (xmod 2)| < 1/2 for all x ∈ {0, 1, . . . , 2m−1}. Then we havem = O(L log(W/L)).\nThe proof of this theorem appears in Section 4. One interesting aspect of the proof is that it does not use Warren’s lemma [13], which is a mainstay of VC-dimension upper bounds [6, 2, 1].\nOur next main result is an upper bound on the VC-dimension of neural networks with any piecewise linear activation function with a constant number of pieces. Recall that ReLU is an example of a piecewise linear activation function.\nTheorem 1.3 (Main upper bound). Consider a piecewise linear neural network withW parameters arranged in L layers. Let F be the set of (real-valued) functions computed by this network. Then VCDim(sgn(F)) = O(WL logW ).\nThe proof of this result appears in Section 5. Prior to our work, the best published upper bounds wereO(W 2) [6, Section 3.1] andO(WL logW+WL2) [2, Theorem 1], both of which hold for piecewise polynomial activation functions; we strictly improve both bounds to O(WL logW ) for the special case of piecewise linear functions.\nTo compare our upper and lower bounds, let d(W,L) denote the largest VC-dimension of a piecewise linear network with W parameters and L layers. Theorems 1.1 and 1.3 imply there exist constants c, C such that\nc ·WL log(W/L) ≤ d(W,L) ≤ C ·WL logW . (1)\nFor neural networks arising in practice it would certainly be the case that L is significantly smaller than W 0.99, in which case our results determine the asymptotic bound d(W,L) = Θ(WL logW ). On the other hand, in the regime L = Θ(W ), which is merely of theoretical interest, we also now have a tight bound d(W,L) = Θ(WL), obtained by combining Theorem 1.1 with results of [6]. There is now only a very narrow regime, sayW 0.99 L W , in which the bounds of (1) are not asymptotically tight, and they differ only in the logarithmic factor.\nOur final result is a upper bound for VC-dimension in terms of W and U (the number of non-linear units, or nodes). This bound is tight in the case d = 1, as discussed in Remark 3.\nTheorem 1.4. Consider a neural network with W parameters and U units with activation functions that are piecewise polynomials of degree at most d. Let F be the set of (real-valued) functions computed by this network. Then VCDim(sgn(F)) = O(WU log(d+ 1)).\nThe proof of this result appears in Section 6. The best known upper bound before our work was O(W 2) (implicitly proven in [6, Section 3.1], for constant d). Our theorem improves this to the tight result O(WU).\nRelated Work. Recently there have been several theoretical papers that establish the power of depth in neural networks. Last year, two striking papers considered the problem of approximating a deep neural network with a shallower network. [12] shows that there is a ReLU network with L layers and U = Θ(L) units such that any network approximating it with only O(L1/3) layers must have Ω(2L 1/3\n) units; this phenomenon holds even for real-valued functions. [5] show an analogous result for a high-dimensional 3-layer network that cannot be approximated by a 2-layer network except with an exponential blow-up in the number of nodes.\nVery recently, several authors have shown that deep neural networks are capable of approximating broad classes of functions. [11] show that a sufficiently non-linear C2 function on [0, 1]d can be approximated with error in L2 by a ReLU network with O(polylog(1/ )) layers and weights, but any such approximation with O(1) layers requires Ω(1/ ) weights. [14] shows that any Cn-function on [0, 1]d can be approximated with error in L∞ by a ReLU network with O(log(1/ )) layers and O(( 1 )\nd/n log(1/ )) weights. [9] show that a sufficiently smooth univariate function can be approximated with error in L∞ by a network with ReLU and threshold gates with Θ(log(1/ )) layers and O(polylog(1/ )) weights, but that Ω(poly(1/ )) weights would be required if there were only o(log(1/ )) layers; they also prove analogous results for multivariate functions. Lastly, [4] draw a connection to tensor factorizations to show that, for non-ReLU networks, the set of functions computable by a shallow network have measure zero among those computable by a deep networks."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "A neural network is defined by an activation function ψ : R → R, a directed acyclic graph, a weight for each edge of the graph, and a bias for each node of the graph. Let W denote the number of parameters (weights and biases) of the network, U denote the number of computation units (nodes), and L denote the number of layers.\nThe nodes at layer 0 are called input nodes, and simply output the real value given by the corresponding input to the network. For the purposes of this paper, we will assume that the graph has a single sink node, which is the unique node at layer L, (the output layer). In the jargon of neural networks, layers 1 through L−1 are called hidden layers.\nThe computation of a neural network proceeds as follows. For i = 1, . . . , L, the input into a computation unit u at layer i is w>x where x is the (real) vector corresponding to the output of the computational units with a directed edge to u and w is the corresponding edge weights. For layers 1, . . . , L − 1, the output of u is ψ(w>x + b) where b is the bias parameter associated with u. For the output layer, we replace ψ with the identity. Since we consider VC-dimension, we will always take the sign of the output of the network, to make the output lie in {0, 1} for binary classification. (Here, we define the sign function as sgn(x) = 1[x > 0].)\nA piecewise polynomial function with p pieces is a function f for which there exists disjoint intervals (pieces) I1, . . . , Ip and polynomials f1, . . . , fp such that if x ∈ Ii then f(x) = fi(x). We assume that p is a constant independent ofW , U andL. A piecewise linear function is a piecewise polynomial function in which each fi is linear. The most common activation function used in practice is the rectified linear unit (ReLU) where I1 = (−∞, 0], I2 = (0,∞) and f1(x) = 0, f2(x) = x. We denote this function by σ(x) := max{0, x}. The set {1, 2, . . . , n} is denoted [n]."
    }, {
      "heading" : "3 Proof of Theorem 1.1",
      "text" : "The proof of our main lower bound uses the “bit extraction” technique that was developed by [2] to prove a Ω(WL) lower bound. We refine their technique in a key way — we partition the input bits into blocks and extract multiple bits at a time instead of a single bit at a time. This yields a more efficient bit extraction network, and hence a stronger VC-dimension lower bound.\nWe show the following result, which immediately implies Theorem 1.1.\nTheorem 3.1. Let r,m, n be positive integers, and let k = dm/re. There exists a ReLU network with 3 + 5k layers, 2 + n+ 4m+ k((11 + r)2r + 2r+ 2) parameters, and m+ 2 + k(5× 2r + r + 1) computation units with VC-dimension ≥ mn.\nRemark. Choosing r = 1 gives a network with W = O(m + n), U = O(m) and VC-dimension Ω(mn) = Ω(WU). This implies that the upper bound O(WU) given in Theorem 1.4 is tight.\nTo prove Theorem 1.1, apply Theorem 3.1 with m = rL/8, r = log2(W/L)/2, and n = W − 5m2r. In the rest of this section we prove Theorem 3.1.\nLet Sn ⊆ Rn denote the standard basis. We shatter the set Sn × Sm. Given an arbitrary function f : Sn × Sm → {0, 1}, we build a ReLU neural network that inputs (x1, x2) ∈ Sn × Sm and outputs f(x1, x2). Define n numbers a1, a2, . . . , an ∈ { 02m , 1 2m , . . . , 2m−1 2m } so that the ith digit of the binary representation of aj equals f(ej , ei). These numbers will be used as the parameters of the network, as described below.\nGiven input (x1, x2) ∈ Sn × Sm, assume that x1 = ei and x2 = ej . The network must output the ith bit of aj . This “bit extraction approach” was used in [2, Theorem 2] to give an Ω(WL) lower bound for the VC-dimension. We use a similar approach but we introduce a novel idea: we split the bit extraction into blocks and extract r bits at a time instead of a single bit at a time. This allows us to prove a lower bound of Ω(WL log(W/L)). One can ask, naturally, whether this approach can be pushed further. Our Theorem 1.2 implies that the bit extraction approach cannot give a lower bound better than Ω(WL log(W/L)) (see Remark 3).\nThe first layer of the network “selects” aj , and the remaining layers “extract” the ith bit of aj . In the first layer we have a single computational unit that calculates\naj = (a1, . . . , an) >x1 = σ ( (a1, . . . , an) >x1 ) .\nThis part uses 1 layer, 1 computation unit, and 1 + n parameters. The rest of the network extracts all bits of aj and outputs the ith bit. The extraction is done in k steps, where in each step we extract the r most significant bits and zero them out. We will use the following building block for extracting r bits.\nLemma 3.2. Suppose positive integers r andm are given. There exists a ReLU network with 5 layers, 5×2r+r+1 units and 11×2r+r2r+2r+2 parameters that given the real number b = 0.b1b2 . . . bm (in binary representation) as input, outputs the (r+ 1)- dimensional vector (b1, b2, . . . , br, 0.br+1br+2 . . . bm).\nFigure 1 shows a schematic of the ReLU network in the above lemma.\nProof. Partition [0, 1) into 2r even subintervals. Observe that the values of b1, . . . , br are determined by knowing which such subinterval b lies in. We first show how to design a two-layer ReLU network that computes the indicator function for an interval to any arbitrary precision. Using 2r of these networks in parallel allows us to determine which subinterval b lies in and hence, determine the bits b1, . . . , br.\nFor any a ≤ b and ε > 0, observe that the function f(x) := σ(1−σ(a/ε−x/ε))+ σ(1− σ(x/ε− b/ε))− 1 has the property that, f(x) = 1 for x ∈ [a, b], and f(x) = 0 for x /∈ (a − ε, b + ε), and f(x) ∈ [0, 1] for all x. Thus we can use f to approximate the indicator function for [a, b], to any desired precision. We will choose ε = 2−m−2 because we are working with m-digit numbers. Thus, the values b1, . . . , br can be generated by adding the corresponding indi-\ncator variables. (For instance, b1 = ∑2r−1 k=2r−1 1[b ∈ [k · 2−r, (k + 1) · 2−r)].) Fi-\nnally, the remainder 0.br+1br+2 . . . bm can be computed as 0.br+1br+2 . . . bm = 2rb−∑r k=1 2\nr−kbk. Now we count the number of layers and parameters: we use 2r small networks that work in parallel for producing the indicators, each has 3 layers, 5 units and 11 parameters. To produce b1, . . . , br we need an additional layer, r× (2r + 1) additional parameters, and r additional units. For producing the remainder we need 1 more layer, 1 more unit, and r + 2 more parameters.\nWe use dm/re of these blocks to extract the bits of aj , denoted by aj,1, . . . , aj,m. Extracting aj,i is now easy, noting that if x, y ∈ {0, 1} then x∧ y = σ(x+ y− 1). So, since x2 = ei, we have\naj,i = m∑ t=1 x2,t ∧ aj,t = m∑ t=1 σ(x2,t + aj,t − 1) = σ ( m∑ t=1 σ(x2,t + aj,t − 1) ) .\nThis calculation needs 2 layers, 1 +m units, and 1 + 4m parameters. Remark. Theorem 1.2 implies an inherent barrier to proving lower bounds using the “bit extraction” approach of [2]. Recall that this technique uses n binary numbers with m bits to encode a function f : Sn×Sm → {0, 1} to show an Ω(mn) lower bound for VC-dimension, where Sk denotes the set of standard basis vectors in Rk. The network begins by selecting one of the n binary numbers, and then extracting a particular bit of that number. [2] showed it is possible to take m = Ω(L) and n = Ω(W ), thus proving a lower bound of Ω(WL) for the VC-dimension. In Theorem 1.1 we showed we can increase m to Ω(L log(W/L)), improving the lower bound to Ω(WL log(W/L)). Theorem 1.2 implies that to extract just the least significant bit, one is forced to have m = O(L log(W/L)); on the other hand, we always have n ≤ W . Hence there is no way to improve the VC-dimension lower bound by more than a constant via the bit extraction technique. In particular, closing the gap for general piecewise polynomial networks will require a different technique."
    }, {
      "heading" : "4 Proof of Theorem 1.2",
      "text" : "For a piecewise polynomial function R → R, breakpoints are the boundaries between the pieces. So if the function has p pieces, it has p− 1 breakpoints.\nLemma 4.1. Let f1, . . . , fk : R → R be piecewise polynomial of degree D, and suppose the union of their breakpoints has size B. Let ψ : R → R be piecewise polynomial of degree d with b breakpoints. Let w1, . . . , wk ∈ R be arbitrary. The function g(x) := ψ( ∑ i wifi(x)) is piecewise polynomial of degree Dd with at most (B + 1)(2 + bD)− 1 breakpoints. Proof. Without loss of generality, assume that w1 = · · · = wk = 1. The function∑ i fi has B+ 1 pieces. Consider one such interval I. We will prove that it will create\nat most 2 + bD pieces in g. In fact, if ∑ i fi is constant on I, g will have 1 piece on I.\nOtherwise, for any point y, the equation ∑ i fi(x) = y has at most D solutions on I.\nLet y1, . . . , yb be the breakpoints ofψ. Suppose we move along the curve (x, ∑ i fi(x)) on I. Whenever we hit a point (t, yi) for some t, one new piece is created in g. So at most bD new pieces are created. In addition, we may have two pieces for the beginning and ending of I. This gives a total of 2 + bD pieces per interval, as required. Finally, note that the number of breakpoints is one fewer than the number of pieces.\nTheorem 4.2. Assume there exists a neural network with W parameters and L layers that computes a function f : R→ R, with the property that |f(x)− (x mod 2)| < 1/2 for all x ∈ {0, 1, . . . , 2m − 1}. Also suppose the activation functions are piecewise polynomial of degree at most d ≥ 1 in each piece, and have at most p ≥ 1 pieces. Then we have\nm ≤ L log2(13pd(L+1)/2 ·W/L).\nIn the special case of ReLU functions, this gives m = O(L log(W/L)).\nProof. For a node v of the network, let f(v) count the number of directed paths from the input node to v. Applying Lemma 4.1 iteratively gives that for a node v at layer i ≥ 1, the number of breakpoints can be bounded by (6p)idi(i−1)/2f(v) − 1. Let o denote the output node. Hence, o has at most (6p)LdL(L−1)/2f(o) pieces. The output of node o is piecewise polynomial of degree at most dL. On the other hand, as we increase x from 0 to 2m − 1, the function x mod 2 flips 2m − 1 many times, which implies the output of o becomes equal to 1/2 at least 2m − 1 times, thus we get\n(6p)LdL(L−1)/2f(o)× dL ≥ 2m − 1. (2)\nLet us now relate f(o) with W and L. Suppose that, for i ∈ [L], there are Wi edges between layer i and previous layers. By the AM-GM inequality,\nf(o) ≤ ∏ i (1 +Wi) ≤ (∑ i 1 +Wi L )L ≤ (2W/L)L. (3)\nCombining Eqs. (2) and (3) gives the theorem.\n[12] showed how to construct a function f which satisfies f(x) = (x mod 2) for x ∈ {0, 1, . . . , 2m−1} using a neural network withO(m) layers andO(m) parameters. By choosing m = k3, Telgarsky showed that any function g computable by a neural network with Θ(k) layers and O(2k) nodes must necessarily have ‖f − g‖1 > c for some constant c > 0.\nOur theorem above implies a similar statement. In particular, if we choose m = k1+ε then for any function g computable by a neural network with Θ(k) layers and O(2k ε\n) parameters, there must exist x ∈ {0, 1, . . . , 2m− 1} such that |f(x)− g(x)| > 1/2."
    }, {
      "heading" : "5 Proof of Theorem 1.3",
      "text" : "The proof of this theorem is very similar to the proof of the upper bound for piecewise polynomial networks in [2, Theorem 1] but optimized for piecewise linear networks. Our proof requires the following lemma, which is a slight improvement of a result in [13].\nLemma 5.1 (Theorem 8.3 in [1], Lemma 1 in [2]). Let p1, . . . , pm be polynomials of degree at most d in n ≤ m variables. Define\nK := |{(sgn(p1(x)), . . . , sgn(pm(x)) : x ∈ Rn}|,\ni.e. K is the number of possible sign vectors given by the polynomials. Then K ≤ 2(2emd/n)n.\nof Theorem 1.3. Suppose the activation function has p pieces, and suppose, for simplicity, that the pieces are (−∞, t1], (t1, t2], . . . , (tp−1,+∞). (It is straightforward to generalize to pieces of arbitrary form.) For i ∈ [L], let Wi denote the total number of parameters up to layer i, and let ki denote the number of computational units at layer i. Note that WL = W , and the total number of computational units is k = ∑L i=1 ki. Let m denote the VC-dimension of the network, let a be the number of input nodes, and let {x1, . . . , xm} ⊂ Ra be a shattered set. (Note that each xi is a real vector in Ra.) If m ≤ W the theorem’s conclusion already holds, so we may assume that m > W . Define\nK := |{(sgn(f(x1, w)), . . . , sgn(f(xm, w))) : w ∈ RW }|.\nIn other words, K is the number of sign patterns that the neural network can output for the sequence of inputs (x1, . . . , xm). By definition of shattering, we haveK = 2m. We will prove geometric upper bounds for K, which will imply upper bounds for m. For the rest of the proof, x1, . . . , xm are fixed, and we view the parameters of the network, denoted w, as a collection of W real variables.\nTo bound K, we will find a partition P of RW such that, for all P ∈ P , the functions f(x1, ·), . . . , f(xm, ·) are polynomials on P . Clearly,\nK ≤ ∑ P∈P |{(sgn(f(x1, w)), . . . , sgn(f(xm, w))) : w ∈ P}|. (4)\nWe will define a sequence of partitions P1,P2, . . . ,PL = P inductively, such that Pi is a refinement of Pi−1. Starting from layer 1, for ` ∈ [m] and j ∈ [k1], let h1,j(x`, w) be the input into the jth computation unit in the first layer and let P1 be a partition of RW such that the vector (sgn(h1,j(x`, ·)− ts))j∈[k1],`∈[m],s∈[p−1] is constant on each P ∈ P1. Since each h1,j(x`, ·) − ts is a polynomial of degree 1, by Lemma 5.1, we\ncan take |P1| ≤ 2(2ek1mp/W1)W1 . Observe that on each P ∈ P1, the output of each computation unit in the first hidden layer is either a polynomial of degree 1 or the zero function.\nNow, suppose the partitions P1, . . . ,Pi−1 have been defined. Let hi,j(x`, w) be the input into the jth computation unit in the ith layer and assume that on any fixed P ∈ Pi−1, the function hi,j(x`, ·) is a polynomial of degree at most i. By Lemma 5.1, there exists a partition PP,i of P with |PP,i| ≤ 2(2ekimip/Wi)Wi such that on each P ′ ∈ PP,i, the vector (sgn(hi,j(x`, ·)− ts))j∈[ki],`∈[m],s∈[p−1] is constant. We define Pi := ∪P∈Pi−1PP,i; which is a partition of RW such that the vector (sgn(hi,j(x`, ·)− ts))j∈[ki],`∈[m],s∈[p−1] is constant on each P ∈ Pi. Moreover, for any 1 < i ≤ L, we have\n|Pi| ≤ 2 ( 2ekimip\nWi\n)Wi |Pi−1|.\nRecall that the last (output) layer has a single unit. Let PL be the partition of RW defined as above, such that on each P ∈ PL, the output of the network is a polynomial of degree at most L in W variables. By Lemma 5.1, each term of the sum in (4) is at most 2(2emL/W )W . Moreover, by our inductive construction, and since ki ≤ k, Wi ≥ 1, ∑L i=1Wi ≤WL, and i ≤ L, we have\n|PL| ≤ 2L L∏ i=1 ( 2ekimip Wi )Wi ≤ 2L(2ekmLp)WL.\nHence, 2m = K ≤ 2(2emL/W )W × |PL| ≤ ( 2(2ekmLp)W )L+1 .\nTaking logarithms and using the crude bounds k, L ≤W gives the condition\nm ≤ (L+ 1) +W (L+ 1) log2(2eW 2mp) ≤ 4W (L+ 1) log2(2eWmp)\nwhich implies m = O(WL log(pW )), as required."
    }, {
      "heading" : "6 Proof of Theorem 1.4",
      "text" : "The idea of the proof is that the sign of the output of a neural network can be expressed as a Boolean formula where each predicate is a polynomial inequality. For example, consider the following toy network, where the activation function of the hidden units is a ReLU.\nThe sign of the output of the network is sgn(y) = sgn(w3σ(w1x) + w4σ(w2x)). Define the following Boolean predicates: p1 = (w1x > 0), p2 = (w2x > 0), q1 =\n(w3w1x > 0), q2 = (w4w2x > 0), and q3 = (w3w1x + w4w2x > 0). Then, we can write\nsgn(y) = (¬p1 ∧ ¬p2 ∧ 0) ∨ (p1 ∧ ¬p2 ∧ q1) ∨ (¬p1 ∧ p2 ∧ q2) ∨ (p1 ∧ p2 ∧ q3).\nA theorem of Goldberg and Jerrum states that any class of functions that can be expressed using a relatively small number of distinct polynomial inequalities has small VC-dimension.\nTheorem 6.1 (Theorem 2.2 of [6]). Let k, n ∈ N and f : Rn × Rk → {0, 1} be a function that can be expressed as a Boolean formula containing s distinct atomic predicates where each atomic predicate is a polynomial inequality or equality in k+n variables of degree at most d. Let F = {f(·, w) : w ∈ Rk}. Then VCDim(F) ≤ 2k log2(8eds).\nof Theorem 1.4. Consider a neural network with W weights and U computation units, and assume that the activation function ψ is piecewise polynomial of degree at most d with p pieces. To apply Theorem 6.1, we will express the sign of the output of the network as a boolean function consisting of less than 2(1+p)U atomic predicates, each being a polynomial inequality of degree at most max{U + 1, 2dU}.\nSince the neural network graph is acyclic, it can be topologically sorted. For i ∈ [U ], let ui denote the ith computation unit in the topological ordering. The input to each computation unit u lies in one of the p pieces of ψ. For i ∈ [U ] and j ∈ [p], we say “ui is in state j” if the input to ui lies in the jth piece.\nFor u1 and any j, the predicate “u1 is in state j” is a single atomic predicate which is the quadratic inequality indicating whether its input lies in the corresponding interval. So, the state of u1 can be expressed as a function of p atomic predicates. Conditioned on u1 being in a certain state, the state of u2 can be determined using p atomic predicates, which are polynomial inequalities of degree at most 2d + 1. Consequently, the state of u2 can be determined using p+ p2 atomic predicates, each of which is a polynomial of degree at most 2d + 1. Continuing similarly, we obtain that for each i, the state of ui can be determined using p(1 + p)i−1 atomic predicates, each of which is a polynomial of degree at most di−1 + ∑i−1 j=0 d\nj . Consequently, the state of all nodes can be determined using less than (1 + p)U atomic predicates, each of which is a polynomial of degree at most dU−1 + ∑U−1 j=0 d\nj ≤ max{U + 1, 2dU} (the output unit is linear). Conditioned on all nodes being in certain states, the sign of the output can be determined using one more atomic predicate, which is a polynomial inequality of degree at most max{U + 1, 2dU}.\nIn total, we have less than 2(1 + p)U atomic polynomial-inequality predicates and each polynomial has degree at most max{U + 1, 2dU}. Thus, by Theorem 6.1, we get an upper bound of 2W log(16e ·max{U+1, 2dU}·(1+p)U ) = O(WU log((1+d)p)) for the VC-dimension."
    } ],
    "references" : [ {
      "title" : "Neural network learning: theoretical foundations",
      "author" : [ "Martin Anthony", "Peter Bartlett" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1999
    }, {
      "title" : "Almost linear VC-dimension bounds for piecewise polynomial networks",
      "author" : [ "Peter Bartlett", "Vitaly Maiorov", "Ron Meir" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1998
    }, {
      "title" : "Learnability and the Vapnik-Chervonenkis dimension",
      "author" : [ "A. Blumer", "A. Ehrenfeucht", "D. Haussler", "M. Warmuth" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1989
    }, {
      "title" : "On the expressive power of deep learning: A tensor analysis",
      "author" : [ "N. Cohen", "O. Sharir", "A. Shashua" ],
      "venue" : "In COLT,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2016
    }, {
      "title" : "The power of depth for feedforward neural networks",
      "author" : [ "Ronen Eldan", "Ohad Shamir" ],
      "venue" : "In COLT,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2016
    }, {
      "title" : "Bounding the Vapnik-Chervonenkis dimension of concept classes parameterized by real numbers",
      "author" : [ "Paul W. Goldberg", "Mark R. Jerrum" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1995
    }, {
      "title" : "Deep Learning",
      "author" : [ "Ian Goodfellow", "Yoshua Bengio", "Aaron Courville" ],
      "venue" : "http://www.deeplearningbook.org",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2016
    }, {
      "title" : "Why deep neural networks",
      "author" : [ "Shyu Liang", "R. Srikant" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2016
    }, {
      "title" : "Neural nets with superlinear VC-dimension",
      "author" : [ "Wolfgang Maass" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1994
    }, {
      "title" : "Depth separation in relu networks for approximating smooth non-linear functions, October 2016",
      "author" : [ "I. Safran", "O. Shamir" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2016
    }, {
      "title" : "Benefits of depth in neural networks",
      "author" : [ "Matus Telgarsky" ],
      "venue" : "In COLT,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2016
    }, {
      "title" : "Lower bounds for approximation by nonlinear manifolds",
      "author" : [ "Hugh E. Warren" ],
      "venue" : "Transactions of the American Mathematical Society,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1968
    }, {
      "title" : "Error bounds for approximations with deep ReLU networks, October 2016",
      "author" : [ "Dmitry Yarotsky" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "A natural foundational question that arises is: what are the theoretical limits on the classification power of these networks? The established way to formalize this question is by considering VCdimension, as it is well known that this asymptotically determines the sample complexity of PAC learning with such classifiers [3].",
      "startOffset" : 321,
      "endOffset" : 324
    }, {
      "referenceID" : 6,
      "context" : "The most common activation function used in practice is, by far, the rectified linear unit, also known as ReLU [7, 8].",
      "startOffset" : 111,
      "endOffset" : 117
    }, {
      "referenceID" : 1,
      "context" : "1 uses the “bit extraction” technique, which was also used in [2] to give an Ω(WL) lower bound.",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 11,
      "context" : "One interesting aspect of the proof is that it does not use Warren’s lemma [13], which is a mainstay of VC-dimension upper bounds [6, 2, 1].",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 5,
      "context" : "One interesting aspect of the proof is that it does not use Warren’s lemma [13], which is a mainstay of VC-dimension upper bounds [6, 2, 1].",
      "startOffset" : 130,
      "endOffset" : 139
    }, {
      "referenceID" : 1,
      "context" : "One interesting aspect of the proof is that it does not use Warren’s lemma [13], which is a mainstay of VC-dimension upper bounds [6, 2, 1].",
      "startOffset" : 130,
      "endOffset" : 139
    }, {
      "referenceID" : 0,
      "context" : "One interesting aspect of the proof is that it does not use Warren’s lemma [13], which is a mainstay of VC-dimension upper bounds [6, 2, 1].",
      "startOffset" : 130,
      "endOffset" : 139
    }, {
      "referenceID" : 5,
      "context" : "1 with results of [6].",
      "startOffset" : 18,
      "endOffset" : 21
    }, {
      "referenceID" : 10,
      "context" : "[12] shows that there is a ReLU network with L layers and U = Θ(L) units such that any network approximating it with only O(L) layers must have Ω(2 1/3 ) units; this phenomenon holds even for real-valued functions.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 4,
      "context" : "[5] show an analogous result for a high-dimensional 3-layer network that cannot be approximated by a 2-layer network except with an exponential blow-up in the number of nodes.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "[11] show that a sufficiently non-linear C function on [0, 1] can be approximated with error in L2 by a ReLU network with O(polylog(1/ )) layers and weights, but any such approximation with O(1) layers requires Ω(1/ ) weights.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "[11] show that a sufficiently non-linear C function on [0, 1] can be approximated with error in L2 by a ReLU network with O(polylog(1/ )) layers and weights, but any such approximation with O(1) layers requires Ω(1/ ) weights.",
      "startOffset" : 55,
      "endOffset" : 61
    }, {
      "referenceID" : 12,
      "context" : "[14] shows that any C-function on [0, 1] can be approximated with error in L∞ by a ReLU network with O(log(1/ )) layers and O(( 1 ) d/n log(1/ )) weights.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "[14] shows that any C-function on [0, 1] can be approximated with error in L∞ by a ReLU network with O(log(1/ )) layers and O(( 1 ) d/n log(1/ )) weights.",
      "startOffset" : 34,
      "endOffset" : 40
    }, {
      "referenceID" : 7,
      "context" : "[9] show that a sufficiently smooth univariate function can be approximated with error in L∞ by a network with ReLU and threshold gates with Θ(log(1/ )) layers and O(polylog(1/ )) weights, but that Ω(poly(1/ )) weights would be required if there were only o(log(1/ )) layers; they also prove analogous results for multivariate functions.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "Lastly, [4] draw a connection to tensor factorizations to show that, for non-ReLU networks, the set of functions computable by a shallow network have measure zero among those computable by a deep networks.",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 1,
      "context" : "The proof of our main lower bound uses the “bit extraction” technique that was developed by [2] to prove a Ω(WL) lower bound.",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 0,
      "context" : "For any a ≤ b and ε > 0, observe that the function f(x) := σ(1−σ(a/ε−x/ε))+ σ(1− σ(x/ε− b/ε))− 1 has the property that, f(x) = 1 for x ∈ [a, b], and f(x) = 0 for x / ∈ (a − ε, b + ε), and f(x) ∈ [0, 1] for all x.",
      "startOffset" : 195,
      "endOffset" : 201
    }, {
      "referenceID" : 1,
      "context" : "2 implies an inherent barrier to proving lower bounds using the “bit extraction” approach of [2].",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 1,
      "context" : "[2] showed it is possible to take m = Ω(L) and n = Ω(W ), thus proving a lower bound of Ω(WL) for the VC-dimension.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 10,
      "context" : "[12] showed how to construct a function f which satisfies f(x) = (x mod 2) for x ∈ {0, 1, .",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "Our proof requires the following lemma, which is a slight improvement of a result in [13].",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 0,
      "context" : "3 in [1], Lemma 1 in [2]).",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 1,
      "context" : "3 in [1], Lemma 1 in [2]).",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 5,
      "context" : "2 of [6]).",
      "startOffset" : 5,
      "endOffset" : 8
    } ],
    "year" : 2017,
    "abstractText" : "We prove new upper and lower bounds on the VC-dimension of deep neural networks with the ReLU activation function. These bounds are tight for almost the entire range of parameters. Letting W be the number of weights and L be the number of layers, we prove that the VC-dimension is O(WL log(W )) and Ω(WL log(W/L)). This improves both the previously known upper bounds and lower bounds. In terms of the number U of non-linear units, we prove a tight bound Θ(WU) on the VC-dimension. All of these results generalize to arbitrary piecewise linear activation functions.",
    "creator" : "LaTeX with hyperref package"
  }
}