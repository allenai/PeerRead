{
  "name" : "1611.02019.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Mickaël Chen", "Ludovic Denoyer" ],
    "emails" : [ "mickael.chen@lip6.fr", "ludovic.denoyer@lip6.fr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Learning over multi-view data is a challenging problem with strong practical applications. Most related studies focus on the classification point of view and assume that all the views are available at any time. We consider an extension of this framework in two directions. First, based on the BiGAN model, the Multi-view BiGAN (MV-BiGAN) is able to perform density estimation from multi-view inputs. Second, it can deal with missing views and is able to update its prediction when additional views are provided. We illustrate these properties on a set of experiments over different datasets."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Many concrete applications involve multiple sources of information generating different views on the same object (Cesa-Bianchi et al., 2010). If we consider human activities for example, GPS values from a mobile phone, navigation traces over the Internet, or even photos published on social networks are different views on a particular user. In multimedia applications, views can correspond to different modalities (Atrey et al., 2010) such as sounds, images, videos, sequences of previous frames, etc...\nThe problem of multi-view machine learning has been extensively studied during the last decade, mainly from the classification point of view. In that case, one wants to predict an output y based on multiple views acquired on an unknown object x. Different strategies have been explored but a general common idea is based on the (early or late) fusion of the different views at a particular level of a deep architecture (Wang et al., 2015; Ngiam et al., 2011; Srivastava & Salakhutdinov, 2012).\nThe existing literature mainly explores problems where outputs are chosen in a discrete set (e.g categorization), and where all the views are available. An extension of this problem is to consider the density estimation problem where one wants to estimate the conditional probabilities of the outputs given the available views. As noted by Mathieu et al. (2015), minimizing classical prediction losses (e.g Mean square error) will not capture the different output distribution modalities.\nIn this article, we propose a new model able to estimate a distribution over the possible outputs given any subset of views on a particular input. This model is based on the (Bidirectional) Generative Adversarial Networks (BiGAN) formalism. More precisely, we bring two main contributions: first, we propose the CV-BiGAN (Conditional Views BiGAN – Section 3) architecture that allows one to model a conditional distribution P (y|.) in an original way. Second, on top of this architecture, we build the Multi-view BiGANs (MV-BiGAN – Section 4) which is able to both predict when only one or few views are available, and to update its prediction if new views are added. We evaluate this model on different multi-views problems and different datasets (Section 5). The related work is provided in Section 6 and we propose some future research directions in Section 7.\nar X\niv :1\n61 1.\n02 01\n9v 1\n[ cs\n.L G\n] 7\nN ov\n2 01"
    }, {
      "heading" : "2 BACKGROUND AND GENERAL IDEA",
      "text" : ""
    }, {
      "heading" : "2.1 NOTATIONS AND TASK",
      "text" : "Let us denote X the space of objects on which different views will be acquired. Each possible input x ∈ X is associated to a target prediction y ∈ Rn. A classical machine learning problem is to estimate P (y|x) based on the training set. But we consider instead a multi-view problem in which different views on x are available, x being unknown. Let us denote V the number of possible views and x̃k the k-th view over x. The description space for view k is Rnk where nk is the size of this space. Moreover, we consider that some of the V views can be missing. The subset of available views for input xi will be represented by an index vector si ∈ S = {0, 1}V so that sik = 1 if view k is available and sik = 0 elsewhere. Note that all the V views will not be available for each input x, and the prediction model must be able to predict an output given any subset of views s ∈ {0; 1}V . In this configuration, our objective is to estimate the distributions p(y|v(s, x)) where v(s, x) is the set of views x̃k so that sk = 1. This distribution p will be estimated based on a training set D of N training examples. Each example is composed of a subset of views si, v(si, xi) associated to an output yi, so that D = { ( y1, s1, v(s1, x1) ) , ..., ( yN , sN , v(sN , xN ) ) } where si is the index vector\nof views available for xi. Note that xi is not directly known in the training set but only observed through its associated views."
    }, {
      "heading" : "2.2 BIDIRECTIONAL GENERATIVE ADVERSARIAL NETS (BIGAN)",
      "text" : "We quickly remind the principle of BiGANs since our model is an extension of this technique. Generative Adversarial Networks (GAN) have been introduced by Goodfellow et al. (2014) and have demonstrated their ability to model complex distributions. They have been used to produce compelling natural images from a simple latent distribution (Radford et al., 2015; Denton et al., 2015). Exploring the latent space has uncovered interesting, meaningful patterns in the resulting outputs. However, GANs lack the ability to retrieve a latent representation given an output, missing out an opportunity to exploit the learned manifold. Bidirectional Generative Adversarial Networks (BiGANs) have been proposed by Donahue et al. (2016) and Dumoulin et al. (2016), independently, to fill that gap. BiGANs simultaneously learn both an encoder function E that models the encoding process PE(z|y) from the space Rn to a latent space RZ , and a generator function G that models the mapping distribution PG(y|z) of any latent point z ∈ RZ to a possible object y ∈ Rn. From both the encoder distribution and the generator distribution, we can model two joint distributions, respectively denoted PE(y, z) and PG(y, z):\nPG(y, z) = P (z)PG(y|z) PE(y, z) = P (y)PE(z|y)\n(1)\nassuming that P (z) = N (0, 1) and P (y) can be estimated over the training set by a uniform sampling. The BiGAN framework also introduces a discriminator network D1 whose task is to determine whether a pair (y, z) is sampled from pG(y, z) or from pE(y, z), whileE andG are trained to fool D1, resulting in the following learning problem:\nmin G,E max D1 Ey∼P (y),z∼PE(z|y) [logD1(y, z)] + Ez∼P (z),y∼PG(y|z) [1− logD1(y, z)] (2)\nIt can be shown, by following the same steps as in Goodfellow et al. (2014), that the optimization problem described in Equation 2 minimizes the Jensen-Shanon divergence between PE(y, z) and PG(y, z), allowing the model to learn both a decoder and a generator over a training set that will model the joint distribution of (y, z) pairs. As proposed by Dumoulin et al. (2016), we consider in the following that PG(y|z) is modeled by a deterministic non-linear model G so that G(z) = y, and PE as a diagonal Gaussian distribution E(z) = (µ(y), σ(y)). G, µ and σ are estimated by using gradient-based descent techniques."
    }, {
      "heading" : "2.3 GENERAL IDEA",
      "text" : "We propose a model based on the Generative Adversarial Networks paradigm adapted to the multiview prediction problem. Our model is based on two different principles:\nConditional Views BiGANs (CV-BiGAN): First, since one wants to model an output distribution based on observations, our first contribution is to propose an adaptation of BiGANs to model conditional probabilities, resulting in a model able to learn P (y|x̃) where x̃ can be either a single view or an aggregation of multiple views. If conditional GANs have already been proposed in the literature (see Section 6) they are not adapted to our problem which require explicit mappings between input space to latent space, and from latent space to output space.\nMulti-View BiGANs (MV-BiGAN): On top of the CV-BiGAN model, we build a multi-view model able to estimate the distribution of possible outputs based on any subset of views v(s, x). If a natural way to extend the Conditional BiGANS for handling multi-view is to define a mapping function which map the set of views to a representation space (see Section 4.1) the resulting model has shown undesirable behaviors (see Section 5.1). Therefore, we propose to constrain the model based on the idea that adding one more view to any subset of views must decrease the uncertainty on the output distribution i.e the more views are provided, the less variance the output distribution has. This behavior is encouraged by using a Kullback-Leibler divergence (KL) regularization (see Section 4.2)."
    }, {
      "heading" : "3 THE CONDITIONAL BIGAN MODEL (CV-BIGAN)",
      "text" : "Our first objective is to extend the BiGAN formalism to handle an input space (e.g a single observed view) in addition to the output space Rn. We will denote x̃ the observation and y the output to predict. In other words, we wish to capture the conditional probability P (y|x̃) from a given training dataset. Assuming one possesses a bidirectional mapping between the input space and an associated representation space, ie. PE(z|y) and PG(y|z), one can equivalently capture P (z|x̃). The CV-BiGAN model keeps the encoder E and generator G defined previously but also includes an additional encoder function denoted H which goal is to map a value x̃ to the latent space RZ . Applying H on any value of x̃ results in a distribution PH(z|x̃) = N (µH(x̃), σH(x̃)) so that a value of z can be sampled from this distribution. This would then allow one to recover a distribution P (y|x̃). Given a pair (x̃, y), we wish a latent representation z sampled from PH(z|x̃) to be similar to one from PE(z|y). As our goal here is to learn P (z|x̃), we define two joint distributions between x̃ and z:\nPH(x̃, z) = PH(z|x̃)P (x̃) PE(x̃, z) = ∑ y PE(z|y)P (x̃, y) (3)\nMinimizing the Jensen-Shanon divergence between these two distributions is equivalent to solving the following adversarial problem:\nmin E,H max D2 Ex̃,y∼p(x̃,y),z∼pE(z|y) [logD2(x̃, z)] + Ex̃,y∼p(x̃,y),z∼pH(z|x) [1− logD2(x̃, z)] (4)\nNote that when applying stochastic gradient-based descent techniques over this objective function, the probability P (x̃, y) is approximated by sampling uniformly from the training set. We can sample from PH(x̃, z) and PE(x̃, z) by forwarding the pair (x̃, y) into the corresponding network.\nBy merging the two objective functions defined in Equation 2 and 4, the final learning problem for our Conditionnal BiGANs is defined as:\nmin G,E,H max D1,D2 Ex̃,y∼P (x̃,y),z∼PE(z|y) [logD1(y, z)] + Ez∼P (z),y∼PG(y|z) [1− logD1(y, z)]\n+Ex̃,y∼P (x̃,y),z∼pE(z|y) [logD2(x̃, z)] + Ex̃,y∼P (x̃,y),z∼PH(z|x̃) [1− logD2(x̃, z)] (5)\nThe general idea of CV-BiGAN is illustrated in Figure 1."
    }, {
      "heading" : "4 MULTI-VIEW BIGAN",
      "text" : ""
    }, {
      "heading" : "4.1 AGGREGATING MULTI-VIEWS FOR CV-BIGAN",
      "text" : "We now consider the problem of computing an output distribution conditioned by multiple different views. In that case, we can use the CV-BiGAN Model (or other conditional approaches) conjointly with a model able to aggregate the different views whereA is the size of the aggregation space. Instead of considering the input x̃, we define an aggregation model Ψ. Ψ(v(s, x)) will be the representation of the aggregation of all the available views x̃k1:\nΨ(v(s, x)) = V∑ k=1 skφk(x̃ k) (6)\nwhere φk is a function that will be learned that maps a particular view in Rnk to the aggregation space RA. By replacing x̃ in Equation 5, one can then simultaneously learn the functions φk and the distributions PH , PE and PD, resulting in a multi-view model able to deal with any subset of views."
    }, {
      "heading" : "4.2 UNCERTAINTY REDUCTION ASSUMPTION",
      "text" : "However, the previous idea suffers from a very high instability when learning, as it is usually noted with complex GANs architectures (see Section 5). In order to stabilize our model, we propose to add a regularization based on the idea that adding new views to an existing subset of views should reduce the uncertainty over the outputs. Indeed, under the assumption that views are consistent one another,\n1 Note that other aggregation scheme can be used like recurrent neural networks for example.\nadding a new view should allow to refine the predictions and reduce the variance of the distribution of the outputs.\nLet us consider an object x and two index vectors s and s′ such that v(x, s) ⊂ v(x, s′) ie. ∀k, s′k ≥ sk. Then, intuitively, P (x|v(x, s′)) should be ”included” in P (x|v(x, s)). In the CVGAN model, since P (y|z) is deterministic, this can be enforced at a latent level by minimizing KL(P (z|v(x, s′) || P (z|v(x, s)). By assuming those two distributions are diagonal gaussian distributions (ie. P (z|v(x, s′) = N (µ1,Σ1) and P (z|v(x, s) = N (µ2,Σ2) where Σk are diagonal matrices with diagonal elements σk(i)), the KL divergence can be computed as in Equation 7 and differentiated.\nKL(P (z|v(x, s′))||P (z|v(x, s))) = 1 2 Z∑ i=1\n( −1− log ( σ21(i)\nσ22(i)\n) + σ21(i)\nσ22(i) +\n(µ1(i) − µ2(i))2\nσ22(i)\n) (7)\nNote that this divergence is written on the estimation made by the function H and will act as a regularization over the latent conditional distribution.\nThe final objective function of the MV-BiGAN can be written as:\nmin G,E,H max D1,D2 Es,x,y∼P (s,x,y),z∼PE(z|y) [logD1(y, z)] + Ez∼P (z),y∼PG(y|z) [1− logD1(y, z)]\n+Es,x,y∼P (s,x,y),z∼PE(z|y) [logD2(v(x, s), z)] + Es,x,y∼P (s,x,y),z∼PH(z|v(x,s)) [1− logD2(v(x, s), z)] +λEx∼P (x) ∑\ns,s′∈Sx ∀k,s′k≥sk\nKL(H(v(x, s′))||H(v(x, s)))\n(8) where λ controls the strength of the regularization. Note that aggregation models Ψ are included into H and D2 and can be optimized conjointly in this objective function."
    }, {
      "heading" : "4.3 LEARNING THE MV-BIGAN",
      "text" : "The different functions E, G, H , D1 and D2 are implemented as parametric neural networks and trained by mini-batch stochastic gradient descent (see Appendix for more details concerning the architectures).We first update the discriminators networks D1 and D2, then we update the generator and encoders G, E and H with gradient steps in the opposite direction.\nAs with most other implementation of GAN-based models, we find that using an alternative objective proposed by Goodfellow et al. (2014) for E, G and H instead leads to more stable training. The new objective consist of swapping the labels for the discriminators instead of reversing the gradient. We also find that we can update all the modules in one pass instead of taking alternate gradient steps while obtaining similar results.\nNote that the MV-BiGAN model is trained based on datasets where all the V views are available for each data point. In order to generate examples where only subsets of views are available, the ideal procedure would be to consider all the possible subsets of views. Due to the number of data points that would be generated by such a procedure, we build random sequences of incremental sets of views and enforce the KL regularization over successive sets."
    }, {
      "heading" : "5 EXPERIMENTS",
      "text" : "We evaluate our model on three different types of experiments, and on two differents datasets. The first dataset we experiment on is the MNIST dataset of handwritten digits. The second dataset is the CelebA (Liu et al., 2015) dataset composed of both images of faces and corresponding attributes. The MNIST dataset is used to illustrate the ability of the MV-BiGAN to handle different subset of views, and to update its prediction when integrating new incoming views. The CelebA dataset is used to demonstrate the ability of MV-BiGAN to deal with different types (heterogeneous) of views."
    }, {
      "heading" : "5.1 MNIST, 4 VIEWS",
      "text" : "We consider the problem where 4 different views can be available, each view corresponding to a particular quarter of the final image to predict – each view is a vector of R(14×14). The MV-BiGAN is used here to recover the original image. The model is trained on the MNIST training digits, and results are provided on the MNIST testing dataset.\nFigure 3 illustrates the results obtained for some digits. In this figure, the first column displays the input (the subset of views), while the other columns shows predicted outputs sampled by the MV-BiGAN. An additional view is added between each row. This experiment shows that when new views are added, the diversity in the predicted outputs decreases due to the KL-contraint introduced in the model, which is the desired behavior i.e more information implied less variance. When removing the KL constraint (Figure 4), the diversity still remains important, even if many views are provided to the model. This show the importance of the KL regularization term in the MV-BiGAN objective."
    }, {
      "heading" : "5.2 MNIST, SEQUENCE OF INCOMING VIEWS",
      "text" : "We made another set of experiments where the views correspond to images with missing values (missing values are replaced by 0.5). This can be viewed as a data imputation problem – Figure 5. Here also, the behavior of the MV-BiGAN exhibits interesting properties: the model is able to predict the desired output as long as enough information has been provided. When only non-informative views are provided, the model produces digits with a high diversity, the diversity decreasing when new information is added."
    }, {
      "heading" : "5.3 CELEBA, INTEGRATING HETEROGENEOUS INFORMATION",
      "text" : "At last, the third experiment aims at measuring the ability of MV-BiGAN to handle heterogeneous inputs. We consider two views: (i) the attribute vector containing information about the person in the picture (hair color, sex, ...), and (ii) a incomplete face. Figure 9 illustrates the results obtained on two faces. The first line corresponds to the faces generated based on the attribute vector. One can see that the attribute information has been captured by the model: for example, the sex of the generated face is constant (only women) showing that MV-BiGan has captured this information from the attribute vector. The second line corresponds to the faces generated when using the incomplete face as an input. One can also see that the generated outputs are ”compatible” with the incomplete information provided to the model. But the attribute are not considered (for example, women and men are generated). At last, the third line corresponds to images generated based on the two partial views (attributes and incomplete face) which are close to the ground-truth image (bottom left). Note that, in this set of experiments, the convergence of the MV-BiGAN was quite difficult to obtain, and the quality of the generated faces is still not satisfying."
    }, {
      "heading" : "6 RELATED WORK",
      "text" : "Multi-view and Representation Learning: Many application fields naturally deal with multi-view data with true advantages. For example, in the multimedia domain, dealing with a bunch of views is usual (Atrey et al., 2010): text, audio, images (different framings from videos) are starting points of these views. Besides, multimedia learning tasks from multi-views led to a large amount of fusionbased ad-hoc approaches and experimental results. The success of multi-view supervised learning approaches in the multimedia community seems to rely on the ability of the systems to deal with the complementary of the information carried by each modality. Comparable studies are of importance in many domains, such as bioinformatics (Sokolov & Ben-Hur, 2011), speech recognition (Arora & Livescu, 2012; Koço et al., 2012), signal-based multimodal integration (Wu et al., 1999), gesture recognition (Wu et al., 2013), etc.\nMoreover, multi-view learning has been theoretically studied mainly under the semi-supervised setting, but only with two facing views (Chapelle et al., 2006; Sun, 2013; Sun & Taylor, 2014; Johnson & Zhang, 2015). In parallel, ensemble-based learning approaches have been theoretically studied, in the supervised setting: many interesting results should concern multi-view learning, as long as the ensemble is built upon many views (Rokach, 2010; Zhang & Zhang, 2011). From the representation learning point of view, recent models are based on the incorporation of some ”fusion” layers in the deep neural network architecture as in (Ngiam et al., 2011) or (Srivastava & Salakhutdinov, 2012) for example. Some other interesting models include the multiview perceptron(Zhu et al., 2014).\nEstimating Complex Distributions: While deep learning has shown great results in many classification task for a decade, training deep generative models still remains a challenge. Deep Boltzmann Machines (Salakhutdinov & Hinton, 2009) are un-directed graphical models organized in a succession of layers of hidden variables. In a multi-view setting, they are able to deal with missing views and have been used to capture the joint distribution in bi-modal text and image data (Srivastava & Salakhutdinov, 2012; Sohn et al., 2014). Another trend started with denoising autoencoder (Vincent et al., 2008), which aims to reconstruct a data from a noisy input have been proved to possess some desirable properties for data generation (Bengio et al., 2013). The model have been generalized under the name Generative Stochastic Networks by replacing the noise function C with a mapping to a latent space (Thibodeau-Laufer et al., 2014). Pulling away from the mixing problems encountered in previous approaches, Variational Autoencoders (Kingma & Welling, 2013) attempts to map the input distribution to a latent distribution which is easy to sample from. The model is trained by optimizing a variational bound on the likelihood, using stochastic gradient descent methods. The Kullback-Leibler regularizer on the latent Gaussian representations used in our model is reminiscent of the one introduced in the variational lower bound used by the VAE.\nThe BiGAN model (Donahue et al., 2016; Dumoulin et al., 2016) that serves as a basis for our work is an extension of the Generative Adversarial Nets (Goodfellow et al., 2014). A GAN extension that captures conditional probabilities (CGAN) has been proposed in (Mirza & Osindero, 2014). However, as noted by (Mathieu et al., 2015) and (Pathak et al., 2016), they display very unstable behavior. More specifically, CGAN have been able to generate image of faces conditioned on an attribute vector (Gauthier, 2014), but fail to model image distribution conditioned on a part of the image or on previous frames. By using a mapping between the output space and a latent space, CV-BiGAN allow us to bypass this difficulty by working at a latent level instead."
    }, {
      "heading" : "7 CONCLUSION AND PERSPECTIVES",
      "text" : "We have proposed the CV-BiGAN model for estimating conditional densities, and its extension MV-BiGAN to handle multi-view inputs. The MV-BiGAN model is able to both handle subsets of views, but also to update its prediction when new views are added. It is based on the idea that the uncertainty of the prediction must decrease when additional information is provided, this idea being handled through a KL constraint in the latent space. This work opens different research directions: the first one concerns the architecture of the model itself since the convergence of MV-BiGAN is still difficult to obtain and has a particularly high training cost. Another direction would be to see if this family of model could be used on data streams for anytime prediction."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "This work was supported by the French project LIVES ANR-15-CE23-0026-03."
    } ],
    "references" : [ {
      "title" : "Kernel cca for multi-view acoustic feature learning using articulatory measurements",
      "author" : [ "R. Arora", "K. Livescu" ],
      "venue" : "In MLSP,",
      "citeRegEx" : "Arora and Livescu.,? \\Q2012\\E",
      "shortCiteRegEx" : "Arora and Livescu.",
      "year" : 2012
    }, {
      "title" : "Multimodal fusion for multimedia analysis: A survey",
      "author" : [ "Pradeep K. Atrey", "M. Anwar Hossain", "Abdulmotaleb El Saddik", "Mohan S. Kankanhalli" ],
      "venue" : "Multimedia Syst.,",
      "citeRegEx" : "Atrey et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Atrey et al\\.",
      "year" : 2010
    }, {
      "title" : "Generalized denoising auto-encoders as generative models",
      "author" : [ "Yoshua Bengio", "Li Yao", "Guillaume Alain", "Pascal Vincent" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2013
    }, {
      "title" : "Guest editorial: Learning from multiple sources",
      "author" : [ "Nicolò Cesa-Bianchi", "David R. Hardoon", "Gayle Leen" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Cesa.Bianchi et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Cesa.Bianchi et al\\.",
      "year" : 2010
    }, {
      "title" : "Semi-supervised Learning",
      "author" : [ "O. Chapelle", "B. Schölkopf", "A. Zien" ],
      "venue" : null,
      "citeRegEx" : "Chapelle et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Chapelle et al\\.",
      "year" : 2006
    }, {
      "title" : "Deep generative image models using a laplacian pyramid of adversarial networks",
      "author" : [ "Emily L Denton", "Soumith Chintala", "Rob Fergus" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Denton et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Denton et al\\.",
      "year" : 2015
    }, {
      "title" : "Adversarial feature learning",
      "author" : [ "Jeff Donahue", "Philipp Krähenbühl", "Trevor Darrell" ],
      "venue" : "arXiv preprint arXiv:1605.09782,",
      "citeRegEx" : "Donahue et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Donahue et al\\.",
      "year" : 2016
    }, {
      "title" : "Adversarially learned inference",
      "author" : [ "Vincent Dumoulin", "Ishmael Belghazi", "Ben Poole", "Alex Lamb", "Martin Arjovsky", "Olivier Mastropietro", "Aaron Courville" ],
      "venue" : "arXiv preprint arXiv:1606.00704,",
      "citeRegEx" : "Dumoulin et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Dumoulin et al\\.",
      "year" : 2016
    }, {
      "title" : "Conditional generative adversarial nets for convolutional face generation. Class Project for Stanford CS231N: Convolutional Neural Networks for Visual Recognition, Winter semester",
      "author" : [ "Jon Gauthier" ],
      "venue" : null,
      "citeRegEx" : "Gauthier.,? \\Q2014\\E",
      "shortCiteRegEx" : "Gauthier.",
      "year" : 2014
    }, {
      "title" : "Generative adversarial nets",
      "author" : [ "Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "Semi-supervised learning with multi-view embedding: theory and application with convolutional neural networks",
      "author" : [ "R. Johnson", "T. Zhang" ],
      "venue" : "CoRR, abs/1504.012555v1,",
      "citeRegEx" : "Johnson and Zhang.,? \\Q2015\\E",
      "shortCiteRegEx" : "Johnson and Zhang.",
      "year" : 2015
    }, {
      "title" : "Auto-encoding variational bayes",
      "author" : [ "Diederik P Kingma", "Max Welling" ],
      "venue" : "In Proceedings of the 2nd International Conference on Learning Representations (ICLR),",
      "citeRegEx" : "Kingma and Welling.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma and Welling.",
      "year" : 2014
    }, {
      "title" : "Applying multiview learning algorithms to human-human conversation classification",
      "author" : [ "Sokol Koço", "Cécile Capponi", "Frédéric Béchet" ],
      "venue" : "In INTERSPEECH,",
      "citeRegEx" : "Koço et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Koço et al\\.",
      "year" : 2012
    }, {
      "title" : "Deep learning face attributes in the wild",
      "author" : [ "Ziwei Liu", "Ping Luo", "Xiaogang Wang", "Xiaoou Tang" ],
      "venue" : "In Proceedings of International Conference on Computer Vision (ICCV),",
      "citeRegEx" : "Liu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep multi-scale video prediction beyond mean square error",
      "author" : [ "Michael Mathieu", "Camille Couprie", "Yann LeCun" ],
      "venue" : "arXiv preprint arXiv:1511.05440,",
      "citeRegEx" : "Mathieu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mathieu et al\\.",
      "year" : 2015
    }, {
      "title" : "Conditional generative adversarial nets",
      "author" : [ "Mehdi Mirza", "Simon Osindero" ],
      "venue" : "arXiv preprint arXiv:1411.1784,",
      "citeRegEx" : "Mirza and Osindero.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mirza and Osindero.",
      "year" : 2014
    }, {
      "title" : "Multimodal deep learning",
      "author" : [ "Jiquan Ngiam", "Aditya Khosla", "Mingyu Kim", "Juhan Nam", "Honglak Lee", "Andrew Y Ng" ],
      "venue" : "In Proceedings of the 28th International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Ngiam et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Ngiam et al\\.",
      "year" : 2011
    }, {
      "title" : "Context encoders: Feature learning by inpainting",
      "author" : [ "Deepak Pathak", "Philipp Krahenbuhl", "Jeff Donahue", "Trevor Darrell", "Alexei A Efros" ],
      "venue" : "arXiv preprint arXiv:1604.07379,",
      "citeRegEx" : "Pathak et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Pathak et al\\.",
      "year" : 2016
    }, {
      "title" : "Unsupervised representation learning with deep convolutional generative adversarial networks",
      "author" : [ "Alec Radford", "Luke Metz", "Soumith Chintala" ],
      "venue" : "arXiv preprint arXiv:1511.06434,",
      "citeRegEx" : "Radford et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2015
    }, {
      "title" : "Ensemble-based classifiers",
      "author" : [ "L. Rokach" ],
      "venue" : "Artif. Intell. Rev.,",
      "citeRegEx" : "Rokach.,? \\Q2010\\E",
      "shortCiteRegEx" : "Rokach.",
      "year" : 2010
    }, {
      "title" : "Deep boltzmann machines",
      "author" : [ "Ruslan Salakhutdinov", "Geoffrey E Hinton" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Salakhutdinov and Hinton.,? \\Q2009\\E",
      "shortCiteRegEx" : "Salakhutdinov and Hinton.",
      "year" : 2009
    }, {
      "title" : "Improved multimodal deep learning with variation of information",
      "author" : [ "Kihyuk Sohn", "Wenling Shang", "Honglak Lee" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Sohn et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sohn et al\\.",
      "year" : 2014
    }, {
      "title" : "Multi-view prediction of protein function",
      "author" : [ "Artem Sokolov", "Asa Ben-Hur" ],
      "venue" : "In ACM-BCB,",
      "citeRegEx" : "Sokolov and Ben.Hur.,? \\Q2011\\E",
      "shortCiteRegEx" : "Sokolov and Ben.Hur.",
      "year" : 2011
    }, {
      "title" : "Multimodal learning with deep boltzmann machines",
      "author" : [ "Nitish Srivastava", "Ruslan R Salakhutdinov" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Srivastava and Salakhutdinov.,? \\Q2012\\E",
      "shortCiteRegEx" : "Srivastava and Salakhutdinov.",
      "year" : 2012
    }, {
      "title" : "A survey of multi-view machine learning",
      "author" : [ "Shiliang Sun" ],
      "venue" : "Neural Comput. Appl.,",
      "citeRegEx" : "Sun.,? \\Q2013\\E",
      "shortCiteRegEx" : "Sun.",
      "year" : 2013
    }, {
      "title" : "PAC-Bayes analysis of multi-view learning",
      "author" : [ "Shiliang Sun", "John-Shawe Taylor" ],
      "venue" : "CoRR, abs/1406.5614,",
      "citeRegEx" : "Sun and Taylor.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sun and Taylor.",
      "year" : 2014
    }, {
      "title" : "Deep generative stochastic networks trainable by backprop",
      "author" : [ "Eric Thibodeau-Laufer", "Guillaume Alain", "Jason Yosinski" ],
      "venue" : null,
      "citeRegEx" : "Thibodeau.Laufer et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Thibodeau.Laufer et al\\.",
      "year" : 2014
    }, {
      "title" : "Extracting and composing robust features with denoising autoencoders",
      "author" : [ "Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio", "Pierre-Antoine Manzagol" ],
      "venue" : "In Proceedings of the 25th international conference on Machine learning,",
      "citeRegEx" : "Vincent et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Vincent et al\\.",
      "year" : 2008
    }, {
      "title" : "On deep multi-view representation learning",
      "author" : [ "Weiran Wang", "Raman Arora", "Karen Livescu", "Jeff Bilmes" ],
      "venue" : "In Proc. of the 32st Int. Conf. Machine Learning (ICML",
      "citeRegEx" : "Wang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2015
    }, {
      "title" : "Fusing multi-modal features for gesture recognition",
      "author" : [ "Jiaxiang Wu", "Jian Cheng", "Chaoyang Zhao", "Hanqing Lu" ],
      "venue" : "In ICMI, pp",
      "citeRegEx" : "Wu et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2013
    }, {
      "title" : "Multimodal integration: a statistical view",
      "author" : [ "L. Wu", "S.L. Oviatt", "P.R. Cohen" ],
      "venue" : "MM, 1(4):334–341,",
      "citeRegEx" : "Wu et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 1999
    }, {
      "title" : "A novel ensemble construction method for multi-view data using random cross-view correlation between within-class examples",
      "author" : [ "Jianchun Zhang", "Daoqiang Zhang" ],
      "venue" : "Pattern Recogn.,",
      "citeRegEx" : "Zhang and Zhang.,? \\Q2011\\E",
      "shortCiteRegEx" : "Zhang and Zhang.",
      "year" : 2011
    }, {
      "title" : "Multi-view perceptron: a deep model for learning face identity and view representations",
      "author" : [ "Zhenyao Zhu", "Ping Luo", "Xiaogang Wang", "Xiaoou Tang" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Zhu et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2014
    }, {
      "title" : "λ is set to 1 · 10−3, and minibatch size is 16",
      "author" : [ "Aggregation space is of size" ],
      "venue" : "The model has been",
      "citeRegEx" : "size,? 1000",
      "shortCiteRegEx" : "size",
      "year" : 1000
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Many concrete applications involve multiple sources of information generating different views on the same object (Cesa-Bianchi et al., 2010).",
      "startOffset" : 113,
      "endOffset" : 140
    }, {
      "referenceID" : 1,
      "context" : "In multimedia applications, views can correspond to different modalities (Atrey et al., 2010) such as sounds, images, videos, sequences of previous frames, etc.",
      "startOffset" : 73,
      "endOffset" : 93
    }, {
      "referenceID" : 28,
      "context" : "Different strategies have been explored but a general common idea is based on the (early or late) fusion of the different views at a particular level of a deep architecture (Wang et al., 2015; Ngiam et al., 2011; Srivastava & Salakhutdinov, 2012).",
      "startOffset" : 173,
      "endOffset" : 246
    }, {
      "referenceID" : 16,
      "context" : "Different strategies have been explored but a general common idea is based on the (early or late) fusion of the different views at a particular level of a deep architecture (Wang et al., 2015; Ngiam et al., 2011; Srivastava & Salakhutdinov, 2012).",
      "startOffset" : 173,
      "endOffset" : 246
    }, {
      "referenceID" : 1,
      "context" : "In multimedia applications, views can correspond to different modalities (Atrey et al., 2010) such as sounds, images, videos, sequences of previous frames, etc... The problem of multi-view machine learning has been extensively studied during the last decade, mainly from the classification point of view. In that case, one wants to predict an output y based on multiple views acquired on an unknown object x. Different strategies have been explored but a general common idea is based on the (early or late) fusion of the different views at a particular level of a deep architecture (Wang et al., 2015; Ngiam et al., 2011; Srivastava & Salakhutdinov, 2012). The existing literature mainly explores problems where outputs are chosen in a discrete set (e.g categorization), and where all the views are available. An extension of this problem is to consider the density estimation problem where one wants to estimate the conditional probabilities of the outputs given the available views. As noted by Mathieu et al. (2015), minimizing classical prediction losses (e.",
      "startOffset" : 74,
      "endOffset" : 1019
    }, {
      "referenceID" : 18,
      "context" : "They have been used to produce compelling natural images from a simple latent distribution (Radford et al., 2015; Denton et al., 2015).",
      "startOffset" : 91,
      "endOffset" : 134
    }, {
      "referenceID" : 5,
      "context" : "They have been used to produce compelling natural images from a simple latent distribution (Radford et al., 2015; Denton et al., 2015).",
      "startOffset" : 91,
      "endOffset" : 134
    }, {
      "referenceID" : 6,
      "context" : "Generative Adversarial Networks (GAN) have been introduced by Goodfellow et al. (2014) and have demonstrated their ability to model complex distributions.",
      "startOffset" : 62,
      "endOffset" : 87
    }, {
      "referenceID" : 5,
      "context" : ", 2015; Denton et al., 2015). Exploring the latent space has uncovered interesting, meaningful patterns in the resulting outputs. However, GANs lack the ability to retrieve a latent representation given an output, missing out an opportunity to exploit the learned manifold. Bidirectional Generative Adversarial Networks (BiGANs) have been proposed by Donahue et al. (2016) and Dumoulin et al.",
      "startOffset" : 8,
      "endOffset" : 373
    }, {
      "referenceID" : 5,
      "context" : ", 2015; Denton et al., 2015). Exploring the latent space has uncovered interesting, meaningful patterns in the resulting outputs. However, GANs lack the ability to retrieve a latent representation given an output, missing out an opportunity to exploit the learned manifold. Bidirectional Generative Adversarial Networks (BiGANs) have been proposed by Donahue et al. (2016) and Dumoulin et al. (2016), independently, to fill that gap.",
      "startOffset" : 8,
      "endOffset" : 400
    }, {
      "referenceID" : 8,
      "context" : "It can be shown, by following the same steps as in Goodfellow et al. (2014), that the optimization problem described in Equation 2 minimizes the Jensen-Shanon divergence between PE(y, z) and PG(y, z), allowing the model to learn both a decoder and a generator over a training set that will model the joint distribution of (y, z) pairs.",
      "startOffset" : 51,
      "endOffset" : 76
    }, {
      "referenceID" : 7,
      "context" : "As proposed by Dumoulin et al. (2016), we consider in the following that PG(y|z) is modeled by a deterministic non-linear model G so that G(z) = y, and PE as a diagonal Gaussian distribution E(z) = (μ(y), σ(y)).",
      "startOffset" : 15,
      "endOffset" : 38
    }, {
      "referenceID" : 9,
      "context" : "As with most other implementation of GAN-based models, we find that using an alternative objective proposed by Goodfellow et al. (2014) for E, G and H instead leads to more stable training.",
      "startOffset" : 111,
      "endOffset" : 136
    }, {
      "referenceID" : 13,
      "context" : "The second dataset is the CelebA (Liu et al., 2015) dataset composed of both images of faces and corresponding attributes.",
      "startOffset" : 33,
      "endOffset" : 51
    }, {
      "referenceID" : 1,
      "context" : "For example, in the multimedia domain, dealing with a bunch of views is usual (Atrey et al., 2010): text, audio, images (different framings from videos) are starting points of these views.",
      "startOffset" : 78,
      "endOffset" : 98
    }, {
      "referenceID" : 12,
      "context" : "Comparable studies are of importance in many domains, such as bioinformatics (Sokolov & Ben-Hur, 2011), speech recognition (Arora & Livescu, 2012; Koço et al., 2012), signal-based multimodal integration (Wu et al.",
      "startOffset" : 123,
      "endOffset" : 165
    }, {
      "referenceID" : 30,
      "context" : ", 2012), signal-based multimodal integration (Wu et al., 1999), gesture recognition (Wu et al.",
      "startOffset" : 45,
      "endOffset" : 62
    }, {
      "referenceID" : 29,
      "context" : ", 1999), gesture recognition (Wu et al., 2013), etc.",
      "startOffset" : 29,
      "endOffset" : 46
    }, {
      "referenceID" : 4,
      "context" : "Moreover, multi-view learning has been theoretically studied mainly under the semi-supervised setting, but only with two facing views (Chapelle et al., 2006; Sun, 2013; Sun & Taylor, 2014; Johnson & Zhang, 2015).",
      "startOffset" : 134,
      "endOffset" : 211
    }, {
      "referenceID" : 24,
      "context" : "Moreover, multi-view learning has been theoretically studied mainly under the semi-supervised setting, but only with two facing views (Chapelle et al., 2006; Sun, 2013; Sun & Taylor, 2014; Johnson & Zhang, 2015).",
      "startOffset" : 134,
      "endOffset" : 211
    }, {
      "referenceID" : 19,
      "context" : "In parallel, ensemble-based learning approaches have been theoretically studied, in the supervised setting: many interesting results should concern multi-view learning, as long as the ensemble is built upon many views (Rokach, 2010; Zhang & Zhang, 2011).",
      "startOffset" : 218,
      "endOffset" : 253
    }, {
      "referenceID" : 16,
      "context" : "From the representation learning point of view, recent models are based on the incorporation of some ”fusion” layers in the deep neural network architecture as in (Ngiam et al., 2011) or (Srivastava & Salakhutdinov, 2012) for example.",
      "startOffset" : 163,
      "endOffset" : 183
    }, {
      "referenceID" : 32,
      "context" : "Some other interesting models include the multiview perceptron(Zhu et al., 2014).",
      "startOffset" : 62,
      "endOffset" : 80
    }, {
      "referenceID" : 21,
      "context" : "In a multi-view setting, they are able to deal with missing views and have been used to capture the joint distribution in bi-modal text and image data (Srivastava & Salakhutdinov, 2012; Sohn et al., 2014).",
      "startOffset" : 151,
      "endOffset" : 204
    }, {
      "referenceID" : 27,
      "context" : "Another trend started with denoising autoencoder (Vincent et al., 2008), which aims to reconstruct a data from a noisy input have been proved to possess some desirable properties for data generation (Bengio et al.",
      "startOffset" : 49,
      "endOffset" : 71
    }, {
      "referenceID" : 2,
      "context" : ", 2008), which aims to reconstruct a data from a noisy input have been proved to possess some desirable properties for data generation (Bengio et al., 2013).",
      "startOffset" : 135,
      "endOffset" : 156
    }, {
      "referenceID" : 26,
      "context" : "The model have been generalized under the name Generative Stochastic Networks by replacing the noise function C with a mapping to a latent space (Thibodeau-Laufer et al., 2014).",
      "startOffset" : 145,
      "endOffset" : 176
    }, {
      "referenceID" : 6,
      "context" : "The BiGAN model (Donahue et al., 2016; Dumoulin et al., 2016) that serves as a basis for our work is an extension of the Generative Adversarial Nets (Goodfellow et al.",
      "startOffset" : 16,
      "endOffset" : 61
    }, {
      "referenceID" : 7,
      "context" : "The BiGAN model (Donahue et al., 2016; Dumoulin et al., 2016) that serves as a basis for our work is an extension of the Generative Adversarial Nets (Goodfellow et al.",
      "startOffset" : 16,
      "endOffset" : 61
    }, {
      "referenceID" : 9,
      "context" : ", 2016) that serves as a basis for our work is an extension of the Generative Adversarial Nets (Goodfellow et al., 2014).",
      "startOffset" : 95,
      "endOffset" : 120
    }, {
      "referenceID" : 14,
      "context" : "However, as noted by (Mathieu et al., 2015) and (Pathak et al.",
      "startOffset" : 21,
      "endOffset" : 43
    }, {
      "referenceID" : 17,
      "context" : ", 2015) and (Pathak et al., 2016), they display very unstable behavior.",
      "startOffset" : 12,
      "endOffset" : 33
    }, {
      "referenceID" : 8,
      "context" : "More specifically, CGAN have been able to generate image of faces conditioned on an attribute vector (Gauthier, 2014), but fail to model image distribution conditioned on a part of the image or on previous frames.",
      "startOffset" : 101,
      "endOffset" : 117
    } ],
    "year" : 2016,
    "abstractText" : "Learning over multi-view data is a challenging problem with strong practical applications. Most related studies focus on the classification point of view and assume that all the views are available at any time. We consider an extension of this framework in two directions. First, based on the BiGAN model, the Multi-view BiGAN (MV-BiGAN) is able to perform density estimation from multi-view inputs. Second, it can deal with missing views and is able to update its prediction when additional views are provided. We illustrate these properties on a set of experiments over different datasets.",
    "creator" : "LaTeX with hyperref package"
  }
}