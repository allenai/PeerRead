{
  "name" : "1312.3811.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Efficient Baseline-free Sampling in Parameter Exploring Policy Gradients: Super Symmetric PGPE",
    "authors" : [ "Frank Sehnke" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Policy Gradient (PG) methods that explore directly in parameter space have some major advantages over standard PG methods, like described in [1,2,3,4,5,6] and [7] and have therefore drawn a lot of attention in the last years. The basic method from the field of Parameter Exploring Policy Gradients (PEPG) [8], Policy Gradients with Parameter-based Exploration (PGPE) [1], uses two samples that are symmetric around the current hypothesis to circumvent misleading reward in asymmetrical reward distributed problems, gathered with the usual baseline approach. [4] shows that Symmetric Sampling (SyS) is superior even to the optimal baseline. The exploration parameters, however, are still updated by a baseline approach - leaving the exploration prone to asymmetric reward distributions. While the optimal baseline improved this issue substantially, like shown again by [4], it is likely that removing the baseline altogether by a SyS wrt. the exploration parameters will be again superior. Because the exploration parameters are standard deviations that are bounded between zero and infinity, there exist no correct symmetric samples wrt. the exploration parameters. ar X iv :1 31 2.\n38 11\nv1 [\ncs .L\nG ]\n1 3\nD ec\n2 01\n3\nWe will, however, show how the exploration parameters can be sampled quasi symmetric. We give therefore a transformation approximation to get quasi symmetric samples without changing the overall sampling distribution significantly, so that the PGPE assumptions based on normal distributed samples still hold. Finally we will demonstrate via experiments that sampling symmetrically also for the exploration parameters is superior in needs of samples and robustness compared to the original sampling approach, if confronted with search spaces with significant amounts of local optima."
    }, {
      "heading" : "2 Method",
      "text" : "In this section we derive the super-symmetric sampling (SupSyS) method. We show how the method relates to SyS and sampling with a baseline, thereby summarizing the derivation from [1] for SyS and baseline sampling PGPE."
    }, {
      "heading" : "2.1 Parameter-Based Exploration",
      "text" : "To stay conform with the nomenclature of [1] and [4], we assume a Markovian environment that produces a cumulative reward r for a fixed length episode, history, trajectory or roll-out. In this setting, the goal of reinforcement learning is to find the optimal policy parameters θ that maximize the agent’s expected reward\nJ(θ) = ∫ H p(h|θ)r(h)dh. (1)\nAn obvious way to maximize J(θ) is to estimate ∇θJ and use it to carry out gradient ascent optimization. The probabilistic policy used in standard PG is replaced with a probability distribution over the parameters θ for PGPE. The advantage of this approach is that the actions are deterministic, and an entire history can therefore be generated from a single parameter sample. This reduction in samples-per-history is what reduces the variance in the gradient estimate (see [1] for details).\nWe name the distribution over parameters in accordance with [1] ρ. The expected reward with a given ρ is\nJ(ρ) = ∫ Θ ∫ H p(h,θ|ρ)r(h)dhdθ. (2)\nDifferentiating this form of the expected return with respect to ρ and applying sampling methods (first choosing θ from p(θ|ρ), then running the agent to generate h from p(h|θ)) yields the following gradient estimator:\n∇ρJ(ρ) ≈ 1\nN N∑ n=1 ∇ρ log p(θ|ρ)r(hn). (3)\nAssuming that ρ consists of a set of means {µi} and standard deviations {σi} that determine an independent normal distribution for each parameter θi in\nθ gives the following forms for the derivative of the characteristic eligibility log p(θ|ρ) with respect to µi and σi\n∇µi log p(θ|ρ) = (θi − µi)\nσ2i , ∇σi log p(θ|ρ) = (θi − µi)2 − σ2i σ3i , (4)\nwhich can be substituted into Eq. (3) to approximate the µ and σ gradients."
    }, {
      "heading" : "2.2 Sampling with a Baseline",
      "text" : "Given enough samples, Eq. (3) will determine the reward gradient to arbitrary accuracy. However each sample requires rolling out an entire state-action history, which is expensive. Following [9], we obtain a cheaper gradient estimate by drawing a single sample θ and comparing its reward r to a baseline reward b given e.g. by a moving average over previous samples. Intuitively, if r > b we adjust ρ so as to increase the probability of θ, and r < b we do the opposite. If, as in [9], we use a step size αi = ασ 2 i in the direction of positive gradient (where α is a constant) we get the following parameter update equations:\n∆µi = α(r − b)(θi − µi), ∆σi = α(r − b) (θi − µi)2 − σ2i\nσi . (5)\nUsually the baseline is realized as decaying or moving average baseline of the form:\nb(n) = γr(hn−1) + (1− γ)b(n− 1) or b(n) = N∑\nn=N−m r(hn)/m (6)\n[4] showed recently that an optimal baseline can be achieved for PGPE and the algorithm converges significantly faster with an optimal baseline of the form:\nb∗ = E[r(h)||∇ρ log p(θ|ρ)||2] E[||∇ρ log p(θ|ρ)||2] . (7)"
    }, {
      "heading" : "2.3 Symmetric Sampling",
      "text" : "While sampling with a baseline is efficient and reasonably accurate for most scenarios, it has several drawbacks. In particular, if the reward distribution is strongly skewed then the comparison between the sample reward and the baseline reward is misleading. A more robust gradient approximation can be found by measuring the difference in reward between two symmetric samples on either side of the current mean. That is, we pick a perturbation from the distribution N (0,σ), then create symmetric parameter samples θ+ = µ+ and θ− = µ− . Defining r+ as the reward given by θ+ and r− as the reward given by θ−. We can insert the two samples into Eq. (3) and make use of Eq. (4) to obtain\n∇µiJ(ρ) ≈ i(r + − r−) 2σ2i , (8)\nwhich resembles the central difference approximation used in finite difference methods. Using the same step sizes as before gives the following update equation for the µ terms\n∆µi = α i(r + − r−) 2 . (9)\nThe updates for the standard deviations are more involved. As θ+ and θ− are by construction equally probable under a given σ, the difference between them cannot be used to estimate the σ gradient. Instead we take the mean r ++ r−\n2 of the two rewards and compare it to the baseline reward b. This approach yields\n∆σi = α\n( r+ + r− 2 − b )( 2i − σ2i σi ) (10)\nSyS removes the problem of misleading baselines, and therefore improves the µ gradient estimates. It also improves the σ gradient estimates, since both samples are equally probable under the current distribution, and therefore reinforce each other as predictors of the benefits of altering σ. Even though symmetric sampling requires twice as many histories per update, [1] and [4] have shown that it gives a considerable improvement in convergence quality and time."
    }, {
      "heading" : "2.4 Super-Symmetric Sampling",
      "text" : "While SyS removes the misleading baseline problem for the µ gradient estimate, the σ gradient still uses a baseline and is prone to this problem. On the other hand there is no correct symmetric sample with respect to the standard deviation, because the standard deviation is bounded on the one side to 0 and is unbounded on the positive side. Another problem is that 23 of the samples are on one side of the standard deviation and only 13 on the other - mirroring the\nsamples to the opposite side of the standard deviation in some way, would therefore deform the normal distribution so much, that it would no longer be a close enough approximation to fulfill the assumptions that lead to the PGPE update rules.\nWe therefore chose to define the normal distribution via the mean and the median deviation φ. The median deviation is due to the nice properties of the normal distribution simply defined by: φ = 0.67449 · σ. We can therefore draw samples from the new defined normal distribution: ∼ Nm(0,φ).\nThe median deviation has by construction an equal amount of samples on either side and solves therefore the symmetry problem of mirroring samples. The update rule Eq. (9) stays unchanged while Eq. (10) is only scaled by 10.67449 (the factor that transforms φ in σ) that can be substituted in ασ.\nWhile the update rules stay the same for normal distributed sampling using the median deviation (despite a larger ασ), the median deviation is still also bounded on one side. Because the mirroring cannot be solved in closed form we resort to approximation via a polynomial that can be transfered to an infinite series. We found a good approximation for mirroring samples by:\nai = φi− | i |\nφi , ∗i = sign( i) · φi ·\n{ e c1 |ai| 3−|ai| log(|ai|)\n+c2|ai| if ai ≤ 0 eai/(1.− a3i )c3ai if ai > 0,\n(11)\nwith the following constants: c1 = −0.06655, c2 = −0.9706, c3 = 0.124. This mirrored distribution has a standard deviation of 1.002 times the original standard deviation and looks like depicted in Fig. 1. Fig. 2 shows the regions of samples that are transfered into each other while generating the quasi symmetric samples.\nAdditional to the symmetric sample with respect to the mean hypothesis, now we also can generate two quasi symmetric samples with respect to the median deviation. We named this set of four samples super symmetric samples (SupSyS-samples). They allow for completely baseline free update rules, not only for the µ update but also for the σ updates.\nTherefore the two symmetric sample pairs are used to update µ according to Eq. (9). σ is updated in a similar way by using the mean reward of each symmetric sample pair, there r++ is the mean reward of the original symmetric sample pair and r−− is the mean reward of the mirrored sample pair. The SupSyS update rule for the σ update is given by:\n∆σi = α 2i−σ 2 i σi (r++ − r−−)\n2 . (12)"
    }, {
      "heading" : "3 Experiments and Results",
      "text" : "We use the square function as search space instance with no local optima and the Rastrigin function (see Fig. 8) as search space with exponentially many local optima, to test the different behavior of SupSyS- and SyS-PGPE. The two metaparameters connected with SyS-PGPE as well as with SupSyS-PGPE, namely\nthe step sizes for the µ and σ updates, were optimized for every experiment via a grid search. The Figures 3 to 6 show the means and standard deviations of 200 independent runs each. It can be seen in Fig. 3 that for a search space with no local optima SupSyS-PGPE shows no advantage over standard SyS-PGPE. However, despite using 4 samples per update the performance is also not reduced by using SupSyS-PGPE — the two methods become merely equivalent. The situation changes drastically if the Rastrigin function is used as test function. Not only needs SupSyS-PGPE about half the samples compared to PGPE, the effect seems also to become stronger the higher dimensional the search space\ngets (see Fig. 4 to Fig. 6). We also added SupSyS-PGPE plots with the for SySPGPE optimal (less greedy) meta parameters to show that the effect is not only due to the more aggressive meta parameters. This runs were also more efficient than for PGPE, the effect was however not so distinct.\nIn Fig. 5 we also show a standard PGPE experiment with 4 samples (2 SyS samples — PGPE4smp) instead of 2 to show that the improved performance is not due to the different samples per update. Fig. 5 additionally shows an experiment (SupIf-PGPE) there symmetric samples are only drawn if the first sample(s) result in worse reward than a decaying average baseline. The intuitive idea behind symmetric samples was initially that changing the parameters away from the current sample if the sample resulted in lower than average reward may move the mean hypothesis still in a worse region of the parameter space. Search spaces like the one given in the Rastrigin function can visualize this problem. For SupIf-PGPE one Sample is drawn. If the reward is larger than the baseline then an update is done immediately. If not, a symmetric sample is drawn. Is the mean reward connected with both samples better than the baseline an SySPGPE update is done. If also this mean reward is worse than the baseline, a full SupSyS-PGPE update with 2 additional SyS samples is performed. As can be seen in Fig. 5 the performance is worse by some degree — the difference is however small enough that maybe the optimal baseline approach would improve this method enough to be challenging to SupSyS-PGPE (see also Sec. 4).\nThe optimal meta-parameters are an exponential function of the search space dimension, like to expect, so that we observe a line in the loglog-plot of Fig. 7. For SupSyS-PGPE the meta-parameters are about 2 times larger than for SySPGPE. This is partly because SupSyS-PGPE uses four samples per update instead of two. But the optimal meta-parameters are also larger than for the PGPE4smp experiment so that the symmetric nature of the four SupSyS samples obviously brings additional stability in the gradient estimate than a pure averaging over 4 samples would."
    }, {
      "heading" : "4 Conclusions and Future Work",
      "text" : "We introduced SupSyS-PGPE, a completely baseline free PGPE that uses quasisymmetric samples wrt. the exploration parameters. We showed that on the Rastrigin function, as example for a test function with exponentially many local optima, this novel method is clearly superior to standard SyS-PGPE and that both methods become equivalent in performance if the search space lack distracting local optima.\nFor future work we want to highlight that SupSyS-PGPE can be easily combined with other extensions of PGPE. Multi-modal PGPE [10] can be equipped straight forward with SupSyS sampling. Also the natural gradient used for PGPE in [3] can be defined over the SupSyS gradient instead over the vanilla gradient. If the full 4 super symmetric sample set is only used if the first samples are worse than a baseline (like described as SupIf-PGPE in Sec. 3) a combination with the optimal baseline (described for PGPE in [4]) can yield a superior method to both SupSyS-PGPE and optimal baseline PGPE. Also importance mixing introduced for PGPE by [5] is applicable to SupSyS-PGPE.\nFinally a big open point for future work is the validation of the mere theoretical findings on real world problems, e.g. robotic tasks, for SupSyS-PGPE and its combination with other PGPE extensions."
    } ],
    "references" : [ {
      "title" : "Parameter-exploring policy gradients",
      "author" : [ "F. Sehnke", "C. Osendorfer", "T. Rückstieß", "A. Graves", "J. Peters", "J. Schmidhuber" ],
      "venue" : "Neural Networks 23(4)",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Exploring parameter space in reinforcement learning",
      "author" : [ "T. Rückstieß", "F. Sehnke", "T. Schaul", "D. Wierstra", "Y. Sun", "J. Schmidhuber" ],
      "venue" : "Paladyn. Journal of Behavioral Robotics 1(1)",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Natural Policy Gradient Methods with Parameter-based Exploration for Control Tasks",
      "author" : [ "A. Miyamae", "Y. Nagata", "I. Ono" ],
      "venue" : "NIPS.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Analysis and improvement of policy gradient estimation",
      "author" : [ "T. Zhao", "H. Hachiya", "G. Niu", "M. Sugiyama" ],
      "venue" : "Neural networks : the official journal of the International Neural Network Society",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Efficient sample reuse in policy gradients with parameter-based exploration",
      "author" : [ "T. Zhao", "H. Hachiya", "V. Tangkaratt", "J. Morimoto", "M. Sugiyama" ],
      "venue" : "arXiv preprint arXiv:1301.3966",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Path integral policy improvement with covariance matrix adaptation",
      "author" : [ "F. Stulp", "O. Sigaud" ],
      "venue" : "arXiv preprint arXiv:1206.4621",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Natural evolution strategies",
      "author" : [ "D. Wierstra", "T. Schaul", "J. Peters", "J. Schmidhuber" ],
      "venue" : "Evolutionary Computation, 2008. CEC 2008., IEEE",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
      "author" : [ "R.J. Williams" ],
      "venue" : "Machine Learning 8",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Multimodal parameterexploring policy gradients",
      "author" : [ "F. Sehnke", "A. Graves", "C. Osendorfer", "J. Schmidhuber" ],
      "venue" : "Machine Learning and Applications (ICMLA), 2010 Ninth International Conference on, IEEE",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Policy Gradient (PG) methods that explore directly in parameter space have some major advantages over standard PG methods, like described in [1,2,3,4,5,6] and [7] and have therefore drawn a lot of attention in the last years.",
      "startOffset" : 141,
      "endOffset" : 154
    }, {
      "referenceID" : 1,
      "context" : "Policy Gradient (PG) methods that explore directly in parameter space have some major advantages over standard PG methods, like described in [1,2,3,4,5,6] and [7] and have therefore drawn a lot of attention in the last years.",
      "startOffset" : 141,
      "endOffset" : 154
    }, {
      "referenceID" : 2,
      "context" : "Policy Gradient (PG) methods that explore directly in parameter space have some major advantages over standard PG methods, like described in [1,2,3,4,5,6] and [7] and have therefore drawn a lot of attention in the last years.",
      "startOffset" : 141,
      "endOffset" : 154
    }, {
      "referenceID" : 3,
      "context" : "Policy Gradient (PG) methods that explore directly in parameter space have some major advantages over standard PG methods, like described in [1,2,3,4,5,6] and [7] and have therefore drawn a lot of attention in the last years.",
      "startOffset" : 141,
      "endOffset" : 154
    }, {
      "referenceID" : 4,
      "context" : "Policy Gradient (PG) methods that explore directly in parameter space have some major advantages over standard PG methods, like described in [1,2,3,4,5,6] and [7] and have therefore drawn a lot of attention in the last years.",
      "startOffset" : 141,
      "endOffset" : 154
    }, {
      "referenceID" : 5,
      "context" : "Policy Gradient (PG) methods that explore directly in parameter space have some major advantages over standard PG methods, like described in [1,2,3,4,5,6] and [7] and have therefore drawn a lot of attention in the last years.",
      "startOffset" : 141,
      "endOffset" : 154
    }, {
      "referenceID" : 6,
      "context" : "Policy Gradient (PG) methods that explore directly in parameter space have some major advantages over standard PG methods, like described in [1,2,3,4,5,6] and [7] and have therefore drawn a lot of attention in the last years.",
      "startOffset" : 159,
      "endOffset" : 162
    }, {
      "referenceID" : 0,
      "context" : "The basic method from the field of Parameter Exploring Policy Gradients (PEPG) [8], Policy Gradients with Parameter-based Exploration (PGPE) [1], uses two samples that are symmetric around the current hypothesis to circumvent misleading reward in asymmetrical reward distributed problems, gathered with the usual baseline approach.",
      "startOffset" : 141,
      "endOffset" : 144
    }, {
      "referenceID" : 3,
      "context" : "[4] shows that Symmetric Sampling (SyS) is superior even to the optimal baseline.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "While the optimal baseline improved this issue substantially, like shown again by [4], it is likely that removing the baseline altogether by a SyS wrt.",
      "startOffset" : 82,
      "endOffset" : 85
    }, {
      "referenceID" : 0,
      "context" : "We show how the method relates to SyS and sampling with a baseline, thereby summarizing the derivation from [1] for SyS and baseline sampling PGPE.",
      "startOffset" : 108,
      "endOffset" : 111
    }, {
      "referenceID" : 0,
      "context" : "To stay conform with the nomenclature of [1] and [4], we assume a Markovian environment that produces a cumulative reward r for a fixed length episode, history, trajectory or roll-out.",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 3,
      "context" : "To stay conform with the nomenclature of [1] and [4], we assume a Markovian environment that produces a cumulative reward r for a fixed length episode, history, trajectory or roll-out.",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 0,
      "context" : "This reduction in samples-per-history is what reduces the variance in the gradient estimate (see [1] for details).",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 0,
      "context" : "We name the distribution over parameters in accordance with [1] ρ.",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 7,
      "context" : "Following [9], we obtain a cheaper gradient estimate by drawing a single sample θ and comparing its reward r to a baseline reward b given e.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 7,
      "context" : "If, as in [9], we use a step size αi = ασ 2 i in the direction of positive gradient (where α is a constant) we get the following parameter update equations:",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 3,
      "context" : "[4] showed recently that an optimal baseline can be achieved for PGPE and the algorithm converges significantly faster with an optimal baseline of the form:",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 0,
      "context" : "Even though symmetric sampling requires twice as many histories per update, [1] and [4] have shown that it gives a considerable improvement in convergence quality and time.",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 3,
      "context" : "Even though symmetric sampling requires twice as many histories per update, [1] and [4] have shown that it gives a considerable improvement in convergence quality and time.",
      "startOffset" : 84,
      "endOffset" : 87
    }, {
      "referenceID" : 8,
      "context" : "Multi-modal PGPE [10] can be equipped straight forward with SupSyS sampling.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 2,
      "context" : "Also the natural gradient used for PGPE in [3] can be defined over the SupSyS gradient instead over the vanilla gradient.",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 3,
      "context" : "3) a combination with the optimal baseline (described for PGPE in [4]) can yield a superior method to both SupSyS-PGPE and optimal baseline PGPE.",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 4,
      "context" : "Also importance mixing introduced for PGPE by [5] is applicable to SupSyS-PGPE.",
      "startOffset" : 46,
      "endOffset" : 49
    } ],
    "year" : 2013,
    "abstractText" : "Policy Gradient methods that explore directly in parameter space are among the most effective and robust direct policy search methods and have drawn a lot of attention lately. The basic method from this field, Policy Gradients with Parameter-based Exploration, uses two samples that are symmetric around the current hypothesis to circumvent misleading reward in asymmetrical reward distributed problems gathered with the usual baseline approach. The exploration parameters are still updated by a baseline approach leaving the exploration prone to asymmetric reward distributions. In this paper we will show how the exploration parameters can be sampled quasi symmetric despite having limited instead of free parameters for exploration. We give a transformation approximation to get quasi symmetric samples with respect to the exploration without changing the overall sampling distribution. Finally we will demonstrate that sampling symmetrically also for the exploration parameters is superior in needs of samples and robustness than the original sampling approach.",
    "creator" : "LaTeX with hyperref package"
  }
}