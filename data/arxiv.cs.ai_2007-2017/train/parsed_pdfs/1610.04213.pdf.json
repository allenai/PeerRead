{
  "name" : "1610.04213.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Reset-free Trial-and-Error Learning for Data-Efficient Robot Damage Recovery",
    "authors" : [ "Konstantinos Chatzilygeroudis", "Vassilis Vassiliades", "Jean-Baptiste Mouret" ],
    "emails" : [ "jean-baptiste.mouret@inria.fr", "konstantinos.chatzilygeroudis@inria.fr,", "vassilis.vassiliades@inria.fr,", "jean-baptiste.mouret@inria.fr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms— Robotics in Hazardous Fields, Learning and Adaptive Systems, Damage Recovery\nI. INTRODUCTION\nDuring the recent DARPA Robotics Challenge (2015), many robots had to be “rescued” by humans because of hardware failures [1], [2]; this is paradoxical for robots that were designed to operate in environments that are too risky for humans! While these robots could certainly have been more robust and could have prevented some falls [1], even the best robots will encounter unforeseen situations: hardware failures will always be a possibility, especially with highly complex robots in post-disaster environments [3], [4]. For instance, C. Atkeson et al. report that the Atlas robot they used in the DARPA Robotics challenge had a “mean time between failures of hours or, at most, days” [1].\nThe traditional method for damage recovery is to first diagnose the failure [5], then update the plans to bypass it [6]. Nevertheless, conceptually, the probability of failing should grow exponentially with the complexity of the robot (e.g. a Roomba vs a humanoid) and of the environment (e.g. an empty arena vs a post-earthquake building); accurate diagnosis therefore becomes more and more challenging and\n*Corresponding author: jean-baptiste.mouret@inria.fr All authors have the following affiliations: - Inria, Villers-lès-Nancy, F-54600, France - CNRS, Loria, UMR 7503, Vandœuvre-lès-Nancy, F-54500, France - Université de Lorraine, Loria, UMR 7503, Vandœuvre-lès-Nancy, F-54500, France Emails: konstantinos.chatzilygeroudis@inria.fr, vassilis.vassiliades@inria.fr, jean-baptiste.mouret@inria.fr\nrequires more and more internal sensors, which, in turn, increases the complexity and the cost of the robots.\nTo overcome these challenges, robots can avoid the diagnosis step and directly learn a compensatory behavior by trial and error [7], [8], [9], [10]. In that case, damage recovery is a reinforcement learning (RL) problem in which a damaged robot has to maximize its performance for the task at hand [11]. State-of-the-art RL algorithms, however, are either too constrained to be easily used with arbitrary robots and arbitrary tasks [12], or they require too many trials that prohibit realistic damage recovery [11].\nFortunately, damage recovery is not a standard RL problem: standard algorithms assume that the robot starts with no knowledge, whereas a trial-and-error damage recovery algorithm can rely on the knowledge of the intact robot [9], [13], [14], [8]. Following this idea, the Intelligent Trial &\nar X\niv :1\n61 0.\n04 21\n3v 1\n[ cs\n.R O\n] 1\n3 O\nct 2\n01 6\nError algorithm (IT&E) allowed, for instance, a hexapod robot to learn to walk after several injuries with a dozen of episodes [8].\nThe main limitation of IT&E and other state-of-the-art algorithms for robot learning is their reliance on episodes [11]: after each trial, the robot needs to be reset to the same state. While this reset is often not a problem for a manipulator, it prevents mobile robots (e.g. a stranded mobile manipulator or a legged robot) to exploit this kind of algorithms to recover from damage in real-world situations.\nIn this paper, we introduce a new learning algorithm that allows robots to learn with a few trials in realistic environments (e.g. with obstacles) without any reset between episodes (Fig. 1). We call this algorithm “Reset-free Trialand-Error” (RTE). It combines three components (Fig. 2): (1) a pre-computed action repertoire of the intact robot that also creates a mapping between the task space and the parameters of the low-level controller (generated by MAP-Elites [15] in our case, Fig. 2A); (2) a probabilistic learning model (Gaussian processes in our case) that corrects the outcome of the actions for the damaged robot (Fig. 2B1); and, (3) a probabilistic planner (Monte Carlo Tree Search in our case) that selects the next action to execute, based on the predictions of the probabilistic model, the environment, the current state of the robot, and the target state (Fig. 2B2). Thanks to this algorithm, robots “learn while doing” instead of “learning and then doing”. We evaluate our algorithm on a hexapod robot that has to sequentially reach target points (Fig. 1) in spite of being damaged in an unknown way."
    }, {
      "heading" : "II. STATE-OF-THE-ART",
      "text" : ""
    }, {
      "heading" : "A. Learning in Robotics",
      "text" : "While there is a consensus that robots should be able to learn new tasks and improve their behavior over time, there is much less agreement on the best place to insert learning in a robot architecture. Many approaches rely on supervised learning algorithms that learn forward or inverse models. Once learned, such models can be combined with control or planning algorithms to achieve the task at hand. A critical aspect of model learning is data acquisition: supervised learning algorithms need labeled data, but random\nbabbling is often insufficient to generate behaviors that are interesting for robots [16], [17]; for instance, with a robotic manipulator, random movements are unlikely to generate grasp-like behaviors. Active learning can help alleviating this issue by exploring behaviors that improve the model “at the right place” [17].\nInstead of learning a model, robots can use a RL algorithm to discover how to behave [11]. Classic RL approaches are, however, designed for discrete state and action spaces [18], [11], whereas robots almost always have to solve continuous tasks, for example balancing by controlling the joint torques. A popular alternative is to view RL as an optimization of the parameters of a policy [19], which can be solved with gradient-based methods [20], [11], evolutionary algorithms [21] or other optimization methods like Bayesian Optimization [8], [22], [23]. Nevertheless, even the most data-efficient direct policy search algorithms typically require hundreds of trials (e.g. about 100 trials to learn the 4 parameters that control a minimalistic biped robot [22]), and most of them are episodic, meaning that the robot has to be put back in the same state after each trial.\nA few algorithms combine ideas from model learning and from RL. In particular, PILCO is a RL algorithm that is based on learning a probabilistic dynamics model, which is then used to optimize a policy [12]. This algorithm provides a data-efficient approach for robot learning (for instance, less than 5 trials are needed to learn how to stabilize a cart pole). Nonetheless, PILCO has a very high computational cost (after a dozen of episodes, PILCO typically needs a few hours to select the next trial, see [24]), works only for lowdimensional state spaces (10-20 dimensions), and strongly constrains the type of policy [22] (for instance, it is unclear if PILCO can be used to learn a walking controller like in [22], [23], [8])."
    }, {
      "heading" : "B. Fault Tolerance and Recovery in Robotics",
      "text" : "Classic approaches to fault tolerance rely on updating the model of the robot, either directly with a self-diagnosis [25], or indirectly with machine learning [26], [5]; the model is then used for planning and/or control. For instance, if a hexapod robot detects that one of its legs is not working\nas expected, it can stop using it and adapt the controller to use only the working legs [27]. However, because of the many perceptual ambiguities on a robot, these approaches need many sensors and/or strong hypotheses about the kind of possible faults.\nAn alternative approach is to let a damaged robot learn a new policy by trial-and-error, which bypasses the need for a diagnosis [7], [9], [8], [10]. In this line of work, the biggest challenge is to design algorithms that are as data-efficient as possible, because too many trials might both damage the robot even more, and are likely to make learning too slow to be used in real-world situations. To minimize the number of trials, several algorithms rely on the transferability hypothesis [8], [9]: the behaviors that do not use the damaged parts are likely to be similar with the damaged and the undamaged robot, therefore simulations of the intact robot can help searching for a new behavior on the damaged robot. Starting with this hypothesis, the IT&E algorithm [8] exploits a dynamic simulation of the intact robot to create a behaviorperformance map that predicts the performance of thousands of different behaviors. If a damage occurs, this map is used as a prior for a Bayesian optimization algorithm that searches for a compensatory behavior [28], [23], [22]. Overall, the experimental results show that IT&E can allow various types of robots (a hexapod robot and an 8-DOF manipulator) to compensate for many different types of injuries in less than 2 minutes [8]."
    }, {
      "heading" : "III. PROBLEM FORMULATION",
      "text" : "Our problem can be cast in the general framework of Markov Decision Processes (MDP) [18]. An MDP is a tuple (S,A, T, r), where S is the state space (continuous or discrete), A is the action space (continuous or discrete), T (st,at, st+1) is the state transition function specifying the probability of transitioning to state st+1 ∈ S when the agent takes action at ∈ A in state st ∈ S, and r : S → R is the immediate reward function (which defines the task of the agent), with r(st+1) being the immediate reward of state st+1 and st+1 may contain both internal variables (such as body position) and external variables (such as obstacles). The objective of the agent (i.e., the robot) is to find a deterministic policy π, i.e., a mapping from states to actions, at = π(st), that maximizes its expected discounted return:\nJπ = E [ ∞∑ t=0 γtr(st+1) ∣∣∣π] (1)\nwhere γ ∈ [0, 1) is a factor that discounts future rewards. T and r describe the environmental dynamics and they are collectively known as the model of the environment. If the agent has access to this model, it can use a planning algorithm to find the optimal policy.\nIn our setting, the robot needs to execute a sequence of related tasks G1, G2, . . . , Gn, each one being a shortest path problem:\nr(st) =  Rgoal if st = goal(Gi)−Rterm if st = terminal(Gi) 0 otherwise\n(2)\nwhere Rgoal > 0, Rterm ≥ 0, goal(Gi) returns the goal state of task Gi, and terminal(Gi) returns a non-goal, terminal state of task Gi, e.g. a colliding state. When st = goal(Gi), the robot finishes task Gi and starts executing task Gi+1."
    }, {
      "heading" : "IV. APPROACH",
      "text" : ""
    }, {
      "heading" : "A. Learning the Action Repertoire",
      "text" : "Controllers for complex robots, for instance legged robots, usually involve numerous parameters, which makes them challenging to learn with a few trials. We circumvent this issue by using the transferability hypothesis and learn controllers with a simulated intact robot (sec. II-B). The learned repertoire will be refined online after each action executed by the damaged robot (sec. IV-B).\nAlgorithm 1 MAP-Elites 1: procedure MAP-ELITES 2: (P← ∅,Θ← ∅) . Performance and feature grids 3: for i = 1→ G do . Initialization: G random θ 4: θ = random solution() 5: add to repertoire(θ,P,Θ) 6: for i = 1→ I do . Main loop, I iterations 7: θ = random selection(Θ) 8: θ′ = random variation(θ) 9: add-to-repertoire(θ′,P,Θ) return repertoire and performance (Θ, P) 10: procedure ADD-TO-REPERTOIRE(θ,P,Θ) 11: a = action descriptor(θ) . Use the forward model 12: p = performance(θ) . Use the forward model 13: if P(a) = ∅ or P(a) < p then . Replace if better 14: P(a) = p 15: Θ(a) = θ\nWe here assume that the robot is controlled by a low-level controller that is parameterized by a vector θ ∈ Rd and that each action corresponds to a different value of θ. We also assume that each point of the task space can be described by a vector a ∈ Rna , which we call “action descriptor”. We would like to create a repertoire that covers the task space as well as possible [8], [29], [30], i.e., to both determine a good set of actions A and a mapping between A and Θ (A→ Θ). Typical repertoires contain about a thousand actions and are organized on a grid so that similar actions are close.\nIf we take a robotic manipulator as an example, the parameter space could be joint positions, the task space could be the (x, y, z) coordinates of the end-effector, and the repertoire will map (x, y, z) positions to joint positions, that is, it would be a discrete representation of the inverse kinematics of the arm. Nonetheless, while an inverse kinematics solver could be used to create a repertoire for a manipulator, most robots do not have access to such inverse models. This is, in particular, the case of walking robots.\nAs a consequence, instead of using an inverse model, we learn the action repertoire with an iterative algorithm called MAP-Elites [15], [8] and a forward model (e.g. a dynamic simulator). Like for the inverse kinematics of redundant\nmanipulators, the mapping from the parameter space to the task space is typically many-to-one. Thus, we need to define a performance function to select the best θ for each point of the task space.\nEssentially, MAP-Elites discretizes the na-dimensional task space to an na-dimensional grid, and then attempts to fill each of the cells thanks to a variation-selection loop [15], [8]. Algorithmically, it starts with G random parameter vectors, simulates the robot with these new parameters, and records both the position of the robot in the task space and the performance (Algo. 1, 3-5). If the cell is free, then the algorithm stores the parameter vector in that cell; if it is already occupied, then the algorithm compares the performance values and keeps only the best parameter vector (Algo. 1, 10-15). Once this initialization is done, MAP-Elites iterates a simple loop (Algo. 1, 6-9): (1) randomly select one of the occupied cell, (2) add a random variation to the parameter vector, (3) simulate the behavior, (4) try to insert the new parameter in the grid.\nWhile MAP-Elites is computationally expensive, it is easy to parallelize and can run on large clusters before deploying the robot. So far, it has been successfully used to generate behaviors for legged robots [8], wheeled robots controlled by neural networks [30], and even adversarial images for deep neural networks [31]."
    }, {
      "heading" : "B. Learning with Gaussian Processes",
      "text" : "MAP-Elites provides not only the set of actions to be used by the policy, but also a prior on how an action modifies the state variables, i.e., a mapping from actions to relative outcomes, f : A → O. These actions are well-organized in the task space, thus, it is possible to define distances between them. Since this prior comes from a simulator, it is only an approximation. More importantly, the simulator uses a model of the intact robot, whereas the real one is damaged. Therefore, to make the real robot perform well, there needs to be a way to correct this mapping.\nTo do so, we use n Gaussian Processes (GP) (where n is the number of dimensions of O) with a mean function that corresponds to the prior provided by MAP-Elites. In other words, the mapping computed with the simulator serves as a prior for the GPs.\nA GP is an extension of the multivariate Gaussian distribution to an infinite-dimension stochastic process for which any finite combination of dimensions will be a Gaussian distribution [28]. It is a distribution over functions, completely specified by its mean function, µ(·) and covariance function, k(·, ·):\nf(a) ∼ GP (µ(a), k(a,a′)) (3)\nAssuming D1:t = {f(a1), · · · , f(at)} is a set of observations, M(·) is the mean from the simulated prior and σ2n the sampling noise, the GP is computed as follows:\np(f(a)|D1:t,a) = N (µt(a), σ2t (a)) (4)\nwhere:\nµt(a) = M(a) + k >(K + σ2nI) −1(D1:t −M(a1:t)) (5)\nσ2t (a) = k(a,a)− k>(K + σ2nI)−1k (6) where K is the kernel matrix with entries Kij = k(ai,aj) and k = k(D1:t,at)."
    }, {
      "heading" : "C. Probabilistic Optimal Planning using MCTS",
      "text" : "We are interested in solving an MDP with an action set that contains thousands of actions. Since GPs are probabilistic models, they provide both a prediction and the uncertainty over each prediction, which can be exploited by probabilistic planners. Here we use Monte Carlo Tree Search (MCTS) [32], as it has already been succesfully used to solve (Partially Observable)-MDPs with stochastic transition functions [33], [34], continuous state spaces, and high branching factors [34], [35].\nMCTS is a best-first, sample-based search algorithm for finding optimal decisions in a given domain by taking random samples in the decision space and building a search tree according to the results. Every state in the search tree is evaluated by the average outcome of Monte-Carlo rollouts from that state. These rollouts are typically random or directed by a simple, domain-dependent heuristic [34].\nAlgorithm 2 General Monte Carlo Tree Search 1: procedure MCTS-SEARCH(s0) 2: while within computational budget do 3: s = s0 4: do 5: a = SelectionPolicy(s) 6: Children(s) = Children(s) ∪ (s,a) 7: (s′, ρ) = ExpansionPolicy(s,a) . see [35] 8: Children(s,a) = Children(s,a) ∪ s′ 9: R(s,a) = ρ 10: s = s′ 11: while n(s) > 0 and s not a terminal state . n(·) returns the number of visits of a state 12: ∆ = Rollout(s) . Use GPs (sec. IV-B, V-D) 13: BackUp(s,∆, R)\nreturn BestChild(s0)\nMCTS (Algo. 2) is an anytime planning algorithm, i.e., it runs until some predefined computational budget — typically a time, memory or iteration constraint — is reached, at which point the search is halted and the best-performing root action is returned. Four steps are applied per search iteration: • SelectionPolicy: Starting at the root node, a child selec-\ntion policy is recursively applied to descend through the tree until the most urgent expandable node is reached. • ExpansionPolicy: One child node, along with the state’s associated reward ρ = r(s), is added to expand the tree, according to the available actions. • Rollout: A rollout is run from the new node according to the default policy to get an estimate value for this node, ∆. We do this by constructing a generative model using the prediciton of the GPs. • BackUp: The rollout result is “backed up” through the selected nodes to update their statistics."
    }, {
      "heading" : "D. Reset-free Trial-and-Error Learning Algorithm",
      "text" : "Connecting all the pieces together, RTE first generates an action repertoire with the MAP-Elites algorithm (Algo. 3, lines 2-3); then, while in mission, it re-plans at each step using MCTS and the current belief of the outcome of the actions (prediction of the GPs), taking into account the uncertainty of the predictions and potential final states (e.g. collisions with obstacle) (lines 9-13); at the end of each step, the GPs are updated with the recorded data (lines 14-15).\nAlgorithm 3 Reset-free Trial-and-Error Learning 1: procedure RTE 2: Create Action Repertoire, A, with MAP-Elites (sec. IV-A) 3: Construct mean function M from MAP-Elites data 4: for i = 1→ dim(O) do 5: GPi : A→ Oi with Mi as prior (sec. IV-B, V-C) 6: while in mission and stopping criteria not met do 7: RTE-STEP(t) 8: t = t+ 1\n9: procedure RTE-STEP(t) 10: st = state of robot at time t 11: at+1 = MCTS-SEARCH(st) (sec. IV-C, V-D) 12: f(at+1) = execute action(at+1) . Execute the\naction and observe its outcome 13: D1:t+1 = {D1:t, f(at+1)} 14: for i = 1→ dim(O) do 15: Update GPi using D1:t+1 (sec. IV-B)"
    }, {
      "heading" : "V. EXPERIMENTAL SETUP",
      "text" : "To investigate the performance of our algorithm, we use a hexapod robot that has to sequentially reach random equidistant target points in an environment (Fig. 1). We perform experiments both in simulation (to gather extensive statistics) and on a physical robot."
    }, {
      "heading" : "A. Parametric Low-level Controller",
      "text" : "The low-level controller is the same as in [8], [29]. It is intentionally kept simple so that this paper can focus on the learning algorithm.\nIn this open-loop controller, the position of the first two joints of each of the 6 legs is governed by a periodic function with 3 parameters: a phase shift, an offset, and an amplitude (the frequency is fixed). The position of the third joint of each leg is the opposite of the position of the second one, so that the last segment is always vertical. This results in 36 realvalued parameters. At each step of the learning algorithm, the low-level controller is executed for 3 seconds with the specified parameters."
    }, {
      "heading" : "B. Learning the Action Repertoire",
      "text" : "The task of the robot is to reach points in Cartesian space (x, y), therefore MAP-Elites should produce a repertoire of actions that each reach a different point in the Cartesian space. As many controllers can reach the same position, we select those that make the robot follow a continuouscurvature trajectory and for which the body points towards\nthe tangent of the overall trajectory at the end of 3-second period. The MAP-Elites performance function for the ith individual is therefore:\npi = |αi − αd| (7)\nwhere αi is the yaw angle of the robot and αd is the desired yaw angle of the robot at the end of the movement. To describe the circular trajectories we only need to keep the (x, y) position of the robot at the end of the movement (since we can compute the desired angle for any point in the 2-D space). Thus, the 2-D descriptor of the ith individual is:\nai = [ x− xmin\nxmax − xmin , y − ymin ymax − ymin ] (8)\nwhere xmin, xmax, ymin and ymax are the boundaries of the reachable space ([−2, 2] meters in our case). To avoid depending on a specific repertoire, we run MAP-Elites two times for 100000 evaluations, leading to two distinct repertoires with about 1500 different actions each (Fig. 3). We use the Sferes2 framework [36] and the DART simulator1 for our implementation."
    }, {
      "heading" : "C. Learning with Gaussian Processes",
      "text" : "The GP inputs are the 2-D descriptors of the actions, and the outputs are predictions of the relative x, y and α displacements. To avoid angle discontinuities, instead of learning the raw angle α we learn the cosα and the sinα. Thus, we learn a mapping from actions to relative outcomes:\na→ (∆x,∆y, cos∆α, sin∆α) (9)\nWe use the Squared Exponential Kernel as the covariance function [37]. We use the limbo C++11 library2 for GP regression."
    }, {
      "heading" : "D. Solving the problem with MCTS",
      "text" : "We instantiate MCTS for our problem making the following choices:\n1https://dartsim.github.io 2https://github.com/resibots/limbo\na) Selection Policy: We choose the Simple Progressive Widening (SPW) selection policy by Rolet et al. [38] that properly handles cases where there is a large number of actions or the action space is continuous.\nb) Expansion Policy: We choose the Double Progressive Widening (DPW) expansion policy by Couëtoux et al. [39] that properly handles cases where the state space is continuous.\nc) Action Sampling Policy: We use A* on a simplified problem to guide the sampling procedure (see sec. V-E)\nd) Generative Model: We construct the generative model using the prediciton of the GPs (with their variance) [35].\ne) Default Policy for evaluation: We sample random actions from the repertoire [34].\nf) Best child criterion: We use Greedy Selection, i.e., we select the action that has the maximum average value [34].\ng) Reward function: We define Rgoal = 10, reward for reach the goal, and Rterm = 1000, penalty for colliding, for each target point.\nTo make the search faster, we implemented root parallelization in MCTS [40] and we ran 4 parallel trees and gave to each one a budget of 5000 iterations. This implementation is available in our C++14 lightweight and generic MCTS library3."
    }, {
      "heading" : "E. Guiding MCTS using A* on a simplified problem",
      "text" : "MCTS traditionally samples actions randomly. To speed it up, we first discretize the space and create a grid map; then, we simulate a virtual point robot with 8 actions (one for\n3https://github.com/resibots/mcts\neach neighboring cell — allowing diagonal moves) and solve the path planning problem using A*. Solving this simplified task is very low-computational. We keep only the first step of the optimized path and use it to know an approximate direction for the next MCTS action. Next, we sample N (100 in our case) random actions from the repertoire and return the one that best matches this direction. Note that we are using the prediction of the GPs to decide which action we should choose. This simple procedure reduces the running time of MCTS to almost being real-time (a few seconds to choose the next action) and still get high-quality results.\nWe use this “trick” because our problem is path-planning, but similar ones can be used in other problems. More generic approaches would be the Blind Value action sampling approach or cRAVE [35], [41]."
    }, {
      "heading" : "VI. RESULTS",
      "text" : ""
    }, {
      "heading" : "A. Simulation results",
      "text" : "We count the number of steps (3s actions) required to sequentially reach 30 equidistant (distance of 3.5m and 3m for each environment respectively) random targets and we count the number of collisions with obstacles (including the walls of the arena). We compare RTE (sec. IV) to MCTSbased planning (with the same action repertoire as RTE). This second treatment corresponds to a classic planning algorithm with a re-planning after each step. We investigate 3 different types of damages (leg shortenings and removals), 2 different environments, and 2 different action repertoires (Fig. 4). Each scenario is replicated 50 times for statistics.\nThe results show that RTE requires fewer steps to reach each target and leads to fewer collisions than the re-planning algorithm (Fig. 4): compared to MCTS alone, RTE needs between 1.18 (in the easiest damage, Fig. 4A) and 2.04 (in\nthe hardest damage, Fig. 4C) times fewer steps to reach each target, and it leads to between 1.30 (in the medium damage, Fig. 4B) and 5.45 (in the hardest damage, Fig. 4C) times fewer collisions. Further analysis shows that the median number of steps to reach each target decreases over time when the robot uses RTE, whereas it stays constant with MCTS alone (Fig. 5). After the first few targets (2-4), RTE is able to make the robot reach each target in around 30s compared to MCTS alone that needs around 50− 60 s."
    }, {
      "heading" : "B. Physical robot results",
      "text" : "We then evaluated RTE on the physical robot with a single damage (we removed the right middle leg), on two environments (with and without a central obstacle) and a single action repertoire; the number of targets to reach were 10 and 5 targets for each environment respectively and the distance between the targets was 2 √ 2m. Each scenario is replicated 5 times. The environment (location of the obstacles) and the position of the robot is acquired using a Motion Capture system (Optitrack), although a SLAM and/or a visual odometry algorithm could have been used [8], [42].\nThe results show that RTE needs fewer steps to reach each target (Environment 1: 13.0 steps, 25th and 75th percentiles [12.0, 14.0], Environment 2: 18.0 steps, [14.0, 19.0]) than MCTS alone (Environment 1: 28.0 steps, [24.0, 31.0], Environment 2: 25.0 steps, [24.0, 43.0]) (Fig. 6). These results are consistent with the simulations, but learning makes a bigger difference with the physical robot. The robot using RTE is able to reach each target in less than a minute (35 s and 55 s for each map respectively), whereas with MCTS alone it needs around 80 s. A demonstration of our approach on the real robot is available in the supplementary video (also available at https://youtu.be/XIETxEpnwSs)."
    }, {
      "heading" : "VII. DISCUSSION AND CONCLUSION",
      "text" : "With robots, like with many complex systems, “we should not wonder if some mishap may happen, but rather ask what\none will do about it when it occurs” [43]. This advice is especially important if we want to be able to send advanced and expensive robots in risky places like a destroyed nuclear plant [4], even if the robots are tele-operated. In such situations, a damaged robot would greatly benefit from lastresort algorithms that would allow it to at least come back to its operators.\nThe RTE learning algorithm makes it possible for robots to overcome such failures in a few minutes. We successfully tested it on a hexapod robot that was damaged in several ways. Contrary to most previous work, our algorithm does not require to put the robot back at the same position after each trial: the robot learns autonomously, while taking into account its environment (obstacles). To our knowledge, this learning algorithm is one of the first ones that allows a physical legged robot to learn to walk without any human intervention, especially when there are obstacles.\nUltimately, RTE should run continuously on the robot to compensate for potential wear or damage, that is, it should be a continuous learning algorithm and not a damage recovery one. However, the current version has a bottleneck: the computational complexity of the prediction of the GPs is cubic in the number of samples, which prevents the robot to use more than a few hundred steps. In future work, we will therefore consider using a time-window to keep this complexity low and/or using sparse GPs [44].\nIn these first experiments, we assumed a perfect knowledge of the position of the obstacles, as well as a perfect knowledge of the position of the robot (via a motion capture system). In the future, we will design a more realistic experiment in which the robot discovers its environment with a SLAM algorithm [42]. To do so, we will take into account the uncertainty of the map in the planning phase (MCTS).\nWhile we performed our experiments with a legged robot, the algorithm introduced here is general enough to be extended to many other robots and tasks. For instance, it could be employed on an arm mounted on a mobile platform that would suffer from some damage (e.g., a blocked joint). In that case, the algorithm will learn a mapping between\nthe (x,y,z) position of the end-effector and the joint/wheel positions, in a similar way as it learned a mapping between the (x,y) position of the hexapod robot and the parametric controller. After each trial, the robot might be in a different position with regards to the target object (e.g. a door knob), but thanks to RTE, it should not have to go back to its starting position to try a different approach.\nAPPENDIX For the squared exponential kernel in the GPs we used l = 0.03. For MCTS we used the following paramaters: α = 0.5 for SPW selection policy, β = 0.6 for the DPW expansion policy, c = 150 for the UCT value and γ = 0.9 for the discount factor. The error threshold for reaching a target was 0.25m for the simulation and 0.2m for the physical robot. The source code of the experiments can be found in http: //members.loria.fr/JBMouret/src/ral_icra2017.tgz."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "This work received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement number 637972, project “ResiBots”). The authors would like to thank Dorian Goepp, Antoine Cully, and Olivier Buffet."
    } ],
    "references" : [ {
      "title" : "No falls, no resets: Reliable humanoid behavior in the DARPA robotics challenge",
      "author" : [ "C. Atkeson" ],
      "venue" : "Proc. of IEEE Humanoid, 2015.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "How UGVs physically fail in the field",
      "author" : [ "J. Carlson", "R.R. Murphy" ],
      "venue" : "IEEE Trans. on Robotics, vol. 21, no. 3, pp. 423–437, 2005.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Fukushima robot operator writes tell-all blog",
      "author" : [ "E. Guizzo" ],
      "venue" : "IEEE Spectrum, August, vol. 23, 2011.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Real-time fault diagnosis",
      "author" : [ "V. Verma", "G. Gordon", "R. Simmons", "S. Thrun" ],
      "venue" : "Robotics & Automation Magazine, IEEE, vol. 11, no. 2, pp. 56–66, 2004.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Generation of whole-body optimal dynamic multi-contact motions",
      "author" : [ "S. Lengagne", "J. Vaillant", "E. Yoshida", "A. Kheddar" ],
      "venue" : "Int. Journal of Robotics Research, vol. 32, no. 9-10, pp. 1104–1119, 2013.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Fault-tolerant gait learning and morphology optimization of a polymorphic walking robot",
      "author" : [ "D.J. Christensen", "J.C. Larsen", "K. Stoy" ],
      "venue" : "Evolving Systems, pp. 1–12, 2013.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Robots that can adapt like animals",
      "author" : [ "A. Cully", "J. Clune", "D. Tarapore", "J.-B. Mouret" ],
      "venue" : "Nature, vol. 521, no. 7553, pp. 503–507, 2015.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Fast Damage Recovery in Robotics with the T-Resilience Algorithm",
      "author" : [ "S. Koos", "A. Cully", "J.-B. Mouret" ],
      "venue" : "Int. Journal of Robotics Research, vol. 32:14, pp. 1700–1723, 2013.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Multiple chaotic central pattern generators with learning for legged locomotion and malfunction compensation",
      "author" : [ "G. Ren", "W. Chen", "S. Dasgupta", "C. Kolodziejski", "F. Wörgötter", "P. Manoonpong" ],
      "venue" : "Information Sciences, vol. 294, pp. 666–682, 2015.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Reinforcement learning in robotics: A survey",
      "author" : [ "J. Kober", "J.A. Bagnell", "J. Peters" ],
      "venue" : "Int. Journal of Robotics Research, vol. 32, no. 11, pp. 1238–1274, 2013.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Gaussian processes for data-efficient learning in robotics and control",
      "author" : [ "M.P. Deisenroth", "D. Fox", "C.E. Rasmussen" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell., vol. 37, no. 2, pp. 408–423, 2015.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Efficient reinforcement learning for robots using informative simulated priors",
      "author" : [ "M. Cutler", "J.P. How" ],
      "venue" : "Proc. of ICRA, 2015.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Combining modelbased policy search with online model learning for control of physical humanoids",
      "author" : [ "I. Mordatch", "N. Mishra", "C. Eppner", "P. Abbeel" ],
      "venue" : "Proc. of ICRA, 2016.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Illuminating search spaces by mapping elites",
      "author" : [ "J.-B. Mouret", "J. Clune" ],
      "venue" : "arXiv preprint arXiv:1504.04909, 2015.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Model learning for robot control: a survey",
      "author" : [ "D. Nguyen-Tuong", "J. Peters" ],
      "venue" : "Cognitive Processing, vol. 12, no. 4, pp. 319–340, 2011.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Active learning of inverse models with intrinsically motivated goal exploration in robots",
      "author" : [ "A. Baranes", "P.-Y. Oudeyer" ],
      "venue" : "Robotics and Autonomous Systems, vol. 61, no. 1, pp. 49–73, 2013.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Reinforcement learning: An introduction",
      "author" : [ "R.S. Sutton", "A.G. Barto" ],
      "venue" : "MIT press,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1998
    }, {
      "title" : "Robot skill learning: From reinforcement learning to evolution strategies",
      "author" : [ "F. Stulp", "O. Sigaud" ],
      "venue" : "Paladyn, Journal of Behavioral Robotics, vol. 4, no. 1, pp. 49–61, 2013.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Reinforcement learning of motor skills with policy gradients",
      "author" : [ "J. Peters", "S. Schaal" ],
      "venue" : "Neural Networks, vol. 21, no. 4, pp. 682–697, 2008.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Encouraging behavioral diversity in evolutionary robotics: an empirical study",
      "author" : [ "J.-B. Mouret", "S. Doncieux" ],
      "venue" : "Evolutionary Computation, vol. 20, no. 1, pp. 91–133, 2012.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "An experimental comparison of Bayesian optimization for bipedal locomotion",
      "author" : [ "R. Calandra", "A. Seyfarth", "J. Peters", "M.P. Deisenroth" ],
      "venue" : "Proc. of ICRA. IEEE, 2014.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Automatic gait optimization with gaussian process regression.",
      "author" : [ "D.J. Lizotte", "T. Wang", "M.H. Bowling", "D. Schuurmans" ],
      "venue" : "in Proc. of IJCAI,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2007
    }, {
      "title" : "Using trajectory data to improve bayesian optimization for reinforcement learning",
      "author" : [ "A. Wilson", "A. Fern", "P. Tadepalli" ],
      "venue" : "The Journal of Machine Learning Research, vol. 15, no. 1, pp. 253–282, 2014.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Diagnosis and fault-tolerant control",
      "author" : [ "M. Blanke", "J. Schröder" ],
      "venue" : null,
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2003
    }, {
      "title" : "Resilient machines through continuous self-modeling",
      "author" : [ "J.C. Bongard", "V. Zykov", "H. Lipson" ],
      "venue" : "Science, vol. 314, no. 5802, pp. 1118– 1121, 2006.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Alternative gaits for multiped robots with leg failures to retain maneuverability",
      "author" : [ "K. Mostafa", "C. Tsai", "I. Her" ],
      "venue" : "International Journal of Advanced Robotic Systems, vol. 7, no. 4, p. 31, 2010.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Taking the human out of the loop: A review of bayesian optimization",
      "author" : [ "B. Shahriari", "K. Swersky", "Z. Wang", "R.P. Adams", "N. de Freitas" ],
      "venue" : "Proceedings of the IEEE, vol. 104, no. 1, pp. 148–175, 2016.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Evolving a behavioral repertoire for a walking robot",
      "author" : [ "A. Cully", "J.-B. Mouret" ],
      "venue" : "Evolutionary Computation, 2015.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "EvoRBC: evolutionary repertoire-based control for robots with arbitrary locomotion complexity",
      "author" : [ "M. Duarte", "J. Gomes", "S.M. Oliveira", "A.L. Christensen" ],
      "venue" : "Proc. of GECCO, 2016.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images",
      "author" : [ "A. Nguyen", "J. Yosinski", "J. Clune" ],
      "venue" : "Proc. of CVPR. IEEE, 2015, pp. 427–436.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Monte-carlo tree search: A new framework for game ai.",
      "author" : [ "G. Chaslot", "S. Bakkes", "I. Szita", "P. Spronck" ],
      "venue" : "AIIDE,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2008
    }, {
      "title" : "Monte-carlo planning in large POMDPs",
      "author" : [ "D. Silver", "J. Veness" ],
      "venue" : "Proc. of NIPS, 2010, pp. 2164–2172.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A survey of monte carlo tree search methods",
      "author" : [ "C.B. Browne" ],
      "venue" : "IEEE Trans. on Computational Intelligence and AI in Games, vol. 4, no. 1, pp. 1–43, 2012.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Monte carlo tree search for continuous and stochastic sequential decision making problems",
      "author" : [ "A. Couëtoux" ],
      "venue" : "Ph.D. dissertation, Université Paris Sud-Paris XI, 2013.",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Sferes v2: Evolvin’in the multi-core world",
      "author" : [ "J.-B. Mouret", "S. Doncieux" ],
      "venue" : "Proc. of IEEE CEC, 2010.",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Gaussian processes for machine learning",
      "author" : [ "C.E. Rasmussen", "C.K.I. Williams" ],
      "venue" : null,
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2006
    }, {
      "title" : "Optimal active learning through billiards and upper confidence trees in continous domains",
      "author" : [ "P. Rolet", "M. Sebag", "O. Teytaud" ],
      "venue" : "Proc. of ECML, 2009, pp. 302–317.",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Continuous upper confidence trees",
      "author" : [ "A. Couëtoux", "J.-B. Hoock", "N. Sokolovska", "O. Teytaud", "N. Bonnard" ],
      "venue" : "Proc. of LION, 2011.",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "On the parallelization of UCT",
      "author" : [ "T. Cazenave", "N. Jouandeau" ],
      "venue" : "Proc. of the Computer Games Workshop, 2007, pp. 93–101.",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Continuous rapid action value estimates",
      "author" : [ "A. Couetoux", "M. Milone", "M. Brendel", "H. Doghmen", "M. Sebag", "O. Teytaud" ],
      "venue" : "Proc. of the 3rd Asian Conference on Machine Learning (ACML2011), 2011.",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Simultaneous localization and mapping: part I",
      "author" : [ "H. Durrant-Whyte", "T. Bailey" ],
      "venue" : "IEEE robotics & automation magazine, vol. 13, no. 2, pp. 99–110, 2006.",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "On Building Systems That Will Fail",
      "author" : [ "F. Corbato" ],
      "venue" : "ACM Turing award lectures, vol. 34, no. 9, pp. 72–81, 2007.",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "A unifying view of sparse approximate Gaussian process regression",
      "author" : [ "J. Quiñonero-Candela", "C.E. Rasmussen" ],
      "venue" : "Journal of Machine Learning Research, vol. 6, no. Dec, pp. 1939–1959, 2005.",
      "citeRegEx" : "44",
      "shortCiteRegEx" : null,
      "year" : 1939
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "During the recent DARPA Robotics Challenge (2015), many robots had to be “rescued” by humans because of hardware failures [1], [2]; this is paradoxical for robots that were designed to operate in environments that are too risky for humans! While these robots could certainly have been more robust and could have prevented some falls [1], even the best robots will encounter unforeseen situations: hardware failures will always be a possibility, especially with highly complex robots in post-disaster environments [3], [4].",
      "startOffset" : 122,
      "endOffset" : 125
    }, {
      "referenceID" : 0,
      "context" : "During the recent DARPA Robotics Challenge (2015), many robots had to be “rescued” by humans because of hardware failures [1], [2]; this is paradoxical for robots that were designed to operate in environments that are too risky for humans! While these robots could certainly have been more robust and could have prevented some falls [1], even the best robots will encounter unforeseen situations: hardware failures will always be a possibility, especially with highly complex robots in post-disaster environments [3], [4].",
      "startOffset" : 333,
      "endOffset" : 336
    }, {
      "referenceID" : 1,
      "context" : "During the recent DARPA Robotics Challenge (2015), many robots had to be “rescued” by humans because of hardware failures [1], [2]; this is paradoxical for robots that were designed to operate in environments that are too risky for humans! While these robots could certainly have been more robust and could have prevented some falls [1], even the best robots will encounter unforeseen situations: hardware failures will always be a possibility, especially with highly complex robots in post-disaster environments [3], [4].",
      "startOffset" : 513,
      "endOffset" : 516
    }, {
      "referenceID" : 2,
      "context" : "During the recent DARPA Robotics Challenge (2015), many robots had to be “rescued” by humans because of hardware failures [1], [2]; this is paradoxical for robots that were designed to operate in environments that are too risky for humans! While these robots could certainly have been more robust and could have prevented some falls [1], even the best robots will encounter unforeseen situations: hardware failures will always be a possibility, especially with highly complex robots in post-disaster environments [3], [4].",
      "startOffset" : 518,
      "endOffset" : 521
    }, {
      "referenceID" : 0,
      "context" : "report that the Atlas robot they used in the DARPA Robotics challenge had a “mean time between failures of hours or, at most, days” [1].",
      "startOffset" : 132,
      "endOffset" : 135
    }, {
      "referenceID" : 3,
      "context" : "The traditional method for damage recovery is to first diagnose the failure [5], then update the plans to bypass it [6].",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 4,
      "context" : "The traditional method for damage recovery is to first diagnose the failure [5], then update the plans to bypass it [6].",
      "startOffset" : 116,
      "endOffset" : 119
    }, {
      "referenceID" : 5,
      "context" : "To overcome these challenges, robots can avoid the diagnosis step and directly learn a compensatory behavior by trial and error [7], [8], [9], [10].",
      "startOffset" : 128,
      "endOffset" : 131
    }, {
      "referenceID" : 6,
      "context" : "To overcome these challenges, robots can avoid the diagnosis step and directly learn a compensatory behavior by trial and error [7], [8], [9], [10].",
      "startOffset" : 133,
      "endOffset" : 136
    }, {
      "referenceID" : 7,
      "context" : "To overcome these challenges, robots can avoid the diagnosis step and directly learn a compensatory behavior by trial and error [7], [8], [9], [10].",
      "startOffset" : 138,
      "endOffset" : 141
    }, {
      "referenceID" : 8,
      "context" : "To overcome these challenges, robots can avoid the diagnosis step and directly learn a compensatory behavior by trial and error [7], [8], [9], [10].",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 9,
      "context" : "In that case, damage recovery is a reinforcement learning (RL) problem in which a damaged robot has to maximize its performance for the task at hand [11].",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 10,
      "context" : "State-of-the-art RL algorithms, however, are either too constrained to be easily used with arbitrary robots and arbitrary tasks [12], or they require too many trials that prohibit realistic damage recovery [11].",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 9,
      "context" : "State-of-the-art RL algorithms, however, are either too constrained to be easily used with arbitrary robots and arbitrary tasks [12], or they require too many trials that prohibit realistic damage recovery [11].",
      "startOffset" : 206,
      "endOffset" : 210
    }, {
      "referenceID" : 7,
      "context" : "Fortunately, damage recovery is not a standard RL problem: standard algorithms assume that the robot starts with no knowledge, whereas a trial-and-error damage recovery algorithm can rely on the knowledge of the intact robot [9], [13], [14], [8].",
      "startOffset" : 225,
      "endOffset" : 228
    }, {
      "referenceID" : 11,
      "context" : "Fortunately, damage recovery is not a standard RL problem: standard algorithms assume that the robot starts with no knowledge, whereas a trial-and-error damage recovery algorithm can rely on the knowledge of the intact robot [9], [13], [14], [8].",
      "startOffset" : 230,
      "endOffset" : 234
    }, {
      "referenceID" : 12,
      "context" : "Fortunately, damage recovery is not a standard RL problem: standard algorithms assume that the robot starts with no knowledge, whereas a trial-and-error damage recovery algorithm can rely on the knowledge of the intact robot [9], [13], [14], [8].",
      "startOffset" : 236,
      "endOffset" : 240
    }, {
      "referenceID" : 6,
      "context" : "Fortunately, damage recovery is not a standard RL problem: standard algorithms assume that the robot starts with no knowledge, whereas a trial-and-error damage recovery algorithm can rely on the knowledge of the intact robot [9], [13], [14], [8].",
      "startOffset" : 242,
      "endOffset" : 245
    }, {
      "referenceID" : 6,
      "context" : "Error algorithm (IT&E) allowed, for instance, a hexapod robot to learn to walk after several injuries with a dozen of episodes [8].",
      "startOffset" : 127,
      "endOffset" : 130
    }, {
      "referenceID" : 9,
      "context" : "The main limitation of IT&E and other state-of-the-art algorithms for robot learning is their reliance on episodes [11]: after each trial, the robot needs to be reset to the same state.",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 13,
      "context" : "2): (1) a pre-computed action repertoire of the intact robot that also creates a mapping between the task space and the parameters of the low-level controller (generated by MAP-Elites [15] in our case, Fig.",
      "startOffset" : 184,
      "endOffset" : 188
    }, {
      "referenceID" : 14,
      "context" : "A critical aspect of model learning is data acquisition: supervised learning algorithms need labeled data, but random babbling is often insufficient to generate behaviors that are interesting for robots [16], [17]; for instance, with a robotic manipulator, random movements are unlikely to generate grasp-like behaviors.",
      "startOffset" : 203,
      "endOffset" : 207
    }, {
      "referenceID" : 15,
      "context" : "A critical aspect of model learning is data acquisition: supervised learning algorithms need labeled data, but random babbling is often insufficient to generate behaviors that are interesting for robots [16], [17]; for instance, with a robotic manipulator, random movements are unlikely to generate grasp-like behaviors.",
      "startOffset" : 209,
      "endOffset" : 213
    }, {
      "referenceID" : 15,
      "context" : "Active learning can help alleviating this issue by exploring behaviors that improve the model “at the right place” [17].",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 9,
      "context" : "Instead of learning a model, robots can use a RL algorithm to discover how to behave [11].",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 16,
      "context" : "Classic RL approaches are, however, designed for discrete state and action spaces [18], [11], whereas robots almost always have to solve continuous tasks, for example balancing by controlling the joint torques.",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 9,
      "context" : "Classic RL approaches are, however, designed for discrete state and action spaces [18], [11], whereas robots almost always have to solve continuous tasks, for example balancing by controlling the joint torques.",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 17,
      "context" : "A popular alternative is to view RL as an optimization of the parameters of a policy [19], which can be solved with gradient-based methods [20], [11], evolutionary algorithms [21] or other optimization methods like Bayesian Optimization [8], [22], [23].",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 18,
      "context" : "A popular alternative is to view RL as an optimization of the parameters of a policy [19], which can be solved with gradient-based methods [20], [11], evolutionary algorithms [21] or other optimization methods like Bayesian Optimization [8], [22], [23].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 9,
      "context" : "A popular alternative is to view RL as an optimization of the parameters of a policy [19], which can be solved with gradient-based methods [20], [11], evolutionary algorithms [21] or other optimization methods like Bayesian Optimization [8], [22], [23].",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 19,
      "context" : "A popular alternative is to view RL as an optimization of the parameters of a policy [19], which can be solved with gradient-based methods [20], [11], evolutionary algorithms [21] or other optimization methods like Bayesian Optimization [8], [22], [23].",
      "startOffset" : 175,
      "endOffset" : 179
    }, {
      "referenceID" : 6,
      "context" : "A popular alternative is to view RL as an optimization of the parameters of a policy [19], which can be solved with gradient-based methods [20], [11], evolutionary algorithms [21] or other optimization methods like Bayesian Optimization [8], [22], [23].",
      "startOffset" : 237,
      "endOffset" : 240
    }, {
      "referenceID" : 20,
      "context" : "A popular alternative is to view RL as an optimization of the parameters of a policy [19], which can be solved with gradient-based methods [20], [11], evolutionary algorithms [21] or other optimization methods like Bayesian Optimization [8], [22], [23].",
      "startOffset" : 242,
      "endOffset" : 246
    }, {
      "referenceID" : 21,
      "context" : "A popular alternative is to view RL as an optimization of the parameters of a policy [19], which can be solved with gradient-based methods [20], [11], evolutionary algorithms [21] or other optimization methods like Bayesian Optimization [8], [22], [23].",
      "startOffset" : 248,
      "endOffset" : 252
    }, {
      "referenceID" : 20,
      "context" : "about 100 trials to learn the 4 parameters that control a minimalistic biped robot [22]), and most of them are episodic, meaning that the robot has to be put back in the same state after each trial.",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 10,
      "context" : "In particular, PILCO is a RL algorithm that is based on learning a probabilistic dynamics model, which is then used to optimize a policy [12].",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 22,
      "context" : "Nonetheless, PILCO has a very high computational cost (after a dozen of episodes, PILCO typically needs a few hours to select the next trial, see [24]), works only for lowdimensional state spaces (10-20 dimensions), and strongly constrains the type of policy [22] (for instance, it is unclear if PILCO can be used to learn a walking controller like in [22], [23], [8]).",
      "startOffset" : 146,
      "endOffset" : 150
    }, {
      "referenceID" : 20,
      "context" : "Nonetheless, PILCO has a very high computational cost (after a dozen of episodes, PILCO typically needs a few hours to select the next trial, see [24]), works only for lowdimensional state spaces (10-20 dimensions), and strongly constrains the type of policy [22] (for instance, it is unclear if PILCO can be used to learn a walking controller like in [22], [23], [8]).",
      "startOffset" : 259,
      "endOffset" : 263
    }, {
      "referenceID" : 20,
      "context" : "Nonetheless, PILCO has a very high computational cost (after a dozen of episodes, PILCO typically needs a few hours to select the next trial, see [24]), works only for lowdimensional state spaces (10-20 dimensions), and strongly constrains the type of policy [22] (for instance, it is unclear if PILCO can be used to learn a walking controller like in [22], [23], [8]).",
      "startOffset" : 352,
      "endOffset" : 356
    }, {
      "referenceID" : 21,
      "context" : "Nonetheless, PILCO has a very high computational cost (after a dozen of episodes, PILCO typically needs a few hours to select the next trial, see [24]), works only for lowdimensional state spaces (10-20 dimensions), and strongly constrains the type of policy [22] (for instance, it is unclear if PILCO can be used to learn a walking controller like in [22], [23], [8]).",
      "startOffset" : 358,
      "endOffset" : 362
    }, {
      "referenceID" : 6,
      "context" : "Nonetheless, PILCO has a very high computational cost (after a dozen of episodes, PILCO typically needs a few hours to select the next trial, see [24]), works only for lowdimensional state spaces (10-20 dimensions), and strongly constrains the type of policy [22] (for instance, it is unclear if PILCO can be used to learn a walking controller like in [22], [23], [8]).",
      "startOffset" : 364,
      "endOffset" : 367
    }, {
      "referenceID" : 23,
      "context" : "Classic approaches to fault tolerance rely on updating the model of the robot, either directly with a self-diagnosis [25], or indirectly with machine learning [26], [5]; the model is then used for planning and/or control.",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 24,
      "context" : "Classic approaches to fault tolerance rely on updating the model of the robot, either directly with a self-diagnosis [25], or indirectly with machine learning [26], [5]; the model is then used for planning and/or control.",
      "startOffset" : 159,
      "endOffset" : 163
    }, {
      "referenceID" : 3,
      "context" : "Classic approaches to fault tolerance rely on updating the model of the robot, either directly with a self-diagnosis [25], or indirectly with machine learning [26], [5]; the model is then used for planning and/or control.",
      "startOffset" : 165,
      "endOffset" : 168
    }, {
      "referenceID" : 25,
      "context" : "as expected, it can stop using it and adapt the controller to use only the working legs [27].",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 5,
      "context" : "An alternative approach is to let a damaged robot learn a new policy by trial-and-error, which bypasses the need for a diagnosis [7], [9], [8], [10].",
      "startOffset" : 129,
      "endOffset" : 132
    }, {
      "referenceID" : 7,
      "context" : "An alternative approach is to let a damaged robot learn a new policy by trial-and-error, which bypasses the need for a diagnosis [7], [9], [8], [10].",
      "startOffset" : 134,
      "endOffset" : 137
    }, {
      "referenceID" : 6,
      "context" : "An alternative approach is to let a damaged robot learn a new policy by trial-and-error, which bypasses the need for a diagnosis [7], [9], [8], [10].",
      "startOffset" : 139,
      "endOffset" : 142
    }, {
      "referenceID" : 8,
      "context" : "An alternative approach is to let a damaged robot learn a new policy by trial-and-error, which bypasses the need for a diagnosis [7], [9], [8], [10].",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 6,
      "context" : "To minimize the number of trials, several algorithms rely on the transferability hypothesis [8], [9]: the behaviors that do not use the damaged parts are likely to be similar with the damaged and the undamaged robot, therefore simulations of the intact robot can help searching for a new behavior on the damaged robot.",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 7,
      "context" : "To minimize the number of trials, several algorithms rely on the transferability hypothesis [8], [9]: the behaviors that do not use the damaged parts are likely to be similar with the damaged and the undamaged robot, therefore simulations of the intact robot can help searching for a new behavior on the damaged robot.",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 6,
      "context" : "Starting with this hypothesis, the IT&E algorithm [8] exploits a dynamic simulation of the intact robot to create a behaviorperformance map that predicts the performance of thousands of different behaviors.",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 26,
      "context" : "If a damage occurs, this map is used as a prior for a Bayesian optimization algorithm that searches for a compensatory behavior [28], [23], [22].",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 21,
      "context" : "If a damage occurs, this map is used as a prior for a Bayesian optimization algorithm that searches for a compensatory behavior [28], [23], [22].",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 20,
      "context" : "If a damage occurs, this map is used as a prior for a Bayesian optimization algorithm that searches for a compensatory behavior [28], [23], [22].",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 6,
      "context" : "Overall, the experimental results show that IT&E can allow various types of robots (a hexapod robot and an 8-DOF manipulator) to compensate for many different types of injuries in less than 2 minutes [8].",
      "startOffset" : 200,
      "endOffset" : 203
    }, {
      "referenceID" : 16,
      "context" : "Our problem can be cast in the general framework of Markov Decision Processes (MDP) [18].",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 6,
      "context" : "We would like to create a repertoire that covers the task space as well as possible [8], [29], [30], i.",
      "startOffset" : 84,
      "endOffset" : 87
    }, {
      "referenceID" : 27,
      "context" : "We would like to create a repertoire that covers the task space as well as possible [8], [29], [30], i.",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 28,
      "context" : "We would like to create a repertoire that covers the task space as well as possible [8], [29], [30], i.",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 13,
      "context" : "As a consequence, instead of using an inverse model, we learn the action repertoire with an iterative algorithm called MAP-Elites [15], [8] and a forward model (e.",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 6,
      "context" : "As a consequence, instead of using an inverse model, we learn the action repertoire with an iterative algorithm called MAP-Elites [15], [8] and a forward model (e.",
      "startOffset" : 136,
      "endOffset" : 139
    }, {
      "referenceID" : 13,
      "context" : "Essentially, MAP-Elites discretizes the na-dimensional task space to an na-dimensional grid, and then attempts to fill each of the cells thanks to a variation-selection loop [15], [8].",
      "startOffset" : 174,
      "endOffset" : 178
    }, {
      "referenceID" : 6,
      "context" : "Essentially, MAP-Elites discretizes the na-dimensional task space to an na-dimensional grid, and then attempts to fill each of the cells thanks to a variation-selection loop [15], [8].",
      "startOffset" : 180,
      "endOffset" : 183
    }, {
      "referenceID" : 6,
      "context" : "So far, it has been successfully used to generate behaviors for legged robots [8], wheeled robots controlled by neural networks [30], and even adversarial images for deep neural networks [31].",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 28,
      "context" : "So far, it has been successfully used to generate behaviors for legged robots [8], wheeled robots controlled by neural networks [30], and even adversarial images for deep neural networks [31].",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 29,
      "context" : "So far, it has been successfully used to generate behaviors for legged robots [8], wheeled robots controlled by neural networks [30], and even adversarial images for deep neural networks [31].",
      "startOffset" : 187,
      "endOffset" : 191
    }, {
      "referenceID" : 26,
      "context" : "A GP is an extension of the multivariate Gaussian distribution to an infinite-dimension stochastic process for which any finite combination of dimensions will be a Gaussian distribution [28].",
      "startOffset" : 186,
      "endOffset" : 190
    }, {
      "referenceID" : 30,
      "context" : "Here we use Monte Carlo Tree Search (MCTS) [32], as it has already been succesfully used to solve (Partially Observable)-MDPs with stochastic transition functions [33], [34], continuous state spaces, and high branching factors [34], [35].",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 31,
      "context" : "Here we use Monte Carlo Tree Search (MCTS) [32], as it has already been succesfully used to solve (Partially Observable)-MDPs with stochastic transition functions [33], [34], continuous state spaces, and high branching factors [34], [35].",
      "startOffset" : 163,
      "endOffset" : 167
    }, {
      "referenceID" : 32,
      "context" : "Here we use Monte Carlo Tree Search (MCTS) [32], as it has already been succesfully used to solve (Partially Observable)-MDPs with stochastic transition functions [33], [34], continuous state spaces, and high branching factors [34], [35].",
      "startOffset" : 169,
      "endOffset" : 173
    }, {
      "referenceID" : 32,
      "context" : "Here we use Monte Carlo Tree Search (MCTS) [32], as it has already been succesfully used to solve (Partially Observable)-MDPs with stochastic transition functions [33], [34], continuous state spaces, and high branching factors [34], [35].",
      "startOffset" : 227,
      "endOffset" : 231
    }, {
      "referenceID" : 33,
      "context" : "Here we use Monte Carlo Tree Search (MCTS) [32], as it has already been succesfully used to solve (Partially Observable)-MDPs with stochastic transition functions [33], [34], continuous state spaces, and high branching factors [34], [35].",
      "startOffset" : 233,
      "endOffset" : 237
    }, {
      "referenceID" : 32,
      "context" : "These rollouts are typically random or directed by a simple, domain-dependent heuristic [34].",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 33,
      "context" : "see [35]",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 6,
      "context" : "The low-level controller is the same as in [8], [29].",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 27,
      "context" : "The low-level controller is the same as in [8], [29].",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 34,
      "context" : "We use the Sferes2 framework [36] and the DART simulator1 for our implementation.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 35,
      "context" : "We use the Squared Exponential Kernel as the covariance function [37].",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 36,
      "context" : "[38] that properly handles cases where there is a large number of actions or the action space is continuous.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 37,
      "context" : "[39] that properly handles cases where the state space is continuous.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 33,
      "context" : "d) Generative Model: We construct the generative model using the prediciton of the GPs (with their variance) [35].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 32,
      "context" : "e) Default Policy for evaluation: We sample random actions from the repertoire [34].",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 32,
      "context" : ", we select the action that has the maximum average value [34].",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 38,
      "context" : "To make the search faster, we implemented root parallelization in MCTS [40] and we ran 4 parallel trees and gave to each one a budget of 5000 iterations.",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 33,
      "context" : "More generic approaches would be the Blind Value action sampling approach or cRAVE [35], [41].",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 39,
      "context" : "More generic approaches would be the Blind Value action sampling approach or cRAVE [35], [41].",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 6,
      "context" : "The environment (location of the obstacles) and the position of the robot is acquired using a Motion Capture system (Optitrack), although a SLAM and/or a visual odometry algorithm could have been used [8], [42].",
      "startOffset" : 201,
      "endOffset" : 204
    }, {
      "referenceID" : 40,
      "context" : "The environment (location of the obstacles) and the position of the robot is acquired using a Motion Capture system (Optitrack), although a SLAM and/or a visual odometry algorithm could have been used [8], [42].",
      "startOffset" : 206,
      "endOffset" : 210
    }, {
      "referenceID" : 41,
      "context" : "one will do about it when it occurs” [43].",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 2,
      "context" : "This advice is especially important if we want to be able to send advanced and expensive robots in risky places like a destroyed nuclear plant [4], even if the robots are tele-operated.",
      "startOffset" : 143,
      "endOffset" : 146
    }, {
      "referenceID" : 42,
      "context" : "In future work, we will therefore consider using a time-window to keep this complexity low and/or using sparse GPs [44].",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 40,
      "context" : "In the future, we will design a more realistic experiment in which the robot discovers its environment with a SLAM algorithm [42].",
      "startOffset" : 125,
      "endOffset" : 129
    } ],
    "year" : 2017,
    "abstractText" : "The high probability of hardware failures prevents many advanced robots (e.g. legged robots) to be confidently deployed in real-world situations (e.g post-disaster rescue). Instead of attempting to diagnose the failure(s), robots could adapt by trial-and-error in order to be able to complete their tasks. However, the best trial-and-error algorithms for robotics are all episodic: between each trial, the robot needs to be put back in the same state, that is, the robot is not learning autonomously. In this paper, we introduce a novel learning algorithm called “Reset-free Trial-and-Error” (RTE) that allows robots to recover from damage while completing their tasks. We evaluate it on a hexapod robot that is damaged in several ways (e.g. a missing leg, a shortened leg, etc.) and whose objective is to reach a sequence of targets in an arena. Our experiments show that the robot can recover most of its locomotion abilities in a few minutes, in an environment with obstacles, and without any human intervention. Overall, this new algorithm makes it possible to contemplate sending robots to places that are truly too dangerous for humans and in which robots cannot be rescued.",
    "creator" : "LaTeX with hyperref package"
  }
}