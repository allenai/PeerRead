{
  "name" : "1707.06341.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "stratos@ttic.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 7.\n06 34\n1v 1\n[ cs\n.C L\n] 2\n0 Ju\nl 2 01\n7\nchitecture that exploits a unique compositional structure of the Korean language. Our method decomposes each character into a small set of primitive phonetic units called jamo letters from which character- and word-level representations are induced. The jamo letters divulge syntactic and semantic information that is difficult to access with conventional character-level units. They greatly alleviate the data sparsity problem, reducing the observation space to 1.6% of the original while increasing accuracy in our experiments. We apply our architecture to dependency parsing and achieve dramatic improvement over strong lexical baselines."
    }, {
      "heading" : "1 Introduction",
      "text" : "Korean is generally recognized as a language isolate: that is, it has no apparent genealogical relationship with other languages (Song, 2006; Campbell and Mixco, 2007). A unique feature of the language is that each character is composed of a small, fixed set of basic phonetic units called jamo letters. Despite the important role jamo plays in encoding syntactic and semantic information of words, it has been neglected in existing modern Korean processing algorithms. In this paper, we bridge this gap by introducing a novel compositional neural architecture that explicitly leverages the sub-character information.\nSpecifically, we perform Unicode decomposition on each Korean character to recover its underlying jamo letters and construct character- and word-level representations from these letters. See\nFigure 1 for an illustration of the decomposition. The decomposition is deterministic; this is a crucial departure from previous work that uses language-specific sub-character information such as radical (a graphical component of a Chinese character). The radical structure of a Chinese character does not follow any systematic process, requiring an incomplete dictionary mapping between characters and radicals to take advantage of this information (Sun et al., 2014; Yin et al., 2016). In contrast, our Unicode decomposition does not need any supervision and can extract correct jamo letters for all possible Korean characters.\nOur jamo architecture is fully general and can be plugged in any Korean processing network. For a concrete demonstration of its utility, in this work we focus on dependency parsing. McDonald et al. (2013) note that “Korean emerges as a very clear outlier” in their cross-lingual parsing experiments on the universal treebank, implying a need to tailor a model for this language isolate. Because of the compositional morphology, Korean suffers extreme data sparsity at the word level: 2,703 out of 4,698 word types (> 57%) in the held-out portion of our treebank are OOV. This makes the language challenging for simple lexical parsers even when\naugmented with a large set of pre-trained word representations.\nWhile such data sparsity can also be alleviated by incorporating more conventional characterlevel information, we show that incorporating jamo is an effective and economical new approach to combating the sparsity problem for Korean. In experiments, we decisively improve the LAS of the lexical BiLSTM parser of Kiperwasser and Goldberg (2016) from 82.77 to 91.46 while reducing the size of input space by 98.4% when we replace words with jamos. As a point of reference, a strong feature-rich parser using gold POS tags obtains 88.61.\nTo summarize, we make the following contribu-\ntions.\n• To our knowledge, this is the first work that leverages jamo in end-to-end neural Ko-\nrean processing. To this end, we develop a novel sub-character architecture based on deterministic Unicode decomposition.\n• We perform extensive experiments on dependency parsing to verify the utility of the ap-\nproach. We show clear performance boost with a drastically smaller set of parameters. Our final model outperforms strong baselines by a large margin.\n• We release an implementation of our jamo architecture which can be plugged in any Ko-\nrean processing network.1"
    }, {
      "heading" : "2 Related Work",
      "text" : "We make a few additional remarks on related work to better situate our work. Our work follows the successful line of work on incorporating sub-lexical information to neural models. Various character-based architectures have been proposed. For instance, Ma and Hovy (2016) and Kim et al. (2016) use CNNs over characters whereas Lample et al. (2016) and Ballesteros et al. (2015) use bidirectional LSTMs (BiLSTMs). Both approaches have been shown to be profitable; we employ a BiLSTM-based approach.\nMany previous works have also considered morphemes to augment lexical models (Luong et al., 2013; Botha and Blunsom, 2014; Cotterell et al., 2016). Sub-character models are\n1 https://github.com/karlstratos/koreannet\nsubstantially rarer; an extreme case is considered by Gillick et al. (2016) who process text as a sequence of bytes. We believe that such byte-level models are too general and that there are opportunities to exploit natural sub-character structure for certain languages such as Korean and Chinese.\nThere exists a line of work on exploiting graphical components of Chinese characters called radicals (Sun et al., 2014; Yin et al., 2016). For instance, 足 (foot) is the radical of 跑 (run). While related, our work on Korean is distinguished in critical ways and should not be thought of as just an extension to another language. First, as mentioned earlier, the compositional structure is fundamentally different between Chinese and Korean. The mapping between radicals and characters in Chinese is nondeterministic and can only be loosely approximated by an incomplete dictionary. In contrast, the mapping between jamos and Korean characters is deterministic (Section 3.1), allowing for systematic decomposition of all possible Korean characters. Second, the previous work on Chinese radicals was concerned with learning word embeddings. We develop an end-to-end compositional model for a downstream task: parsing."
    }, {
      "heading" : "3 Method",
      "text" : ""
    }, {
      "heading" : "3.1 Jamo Structure of the Korean Language",
      "text" : "Let W denote the set of word types and C the set of character types. In many languages, c ∈ C is the most basic unit that is meaningful. In Korean, each character is further composed of a small fixed set of phonetic units called jamo letters J where |J | = 51. The jamo letters are categorized as head consonants Jh, vowels Jv, or tail consonants Jt. The composition is completely systematic. Given any character c ∈ C, there exist ch ∈ Jh, cv ∈ Jv, and ct ∈ Jt such that their composition yields c. Conversely, any ch ∈ Jh, cv ∈ Jv, and ct ∈ Jt can be composed to yield a valid character c ∈ C.\nAs an example, consider the word갔다 (went). It is composed of two characters, 갔,다 ∈ C. Each character is furthermore composed of three jamo letters as follows:\n• 갔 ∈ C is composed of ㄱ ∈ Jh, ㅏ ∈ Jv, and ㅆ ∈ Jt.\n• 다 ∈ C is composed of ㄷ ∈ Jh, ㅏ ∈ Jv, and an empty letter ∅ ∈ Jt.\nThe tail consonant can be empty; we assume a special symbol ∅ ∈ Jt to denote an empty letter. Figure 1 illustrates the decomposition of a Korean sentence down to jamo letters.\nNote that the number of possible characters is combinatorial in the number of jamo letters, loosely upper bounded by 513 = 132, 651. This upper bound is loose because certain combinations are invalid. For instance, ㅁ ∈ Jh ∩ Jt but ㅁ 6∈ Jv whereas ㅏ ∈ Jv but ㅏ 6∈ Jh ∪ Jt.\nThe combinatorial nature of Korean characters motivates the compositional architecture below. For completeness, we describe the entire forward pass of the transition-based BiLSTM parser of Kiperwasser and Goldberg (2016) that we use in our experiments."
    }, {
      "heading" : "3.2 Jamo Architecture",
      "text" : "The parameters associated with the jamo layer are\n• Embedding el ∈ Rd for each letter l ∈ J\n• UJ , V J ,WJ ∈ Rd×d and bJ ∈ Rd\nGiven a Korean character c ∈ C, we perform Unicode decomposition (Section 3.3) to recover the underlying jamo letters ch, cv, ct ∈ J . We compose the letters to induce a representation of c as\nhc = tanh ( UJ ech + V J ecv +WJ ect + bJ )\nThis representation is then concatenated with a character-level lookup embedding, and the result is fed into an LSTM to produce a word representation. We use an LSTM (Hochreiter and Schmidhuber, 1997) simply as a mapping φ : Rd1 ×Rd2 → Rd2 that takes an input vector x and a state vector h to output a new state vector h′ = φ(x, h). The parameters associated with this layer are\n• Embedding ec ∈ Rd ′ for each c ∈ C\n• Forward LSTM φf : Rd+d ′ × Rd → Rd\n• Backward LSTM φb : Rd+d ′ ×Rd → Rd\n• UC ∈ Rd×2d and bC ∈ Rd\nGiven a word w ∈ W and its character sequence c1 . . . cm ∈ C, we compute\nf ci = φ f\n([\nhci eci\n]\n, f ci−1\n)\n∀i = 1 . . . m\nbci = φ b\n([\nhci eci\n]\n, bci+1\n)\n∀i = m. . . 1\nand induce a representation of w as\nhw = tanh\n( UC [ f cm bc1 ] + bC )\nLastly, this representation is concatenated with a word-level lookup embedding (which can be initialized with pre-trained word embeddings), and the result is fed into a BiLSTM network. The parameters associated with this layer are\n• Embedding ew ∈ RdW for each w ∈ W\n• Two-layer BiLSTM Φ that maps h1 . . . hn ∈ R d+dW to z1 . . . zn ∈ R d∗\n• Feedforward for predicting transitions\nGiven a sentence w1 . . . wn ∈ W , the final d ∗- dimensional word representations are given by\n(z1 . . . zn) = Φ\n([\nhw1 ew1\n]\n. . .\n[\nhwn ewn\n])\nThe parser then uses the feedforward network to greedily predict transitions based on words that are active in the system. The model is trained end-toend by optimizing a max-margin objective. Since this part is not a contribution of this paper, we refer to Kiperwasser and Goldberg (2016) for details.\nBy setting the embedding dimension of jamos d, characters d′, or words dW to zero, we can configure the network to use any combination of these units. We report these experiments in Section 4."
    }, {
      "heading" : "3.3 Unicode Decomposition",
      "text" : "Our architecture requires dynamically extracting jamo letters given any Korean character. This is achieved by simple Unicode manipulation. For any Korean character c ∈ C with Unicode value U(c), let U(c) = U(c) − 44032 and T (c) = U(c) mod 28. Then the Unicode values U(ch), U(cv), and U(ct) corresponding to the head consonant, vowel, and tail consonant are obtained by\nU(ch) = 1 +\n⌊\nU(c)\n588\n⌋\n+ 0x10ff\nU(cv) = 1 +\n⌊\n(U(c)− T (c)) mod 588\n28\n⌋\n+ 0x1160\nU(ct) = 1 + T (c) + 0x11a7\nwhere ct is set to ∅ if T (ct) = 0."
    }, {
      "heading" : "3.4 Why Use Jamo Letters?",
      "text" : "The most obvious benefit of using jamo letters is alleviating data sparsity by flattening the combinatorial space of Korean characters. We discuss some additional explicit benefits. First, jamo letters often indicate syntactic properties of words. For example, a tail consonant ㅆ strongly implies that the word is a past tense verb as in 갔다 (went), 왔다 (came), and 했다 (did). Thus a jamo-level model can identify unseen verbs more effectively than word- or character-level models. Second, jamo letters dictate the sound of a character. For example, 갔 is pronounced as got because the head consonant ㄱ is associated with the sound g, the vowel ㅏ with o, and the tail consonant ㅆ with t. This is clearly critical for speech recognition/synthesis and indeed has been investigated in the speech community (Lee et al., 1994; Sakti et al., 2010). While speech processing is not our focus, the phonetic signals can capture useful lexical correlation (e.g., for onomatopoeic words)."
    }, {
      "heading" : "4 Experiments",
      "text" : "Data We use the publicly available Korean treebank in the universal treebank version 2.0 (McDonald et al., 2013).2 The dataset comes with a train/development/test split; data statistics are shown in Table 1. Since the test portion is significantly smaller than the dev portion, we report performance on both.\nAs expected, we observe severe data sparsity with words: 24,814 out of 31,060 elements in the vocabulary appear only once in the training data. On the dev set, about 57% word types and 3% character types are OOV. Upon Unicode decomposition, we obtain the following 48 jamo types:\n2 https://github.com/ryanmcd/uni-dep-tb\nㄱㄳㄲㄵㄴㄷㄶㄹㄸㄻㄺ ㄼㅁ ㅀ ㅃ ㅂ ㅅ ㅄ ㅇ ㅆ ㅉ ㅈ ㅋ ㅊ ㅍ ㅌ ㅏㅎㅑㅐㅓㅒㅕㅔㅗㅖㅙㅘㅛㅚ ㅝ ㅜㅟ ㅞ ㅡㅠ ㅣ ㅢ\nnone of which is OOV in the dev set.\nImplementation and baselines We implement our jamo architecture using the DyNet library (Neubig et al., 2017) and plug it into the BiLSTM parser of Kiperwasser and Goldberg (2016).3 For Korean syllable manipulation, we use the freely available toolkit by Joshua Dong.4 We train the parser for 30 epochs and use the dev portion for model selection. We compare our approach to the following baselines:\n• McDonald13: A cross-lingual parser originally reported in McDonald et al. (2013).\n• Yara: A beam-search transition-based parser of Rasooli and Tetreault (2015) based on the\nrich non-local features in Zhang and Nivre (2011). We use beam width 64. We use 5-fold jackknifing on the training portion to provide POS tag features. We also report on using gold POS tags.\n• K&G16: The basic BiLSTM parser of Kiperwasser and Goldberg (2016) without\nthe sub-lexical architecture introduced in this work.\n• Stack LSTM: A greedy transition-based parser based on stack LSTM representa-\ntions. Dyer15 denotes the word-level variant (Dyer et al., 2015). Ballesteros15 denotes the character-level variant (Ballesteros et al., 2015).\nFor pre-trained word embeddings, we apply the spectral algorithm of Stratos et al. (2015) on a 2015 Korean Wikipedia dump to induce 285,933 embeddings of dimension 100.\nParsing accuracy Table 2 shows the main result. The baseline test LAS of the original crosslingual parser of McDonald13 is 55.85. Yara achieves 85.17 with predicted POS tags and 88.61 with gold POS tags. The basic BiLSTM model of K&G16 obtains 82.77 with pre-trained word embeddings (78.95 without). The stack LSTM parser is comparable to K&G16 at the word level\n3https://github.com/elikip/bist-parser 4 https://github.com/JDongian/python-jamo\n(Dyer15), but it performs significantly better at the character level (Ballesteros15) reaching 86.25 test LAS.\nWe observe decisive improvement when we incorporate sub-lexical information into the parser of K&G16. In fact, a strictly sub-lexical parser using only jamos or characters clearly outperforms its lexical counterpart despite the fact that the model is drastically smaller (e.g., 90.77 with 500× 100 jamo embeddings vs 82.77 with 298115×100 word embeddings). Notably, jamos alone achieve 91.46 which is not far behind the best result 92.31 obtained by using word, character, and jamo units in conjunction. This demonstrates that our compositional architecture learns to build effective representations of Korean characters and words for parsing from a minuscule set of jamo letters."
    }, {
      "heading" : "5 Discussion of Future Work",
      "text" : "We have presented a natural sub-character architecture to model the unique compositional orthography of the Korean language. The architecture induces word-/sentence-level representations from a small set of phonetic units called jamo letters. This is enabled by efficient and deterministic Unicode decomposition of characters.\nWe have focused on dependency parsing to demonstrate the utility of our approach as an economical and effective way to combat data sparsity. However, we believe that the true benefit of this architecture will be more evident in speech processing as jamo letters are definitions of sound in the language. Another potentially interesting application is informal text on the internet. Ill-formed words such as ㅎㅎㅎ (shorthand for 하하하, an\nonomatopoeic expression of laughter) and ㄴㄴ (shorthand for노노, a transcription of no no) are omnipresent in social media. The jamo architecture can be useful in this scenario, for instance by correlating ㅎㅎㅎ and 하하하 which might otherwise be treated as independent."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The author would like to thank Lingpeng Kong for his help with using the DyNet library, Mohammad Rasooli for his help with using the Yara parser, and Karen Livescu for helpful comments."
    } ],
    "references" : [ {
      "title" : "Improved transition-based parsing by modeling characters instead of words with lstms",
      "author" : [ "Miguel Ballesteros", "Chris Dyer", "Noah A. Smith." ],
      "venue" : "Proc. EMNLP.",
      "citeRegEx" : "Ballesteros et al\\.,? 2015",
      "shortCiteRegEx" : "Ballesteros et al\\.",
      "year" : 2015
    }, {
      "title" : "Compositional morphology for word representations and language modelling",
      "author" : [ "Jan A Botha", "Phil Blunsom." ],
      "venue" : "ICML, pages 1899–1907.",
      "citeRegEx" : "Botha and Blunsom.,? 2014",
      "shortCiteRegEx" : "Botha and Blunsom.",
      "year" : 2014
    }, {
      "title" : "A glossary of historical linguistics",
      "author" : [ "Lyle Campbell", "Mauricio J Mixco." ],
      "venue" : "Edinburgh University Press.",
      "citeRegEx" : "Campbell and Mixco.,? 2007",
      "shortCiteRegEx" : "Campbell and Mixco.",
      "year" : 2007
    }, {
      "title" : "Morphological smoothing and extrapolation of word embeddings",
      "author" : [ "Ryan Cotterell", "Hinrich Schütze", "Jason Eisner." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, volume 1, pages 1651–1660.",
      "citeRegEx" : "Cotterell et al\\.,? 2016",
      "shortCiteRegEx" : "Cotterell et al\\.",
      "year" : 2016
    }, {
      "title" : "Transitionbased dependency parsing with stack long shortterm memory",
      "author" : [ "Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith." ],
      "venue" : "Proc. ACL.",
      "citeRegEx" : "Dyer et al\\.,? 2015",
      "shortCiteRegEx" : "Dyer et al\\.",
      "year" : 2015
    }, {
      "title" : "Multilingual language processing from bytes",
      "author" : [ "Dan Gillick", "Cliff Brunk", "Oriol Vinyals", "Amarnag Subramanya." ],
      "venue" : "Proceedings of NAACL.",
      "citeRegEx" : "Gillick et al\\.,? 2016",
      "shortCiteRegEx" : "Gillick et al\\.",
      "year" : 2016
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation, 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Character-aware neural language models",
      "author" : [ "Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush." ],
      "venue" : "Thirtieth AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Kim et al\\.,? 2016",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2016
    }, {
      "title" : "Simple and accurate dependency parsing using bidirectional lstm feature representations",
      "author" : [ "Eliyahu Kiperwasser", "Yoav Goldberg." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 4:313–327.",
      "citeRegEx" : "Kiperwasser and Goldberg.,? 2016",
      "shortCiteRegEx" : "Kiperwasser and Goldberg.",
      "year" : 2016
    }, {
      "title" : "Neural architectures for named entity recognition",
      "author" : [ "Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "KazuyaKawakami", "Chris Dyer." ],
      "venue" : "Proceedings of NAACL.",
      "citeRegEx" : "Lample et al\\.,? 2016",
      "shortCiteRegEx" : "Lample et al\\.",
      "year" : 2016
    }, {
      "title" : "Phonemie-level, speech and natural, language integration for agglutinative languages",
      "author" : [ "Geunbae Lee", "Jong-Hyeok Lee", "Kyunghee Kim." ],
      "venue" : "GGGGGGGG 0.",
      "citeRegEx" : "Lee et al\\.,? 1994",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 1994
    }, {
      "title" : "Better word representations with recursive neural networks for morphology",
      "author" : [ "Thang Luong", "Richard Socher", "Christopher D Manning." ],
      "venue" : "CoNLL, pages 104–113.",
      "citeRegEx" : "Luong et al\\.,? 2013",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2013
    }, {
      "title" : "End-to-end sequence labeling via bi-directional lstm-cnns-crf",
      "author" : [ "Xuezhe Ma", "Eduard Hovy." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1064–1074, Berlin, Ger-",
      "citeRegEx" : "Ma and Hovy.,? 2016",
      "shortCiteRegEx" : "Ma and Hovy.",
      "year" : 2016
    }, {
      "title" : "Universal dependency annotation for multilingual parsing",
      "author" : [ "Ryan TMcDonald", "Joakim Nivre", "Yvonne QuirmbachBrundage", "Yoav Goldberg", "Dipanjan Das", "Kuzman Ganchev", "Keith B Hall", "Slav Petrov", "Hao Zhang", "Oscar Täckström" ],
      "venue" : null,
      "citeRegEx" : "TMcDonald et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "TMcDonald et al\\.",
      "year" : 2013
    }, {
      "title" : "Dynet: The dynamic neural network toolkit",
      "author" : [ "Lingpeng Kong", "Adhiguna Kuncoro", "Gaurav Kumar", "Chaitanya Malaviya", "Paul Michel", "Yusuke Oda", "Matthew Richardson", "Naomi Saphra", "Swabha Swayamdipta", "Pengcheng Yin." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Kong et al\\.,? 2017",
      "shortCiteRegEx" : "Kong et al\\.",
      "year" : 2017
    }, {
      "title" : "Yara parser: A fast and accurate dependency parser",
      "author" : [ "Rasooli", "Joel R. Tetreault." ],
      "venue" : "CoRR, abs/1503.06733.",
      "citeRegEx" : "Rasooli and Tetreault.,? 2015",
      "shortCiteRegEx" : "Rasooli and Tetreault.",
      "year" : 2015
    }, {
      "title" : "Korean pronunciation variation modeling with probabilistic bayesian networks",
      "author" : [ "Sakriani Sakti", "Andrew Finch", "Ryosuke Isotani", "Hisashi Kawai", "Satoshi Nakamura." ],
      "venue" : "Universal Communication Symposium (IUCS), 2010 4th International,",
      "citeRegEx" : "Sakti et al\\.,? 2010",
      "shortCiteRegEx" : "Sakti et al\\.",
      "year" : 2010
    }, {
      "title" : "The Korean language: Structure, use and context",
      "author" : [ "Jae Jung Song." ],
      "venue" : "Routledge.",
      "citeRegEx" : "Song.,? 2006",
      "shortCiteRegEx" : "Song.",
      "year" : 2006
    }, {
      "title" : "Model-based word embeddings from decompositions of count matrices",
      "author" : [ "Karl Stratos", "Michael Collins", "Daniel Hsu." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint",
      "citeRegEx" : "Stratos et al\\.,? 2015",
      "shortCiteRegEx" : "Stratos et al\\.",
      "year" : 2015
    }, {
      "title" : "Radical-enhanced chinese character embedding",
      "author" : [ "Yaming Sun", "Lei Lin", "Nan Yang", "Zhenzhou Ji", "Xiaolong Wang." ],
      "venue" : "International Conference on Neural Information Processing, pages 279–286. Springer.",
      "citeRegEx" : "Sun et al\\.,? 2014",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2014
    }, {
      "title" : "Multi-granularity chinese word embedding",
      "author" : [ "Rongchao Yin", "Quan Wang", "Rui Li", "Peng Li", "Bin Wang." ],
      "venue" : "Proceedings of the Empiricial Methods in Natural Language Processing.",
      "citeRegEx" : "Yin et al\\.,? 2016",
      "shortCiteRegEx" : "Yin et al\\.",
      "year" : 2016
    }, {
      "title" : "Transition-based dependency parsing with rich non-local features",
      "author" : [ "Yue Zhang", "Joakim Nivre." ],
      "venue" : "Proceedings of the 49th AnnualMeeting of the Association for Computational Linguistics: Human Language Technologies: short papers-Volume 2, pages",
      "citeRegEx" : "Zhang and Nivre.,? 2011",
      "shortCiteRegEx" : "Zhang and Nivre.",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 17,
      "context" : "Korean is generally recognized as a language isolate: that is, it has no apparent genealogical relationship with other languages (Song, 2006; Campbell and Mixco, 2007).",
      "startOffset" : 129,
      "endOffset" : 167
    }, {
      "referenceID" : 2,
      "context" : "Korean is generally recognized as a language isolate: that is, it has no apparent genealogical relationship with other languages (Song, 2006; Campbell and Mixco, 2007).",
      "startOffset" : 129,
      "endOffset" : 167
    }, {
      "referenceID" : 19,
      "context" : "The radical structure of a Chinese character does not follow any systematic process, requiring an incomplete dictionary mapping between characters and radicals to take advantage of this information (Sun et al., 2014; Yin et al., 2016).",
      "startOffset" : 198,
      "endOffset" : 234
    }, {
      "referenceID" : 20,
      "context" : "The radical structure of a Chinese character does not follow any systematic process, requiring an incomplete dictionary mapping between characters and radicals to take advantage of this information (Sun et al., 2014; Yin et al., 2016).",
      "startOffset" : 198,
      "endOffset" : 234
    }, {
      "referenceID" : 8,
      "context" : "In experiments, we decisively improve the LAS of the lexical BiLSTM parser of Kiperwasser and Goldberg (2016) from 82.",
      "startOffset" : 78,
      "endOffset" : 110
    }, {
      "referenceID" : 11,
      "context" : "Many previous works have also considered morphemes to augment lexical models (Luong et al., 2013; Botha and Blunsom, 2014; Cotterell et al., 2016).",
      "startOffset" : 77,
      "endOffset" : 146
    }, {
      "referenceID" : 1,
      "context" : "Many previous works have also considered morphemes to augment lexical models (Luong et al., 2013; Botha and Blunsom, 2014; Cotterell et al., 2016).",
      "startOffset" : 77,
      "endOffset" : 146
    }, {
      "referenceID" : 3,
      "context" : "Many previous works have also considered morphemes to augment lexical models (Luong et al., 2013; Botha and Blunsom, 2014; Cotterell et al., 2016).",
      "startOffset" : 77,
      "endOffset" : 146
    }, {
      "referenceID" : 6,
      "context" : "For instance, Ma and Hovy (2016) and Kim et al.",
      "startOffset" : 14,
      "endOffset" : 33
    }, {
      "referenceID" : 4,
      "context" : "For instance, Ma and Hovy (2016) and Kim et al. (2016) use CNNs over characters whereas Lample et al.",
      "startOffset" : 37,
      "endOffset" : 55
    }, {
      "referenceID" : 4,
      "context" : "For instance, Ma and Hovy (2016) and Kim et al. (2016) use CNNs over characters whereas Lample et al. (2016) and Ballesteros et al.",
      "startOffset" : 37,
      "endOffset" : 109
    }, {
      "referenceID" : 0,
      "context" : "(2016) and Ballesteros et al. (2015) use bidirectional LSTMs (BiLSTMs).",
      "startOffset" : 11,
      "endOffset" : 37
    }, {
      "referenceID" : 5,
      "context" : "com/karlstratos/koreannet substantially rarer; an extreme case is considered by Gillick et al. (2016) who process text as a sequence of bytes.",
      "startOffset" : 80,
      "endOffset" : 102
    }, {
      "referenceID" : 19,
      "context" : "There exists a line of work on exploiting graphical components of Chinese characters called radicals (Sun et al., 2014; Yin et al., 2016).",
      "startOffset" : 101,
      "endOffset" : 137
    }, {
      "referenceID" : 20,
      "context" : "There exists a line of work on exploiting graphical components of Chinese characters called radicals (Sun et al., 2014; Yin et al., 2016).",
      "startOffset" : 101,
      "endOffset" : 137
    }, {
      "referenceID" : 8,
      "context" : "For completeness, we describe the entire forward pass of the transition-based BiLSTM parser of Kiperwasser and Goldberg (2016) that we use in our experiments.",
      "startOffset" : 95,
      "endOffset" : 127
    }, {
      "referenceID" : 6,
      "context" : "We use an LSTM (Hochreiter and Schmidhuber, 1997) simply as a mapping φ : R1 ×R2 → R2 that takes an input vector x and a state vector h to output a new state vector h = φ(x, h).",
      "startOffset" : 15,
      "endOffset" : 49
    }, {
      "referenceID" : 8,
      "context" : "Since this part is not a contribution of this paper, we refer to Kiperwasser and Goldberg (2016) for details.",
      "startOffset" : 65,
      "endOffset" : 97
    }, {
      "referenceID" : 10,
      "context" : "This is clearly critical for speech recognition/synthesis and indeed has been investigated in the speech community (Lee et al., 1994; Sakti et al., 2010).",
      "startOffset" : 115,
      "endOffset" : 153
    }, {
      "referenceID" : 16,
      "context" : "This is clearly critical for speech recognition/synthesis and indeed has been investigated in the speech community (Lee et al., 1994; Sakti et al., 2010).",
      "startOffset" : 115,
      "endOffset" : 153
    }, {
      "referenceID" : 8,
      "context" : ", 2017) and plug it into the BiLSTM parser of Kiperwasser and Goldberg (2016). For Korean syllable manipulation, we use the freely available toolkit by Joshua Dong.",
      "startOffset" : 46,
      "endOffset" : 78
    }, {
      "referenceID" : 15,
      "context" : "• Yara: A beam-search transition-based parser of Rasooli and Tetreault (2015) based on the rich non-local features in Zhang and Nivre (2011).",
      "startOffset" : 49,
      "endOffset" : 78
    }, {
      "referenceID" : 15,
      "context" : "• Yara: A beam-search transition-based parser of Rasooli and Tetreault (2015) based on the rich non-local features in Zhang and Nivre (2011). We use beam width 64.",
      "startOffset" : 49,
      "endOffset" : 141
    }, {
      "referenceID" : 8,
      "context" : "• K&G16: The basic BiLSTM parser of Kiperwasser and Goldberg (2016) without the sub-lexical architecture introduced in this work.",
      "startOffset" : 36,
      "endOffset" : 68
    }, {
      "referenceID" : 4,
      "context" : "Dyer15 denotes the word-level variant (Dyer et al., 2015).",
      "startOffset" : 38,
      "endOffset" : 57
    }, {
      "referenceID" : 0,
      "context" : "Ballesteros15 denotes the character-level variant (Ballesteros et al., 2015).",
      "startOffset" : 50,
      "endOffset" : 76
    }, {
      "referenceID" : 18,
      "context" : "For pre-trained word embeddings, we apply the spectral algorithm of Stratos et al. (2015) on a 2015 Korean Wikipedia dump to induce 285,933 embeddings of dimension 100.",
      "startOffset" : 68,
      "endOffset" : 90
    } ],
    "year" : 2017,
    "abstractText" : "We introduce a novel sub-character architecture that exploits a unique compositional structure of the Korean language. Our method decomposes each character into a small set of primitive phonetic units called jamo letters from which characterand word-level representations are induced. The jamo letters divulge syntactic and semantic information that is difficult to access with conventional character-level units. They greatly alleviate the data sparsity problem, reducing the observation space to 1.6% of the original while increasing accuracy in our experiments. We apply our architecture to dependency parsing and achieve dramatic improvement over strong lexical baselines.",
    "creator" : "LaTeX with hyperref package"
  }
}