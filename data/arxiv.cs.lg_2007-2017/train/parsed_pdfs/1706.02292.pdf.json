{
  "name" : "1706.02292.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "STACKED CONVOLUTIONAL AND RECURRENT NEURAL NETWORKS FOR MUSIC EMOTION RECOGNITION",
    "authors" : [ "Miroslav Malik", "Sharath Adavanne", "Konstantinos Drossos", "Tuomas Virtanen", "Dasa Ticha", "Roman Jarina" ],
    "emails" : [ "firstname.lastname@fel.uniza.sk", "firstname.lastname@tut.fi" ],
    "sections" : [ {
      "heading" : "1. INTRODUCTION",
      "text" : "Music has been used to transfer and convey emotions since the initial era of human communication [1, 2]. In the field of music signal processing, either perceived or induced emotion (see [3] for details) has been proven to be a strong and content-oriented attribute of music and has been employed for categorization tasks. For example, published works focus on emotion recognition from music in order to provide an alternative and content-oriented categorization scheme to the typical “Artist/Band/Year” one [4, 5].\nThere are two main categories of models for emotions; i) the discrete, and ii) the dimensional models. Discrete models of emotion employ a discrete group of words to model emotions. The basic emotions model [6] and the Hevner’s list [7] are the important representative examples of this group. The main drawback of discrete models of emotion is the subjective perception of the meaning of each word that is employed to describe an emotion. This leads to confusion when different words are utilized for the same emotion, e.g. “Joy”–“Enjoyment”\n*Equally contributing authors in this paper. The research leading to these results has received funding from the European Research Council under the European Unions H2020 Framework Programme through ERC Grant Agreement 637422 EVERYSOUND. Part of the computations leading to these results were performed on a TITAN-X GPU donated by NVIDIA. The authors also wish to acknowledge CSC-IT Center for Science, Finland, for computational resources.\nCopyright: c© 2017 Miroslav Malik et al. This is an open-access article distributed\nunder the terms of the Creative Commons Attribution 3.0 Unported License, which\npermits unrestricted use, distribution, and reproduction in any medium, provided\nthe original author and source are credited.\nand “Cheerfulness”–“Happiness” [2, 8]. Despite this confusion, discrete models are heavily used in works where the focus is on a specific emotion or emotional condition [9].\nThis categorical approach is also employed in the audio mood classification (AMC) task, under the music information retrieval evaluation exchange (MIREX) [10]. At AMC, the participants were requested to propose methods and systems capable of classifying audio files into five discrete emotion classes, using one class for the whole audio clip. The five classes contained five to seven adjectives and these adjectives were derived from online tags and were not based on some of the accepted categorical models of emotion. On average, the accuracy of the proposed methods and systems was below 70% [11, 12].\nDimensional models represent emotion in a continuous N -dimensional space. Usually, N = 2 and one dimension corresponds to arousal and the other to valence [13]. The emotion in a song is usually not spread equally through the time and it varies around some bias value. The bias value usually stays unchanged for a significant fraction of the duration of the song [14]. This property can be hardly observable in the categorical approach, but with the dimensional approach, the continuous changes can be captured more easily.\nA dimensional model was employed for the emotion in music (EiM) challenge, organized under the MediaEval benchmark [15]. To the best of authors’ knowledge, MediaEval along with MIREX are the two widely known international scientific challenges for music emotion recognition. The MediaEval challenge consisted of two tasks, one focusing on static and the other focused on the dynamic presence of emotion in music. In the second (dynamic) task of the EiM, the methods proposed were based on support vector regression (SVR) [16], continuous conditional neural fields [17], continuous conditional random fields [18], feed-forward neural networks [19], and recurrent neural networks [20]. The dataset for this dynamic task of EiM consisted of extracted audio features and emotional annotations. The features and the annotations were for frames of audio of length 500 ms. The best-reported results for this task were obtained using an ensemble of six long short-term memory (LSTM) RNNs with different input sequence lengths. The final output was predicted from the ensemble using an extreme learning machine. This method achieved a root-mean-square error (RMSE) of 0.230 for arousal and 0.303 for valence [21]. ar X iv :1 70 6.\n02 29\n2v 1\n[ cs\n.S D\n] 7\nJ un\n2 01\n7\nIn this paper, we propose a method that exploits the combined modeling capacities of the CNN, RNN, and fully connected (FC) network. We call this the stacked convolutional and recurrent neural network (CRNN). Similar architecture of stacked CNN, RNN, and FC has been used in literature and shown to consistently outperform the previous network configurations for sound event detection [22], speech recognition [23], and music classification [24]. We approach the problem as a regression one and evaluate the method using the EiM dynamic task dataset. We use RMSE as the metric in order to provide a direct comparison with results from existing works. Our proposed method employs only a small fraction of parameters compared to the recent state-of-the-art DBLSTM-based system by Li at al. [25] and performs with significantly better results (i.e. lower RMSE for both arousal and valence recognition). Additionally, we evaluate our method with a raw feature set (log mel-band energy). This is motivated from the fact that neural networks can by themselves learn the first order derivatives and statistics existing in the feature set provided in EiM dataset. These raw features are extracted from the same songs that were utilized in the EiM dataset. The results using the raw feature set are comparable to the ones achieved with the Li et al. system.\nThe rest of the paper is organized as follows. In Section 2 we present our proposed method. Section 3 describes the dataset, features, metric, baseline and the evaluation procedure. We analyze and discuss the obtained results in Section 4. Finally concluding the paper in Section 5."
    }, {
      "heading" : "2. PROPOSED METHOD",
      "text" : "The input to the proposed method is a sequence of audio features extracted from audio. We evaluate the method with two separate audio features - baseline (Section 3.2.1) and raw (Section 3.2.2). Our proposed method, illustrated in Figure 1 is a neural network architecture obtained by stacking a CNN with two branches of FC and RNN one each for arousal and valence. This combined CRNN architecture maps the input audio features into their respective arousal and valence values. The output of the method, arousal and valence values, is in the range of [-1, 1].\nIn our method, the local shift-invariant features are extracted from the audio features using a CNN with a receptive filter size of 3 × 3. The feature map of the CNN acts as the input to two parallel but identical branches, one for arousal and the other one for valence. Each of these branches consists of the FC, the bidirectional gated recurrent unit (GRU) [26], and the output layer consisting of one node of the maxout layer [27]. Both FC and maxout had their weights shared across time steps. The FC was utilized to reduce the dimensionality of the feature maps from the CNN and, consequently, reduce the number of parameters required by the GRU. The bidirectional GRU layer was utilized to learn the temporal information in the features. The maxout layer was employed as the regression layer due to its ability to approximate a convex, piecewise linear activation function [27].\nWe use the rectified linear unit (ReLU) [28] activation and the batch normalization [29] for the CNN layer. The\nFC uses the linear activation and the GRU’s use the tanh activation. In the bidirectional GRU, we concatenate together the forward and backward activations. The network was trained using the backpropagation through time alorithm [30], the Adam optimizer with the default parameters in [31], and the RMSE as the loss function. In order to reduce overfitting of the network to training data we use dropout [32] for the CNN and RNN layers and the ElasticNet [33] regularization for the weights and activity of the CNN. The network was implemented using the Keras framework [34] and the Theano backend [35]."
    }, {
      "heading" : "3. EVALUATION",
      "text" : ""
    }, {
      "heading" : "3.1 Dataset",
      "text" : "For the evaluation of our method, we utilized the dataset provided for the MediaEval’s EiM task [15]. For annotations, the Russell’s two-dimensional continuous emotional space [36] was used. The values of arousal and valence belonging to every 500 ms segment were annotated by five to seven annotators per song from the Amazon Mechanical Turk.\nThe training set consisted of 431 audio excerpts with a length of 45 seconds from the Free music archive. The first 15 seconds of excerpts were used for annotator’s adaptation. The annotations and features for the remaining 30 seconds were provided as the development set. This amounts to 60 annotations per audio file (30 s of audio with annotations every 500 ms). Each of the annotations are in the range of [-1, 1] for both arousal and valence. Negative values represents low arousal/valence and positive high.\nThe evaluation set consists of 58 full songs from the royalty free multitrack MedleyDB dataset and Jamendo music website. Similar features and annotations as development set were provided for the evaluation set."
    }, {
      "heading" : "3.2 Audio features",
      "text" : ""
    }, {
      "heading" : "3.2.1 Baseline features",
      "text" : "The feature set that we used is the one that was utilized in the EiM challenge. The most of research is based on this feature set, so we call it the baseline features set. It consists\nof the mean and standard deviation of 65 low-level acoustic descriptors and their first order derivatives from the 2013 INTERSPEECH Computational Paralinguistic Challenge [37], extracted with the openSMILE toolbox [38]. In the feature set are included mel frequency cepstral coefficients (MFCCs), spectral features such as flux, centroid, kurtosis, and rolloff, and voice related features such as jitter and shimmer. The total amount of features is 260 and they were extracted from non overlapping segments of length 500 ms. The mean and standard deviation of the features for this 500 ms were estimated from frames of 60 ms with 50 ms (≈ 80%) overlap."
    }, {
      "heading" : "3.2.2 Raw audio feature",
      "text" : "The above baseline features consist of mean, standard deviation, first and second order derivatives of different raw features. Neural networks by themselves can extract such statistics from the raw features directly. Thus, we argue that these features are not the best choice for neural networks. In order to study the performance of the proposed network with raw features, we employ just the log melband energies. Mel-band related features have been used previously for MediaEval EiM task [18] and MIREX AMC task [39]. The librosa python library [40] was used to extract the mel-band features from 500 ms segments, in a similar fashion to the baseline feature set."
    }, {
      "heading" : "3.3 Metric",
      "text" : "The RMSE is widely used in many areas as a statistical metric to measure a model performance. It represents a standard deviation of the differences of predicted values from the line of best fit at the sample level. Given N predicted samples ŷn and the corresponding reference samples yn, then RMSE between them can be written as:\nRMSE =\n√∑N n=1(ŷn − yn)2\nN (1)"
    }, {
      "heading" : "3.4 Baseline",
      "text" : "The proposed method is compared to the system proposed in [25]. To the best of our knowledge, this method has the top results on the MediaEval’s EiM dataset using the baseline audio features. This result was obtained using an ensemble of six bidirectional LSTM (BLSTM) networks of five layers and 256 units each, trained on sequence length of 10. The first two layers of these were pre-trained with the baseline features. The six networks of the ensemble were chosen such that they covered all the training data, and sequence lengths of 10, 20, 30 and 60. The output of the ensemble of networks was fused using SVR with a third order polynomial kernel and artificial neural network (ANN) of 14 nodes. The average output of SVR and ANN was used as the final arousal and valence values.\nIn terms of complexity of this baseline method, a five layer 256 input and output units BLSTM has about 2 million parameters. An ensemble of six BLSTMs compounded in this manner will have about 12 million param-"
    }, {
      "heading" : "8, time distributed FC",
      "text" : "eters. Additionally, SVR and ANN adds to further complexity of the overall method."
    }, {
      "heading" : "3.5 Evaluation procedure",
      "text" : "The hyperparameter estimation of the proposed method was done by varying the number of layers of the CNN, FC, and GRU from one to three, and the number of units for each of these were varied in the set of {4, 8, 16, 32}. Identical dropout rates were used for CNN and GRU and varied in the set of {0.25, 0.5, 0.75}. The ElasticNet variables L1 and L2 were each varied in the set of {1, 0.1, 0.01, 0.001, 0.0001}. The parameters were decided based on the best RMSE score on the development set, using the baseline features, mini-batch size of 32, and the maximal sequence length of 60. The mini-batch size of 32 was chosen from the set of {16, 32, 64, 128} based on the variance in the training set error and the number of iterations taken to achieve the best RMSE. The sequence length was chosen to be the same length as the number of annotations per audio file in the training set, therefore 30 s per every audio clip annotated every 0.5 s and this sequence length varied from 10 s to 60 s with the multiply of 2. The network was trained to achieve the lowest average RMSE of valence and arousal. The best configuration with least number of parameters had one layer each of CNN, FC and GRU with eight units each, a dropout rate of 0.25, L1 of 0.1 and L2 of 0.001. Figure 1 shows the configuration and the feature map dimensions of the proposed method.\nIn order to provide the comparison of the performance of CRNN without these two branches (CRNN NB), we removed one branch in the above configuration and trained the CRNN with both the valence and arousal on the same branch as seen in Figure 2.\nThe proposed CRNN method was trained with the raw features (log mel-band energy). In order to give a direct comparison of performance, we keep the exact configuration as the proposed CRNN with only the dropout changed to 0.75. The network was seen to overfit to the training\ndata with 0.25 dropout rate that used for baseline features. A dropout rate of 0.75 was chosen after tuning in the set of {0.25, 0.5, 0.75}."
    }, {
      "heading" : "4. RESULTS AND DISCUSSION",
      "text" : "Tables 1a and 1b present the RMSE scores on the development and evaluation datasets for the baseline and log melband features, respectively. These scores are the mean and standard deviation over five separate runs of the method with different initialization of network weights.\nThe proposed CRNN method with baseline features gave an average RMSE of 0.242 on evaluation set (see average RMSE for sequence length 60 in Table 1a). The Li et al. baseline method gave an average RMSE of 0.255 (see Table 2). In comparison, the CRNN method has only about 30 k parameters (about 400 times fewer) and fares significantly better in RMSE sense than the Li et al. system. Potentially, further improvement can be achieved by using an ensemble of the proposed method covering different sequence lengths and all training data as proposed in the Li et al. system.\nThe configuration of the CRNN NB method had about 17 k parameters and gave the same average RMSE as the Li et al. system (see CRNN NB in Table 2). This shows the robustness of the proposed method for the emotion recognition task.\nDifferent sequence lengths were experimented with the proposed CRNN, and the results are presented in Table 1. We use a maximum length of 60 which is equal to the number of annotations from a single file in development set, and its factors of 10, 20 and 30. We see that lower sequence lengths of 10 and 20 perform better than using the full-length sequence of 60. Similar observation was reported in [25]. The best average RMSE of 0.235 was achieved with sequences 10 and 20, this is the best result achieved in this paper and is 0.02 less than Li et al.\nWe compare the performance of the proposed CRNN with log mel-band features in Table 1b. Due to the reduced number of features, this network has only about 10 k parameters. With a simple and not so finely tuned network and raw features we get a best average RMSE of 0.258 with sequence length 10. This is very close to the Li et al. system performance with 1200 times fewer parameters. This proves our hypothesis that the neural network can learn the information of first and second order derivatives and first order statistics from the raw features on its own. The trend of shorter sequences performing better than longer sequences is also observed with log mel-band energy. A network tuned for log mel-band features specifically has a potential to perform better than the Li et al. system."
    }, {
      "heading" : "5. CONCLUSION",
      "text" : "In this paper, we proposed a method consisting of stacked convolutional and recurrent neural networks for continuous prediction of emotion in two-dimensional V-A space. The proposed method used significantly less amount of parameters than the Li et al. system and the obtained results outperform those of the Li et al. system.\nThe proposed CRNN was evaluated with different lengths of sequences, and the smaller sequence lengths were seen to perform better than the longer lengths. Log mel-band energy feature was proposed in place of baseline features, and the proposed CRNN was seen to learn information equivalent to that of baseline features from just the melband features."
    }, {
      "heading" : "6. REFERENCES",
      "text" : "[1] P. N. Juslin and K. R. Scherer, Vocal expression of affect. Oxford University Press, 2005, pp. 65–135.\n[2] P. N. Juslin and P. Laukka, “Communication of emotions in vocal expression and music performance: different channels, same code?” Psychological bulletin, vol. 129, no. 5, pp. 770–814, Sep 2003.\n[3] ——, “Expression, perception, and induction of musical emotions: A review and a questionnaire study of everyday listening,” Journal of New Music Research, vol. 33, no. 3, pp. 217–238, 2004.\n[4] S. Zhang, Q. Tian, S. Jiang, Q. Huang, and W. Gao, “Affective MTV analysis based on arousal and valence features,” in 2008 IEEE International Conference on Multimedia and Expo, June 2008, pp. 1369–1372.\n[5] T. Li and M. Ogihara, “Content-based music similarity search and emotion detection,” in 2004 IEEE International Conference on Acoustics, Speech, and Signal Processing, vol. 5, May 2004, pp. V–705–8 vol.5.\n[6] P. Ekman, “An argument for basic emotions,” Cognition and Emotion, vol. 6, pp. 169–200, May 1992.\n[7] K. Hevner, “Experimental studies of the elements of expression in music,” The American Journal of Psychology, vol. 48, pp. 246–268, 1936.\n[8] K. Drossos, R. Kotsakis, G. Kalliris, and A. Floros, “Sound events and emotions: Investigating the relation of rhythmic characteristics and arousal,” in IISA 2013, July 2013, pp. 1–6.\n[9] K. Drossos, A. Floros, and N.-G. Kanellopoulos, “Affective acoustic ecology: Towards emotionally enhanced sound events,” in Proceedings of the 7th Audio Mostly Conference: A Conference on Interaction with Sound, ser. AM ’12. New York, NY, USA: ACM, 2012, pp. 109–116. [Online]. Available: http://doi.acm.org/10.1145/2371456.2371474\n[10] X. Downie, C. Laurier, and M. Ehmann, “The 2007 MIREX audio mood classification task: Lessons learned,” in Proc. 9th Int. Conf. Music Inf. Retrieval, 2008, pp. 462–467.\n[11] “MIREX 2016: Audio train/test: Mood classification - MIREX08 dataset,” accessed 14.2.2017. [Online]. Available: http://www.music-ir.org/nema out/ mirex2016/results/act/mood report/summary.html\n[12] “MIREX 2011: MIREX 2010: Audio train/test: Mood classification - MIREX08 dataset,” accessed 14.2.2017. [Online]. Available: http://www.music-ir.org/nema out/mirex2011/ results/act/mood report/summary.html\n[13] K. Drossos, A. Floros, A. Giannakoulopoulos, and N. Kanellopoulos, “Investigating the impact of sound angular position on the listener affective state,” IEEE Transactions on Affective Computing, vol. 6, no. 1, pp. 27–42, Jan 2015.\n[14] H. Xianyu, X. Li, W. Chen, F. Meng, J. Tian, M. Xu, and L. Cai, “SVR based double-scale regression for dynamic emotion prediction in music,” in Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on. IEEE, 2016, pp. 549– 553.\n[15] A. Aljanaki, Y.-H. Yang, and M. Soleymani, “Emotion in music task at MediaEval 2015.” in MediaEval, 2015.\n[16] M. Chmulik, I. Guoth, M. Malik, and R. Jarina, “UNIZA system for the” emotion in music” task at mediaeval 2015.” in MediaEval, 2015.\n[17] V. Imbrasaite and P. Robinson, “Music emotion tracking with continuous conditional neural fields and relative representation.” in MediaEval, 2014.\n[18] K. Cai, W. Yang, Y. Cheng, D. Yang, and X. Chen, “PKU-AIPL’solution for MediaEval 2015 emotion in music task.” in MediaEval, 2015.\n[19] B. G. Patra, P. Maitra, D. Das, and S. Bandyopadhyay, “MediaEval 2015: Music emotion recognition based on feed-forward neural network.” in MediaEval, 2015.\n[20] T. Pellegrini and V. Barrière, “Time-continuous estimation of emotion in music with recurrent neural networks,” in MediaEval 2015 Multimedia Benchmark Workshop (MediaEval 2015), 2015, pp. 1–3.\n[21] M. Xu, X. Li, H. Xianyu, J. Tian, F. Meng, and W. Chen, “Multi-Scale approaches to the MediaEval 2015” emotion in music” task.” in MediaEval, 2015.\n[22] S. Adavanne, P. Pertilä, and T. Virtanen, “Sound event detection using spatial features and convolutional recurrent neural network,” in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2017.\n[23] T. N. Sainath, O. Vinyals, A. Senior, and H. Sak, “Convolutional, long short-term memory, fully connected deep neural networks,” in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015.\n[24] K. Choi, G. Fazekas, M. Sandler, and K. Cho, “Convolutional recurrent neural networks for music classification,” in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2017.\n[25] X. Li, J. Tian, M. Xu, Y. Ning, and L. Cai, “DBLSTMbased multi-scale fusion for dynamic emotion prediction in music,” in Multimedia and Expo (ICME), 2016 IEEE International Conference on. IEEE, 2016, pp. 1–6.\n[26] K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio, “Learning phrase representations using RNN encoderdecoder for statistical machine translation,” arXiv preprint arXiv:1406.1078, 2014.\n[27] I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and Y. Bengio, “Maxout networks,” in International Conference on Machine Learning (ICML), 2013.\n[28] V. Nair and G. E. Hinton, “Rectified linear units improve restricted boltzmann machines,” in Proceedings of the 27th international conference on machine learning (ICML-10), 2010, pp. 807–814.\n[29] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep network training by reducing internal covariate shift,” CoRR, vol. abs/1502.03167, 2015.\n[30] P. J. Werbos, “Backpropagation through time: what it does and how to do it,” Proceedings of the IEEE, vol. 78, no. 10, pp. 1550–1560, 1990.\n[31] D. Kingma and J. Ba, “Adam: A method for stochastic optimization,” in arXiv:1412.6980 [cs.LG], 2014.\n[32] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, “Dropout: A simple way to prevent neural networks from overfitting,” in Journal of Machine Learning Research (JMLR), 2014.\n[33] H. Zou and T. Hastie, “Regularization and variable selection via the elastic net,” in Journal of the Royal Statistical Society: Series B (Statistical Methodology), vol. 67, no. 2, 2005, pp. 301–320.\n[34] F. Chollet, “Keras,” https://github.com/fchollet/keras, 2015, accessed 16.01.2017.\n[35] Theano Development Team, “Theano: A Python framework for fast computation of mathematical expressions,” vol. abs/1605.02688, May 2016. [Online]. Available: http://arxiv.org/abs/1605.02688\n[36] J. A. Russell, “A circumplex model of affect,” Journal of Personality and Social Psychology, vol. 39, no. 6, pp. 1161–1178, 1980.\n[37] F. Weninger, F. Eyben, B. W. Schuller, M. Mortillaro, and K. R. Scherer, “On the acoustics of emotion in audio: what speech, music, and sound have in common,” Frontiers in Psychology, vol. 4, p. 292, 2013. [Online]. Available: http://journal.frontiersin. org/article/10.3389/fpsyg.2013.00292\n[38] F. Eyben, F. Weninger, F. Gross, and B. Schuller, “Recent developments in opensmile, the munich opensource multimedia feature extractor,” in Proceedings of the 21st ACM international conference on Multimedia. ACM, 2013, pp. 835–838.\n[39] T. Lidy and A. Schindler, “Parallel convolutional neural networks for music genre and mood classification,” MIREX2016, 2016.\n[40] B. McFee, M. McVicar, C. Raffel, D. Liang, O. Nieto, E. Battenberg, J. Moore, D. Ellis, R. Yamamoto, R. Bittner, D. Repetto, P. Viktorin, J. F. Santos, and A. Holovaty, “librosa: 0.4.1,” accessed 16.01.2017. [Online]. Available: http:\n//dx.doi.org/10.5281/zenodo.32193"
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "This paper studies the emotion recognition from musical tracks in the 2-dimensional valence-arousal (V-A) emotional space. We propose a method based on convolutional (CNN) and recurrent neural networks (RNN), having significantly fewer parameters compared with state-of-theart method for the same task. We utilize one CNN layer followed by two branches of RNNs trained separately for arousal and valence. The method was evaluated using the “MediaEval2015 emotion in music” dataset. We achieved an RMSE of 0.202 for arousal and 0.268 for valence, which is the best result reported on this dataset.",
    "creator" : "LaTeX with hyperref package"
  }
}