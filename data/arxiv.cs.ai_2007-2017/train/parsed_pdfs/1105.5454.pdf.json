{
  "name" : "1105.5454.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "David E. Joslin", "David P. Clements" ],
    "emails" : [ "joslin@i2.com", "clements@cirl.uoregon.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Journal of Arti cial Intelligence Research 10 (1999) 353-373 Submitted 8/98; published 5/99 \\Squeaky Wheel\" OptimizationDavid E. Joslin david joslin@i2.comi2 Technologies909 E. Las Colinas Blvd.Irving, TX 75039David P. Clements clements@cirl.uoregon.eduComputational Intelligence Research Laboratory1269 University of OregonEugene, OR 97403-1269 AbstractWe describe a general approach to optimization which we term \\Squeaky Wheel\" Op-timization (swo). In swo, a greedy algorithm is used to construct a solution which isthen analyzed to nd the trouble spots, i.e., those elements, that, if improved, are likelyto improve the objective function score. The results of the analysis are used to generatenew priorities that determine the order in which the greedy algorithm constructs the nextsolution. This Construct/Analyze/Prioritize cycle continues until some limit is reached, oran acceptable solution is found.SWO can be viewed as operating on two search spaces: solutions and prioritizations.Successive solutions are only indirectly related, via the re-prioritization that results fromanalyzing the prior solution. Similarly, successive prioritizations are generated by con-structing and analyzing solutions. This \\coupled search\" has some interesting properties,which we discuss.We report encouraging experimental results on two domains, scheduling problems thatarise in ber-optic cable manufacturing, and graph coloring problems. The fact that thesedomains are very di erent supports our claim that swo is a general technique for optimiza-tion.1. OverviewWe describe a general approach to optimization which we term \\Squeaky Wheel\" Optimiza-tion (SWO) (Joslin & Clements, 1998). The core of swo is a Construct/Analyze/Prioritizecycle, illustrated in Figure 1. A solution is constructed by a greedy algorithm, making de-cisions in an order determined by priorities assigned to the elements of the problem. Thatsolution is then analyzed to nd the elements of the problem that are \\trouble makers.\" Thepriorities of the trouble makers are then increased, causing the greedy constructor to dealwith them sooner on the next iteration. This cycle repeats until a termination conditionoccurs.On each iteration, the analyzer determines which elements of the problem are causingthe most trouble in the current solution, and the prioritizer ensures that the constructorgives more attention to those elements on the next iteration. (\\The squeaky wheel gets thegrease.\") The construction, analysis and prioritization are all in terms of the elements thatc 1999 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.\nJoslin & Clements Analyzer\nPriorities\nBlameSolution\nConstructor PrioritizerFigure 1: The Construct/Analyze/Prioritize cyclede ne a problem domain. In a scheduling domain, for example, those elements might betasks. In graph coloring, those elements might be the nodes to be colored.The three main components of swo are:Constructor. Given a sequence of problem elements, the constructor generates a solutionusing a greedy algorithm, with no backtracking. The sequence determines the orderin which decisions are made, and can be thought of as a \\strategy\" or \\recipe\" forconstructing a new solution. (This \\solution\" may violate hard constraints.)Analyzer. The analyzer assigns a numeric \\blame\" factor to the problem elements thatcontribute to aws in the current solution. For example, if minimizing lateness ina scheduling problem is one of the objectives, then blame would be assigned to latetasks.A key principle behind swo is that solutions can reveal problem structure. By ana-lyzing a solution, we can often identify elements of that solution that work well, andelements that work poorly. A resource that is used at full capacity, for example, mayrepresent a bottleneck. This information about problem structure is local, in that itmay only apply to the part of the search space currently under examination, but maybe useful in determining where the search should go next.Prioritizer. The prioritizer uses the blame factors assigned by the analyzer to modifythe previous sequence of problem elements. Elements that received blame are movedtoward the front of the sequence. The higher the blame, the further the element ismoved.The priority sequence plays a key role in swo. As a di cult problem element movesforward in the sequence it is handled sooner by the constructor. It also tends to be handledbetter, thus decreasing its blame factor. Di cult elements rise rapidly to a place in thesequence where they are handled well. Once there, the blame assigned to them drops,causing them to slowly sink in the sequence as other parts of the problem that are nothandled as well are given increased priority. Eventually, di cult elements sink back to thepoint where they are no longer handled well, causing them to receive higher blame and tomove forward in the sequence again. Elements that are always easy to handle sink to theend of the sequence and stay there. 354\n\\Squeaky Wheel\" Optimization C A B 50403020100Priorities: C,A,B Late: A (20), B (30) Iteration: 1\nA CB\n50403020100Priorities: B,A,C Late: A (20), C (10) Iteration: 2\nA C B\n50403020100Priorities: A,C,B Late: B (30) Iteration: 3\n10 20 20\n10 20 40 Task Duration Deadline A B C\nFigure 2: Simple exampleTo illustrate the swo cycle, consider a simpli ed scheduling example. Suppose we havea single production line, and three tasks to schedule, A, B and C. Only one task can beperformed at a time. Execution starts at t = 0. The duration and deadline for each taskis shown in Figure 2. The objective is to minimize the number of late tasks. An optimalsolution will have one late task.Suppose our initial priority sequence is hC;A;Bi, and the constructor schedules tasksin order, at the earliest possible time. The resulting schedule has two late tasks (B andA). Suppose that our analyzer assigns one point of \\blame\" to each late task, for eachunit of time it is late. In this case, A, B, and C receive 20, 30 and 0 units of blame,respectively. Figure 2 shows the prioritization, the schedule that the constructor buildsfrom that prioritization, and the late tasks with the blame assigned to each.For the next cycle, the prioritizer must take the previous priority sequence, and theblame assigned by the analyzer, and generate a new priority sequence. A simple prioritizermight just sort the tasks by their numeric blame in descending order, resulting in the newpriority sequence hB;A;Ci.After the second cycle, tasks A and C are late, scoring 20 and 10 points of blame,respectively. The new priority sequence is then hA;C;Bi.The third solution, constructed from this priority sequence, has only one late task, B,which receives 30 points of blame. At this point we have an optimal solution. If we continuerunning swo, however, as we might expect to do since we typically do not know when wehave reached optimality, swo will attempt to x what was wrong with the current solution.Here, since task B was late, its priority would be increased, and the resulting solution would x that problem at the expense of others. (We would also enter a short cycle, alternatingbetween the last two schedules. We address this by introducing some randomization in theprioritizer.)Although this example is highly simpli ed, and there would clearly be better and moresophisticated ways to implement each of the three modules, Figure 3 shows that the behaviorillustrated by the simple example is re ected in a real domain. The gure shows the changingposition in the priority sequence of three tasks in the scheduling domain that is describedin detail in the following section. One task (\\Job 24\") starts out with a high priority, andremains at a relatively high priority level. We can see that when the task is scheduled355\nJoslin & Clements\nJob 24\n9 10 11 12 13 14 15 16 17 18 19 201 2 3 4 5 6 7 8\nJob 39 Job 26\nPr io\nri ty\nIteration\n(h ig h) (l ow )\nFigure 3: Examples of priority changes over timee ectively, and therefore receives little or no blame, its priority tends to drop, but it doesnot have to drop very far before it ceases to be scheduled well, acquires a signi cant levelof blame, and moves quickly back to a higher priority.The other two tasks shown in the gure behave quite di erently. One task (\\Job 39\")starts out with a relatively high priority, but this task is \\easy\" to schedule, with littleblame, even when it is scheduled late in the sequence. Over successive iterations, thepriority of such a task will tend to decrease steadily. The other task illustrated here (\\Job26\") does just the opposite, starting at a low priority and moving fairly steadily toward ahigh priority.The following section discusses the characteristics of swo that make it an e ectivetechnique for optimization. We then discuss implementations of swo for scheduling andfor graph coloring problems. The nal sections discuss related work, describe directions forfuture research, and summarize our ndings.2. Key ideasAs the experimental results below show, swo is a general approach to optimization. In thissection, we explore a few insights into what makes swo e ective.It is useful to think of swo as searching two coupled spaces, as illustrated in Figure 4.One search space is the familiar solution space, and the other is priority space. Moves inthe solution space are made indirectly, via the re-prioritization that results from analyzingthe prior solution. Similarly, successive prioritizations are generated by constructing andanalyzing a solution, and then using the blame that results from that analysis to modifythe previous prioritization.A point in the solution space represents a potential solution to the problem, and acorresponding point in priority space, derived by analyzing the solution, is an attempt to356\n\\Squeaky Wheel\" Optimization"
    }, {
      "heading" : "Analyze/ Prioritize",
      "text" : "Solution space\nPriority space\np\ns\nConstruct\np’\nConstruct\ns’\nFigure 4: Coupled search spacescapture information about the structure of the search space in the vicinity of the solution.As swo constructs a new solution from scratch, the priorities can be thought of as pro-viding information about pitfalls common to the current region of the solution space. Ifsome elements of the solution have tended to be sources of di culty over some number ofiterations, increasing their priority makes it more likely that the constructor will handlethose elements in a good way.One consequence of the coupled search spaces is that a small change in the sequence ofelements generated by the prioritizer may correspond to a large change in the correspondingsolution generated by the constructor, compared to the solution from the previous itera-tion. Moving an element forward in the sequence can signi cantly change its state in theresulting solution. In addition, any elements that now occur after it in the sequence mustaccommodate that element's state. For example, in the scheduling domain, moving a taskearlier in the priority sequence may allow it to be placed on a di erent manufacturing line,thus possibly changing the mix of jobs that can run on that line, and on the line it wasscheduled on in the previous iteration. One small change can have consequences for anyelement that follows it, with lower-priority tasks having to \\ ll in the gaps\" that are leftafter higher-priority tasks have been scheduled.The result is a large move that is \\coherent\" in the sense that it is similar to what wemight expect from moving the higher priority task, then propagating the e ects of thatchange by moving lower priority tasks as needed. This single move may correspond to alarge number of moves for a search algorithm that only looks at local changes to the solution,and it may thus be di cult for such an algorithm to nd.The fact that swo makes large moves in both search spaces is one obvious di erencebetween swo and traditional local search techniques, such as wsat (Selman, Kautz, &Cohen, 1993). Another di erence is that with swo, moves are never selected based on theire ect on the objective function. Instead, unlike hillclimbing techniques, each move is made357\nJoslin & Clementsin response to \\trouble spots\" found in the current solution. The resulting move may beuphill, but the move is always motivated by those trouble spots.In priority space the only \\local optima\" are those in which all elements of a solution areassigned equal blame. swo tends to avoid getting trapped in local optima, because analysisand prioritization will always (in practice) suggest changes in the sequence, thus changingthe solution generated on the next iteration. This does not guarantee that swo will notbecome trapped in a small cycle, however. In our implementations we have introducedsmall amounts of randomness in the basic cycle. We also restart swo periodically with anew initial sequence.Another aspect of local search is that typically each point in the solution space is as-sociated with a single value, the objective function score for that solution. When we talkabout hillclimbing, we generally refer to the \\terrain\" described by this objective functionscore, over the space of solutions. The process of analysis in swo can be thought of assynthesizing a more complex description of that terrain, by breaking a solution down intoits component elements and assigning a score to each. Prioritization then translates theanalysis into a \\strategy\" that the constructor can use to generate the next solution.Assigning scores to the individual elements of a solution allows swo to take advantageof the fact that real problems often combine some elements that are di cult to get right,plus others that are easy. In the scheduling problems presented below, some tasks can beassigned to just a few production lines, while others allow for much more exibility. Somehave due dates close to their release time, while others have a lot of leeway. It is sometimespossible to identify \\di cult\" elements of a problem with static analysis, but interactionscan be complex, and elements that are causing di culty in one part of the search space maybe no trouble at all in another. Rather than trying to identify elements that are globallydi cult by analyzing the entire problem, swo analyzes individual solutions in order to ndelements that are locally di cult. Globally di cult elements tend to be identi ed over time,as they are di cult across large parts of the search space.By assigning blame and adjusting priorities based on identi ed problems in actual so-lutions, swo avoids dependence on complex, domain dependent heuristics. It is our beliefthat this independence is particularly important in complex domains where even the bestheuristics will miss some key interactions and therefore inhibit the search from exploringgood areas that the heuristic incorrectly labels as unpromising. swo uses actual solutionsto discover which areas of the search space are promising and which are not.3. SWO for schedulingThis section describes an application of swo to a ber-optic production line schedulingproblem, derived from data provided by Lucent Technologies. In this particular plant, acable may be assembled on any one of 13 parallel production lines. For each cable type,only a subset of the production lines are compatible, and the time required to produce thecable will depend on which of the compatible lines is selected. Each cable also has a setuptime, which depends on its own cable type and that of its predecessor. Setups betweencertain pairs of cable types are infeasible. Task preemption is not allowed, i.e. once a cablehas started processing on a line, it nishes without interruption.358\n\\Squeaky Wheel\" OptimizationEach cable is assigned a release time and due date. Production cannot begin before therelease time. The objective function includes a penalty for missing due dates, and a penaltyfor setup times.3.1 ImplementationWe describe the implementation in terms of the three main components of swo:Constructor. The constructor builds a schedule by adding tasks one at a time, in theorder they occur in the priority sequence. A task is added by selecting a line and aposition relative to the tasks already in that line. A task may be inserted betweenany two tasks already in the line or at the beginning or end of that line's schedule.Changes to the relative positions of the tasks already in the line are not considered.Each task in the line is then assigned to its earliest possible start time, subject to theordering, i.e., a task starts at either its release time, or immediately after the previoustask on that line, whichever is greater.For each of the possible insertion points in the schedule, relative to the tasks already ineach line, the constructor calculates the e ect on the objective function, and the taskis placed at the best-scoring location. Ties are broken randomly. After all tasks havebeen placed, the constructor applies swo to the individual line schedules, attemptingto improve the score for each line by reordering the cables that were assigned to it.Analyzer. To assign blame to each task in the current schedule, the analyzer rst calculatesa lower bound on the minimum possible cost that each task could contribute to anyschedule. For example, if a task has a release time that is later than its due date,then it will be late in every schedule, and the minimum possible cost already includesthat penalty. Minimum possible setup costs are also included. For a given schedule,the blame assigned to each task is its \\excess cost,\" the di erence between its actualcost and its minimum possible cost. Excess lateness costs are assigned to tasks thatare late, and excess setup costs are split between adjacent tasks.Prioritizer. Once the blame has been assigned, the prioritizer modi es the previous se-quence of tasks by moving tasks with non-zero blame factors forward in the sequence.Tasks are moved forward a distance that increases with the magnitude of the blame.To move from the back of the sequence to the front, a task must have a high blamefactor over several iterations. We call this a \\sticky sort.\"Our current implementation has considerable room for improvement. The analysis andfeedback currently being used are very simple, and the construction of schedules could takevarious heuristics into account, such as preferring to place a task in a line that has more\\slack,\" all other things being equal.3.2 Experimental resultsWe have six sets of test data, ranging in size from 40 to 297 tasks, all with 13 parallelproduction lines. The largest problem was the largest that the manufacturer required inpractice. We compare the following solution methods:359\nJoslin & Clements swo tabu ipData Best Avg AvgSet Obj Obj Time Obj Time Obj Time40 1890 1890 48 1911 425 1934 2050 3101 3156 57 3292 732 3221 17560 2580 2584 87 2837 1325 2729 614470 2713 2727 124 2878 2046 2897 4950148 8869 8927 431 10421 17260 | |297 17503 17696 1300 | | | |Table 1: Experimental results: schedulingswo Applies the swo architecture to the problem, running for a xed number of iterationsand returning the best schedule it nds.tabu Uses tabu search (Glover & Laguna, 1997), a local search algorithm in which movesthat increase cost are permitted to avoid getting trapped at local optima. To avoidcycling, when an \\uphill\" move is made, it is not allowed to be immediately undone.ip Applies an Integer Programming (IP) solver, using an encoding described in (Clementset al., 1997).On the 297 task problem, swo was far more e ective than either tabu or ip. tabu,for example, failed to nd a feasible schedule after running for over 24 hours. On thesmallest problems, tabu and ip were able to nd solutions, but swo outperformed both bya substantial margin.Table 1 presents results on each problem for swo, tabu and ip. For swo, ten trialswere run and the results averaged. The tabu and ip implementations were deterministic,so only the results of a single run are shown. The second column of the table shows the bestobjective function value we have ever observed on each problem. The remaining columnsshow the objective function value and running times for swo, tabu and ip. All but the ipexperiments were run on a Sun Sparcstation 10 Model 50. The ip experiments were run onan IBM RS6000 Model 590 (a faster machine).The best values observed have been the result of combining swo with ip, as reportedin (Clements et al., 1997). In that work, swo generated solutions, running until it hadproduced a number of \\good\" schedules. An ip solver was then invoked to re-combineelements of those solutions into a better solution. Although the improvements achievedby the IP solver were relatively small, on the order of 1.5%, it achieved this improvementquickly, and swo was unable to achieve the same degree of optimization even when givensubstantially more time. While noting that the hybrid approach can be more e ective thanswo alone, and much more e ective than ip alone, here we focus on the performance of theindividual techniques.We also note that our very rst, fairly naive implementation of swo for these schedulingproblems already outperformed both tabu and ip. Moreover, our improved implementation,360\n\\Squeaky Wheel\" Optimization 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 0 2 4 6 8 10\n0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 0\n2\n4\n6\n8\n10\nPosition in priority sequence\n# of\nli ne\ns jo\nb in\nth at\np os\niti on\nc an\nr un\no n\nPosition in priority sequence Order based on # of lines job can run on\nOrder after 14th iteration, producing a solution 0.05% over best knownFigure 5: Comparison of heuristic priorities and priorities derived by sworeported above, is still fairly simple, and is successful without relying on domain-dependentheuristics. We take this as evidence that the e ectiveness of our approach is not dueto cleverness in the construction, analysis and prioritization techniques, but due to thee ectiveness of the swo cycle at identifying and responding to whatever elements of theproblem happen to be causing di culty in the local region of the search.It is also instructive to compare the results of a good heuristic ordering, with the se-quence derived by swo. A good heuristic for this scheduling domain (and the one thatis used to initially populate the priority sequence) is to sort the tasks by the number ofproduction lines on which a task could be feasibly assigned in an empty schedule. A taskthat can be scheduled on many lines is likely to be easier to schedule than one that iscompatible with only a small number of lines, and should therefore be expected to need alower priority. The top graph in Figure 5 shows the sequence of tasks, as determined bythis heuristic. The lower graph illustrates the changes in priority of these tasks, after swohas run for fourteen iterations (enough to improve the solution derived from the sequenceto within 0.05 percent of the best known solution).As the gure illustrates, the heuristic is generally accurate, but swo has had to movesome tasks that are compatible with most of the production lines to positions of relativelyhigh priority, re ecting the fact that contrary to the heuristic, these tasks turned out to berelatively di cult to schedule well. Other tasks that are compatible with only a few pro-duction lines are actually easy to schedule well, and have moved to relatively low priorities.361\nJoslin & Clements Iterations Feasible < 18000 < 17700per Success Mean Success Mean Success Mean SampleRestart Rate Cost Rate Cost Rate Cost Size10 0.8542 5.9 0.0504 195.3 0.0002 49994.5 1000020 0.9722 6.0 0.2052 90.9 0.0006 33328.3 500030 0.9955 5.8 0.3812 67.5 0.0030 9895.5 330040 0.9996 5.8 0.5488 56.7 0.0060 6658.2 250050 0.9995 6.0 0.6330 57.0 0.0160 3112.7 200060 1.0000 5.7 0.7242 52.9 0.0188 3170.4 165070 1.0000 5.7 0.8079 50.2 0.0350 1973.5 140080 1.0000 6.2 0.8552 49.5 0.0296 2670.0 125090 1.0000 5.8 0.8827 48.9 0.0300 2965.3 1100100 1.0000 5.9 0.8840 52.4 0.0400 2452.3 1000200 1.0000 6.0 0.9680 53.0 0.0600 3204.3 500300 1.0000 5.3 0.9967 50.1 0.0567 5090.8 300400 1.0000 5.8 1.0000 52.9 0.0720 5320.2 250500 1.0000 5.8 1.0000 52.8 0.1000 4692.6 200600 1.0000 5.8 1.0000 57.2 0.0867 6590.8 150700 1.0000 6.1 1.0000 42.4 0.1200 5472.4 100800 1.0000 5.6 1.0000 53.0 0.1200 6210.3 100900 1.0000 5.3 1.0000 45.8 0.1700 4691.6 1001000 1.0000 6.0 1.0000 45.4 0.1800 4838.1 100Table 2: Experimental results: restarts in the scheduling domain3.3 RestartsThe swo solver used to produce the results reported in Table 1 restarted the priority queueevery n=2 iterations, where n is the number of jobs in the problem. The same noisy heuristicthat was used to initially populate the priority queue was also used to restart it. This restartcuto was picked in a rather ad hoc manner. A more careful analysis of di erent restartcuto values might lead to producing better solutions faster, and to some additional insighton the workings of swo.Restarts are often used in non-systematic search to avoid getting trapped in local optimaor in cycles. (See Parkes and Walser, 1996, for an empirical study of wsat and furtherreferences.) Restarts have also been used in systematic search to escape exponentially largeregions of the search space that do not contain a solution (Gomes, Selman, & Kautz, 1998).Local optima pose little threat to swo, since it is not directly driven by uphill/downhillconsiderations. swo, through its use of large coherent moves, also tends to escape un-promising parts of the search space quickly. However, swo is open to getting trapped in acycle, and restarts are used as a means to escape them.For these scheduling problems, swo is unlikely to get into a tight cycle where priorityqueues and solutions repeat exactly. This is due to the presence of random tie breaking inseveral places, and to the presence of noise in the prioritizer. However, it is our belief thatswo can get trapped in a cycle where similar priority queues and solutions repeat.We ran a series of experiments with the 297 task problem to determine the impact ofvarious restart cuto s. The results are summarized in Table 2. Restart cuto s ranged fromafter every 10 iterations to after every 1000 iterations. The success rate and mean cost are362\n\\Squeaky Wheel\" Optimizationshown for each value for each of three di erent solution qualities. The success rate indicatesthe probability that a solution of at least the given quality was found in a given pass. Themean cost is the average number of total iterations to get a solution of that quality.For the feasible and 18000 solution thresholds, swo reaches a 100 percent success ratewell before reaching the maximum restart cuto of 1000 used in these experiments. In somesense, it is easy for swo to produce solutions that are at least of these qualities. The resultsfor these 2 thresholds indicate that when it is easy for swo to solve the problem, any cuto greater than the average number of uninterrupted iterations it takes to produce a solutioncan be used to solve the problem at minimum cost. For such \\easy\" problems, it appearsthat too small a restart cuto can hurt, but that too big a cuto will not.The numbers for the 17700 solution quality threshold, tell a di erent story. The successrate is still climbing when the experiment ends, and the mean cost has actually risen aboveits minimum. For this solution quality, the restart cuto that minimizes mean cost fallsaround the range of 70 to 100. Mean costs rise steeply for restart cuto s below this range,and slowly for cuto s larger than that. This is an example of a hard problem for swo, and itshows that some care needs to be taken when choosing a restart strategy for such problems.Additional research is needed to determine how to set the restart cuto automatically forarbitrary problems.This data indicates that swo does bene t from restarts, up to a point. With the 17700threshold, for restart cuto s up to 100, each increase in the cuto in general led to asuperlinear increase in the success rate. (This is also another indicator that swo is learningfrom iteration to iteration.) Above 100 iterations per restart, the success rate initiallyclimbs sublinearly and then appears to level out. It is an open question what this tells usabout the search space.4. SWO for graph coloringWe have also applied swo to a very di erent domain, graph coloring. Here the objectiveis to color the nodes of a graph such that no two adjoining nodes have the same color,minimizing the number of colors.4.1 ImplementationThe priority sequence for graph coloring consists of an ordered list of nodes. The solver isalways trying to produce a coloring that uses colors from the target set, which has one lesscolor than was used to color the best solution so far. Again, we describe the implementationin terms of the three main components of swo:Constructor. The constructor assigns colors to nodes in priority sequence order. If anode's color in the previous solution is still available (i.e. no adjacent node is usingit yet), and is in the target set, then that color is assigned. If that fails, it tries toassign a color in the current target set, picking the color that is least constraining onadjacent uncolored nodes, i.e. the color that reduces the adjacent nodes' remainingcolor choices the least. If none of the target colors are available, the constructor triesto \\grab\" a color in the target set from its neighbors. A color can only be grabbedif all neighbor nodes with that color have at least one other choice within the target363\nJoslin & Clementsset. If multiple colors can be grabbed, then the least constraining one is picked. If nocolor in the target set can be grabbed then a color outside the target set is assigned.Nodes that are early in the priority sequence are more likely to have a wide range ofcolors to pick from. Nodes that come later may grab colors from earlier nodes, butonly if the earlier nodes have other color options within the target set.Analyzer. Blame is assigned to each node whose assigned color is outside the target set,with the amount of blame increasing for each additional color that must be addedto the target set. We ran experiments with several di erent variations of color-basedanalysis. All of them performed reasonably.Prioritizer. The prioritizer modi es the previous sequence of nodes by moving nodes withblame forward in the sequence according to how much blame each received. This isdone the same way it is done for the scheduling problems. The initial sequence is a listof nodes sorted in decreasing degree order, with some noise added to slightly shu ethe sort.4.2 Experimental resultsWe applied swo to a standard set of graph coloring problems, including random graphs andapplication graphs that model register allocation and class scheduling problems. These werecollected for the Second DIMACS Implementation Challenge (Johnson & Trick, 1996), whichincludes results for several algorithms on these problems (Culberson & Luo, 1993; Glover,Parker, & Ryan, 1993; Lewandowski & Condon, 1993; Morgenstern, 1993). Problems rangefrom 125 nodes with 209 edges to 4000 nodes with 4,000,268 edges.Glover et al. (1993) is the only paper that is based on a general search technique, tabuwith branch and bound, rather than a graph coloring speci c algorithm. This approachhad the worst reported average results in the group. Morgenstern (1993) used a distributedimpasse algorithm and had the best overall colorings, but also required that the targetnumber of colors, as well as several other problem speci c parameters be passed to thesolver. Lewandowski & Condon (1993) also found good solutions for this problem set.Their approach used a hybrid of parallel impasse and systematic search on a 32 processorCM-5. Culberson & Luo (1993) used an Iterated Greedy (ig) algorithm that bears somesimilarity to swo. ig is the simplest algorithm in the group. Its solution quality fallsbetween the impasse algorithms and tabu but solves the entire set in 1 to 2 percent of thetime taken by the other methods. Both ig and impasse are discussed further under relatedwork.Table 3 compares swo with the results for ig (Culberson & Luo, 1993), distributedimpasse (Morgenstern, 1993), parallel impasse (Lewandowski & Condon, 1993), and tabu(Glover et al., 1993). For each, one column shows the number of colors required for eachproblem, and the run time (in CPU seconds). Bold face indicates that the number of colorsis within 0.5 of the best result in the table.We used a Pentium Pro 333MHz workstation running Linux for the swo graph coloringexperiments. The times shown for the other four algorithms are based on those reported in(Johnson & Trick, 1996). The results for ig, impasse and tabu are normalized to our times364\n\\Squeaky Wheel\" Optimization swo ig Dist. impasse Par. impasse tabuProblem colors time colors time colors time colors time colors timeDSJC125.5 18.3 1.6 18.9 2.5 17.0 6.3 17.0 4043.6 20.0 153.3DSJC250.5 31.9 8.3 32.8 6.9 28.0 268.5 29.2 4358.1 35.0 3442.2DSJC500.5 56.3 40.9 58.6 18.2 49.0 8109.1 53.0 4783.9 65.0 3442.2DSJC1000.5 101.5 208.6 104.2 67.6 89.0 41488.7 100.0 5333.8 117.0 3442.2C2000.5 185.7 1046.2 190.0 272.4 165.0 14097.9 | | | |C4000.5 341.6 4950.8 346.9 1054.1 | | | | | |R125.1 5.0 0.2 5.0 2.0 5.0 0.2 5.0 64.6 5.0 0.4R125.1c 46.0 5.1 46.0 1.1 46.0 0.2 46.0 85.0 46.0 0.9R125.5 36.0 2.8 36.9 1.9 36.0 0.2 37.0 33.0 36.0 0.7R250.1 8.0 0.5 8.0 7.0 8.0 0.2 8.0 22.0 8.0 0.2R250.1c 64.0 30.6 64.0 4.6 64.0 0.5 64.0 278.2 65.0 46.4R250.5 65.0 14.7 68.4 8.3 65.0 82.2 66.0 39.9 66.0 59.0DSJR500.1 12.0 2.0 12.0 21.1 12.0 0.2 12.0 26.6 12.0 0.5DSJR500.1c 85.2 96.9 85.0 14.6 85.0 59.1 85.2 5767.7 87.0 3442.2DSJR500.5 124.1 68.7 129.6 26.1 123.0 175.3 128.0 90.5 126.0 395.1R1000.1 20.0 8.0 20.6 87.2 20.0 8.2 20.0 49.9 20.0 1.7R1000.1c 101.7 433.2 98.8 49.1 98.0 563.3 102.6 3940.0 105.0 3442.2R1000.5 238.9 574.5 253.2 102.9 241.0 944.0 245.6 215.9 248.0 3442.2 at300 20 0 25.3 16.4 20.2 3.8 20.0 0.2 20.0 274.3 39.0 3442.2 at300 26 0 35.8 12.0 37.1 7.7 26.0 10.0 32.4 6637.1 41.0 3442.2 at300 28 0 35.7 11.9 37.0 9.6 31.0 1914.2 33.0 1913.5 41.0 3442.2 at1000 50 0 100.0 203.9 65.6 146.3 50.0 0.2 97.0 7792.7 | | at1000 60 0 100.7 198.0 102.5 87.3 60.0 0.2 97.8 6288.4 | | at1000 76 0 100.6 208.4 103.6 79.6 89.0 11034.0 99.0 6497.9 | |latin sqr 10 111.5 369.2 106.7 59.7 98.0 5098.0 109.2 6520.1 130.0 3442.2le450 15a 15.0 5.5 17.9 17.0 15.0 0.2 15.0 162.6 16.0 17.8le450 15b 15.0 6.1 17.9 16.2 15.0 0.2 15.0 178.4 15.0 28.4le450 15c 21.1 8.0 25.6 14.5 15.0 57.2 16.6 2229.6 23.0 3442.2le450 15d 21.2 7.8 25.8 13.5 15.0 36.3 16.8 2859.6 23.0 3442.2mulsol.i.1 49.0 5.9 49.0 4.2 49.0 0.2 49.0 27.2 49.0 0.3school1 14.0 8.4 14.0 10.5 14.0 0.2 14.0 46.3 29.0 90.7school1 nsh 14.0 7.2 14.1 8.9 14.0 0.2 14.0 66.4 26.0 31.2Table 3: Experimental results: graph coloring problemsusing the DIMACS benchmarking program dfmax, provided for this purpose. Therefore,timing comparisons are only approximate. Our machine ran the dfmax r500.5 benchmarkin 86.0 seconds; the times reported for the machines used on the other algorithms were86.9 seconds for the tabu experiments, 192.6 seconds for ig, 189.3 seconds for impasse,and 2993.6 seconds for parallel impasse. Because the dfmax benchmark runs on a singleprocessor, it is unsuitable for normalizing the times for parallel IMPASSE. We report theirunnormalized times.A variety of termination conditions were used. swo terminated after 1000 iterations.ig terminated after 1000 iterations without improvement. Distributed impasse used awide variety of di erent termination conditions to solve the di erent problems. The onlycommon element across problems was that distributed impasse stopped when the targetnumber of colors, provided as an input parameter, was reached. The times reported for365\nJoslin & Clements\n0\n5\n10\n15\n20\n25\n0 10000 20000 30000 40000 50000 60000\nA vg\n. p er\nce nt\no ve\nr be\nst in\ng ro\nup\nTime (CPU seconds)\nIterated Greedy\nSqueaky Wheel\nTABU\nPar IMPASSE\nDist IMPASSEFigure 6: Experimental results: quality of solution vs. timeparallel impasse are the times it took to nd the best solution that was found, not the timeit took the algorithm to terminate, which was always 3 hours. tabu ran until the algorithmdetermined that it could make no further progress, or an hour had passed, whichever came rst.The tabu numbers are for a single run on each problem. The numbers for the otheralgorithms are averages for 4 runs (parallel impasse), 5 runs (distributed impasse, parallelimpasse) or 10 runs (swo, ig, distributed impasse) on each problem.Figure 6 summarizes the performance of each technique on the set of 27 problems thatall of the algorithms solved. For each solver the graph indicates the average solution qualityand the average amount of time needed to solve the set. The ideal location on the graphis the origin, producing high quality solutions in very little time. The points shown for theother techniques are the points reported in each of the papers. The curve shown for swoshows how it performs when given varying amounts of time to solve the set. As the graphshows, swo clearly outperforms tabu, the only other general purpose technique, both interms of quality and speed. swo also outperforms ig, a graph coloring speci c algorithm,both in terms of quality and speed. The impasse solvers clearly produce the best solutionsin the group. However, impasse is a domain speci c method, and both solvers representmuch more programming e ort. The swo solver uses a general purpose search techniqueand was implemented in less than a month by a single programmer.4.3 Alternate con gurations of swoWe note that, as with the scheduling work, our rst, naive implementation of swo for graphcoloring produced respectable results. Even without color reuse, color grabbing, or the leastconstraining heuristic (the rst free color found was picked), swo matched ig on 6 problems366\n\\Squeaky Wheel\" Optimizationand beat it on 10. However, on half of the remaining problems ig did better by 10 or morecolors.To explore the sensitivity of swo to such implementation details we tried the followingapproaches in the constructor and prioritizer, and ran swo using all combinations:Construction: With or without color grabbingAnalysis: Either blame all nodes that receive a color outside the target set, or only the rst node (in the priority sequence) that causes a new color outside the target set tobe introduced. If color grabbing is used, the determination of blame is based on the nal color assigned to the node.The di erence in solution quality from the worst combination to the best combinationwas less than 15 percent. Even when the alternative of using a standard sort instead ofthe \\sticky\" sort (a fairly fundamental change) was added to the mix, the spread betweenworst and best was still under 20 percent.5. Related workThe importance of prioritization in greedy algorithms is not a new idea. The \\First Fit\"algorithm for bin packing, for example, relies on placing items into bins in decreasing orderof size (Garey & Johnson, 1979). Another example is grasp (Greedy Randomized AdaptiveSearch Procedure) (Feo & Resende, 1995). grasp di ers from our approach in several ways.First, the prioritization and construction aspects are more closely coupled in grasp. Aftereach element is added to the solution being constructed, the remaining elements are re-evaluated by some heuristic. Thus the order in which elements are added to the solutionmay depend on previous decisions. Second, the order in which elements are selected in eachtrial is determined only by the heuristic (and randomization), so the trials are independent.There is no learning from iteration to iteration in grasp.Doubleback Optimization (dbo) (Crawford, 1996) was to some extent the inspiration forboth swo and another similar algorithm, Abstract Local Search (als) (Crawford, Dalal,& Walser, 1998). In designing swo, we began by looking at dbo, because it had beenextremely successful in solving a standard type of scheduling problem. However, dbo isonly useful when the objective is to minimize makespan, and is also limited in the typesof constraints it can handle. Because of these limitations, we began thinking about theprinciples behind dbo, looking for an e ective generalization of that approach. dbo can,in fact, be viewed as an instance of swo. dbo begins by performing a \\right shift\" on aschedule, shifting all tasks as far to the right as they can go, up to some boundary. In theresulting right-shifted schedule, the left-most tasks are, to some extent, those tasks that aremost critical. This corresponds to analysis in swo. Tasks are then removed from the right-shifted schedule, taking left-most tasks rst. This ordering corresponds to the prioritizationin swo. As each task is removed, it is placed in a new schedule at the earliest possible starttime, i.e., greedy construction.Like swo, als was the result of an attempt to generalize dbo. als views priority space(to use the terminology from swo) as a space of \\abstract schedules,\" and performs a localsearch in that space. Unlike swo, if a prioritization is modi ed, and the corresponding367\nJoslin & Clementsmove in solution space is downhill (away from optimal), then the modi ed prioritizationis discarded, and the old prioritization is restored. As is usual with local search, als alsosometimes makes random moves, in order to escape local minima.als, and also List Scheduling (Pinson, Prins, & Rullier, 1994), are scheduling algorithmsthat deal with domains that include precedence constraints on tasks. Both accommodateprecedence constraints by constructing schedules left-to-right temporally. A task cannotbe placed in the schedule until all of its predecessors have been placed. In order for theanalysis, prioritization and construction to be appropriately coupled, it is not su cient tosimply increase the priority of a task that is late, because the constructor may not be ableto place that task until after a lot of other decisions have been made. Consequently, someamount of blame must be propagated to the task's predecessors.The commercial scheduler optiflex (Syswerda, 1994) uses a genetic algorithm approachto modify a sequence of tasks, and a constraint-based schedule constructor that generatesschedules from those sequences. optiflex can also be viewed as an instance of swo, witha genetic algorithm replacing analysis. In e ect, the \\analysis\" instead emerges from therelative tness of the members of the population.Two graph coloring algorithms also bear some similarity to swo. Impasse Class Col-oration Neighborhood Search (impasse) (Morgenstern, 1993; Lewandowski & Condon,1993), like swo, maintains a target set of colors and produces only feasible colorings. Given acoloring, impasse places any nodes that are colored outside of the target set into an impasseset. On each iteration a node is selected from the impasse set, using a noisy degree-basedheuristic, and assigned a random color from the target set. Any neighbor nodes that arenow in con ict are moved to the impasse set.Iterated Greedy (ig) (Culberson & Luo, 1993), like swo, uses a sequence of nodes tocreate a new coloring on each iteration, and then uses that coloring to produce a newsequence for the next iteration. The method used to generate each new sequence di ersfrom swo. The key observation behind ig is that if all nodes with the same color in thecurrent solution are grouped together in the next sequence (i.e. adjacent to each other inthe sequence), then the next solution will be no worse than the current solution. ig achievesimprovement by manipulating the order in which the groups occur in the new sequence,using several heuristics including random based on color, descending based on color, andascending based on the cardinality of each group. ig learns groupings of nodes as it runs,but it does not learn about about the di culty of any nodes. A node's place in the sequenceindicates nothing about its expected or detected di culty.6. Analysis and future workThis section summarizes several areas of future research suggested by the results reportedin the previous sections.6.1 ScalingWhile swo uses fast, greedy algorithms for constructing solutions, and we have demon-strated its e ectiveness on problems of realistic size, the greatest threat to the scalability ofswo is that it constructs a new solution from scratch on each iteration. A partial solutionto this problem is seen in the use of a \\history\" mechanism for the graph coloring problems.368\n\\Squeaky Wheel\" OptimizationUsing the same color for a node as in the previous solution means that in many cases wedo not need to check any of the other possible colors. This signi cantly speeds up theconstruction.A more fundamental solution to this problem would be to develop an incremental versionof swo. The selective reuse of colors in the graph coloring solver is a small step in thisdirection. This allows the constructor to avoid spending time evaluating other alternativeswhen the previous choice still works. More generally, it may be possible to look at thechanges made to a prioritization, and modify the corresponding solution in a way thatgenerates the same solution that would be constructed from scratch based on the newprioritization. It seems feasible that this could be done for some domains, at least for smallchanges to the prioritization, because there may be large portions of a solution that areuna ected.A more interesting possibility is based on the view of swo as performing local searchplus a certain kind of propagation. A small change in priorities may correspond to a largechange in the solution. For example, increasing the priority of one task in a schedulingproblem may change its position in the schedule, and, as a consequence, some lower prioritytasks may have to be shu ed around to accommodate that change. This is similar to whatwe might expect from moving the higher priority task, then propagating the e ects of thatchange by moving lower priority tasks as well. This single move may correspond to a largenumber of moves in a search algorithm that only looks at local changes to the schedule, andmay thus be di cult for such an algorithm to nd.Based on this view, we are investigating an algorithm we call \\Priority-Limited Propa-gation\" (plp). With plp, local changes are made to the solution, and then propagation isallowed to occur, subject to the current prioritization. Propagation is only allowed to occurin the direction of lower-priority elements. In e ect, a small change is made, and then theconsequences of that change are allowed to \\ripple\" through the plan. Because propagationcan only occur in directions of decreasing priority, these ripples of propagation decrease inmagnitude until no more propagation is possible. A new prioritization is then generated byanalyzing the resulting solution. (It should be possible to do this analysis incrementally,as well.) The resulting approach is not identical to swo, but has many of its interestingcharacteristics.6.2 Coordination of modulesFor swo to be e ective, it is obvious that analysis, prioritization and construction must allwork together to improve the quality of solutions. We have already discussed the compli-cations that can arise when constraints are placed on the order in which the constructorcan make decisions, as is the case for List Scheduling and als, where construction is donestrictly left-to-right. Without more complex analysis, the search spaces can e ectively be-come uncoupled, so that changes in priority don't cause the constructor to x problemsdiscovered by analysis.Another way the search can become uncoupled is related to the notion of \\excess cost,\"discussed for the scheduling implementation. The calculation of excess cost in the analyzerturned out to be a key idea for improving the performance of swo. However, problemssometimes have tasks that must be handled badly in order to achieve a good overall solu-369\nJoslin & Clementstion. One of the scheduling problems described previously has two such \\sacri cial\" tasks.Whenever a good solution is found, the analyzer assigns high blame to these sacri cial tasks,and the constructor handles them well on the next iteration. This means that the resultingsolution is of poor overall quality, and it is not until other aws cause other tasks to moveahead of the sacri cial tasks in the priority sequence that swo can again, brie y, explorethe space of good solutions. In such cases, to some extent the analysis is actually hurtingthe ability of swo to converge on good solutions.Ideally, we would like to generalize the notion of excess cost to recognize sacri cialtasks, and allow those tasks to be handled badly without receiving proportionate blame.For problems in which a task must be sacri ced in all solutions, it may be possible to usea learning mechanism to accomplish this.However, the notion of a sacri cial task can be more subtle than this. Suppose forexample that we are scheduling the construction of two airplanes, P1 and P2, and thateach has a key task, T1 and T2, respectively, requiring all of some shared resource, R.Because of the resource con ict, we must either give R to T1 early in the schedule, startingconstruction on plane P1 before P2, or we must give R to T2 early in the schedule, withthe opposite result. Whichever of the two tasks is started early will nish on time, but theother will be late.Suppose we construct a schedule in which T1 goes rst, and T2 is late, thus receiving aheavy blame factor. swo increases the priority on T2, and as a consequence, T2 goes rstin the subsequent schedule. But then T1 is late, and on the next iteration it will again go rst. We could alternate in this manner forever, and the result would be that swo wouldfail to explore either option very e ectively, because it would be jumping back and forthbetween the option of building plane P1 rst, and the option of building plane P2 rst,without remaining in one region of the search space long enough to re ne a solution.The di culty is that neither T1 nor T2 can be identi ed as a sacri cial task. Assumingthe two planes are not identical, we cannot simply argue from symmetry that we shouldjust pick one of the two tasks to be sacri ced. If, however, we could identify a sacri cialtask by the role it plays in a solution, we could achieve what we need. Here, the task to besacri ced must be the one that belongs to whichever plane is started later. If the analyzercould reduce the blame assigned to that task in a schedule, whichever task it happens tobe, it would allow swo to explore that region of the search much more e ectively.This problem of interchangeable roles would arise even more clearly with the introduc-tion of conditional elements in a solution. Suppose, for example, we have a schedulingproblem in which the constructor may choose to include or not include task instances ofsome type, adding however many instances are needed to satisfy a resource requirement.If those tasks are all instances of the same task type, then they are interchangeable, andpenalizing one may simply cause a shu ing of those instances that does not really addressthe problem. Moreover, with conditional tasks, it is not clear how the analyzer shouldassign blame when the set of task instances in the current schedule may be very di erentfrom the set of task instances in successor schedules.To address these concerns, the notion of prioritization could be generalized to apply toadditional aspects of a problem. In scheduling this might mean not just prioritizing tasks,but also resources over various time intervals. We also propose that the these prioritizationsbe limited to the \\ xed\" elements of a problem. In scheduling problems, for example, these370\n\\Squeaky Wheel\" Optimizationmay be the non-conditional tasks, resources, etc. (In our example domains, all of theelements are xed in this sense, so this was not an issue.)One intuition behind this proposal is that these are the elements that will tend to de neroles. In the earlier example with tasks T1 and T2, corresponding to the two planes beingbuilt, the critical element is not either task per se, but actually resource R, early in theschedule. If this phase of resource R receives a high priority, and the later phase of resourceR receives a lower priority, then whichever of the two tasks occurs later will be recognizedas less critical. While this does not exactly capture the notion of \\role\" that we would like,it comes a lot closer than the current approach. In addition, assigning priorities to the xedelements of a problem has the advantage of being applicable to problems with conditionaltasks. Research is currently under way to explore this approach.6.3 swo and local searchAlthough the ability to make large, coherent moves is a strength of the approach, it is alsoa weakness. swo is poor at making small \\tuning\" moves in the solution space, but thecoupled-search view of swo suggests an obvious remedy. swo could be combined with localsearch in the solution space, to look for improvements in the vicinity of good solutions.Similarly, making small changes to a prioritization would generally result in smaller movesin the solution space than result from going through the full analysis and re-prioritizationcycle.Yet another alternative is genetic algorithm techniques for \\crossover\" and other types ofmutation to a pool of nodes, as is done in optiflex. Many hybrid approaches are possible,and we believe that the coupled-search view of swo helps to identify some interestingstrategies for combining moves of various sizes and kinds, in both search spaces, adaptingdynamically to relative solution qualities.7. ConclusionsOur experience has been that it is fairly straightforward to implement swo in a new domain,because there are usually fairly obvious ways to construct greedy solutions, and to analyzea solution to assign \\blame\" to some of the elements. Naive implementations of swo tendto perform reasonably well.We have found the view of swo as performing a \\coupled search\" over two di erentsearch spaces to be very informative. It has been helpful to characterize the kinds of movesthat swo makes in each of the search spaces, and the e ect this has on avoiding localoptima, etc. We hope that by continuing to gain a deeper understanding of what makesswo work we will be able to say more about the e ective design of swo algorithms.As the number of directions for future research suggests, we have only begun to scratchthe surface of \\Squeaky Wheel\" Optimization.AcknowledgementsThe authors wish to thank Robert Stubbs of Lucent Technologies for providing the dataused for the scheduling experiments. The authors also wish to thank George L. Nemhauser,371\nJoslin & ClementsMarkus E. Puttlitz and Martin W. P. Savelsbergh with whom we collaborated on using swoin a hybrid AI/OR approach. Many useful discussions came out of that collaboration, andwithout them we would not have had access to the Lucent problems. Markus also wrotethe framework for the scheduling experiments and the tabu and ip implementations.The authors also thank the members of CIRL, and James Crawford at i2 Technologies,for their helpful comments and suggestions. We would like to thank Andrew Parkes inparticular for suggestions and insights in the graph coloring domain.This e ort was sponsored by the Air Force O ce of Scienti c Research, Air Force Ma-teriel Command, USAF, under grant number F49620-96-1-0335; by the Defense AdvancedResearch Projects Agency (DARPA) and Rome Laboratory, Air Force Materiel Command,USAF, under agreements F30602-95-1-0023 and F30602-97-1-0294; and by the NationalScience Foundation under grant number CDA-9625755.The U.S. Government is authorized to reproduce and distribute reprints for Governmen-tal purposes notwithstanding any copyright annotation thereon. The views and conclusionscontained herein are those of the authors and should not be interpreted as necessarily rep-resenting the o cial policies or endorsements, either expressed or implied, of the DefenseAdvanced Research Projects Agency, Rome Laboratory, the Air Force O ce of Scienti cResearch, the National Science Foundation, or the U.S. Government.Most of the work reported in this paper was done while both authors were at CIRL.ReferencesClements, D., Crawford, J., Joslin, D., Nemhauser, G., Puttlitz, M., & Savelsbergh, M.(1997). Heuristic optimization: A hybrid AI/OR approach. In Proceedings of theWorkshop on Industrial Constraint-Directed Scheduling. In conjunction with theThird International Conference on Principles and Practice of Constraint Program-ming (CP97).Crawford, J., Dalal, M., & Walser, J. (1998). Abstract local search. In Proceedings of theAIPS-98 Workshop on Planning as Combinatorial Search. In conjunction with theFourth International Conference on Arti cial Intelligence Planning Systems (AIPS-98).Crawford, J. M. (1996). An approach to resource constrained project scheduling. In Proceed-ings of the 1996 Arti cial Intelligence and Manufacturing Research Planning Work-shop, pp. 35{39.Culberson, J. C., & Luo, F. (1993). Exploring the k{colorable landscape with iteratedgreedy. In (Johnson & Trick, 1996), pp. 245{284.Feo, T. A., & Resende, M. G. (1995). Greedy randomized adaptive search procedures.Journal of Global Optimization, 6, 109{133.Garey, M. R., & Johnson, D. S. (1979). Computers and Intractability: A Guide to theTheory of NP-Completeness. W. H. Freeman.Glover, F., & Laguna, M. (1997). Tabu Search. Kluwer.372\n\\Squeaky Wheel\" OptimizationGlover, F., Parker, M., & Ryan, J. (1993). Coloring by tabu branch and bound. In (Johnson& Trick, 1996), pp. 285{307.Gomes, C., Selman, B., & Kautz, H. (1998). Boosting combinatorial search through ran-domization. In Proceedings of AAAI-98, pp. 431{437.Johnson, D. S., & Trick, M. A. (Eds.). (1996). Cliques, Coloring, and Satis ability: SecondDIMACS Implementation Challenge, 1996, Vol. 26 of DIMACS Series in DiscreteMathematics and Theoretical Computer Science. American Mathematical Society.Joslin, D., & Clements, D. (1998). \\Squeaky wheel\" optimization. In Proceedings of AAAI-98, pp. 340{346.Lewandowski, G., & Condon, A. (1993). Experiments with parallel graph coloring heuristicsand applications of graph coloring. In (Johnson & Trick, 1996), pp. 309{334.Morgenstern, C. (1993). Distributed coloration neighborhood search. In (Johnson & Trick,1996), pp. 335{357.Parkes, A., & Walser, J. (1996). Tuning local search for satis ability testing. In Proceedingsof AAAI-96, pp. 356{362.Pinson, E., Prins, C., & Rullier, F. (1994). Using tabu search for solving the resource-constrained project scheduling problem. In EURO-WG PMS 4 (EURO WorkingGroup on Project Management and Scheduling), pp. 102{106 Louvain, Belgium.Selman, B., Kautz, H. A., & Cohen, B. (1993). Local search strategies for satis abilitytesting. In (Johnson & Trick, 1996), pp. 521{531.Syswerda, G. P. (1994). Generation of schedules using a genetic procedure.. U.S. Patentnumber 5,319,781.\n373"
    } ],
    "references" : [ {
      "title" : "Abstract local search",
      "author" : [ "J. Crawford", "M. Dalal", "J. Walser" ],
      "venue" : null,
      "citeRegEx" : "Crawford et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Crawford et al\\.",
      "year" : 1998
    }, {
      "title" : "An approach to resource constrained project scheduling",
      "author" : [ "J.M. Crawford" ],
      "venue" : null,
      "citeRegEx" : "Crawford,? \\Q1996\\E",
      "shortCiteRegEx" : "Crawford",
      "year" : 1996
    }, {
      "title" : "Exploring the k{colorable landscape with iterated",
      "author" : [ "J.C. Culberson", "F. Luo" ],
      "venue" : null,
      "citeRegEx" : "Culberson and Luo,? \\Q1993\\E",
      "shortCiteRegEx" : "Culberson and Luo",
      "year" : 1993
    }, {
      "title" : "Greedy randomized adaptive search procedures",
      "author" : [ "T.A. Feo", "M.G. Resende" ],
      "venue" : null,
      "citeRegEx" : "Feo and Resende,? \\Q1995\\E",
      "shortCiteRegEx" : "Feo and Resende",
      "year" : 1995
    }, {
      "title" : "Computers and Intractability: A Guide",
      "author" : [ "M.R. Garey", "D.S. Johnson" ],
      "venue" : null,
      "citeRegEx" : "Garey and Johnson,? \\Q1979\\E",
      "shortCiteRegEx" : "Garey and Johnson",
      "year" : 1979
    }, {
      "title" : "Coloring by tabu branch and bound",
      "author" : [ "F. Glover", "M. Parker", "J. Ryan" ],
      "venue" : null,
      "citeRegEx" : "Glover et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Glover et al\\.",
      "year" : 1993
    }, {
      "title" : "Boosting combinatorial search through",
      "author" : [ "C. Gomes", "B. Selman", "H. Kautz" ],
      "venue" : null,
      "citeRegEx" : "Gomes et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Gomes et al\\.",
      "year" : 1998
    }, {
      "title" : "Cliques, Coloring, and Satis ability",
      "author" : [ "D.S. Johnson", "M.A. Trick" ],
      "venue" : null,
      "citeRegEx" : "Johnson and Trick,? \\Q1996\\E",
      "shortCiteRegEx" : "Johnson and Trick",
      "year" : 1996
    }, {
      "title" : "Squeaky wheel\" optimization",
      "author" : [ "D. Joslin", "D. Clements" ],
      "venue" : "In Proceedings of AAAI-",
      "citeRegEx" : "Joslin and Clements,? \\Q1998\\E",
      "shortCiteRegEx" : "Joslin and Clements",
      "year" : 1998
    }, {
      "title" : "Experiments with parallel graph coloring heuristics",
      "author" : [ "G. Lewandowski", "A. Condon" ],
      "venue" : null,
      "citeRegEx" : "Lewandowski and Condon,? \\Q1993\\E",
      "shortCiteRegEx" : "Lewandowski and Condon",
      "year" : 1993
    }, {
      "title" : "Distributed coloration neighborhood search",
      "author" : [ "C. Morgenstern" ],
      "venue" : null,
      "citeRegEx" : "Morgenstern,? \\Q1993\\E",
      "shortCiteRegEx" : "Morgenstern",
      "year" : 1993
    }, {
      "title" : "Tuning local search for satis ability testing",
      "author" : [ "A. Parkes", "J. Walser" ],
      "venue" : null,
      "citeRegEx" : "Parkes and Walser,? \\Q1996\\E",
      "shortCiteRegEx" : "Parkes and Walser",
      "year" : 1996
    }, {
      "title" : "Using tabu search for solving the resource",
      "author" : [ "E. Pinson", "C. Prins", "F. Rullier" ],
      "venue" : null,
      "citeRegEx" : "Pinson et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Pinson et al\\.",
      "year" : 1994
    }, {
      "title" : "Local search strategies for satis ability",
      "author" : [ "B. Selman", "H.A. Kautz", "B. Cohen" ],
      "venue" : null,
      "citeRegEx" : "Selman et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Selman et al\\.",
      "year" : 1993
    }, {
      "title" : "Generation of schedules using a genetic procedure",
      "author" : [ "G.P. Syswerda" ],
      "venue" : "U.S. Patent",
      "citeRegEx" : "Syswerda,? \\Q1994\\E",
      "shortCiteRegEx" : "Syswerda",
      "year" : 1994
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "These were collected for the Second DIMACS Implementation Challenge (Johnson & Trick, 1996), which includes results for several algorithms on these problems (Culberson & Luo, 1993; Glover, Parker, & Ryan, 1993; Lewandowski & Condon, 1993; Morgenstern, 1993).",
      "startOffset" : 157,
      "endOffset" : 257
    }, {
      "referenceID" : 10,
      "context" : "Table 3 compares swo with the results for ig (Culberson & Luo, 1993), distributed impasse (Morgenstern, 1993), parallel impasse (Lewandowski & Condon, 1993), and tabu (Glover et al.",
      "startOffset" : 90,
      "endOffset" : 109
    }, {
      "referenceID" : 5,
      "context" : "Table 3 compares swo with the results for ig (Culberson & Luo, 1993), distributed impasse (Morgenstern, 1993), parallel impasse (Lewandowski & Condon, 1993), and tabu (Glover et al., 1993).",
      "startOffset" : 167,
      "endOffset" : 188
    }, {
      "referenceID" : 5,
      "context" : "Glover et al. (1993) is the only paper that is based on a general search technique, tabu with branch and bound, rather than a graph coloring speci c algorithm.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 5,
      "context" : "Glover et al. (1993) is the only paper that is based on a general search technique, tabu with branch and bound, rather than a graph coloring speci c algorithm. This approach had the worst reported average results in the group. Morgenstern (1993) used a distributed impasse algorithm and had the best overall colorings, but also required that the target number of colors, as well as several other problem speci c parameters be passed to the solver.",
      "startOffset" : 0,
      "endOffset" : 246
    }, {
      "referenceID" : 5,
      "context" : "Glover et al. (1993) is the only paper that is based on a general search technique, tabu with branch and bound, rather than a graph coloring speci c algorithm. This approach had the worst reported average results in the group. Morgenstern (1993) used a distributed impasse algorithm and had the best overall colorings, but also required that the target number of colors, as well as several other problem speci c parameters be passed to the solver. Lewandowski & Condon (1993) also found good solutions for this problem set.",
      "startOffset" : 0,
      "endOffset" : 476
    }, {
      "referenceID" : 5,
      "context" : "Glover et al. (1993) is the only paper that is based on a general search technique, tabu with branch and bound, rather than a graph coloring speci c algorithm. This approach had the worst reported average results in the group. Morgenstern (1993) used a distributed impasse algorithm and had the best overall colorings, but also required that the target number of colors, as well as several other problem speci c parameters be passed to the solver. Lewandowski & Condon (1993) also found good solutions for this problem set. Their approach used a hybrid of parallel impasse and systematic search on a 32 processor CM-5. Culberson & Luo (1993) used an Iterated Greedy (ig) algorithm that bears some similarity to swo.",
      "startOffset" : 0,
      "endOffset" : 642
    }, {
      "referenceID" : 1,
      "context" : "Doubleback Optimization (dbo) (Crawford, 1996) was to some extent the inspiration for both swo and another similar algorithm, Abstract Local Search (als) (Crawford, Dalal, & Walser, 1998).",
      "startOffset" : 30,
      "endOffset" : 46
    }, {
      "referenceID" : 14,
      "context" : "The commercial scheduler optiflex (Syswerda, 1994) uses a genetic algorithm approach to modify a sequence of tasks, and a constraint-based schedule constructor that generates schedules from those sequences.",
      "startOffset" : 34,
      "endOffset" : 50
    }, {
      "referenceID" : 10,
      "context" : "Impasse Class Coloration Neighborhood Search (impasse) (Morgenstern, 1993; Lewandowski & Condon, 1993), like swo, maintains a target set of colors and produces only feasible colorings.",
      "startOffset" : 55,
      "endOffset" : 102
    } ],
    "year" : 2011,
    "abstractText" : null,
    "creator" : "dvipsk 5.58f Copyright 1986, 1994 Radical Eye Software"
  }
}