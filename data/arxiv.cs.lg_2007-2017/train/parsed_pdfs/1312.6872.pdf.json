{
  "name" : "1312.6872.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Matrix Recovery using Split Bregman",
    "authors" : [ "Anupriya Gogna", "Ankita Shukla", "Angshul Majumdar" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "a matrix, with inherent low rank structure, from its lower dimensional projections. This problem is frequently encountered in wide range of areas including pattern recognition, wireless sensor networks, control systems, recommender systems, image/video reconstruction etc. Both in theory and practice, the most optimal way to solve the low rank matrix recovery problem is via nuclear norm minimization. In this paper, we propose a Split Bregman algorithm for nuclear norm minimization. The use of Bregman technique improves the convergence speed of our algorithm and gives a higher success rate. Also, the accuracy of reconstruction is much better even for cases where small number of linear measurements are available. Our claim is supported by empirical results obtained using our algorithm and its comparison to other existing methods for matrix recovery. The algorithms are compared on the basis of NMSE, execution time and success rate for varying ranks and sampling ratios.\nKeywords—augmented lagrangian, bregman divergence,\nnuclear norm, rank minimization, split bregman\nI. INTRODUCTION\nMatrix recovery problem is commonly encountered in various areas of engineering and technology such as collaborative filtering [1], control system (for system identification) [2], wireless sensor networks (WSN) (for sensor localization) [3], computer vision (for inferring scene geometry) [4], data mining and pattern recognition etc. In the absence of any additional knowledge it is impossible to fill in the matrix given its subsampled version; as there can be infinitely many ways in which missing values can be filled. However, if the matrix to be recovered is (estimated to be approximately) low rank, the problem becomes feasible as one can search for the lowest rank matrix amongst all possible candidate solutions.\nWe give a few examples of low rank matrix recovery. In the field of collaborative filtering, we are required to find items of interest for a particular user, based on his/her ratings on other items and other user’s ratings on similar items. This entails the task of filling an incompletely observed user-item rating matrix. The rating matrix can be considered to be low rank as the user preference for any item is assumed to be influenced by only a small number of factors. Similarly in the area of WSN, the task is to complete a partially filled distance matrix (wherein each element represents the distance between a pair of sensor nodes). It is known that such a matrix has a rank equal to the dimensionality of the space plus two. For such cases, matrix recovery problem becomes equivalent to\nobtaining the lowest rank solution consistent with the observed set of measurements.\nSo far we have discussed about matrix completion from partially observed entries. A somewhat different problem arises in dynamic MRI reconstruction. In MRI, the data is acquired in the Fourier frequency space. In dynamic MRI, the problem is to reconstruct the frames in time, given the sampled Frequency space. If we assume that the MRI frames to be reconstructed are stacked as columns of a matrix, the thus formed matrix is of low rank. This is because, the frames are correlated in time and hence the columns of the matrix are not independent from each other. Thus, dynamic MRI reconstruction turns out to be a low rank matrix recovery problem from sampled Fourier measurements.\nIn this work, we solve the problem of recovering a low rank matrix from its lower dimensional projections. Formally this is expressed as follows\nmin ( )\n( )\nZ rank Z\nSubject to Y A Z (1)\nWhere, A is the linear projection operator (for matrix completion, A is a binary mask, for dynamic MRI, it is a Fourier mapping), Y is the acquired measurements and Z is the low rank matrix to be recovered.\nUnfortunately, the aforesaid problem (1) is NP hard with doubly exponential complexity. It has been proven in [5, 6] that when certain conditions are met, the NP hard rank minimization (1) can be surrogated by its nearest convex envelope – the nuclear norm (2),\n*min || ||\n( )\nZ Z\nSubject to Y A Z (2)\nWhere the nuclear norm *||Z|| is the sum of singular values\nof the matrix.\nThe contribution of our work is the use of Split Bregman algorithm to solve (2) for obtaining the lowest rank solutions consistent with the given observation matrix Y. There are a handful of algorithms to solve the matrix completion problem, but there is hardly any algorithm to solve the general low rank matrix recovery problem from its lower dimensional projections. In this work, we develop a general algorithm for low rank matrix recovery using Split Bregman approach. It is shown that use of Split Bregman technique improves the accuracy of results as well as the speed of convergence. The\nrest of the paper is organized as follows. In section II, the previous work in the domain of matrix completion is discussed. Section III describes briefly Bregman iterations and Split Bregman algorithm. In section IV we introduce our proposed algorithm. Section V includes the experiments and results. Section VI contains the conclusions.\nII. REVIEW OF PREVIOUS STUDIES IN MATRIX"
    }, {
      "heading" : "COMPLETION",
      "text" : "As mentioned before, there are a handful of algorithms to solve the matrix completion problem. It is not possible to review all of them; here we review two popular ones - Singular Value Thresholding (SVT) [7] and Fixed Point Continuation (FPC) [8]. Both solve the problem of the form (3)\n*min || ||\n( )\nZ Z\nSubject to A Z b (3)\nWhere, Z is the matrix to be recovered given the observed vector b and A is a linear operator. Matrix completion is a special case where A is a binary mask.\nSVT uses linearized Bregman iterations for solving (3) using the following iterates\n1\n1\n( ( ))\n( ( ))\nk T k\nk k k\nk\nZ D A Y\nY Y b A Z\n\n\n\n\n\n  (4)\nWhere, and is the (soft thresholding) shrinkage operator. is the step size at the kth iteration. However, SVT is well suited only for very low rank matrices and require a large number of iterations to converge. Thus the run times for SVT is very high for cases where only a few linear measurements are available.\nFPC algorithm for matrix completion proposed in [8], uses operator splitting to recover a low rank matrix via nuclear norm minimization (5).\n1\n( )\n( )\nk k T k\nk k\nY Z A AZ\nZ S\nb\nY\n   \n (5)\nWhere, is the matrix shrinkage operator that performs soft thresholding on the singular values of the argument (matrix) by an amount a.\nIt should be noted that both SVT and FPC were developed to solve the matrix completion problem and not the matrix recovery problem. However, both of them can be modified to solve the matrix recovery problem as shown here.\nIII. BACKGROUND FOR FORMULATION\nThis section presents a brief description of the background for our formulation. Here we discuss the concepts of Bregman divergence and Split Bregman method; the underlying fundamentals for our work.\nBregman iterative algorithms [9] are used to solve many constrained optimization problems, such as Basis Pursuit and TV denoising, because of its better convergence behavior and improved accuracy of reconstruction. Bregman distance forms\nthe basis of formulation of these algorithms. For a convex function E: X→R, where u, v X and p belongs to the set of sub gradient of the function, Bregman distance is given by D pE .\n( , ) ( ) ( ) ,pED u v E u E v p u v   (6)\nBregman distance is a non-negative quantity but is not a conventional distance metric as it is not symmetric and does not follow triangular inequality. It is just a measure of separation of two points. Consider the following constrained optimization problem.\nmin ( )\n( ) 0\nu E u\nSubject to H u  (7)\nWhere, E and H are convex functions defined on Rn with H being differentiable. The corresponding unconstrained formulation can be written as\nmin ( ) ( ) u E u H u (8)\nEquation (8) is solved iteratively by Bregman iterative algorithm in the following steps\n1\n1 1\nmin ( , ) ( )\n( )\nk p k\nE u\nk k k\nu D u u H u\np p H u\n\n \n \n  (9)\nBregman provides excellent convergence results because H monotonically decreases with every iteration and converges to the minimizer of the function. An advancement over Bregman iterations is the linearized Bregman algorithm which makes computation efficient by combining the minimizing of unconstrained problem and Bregman update into a single step; which can be solved exactly. However, linearized Bregman cannot solve problems involving multiple L1 regularization terms. To handle this issue Split Bregman method [10] was proposed. Consider the objective function given below where\n(u) and H(u) are convex and H is differentiable.\n1min | ( ) | ( ) u u H u  (10)\nThe underlying concept of Split Bregman is decomposition of L1 and L2 terms such that they form different sub problems which can be solved way easily than the original composite objective function. Rewriting equation (10) by letting\nd= (u) the constrained formulation becomes\n1min | | ( )\n( )\nd H u\nSubject to d u\n\n  (11)\nThe unconstrained equivalent of (11) is obtained by adding a penalization function to the problem as in (12).\n2\n1 2min | | ( ) || ( ) || 2u\nd H u d u     (12)\nNow comparing this to the form in (7), we\nconsider 1( , ) | | ( )E u d d H u  ,\n   \n   \n \n1 1 2\n2\n1 1 1\n1 1 1\n( , ) min , , , || || 2\nk k p k k\nE u\nTk k k k\nu u\nk k k k\nd d\nu d D u u d d d u\np p u d\np p d u\n\n\n\n \n  \n  \n  \n    \n  \n(13)\nThe 1st update step can be solved using ADMM (alternating direction method of multipliers) [11] by alternatively keeping one variable fixed and optimizing over the other. Simplified form of (13) is given below.\n   \n \n \n1 2\n2\n1 2\n21\n1 1 1\nmin || || 2\n||min || 2\nk k\nu\nk k\nd\nk k k k\nu H u d u b\nd d d u b\nb b u d\n\n\n\n\n  \n   \n   \n   \n(14)\nSince H(u) is smooth and differentiable everywhere, updation for u can be solved analytically. Solution for d is nothing but the solution for synthesis prior formulation and is obtained directly by skrinkage (soft thresholding) operator. Last step is the updation of Bregman variable.\nOne important advantage that Split Bregman provides over other algorithms is that one can keep lambda (the regularization parameter) constant to a value that achieves fast convergence.\nIV. ALGORITHM FORMULATION\nThe general problem is to solve an underdetermined linear system of equations:\n1 ( )N m ny A Z  (15)\nWhere : , m n NA R R N m n   \nThis has infinitely many solutions. We are interested in a low-rank solution. As mentioned before, one ideally needs to solve a rank minimization problem (1) in order to achieve this; but it has been theoretically proven that under certain conditions the same (low rank) solution can be achieved by nuclear norm minimization. In this work, we proposed to derive a new algorithm for nuclear norm minimization.\nSo far, we discussed the noiseless scenario. In all practical situations, the system will be corrupted by some noise; and we will have (16) instead,\n2( ) , ~ (0, )Y A Z N    (16)\nTo recover Z from (16) one ideally needs to solve (17) [12]:\n2\n* min ( ) froZ Z subject to Y A Z   (17)\nWhere ‘fro’ denotes the Frobenius norm and ε=Nσ2.\nSolving the constrained problem directly is difficult, so we propose to solve its unconstrained Lagrangian (18) instead. The constrained and the unconstrained counterparts are the same for correct choice of λ1 given ε. In this work, we assume that λ1 is known.\n2 2 1 *\n1 min || || || ||\n2Z y Az Z  (18)\nWhere, A is a projection operator, z is the vectorized\n(column concatenated) form of matrix Z i.e. ( )A Z Az , where\nfor “Az” formulation, A is a block diagonal matrix such that each block of A acts on a column of matrix Z. In this section we derive an algorithm to solve (18). Split Bregman and ADMM techniques are adopted to ensure better accuracy and faster convergence. We reformulate (18) as (19) by introducing a proxy variable W.\n2\n2 1 * ,\n1 min || || || ||\n2\nZ W y Az W\nSubject to Z W\n \n\n(19)\nThe above formulation can be converted into unconstrained convex optimization problem (20) by use of augmented Lagrangian and Split Bregman techniques as discussed in section III.\n2 2\n2 1 * 2 Z,\n1 min || || || || || ||\n2 2W y Az W W Z B\n            \n     (20)\nWhere, B is the Bregman (relaxation) variable.\nThe use of 2nd L2 term (augmented Lagrangian) improves the robustness of the algorithm as it eliminates the need for strictly reinforcing the equality constraint while simultaneously penalizing for any deviation. Use of Bregman variable makes sure that the value of can be chosen to optimize the convergence speed. Hence the speed of algorithm is dependent on how fast we can optimize each of the sub problems. Equation (20) can be split into two simpler sub problems which can be solved by alternatively fixing one variable and minimizing over the other (via ADMM).\nSub problem 1\n2 2 2 2\n1 min || || || ||\n2 2Z y Az W Z B\n         \n     (21)\nSub problem 2\n2\n1 * 2min || || || || 2W\nW W Z B    \n      (22)\nThe first sub problem (21) is a simple least square problem (Tikhonov regularization) which can be solved easily using any gradient descent algorithm such as conjugate gradient. Sub problem 2 (22) can be solved iteratively by soft thresholding operation (23) on singular values of matrix W [13, 14].\n      , max 0,Soft t u sign t t u  (23)\n1 ( ,4 / )W Soft Z B    (24)\nBetween every consecutive iteration of the two sub problems, Bregman variable is updated as follows\n1 1 1( )k k k kB B W Z    (25)\nUsing Bregman variable causes initial few iterations to return over-regularized results. However updation of Bregman variable (25) as iterations proceed makes sure that any information that is not captured is added back. This scheme makes sure that the run times are lower and accuracy of recovery is improved. For solving the least square problem, we used lsqr [15]. Complete algorithm is given below.\nThere are two stopping criterions. Iterations continue till the objective function converges; by convergence we mean that the difference between the objective functions between two successive iterations is very small (1E-7). The other stopping criterion is a limit on the maximum number of iterations. We have kept it to be 500.\nV. RESULTS\nExperiments were carried out in two parts. In the first part, we experimented with synthetic datasets. In the second part, we experimented on actual collaborative filtering problem."
    }, {
      "heading" : "A. Simulations on Synthetic data",
      "text" : "For testing the algorithm, matrices of dimension of varying rank (lower than the dimensionality of the matrix) were constructed. They were sub-sampled (at varying sampling ratios) using a binary (linear) operator A as in (2). For our algorithm, the values of regularization\nparameters 1 and  were chosen to be 0.001. Bregman variable was initialized to a vector containing all ones. These values were chosen to yield the best convergence behavior. We compared our results against those obtained using FPC and SVT. For FPC the value of mu_final was taken to be 0.01 and tolerance was taken to be 1E-3. For SVT maximum number of iterations were taken to be 500.\nFig. 1-3 illustrate the success rates of FPC, SVT and MSB algorithms as a function of sampling ratio for different ranks. To compute the success rate, 100 independent runs of each algorithm were carried out. This was done for all combinations of sampling ratios and ranks. Success rate was computed as the number of attempts (out of 100) in which NMSE of less than 1E-3 was achieved. It can be seen that our proposed approach (MSB) always yields the best recovery; i.e. it can successfully recover matrices from low sampling ratios unlike the other two.\nThe run times for all three algorithms are shown in table I. The table clearly shows that our algorithm converges faster to the optimum value. The execution time for MSB is much lower than that for both FPC and SVT especially for matrices with high rank and very few (available) sampled values. SVT has very poor run times for high rank matrices making it inefficient for use in such cases.\n0 0\n1\n1 1\n1 1 1\n1 1\n: 1; 0; 0.001; 0.001\n500 & &\n( _ ( ) - _ ( -1)) 1 - 7\n[ ] [ ( ) ]\n4 ,\n/ /\nk T k k T\nk k k\nk k k\nInitialization B W\nwhile k\nabs Obj func k Obj func k E\nZ A I W B A y\nW Z B\nUpdate the Bregman Variable\nB B W\nA\n \n \n\n\n \n \n \n   \n\n\n   \n     \n\n\n  1 kZ\nend while\n\nThe success rate gives an overall picture. In order to assess the recovery accuracy, the success rate graphs are not enough. We need to observe the reconstruction errors. For this purpose, the average (Normalized Mean Squared Error) NMSE for all three algorithms (MSB, SVT and FPC), computed over 100 independent runs of each algorithm, is shown in table II. It is evident from the numerical values that our algorithm provides much lower NMSE, in all the cases, as compared to both SVT and FPC."
    }, {
      "heading" : "Rank Algorithm",
      "text" : ""
    }, {
      "heading" : "B. Collaborative Filtering",
      "text" : "Matrix completion, a special case of matrix recovery is required extensively in Collaborative Filtering (CF). In CF we have a sparsely filled rating matrix, which contains the ratings given by each user to some of the available items (e.g. Movies in the movielens data set). The task is to find the user’s liking/disliking for other items (movies) based on the ratings explicitly provided by him/her for a few items. It is believed that each item can be characterized by certain features (for example genre, cast etc. for a movie) and users liking/disliking for an item is based on their affinity towards these features. Thus the underlying matrix structure is low rank as the number of features characterizing it are far less than the dimensionality of the rating matrix.\nHere, we tested our algorithm on movielens (100K) data set [16]. This data set consists of 100,000 ratings (1-5) from 943 users on 1682 movies. Each user has rated at least 20 movies. We divided the data set into test and training data. Out of 100K values, 80K values were used as part of the training set and test set consisted of the remaining 20K values. Using training data set matrix recovery was performed and (Mean Absolute Error) MAE was computed between the test set data and the recovered values. MAE obtained using SVT and FPC is 1.41 and 0.82 respectively. Our proposed algorithm gives MAE of 0.78. All values were computed over 50 independent runs.\nThe fame of collaborative filtering owes to the famous 1 million dollar Netflix prize. The prize was won by the group who brought down the relative error by 8.43%. In this work, we have improved upon FPC by reducing the error by 5%. This is significant improvement by collaborative filtering standards.\nVI. CONCLUSION\nIn this paper we presented an algorithm for recovering a low rank matrix given it’s under sampled measurements. Ideally this requires solving a rank minimization problem; but as rank minimization is NP hard, it is surrogated by nuclear norm minimization. Nuclear norm minimization is a convex problem which can be solved via semi-definite programming. However SDP is slow and over the years, various fast alternatives have been proposed.\nAlmost all of these studies were formulated around the low-rank matrix completion problem. There is hardly any offthe-shelf algorithm for the general matrix recovery problem. In this work we use Bregman iterations and Split Bregman method to derive an algorithm for matrix recovery. Our algorithm shows faster convergence and much smaller run times than existing methods compared against. The accuracy of recovered results, quantified in terms of NMSE, and the empirical success rate of our algorithm is also far superior to that obtained from the methods compared against.\nIn case of real world collaborative filtering problem also, our algorithm is able to achieve a reduction of 5% in MAE as compared to existing methods (FPC and SVT)."
    } ],
    "references" : [ {
      "title" : "Trace norm regularized matrix factorization for service recommendation",
      "author" : [ "Q. Yu", "Z. Zheng", "H. Wang" ],
      "venue" : "IEEE 20 International conference on web services, pp. 34-41, June-July 2013",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Interior-point method for nuclear norm approximation with application to system identification",
      "author" : [ "Z. Liu", "L. Vandenberghe" ],
      "venue" : "SIAM Journal on Matrix Analysis and Applications,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2009
    }, {
      "title" : "Localization of Wireless Sensors via Nuclear Norm for Rank Minimization",
      "author" : [ "C. Feng", "S. Valee", "W.S.A. Au", "Z. Tan" ],
      "venue" : "IEEE Global Telecommunications Conference, pp.1-5, December 2010",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A sequential factorization method for recovering shape and motion from image streams",
      "author" : [ "T. Morita", "T. Kanade" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1997
    }, {
      "title" : "Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization",
      "author" : [ "B. Recht", "M. Fazel", "P.A. Parrilo" ],
      "venue" : "SIAM Rev. Vol",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2010
    }, {
      "title" : "A singular value thresholding algorithm for matrix completion",
      "author" : [ "J.F. Cai", "E.J. Candes", "Z.W. Shen" ],
      "venue" : "Technical report, Septermber 2008.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Fixed point and Bregman iterative methods for matrix rank minimization",
      "author" : [ "S. Ma", "D. Goldfarb", "L. Chen" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2009
    }, {
      "title" : "Bregman iterative algorithms for l1-minimization with applications to compressed sensing",
      "author" : [ "W. Yin", "S. Osher", "D. Goldfarb", "J. Darbon" ],
      "venue" : "SIAM Journal on imaging sciences,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2008
    }, {
      "title" : "The split bregman method for L1-regularizd problems",
      "author" : [ "T. Goldstein", "S. Osher" ],
      "venue" : "SIAM journal on imaging sciences,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2009
    }, {
      "title" : "Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers",
      "author" : [ "S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein" ],
      "venue" : "Foundations and trends in machine learning,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2010
    }, {
      "title" : "Matrix completion with noise",
      "author" : [ "E.J. Candès", "Y. Plan" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2010
    }, {
      "title" : "Some Empirical Advances in Matrix Completion",
      "author" : [ "A. Majumdar", "R.K. Ward" ],
      "venue" : "Signal Processing,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2011
    }, {
      "title" : "Nonconvex splitting for regularized low-rank + sparse decomposition",
      "author" : [ "R. Chartrand" ],
      "venue" : "IEEE Trans. Signal Process.,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2012
    }, {
      "title" : "LSQR: An Algorithm for Sparse Linear Equations And Sparse Least Squares",
      "author" : [ "C.C. Paige", "M.A. Saunders" ],
      "venue" : "ACM Trans. Math. Soft., Vol.8, pp. 43-71, 1982",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1982
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "INTRODUCTION Matrix recovery problem is commonly encountered in various areas of engineering and technology such as collaborative filtering [1], control system (for system identification) [2], wireless sensor networks (WSN) (for sensor localization) [3], computer vision (for inferring scene geometry) [4], data mining and pattern recognition etc.",
      "startOffset" : 140,
      "endOffset" : 143
    }, {
      "referenceID" : 1,
      "context" : "INTRODUCTION Matrix recovery problem is commonly encountered in various areas of engineering and technology such as collaborative filtering [1], control system (for system identification) [2], wireless sensor networks (WSN) (for sensor localization) [3], computer vision (for inferring scene geometry) [4], data mining and pattern recognition etc.",
      "startOffset" : 188,
      "endOffset" : 191
    }, {
      "referenceID" : 2,
      "context" : "INTRODUCTION Matrix recovery problem is commonly encountered in various areas of engineering and technology such as collaborative filtering [1], control system (for system identification) [2], wireless sensor networks (WSN) (for sensor localization) [3], computer vision (for inferring scene geometry) [4], data mining and pattern recognition etc.",
      "startOffset" : 250,
      "endOffset" : 253
    }, {
      "referenceID" : 3,
      "context" : "INTRODUCTION Matrix recovery problem is commonly encountered in various areas of engineering and technology such as collaborative filtering [1], control system (for system identification) [2], wireless sensor networks (WSN) (for sensor localization) [3], computer vision (for inferring scene geometry) [4], data mining and pattern recognition etc.",
      "startOffset" : 302,
      "endOffset" : 305
    }, {
      "referenceID" : 4,
      "context" : "It has been proven in [5, 6] that when certain conditions are met, the NP hard rank minimization (1) can be surrogated by its nearest convex envelope – the nuclear norm (2),",
      "startOffset" : 22,
      "endOffset" : 28
    }, {
      "referenceID" : 5,
      "context" : "It is not possible to review all of them; here we review two popular ones - Singular Value Thresholding (SVT) [7] and Fixed Point Continuation (FPC) [8].",
      "startOffset" : 110,
      "endOffset" : 113
    }, {
      "referenceID" : 6,
      "context" : "It is not possible to review all of them; here we review two popular ones - Singular Value Thresholding (SVT) [7] and Fixed Point Continuation (FPC) [8].",
      "startOffset" : 149,
      "endOffset" : 152
    }, {
      "referenceID" : 6,
      "context" : "FPC algorithm for matrix completion proposed in [8], uses operator splitting to recover a low rank matrix via nuclear norm minimization (5).",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 7,
      "context" : "Bregman iterative algorithms [9] are used to solve many constrained optimization problems, such as Basis Pursuit and TV denoising, because of its better convergence behavior and improved accuracy of reconstruction.",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 8,
      "context" : "To handle this issue Split Bregman method [10] was proposed.",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 9,
      "context" : "The 1 update step can be solved using ADMM (alternating direction method of multipliers) [11] by alternatively keeping one variable fixed and optimizing over the other.",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 10,
      "context" : "To recover Z from (16) one ideally needs to solve (17) [12]:",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 11,
      "context" : "Sub problem 2 (22) can be solved iteratively by soft thresholding operation (23) on singular values of matrix W [13, 14].",
      "startOffset" : 112,
      "endOffset" : 120
    }, {
      "referenceID" : 12,
      "context" : "Sub problem 2 (22) can be solved iteratively by soft thresholding operation (23) on singular values of matrix W [13, 14].",
      "startOffset" : 112,
      "endOffset" : 120
    }, {
      "referenceID" : 13,
      "context" : "For solving the least square problem, we used lsqr [15].",
      "startOffset" : 51,
      "endOffset" : 55
    } ],
    "year" : 2013,
    "abstractText" : "In this paper we address the problem of recovering a matrix, with inherent low rank structure, from its lower dimensional projections. This problem is frequently encountered in wide range of areas including pattern recognition, wireless sensor networks, control systems, recommender systems, image/video reconstruction etc. Both in theory and practice, the most optimal way to solve the low rank matrix recovery problem is via nuclear norm minimization. In this paper, we propose a Split Bregman algorithm for nuclear norm minimization. The use of Bregman technique improves the convergence speed of our algorithm and gives a higher success rate. Also, the accuracy of reconstruction is much better even for cases where small number of linear measurements are available. Our claim is supported by empirical results obtained using our algorithm and its comparison to other existing methods for matrix recovery. The algorithms are compared on the basis of NMSE, execution time and success rate for varying ranks and sampling ratios. Keywords—augmented lagrangian, bregman divergence, nuclear norm, rank minimization, split bregman",
    "creator" : "Microsoft® Word 2013"
  }
}