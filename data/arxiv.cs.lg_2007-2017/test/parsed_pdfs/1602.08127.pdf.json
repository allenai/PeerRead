{
  "name" : "1602.08127.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Auto-JacoBin: Auto-encoder Jacobian Binary Hashing",
    "authors" : [ "Xiping Fu", "Brendan McCane", "Steven Mills", "Michael Albert", "Lech Szymanski" ],
    "emails" : [ "lechszym}@cs.otago.ac.nz" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Auto-JacoBin: Auto-encoder Jacobian Binary Hashing\nXiping Fu, Brendan McCane, Steven Mills, Michael Albert, and Lech Szymanski Department of Computer Science, University of Otago, Dunedin, New Zealand\n{xiping, mccane, steven, malbert, lechszym}@cs.otago.ac.nz\nBinary codes can be used to speed up nearest neighbor search tasks in large scale data sets as they are efficient for both storage and retrieval. In this paper, we propose a robust auto-encoder model that preserves the geometric relationships of high-dimensional data sets in Hamming space. This is done by considering a noise-removing function in a region surrounding the manifold where the training data points lie. This function is defined with the property that it projects the data points near the manifold into the manifold wisely, and we approximate this function by its first order approximation. Experimental results show that the proposed method achieves better than state-of-the-art results on three large scale high dimensional data sets.\nIndex Terms—Approximate nearest neighbor, hashing, binary encoding, auto-encoder model.\nI. INTRODUCTION\nFinding nearest neighbor points is a computational task which arises frequently in machine learning, information retrieval and computer vision. Naive search is infeasible for massive real-word datasets since it takes linear time complexity to retrieve the nearest point in the dataset. For low dimensional datasets, the nearest neighbor searching problem can be addressed by efficient and effective tree based approaches such as KD-tree [1] and R-tree [2]. Due to the curse of dimensionality, when it comes to the high dimensional scenario, tree based approaches deteriorate abruptly and can become worse than linear search complexity [3]. Recent years have witnessed a surge of research interest in hashing methods which return approximate nearest neighbor (ANN) data points. It has been shown that binary hashing based ANN, which indexes data points into binary codes and retrieves data points in Hamming space, often has good enough performance for many real world applications. For example, hashing has been used in large scale medical image searching [4], image matching for 3D reconstruction [5], collaborative filtering for recommender systems [6] and approximating the SVM kernel [7].\nWhen high dimensional data points are indexed into compact binary codes, information loss is the main concern. Most previous work focuses on modeling the obtained binary codes optimally, i.e., attempting to ensure that the geometric relationships among data points in the new space are similar to those in the original space. Locality sensitive hashing (LSH) [8] was an early exploration of hashing to encode data points into binary codes. In LSH, each binary code is obtained by a randomly generated projection matrix. The randomness of the projection matrix insures that similarities between data points are inherited by the binary codes if sufficiently many bits are used to encode the data points. In more recent research, LSH has been used for the nearest neighbor searching problem for the Lp norm [9], the learned Mahalanobis metric [10], the reproducing kernel Hilbert space [11] and for maximising inner products [6]. All of these algorithms belong to the category of data independent hashing algorithms where the binary codes are obtained from some random matrices.\nOther algorithms compute hashes based on the data being\nindexed. Since the bits are used effectively, compact binary codes obtained from data dependent methods often have better performance when they are used for retrieving nearest neighbor points. One of the seminal approaches in this category is Spectral Hashing [12], which models neighborhood relationships from the original space in Hamming space. That is, the binary codes in the Hamming space should be close if their corresponding data points in the original space are close. This hashing approach has been extensively developed in recent years [13], [14], [15], [16]. In some data sets, label information is available for supervised methods. For example, in both supervised hashing [17] and minimal loss hashing [18], the similarity matrix is used to construct the loss function. Alternatively, binary codes may be designed by minimizing the quantization error between the binary codes and the features obtained from original data set [19], [20], [21], [22].\nMotivated by the success that auto-encoder models have had in preserving geometric information of high dimensional data sets [23], [24], we propose to use auto-encoders to learn binary codes for hashing. Our optimisation model is constructed from both the auto-encoder model and the optimal binary code’s perspectives.\nFrom the auto-encoder model, we use a three layer network to keep geometric information consistent with the high dimensional data points. The common approach for auto-encoder models is to assume that the data points are sampled from some manifold. The task is to find a set of parameters for the neural network such that the data points from the manifold are reconstructible. This means that the gap between the output data points and the input training data points is as small as possible. Points drawn from a domain around the manifold can be thought of as noisy data points. It is one of the features of the neural network model that it can process such points. Indeed, properly modeling the functions in this larger domain gives better feature learning capability for the model. Motivated by some recent efforts which aim to remove noise during the neural network forward propagation [25], [26], we define the optimal noise removing function directly and investigate its first order approximation which is used as a component in our later optimization model. We believe that the first order information from the optimal noise removing ar X\niv :1\n60 2.\n08 12\n7v 2\n[ cs\n.C V\n] 1\nM ar\n2 01\n6\nfunction makes the learned auto-encoder model robust, and enables the discovery of local relationships between the data points consistent with geometric information.\nFrom the optimal binary code perspective, our final interest is to obtain binary codes. Since the auto-encoder model does not provide these codes directly, we make use of the common constraint that minimizes the gap between the learned features and the ideal binary codes. We use an approximated 1-norm to model the gap minimizing problem aiming for a better distribution in Hamming space for the binary code.\nOur main contributions are as follows: • A novel noise removing function. We investigate a func-\ntion which has a noise absorbing property, and find that its Jacobian matrix has an intimate relationship with the tangent space in the local region. • A robust constraint that encourages the binary codes to have an optimal distribution in Hamming space. • A novel hashing algorithm. The auto-encoder model is adapted for generating effective binary codes as well as preserving geometric relationships of the original realworld data sets."
    }, {
      "heading" : "II. BACKGROUND",
      "text" : "A. Notation\nSuppose X = [x1, x2, · · · , xN ] ∈ RD×N are the training data points, we denote the underlying distribution of the training data as the manifold M, and, for each xi ∈ M, Ti ∈ RD×dM is a basis for the tangent space at xi. We use the three layer auto-encoder model to ensure the second layer (hidden layer) captures the geometric relationships in the training data set. Fig. 1 displays the three layer neural network used in our method. Denote W1 ∈ Rd×D and W2 ∈ RD×d as the weight matrices which connect the neural network from the first layer (input layer) to second layer and the second to third layer (output layer), and the biases in the second and third layers are denoted as b1 and b2. The features from the second layer are denoted as Y = [y1, y2, · · · , yN ] ∈ Rd×N where yi = tanh(W1xi + b1) and the features in the third layer are denoted as Z = [z1, z2, · · · , zN ] ∈ RD×N where zi = tanh(W2yi + b2). The main assumption of auto-encoder model is that input and output should be as similar as possible. Thus the object of the auto-encoder model is to find a set of parameters for the neural network such that\nC1(W1,W2, b1, b2) = N∑ i=1 ||zi − xi||2 (1)\nis minimized. The final binary code is obtained by B = sign(W1X) ∈ {−1, 1}d×N .\nB. Unified view of auto-encoders\nFor noise-free data, we expect that data points can be reconstructed exactly. Thus, it can be directly done by minimizing C1(W1,W2, b1, b2). For noisy data, consider a data point x out of the manifold M and z is the corresponding output, minimising the distance between x and z might deteriorate the final performance of the auto-encoder model since the\nideal output should be the noiseless data point m. In order to account for both the noise-free and noisy scenarios, we reformulate the optimization objective (Equation (1)) of the auto-encoder model in a larger region in a unified way: denote f as a function which is defined around the manifold M and the restriction of f to M is the identity, i.e., f(x) = x for ∀x ∈ M. Thus the unified optimization objective is to find a set of parameters for the neural network such that\nC2(W1,W2, b1, b2) = N∑ i=1 ||zi − f(xi)||2 (2)\nNote that for noise-free data, Equation (1) is equivalent to Equation (2), while for noisy data, the condition f(xi) = xi might not be satisfied and we have to define f appropriately.\nC. Related work\nRecent efforts for dealing with noisy data are the denoising auto-encoders (DAEs) [25] and contractive auto-encoders (CAEs) [26]. The main starting points of these algorithms are that the learned features in the hidden layer keep the important intrinsic structure of the original data set while unimportant information, such as the noise, is discarded as much as possible. In DAEs, noise is manually added to the training data points; since we know the correspondence between the noisy and clean versions, a constraint is introduced to minimize the gap between these two versions according to some loss function. In CAEs, the Jacobian norm of the function, which maps the input data points to the hidden layer, is minimized for a contractive effect. In the extreme case, CAEs may contract all of the data points in the original space to a single point. This constraint is used to discard noise, but the distance between the input and output points is also considered. In the balance of these two constraints, data on the manifold will be unchanged and data outside the manifold will contract to the manifold.\nBoth DAEs and CAEs focus on training a model such that noisy data are projected onto the manifold and non-noisy data are left alone. When the data points have some noise, i.e., they are distributed around the manifold, the output and hidden layers should keep the important information and at the same time be robust to some degree of noise. This process is\ndone implicitly with both approaches. DAEs introduce artificial noise, and assume that eliminating artificial noise will also help to eliminate real noise, i.e., the ideal noise data point is projected to the original data point through the forward propagation of the learned neural network. For CAEs, the noise discarding process is done through balancing geometric consistency and the contractive property by minimizing the norm of the Jacobian matrix. This motivates us to explore an explicit function which has the ideal noise absorbing property.\nFig. 2 displays one of the functions we are going to explore. Notice that each data point around the manifold is exactly projected to the nearest point on the manifold, while DAEs and CAEs do not have this kind of guarantee albeit they are designed to discard noise."
    }, {
      "heading" : "III. ALGORITHM",
      "text" : "A. Motivation\nThe binary codes we seek have some intersections with general feature learning in auto-encoder models. On one hand, the features of both approaches are from the hidden layer with the aim that features in the hidden layer have the same amount of information as in the original data set. On the other hand, previous auto-encoder models focus on obtaining discriminative features since their main purpose in learning these features is to train a classifier, and the auto-encoder model is viewed as a building block for a deep neural network. For hashing, we hope that the features in hidden layers are geometrically consistent with the original space, i.e., the relative order information between the data points is preserved, and the hidden features should be close to the binary codes under some metric.\nNotice that both CAEs and DAEs try to project data points around the manifold to data points on the manifold. This motivates us to define a function f with this property directly, and the data points around the manifold can be viewed as noisy data points, i.e., they are generated by data points from the manifold plus some noise. Without any prior information we assume that each noisy data point is generated from the\nclosest point on the manifold. Formally, we seek a function, f , such that\nf(x) = arg min m∈M\n||x−m||22. (3)\nThe object of training is to find a set of parameters for the network such that ∑N i=1 ||zi − f(xi)||2 is minimized.\nThe function f is only valid in some proper regions. This is because for data points outside the manifold, their closest point on the manifold might not be unique. In practice, we assume that the noise is constrained to a limited region around the manifold and therefore f is valid most of the time. Minimizing the distance between x and z can be viewed as the 0th-order approximation of the function f . More information about f can be captured by using higher order approximations and the Jacobian matrix of f is intimately related with the tangent space of M:\nTheorem 1. Suppose M is a d dimensional compact smooth submanifold in RD, f is a function defined as f(x) = arg minm∈M ||x −m||22, ∀x ∈ RD . For each m in M, let Tm ∈ RD×d be the local normal basis of the tangent space to M at m. Then, the Jacobian matrix of f at m is TmT ′m. (The detailed proof is provided in Appendix I.)\nIf we view the tangent space as a point in a Grassmannian manifold, Theorem 1 tells us that the Jacobian matrix at m ∈M is exactly the point where the Grassmannian point Tm is embedded in RD×D. Assuming the training data points are sampled from the manifoldM, for each point xi, we estimate its tangent space Ti by the local PCA technique. So now, the manifold M is approximated by tangent patches at the training data points. On the other hand, when parameters of the three layer neural network are fixed, the output features can be viewed as a function of the input features. Thus, the Jacobian matrix of the function obtained from the neural network can be analytically expressed. Specifically, for the data point xi, the Jacobian matrix Ji can be calculated by chain rule on the network:\nJi = W ′ 1 ( W ′2 ( 1− z22 ) ( 1− z23 )′) , (4)\nwhere is the point-wise product operator between two matrices, z2 and z3 are the corresponding features in second and third layer of the network, and (·)2 is the element-wise square of the entries of a vector. For the ith data point, we have Ji based on the parameters of the network, and we also have an approximation for Ti based on the data. Therefore, we propose to minimize the distance between Ji and TiT ′i in our optimization model.\nSince the features in the hidden layer range from −1 to 1, and binary codes are what we really need, we use the metric ||Y Y ′ − NI||1 to constrain the features in the hidden layer. The reason for this metric is that since the ideal elements in Y are either 1 or −1, the ideal diagonal position of Y Y ′ should be N exactly, and the non-diagonal position in Y Y ′ should be 0 to favour uncorrelated binary codes. The 1-norm, || · ||1, is calculated by summing the absolute value of the matrix’s elements. Since the absolute value function is nondifferentiable at 0, we approximate it by introducing a constant\nvalue = 0.0001, giving ||a||1 ≈ √ a2 + . We denote this approximate 1-norm as || · || 1. Finally, the optimization objective is\nmin C3(W1,W2, b1, b2) (5)\n= N∑ i=1 (||xi − zi||2F + ||Ji − TiT ′i ||2F ) + α||Y Y ′ −NI|| 1,\nwhere N is the number of training data points and α is a weight parameter balancing the optimal binary codes and the geometric relationships from the training data set. We haven’t introduced a weight for the Jacobian term to keep it consistent with the Taylor series expansion.\nFig. 3 shows a toy example for the visualization of the effects of the two optimization components. The data points are randomly sampled from a plane {(x1, x2, x3) | x1+x2+x3 = 1 and xi > 0}. The top picture in Fig. 3 shows the training data points in the 3D Euclidean space. After optimization, the hidden features are plotted in the bottom picture. Since\nwe want to encode the data points into binary vectors, the data points are distinguished according to their positions in different quadrants. The visualization of the hidden features fully explains the motivation of the proposed optimization model. Firstly, the robust auto-encoder is used to preserve the geometric information. For example, the transform from the training data points to the hidden features behaves like warping a piece of paper and the relative (distance) order information in local region is kept. Secondly, the constraint on the hidden features has the effect to disperse the hidden features to the vertices of the cube. Note that we only use three bits to encode the training data set which is sampled from a 2D plane, the number of different binary codes is at most 7. From the color in the bottom picture, we can see that the data points are encoded with 7 different binary codes according to their positions in different quadrants. Thus, the constraint makes us use the binary codes fully. In all, the proposed optimization model is capable of maintaining geometric information as well as learning optimal binary codes.\nB. Optimization To optimize the objective function C, we update the parameters by a mini-batch stochastic gradient descent method [27]. Since Equation (5) is non-convex, using mini-batches achieves a better solution in general, which motivates the use of the stochastic gradient descent method for learning the parameters. The maximum number of epoch is set to be Imax. In each epoch, the training data is randomly divided into m mini-batches and the parameters are updated with each minibatch. For updating the parameters, we have to calculate the gradients: ∂C∂W1 , ∂C ∂W2 , ∂C∂b1 and ∂C ∂b2\nrespectively. The detailed gradient calculation is provided in Appendix II. Denote θi as the vector of parameters W1, W2, b1 and b2 at the ith iteration and Gi as the gradient of C at the ith iteration. The step length λi is found by satisfying the Wolfe conditions [28].\nSince tanh is used in the neural network, the data points have to be normalized properly. In our experiments, we scale the data points by the inverse of the maximum norm from the training data set, and then scale the data points by 0.8 again as the training data points might not fully cover the distribution of the base and query data sets. For initializing the parameters, suppose µ is the mean of the training data points, W1 is initialized by the PCA projection times a random rotation matrix, and W2, b1 and b2 are initialized as W ′1, −W1µ and µ respectively. The reason that we use this set of initial parameters is that tanh can be approximated by a linear function around the origin by Taylor expansion method, and this set of parameters is optimal for reconstruction error when a linear function is used in the neural network. For estimating the tangent plane Txi(M), we retrieve the (D+d) nearest data points for each xi, and take the PCA projection which preserves 98% of the energy or the most energetic d dimensions, whichever is smaller. Algorithm 1 is a summary of the proposed training process.\nC. Computational complexity For the tangent plane estimation stage, the K nearest neighbors are located by brute-force which takes Θ(N2K) compu-\nAlgorithm 1 Auto-JacoBin 1: Input: Training data points x1, x2, · · · , xN , the maximum\nnumber of iteration Imax, and the number of batches m. 2: Output: The parameters W1,W2, b1 and b2. 3: Initialize W1,W2, b1, b2, and then vectorize them into a\nvector θ. 4: for i = 1 to N do 5: Estimate the tangent space Txi(M). 6: end for 7: for i = 1 to Imax do 8: Randomly permute the order of the training data set, and divided the training data into m subsets. 9: for j = 1 to m do\n10: Calculate the gradient Gij of the current jth subset. 11: Find the optimal step length λij such that the Wolfe conditions are satisfied. 12: Update the parameter θ:\nθ ← θ − λijGij\n13: end for 14: end for 15: Reshape the vector θ into parameters W1,W2, b1 and b2.\ntation time and the spectral decomposition takes Θ(ND3). In each iteration, calculating Y and Z takes Θ(NDd) time, and both the Jacobian matrices and the gradient calculation take Θ(ND2d). Since we use a line search method to find the step length, it might take multiple evaluations to satisfy the Wolfe conditions. It is too computationally and memory expensive to use all training data points at each optimisation step, and therefore we approximate by using a batch size of 1, 000 points. So m is N1000 in our experiments.\nThe proposed method takes relatively high computation during the training stage. In our experiment, for SIFT1M data set (N=10,000, D=128 and d=64), it takes about 2 minutes to estimate the tangent planes and 6 minutes to learn the parameters of the neural network, while the training times for NOKMeans, OKMeans, ITQ and Spectral Hashing are 15.18, 3.67, 1.79 and 1.229 seconds respectively. We should note that the offline training stage is only done once, and at runtime the method has the same encoding and retrieving time as other hashing methods."
    }, {
      "heading" : "IV. EXPERIMENTS",
      "text" : "To evaluate the performance of Auto-JacoBin, its performance is compared with several state-of-the-art hashing algorithms including LSH [8], Spectral Hashing [12], ITQ [19], OKMeans [21], and NOKmeans [22]. We also compare variants of Auto-JacoBin including different constraints and different optimization methods. We conduct experiments on two benchmark image data sets and two local feature data sets. Two data sets use global image features: 960D GIST features [29] in the GIST1M [30] set; and 128D wavelet texture feature [31] in the NUS-WIDE [32] set. We also use two local feature data sets, SIFT1M [30] and SIFT10M [22]. In SIFT1M, each data point is a 128D local SIFT feature [33]\nand extracted from Flickr images and INRIA Holidays images [34].\nSIFT10M is another SIFT feature data set, and it is extracted from Caltech-256 [35]. For each SIFT feature in SIFT10M data set, we have the corresponding image patch which provides a kind of ‘visualization’ of the corresponding SIFT feature and helps us analyze the performance of different hashing methods. Fig. 4 shows some random collection of these patches. From the figure, we can see the corresponding patches of the SIFT features in this data set.\nIn all four data sets, 10, 000 points are chosen for training. For GIST1M and SIFT1M, we use the provided query and base data sets. For SIFT10M, the base set consists of 10, 000, 000 points and it has the same number of points in the training and query sets as SIFT1M. For NUS-WIDE, we choose 10, 000 points as the query points, and the remaining as the base points. Summary information about these four data sets is given in Table I.\nA. Performance measurements\nTo evaluate the performance of different hashing methods, we first investigate how hashing methods are used for ANN problems. We take SIFT10M as an example since the corresponding patches provide the visualization of the retrieval results. Fig. 5 shows an example of finding the nearest neighbor data points for the query point. The top picture shows the patches of the query point and its true 99 nearest neighbor points among the 10, 000, 000 data points. Here the patch of the query point is highlighted by a red border, and the patches of the true 99 nearest neighbor points are ordered from best to worst along the rows. From top picture, we can see that SIFT features provide a faithful representation of image patches since the corresponding patches of the neighboring features are visually similar.\nFor each hashing method, we use 128 bits to encode each SIFT feature and retrieve the 1, 000 nearest neighbor data points in sense of the Hamming distance. Here we should note that the 1, 000 data points might not be exactly the same as the true 1, 000 nearest neighbor data points in sense of the Euclidean distance and we only retrieve 0.01% of the base data set. Then we retrieve the 99 nearest neighbor data points among the 1, 000 selected points in sense of the Euclidean distance, and the 99 data points are placed accordingly in each subfigure. If the data point is among the true 99 nearest neighbor points of the query in sense of the Euclidean distance, we highlight it by an orange border, otherwise the patch is plotted without any border.\nFrom Fig. 5, we can see that although the hashing methods return approximate nearest neighbor data points, i.e., the retrieved 99 points are not exactly the same as the true 99 nearest data points, the returned data points are very close to the query data points since their corresponding patches have similar visual appearance. Thus, this is the main reason why ANN is useful in practice. On the other hand, although the hashing methods return satisfactory results, we hope it can return the true nearest neighbor data points in sense of Euclidean distance as frequently as possible since this would guarantee the reliability of image retrieval or feature matching. Thus, the percentage of the true nearest neighbor data points retrieved is an important indicator for evaluating the performance of different hashing methods. From the number of orange borders in Fig. 5, we can see that most of the true nearest neighbor data points are retrieved when only 0.01% of the base data set is considered. According to this indication, Auto-JaboBin has the leading performance for this query point since it has the highest percentage for retrieving the true 99 nearest neighbor data points.\nBased on the desirable features of an ANN method in practice as outlined above, we adopt recall as the main indicator for evaluating retrieval performance. For the jth query data point, denote TEj as the set of k true Euclidean nearest neighbor points and THij as the i data points retrieved in Hamming space, the recall is defined as the proportion of true nearest neighbor points retrieved and Recall@i is defined as the average recall performance of all query points. Formally,\nit is defined as\nRecall@i = 1\nN N∑ j=1 |TEj ∩ THij | |TEj |\n(6)\nIn order to see the performance of the hashing methods when different number of data points in Hamming space are retrieved, i is varied from 1 to K = 10, 000 in the following experiments.\nFor evaluating the overall retrieval performance, we use the m-Recall measure which averages Recall@i when the number of data points retrieved in Hamming space is up to K and it is defined as:\nm-Recall = ∑K i=1 Recall@i\nK . (7)\nAnother related indicator for measuring the retrieval performance is precision. Actually, the only difference of the definitions between recall and precision is the denominator, which is the number of true nearest data points for recall and the number of retrieved data points in precision. From the definitions, we can see that recall emphasizes the percentage of true nearest neighbor points retrieved, while precision emphasizes the percentage of true nearest neighbor points among the retrieved data points. As discussed previously k is much smaller than K for the ANN problem, the precision will be almost 0 due to the high value of K regardless of whether the true nearest neighbor points are retrieved or not. For massive data sets, we are most interested in the recall performance when only a small proportion of the database is retrieved. From the definition of m-Recall, we can see that it reflects overall performance for this scenario.\nB. Parameter selection\nThere are two parameters that need to be chosen for AutoJacobin: the number of iterations of the line search optimization process and the weight parameter α. We have analyzed the\ncosts during optimization and notice that it converges quickly. As a case in point, Fig. 6 shows the cost after each iteration. The experiment is conducted on NUS-WIDE data set. The task is to learn 64 bits for encoding the base data set, and the weight parameter is set to be 0.1. From the figure, we can see that the objective function converges quickly and it makes tiny changes after the first 20 iterations. Another observation from the costiteration plot is that the cost decreases monotonically despite lack of guarantees that this would occur since the input data points keep changing in each iteration. Thus, we can either set it to be a fixed value as done in our experiments or terminate when the cost is below a threshold or set it to stop when the change in cost over recent iterations drops below some threshold. In our experiments, the number of training data points is 10, 000, the batch size is 1, 000, and the training data points are randomly shuffled in each epoch. Thus, each pass of the training data set takes m = 10 iterations. We set the total number of iterations to be 50 in accordance with other hashing methods. Therefore, the optimization cycles through the training data set Imax = 5 times in total.\nFor the weight parameter α, we empirically test its value at 0.01, 0.1, 1, 10. Fig. 7 shows the m-Recall when the data\npoints are encoded with 64, 96 and 128 bits respectively for the nearest point search for each query in the NUS-WIDE data set. For visual comparison, we have included the corresponding m-Recall performance of NOKMeans which is a competing hashing method as shown in the experiments later. From the chart in Fig. 7, we can see that the proposed approach has stable performance when the weight parameter α ranges from 0.01 to 10, and its overall performance is always better than NOKMeans on the NUS-WIDE data set. In the following experiments, we fix the weight parameter α to be 0.1.\nC. Performance with different auto-encoder models\nIn order to see the impact of the first order constraint for Auto-JacoBin, we report the comparative results for AutoBin - an optimization without the Jacobian component. The cost function of AutoBin is optimized with two different methods. One, referred to as AutoBin1, uses the same optimization method as in Auto-JacoBin. The other, referred to as AutoBin2, is using the whole training data set in each iteration.\nThe proposed auto-encoder is motivated by DAEs and CAEs which are designed to have a noise removing effect. It is also interesting to see the performance of the corresponding hashing methods when the auto-encoder component is replaced with DAEs and CAEs respectively. We refer these two new hashing methods as denoising auto-encoder binary hashing (DAutoBin) and contractive auto-encoder binary hashing (CAutoBin). For DAutoBin, the noise is injected by randomly setting a fixed percentage of entries of original data points as 0 as it is used in [25]. Take the 960D GIST feature as an example, suppose we fix the percentage by threshold t, we generate a random number ri (i = 1, 2, · · · , 960) between 0 and 1 for each entry of x = (x1, x2, · · · , x960)′ ∈ R960, and set\nxi = { xi, ri > t 0, ri ≤ t\n(8)\nWe empirically test the parameter t from a set {0.01, 0.05, 0.1, 0.2} and the weight parameter α from\na set {0.01, 0.1, 1, 10}. For CAutoBin, both the parameter λ for the Jacobian norm and the weight parameter α are tuned from the set {0.01, 0.1, 1, 10}. Finally, the optimization for DAutoBin and CAutoBin is similar to Algorithm 1. The main difference is the gradient calculations which can be derived in the same way as for Auto-JacoBin.\nFig. 8 shows the performance of the hashing methods based on different auto-encoder models on NUS-WIDE. Each data point is encoded with 128 bits, k is set to 100, and the Recall@i performance is reported when i ranges from 1 to 10, 000. From the figure, we can see that the Recall@i of AutoJacoBin is consistently better than the performance of other hashing methods. DAutoBin, CAutoBin and AutoBin1 have comparable results, and AutoBin2 gives the lowest Recall@i. Although the objective function of AutoBin1 and AutoBin2 is the same, their performance is quite different, which can be attributed to mini-batch stochastic gradient descent being less likely to get stuck in local optima.\nFrom our experiments, we note that, DAutoBin is slightly better than AutoBin1, and AutoBin1 is slightly better than CAutoBin. The α in CAutoBin is set to be 0.01, we found that the performance of CAutoBin decreases when α increases in our experiments. The possible reason is that the denoising effect in CAutoBin is achieved implicitly by balancing two components. Due to the contractive component, the recovery of the original data points is not as good as using auto-encoder directly. Thus the performance of CAutoBin is decreased slightly. For DAutoBin, the noise is injected manually and the neural network is trained to recover the data point. One of the problems for this approach is that it is unclear why the original data is the best recovery of the noisy data. Here is an extreme case, suppose x1 and x2 are two close but different data points, thus they might have the same noisy data point x. According to the definition, when x is an input data point, the output of DAutoBin is assumed to be x1 and x2 at the same time. This contradicts the fact that x1 and x2 are two distinct points. On the other hand, Auto-JacoBin does not have\nthis kind of contradictory behavior since it estimates the best projection of the noisy data point. We believe this is one of the reasons that Auto-JacoBin has a better performance than DAutoBin.\nD. Results on Benchmark data sets\nThe computational task is to find the k nearest neighbor points in the base data set for each query. Here we evaluate scenarios where k ∈ {1, 5, 10, 50, 100}. The Recall@i performance is reported when i ranges from 1 to 10, 000. Therefore, only a part of the base data set is retrieved (1%, 0.1% and 0.1% for the NUS-WIDE, GIST1M and SIFT1M respectively). For both NUS-WIDE and SIFT1M, we report the Recall@i performance when the number of bits, which are used to encode the data points, is up to its feature’s dimension, i.e., they are tested with 64, 96 and 128 bit encodings. For the GIST1M data set, more bits are used to encode the data points because the underlying features have more dimensions. In our experiments, GIST1M is tested with 64, 128 and 256 bit encodings. The results for NUS-WIDE are shown in Fig. 9 and the results for GIST1M is in Fig. 10. The Recall@i\nperformance of different hashing approaches for SIFT1M are much closer and more difficult to see in a graph, and therefore we present the m-Recall in a table. Table II shows the m-Recall performance of different hashing methods on the SIFT1M data set. Each row corresponds to one specific setting, where (i, j) means the retrieval task is to find the i nearest neighbor points for each query and j bits are used for encoding the data set.\nFrom Fig. 9 and Fig. 10, it is clear that the proposed method performs much better than previous techniques. Other observations are that (unsurprisingly) using more bits results in better performance for all methods, and that the performance decreases as k increases. Somewhat surprisingly, the data independent LSH algorithm performs comparably with many of the data-dependent algorithms, especially as the number of bits increases. One of the main reasons is that when modeling the data distribution, the data dependent hashing algorithms are done by some kind of relaxation such as obtaining the binary code from some threshold or encoding the data point into its nearest binary code. It is inevitable that the information contained in the original data set is lost to some degree when a limited number of bits are used to encode each data point during this process. While for\nLSH, the obtained binary codes can preserve the similarity between the data points even though the projection is randomly generated. Another interesting point is that the comparative performance of different hashing algorithms depends on the data set. For NUS-WIDE, when few bits are used to encode the data points, Spectral Hashing has the lowest score. When the number of bits is increased, the performance of Spectral Hashing is comparable to LSH and NOKMeans, and has much better performance than ITQ and OKMeans. Both ITQ and OKMeans assume that their projection matrix is orthogonal. This constraint limits the separating capacity of different bits. That is why when more bits are used for encoding of the data points, the performance does not increase as much as other algorithms. For the proposed hashing method and Spectral Hashing and LSH, the projection matrices do not have this kind of constraint. Thus the potential partition capability of these algorithms is better and they show more consistent performance.\nOverall, the benefits of the proposed method are clear. Even with a limited bit budget, the proposed hashing method has excellent Recall@i performance. For instance, when 64 bits are used to encode each data point on the NUS-WIDE data set, Recall@1000 is 0.92, whilst the previous best is NOKMeans at 0.81. Also, to ensure good recall, when using 256 bits on the GIST1M data set, the proposed method requires K = 2, 000 to achieve a 0.9 Recall@i. Compared with K = 6, 000+ for the other methods, ours allows for improved computational performance in practice.\nFor the local feature data set, the proposed method has comparable performance to state-of-the-art results (NOKMeans, OKMeans and ITQ). For better visualization, we summarize some of the results based on the m-Recall indicator. Table II shows the m-Recall when the data points are encoded with different bits and for different retrieving tasks. From this table, we can see that the proposed method has comparable performance with other state-of-the-art algorithms. Its performance becomes better when more bits are used to encode the data points. Another observation we can make is that, despite its good performance on NUS-WIDE and GIST1M, LSH is consistently the worst, or nearly the worst, performer for this\ndata set."
    }, {
      "heading" : "V. SUMMARY",
      "text" : "In this work, we have proposed a novel hashing algorithm which adopts the auto-encoder model to produce binary codes that are geometrically consistent with the data points in the original space. The model we used leverages the merits of state-of-the-art models for auto-encoders. We have introduced a new objective function that makes use of the first order properties of geometric conservation (the Jacobian matrix) and has good noise-reduction properties. We proved that the Jacobian of the defined function can be used to approximate the tangent space of the manifold of the training data set, and show how the Jacobian can be expressed analytically.\nThe experiments are conducted on three large scale feature data sets. The performance of the proposed method is compared with several state-of-the-art hashing algorithms. It has the best performance for global image features and comparable performance for the SIFT feature data sets.\nCurrently, we are interested in binary codes for large-scale high-dimensional data sets. Thus, we adapt the proposed autoencoder model by adding constraints in the hidden layer. In the future, we plan to investigate the applications of the new auto-encoder model in other feature learning fields including dimensionality reduction and deep learning."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "Binary codes can be used to speed up nearest neighbor search tasks in large scale data sets as they are efficient for both storage and retrieval. In this paper, we propose a robust auto-encoder model that preserves the geometric relationships of high-dimensional data sets in Hamming space. This is done by considering a noise-removing function in a region surrounding the manifold where the training data points lie. This function is defined with the property that it projects the data points near the manifold into the manifold wisely, and we approximate this function by its first order approximation. Experimental results show that the proposed method achieves better than state-of-the-art results on three large scale high dimensional data sets.",
    "creator" : "LaTeX with hyperref package"
  }
}