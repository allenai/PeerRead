{
  "name" : "1605.05906.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Automatic TM Cleaning through MT and POS Tagging: Autodesk’s Submission to the NLP4TM 2016 Shared Task",
    "authors" : [ "Alena Zwahlen", "Olivier Carnal", "Samuel Läubli" ],
    "emails" : [ "samuel.laubli}@autodesk.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: Translation Memory, Machine Learning, Machine Translation, Part-of-Speech Tagging"
    }, {
      "heading" : "1. Introduction",
      "text" : "Autodesk has accumulated more than 40 million professionally translated segments over the past 17 years. These translation units (TUs) mainly stem from user interfaces and documentation of software products localized into 32 languages. As we are now unifying and centralizing all translations in a single repository, it is high time to sort out duplicate, outdated, and erroneous TUs. Exploring methods to handle the latter – clearly more challenging than removing duplicate and outdated material – motivated us to participate in the First Shared Task on Translation Memory Cleaning (Barbu et al., 2016). Going forward, we strive to make human translation more efficient (by showing translators less erroneous fuzzy matches) and machine translation more accurate (by reducing noise in training data). In this paper, we describe our submitted system for distinguishing correct from incorrect TUs. Rather than tailoring it to individual languages, we aimed at a languageindependent solution to cover all of the language pairs in this shared task or, looking to the future, Autodesk’s production environments. The system is based on previous work by Barbu (2015) and uses language-independent features with language-specific plug-ins, such as machine translation, part-of-speech tagging, and language classification. Specifics about previous work are given in the next section. In Section 3, we describe our method and, in Section 4, show how it compares to Barbu’s (2015) approach as well as other submissions to this shared task. Lastly, we offer preliminary conclusions and outline future work in Section 5."
    }, {
      "heading" : "2. Background",
      "text" : "TM cleaning functionality in commercial tools is mostly rule-based, centering around the removal of duplicate entries, ensuring markup validity (e.g., no unclosed tags), or controlling for client or project specific terminology . Although helpful, these methods fall short of identifying spurious entries that contain language errors or partial translations. With crowd-sourced and automatically constructed TMs in particular, it is also necessary to identify transla-\ntion units with source and target segments that do not correspond at all (e.g., Trombetti, 2009; Tiedemann, 2012). Barbu (2015) has proposed to cast the identification of such incorrect translations as a supervised classification problem. In his work, 1,243 labelled TUs were used to train binary classifiers based on 17 features. The “most important” of them, according to the author, were bisegment_similarity and lang_diff : the former is defined as the cosine similarity between a target segment and its machine translated source segment, while the latter denotes whether the language codes declared in a translation unit correspond with the codes detected by a language detector. The best classifier, a support vector machine with linear kernel, achieved 82% precision and 81% recall on a held-out test set of 309 TUs. To the best of our knowledge, Barbu provided the first and so far only research contribution on automatic TM cleaning, which the author himself described as “a neglected research area” (Barbu, 2015). With our participation to this shared task, we seek to extend his work by examining new features based on statistical MT and POS tagging. As outlined above, comparing machine translated source segments to their actual target segments has proven effective in Barbu’s (2015) experiments. We propose to complement or replace the similarity function used for this comparison (cosine similarity) by two automatic MT evaluation metrics, Bleu (Papineni et al., 2002) and characterbased Levenshtein distance, in order to reward higher-order n-gram (n > 1) and partial word overlaps, respectively. Furthermore, we introduce a recall-based MT feature that takes multiple MT hypotheses (n-best translations) of a given source segment into account, based on the assumption that alternative translations of words (such as “buy” and “purchase”) or phrases (such as “despite” and “in spite of”) should not be punished. We also experiment with part-of-speech information to identify spurious translation units. With closely related languages in particular, the rationale would be that adjectives (to name an example) in a source segment are likely to be reflected in the corresponding target segment in case of a valid translation. The comparison of POS tags from language-specific tagsets will be based on a mapping to ar X iv :1 60 5. 05 90 6v 1 [ cs .C L ] 1 9 M ay 2\n01 6\neleven coarse-grained, language-independent grammatical groups (Petrov et al., 2011). We acknowledge that the use of MT is discouraged by the organizers of this shared task to foster contributions that require less compute power. However, as MT was found to be valuable in previous work (see above) and computational resources are hardly a limiting factor in corporate environments (see Section 3.2), we decided not to refrain from including MT-based features in our submissions."
    }, {
      "heading" : "3. Method",
      "text" : "Our system uses labelled TUs to train classifiers based on language-independent features (see Section 3.1) with language-specific plug-ins (see Section 3.2). The feature extraction pipeline is implemented in Scala (see Section 3.3), and our final submission – geared to distinguish correct or almost correct from incorrect TUs – is based on a selection of nine features (see Section 3.4)."
    }, {
      "heading" : "3.1. Features",
      "text" : "We re-implemented the 17 features proposed by Barbu (2015, see also Section 2). In addition, we explore\n• mt_coverage the percentage of target words contained in the n-best machine translations of the source segment. We use n = 20 in our experiments.\n• mt_cfs the character-based Levenshtein distance between target segment and machine translated source segment. We normalise this score such that identical and completely dissimilar segments result in scores of 1.0 and 0.0 respectively, i.e.,\ncfs = 1− Levensthein distance in characters number of characters in longer segment.\nThis score is computed individually for each of the 20- best translation options; the best of these scores instantiates the feature value.\n• mt_bleu the BLEU score (Papineni et al., 2002) between target segment and machine translated source segment. We employ the sentence-level version of the metric as implemented in Phrasal (Green et al., 2014). As with mt_cfs, individual scores are computed for each of the 20-best translation options; the best score instantiates the feature value.\n• pos_sim_all the cosine similarity between the partof-speech (POS) tags found in the source and target segment.\n• pos_sim_some the cosine similarity between source and target segment in terms of nouns (NOUN), verbs (VERB), adjectives (ADJ), and pronouns (PRON).\n• pos_exact whether or not the POS tag sequence in source and target segment is identical.\n• language_detection whether or not a state-of-the-art language classifier confirms the target segment’s language declared in the translation unit.\n• ratio_words the ratio between number of words in source and target segment.\n• ratio_chars the ratio between number of characters in source and target segment."
    }, {
      "heading" : "3.2. Resources",
      "text" : "Some of the features described in the previous section require natural language processing (NLP) facilities. For machine translation, we use our in-house systems (Plitt and Masselot, 2010; Zhechev, 2014) based on the Moses SMT framework (Koehn et al., 2007). They are trained on translated software and user manuals from Autodesk products only and chosen for the sake of convenience; we would expect better performance of our MT-based features in conjunction with MT engines geared to the text domains used in this shared task (listed in Table 1). Our engines are integrated into a scalable infrastructure deployed on an elastic compute cloud, allowing high throughput even with large translation memories to be cleaned. For POS tagging, we rely on Schmid’s (1995) TreeTagger and its readily available models1 for English, German, Italian, and Spanish. To make POS tags comparable across these languages, they are mapped2 to the Universal Tagset proposed by Petrov et al. (2011). Lastly, we use the publicly available Xerox Language Identifier API3 for language detection."
    }, {
      "heading" : "3.3. Classification",
      "text" : "Our feature extraction pipeline, including Barbu’s (2015) as well as our own features (see Section 3.1), is implemented in Scala. This pipeline is used to transform translation units into feature vectors and train classifiers using the scikitlearn framework (Pedregosa et al., 2011). From the various classification algorithms we tested, Random Forests performed best with our selection of features (see below)."
    }, {
      "heading" : "3.4. Feature Selection",
      "text" : "For the reasons mentioned in Section 1, we aimed at finding a combination of features that would perform well with all language pairs rather than tailoring solutions to individual\n1http://www.cis.uni-muenchen.de/~schmid/ tools/TreeTagger/\n2https://github.com/slavpetrov/ universal-pos-tags\n3https://open.xerox.com/Services/ LanguageIdentifier\nlanguages. We focused on gearing our classifiers to distinguish correct or almost correct (classes 1, 2) from incorrect TUs (class 3) – i.e., the Binary Classification (II) task – by optimising the weighted F1-score (F1) on training data (see Tables 2a and 2b). From the various feature combinations we tested, we found the following to be most successful: ratio_words, pos_sim_all, language_detection, mt_cfs, mt_bleu, ratio_chars (as described in Section 3.1), alongside cg_score, only_capletters_dif, and punctuation_similarity (from Barbu, 2015). Evaluation results are given in the next section."
    }, {
      "heading" : "4. Results",
      "text" : "We tested our final submission – a Random Forests classifier based on the nine features described in Section 3.4 – on three language pairs (en–de, en–es, en–it) and two tasks: Binary II and Fine-Grained Classification (see Sections 4.1 and 4.2, respectively). The classifier was trained solely on data provided by the organizers of this shared task for each of the language × task conditions. Each TU in this data was annotated with one of three labels: correct, almost correct, and incorrect (see Table 1)."
    }, {
      "heading" : "4.1. Binary Classification (II)",
      "text" : "Our rationale for focusing on telling apart correct or almost correct from incorrect TUs was that a first application of our method, if successful, would most likely be the filtering of TM data for MT training. While eliminating almost correct TUs might decrease rather than increase MT quality, filtering out incorrect segments can have a positive impact (Vogel, 2003). Prior to submission, we benchmarked our system against the two baselines provided by the organizers: a dummy classifier assigning random classes according to the overall class distribution in the training data (Baseline 1), and a classifier based on the Church-Gale algorithm as adapted by Barbu (2015) (Baseline 2). More importantly, however, we compared our system to Barbu’s (2015) approach, using the classification algorithms which reportedly worked best with the 17 features in his work. Our system performed well in this comparison, surpassing Barbu’s approach in all language pairs except en–de, where both systems were en par. Details are shown in Table 2a, where we report weighted precision (P), recall (R), and F1-scores averaged over 5-fold cross-validation with 2/3–1/3 splits of the training data.\nThe final evaluation and ranking produced by the organizers, shown in Table 3a, confirms our findings from experimenting with training data: our system performs well on the en–es and en–it test sets (best in class), while performance is substantially lower on the en–de test set. The reasons for this are yet to be ascertained (see also Section 5)."
    }, {
      "heading" : "4.2. Fine-Grained Classification",
      "text" : "Although geared to the Binary Classification (II) task (see above), we also assessed our system on the Fine-Grained Classification task. Here, the goal was to distinguish between all of the three classes, i.e., determine whether a TU is correct, almost correct, or incorrect. Again, we compared our system’s performance to Barbu’s (2015) method, using 2/3–1/3 splits of the training data (5- fold cross-validation). The results, shown in Table 2b, implied that the nine features we selected would not suffice for a more fine-grained classification of TUs. This was confirmed in the official evaluation and ranking: our system scored low on en–de and mediocre on en–es and en–it. Further work will be needed to analyse these results in more detail."
    }, {
      "heading" : "5. Conclusions",
      "text" : "We have proposed a machine learning based method to identify incorrect entries in translation memories. It is applicable to any language pair for which an MT system, a POS tagger, and a language identifier are available. Implemented using off-the-shelf tools, our system achieved the best classification results for two out of three language pairs (English–Italian and English–Spanish) in the Binary Classification (II) task. In future work, we would like to assess the impact of gearing NLP components to target domains on classification accuracy. The training data in this shared task stems from news (German) and medical texts (Italian, Spanish) which our MT systems, for example, were not optimized for. This domain mismatch might partially explain why our system did not perform well on the English–German test set. More importantly, however, we would like to test our implementation as-is in Autodesk’s production environments for software localization. Removing incorrect segments from TMs could ultimately help make professional translation more efficient by providing better MT (through filtered training data) and more accurate fuzzy matches."
    }, {
      "heading" : "6. Acknowledgements",
      "text" : "We would like to thank Valéry Jacot for his vital support and guidance."
    }, {
      "heading" : "7. Bibliographical References",
      "text" : "Barbu, E. (2015). Spotting false translation segments in\ntranslation memories. In Proceedings of the Workshop on Natural Language Processing for Translation Memories (NLP4TM), pages 9–16, Hissar, Bulgaria.\nBarbu, E., Escartín, C. P., Bentivogli, L., Negri, M., Turchi, M., Federico, M., Mastrostefano, L., and Orasan, C. (2016). 1st shared task on automatic translation memory cleaning preparation and lessons learned. In Proceedings\nof the 2nd Workshop on Natural Language Processing for Translation Memories (NLP4TM), Portorož, Slovenia.\nGreen, S., Cer, D., and Manning, C. D. (2014). Phrasal: A toolkit for new directions in statistical machine translation. In Proceedings of the 9th Workshop on Statistical Machine Translation (WMT), pages 114–121, Baltimore, USA.\nKoehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico, M., Bertoldi, N., Cowan, B., Shen, W., Moran, C., Zens, R., Dyer, C., Bojar, O., Constantin, A., and Herbst, E. (2007). Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL), pages 177–180, Prague, Czech Republic.\nPapineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2002). Bleu: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 311–318, Philadelphia, Pennsylvania.\nPedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and Duchesnay, E. (2011). Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830.\nPetrov, S., Das, D., and McDonald, R. (2011). A universal part-of-speech tagset. arXiv preprint, arXiv:1104.2086.\nPlitt, M. and Masselot, F. (2010). A productivity test of statistical machine translation post-editing in a typical localisation context. Prague Bulletin of Mathematical Linguistics, 93:7–16.\nSchmid, H. (1995). Improvements in part-of-speech tagging with an application to German. In Proceedings of the ACL SIGDAT Workshop, pages 47–50, Dublin, Ireland.\nTiedemann, J. (2012). Parallel data, tools and interfaces in OPUS. In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC), pages 2214–2218, Istanbul, Turkey.\nTrombetti, M. (2009). Creating the world’s largest translation memory. In Proceedings of the 12th Machine Translation Summit (MT Summit), Ottawa, Canada.\nVogel, S. (2003). Using noisy biligual data for statistical machine translation. In Proceedings of Meeting of the 10th Conference of the European Chapter of the Association of Computational Linguistics (EACL), pages 175– 178, Budapest, Hungary.\nZhechev, V. (2014). Analysing the post-editing of machine translation at Autodesk. In O’Brian, S., Balling, L. W., Carl, M., Simard, M., and Specia, L., editors, Post-editing of Machine Translation: Processes and Applications, pages 2–23. Cambridge Scholars Publishing."
    } ],
    "references" : [ {
      "title" : "Spotting false translation segments in translation memories",
      "author" : [ "E. Barbu" ],
      "venue" : "Proceedings of the Workshop on Natural Language Processing for Translation Memories (NLP4TM), pages 9–16, Hissar, Bulgaria.",
      "citeRegEx" : "Barbu,? 2015",
      "shortCiteRegEx" : "Barbu",
      "year" : 2015
    }, {
      "title" : "1st shared task on automatic translation memory cleaning preparation and lessons learned",
      "author" : [ "E. Barbu", "C.P. Escartín", "L. Bentivogli", "M. Negri", "M. Turchi", "M. Federico", "L. Mastrostefano", "C. Orasan" ],
      "venue" : "Proceedings",
      "citeRegEx" : "Barbu et al\\.,? 2016",
      "shortCiteRegEx" : "Barbu et al\\.",
      "year" : 2016
    }, {
      "title" : "Phrasal: A toolkit for new directions in statistical machine translation",
      "author" : [ "S. Green", "D. Cer", "C.D. Manning" ],
      "venue" : "Proceedings of the 9th Workshop on Statistical Machine Translation (WMT), pages 114–121, Baltimore, USA.",
      "citeRegEx" : "Green et al\\.,? 2014",
      "shortCiteRegEx" : "Green et al\\.",
      "year" : 2014
    }, {
      "title" : "Moses: Open source toolkit for statistical machine translation",
      "author" : [ "P. Koehn", "H. Hoang", "A. Birch", "C. Callison-Burch", "M. Federico", "N. Bertoldi", "B. Cowan", "W. Shen", "C. Moran", "R. Zens", "C. Dyer", "O. Bojar", "A. Constantin", "E. Herbst" ],
      "venue" : "Proceedings of the 45th Annual",
      "citeRegEx" : "Koehn et al\\.,? 2007",
      "shortCiteRegEx" : "Koehn et al\\.",
      "year" : 2007
    }, {
      "title" : "Bleu: A method for automatic evaluation of machine translation",
      "author" : [ "K. Papineni", "S. Roukos", "T. Ward", "Zhu", "W.-J." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 311–318, Philadelphia, Pennsylvania.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Scikit-learn: Machine learning in Python",
      "author" : [ "F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay" ],
      "venue" : "Jour-",
      "citeRegEx" : "Pedregosa et al\\.,? 2011",
      "shortCiteRegEx" : "Pedregosa et al\\.",
      "year" : 2011
    }, {
      "title" : "A universal part-of-speech tagset",
      "author" : [ "S. Petrov", "D. Das", "R. McDonald" ],
      "venue" : "arXiv preprint, arXiv:1104.2086.",
      "citeRegEx" : "Petrov et al\\.,? 2011",
      "shortCiteRegEx" : "Petrov et al\\.",
      "year" : 2011
    }, {
      "title" : "A productivity test of statistical machine translation post-editing in a typical localisation context",
      "author" : [ "M. Plitt", "F. Masselot" ],
      "venue" : "Prague Bulletin of Mathematical Linguistics, 93:7–16.",
      "citeRegEx" : "Plitt and Masselot,? 2010",
      "shortCiteRegEx" : "Plitt and Masselot",
      "year" : 2010
    }, {
      "title" : "Improvements in part-of-speech tagging with an application to German",
      "author" : [ "H. Schmid" ],
      "venue" : "Proceedings of the ACL SIGDAT Workshop, pages 47–50, Dublin, Ireland.",
      "citeRegEx" : "Schmid,? 1995",
      "shortCiteRegEx" : "Schmid",
      "year" : 1995
    }, {
      "title" : "Parallel data, tools and interfaces in OPUS",
      "author" : [ "J. Tiedemann" ],
      "venue" : "Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC), pages 2214–2218, Istanbul, Turkey.",
      "citeRegEx" : "Tiedemann,? 2012",
      "shortCiteRegEx" : "Tiedemann",
      "year" : 2012
    }, {
      "title" : "Creating the world’s largest translation memory",
      "author" : [ "M. Trombetti" ],
      "venue" : "Proceedings of the 12th Machine Translation Summit (MT Summit), Ottawa, Canada.",
      "citeRegEx" : "Trombetti,? 2009",
      "shortCiteRegEx" : "Trombetti",
      "year" : 2009
    }, {
      "title" : "Using noisy biligual data for statistical machine translation",
      "author" : [ "S. Vogel" ],
      "venue" : "Proceedings of Meeting of the 10th Conference of the European Chapter of the Association of Computational Linguistics (EACL), pages 175– 178, Budapest, Hungary.",
      "citeRegEx" : "Vogel,? 2003",
      "shortCiteRegEx" : "Vogel",
      "year" : 2003
    }, {
      "title" : "Analysing the post-editing of machine translation at Autodesk",
      "author" : [ "V. Zhechev" ],
      "venue" : "O’Brian, S., Balling, L. W., Carl, M., Simard, M., and Specia, L., editors, Post-editing of Machine Translation: Processes and Applications, pages 2–23. Cambridge Scholars Publishing.",
      "citeRegEx" : "Zhechev,? 2014",
      "shortCiteRegEx" : "Zhechev",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "It extends previous work by Barbu (2015) through incorporating recall-based machine translation and part-of-speech-tagging features.",
      "startOffset" : 28,
      "endOffset" : 41
    }, {
      "referenceID" : 1,
      "context" : "Exploring methods to handle the latter – clearly more challenging than removing duplicate and outdated material – motivated us to participate in the First Shared Task on Translation Memory Cleaning (Barbu et al., 2016).",
      "startOffset" : 198,
      "endOffset" : 218
    }, {
      "referenceID" : 0,
      "context" : "The system is based on previous work by Barbu (2015) and uses language-independent features with language-specific plug-ins, such as machine translation, part-of-speech tagging, and language classification.",
      "startOffset" : 40,
      "endOffset" : 53
    }, {
      "referenceID" : 0,
      "context" : "In Section 3, we describe our method and, in Section 4, show how it compares to Barbu’s (2015) approach as well as other submissions to this shared task.",
      "startOffset" : 80,
      "endOffset" : 95
    }, {
      "referenceID" : 9,
      "context" : "With crowd-sourced and automatically constructed TMs in particular, it is also necessary to identify translation units with source and target segments that do not correspond at all (e.g., Trombetti, 2009; Tiedemann, 2012).",
      "startOffset" : 181,
      "endOffset" : 221
    }, {
      "referenceID" : 0,
      "context" : "To the best of our knowledge, Barbu provided the first and so far only research contribution on automatic TM cleaning, which the author himself described as “a neglected research area” (Barbu, 2015).",
      "startOffset" : 185,
      "endOffset" : 198
    }, {
      "referenceID" : 4,
      "context" : "We propose to complement or replace the similarity function used for this comparison (cosine similarity) by two automatic MT evaluation metrics, Bleu (Papineni et al., 2002) and characterbased Levenshtein distance, in order to reward higher-order n-gram (n > 1) and partial word overlaps, respectively.",
      "startOffset" : 150,
      "endOffset" : 173
    }, {
      "referenceID" : 0,
      "context" : "As outlined above, comparing machine translated source segments to their actual target segments has proven effective in Barbu’s (2015) experiments.",
      "startOffset" : 120,
      "endOffset" : 135
    }, {
      "referenceID" : 6,
      "context" : "eleven coarse-grained, language-independent grammatical groups (Petrov et al., 2011).",
      "startOffset" : 63,
      "endOffset" : 84
    }, {
      "referenceID" : 4,
      "context" : "• mt_bleu the BLEU score (Papineni et al., 2002) between target segment and machine translated source segment.",
      "startOffset" : 25,
      "endOffset" : 48
    }, {
      "referenceID" : 2,
      "context" : "We employ the sentence-level version of the metric as implemented in Phrasal (Green et al., 2014).",
      "startOffset" : 77,
      "endOffset" : 97
    }, {
      "referenceID" : 7,
      "context" : "For machine translation, we use our in-house systems (Plitt and Masselot, 2010; Zhechev, 2014) based on the Moses SMT framework (Koehn et al.",
      "startOffset" : 53,
      "endOffset" : 94
    }, {
      "referenceID" : 12,
      "context" : "For machine translation, we use our in-house systems (Plitt and Masselot, 2010; Zhechev, 2014) based on the Moses SMT framework (Koehn et al.",
      "startOffset" : 53,
      "endOffset" : 94
    }, {
      "referenceID" : 3,
      "context" : "For machine translation, we use our in-house systems (Plitt and Masselot, 2010; Zhechev, 2014) based on the Moses SMT framework (Koehn et al., 2007).",
      "startOffset" : 128,
      "endOffset" : 148
    }, {
      "referenceID" : 3,
      "context" : "For machine translation, we use our in-house systems (Plitt and Masselot, 2010; Zhechev, 2014) based on the Moses SMT framework (Koehn et al., 2007). They are trained on translated software and user manuals from Autodesk products only and chosen for the sake of convenience; we would expect better performance of our MT-based features in conjunction with MT engines geared to the text domains used in this shared task (listed in Table 1). Our engines are integrated into a scalable infrastructure deployed on an elastic compute cloud, allowing high throughput even with large translation memories to be cleaned. For POS tagging, we rely on Schmid’s (1995) TreeTagger and its readily available models1 for English, German, Italian, and Spanish.",
      "startOffset" : 129,
      "endOffset" : 656
    }, {
      "referenceID" : 3,
      "context" : "For machine translation, we use our in-house systems (Plitt and Masselot, 2010; Zhechev, 2014) based on the Moses SMT framework (Koehn et al., 2007). They are trained on translated software and user manuals from Autodesk products only and chosen for the sake of convenience; we would expect better performance of our MT-based features in conjunction with MT engines geared to the text domains used in this shared task (listed in Table 1). Our engines are integrated into a scalable infrastructure deployed on an elastic compute cloud, allowing high throughput even with large translation memories to be cleaned. For POS tagging, we rely on Schmid’s (1995) TreeTagger and its readily available models1 for English, German, Italian, and Spanish. To make POS tags comparable across these languages, they are mapped2 to the Universal Tagset proposed by Petrov et al. (2011). Lastly, we use the publicly available Xerox Language Identifier API3 for language detection.",
      "startOffset" : 129,
      "endOffset" : 870
    }, {
      "referenceID" : 5,
      "context" : "This pipeline is used to transform translation units into feature vectors and train classifiers using the scikitlearn framework (Pedregosa et al., 2011).",
      "startOffset" : 128,
      "endOffset" : 152
    }, {
      "referenceID" : 0,
      "context" : "Our feature extraction pipeline, including Barbu’s (2015) as well as our own features (see Section 3.",
      "startOffset" : 43,
      "endOffset" : 58
    }, {
      "referenceID" : 0,
      "context" : "74 Barbu (2015) SVM (linear kernel) .",
      "startOffset" : 3,
      "endOffset" : 16
    }, {
      "referenceID" : 0,
      "context" : "74 Barbu (2015) SVM (linear kernel) .74 .85 .78 .84 .85 .83 .83 .83 .79 Barbu (2015) Random Forests .",
      "startOffset" : 3,
      "endOffset" : 85
    }, {
      "referenceID" : 11,
      "context" : "While eliminating almost correct TUs might decrease rather than increase MT quality, filtering out incorrect segments can have a positive impact (Vogel, 2003).",
      "startOffset" : 145,
      "endOffset" : 158
    }, {
      "referenceID" : 0,
      "context" : "Prior to submission, we benchmarked our system against the two baselines provided by the organizers: a dummy classifier assigning random classes according to the overall class distribution in the training data (Baseline 1), and a classifier based on the Church-Gale algorithm as adapted by Barbu (2015) (Baseline 2).",
      "startOffset" : 290,
      "endOffset" : 303
    }, {
      "referenceID" : 0,
      "context" : "Prior to submission, we benchmarked our system against the two baselines provided by the organizers: a dummy classifier assigning random classes according to the overall class distribution in the training data (Baseline 1), and a classifier based on the Church-Gale algorithm as adapted by Barbu (2015) (Baseline 2). More importantly, however, we compared our system to Barbu’s (2015) approach, using the classification algorithms which reportedly worked best with the 17 features in his work.",
      "startOffset" : 290,
      "endOffset" : 385
    }, {
      "referenceID" : 0,
      "context" : "Again, we compared our system’s performance to Barbu’s (2015) method, using 2/3–1/3 splits of the training data (5fold cross-validation).",
      "startOffset" : 47,
      "endOffset" : 62
    } ],
    "year" : 2016,
    "abstractText" : "We describe a machine learning based method to identify incorrect entries in translation memories. It extends previous work by Barbu (2015) through incorporating recall-based machine translation and part-of-speech-tagging features. Our system ranked first in the Binary Classification (II) task for two out of three language pairs: English–Italian and English–Spanish.",
    "creator" : "LaTeX with hyperref package"
  }
}