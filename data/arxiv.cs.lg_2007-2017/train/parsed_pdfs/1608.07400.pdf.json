{
  "name" : "1608.07400.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Collaborative Filtering with Recurrent Neural Networks",
    "authors" : [ "Robin Devooght" ],
    "emails" : [ "robin.devooght@ulb.ac.be", "bersini@ulb.ac.be" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 8.\n07 40\n0v 1\n[ cs\n.I R\n] 2\n6 A\nug 2\n01 6\nWe show that collaborative filtering can be viewed as a sequence prediction problem, and that given this interpretation, recurrent neural networks offer very competitive approach. In particular we study how the long short-term memory (LSTM) can be applied to collaborative filtering, and how it compares to standard nearest neighbors and matrix factorization methods on movie recommendation. We show that the LSTM is competitive in all aspects, and largely outperforms other methods in terms of item coverage and short term predictions.\nKeywords. Collaborative filtering, recommendation systems, recurrent neural network, LSTM, deep learning."
    }, {
      "heading" : "1 Introduction",
      "text" : "Collaborative filtering is the problem of recommending items to users based on past interactions between users and items. The intuition is that if two users have had similar interactions in the past, they should look to each other for recommendations. The nearest neighbors approaches (KNN) are based on that idea, and offer multiple mechanisms to identify similar users and pool recommendations. Next to KNN, the other main approach for collaborative filtering is matrix factorization (MF), which frames recommendation as a dimensionality reduction problem. MF attempts to represents users and items as points in a feature space according to an optimization criterion based on the interactions between users and items (i.e. the users should be close to the items that they liked). Although useful, those methods are far from perfect and improvements are coming more and more slowly. Moreover, they are unadapted to capture the temporal aspects of recommendations, such as evolving taste or context-dependent interests. We explore a new approach to collaborative filtering, based on recurrent neural networks. Modern recurrent neural networks such as the LSTM are powerful tools for sequence prediction problems and are well-suited to capture the evolution of users taste. In order to apply them to recommendations, we need to reframe collaborative filtering as a sequence prediction problem.\nIn the following sections, we will describe collaborative filtering as a sequence prediction problem, then show how the LSTM can be trained on such a problem and compare it to traditional collaborative filtering approaches. We will then explore some of the many design choices of the LSTM."
    }, {
      "heading" : "2 Collaborative filtering as a se-",
      "text" : "quence prediction problem\nIn a typical top-N recommendation problem, we consider a user i at a time t, with St−i being the set of items that the user consumed before t and St+i the set of items that he consumed after t. The goal of the recommendation system is to predict the items in St+i as a function of St−i . In this setting, that we call static, the order in which items are consumed is irrelevant for the recommendation system.\nInstead, we suggest that the recommendations should be based not only on the set of items that the user consumed, but on the order in which he consumed them. The user is represented by its sequence of action: he consumed x1, then x2, then x3, and the goal is to predict his next actions (x4, x5, etc.) based on the beginning of the sequence.\nFraming collaborative as a sequence prediction problem naturally leads to a distinction between what we call short-term and long-term predictions. A short-term prediction aims to predict which item will the user consume next (i.e. right after the last one), while a long term prediction aims to predict which items will the user consume eventually. In the static setting, this distinction does not make sense because the order of items in St+i is ignored, and predictions in a static setting are equivalent to long term prediction. In sequence prediction however, the models are usually trained to produce short-term prediction.\nIt is hard to argue for which type of prediction is more interesting in recommender systems, and ultimately, this choice lies with the user, but it is useful to remember that a sequence prediction approach is better suited for shortterm predictions. Moreover, we will show in Section 3 that training for short-prediction is likely to improve the\ndiversity of recommendations. In the following experiments, we will report performances on both short-term and long-term predictions.\nThe basic motivation supporting this work is that, while neglected so far, the information contained in the sequence of action could be of great importance for producing better recommendations. For example, this sequence can reveal the evolution of a user’s taste. It might help to identify which items became irrelevant with regards to the current user’s interest, or which items make part of a vanishing interest. It might also help to identify which items are more influential in changing users taste.\nMethods based on sequence prediction should produce richer models. As a matter of fact, a static recommendation system models users and items as points in a feature space, frozen in time. It will recommend items that are close to the user in that specific poorly informative space. Given a sequence prediction approach, users are rather represented by trajectories in the feature space, entailing a more accurate prediction of the evolution of the users interest (because pursuing this same trajectory)."
    }, {
      "heading" : "3 Sequence prediction favors di-",
      "text" : "versity in recommendations\nAs said in the previous section, methods that ignore the sequence of events are trained to produce long-term predictions, while methods based on sequence prediction are usually trained to produce short-term predictions. The typical metrics to evaluate long term predictions are the precision and recall. If Pi are the predictions based on St−i , then the precision is |Pi∩St +\ni |/|Pi| and the recall is |Pi ∩ St + i |/|St +\ni | (we usually compute the precision and recall “at k”, where k|Pi|). In order to evaluate the quality of short-term prediction, we propose a simple metric that we will call the “sequence prediction success at k” (sps@k) : |Pi ∩ {xi}|, where xi is the first item of the sequence St+i . An interesting side-effect of training recommendation systems to optimize short-term rather than long term predictions, is that it increases the diversity of the recommendations. Although we do not have a formal proof, the reasoning is as follows: the correct short term predictions are a subset of the correct long term predictions; because of that, any given item will be a correct prediction for more users in terms of long term prediction than in terms of short term prediction; the result is that it takes fewer items to make correct long term predictions for a given percentage of users than it takes to make correct short term predictions.\nWe illustrate that with a simple experiment. Consider an oracle (i.e. a perfect recommendation system) that can only recommend items within the t most popular ones; Figure 1 shows how the rec@10, prec@10 and sps@10 increase as t increases on two real datasets. The rec@10 and prec@10 converge very fast, they reach 80%\nof their maximum value with a small fraction of the items, and each new item brings only a marginal improvement. The sps@10 on the other hand has a much slower convergence and requires therefore a higher diversity of recommendation to reach 80% of its maximum value; we therefore expect that optimizing a recommendation system for short term prediction will force it to produce more diverse recommendations."
    }, {
      "heading" : "4 Methods comparison",
      "text" : "Recurrent neural networks (RNNs) might be the most versatile methods of sequence prediction, and their recent success in speech recognition, translation and other domains [6, 17] make them good candidates for the recommendation problem. In this section we evaluate RNN on a problem of item recommendation for two movie recommendation datasets, and compare it to collaborative filtering methods (that makes no use of the sequence information)."
    }, {
      "heading" : "4.1 Datasets",
      "text" : "Our major constraint in finding datasets is the presence of information on the order of events (often in the form of a timestamp). This information is unfortunately missing in most collaborative filtering dataset, but is available in two well known datasets of movie recommendation:\n• Movielens 1M: a rather small version of the Movielens dataset, with 6040 users and 1,000,209 ratings over 3706 movies. This dataset has extra information about the users (age, sex, occupation) and about the movies (year, genre) that will be exploited in Section 5.\n• Netflix: A much larger dataset, with about 480k users and 100M ratings over 17770 movies.\nEach set has been splitted into training, validation and test subsets. These subsets where obtained by dividing the users into 3 groups: N randomly chosen users and all their ratings to constitute the test set, N others to constitute the validation set and all the remaining users for the training set. We used N = 500 for Movielens 1M and N = 1000 for Netflix.\nIn those datasets, any item can appear only once in the rating history of any user. We therefore helped all the methods by forcing them to recommend items that the user had not yet seen.\nAlthough the datasets have explicit feedback of users in the form of ratings, none of the methods presented here used the values of the rating to build their model or make predictions, they just used the fact that an item was rated or not by a user."
    }, {
      "heading" : "4.2 Recurrent neural networks",
      "text" : "RNNs are commonly used for language modeling, where they are trained to learn sequences of words [11]. We took a similar approach by considering each item as a word, the catalog of items as the full vocabulary, and the history of each user as a sample sequence. The RNN runs through the sequence of items consumed by a user, item by item. At each time step, the input is the one-hot encoding of the current item, and the output is a softmax layer with a neuron for each item in the catalog. The k items whose neurons are activated the most are used as the k recommendations.\nThe state-of-the-art in recurrent neural networks is what are called “gated” RNNs, where the internal state of the RNN is controled by one or more small neural networks called gates. The original gated RNN is the LSTM[9], but it has spawned multiple variants [3, 7].\nWe trained the RNN to minimize the categorical crossentropy loss, with the only correct item being the next item in the sequence. As said earlier, this approach trains the RNN to focus on short-term prediction so that we can unsurprisingly expect to have a high sps.\nFor the following experiments, we used a unidirectional single-layered LSTM, we tuned the number of hidden neurons and the learning mechanism over the validation set. The results shown here are obtained using Adagrad [4] with a learning rate of 0.1, 20 hidden neurons for the Movielens dataset and 100 for the Netflix dataset."
    }, {
      "heading" : "4.3 Competing methods",
      "text" : "We compare the RNN with two static methods of topN recommendation: one based on nearest neighbors and one on matrix factorization. We also compare the RNN with a simple Markov chain that can be seen as a baseline for the sequence prediction approach."
    }, {
      "heading" : "4.3.1 Markov chain",
      "text" : "In this simple method, the users’ behavior is modelled by a Markov chain whose states are the different items. The transition probabilities between items are inferred from the transition frequencies observed in the training set. At any time the state of a user corresponds to the last item that he consumed, and the recommendations for that user will be the k items with the highest transition probabilities from that state. In other words, if the last item consumed by a user is the item j, the k first recommendations of the Markov model will be the k items that followed most often the item j in the sequences coming from other users. This is equivalent to a bigram model in language modeling in which the words become the items."
    }, {
      "heading" : "4.3.2 User-based nearest neighbors",
      "text" : "User-based nearest neighbors or user KNN is one of the oldest method of collaborative filtering, yet still a strong baseline for top-N recommendation. A score sij is computed between a user i and an item j:\nsij = ∑\nu∈Nk(i)\nciu1(j ∈ Su) (1)\nWhere ciu is the similarity between users i and u, Nk(i) is the set k users closest to i according to the similarity measure c, and 1(j ∈ Su) is the indicator function that evaluates to 1 if item j belongs to the sequence of items of user u, or else evaluates to 0. We used the cosine similarity measure, which is usualy prefered for item recommendation:\nciu = |Si ∩ Su|/ √ |Si||Su| (2)\nThe size of the neighborhood (k) was optimized by means of a validation set."
    }, {
      "heading" : "4.3.3 BPR-MF",
      "text" : "BPR-MF is a state-of-the-art matrix factorization method for top-N recommendation devised by [14]. It\nis based on the Bayesian personalized ranking: an objective function similar to the AUC (area under the ROC curve) that can be trained trough stochastic gradient descent. We used the original implementation of BPR-MF, available in the MyMediaLite framework [5]. BPR-MF has many parameters; we selected the most important (number of features and number of iterations) on the basis of a validation set, and kept the default values of MyMediaLite for the others."
    }, {
      "heading" : "4.4 Metrics",
      "text" : "We compare the different methods through a range of metrics finely designed to capture various qualities of the recommendation systems.\n• sps. The Short-term Prediction Success captures the ability of the method to predict the next item. It is 1 if the next item is present in the recommendations, 0 else.\n• Recall. The usual metrics for top-N recommendation captures the ability of the method to do long term predictions.\n• User coverage. The fraction of users who received at least one correct recommendation. Average recall (and precision) hide the distribution of success among users. A high recall could still mean that many users do not receive any good recommendation. This metrics captures the generality of the method.\n• Item coverage. The number of distinct items that were correctly recommended. It captures the capacity of the method to make diverse, successful, recommendations.\nAll those metrics are computed “at 10”, i.e. in a setting where the recommendation systems produces ten recommendations for each user."
    }, {
      "heading" : "4.5 Testing procedure",
      "text" : "For each user of the test set, the method can base its recommendations on the first half of the user’s ratings and on the model previously built on the training set (or on the training set directly, in the case of the nearest neighbours methods). Those recommendations are evaluated against the second half of the user’s rating, using the metrics described in Section 4.4. The metrics are then averaged over all test users. The only method that needs the ratings of a user during the construction of the model is BPR-MF. For that reason, BPR-MF is trained on a larger training set than the other methods, that includes the first half of the ratings of each of the test users. This is an unfair but unfortunately unavoidable advantage for the BPR-MF method.\nSince BPR-MF and RNN produce stochastic models, table 1 gives the average and the standard deviation of the results over ten models."
    }, {
      "heading" : "4.6 Analysis",
      "text" : "The results are shown in Table 1. The methods using the sequence information are impressively much better than the others in terms of sps. It is worth underlying the quality of those results: given ten trials (i.e. ten recommendations), the RNN is able to predict the next movie seen by 33% of the users of Movielens and 40% of the users of Netflix, while methods not based on the sequence are below 15%. As predicted in Section 3, methods with the best sps also have a larger item coverage, which is an important, but often overlooked aspect of recommendations. In particular, the item coverage of the RNN is more than twice the one of the User KNN. As a global observation, the RNN proves to be a very promising method for all considered metrics. It dominates the other methods in every aspects on the Movielens dataset and is only beaten in terms of recall by BPRMF and User KNN on Netflix."
    }, {
      "heading" : "5 Variations of the recurrent neu-",
      "text" : "ral networks\nAs we apply RNN to a new problem, it is worth exploring how to adapt it to the specificities of the new problem. In this section we study technical details of the implementation such as the learning rate and the number of hidden neurons, then we present a modification of the loss function that leads to more diverse recommendations, and we study how the RNN can use extra information such as users’ age or ratings value to built better models."
    }, {
      "heading" : "5.1 Learning method",
      "text" : "Modern neural networks are rarely trained using the vanilla SGD approach: a set of mechanisms have been developped to speed up learning and schedule the decrease of the learning rate. Two mechanisms have been found to be generally useful:\n• Momentum: smooths the gradients variations over time in order to avoid “zig-zags” during learning[13].\n• Adaptive learning: decrease the learning rate for frequently updated parameter (Adagrad[4], rmsprop).\nSome methods, such as Adadelta[18] and Adam[10] combine both approaches. We observed that in this context, adaptive learning seems more important than momentum, with the methods Adagrad, rmsprop and Adam working particularly well. Adagrad is especially appealing because it requires to tune only one parameter: the initial learning rate,\nwhile rmsprop requires two, and Adam three. The Adagrad updates are computed in the following way:\nθt+1 = θt − η√\nGt + ǫ ⊙ gt (3)\nWhere θ is the vector of parameters, η is the initial learning rate, ǫ is a small value to avoid division by zero, gt is the gradient of the objective function with regards to the parameters at time t, and Gt = ∑t i=0 g 2 i is the sum of previous square gradients. All parameters start with the same learning rate, but during training the learning rate decreases faster for the parameters that are often associated with large gradients. Those parameters will therefore be fixed early during training, making it easier to learn the other parameters, which have more subtle influences on the objective function. Figure 2 shows the influence of the learning rate with the Adagrad method. Learning rates around 0.01 gave the best result, and similar observations where made on the Netflix dataset."
    }, {
      "heading" : "5.2 Influence of the cell size",
      "text" : "Another important parameter of the of the LSTM is the number of neurons in the cell. The LSTM will not be able too learn a rich enough model if the number of neurons is too small, but more neurons lead to a slower learning and\nit may increase the risk of over-fitting. A good first choice for the number of neurons in the LSTM cell is the number of feature you would use to solve the same problem with matrix factorization. Figure 3 shows the validation error during training for several cell size. We observe that with 10 neurons the model is severely limited, and the highest sps seems to be reached with around 100 neurons. However, we also observe a faster learning for the smaller cell of 20 neurons."
    }, {
      "heading" : "5.3 Architecture",
      "text" : "Although we used a vanilla LSTM in all our experiments, many other RNN architectures could have been used. In this section we very briefly explore some of the other possible types of RNNs:\n• Bidirectional LSTM: two LSTM are used in parallel, one reading the inputs in chronological order and the other in the reverse order. The output of both LSTM is fed into the last (softmax) output layer.\n• 2-layered LSTM: RNN can be stacked, the output of one LSTM feeding the next one. We tried the simplest version, with two layers of LSTM, the first reading the initial input, and the second reading the output of the previous layer and producing the predictions.\n• GRU: the gated recurrent unit is simpler than the LSTM, with fewer gates and fewer parameters to tune[3].\nFigure 4 compares the different architectures with the vanilla LSTM. We report the evolution of the validation error during training, both we regards to the number of epochs and to the training time because the choice of architecture has a significant impact on the speed at which the network is able to process a sequence. In particular, the GRU is faster than the LSTM-based RNNs, and the gain in speed does not seem to impact the quality of the prediction. On the long run however, the difference between GRU and LSTM is negligible. The bidirectional and the 2-layered LSTM slightly under-perform, but the\noverall effect of the choice of architecture appears to be small."
    }, {
      "heading" : "5.4 Diversity bias",
      "text" : "As expressed earlier, diversity is a key challenge of recommendation systems. The difficulty comes mainly from the fact that the distribution of popularity among items is usually very skewed, in the presence of very few popular items and much more rarer items. Naturally, any model trained to optimize the chance of correct recommendations will learn to often propose the most popular items. Unfortunately, most of the times, those popular items turn out to be trivial, useless recommendations. We present here a small modification of the objective function of the RNN, aimed to increase the diversity of the recommendation while not loosing too much precision. In Section 4 we used the categorical cross-entropy as the objective function of the RNN: L = − log(ocorrect), where ocorrect is the value of the output neuron corresponding to the correct item. We test here a slightly different objective function that lowers the error associated with mispredicting the most popular items. The underlying rational is that insisting on the rarest items should counteract the bias toward popular items caused by the skewed distribution of popularity. Our objective function is thus transformed as follows:\nLδ = − log(ocorrect)/eδpcorrect (4)\nWhere δ ∈ [0, inf) is the diversity bias parameter and pcorrect is a measure of popularity associated with the correct item. p could be for example the number of views or the number of purchases of the item. In our experiment, we constructed p by dividing the items into ten bins of logarithmic size, the smaller bin containing the most popular items and the largest bin containing the least popular (in terms of number of ratings). Then we set p = 1 for all the items in the largest bin, p = 2 for the items in the second largest, etc. When δ = 0, the objective function is reduced to the categorical cross-entropy, and increasing δ increases the bias towards infrequent items. Figure 5 shows that the diversity bias offers an easy way to trade precision for item coverage. For small values of δ (under 0.2) we can significantly increase the item coverage with a small loss in sps. With larger values of δ, the RNN keeps producing more diverse recommendations, but the precision becomes so low that the item coverage actually decreases (the item coverage is the number of distinct items correctly recommended)."
    }, {
      "heading" : "5.5 Extra features",
      "text" : "The input of the RNN used in Section 4 consisted only in the sequence of items (in the form of a one hot encoding of the last item at each time step). However, many datasets possess much richer information and those same\ninformation might improve the model discovered by the RNN. We distinguish three types of extra information: the ones that concern a user (age, sex, etc.), the ones that concern an item (category, price, etc.) and the ones that concern a specific user-item interaction (rating, review, etc.). Movielens 1M has some of each category:\n• users’ features: age (approximated by seven age ranges), sex, and occupation (chosen among twentyone options)\n• items’ features: year of release (we used only the decade) and genre (chosen among eighteen categories)\n• interactions’ features: rating value (ten possible values)\nWe represented each feature with a one hot encoding and added them to the normal input. In other words, the input of the RNN that uses the sequence plus, for instance, the users’ features, consists in 3736 neurons: 3706 for the one hot encoding of the movie, 7 for the age range, 2 for the sex and 21 for the occupation. Table 2 shows the influence of using the users’ features, the items’ features or the interactions’ features separately, followed at last by their combined effect. Interestingly, the gain is only significant when all the extra features are combined, and even then this gain remains small. This suggests that the sequence of actions already contains most of the information and those extra features are in a way or another already implicit in these sequences."
    }, {
      "heading" : "6 Related Work",
      "text" : "Deep learning techniques are getting more and more attention in the recommender system community, but we know of only two attempts to use recurrent neural network for collaborative filtering. Spotify might have been\nusing it as far back as 2014 [1] to build playlists. They however do not seem to be using gated RNN, and they are using a hierachical softmax output in order to deal with the very large number of items (they have much more songs than netflix or movielens have movies). More recently, [8] has applied gated RNN to session based collaborative filtering. Interestingly, they have used other objective functions than the categorical cross-entropy, namely the Bayesian personalized ranking (the same used by BPR-MF) and an other ranking loss called TOP1 that they devised for the task.\nSome earlier works have framed collaborative filtering as a sequence prediction problem and used simpler Markov chain methods to solve it. In the early 2000s, [19] used a simple Markov model and tested it for web-page recommendation. [12] adopted a similar approach, using sequential pattern mining. Both showed the superiority of methods based on sequence over nearest-neighbors approaches. In [16, 2], Brafman et al. defended the view of recommendation systems as a Markov decision process, and although the predictive model was not their main focus, they did present in [16] a Markov chain approach, improved by some heuristics such as skipping and clustering.\nMore recently, [15] introduced a rather fair approach to build personalized Markov chain, exploiting matrix factorization to fight the sparsity problem. Their method is mainly designed for the next basket recommendation problem, but it would be of great interest to adapt it for a more general recommendation problem."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We explored the use of recurrent neural network, and in particular the LSTM, for the collaborative filtering problem. Using RNNs requires the re-frame collaborative filtering as a sequence prediction problem, and it could lead to richer models, taking the evolution of users’ taste into account. Our experiments showed that the LSTM produces very good results on the Movielens and Netflix datasets, and is especially good in terms of short term prediction and item coverage.\nBetter performance still could be achieve by designing RNNs specifically for the collaborative filtering task, especially at the level of the objective function, but the fact\nthat standard LSTM works already so well is yet another proof of its ability to tackle general problems."
    }, {
      "heading" : "Acknowledgments",
      "text" : "R. Devooght is supported by the Belgian Fonds pour la Recherche dans l’Industrie et l’Agriculture (FRIA, 1.E041.14)."
    } ],
    "references" : [ {
      "title" : "Recurrent neural networks for collaborative filtering, 2014. [Online; accessed 20-Mai- 2016",
      "author" : [ "E. Bernhardsson" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2016
    }, {
      "title" : "Recommendation as a stochastic sequential decision problem",
      "author" : [ "R.I. Brafman", "D. Heckerman", "G. Shani" ],
      "venue" : "In ICAPS,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2003
    }, {
      "title" : "On the properties of neural machine translation: Encoder-decoder approaches",
      "author" : [ "K. Cho", "B. Van Merriënboer", "D. Bahdanau", "Y. Bengio" ],
      "venue" : "arXiv preprint arXiv:1409.1259,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2014
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "J. Duchi", "E. Hazan", "Y. Singer" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2011
    }, {
      "title" : "MyMediaLite: A free recommender system library",
      "author" : [ "Z. Gantner", "S. Rendle", "C. Freudenthaler", "L. Schmidt-Thieme" ],
      "venue" : "In Proceedings of the 5th ACM Conference on Recommender Systems (RecSys",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2011
    }, {
      "title" : "Speech recognition with deep recurrent neural networks",
      "author" : [ "A. Graves", "A.-r. Mohamed", "G. Hinton" ],
      "venue" : "In Acoustics, Speech and Signal Processing (ICASSP),",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2013
    }, {
      "title" : "Lstm: A search space odyssey",
      "author" : [ "K. Greff", "R.K. Srivastava", "J. Koutník", "B.R. Steunebrink", "J. Schmidhuber" ],
      "venue" : "arXiv preprint arXiv:1503.04069,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2015
    }, {
      "title" : "Session-based recommendations with recurrent neural networks",
      "author" : [ "B. Hidasi", "A. Karatzoglou", "L. Baltrunas", "D. Tikk" ],
      "venue" : "CoRR, abs/1511.06939,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2015
    }, {
      "title" : "Long short-term memory",
      "author" : [ "S. Hochreiter", "J. Schmidhuber" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1997
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "D. Kingma", "J. Ba" ],
      "venue" : "arXiv preprint arXiv:1412.6980,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2014
    }, {
      "title" : "Recurrent neural network based language model",
      "author" : [ "T. Mikolov", "M. Karafiát", "L. Burget", "J. Cernockỳ", "S. Khudanpur" ],
      "venue" : "In INTERSPEECH,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2010
    }, {
      "title" : "Using sequential and non-sequential patterns in predictive web usage mining tasks",
      "author" : [ "B. Mobasher", "H. Dai", "T. Luo", "M. Nakagawa" ],
      "venue" : "In Data Mining,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2002
    }, {
      "title" : "On the momentum term in gradient descent learning algorithms",
      "author" : [ "N. Qian" ],
      "venue" : "Neural networks,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1999
    }, {
      "title" : "Bpr: Bayesian personalized ranking from implicit feedback",
      "author" : [ "S. Rendle", "C. Freudenthaler", "Z. Gantner", "L. Schmidt-Thieme" ],
      "venue" : "In Proceedings of the twenty-fifth conference on uncertainty in artificial intelligence,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2009
    }, {
      "title" : "Factorizing personalized markov chains for next-basket recommendation",
      "author" : [ "S. Rendle", "C. Freudenthaler", "L. Schmidt- Thieme" ],
      "venue" : "In Proceedings of the 19th international conference on World wide web,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2010
    }, {
      "title" : "An mdp-based recommender system",
      "author" : [ "G. Shani", "R.I. Brafman", "D. Heckerman" ],
      "venue" : "In Proceedings of the Eighteenth conference on Uncertainty in artificial intelligence,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2002
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "I. Sutskever", "O. Vinyals", "Q.V. Le" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2014
    }, {
      "title" : "Adadelta: an adaptive learning rate method",
      "author" : [ "M.D. Zeiler" ],
      "venue" : "arXiv preprint arXiv:1212.5701,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2012
    }, {
      "title" : "Using temporal data for making recommendations",
      "author" : [ "A. Zimdars", "D.M. Chickering", "C. Meek" ],
      "venue" : "In Proceedings of the Seventeenth conference on Uncertainty in artificial intelligence,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2001
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "Recurrent neural networks (RNNs) might be the most versatile methods of sequence prediction, and their recent success in speech recognition, translation and other domains [6, 17] make them good candidates for the recommendation problem.",
      "startOffset" : 171,
      "endOffset" : 178
    }, {
      "referenceID" : 16,
      "context" : "Recurrent neural networks (RNNs) might be the most versatile methods of sequence prediction, and their recent success in speech recognition, translation and other domains [6, 17] make them good candidates for the recommendation problem.",
      "startOffset" : 171,
      "endOffset" : 178
    }, {
      "referenceID" : 10,
      "context" : "RNNs are commonly used for language modeling, where they are trained to learn sequences of words [11].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 8,
      "context" : "The original gated RNN is the LSTM[9], but it has spawned multiple variants [3, 7].",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 2,
      "context" : "The original gated RNN is the LSTM[9], but it has spawned multiple variants [3, 7].",
      "startOffset" : 76,
      "endOffset" : 82
    }, {
      "referenceID" : 6,
      "context" : "The original gated RNN is the LSTM[9], but it has spawned multiple variants [3, 7].",
      "startOffset" : 76,
      "endOffset" : 82
    }, {
      "referenceID" : 3,
      "context" : "The results shown here are obtained using Adagrad [4] with a learning rate of 0.",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 13,
      "context" : "BPR-MF is a state-of-the-art matrix factorization method for top-N recommendation devised by [14].",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 4,
      "context" : "We used the original implementation of BPR-MF, available in the MyMediaLite framework [5].",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 12,
      "context" : "• Momentum: smooths the gradients variations over time in order to avoid “zig-zags” during learning[13].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 3,
      "context" : "• Adaptive learning: decrease the learning rate for frequently updated parameter (Adagrad[4], rmsprop).",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 17,
      "context" : "Some methods, such as Adadelta[18] and Adam[10] combine both approaches.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 9,
      "context" : "Some methods, such as Adadelta[18] and Adam[10] combine both approaches.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 2,
      "context" : "• GRU: the gated recurrent unit is simpler than the LSTM, with fewer gates and fewer parameters to tune[3].",
      "startOffset" : 103,
      "endOffset" : 106
    }, {
      "referenceID" : 0,
      "context" : "using it as far back as 2014 [1] to build playlists.",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 7,
      "context" : "More recently, [8] has applied gated RNN to session based collaborative filtering.",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 18,
      "context" : "In the early 2000s, [19] used a simple Markov model and tested it for web-page recommendation.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 11,
      "context" : "[12] adopted a similar approach, using sequential pattern mining.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "In [16, 2], Brafman et al.",
      "startOffset" : 3,
      "endOffset" : 10
    }, {
      "referenceID" : 1,
      "context" : "In [16, 2], Brafman et al.",
      "startOffset" : 3,
      "endOffset" : 10
    }, {
      "referenceID" : 15,
      "context" : "defended the view of recommendation systems as a Markov decision process, and although the predictive model was not their main focus, they did present in [16] a Markov chain approach, improved by some heuristics such as skipping and clustering.",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 14,
      "context" : "More recently, [15] introduced a rather fair approach to build personalized Markov chain, exploiting matrix factorization to fight the sparsity problem.",
      "startOffset" : 15,
      "endOffset" : 19
    } ],
    "year" : 2016,
    "abstractText" : "We show that collaborative filtering can be viewed as a sequence prediction problem, and that given this interpretation, recurrent neural networks offer very competitive approach. In particular we study how the long short-term memory (LSTM) can be applied to collaborative filtering, and how it compares to standard nearest neighbors and matrix factorization methods on movie recommendation. We show that the LSTM is competitive in all aspects, and largely outperforms other methods in terms of item coverage and short term predictions.",
    "creator" : "gnuplot 4.6 patchlevel 4"
  }
}