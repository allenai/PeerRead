{
  "name" : "1609.08663.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Safoora Yousefi", "Congzheng Song", "Nelson Nauata", "Lee Cooper" ],
    "emails" : [ "lee.cooper}@emory.edu" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Genomics provide a window into the complex molecular workings of disease. In the treatment of cancer, genomic analysis of a tissue biopsy can reveal specific molecular vulnerabilities that can be matched to targeted therapies, or to prognosticate the future behavior of a patient’s disease and expected survival in order to better inform clinical interventions including surgery and radiation therapy. Although genomic analysis generates rich high-dimensional signals that contain hundreds to hundreds-of-thousands of variables, typically only several variables are used for prognostication for any given cancer type. Typically, these variables are used to assign patients into discrete disease classes or ”subtypes” that associate with response to specific therapies, or with varying degrees of disease aggressiveness. Learning the underlying latent prognostic variables from high-dimensional genomic profiles can extract additional prognostic value from unused variables, and is critical in realizing the promise of genomic medicine. This problem presents significant challenges, ranging from the familiar ”large p small N”, to how to adapt developments in the machine learning domain to the analysis of time-to-event survival data.\nIn this abstract we present an investigation in building survival prediction neural networks to learn representations from genomic data for survival prediction. We use backpropagation to train neural networks to maximize the Cox proportional hazards likelihood of time-to-event data, and apply these predictive models to molecular profiles of brain tumor patients from The Cancer Genome Atlas where survival ranges from 6 months to 10+ years. We compare our methods to state-of-theart survival analysis algorithms based on elastic-net (linear combination of L1 and L2) regularization of Cox hazard models and random forest based methods, and demonstrate improvements in survival prediction accuracy for neural network approaches."
    }, {
      "heading" : "2 BACKGROUND AND RELATED WORK",
      "text" : ""
    }, {
      "heading" : "2.1 HAZARD MODELS AND LIKELIHOOD FUNCTIONS",
      "text" : "Survival analysis involves predicting the time to some event of interest, which in cancer is often death or progression of disease. It differs from ordinary regression due to incomplete followup, where a death or relapse event is not observed at or before the final encounter with the patient. These censored observations provide critical information, and often represent an important population of long-term survivors or treatment responders that are very important to incorporate into the model. The most commonly used regression approach to survival analysis is the Cox proportional hazards\nar X\niv :1\n60 9.\n08 66\n3v 1\n[ cs\n.N E\n] 2\n7 Se\np 20\n16\nmodel proposed by Cox (1972). At time t, the hazard for a sample with covariates x is given by the following hazard function:\nλ(t|x) = λ0(t)eβx, (1) where λ0(t) is baseline hazard. The hazard function is an exponential linear function of the covariates x and model coefficients β, with the effect of any covariate x(i) assumed to be the same over time. Since the evaluation criteria of the models in this paper is based on ranking predicted survival times, the baseline hazard is left unspecified and we maximize the partial likelihood function during training:\nl(β,X) = − ∑ i∈U ( Xiβ − log ∑ j⊂Ri eXjβ )\n(2)\nwhere U is the set of all uncensored patients, and Ri is the set of patients whose time of death or last follow-up is later than time of death of i.\nWe measured model performance using concordance index (CI) that captures the rank correlation of predicted and actual survival. Denoting the ith patient with Xi and the set of all patients with X , where ti represents either the time of death or the time of last follow-up of the ith patient, CI was calculated in the following way:\nCI(β,X) = ∑ P I(i,j) |P | (3)\nI(i, j) = { 1, if Riskj > Riski and tj > ti 0, otherwise\n(4)\nWhere P is the set of orderable pairs. A pair of samples (Xi, Xj) is orderable if either the event is observed for both Xi and Xj , or Xj is censored and tj > ti. Intuitively, CI measures the pairwise agreement of the prognostic scoresRiski,Riskj predicted by the model and the actual time of death for all orderable pairs. Attempts have been made to propose differentiable versions of CI based on sigmoid and exponential functions and optimize it directly. But recent studies show that optimizing the cox partial likelihood is equivalent to optimizing CI (Steck et al., 2008)."
    }, {
      "heading" : "2.2 RELATED WORKS",
      "text" : "Regularization techniques have been proposed for feature selection in survival analysis of high dimensional data (Zou & Hastie, 2005). Efforts have been made to introduce successful machine learning algorithms such as random forests to survival analysis (Ishwaran et al., 2008). Deep learning techniques have been employed for cancer diagnosis using genomic data and medical images, such as Fakoor et al. (2013) and Esteva et al. To the best of our knowledge, representation learning techniques have not been applied to survival prediction from genomic data, and the previous work investigating neural networks for survival analysis dealt with low dimensional data and different cost functions (Lisboa et al., 2003)."
    }, {
      "heading" : "3 SURVIVAL PREDICTION NEURAL NETWORK",
      "text" : ""
    }, {
      "heading" : "3.1 PRETRAINING AND FINE-TUNING",
      "text" : "In this work we trained an autoencoder to represent genomic data and fine tune this representation using partial log Cox likelihood. In training, we employ stacked denoising autoencoders proposed in Vincent et al. (2008). We train the auto-encoders using 183-dimensional genomic features, then we add a risk prediction output layer as shown in Figure 1-a.\nWe use survival times and censoring status to calculate the Cox partial log likelihood given by equation 2 and differentiate it with respect to X:\n∂l(β,X)\n∂Xi = ciβ − ∑ j∈U :i∈Rj βeXiβ∑ k∈Rj e Xkβ (5)\nwhere ci is 1 if sample i is not censored, and is 0 otherwise, and β denotes the parameters of the risk prediction layer. This derivative is then back-propagated through the network to fine tune the learned representation specifically for the task of survival analysis."
    }, {
      "heading" : "3.2 MODEL SELECTION",
      "text" : "The training of a neural network involves many hyperparameters: type of nonlinearities used, number of layers, number of hidden units in each layer, learning rates for pretraining and fine tuning and regularization parameters. Since this is the first work where deep neural networks are used to address survival analysis, we could not look at existing literature for a conventional choice of hyperparameters. Unlike areas such as image classification, no rule of thumb has been developed for setting hyper-parameters in survival analysis. Therefore we employed bayesian optimization (MartinezCantin, 2014) with Gaussian prior to decrease the number of objective function evaluations needed to reach a decent choice of hyperparameters. More shallow networks demonstrate superior performance over deeper architectures in our experiments. This could be justified considering the small number of training samples (628) and the scarcity of labels within the available samples. Our average choice of configuration is 2 fully connected layers of 250 hidden units each. On average, we use a learning rate .001 for pre-training and .0009 for fine-tuning."
    }, {
      "heading" : "3.3 EVALUATION",
      "text" : "Due to the small size of available training data, performance of the model might considerably depend on the partitioning of the data into testing and training. To mitigate this, we randomly sampled from the data set 10 times without replacement to have 10 permutations of the same data set. Then in each of the 10 sets, we used the first %70 of the data for training, half of the remaining %30 of data for model selection and the other half for model assessment. We performed training, model selection and testing on these 10 permutation datasets separately. The reported CI in Figure1-b is averaged over these 10 experiments to represent the generalization error of the model. The exact same setting was used for hyper-parameter tuning and assessment for competing methods. We picked the learning rate and elastic-net mixture coefficient for regularized Cox regression (Hastie & Qian (2014)) based on performance on the same validation sets we used for the neural networks. We tuned number of trees, leaf size, and number of split points for random survival forest in the same fashion. Our experiments reveal that Random Survival Forests do not adapt well to high dimensionality and are markedly outperformed by survival neural networks (See Figure 1-b). Neural networks also achieve %5 absolute improvement over regularized Cox regression with ReLU activation and %3 with sigmoid activation."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "This work was supported by US Public Health Service National Institutes of Health (NIH) grants K22LM011576-03 and U24CA194362-01."
    } ],
    "references" : [ {
      "title" : "Regression models and life tables (with discussion)",
      "author" : [ "David R Cox" ],
      "venue" : "Journal of the Royal Statistical Society,",
      "citeRegEx" : "Cox.,? \\Q1972\\E",
      "shortCiteRegEx" : "Cox.",
      "year" : 1972
    }, {
      "title" : "Using deep learning to enhance cancer diagnosis and classification",
      "author" : [ "Rasool Fakoor", "Faisal Ladhak", "Azade Nazi", "Manfred Huber" ],
      "venue" : "In The 30th International Conference on Machine Learning (ICML 2013),WHEALTH workshop,",
      "citeRegEx" : "Fakoor et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Fakoor et al\\.",
      "year" : 2013
    }, {
      "title" : "Glmnet vignette",
      "author" : [ "Trevor Hastie", "Junyang Qian" ],
      "venue" : "Technical report, Technical report,",
      "citeRegEx" : "Hastie and Qian.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hastie and Qian.",
      "year" : 2014
    }, {
      "title" : "Random survival forests",
      "author" : [ "Hemant Ishwaran", "Udaya B Kogalur", "Eugene H Blackstone", "Michael S Lauer" ],
      "venue" : "The annals of applied statistics,",
      "citeRegEx" : "Ishwaran et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Ishwaran et al\\.",
      "year" : 2008
    }, {
      "title" : "A bayesian neural network approach for modelling censored data with an application to prognosis after surgery for breast cancer",
      "author" : [ "Paulo JG Lisboa", "H Wong", "P Harris", "Ric Swindell" ],
      "venue" : "Artificial intelligence in medicine,",
      "citeRegEx" : "Lisboa et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Lisboa et al\\.",
      "year" : 2003
    }, {
      "title" : "Bayesopt: A bayesian optimization library for nonlinear optimization, experimental design and bandits",
      "author" : [ "Ruben Martinez-Cantin" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Martinez.Cantin.,? \\Q2014\\E",
      "shortCiteRegEx" : "Martinez.Cantin.",
      "year" : 2014
    }, {
      "title" : "On ranking in survival analysis: Bounds on the concordance index",
      "author" : [ "Harald Steck", "Balaji Krishnapuram", "Cary Dehing-oberije", "Philippe Lambin", "Vikas C Raykar" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Steck et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Steck et al\\.",
      "year" : 2008
    }, {
      "title" : "Extracting and composing robust features with denoising autoencoders",
      "author" : [ "Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio", "Pierre-Antoine Manzagol" ],
      "venue" : "In Proceedings of the 25th international conference on Machine learning,",
      "citeRegEx" : "Vincent et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Vincent et al\\.",
      "year" : 2008
    }, {
      "title" : "Regularization and variable selection via the elastic net",
      "author" : [ "Hui Zou", "Trevor Hastie" ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
      "citeRegEx" : "Zou and Hastie.,? \\Q2005\\E",
      "shortCiteRegEx" : "Zou and Hastie.",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "model proposed by Cox (1972). At time t, the hazard for a sample with covariates x is given by the following hazard function: λ(t|x) = λ0(t)e, (1) where λ0(t) is baseline hazard.",
      "startOffset" : 18,
      "endOffset" : 29
    }, {
      "referenceID" : 6,
      "context" : "But recent studies show that optimizing the cox partial likelihood is equivalent to optimizing CI (Steck et al., 2008).",
      "startOffset" : 98,
      "endOffset" : 118
    }, {
      "referenceID" : 3,
      "context" : "Efforts have been made to introduce successful machine learning algorithms such as random forests to survival analysis (Ishwaran et al., 2008).",
      "startOffset" : 119,
      "endOffset" : 142
    }, {
      "referenceID" : 4,
      "context" : "To the best of our knowledge, representation learning techniques have not been applied to survival prediction from genomic data, and the previous work investigating neural networks for survival analysis dealt with low dimensional data and different cost functions (Lisboa et al., 2003).",
      "startOffset" : 264,
      "endOffset" : 285
    }, {
      "referenceID" : 1,
      "context" : "Deep learning techniques have been employed for cancer diagnosis using genomic data and medical images, such as Fakoor et al. (2013) and Esteva et al.",
      "startOffset" : 112,
      "endOffset" : 133
    }, {
      "referenceID" : 0,
      "context" : "1 PRETRAINING AND FINE-TUNING In this work we trained an autoencoder to represent genomic data and fine tune this representation using partial log Cox likelihood. In training, we employ stacked denoising autoencoders proposed in Vincent et al. (2008). We train the auto-encoders using 183-dimensional genomic features, then we add a risk prediction output layer as shown in Figure 1-a.",
      "startOffset" : 147,
      "endOffset" : 251
    }, {
      "referenceID" : 0,
      "context" : "We picked the learning rate and elastic-net mixture coefficient for regularized Cox regression (Hastie & Qian (2014)) based on performance on the same validation sets we used for the neural networks.",
      "startOffset" : 80,
      "endOffset" : 117
    } ],
    "year" : 2016,
    "abstractText" : "Genomics are rapidly transforming medical practice and basic biomedical research, providing insights into disease mechanisms and improving therapeutic strategies, particularly in cancer. The ability to predict the future course of a patient’s disease from high-dimensional genomic profiling will be essential in realizing the promise of genomic medicine, but presents significant challenges for state-of-the-art survival analysis methods. In this abstract we present an investigation in learning genomic representations with neural networks to predict patient survival in cancer. We demonstrate the advantages of this approach over existing survival analysis methods using brain tumor data.",
    "creator" : "LaTeX with hyperref package"
  }
}