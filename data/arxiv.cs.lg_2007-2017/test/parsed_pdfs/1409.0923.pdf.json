{
  "name" : "1409.0923.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Ahmad Basheer Hassanat" ],
    "emails" : [ "ahmad.hassanat@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "A similarity measure is a function that gives a non-negative number to each pair of vectors to define a notion of likeness (Hagedoorn 2000). Such a measure is vital for a large number of applications and research areas including – but not limited to – pattern matching algorithms, artificial intelligence (Hagedoorn 2000), machine learning (He, Chen and Chen 2013), regression analysis (Wessela and Schork 2006) and data mining (Geng and Hamilton 2006), in addition to other research areas such as social science, economy, null theory testing etc. Similarity measures are needed in almost all knowledge disciplines. A large number of similarity measures are proposed in the literature, perhaps the most famous and well known being the Euclidean distance stated by Euclid two thousand years ago. Over the last century great efforts have been made to find new metrics and similarity measures to satisfy the needs of different applications. New similarity measures are needed in particular for use in distance learning (Yang 2006), where classifiers such as the k-nearest neighbor (KNN) are heavily depended upon for choosing the best distance. Optimizing the distance metric is valuablein several computer vision tasks, such as object detection, content-based image retrieval, image segmentation and classification. Cha has categorized similarity measures into eight families (Cha, 2007) and (Cha, 2008): 1- The Minkowski family, which includes Euclidean distance, City block or Manhattan distance and Chebyshev distance. 2- The absolute differencefamily, which includes Sørensen, Gower, Soergel, Kulczynski, Canberra and Lorentzian distances.\n3- The intersection family, which includes Intersection, Czekanowski, Motyka, Kulczynski, Ruzicka and Tanimoto distances. 4- The inner product family, which includes Inner Product, Harmonic Mean, Cosine, KumarHassebrook, Jaccard and Dice similarity measures. 5- The Fidelity family, which includes Fidelity similarity measure, Bhattacharyya, Hellinger, Matusita and Squared-chord distances. 6- Squared L2 family, which includes Squared Euclidean, Pearson, Neyman, Squared, Probabilistic Symmetric, Divergence, Clark and Additive Symmetric distances. 7- Shannon’s entropy family, which includes Kullback–Leibler, Jeffreys, K divergence, Topsøe, Jensen-Shannon and Jensen difference distances. 8- Combinations (of the previous measure) family, which includes Taneja and Kumar-Johnson distances. A much larger number of distances and similarity measures are illustrated in the work of (Deza and Deza 2009) showing the applications of each similarity measure. None of these functions measure similarity perfectly for all problems, as each deals with a specific data context and assumptions. According to the no free lunch theorem (Duda, Hart and Stork 2001), no distance function performs better than the other; the use of a particular distance is problem and data dependent. The similarity measures that are used the most are Euclidean (ED) and Manhattan distances (MD)– both assume the same weight to all directions. In addition, the difference between vectors at each dimension might approach infinity to imply dissimilarity (Bharkad and Kokare 2011), therefore if there is abnormality in one value in any direction this will be reflected in the final result of the distance; for example, if we have two vectors of size 100,\nJournal of American Science\nV1=(1,2,3,...,99,100) and V2=(1,2,3,...,99,0), then ED(V1,V2) and MD(V1,V2) is 100. Perhaps both vectors are equal, but the last value is changed for some reason; noise, for instance, which shows that such distances are sensitive to large differences in any direction, and this allows some dimensions to dominate the distance even in the absence of noise. To solve the previous problem, researchers usually opt for either normalization or standardization of the data. However, both have their own weaknesses. If there are outliers, most of the data will be forced to scale down using normalization; on the other hand, standardization degrades data and does not provide bounded data(Saitta 2007). The so-called \"Wave-Hedges distance shown in Eq(1) solves part of the previous problem. This measure has been applied to compressed image retrieval (Hatzigiorgaki and Skodras 2003) probability density function similarity content based video retrieval (Patel and Meshram 2012), Image Retrieval (Khapli and Bhalchandra 2011), (Braveen and Dhavachelvan 2009) classification (Giusti and Batista 2013) retrieval (Jasiewicz, Netzel and Stepinski 2013) image fidelity (Macklem 2002), Histogram Distance Measures (Cha, 2008) and finger print recognition (Bharkad and Kokare 2011). Interestingly, the source of the \"Wave-Hedges\" metric has not been correctly cited, and some of the previously mentioned resources allude to it incorrectly as The source of this metric eludes the author, despite best efforts otherwise. Even the name of the distance \" Hedges\" is questioned, and therefore will not be used in the rest of this paper. Rather, we will refer to this distance as Eq(1) for the rest of this paper. , ∑ 1"
    }, {
      "heading" : "2. Material and Methods",
      "text" : "There are 3 problems associated with Eq(1), those are: 1- It cannot deal correctly with points having 0 values. For example, if Ai is equal to 0, then the distance between 0 and any other positive non value is 1, no matter how large or small that value is. 2- The distance between 0 and 0 is undefined. 3- In addition, the distance is not well defined on points with negative values. For example, the distance between -1 and -2 is equal to distance between 1 and 2 is 0.5. This paper presents a new similarity measure based on Eq(1) to solve all the above mentioned problems. The proposed similarity\n2014;10(8) http://www.jofamericanscience.org\n\"\n, (Cha, 2007),\n, time series , landscape\n,\n(Hedges 1976).\nWave-\n, , (1)\n-zero\n-1, while the\n-\nfunction between two points in two vectors is written as: , 1 , , 1 , | , , | ,\nAnd for all the vectors dimensions we get:D\"#$\"$%&' A, B where A and B are both vectors with size m. A Bi are real numbers. As can be seen from Eq(2 and 3), the proposed measure is bounded by [0,1[. It reaches 1 when the maximum v assuming the minimum is finite, or when the minimum value approaches minus infinity assuming the maximum is finite. This is shown by Figure 1 and the following equations. lim -.,/. →∞1D A2, B2 3 lim 2 -.,\nThis means that the more a pair of values is\nsimilar, the nearest to zero the measure will be, and the more a pair of values is one the measure will be. In other words, no matter what the difference between two values is, the distance will be in the range of 0 to 1. By studying some properties of the proposed measure, such as non symmetry and Triangle inequality we may state the following Lemmas. Lemma 1: the proposed similarity measure is non negative function. Proof: for any two positive numbers, the proposed distance uses the first part of Eq(1) so we need to prove that 1 , , 4 0 This is equivalent to , 6 , , 4 0 we get\nSince we assume two positive numbers then max , 4 0 , by dividing positive value | | by another positive value we get positive value, which is greater than 0.\n, 9:; , 4 0 | | , 9:; , < 0 = (2) ∑ 1D A2, B2 3 2 (3)\ni and\nalue approaches infinity\n/. →6∞1D A2, B2 3 1 (4)\nn, where n belongs\ndissimilar, the nearest to\n-negativity, equivalently,\n-\n| 6 | , 4 0\nIf the minimum number is negative, the piecewise function uses the other formula which deals with negative numbers by adding the absolute value of the minimum number to both numerator and denominator. So we need to prove that\n1 − ( , ) | ( , )|\n( , ) | ( , )| ≥ 0\nWe know that 9: ;( , ) + |9: ;( , )| = 0 so we get\n( , ) | ( , )|\n( , ) | ( , )| ≥ 0\nWe get 9?@ ( , ) + |9: ;( , )| ≥ 0 Now if 9? @( , ) ≥ 0 the previous inequality is true. Else, we know that\n|9: ;( , )| ≥ |9?@( , )| By subtracting a smaller value (−9?@ ( , ) ) from a larger value |9: ;( , )| we get positive value, and this prove the previous inequality. Lemma 2: the proposed similarity measure is an equivalence function. Proof: a function D(A,B)=0 if and only if A coincides with B. If A coincides with B then the min(A,B)=max(A,B). By dividing a value by itself we get 1. By subtracting this 1 from 1 as in our formula we get 0.We need to prove that:\n1 − ( , )\n( , ) = 0 (5)\nBy moving the negative part to the right we get:\n1 = ( , )\n( , ) (6)\nIf Ai coincides with Bi then: 9: n( , ) = max( , ) (7) By replacing the min function with the max function we get:\n1 = ( , )\n( , ) (8)\nAnd this gives: 1 = 1 □ (9) Another approach for proving equivalently is to prove that D(A,A)=0, since we are considering the distance of a vector from itself. The min(Ai,Ai)=max(Ai,Ai)), by replacing the min with the max in Eq(5) we end up with 1-1=0. Lemma 3: the proposed similarity function is symmetric. Proof: a function D is symmetric if and only if D(A,B)= D(B,A) for all points A , B. so the proposed similarity measure should satisfy:\n1 − ( , )\n( , ) = 1 −\n( , ) ( , ) (10)\nHere, we rely on the fact that the minimum\nand maximum functions are symmetric: min(A,B)=min(B,A) and max(A,B)=max(B,A). By substitution in the previous equation we get:\n1 − ( , )\n( , ) = 1 −\n( , ) ( , ) (11)\nMoving the negative part to the right side of Eq(11) we get:\n1 = 1 − ( , )\n( , ) +\n( , ) ( , ) =1 (12)\nBoth sides of the equation are equal, and then D is symmetric. Lemma 4: the proposed similarity function satisfies the Triangle inequality. Proof: a similarity function D satisfies the Triangle inequality if and only if D(A,C) ≤ D(A,B) + D(B,C) for all points A , B and C. so the proposed similarity measure should satisfy: 1 − ( ,B )\n( ,B ) ≤ 1 −\n( , ) ( , ) + 1 −\n( ,B ) ( ,B ) (13)\nFor simplicity, assume that ac=1+min(A,C), ab=1+min(A,B), bc=1+min(B,C), AC=1+max(A,C), AB=1+max(A,B) and BC=1+max(B,C) then Eq(13) becomes:\n1 − D\nB ≤ 1 −\nE\n+ 1 −\nED\nB (14)\nMoving the negative values to the opposite side of the equation we have:\n1 + E + ED\nB ≤ 2 +\nD B (15)\nMoving the 1 to the right we get: E\n+\nED\nB ≤ 1 +\nD B (16)\nNotice that ac<= AC, because the minimum of two values is less than or equal to their maximum, therefore ac/AC<=1. Adding this inequality to the Eq(16) we get:\nE\n+\nED\nB +\nD B ≤ 2 + D B (17)\nBy subtracting ac/AC from both sides we get: E\n+\nED\nB ≤ 2 (18)\nNotice that ab<=AB for the same previous reason, therefore:\nE\n≤ 1 (19)\nAnd bc<=BC, therefore: ED\nB ≤ 1 (20)\nCombining the last two inequalities gives and proves Eq(18). □ According to the previous discussion we can state the following theorem. Theorem 1: the proposed similarity function is a metric. Proof: a distance function should satisfy the following properties to be called a metric (Peeters, et al. 2008) and (Cha & Sriharib, 2002): 1- Non-negativity: D(A,B)>=0 for all points A , B. 2- Equivalently: D(A,B)=0 if and only if A coincides with B. 3- Symmetry: D(A,B)= D(B,A) for all points A , B. 4- Triangle inequality: D(A,C) <= D(A,B) + D(B,C) for all points A , B and C.\nThe previously proved Lemmas 1, 2, 3 and 4, show that the proposed distance function satisfies all the previous properties and therefore is a metric. 3. Results For further study of the proposed metric, we opted for the KNN classifier, which naturally depends on similarity measures. We used only one neighbor 1-NN to ensure that any enhancement in the performance came from the similarity measure rather than the number of neighbors taken. For the experiments, we chose 19 different data sets from the UCI Machine Learning Repository (Bache and Lichman 2013). Table 1 depicts the data sets used.\nEach data set is divided into two data sets– one for training and the other for testing. 30% of the data set is used for testing, and the rest of the data is for training. Each time the 1-NN is used to classify the test samples using ED, MD, Eq(1) and the proposed distance shown in Eq(3). The 30% of data which is used as a test sample is chosen randomly, and each experiment on each dataset using a different distance is repeated 10 times to get random examples for testing and\ntraining. Table 2 shows the results of the experiments. The accuracy is averaged over the 10 runs."
    }, {
      "heading" : "4. Discussions",
      "text" : "As can be noticed from Table 2, the proposed distance achieved good results, outperforming the other distances in 10 datasets, and the overall average accuracy is the best among all the tested distances. The lowest accuracies were recorded by Eq(1) as expected. This happens because of the problems mentioned earlier associated with Eq(1). For example, the accuracies of Eq(1) on Vowel, Segmen and Waveform40 are 7.30%, 14.20% and 38.10% respectively. The reason for this low performance is because of the data type used on those datasets that contain real numbers (positive and negative values, see Table 1), while Eq(1) is not well defined in negative values. Eq(1) also achieved low performance on other datasets, such as Glass, Monkey1and Heart, with accuracies of 35.80%, 47.50% and 49.80% respectively. This is justified by the number of zero\nvalues in those datasets. The data type of Monkey1is binary; this gives the zero values about a 50% chance of appearing in such a dataset. Glass and Heart also have a large number of zero values, This increases the probability of getting the distance from 0 to any number, which in this case will be 1 all the time, in addition to the \"division by zero\" problem which is also not defined in Eq(1). On the contrary, we noted that Eq(1) performed very well on the Wine and Balance datasets. This is because all values there are nonnegatives and non-zeros. It also performed well in Sonar and German datasets, where all values are positive with some zeros. These results justify the invention of the proposed metric Eq(3), which is inspired by Eq(1) and the modification made to fit all data types. Interestingly, the Manhattan distance outperformed Euclidean distance in most datasets. This is because the difference between the paired values is squared in the ED, which emphasizes/reinforces the difference and allows one direction (feature) to dominate the result of the distance. This complies with some other researchers' results such as (Bonet, et al. 2008) and (Al Gindi, Attiatalla and Sami 2014), and contradicts others such as (Liu, et al. 2008). This reminds us again of the no-free-lunch theorem, i.e. there is no distance measure better than the other (including the proposed one) – it mainly depends on the problem and the data used. This work proposes a new similarity measure, which we have proved mathematically as a metric function. This metric was compared to other well known metrics such as ED and MD in terms of accuracy. Our results based on mathematical proofs and experiments on real data, show that the proposed metric is a promising distance measure, not only for the KNN classification but also for other problems and domains. This complies with what other researchers have shown: that a well-defined distance metric may notably benefit KNN classifier performance compared to Euclidean distance (Yang 2006), (He, et al. 2004) and (Hastie and Tibshirani 1996). Future work includes applying the proposed metric to other related problems such as k-mean clustering and hierarchal clustering to investigate its superiority in solving such problems."
    }, {
      "heading" : "Acknowledgements:",
      "text" : "The author would like to acknowledge both Professor Christopher Farah of the University of Maine, and Fausto Galetto, ex-professor of Industrial Quality Management, Politecnico di Torino for their\nvaluable discussions and comments that enriched this work."
    }, {
      "heading" : "Corresponding Author:",
      "text" : "Dr. Ahmad Basheer Hassanat IT Department Mutah University, Mutah – Karak, Jordan, E-mail: ahmad.hassanat@gmail.com"
    } ],
    "references" : [ {
      "title" : "A Comparative Study for Comparing Two Feature Extraction Methods and Two Classifiers in Classification of Earlystage Lung Cancer Diagnosis of chest x-ray images.",
      "author" : [ "Al Gindi", "Amal M", "Tawfik A Attiatalla", "Moustafa M Sami" ],
      "venue" : "J Am Sci 10,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2014
    }, {
      "title" : "Performance Evaluation of Distance Metrics: Application to Fingerprint Recognition.",
      "author" : [ "Bharkad", "Sangita D", "Manesh Kokare" ],
      "venue" : "International Journal of Pattern Recognition and Artificial Intelligence 25,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2011
    }, {
      "title" : "Comparing Distance Measures with Visual Methods.",
      "author" : [ "Bonet", "Isis", "Abdel Rodríguez", "Ricardo Grau", "Maria M García", "Yvan Saez", "Ann Nowé" ],
      "venue" : "7th Mexican International Conference on Artificial Intelligence,. Atizapán de Zaragoza, Mexico: Springer,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2008
    }, {
      "title" : "Evaluation of Content Based Image Retrieval Systems Based on Color Feature.",
      "author" : [ "M Braveen", "P Dhavachelvan" ],
      "venue" : "International Journal of Recent Trends in Engineering",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2009
    }, {
      "title" : "Comprehensive Survey on Distance/Similarity Measures between Probability Density Functions.",
      "author" : [ "Cha", "S-H" ],
      "venue" : "International Journal of Mathematical Models And Methods In Applied Sciences 1, no",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2007
    }, {
      "title" : "Taxonomy of Nominal Type Histogram Distance Measures.",
      "author" : [ "Cha", "S-H" ],
      "venue" : "AMERICAN CONFERENCE ON APPLIED MATHEMATICS (MATH '08)",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2008
    }, {
      "title" : "Sriharib. “On measuring the distance between histograms.",
      "author" : [ "Cha", "S-H", "S.N" ],
      "venue" : "Pattern Recognition",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2002
    }, {
      "title" : "Encyclopedia of Distances",
      "author" : [ "M Deza", "E Deza" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2009
    }, {
      "title" : "Interestingness Measures for Data Mining: A Survey.",
      "author" : [ "Geng", "Liqiang", "Howard J Hamilton" ],
      "venue" : "ACM Computing Surveys 38,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2006
    }, {
      "title" : "An Empirical Comparison of Dissimilarity Measures for Time Series Classification.",
      "author" : [ "Giusti", "Rafael", "GE Batista" ],
      "venue" : "Brazilian Conference on Intelligent Systems (BRACIS). Fortaleza: IEEE,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2013
    }, {
      "title" : "Pattern Matching using Similarity Measures",
      "author" : [ "M. Hagedoorn" ],
      "venue" : "PhD. Thesis, Universiteit Utrecht,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2000
    }, {
      "title" : "Discriminant adaptive nearest neighbor classification.",
      "author" : [ "T Hastie", "R Tibshirani" ],
      "venue" : "IEEE Pattern Analysis and Machine Intelligence (IEEE) 18,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1996
    }, {
      "title" : "Compressed domain image retrieval: a comparative study of similarity metrics.",
      "author" : [ "M Hatzigiorgaki", "A.N Skodras" ],
      "venue" : "Proceedings of SPIE 5150,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2003
    }, {
      "title" : "Manifold ranking based image retrieval.",
      "author" : [ "J He", "M Li", "H.-J Zhang", "H Tong", "C Zhang" ],
      "venue" : "Proceedings of the 12th annual ACM international conference on Multimedia",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2004
    }, {
      "title" : "Kernel Density Metric Learning",
      "author" : [ "He", "Yujie", "Wenlin Chen", "Yixin Chen" ],
      "venue" : "Technical report, St. Louis: Washington University,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2013
    }, {
      "title" : "An empirical modication to linear wave theory.",
      "author" : [ "T.S. Hedges" ],
      "venue" : "Proc. Inst. Civ. Eng",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1976
    }, {
      "title" : "Content-based landscape retrieval using geomorphons.",
      "author" : [ "Jasiewicz", "Jaroslaw", "Pawel Netzel", "Tomasz F Stepinski" ],
      "venue" : "Geomorphometry. Nanjing,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2013
    }, {
      "title" : "Comparison of Similarity Metrics for Thumbnail Based Image Retrieval.",
      "author" : [ "Khapli", "Vidya R", "Anjali S Bhalchandra" ],
      "venue" : "JOURNAL OF COMPUTER SCIENCE AND ENGINEERING 5,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2011
    }, {
      "title" : "Comparing dissimilarity measures for contentbased image retrieval.",
      "author" : [ "H Liu", "D Song", "S Ruger", "R Hu", "V Uren" ],
      "venue" : "4th Asia Information Retrieval Symposium, AIRS. Harbin, China: Springer,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2008
    }, {
      "title" : "Multidimensional Modelling Of Image Fidelity Measures",
      "author" : [ "Macklem", "Mason" ],
      "venue" : "M.Sc. thesis, Burnaby, BC, Canada: Simon Fraser University,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2002
    }, {
      "title" : "Content Based Video Retrieval Systems.",
      "author" : [ "BV Patel", "BB Meshram" ],
      "venue" : "International Journal of UbiComp (IJU) 3, no",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2012
    }, {
      "title" : "Analysis of distance/similarity measures for diffusion tensor imaging.",
      "author" : [ "Peeters", "T.H.J.M", "P.R Rodrigues", "A Vilanova", "B.M Romeny" ],
      "venue" : "Visualization and Processing of Tensor Fields: Advances and Perspectives. Berlin: Springer,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2008
    }, {
      "title" : "Standardization vs. normalization.",
      "author" : [ "Saitta", "Sandro" ],
      "venue" : "Data Mining Research",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2007
    }, {
      "title" : "Generalized Genomic Distance–Based Regression Methodology for Multilocus Association Analysis.",
      "author" : [ "Wessela", "Jennifer", "Nicholas J Schork" ],
      "venue" : "AJHG (Elsevier)",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2006
    }, {
      "title" : "Distance metric learning: A comprehensive survey",
      "author" : [ "L. Yang" ],
      "venue" : "Technical report,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2006
    } ],
    "referenceMentions" : [ ],
    "year" : 2014,
    "abstractText" : "This paper presents a new similarity measure to be used for general tasks including supervised learning, which is represented by the K-nearest neighbor classifier (KNN). The proposed similarity measure is invariant to large differences in some dimensions in the feature space. The proposed metric is proved mathematically to be a metric. To test its viability for different applications, the KNN used the proposed metric for classifying test examples chosen from a number of real datasets. Compared to some other well known metrics, the experimental results show that the proposed metric is a promising distance measure for the KNN classifier with strong potential for a wide range of applications. [Hassanat B. A. Dimensionality Invariant Similarity Measure. J Am Sci 2014;10(8):221-226]. (ISSN: 15451003). http://www.jofamericanscience.org. 31",
    "creator" : "PDFCreator Version 1.7.3"
  }
}