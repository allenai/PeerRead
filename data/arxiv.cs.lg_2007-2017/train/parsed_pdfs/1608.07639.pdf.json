{
  "name" : "1608.07639.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning to generalize to new compositions in image understanding",
    "authors" : [ "Yuval Atzmon", "Jonathan Berant", "Vahid Kezami", "Amir Globerson", "Gal Chechik" ],
    "emails" : [ "yuval.atzmon@biu.ac.il" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Training models that describe images with natural language embodies fundamental problems in both language and image understanding. It allows to ground the meaning of language in visual data, and\nuse language compositionality to understand rich visual scenes. Recently, deep neural networks have been successfully used for this task (MS-COCO, 2015). While the results were both inspiring and impressive, it became clear in the aftermath of analyzing the results, that current approaches suffer from two fundamental issues. First, generalization was poor for images describing scenarios not seen at training time. Second, evaluating descriptions was challenging, because strong language models can generate sensible descriptions that are missing es-\nar X\niv :1\n60 8.\n07 63\n9v 1\n[ cs\n.C V\n] 2\n7 A\nug 2\nsential components in the image. However, a quantitative evaluation of these two problems is still missing.\nIn this paper, we propose to address these issues by focusing on structured representations for image descriptions. As a first step, we use simple structured representations consisting of subject-relationobject (SRO) triplets (Farhadi et al., 2010). By reducing full sentences to an SRO representation, we focus on the composition of entities in an image. This has two main advantages. First, it allows to quantify the quality of model predictions directly using the accuracy of SRO predictions. Second, it allows to partition the data such that the model is tested only on new combinations, which are not included in the training set. This allows to evaluate compositional generalization to unseen scenarios, as illustrated in Figure 1.\nWe partition the MS-COCO dataset using a compositional split and compare a state-of-the-art recurrent attention model, Show-Attend-and-Tell, (Xu et al., 2015) to a structured prediction model built on top of a deep CNN. The recurrent model achieves similar performance on the traditional MS-COCO split. However, we find that it only achieves ∼ 14% the accuracy of the structured model when tested on the new partitioning that requires generalization to new combinations."
    }, {
      "heading" : "2 Generalizing to novel compositions",
      "text" : "Our key observation is that one should separate two kinds of generalization that are of interest when generating image descriptions. The first, generalizing to new images of the same class, is routinely being evaluated, including in the current data split of the MS-COCO challenge (Lin et al., 2014). The second type, which we focus on, is concerned with generalizing to new scenarios, akin to transfer or zeroshot learning (Fei-Fei et al., 2006), where learning is extended to semantically-similar classes. Importantly, this generalization is the crux of learning in complex scenes, since both language and visual scenes are compositional, resulting in an exponentially large set of possible descriptions. Hence, a key goal of learning to describe images would be to properly quantify generalization to new combinations of known entities and relations.\nTo tease out compositional generalization from\nstandard within-class generalization, we propose to construct a test set that only contains scenarios that never appeared in the training data.\nIn practice, we first map image descriptions to short open-IE style phrases of the form subjectrelation-object (termed SRO triplets). We then partition the examples such that the test and training sets share no common images or SRO triplets (see Figure 1). This compositional split is a natural way to test generalization in short utterances of natural language, since a small training set could be used to train for the large set of possible combinations at test time. While some overlap between scenarios in the training and test set can still occur due to synonymy, we hypothesize that this partitioning leads to a much stronger need for generalization."
    }, {
      "heading" : "3 A Structured Prediction Model",
      "text" : "To jointly predict an SRO triplet, we train a structured-prediction model on top of a deep convolutional network. First, an image is analyzed to produce candidate bounding boxes (Erhan et al., 2014) with their labels (Szegedy et al., 2015). Similar to Xu et al. (2015), the classifier was trained on a large dataset without fine-tuning on the current data.\nFor the structured model on top of the deep network, we used structured SVM (SSVM) (Tsochantaridis et al., 2005), minimizing the hinge loss between the predicted and ground-truth SRO triplets. Specifically, our model learns a score function f(s, r, o) on SRO triplets, decomposed as:\nf(s, r, o) = wSfS(s) + wOfO(o) + wRfR(r) +\nwSRfSR(s, r) + wROfRO(r, o),\nwhere wS , wO, wR, wSR, wRO are scalar weights learned by the algorithm. Here, fS(s) is a score assigned to the subject s, fO(o) is a score assigned to the object, fR(r) is a score assigned to the relation, fSR(s, r) is the binary feature over the subject and relation and similarly for fRO(r, o). For details of the model see Appendix A. To get a better understanding of the signals that are useful for the SRO prediction we experimented with multiple variants for the model potentials, for details see section 4.4.3"
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 The Data",
      "text" : "We evaluated image captioning on the MS-COCO data (Lin et al., 2014), currently the standard benchmark for evaluating image captioning models (328K images, ≤ 5 textual descriptions per image). We parsed MS-COCO descriptions into SRO triplets by first constructing dependency parse trees for each description (Andor et al., 2016), and then using manually-constructed patterns to extract triplets from each description. Finally, each word was stemmed. Removing descriptions without SROs (due to noun phrases, rare prepositions, or parsing errors), yielded 444K unique (image, SRO) pairs 1.\nAnalyzing structured phrases and images naturally involves grounding entities to specific image locations. Datasets like Visual-Genome (Krishna et al., 2016) and MS-COCO provide human-marked bounding boxes for many entities. Here, with the goal of being able to generalize to new entities and larger datasets, we instead inferred bounding boxes using a pre-trained deep-network localizer (Erhan et al., 2014). We limited nouns to a vocabulary from the 750 most frequent nouns, selecting the 300 entities that were localizable. and the vocabulary of relations to the top 50 relations, yielding 136K SRO triplets.\nThe vocabulary of the visual entity recognition used by the localizer does not fully overlap the the vocabulary of captions. For instance, the term “cow” may appear in the captions, while the terms {“ox”, “bull” and “calf”} may obtain high scores by the localizer. To match the two vocabularies we followed the procedure of Zitnick et al. (2013), see Appendix B for details. This mapping was used to select images whose predicted entities matched entities in the captions. When an image had several bounding boxes for the same label, we selected the one with the highest score. We also removed duplicate triplets per image, and triplets where the subject and object have the same bounding box. After keeping only images with bounding boxes for both subject and object we were left with 21,213 (image, SRO) pairs with 14,577 unique images .\nThis dataset was split in two ways: by intersecting\n1The templates and SRO triplets are available online at http://chechiklab.biu.ac.il/˜ yuvval/CompCRF\nwith the COCO benchmark split, and in a compositional way as described in Section 2."
    }, {
      "heading" : "4.2 Compared Methods",
      "text" : "We compared the following methods and baselines:\n1. SSVM/Conv Our model described in Sec. 3.\n2. Show-Attend-and-Tell (SA&T). A stateof-the-art RNN attention model for caption generation (Xu et al., 2015). We re-trained the decoder layers to predict SRO triplets with soft-attention. Hyper-parameters were tuned to maximize accuracy on an evaluation set, learning rate in (0.1, 0.05, 10−1, 10−3) and weight decay in (0, 10−8, 10−7, . . . , 10−2). Importantly, we also controlled for model capacity by tuning the embedding dimensionality (100, 200, 400, . . . , 1600 and the default 512) and the LSTM dimensionality (26, 27, . . . , 211) See Section 4.4. The remaining parameters were set as in the implementation provided by Xu et al. (2015).\n3. Stochastic conditional (SC). Draw R based on the training distribution, then draw S and O based on the training distribution ptrain(S|R), ptrain(O|R). This baseline is designed to capture the gain that can be attributed to bigram statistics.\n4. Most frequent triplet (MF). Predict an SRO consisting of the most frequent subject, most frequent relation, and most frequent object, based on the training set distribution. By construction, by the way the compositional split is constructed, the most frequent full SRO triplet in the training set can not appear in the test set."
    }, {
      "heading" : "4.3 Evaluation procedure",
      "text" : "We test all candidate pairs of bounding boxes (BB) for an image. For each BB pair, all candidate SRO triplets are ranked by their scores and compared against the set of ground-truth SRO triplets to compute precision@k for that image. Images may have more than one ground-truth SRO since they are associated with up to 5 descriptions. For image captioning, BLEU score is a common metric. Here, SROaccuracy is equivalent to BLEU-3, and single-term\naccuracy is equivalent to BLEU-1. We found computing BLEU between a description and its SRO to be too noisy.\nOur evaluation metric does not handle semantic smearing, namely, the case where an image can be described in several ways, all semantically adequate, but using different words and hence counted as errors. This issue is often addressed by representing words in continuous semantic spaces. For keeping this paper focused, we leave this outside of current evaluations\nWe experimented with two cross-validation procedures. First, COCO split, we used the train-test split provided by ms-coco, restricted to the set of images with SROs (COCO split). Second, Compositional split, was applied to unique SRO triplets to create a (80%/20%) 5 fold cross validation split. Any object or subject that did not appear in the train set, were moved from the test to the training set with all their triplets (since otherwise they cannot be evaluated). When an object or a subject class appeared only on the test set, then its triplets were moved to the train set. Subject or object appearing less than 5 times were removed from training set. The same (random) set of images was used across all approaches. The fraction of images sometimes deviates from (80%/20%) since some triplets have more images than others."
    }, {
      "heading" : "4.4 Results",
      "text" : ""
    }, {
      "heading" : "4.4.1 Compositional vs. within-class generalization",
      "text" : "Figure 2 and Table 1 show average precision@k across images, comparing SSVM to SA&T for both their test and training performance. In the top panel, both methods are trained and tested on the MSCOCO split. The SSVM/Conv model (blue) wins with precision of p@1 = 10.6% and the SA&T model (green) achieves p@1 = 9.4%. Test precision of the baselines was p@1 = 0.028% for SC. The most frequent S, R and O in the dataset were man, with and table, but the triplet (man with table) did not appear at all in the data, yielding 0% MF accuracy.\nThe effect is noticeable for the compositional split (bottom panel). Here, the SSVM/conv model transfers well to new combinations (compare training\np@1 = 8.3% and test p@1 = 6% ± 0.7%). Importantly, SA&T dramatically fails on new combinations, with a large generalization gap as apparent by the difference between precision on the training set (p@1 = 15%) and the test set only (p@1 = 0.85%± 0.2%). Test precision of the baselines was p@1 = 0.014% for SC, and 0% for MF."
    }, {
      "heading" : "4.4.2 Model complexity",
      "text" : "Generalization gap is often due to over-fitting, which can be controlled by reducing model capacity. We therefore tested SA&T with different capacities, varying the number of parameters (word dimensionality and LSTM hidden state dimensionality). As expected, training error decreased with the number of parameters. Importantly, test error decreased up to some point and then started rising\ndue to over-fitting. For the MS-COCO split, the SA&T best test error was better than the SSVM model, but for the compositional split it was significantly worse. In other words, A wide range of LSTM parameters still does not generalize well to the compositional split. Importantly, the number of examples in our experiments is well within the range of dataset sizes that SA&T was originally used in (Flickr8k, Flickr30k, COCO). At the same time the SSVM model is limited to bigram potentials, and as such unable to memorize SRO triplets, which the LSTM model may do. We conclude that merely reducing the capacity of the SA&T model was not sufficiently effective to control overfitting for the compositional case."
    }, {
      "heading" : "4.4.3 Comparing SSVM models",
      "text" : "To get a better understanding of the signals that are useful for the SRO prediction, we compared multiple variants of the SSVM model, each using different features as the R-node potential inputs, for details on the potentials see Appendix A.\n1. SSVM R-subject+object: The R node potential takes the object (O) category and subject (S) category, each is represented as a sparse ”one-hot” vector.\n2. SSVM R-object: The R node potential takes only the object (O) category, represented as a sparse ”one-hot” vector.\n3. SSVM R-subject: Same for the subject (S), again represented as a sparse ”one-hot” vector.\n4. SSVM spatial: The R node potential inputs include only spatial features.\n5. SSVM R-spatial+object: Inputs include both the spatial features and the object category represented as a one-hot vector.\n6. SSVM no relation features: The R node potential takes no input features, and is only based on the labels frequencies of R in the training set.\nTable 1 compares the performance of these models. The best performance is achieved when only taking the predicted labels of the object and subject\nas input features for the R node potential. These results suggest that the information in the spatial features is small compared to information in the labels predicted from the pixels."
    }, {
      "heading" : "4.4.4 Manual evaluation",
      "text" : "Since images can be described in myriad ways, we manually sampled 100 random predictions of the SSVM model to assess the true model accuracy. For every SRO prediction we answered two questions: (a) Does this SRO exist in the image (b) Is this a reasonable SRO description for the image. In 32% of the cases, SSVM produced an SRO that exists in the image, and 23% of the cases it was a reasonable description of the image."
    }, {
      "heading" : "5 Related Work",
      "text" : "Automatic description of images was developed by several groups (Xu et al., 2015; Karpathy and FeiFei, 2015; Mao et al., 2014; Kiros et al., 2014; Donahue et al., 2015; Vinyals et al., 2015; Venugopalan et al., 2014; Chen and Zitnick, 2014; Fang et al., 2015), and was also applied to parts of images (Johnson et al., 2015a; Krishna et al., 2016).\nCompositional aspects of language and images have been recently explored by (Andreas et al., 2015), who approached a visual QA task by breaking questions into substructures, and re-using modular networks. (Johnson et al., 2015b) combined subjects, objects and relationships in a graph structure for image retrieval. (Kulkarni et al., 2011) learned spatial relations for generating descriptions based on a template. (Zitnick et al., 2013) modelled synthetic scenes generated using CRF. The dataset of (Yatskar et al., 2016) has combinations of entities modelled with CRF. (Farhadi et al., 2010) developed ways to match sentences and images, through a space of meaning parametrized by subject-verbobject triplets which our structured model is closely related to. Very recently, (Lu et al., 2016) trained a model that leverages language priors from semantic embeddings to predict subject-relation-object tuples. The performance of their model on the unseencompositions subset in their test set, exhibits a very large generalization gap. Finally, generalization to new objects has often been achieved by “smearing” to semantically-related entities (Frome et al., 2013;\nAndreas et al., 2015; Xian et al., 2016), but this is outside the scope of this paper."
    }, {
      "heading" : "6 Summary",
      "text" : "This paper has two main contributions. First, we highlight the role of generalization to new combinations of known objects in vision-to-language problems, and propose an experimental framework to measure such compositional generalization. Second, we find that existing state-of-the-art image captioning models generalize poorly to new combinations compared to a structured-prediction model. In future work, we plan to extend our approach to full captions and handle deeper semantic structures, including modifiers, adjectives and more."
    }, {
      "heading" : "Appendix A: A structured-SVM model",
      "text" : "Our model learns a score function f(s, r, o) on SRO triplets, decomposed as:\nwSfS(s) + wOfO(o) + wRfR(r) +\nwSRfSR(s, r) + wROfRO(r, o),\nwhere wS , wO, wR, wSR, wRO are scalar weights learned by the algorithm.\nSubject node potential fS(s). We learned a sparse linear transformation matrix from the localizer vocabulary to the caption entities vocabulary, bases on empirical joint probability on training data. For example, fS(”cow”) was learned to be a weighted combination of the likelihood scores that the localizer gives to the classes {“ox”, “bull”, “calf”}.\nObject node potential fO(o). The fO(o) potential is defined similarly to fS(s). The relation node potential fR(r). The relation node was trained in a separate stage using the same train-test folds, as follows. A multiclass SVM is trained to predict R from features of the subject and object bounding boxes. At inference time, fR(r) is set as the score that the SVM assigns to relation r in the given image. For input features on some experiments (Section 4.4.3), we used the subject or object one-hot-vector or both. Each one-hot-vector is 300 features. For spatial features we use the following:\n• The position, dimension and log dimension of the two boxes (height, width, x, y). • The distance and log distance of a vector con-\nnecting the center of the subject box with that of the object. • The angle of a vector connecting the center of\nthe subject box with the object box, represented as a x,y pair normalized to length 1. • Aspect ratio combinations of box dimensions,\nincluding hS/wS , hS/hO and similar ratios. • The square root of box areas, and the ratio and\nlog-ratio of square root box areas. • The area of intersection and the intersection\nover union. • The square root relative overlap of the subject-\nobject areas (intersect(SO)/area( O)) 1 2 . Simi-\nlarly for object-subject. • Binary conditions, including\n– Relative overlap (SO) < 0.25 – Relative overlap (OS) < 0.25 – Relative overlap (OS) > 0.85\n– xS < xO – yS < yO – (yS < yO) and (xS < xO) – (yS < yO) and not(xS < xO)\nThe spatial features were then normalized to zero mean and unit variance. The pairwise feature fSR(s, r). This potential was set as the bigram probability of the combination (s, r), as estimated from the training data, and similarly for fRO(r, o)."
    }, {
      "heading" : "Appendix B: matching visual entities to caption terms",
      "text" : "When creating the dataset, we selected those images where the visual entities can be mapped to terms in the captions. Since the vocabulary of the visual entity recognition (used by the localizer) differs from the vocabulary of captions, we estimated a mapping from the locaizer vocabulary to the caption terms following the procedure of Zitnick et al. (2013).\nSpecifically, (1) We computed PMI between the labels predicted by the localizer for the bounding boxes (BBLs) and the nouns in the SRO. (2) We considered the top-5 matches for each S/O vocabulary word, and manually pruned outliers (for instance, the term bed had high MI with cat detections). (3) We removed a data sample if the S/O caption terms did not match any of the BBLs. This PMI step results in having 300 entities.\nThis transformation was only used for selecting the subset of the data that contains the set of entities in the S/O vocabulary."
    } ],
    "references" : [ {
      "title" : "Deep compositional question answering with neural module networks. arXiv preprint arXiv:1511.02799",
      "author" : [ "Marcus Rohrbach", "Trevor Darrell", "Dan Klein" ],
      "venue" : null,
      "citeRegEx" : "Andreas et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Andreas et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning a recurrent visual representation for image caption generation",
      "author" : [ "Chen", "Zitnick2014] Xinlei Chen", "Lawrence Zitnick" ],
      "venue" : "arXiv preprint arXiv:1411.5654",
      "citeRegEx" : "Chen et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2014
    }, {
      "title" : "Long-term recurrent convolutional networks for visual recognition and description",
      "author" : [ "Lisa Anne Hendricks", "Sergio Guadarrama", "Marcus Rohrbach", "Subhashini Venugopalan", "Kate Saenko", "Trevor Darrell" ],
      "venue" : null,
      "citeRegEx" : "Donahue et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Donahue et al\\.",
      "year" : 2015
    }, {
      "title" : "Scalable object detection using deep neural networks",
      "author" : [ "Erhan et al.2014] Dumitru Erhan", "Christian Szegedy", "Alexander Toshev", "Dragomir Anguelov" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Erhan et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Erhan et al\\.",
      "year" : 2014
    }, {
      "title" : "From captions to visual concepts and back",
      "author" : [ "Fang et al.2015] Hao Fang", "Saurabh Gupta", "Forrest Iandola", "Rupesh K Srivastava", "Li Deng", "Piotr Dollár", "Jianfeng Gao", "Xiaodong He", "Margaret Mitchell", "John C Platt" ],
      "venue" : "Proceedings of the IEEE Conference",
      "citeRegEx" : "Fang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Fang et al\\.",
      "year" : 2015
    }, {
      "title" : "Every picture tells a story: Generating sentences from images",
      "author" : [ "Farhadi et al.2010] Ali Farhadi", "Mohsen Hejrati", "Mohammad Amin Sadeghi", "Peter Young", "Cyrus Rashtchian", "Julia Hockenmaier", "David Forsyth" ],
      "venue" : null,
      "citeRegEx" : "Farhadi et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Farhadi et al\\.",
      "year" : 2010
    }, {
      "title" : "One-shot learning of object categories",
      "author" : [ "Fei-Fei et al.2006] Li Fei-Fei", "Rob Fergus", "Pietro Perona" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions",
      "citeRegEx" : "Fei.Fei et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Fei.Fei et al\\.",
      "year" : 2006
    }, {
      "title" : "Devise: A deep visual-semantic embedding model",
      "author" : [ "Frome et al.2013] Andrea Frome", "Greg S Corrado", "Jon Shlens", "Samy Bengio", "Jeff Dean", "Tomas Mikolov" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Frome et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Frome et al\\.",
      "year" : 2013
    }, {
      "title" : "2015a. Densecap: Fully convolutional localization networks for dense captioning",
      "author" : [ "Andrej Karpathy", "Li Fei-Fei" ],
      "venue" : "arXiv preprint arXiv:1511.07571",
      "citeRegEx" : "Johnson et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2015
    }, {
      "title" : "2015b. Image retrieval using scene graphs",
      "author" : [ "Ranjay Krishna", "Michael Stark", "Li-Jia Li", "David A Shamma", "Michael S Bernstein", "Li Fei-Fei" ],
      "venue" : "In Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Johnson et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep visual-semantic alignments for generating image descriptions",
      "author" : [ "Karpathy", "Fei-Fei2015] Andrej Karpathy", "Li FeiFei" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Karpathy et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Karpathy et al\\.",
      "year" : 2015
    }, {
      "title" : "Unifying visual-semantic embeddings with multimodal neural language models. arXiv preprint arXiv:1411.2539",
      "author" : [ "Kiros et al.2014] Ryan Kiros", "Ruslan Salakhutdinov", "Richard S Zemel" ],
      "venue" : null,
      "citeRegEx" : "Kiros et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kiros et al\\.",
      "year" : 2014
    }, {
      "title" : "Visual genome: Connecting language and vision using crowdsourced dense image",
      "author" : [ "Yuke Zhu", "Oliver Groth", "Justin Johnson", "Kenji Hata", "Joshua Kravitz", "Stephanie Chen", "Yannis Kalantidis", "Li-Jia Li", "David A Shamma" ],
      "venue" : null,
      "citeRegEx" : "Krishna et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Krishna et al\\.",
      "year" : 2016
    }, {
      "title" : "Baby talk: Understanding and generating image descriptions",
      "author" : [ "Visruth Premraj", "Sagnik Dhar", "Siming Li", "Yejin Choi", "Alexander C. Berg", "Tamara L. Berg" ],
      "venue" : "In Proceedings of the 24th CVPR",
      "citeRegEx" : "Kulkarni et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Kulkarni et al\\.",
      "year" : 2011
    }, {
      "title" : "Microsoft coco: Common objects in context",
      "author" : [ "Lin et al.2014] Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Dollár", "Lawrence Zitnick" ],
      "venue" : "In Computer Vision–ECCV",
      "citeRegEx" : "Lin et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2014
    }, {
      "title" : "Visual relationship detection with language priors",
      "author" : [ "Lu et al.2016] Cewu Lu", "Ranjay Krishna", "Michael Bernstein", "Li Fei-Fei" ],
      "venue" : "In European Conference on Computer Vision",
      "citeRegEx" : "Lu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2016
    }, {
      "title" : "Explain images with multimodal recurrent neural networks. arXiv preprint arXiv:1410.1090",
      "author" : [ "Mao et al.2014] Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang", "Alan L Yuille" ],
      "venue" : null,
      "citeRegEx" : "Mao et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mao et al\\.",
      "year" : 2014
    }, {
      "title" : "Going deeper with convolutions",
      "author" : [ "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich" ],
      "venue" : null,
      "citeRegEx" : "Szegedy et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2015
    }, {
      "title" : "Large margin methods for structured and interdependent output variables",
      "author" : [ "Thorsten Joachims", "Thomas Hofmann", "Yasemin Altun" ],
      "venue" : "In Journal of Machine Learning Research,",
      "citeRegEx" : "Tsochantaridis et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Tsochantaridis et al\\.",
      "year" : 2005
    }, {
      "title" : "Translating videos to natural language using deep recurrent neural networks. arXiv preprint arXiv:1412.4729",
      "author" : [ "Huijuan Xu", "Jeff Donahue", "Marcus Rohrbach", "Raymond Mooney", "Kate Saenko" ],
      "venue" : null,
      "citeRegEx" : "Venugopalan et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Venugopalan et al\\.",
      "year" : 2014
    }, {
      "title" : "Show and tell: A neural image caption generator",
      "author" : [ "Alexander Toshev", "Samy Bengio", "Dumitru Erhan" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Vinyals et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "Latent embeddings for zero-shot classification",
      "author" : [ "Xian et al.2016] Yongqin Xian", "Zeynep Akata", "Gaurav Sharma", "Quynh Nguyen", "Matthias Hein", "Bernt Schiele" ],
      "venue" : "arXiv preprint arXiv:1603.08895",
      "citeRegEx" : "Xian et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Xian et al\\.",
      "year" : 2016
    }, {
      "title" : "Show, attend and tell: Neural image caption generation with visual attention",
      "author" : [ "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1502.03044",
      "citeRegEx" : "Bengio.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bengio.",
      "year" : 2015
    }, {
      "title" : "Situation recognition: Visual semantic role labeling for image understanding",
      "author" : [ "Yatskar et al.2016] Mark Yatskar", "Luke Zettlemoyer", "Ali Farhadi" ],
      "venue" : "In Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Yatskar et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Yatskar et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning the visual interpretation of sentences",
      "author" : [ "Devi Parikh", "Lucy Vanderwende" ],
      "venue" : "In Proceedings of the IEEE International Conference on Computer Vision,",
      "citeRegEx" : "Zitnick et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Zitnick et al\\.",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "As a first step, we use simple structured representations consisting of subject-relationobject (SRO) triplets (Farhadi et al., 2010).",
      "startOffset" : 110,
      "endOffset" : 132
    }, {
      "referenceID" : 14,
      "context" : "The first, generalizing to new images of the same class, is routinely being evaluated, including in the current data split of the MS-COCO challenge (Lin et al., 2014).",
      "startOffset" : 148,
      "endOffset" : 166
    }, {
      "referenceID" : 6,
      "context" : "The second type, which we focus on, is concerned with generalizing to new scenarios, akin to transfer or zeroshot learning (Fei-Fei et al., 2006), where learning is extended to semantically-similar classes.",
      "startOffset" : 123,
      "endOffset" : 145
    }, {
      "referenceID" : 3,
      "context" : "First, an image is analyzed to produce candidate bounding boxes (Erhan et al., 2014)",
      "startOffset" : 64,
      "endOffset" : 84
    }, {
      "referenceID" : 17,
      "context" : "with their labels (Szegedy et al., 2015).",
      "startOffset" : 18,
      "endOffset" : 40
    }, {
      "referenceID" : 17,
      "context" : "with their labels (Szegedy et al., 2015). Similar to Xu et al. (2015), the classifier was trained on a large dataset without fine-tuning on the current data.",
      "startOffset" : 19,
      "endOffset" : 70
    }, {
      "referenceID" : 18,
      "context" : "For the structured model on top of the deep network, we used structured SVM (SSVM) (Tsochantaridis et al., 2005), minimizing the hinge loss be-",
      "startOffset" : 83,
      "endOffset" : 112
    }, {
      "referenceID" : 14,
      "context" : "We evaluated image captioning on the MS-COCO data (Lin et al., 2014), currently the standard bench-",
      "startOffset" : 50,
      "endOffset" : 68
    }, {
      "referenceID" : 12,
      "context" : "Datasets like Visual-Genome (Krishna et al., 2016) and MS-COCO provide human-marked bounding boxes for many entities.",
      "startOffset" : 28,
      "endOffset" : 50
    }, {
      "referenceID" : 3,
      "context" : "larger datasets, we instead inferred bounding boxes using a pre-trained deep-network localizer (Erhan et al., 2014).",
      "startOffset" : 95,
      "endOffset" : 115
    }, {
      "referenceID" : 24,
      "context" : "To match the two vocabularies we followed the procedure of Zitnick et al. (2013), see Appendix B for details.",
      "startOffset" : 59,
      "endOffset" : 81
    }, {
      "referenceID" : 16,
      "context" : "several groups (Xu et al., 2015; Karpathy and FeiFei, 2015; Mao et al., 2014; Kiros et al., 2014; Donahue et al., 2015; Vinyals et al., 2015; Venugopalan et al., 2014; Chen and Zitnick, 2014; Fang et al., 2015), and was also applied to parts of images",
      "startOffset" : 15,
      "endOffset" : 210
    }, {
      "referenceID" : 11,
      "context" : "several groups (Xu et al., 2015; Karpathy and FeiFei, 2015; Mao et al., 2014; Kiros et al., 2014; Donahue et al., 2015; Vinyals et al., 2015; Venugopalan et al., 2014; Chen and Zitnick, 2014; Fang et al., 2015), and was also applied to parts of images",
      "startOffset" : 15,
      "endOffset" : 210
    }, {
      "referenceID" : 2,
      "context" : "several groups (Xu et al., 2015; Karpathy and FeiFei, 2015; Mao et al., 2014; Kiros et al., 2014; Donahue et al., 2015; Vinyals et al., 2015; Venugopalan et al., 2014; Chen and Zitnick, 2014; Fang et al., 2015), and was also applied to parts of images",
      "startOffset" : 15,
      "endOffset" : 210
    }, {
      "referenceID" : 20,
      "context" : "several groups (Xu et al., 2015; Karpathy and FeiFei, 2015; Mao et al., 2014; Kiros et al., 2014; Donahue et al., 2015; Vinyals et al., 2015; Venugopalan et al., 2014; Chen and Zitnick, 2014; Fang et al., 2015), and was also applied to parts of images",
      "startOffset" : 15,
      "endOffset" : 210
    }, {
      "referenceID" : 19,
      "context" : "several groups (Xu et al., 2015; Karpathy and FeiFei, 2015; Mao et al., 2014; Kiros et al., 2014; Donahue et al., 2015; Vinyals et al., 2015; Venugopalan et al., 2014; Chen and Zitnick, 2014; Fang et al., 2015), and was also applied to parts of images",
      "startOffset" : 15,
      "endOffset" : 210
    }, {
      "referenceID" : 4,
      "context" : "several groups (Xu et al., 2015; Karpathy and FeiFei, 2015; Mao et al., 2014; Kiros et al., 2014; Donahue et al., 2015; Vinyals et al., 2015; Venugopalan et al., 2014; Chen and Zitnick, 2014; Fang et al., 2015), and was also applied to parts of images",
      "startOffset" : 15,
      "endOffset" : 210
    }, {
      "referenceID" : 12,
      "context" : "(Johnson et al., 2015a; Krishna et al., 2016).",
      "startOffset" : 0,
      "endOffset" : 45
    }, {
      "referenceID" : 0,
      "context" : "have been recently explored by (Andreas et al., 2015), who approached a visual QA task by breaking questions into substructures, and re-using modular networks.",
      "startOffset" : 31,
      "endOffset" : 53
    }, {
      "referenceID" : 13,
      "context" : "(Kulkarni et al., 2011) learned spatial relations for generating descriptions based on a template.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 24,
      "context" : "(Zitnick et al., 2013) modelled synthetic scenes generated using CRF.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 23,
      "context" : "The dataset of (Yatskar et al., 2016) has combinations of entities",
      "startOffset" : 15,
      "endOffset" : 37
    }, {
      "referenceID" : 5,
      "context" : "(Farhadi et al., 2010) developed ways to match sentences and images, through a space of meaning parametrized by subject-verbobject triplets which our structured model is closely related to.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 15,
      "context" : "Very recently, (Lu et al., 2016) trained a model that leverages language priors from semantic embeddings to predict subject-relation-object tuples.",
      "startOffset" : 15,
      "endOffset" : 32
    }, {
      "referenceID" : 24,
      "context" : "tity recognition (used by the localizer) differs from the vocabulary of captions, we estimated a mapping from the locaizer vocabulary to the caption terms following the procedure of Zitnick et al. (2013).",
      "startOffset" : 182,
      "endOffset" : 204
    } ],
    "year" : 2016,
    "abstractText" : "Recurrent neural networks have recently been used for learning to describe images using natural language. However, it has been observed that these models generalize poorly to scenes that were not observed during training, possibly depending too strongly on the statistics of the text in the training data. Here we propose to describe images using short structured representations, aiming to capture the crux of a description. These structured representations allow us to tease-out and evaluate separately two types of generalization: standard generalization to new images with similar scenes, and generalization to new combinations of known entities. We compare two learning approaches on the MS-COCO dataset: a state-of-the-art recurrent network based on an LSTM (Show, Attend and Tell), and a simple structured prediction model on top of a deep network. We find that the structured model generalizes to new compositions substantially better than the LSTM, ∼7 times the accuracy of predicting structured representations. By providing a concrete method to quantify generalization for unseen combinations, we argue that structured representations and compositional splits are a useful benchmark for image captioning, and advocate compositional models that capture linguistic and visual structure.",
    "creator" : "LaTeX with hyperref package"
  }
}