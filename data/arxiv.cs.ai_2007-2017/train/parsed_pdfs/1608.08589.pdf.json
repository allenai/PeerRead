{
  "name" : "1608.08589.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Game-Theoretic Modeling of Driver and Vehicle Interactions for Verification and Validation of Autonomous Vehicle Control Systems",
    "authors" : [ "Nan Li", "Dave Oyler", "Mengxuan Zhang", "Yildiray Yildiz", "Ilya Kolmanovsky", "Anouck Girard" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms—Game Theory, Reinforcement Learning, Traffic Modeling, Autonomous Vehicles, Verification and Validation\nI. INTRODUCTION\nONE of the most significant challenges that must beaddressed before autonomous cars can be deployed in mass production is the Verification and Validation (V&V) of their control systems in terms of safety and performance [1], [2]. It has been estimated that autonomous vehicles need to be driven 275 million miles without fatality to assure the same rate of reliability as existing human driven cars [3]. Hence testing and calibration of decision and control systems of autonomous vehicles in simulation becomes necessary to\nN. Li, D. Oyler, M. Zhang, I. Kolmanovsky, and A. Girard are with the Department of Aerospace Engineering, University of Michigan, Ann Arbor, MI, 48109 USA.\nY. Yildiz is with the Department of Mechanical Engineering, Bilkent University, 06800 Cankaya, Ankara, Turkey.\nIlya Kolmanovsky and Nan Li acknowledge the support for this research of the National Science Foundation under Award Number CNS 1544844 to the University of Michigan.\nAnouck Girard acknowledges the support for this research of the Air Force Research Laboratory grant FA 8650-07-2-3744 to the University of Michigan.\ncomplement the on-the-road tests and the formal methodsbased and reachability analysis-based procedures (e.g., see [4], [5], [6], [7]).\nOne common approach to the design of control systems for autonomous vehicles is to utilize a hierarchical control structure, wherein a higher level outer loop controller generates reference trajectories for the lower level inner loop controller which, in turn, determines the steering angle and acceleration/deceleration inputs required to follow the reference trajectory [8]. Designing these control systems is a challenging task as they need to provide a safe ride together with an acceptable performance and comfort in an uncertain traffic environment. Uncertainties generally emanate from unpredictable behaviors of other drivers, pedestrians, unexpected obstacles, and changing road and weather conditions. Several control approaches have been proposed for autonomous vehicles including decision trees [9], [10], Partially Observable Markov Decision Processes (POMDPs) [11] and methods based on multi-policy decision making [12], that are mainly employed as outer loop controllers. For the inner loop, one of the most common approaches is based on Model Predictive Control (MPC) [13]-[14].\nNote that advanced driver behavioral models may also be used in the outer loop [15], [16], with the motivation that an autonomous vehicle should be able to drive at least as well as a human driver. In fact, some experts have suggested that autonomous vehicles should be permitted on public roads only after it is proven that they are superior to human drivers [2].\nSimulators can facilitate the development and testing of autonomous vehicle control algorithms, complementing the road tests. Since autonomous vehicles will be interacting with human driven vehicles in traffic, such simulators need to reflect driver and vehicle interactions. Several methods have been proposed in the literature to address this problem. In [17] and [18], a Hidden Markov Model (HMM) based driver model is proposed based on real driving data. In [19] and [20], k-means clustering is used to determine the driving mode and define an approach to predicting and overbounding future vehicle trajectories. It is shown that the performance of an assisted driving algorithm can be improved through the prediction of driver inputs. In [21], a “cognitive architectures” approach, which is “a computational framework that incorporates builtin, well-tested parameters and constraints on cognitive and perceptual-motor processes,” is utilized for driver modeling. Built-in logical rules (if-then-else) are used to represent the human decision making process. In [22], lane change bear X\niv :1\n60 8.\n08 58\n9v 1\n[ cs\n.A I]\n3 0\nA ug\n2 01\n6\nhavior of drivers is modeled using a multi agent simulation system called “Simulation of Intelligent TRAnsport Systems (SITRAS).” Several logical algorithms are used to model the decision making during lane changes. The resulting actions of the drivers are therefore predefined with strict rules even though driver aggressiveness can be incorporated into the model by tuning certain parameters.\nWith respect to the existing approaches, the present paper is distinguished by the advanced modeling of driver and vehicle interactions in traffic using a specific game theoretic formulation that is scalable to multiple vehicles. The proposed method has the following advantages: a) actions of drivers and vehicles are determined by utilizing a decision making process, instead of assuming that these actions are prescribed in advance as functions of time or as functions of the state of the system; b) multiple interactions between human driven vehicles and autonomous vehicles can be modeled simultaneously and in a computationally tractable way; c) all the vehicles in a traffic scenario are simultaneously modeled as decision makers as opposed to predicting the decisions of one vehicle while assuming that the rest of the vehicles move based on certain kinematic and dynamic constraints. The proposed game theoretic model makes it possible to conduct a quantitative analysis of the traffic. For example, a) the increase in the number of incidents based on the traffic density can be assessed; b) the effect of a certain parameter value in an autonomous vehicle control algorithm on the safety of the vehicle, e.g., quantified by the number of incidents, can be determined; c) various autonomous vehicle control algorithms can be compared quantitatively in a multivehicle time-extended scenario, based on certain safety and performance metrics; and d) optimization of a cost function that reflects safety and performance can be performed.\nOur approach uniquely combines a specific game theoretic formalism, which is used to model intelligent agent interactions, and reinforcement learning, which is used to evolve these interactions in a time-extended (multi-move) scenario. The core ideas are synergistic with the framework of “semi network-form games,” [23], [24] and help us obtain the probable outcomes of a complex traffic scenario driven by multiple interactions. To the authors’ knowledge, such an approach has not been previously exploited for automotive traffic modeling.\nOther game theoretic approaches, in particular, based on Stackelberg games, have been studied for application to vehicle highway driving problems in [15] and [16]. Although these approaches represent driver interactions in traffic using a game theoretic setting, they do not consider dynamic (multimove) scenarios. The latter are considered in [25] for Hybrid Electric Vehicle (HEV) energy management where the driver and the powertrain are considered to be two players in a game. However, increasing the number of players (drivers, in our case) beyond three complicates computing a Stackelberg solution, especially in a time-extended (multi-move) scenario. On the other hand, the hierarchical reasoning based game theoretic approach exploited in this paper is easily scalable. Indeed, an implementation of the proposed approach for a 50 player game can be found in [26], and scenarios with up to 30 vehicles are handled in this paper.\nTo summarize, the contributions of this paper are as follows: a) we develop a novel and scalable to multiple vehicles traffic model and simulation environment based on a specific game theoretic modeling approach; b) our approach allows the representation of driver interactions for many-move, manyvehicle traffic scenarios using a computationally tractable game theoretic approach, whereas many existing methods consider only one-move interaction between two vehicles; c) we demonstrate that autonomous vehicle control algorithms can be quantitatively evaluated in our simulator and compared based on safety and performance metrics applied to simulation outcomes; two policies proposed in the literature serve as case studies, while other autonomous driving algorithms could also be easily integrated and tested; d) we provide a case study to demonstrate that probabilistic outcomes of traffic scenarios can be utilized to obtain the optimal calibration for an autonomous vehicle controller.\nPreliminary results have appeared in conference papers [27], [28]. Differently from [27], [28], in this paper, a) we incorporate a more realistic action space including harder brakes and faster accelerations; b) we develop a more realistic traffic model with more representative distance constraint violation rates via improvements in the reinforcement learning procedure; and c) we demonstrate that optimal parameter values for an autonomous vehicle control algorithm can be obtained using a cost function based on safety and performance. Furthermore, this paper has additional details and interpretations not found in our preliminary conference papers.\nThe paper is organized as follows. In Section II, we define the problem being treated in this paper and the vehicle model being used. In Section III, we present the level-k game theory and the reinforcement learning approach to obtain the policies. In Section IV, we introduce two autonomous vehicle control algorithms that will be tested by our simulator. In Section V, we describe our simulator and its implementation results. We then summarize the key developments in our paper in Section VI."
    }, {
      "heading" : "II. PROBLEM DEFINITION",
      "text" : "The problem we treat is to model the behavior of drivers in a traffic scenario where the cars are driven on a 3-lane highway. We later demonstrate that such models can be used in simulators to evaluate autonomous vehicle control policies. Fig. 1 shows an example scenario with 6 cars. Note that this is not a restriction of the proposed method, and that scenarios with more cars and lanes can be handled. Simulated cars are assumed to be traveling in the same direction and to be driven by human drivers who obey the general traffic laws."
    }, {
      "heading" : "A. Physical models",
      "text" : "The discrete-time equations of motion for each vehicle during forward motion and lane changes are given by\nx(t+ 1) = x(t) + vx(t)∆t,\nvx(t+ 1) = vx(t) + a(t)∆t, (1) y(t+ 1) = y(t) + vy(t)∆t,\nwhere x, y are the vehicle longitudinal and lateral positions, respectively, vx and vy are vehicle longitudinal and lateral velocities, respectively, a(t) is the vehicle longitudinal acceleration, and ∆t is the update period. The longitudinal acceleration a(t) and the lateral velocity vy(t) are the two control inputs. We assume that all cars accelerate and decelerate at either ±a1[m/s2] or ±a2[m/s2]. The nominal values, ±a1[m/s2], reflect the acceleration/deceleration a human driver would apply in normal situations, while the hard accelerations/decelerations of ±a2[m/s2], reflect the values used in urgent situations, based on the maximum acceleration/deceleration capability of a vehicle. We also assume that lane changes occur with constant lateral velocity such that the total time to change lanes is tcl[s]. During lane changes, the longitudinal velocity remains constant, and once a lane change begins, it always continues to completion.\nWhile these assumptions represent a reasonable approximation, formulations with more acceleration/deceleration levels can be similarly introduced at the cost of the increased time and effort to compute the policies."
    }, {
      "heading" : "B. Observation space",
      "text" : "In real traffic flow, a driver can neither observe nor process all the information about all cars on the road. A human can possibly observe and use the information he/she obtains from the cars in a certain vicinity of their own. In particular, a human driver can hardly measure his/her exact distances from other cars. He/she can only estimate the distances and specify them as “close”,“far”, etc. Therefore, we assign the following observation space for the drivers:\n1) The distance from our car to the car directly in front, called range, quantified as “close” (distance ≤ dc[m]), “nominal” (dc[m] < distance ≤ df [m]) or “far” (distance > df [m]), 2) The range to the car in the front left lane, quantified as “close”, “nominal” or “far,” 3) The range to the car in the front right lane, quantified as “close”, “nominal” or “far,” 4) The range to the car in the rear left lane, quantified as “close”, “nominal” or “far,” 5) The range to the car in the rear right lane, quantified as “close”, “nominal” or “far,” 6) The range rate to the car in front, quantified as “approaching” (distance decreasing), “stable” (distance not changing), or “moving away” (distance increasing),\n7) The range rate to the car in the front left lane, quantified as “approaching”, “stable” or “moving away,” 8) The range rate to the car in the front right lane, quantified as “approaching”, “stable” or “moving away,” 9) The range rate to the car in the rear left lane, quantified as “approaching”, “stable” or “moving away,” 10) The range rate to the car in the rear right lane, quantified as “approaching”, “stable” or “moving away,” 11) The lane index of the car, quantified as “lane 1”, “lane 2” or “lane 3.”\nNote that this description of the observation space reflects the uncertainty (or noise) present in real life driver observation."
    }, {
      "heading" : "C. Action space",
      "text" : "Drivers have 7 basic actions: 1) “Maintain” current lane and speed, 2) “Accelerate” at rate = a1[m/s2], provided velocity does\nnot exceed vmax[km/h], 3) “Decelerate” at rate = −a1[m/s2], provided velocity is\nabove vmin[km/h], 4) “Hard Accelerate” at rate = a2[m/s2], provided velocity\ndoes not exceed vmax[km/h], 5) “Hard Decelerate” at rate = −a2[m/s2], provided veloc-\nity is above vmin[km/h], 6) Change lane to the left, provided there is a lane on the\nleft, 7) Change lane to the right, provided there is a lane on the\nright."
    }, {
      "heading" : "D. Reward function",
      "text" : "A “reward function” is a mathematical representation of the goals of a driver. Basic goals of the drivers in real traffic are 1) to not have an accident, such as a car crash (safety), 2) to minimize the time needed to reach the destination (performance), 3) to keep a reasonable headway from preceding cars (comfort) and 4) to minimize driving effort (comfort).\nThese goals can be reflected in the reward function given as\nR = w1c+ w2v + w3h+ w4e, (2)\nwhere wi, i = 1, 2, 3, 4, are the weights for each term and c, v, h and e represent “constraint violation,” “velocity,” “headway” and “effort” metrics. In particular, we define a safe zone for each car (a rectangular area that over-bounds the geometric contour of the car with a safety margin) whose boundaries are treated as distance constraints to represent the safety of the car.\nThe weighting terms, wi, may change depending on the aggressiveness of the driver, but, intuitively, to avoid distance constraint violation should be of the most importance, i.e.,\nw1 w2, w3, w4. (3)\nThe terms, c, v, h, e, are explained below. c (constraint violation): The term c is assigned a value of -1 when a distance constraint violation occurs and a value of 0 otherwise.\nv (velocity): The term v is assigned the value\nv = vx − vnominal\na1 , vnominal = vmin + vmax 2 . (4)\nHere the reason for dividing by a1 is to make this term of the same order of magnitude as the other terms, to facilitate the choice of weights. h (headway): The term h takes the following values depending on the headway distance (the range to the car directly in front):\nh =  −1 if headway ∈ “close,”0 if headway ∈ “nominal,” 1 if headway ∈ “far.”\n(5)\ne (effort): The term e takes the value 0 if the driver’s action is “maintain,” e2 = −5 if the driver’s action is “hard accelerate” or “hard decelerate,” and e1 = −1 otherwise. This term discourages the driver from making unnecessary maneuvers. In particular, a higher penalty discourages the driver from unnecessarily applying “hard accelerate” or “hard decelerate.” But in the case where another maneuver cannot avoid a constraint violation, the driver would apply “hard accelerate” or “hard decelerate” to keep safe. Note that the ratio between e1 and e2 depends on the driver’s behavior and could be tuned to match the driving behavior of different human drivers."
    }, {
      "heading" : "E. Constraints",
      "text" : "The reward function (2) already reflects the penalty for distance constraint violations, which may be viewed as imposing soft constraints on the control. For some combinations of states and actions that obviously lead to constraint violations, we can also impose hard constraints to avoid the occurrence of such combinations. In particular, we introduce the following hard constraints, which make certain actions unavailable in certain situations:\n1) If a car in the left lane is in a parallel position, the controlled car cannot change lane to the left, 2) If a car in the right lane is in a parallel position, the controlled car cannot change lane to the right, 3) If a car in the left lane is “close” and “approaching,” the controlled car cannot change lane to the left, 4) If a car in the right lane is “close” and “approaching,” the controlled car cannot change lane to the right.\nNote that two cars are assumed to be in a parallel position if the safe zones of these two cars intersect in the longitudinal direction. The use of these hard constrains eliminates the clearly undesirable behaviors better than through penalizing them in the reward function, and also increases the learning speed during training."
    }, {
      "heading" : "III. DRIVER INTERACTION MODEL",
      "text" : "The driver interaction model developed in this study enables the modeling of driver-driver and driver-autonomous vehicle interactions through the use of hierarchical decision making and reinforcement learning. The model is a “policy,” which is a stochastic map from the observation space of the driver\nto his/her action space (see Section II). In other words, this map assigns a probability distribution over possible actions for every observation. Below, we explain how this model is generated."
    }, {
      "heading" : "A. Hierarchical decision making",
      "text" : "The developed interaction model is premised on the idea that intelligent agents (such as drivers) have different levels of reasoning. According to this observation, a level-0 agent does not consider probable actions of other agents that he/she is interacting with but rather behaves reflexively. For example, if a driver makes a hard brake when he/she observes an obstacle on the road, without considering how the car following behind would react to this sudden deceleration, this behavior is referred to as a level-0 behavior and the driver is referred to as a level-0 driver. On the other hand, if this driver assumes that the car following behind is being driven by a level-0 driver, who would make a hard brake in case of an obstacle, which may not be enough to avoid a collision, and therefore decides to make a lane change to avoid that collision, then this driver is referred to as a level-1 driver. Similarly, if a driver assumes that the other drivers are level-1 and takes an action accordingly, this driver is a level-2 driver. Higher level driver behavior can be modeled using the same logic. A detailed explanation of this hierarchical modeling method is given in [29] and [30].\nTo obtain higher level policies, one needs to start by defining a level-0 policy. There are various ways to do this, such as selecting each possible action with equal probability regardless of the observation, or constructing a very simple policy, which provides a minimally reasonable behavior for a range of observations.\nIn this study, a level-0 policy is formulated as follows:\nactionl0 =  “Decelerate,” if the car in front is “nominal” and “approaching,” or is “close” and “stable,” “Hard Decelerate,” if the car in front is “close” and “approaching,”\n“Maintain,” otherwise. (6)"
    }, {
      "heading" : "B. Reinforcement learning to solve the Partially Observable Markov Decision Process",
      "text" : "The problem treated in this paper is a multi-agent decision making problem. We use a reinforcement learning (RL) algorithm to determine the policies for each agent based on the reward function defined in Section II-D. To achieve the maximum reward, the RL algorithm exploits two steps including 1) “policy evaluation,” where the state-action pairs are assigned values based on the accumulated rewards they gain, and 2) “policy improvement,” where the probability of choosing the actions that have higher reward values are increased. For more details on RL, see [31].\nConventional RL algorithms require the process to be Markov for convergence guarantees. Note that although the\nunderlying dynamics of the highway problem studied in this work is Markov, each agent (driver) can observe only a subspace of the whole state space (see Section II-B) and therefore has to solve a Partially Observable Markov Decision Process (POMDP) problem. In RL literature, the observations are commonly referred to as “messages.” In this work we employ the Jaakkola RL algorithm [32], which distinguishes itself from conventional approaches by guaranteeing to converge at least to a local maximum in terms of average rewards, when the problem is of POMDP type. Below, the Jaakkola RL algorithm is summarized. See [32] for further details.\nThe following steps are followed to obtain the driver policies using Jaakkola RL:\nStep 1. Evaluate the value function for the messages V (m|πt) and Q-values for the message-action pairs Q(m, a|πt) associated with the driver policy π at step t, using the following equations:\nβtm(m) =(1− χtm(m)\nKtm(m) )γ(t)βt−1m (m) +\nχtm(m) Ktm(m) ,\nV (m|πt) =(1− χ t m(m)\nKtm(m) )V (m|πt−1)\n+ βtm(m)[R t − R̄(πt)],\nβta(m, a) =(1− χta(m, a)\nKta(m, a) )γ(t)βt−1a (m, a) +\nχta(m, a) Kta(m, a) ,\nQ(m, a|πt) =(1− χ t a(m, a)\nKta(m, a) )Q(m, a|πt−1)\n+ βta(m, a)[R t − R̄(πt)], (7)\nwhere m and a designate “messages” (observations) and actions, respectively, the superscript t refers to the time step, while the subscript (m or a) indicates whether the function is associated with messages or with message-action pairs. The function χ is an indicator function, taking the value 1 if the message/message-action pair is visited and 0 otherwise. The function K represents the number of times a particular message/message-action pair is visited. The positive sequence γ(t) represents a discount factor and is converging to 1 in the limit as t→∞. The Rt is the reward obtained at the current time instant t, while R̄(πt) is the average reward associated with the policy πt, i.e., if this policy were executed for an infinite time [33].\nIn the implementation of the above algorithm, the true average reward R̄(πt) is replaced with an estimate, which is computed as an average reward over a past window, making use of the fact that the policy is slowly varying in time.\nStep 2. Update the driver policy π, which is a probabilistic mapping from observations (messages) to actions, using the following equation:\nπt+1(a|m) = (1− ε)πt(a|m) + επ̂t(a|m), (8)\nwhere 0 < ε < 1 is the learning rate and π̂t is chosen such that J(m|π̂t) = max\na\n( Q(m, a|πt)−V (m|πt) ) . For any policy\nπ(a|m), J(m|π) is defined as:\nJ(m|π) = ∑\na π(a|m)\n( Q(m, a|π)− V (m|π) ) . (9)\nNote that based on the process defined above, π̂t should be defined in such a way that it has probability 1 for picking the action a that has the highest Q(m, a|πt) value and probability 0 for picking the other actions.\nGoing through step 1 and 2 at each time t, the driver model, or the converged policy, denoted by π∗, is obtained once the policy converges during the iterative process. The convergence criterion is based on the convergence of the average reward, i.e., change in the average reward within a given number of steps less than a tolerance in absolute value."
    }, {
      "heading" : "C. The role of hierarchical decision making in obtaining driver policies",
      "text" : "The process of obtaining driver policies is called “training,” where the trained driver is a learner and the other vehicles and automation constitute the environment. During the training process, the model of the environment is needed to obtain state transitions as a result of driver actions, where the hierarchical decision making approach plays a crucial rule: for the training of a level-k driver policy, all of the traffic but the trained driver are assigned level-(k-1) policies. The process starts with the determination of a level-0 policy (see Section III-A), which represents the lowest level where the drivers do not consider interaction with other drivers and do not explicitly take into account their possible actions. Once a level-0 policy is determined, the RL algorithm is run by assigning level-0 policies to all of the vehicles except the one that is being trained. At the end of the training process, a level-1 policy is obtained. Similarly, while training a level-2 policy, all of the vehicles but the trained vehicle are assigned a level-1 policy. This hierarchical assignment continues until the desired highest level is obtained. In experimental studies [30], it is shown that in human interactions, level-3 players are very rarely encountered and therefore in our results we trained policies up to and including level-2."
    }, {
      "heading" : "IV. AUTONOMOUS DRIVING CONTROL APPROACHES",
      "text" : "The proposed traffic model has been employed to build a simulator to test and evaluate the performance of autonomous driving control algorithms. As specific examples, two autonomous driving approaches, based on Stackelberg policies and decision trees, will be evaluated and compared, using a simulator in which the traffic, other than the controlled vehicle, consists of drivers modeled using our game theoretic policies. In this section, the control algorithms that will be tested are briefly explained and in the next section simulation evaluations are provided.\nThe Stackelberg policies and the decision tree policies that are compared in this study were originally developed in [10], [15] and [16]. Since these policies were developed under assumptions representing a simpler traffic environment, some necessary modifications were made to let the autonomous vehicle, which will employ these policies, be able to operate in the traffic environment investigated in this study. For example, the originally proposed Stackelberg and decision tree policies consider only lane change actions. To make them more compatible with the test environment, acceleration/deceleration\nactions are added to their action space. Fig. 2 shows the necessity of this modification, where each rectangle represents a car and the arrows represent both the driving direction and velocity (the longer the arrow, the larger the velocity). In the figure, three yellow cars are in front of the red test car and since the speed of the test car is larger than that of the yellow cars, there is a danger of collision that cannot be resolved only with a lane change."
    }, {
      "heading" : "A. Stackelberg policies",
      "text" : "To generate Stackelberg policies for the autonomous vehicle, we consider three vehicles as players, and the rest of the vehicles are considered to be the environment. The three players are assigned roles as the “leader,” “first follower,” and “second follower,” and they choose their actions sequentially: the leader chooses its action first, followed by the first follower, and, finally, the second follower. Each player evaluates their actions according to a utility function that consists of two parts. The first part, known as the positive utility, is defined as follows:\nUpos = { min(d4, dv), if there is a vehicle ahead, dv, otherwise, (10)\nwhere d4 is the distance to the car directly in front, i.e., the headway distance, and dv is the maximum visibility distance. The second part of the utility is known as the negative utility:\nUneg = d∇ − vrT − dmin, (11)\nwhere d∇ and vr are the distance to and the relative velocity of the car immediately behind, T is a prediction time window, and dmin is the minimum distance required to allow a lane change; here, dmin is set to the car safe zone length. Thus, overtaking vehicles are taken into consideration, and lane changes that cut off overtaking vehicles are discouraged.\nThe actions chosen by the leader, first follower, and second follower, denoted by γ`, γf1, and γf2, respectively, are the Stackelberg Equilibrium actions; i.e., the leader chooses its actions to maximize its utility for the worst-case actions that the two followers might choose. Thus, the leader chooses:\nγ∗` = max γ` min γf1 min γf2\n[ U ′pos + U ′ neg ] , (12)\nwhere U ′pos and U ′ neg are the utilities that correspond to a specific set of actions {γ`, γf1, γf2}. The two followers maximize their own utilities with the known choice of γ`. In this paper, when constructing Stackelberg policies, the\ncontrolled vehicle is the leader, and the two cars immediately behind are the followers. An alternative choice in which the controlled vehicle is one of the followers instead of the leader can be treated similarly.\nIt is noted that in [15] and [16] vehicle dynamics are different than the ones used in this paper. Furthermore, some aspects of the modeling such as uncertainties in measuring distance, side-viewing and response delays that are considered in these references are omitted here to simplify the analysis but can be easily integrated."
    }, {
      "heading" : "B. Decision tree policies",
      "text" : "In the decision tree approach to autonomous driving, the vehicle’s actions are determined by a path planner that evaluates a specified number of pre-selected action profiles by building a tree of potential action sequences and evaluating each branch according to a specified metric.\nIn this paper, the decision tree consists of two layers, where each layer allows the seven actions listed in Section II-C. Thus, the action profiles consist of two actions each, and 49 profiles are evaluated. The evaluation metric is based on the reward function (2), which is also used for the training of the levelk policies. In particular, the total reward is calculated as a weighted sum of the rewards obtained from the two layers:\nRtotal = wl1Rl1 + wl2Rl2, (13)\nwhere wl1, wl2 ∈ R+ are the weighting terms. The car applies the first-layer action of the profile that has the highest total reward among all profiles, and repeats this procedure at each step. When evaluating the action profiles, the controlled vehicle assumes that all other vehicles would apply the action “maintain” during the prediction horizon.\nIn [10], it was assumed that the environment evolves deterministically over a planning horizon, independently of the controlled vehicle’s actions. However, in our simulator, not only the controlled vehicle responds to the traffic, but also the traffic responds to the controlled vehicle’s actions."
    }, {
      "heading" : "C. Path planner triggering threshold",
      "text" : "Both the Stackelberg and the decision tree policies are used as path planners and triggered only when necessary and beneficial. When the policies are not triggered, the vehicle follows a predefined driving pattern. More specifically, with the help of Fig. 4, the driving algorithm can be explained as follows:\n1) “Accelerate,” if there are no other cars in region A, 2) Path planner is triggered, if there are cars in region A,\nbut no cars in region B, 3) “Safe Mode,” which in this paper is the same as the\nlevel-0 policy, is applied if there are cars in region B. It is clear that if there are no cars in region A, the autonomous vehicle may accelerate safely, while if there are cars in region B, the autonomous vehicle is supposed to decelerate to keep a reasonable headway distance. The activation logic is designed 1) to increase the safety, and 2) to reduce the computational cost by preventing unnecessary action evaluations. In particular, region A is designed to cover the center of adjacent lanes, while region B is designed to cover the boundary lines of the current lane, so that when some vehicle in the vicinity is changing lanes into the test vehicle’s lane, the test vehicle becomes aware of it before the other vehicle actually enters its lane."
    }, {
      "heading" : "V. RESULTS",
      "text" : ""
    }, {
      "heading" : "A. Environment and set-up",
      "text" : "We model the environment as follows. The width of a lane is 3.6[m], and the safe zones around the cars, which should not be violated, are modeled as 6[m] × 2[m] boxes. Cars always drive at the center of a lane unless they are changing lanes. Cars only accelerate or decelerate at ±a1 = ±2.5[m/s2] or ±a2 = ±5[m/s2], and lane changes occur with constant lateral velocity such that the total time to change lanes is tcl = 2[s]. During lane changes, the longitudinal velocity remains constant and once a lane change begins, it always continues to completion. The longitudinal axis is called x, and its origin is collocated with the car that is to be trained or evaluated.\nTo configure the simulation, the following values need to be specified:\n1) the number of lanes, n`, 2) the number of cars, nc, 3) the maximum allowable initialization distance, x0max, 4) the simulation duration, tf . The following procedure is employed to initialize a simulation: A car is assigned to a lane that is determined randomly based on the uniform distribution. The specific location of the car within the assigned lane is determined randomly based on the uniform distribution in [−x0max, x0max]. Then, the initial longitudinal velocity of the car is given randomly based on the uniform distribution within the range [vmin, vmax] = [62, 98][km/h]. The initial action is set as “maintain.” The car is then assigned a policy to follow (level-0, 1, or 2). This process is repeated until all cars are configured. While locating each car, it is required that the minimum initial distance between the cars be 30[m]. Once the initialization is completed, the simulation is run according to Algorithm 1 given below.\n1 t = 0. 2 while t < tf do 3 foreach car do 4 Obtain observations from the environment. 5 Given the observations, determine an action based on assigned policy. 6 Given the action, update position and velocity. 7 end 8 if training a policy then 9 Evaluate reward function for trainee.\n10 Update value function. 11 end 12 if trainee/test vehicle is in a constraint violation state then 13 End the simulation. 14 end 15 t = t+ ∆t. 16 end\nAlgorithm 1: Single Episode Simulation\nFive cars are observable by the test vehicle, as described in Section II-B, and a car is considered “close” if its relative longitudinal position satisfies |xr| ≤ dc = 21[m], “nominal” if dc < |xr| ≤ df = 42[m], and “far” if df < |xr| ≤ dv = 63[m], where dv is the maximum visibility distance. Cars farther away than dv[m] are considered to be out of visual range and unobservable. If no car can be observed in a position, this is considered equivalent to a car that is “far” and “moving away.” Note that dc = 21[m] is determined by considering the minimum distance needed for a car to safely avoid a distance constraint violation by braking with the maximum allowable deceleration in the worst case scenario where a car in front is approaching with maximum relative speed and entering the “close” range.\nFig. 5 shows a snapshot of an example simulation setup with three lanes. The rectangles represent the safe zones around the\ncars, and the arrows attached to the rectangles show the relative velocities of the cars with respect to the test vehicle, that is located in the center lane at x = 0. Specific observations by the test vehicle are given as follows:\n• Front left: close, moving away, • Front center: far, moving away, • Front right: far, approaching, • Rear left: nominal, approaching, • Rear right: nominal, stable, • Lane index: lane 2.\nNote that two cars in Fig. 5 are unobservable: The car in the front center position is beyond the visual range – so the corresponding observed status is “moving away” even though the car is actually “stable;” and the car in the rear right “far” position is hidden by the car in the rear right “nominal” position. These limitations in the observation space reflect the POMDP nature of the problem, as discussed previously."
    }, {
      "heading" : "B. Level-0 driver behavior",
      "text" : "In this section, we present simulation results to show the driving behavior of a level-0 car. Furthermore, we present the simulator user interface.\nIn Fig. 6, the red car in the center is the trainee/test vehicle, while the yellow cars make up the traffic environment. The red arrow in front of the red car indicates its travel direction, and arrow size indicates how fast the car is traveling. The panel on the left is a speedometer, and the steering wheel on the right indicates the lateral motion of the car. The green box and red box in the middle indicate the gas pedal and the brake pedal, respectively, and when any of them turns blue, that indicates the pedal is pressed. The coordinate axis is fixed on the test car and the motions of the other cars can be tracked by their relative distance to the red car. In Fig. 6(a), a yellow car directly in front of the red car (“far”) is “approaching” because the red car is faster. At this moment neither the gas pedal nor the brake pedal is pressed. In Fig. 6(b), the yellow car enters the “nominal” range, and consistently with the level-0 policy, the red car brakes and decreases its speed. In Fig. 6(c), the red car gets to a lower speed, and the yellow car is now “stable.”"
    }, {
      "heading" : "C. Training process",
      "text" : "When training a new policy, the observation value function, V , for observed message m, and the action value function, Q,\nfor message-action pair (m, a), are initialized as follows:\n∀m, V (m) = 0; ∀m,∀a, Q(m, a) = 0.\n(14)\nFor each observation, the actions are assigned equal probability of selection at initialization, and during each policy improvement step, if\nmax a Q(m, a) > V (m), (15)\nthen 0.01 is added to the probability of selecting argmaxaQ(m, a), after which the action probabilities are normalized.\nThe observation space described in Section II-B has 311 different observations. In order to ensure that the learning algorithm is exposed to a large portion of the observation space, the trainee needs to be exposed to both sparse and dense traffic environments. Therefore, during training, the number of cars in the environment is selected randomly, based on the uniform distribution, where 0 ≤ nc ≤ nmaxc . The maximum number of cars, nmaxc , is chosen based on the number of lanes and x0max, such that if n max c cars are placed in the environment, the road is near full capacity. Finally, after sufficient training time, the level-0 policy is assigned to the observations (messages) that are still not visited enough during training, so that conservative actions are performed in such rarely encountered cases.\nTraining then proceeds according to Algorithm 2, given below:\n1 step=0; 2 while step < desired training cycles do 3 Randomly select nc ∈ [0, nmaxc ]. 4 Initialize all cars with level-(k-1) policies. 5 Evaluate the level-k policy using Algorithm 1. 6 Improve the policy. 7 step=step+1. 8 end 9 Assign level-0 policy to the not well trained cases.\nAlgorithm 2: Training Process\nFig. 7 shows the evolution of the average reward during level-1 and level-2 trainings. The reward function weights are chosen as: w1 = 10000, w2 = 5, w3 = w4 = 1.\nD. Level-k driver interactions\nWe first present simulation results to show the driving behavior of a level-1 car in a level-0 traffic environment (Fig. 8), and then of a level-2 car in a level-1 traffic environment (Fig. 9). It is noted that in both figures, the traffic is moving towards the right. The test vehicle is red and the rest of the vehicles are yellow.\nIn Fig. 8(a), the car directly in front of the test vehicle is “approaching” with a large relative speed and enters the “far” distance range. In Fig. 8(b), the red car decelerates and begins to steer. In Fig. 8(c), the red car starts to move into the lane on its left. In Fig. 8(d), the lane changing is completed, after 2[s]; while another car in front approaches the red car. In Fig. 8(e), the red car decelerates until being able to keep a stable headway, which is shown in Fig. 8(f). All these actions represent a reasonable driving behavior.\nSimilarly, Fig. 9 shows a simulation result of a level-2 car in a level-1 traffic environment. In Fig. 9(a), the yellow car in the middle lane is “approaching” the red car, while the yellow car in the left lane is “moving away,” so the red car decides to change lane to the left. In Fig. 9(b), the yellow car in the middle lane starts to change lane to the left, so the red car needs to brake. In Fig. 9(c), there is no car directly in front of the red car, in which case a level-1 car would accelerate. However, because the longitudinal distance of the red car to the yellow car in the middle lane is too small and the red car\nhas no confidence that the yellow car won’t change lane to the left, in which case the red car’s acceleration would lead to a\ndistance constraint violation, the red car decides to maintain its current speed. In Fig. 9(d), there is a car in the traffic moving into the red car’s lane, which forces the red car to decelerate.\nDifferent driving patterns of level-1 and level-2 cars, in similar situations, reflect different levels of reasoning in decision making. It is noted that in Fig. 9, the cars in the environment (in yellow) also change lanes, unlike the ones in Fig. 8, because their actions are based on the level-1 policy, which makes them react to their own surroundings.\nFig. 10 shows the constraint violation rates of the level-1 and level-2 test cars for varying numbers of cars in the traffic. Here, “constraint violation” refers to the violation of the safe zone of the test car by any of the vehicles in the simulations. To obtain these rates, 10,000 simulations are run for each case (i.e., for each value of the number of cars). Each simulation lasts 200 seconds and the rates are provided as the percentage of simulation runs during which safe zones are violated. Note that the level-2 test car is placed in traffic of level-1 vehicles, and the level-1 car is placed in the environment of level-0\nvehicles. It is seen that the level-2 test car experiences higher violation rates than the level-1 test car. One explanation for this is that the traffic flow consisting of level-1 cars, where the level-2 policy is evaluated, is much harder to predict compared to the one consisting of level-0 cars, where the level-1 policy is tested."
    }, {
      "heading" : "E. A comparative quantitative evaluation of Stackelberg and decision tree policies",
      "text" : "At first, we test the Stackelberg and the decision tree policies in a traffic consisting of only level-0 vehicles. A defining feature of the level-0 policy is that the vehicles do not change lanes. It is observed that both the Stackelberg and the decision tree policies perform well in this environment, i.e., constraint violations are not observed. This is also in agreement with the results in [15], [16] and [10]. The figures are omitted as they provide no additional information.\nNext, we consider a simulated traffic environment where 10% of the drivers make decisions based on level-0 policies, 60% of the drivers act based on level-1 policies and 30% use level-2 policies. These percentages of various levels are assumed based on an experimental study conducted in [30]. Fig. 11 shows the distance constraint violation rates for the Stackelberg and decision tree policies vs. the number of cars in the traffic. Each simulation is 200 seconds long, and 10,000 simulations are run for each case (i.e., for each value of the number of cars).\nAs seen in Fig. 11, both approaches exhibit significant distance constraint violation percentages. Note that these violations could also be caused by our level-k drivers, but as shown in Fig. 10, compared to the numbers in Fig. 11, the constraint violation rates of the level-k policies are small.\nThe main reason for the distance constraint violations is that the developed traffic model with interacting drivers is more complex than the traffic models used for the development of the Stackelberg and decision tree algorithms. To the best of our knowledge, few works addressing the development of\nautonomous vehicle control policies consider traffic of similar complexity.\nFig. 12 shows two cases of driver interactions that are responsible for many distance constraint violations. The red rectangle indicates the car being tested, while the yellow rectangles are the cars in the traffic environment. The black arrows indicate the longitudinal velocities of the cars, while green arrows indicate the lateral velocities. The left hand side presents the scenarios at time t, while the right hand side presents the scenarios at time t + 1. In Fig. 12(a), the red car is starting to change lane to the left, trying to overtake the front car, while at the same time the front car is also starting to change lane because of some other car ahead of it (not shown in this figure). As a result, both cars are changing lanes while their longitudinal distance keeps decreasing until below the safe distance constraint. Although the red car may brake hard after the lane change, trying to avoid a distance constraint violation, at that time its range is already too small for it to avoid that by braking. In Fig. 12(b), the red car is starting to change lane to the left to overtake the car in front of it. Although the car in front remains in its own lane, there is some other car in the leftmost lane starting to move into the middle lane as well. As a result, both the red car and the yellow car in the leftmost lane are changing lanes to the middle and eventually violate the safe distance between them.\nWe note that challenging scenarios as the ones above can greatly facilitate the testing of autonomous driving control algorithms. In fact, the above two cases are also dangerous situations for a human driver. Many traffic accidents result from the driver misjudging the potential actions of the surrounding vehicles.\nNote also that the constraint violation rate first increases and then decreases as the number of cars in the environment, which reflects the traffic density, increases. As Fig. 13 shows, when the traffic is very sparse, cars on the road can drive almost freely and have low chance of having a constraint violation ( 1©). As the number of cars in the traffic increases, the chance of experiencing an incident also increases ( 2©) (until a peak\n3©). When the traffic becomes very dense, for instance, in a traffic jam, the average traveling speed becomes low, and at the same time each car mostly stays in its own lane and has few lane change maneuvers. As a result, the probability of having constraint violations becomes low again ( 4©).\nApart from using the constraint violation rate as a metric to measure the safety and robustness of the Stackelberg and decision tree policies, we also use the average travel speed to measure the driving performance. It can be observed from Fig. 14 that the decision tree policy has better driving performance compared to the Stackelberg policy.\nFig. 15 compares the computational load of the two autonomous driving control methods. The numbers shown in the plot are the average time consumed to run the 200[s] simulation. Because the decision tree policy needs to evaluate 49 profiles of 2-layer actions in total, its required computation load is higher than that of the Stackelberg policy. As a summary, our implementation of the decision tree algorithm is better than our implementation of the Stackelberg algorithm in terms of both safety and performance, while the price paid is the higher computational cost. The numbers in Fig. 15 are obtained using the Java System.nanoT ime() function, and the simulations are run on a desktop with i7-4790 3.60GHz CPU and with Eclipse Java Neon platform. Note also that as Fig. 15 indicates the simulator is quite fast.\n0 10 20 30 Number of cars\n0\n0.1\n0.2\n0.3\n0.4\n0.5\nT im\ne [s\n]\nStackelberg Decision Tree\nFig. 15: Computational time of the Stackelberg and decision tree policies."
    }, {
      "heading" : "F. Optimal autonomous driving controller calibration",
      "text" : "One of the potential uses of the proposed traffic modeling approach is for calibrating parameter values in the autonomous driving control algorithms. We illustrate this using the decision tree policy as an example.\nThe optimization objective is defined by considering both safety and performance: the goal is to maximize the following reward function:\nRobj = p1(−c̄) + p2 v̄x − vmin vmax − vmin , (16)\nwhere the weights p1 and p2 are determined by the user, c̄ is the constraint violation rate defined as in Fig. 11, v̄x is the average speed during the 200[s]-simulations, and vmin and vmax are the lower and the upper bounds of the test vehicle’s speed, respectively. Note that this reward function is designed such that each of its terms is dimensionless. The parameters optimized in this example are the ratio wl1wl2 , representing the weighting of the two layers in the evaluation\nmetric function (13) in the decision tree evaluations, and xB , the size of region B in the longitudinal direction – a threshold of triggering the path planner. These two parameters are selected for optimization since they have indirect influence on safety and performance, making them difficult to set from intuition. Note that the influence of other parameters, for example, w1, · · · , w4, in the decision tree evaluation metric function, is more intuitive and they can be tuned more easily.\nFig. 16 shows the surface of the reward values as a function of wl1wl2 , xB corresponding to different weight selections p1, p2, in the presence of a 20-car traffic. These figures can be used to pick the best pair of (wl1wl2 , xB) for a given reward function. For example, for maximum safety (p1 = 1, p2 = 0), Fig. 16(a) shows that the best pair is (2.5, 23). The rate of constraint violation with this pair is 27.5%; while for the original selection (2, 21) in the previous section, the corresponding violation rate is 31.8%.\n-0.4 23\n-0.35\n22\n-0.3\n21\n-0.25\n1 1.520 2\n2.519 3\n0.5 23\n0.6\n22\n0.7\n21\n0.8\n1 1.520 2\n2.519 3\n-0.05 23\n-0.04\n-0.03\n22\n-0.02\n21 1\n-0.01\n1.520 2 2.519 3\n0.04 23\n0.06\n22\n0.08\n21\n0.1\n1 1.520 2\n2.519 3\n(a) (b)\n(c) (d)\nwl1 wl2 xB wl1 wl2 xB\nwl1 wl2 xB wl1 wl2 xB\nRobj Robj\nRobj Robj\nFig. 16: Parameter optimization results corresponding to different reward function designs. (a) p1=1, p2=0, (b) p1=0, p2=1, (c) p1=0.7, p2=0.3, (d) p1=0.6, p2=0.4."
    }, {
      "heading" : "VI. SUMMARY",
      "text" : "In this paper, a hierarchical reasoning game theory based approach to model interacting driver behavior in traffic was presented. The proposed method provides an approach to simulate interactive driver behavior under the given traffic conditions.\nA traffic simulator was developed using level-k driver models. It can be used for testing and verification of autonomous driving algorithms, and for discovery of challenging trajectories and scenarios that can facilitate the testing of future autonomous vehicles. To illustrate the simulator use, we have defined and tested two autonomous vehicle control policies\nin terms of safety and performance. Our traffic simulator can also be used for parameter calibration of these policies by a simulation-based optimization approach."
    } ],
    "references" : [ {
      "title" : "Autonomous driving in urban environments: approaches, lessons and challenges",
      "author" : [ "M. Campbell", "M. Egerstedt", "J.P. How", "R.M. Murray" ],
      "venue" : "Philosophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sciences, vol. 368, no. 1928, pp. 4649–4672, 2010.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1928
    }, {
      "title" : "Autonomous vehicle technology: A guide for policymakers",
      "author" : [ "J.M. Anderson", "N. Kalra", "K.D. Stanley", "P. Sorensen", "C. Samaras", "O.A. Oluwatola" ],
      "venue" : "RAND Corporation, Santa Monica, CA, Research Report, 2016. Available at http:www.rand.orgpubsresearch reportsRR443- 2.html.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Australian innovation",
      "author" : [ "N. Kalra. (2016", "Feb. 01" ],
      "venue" : "[Weblog entry], THERANDBLOG, Available: http:www.rand.orgblog201602withdriverless-cars-how-safe-is-safe-enough.html [Apr 03, 2016].",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Formal verification of an autonomous vehicle system",
      "author" : [ "T. Wongpiromsarn", "R.M. Murray" ],
      "venue" : "Conference on Decision and Control, May, 2008.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Periodically controlled hybrid systems: Verifying a controller for an autonomous vehicle",
      "author" : [ "T. Wongpiromsarn", "S. Mitra", "R.M. Murray", "A. Lamperski" ],
      "venue" : "Proceedings of the HSCC: Hybrid Systems, Computation and Control Conference. California Institute of Technology, 2008.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Verified hybrid controllers for automated vehicles",
      "author" : [ "J. Lygeros", "D.N. Godbole", "S. Sastry" ],
      "venue" : "IEEE Transactions on Automatic Control, vol. 43, no. 4, pp. 522–539, 1998.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Online verification of automated road vehicles using reachability analysis",
      "author" : [ "M. Althoff", "J.M. Dolan" ],
      "venue" : "IEEE Transactions on Robotics, vol. 30, no. 4, pp. 903–918, 2014.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Automated driving: The role of forecasts and uncertainty-a control perspective",
      "author" : [ "A. Carvalho", "S. Lefevre", "G. Schildbach", "J. Kong", "F. Borrelli" ],
      "venue" : "European Journal of Control, vol. 24, pp. 14–32, 2015.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Team cornell’s skynet: Robust perception and planning in an urban environment",
      "author" : [ "I. Miller", "M. Campbell", "D. Huttenlocher", "F.-R. Kline", "A. Nathan", "S. Lupashin", "J. Catlin", "B. Schimpf", "P. Moran", "N. Zych", "E. Garcia", "M. Kurdziel", "H. Fujishima" ],
      "venue" : "Journal of Field Robotics, vol. 25, no. 8, pp. 493–527, 2008.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "A path planner for autonomous driving on highway human mimicry approach with binary decision diagrams",
      "author" : [ "L. Claussman", "A. Carvalho", "G. Schildbach" ],
      "venue" : "Proceedings of the European Control Conference, Linz, Austria, July 2015.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Probabilistic decision-making under uncertainty for autonomous driving using continuous pomdps",
      "author" : [ "S. Brechtel", "T. Gindele", "R. Dillmann" ],
      "venue" : "Proc. IEEE Intelligent Transportation Systems (ITSC), October 2014, pp. 392–399.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Multipolicy decision-making for autonomous driving via changepoint-based behavior prediction",
      "author" : [ "E. Galceran", "A.G. Cunnigham", "R.M. Eustice", "E. Olson" ],
      "venue" : "Proceedings of Robotics: Science and Systems, Rome, Italy, 2015.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Predictive active steering control for autonomous vehicle systems",
      "author" : [ "P. Falcone", "F. Borrelli", "J. Asgari", "H.E. Tseng", "D. Hrovat" ],
      "venue" : "IEEE Transactions on Control Systems Technology, vol. 15, no. 3, pp. 566–580, 2007.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Predictive control of an autonomous ground vehicle using linearization approach",
      "author" : [ "A. Carvalho", "Y. Gao", "A. Gray", "H.E. Tseng", "F. Borrelli" ],
      "venue" : "Proceedings of the 16th IEEE Annual Conference on Intelligent Transportation Systems, The Hague, The Netherlands, October 2013.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Stackelberg game based model of highway driving",
      "author" : [ "J.H. Yoo", "R. Langari" ],
      "venue" : "Proc. ASME Dynamic Systems and Control Conference joint with JSME Motion and Vibration Conference, Fort Lauderdale, Florida, Oct. 2012.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A stackelberg game theoretic driver model for merging",
      "author" : [ "——" ],
      "venue" : "Proc. ASME Dynamic Systems and Control Conference, Palo Alto, California, Oct. 2013.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Lane keeping assistance with learning-based driver model and model predictive control",
      "author" : [ "S. Lefevre", "Y. Gao", "D. Vasquez", "E. Tseng", "R. Bajcsy", "F. Borrelli" ],
      "venue" : "Proceedings of the 12th international symposium on advanced vehicle control, 2014.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Autonomous car following: A learning-based approach",
      "author" : [ "S. Lefevre", "A. Carvalho", "F. Borrelli" ],
      "venue" : "IEEE Intelligent Vehicles Symposium, 2015, pp. 920–926.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Safe semi-autonomous control with enhanced driver modeling",
      "author" : [ "R. Vasudevan", "V. Shia", "Y. Gao", "R. Cervera-Navarro", "R. Bajcsy", "F. Borrelli" ],
      "venue" : "Proc. Amer. Control Conf., 2012, pp. 2896–2903.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Semiautonomous vehicular control using driver modeling",
      "author" : [ "V. Shia", "Y. Gao", "R. Vasudevan", "K.D. Campbell", "T. Lin", "F. Borrelli", "R. Bajcsy" ],
      "venue" : "IEEE Transactions on Intelligent Transportation Systems, vol. 15, no. 6, pp. 2696–2709, 2014.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Toward an integrated model of driver behavior in cognitive architecture",
      "author" : [ "D. Salvucci", "E. Boer", "A. Liu." ],
      "venue" : "Transportation Research Record: Journal of the Transportation Research Board, vol. 1779, pp. 9–16, 2001.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Modelling lane changing and merging in microscopic traffic simulation",
      "author" : [ "P. Hidas" ],
      "venue" : "Transportation Research Part C: Emerging Technologies, vol. 10, no. 5, pp. 351–371, 2002.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Chapter: Game theoretic modeling of pilot behavior during mid-air encounters. in Decision making with multiple imperfect decision makers",
      "author" : [ "R. Lee", "D. Wolpert" ],
      "venue" : "Intelligent Systems Reference Library Series. Springer,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2011
    }, {
      "title" : "Cyber-physical security: A game theory model of humans interacting over control systems",
      "author" : [ "S. Backhaus", "R. Bent", "J. Bono", "R. Lee", "B. Tracey", "D. Wolpert", "D. Xie", "Y. Yildiz" ],
      "venue" : "IEEE Transactions on Smart Grid, vol. 4, no. 4, pp. 2320–2327, 2013.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Game theory controller for hybrid electric vehicles",
      "author" : [ "C. Dextreit", "I.V. Kolmanovsky" ],
      "venue" : "IEEE Transactions on Control Systems Technology, vol. 22, no. 2, pp. 652–663, 2014.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Predicting pilot behavior in medium-scale scenarios using game theory and reinforcement learning",
      "author" : [ "Y. Yildiz", "A. Agogino", "G. Brat" ],
      "venue" : "Journal of Guidance, Control, and Dynamics, vol. 37, no. 4, pp. 1335– 1343, 2014.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A game theoretical model of traffic with multiple interacting drivers for use in autonomous vehicle development",
      "author" : [ "D. Oyler", "Y. Yildiz", "A. Girard", "N.I. Li", "I. Kolmanovsky" ],
      "venue" : "Proc. Amer. Control Conf., Boston, MA, 2016.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Hierarchical reasoning game theory based approach for evaluation and testing of autonomous vehicle control systems",
      "author" : [ "N. Li", "D. Oyler", "M. Zhang", "Y. Yildiz", "A. Girard", "I. Kolmanovsky" ],
      "venue" : "IEEE Conference on Decision and Control, Las Vegas, accepted for publication, 2016.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "On players models of other players: Theory and experimental evidence",
      "author" : [ "D. Stahl", "P. Wilson" ],
      "venue" : "Games and Economic Behavior, vol. 10, no. 1, p. 218254, 1995.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Comparing models of strategic thinking in Van Huyck, Battalio, and Beil’s coordination games",
      "author" : [ "M.A. Costa-Gomes", "V.P. Crawford", "N. Iriberri" ],
      "venue" : "Journal of the European Economic Association, vol. 7, no. 2-3, pp. 365–376, 2009.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Reinforcement learning: An introduction",
      "author" : [ "R.S. Sutton", "A.G. Barto" ],
      "venue" : "Cambridge: MIT press,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 1998
    }, {
      "title" : "Reinforcement learning algorithm for partially observable markov decision problems",
      "author" : [ "T. Jaakkola", "P.S. Satinder", "I. Jordan." ],
      "venue" : "Advances in Neural Information Processing Systems 7: Proceedings of the 1994 Conference, 1994.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Average reward reinforcement learning: Foundations, algorithms, and empirical results",
      "author" : [ "S. Mahadevan" ],
      "venue" : "Machine learning, vol. 22, no. 1-3, pp. 159–195, 1996.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 1996
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "ONE of the most significant challenges that must be addressed before autonomous cars can be deployed in mass production is the Verification and Validation (V&V) of their control systems in terms of safety and performance [1], [2].",
      "startOffset" : 221,
      "endOffset" : 224
    }, {
      "referenceID" : 1,
      "context" : "ONE of the most significant challenges that must be addressed before autonomous cars can be deployed in mass production is the Verification and Validation (V&V) of their control systems in terms of safety and performance [1], [2].",
      "startOffset" : 226,
      "endOffset" : 229
    }, {
      "referenceID" : 2,
      "context" : "It has been estimated that autonomous vehicles need to be driven 275 million miles without fatality to assure the same rate of reliability as existing human driven cars [3].",
      "startOffset" : 169,
      "endOffset" : 172
    }, {
      "referenceID" : 3,
      "context" : ", see [4], [5], [6], [7]).",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 4,
      "context" : ", see [4], [5], [6], [7]).",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 5,
      "context" : ", see [4], [5], [6], [7]).",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 6,
      "context" : ", see [4], [5], [6], [7]).",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 7,
      "context" : "One common approach to the design of control systems for autonomous vehicles is to utilize a hierarchical control structure, wherein a higher level outer loop controller generates reference trajectories for the lower level inner loop controller which, in turn, determines the steering angle and acceleration/deceleration inputs required to follow the reference trajectory [8].",
      "startOffset" : 372,
      "endOffset" : 375
    }, {
      "referenceID" : 8,
      "context" : "Several control approaches have been proposed for autonomous vehicles including decision trees [9], [10], Partially Observable Markov Decision Processes (POMDPs) [11] and methods based on multi-policy decision making [12], that are mainly employed as outer loop controllers.",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 9,
      "context" : "Several control approaches have been proposed for autonomous vehicles including decision trees [9], [10], Partially Observable Markov Decision Processes (POMDPs) [11] and methods based on multi-policy decision making [12], that are mainly employed as outer loop controllers.",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 10,
      "context" : "Several control approaches have been proposed for autonomous vehicles including decision trees [9], [10], Partially Observable Markov Decision Processes (POMDPs) [11] and methods based on multi-policy decision making [12], that are mainly employed as outer loop controllers.",
      "startOffset" : 162,
      "endOffset" : 166
    }, {
      "referenceID" : 11,
      "context" : "Several control approaches have been proposed for autonomous vehicles including decision trees [9], [10], Partially Observable Markov Decision Processes (POMDPs) [11] and methods based on multi-policy decision making [12], that are mainly employed as outer loop controllers.",
      "startOffset" : 217,
      "endOffset" : 221
    }, {
      "referenceID" : 12,
      "context" : "For the inner loop, one of the most common approaches is based on Model Predictive Control (MPC) [13]-[14].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 13,
      "context" : "For the inner loop, one of the most common approaches is based on Model Predictive Control (MPC) [13]-[14].",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 14,
      "context" : "Note that advanced driver behavioral models may also be used in the outer loop [15], [16], with the motivation that an autonomous vehicle should be able to drive at least as well as a human driver.",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 15,
      "context" : "Note that advanced driver behavioral models may also be used in the outer loop [15], [16], with the motivation that an autonomous vehicle should be able to drive at least as well as a human driver.",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 1,
      "context" : "In fact, some experts have suggested that autonomous vehicles should be permitted on public roads only after it is proven that they are superior to human drivers [2].",
      "startOffset" : 162,
      "endOffset" : 165
    }, {
      "referenceID" : 16,
      "context" : "In [17] and [18], a Hidden Markov Model (HMM) based driver model is",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 17,
      "context" : "In [17] and [18], a Hidden Markov Model (HMM) based driver model is",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 18,
      "context" : "In [19] and [20], k-means clustering is used to determine the driving mode and define an approach to predicting and overbounding future vehicle trajectories.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 19,
      "context" : "In [19] and [20], k-means clustering is used to determine the driving mode and define an approach to predicting and overbounding future vehicle trajectories.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 20,
      "context" : "In [21], a “cognitive architectures” approach, which is “a computational framework that incorporates builtin, well-tested parameters and constraints on cognitive and perceptual-motor processes,” is utilized for driver modeling.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 21,
      "context" : "In [22], lane change bear X iv :1 60 8.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 22,
      "context" : "The core ideas are synergistic with the framework of “semi network-form games,” [23], [24] and help us obtain the probable outcomes of a complex traffic scenario driven by multiple interactions.",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 23,
      "context" : "The core ideas are synergistic with the framework of “semi network-form games,” [23], [24] and help us obtain the probable outcomes of a complex traffic scenario driven by multiple interactions.",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 14,
      "context" : "Other game theoretic approaches, in particular, based on Stackelberg games, have been studied for application to vehicle highway driving problems in [15] and [16].",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 15,
      "context" : "Other game theoretic approaches, in particular, based on Stackelberg games, have been studied for application to vehicle highway driving problems in [15] and [16].",
      "startOffset" : 158,
      "endOffset" : 162
    }, {
      "referenceID" : 24,
      "context" : "The latter are considered in [25] for Hybrid Electric Vehicle (HEV) energy management where the driver and the powertrain are considered to be two players in a game.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 25,
      "context" : "Indeed, an implementation of the proposed approach for a 50 player game can be found in [26], and scenarios with up to 30 vehicles are handled in this paper.",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 26,
      "context" : "Preliminary results have appeared in conference papers [27], [28].",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 27,
      "context" : "Preliminary results have appeared in conference papers [27], [28].",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 26,
      "context" : "Differently from [27], [28], in this paper, a) we incorporate a more realistic action space including harder brakes and faster accelerations; b) we develop a more realistic traffic model with more representative distance constraint violation rates via improvements in the reinforcement learning procedure; and c) we demonstrate that optimal parameter values for an autonomous vehicle control algorithm can be obtained using a cost function based on safety and performance.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 27,
      "context" : "Differently from [27], [28], in this paper, a) we incorporate a more realistic action space including harder brakes and faster accelerations; b) we develop a more realistic traffic model with more representative distance constraint violation rates via improvements in the reinforcement learning procedure; and c) we demonstrate that optimal parameter values for an autonomous vehicle control algorithm can be obtained using a cost function based on safety and performance.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 28,
      "context" : "A detailed explanation of this hierarchical modeling method is given in [29] and [30].",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 29,
      "context" : "A detailed explanation of this hierarchical modeling method is given in [29] and [30].",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 30,
      "context" : "For more details on RL, see [31].",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 31,
      "context" : "” In this work we employ the Jaakkola RL algorithm [32], which distinguishes itself from conventional approaches by guaranteeing to converge at least to a local maximum in terms of average rewards, when the problem is of POMDP type.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 31,
      "context" : "See [32] for further details.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 32,
      "context" : ", if this policy were executed for an infinite time [33].",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 29,
      "context" : "In experimental studies [30], it is shown that in human interactions, level-3 players are very rarely encountered and therefore in our results we trained policies up to and including level-2.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 9,
      "context" : "The Stackelberg policies and the decision tree policies that are compared in this study were originally developed in [10], [15] and [16].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 14,
      "context" : "The Stackelberg policies and the decision tree policies that are compared in this study were originally developed in [10], [15] and [16].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 15,
      "context" : "The Stackelberg policies and the decision tree policies that are compared in this study were originally developed in [10], [15] and [16].",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 14,
      "context" : "It is noted that in [15] and [16] vehicle dynamics are different than the ones used in this paper.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 15,
      "context" : "It is noted that in [15] and [16] vehicle dynamics are different than the ones used in this paper.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 9,
      "context" : "In [10], it was assumed that the environment evolves deterministically over a planning horizon, independently of the controlled vehicle’s actions.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 14,
      "context" : "This is also in agreement with the results in [15], [16] and [10].",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 15,
      "context" : "This is also in agreement with the results in [15], [16] and [10].",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 9,
      "context" : "This is also in agreement with the results in [15], [16] and [10].",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 29,
      "context" : "These percentages of various levels are assumed based on an experimental study conducted in [30].",
      "startOffset" : 92,
      "endOffset" : 96
    } ],
    "year" : 2016,
    "abstractText" : "Autonomous driving has been the subject of increased interest in recent years both in industry and in academia. Serious efforts are being pursued to address legal, technical and logistical problems and make autonomous cars a viable option for everyday transportation. One significant challenge is the time and effort required for the verification and validation of the decision and control algorithms employed in these vehicles to ensure a safe and comfortable driving experience. Hundreds of thousands of miles of driving tests are required to achieve a well calibrated control system that is capable of operating an autonomous vehicle in an uncertain traffic environment where multiple interactions between vehicles and drivers simultaneously occur. Traffic simulators where these interactions can be modeled and represented with reasonable fidelity can help decrease the time and effort necessary for the development of the autonomous driving control algorithms by providing a venue where acceptable initial control calibrations can be achieved quickly and safely before actual road tests. In this paper, we present a gametheoretic traffic model that can be used to 1) test and compare various autonomous vehicle decision and control systems and 2) calibrate the parameters of an existing control system. We demonstrate two example case studies, where, in the first case, we test and quantitatively compare two autonomous vehicle control systems in terms of their safety and performance, and, in the second case, we optimize the parameters of an autonomous vehicle control system, utilizing the proposed traffic model and simulation environment.",
    "creator" : "LaTeX with hyperref package"
  }
}