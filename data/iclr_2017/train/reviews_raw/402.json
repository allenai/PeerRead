{"conference": "ICLR 2017 conference submission", "title": "Hyperband: Bandit-Based Configuration Evaluation for Hyperparameter Optimization", "abstract": "Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters.  While recent approaches use Bayesian Optimization to adaptively select configurations, we focus on speeding up random search through adaptive resource allocation.  We present Hyperband,  a novel algorithm for hyperparameter optimization that is simple, flexible, and theoretically sound.  Hyperband is a principled early-stoppping method that adaptively allocates a predefined resource, e.g., iterations, data samples or number of features, to randomly sampled configurations.  We compare Hyperband with state-of-the-art Bayesian Optimization methods on several hyperparameter optimization problems.  We observe that Hyperband can provide over an order of magnitude speedups over competitors on a variety of neural network and kernel-based learning problems.", "reviews": [{"is_meta_review": true, "comments": "This paper discusses Hyperband, an extension of successive halving by Jamieson & Talwalkar (AISTATS 2016). Successive halving is a very nice algorithm that starts evaluating many configurations and repeatedly cuts off the current worst half to explore many configuration for a limited budget.\n\nHaving read the paper for the question period and just rereading it again, I am now not entirely sure what its contribution is meant to be: the only improvement of Hyperband vs. successive halving is in the theoretical worst case bounds (not more than 5x worse than random search), but you can (a) trivially obtain that bound by using a fifth of your time for running random configurations to completion and (b) the theoretical analysis to show this is said to be beyond the scope of the paper. That makes me wonder whether the theoretical results are the contribution of this paper, or whether they are the subject of a different paper and the current paper is mostly an empirical study of the method?\nI hope to get a response by the authors and see this made clearer in an updated version of the paper.\n\nIn terms of experiments, the paper fails to show a case where Hyperband actually performs better than the authors' previous algorithm successive halving with its most agressive setting of bracket b=4. Literally, in every figure, bracket b=4 is at least as good (and sometimes substantially better) than Hyperband. That makes me think that in practice I would prefer successive halving with b=4 over Hyperband. (And if I really want Hyperband's guarantee of not being more than 5x worse than random search I can run random search on a fifth of my machines.) \nThe experiments also compare to some Bayesian optimization methods, but not to the most relevant very closely related Multi-Task Bayesian Optimization methods that have been dominating effective methods for deep learning in that area in the last 3 years: \"Multi-Task Bayesian Optimization\" by Swersky, Snoek, and Adams (2013) already showed 5x speedups for deep learning by starting with smaller datasets, and there have been several follow-up papers showing even larger speedups. \n\nGiven that this prominent work on multitask Bayesian optimization exists, I also think the introduction, which sells Hyperband as a very new approach to hyperparameter optimization is misleading. I would've much preferred a more down-to-earth pitch that says \"configuration evaluation\" has been becoming a very important feature in hyperparameter optimization, including Bayesian optimization, that sometimes yields very large speedups (this can be quantified by examples from existing papers) and this paper adds some much-needed theoretical understanding to this and demonstrates how important configuration evaluation is even in the simplest case of being used with random search. I think this could be done easily and locally by adding a paragraph to the intro.\n\nAs another point regarding novelty, I think the authors should make clear that approaches for adaptively deciding how many resources to use for which evaluation have been studied for (at least) 23 years in the ML community -- see Maron & Moore, NIPS 1993: \"Hoeffding Races: Accelerating Model Selection Search for Classification and Function Approximation\" (", "IS_META_REVIEW": true}, {"TITLE": "ICLR committee final decision", "OTHER_KEYS": "ICLR 2017 pcs", "comments": "This paper presents a simple strategy for hyperparameter optimization that gives strong empirical results. The reviewers all agreed that the paper should be accepted and that it would be interesting and useful to the ICLR community. However, they did have strong reservations about the claims made in the paper and one reviewer stated that their accept decision was conditional on a better treatment of the related literature. \n \n While it is natural for authors to argue for the advantages of their approach over existing methods, some of the claims made are unfounded. For example, the claim that the proposed method is guaranteed to converge while \"methods that rely on these heuristics are not endowed with any\n theoretical consistency guarantees\" is weak. Any optimization method can trivially add this guarantee by adopting a simple strategy of adding a random experiment 1/n of the time (in fact, SMAC does this I believe). This claim is true of random search compared to gradient based optimization on non-convex functions as well, yet no one optimizes their deep nets via random search. Also, the authors claim to compare to state-of-the-art in hyperparameter optimization but all the comparisons are to algorithms published in either 2011 or 2012. Four years of continued research on the subject are ignored (e.g. methods in Bayesian optimization for hyperparameter tuning have evolved considerably since 2012 - see e.g. the work of Miguel Hern\u2021ndez Lobato, Matthew Hoffman, Nando de Freitas, Ziyu Wang, etc.). It's understood that it is difficult to compare to the latest literature (and the authors state that they had trouble running recent algorithms), but one can't claim to compare to state-of-the-art without actually comparing to state-of-the-art.\n \n Please address the reviewers concerns and tone down the claims of the paper.", "IS_META_REVIEW": false, "DATE": "06 Feb 2017", "is_meta_review": false}, {"TITLE": "Inconclusive comparison to CVST ", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "Given the authors' discussion of CVST, I had expected Hyperband to do much better, but their experiment does not show that: \n- Despite Hyperband being an anytime algorithm, the authors ran it much shorter than CVST and got consistently worse mean results. Maybe not far worse, but no characteristics of the task are presented, so one might already get results within the error bars of CVST by picking a random configuration at no cost at all ... Why not run Hyperband as long as CVST and then compare apples with apples?\n\n- Also, for this experiment, the authors ran Hyperband with a different \\eta than for all other experiments. This begs the question: How much do you need to tune the method to work? What would be the result of using the same \\eta=4 as elsewhere?\n", "IS_META_REVIEW": false, "DATE": "19 Jan 2017", "is_meta_review": false}, {"TITLE": "Response to Reviewer Comments", "OTHER_KEYS": "Lisha Li", "comments": "We thank the reviewers for providing thoughtful comments and feedback for our paper.  Below please find our responses to these comments, grouped by theme. \n\n***Main Contribution***\n\nTo borrow the words of AnonReviewer1, the contribution of this paper is a \"very simple method [with] great empirical results for several deep learning tasks\u201d that is also endowed with theoretical guarantees. While we briefly allude to the theoretical properties of Hyperband in Section 3, a thorough theoretical treatment is beyond the scope of the paper (see the arXiv version for detailed theoretical analysis).  We have updated the introduction to clarify our contributions in this work.\n\n***Related Work***\n\nWe did not intend to suggest that optimizing configuration evaluation is a new idea and recognize that there is a long line of work in this area that stretches back many years.  To address AnonReviewer1\u2019s concerns, we moved the related work to section 2, expanded on existing configuration evaluation approaches, and added further discussion of existing approaches that combine configuration selection and evaluation.  Also, as per the suggestion of AnonReviewer2, we have added a reference to Bergstra & Bengio 2012 for the random search baseline.  \n\n***Parallelism***\n\nThe reviewers are correct that there are non-trivial design decisions in parallelizing Hyperband, and the updated version of the paper only talks about this topic as an interesting and important avenue of future research.\n\n***Worst Case Bound and Theoretical Properties***\n\n- \u201cIt's only [Hyperband\u2019s] worst-case analysis that makes no assumption [on the convergence behavior]\u201d\n\nHyperband is endowed with both worst-case and sample complexity guarantees, and neither of these results rely on any assumptions on the shape or rate of convergence of the validation error. In both cases, our only assumption is that the validation error eventually converges.\n\n- \u201conly improvement of Hyperband vs. successive halving is in the theoretical worst case bounds (not more than 5x worse than random search), but you can (a) trivially obtain that bound by using a fifth of your time for running random configurations to completion\u201d; \n\nHyperband crucially extends the previous SH work by addressing the n vs B/n problem that we discuss in Section 3. Indeed, our assumption-free complexity results for Hyperband match the oracle SH results up to a log factor (which we discuss further in the next paragraph).  We also note that our current work also introduces a novel variant of SH amenable to the finite horizon, where the maximum budget per configuration is limited.\n\nAs a further note regarding this log factor, Hyperband\u2019s guarantee actually shows that it is no worse than 5x the best SH bracket, in hindsight. If the best bracket is 50x faster than uniform allocation with random search, then overall, Hyperband is at least 10x faster than uniform allocation. But we agree that the fall-back guarantee is somewhat pessimistic. Ideally, instead of looping over the different kinds of brackets, one would treat each bracket as a separate \u201cmeta-arm\u201d of an outerloop bandit algorithm so that we could claim something like \u201cthe sub-optimal brackets are only played a finite number of times.\u201d However, this remains as future work.\n\n***Empirical Studies***\n\n- \u201cFigure 3 run out far enough that the other methods have had time to converge in order to see what this gap between optimal and near-optimal really is\u201d; \u201cif random2x is shown, then I would also like to see SMAC2x, Spearmint2x, TPE2x...it would be worth seeing 3x, 10x, and so forth.\u201d\n\nThe experiments in Section 4 are resource intensive and we chose our comparison set to be as informative as possible given the high cost of running these experiments (the total cost was over 10k in EC2 credits and the CNN experiments took over 10k GPU hours).  That said, we agree that extending the results would be interesting.  To this end, we are currently running experiments to extend the chart for CIFAR-10 trained using CNN to twice the current x-axis range to bring all competitors closer to convergence.  We will update the paper once the additional trials are complete.\n\n- \u201cThe experiments also compare to some Bayesian optimization methods, but not to the most relevant very closely related Multi-Task Bayesian Optimization\u201d\n\nWe did not compare to Multi-Task Bayesian Optimization for two main reasons: (1) the method mainly addresses the problem of transfer learning/meta-learning; and (2) in its simplest form, it may suffer from the \u201cn vs B/n\u201d problem since trying several subset sizes in conjunction exponentially increases the number of MTBO\u2019s own hyperparameters. Instead, we opted to use the early stopping method presented in Domhan et. al. 2015 to get an idea of how combined configuration selection and configuration evaluation approaches would perform. We have however added a citation for MTBO in our related work.\n\n- \u201cI am looking forward to seeing the details on these [CVST] experiments\u201d\n\nWe have added a comparison CVST to the updated paper (see Section 2 and also Appendix A.1).  \n\n- \u201cbracket b=4 is at least as good (and sometimes substantially better) than Hyperband.\u201d \n\nFor the deep learning and kernel experiments studied in Section 4, bracket s=4 does indeed perform very well and one could conceivably just run SH with that particular point in the n vs B/n tradeoff. However, in the LeNet experiment discussed in Section 3, Figure 2 shows that bracket s=3 performed the best. In practice, if meta-data or previous experience suggests that a certain n vs B/n tradeoff is likely to work well in practice, one can exploit this information and simply run SH with that tradeoff. However, oftentimes, this a priori knowledge is not available, thus motivating the use of Hyperband. Indeed, our empirical results demonstrate the efficacy of Hyperband in this standard setting where the choice of n vs B/n is unknown.", "IS_META_REVIEW": false, "DATE": "11 Jan 2017", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "interesting extension to successive halving, still looking forward to the parallel asynchronous version", "is_meta_review": false, "comments": "This was an interesting paper. The algorithm seems clear, the problem well-recognized, and the results are both strong and plausible.\n\nApproaches to hyperparameter optimization based on SMBO have struggled to make good use of convergence during training, and this paper presents a fresh look at a non-SMBO alternative (at least I thought it did, until one of the other reviewers pointed out how much overlap there is with the previously published successive halving algorithm - too bad!). Still, I'm excited to try it. I'm cautiously optimistic that this simple alternative to SMBO may be the first advance to model search for the skeptical practitioner since the case for random search > grid search (", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "23 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "A nice paper, just needs to relate to the existing literature better", "is_meta_review": false, "comments": "This paper discusses Hyperband, an extension of successive halving by Jamieson & Talwalkar (AISTATS 2016). Successive halving is a very nice algorithm that starts evaluating many configurations and repeatedly cuts off the current worst half to explore many configuration for a limited budget.\n\nHaving read the paper for the question period and just rereading it again, I am now not entirely sure what its contribution is meant to be: the only improvement of Hyperband vs. successive halving is in the theoretical worst case bounds (not more than 5x worse than random search), but you can (a) trivially obtain that bound by using a fifth of your time for running random configurations to completion and (b) the theoretical analysis to show this is said to be beyond the scope of the paper. That makes me wonder whether the theoretical results are the contribution of this paper, or whether they are the subject of a different paper and the current paper is mostly an empirical study of the method?\nI hope to get a response by the authors and see this made clearer in an updated version of the paper.\n\nIn terms of experiments, the paper fails to show a case where Hyperband actually performs better than the authors' previous algorithm successive halving with its most agressive setting of bracket b=4. Literally, in every figure, bracket b=4 is at least as good (and sometimes substantially better) than Hyperband. That makes me think that in practice I would prefer successive halving with b=4 over Hyperband. (And if I really want Hyperband's guarantee of not being more than 5x worse than random search I can run random search on a fifth of my machines.) \nThe experiments also compare to some Bayesian optimization methods, but not to the most relevant very closely related Multi-Task Bayesian Optimization methods that have been dominating effective methods for deep learning in that area in the last 3 years: \"Multi-Task Bayesian Optimization\" by Swersky, Snoek, and Adams (2013) already showed 5x speedups for deep learning by starting with smaller datasets, and there have been several follow-up papers showing even larger speedups. \n\nGiven that this prominent work on multitask Bayesian optimization exists, I also think the introduction, which sells Hyperband as a very new approach to hyperparameter optimization is misleading. I would've much preferred a more down-to-earth pitch that says \"configuration evaluation\" has been becoming a very important feature in hyperparameter optimization, including Bayesian optimization, that sometimes yields very large speedups (this can be quantified by examples from existing papers) and this paper adds some much-needed theoretical understanding to this and demonstrates how important configuration evaluation is even in the simplest case of being used with random search. I think this could be done easily and locally by adding a paragraph to the intro.\n\nAs another point regarding novelty, I think the authors should make clear that approaches for adaptively deciding how many resources to use for which evaluation have been studied for (at least) 23 years in the ML community -- see Maron & Moore, NIPS 1993: \"Hoeffding Races: Accelerating Model Selection Search for Classification and Function Approximation\" (", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Good extension of successive halving and random search", "is_meta_review": false, "comments": "This paper presents Hyperband, a method for hyperparameter optimization where the model is trained by gradient descent or some other iterative scheme. The paper builds on the successive halving + random search approach of Jamieson and Talwalkar and addresses the tradeoff between training fewer models for a longer amount of time, or many models for a shorter amount of time. Effectively, the idea is to perform multiple rounds of successive halving, starting from the most exploratory setting, and then in each round exponentially decreasing the number of experiments, but granting them exponentially more resources. In contrast to other recent papers on this topic, the approach here does not rely on any specific model of the underlying learning curves and therefore makes fewer assumptions about the nature of the model. The results seem to show that this approach can be highly effective, often providing several factors of speedup over sequential approaches.\n\nOverall I think this paper is a good contribution to the hyperparameter optimization literature. It\u2019s relatively simple to implement, and seems to be quite effective for many problems. It seems like a natural extension of the random search methodology to the case of early stopping. To me, it seems like Hyperband would be most useful on problems where a) random search itself is expected to perform well and b) the computational budget is sufficiently constrained so that squeezing out the absolute best performance is not feasible and near-optimal performance is sufficient. I would personally like to see the plots in Figure 3 run out far enough that the other methods have had time to converge in order to see what this gap between optimal and near-optimal really is (if there is one).\n\nI\u2019m not sure I agree with the use of random2x as a baseline. I can see why it\u2019s a useful comparison because it demonstrates the benefit of parallelism over sequential methods, but virtually all of these other methods also have parallel extensions. I think if random2x is shown, then I would also like to see SMAC2x, Spearmint2x, TPE2x, etc. I also think it would be worth seeing 3x, 10x, and so forth and how Hyperband fares against these baselines.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "intuition around worst-case", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "IS_META_REVIEW": false, "DATE": "11 Dec 2016", "is_meta_review": false}, {"TITLE": "Lack of baselines for \"configuration evaluation\"", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "IS_META_REVIEW": false, "DATE": "03 Dec 2016", "is_meta_review": false}, {"TITLE": "Consistency and a question about Figure 2", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "is_meta_review": false}], "SCORE": 7, "authors": "Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, Ameet Talwalkar", "accepted": true, "id": ""}
