{
  "name" : "1704.03279.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Unfolding and Shrinking Neural Machine Translation Ensembles",
    "authors" : [ "Felix Stahlberg" ],
    "emails" : [ "fs439@cam.ac.uk", "wjb31@cam.ac.uk" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The top systems in recent machine translation evaluation campaigns on various language pairs use ensembles of a number of NMT systems (Bojar et al., 2016; Sennrich et al., 2016a; Chung et al., 2016; Neubig, 2016; Wu et al., 2016; Cromieres et al., 2016; Durrani et al., 2017). Ensembling (Dietterich, 2000; Hansen and Salamon, 1990) of neural networks is a simple yet very effective technique to improve the accuracy of NMT.\nThe decoder makes use of K NMT networks which are either trained independently (Sutskever et al., 2014; Chung et al., 2016; Neubig, 2016; Wu et al., 2016) or share some amount of training iterations (Sennrich et al., 2016b,a; Cromieres et al., 2016; Durrani et al., 2017). The ensemble decoder computes predictions from each of the individual models which are then combined using the arithmetic average (Sutskever et al., 2014) or the geometric average (Cromieres et al., 2016).\nEnsembling consistently outperforms single NMT by a large margin. However, the decoding speed is significantly worse since the decoder needs to apply K NMT models rather than only one. Therefore, a recent line of research transfers the idea of knowledge distillation (Bucilu et al., 2006; Hinton et al., 2014) to NMT and trains a smaller network (the student) by minimizing the cross-entropy to the output of the ensemble system (the teacher) (Kim and Rush, 2016; Freitag et al., 2017). This paper presents an alternative to knowledge distillation as we aim to speed up decoding to be comparable to single NMT while retaining the boost in translation accuracy from the ensemble. In a first step, we describe how to construct a single large neural network which imitates the output of an ensemble of multiple networks with the same topology. We will refer to this process as unfolding. GPU-based decoding with the unfolded network is often much faster than ensemble decoding since more work can be done on the GPU. In a second step, we explore methods to reduce the size of the unfolded network. This idea is justified by the fact that ensembled neural networks are often over-parameterized and have a large degree of redundancy (LeCun et al., 1989; Hassibi et al., 1993; Srinivas and Babu, 2015). Shrinking the unfolded network leads to a smaller model which consumes less space on the disk and in the memory; a crucial factor on mobile devices. More importantly, the ar X iv :1 70 4. 03 27 9v 2 [ cs .C L\n] 2\n1 Ju\nl 2 01\n7\ndecoding speed on all platforms benefits greatly from the reduced number of neurons. We find that the dimensionality of linear embedding layers in the NMT network can be reduced heavily by lowrank matrix approximation based on singular value decomposition (SVD). This suggest that high dimensional embedding layers may be needed for training, but do not play an important role for decoding. The NMT network, however, also consists of complex layers like gated recurrent units (Cho et al., 2014, GRUs) and attention (Bahdanau et al., 2015). Therefore, we introduce a novel algorithm based on linear combinations of neurons which can be applied either during training (data-bound) or directly on the weight matrices without using training data (data-free). We report that with a mix of the presented shrinking methods we are able to reduce the size of the unfolded network to the size of the single NMT network while keeping the boost in BLEU score from the ensemble. Depending on the aggressiveness of shrinking, we report either a gain of 2.2 BLEU at the same decoding speed, or a 3.4× CPU decoding speed up with only a minor drop in BLEU compared to the original single NMT system. Furthermore, it is often much easier to stage a single NMT system than an ensemble in a commercial MT workflow, and it is crucial to be able to optimize quality at specific speed and memory constraints. Unfolding and shrinking address these problems directly."
    }, {
      "heading" : "2 Unfolding K Networks into a Single Large Neural Network",
      "text" : "The first concept of our approach is called unfolding. Unfolding is an alternative to ensembling of multiple neural networks with the same topology. Rather than averaging their predictions, unfolding constructs a single large neural net out of the indi-\nvidual models which has the same number of input and output neurons but larger inner layers. Our main motivation for unfolding is to obtain a single network with ensemble level performance which can be shrunk with the techniques in Sec. 3.\nSuppose we ensemble two single layer feedforward neural nets as shown in Fig. 1. Normally, ensembling is implemented by performing an isolated forward pass through the first network (Fig. 1(a)), another isolated forward pass through the second network (Fig. 1(b)), and averaging the activities in the output layers of both networks. This can be simulated by merging both networks into a single large network as shown in Fig. 1(c). The first neurons in the hidden layer of the combined network correspond to the hidden layer in the first single network, and the others to the hidden layer of the second network. A single pass through the combined network yields the same output as the ensemble if the output layer is linear (up to a factor 2). The weight matrices in the unfolded network can be constructed by stacking the corresponding weight matrices (either horizontally or vertically) in network 1 and 2. This kind of aggregation of multiple networks with the same topology is not only possible for single-layer feedforward architectures but also for complex networks consisting of multiple GRU layers and attention.\nFor a formal description of unfolding we address layers with indices d = 0, 1, . . . , D. The special layer 0 has a single neuron for modelling bias vectors. Layer 1 holds the input neurons and layer D is the output layer. We denote the size of a layer in the individual models as s(d). When combining K networks, the layer size s′(d) in the unfolded network is increased by factor K if d is an inner layer, and equal to s(d) if d is the in-\nput or output layer. We denote the weight matrix between two layers d1, d2 ∈ [0, D] in the k-th individual model (k ∈ [1,K]) as Wk(d1, d2) ∈ Rs(d1)×s(d2), and the corresponding weight matrix in the unfolded network as W ′(d1, d2) ∈ Rs′(d1)×s′(d2). We explicitly allow d1 and d2 to be non-consecutive or reversed to be able to model recurrent networks. We use the zero-matrix if layers d1 and d2 are not connected. The construction of the unfolded weight matrix W ′(d1, d2) from the individual matrices Wk(d1, d2) depends on whether the connected layers are inner layers or not. The complete formula is listed in Fig. 2.\nUnfolded NMT networks approximate but do not exactly match the output of the ensemble due to two reasons. First, the unfolded network synchronizes the attentions of the individual models. Each decoding step in the unfolded network computes a single attention weight vector. In contrast, ensemble decoding would compute one attention weight vector for each of the K input models. A second difference is that the ensemble decoder first applies the softmax at the output layer, and then averages the prediction probabilities. The unfolded network averages the neuron activities (i.e. the logits) first, and then applies the softmax function. Interestingly, as shown in Sec. 4, these differences do not have any impact on the BLEU score but yield potential speed advantages of unfolding since the computationally expensive softmax layer is only applied once."
    }, {
      "heading" : "3 Shrinking the Unfolded Network",
      "text" : "After constructing the weight matrices of the unfolded network we reduce the size of it by iteratively shrinking layer sizes. In this section we denote the incoming weight matrix of the layer to\nshrink as U ∈ Rmin×m and the outgoing weight matrix as V ∈ Rm×mout . Our procedure is inspired by the method of Srinivas and Babu (2015). They propose a criterion for removing neurons in inner layers of the network based on two intuitions. First, similarly to Hebb’s learning rule, they detect redundancy by the principle neurons which fire together, wire together. If the incoming weight vectors U:,i and U:,j are exactly the same for two neurons i and j, we can remove the neuron j and add its outgoing connections to neuron i (Vi,: ← Vi,: + Vj,:) without changing the output.1 This holds since the activity in neuron j will always be equal to the activity in neuron i. In practice, Srinivas and Babu use a distance measure based on the difference of the incoming weight vectors to search for similar neurons as exact matches are very rare.\nThe second intuition of the criterion used by Srinivas and Babu (2015) is that neurons with small outgoing weights contribute very little overall. Therefore, they search for a pair of neurons i, j ∈ [1,m] according the following term and remove the j-th neuron.2\nargmin i,j∈[1,m]\n||U:,i − U:,j ||22||Vj,:||22 (1)\nNeuron j is selected for removal if (1) there is another neuron i which has a very similar set of incoming weights and if (2) j has a small outgoing weight vector. Their criterion is data-free since\n1We denote the i-th row vector of a matrix A with Ai,: and the i-th column vector as A:,i.\n2Note that the criterion in Eq. 1 generalizes the criterion of Srinivas and Babu (2015) to multiple outgoing weights. Also note that Srinivas and Babu (2015) propose some heuristic improvements to this criterion. However, these heuristics did not work well in our NMT experiments.\nit does not require any training data. For further details we refer to Srinivas and Babu (2015)."
    }, {
      "heading" : "3.1 Data-Free Neuron Removal",
      "text" : "Srinivas and Babu (2015) propose to add the outgoing weights of j to the weights of a similar neuron i to compensate for the removal of j. However, we have found that this approach does not work well on NMT networks. We propose instead to compensate for the removal of a neuron by a linear combination of the remaining neurons in the layer. Data-free shrinking assumes for the sake of deriving the update rule that the neuron activation function is linear. We now ask the following question: How can we compensate as well as possible for the loss of neuron j such that the impact on the output of the whole network is minimized? Datafree shrinking represents the incoming weight vector of neuron j (U:,j) as linear combination of the incoming weight vectors of the other neurons. The linear factors can be found by satisfying the following linear system:\nU:,¬jλ = U:,j (2)\nwhere U:,¬j is matrix U without the j-th column. In practice, we use the method of ordinary least squares to find λ because the system may be overdetermined. The idea is that if we mix the outputs of all neurons in the layer by the λ-weights, we get the output of the j-th neuron. The row vector Vj,: contains the contributions of the j-th neuron to each of the neurons in the next layer. Rather than using these connections, we approximate their effect by adding some weight to the outgoing connections of the other neurons. How much weight depends on λ and the outgoing weights Vj,:. The factor Dk,l which we need to add to the outgoing connection of the k-th neuron to compensate for the loss of the j-th neuron on the l-th neuron in the next layer is:\nDk,l = λkVj,l (3)\nTherefore, the update rule for V is:\nV ← V +D (4)\nIn the remainder we will refer to this method as data-free shrinking. Note that we recover the update rule of Srinivas and Babu (2015) by setting λ to the i-th unit vector. Also note that the error introduced by our shrinking method is due to the\nfact that we ignore the non-linearity, and that the solution for λ may not be exact. The method is error-free on linear layers as long as the residuals of the least-squares analysis in Eq. 2 are zero.\nGRU layers The terminology of neurons needs some further elaboration for GRU layers which rather consist of update and reset gates and states (Cho et al., 2014). On GRU layers, we treat the states as neurons, i.e. the j-th neuron refers to the j-th entry in the GRU state vector. Input connections to the gates are included in the incoming weight matrix U for estimating λ in Eq. 2. Removing neuron j in a GRU layer means deleting the j-th entry in the states and both gate vectors."
    }, {
      "heading" : "3.2 Data-Bound Neuron Removal",
      "text" : "Although we find our data-free approach to be a substantial improvement over the methods of Srinivas and Babu (2015) on NMT networks, it still leads to a non-negligible decline in BLEU score when applied to recurrent GRU layers. Our data-free method uses the incoming weights to identify similar neurons, i.e. neurons expected to have similar activities. This works well enough for simple layers, but the interdependencies between the states and the gates inside gated layers like GRUs or LSTMs are complex enough that redundancies cannot be found simply by looking for similar weights. In the spirit of Babaeizadeh et al. (2016), our data-bound version records neuron activities during training to estimate λ. We compensate for the removal of the j-th neuron by using a linear combination of the output of remaining neurons with similar activity patterns. In each layer, we prune 40 neurons each 450 training iterations until the target layer size is reached. Let A be the matrix which holds the records of neuron activities in the layer since the last removal. For example, for the decoder GRU layer, a batch size of 80, and target sentence lengths of 20,A has 20 · 80 · 450 = 720K rows and m (the number of neurons in the layer) columns.3 Similarly to Eq. 2 we find interpolation weights λ using the method of least squares on the following linear system.\nA:,¬jλ = A:,j (5)\nThe update rule for the outgoing weight matrix is the same as for our data-free method (Eq. 4).\n3In practice, we use a random sample of 50K rows rather than the full matrix to keep the complexity of the leastsquares analysis under control.\nThe key difference between data-free and databound shrinking is the way λ is estimated. Datafree shrinking uses the similarities between incoming weights, and data-bound shrinking uses neuron activities recorded during training. Once we select a neuron to remove, we estimate λ, compensate for the removal, and proceed with the shrunk network. Both methods are prior to any decoding and result in shrunk parameter files which are then loaded to the decoder. Both methods remove neurons rather than single weights.\nThe data-bound algorithm runs gradient-based optimization on the unfolded network. We use the AdaGrad (Duchi et al., 2011) step rule, a small learning rate of 0.0001, and aggressive step clipping at 0.05 to avoid destroying useful weights which were learned in the individual networks prior to the construction of the unfolded network.\nOur data-bound algorithm uses a data-bound version of the neuron selection criterion in Eq. 1 which operates on the activity matrix A. We search for the pair i, j ∈ [1,m] according the following term and remove neuron j.\nargmin i,j∈[1,m]\n||A:,i −A:,j ||22||A:,j ||22 (6)"
    }, {
      "heading" : "3.3 Shrinking Embedding Layers with SVD",
      "text" : "The standard attention-based NMT network architecture (Bahdanau et al., 2015) includes three linear layers: the embedding layer in the encoder, and the output and feedback embedding layers in the decoder. We have found that linear layers are particularly easy to shrink using low-rank matrix approximation. As before we denote the incoming weight matrix as U ∈ Rmin×m and the outgoing weight matrix as V ∈ Rm×mout . Since the layer is linear, we could directly connect the previous layer with the next layer using the product of both weight matrices X = U · V . However, X may be very large. Therefore, we approximate X as a product of two low rank matrices Y ∈ Rmin×m′ and Z ∈ Rm′×mout (X ≈ Y Z) where m′ m is the desired layer size. A very common way to find such a matrix factorization is using truncated singular value decomposition (SVD). The layer is eventually shrunk by replacing U with Y and V with Z."
    }, {
      "heading" : "4 Results",
      "text" : "The individual NMT systems we use as source for constructing the unfolded networks are trained us-\ning AdaDelta (Zeiler, 2012) on the Blocks/Theano implementation (van Merriënboer et al., 2015; Bastien et al., 2012) of the standard attentionbased NMT model (Bahdanau et al., 2015) with: 1000 dimensional GRU layers (Cho et al., 2014) in both the decoder and bidrectional encoder; a single maxout output layer (Goodfellow et al., 2013); and 620 dimensional embedding layers. We follow Sennrich et al. (2016b) and use subword units based on byte pair encoding rather than words as modelling units. Our SGNMT decoder (Stahlberg et al., 2017)4 with a beam size of 12 is used in all experiments. Our primary corpus is the Japanese-English (Ja-En) ASPEC data set (Nakazawa et al., 2016). We select a subset of 500K sentence pairs to train our models as suggested by Neubig et al. (2015). We report cased BLEU scores calculated with Moses’ multi-bleu.pl to be strictly comparable to the evaluation done in the Workshop of Asian Translation (WAT). We also apply our method to the WMT data set for English-German (En-De), using the news-test2014 as a development set, and keeping news-test2015 and news-test2016 as test sets. En-De BLEU scores are computed using mteval-v13a.pl as in the WMT evaluation. We set the vocabulary sizes to 30K for Ja-En and 50K for En-De. We also report the size factor for each model which is the total number of model parameters (sum of all weight matrix sizes) divided by the number of parameters in the original NMT network (86M for Ja-En and 120M for EnDe). We choose a widely used, simple ensembling method (prediction averaging) as our baseline. We feel that the prevalence of this method makes it a reasonable baseline for our experiments.\nShrinking the Unfolded Network First, we investigate which shrinking methods are effective for which layers. Tab. 1 summarizes our results on a 2-unfold network for Ja-En, i.e. two separate NMT networks are combined in a single large network as described in Sec. 2. The layers in the combined network are shrunk to the size of the original networks using the methods discussed in Sec. 3.\nShrinking the linear embedding layers with SVD (Sec. 3.3) is very effective. The unfolded model with shrunk embedding layers performs at the same level as the ensemble (compare rows (b) and (c)). In our initial experiments, we applied the method of Srinivas and Babu (2015) to\n4‘vanilla’ decoding strategy\nshrink the other layers, but their approach performed very poorly on this kind of network: the BLEU score dropped down to 15.5 on the development set when shrinking all layers except the decoder maxout and embedding layers, and to 9.9 BLEU when applying their method only to embedding layers.5 Row (e) in Tab. 1 shows that our data-free algorithm from Sec. 3.1 is better suited for shrinking the GRU and attention layers, leading to a drop of only 1 BLEU point compared to the ensemble (b) (i.e. 0.8 BLEU better than the single system (a)). However, using the data-bound version of our shrinking algorithm (Sec. 3.2) for the GRU layers performs best.6 The shrunk model yields about the same BLEU score as the ensemble on the test set (25.2 in (b) and 25.3 in (f)). Shrinking the maxout layer remains more of a challenge (rows (g) and (h)), but the number of parameters in this layer is small. Therefore, shrinking all layers except the maxout layer leads to almost the same number of parameters (factor 1.05 in row (f)) as the original NMT network (a), and thus to a similar storage size, memory consumption, and decoding speed, but with a 1.8 BLEU gain. Based on these results we fix the shrinking method used for each layer for all remaining experiments as follows: We shrink linear embedding layers with our SVD-based method, GRU layers with our databound method, the attention layer with our datafree method, and do not shrink the maxout layer.\nOur data-bound algorithm from Sec. 3.2 has two mechanisms to compensate for the removal of a neuron. First, we use a linear combination of the remaining neurons to update the outgoing weight matrix by imitating its activations (Eq. 4). Second, stochastic gradient descent (SGD) fine-tunes all\n5Results with the original method of Srinivas and Babu (2015) are not included in Tab. 1.\n6If we apply different methods to different layers of the same network, we first apply SVD-based shrinking, then the data-free method, and finally the data-bound method.\nweights during this process. Tab. 2 demonstrates that both mechanisms are crucial for minimizing the effect of shrinking on the BLEU score.\nDecoding Speed Our testing environment is an Ubuntu 16.04 with Linux 4.4.0 kernel, 32 GB RAM, an Intel R© Core i7-6700 CPU at 3.40 GHz and an Nvidia GeForce GTX Titan X GPU. CPU decoding uses a single thread. We used the first 500 sentences of the Ja-En WAT development set for the time measurements.\nOur results in Tab. 3 show that decoding with ensembles (rows (b) and (e)) is slow: combining the predictions of the individual models on the CPU is computationally expensive, and ensemble decoding requires K passes through the softmax layer which is also computationally expensive. Unfolding the ensemble into a single network and shrinking the embedding and attention layers improves the runtimes on the GPU significantly without noticeable impact on BLEU (rows (c) and (f)). This can be attributed to the fact that unfolding can reduce the communication overhead between CPU and GPU. Comparing rows (d) and (g) with row (a) reveals that shrinking the unfolded networks even further speeds up CPU and GPU decoding almost to the level of single system decoding. However, more aggressive shrinking yields a BLEU score of 25.3 when combining three systems (row (g)) – 1.8 BLEU better than the single system, but 0.6 BLEU worse than the 3-\nensemble. Therefore, we will investigate the impact of shrinking on the different layers in the next sections more thoroughly.\nDegrees of Redundancy in Different Layers We applied our shrinking methods to isolated layers in the 2-Unfold network of Tab. 1 (f). Fig. 3 plots the BLEU score when isolated layers are shrunk even below their size in the original NMT network. The attention layer is very robust against shrinking and can be reduced to 100 neurons (10% of the original size) without impacting the BLEU score. The embedding layers can be reduced to 60% but are sensitive to more aggressive pruning. Shrinking the GRU layers affects the BLEU score the most but still outperforms the single system when the GRU layers are shrunk to 30%.\nAdjusting the Target Sizes of Layers Based on our previous experiments we revise our approach to shrink the 3-Unfold system in Tab. 3. Instead\nof shrinking all layers except the maxout layer to the same degree, we adjust the aggressiveness of shrinking for each layer. We suggest three different setups (Normal, Small, and Tiny) with the layer sizes specified in Tab. 4. 3-Unfold-Normal has the same number of parameters as the original NMT networks (size factor: 1.0), 3-UnfoldSmall is only half their size (size factor: 0.5), and 3-Unfold-Tiny reduces the size by two thirds (size factor: 0.33). When comparing rows (a) and (c) in Tab. 5 we observe that 3-Unfold-Normal yields a gain of 2.2 BLEU with respect to the original single system and a slight improvement in decoding speed at the same time.7 Networks with the size factor 1.0 like 3-Unfold-Normal are very likely to yield about the same decoding speed as the Single network regardless of the decoder implementation, machine learning framework, and hardware. Therefore, we think that similar results are possible on other platforms as well.\nCPU decoding speed directly benefits even more from smaller setups – 3-Unfold-Tiny is only 0.3 BLEU worse than Single but decoding on a single CPU is 3.4 times faster (row (a) vs. row (e) in Tab. 5). This is of great practical use: batch decoding with only two CPU threads surpasses production speed which is often set to 2000 words per minute (Beck et al., 2016). Our initial experiments in Tab. 6 suggest that the Normal setup is applicable to En-De as well, with substantial improve-\n7To validate that the gains come from ensembling and unfolding and not from the layer sizes in 3-Unfold-Normal we trained a network from scratch with the same dimensions. This network performed similarly to our Single system.\nments in BLEU compared to Single with about the same decoding speed."
    }, {
      "heading" : "5 Related Work",
      "text" : "The idea of pruning neural networks to improve the compactness of the models dates back more than 25 years (LeCun et al., 1989). The literature is therefore vast (Augasta and Kathirvalavakumar, 2013). One line of research aims to remove unimportant network connections. The connections can be selected for deletion based on the secondderivative of the training error with respect to the weight (LeCun et al., 1989; Hassibi et al., 1993), or by a threshold criterion on its magnitude (Han et al., 2015). See et al. (2016) confirmed a high degree of weight redundancy in NMT networks.\nIn this work we are interested in removing neurons rather than single connections since we strive to shrink the unfolded network such that it resembles the layout of an individual model. We argued in Sec. 4 that removing neurons rather than connections does not only improve the model size but also the memory footprint and decoding speed. As explained in Sec. 3.1, our data-free method is an extension of the approach by Srinivas and Babu (2015); our extension performs significantly better on NMT networks. Our data-bound method (Sec. 3.2) is inspired by Babaeizadeh et al. (2016) as we combine neurons with similar activities during training, but we use linear combinations of multiple neurons to compensate for the loss of a neuron rather than merging pairs of neurons.\nUsing low rank matrices for neural network compression, particularly approximations via SVD, has been studied widely in the literature (Denil et al., 2013; Denton et al., 2014; Xue et al., 2013; Prabhavalkar et al., 2016; Lu et al., 2016). These approaches often use low rank matrices to approximate a full rank weight matrix in the original network. In contrast, we shrink an entire linear layer by applying SVD on the product of the incoming and outgoing weight matrices (Sec. 3.3).\nIn this paper we mimicked the output of the high performing but cumbersome ensemble by constructing a large unfolded network, and shrank this\nnetwork afterwards. Another approach, known as knowledge distillation, uses the large model (the teacher) to generate soft training labels for the smaller student network (Bucilu et al., 2006; Hinton et al., 2014). The student network is trained by minimizing the cross-entropy to the teacher. This idea has been applied to sequence modelling tasks such as machine translation and speech recognition (Wong and Gales, 2016; Kim and Rush, 2016; Freitag et al., 2017). Our approach can be computationally more efficient as the training set does not have to be decoded by the large teacher network.\nJunczys-Dowmunt et al. (2016a; 2016b) reported gains from averaging the weight matrices of multiple checkpoints of the same training run. However, our attempts to replicate their approach were not successful. Averaging might work well when the behaviour of corresponding units is similar across networks, but that cannot be guaranteed when networks are trained independently."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We have described a generic method for improving the decoding speed and BLEU score of single system NMT. Our approach involves unfolding an ensemble of multiple systems into a single large neural network and shrinking this network by removing redundant neurons. Our best results on Japanese-English either yield a gain of 2.2 BLEU compared to the original single NMT network at about the same decoding speed, or a 3.4×CPU decoding speed up with only a minor drop in BLEU.\nThe current formulation of unfolding works for networks of the same topology as the concatenation of layers is only possible for analogous layers in different networks. Unfolding and shrinking diverse networks could be possible, for example by applying the technique only to the input and output layers or by some other scheme of finding associations between units in different models, but we leave this investigation to future work as models in NMT ensembles in current research usually have the same topology (Bojar et al., 2016; Sennrich et al., 2016a; Chung et al., 2016; Neubig, 2016; Wu et al., 2016; Durrani et al., 2017)."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was supported by the U.K. Engineering and Physical Sciences Research Council (EPSRC grant EP/L027623/1)."
    }, {
      "heading" : "Appendix: Probabilistic Interpretation of Data-Free and Data-Bound Shrinking",
      "text" : "Data-free and data-bound shrinking can be interpreted as setting the expected difference between network outputs before and after a removal operation to zero under different assumptions.\nFor simplicity, we focus our probabilistic treatment of shrinking on single layer feedforward networks. Such a network maps an input x ∈ Rmin to an output y ∈ Rmout . The l-th output yl is computed according the following equation\nyl = ∑\nk∈[1,m]\nσ(xuTk )Vk,l (7)\nwhere uk ∈ Rmin is the incoming weight vector of the k-th hidden neuron (denoted as U:,k in the main paper) and V ∈ Rm×mout the outgoing weight matrix of the m-dimensional hidden layer. We now remove the j-th neuron in the hidden layer and modify the outgoing weights to compensate for the removal:\ny′l = ∑\nk∈[1,m]\\{j}\nσ(xuTk )V ′ k,l (8)\nwhere y′l is the output after the removal operation and V ′ ∈ Rm×mout are the modified outgoing weights. Our goal is to choose V ′ such that the expected error introduced by removing neuron j is zero:\nEx(yl − y′l) = 0 (9)\nData-free shrinking Data-free shrinking makes two assumptions to satisfy Eq. 9. First, we assume that the incoming weight vector uj can be represented as linear combination of the other weight vectors.\nuj = ∑\nk∈[1,m]\\{j}\nλkuk (10)\nSecond, it assumes that the neuron activation function σ(·) is linear. Starting with Eqs. 7 and 8 we can write Ex(yl − y′l) as\nEx ( σ(xuTj )Vj,l + ∑ k∈[1,m]\\{j}\nσ(xuTk )(Vk,l − V ′k,l)︸ ︷︷ ︸ :=R\n)\nEq. 10 = Ex ( σ(x( ∑ k∈[1,m]\\{j} λkuk) T )Vj,l +R ) σ(·) lin. = Ex\n( ∑ k∈[1,m]\\{j} σ(xuTk )λkVj,l +R )\n= ∑\nk∈[1,m]\\{j}\nEx ( σ(xuTk ) ) (Vk,l − V ′k,l + λkVj,l)\nWe set this term to zero (and thus satisfy Eq. 9) by setting each component of the sum to zero.\n∀k ∈ [1,m] \\ {j} : V ′k,l = Vk,l + λkVj,l (11)\nThis condition is directly implemented by the update rule in our shrinking algorithm (Eq. 3 and 4).\nData-bound shrinking Data-bound shrinking does not require linearity in σ(·). It rather assumes that the expected value of the neuron activity j is a linear combination of the expected values of the other activities:\nEx(σ(xuTj )) = ∑\nk∈[1,m]\\{j}\nλkEx(σ(xuTk )) (12)\nEx(·) is estimated using importance sampling:\nÊx(σ(xuTk );X ) = 1 |X | ∑ x′∈X σ(x′uTk ) (13)\nIn practice, the samples inX are collected in the activity matrix A from Sec. 3.2. We can satisfy Eq. 9 by using the λ-values from Eq. 12, so that Ex(yl − y′l) becomes\nEqs. 7,8 = Ex ( σ(xuTj )Vj,l\n+ ∑\nk∈[1,m]\\{j}\nσ(xuTk )(Vk,l − V ′k,l) )\n= Ex(σ(xuTj )Vj,l) + ∑ k∈[1,m]\\{j} Ex(σ(xuTk ))(Vk,l − V ′k,l)\nEq. 12 = ∑ k∈[1,m]\\{j} Ex(σ(xuTk ))(Vk,l − V ′k,l + λkVj,l)\nAgain, we set this to zero using Eq. 11."
    } ],
    "references" : [ {
      "title" : "Pruning algorithms of neural networks – a comparative study",
      "author" : [ "M. Gethsiyal Augasta", "Thangairulappan Kathirvalavakumar." ],
      "venue" : "Central European Journal of Computer Science, 3(3):105–115.",
      "citeRegEx" : "Augasta and Kathirvalavakumar.,? 2013",
      "shortCiteRegEx" : "Augasta and Kathirvalavakumar.",
      "year" : 2013
    }, {
      "title" : "NoiseOut: A simple way to prune neural networks",
      "author" : [ "Mohammad Babaeizadeh", "Paris Smaragdis", "Roy H. Campbell." ],
      "venue" : "Proceedings of the 1st International Workshop on Efficient Methods for Deep Neural Networks (EMDNN).",
      "citeRegEx" : "Babaeizadeh et al\\.,? 2016",
      "shortCiteRegEx" : "Babaeizadeh et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "ICLR, Toulon, France.",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Theano: new features and speed improvements",
      "author" : [ "Frédéric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "David Warde-Farley", "Yoshua Bengio." ],
      "venue" : "NIPS, South Lake Tahoe, Nevada,",
      "citeRegEx" : "Bastien et al\\.,? 2012",
      "shortCiteRegEx" : "Bastien et al\\.",
      "year" : 2012
    }, {
      "title" : "Speedconstrained tuning for statistical machine translation using Bayesian optimization",
      "author" : [ "Daniel Beck", "Adrià de Gispert", "Gonzalo Iglesias", "Aurelien Waite", "Bill Byrne." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of",
      "citeRegEx" : "Beck et al\\.,? 2016",
      "shortCiteRegEx" : "Beck et al\\.",
      "year" : 2016
    }, {
      "title" : "Findings of the 2016 conference on machine translation",
      "author" : [ "Post", "Raphael Rubino", "Carolina Scarton", "Lucia Specia", "Marco Turchi", "Karin Verspoor", "Marcos Zampieri." ],
      "venue" : "Proceedings of the First Conference on Machine Translation, pages 131–",
      "citeRegEx" : "Post et al\\.,? 2016",
      "shortCiteRegEx" : "Post et al\\.",
      "year" : 2016
    }, {
      "title" : "Model compression",
      "author" : [ "Cristian Bucilu", "Rich Caruana", "Alexandru Niculescu-Mizil." ],
      "venue" : "Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 535–541. ACM.",
      "citeRegEx" : "Bucilu et al\\.,? 2006",
      "shortCiteRegEx" : "Bucilu et al\\.",
      "year" : 2006
    }, {
      "title" : "Learning phrase representations using RNN encoder–decoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Cho et al\\.,? 2014",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "A character-level decoder without explicit segmentation for neural machine translation",
      "author" : [ "Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1:",
      "citeRegEx" : "Chung et al\\.,? 2016",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2016
    }, {
      "title" : "Kyoto university participation to WAT 2016",
      "author" : [ "Fabien Cromieres", "Chenhui Chu", "Toshiaki Nakazawa", "Sadao Kurohashi." ],
      "venue" : "Proceedings of the 3rd Workshop on Asian Translation (WAT2016), pages 166–174, Osaka, Japan. The COLING 2016 Orga-",
      "citeRegEx" : "Cromieres et al\\.,? 2016",
      "shortCiteRegEx" : "Cromieres et al\\.",
      "year" : 2016
    }, {
      "title" : "Predicting parameters in deep learning",
      "author" : [ "Misha Denil", "Babak Shakibi", "Laurent Dinh", "Nando de Freitas" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Denil et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Denil et al\\.",
      "year" : 2013
    }, {
      "title" : "Exploiting linear structure within convolutional networks for efficient evaluation",
      "author" : [ "Emily L. Denton", "Wojciech Zaremba", "Joan Bruna", "Yann LeCun", "Rob Fergus." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 1269–1277.",
      "citeRegEx" : "Denton et al\\.,? 2014",
      "shortCiteRegEx" : "Denton et al\\.",
      "year" : 2014
    }, {
      "title" : "Ensemble methods in machine learning",
      "author" : [ "Thomas G. Dietterich." ],
      "venue" : "International workshop on multiple classifier systems, pages 1–15. Springer.",
      "citeRegEx" : "Dietterich.,? 2000",
      "shortCiteRegEx" : "Dietterich.",
      "year" : 2000
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "John Duchi", "Elad Hazan", "Yoram Singer." ],
      "venue" : "Journal of Machine Learning Research, pages 2121–2159.",
      "citeRegEx" : "Duchi et al\\.,? 2011",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2011
    }, {
      "title" : "QCRI machine translation systems for IWSLT 16",
      "author" : [ "Nadir Durrani", "Fahim Dalvi", "Hassan Sajjad", "Stephan Vogel." ],
      "venue" : "arXiv preprint arXiv:1701.03924.",
      "citeRegEx" : "Durrani et al\\.,? 2017",
      "shortCiteRegEx" : "Durrani et al\\.",
      "year" : 2017
    }, {
      "title" : "Ensemble distillation for neural machine translation",
      "author" : [ "Markus Freitag", "Yaser Al-Onaizan", "Baskaran Sankaran." ],
      "venue" : "arXiv preprint arXiv:1702.01802.",
      "citeRegEx" : "Freitag et al\\.,? 2017",
      "shortCiteRegEx" : "Freitag et al\\.",
      "year" : 2017
    }, {
      "title" : "Maxout networks",
      "author" : [ "Ian Goodfellow", "David Warde-Farley", "Mehdi Mirza", "Aaron Courville", "Yoshua Bengio." ],
      "venue" : "ICML, pages 1319–1327.",
      "citeRegEx" : "Goodfellow et al\\.,? 2013",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2013
    }, {
      "title" : "Learning both weights and connections for efficient neural network",
      "author" : [ "Song Han", "Jeff Pool", "John Tran", "William Dally." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 1135–1143.",
      "citeRegEx" : "Han et al\\.,? 2015",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural network ensembles",
      "author" : [ "Lars Kai Hansen", "Peter Salamon." ],
      "venue" : "IEEE transactions on pattern analysis and machine intelligence, 12(10):993– 1001.",
      "citeRegEx" : "Hansen and Salamon.,? 1990",
      "shortCiteRegEx" : "Hansen and Salamon.",
      "year" : 1990
    }, {
      "title" : "Second order derivatives for network pruning: Optimal brain surgeon",
      "author" : [ "Babak Hassibi", "David G. Stork" ],
      "venue" : "Advances in neural information processing systems,",
      "citeRegEx" : "Hassibi and Stork,? \\Q1993\\E",
      "shortCiteRegEx" : "Hassibi and Stork",
      "year" : 1993
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean." ],
      "venue" : "NIPS Deep Learning Workshop.",
      "citeRegEx" : "Hinton et al\\.,? 2014",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2014
    }, {
      "title" : "Is neural machine translation ready for deployment? A case study on 30 translation directions",
      "author" : [ "Marcin Junczys-Dowmunt", "Tomasz Dwojak", "Hieu Hoang." ],
      "venue" : "International Workshop on Spoken Language Translation IWSLT.",
      "citeRegEx" : "Junczys.Dowmunt et al\\.,? 2016a",
      "shortCiteRegEx" : "Junczys.Dowmunt et al\\.",
      "year" : 2016
    }, {
      "title" : "The AMU-UEDIN submission to the WMT16 news translation task: Attention-based NMT models as feature functions in phrase-based SMT",
      "author" : [ "Marcin Junczys-Dowmunt", "Tomasz Dwojak", "Rico Sennrich." ],
      "venue" : "Proceedings of the First Conference on",
      "citeRegEx" : "Junczys.Dowmunt et al\\.,? 2016b",
      "shortCiteRegEx" : "Junczys.Dowmunt et al\\.",
      "year" : 2016
    }, {
      "title" : "Sequencelevel knowledge distillation",
      "author" : [ "Yoon Kim", "Alexander M. Rush." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1317–1327, Austin, Texas. Association for Computational Linguistics.",
      "citeRegEx" : "Kim and Rush.,? 2016",
      "shortCiteRegEx" : "Kim and Rush.",
      "year" : 2016
    }, {
      "title" : "Optimal brain damage",
      "author" : [ "Yann LeCun", "John S. Denker", "Sara A. Solla", "Richard E. Howard", "Lawrence D. Jackel." ],
      "venue" : "NIPS, volume 2, pages 598–605.",
      "citeRegEx" : "LeCun et al\\.,? 1989",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1989
    }, {
      "title" : "Learning compact recurrent neural networks",
      "author" : [ "Zhiyun Lu", "Vikas Sindhwani", "Tara N. Sainath." ],
      "venue" : "Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on, pages 5960–5964. IEEE.",
      "citeRegEx" : "Lu et al\\.,? 2016",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2016
    }, {
      "title" : "Blocks and Fuel: Frameworks for deep learning",
      "author" : [ "Bart van Merriënboer", "Dzmitry Bahdanau", "Vincent Dumoulin", "Dmitriy Serdyuk", "David Warde-Farley", "Jan Chorowski", "Yoshua Bengio." ],
      "venue" : "CoRR.",
      "citeRegEx" : "Merriënboer et al\\.,? 2015",
      "shortCiteRegEx" : "Merriënboer et al\\.",
      "year" : 2015
    }, {
      "title" : "ASPEC: Asian scientific paper excerpt corpus",
      "author" : [ "Toshiaki Nakazawa", "Manabu Yaguchi", "Kiyotaka Uchimoto", "Masao Utiyama", "Eiichiro Sumita", "Sadao Kurohashi", "Hitoshi Isahara." ],
      "venue" : "LREC, pages 2204–2208, Portoroz, Slovenia.",
      "citeRegEx" : "Nakazawa et al\\.,? 2016",
      "shortCiteRegEx" : "Nakazawa et al\\.",
      "year" : 2016
    }, {
      "title" : "Lexicons and minimum risk training for neural machine translation: NAISTCMU at WAT2016",
      "author" : [ "Graham Neubig." ],
      "venue" : "Proceedings of the 3rd Workshop on Asian Translation (WAT2016), pages 119– 125, Osaka, Japan. The COLING 2016 Organizing",
      "citeRegEx" : "Neubig.,? 2016",
      "shortCiteRegEx" : "Neubig.",
      "year" : 2016
    }, {
      "title" : "Neural reranking improves subjective quality of machine translation: NAIST at WAT2015",
      "author" : [ "Graham Neubig", "Makoto Morishita", "Satoshi Nakamura." ],
      "venue" : "Workshop on Asian Translation, pages 35–41.",
      "citeRegEx" : "Neubig et al\\.,? 2015",
      "shortCiteRegEx" : "Neubig et al\\.",
      "year" : 2015
    }, {
      "title" : "On the compression of recurrent neural networks with an application to LVCSR acoustic modeling for embedded speech recognition",
      "author" : [ "Rohit Prabhavalkar", "Ouais Alsharif", "Antoine Bruguier", "Lan McGraw." ],
      "venue" : "Acoustics, Speech and Signal Pro-",
      "citeRegEx" : "Prabhavalkar et al\\.,? 2016",
      "shortCiteRegEx" : "Prabhavalkar et al\\.",
      "year" : 2016
    }, {
      "title" : "Compression of neural machine translation models via pruning",
      "author" : [ "Abigail See", "Minh-Thang Luong", "D. Christopher Manning." ],
      "venue" : "Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning, pages 291–301. Asso-",
      "citeRegEx" : "See et al\\.,? 2016",
      "shortCiteRegEx" : "See et al\\.",
      "year" : 2016
    }, {
      "title" : "Edinburgh neural machine translation systems for WMT 16",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the First Conference on Machine Translation, pages 371– 376, Berlin, Germany. Association for Computa-",
      "citeRegEx" : "Sennrich et al\\.,? 2016a",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715–",
      "citeRegEx" : "Sennrich et al\\.,? 2016b",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Data-free parameter pruning for deep neural networks",
      "author" : [ "Suraj Srinivas", "R. Venkatesh Babu." ],
      "venue" : "Proceedings of the British Machine Vision Conference (BMVC), pages 31.1–31.12. BMVA Press.",
      "citeRegEx" : "Srinivas and Babu.,? 2015",
      "shortCiteRegEx" : "Srinivas and Babu.",
      "year" : 2015
    }, {
      "title" : "SGNMT – A flexible NMT decoding platform for quick prototyping of new models and search strategies",
      "author" : [ "Felix Stahlberg", "Eva Hasler", "Danielle Saunders", "Bill Byrne." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Stahlberg et al\\.,? 2017",
      "shortCiteRegEx" : "Stahlberg et al\\.",
      "year" : 2017
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le." ],
      "venue" : "Proceedings of the 27th International Conference on Neural Information Processing Systems, pages 3104–3112. MIT Press.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Sequence student-teacher training of deep neural networks",
      "author" : [ "Jeremy HM. Wong", "Mark JF. Gales." ],
      "venue" : "Interspeech 2016, pages 2761–2765.",
      "citeRegEx" : "Wong and Gales.,? 2016",
      "shortCiteRegEx" : "Wong and Gales.",
      "year" : 2016
    }, {
      "title" : "Restructuring of deep neural network acoustic models with singular value decomposition",
      "author" : [ "Jian Xue", "Jinyu Li", "Yifan Gong." ],
      "venue" : "Interspeech, pages 2365–2369.",
      "citeRegEx" : "Xue et al\\.,? 2013",
      "shortCiteRegEx" : "Xue et al\\.",
      "year" : 2013
    }, {
      "title" : "ADADELTA: an adaptive learning rate method",
      "author" : [ "Matthew D. Zeiler." ],
      "venue" : "arXiv preprint arXiv:1212.5701.",
      "citeRegEx" : "Zeiler.,? 2012",
      "shortCiteRegEx" : "Zeiler.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 32,
      "context" : "use ensembles of a number of NMT systems (Bojar et al., 2016; Sennrich et al., 2016a; Chung et al., 2016; Neubig, 2016; Wu et al., 2016; Cromieres et al., 2016; Durrani et al., 2017).",
      "startOffset" : 41,
      "endOffset" : 182
    }, {
      "referenceID" : 8,
      "context" : "use ensembles of a number of NMT systems (Bojar et al., 2016; Sennrich et al., 2016a; Chung et al., 2016; Neubig, 2016; Wu et al., 2016; Cromieres et al., 2016; Durrani et al., 2017).",
      "startOffset" : 41,
      "endOffset" : 182
    }, {
      "referenceID" : 28,
      "context" : "use ensembles of a number of NMT systems (Bojar et al., 2016; Sennrich et al., 2016a; Chung et al., 2016; Neubig, 2016; Wu et al., 2016; Cromieres et al., 2016; Durrani et al., 2017).",
      "startOffset" : 41,
      "endOffset" : 182
    }, {
      "referenceID" : 9,
      "context" : "use ensembles of a number of NMT systems (Bojar et al., 2016; Sennrich et al., 2016a; Chung et al., 2016; Neubig, 2016; Wu et al., 2016; Cromieres et al., 2016; Durrani et al., 2017).",
      "startOffset" : 41,
      "endOffset" : 182
    }, {
      "referenceID" : 14,
      "context" : "use ensembles of a number of NMT systems (Bojar et al., 2016; Sennrich et al., 2016a; Chung et al., 2016; Neubig, 2016; Wu et al., 2016; Cromieres et al., 2016; Durrani et al., 2017).",
      "startOffset" : 41,
      "endOffset" : 182
    }, {
      "referenceID" : 12,
      "context" : "Ensembling (Dietterich, 2000; Hansen and Salamon, 1990) of neural networks is a simple yet very effective technique to improve the accuracy of NMT.",
      "startOffset" : 11,
      "endOffset" : 55
    }, {
      "referenceID" : 18,
      "context" : "Ensembling (Dietterich, 2000; Hansen and Salamon, 1990) of neural networks is a simple yet very effective technique to improve the accuracy of NMT.",
      "startOffset" : 11,
      "endOffset" : 55
    }, {
      "referenceID" : 36,
      "context" : "The decoder makes use of K NMT networks which are either trained independently (Sutskever et al., 2014; Chung et al., 2016; Neubig, 2016; Wu et al., 2016) or share some amount of training iterations (Sennrich et al.",
      "startOffset" : 79,
      "endOffset" : 154
    }, {
      "referenceID" : 8,
      "context" : "The decoder makes use of K NMT networks which are either trained independently (Sutskever et al., 2014; Chung et al., 2016; Neubig, 2016; Wu et al., 2016) or share some amount of training iterations (Sennrich et al.",
      "startOffset" : 79,
      "endOffset" : 154
    }, {
      "referenceID" : 28,
      "context" : "The decoder makes use of K NMT networks which are either trained independently (Sutskever et al., 2014; Chung et al., 2016; Neubig, 2016; Wu et al., 2016) or share some amount of training iterations (Sennrich et al.",
      "startOffset" : 79,
      "endOffset" : 154
    }, {
      "referenceID" : 9,
      "context" : ", 2016) or share some amount of training iterations (Sennrich et al., 2016b,a; Cromieres et al., 2016; Durrani et al., 2017).",
      "startOffset" : 52,
      "endOffset" : 124
    }, {
      "referenceID" : 14,
      "context" : ", 2016) or share some amount of training iterations (Sennrich et al., 2016b,a; Cromieres et al., 2016; Durrani et al., 2017).",
      "startOffset" : 52,
      "endOffset" : 124
    }, {
      "referenceID" : 36,
      "context" : "computes predictions from each of the individual models which are then combined using the arithmetic average (Sutskever et al., 2014) or the geometric average (Cromieres et al.",
      "startOffset" : 109,
      "endOffset" : 133
    }, {
      "referenceID" : 9,
      "context" : ", 2014) or the geometric average (Cromieres et al., 2016).",
      "startOffset" : 33,
      "endOffset" : 57
    }, {
      "referenceID" : 6,
      "context" : "Therefore, a recent line of research transfers the idea of knowledge distillation (Bucilu et al., 2006; Hinton et al., 2014) to NMT and trains a smaller network (the student) by minimizing the cross-entropy to the output of the ensemble system",
      "startOffset" : 82,
      "endOffset" : 124
    }, {
      "referenceID" : 20,
      "context" : "Therefore, a recent line of research transfers the idea of knowledge distillation (Bucilu et al., 2006; Hinton et al., 2014) to NMT and trains a smaller network (the student) by minimizing the cross-entropy to the output of the ensemble system",
      "startOffset" : 82,
      "endOffset" : 124
    }, {
      "referenceID" : 23,
      "context" : "(the teacher) (Kim and Rush, 2016; Freitag et al., 2017).",
      "startOffset" : 14,
      "endOffset" : 56
    }, {
      "referenceID" : 15,
      "context" : "(the teacher) (Kim and Rush, 2016; Freitag et al., 2017).",
      "startOffset" : 14,
      "endOffset" : 56
    }, {
      "referenceID" : 24,
      "context" : "This idea is justified by the fact that ensembled neural networks are often over-parameterized and have a large degree of redundancy (LeCun et al., 1989; Hassibi et al., 1993; Srinivas and Babu, 2015).",
      "startOffset" : 133,
      "endOffset" : 200
    }, {
      "referenceID" : 34,
      "context" : "This idea is justified by the fact that ensembled neural networks are often over-parameterized and have a large degree of redundancy (LeCun et al., 1989; Hassibi et al., 1993; Srinivas and Babu, 2015).",
      "startOffset" : 133,
      "endOffset" : 200
    }, {
      "referenceID" : 2,
      "context" : ", 2014, GRUs) and attention (Bahdanau et al., 2015).",
      "startOffset" : 28,
      "endOffset" : 51
    }, {
      "referenceID" : 34,
      "context" : "Our procedure is inspired by the method of Srinivas and Babu (2015). They propose a criterion for removing neurons in inner layers of the network based on two intu-",
      "startOffset" : 43,
      "endOffset" : 68
    }, {
      "referenceID" : 34,
      "context" : "The second intuition of the criterion used by Srinivas and Babu (2015) is that neurons with small outgoing weights contribute very little overall.",
      "startOffset" : 46,
      "endOffset" : 71
    }, {
      "referenceID" : 34,
      "context" : "1 generalizes the criterion of Srinivas and Babu (2015) to multiple outgoing weights.",
      "startOffset" : 31,
      "endOffset" : 56
    }, {
      "referenceID" : 34,
      "context" : "1 generalizes the criterion of Srinivas and Babu (2015) to multiple outgoing weights. Also note that Srinivas and Babu (2015) propose some heuristic improvements to this criterion.",
      "startOffset" : 31,
      "endOffset" : 126
    }, {
      "referenceID" : 34,
      "context" : "details we refer to Srinivas and Babu (2015).",
      "startOffset" : 20,
      "endOffset" : 45
    }, {
      "referenceID" : 34,
      "context" : "Note that we recover the update rule of Srinivas and Babu (2015) by setting λ to the i-th unit vector.",
      "startOffset" : 40,
      "endOffset" : 65
    }, {
      "referenceID" : 7,
      "context" : "GRU layers The terminology of neurons needs some further elaboration for GRU layers which rather consist of update and reset gates and states (Cho et al., 2014).",
      "startOffset" : 142,
      "endOffset" : 160
    }, {
      "referenceID" : 34,
      "context" : "Although we find our data-free approach to be a substantial improvement over the methods of Srinivas and Babu (2015) on NMT networks, it",
      "startOffset" : 92,
      "endOffset" : 117
    }, {
      "referenceID" : 1,
      "context" : "In the spirit of Babaeizadeh et al. (2016), our data-bound version records neuron",
      "startOffset" : 17,
      "endOffset" : 43
    }, {
      "referenceID" : 13,
      "context" : "We use the AdaGrad (Duchi et al., 2011) step rule, a small learning rate of 0.",
      "startOffset" : 19,
      "endOffset" : 39
    }, {
      "referenceID" : 2,
      "context" : "The standard attention-based NMT network architecture (Bahdanau et al., 2015) includes three lin-",
      "startOffset" : 54,
      "endOffset" : 77
    }, {
      "referenceID" : 39,
      "context" : "The individual NMT systems we use as source for constructing the unfolded networks are trained using AdaDelta (Zeiler, 2012) on the Blocks/Theano implementation (van Merriënboer et al.",
      "startOffset" : 110,
      "endOffset" : 124
    }, {
      "referenceID" : 2,
      "context" : ", 2012) of the standard attentionbased NMT model (Bahdanau et al., 2015) with: 1000 dimensional GRU layers (Cho et al.",
      "startOffset" : 49,
      "endOffset" : 72
    }, {
      "referenceID" : 7,
      "context" : ", 2015) with: 1000 dimensional GRU layers (Cho et al., 2014) in both the decoder and bidrectional encoder; a single maxout output layer (Goodfellow et al.",
      "startOffset" : 42,
      "endOffset" : 60
    }, {
      "referenceID" : 16,
      "context" : ", 2014) in both the decoder and bidrectional encoder; a single maxout output layer (Goodfellow et al., 2013); and 620 dimensional embedding layers.",
      "startOffset" : 83,
      "endOffset" : 108
    }, {
      "referenceID" : 35,
      "context" : "Our SGNMT decoder (Stahlberg et al., 2017)4 with a beam size of 12 is used in all experiments.",
      "startOffset" : 18,
      "endOffset" : 42
    }, {
      "referenceID" : 27,
      "context" : "Our primary corpus is the Japanese-English (Ja-En) ASPEC data set (Nakazawa et al., 2016).",
      "startOffset" : 66,
      "endOffset" : 89
    }, {
      "referenceID" : 2,
      "context" : ", 2012) of the standard attentionbased NMT model (Bahdanau et al., 2015) with: 1000 dimensional GRU layers (Cho et al., 2014) in both the decoder and bidrectional encoder; a single maxout output layer (Goodfellow et al., 2013); and 620 dimensional embedding layers. We follow Sennrich et al. (2016b) and use subword units based on byte pair encoding rather than words as modelling units.",
      "startOffset" : 50,
      "endOffset" : 300
    }, {
      "referenceID" : 2,
      "context" : ", 2012) of the standard attentionbased NMT model (Bahdanau et al., 2015) with: 1000 dimensional GRU layers (Cho et al., 2014) in both the decoder and bidrectional encoder; a single maxout output layer (Goodfellow et al., 2013); and 620 dimensional embedding layers. We follow Sennrich et al. (2016b) and use subword units based on byte pair encoding rather than words as modelling units. Our SGNMT decoder (Stahlberg et al., 2017)4 with a beam size of 12 is used in all experiments. Our primary corpus is the Japanese-English (Ja-En) ASPEC data set (Nakazawa et al., 2016). We select a subset of 500K sentence pairs to train our models as suggested by Neubig et al. (2015). We report cased BLEU scores calculated with Moses’",
      "startOffset" : 50,
      "endOffset" : 673
    }, {
      "referenceID" : 34,
      "context" : "plied the method of Srinivas and Babu (2015) to",
      "startOffset" : 20,
      "endOffset" : 45
    }, {
      "referenceID" : 34,
      "context" : "Results with the original method of Srinivas and Babu (2015) are not included in Tab.",
      "startOffset" : 36,
      "endOffset" : 61
    }, {
      "referenceID" : 4,
      "context" : "This is of great practical use: batch decoding with only two CPU threads surpasses production speed which is often set to 2000 words per minute (Beck et al., 2016).",
      "startOffset" : 144,
      "endOffset" : 163
    }, {
      "referenceID" : 24,
      "context" : "The idea of pruning neural networks to improve the compactness of the models dates back more than 25 years (LeCun et al., 1989).",
      "startOffset" : 107,
      "endOffset" : 127
    }, {
      "referenceID" : 0,
      "context" : "The literature is therefore vast (Augasta and Kathirvalavakumar, 2013).",
      "startOffset" : 33,
      "endOffset" : 70
    }, {
      "referenceID" : 24,
      "context" : "The connections can be selected for deletion based on the secondderivative of the training error with respect to the weight (LeCun et al., 1989; Hassibi et al., 1993),",
      "startOffset" : 124,
      "endOffset" : 166
    }, {
      "referenceID" : 17,
      "context" : "or by a threshold criterion on its magnitude (Han et al., 2015).",
      "startOffset" : 45,
      "endOffset" : 63
    }, {
      "referenceID" : 17,
      "context" : "or by a threshold criterion on its magnitude (Han et al., 2015). See et al. (2016) confirmed a high degree of weight redundancy in NMT networks.",
      "startOffset" : 46,
      "endOffset" : 83
    }, {
      "referenceID" : 10,
      "context" : "Using low rank matrices for neural network compression, particularly approximations via SVD, has been studied widely in the literature (Denil et al., 2013; Denton et al., 2014; Xue et al., 2013; Prabhavalkar et al., 2016; Lu et al., 2016).",
      "startOffset" : 135,
      "endOffset" : 238
    }, {
      "referenceID" : 11,
      "context" : "Using low rank matrices for neural network compression, particularly approximations via SVD, has been studied widely in the literature (Denil et al., 2013; Denton et al., 2014; Xue et al., 2013; Prabhavalkar et al., 2016; Lu et al., 2016).",
      "startOffset" : 135,
      "endOffset" : 238
    }, {
      "referenceID" : 38,
      "context" : "Using low rank matrices for neural network compression, particularly approximations via SVD, has been studied widely in the literature (Denil et al., 2013; Denton et al., 2014; Xue et al., 2013; Prabhavalkar et al., 2016; Lu et al., 2016).",
      "startOffset" : 135,
      "endOffset" : 238
    }, {
      "referenceID" : 30,
      "context" : "Using low rank matrices for neural network compression, particularly approximations via SVD, has been studied widely in the literature (Denil et al., 2013; Denton et al., 2014; Xue et al., 2013; Prabhavalkar et al., 2016; Lu et al., 2016).",
      "startOffset" : 135,
      "endOffset" : 238
    }, {
      "referenceID" : 25,
      "context" : "Using low rank matrices for neural network compression, particularly approximations via SVD, has been studied widely in the literature (Denil et al., 2013; Denton et al., 2014; Xue et al., 2013; Prabhavalkar et al., 2016; Lu et al., 2016).",
      "startOffset" : 135,
      "endOffset" : 238
    }, {
      "referenceID" : 29,
      "context" : "1, our data-free method is an extension of the approach by Srinivas and Babu (2015); our extension performs significantly better on NMT networks.",
      "startOffset" : 59,
      "endOffset" : 84
    }, {
      "referenceID" : 1,
      "context" : "2) is inspired by Babaeizadeh et al. (2016) as we combine neurons with similar activities during training, but we use linear combinations of multiple neurons to compensate for the loss of a neuron rather than merging pairs of neurons.",
      "startOffset" : 18,
      "endOffset" : 44
    }, {
      "referenceID" : 6,
      "context" : "teacher) to generate soft training labels for the smaller student network (Bucilu et al., 2006; Hinton et al., 2014).",
      "startOffset" : 74,
      "endOffset" : 116
    }, {
      "referenceID" : 20,
      "context" : "teacher) to generate soft training labels for the smaller student network (Bucilu et al., 2006; Hinton et al., 2014).",
      "startOffset" : 74,
      "endOffset" : 116
    }, {
      "referenceID" : 37,
      "context" : "This idea has been applied to sequence modelling tasks such as machine translation and speech recognition (Wong and Gales, 2016; Kim and Rush, 2016; Freitag et al., 2017).",
      "startOffset" : 106,
      "endOffset" : 170
    }, {
      "referenceID" : 23,
      "context" : "This idea has been applied to sequence modelling tasks such as machine translation and speech recognition (Wong and Gales, 2016; Kim and Rush, 2016; Freitag et al., 2017).",
      "startOffset" : 106,
      "endOffset" : 170
    }, {
      "referenceID" : 15,
      "context" : "This idea has been applied to sequence modelling tasks such as machine translation and speech recognition (Wong and Gales, 2016; Kim and Rush, 2016; Freitag et al., 2017).",
      "startOffset" : 106,
      "endOffset" : 170
    }, {
      "referenceID" : 32,
      "context" : "Unfolding and shrinking diverse networks could be possible, for example by applying the technique only to the input and output layers or by some other scheme of finding associations between units in different models, but we leave this investigation to future work as models in NMT ensembles in current research usually have the same topology (Bojar et al., 2016; Sennrich et al., 2016a; Chung et al., 2016; Neubig, 2016; Wu et al., 2016; Durrani et al., 2017).",
      "startOffset" : 342,
      "endOffset" : 459
    }, {
      "referenceID" : 8,
      "context" : "Unfolding and shrinking diverse networks could be possible, for example by applying the technique only to the input and output layers or by some other scheme of finding associations between units in different models, but we leave this investigation to future work as models in NMT ensembles in current research usually have the same topology (Bojar et al., 2016; Sennrich et al., 2016a; Chung et al., 2016; Neubig, 2016; Wu et al., 2016; Durrani et al., 2017).",
      "startOffset" : 342,
      "endOffset" : 459
    }, {
      "referenceID" : 28,
      "context" : "Unfolding and shrinking diverse networks could be possible, for example by applying the technique only to the input and output layers or by some other scheme of finding associations between units in different models, but we leave this investigation to future work as models in NMT ensembles in current research usually have the same topology (Bojar et al., 2016; Sennrich et al., 2016a; Chung et al., 2016; Neubig, 2016; Wu et al., 2016; Durrani et al., 2017).",
      "startOffset" : 342,
      "endOffset" : 459
    }, {
      "referenceID" : 14,
      "context" : "Unfolding and shrinking diverse networks could be possible, for example by applying the technique only to the input and output layers or by some other scheme of finding associations between units in different models, but we leave this investigation to future work as models in NMT ensembles in current research usually have the same topology (Bojar et al., 2016; Sennrich et al., 2016a; Chung et al., 2016; Neubig, 2016; Wu et al., 2016; Durrani et al., 2017).",
      "startOffset" : 342,
      "endOffset" : 459
    } ],
    "year" : 2017,
    "abstractText" : "Ensembling is a well-known technique in neural machine translation (NMT) to improve system performance. Instead of a single neural net, multiple neural nets with the same topology are trained separately, and the decoder generates predictions by averaging over the individual models. Ensembling often improves the quality of the generated translations drastically. However, it is not suitable for production systems because it is cumbersome and slow. This work aims to reduce the runtime to be on par with a single system without compromising the translation quality. First, we show that the ensemble can be unfolded into a single large neural network which imitates the output of the ensemble system. We show that unfolding can already improve the runtime in practice since more work can be done on the GPU. We proceed by describing a set of techniques to shrink the unfolded network by reducing the dimensionality of layers. On JapaneseEnglish we report that the resulting network has the size and decoding speed of a single NMT network but performs on the level of a 3-ensemble system.",
    "creator" : "LaTeX with hyperref package"
  }
}