{
  "name" : "1704.07287.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Joint Modeling of Text and Acoustic-Prosodic Cues for Neural Parsing",
    "authors" : [ "Trang Tran", "Shubham Toshniwal", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu", "Mari Ostendorf" ],
    "emails" : [ "ostendor}@uw.edu,", "mbansal@cs.unc.edu,", "klivescu}@ttic.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "While constituent parsing has become a relatively mature technology for written text, parser performance on conversational speech lags behind results on text. Speech poses challenges for parsing. First, transcripts may contain errors and lack punctuation. Joint speech recognition and parsing is useful for dealing with word errors and sentence segmentation (Kahn and Ostendorf, 2012). Second, even perfect transcripts can be difficult to handle because of disfluencies (restarts, repetitions, and self-corrections), filled pauses (“um”, “uh”), interjections (“like”), parentheticals (“you know”, “I mean”), and sentence fragments that are common\n∗Equal Contribution.\nin spontaneous speech. Some of these phenomena can be handled in standard grammars. Disfluencies typically require extensions of the model, and different approaches have been explored in both constituent parsing (Charniak and Johnson, 2001; Johnson and Charniak, 2004) and dependency parsing (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014).\nIn addition to these challenges, speech also carries helpful extra information – beyond the words – associated with the prosodic structure of an utterance and encoded via variation in timing and intonation. Studies show that speakers pause in locations that are correlated with syntactic structure (Grosjean et al., 1979), and listeners are able to use prosodic structure in resolving syntactic ambiguities (Price et al., 1991). Prosodic cues also signal disfluencies by marking the interruption point (Shriberg, 1994). However, most speech parsing systems in practice take little advantage of these cues. This study focuses on this last challenge, aiming to incorporate prosodic cues in a state-of-the-art neural parser.\nResearchers have explored a number of approaches for incorporating prosody in parsing. A challenge of using prosodic features is that multiple acoustic cues interact to signal prosodic structure, including pauses, duration lengthening, fundamental frequency modulation, and even spectral shape. These cues also vary with the phonetic segment, emphasis, emotion and speaker, so feature extraction has typically involved experimenting with multiple different time windows and normalization techniques. Researchers have dealt with this by explicitly predicting prosodic structure. The work proposed here takes advantage of advances in neural networks to automatically learn a good feature representation for parsing without the need for ar X\niv :1\n70 4.\n07 28\n7v 1\n[ cs\n.C L\n] 2\n4 A\npr 2\n01 7\nexplicitly representing prosodic constituents. To narrow the scope of this work and facilitate error analysis, our experiments use hand transcripts and known sentence segmentation.\nOur work offers the following contributions: We present a neural encoder-decoder model for parsing conversational speech that provides a framework for directly integrating acoustic-prosodic features with text; we demonstrate that significant improvements in parsing performance are obtained without requiring hand-annotated prosodic structure; and we provide analyses that show that combining multiple types of prosodic cues lead to gains for specific types of ambiguities."
    }, {
      "heading" : "2 Related Work",
      "text" : "Related work on parsing conversational speech has mainly addressed two problems: i) handling disfluencies and ii) integrating prosodic cues.\nOne major challenge of parsing conversational speech is the presence of disfluencies, which are grammatical (and prosodic) interruptions. Disfluencies include repetitions (‘I am + I am’), repairs (‘I am + we are’), and restarts (‘What I + Today is the...’), where the ‘+’ corresponds to an interruption point. Repairs often involve parallel grammatical constructions, but they can be more complex, involving hedging, clarifications, etc. One solution for handling dislfuencies is to first automatically detect them, remove the edited words (reparandum), and then parse the cleaned-up text. Disfluency detection is an active area of research (Georgila, 2009; Qian and Liu, 2013; Ferguson et al., 2015; Zayats et al., 2015, 2016). While disfluency detection has improved greatly, it still does not achieve high accuracy for a broad range of tasks, so researchers have emphasized joint parsing and disfluency detection. Such models have been explored for syntactic constituents (Charniak and Johnson, 2001; Kahn et al., 2005), or dependencies (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014).\nResearchers have been exploring different approaches to incorporating prosody in parsing since the 1990s. One study used quantized acousticprosodic cues as tokens in parsing, similar to punctuation, and observed a degradation in performance (Gregory et al., 2004). They hypothesize that these tokens are problematic because they break n-gram dependencies. Because multiple prosodic cues interact to signal prosodic structure, most studies have leveraged symbolic prosodic boundary mark-\ners (prosodic breaks) based on the ToBI prosodic annotation system (Silverman et al., 1992). In some studies, automatically predicted prosodic breaks are incorporated directly (Hale et al., 2006; Dreyer and Shafran, 2007; Huang and Harper, 2010). Another study uses prosodic break posteriors in parsing (Kahn et al., 2005). In either case, the use of symbolic prosodic breaks requires availability of a hand-annotated corpus for training break detection models. In addition, the parsing approaches that are most successful in using prosody leverage N - best hypothesis rescoring, limiting the impact of prosodic features to a small number of hypotheses.\nRecently, attention-enabled encoder-decoder models (Bahdanau et al., 2015) have gained traction for constituency parsing, with Vinyals et al. (2015) achieving state-of-the-art results for the Wall Street Journal (WSJ) corpus using an ensemble. Performance has since been improved by another ensemble of encoder-decoder models trained in a multi-task setting (Luong et al., 2016). Our models are based on this attention-based approach."
    }, {
      "heading" : "3 Task and Model Description",
      "text" : "We next describe our encoder-attention-decoder model that maps a sequence of word-level input features to a linearized parse output sequence. The word-level input feature vector consists of the concatenation of (learnable) word embeddings ei and several types of acoustic-prosodic features, described in Section 3.3."
    }, {
      "heading" : "3.1 Task Setup",
      "text" : "We assume the availability of a training treebank of conversational speech (in our case, SwitchboardNXT (Calhoun et al., 2010)) and corresponding constituent parses. The transcriptions are preprocessed by removing punctuation and lower-casing all text to better mimic the speech recognition setting. Following Vinyals et al. (2015), the parse trees are linearized, with pre-terminals also normalized as “XX”. Figure 1 illustrates how we convert standard treebank parses into encoder inputs and gold standard linearized parses."
    }, {
      "heading" : "3.2 Encoder-Attention-Decoder Parser",
      "text" : "Our attention-based encoder-decoder model is similar to the one used by Vinyals et al. (2015). The encoder is a deep long short-term memory recurrent neural network (LSTM-RNN) (Hochreiter and Schmidhuber, 1997) that reads in a wordlevel input feature sequence1, represented as a sequence of vectors x = (x1, · · · ,xTs) and outputs high-level features h = (h1, · · · ,hTs) where hi = LSTM(xi,hi−1).2\nThe parse decoder is also a deep LSTM-RNN that predicts the linearized parse sequence y =\n1As in Vinyals et al. (2015), the input sequence is processed in reverse order, as shown in Figure 2.\n2For brevity we omit the LSTM equations. The details can be found, e.g., in Zaremba et al. (2014).\n(y1, · · · , yTo) as follows:\nP (y|x) = To∏ t=1 P (yt|h,y<t)\nIn attention-based models, the posterior distribution of the output yt at time step t is given by:\nP (yt|h,y<t) = softmax(W s[ct;dt] + bs),\nwhere vector bs and matrix W s are learnable parameters; ct is referred to as a context vector that summarizes the encoder’s output h; and dt is the decoder hidden state at time step t, which captures the previous output sequence context y<t.\nThe attention mechanism used by Vinyals et al. (2015) computes the context vector ct as follows:\nuit = v > tanh(W 1hi +W 2dt + ba)\nαt = softmax(ut)\nct = Ts∑ i=1 αtihi\nwhere vectors v, ba and matrices W 1, W 2 are learnable parameters; ut and αt are the attention score and attention weight vector, respectively, for decoder time step t.\nThe above attention mechanism is only contentbased, i.e., it is only dependent on hi, dt. It is not location-aware, i.e., it does not consider the “location” of the previous attention vector. For parsing conversational text, location awareness can be crucial since disfluent structures can have duplicate words/phrases that may “confuse” the attention mechanism.3\nIn order to make the model location-aware, the attention mechanism takes into account the previous attention weight vector αt−1. In particular, we use the attention mechanism proposed by Chorowski et al. (2015), in which αt−1 is represented via a feature vector:\nf t = F ∗αt−1 where F ∈ Rk×r represents k learnable convolution filters of width r. The filters are used for performing 1-D convolution over αt−1 to extract k features f ti for each time step i of the input sequence. The extracted features are then incorporated in the alignment score calculation as:\nuit = v > tanh(W 1hi +W 2dt +W ff ti + ba)\n3This phenomenon has been observed in encoder-decoder models for speech recognition (Chorowski et al., 2014).\nwhereW f is another learnable parameter matrix. Finally, the decoder hidden state dt is computed as: dt = LSTM([ỹt−1; ct−1],dt−1)\nwhere ỹt−1 is the embedding vector corresponding to the previous output symbol yt−1."
    }, {
      "heading" : "3.3 Acoustic-Prosodic Features",
      "text" : "In previous work using encoder-decoder models for parsing (Vinyals et al., 2015; Luong et al., 2016), vector xi is simply the word embedding ei of the word at position i of the input sentence. For parsing conversational speech, we can also use acoustic prosodic features. Here we explore four types of features widely used in computational models of prosody: pauses, duration lengthening, fundamental frequency and energy. Since the features are different in nature (word-level measurements vs. time series), they are integrated with the encoderdecoder using different mechanisms.\nAll features are extracted from transcriptions that are time-aligned at the word level. In a small number of cases, the time alignment for a particular word boundary is missing, mainly due to tokenization. For example, contractions, such as don’t in the original transcript, are labeled as separated words, do and n’t, for the parser and the internal word boundary time is missing. In many cases, these internal times are estimated, but in a few cases estimation was difficult. For the roughly 1% of sentences where time alignments are missing, we simply backoff to the text-based parser.\nThe pause feature vector pi for word i is the concatenation of pre-word pause feature ppre,i and\npost-word pause feature ppost,i, where each subvector is a learned embedding for 6 pause categories: off (no pause), not-available, 0 < p ≤ 0.05 s, 0.05 s < p ≤ 0.2 s, 0.2 < p ≤ 1 s, and p > 1 s. This bucketing scheme for pause duration is motivated by the distribution of pause durations in our data and the assumption that longer pause length differences are not useful.\nWord-final duration lengthening is a strong cue to prosodic phrase boundaries (Wightman et al., 1992). The word duration feature δi is a realvalued scalar, equal to the actual word duration divided by the mean duration of the word, clipped to a maximum value of 5. The sample mean is used for frequent words (count ≥ 15). For infrequent words we estimate the mean as the sum over the sample means for the phonemes in the word’s dictionary pronunciation.\nFor fundamental frequency (f0) and energy (E) contours, we use a CNN to automatically learn a mapping from the time series to a word-level vector. The contour features are extracted with 25-ms frames with 10-ms hops using Kaldi (Povey et al., 2011), motivated by the success of the associated f0 features in speech recognition (Ghahremani et al., 2014). Three f0 features are used: warped Normalized Cross Correlation Function (NCCF), logpitch with Probability of Voicing (POV)-weighted mean subtraction over a 1.5-second window, and the estimated derivative (delta) of the raw log pitch. Three energy features are extracted from the Kaldi 40-mel-frequency filter bank features: Etotal, the log of total energy normalized by dividing by the speaker side’s max total energy; Elow, the log of\ntotal energy in the lower 20 mel-frequency bands, normalized by total energy, and Ehigh, the log of total energy in the higher 20 mel-frequency bands, normalized by total energy. Multi-band energy features are used as a simple mechanism to capture articulatory strengthening at prosodic constituent onsets (Fourgeron and Keating, 1997).\nFigure 3 summarizes the feature learning approach. The f0 and E features are processed at the word level: each sequence of frames corresponding to a word (and potentially its surrounding context) is convolved with N filters of m sizes (a total of mN filters). The motivation for the multiple filter sizes is to enable the computation of features that capture information on different time scales. For each filter, we perform a 1-D convolution over the 6-dimensional f0/E features with a stride of 1. Each filter output is max-pooled, resulting in mN -dimensional speech features si.\nOur overall acoustic-prosodic feature vector is the concatenation of pi, δi, and si in various combinations."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Dataset",
      "text" : "Our core corpus is Switchboard-NXT (Calhoun et al., 2010), a subset of the Switchboard corpus (Godfrey and Holliman, 1993). Switchboard I – Release 2 (Godfrey and Holliman, 1993) is a collection of about 2,400 telephone conversations between strangers; 650 such conversations were later hand-annotated with syntactic parses as part of the Penn Treebank Release 3 dataset (Marcus et al., 1999), and 642 of these were further augmented with richer layers of annotation facilitated by the NITE XML toolkit (Calhoun et al., 2010). Our sentence segmentations and syntactic trees are based on the annotations from the Treebank 3 set, with a few manual corrections from the NXT release. This core dataset consists of 100K sentences, totaling 1M tokens forming a vocabulary of 13.5K words. We follow the sentence boundaries defined by the parsed data available 4.\nWe follow the data split defined by Charniak and Johnson (2001), as well as related work done on Switchboard (Johnson and Charniak, 2004; Kahn et al., 2005; Gregory et al., 2004; Honnibal and Johnson, 2014)5: Conversations sw2000 to sw3000\n4Note that these sentence units might be inconsistent with other layers of Switchboard annotations, such as slash units\n5Part of our data preprocessing pipeline uses\nfor training, sw4500 to sw4936 for validation (dev), and sw4000 to sw4153 for evaluation (test). In addition, previous work has reserved sw4154 to sw4500 for “future use” (dev2), but we added this set to our training set. That is, all of our models are trained on Switchboard conversations sw2000 to sw3000 as well as sw4154 to sw4500. The overall data statistics are shown in Table 1."
    }, {
      "heading" : "4.2 Evaluation Metric",
      "text" : "The standard evaluation metric for constituent parsing is the parseval metric which uses bracketing precision, recall, and F1, as in the canonical implementation of EVALB.6 Transcribed speech, however, includes disfluencies, with speech repairs (labeled under “EDITED” nodes in Switchboard parse trees) being particularly problematic for statistical parsers, as explained by Charniak and Johnson (2001). For this reason, previous work has often explicitly detected EDIT regions in conjunction with parsing or as a preprocessing step.\nIn our context, we are not addressing the disfluency detection problem. Therefore, besides the fact that we are using a larger training data set, our results are not comparable with those of previous work on Switchboard constituent parsing, which involved removing and re-inserting EDIT nodes during evaluation according to a set of rules (Charniak and Johnson, 2001; Kahn et al., 2005). Nevertheless, acknowledging the fact that we are evaluating parsing of transcribed speech, we also report flattened-edit parseval F1 scores (“flat-F1”), which is parseval computed on trees with “flattened” edit nodes, where the structure under edit nodes has been eliminated so that all leaves are immediate children."
    }, {
      "heading" : "4.3 Model Parameters and Training Details",
      "text" : "Both the encoder and decoder are 3-layer deep LSTM-RNNs with 256 hidden units in each layer. For the location-aware attention, the convolution operation uses 5 convolution filters of width 40\nhttps://github.com/syllog1sm/swbd_tools 6http://nlp.cs.nyu.edu/evalb/\neach. We use 512-dimensional embedding vectors to represent words and linearized parsing symbols, such as “(S”.\nA relatively small number of configurations are explored for the acoustic-prosodic features, tuning based on dev set parsing performance. Pause embeddings are tuned over {4, 16, 32} dimensions. For the CNN, we try different configurations of filter size combinations ∈ {[10, 25, 50], [5, 10, 25, 50]}, and number of filters per filter size N ∈ {16, 32, 64, 128}7. These filter size combinations are motivated by the fact that the average word length in our dataset is 25 frames, so intuitively the different filter sizes are capturing f0 and energy phenomena on various levels: w = 5, 10 for sub-word, w = 25 for word, and w = 50 for word and context.\nFor optimization we use Adam (Kingma and Ba, 2014) with a minibatch size of 64. The initial learning rate is 0.001 which is decayed by a factor of 0.9 whenever training loss, calculated after every 500 updates, degrades w.r.t. to the worst of its previous 3 values. All models are trained for up to 50 epochs with early stopping. For regularization, dropout with 0.3 probability is applied on the output of all LSTM layers (Pham et al., 2014). We use TensorFlow (Abadi et al., 2015) to implement all models."
    }, {
      "heading" : "4.4 Inference",
      "text" : "For inference, we use a greedy decoder to generate the linearized parse sequence. The output token with maximum posterior probability is chosen at every time step and fed as input in the next time step. The decoder stops upon producing the endof-sentence symbol. As shown in Figure 1, we also use a post-processing step that merges the original sentence tokens with the decoder output to obtain\n7So when we use filter sizes [10, 25, 50] with 16 of each type, we have a total of 48 filters. Also, note that the filter sizes are actually 6 × 10, 6 × 25, etc., but since the feature dimension is fixed (6 in our case), we specify only the filter width.\nthe standard constituent tree representation.8"
    }, {
      "heading" : "5 Results",
      "text" : ""
    }, {
      "heading" : "5.1 Text-only Results",
      "text" : "We first show our text-only model results (i.e. xi = ei) to establish a strong baseline, on top of which we can add acoustic-prosodic features. We experiment with the content-only attention model used by Vinyals et al. (2015) and the content+location attention model proposed by Chorowski et al. (2015). For comparison with previous non-neural models, we use a high-quality latent-variable parser, the Berkeley parser (Petrov et al., 2006), retrained on our Switchboard data. Table 2 compares the three text-only models. In terms of F1, the content+location attention beats the Berkeley parser by about 2.5% and content-only attention by about 4.5%. Interestingly, flat-F1 scores for both encoderdecoder models is lower than their corresponding F1 scores. This suggests that the encoder-decoder models do well on predicting the internal structure of EDIT nodes. The reverse is true for the Berkeley parser: Flattening the EDIT nodes results in a better score.\nTo explain the gains of content+location attention over content-only attention, we compare their scores on fluent (without EDIT nodes) and disfluent sentences, shown in Table 3. From the table, it is clear that most of the gains for content+location attention are from disfluent sentences. The reason may be that disfluent sentences can have duplicate words or phrases, which can be problematic for a content-only attention model. Since our best model is the content+location attention model, we will henceforth refer to it as our “text-only” model.\n8In rare cases (and virtually none as our models converge), the decoder does not generate a valid parse sequence, due to the mismatch in brackets and/or the mismatch in the number of pre-terminals and terminals, i.e., num(XX) 6= num(tokens). In such cases, we simply add/remove brackets from either end of the parse, or add/remove pre-terminal symbols XX in the middle of the parse to match the number of input tokens."
    }, {
      "heading" : "5.2 Adding Acoustic-Prosodic Features",
      "text" : "We extend our text-only model with the three kinds of acoustic-prosodic features: pause (p), word duration (δ), and CNN mappings of fundamental frequency (f0) and energy (E) features (f0/E-CNN).\nThere are 23 − 1 = 7 model configurations corresponding to the possible combinations of the acoustic-prosodic features. The results of the 7 model configurations and the text-only models on our dev set are presented in Table 4. 9 First, we note that adding any combination of acoustic-prosodic features (individually or in sets) improves performance over the text-only baseline. However, certain combinations of acoustic-prosodic features are not guaranteed to be better than their subsets, sug-\n9For about 1% of the sentences in both dev and test, no time-alignments were available. Thus, we don’t have acousticprosodic features in such cases, and we have to back-off to the text-only model during evaluation.\ngesting negative interaction among features (e.g. text+p+wd). The best model, text + p + δ + f0/ECNN, uses all three types of features and has a gain of 0.7%, over the already-strong text-only baseline.\nTable 5 presents the results on the test set. Again, adding the acoustic-prosodic features improves over the text-only baseline. The gains are statistically significant for the text + f0/E-CNN, and text + p + δ + f0/E-CNN models at p-value < 0.02. The p-values are estimated using a bootstrap test (Efron and Tibshirani, 1993) that simulates 105 random test draws. Our best-performing model, text + p + δ + f0/E-CNN, will henceforth be referred to as “best model.”"
    }, {
      "heading" : "6 Analysis",
      "text" : "We first study performance differences between our best model and the text-only model for varying sentence lengths, shown in Figure 5. Both models do worse on longer sentences, which is not surprising since the corresponding parse trees tend to be more\ncomplex. The performance difference between our best model and the text-only model increases with sentence length. This may also be expected, since longer sentences are more likely to have multiple prosodic phrases and disfluencies.\nBecause sentence boundaries are given, and so many sentences in spontaneous speech are short, there is a possibility that the benefit from prosody is mainly related to disfluencies. Table 6 presents parse scores on the subset of fluent and disfluent sentences, suggesting that this may be the case.\nWe use the Berkeley Parser Analyzer (Kummerfeld et al., 2012) to compare the types of errors made by the different parsers.10 Table 7 presents the relative error reductions over the text-only baseline achieved by the text + p model and our best model, for disfluent sentences.\nThis analysis shows that, by including only pause information, we see the largest improvements on PP attachment and Modifier attachment errors. Adding the remaining acoustic-prosodic features helps to correct more types of attachment errors, especially VP and NP attachment. The text\n10This analysis omits the 1% of the sentences that did not have timing information.\n+ p model and the best model differ quite a bit in the types of error reductions they provide. A more detailed analysis of what information each type of acoustic-prosodic feature is capturing would be an interesting topic for future work.\nFigure 4 demonstrates one case where the pause feature helps in correcting a PP attachment error made by a text-only parser. Other interesting examples (see Appendix) are those where the learned f0/E features are valuable in avoiding NP attachment errors in cases where the audio reveals a prominent word at the constituent boundary, even though there is no pause at that word."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We have presented an attention-based encoderdecoder model for parsing conversational sentences, obtaining strong results when parsing the text transcriptions and further improved results when including word-level acoustic-prosodic features. Unlike recent prior work, we do not use an explicit disfluency detection step, and we automatically learn mappings of f0 and energy contours using a CNN that is jointly trained with the attention model. The acoustic-prosodic features provide the largest gains when sentences are disfluent or long, and analysis of error types shows that these features are especially helpful in repairing attachment errors. Further analysis may be helpful for revealing more specific contrasts, understanding how fine-grained duration would impact the findings, and discovering which aspects of prosody are learned by the CNN. The significant improvement\nobtained when using only automatically learned features suggests that they may also be useful in other related problems, such as dialog act recognition. In future work we would like to explore direct speech parsing jointly with speech recognition."
    }, {
      "heading" : "Acknowledgement",
      "text" : "We thank Pranava Swaroop Madhyastha, Hao Tang, Jon Cai, Hao Cheng, and Navdeep Jaitly for their help with initial discussions and code setup. This research was partially funded by a Google Faculty Research Award to Mohit Bansal, Karen Livescu, and Kevin Gimpel; and NSF grant no. IIS-1617176. The opinions expressed in this work are those of the authors and do not necessarily reflect the views of the funding agency."
    }, {
      "heading" : "8 Appendix",
      "text" : ""
    }, {
      "heading" : "8.1 Tree Examples",
      "text" : "In figures 6, 8, 9, and 10, we follow node correction notations as in (Kummerfeld et al., 2012). In particular, missing nodes are marked in blue on the gold tree, extra nodes are marked red in the predicted tree, and yellow nodes denote crossing."
    }, {
      "heading" : "8.2 Miscellany",
      "text" : "Figure 7 shows the distribution of pause duration in our training data. Our pause buckets of 0 < p ≤\n0.05 s, 0.05 s < p ≤ 0.2 s, 0.2 < p ≤ 1 s, and p > 1 s described in the main paper were based on this figure.\nTable 8 shows the comprehensive error counts in all error categories in both the fluent and disfluent subsets."
    } ],
    "references" : [ {
      "title" : "TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems",
      "author" : [ "Fernanda Viégas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng" ],
      "venue" : null,
      "citeRegEx" : "Viégas et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Viégas et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "Proc. ICLR.",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "The NXT-format Switchboard Corpus: a rich resource for investigating the syntax, semantics, pragmatics and prosody of dialogue",
      "author" : [ "Sasha Calhoun", "Jean Carletta", "Jason M. Brenier", "Neil Mayo", "Dan Jurafsky", "Mark Steedman", "David Beaver." ],
      "venue" : "Lan-",
      "citeRegEx" : "Calhoun et al\\.,? 2010",
      "shortCiteRegEx" : "Calhoun et al\\.",
      "year" : 2010
    }, {
      "title" : "Edit Detection and Parsing for Transcribed Speech",
      "author" : [ "Eugene Charniak", "Mark Johnson." ],
      "venue" : "Proc. NAACL.",
      "citeRegEx" : "Charniak and Johnson.,? 2001",
      "shortCiteRegEx" : "Charniak and Johnson.",
      "year" : 2001
    }, {
      "title" : "End-to-end Continuous Speech Recognition using Attention-based Recurrent NN: First results",
      "author" : [ "Jan Chorowski", "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "CoRR abs/1412.1602.",
      "citeRegEx" : "Chorowski et al\\.,? 2014",
      "shortCiteRegEx" : "Chorowski et al\\.",
      "year" : 2014
    }, {
      "title" : "Attention-Based Models for Speech Recognition",
      "author" : [ "Jan Chorowski", "Dzmitry Bahdanau", "Dmitriy Serdyuk", "KyungHyun Cho", "Yoshua Bengio." ],
      "venue" : "CoRR abs/1506.07503.",
      "citeRegEx" : "Chorowski et al\\.,? 2015",
      "shortCiteRegEx" : "Chorowski et al\\.",
      "year" : 2015
    }, {
      "title" : "Exploiting prosody for PCFGs with latent annotations",
      "author" : [ "Markus Dreyer", "Izhak Shafran." ],
      "venue" : "Proc. Interspeech.",
      "citeRegEx" : "Dreyer and Shafran.,? 2007",
      "shortCiteRegEx" : "Dreyer and Shafran.",
      "year" : 2007
    }, {
      "title" : "An Introduction to the Bootstrap",
      "author" : [ "Bradley Efron", "Robert J. Tibshirani" ],
      "venue" : null,
      "citeRegEx" : "Efron and Tibshirani.,? \\Q1993\\E",
      "shortCiteRegEx" : "Efron and Tibshirani.",
      "year" : 1993
    }, {
      "title" : "Disfluency Detection with a Semi-Markov Model and Prosodic Features",
      "author" : [ "James Ferguson", "Greg Durrett", "Dan Klein." ],
      "venue" : "Proc. NAACL.",
      "citeRegEx" : "Ferguson et al\\.,? 2015",
      "shortCiteRegEx" : "Ferguson et al\\.",
      "year" : 2015
    }, {
      "title" : "Articulatory strengthening at edges of prosodic domains",
      "author" : [ "Cécile Fourgeron", "Patricia A. Keating." ],
      "venue" : "Journal of the Acoustical Society of America 101(6):3728–3740.",
      "citeRegEx" : "Fourgeron and Keating.,? 1997",
      "shortCiteRegEx" : "Fourgeron and Keating.",
      "year" : 1997
    }, {
      "title" : "Using Integer Linear Programming for Detecting Speech Disfluencies",
      "author" : [ "Kallirroi Georgila." ],
      "venue" : "Proc. NAACL.",
      "citeRegEx" : "Georgila.,? 2009",
      "shortCiteRegEx" : "Georgila.",
      "year" : 2009
    }, {
      "title" : "A pitch extraction algorithm tuned for automatic speech recognition",
      "author" : [ "Pegah Ghahremani", "Bagher BabaAli", "Daniel Povey", "Korbinian Riedhammer", "Jan Trmal", "Sanjeev Khudanpur." ],
      "venue" : "Proc. ICASSP.",
      "citeRegEx" : "Ghahremani et al\\.,? 2014",
      "shortCiteRegEx" : "Ghahremani et al\\.",
      "year" : 2014
    }, {
      "title" : "Switchboard-1 Release 2",
      "author" : [ "John J. Godfrey", "Edward Holliman." ],
      "venue" : "Linguistic Data Consortium.",
      "citeRegEx" : "Godfrey and Holliman.,? 1993",
      "shortCiteRegEx" : "Godfrey and Holliman.",
      "year" : 1993
    }, {
      "title" : "Sentence-Internal Prosody Does not Help Parsing the Way Punctuation Does",
      "author" : [ "Michelle L Gregory", "Mark Johnson", "Eugene Charniak." ],
      "venue" : "Proc. NAACL.",
      "citeRegEx" : "Gregory et al\\.,? 2004",
      "shortCiteRegEx" : "Gregory et al\\.",
      "year" : 2004
    }, {
      "title" : "The patterns of silence: Performance structures in sentence production",
      "author" : [ "Franćois Grosjean", "Lysiane Grosjean", "Harlan Lane." ],
      "venue" : "Cognitive Psychology .",
      "citeRegEx" : "Grosjean et al\\.,? 1979",
      "shortCiteRegEx" : "Grosjean et al\\.",
      "year" : 1979
    }, {
      "title" : "Pcfgs with syntactic and prosodic indicators of speech repairs",
      "author" : [ "John Hale", "Izhak Shafran", "Lisa Yung", "Bonnie Dorr", "Mary Harper", "Anna Krasnyanskaya", "Matthew Lease", "Yang Liu", "Brian Roark", "Mathew Snover", "Robin Stewart." ],
      "venue" : "Proc. COLING-ACL.",
      "citeRegEx" : "Hale et al\\.,? 2006",
      "shortCiteRegEx" : "Hale et al\\.",
      "year" : 2006
    }, {
      "title" : "Long Short-Term Memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural Computation 9(8).",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Joint Incremental Disfluency Detection and Dependency Parsing",
      "author" : [ "Matthew Honnibal", "Mark Johnson." ],
      "venue" : "TACL .",
      "citeRegEx" : "Honnibal and Johnson.,? 2014",
      "shortCiteRegEx" : "Honnibal and Johnson.",
      "year" : 2014
    }, {
      "title" : "Appropriately Handled Prosodic Breaks Help PCFG Parsing",
      "author" : [ "Zhongqiang Huang", "Mary Harper." ],
      "venue" : "Proc. NAACL.",
      "citeRegEx" : "Huang and Harper.,? 2010",
      "shortCiteRegEx" : "Huang and Harper.",
      "year" : 2010
    }, {
      "title" : "A TAGbased Noisy Channel Model of Speech Repairs",
      "author" : [ "Mark Johnson", "Eugene Charniak." ],
      "venue" : "Proc. ACL.",
      "citeRegEx" : "Johnson and Charniak.,? 2004",
      "shortCiteRegEx" : "Johnson and Charniak.",
      "year" : 2004
    }, {
      "title" : "Effective Use of Prosody in Parsing Conversational Speech",
      "author" : [ "Jeremy G. Kahn", "Matthew Lease", "Eugene Charniak", "Mark Johnson", "Mari Ostendorf." ],
      "venue" : "Proc. HLT/EMNLP.",
      "citeRegEx" : "Kahn et al\\.,? 2005",
      "shortCiteRegEx" : "Kahn et al\\.",
      "year" : 2005
    }, {
      "title" : "Joint reranking of parsing and word recognition with automatic segmentation",
      "author" : [ "Jeremy G. Kahn", "Mari Ostendorf." ],
      "venue" : "Computer Speech & Language .",
      "citeRegEx" : "Kahn and Ostendorf.,? 2012",
      "shortCiteRegEx" : "Kahn and Ostendorf.",
      "year" : 2012
    }, {
      "title" : "Adam: A Method for Stochastic Optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "CoRR abs/1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Parser Showdown at the Wall Street Corral: An Empirical Investigation of Error Types in Parser Output",
      "author" : [ "Jonathan K. Kummerfeld", "David Hall", "James R. Curran", "Dan Klein." ],
      "venue" : "Proc. EMNLP.",
      "citeRegEx" : "Kummerfeld et al\\.,? 2012",
      "shortCiteRegEx" : "Kummerfeld et al\\.",
      "year" : 2012
    }, {
      "title" : "Multi-task Sequence to Sequence Learning",
      "author" : [ "Minh-Thang Luong", "Quoc V. Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser." ],
      "venue" : "Proc. ICLR.",
      "citeRegEx" : "Luong et al\\.,? 2016",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2016
    }, {
      "title" : "Treebank-3",
      "author" : [ "Mitchell P. Marcus", "Beatrice Santorini", "Mary A. Marcinkiewicz", "Ann Taylor." ],
      "venue" : "Technical report, Linguistic Data Consortium.",
      "citeRegEx" : "Marcus et al\\.,? 1999",
      "shortCiteRegEx" : "Marcus et al\\.",
      "year" : 1999
    }, {
      "title" : "Learning Accurate, Compact, and Interpretable Tree Annotation",
      "author" : [ "Slav Petrov", "Leon Barrett", "Romain Thibaux", "Dan Klein." ],
      "venue" : "Proc. COLINGACL.",
      "citeRegEx" : "Petrov et al\\.,? 2006",
      "shortCiteRegEx" : "Petrov et al\\.",
      "year" : 2006
    }, {
      "title" : "Dropout improves Recurrent Neural Networks for Handwriting Recognition",
      "author" : [ "Vu Pham", "Théodore Bluche", "Christopher Kermorvant", "Jérôme Louradour." ],
      "venue" : "Proc. International Conference on Frontiers in Handwriting Recognition (ICFHR).",
      "citeRegEx" : "Pham et al\\.,? 2014",
      "shortCiteRegEx" : "Pham et al\\.",
      "year" : 2014
    }, {
      "title" : "The Kaldi Speech Recognition",
      "author" : [ "Daniel Povey", "Arnab Ghoshal", "Gilles Boulianne", "Lukas Burget", "Ondrej Glembek", "Nagendra Goel", "Mirko Hannemann", "Petr Motlicek", "Yanmin Qian", "Petr Schwarz", "Jan Silovsky", "Georg Stemmer", "Karel Vesely" ],
      "venue" : null,
      "citeRegEx" : "Povey et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Povey et al\\.",
      "year" : 2011
    }, {
      "title" : "The Use of Prosody in Syntactic Disambiguation",
      "author" : [ "Patti Price", "Mari Ostendorf", "Stefanie ShattuckHufnagel", "Cynthia Fong." ],
      "venue" : "Proc. Workshop on Speech and Natural Language.",
      "citeRegEx" : "Price et al\\.,? 1991",
      "shortCiteRegEx" : "Price et al\\.",
      "year" : 1991
    }, {
      "title" : "Disfluency Detection Using Multi-step Stacked Learning",
      "author" : [ "Xian Qian", "Yang Liu." ],
      "venue" : "Proc. NAACL.",
      "citeRegEx" : "Qian and Liu.,? 2013",
      "shortCiteRegEx" : "Qian and Liu.",
      "year" : 2013
    }, {
      "title" : "Joint Parsing and Disfluency Detection in Linear Time",
      "author" : [ "Mohammad Sadegh Rasooli", "Joel Tetreault." ],
      "venue" : "Proc. EMNLP.",
      "citeRegEx" : "Rasooli and Tetreault.,? 2013",
      "shortCiteRegEx" : "Rasooli and Tetreault.",
      "year" : 2013
    }, {
      "title" : "Preliminaries to a theory of speech disfluencies",
      "author" : [ "Elizabeth Shriberg." ],
      "venue" : "Ph.D. thesis, Department of Psychology, University of California, Berkeley, CA.",
      "citeRegEx" : "Shriberg.,? 1994",
      "shortCiteRegEx" : "Shriberg.",
      "year" : 1994
    }, {
      "title" : "Tobi: A Standard for Labeling English Prosody",
      "author" : [ "Kim Silverman", "Mary Beckman", "John Pitrelli", "Mari Ostendorf", "Colin Wightman", "Patti Price", "Janet Pierrehumbert", "Julia Hirschberg." ],
      "venue" : "Proc. ICSLP.",
      "citeRegEx" : "Silverman et al\\.,? 1992",
      "shortCiteRegEx" : "Silverman et al\\.",
      "year" : 1992
    }, {
      "title" : "Grammar as a Foreign Language",
      "author" : [ "Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton." ],
      "venue" : "Proc. NIPS.",
      "citeRegEx" : "Vinyals et al\\.,? 2015",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "Segmental durations in the vicinity of prosodic phrase boundaries",
      "author" : [ "Colin W. Wightman", "Stefanie Shattuck-Hufnagel", "Mari Ostendorf", "Patti J. Price." ],
      "venue" : "Journal of the Acoustical Society of America 91(3).",
      "citeRegEx" : "Wightman et al\\.,? 1992",
      "shortCiteRegEx" : "Wightman et al\\.",
      "year" : 1992
    }, {
      "title" : "Recurrent Neural Network Regularization",
      "author" : [ "Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals." ],
      "venue" : "CoRR abs/1409.2329.",
      "citeRegEx" : "Zaremba et al\\.,? 2014",
      "shortCiteRegEx" : "Zaremba et al\\.",
      "year" : 2014
    }, {
      "title" : "Disfluency Detection using a Bidirectional LSTM",
      "author" : [ "Victoria Zayats", "Hannaneh Hajishirzi", "Mari Ostendorf." ],
      "venue" : "Proc. Interspeech.",
      "citeRegEx" : "Zayats et al\\.,? 2016",
      "shortCiteRegEx" : "Zayats et al\\.",
      "year" : 2016
    }, {
      "title" : "Unediting: Detecting Disfluencies Without Careful Transcripts",
      "author" : [ "Victoria Zayats", "Mari Ostendorf", "Hannaneh Hajishirzi." ],
      "venue" : "Proc. NAACL.",
      "citeRegEx" : "Zayats et al\\.,? 2015",
      "shortCiteRegEx" : "Zayats et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 21,
      "context" : "Joint speech recognition and parsing is useful for dealing with word errors and sentence segmentation (Kahn and Ostendorf, 2012).",
      "startOffset" : 102,
      "endOffset" : 128
    }, {
      "referenceID" : 3,
      "context" : "Disfluencies typically require extensions of the model, and different approaches have been explored in both constituent parsing (Charniak and Johnson, 2001; Johnson and Charniak, 2004) and dependency parsing (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014).",
      "startOffset" : 128,
      "endOffset" : 184
    }, {
      "referenceID" : 19,
      "context" : "Disfluencies typically require extensions of the model, and different approaches have been explored in both constituent parsing (Charniak and Johnson, 2001; Johnson and Charniak, 2004) and dependency parsing (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014).",
      "startOffset" : 128,
      "endOffset" : 184
    }, {
      "referenceID" : 31,
      "context" : "Disfluencies typically require extensions of the model, and different approaches have been explored in both constituent parsing (Charniak and Johnson, 2001; Johnson and Charniak, 2004) and dependency parsing (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014).",
      "startOffset" : 208,
      "endOffset" : 265
    }, {
      "referenceID" : 17,
      "context" : "Disfluencies typically require extensions of the model, and different approaches have been explored in both constituent parsing (Charniak and Johnson, 2001; Johnson and Charniak, 2004) and dependency parsing (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014).",
      "startOffset" : 208,
      "endOffset" : 265
    }, {
      "referenceID" : 14,
      "context" : "Studies show that speakers pause in locations that are correlated with syntactic structure (Grosjean et al., 1979), and listeners are able to use prosodic structure in resolving syntactic ambiguities (Price et al.",
      "startOffset" : 91,
      "endOffset" : 114
    }, {
      "referenceID" : 29,
      "context" : ", 1979), and listeners are able to use prosodic structure in resolving syntactic ambiguities (Price et al., 1991).",
      "startOffset" : 93,
      "endOffset" : 113
    }, {
      "referenceID" : 32,
      "context" : "Prosodic cues also signal disfluencies by marking the interruption point (Shriberg, 1994).",
      "startOffset" : 73,
      "endOffset" : 89
    }, {
      "referenceID" : 10,
      "context" : "Disfluency detection is an active area of research (Georgila, 2009; Qian and Liu, 2013; Ferguson et al., 2015; Zayats et al., 2015, 2016).",
      "startOffset" : 51,
      "endOffset" : 137
    }, {
      "referenceID" : 30,
      "context" : "Disfluency detection is an active area of research (Georgila, 2009; Qian and Liu, 2013; Ferguson et al., 2015; Zayats et al., 2015, 2016).",
      "startOffset" : 51,
      "endOffset" : 137
    }, {
      "referenceID" : 8,
      "context" : "Disfluency detection is an active area of research (Georgila, 2009; Qian and Liu, 2013; Ferguson et al., 2015; Zayats et al., 2015, 2016).",
      "startOffset" : 51,
      "endOffset" : 137
    }, {
      "referenceID" : 3,
      "context" : "Such models have been explored for syntactic constituents (Charniak and Johnson, 2001; Kahn et al., 2005), or dependencies (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014).",
      "startOffset" : 58,
      "endOffset" : 105
    }, {
      "referenceID" : 20,
      "context" : "Such models have been explored for syntactic constituents (Charniak and Johnson, 2001; Kahn et al., 2005), or dependencies (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014).",
      "startOffset" : 58,
      "endOffset" : 105
    }, {
      "referenceID" : 31,
      "context" : ", 2005), or dependencies (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014).",
      "startOffset" : 25,
      "endOffset" : 82
    }, {
      "referenceID" : 17,
      "context" : ", 2005), or dependencies (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014).",
      "startOffset" : 25,
      "endOffset" : 82
    }, {
      "referenceID" : 13,
      "context" : "One study used quantized acousticprosodic cues as tokens in parsing, similar to punctuation, and observed a degradation in performance (Gregory et al., 2004).",
      "startOffset" : 135,
      "endOffset" : 157
    }, {
      "referenceID" : 33,
      "context" : "ers (prosodic breaks) based on the ToBI prosodic annotation system (Silverman et al., 1992).",
      "startOffset" : 67,
      "endOffset" : 91
    }, {
      "referenceID" : 15,
      "context" : "In some studies, automatically predicted prosodic breaks are incorporated directly (Hale et al., 2006; Dreyer and Shafran, 2007; Huang and Harper, 2010).",
      "startOffset" : 83,
      "endOffset" : 152
    }, {
      "referenceID" : 6,
      "context" : "In some studies, automatically predicted prosodic breaks are incorporated directly (Hale et al., 2006; Dreyer and Shafran, 2007; Huang and Harper, 2010).",
      "startOffset" : 83,
      "endOffset" : 152
    }, {
      "referenceID" : 18,
      "context" : "In some studies, automatically predicted prosodic breaks are incorporated directly (Hale et al., 2006; Dreyer and Shafran, 2007; Huang and Harper, 2010).",
      "startOffset" : 83,
      "endOffset" : 152
    }, {
      "referenceID" : 20,
      "context" : "Another study uses prosodic break posteriors in parsing (Kahn et al., 2005).",
      "startOffset" : 56,
      "endOffset" : 75
    }, {
      "referenceID" : 1,
      "context" : "Recently, attention-enabled encoder-decoder models (Bahdanau et al., 2015) have gained traction for constituency parsing, with Vinyals et al.",
      "startOffset" : 51,
      "endOffset" : 74
    }, {
      "referenceID" : 24,
      "context" : "Performance has since been improved by another ensemble of encoder-decoder models trained in a multi-task setting (Luong et al., 2016).",
      "startOffset" : 114,
      "endOffset" : 134
    }, {
      "referenceID" : 1,
      "context" : "Recently, attention-enabled encoder-decoder models (Bahdanau et al., 2015) have gained traction for constituency parsing, with Vinyals et al. (2015) achieving state-of-the-art results for the Wall Street Journal (WSJ) corpus using an ensemble.",
      "startOffset" : 52,
      "endOffset" : 149
    }, {
      "referenceID" : 2,
      "context" : "1 Task Setup We assume the availability of a training treebank of conversational speech (in our case, SwitchboardNXT (Calhoun et al., 2010)) and corresponding constituent parses.",
      "startOffset" : 117,
      "endOffset" : 139
    }, {
      "referenceID" : 2,
      "context" : "1 Task Setup We assume the availability of a training treebank of conversational speech (in our case, SwitchboardNXT (Calhoun et al., 2010)) and corresponding constituent parses. The transcriptions are preprocessed by removing punctuation and lower-casing all text to better mimic the speech recognition setting. Following Vinyals et al. (2015), the parse trees are linearized, with pre-terminals also normalized as “XX”.",
      "startOffset" : 118,
      "endOffset" : 345
    }, {
      "referenceID" : 16,
      "context" : "The encoder is a deep long short-term memory recurrent neural network (LSTM-RNN) (Hochreiter and Schmidhuber, 1997) that reads in a wordlevel input feature sequence1, represented as a sequence of vectors x = (x1, · · · ,xTs) and outputs high-level features h = (h1, · · · ,hTs) where hi = LSTM(xi,hi−1).",
      "startOffset" : 81,
      "endOffset" : 115
    }, {
      "referenceID" : 33,
      "context" : "2 Encoder-Attention-Decoder Parser Our attention-based encoder-decoder model is similar to the one used by Vinyals et al. (2015). The encoder is a deep long short-term memory recurrent neural network (LSTM-RNN) (Hochreiter and Schmidhuber, 1997) that reads in a wordlevel input feature sequence1, represented as a sequence of vectors x = (x1, · · · ,xTs) and outputs high-level features h = (h1, · · · ,hTs) where hi = LSTM(xi,hi−1).",
      "startOffset" : 107,
      "endOffset" : 129
    }, {
      "referenceID" : 34,
      "context" : "As in Vinyals et al. (2015), the input sequence is processed in reverse order, as shown in Figure 2.",
      "startOffset" : 6,
      "endOffset" : 28
    }, {
      "referenceID" : 34,
      "context" : "As in Vinyals et al. (2015), the input sequence is processed in reverse order, as shown in Figure 2. For brevity we omit the LSTM equations. The details can be found, e.g., in Zaremba et al. (2014). (y1, · · · , yTo) as follows: P (y|x) = To ∏",
      "startOffset" : 6,
      "endOffset" : 198
    }, {
      "referenceID" : 34,
      "context" : "The attention mechanism used by Vinyals et al. (2015) computes the context vector ct as follows: uit = v > tanh(W 1hi +W 2dt + ba) αt = softmax(ut) ct = Ts ∑",
      "startOffset" : 32,
      "endOffset" : 54
    }, {
      "referenceID" : 4,
      "context" : "The extracted features are then incorporated in the alignment score calculation as: uit = v > tanh(W 1hi +W 2dt +W ff ti + ba) This phenomenon has been observed in encoder-decoder models for speech recognition (Chorowski et al., 2014).",
      "startOffset" : 210,
      "endOffset" : 234
    }, {
      "referenceID" : 4,
      "context" : "In particular, we use the attention mechanism proposed by Chorowski et al. (2015), in which αt−1 is represented via a feature vector: f t = F ∗αt−1 where F ∈ Rk×r represents k learnable convolution filters of width r.",
      "startOffset" : 58,
      "endOffset" : 82
    }, {
      "referenceID" : 34,
      "context" : "3 Acoustic-Prosodic Features In previous work using encoder-decoder models for parsing (Vinyals et al., 2015; Luong et al., 2016), vector xi is simply the word embedding ei of the word at position i of the input sentence.",
      "startOffset" : 87,
      "endOffset" : 129
    }, {
      "referenceID" : 24,
      "context" : "3 Acoustic-Prosodic Features In previous work using encoder-decoder models for parsing (Vinyals et al., 2015; Luong et al., 2016), vector xi is simply the word embedding ei of the word at position i of the input sentence.",
      "startOffset" : 87,
      "endOffset" : 129
    }, {
      "referenceID" : 35,
      "context" : "Word-final duration lengthening is a strong cue to prosodic phrase boundaries (Wightman et al., 1992).",
      "startOffset" : 78,
      "endOffset" : 101
    }, {
      "referenceID" : 28,
      "context" : "The contour features are extracted with 25-ms frames with 10-ms hops using Kaldi (Povey et al., 2011), motivated by the success of the associated f0 features in speech recognition (Ghahremani et al.",
      "startOffset" : 81,
      "endOffset" : 101
    }, {
      "referenceID" : 11,
      "context" : ", 2011), motivated by the success of the associated f0 features in speech recognition (Ghahremani et al., 2014).",
      "startOffset" : 86,
      "endOffset" : 111
    }, {
      "referenceID" : 9,
      "context" : "Multi-band energy features are used as a simple mechanism to capture articulatory strengthening at prosodic constituent onsets (Fourgeron and Keating, 1997).",
      "startOffset" : 127,
      "endOffset" : 156
    }, {
      "referenceID" : 2,
      "context" : "1 Dataset Our core corpus is Switchboard-NXT (Calhoun et al., 2010), a subset of the Switchboard corpus (Godfrey and Holliman, 1993).",
      "startOffset" : 45,
      "endOffset" : 67
    }, {
      "referenceID" : 12,
      "context" : ", 2010), a subset of the Switchboard corpus (Godfrey and Holliman, 1993).",
      "startOffset" : 44,
      "endOffset" : 72
    }, {
      "referenceID" : 12,
      "context" : "Switchboard I – Release 2 (Godfrey and Holliman, 1993) is a collection of about 2,400 telephone conversations between strangers; 650 such conversations were later hand-annotated with syntactic parses as part of the Penn Treebank Release 3 dataset (Marcus et al.",
      "startOffset" : 26,
      "endOffset" : 54
    }, {
      "referenceID" : 25,
      "context" : "Switchboard I – Release 2 (Godfrey and Holliman, 1993) is a collection of about 2,400 telephone conversations between strangers; 650 such conversations were later hand-annotated with syntactic parses as part of the Penn Treebank Release 3 dataset (Marcus et al., 1999), and 642 of these were further augmented with richer layers of annotation facilitated by the NITE XML toolkit (Calhoun et al.",
      "startOffset" : 247,
      "endOffset" : 268
    }, {
      "referenceID" : 2,
      "context" : ", 1999), and 642 of these were further augmented with richer layers of annotation facilitated by the NITE XML toolkit (Calhoun et al., 2010).",
      "startOffset" : 118,
      "endOffset" : 140
    }, {
      "referenceID" : 19,
      "context" : "We follow the data split defined by Charniak and Johnson (2001), as well as related work done on Switchboard (Johnson and Charniak, 2004; Kahn et al., 2005; Gregory et al., 2004; Honnibal and Johnson, 2014)5: Conversations sw2000 to sw3000 Note that these sentence units might be inconsistent with other layers of Switchboard annotations, such as slash units Part of our data preprocessing pipeline uses Section # sentences # words Train 97,113 729,252 Dev 5,769 50,445 Test 5,901 48,625 Table 1: Data statistics.",
      "startOffset" : 109,
      "endOffset" : 206
    }, {
      "referenceID" : 20,
      "context" : "We follow the data split defined by Charniak and Johnson (2001), as well as related work done on Switchboard (Johnson and Charniak, 2004; Kahn et al., 2005; Gregory et al., 2004; Honnibal and Johnson, 2014)5: Conversations sw2000 to sw3000 Note that these sentence units might be inconsistent with other layers of Switchboard annotations, such as slash units Part of our data preprocessing pipeline uses Section # sentences # words Train 97,113 729,252 Dev 5,769 50,445 Test 5,901 48,625 Table 1: Data statistics.",
      "startOffset" : 109,
      "endOffset" : 206
    }, {
      "referenceID" : 13,
      "context" : "We follow the data split defined by Charniak and Johnson (2001), as well as related work done on Switchboard (Johnson and Charniak, 2004; Kahn et al., 2005; Gregory et al., 2004; Honnibal and Johnson, 2014)5: Conversations sw2000 to sw3000 Note that these sentence units might be inconsistent with other layers of Switchboard annotations, such as slash units Part of our data preprocessing pipeline uses Section # sentences # words Train 97,113 729,252 Dev 5,769 50,445 Test 5,901 48,625 Table 1: Data statistics.",
      "startOffset" : 109,
      "endOffset" : 206
    }, {
      "referenceID" : 17,
      "context" : "We follow the data split defined by Charniak and Johnson (2001), as well as related work done on Switchboard (Johnson and Charniak, 2004; Kahn et al., 2005; Gregory et al., 2004; Honnibal and Johnson, 2014)5: Conversations sw2000 to sw3000 Note that these sentence units might be inconsistent with other layers of Switchboard annotations, such as slash units Part of our data preprocessing pipeline uses Section # sentences # words Train 97,113 729,252 Dev 5,769 50,445 Test 5,901 48,625 Table 1: Data statistics.",
      "startOffset" : 109,
      "endOffset" : 206
    }, {
      "referenceID" : 2,
      "context" : "1 Dataset Our core corpus is Switchboard-NXT (Calhoun et al., 2010), a subset of the Switchboard corpus (Godfrey and Holliman, 1993). Switchboard I – Release 2 (Godfrey and Holliman, 1993) is a collection of about 2,400 telephone conversations between strangers; 650 such conversations were later hand-annotated with syntactic parses as part of the Penn Treebank Release 3 dataset (Marcus et al., 1999), and 642 of these were further augmented with richer layers of annotation facilitated by the NITE XML toolkit (Calhoun et al., 2010). Our sentence segmentations and syntactic trees are based on the annotations from the Treebank 3 set, with a few manual corrections from the NXT release. This core dataset consists of 100K sentences, totaling 1M tokens forming a vocabulary of 13.5K words. We follow the sentence boundaries defined by the parsed data available 4. We follow the data split defined by Charniak and Johnson (2001), as well as related work done on Switchboard (Johnson and Charniak, 2004; Kahn et al.",
      "startOffset" : 46,
      "endOffset" : 930
    }, {
      "referenceID" : 3,
      "context" : "Therefore, besides the fact that we are using a larger training data set, our results are not comparable with those of previous work on Switchboard constituent parsing, which involved removing and re-inserting EDIT nodes during evaluation according to a set of rules (Charniak and Johnson, 2001; Kahn et al., 2005).",
      "startOffset" : 267,
      "endOffset" : 314
    }, {
      "referenceID" : 20,
      "context" : "Therefore, besides the fact that we are using a larger training data set, our results are not comparable with those of previous work on Switchboard constituent parsing, which involved removing and re-inserting EDIT nodes during evaluation according to a set of rules (Charniak and Johnson, 2001; Kahn et al., 2005).",
      "startOffset" : 267,
      "endOffset" : 314
    }, {
      "referenceID" : 3,
      "context" : "6 Transcribed speech, however, includes disfluencies, with speech repairs (labeled under “EDITED” nodes in Switchboard parse trees) being particularly problematic for statistical parsers, as explained by Charniak and Johnson (2001). For this reason, previous work has often explicitly detected EDIT regions in conjunction with parsing or as a preprocessing step.",
      "startOffset" : 204,
      "endOffset" : 232
    }, {
      "referenceID" : 22,
      "context" : "For optimization we use Adam (Kingma and Ba, 2014) with a minibatch size of 64.",
      "startOffset" : 29,
      "endOffset" : 50
    }, {
      "referenceID" : 27,
      "context" : "3 probability is applied on the output of all LSTM layers (Pham et al., 2014).",
      "startOffset" : 58,
      "endOffset" : 77
    }, {
      "referenceID" : 26,
      "context" : "For comparison with previous non-neural models, we use a high-quality latent-variable parser, the Berkeley parser (Petrov et al., 2006), retrained on our Switchboard data.",
      "startOffset" : 114,
      "endOffset" : 135
    }, {
      "referenceID" : 31,
      "context" : "We experiment with the content-only attention model used by Vinyals et al. (2015) and the content+location attention model proposed by Chorowski et al.",
      "startOffset" : 60,
      "endOffset" : 82
    }, {
      "referenceID" : 4,
      "context" : "(2015) and the content+location attention model proposed by Chorowski et al. (2015). For comparison with previous non-neural models, we use a high-quality latent-variable parser, the Berkeley parser (Petrov et al.",
      "startOffset" : 60,
      "endOffset" : 84
    }, {
      "referenceID" : 7,
      "context" : "The p-values are estimated using a bootstrap test (Efron and Tibshirani, 1993) that simulates 105 random test draws.",
      "startOffset" : 50,
      "endOffset" : 78
    }, {
      "referenceID" : 23,
      "context" : "We use the Berkeley Parser Analyzer (Kummerfeld et al., 2012) to compare the types of errors made by the different parsers.",
      "startOffset" : 36,
      "endOffset" : 61
    }, {
      "referenceID" : 23,
      "context" : "1 Tree Examples In figures 6, 8, 9, and 10, we follow node correction notations as in (Kummerfeld et al., 2012).",
      "startOffset" : 86,
      "endOffset" : 111
    } ],
    "year" : 2017,
    "abstractText" : "In conversational speech, the acoustic signal provides cues that help listeners disambiguate difficult parses. For automatically parsing a spoken utterance, we introduce a model that integrates transcribed text and acoustic-prosodic features using a convolutional neural network over energy and pitch trajectories coupled with an attention-based recurrent neural network that accepts text and word-based prosodic features. We find that different types of acoustic-prosodic features are individually helpful, and together improve parse F1 scores significantly over a strong text-only baseline. For this study with known sentence boundaries, error analysis shows that the main benefit of acoustic-prosodic features is in sentences with disfluencies and that attachment errors are most improved.",
    "creator" : "LaTeX with hyperref package"
  }
}