{
  "name" : "1106.1113.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Complexity Analysis of Vario-eta through Structure",
    "authors" : [ "Alejandro Chinea", "Elka Korutcheva" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: image analysis, machine learning, analytic combinatorics."
    }, {
      "heading" : "1 Introduction",
      "text" : "In general, an image can always be interpreted as a combination or mixture of simpler entities. The complexity of image analysis is usually due to the difficulties involved in discovering the nature of the relationship of these constituent elements. Surprisingly, species ranging from insects to mammals solve visual recognition problems extremely well. They have an inherent capacity to represent the temporal structure of experience, such information representation making it possible for an animal to adapt its behaviour to the temporal structure of an event in its behavioural space [9]. In particular, humans can recognize an object from a single presentation of its image without the necessity of integrating information over multiple time steps as would be required by classic machine learning paradigms. Indeed, the human perception system organizes information by using cohesive structures [13, 14]. Specifically, the human perception process always tries to assign structure to any perceptual information based upon previously stored knowledge structures (e.g. an image is recognized by identifying its structural components). This fact validates a well-known premise from\nthe machine learning field that states that the structure of an entity is very important for both classification and description. Furthermore, these ideas have motivated the development of a new branch of machine learning algorithms [7,8,10,15,17,19] that use structured representations (i.e. graph-based representations) of data for both classification and regression tasks instead of classic vector-based representations. For instance, an image can be represented by its region adjacency graph [1, 16] (see figure 1) which is extracted from the image by associating a node to each homogeneous region, and linking the nodes related to adjacent regions. In the region adjacency representation each node is labelled by a real vector, that represents features of the region (position, area, mean colour, texture, etc.). Thus, not only perceptual features of the image are captured with this representation but also its spatial arrangement. It is important to note that the notion of information content is strongly linked to the notion of structure.\nMoreover, image processing is done on the numerical representation of the image, i.e. a large matrix, therefore, these arrays can be enormous, and as soon as one deals with a sequence of images, such as in television or video applications, the volume of numerical data that must be processed becomes immense. Taking into account these considerations it can be deduced that the classification of large image data sets is a difficult computational task that becomes even more complex when dealing with structured representations of images. Therefore, from a machine learning point of view a great deal of research has been devoted to the development of fast computation learning schemes. In particular, for structured representations of data vario-eta through structure [3] has been recently proposed as an efficient learning scheme offering a good trade-off between speed of convergence and associated computational cost. Furthermore, this learning scheme achieves a rate of convergence similar to a quasi-Newton method at the computational cost of a first order method, and additionally, it offers the possibility of working in both sequential and in batch modes.\nThe present paper explores the reliability of this approximate second order stochastic gradient-based search optimization procedure for large scale learning problems. The rest of the paper is organized as follows: in the next section, some background topics on the vario-eta gradient-based optimization technique are introduced. Furthermore, the assumptions made by this optimization procedure are assessed from a theoretical point of view in order to check their reliability. Section 3 is devoted to the complexity analysis of the algorithm for structured domains. Specifically, the generating functions of the recursions associated to this learning scheme are analysed from an analytic combinatorics point of view. Section 4, is focused on the practical applications of the complexity analysis performed in section 3. Finally, section 5 provides a summary of the present study together with some concluding remarks."
    }, {
      "heading" : "2 Vario-eta Learning Rule",
      "text" : "The problem of learning in both deterministic and probabilistic machine learning models is frequently formulated in terms of parameter optimization techniques. The optimization problem is usually expressed in terms of the minimization of an error function E. This error is a function of the adaptive parameters of the model. One of the simplest techniques is gradient-search optimization which is one which has been widely studied. Here we investigate an approximate second order stochastic gradientsearch learning rule known as vario-eta [20]."
    }, {
      "heading" : "2.1 Mathematical Description",
      "text" : "Without a loss of generality, let us denote as E(w) the error function to be minimized. In addition, let us suppose that we express the parameters of our model in a vector w=[w1,w2,w3,….,wm]. A perturbation of the error function around some point of the model parameters can be written as follows: E(w+∆w) = E(w1+ ∆w1, w2+ ∆w2,…., wm+ ∆wm). Considering the Taylor expansion of the error function around the perturbation ∆w we obtain:\n..........)( !3\n1\n)( !2\n1)()()(\n3\n1\n2\n1 1\n+⎟⎟ ⎠ ⎞ ⎜⎜ ⎝ ⎛ ∆ ∂ ∂ +\n+⎟⎟ ⎠ ⎞ ⎜⎜ ⎝ ⎛ ∆ ∂ ∂ +∆ ∂ ∂ +=∆+\n∑\n∑ ∑\n=\n= =\nm\ni i\ni\nm\ni\nm\ni i i i i\nw w E\nw w Ew w EwEwwE\n(1)\nIn the batch version of gradient-descent approach, we start with some initial guess of model parameters. Then, model parameters are updated iteratively using the entire data set. In the sequential [14, 15] version of gradient descent, the error function gradient is evaluated for just one pattern (or a batch of patterns smaller when compared to the size of the data set for the case of the vario-eta learning rule) at a\ntime. Each update of model parameters can be viewed as perturbations (i.e. noise) around the current point given by the m dimensional vector of model parameters. Let us assume a given sequence of N disturbance vectors ∆w. Considering the error as a random variable and ignoring third and higher order terms in expression (1), the expectation of the error <E(w)> can be expressed as:\n∑ =\n∆+≅ N\nn\nnwwE N wE 1\n)(1)( (2)\nSubstituting the Taylor expansion of the error function in expression (2) and rearranging terms we obtain a series expansion of the expectation of the error as a function of the moments of the random perturbations:\n∑∑ ∑ <= = ∂\n∂ ∆∆+ ∂ ∂ ∆+ ∂ ∂ ∆+≅ ji ji ji i\nm\ni\nm\ni i i i ww Eww w Ew w EwwEwE\n2\n2\n2\n1 1\n2)( 2 1)()(\n(3)\nIn addition, the weight increment associated to the gradient descent rule is ∆wi = - ηgi. The third term of expression (3) concerning the covariance can be ignored supposing that the elements of the disturbance vectors are uncorrelated over the index n. This is a plausible hypothesis given that patterns of the data set are selected randomly during the optimization procedure. Moreover, close to a local minimum we can assume that 0≅∆ iw . Taking into account these considerations the expectation of the error is then given by:\n2\n2\n1\n2 2\n2\n2\n1\n2 )( 2 )()( 2 1)()(\ni\nm\ni i\ni\nm\ni i w EgwE w EwwEwE ∂ ∂ += ∂ ∂ ∆+≅ ∑∑ == ση (4)\nFrom equation (4), it is easy to deduce that the expected value of the error increases as the variance (represented by the symbol σ2) of the gradients increases. This observation suggests that the error function should be strongly penalized around such model parameters. Therefore, to cancel the noise term in the expected value of the error function the gradient descent rule must be changed to ∆wi = -ηgi/σ(gi). This normalization is known as vario-eta and was proposed in [20] for training neural networks. Specifically, the learning rate is renormalized by the stochasticity of the error gradient signals."
    }, {
      "heading" : "2.2 Virtues and Limitations of the Approximation",
      "text" : "Expression (4) was obtained under the hypothesis that the expectation of the perturbations ∆wi could be neglected near to a local minimum. To confirm the validity of this approximation, let us firstly express the expectation of the perturbations ∆wi in terms of the error gradients gi (using the expression of the gradient optimization update rule) :\n∑∑ == ==\n− −=−=∆\nN\nn\nn ii\nN\nn\nn ii\niii\ni\ni\ni i\ng N gg N g\nggg\ng g g w\n1\n22\n1\n2\n)(1)(1\n)( η σ η\n(5)\n(6)\nSubstituting expression (6) (i.e. the values of the first and second order moments of the error gradients) in expression (5) and rearranging, we get:\n1 ),....,,,(1 )( 321 2\n1\n1\n2 −\n− =\n−\n⎟ ⎠ ⎞ ⎜ ⎝ ⎛\n− =∆\n∑\n∑\n=\n= N iiii\nN\nn\nn i\nN\nn\nn i\ni\nggggf N\ng\ng N\nw ηη\n(7)\nIn the previous expression, we introduced the multidimensional function f. This function depends on the error gradients obtained through N iteration steps. Furthermore, it is easy to show (see expression (7)) that the minimum and maximum values of the function are 1 and N respectively. Specifically, the function reaches its maximum when the error gradients are identical.\n∑\n∑\n∑\n∑\n=\n<\n=\n= += ⎟ ⎠\n⎞ ⎜ ⎝ ⎛\n= N\nn\nn i\nkn\nk i n i\nN\nn\nn i\nN\nn\nn i\nN iiii\ng\ngg\ng\ng ggggf\n1\n2\n1\n2\n2\n1321\n)( 21 )( ),....,,,(\n(8)\nThis fact is of particular interest as it implies that the error gradients must be identical through N iteration steps. Taking into account that N represents the size of the batch, the probability of such event is NM − , where M is the total size of the training set. It is important to note that in the batch version of gradient descent M=N.\nTherefore, such event can be ignored as its probability can be considered zero as in practice M>>N>>1. However, numerical precision errors could make the contribution of the term (7) big enough to break the validity of the hypothesis. This scenario would cause oscillations of the error E during the optimization procedure. Therefore, the approximation is limited in practice by the precision of the machine; although their impact is minimal, taking into account the precision achieved by current 64 bit platforms of modern computers. Moreover, in order to alleviate eventual numerical problems a small constant 0 <φ << 1 can be summed to the standard deviation of the error gradients (e.g. φ = 10-6) as suggested in [3]."
    }, {
      "heading" : "3 Complexity Analysis",
      "text" : "From a computational point of view machine learning models that work with structured representations of data are intrinsically more complex than their vectorbased counterparts are. The fact of using structured representations of data is translated into a substantial gain in information content but also in an increase on computational complexity. Indeed, the principal drawback of these models is an excessively expensive computational learning phase. The learning rule studied in the previous section was adapted and expressed in a recursive form in [3] for working with structured representations of data. The fact of expressing such calculations in a recursive form permitted a considerable reduction in memory storage something fundamental for learning problems composed of huge data sets.\nIn this section, a complexity analysis of such algorithm is performed using elements of the theory of analytic combinatorics [4, 6]. The main objective of analytic combinatorics is to provide quantitative predictions of the properties of large combinatorial structures. This theory has emerged over recent decades as an essential tool for the analysis of algorithms and for the study of scientific models in many disciplines.\nHere, our intention is to study the asymptotic behaviour of the learning rule to reduce its computational requirements for large-scale learning problems. To this end, in subsection 3.1, we firstly provide some background topics on analytic combinatorics. Afterwards, in subsection 3.2 we model the algorithmic structure of the learning rule in terms of generating functions. Finally, in subsection 3.3 the singularities of the generating functions are analysed aimed at obtaining an asymptotic approximation of its coefficients for reducing the computational requirements associated to this learning rule."
    }, {
      "heading" : "3.1 Theoretical Background",
      "text" : "Let us introduce some basic definitions to be used throughout the rest of this paper:\nDefinition 3.1.1: The ordinary generating function of a sequence nA is the formal power series:\n∑ ∞\n≥ = 0 )( n n n zAzA\n(9)\nGenerating functions are the central object of study of analytic combinatorics. Their algebraic structure directly reflects the structure of the underlying combinatorial object. Furthermore, they can be viewed as analytic transformations in the complex plane and its singularities account for the asymptotic rate of growth of function’s coefficients. In addition, the theory elaborates a collection of methods (e.g. singularity analysis or saddle point method) by which one can extract asymptotic counting information from generating functions.\nDefinition 3.1.2: we let generally )(][ zfzn denote the operation of extracting the\ncoefficient of nz in the formal power series ∑= nn zfzf )(\n∫∑ + ∞\n≥\n==⎟ ⎠ ⎞ ⎜ ⎝ ⎛ c nn n n n n dz z zf i fzfz 1 0 )( 2 1][ π\n(10)\nIn expression (10), Cauchy's integral formula expresses coefficients of analytic functions as contour integrals. Therefore, an appropriate use of Cauchy’s integral formula then makes it possible to estimate such coefficients by suitably selecting an appropriate contour of integration."
    }, {
      "heading" : "3.2 Vario-eta Generating Functions",
      "text" : "Generally speaking, an algorithm can be interpreted as a mathematical object that is built iteratively from a set of finite rules that work on finite data structures. Furthermore, they have an inherent combinatorial structure that can be modelled in terms of generating functions. In the following, we model the algorithmic description of the vario-eta learning rule in terms of generating functions.\nThe algorithmic description of the learning rule provided in [3] is composed of two recurrences expressed in matrix form. These two finite differences equations accounted for the calculation of the mean and variance of the error gradients during the gradient-descent optimization loop. In order to simplify the notation, let us write without a loss of generality such equations in a single variable form:\nnnnnn gbgag += −1ˆˆ ∑ =\n=⇔ n\nk kn gn g 1\n1ˆ\n( )212 112 ˆ −−− −+= nnnnnn ggba σσ ∑ = − −\n=⇔ n\nk nkn ggn 1 22 )ˆ( 1\n1σ\n(11)\n(12)\nThe generating functions associated to the recurrences for the mean and variance of the error gradients can be computed applying the transformation (9) to both sides of equations (11) and (12). Specifically, equation (12) was expressed only in terms of the\naveraged error gradients and their variance to eliminate its dependency with the unknown generating function of the error gradients g(z).\n∑ ∑ ∑ ∞\n≥\n∞\n≥\n∞\n≥ − +−= 0 0 0 1 1ˆ)11(ˆ n n n n n n n n n zgn zg n zg\n∑\n∑ ∑ ∑ ∑ ∞\n≥ −\n∞\n≥\n∞\n≥\n∞\n≥\n∞\n≥ −−\n−\n+ − −=\n0 1\n0 0 0 0\n22 1 2 1 2\nˆˆ2\nˆ 1\n1\nn\nn nn\nn n n n\nn n n n n n n n\nzggn\nzgnz n zz σσσ\n(13)\n(14)\nAfter doing some algebra and using the complex convolution theorem [18], we obtain the following integral equations in the complex variable z for the generating functions of mean of error gradients )(ˆ zg and their variance )(2 zσ respectively:\n∫ −= dzzgzzzg )()1( 1)(ˆ\n∫ ∫ −=+− c duuzgdu ugdu i dzz z zzz )/(ˆ)( ˆ )1( 2 1)(1)()1( 22 π σσ\n(15)\n(16)\nDespite we do not know the form of the generating function of the error gradients neither their counting sequence ng we can make reasonable hypothesis about their form turning into a probabilistic framework. More specifically, the counting sequence ng is the result of computing the error gradient associated to a pattern randomly selected at iteration step n from the training set during the optimization loop. Therefore, we can consider each of them as n independent random variables. Furthermore, during the optimization loop the parameters of the machine learning model are not updated, therefore the n independent random variables representing the error gradients will be statistically equally distributed. It is important to note that the machine learning model can be viewed as a functional of the optimization parameters. Moreover, error gradients are calculated evaluating the functional by using the patterns of the data set. Taking into account we are interested in large-scale learning problems the statistical distribution of the averaged error gradients nĝ will tend, by virtue of Laplace’s central-limit theorem, to a Gaussian distribution at the extent the value of n increases. It is important to note that the value n represents the size of the data set for the case of batch learning and the size of the batch of patterns for sequential learning problems. Thus, its probability generating function will have the following expression:\n22 2 1\n)(ˆ zz ezg σµ − ≅\n(17)\nThe complex function )(ˆ zg is analytic for |z| < ¶ , it is also holomorphic (i.e. complex-differentiable), which is equivalent to saying that it is also analytic. Additionally, it can be proved that )/1(ˆ zg is also analytic. Moreover, the function under the contour integral of expression (16) will be analytic in |z| < 1. Hence, by virtue of the Cauchy’s residue theorem this integral is zero as the contour of integration does not contain any singularity (Null integral property). Taking these considerations into account we can get a closed expression for )(2 zσ as follows:\n0)()()1( 2\n2\n22\n=−− dz zdz dz zdzz σσ =>\n⎟⎟ ⎠ ⎞ ⎜⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ −++= z zzz 1log11)(2σ\n(18)"
    }, {
      "heading" : "3.3 Singularity Analysis",
      "text" : "The basic principle of singularity analysis is the existence of a general correspondence between the asymptotic expansion of a function near its dominant singularities and the asymptotic expansion of the function’s coefficients. Specifically, the method is mainly based on Cauchy's coefficient formula used in conjunction with special contours of integration known as Hankel contours [5]. Here we are interested in obtaining an asymptotic expression for the coefficients of the generating function obtained in (18).Hence, the first step consist in expressing such coefficients as a contour integral using the Cauchy’s coefficient formula:\n∫∫∫\n∫\n⎟ ⎠ ⎞ ⎜ ⎝ ⎛ −++\n==\n+\n+\nc nc nc n\nc n n\ndz z z zi dz zi dz zi\ndz z z i zz\n1log1 2 11 2 11 2 1\n)( 2 1)(][\n1\n1\n2 2\nπππ\nσ π σ\n(19)\nThe second step is to express the contour integral (19) using a Hankel contour. To this end, under the change of variables z = 1 + t/n , the kernel 1−−nz in the integral (19) transform asymptotically into an exponential. Using the aforementioned change of variables in expression (19) together with a Hankel contour we obtain:\n∫\n∫∫\n∞+\n−\n∞+\n−\n∞+\n−−\n⎟ ⎠ ⎞ ⎜ ⎝ ⎛ + ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ++\n+⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ++⎟ ⎠ ⎞ ⎜ ⎝ ⎛ +=\n)0(\n)0()0( 1\n2\n/1 /log1 2 1\n1 2 11 2 1)(][\ndt nt nt n t ni\ndt n t ni dt n t ni zz\nn\nnn n\nπ\nππ σ\n(20)\nThe contour and the associated rescaling capture the behaviour of the function near its singularities, enabling coefficient estimation when n Ø ¶.\n∫ ∞+ −−\n− =\nΓ\n)0( )( 2 1 )( 1 dtet is ts π\n(21)\nRe-arranging terms and expressing the integrals in terms of the gamma function (see expression 21), we finally get the asymptotic expansion for the variance of the error gradients:\nnnnn zz nn\nγγσσ ≅−−≅= 32 22 2 11)(][\n(22)\nThe result achieved by expression (22), where γ is the Euler number, is particularly interesting. It implies that for large enough values of n (data set size) we do not need to perform the iterative or the direct calculation of the variance (see expression (12) ) as it can be approximated using the above expression that is dependent exclusively on the size of the data set (or the size of the batch for sequential learning)."
    }, {
      "heading" : "4 Practical Results",
      "text" : "Expression (22) provides the law of asymptotic growth of the coefficients associated to the generating function describing the variance of the averaged error gradients. Figure 2 shows the result of computing the variance of a random variable. That is it shows the result of averaging n uniformly distributed random variables in the interval [0,1] for values of n (i.e. size of the data set), ranging from 500 up to 1000,000 (see the solid line style with squares) against the asymptotic approximation obtained in section 3 (see the dashed line style using diamonds at the sampling points).\nFrom inspection of the graph, it can be observed that the approximation works quite well for values of n bigger or equal to 50,000. For example, at n = 49,500 the error between the real value and its approximation is less than 10-5. Similarly, for n = 5,000 the error is less than 10-4. These results are of particular interest for huge data sets (i.e. n ¥ 106) where batch learning becomes impractical even for sophisticated gradient-based acceleration techniques like conjugate-gradients methods [2, 11] due to the memory storage requirements and associated computational cost. Furthermore, these kinds of acceleration techniques are not available for sequential learning. In this regard, vario-eta offers the possibility of working in sequential and batch learning modes.\nLet us suppose that the size of the data set is M, and let us denote by N the size of the batch. The convergence of the learning rule in a sequential learning scenario is always guaranteed by the Robbins-Monro theorem [12] if the condition M>>N is satisfied. Therefore, under the hypothesis of a huge value of M if we further impose the condition N>>1 the asymptotic approximation (22) will hold, thus we can obtain for a large-scale sequential learning problem a speed of convergence of an approximate second order algorithm at an extremely reduced computational cost. Nevertheless, it is important to note that these theoretical results must be interpreted carefully as real-world data sets usually contains correlated patterns that could break\nin certain cases the independence assumption made in section 3. Hence, providing a more practical guideline for real-world data sets remains for future work."
    }, {
      "heading" : "5 Conclusions",
      "text" : "In this paper, we have investigated the applicability of an approximate second order stochastic learning rule to large-scale learning problems. Throughout this paper we have referred to the concept of structured representations of data as a way of increase the information content of a representation. In particular, within a machine learning context we have described the advantages of graph-based representation of images for classification purposes. However, this kind of representation requires new learning protocols able to deal not only with the increased complexity associated to the use of structured representations of information but also with huge learning data sets.\nIn this context, we have presented a mathematical description of the vario-eta learning rule. We have also assessed through a detailed analysis the reliability of its working hypothesis. Moreover, we have presented a careful complexity analysis of the algorithm aimed at understanding its asymptotic properties. As a result of this analysis we deduced an asymptotic expression for the learning rule. Specifically, such an approximation achieves a considerable reduction on the computational cost associated to the learning rule when dealing with large-scale learning problems.\nAcknowledgments: The authors acknowledge the financial support by grant FIS 2009-9870 from the Spanish Ministry of Science and Innovation."
    } ],
    "references" : [ {
      "title" : "Recursive Processing of Cyclic Graphs",
      "author" : [ "M. Bianchini", "M Gori", "L. Sarti", "F. Scarselli" ],
      "venue" : "IEEE Transactions on Neural Networks 9 (17), pp. 10-18",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Neural Networks for Pattern Recognition",
      "author" : [ "C.M. Bishop" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1997
    }, {
      "title" : "Understanding the Principles of Recursive Neural Networks: A Generative Approach to Tackle Model Complexity",
      "author" : [ "A. Chinea" ],
      "venue" : "Alippi, C., Polycarpou, M., Panayiotou, C., Ellinas, G. (eds.) ICANN 2009. LNCS 5768, pp. 952-963. Springer, Heidelberg",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Advanced Combinatorics: The Art of Finite and Infinite Expansions",
      "author" : [ "L. Comtet" ],
      "venue" : "Reidel Publishing Company, Dordrecht",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1974
    }, {
      "title" : "Singularity Analysis of Generating Functions",
      "author" : [ "P. Flajolet", "A.M. Odlyzko" ],
      "venue" : "SIAM Journal on Algebraic and Discrete Methods 3,2 New York",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "Analytic Combinatorics",
      "author" : [ "P. Flajolet", "Sedgewick R." ],
      "venue" : "Cambridge University Press, Cambridge",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A Field Guide to Dynamical Recurrent Networks",
      "author" : [ "P. Frasconi", "M. Gori", "A. Kuchler", "A. Sperdutti" ],
      "venue" : "Kolen, J., Kremer, S. (Eds), pp. 351-364. IEEE Press, Inc., New York",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "A General Framework for Adaptive Processing of Data Structures",
      "author" : [ "P. Frasconi", "M Gori", "A. Sperduti" ],
      "venue" : "IEEE Transactions on Neural Networks 9 (5), pp. 768-786",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "The Organization of Learning",
      "author" : [ "C.R. Gallistel" ],
      "venue" : "MIT Press, Cambridge",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "A New Model for Learning in Graph Domains",
      "author" : [ "M. Gori", "G. Monfardini", "L. Scarselli" ],
      "venue" : "Proceedings of the 18 IEEE International Joint Conference on Neural Networks, pp. 729734, Montreal",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Neural Networks: a Comprehensive Foundation",
      "author" : [ "S. Haykin" ],
      "venue" : "Prentice Hall, New Jersey",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Stochastic Approximation Algorithms and Applications",
      "author" : [ "H.J. Kushner", "G.G. Yin" ],
      "venue" : "Springer-Verlag, New York",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "A Generative Theory of Shape",
      "author" : [ "M. Leyton" ],
      "venue" : "LNCS, vol 2145, pp. 1-76. Springer-Verlag",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Symmetry, Causality, Mind",
      "author" : [ "M. Leyton" ],
      "venue" : "MIT Press, Massachusetts",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Graph Kernels Based on Tree Patterns for Molecules",
      "author" : [ "P. Mahé", "Vert", "J.-P." ],
      "venue" : "Machine Learning, 75(1), pp. 3-35",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Similarity Learning for Graph-based Image Representations",
      "author" : [ "C.D. Mauro", "M. Diligenti", "M Gori", "M. Maggini" ],
      "venue" : "Pattern Recognition Letters, vol. 24, no. 8, pp. 115-1122,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Contextual Processing of Structured Data by Recursive Cascade Correlation",
      "author" : [ "A. Micheli", "D. Sona", "A. Sperduti" ],
      "venue" : "IEEE Transactions on Neural Networks 15(6)",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Discrete-Time Signal Processing",
      "author" : [ "A.V. Oppenheim", "R. Schafer" ],
      "venue" : "Prentice Hall, New Jersey",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Support Vector Machine Learning for Interdependent and Structured Output Spaces",
      "author" : [ "I. Tsochantaridis", "T. Hofmann", "T. Joachims", "Y. Altun" ],
      "venue" : "Brodley, C. E. (Ed.), ICML’04: Twenty-first international conference on Machine Learning. ACM Press, New York (2004) 20.Zimmermann, H. G., Neuneier, R.: How to Train Neural Networks. In: Orr, G.B., Müller, K.-R. (eds.): NIPS-WS 1996. LNCS, vol. 1524, pp. 395-399. Springer, Heidelberg",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 1998
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "They have an inherent capacity to represent the temporal structure of experience, such information representation making it possible for an animal to adapt its behaviour to the temporal structure of an event in its behavioural space [9].",
      "startOffset" : 233,
      "endOffset" : 236
    }, {
      "referenceID" : 12,
      "context" : "Indeed, the human perception system organizes information by using cohesive structures [13, 14].",
      "startOffset" : 87,
      "endOffset" : 95
    }, {
      "referenceID" : 13,
      "context" : "Indeed, the human perception system organizes information by using cohesive structures [13, 14].",
      "startOffset" : 87,
      "endOffset" : 95
    }, {
      "referenceID" : 6,
      "context" : "Furthermore, these ideas have motivated the development of a new branch of machine learning algorithms [7,8,10,15,17,19] that use structured representations (i.",
      "startOffset" : 103,
      "endOffset" : 120
    }, {
      "referenceID" : 7,
      "context" : "Furthermore, these ideas have motivated the development of a new branch of machine learning algorithms [7,8,10,15,17,19] that use structured representations (i.",
      "startOffset" : 103,
      "endOffset" : 120
    }, {
      "referenceID" : 9,
      "context" : "Furthermore, these ideas have motivated the development of a new branch of machine learning algorithms [7,8,10,15,17,19] that use structured representations (i.",
      "startOffset" : 103,
      "endOffset" : 120
    }, {
      "referenceID" : 14,
      "context" : "Furthermore, these ideas have motivated the development of a new branch of machine learning algorithms [7,8,10,15,17,19] that use structured representations (i.",
      "startOffset" : 103,
      "endOffset" : 120
    }, {
      "referenceID" : 16,
      "context" : "Furthermore, these ideas have motivated the development of a new branch of machine learning algorithms [7,8,10,15,17,19] that use structured representations (i.",
      "startOffset" : 103,
      "endOffset" : 120
    }, {
      "referenceID" : 18,
      "context" : "Furthermore, these ideas have motivated the development of a new branch of machine learning algorithms [7,8,10,15,17,19] that use structured representations (i.",
      "startOffset" : 103,
      "endOffset" : 120
    }, {
      "referenceID" : 0,
      "context" : "For instance, an image can be represented by its region adjacency graph [1, 16] (see figure 1) which is extracted from the image by associating a node to each homogeneous region, and linking the nodes related to adjacent regions.",
      "startOffset" : 72,
      "endOffset" : 79
    }, {
      "referenceID" : 15,
      "context" : "For instance, an image can be represented by its region adjacency graph [1, 16] (see figure 1) which is extracted from the image by associating a node to each homogeneous region, and linking the nodes related to adjacent regions.",
      "startOffset" : 72,
      "endOffset" : 79
    }, {
      "referenceID" : 2,
      "context" : "In particular, for structured representations of data vario-eta through structure [3] has been recently proposed as an efficient learning scheme offering a good trade-off between speed of convergence and associated computational cost.",
      "startOffset" : 82,
      "endOffset" : 85
    }, {
      "referenceID" : 13,
      "context" : "In the sequential [14, 15] version of gradient descent, the error function gradient is evaluated for just one pattern (or a batch of patterns smaller when compared to the size of the data set for the case of the vario-eta learning rule) at a",
      "startOffset" : 18,
      "endOffset" : 26
    }, {
      "referenceID" : 14,
      "context" : "In the sequential [14, 15] version of gradient descent, the error function gradient is evaluated for just one pattern (or a batch of patterns smaller when compared to the size of the data set for the case of the vario-eta learning rule) at a",
      "startOffset" : 18,
      "endOffset" : 26
    }, {
      "referenceID" : 2,
      "context" : "φ = 10) as suggested in [3].",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 2,
      "context" : "The learning rule studied in the previous section was adapted and expressed in a recursive form in [3] for working with structured representations of data.",
      "startOffset" : 99,
      "endOffset" : 102
    }, {
      "referenceID" : 3,
      "context" : "In this section, a complexity analysis of such algorithm is performed using elements of the theory of analytic combinatorics [4, 6].",
      "startOffset" : 125,
      "endOffset" : 131
    }, {
      "referenceID" : 5,
      "context" : "In this section, a complexity analysis of such algorithm is performed using elements of the theory of analytic combinatorics [4, 6].",
      "startOffset" : 125,
      "endOffset" : 131
    }, {
      "referenceID" : 2,
      "context" : "The algorithmic description of the learning rule provided in [3] is composed of two recurrences expressed in matrix form.",
      "startOffset" : 61,
      "endOffset" : 64
    }, {
      "referenceID" : 17,
      "context" : "After doing some algebra and using the complex convolution theorem [18], we obtain the following integral equations in the complex variable z for the generating functions of mean of error gradients ) ( ˆ z g and their variance ) ( 2 z σ respectively:",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 4,
      "context" : "Specifically, the method is mainly based on Cauchy's coefficient formula used in conjunction with special contours of integration known as Hankel contours [5].",
      "startOffset" : 155,
      "endOffset" : 158
    }, {
      "referenceID" : 0,
      "context" : "That is it shows the result of averaging n uniformly distributed random variables in the interval [0,1] for values of n (i.",
      "startOffset" : 98,
      "endOffset" : 103
    }, {
      "referenceID" : 1,
      "context" : "n ¥ 10) where batch learning becomes impractical even for sophisticated gradient-based acceleration techniques like conjugate-gradients methods [2, 11] due to the memory storage requirements and associated computational cost.",
      "startOffset" : 144,
      "endOffset" : 151
    }, {
      "referenceID" : 10,
      "context" : "n ¥ 10) where batch learning becomes impractical even for sophisticated gradient-based acceleration techniques like conjugate-gradients methods [2, 11] due to the memory storage requirements and associated computational cost.",
      "startOffset" : 144,
      "endOffset" : 151
    }, {
      "referenceID" : 11,
      "context" : "The convergence of the learning rule in a sequential learning scenario is always guaranteed by the Robbins-Monro theorem [12] if the condition M>>N is satisfied.",
      "startOffset" : 121,
      "endOffset" : 125
    } ],
    "year" : 2011,
    "abstractText" : "Graph-based representations of images have recently acquired an important role for classification purposes within the context of machine learning approaches. The underlying idea is to consider that relevant information of an image is implicitly encoded into the relationships between more basic entities that compose by themselves the whole image. The classification problem is then reformulated in terms of an optimization problem usually solved by a gradient-based search procedure. Vario-eta through structure is an approximate second order stochastic optimization technique that achieves a good trade-off between speed of convergence and the computational effort required. However, the robustness of this technique for large scale problems has not been yet assessed. In this paper we firstly provide a theoretical justification of the assumptions made by this optimization procedure. Secondly, a complexity analysis of the algorithm is performed to prove its suitability for large scale learning problems.",
    "creator" : "PScript5.dll Version 5.2.2"
  }
}