{
  "name" : "1301.6706.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Estimating the Value of Computation in Flexible Information Refinement",
    "authors" : [ "Michael C. Horsch", "David Poole" ],
    "emails" : [ "horsch@cs.ubc.ca", "poole@cs.ubc.ca", "mhorsch@cs." ],
    "sections" : [ {
      "heading" : null,
      "text" : "We outline a method to estimate the value of computation for a flexible algorithm using em pirical data. To determine a reasonable trade-off between cost and value, we build an empirical model of the value obtained through computa tion, and apply this model to estimate the value of computation for quite different problems. In par ticular, we investigate this trade-off for the prob lem of constructing policies for decision prob lems represented as influence diagrams. We show how two features of our anytime algorithm pro vide reasonable estimates of the value of compu tation in this domain.\n1 INTRODUCTION\nAnytime algorithms are designed to construct solutions to difficult computational problems by incrementally im proving an existing (sub-optimal) solution [Drummond & Bresina, 1990]. This kind of algorithm is interruptible; without interruption, computation may continue well past the point at which the computation is no longer valuable, if one were to consider the cost of computation.\nFlexible algorithms are designed to solve difficult compu tational problems by smoothly trading off the value of the sub-optimal solution with the cost of computing such a so lution [Horvitz, 1990; Russell & Wefald, 1992]. The prob lem faced by flexible algorithms is that the problem of find ing an appropriate tradeoff point is a meta-level problem, which can be solved only if computation costs at the meta level are less than the computation cost at the object level. Usually, simplifying assumptions are made at the meta level to keep the analysis feasible.\n*Current address: Intelligent Sy stems Lab, School of Com puting Science, Simon Fraser University, Burnaby, B.C., Canada VSA 1S6. Email: mhorsch@cs. sfu. ca\nIn this paper, we study a particular anytime algorithm for the problem of constructing policies for decision problems represented as influence diagrams [Horsch & Poole, 1998; Horsch, 1998]. This algorithm has a number of general fea tures: the optimal solution is not known before it is com puted; the current best solution is incrementally improved, although it is not known in advance how much improve ment will be gained by a single computational step; the value of the current best solution is known; the cost of a sin gle computational step is known.\nOur trade-off between computational cost and solution value is based on an estimate of the expected value of com putation (EVC), which we determine empirically. Using data collected during the course of using information refine ment to a large number of simple influence diagrams, we de rive a linear model which provides a basis for predicting the expected value of an optimal policy. Based on this predic tion, we show how a decision maker can estimate the incre mental value of refinement (i.e., the value of doing one more step), making use of information available to the anytime process. The prediction would have negligible cost during the anytime process.\nWe present some preliminary empirical results using this approach. Our anytime algorithm was applied to number of large influence diagrams which model an agent navigat ing a maze. For most of these influence diagrams, the opti mal policy is not known. The estimated maximum expected value predicted by the model on these influence diagrams is reasonable. The estimate of the incremental value of re finement seems to be somewhat optimistic with respect to value. We are pursuing this issue further.\n2 BACKGROUND\nAn influence diagram is a DAG representing a sequential decision problem under uncertainty [Howard & Matheson, 1984]. An ID models the subjective beliefs, preferences, and available actions from the perspective of a single de cision maker. A policy prescribes an action for each pos sible combination of observation. An optimal policy max-\n298 Horsch and Poole\nimizes the decision maker's expected value, without re gard to the cost of finding such a policy. If computational costs are negligible, the decision maker's expected value depends only on the expected value of an optimal policy. Traditional algorithms which compute the optimal policy using dynamic programming [Howard & Matheson, 1984; Shachter, 1986] usually assume computational costs to be negligible.\n2.1 FLEXIBLE COMPUTATION\nIn situations in which there is uncertainty about the state of the world and uncertainty about the possible outcomes of action, it has been argued that a rational decision maker should act so as to maximize expected utility[von Neuman & Morgenstern, 194 7; Savage, 1972]. The situation be comes a little more complex when the actions which can be taken include computation.\nWe treat computation as a meta-level action. That is, the decision maker is faced with a sequential decision problem which has been abstracted in such a way as to ignore com putational costs; this level is called the object level prob lem. The decision to invest computational resources to wards finding a policy in the object level problem is a meta level problem. This approach is the basis for flexible com putation [Horvitz, 1990; Russell & Wefald, 1992].\nWe also define two kinds of \"value\" for a policy 8. The first is the object value of the policy, EVI ( 8), which is the expected value of the policy assuming that computational costs on either level are negligible. The second is the com prehensive value, EVu(8), which includes an accounting for computational costs at the object Ievel.1\nWe consider in this paper those problems for which the comprehensive value is separable; that is, the comprehen sive value can be separated into two terms, one for the ob ject level value, and one for the computational costs, e.g.:\nEVu(8) = EV1(8)- c(8)\nwhere c( 8) is the cost of computing the object level pol icy. Figure I gives a prototypical situation: we show three curves: the object value, the computational costs, and the comprehensive value which is the difference between the object value and computational cost [Horvitz, 1990; Rus sell & Wefald, 1992].\nWhen computational costs are negligible (i.e., c( 8) = 0), the decision maker maximizes EVI I ( 8) by maximizing EVI ( 8). In Figure I, this happens at the rightmost edge of the graph. When costs are not negligible, the policy which maximizes EVI ( 8) may not maximize EVI I ( 6), as the cost\n1 The subscripts I and I I are employed here as a reference to the ideas of Good [Good, 1972], who identified two types of \"rationality;\" the first, type I, is without regard to computational costs, and the second, type I I, accounting for computational costs.\nmay be too high. In general, EVu(8) :<::: EV1(8); i.e., a given policy never increases in value when costs are figured into the value.\n2.2 INFORMATION REFINEMENT\nInformation refinement is an iterative approach to con structing policies for decision problems [Horsch & Poole, 1998; Horsch, 1998]. This approach is closely related to the work on compilation of decision models [Heckerman, Breese, & Horvitz, 1989; Lehner & Sadigh, 1993].\nThe basis of the information refinement algorithm is a pro cess which builds policies in the form of decision trees. A policy is represented by a collection of decision trees, one for each decision node in the influence diagram. These de cision trees prescribe actions for contexts which may not make use of all the information available to the decision maker.\nThe initial policy makes use of none of the information which is available to the decision maker at the time a de cision must be made. The available information consists of the agent's observations and previous actions. Each re finement step increases the policy's use of available infor mation; by conditioning action on available information, the process can determine actions which are better suited to more specific situations. The policy is refined by choosing a leaf from one of these trees and applying a single refine ment to the leaf, keeping the rest of the policy fixed.\nThe information refinement algorithm is an anytime algo rithm. There is no a priori order in which the trees are re fined; this is a departure from standard dynamic program ming techniques for building an optimal policy. Domain in dependent heuristics guide the algorithm, applying refine ments to decision trees in the problem.\nThe algorithm always has a current best policy available, which it refines until the decision maker interrupts the pro cess to act. The expected object value of the current best policy is known throughout the anytime process, but the in crease in value that may arise in future refinement is not known in advance.\nThe technique is able to find reasonably good policies for very large problems [Horsch, 1998]. Ignoring computa tional costs, the value of the policies tends to increase as computational resources are invested in the process. Our approach is able to make decisions with reasonably high ex pected value with reasonably small computational costs, on problems large enough to make traditional methods infeasi ble.\nEstimating the Value of Computation 299\n3 FL EXffiLE INFORMATION\nREFINEMENT\nIn this section we consider the anytime algorithm for infor mation refinement. We show how we have made use of in formation available during information refinement to esti mate the value of computation.\nOne of the problems faced in our particular situation is that a refinement is not guaranteed to increase the object value of the policy. The value of computation of a single refinement is not necessarily zero, even if it results in no net increase in object value. The investment of computational resources may pay off in future refinement steps.\nFor example, consider the problem of learning a decision tree representation for the Exor function on two Boolean random variables. Both variables are necessary to repre sent the Exor function, but individually, neither one pro vides any information. In the information refinement algo rithm, a similar situation arises when no single additional observation increases the value of the current best policy, but observing two (or more) variables would do so.\nThus, the myopic information refinement algorithm is prone to plateaus in which the expected object value does not change as the policy is refined. These plateaus in the object value profile lead to local maxima in the comprehen sive value profile if computational costs increase.\nBecause of the incremental nature of information refine ment, we define the incremental value of computation for each refinement as the expected object value of the next re finement. This value, IV C, depends on knowing the re sults of future computation. Because there is no random ness in the information refinement process, IV C is deter-\nmined by the input problem and the information refinement algorithm. Since we do not know IVC, we make a simple estimate for it.\nAt any point in the refinement process, there are a finite number of possible contexts in the current policy refine ments which might be refined. Each of these may lead to some increase in object value, although perhaps not imme diately. The total object value latent in these possible re finements is EVt - EV1; that is, the optimal expected ob ject value minus the current object value. This value is dis tributed throughout the possible refinements with some un known distribution. We make the simple assumption that the total latent object value is distributed uniformly over the possible refinements. The latent value in any single refine ment step is\nLVR = (EVt- EVI)/n\nwhere n is the number of contexts which can be refined in the current policy. We use LV R as an estimate for IVC.\nIt may seem that we have traded one unknown quantity, IVC, for another, EVt (EV1 is known). In the next sec tion, we will estimate EVt based on data gathered by ap plying information refinement to single stage influence di agrams.\n4 EMPIRICAL RESULTS\nWe applied information refinement to one hundred ran domly constructed single decision influence diagrams. The data we collected was used to learn a linear model to predict an estimate for expected value. This model was applied to much larger influence diagrams. We describe our experi ment in detail below.\n4.1 SAMPLE INFLUENCE DIAGRAMS\nThe influence diagrams are randomly sampled from a class of diagrams with very specific properties, which we discuss here. A template problem for this class of influence diagram is pictured in Figure 2. For brevity, this class is called the \"1-ID(n)\" class, where n is the number of chance nodes.\nWe use this class of influence diagram because the sam ple space of all influence diagrams is very large. We also wish to avoid creating essentially random problems with no properties in common with \"real\" problems. Many of our choices for sampling from this class are based on simplic ity, all other things being equal.\nThe 1 -ID(n) class has the property that all the chance nodes are parents of the decision node and the value node. As well, chance nodes in the 1-ID(n) class are conditionally in dependent.\nWe point out that any influence diagram with a single deci sion node can be reduced to one in which the only chance nodes are information predecessors (by summing out all the chance nodes which are not information predecessors (using variable elimination, for example [Zhang & Poole, 1996 ]).\nThe conditional independence between information prede cessors is used to keep the sample space as simple as possi ble. As well, we consider only binary-valued chance nodes, and a binary-valued decision node.\nThe 1-ID(n) class permits some interesting variation in terms of the probability distributions for the chance nodes. For this experiment, the prior probability distribution for each chance node was selected at random from a uniform distribution: for each chance node C; in the influence dia gram, one parameter x; was drawn from [0, 1] with a uni form distribution. The conditional probability table for the chance node given to the chance node is (x;, 1 - x;).\nThe 1-ID(n) class also permits some variation in terms of the dependency of the value function on its inputs, i.e., the chance nodes plus the decision node. In these influence di agrams, a value function has n + 1 inputs, but may not de pend functionally on all of these inputs. In particular, there may be combinations of a subset of the inputs which render\nthe remainder irrelevant. For example, a decision maker's preference may depend on chance node A when B is true, but may not depend on A when B is false.\nIn order to construct value functions with varying depen dencies on its inputs, the following procedure was used. The value function is constructed as a tree, with the inputs as internal nodes, and real values as leaves. The parent nodes of the value node were represented in a list. With probability b, the first of these nodes would be used to split the value tree at the current position; with probability 1 - b, the first node was discarded. This procedure was repeated for every node in the list. The decision node was always used as the last split (i.e., with probability 1), meaning that the actions of the agent always always had an effect on the value. The leaves of the value tree were selected from [0, 1 J with a uniform probability distribution.\nBy varying the parameter b, value functions with more or less dependence on its inputs can be constructed. In the ex periment described here, the value b = 0. 7794 was used. This results in value functions (when represented as trees) which are expected to have 200 internal nodes. When n = 8 , as in our experiment, a value function could have as many as 511 internal nodes. Thus, the value functions used in this part of the experiment are expected to have a significant de gree of structure. The information refinement process ex ploits this kind of structure.\n4.2 THE TRAINING DATA\nOne hundred influence diagrams with the properties de scribed above were constructed, and information refine ment was applied to each. The process uses heuristic in formation as guidance; in the experiment described here, we used a heuristic which we call \"the second best ac tion heuristic,\" which has been described in previous work [Horsch & Poole, 1998]. It is used to determine which part of the policy to refine, and is based on the observation that if there is a large difference in expected value between the best action and the second best action in any context, it is probably the case that a refinement to the context will not lead to significant improvements to the value of the policy. The heuristic value H for a context is computed as follows:\nv H=p\nv•\nwhere pis the marginal probability of the given context, v• is the expected value of the best action in the given context, and v is the expected value of the second best action in the context. We note that H will tend to decrease: as contexts get more specific, p will decrease. As well, H need not con verge to zero, as there need not be an action that results in an expected value of v = 0.\nThe following quantities were recorded at each step of the information refinement process: the object value of the cur rent best policy, EV1; the heuristic value for the current\nrefinement step H; the number of possible myopic refine ments which can be made at the current step. The experi ment also determined the optimal expected value, EVJ\" for each influence diagram.\nThe data collected for each influence diagram in the sample set contains a profile for each step in the refinement process. The points in the profile are not independent in a probabilis tic sense. One data point was extracted from each profile, so that the data would be independent. We chose to extract the point in the profile after the tenth refinement step. The profiles at the tenth step have not yet converged; we want to avoid training on data from the regions of the profile at which the process has converged. The tenth step was cho sen because the process converges to the optimal policy for these problems after about 60 refinement steps on average.\nThree linear models were fit to the data, and a least squares estimate was made for the parameters of polynomial sur faces of degree I, 2 and 3 . The dependent variable was the maximum expected object value, the quantity we wish to predict; the independent variables were the heuristic mea sure, H, which guides the refinement process, and the ob ject value of the current policy EV1.\nThe three models were examined informally for evidence of over-fitting, and the surfaces of degree 2 and 3 were rejected by geometric considerations. While the sum of squares er ror for these surfaces was quite small for the training data, the surfaces did not make reasonable extrapolations outside the range of the data. The high degree surfaces extended into negative values, and positive values greater than I.\nThe remaining model, a plane in 3 -space, had the following form:\nwhere\nC{) 0.1328 C! = 0.8 415\nC2 = 0.1753\nObserve that EV1, the object value of the current best pol icy, is the biggest factor in the prediction of the value of the optimal policy. This agrees with our intuitions: at the start of the refinement process, the current policy is relatively far from optimal, and the heuristic value should be high. As the refinement process proceeds, the current policy converges to optimal, and the heuristic value decreases.\nThe sum of squares error was quite small: 0.0634, over 100 data points. We applied this model to all the data in all the profiles collected from the single stage influence diagrams. The sum of squares error for the 25300 data points from all the profiles was very small as well: 29.4 .\nEstimating the Value of Computation 301\n4.3 TESTING THE MODEL\nThe simple linear model obtained in the previous section was applied to 16 multi-stage influence diagrams. Each of these influence diagrams model an agent navigating a maze; we modelled 4 different agents, which vary in the noisiness of actuators and sensors, and four different mazes, which vary in topology. The decision problem is to determine a policy which gets the agent to a goal location in the maze, starting from anywhere in the maze.\nEach is a ten stage influence diagram, which implies that the agent must arrive at the goal within ten steps to achieve the reward of I (the maximum value); being in any other loca tion is worth nothing to the agent. The information avail able to the agent is in the form of 4 sensors, one for each compass direction. The agent cannot directly observe its position.\nThe information space of these problems contains about 260 states. For two of the 16 problems, an optimal policy is known; in both problems, the agent has noiseless sensors and actuators, and a policy was constructed which guaran tees the agent will arrive at the goal position from any other position in the maze. However, for the remaining 14 prob lems, an optimal policy is not known. These problems are described in more detail in [Horsch, 199 8 ].\nThe information refinement procedure was applied for 30 refinement steps on each influence diagram. The average time required for these steps was 20.6 minutes.\nThe linear model (determined in the previous section based on the 1-ID(n) data) predicted optimal policy values which were on average 0.19 higher than the current policy at each refinement step; (std. dev. 0.04 7 ). We emphasize that this is not an error measurement; the optimal policy may be higher than any policy we have constructed. The average differ ence between the estimated value of the optimal policy, and the best known policy for these problems is 0.027 (std. dev. 0.14 ); the estimate is often low at the beginning of the re finement process, and increases with time.\nOn the two influence diagrams whose optimal policy is known to be 1.0, the initial estimates of the optimal ex pected value were 0.334 and 0.327 ; after 30 refinement steps, the estimates were 1.07 and 1.08 , respectively. Ta ble 4.3 summarizes the results for all l6 influence diagrams (see the rows labelled (1-ID(n)).\nThe difference between the estimates and the known values may be due to the fact that the optimal policy is unknown (and the best policies found are about this far from optimal). On the other hand, the estimates may be inaccurate with re spect to these problems because the training data is not a good model for the larger influence diagrams.\nTo investigate these possibilities further, we constructed 40 new influence diagrams similar to the 16 test problems.\nThe new problems were smaller instances of the test set (a smaller maze size, and only 5 stages). Information refine ment was applied to these smaller influence diagrams and data were collected as for the 1-ID(n) problems. We fit a linear model to the data, and the model had the following form:\nwhere\nCo 0.038 8\nc1 = 0.9252\nC2 = 0.138 4\nWe applied this model to the 16 larger influence diagrams, as before. Table 4 .3 summarizes the results (see the rows labelled Similar). In general, the estimates of EVt are smaller using problems similar to the test set than the 1- ID(n) problems.\n4.4 USING THE MODEL\nWe were interested in using our model to estimate the com prehensive value of computation. We used the estimate of the object value of the optimal policy at each refinement step to determine the incremental cost of computation, as outlined above. Figure 3 shows a typical result. In this plot, we plot value as a function of the number of refinement steps in our process. The object value of the current best policy is increasing. We also have provided a cost model for this example, that increases exponentially with the number of refinement steps. The comprehensive value of the pol icy consists of the difference between the object value and its cost at each step. We note that the object value and the comprehensive value profiles are retrospective; a decision maker faced with a resource bounded problem will not see the entire profile, but only that part which it has computed during information refinement. The comprehensive value is\nmaximized at 3 refinement steps for this particular problem and the given cost function.\nThe figure also shows the two estimates of the value of the optimal policy. Note how the estimates are higher than the current object value throughout the profile. As indicated above, the object value of the optimal policy is not known for this problem. After 30 refinement steps, the 1-ID(n) data predicts an optimal value of roughly 0.73; the data based on the smaller mazer walking problems predicts an optimal value of about 0.66 . Neither of these estimates are unreasonable for this problem. The best known policy ( af ter 40 refinement steps) is 0.6 81.\nFigure 4 shows the latent value of refinement as predicted by the two data sets (Section 3 . Latent value is computed using the estimate of the value of the optimal policy, as de scribed in Section 3 . This quantity is decreasing, and is an estimate intended to model the value of future refine ments made possible by the current refinement step. Note the small scale of the vi!lues, which reflects the fact that the difference between the current object value and the es timated optimal value is assumed to be distributed evenly across the possible refinements.\nThe graph also shows the differential value for each refine ment step. This value represents the difference between the latent value of refinement and the cost of performing a single refinement step. When this difference is positive, the comprehensive value of the next step is expected to in crease; when it is negative, the comprehensive value is ex pected to decrease.\nAs the graph shows, our estimate of LV R does not predict the maximum comprehensive value for this problem. At I 0 refinement steps, the differential value predicted by the 1- ID(n) data set goes negative. This comes 7 steps after the global maximum in comprehensive value attained. The dif ferential value based on the smaller maze walking problems\nEstimating the Value of Computation 303\nCosts and Values .for ID: Agent 2 in Maze 3\n0.9 )f-----k Current Best Policy +---+Estimated Optimal PoliCy (1-ID(n)) -------. Estimated Optimal Policy (Similar) o a - - Cost of Computation · G---e Comprehensive value\n0.7\nQl 0.6 � � \" rs .ll 0.4\n/ /\n/ /\noL-���L-------�------J-------�------�-------\" 0 5 10 15 w � �\nRefinement Steps\nFigure 3: Value functions for one of the 16 large influence diagrams. Also shown are the object value of the current best policy, and the estimated optimal value functions based on the two data sets. A prototypical cost function is given, and the comprehensive value of the current best policy is derived using this cost function.\npredicts a zero after 6 refinement steps. In this example, our models over-estimates the incremental value of refine ment, and therefore the differential value reaches zero after the comprehensive value is maximized. We are investigat ing this issue further.\n5 CONCLUSIONS AND FUTURE WORK\nIn this paper we have looked at the problem of using avail able information to estimate when to interrupt an anytime algorithm. Our approach estimated the expected value of the optimal policy from empirical data, and from this, de rived an estimate of the value of an investment of computa tional resources.\nOur investigation is specific to the information refinement process. We derived two models for the dependence of ex pected value of an optimal policy, which is not known dur ing the information refinement process, on measures which are known during the process. The first model was based on data collected while applying information refinement on a large number of simple influence diagrams which could be solved optimally.\nThis model was applied to a test set of influence diagrams for which finding an optimal policy is infeasible. The esti mated value of the optimal policies for the larger influence diagrams was consistently higher than the value for the best known policy during information refinement. However, the\nestimated values were not unreasonably high.\nA second model was based on problems which were simi lar to the test set; these were smaller than the test set, but still too large to solve for the optimal policy. Again, the es timated value of the optimal policy was consistently higher than the best known policy, but not unreasonable.\nWe believe that these preliminary results are encouraging. We have shown that a reasonably predictive model can be derived from information which is available to the decision maker during the information refinement process. This data can be collected by a decision making agent, and used to improve future comprehensive performance. More sophis ticated learning techniques could be used to provide more accurate estimates.\nAcknowledgments\nThe authors would like to thank Brent Boerlage of Norsys Software Corp. for advice and support in the use of the Net ica API as our Bayesian network engine. The second author is supported by the Institute for Robotics and Intelligent Systems, Project IC-7, and the National Sciences and Engi neering Council of Canada Operating Grant OGP0044121.\n304 Horsch and Poole\nReferences\n[Drummond & Bresina, 1990] Drummond, M., and Bresina, J. 1990. Anytime synthetic projection: Maximizing the probability of goal satisfaction. In Proceedings of the Eighth National Conference on Artificial Intelligence, 138- 1 44.\n[Good, 1972] Good, I. J. 1972. Twenty-seven prin ciples of rationality. In Godambe, V. P., and Sprott, D., eds., Foundations of Statistical Inference. Toronto: Ho\\t,Rinehart, Winston. 108-141.\n[Heckerman, Breese, & Horvitz, 1989] Heckerman, D. E.; Breese, J. S.; and Horvitz, E. J. 1989. The compilation of decision models. In Uncertainty in Artificial Intelligence 5, 162-173.\n[Horsch & Poole, 1998] Horsch, M. C., and Poole, D. 1998. An anytime algorithm for decision making under uncertainty. In Proceedings of the Fourteenth Confer ence on Uncertainty in Artificial Intelligence, 2 46-255 .\n[Horsch, 1998] Horsch, M. C. 1998. Flexible Policy Con struction by Information Refinement. Ph.D. Dissertation, Department of Computer Science, University of British Columbia.\n[Horvitz, 1990] Horvitz, E. J. 1990. Computation and ac tion under bounded resources. Technical Report KSL90-76 , Departments of Computer Science and Medicine, Stanford University.\n[Howard & Matheson, 1984] Howard, R., and Matheson, J., eds. 1984. Readings on the Principles and Appli cations of Decision Analysis. CA: Strategic Decisions Group.\n[Lehner & Sadigh, 1993] Lehner, P. E., and Sadigh, A. 1993 . Two procedures for compiling influence diagrams. In Proceedings of the Ninth Conference on Uncertainty in Artificial Intelligence, 335-3 41.\n[Russell & Wefald, 1992] Russell, S., and Wefald, E. 1992. Do the Right Thing: Studies in Limited Rational ity. Cambridge, Mass.: MIT Press.\n[Savage, 1972] Savage, L. J. 1972. The Foundations of Statistics. Dover Publications, Inc.\n[Shachter, 1986] Shachter, R. D. 1986 . Evaluating influ ence diagrams. Operations Research 34(6 ):871-882.\n[von Neuman & Morgenstern, 1947] von Neuman, J., and Morgenstern, 0. 1947 . The Theory of Games and Eco nomic Behaviour. Princeton University Press.\n[Zhang & Poole, 1996] Zhang, N. L., and Poole, D. 1996 . Exploiting Causal Independence in Bayesian Network Inference. Journal of Artificial Intelligence Research 5 :30 1-328."
    } ],
    "references" : [ {
      "title" : "Anytime synthetic projection",
      "author" : [ "J. Bresina" ],
      "venue" : null,
      "citeRegEx" : "Bresina,? \\Q1990\\E",
      "shortCiteRegEx" : "Bresina",
      "year" : 1990
    }, {
      "title" : "I",
      "author" : [ "Good" ],
      "venue" : "J.",
      "citeRegEx" : "Good. 1972",
      "shortCiteRegEx" : null,
      "year" : 1972
    }, {
      "title" : "The compilation",
      "author" : [ "J.S. Breese", "E.J. Horvitz" ],
      "venue" : null,
      "citeRegEx" : "Breese and Horvitz,? \\Q1989\\E",
      "shortCiteRegEx" : "Breese and Horvitz",
      "year" : 1989
    }, {
      "title" : "M",
      "author" : [ "Horsch" ],
      "venue" : "C.",
      "citeRegEx" : "Horsch. 1998",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "E",
      "author" : [ "Horvitz" ],
      "venue" : "J.",
      "citeRegEx" : "Horvitz. 1990",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "and Matheson",
      "author" : [ "R. Howard" ],
      "venue" : "J., eds.",
      "citeRegEx" : "Howard . Matheson. 1984",
      "shortCiteRegEx" : null,
      "year" : 1984
    }, {
      "title" : "and Wefald",
      "author" : [ "S. Russell" ],
      "venue" : "E.",
      "citeRegEx" : "Russell . Wefald. 1992",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "L",
      "author" : [ "Savage" ],
      "venue" : "J.",
      "citeRegEx" : "Savage. 1972",
      "shortCiteRegEx" : null,
      "year" : 1972
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "Flexible algorithms are designed to solve difficult compu­ tational problems by smoothly trading off the value of the sub-optimal solution with the cost of computing such a so­ lution [Horvitz, 1990; Russell & Wefald, 1992].",
      "startOffset" : 184,
      "endOffset" : 223
    }, {
      "referenceID" : 6,
      "context" : "Flexible algorithms are designed to solve difficult compu­ tational problems by smoothly trading off the value of the sub-optimal solution with the cost of computing such a so­ lution [Horvitz, 1990; Russell & Wefald, 1992].",
      "startOffset" : 184,
      "endOffset" : 223
    }, {
      "referenceID" : 3,
      "context" : "ca In this paper, we study a particular anytime algorithm for the problem of constructing policies for decision problems represented as influence diagrams [Horsch & Poole, 1998; Horsch, 1998].",
      "startOffset" : 155,
      "endOffset" : 191
    }, {
      "referenceID" : 5,
      "context" : "An influence diagram is a DAG representing a sequential decision problem under uncertainty [Howard & Matheson, 1984].",
      "startOffset" : 91,
      "endOffset" : 116
    }, {
      "referenceID" : 5,
      "context" : "Traditional algorithms which compute the optimal policy using dynamic programming [Howard & Matheson, 1984; Shachter, 1986] usually assume computational costs to be negligible.",
      "startOffset" : 82,
      "endOffset" : 123
    }, {
      "referenceID" : 7,
      "context" : "In situations in which there is uncertainty about the state of the world and uncertainty about the possible outcomes of action, it has been argued that a rational decision maker should act so as to maximize expected utility[von Neuman & Morgenstern, 194 7; Savage, 1972].",
      "startOffset" : 223,
      "endOffset" : 270
    }, {
      "referenceID" : 4,
      "context" : "This approach is the basis for flexible com­ putation [Horvitz, 1990; Russell & Wefald, 1992].",
      "startOffset" : 54,
      "endOffset" : 93
    }, {
      "referenceID" : 6,
      "context" : "This approach is the basis for flexible com­ putation [Horvitz, 1990; Russell & Wefald, 1992].",
      "startOffset" : 54,
      "endOffset" : 93
    }, {
      "referenceID" : 1,
      "context" : "1 The subscripts I and I I are employed here as a reference to the ideas of Good [Good, 1972], who identified two types of \"rationality;\" the first, type I, is without regard to computational costs, and the second, type I I, accounting for computational costs.",
      "startOffset" : 81,
      "endOffset" : 93
    }, {
      "referenceID" : 3,
      "context" : "The technique is able to find reasonably good policies for very large problems [Horsch, 1998].",
      "startOffset" : 79,
      "endOffset" : 93
    } ],
    "year" : 2011,
    "abstractText" : "We outline a method to estimate the value of computation for a flexible algorithm using em­ pirical data. To determine a reasonable trade-off between cost and value, we build an empirical model of the value obtained through computa­ tion, and apply this model to estimate the value of computation for quite different problems. In par­ ticular, we investigate this trade-off for the prob­ lem of constructing policies for decision prob­ lems represented as influence diagrams. We show how two features of our anytime algorithm pro­ vide reasonable estimates of the value of compu­ tation in this domain.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}