{
  "name" : "1412.6181.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Pengtao Xie" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Recently, many efforts have been devoted to cloud machine learning (CML), where machine learning (ML) services are running on commercial providers’ infrastructure. Examples include Microsoft Azure Machine Learning1, Google Prediction API2, GraphLab3 and Ersatz Labs4, to name a few. CML allows training and deploying models on cloud servers. Once deployed users can use these models to make predictions without having to worry about maintaining the service and the models. Moreover, it allows the model owner to be paid for every prediction being made by the model. In\n∗The work was done while this author was visiting Microsoft. 1http://azure.microsoft.com/en-us/services/machine-learning/ 2https://developers.google.com/prediction/ 3http://graphlab.com/ 4http://www.ersatzlabs.com/\nar X\niv :1\n41 2.\n61 81\nv1 [\ncs .L\nG ]\na broader sense, it enables a model of Machine Learning as a Service (MLaaS), where there is a separation between the data owner, the model owner and the compute provider (the cloud).\nDespite the attractive benefits provided by MLaaS, it suffers from a severe problem, namely the invasion of the security and privacy of users’ data. Traditional ML solutions require access to the raw data, which creates a potential security and privacy risk. In some cases, for example that of medical data, regulations may make these usage patterns illegal. Therefore, the goal of this work is to enable data owners to use MLaaS without exposing their data.\nThis problem has been addressed before by Graepel et al. (2013). They proposed to perform machine learning on encrypted data utilizing homomorphic encryption. A homomorphic encryption scheme (Rivest et al., 1978) allows a certain computation to be performed on the encrypted data by manipulating the corresponding ciphertexts without the need to decrypt them first. A fully homomorphic encryption scheme (Gentry, 2009) allows arbitrary operations over encrypted data and therefore, any function can be computed. However, fully homomorphic encryption schemes are still too inefficient for practical use. One way to obtain better efficiency is to only use so-called somewhat homomorphic schemes that only allow the evaluation of functions up to a certain complexity. Such schemes are often the cores of corresponding fully homomorphic encryption schemes. They usually provide operations corresponding to addition and multiplication of encrypted integer values, and therefore, are suitable to evaluate polynomial functions up to a certain maximal degree. The required degree of the polynomial function along with the desired security level determines the scheme parameters and thus has great implications on the size of the ciphertext as well as the computational complexity of the cryptographic operations. Therefore, Graepel et al. (2013) suggested using linear or other low degree models. While this method preserves the privacy and security of the data, it does not allow for highly accurate predictions since linear models cannot compete with the state-of-the-art in terms of accuracies on problems such as object recognition in image or speech data.\nIn this paper, we investigate how to perform neural network prediction on encrypted data. A neural network is a nonlinear machine learning model with large model capacity. It has achieved great success in speech recognition, image classification and natural language processing. Figure 1 illustrates the scenario of making secure predictions on encrypted data with a neural network. On the cloud side, there is a neural network model trained on plaintext data. For example, let us assume that the trained neural network takes medical images and predicts the likelihood of a pathology (dis-\nease). A user possesses a medical image and wants to use the neural network model in the cloud to predict whether he has the disease. Meanwhile, the user does not want the image to be seen by the cloud, because it may leak his health conditions. The user encrypts the image into a ciphertext and sends the ciphertext to the cloud. The cloud service evaluates the neural network prediction by operating on the ciphertext only and produces a prediction result in encrypted form that the cloud cannot decipher. The encrypted result is sent back to the user, who decrypts locally and retrieves the result as readable plaintext. In this process, both the input image and the output prediction are held in encrypted form. The cloud does not learn any information about the users’ input data and the prediction result. Thereby, confidentiality of the user’s data and prediction results are guaranteed.\nThe main challenge in realizing this solution is the fact that the commonly used activation functions in neural networks are not in polynomial form. This includes functions such as the sigmoid and rectified linear functions. We first show that from theoretical point of view, since these functions are continuous, they can be approximated by polynomials and therefore, the entire computation can be thought of as applying a polynomial to the data. We also discuss ways to minimize the degree of these polynomials such that the time to compute will remain feasible. We call this type of neural networks crypto-nets."
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "Using Homomorphic Encryption (HE) to do machine learning and statistical analysis on encrypted data has been investigated in (Bos et al., 2014; Bost et al., 2014; Graepel et al., 2013; Lauter et al., 2014; Nikolaenko et al., 2013a;b; Wu & Haven, 2012). These works have studied how to do HEbased privacy-preserving training or prediction of linear regression (Nikolaenko et al., 2013b; Wu & Haven, 2012), linear classifiers (Bos et al., 2014; Bost et al., 2014; Graepel et al., 2013), decision trees (Bost et al., 2014), matrix factorization (Nikolaenko et al., 2013a). As far as we know, ours is the first work to show how to apply neural networks to encrypted data and therefore allow the use of models that have been shown to be very accurate.\nOrlandi et al. (2007) suggested a scheme for using homomorphic encryption with neural networks. They suggest solving the problem of non-linear activation functions by creating an interactive protocol between the data owner and the model owner. In a nut-shell, every non-linear transformation is computed by the data-owner: the model sends the input to the non-linear transformation in encrypted form to the data owner who decrypts the message, applies the transformation, encrypts the result and sends it back. Unfortunately, this interaction requires large latencies and increases the complexity on the data owner side, effectively making it impractical. Moreover, it leaks information about the model. Therefore, Orlandi et al. (2007) had to introduce safety mechanisms, such as random order of execution, to mitigate this issue. In comparison, the procedure we introduce does not require complicated communication schemes: the data owner encrypts the data and sends it. The model does its computation and sends back the (encrypted) prediction. Therefore, it allows for asynchronous communication and it does not leak unnecessary information about the model.\nAnother line of work focuses on differential privacy (Chaudhuri et al., 2011; Duchi et al., 2012; Dwork, 2008; Smith, 2011; Wasserman & Zhou, 2010). Differential privacy aims at allowing to gather statistics from a database without revealing information about individual records. However, this method is not suitable for privacy-preserving prediction since by its nature, in the inference phase, a single record is being used and therefore fully exposed. Moreover, the method proposed here provides a much higher level of security. For example, not only the row records are not exposed, even the predicted value is not accessible to any party except the data owner since it is encrypted, not even to the cloud service that computed it, since it is encrypted."
    }, {
      "heading" : "3 HOMOMORPHIC ENCRYPTION",
      "text" : "A Homomorphic Encryption (HE) scheme (Rivest et al., 1978) preserves some structure of the original message space. Here, we assume that it provides methods to add and multiply encrypted messages and therefore preserves the message space ring structure. We also assume that it can be used to operate on the ring of integers. In that case, messages are integers and the scheme preserves the ability to perform additions and multiplications of such integers.\nFor our purpose, a (secret key) HE scheme consists of four algorithms: encryption (E), decryption (D), addition (⊕) and multiplication (⊗). The encryption algorithm takes as input a message and a secret key k. We denote the dependence on the key by Ek, but will drop the subscript later when use is clear from the context. The decryption takes as input an element from the ciphertext space and a key, while the algorithms ⊕ and ⊗ do not depend on the secret key and only take two ciphertexts as input. Let m1 and m2 be integer messages and let k be a secret key. Then the above algorithms have the following properties:\n1. Given Ek(m1), it is computationally infeasible to compute m1 without the private key k. 2. It holds that m1 = Dk(Ek(m1)). 3. It holds that m1 +m2 = Dk (Ek (m1)⊕ Ek (m2)). 4. It holds that m1 ×m2 = Dk (Ek (m1)⊗ Ek (m2)). 5. The algorithms ⊕ and ⊗ do not use the secret key used for encryption.\nFurthermore, we require that the scheme can evaluate the algorithms ⊗ and ⊕ repeatedly for a certain number of times, while decryption still gives the correct result. More precisely, let P be a polynomial on n variables of degree at most d. Denote by P̃ the function on input n ciphertexts, which is given by replacing each addition in P by the algorithm ⊕ and each multiplication by ⊗. Let m1, . . . ,mn be messages. Then the above algorithms satisfy the following property:\nP (m1, . . . ,mn) = D(P̃ (E(m1), . . . , E(mn))).\nThis means that our HE scheme allows to compute any degree-bounded polynomial function P as above over encrypted messages without decrypting them first.\nGentry (2009) was the first to show that it is possible to construct a Fully Homomorphic Encryption (FHE) scheme, which means that there is no limit on the degree of the polynomial P above. In theory, this allows to evaluate arbitrary computations (since any computation can be written as a binary polynomial in terms of binary addition and multiplication on the single bits of the input). Even though there has been great progress in making FHE schemes more efficient and secure (see, for example, Brakerski & Vaikuntanathan (2014)), this approach is currently not feasible for practical applications. Efficiency can be increased by restricting to somewhat homomorphic schemes and by operating on integers instead of bits, see Lauter et al. (2011). With this approach, both the computational complexity and the length of ciphertexts increase with the number of desired operations performed on the encrypted data in order to guarantee correct decryption after polynomial evaluation. While this increase is benign when increasing the number of additions, it is more significant when adding multiplications. Thus, a solution that builds upon these encryption schemes has to be restricted to computing low degree polynomials."
    }, {
      "heading" : "4 POLYNOMIAL APPROXIMATION TO NEURAL NETWORKS",
      "text" : "From the discussion above, in Section 3 we conclude that certain polynomial functions can be computed over encrypted data given that their degree is not too large. However, activation functions such as sigmoids and rectified-linear functions are not polynomials and the same applies to other, commonly used non-linear transformations in neural-networks such as max pooling. Nevertheless, since all these functions are continuous, the results, that is the neural net, viewed as a function, is a continuous function. If the domain, that is the input space, is a compact set, then from the StoneWeierstrass theorem (Stone, 1948) it follows that it can be approximated uniformly by polynomials. We will begin the discussion with the inference case, therefore we assume that the neural network has already been trained and the goal is to apply it to encrypted data. Lemma 1. Let N be a neural network in which all non-linear transformations are continuous. Let X ⊂ Rn be the domain on which N acts and assume that X is compact, then for every > 0 there exists a polynomial P such that\nsup x∈X ‖N(x)− P (x)‖ < .\nProof. The functionN is constructed by compositions, additions and multiplications over the inputs and the non-linear transformations. Since compositions, additions and multiplications of continuous\nfunctions are continuous, the function N is continuous. Since N is a continuous function over a compact space and since the set of polynomials is an Algebra that separates points it follows from the Stone-Weierstrass theorem (Stone, 1948) that there exists a polynomial P such that\nsup x∈X ‖N(x)− P (x)‖ < .\nNote that the assumption that the non-linearity is continuous is very mild since the back propagation algorithm used for learning neural networks assumes the existence of a gradient or a sub-gradient to these functions which implies continuity. Theorem 1. Let (E,D) be the encryption and decryption functions of a HE system. Let N be a neural network in which all non-linear transformations are continuous. Let X ⊂ Rn be the domain on which N acts and assume that X is compact, then for every > 0 there exists a function N ′ such that\nsup x∈X ‖N(x)−D (N ′ (E(x)))‖ <\nNote that for a vector x = (x1, . . . , xn) we use the notation E(x) = (E(x1), . . . , E(xn)) and D(x) = (D(x1), . . . , D(xn))\nProof. From Lemma 1 it follows that there exists a polynomial P such that supx∈X ‖N(x)− P (x)‖ < . N ′ can be constructed from P by replacing the addition and multiplications by the appropriate HE functions (⊕,⊗) and by replacing the constants in the polynomials by the encrypted versions of these constants. This can be done by accessing only the public encryption function E.\nTheorem 2 shows that an existing neural network can be applied to encrypted data. This is done by a two stage process: first the network is approximated by a polynomial and next this polynomial is ”encrypted”. Next we look at the learning process. The common way to learn a neural network is using back-propagation. This is a gradient descent type algorithm. That requires computing the derivative of the neural network with respect to the weights. If the neural network is a polynomial function (or is approximated by one) then the derivatives are polynomials as well and hence can be computed over encrypted data. However, some further restrictions are needed in some cases. Theorem 2. Fix the topology of a neural network and assume that all the non-linear transformations and the loss function are polynomials. Then the back propagation algorithm can be converted to work on encrypted data such that it will learn the encrypted version of the coefficients that the back propagation will learn on plain data.\nProof. Since all transformations are polynomials then the function that the neural network computes is a polynomial. Since the loss function is polynomial as well it implies that the gradient is a polynomial too and therefore it can be computed over encrypted data.\nTheorem 2 suggests the following method for learning with encrypted data: first approximate all non-linear transformations with polynomials which will result in a polynomial network that can be learned exactly even when the data is encrypted. Note, however that when learning over encrypted data the results, that is the weights, are encrypted and if the learning algorithm does not have access to the secret key for use in the decryption function D it will not be able to know what these coefficients are.\nAnother approach for learning with encrypted data is to approximate the back-propagation step with polynomials as illustrated by the following theorem. Theorem 3. Assume that the domain of the networkX ⊂ Rn is compact. Assume that the non-linear transformations and the loss function have continuous derivatives. Let L be the back-propagation learning algorithm that maps a sample of size T to the weight vector of the neural net. For every > 0 there exists a learning algorithm L′ such that if L learns the weights w1, . . . , wk from the sample S of size T then L′ learns the weights E(w′1), . . . , E(w ′ k) form the sample E(S) such that ∀j, |wj − w′j | < .\nThe proof is very similar to previous proofs and therefore we skip details.\nProof. The learning algorithm L is made of addition, multiplication and compositions of the constants, non-linear transformations, the loss function and their gradients. According to the assumption of this theorem, all these functions are continuous and therefore L is a continuous function over a compact space which can be approximated by a polynomial. The algorithm L′ is this polynomial approximation of L after all constants have been replaced by their encrypted versions and additions and multiplications have been replaced by the ⊕,⊗ operations."
    }, {
      "heading" : "5 PRACTICAL CONSIDERATION",
      "text" : "In Section 4 we have shown that it is possible to learn neural networks over encrypted data and to apply neural networks to encrypted data. However, some scenarios may be infeasible due to excessive computational complexity. In this section we discuss practical considerations in more details.\nWhile HE schemes allow the evaluation of polynomial functions, these computations are much slower than computations done on plain data. Furthermore, in current implementations of HE, high degree polynomials are slower to compute than lower degree polynomials. The reason for that, in a nut shell, is that as part of the encryption process some random noise is added to the message. When adding two numbers via the ⊕ operation, the noise in the resulting ciphertext increases linearly with respect to the number of additions, however, when multiplying, the noise grows super–linearly. For an FHE scheme, when the noise size reaches a certain level, a time consuming cleaning process is performed which slows down the entire process. For HE schemes as the one considered in this work, the parameters of the scheme have to be chosen to accommodate the noise growth incurred by the desired computation. A higher complexity requires larger parameters, which leads to slower execution of the algorithms. Therefore, special considerations should be taken to approximate the neural network with polynomials with the lowest degree possible.\nLet N be a neural network with l layers. If the composition of the activation function and pooling functions in each layer is approximated by a polynomial of degree d then the polynomial approximation of N will be a polynomial of degree dl since when composing polynomials, the degrees of the polynomials multiply. Therefore, in order to end up with low degree polynomials, we need both d and l to be small. Minimizing d, the degree of the polynomial approximation to non-linear functions, is a standard exercise in approximation theory. Tools, such as, Chebyshev polynomials, can be used to find optimal or close to optimal approximations. Even more significant is minimizing the number of layers l. This goes against the current trend of learning deep neural networks. However, recent work on model compression (Bucilu et al., 2006; Ba & Caruana, 2014) show that deep nets can be closely approximated by shallow nets (1-2 hidden layers). These studies suggest that the success of deep nets might be due to better optimization and not necessarily from the kind of function space spanned by deep nets. Therefore, once you have a deep net, you can use it to train a shallow net by labeling a large set of unlabeled instances. This procedure converts deep nets to shallow, but wider, nets. In terms of polynomials, the deep nets convert to high degree polynomials while the shallow but wide nets convert to low degree polynomials with many monomials. Hence this conversions results in polynomials that are faster to execute on encrypted data.\nWhile inference using crypto-nets may be feasible, learning is a more difficult to scale tasks. Training neural networks is a computational intensive task. Even without encryption, high throughput computing units such as GPUs or multi-node clusters are needed to make learning neural nets feasible on large datasets (Dean et al., 2012; Coates et al., 2013). Furthermore, assuming, as before, that the neural network has l layers such that each layer is approximated by a polynomial of degree d results in the neural network of degree dl. The gradient of this network, with respect to the weight vector, is a polynomial of the same degree. To make gradient step, the gradient polynomial is evaluated on the value predicted by the current network. Therefor, the gradient step is a polynomial of degree d2l. On top of that, the loss function needs to be taken into account which will make the degree even higher. Hence, learning from encrypted data in the way proposed here is feasible only for small datasets or for simple models such as linear models."
    }, {
      "heading" : "6 DISCUSSION",
      "text" : "In Section 1 we have seen that from a theoretical point of view, it is possible to learn over encrypted data as well as to apply networks to encrypted data. However, in Section 5 we have seen that from practical consideration, some applications of crypto-nets are not feasible with the current construction. Therefore, it makes sense to study different use-cases and discuss the theoretical and practical implications of these scenarios.\nDoing inference with crypto-nets is a promising direction. In this scenario, the net is learned over plain data and is applied to encrypted data. For example, consider a dentist that may take X-ray images of suspect tooth and send them to be classified in a cloud service. With crypto-nets, the dentist can encrypt the image and send for evaluation without compromising the privacy of clients since not only the image is encrypted but also the prediction is only visible to the dentist and not to the owner of the predictive models. Another example includes a client that would like to apply for a loan from a bank. Currently, the client has to reveal private financial details to allow the bank to predict the risk associated with the loan. However, with crypto-nets, this can be done without revealing any private information. At the same time, inference over encrypted data is still slower than inference on plain data and hence suitable only in cases where latency and throughput are not major concerns.\nLearning with crypto-nets requires more detailed inspection. We propose three scenarios of learning with encrypted data.\n1. Assume that a sample is encrypted and the goal is to learn a model from this sample. As discussed in Section 4, the theory suggests that this is possible. However, in practice this is feasible only if the sample is small or the network is shallow.\n2. Assume that there are multiple samples, each encrypted with a different key, and the goal is to learn a model by aggregating these datasets. This is the case, for example, if multiple dentists store the medical records of their patients, each dentist using a different key. This scenario is not supported by the kind of homomorphic encryption we discussed so far. However, this could be addressed by secure multi-party computation (Du & Atallah, 2001). López-Alt et al. (2012) presented a fully homomorphic encryption scheme that allows joint computation over data that was encrypted with different keys. The result would be owned by all parties that contributed data in the sense that decryption requires all data owners who contributed data to the computation to jointly decrypt.\n3. Assume that a model has been trained using plain data but users may wish to adapt it to their data. Therefore, the model is already trained and the goal is to perform few gradient steps to fine tune it. This scenario is theoretically feasible and may be practical provided that the data size is small and that the network can be approximated by a polynomial of not-too-high degree."
    }, {
      "heading" : "7 CONCLUSION",
      "text" : "In this work we have presented crypto-nets: a way to learn and apply neural networks to encrypted data. We have discussed the theoretical aspects of learning and inferencing over encrypted data as well as the practical implications. We conjecture that for medical and financial applications, cryptonets may be feasible for the inference stage and maybe even for some limited learning. Implementing crypto-nets require careful work both in the machine learning side and in the cryptology side and is subject of ongoing research."
    } ],
    "references" : [ {
      "title" : "Do deep nets really need to be deep",
      "author" : [ "J. Ba", "R. Caruana" ],
      "venue" : "In Proceedings of the Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Ba and Caruana,? \\Q2014\\E",
      "shortCiteRegEx" : "Ba and Caruana",
      "year" : 2014
    }, {
      "title" : "Private predictive analysis on encrypted medical data",
      "author" : [ "Bos", "Joppe W", "Lauter", "Kristin", "Naehrig", "Michael" ],
      "venue" : "Journal of biomedical informatics,",
      "citeRegEx" : "Bos et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bos et al\\.",
      "year" : 2014
    }, {
      "title" : "Machine learning classification over encrypted data",
      "author" : [ "Bost", "Raphael", "Popa", "Raluca Ada", "Tu", "Stephen", "Goldwasser", "Shafi" ],
      "venue" : "Cryptology ePrint Archive, Report 2014/331,",
      "citeRegEx" : "Bost et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bost et al\\.",
      "year" : 2014
    }, {
      "title" : "Efficient fully homomorphic encryption from (standard) LWE",
      "author" : [ "Brakerski", "Zvika", "Vaikuntanathan", "Vinod" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "Brakerski et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Brakerski et al\\.",
      "year" : 2014
    }, {
      "title" : "Differentially private empirical risk minimization",
      "author" : [ "Chaudhuri", "Kamalika", "Monteleoni", "Claire", "Sarwate", "Anand D" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Chaudhuri et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Chaudhuri et al\\.",
      "year" : 2011
    }, {
      "title" : "Deep learning with cots hpc systems",
      "author" : [ "Coates", "Adam", "Huval", "Brody", "Wang", "Tao", "Wu", "David", "Catanzaro", "Bryan", "Andrew", "Ng" ],
      "venue" : "In Proceedings of The 30th International Conference on Machine Learning,",
      "citeRegEx" : "Coates et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Coates et al\\.",
      "year" : 2013
    }, {
      "title" : "Large scale distributed deep networks",
      "author" : [ "Dean", "Jeffrey", "Corrado", "Greg", "Monga", "Rajat", "Chen", "Kai", "Devin", "Matthieu", "Mao", "Mark", "Senior", "Andrew", "Tucker", "Paul", "Yang", "Ke", "Le", "Quoc V" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Dean et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Dean et al\\.",
      "year" : 2012
    }, {
      "title" : "Secure multi-party computation problems and their applications: a review and open problems",
      "author" : [ "Du", "Wenliang", "Atallah", "Mikhail J" ],
      "venue" : "In Proceedings of the 2001 workshop on New security paradigms,",
      "citeRegEx" : "Du et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Du et al\\.",
      "year" : 2001
    }, {
      "title" : "Privacy aware learning",
      "author" : [ "Duchi", "John C", "Jordan", "Michael I", "Wainwright", "Martin J" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Duchi et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2012
    }, {
      "title" : "Differential privacy: A survey of results",
      "author" : [ "Dwork", "Cynthia" ],
      "venue" : "In Theory and Applications of Models of Computation,",
      "citeRegEx" : "Dwork and Cynthia.,? \\Q2008\\E",
      "shortCiteRegEx" : "Dwork and Cynthia.",
      "year" : 2008
    }, {
      "title" : "Fully homomorphic encryption using ideal lattices",
      "author" : [ "Gentry", "Craig" ],
      "venue" : "In STOC,",
      "citeRegEx" : "Gentry and Craig.,? \\Q2009\\E",
      "shortCiteRegEx" : "Gentry and Craig.",
      "year" : 2009
    }, {
      "title" : "ML confidential: Machine learning on encrypted data",
      "author" : [ "Graepel", "Thore", "Lauter", "Kristin", "Naehrig", "Michael" ],
      "venue" : "In Information Security and Cryptology–ICISC",
      "citeRegEx" : "Graepel et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Graepel et al\\.",
      "year" : 2012
    }, {
      "title" : "Can homomorphic encryption be practical",
      "author" : [ "Lauter", "Kristin", "Naehrig", "Michael", "Vaikuntanathan", "Vinod" ],
      "venue" : "In Proceedings of the 3rd ACM workshop on Cloud computing security workshop,",
      "citeRegEx" : "Lauter et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Lauter et al\\.",
      "year" : 2011
    }, {
      "title" : "Private computation on encrypted genomic data",
      "author" : [ "Lauter", "Kristin", "López-Alt", "Adriana", "Naehrig", "Michael" ],
      "venue" : "LATINCRYPT",
      "citeRegEx" : "Lauter et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Lauter et al\\.",
      "year" : 2014
    }, {
      "title" : "On-the-fly multiparty computation on the cloud via multikey fully homomorphic encryption",
      "author" : [ "López-Alt", "Adriana", "Tromer", "Eran", "Vaikuntanathan", "Vinod" ],
      "venue" : "In STOC,",
      "citeRegEx" : "López.Alt et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "López.Alt et al\\.",
      "year" : 2012
    }, {
      "title" : "Privacy-preserving matrix factorization",
      "author" : [ "Nikolaenko", "Valeria", "Ioannidis", "Stratis", "Weinsberg", "Udi", "Joye", "Marc", "Taft", "Nina", "Boneh", "Dan" ],
      "venue" : "In Proceedings of the 2013 ACM SIGSAC conference on Computer & communications security,",
      "citeRegEx" : "Nikolaenko et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Nikolaenko et al\\.",
      "year" : 2013
    }, {
      "title" : "Privacy-preserving ridge regression on hundreds of millions of records",
      "author" : [ "Nikolaenko", "Valeria", "Weinsberg", "Udi", "Ioannidis", "Stratis", "Joye", "Marc", "Boneh", "Dan", "Taft", "Nina" ],
      "venue" : "In Security and Privacy (SP),",
      "citeRegEx" : "Nikolaenko et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Nikolaenko et al\\.",
      "year" : 2013
    }, {
      "title" : "Oblivious neural network computing via homomorphic encryption",
      "author" : [ "Orlandi", "Claudio", "Piva", "Alessandro", "Barni", "Mauro" ],
      "venue" : "EURASIP Journal on Information Security,",
      "citeRegEx" : "Orlandi et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Orlandi et al\\.",
      "year" : 2007
    }, {
      "title" : "On data banks and privacy homomorphisms",
      "author" : [ "Rivest", "Ronald L", "Adleman", "Len", "Dertouzos", "Michael L" ],
      "venue" : "Foundations of secure computation,",
      "citeRegEx" : "Rivest et al\\.,? \\Q1978\\E",
      "shortCiteRegEx" : "Rivest et al\\.",
      "year" : 1978
    }, {
      "title" : "Privacy-preserving statistical estimation with optimal convergence rates",
      "author" : [ "Smith", "Adam" ],
      "venue" : "In Proceedings of the forty-third annual ACM symposium on Theory of computing,",
      "citeRegEx" : "Smith and Adam.,? \\Q2011\\E",
      "shortCiteRegEx" : "Smith and Adam.",
      "year" : 2011
    }, {
      "title" : "The generalized Weierstrass approximation theorem",
      "author" : [ "M.H. Stone" ],
      "venue" : "Mathematics Magazine,",
      "citeRegEx" : "Stone,? \\Q1948\\E",
      "shortCiteRegEx" : "Stone",
      "year" : 1948
    }, {
      "title" : "A statistical framework for differential privacy",
      "author" : [ "Wasserman", "Larry", "Zhou", "Shuheng" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Wasserman et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Wasserman et al\\.",
      "year" : 2010
    }, {
      "title" : "Using homomorphic encryption for large scale statistical analysis",
      "author" : [ "Wu", "David", "Haven", "Jacob" ],
      "venue" : null,
      "citeRegEx" : "Wu et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 18,
      "context" : "A homomorphic encryption scheme (Rivest et al., 1978) allows a certain computation to be performed on the encrypted data by manipulating the corresponding ciphertexts without the need to decrypt them first.",
      "startOffset" : 32,
      "endOffset" : 53
    }, {
      "referenceID" : 11,
      "context" : "This problem has been addressed before by Graepel et al. (2013). They proposed to perform machine learning on encrypted data utilizing homomorphic encryption.",
      "startOffset" : 42,
      "endOffset" : 64
    }, {
      "referenceID" : 11,
      "context" : "This problem has been addressed before by Graepel et al. (2013). They proposed to perform machine learning on encrypted data utilizing homomorphic encryption. A homomorphic encryption scheme (Rivest et al., 1978) allows a certain computation to be performed on the encrypted data by manipulating the corresponding ciphertexts without the need to decrypt them first. A fully homomorphic encryption scheme (Gentry, 2009) allows arbitrary operations over encrypted data and therefore, any function can be computed. However, fully homomorphic encryption schemes are still too inefficient for practical use. One way to obtain better efficiency is to only use so-called somewhat homomorphic schemes that only allow the evaluation of functions up to a certain complexity. Such schemes are often the cores of corresponding fully homomorphic encryption schemes. They usually provide operations corresponding to addition and multiplication of encrypted integer values, and therefore, are suitable to evaluate polynomial functions up to a certain maximal degree. The required degree of the polynomial function along with the desired security level determines the scheme parameters and thus has great implications on the size of the ciphertext as well as the computational complexity of the cryptographic operations. Therefore, Graepel et al. (2013) suggested using linear or other low degree models.",
      "startOffset" : 42,
      "endOffset" : 1338
    }, {
      "referenceID" : 1,
      "context" : "Using Homomorphic Encryption (HE) to do machine learning and statistical analysis on encrypted data has been investigated in (Bos et al., 2014; Bost et al., 2014; Graepel et al., 2013; Lauter et al., 2014; Nikolaenko et al., 2013a;b; Wu & Haven, 2012).",
      "startOffset" : 125,
      "endOffset" : 251
    }, {
      "referenceID" : 2,
      "context" : "Using Homomorphic Encryption (HE) to do machine learning and statistical analysis on encrypted data has been investigated in (Bos et al., 2014; Bost et al., 2014; Graepel et al., 2013; Lauter et al., 2014; Nikolaenko et al., 2013a;b; Wu & Haven, 2012).",
      "startOffset" : 125,
      "endOffset" : 251
    }, {
      "referenceID" : 13,
      "context" : "Using Homomorphic Encryption (HE) to do machine learning and statistical analysis on encrypted data has been investigated in (Bos et al., 2014; Bost et al., 2014; Graepel et al., 2013; Lauter et al., 2014; Nikolaenko et al., 2013a;b; Wu & Haven, 2012).",
      "startOffset" : 125,
      "endOffset" : 251
    }, {
      "referenceID" : 1,
      "context" : ", 2013b; Wu & Haven, 2012), linear classifiers (Bos et al., 2014; Bost et al., 2014; Graepel et al., 2013), decision trees (Bost et al.",
      "startOffset" : 47,
      "endOffset" : 106
    }, {
      "referenceID" : 2,
      "context" : ", 2013b; Wu & Haven, 2012), linear classifiers (Bos et al., 2014; Bost et al., 2014; Graepel et al., 2013), decision trees (Bost et al.",
      "startOffset" : 47,
      "endOffset" : 106
    }, {
      "referenceID" : 2,
      "context" : ", 2013), decision trees (Bost et al., 2014), matrix factorization (Nikolaenko et al.",
      "startOffset" : 24,
      "endOffset" : 43
    }, {
      "referenceID" : 4,
      "context" : "Another line of work focuses on differential privacy (Chaudhuri et al., 2011; Duchi et al., 2012; Dwork, 2008; Smith, 2011; Wasserman & Zhou, 2010).",
      "startOffset" : 53,
      "endOffset" : 147
    }, {
      "referenceID" : 8,
      "context" : "Another line of work focuses on differential privacy (Chaudhuri et al., 2011; Duchi et al., 2012; Dwork, 2008; Smith, 2011; Wasserman & Zhou, 2010).",
      "startOffset" : 53,
      "endOffset" : 147
    }, {
      "referenceID" : 18,
      "context" : "A Homomorphic Encryption (HE) scheme (Rivest et al., 1978) preserves some structure of the original message space.",
      "startOffset" : 37,
      "endOffset" : 58
    }, {
      "referenceID" : 12,
      "context" : "Efficiency can be increased by restricting to somewhat homomorphic schemes and by operating on integers instead of bits, see Lauter et al. (2011). With this approach, both the computational complexity and the length of ciphertexts increase with the number of desired operations performed on the encrypted data in order to guarantee correct decryption after polynomial evaluation.",
      "startOffset" : 125,
      "endOffset" : 146
    }, {
      "referenceID" : 20,
      "context" : "If the domain, that is the input space, is a compact set, then from the StoneWeierstrass theorem (Stone, 1948) it follows that it can be approximated uniformly by polynomials.",
      "startOffset" : 97,
      "endOffset" : 110
    }, {
      "referenceID" : 20,
      "context" : "Since N is a continuous function over a compact space and since the set of polynomials is an Algebra that separates points it follows from the Stone-Weierstrass theorem (Stone, 1948) that there exists a polynomial P such that",
      "startOffset" : 169,
      "endOffset" : 182
    }, {
      "referenceID" : 6,
      "context" : "Even without encryption, high throughput computing units such as GPUs or multi-node clusters are needed to make learning neural nets feasible on large datasets (Dean et al., 2012; Coates et al., 2013).",
      "startOffset" : 160,
      "endOffset" : 200
    }, {
      "referenceID" : 5,
      "context" : "Even without encryption, high throughput computing units such as GPUs or multi-node clusters are needed to make learning neural nets feasible on large datasets (Dean et al., 2012; Coates et al., 2013).",
      "startOffset" : 160,
      "endOffset" : 200
    }, {
      "referenceID" : 14,
      "context" : "López-Alt et al. (2012) presented a fully homomorphic encryption scheme that allows joint computation over data that was encrypted with different keys.",
      "startOffset" : 0,
      "endOffset" : 24
    } ],
    "year" : 2017,
    "abstractText" : "The problem we address is the following: how can a user employ a predictive model that is held by a third party, without compromising private information. For example, a hospital may wish to use a cloud service to predict the readmission risk of a patient. However, due to regulations, the patient’s medical files cannot be revealed. The goal is to make an inference using the model, without jeopardizing the accuracy of the prediction or the privacy of the data. To achieve high accuracy, we use neural networks, which have been shown to outperform other learning models for many tasks. To achieve the privacy requirements, we use homomorphic encryption in the following protocol: the data owner encrypts the data and sends the ciphertexts to the third party to obtain a prediction from a trained model. The model operates on these ciphertexts and sends back the encrypted prediction. In this protocol, not only the data remains private, even the values predicted are available only to the data owner. Using homomorphic encryption and modifications to the activation functions and training algorithms of neural networks, we present crypto-nets and prove that they can be constructed and may be feasible. This method paves the way to build a secure cloud-based neural network prediction services without invading users’ privacy.",
    "creator" : "LaTeX with hyperref package"
  }
}