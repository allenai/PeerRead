{
  "name" : "1206.6850.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Visualization of Collaborative Data",
    "authors" : [ "Guobiao Mei", "Christian R. Shelton" ],
    "emails" : [ "gmei@cs.ucr.edu", "cshelton@cs.ucr.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Collaborative data consist of ratings relating two distinct sets of objects: users and items. Much of the work with such data focuses on filtering: predicting unknown ratings for pairs of users and items. In this paper we focus on the problem of visualizing the information. Given all of the ratings, our task is to embed all of the users and items as points in the same Euclidean space. We would like to place users near items that they have rated (or would rate) high, and far away from those they would give low ratings. We pose this problem as a real-valued non-linear Bayesian network and employ Markov chain Monte Carlo and expectation maximization to find an embedding. We present a metric by which to judge the quality of a visualization and compare our results to Eigentaste, locally linear embedding and cooccurrence data embedding on three real-world datasets."
    }, {
      "heading" : "1 Introduction",
      "text" : "Collaborative data, which are composed of correlated users and items, are abundant: movie recommendations, music rankings, and book reviews, for example. Most of the work on such data has been in the area of collaborative filtering: making predictions or recommendations based on prior user ratings.\nHowever, no previous algorithms have approached the problem of visualizing collaborative information. In this work, we initiate this problem and propose an approach. Many visualization problems are “soft” in nature and it is difficult to compare alternative methods. For this task, we introduce a simple evaluation criterion which is natural and allows for numeric comparisons of possible visualizations.\nUsing all the ratings, the visualization problem is to extract the intrinsic similarities or dissimilarities between all\nthe users and items involved, and represent them graphically. This has a wide range of applications, for example guided on-line shopping. Traditional stores allow for easy browsing by physically walking up and down the aisles and visually inspecting the store’s contents. Such browsing is not easy on-line. Amazon.com, for instance, has thousands of items in many categories. While collaborative filtering allows an on-line seller to recommend a list of objects that the buyer might also like, it does not supply a good way of browsing an on-line collection in a more free-form fashion. We propose building an embedded graph of all the items using the collaborative rating data, and allowing the shopper to zoom in on a portion of the graph and scroll around as he or she searches for items of interest. If constructed well, nearby items will also be of interest to the shopper and local directions in the space will have “meaning” to the user. Spatial layouts have been shown in the past to increase interest in exploration and to aid in finding information (Chennawasin et al., 1999).\nWe assume a D-dimensional Euclidean space, which we call the embedded space (for most computer interfaces, D = 2). Each user or item is represented by a point in this space. Intuitively, if two user (or item) points are near each other in the embedded space, the two users (items) are likely to have similar preferences (properties). In the same manner, the closer a user point is to an item point in the embedded space, the higher the rating the user has given (or would give) the item.\nWe formulate the visualization problem in Section 2, and then propose with our approach in Section 3. In Section 4 we show our results on three real-world datasets."
    }, {
      "heading" : "1.1 Prior Work",
      "text" : "There are many existing collaborative filtering algorithms focusing on the task of prediction. We cannot review all of them and they do not directly pertain to the task of visualization. Breese et al. (1998) classifies the approaches into two major categories: memory based and model based. Memory based collaborative filtering algorithms make pre-\ndictions according to all the existing preferences stored beforehand, while model based algorithms first try to learn the parameters of a particular model for the existing user preferences, and then make predictions according to the learned model. Our work is mostly a model based approach.\nSome of the filtering methods have a “geometric” flavor. Pennock et al. (2000) propose a collaborative filter based on personality diagnosis. They associate each user with a vector Rtrue, indicating the true rating of this user for every item in the system. The actual rating is assumed to be a random variable drawn from a Gaussian distribution with mean equals to the corresponding element of that item in the user’s Rtrue vector. If there is a missing rating, the correspondingRtrue element is a uniform random variable.\nGoldberg et al. (2001) propose Eigentaste (ET). It has two phases: offline and online. It treats the entire rating matrix as a high dimensional space and employs principal component analysis for dimensionality reduction during the offline phase. It projects the users into a low dimensional space and then partitions the embedded space into sets of users and uses the maximally rated items in a given set as predictions during the online phase. While this algorithm does place the users in a geometric space, it does not place the items in the same space, and it requires that there be a set of items (the gauge set) which every user has rated.\nSaul and Roweis (2003) propose locally linear embedding (LLE) as an algorithm for embedding points into a (low dimensional) space. It is not geared to collaborative data: it does not deal well with sparse data, and it can only embed either users or items. However, it has had good success with non-linear embeddings, so it is a candidate method for the collaborative visualization problem.\nCo-occurrence data have been used to produce embeddings of two classes of objects in the same space. CODE (Globerson et al., 2005) is one such recent example. It tries to embed objects of two types into the same Euclidean space based on their co-occurrence statistics. Unfortunately, collaborative filtering data are usually given as ratings and not co-occurrence statistics. Even if we take the ratings to be proportional to the corresponding co-occurrences (an unjustified assumption), we still have missing statistics (which cannot be taken to be zero). Co-occurrence algorithms do not currently deal with such missing values.\nNone of the above three algorithms (ET, LLE and CODE) was developed for our task. However, there are no other algorithms designed for collaborative visualization. Therefore, we compare to them (to the extent possible) in Section 4."
    }, {
      "heading" : "2 The Visualization Problem",
      "text" : "We first introduce our notations in 2.1. In 2.2 we formulate the visualization problem of collaborative data. We specify\nour distributional assumptions in 2.3."
    }, {
      "heading" : "2.1 Notation",
      "text" : "Let U = {u1, u2, . . . , um} and G = {g1, g2, . . . , gn} be the sets of all users and items, respectively. Without ambiguity, we will use ui to refer both to the i-th user and to the corresponding point in the embedded space for that user. We use the same notation for gj . We define δij to be 1 if ui rated gj and 0 otherwise.\nLet rij be the rating of ui of gj if δij = 1. Here we normalize all the ratings to the range [0, 1] (i.e. 1 is the highest rating, and 0 is the lowest). Let R = {rij | δij = 1}.\nWe further denote Gi = {gj | δij = 1} and U j = {ui | δij = 1}. Gi is the set of all the items that ui rated, and U j is the set of all the users that rated gj ."
    }, {
      "heading" : "2.2 Formulation of the Problem",
      "text" : "The visualization problem is to find an embedding of all the points U and G in a Euclidean space we call the embedded space. The embedding should be one in which the distance between a user and an item is related to the corresponding rating.\nIn the embedded space, we assume each ui and gj are random variables drawn independently from prior distributions Pu(ui) and Pg(gj). We introduce a rating function f : <+0 7→ [0, 1], which maps the distance between two points (a user and an item) in the embedded space to a real value on [0, 1]: the expected rating for the two points. f(x) is a monotonically non-increasing function, f(0) = 1, and f(∞) = 0. Intuitively, two points with a smaller mutual distance should have a higher expected rating. At this point, we will assume that the rating function f(x) is given. In Section 3.4 we will show how this function can be learned from data. The actual rating rij between two points ui and gj in the embedded space is a random variable drawn from a distribution Pf (rij | ui, gj) with mean f(‖ui − gj‖).\nGiven all the ratings,R, as evidence and the rating function, f , our task is to put all the user points U and item points G into the embedded space so that the likelihood of observed ratings R is maximized. That is, we want to find the U and G points that maximize the posterior:\n[U∗, G∗] = argmax U,G P (U,G | R) (1)\n= argmax U,G ∏ i,j|δij=1 Pf (rij |ui, gj) ∏ i Pu(ui) ∏ j Pg(gj) .\nP (U,G,R) is a real-valued Bayesian network in which each user and item variable has no parents and each rating variable has two parents (one user and one item). The ratings are given as evidence and the task is to determine the\nmost probable joint assignment to the user and item variables given the ratings."
    }, {
      "heading" : "2.3 Gaussian Assumptions",
      "text" : "We assume that all the distributions are from the Gaussian family. To be specific,\nPu = N (0,Σu) Pg = N (0,Σg)\nPf (rij |ui, gj) = N (f(‖ui − gj‖), σr) .\nNote that while these distributions are all normal, the function f is non-linear and therefore the resulting joint distribution is not Gaussian."
    }, {
      "heading" : "3 Approach",
      "text" : "It is intractable to compute the posterior in Equation 1 directly. We use Markov chain Monte Carlo sampling."
    }, {
      "heading" : "3.1 Metropolis-Hastings Algorithm",
      "text" : "In particular, we use the Metropolis-Hastings (MH) algorithm (Metropolis et al., 1953), which was extended to graphical models. Given a graphical model over the random variables X = {x1, x2, . . . , xN}, assume a target distribution π over X . For each variable xi, there is an associated proposal distribution Qxi , the distribution of new samples for that variable.\nGiven a current assignment to X , MH randomly picks a variable xi and tries to replace its value with a new sample x′i drawn from the proposal Qxi . Let Y = X − {xi}.\nThe transition gain ratio for changing the sample xi to x′i is defined as\nT Q(xi x′i) = Qx′i(xi)π(Y, x\n′ i)\nQxi(x′i)π(Y, xi) . (2)\nThe probability of accepting this new sample x′i is\nA(xi x′i) = min { 1, T Q(xi x′i) } . (3)\nUsing the local independencies of the graph, this can be decomposed into a set of local probabilities."
    }, {
      "heading" : "3.2 Sampling Approach",
      "text" : "In our visualization problem, π is the posterior distribution of U andG givenR (Equation 1). Initially, we sample from Pu for every ui and sample from Pg for every gj . This jointly form a single starting sample (a joint assignment to U and G) for our MCMC method.\nWe use proposal distributions Qui for the node ui and Qgj for the node gj . We set the proposal distributions to be\nGaussian with means at the previous embedded position:\nQui = N (ui,Σ′u) Qgj = N (gj ,Σ′g) .\nIf we choose the node ui to be resampled, we draw u′i from Qui , and then compute the accept ratio for this change according to Equation 3. Using the local independence properties, the transition gain ratio with respect to the rating function f is given by\nT Qf (ui u′i) = Qu′i(ui)Pu(u\n′ i) ∏ j∈Gi Pf (rij |u′i, gj)\nQui(u′i)Pu(ui) ∏ j∈Gi Pf (rij |ui, gj) . (4)\nSimilarly, the transition gain ratio for an item node, gj is\nT Qf (gj g′j) = Qg′j (gj)Pg(g\n′ j) ∏ i∈Uj Pf (rij |ui, g′j)\nQgj (g′j)Pg(gj) ∏ i∈Uj Pf (rij |ui, gj) . (5)\nWe repeat the above resampling phase until the process has mixed. The stationary distribution of this procedure is the true posterior P (U,G|R)."
    }, {
      "heading" : "3.3 Simulated Annealing",
      "text" : "Recall that we want the argmaxU,G P (U,G|R). The Metropolis-Hastings algorithm will give us joint samples of U and G, drawn from that posterior. To get the samples that maximize the posterior, we modify the standard MH algorithm along the lines of the simulated annealing (SA) algorithm (Kirkpatrick et al., 1983).\nIn particular, we modify Equation 2 to add an annealing temperature, β:\nT Q(xi x′i) = Qx′i(xi)π(Y, x\n′ i) β\nQxi(x′i)π(Y, xi)β\nThe transition gain ratios of equations 4 and 5 are then\nT Qf (ui u′i) = Qu′i(ui)\n[ Pu(u′i) ∏ j∈Gi Pf (rij |u′i, gj) ]β\nQui(u′i) [ Pu(ui) ∏ j∈Gi Pf (rij |ui, gj) ]β\nT Qf (gj g′j) = Qg′j (gj)\n[ Pg(g′j) ∏ i∈Uj Pf (rij |ui, g′j) ]β\nQgj (g′j) [ Pg(gj) ∏ i∈Uj Pf (rij |ui, gj) ]β\nThe added temperature factor β grows gradually from 1 to ∞. Initially β = 1, and this method is the same as the\nstandard Metropolis-Hastings algorithm. As β grows, the simulated annealing algorithm penalizes changes resulting in lower likelihoods; the algorithm tends to only climb uphill in the posterior distribution."
    }, {
      "heading" : "3.4 Learn the Rating Function",
      "text" : "Until now, we have assumed that the rating function f was known. However, we would like this function to adapt to the collaborative data.\nIt would be straight-forward to select f from a family (for example the exponential, f(x) = e−λx). However, the actual rating function may have a very different shape. Instead we note that all collaborative datasets of which we are aware have a finite number of values for the ratings. Many are binary (“like” or “do not like”) and others are based on a five- or ten-point scale. Continuous, real-valued ratings are seldom used. We therefore let f be a step function with discrete quantizations.\nWe discretize f into K quantizations. Let Θ = {θi | i = 1, . . . ,K} be the set of K splitting points in sorted order, with θK = ∞. Given the set of splitting points Θ, the rating function is\nf(x; Θ) = 1− i− 1 K − 1 , if θi−1 ≤ x < θi .\nThe problem of learning f is now transformed to the problem of learning Θ:\nΘ∗ = argmin Θ ∑ i,j|δij=1 E[(f(‖ui − gj‖; Θ)− rij)2] (6)\nwhere the expectation is with respect to the posterior distribution over U and G. This formulation is equivalent to maximizing the probability of the ratings; the squared error in the above equation comes directly from the Gaussian assumption regarding the distribution Pf .\nWe use expectation maximization (EM) algorithm (Dempster et al., 1977) to learn the rating function. We initially set Θ = Θ0, a random starting point that meets our requirements for f .\nThe E-step employs MH to sample from the expectations in Equation 6 using the rating function fk = f(·; Θk) at the k-th iteration.\nThe M-step updates Θk+1 based on the generated sample configurations of the embedded space (which approximate the expectations of Equation 6). Using all the ui and gj points, the optimal rating function is updated according to Equation 6. Let N be the number of terms in the summation of Equation 6 (one for each rating for each sample). The M-step optimization can be done efficiently (and exactly) in O(NK) time using dynamic programming.\nBefore updating the rating function in the M-step, we renormalize all the points in the embedding space. Due to our assumptions that Pu and Pg are fixed Gaussian distributions with zero mean and that we have the freedom to change f , if the above procedure were run without modification, all the points would collapse together toward the origin. Consequently, the learned rating function would have splitting points with smaller and smaller values. We fix this by a simple normalization step that scales and translates the points to reset the mean of all of the points to zero and the variance of their positions to one. Note that this is not a general “whitening” step in that we only multiply the points by a scalar, not a matrix.\nAlgorithm 1 [U,G]⇐Embed-Graph(R,D) Inputs: R: rating matrix, D: embedding dimensionality Outputs: U and G: embedded points\nβ ⇐ 1 Pu ⇐ N (0,Σu), Pg ⇐ N (0,Σg) Qui ⇐ N (ui,Σ′u), Qgj ⇐ N (gj ,Σ′g) Pf (rij | ui, gj)⇐ N (f(‖ui − gj‖), σr) f ⇐ f(·; Θ0) Sample {ui ∼ Pu}mi=1 Sample {gj ∼ Pg}nj=1 repeat\n// E-Step: S ⇐ ∅ for k = 1 to lb + ls do\nRandomly pick a point xi from samples in [U,G] if xi is a user point ui then\nSample u′i ∼ Qui ui ⇐ u′i with probability Af (ui u′i) else if xi is an item point gj then Sample g′j ∼ Qgj gj ⇐ g′j with probability Af (gj g′j) end if if k > lb then // burn in for lb iterations\nAdd (U,G) to S // Save last ls iterations end if [U,G]⇐normalize([U,G])\nend for β ⇐ (1 + )β\n// M-Step: f(·; Θ)⇐learned rating function using S\nuntil The current sample [U,G] is stable"
    }, {
      "heading" : "3.5 Overview of the Full Algorithm",
      "text" : "To put everything together, the overall algorithm in our approach is listed in Algorithm 1. The parameters of this algorithm are ls (the number of samples used for estimating the expectation), lb (the number of samples necessary for the MCMC process to converge), (the amount by which to increase β), and the variances of the Gaussian distribu-\ntions. In Section 4.2 we specify these quantities for the experiments we ran."
    }, {
      "heading" : "4 Experiments and Results",
      "text" : "We discuss our three datasets, our methodology for comparison, and then compare our algorithm to three others."
    }, {
      "heading" : "4.1 Experiment Datasets",
      "text" : "The SAT dataset contains SAT II subject examination scores for 40 questions chosen from a study guide of historic questions and 296 users. SAT II is a standard exam taken by high school seniors applying to colleges in the United States. All the scores are either 0 or 1 (indicating whether the student got the question correct), and there are no missing values. The 40 questions are from the subjects French, Mathematics, History and Biology. The exam was administered on-line over the course of one week.\nThe BGG dataset comes from www.boardgamegeek.com which contains ratings from thousands of game players and thousands of board games. The ratings range, in halfinteger increments, from 0 to 10. We picked the 400 users and 80 games with the highest rating density. The rating matrix density, which we define as P i,j δijP i,j 1\n, is 63.4% for this subset. Our snapshot of the dataset is from January 2005. The ratings are available publicly from the website.\nFinally, the MovieLens dataset contains ratings from users on a variety of movies. All the ratings are integers from 1 to 5. We picked 400 users and 50 movies, again to maximize the rating density. The rating matrix density is 41.0% on this subset. This dataset is publicly available from movielens.umn.edu."
    }, {
      "heading" : "4.2 Algorithm Initialization",
      "text" : "Because we are learning the rating function f , the absolute positions of the embedded points will not affect our approach directly. Rather, the relative positions of each point matter. Therefore, the overall scale of Σu and Σg do not affect the result.\nIn particular, for all the three datasets, we set Σu,Σg,Σ′u and Σ′g each to be the identity matrix. We set σr = 0.25 for SAT, σr = 0.1 for MovieLens, and σr = 0.05 for BGG. These values directly reflect the discretization of the rating scores. Our informal tests show that the algorithm is not sensitive to these particular numbers and we have made no effort to tune them.\nFor the rating function f , we choose to set Θ0 directly using an M-step from the samples drawn from their priors. For our experiments, we set ls = 2000, lb = 1000, and = 0.02 for all three datasets."
    }, {
      "heading" : "4.3 Implementation Issues",
      "text" : "We compare our results with Locally Linear Embedding (LLE) (Saul & Roweis, 2003), Eigentaste (ET) (Goldberg et al., 2001) and co-occurrence data embedding (CODE) (Globerson et al., 2005). None of the algorithms is exactly suited to our problem, so we discuss our adaptations in this section.\nIf we consider the rating matrix as a set of points in the high dimensional space, we can use LLE to embed them into a lower dimensional space. The LLE algorithm requires a full rating matrix R. This is not available for the MovieLens and BGG datasets. We use linear regression to fill the missing ratings. To be specific, we first fill each missing rating rij with average rating of ui. This results in full rating matrix. To predict the missing rating rij using linear regression, for each item gk, let xk = [r1k, . . . , r(i−1)k, r(i+1)k, . . . , rmk]>, i.e. xk is the vector containing all the ratings for gk except from ui. We then use linear regression to find[\nŵi, b̂i ] = argmin\nw,b ∑ k (w>xk + b− rik)2\nThe predicted rating is then given by\nr̂ij = ŵ>i xj + b̂i .\nBoth LLE and ET can embed either users or items into an Euclidean space. Yet, neither of them can embed both in the same space. We tried several ways to extend them and to make them comparable. One straight-forward way is to embed all the user points first into the space. Then for every item, find all the users who gave it its highest rating, and place this item at the mean of those users points.\nFor our results, we used an alternative method, which performed better than the one above. Let R̂ be the full rating matrix filled in using linear regression. We introduce a correlation matrix C among all n items. The diagonal Cii is set to 1. Let Ri be the i-th column of R̂,\nCij = R>i Rj\n‖Ri‖ · ‖Rj‖ .\nWe then let X = [C R̂>] and use LLE or ET to embed X into the target Euclidean space. The first n points correspond to the items and the last m points to the users.\nET only works if there is a gauge set of items which all users have rated. However, in the MovieLens and BGG datasets, no such gauge set exists. Using the above regression technique to fill in a gauge set results in bad (and misleading) results, so we omitted them and have only included ET results for the SAT dataset.\nThe CODE algorithm requires co-occurrence statistics between users and items. The relationship between co-\noccurrences and ratings is not clear. However, it is natural to assume rij is proportional to the probability of the co-occurrence of ui and gj . Intuitively, a higher rating indicates it is more likely that the user and item “occur” at the same time. We set the empirical distribution of (u, g) to be proportional to the rating matrix (filled by linear regression if there are missing ratings). We initialize the mappings uniformly and randomly from the set [−0.5, 0.5]D as the starting point for the optimization.\nOur linear regression method for filling in missing values has proven reasonable on the prediction task, but admittedly it is not the most sophisticated algorithm possible. Therefore, to distinguish embedding factors from data completion factors, we also ran our MCMC algorithm on the completed rating matrix from linear regression.\nBoth our method and CODE have variable running times (number of EM iterations in our case, number of random restarts for CODE). For the results reported here, we gave each 30 seconds of CPU time on a 2.8 GHz processor."
    }, {
      "heading" : "4.4 Sample Results",
      "text" : "The SAT data was selected because of our ability to extract a “ground truth.” In particular, we expect that when embedded, the questions from the same subjects should be grouped together. Figure 1 shows the embedding for dimension 2 using the simulated annealing approach (along with the three other approaches).\nThere are ten questions in each category. We can clearly see that our method clusters all the French questions tightly together. The same happens for the Math questions. (There are eight Math questions that overlap in a small area.) The other methods do not produce as tight clusters.\nThe History and Biology questions do not cluster as well. Further data analysis has shown that there is very little predictability in the History and Biology questions, so this result is perhaps not surprising.1\n1The French and Math questions tended to test a body of knowledge that is often retained as a coherent block, where as the History and Biology questions on this exam tended to test more\nFigure 1 also shows that the user points and the item points intermix more evenly with our approach. This meets our expectation that for any user, we can always find things they like or dislike (questions on which they perform well or poorly). In the embedding results of LLE, ET, and CODE, a large number of user points lie in parts of the graph outside the convex hull of the the question points. This makes it impractical to make recommendations to those users based on the visualizations."
    }, {
      "heading" : "4.5 Evaluation Criteria",
      "text" : "There are no prior standard metrics for evaluating the quality of the embedded graph. In this work, we introduce Kendall’s tau (Kendall, 1955) as a suitable evaluation criterion. Kendall’s tau is used to compute the correlation in ordering between two sequences X and Y . It is especially useful for evaluating the correlation between two sequences that may have many ties.\nGiven two sequences X and Y of the same length, a pair (i, j), i 6= j is called concordant if the ordering of Xi and Xj is the same as the ordering of Yi and Yj . By contrast, if the relative ordering is different, this pair (i, j) is called discordant. If Xi = Xj or Yi = Yj , then (i, j) is neither concordant or discordant, and it is called an extra x pair or extra y pair, respectively.\nKendall’s tau is defined as\nτ = C −D√\nC +D + Ey √ C +D + Ex\n(7)\nwhere C is the number of all concordant pairs, and D is the number of all discordant pairs. Ex and Ey are the numbers of extra x pairs and extra y pairs.\nIt is easy to verify that τ is always between−1 and 1. τ = 1 indicates the two sequences have perfect positive correlation, and τ = −1 indicates perfect negative correlation. τ = 0 indicates their orderings are independent.\nTo evaluate the quality of the graph in the embedded space, for each test we randomly select a set of users, Ū , and\nisolated blocks of knowledge.\nitems, Ḡ, as testing users and items. All the ratings between users in Ū and items in Ḡ are held out for testing and are not used in generating the embedding.\nThe embedding algorithms will produce the embedded points for those nodes in Ū and Ḡ given some additional ratings. In order to evaluate the embedding quality, we generate two sequences and compute Kendall’s tau between them: sequence X contains the actual ratings between all the pairs ui ∈ Ū and gj ∈ Ḡ such that δij = 1, and sequence Y contains the distances between the corresponding ui and gj in the embedded graph.\nA good embedding will place ui far from gj if rij is small, and close if rij is large. Kendall’s tau for the above two sequences exactly evaluate this correlation. Denote τ to be the Kendall’s tau for the sequences X and Y . Negative values of τ indicate good embeddings, and we expect the values from embedding algorithms to be smaller than 0 (that of a random embedding)."
    }, {
      "heading" : "4.6 Experimental Results",
      "text" : "We ran our MCMC algorithm both with and without simulated annealing, along with LLE, ET and CODE. We randomly selected one quarter of the users and one quarter of the items for testing (Ū and Ḡ from above). We randomly selected other users and items to form a training set (Ũ and G̃). All ratings between members of G̃ and Ũ , G̃ and Ū , and Ḡ and Ũ are used for training. As stated previously, the ratings between members of Ḡ and Ū are used for testing. It is necessary to include the ratings between G̃ and Ū (and likewise between Ḡ and Ũ ) in order to connect the test users and items with the training users and items.\nBecause the existence of the test set, there are always missing ratings in the rating matrices used. We use linear regression to fill those ratings. We also ran the MCMC algorithm on the same filled data as LLE, ET and CODE used (MCMC-REG in the graphs).\nFor each dataset size (number of items in G̃), we ran 25 independent experiments and recorded the means and stan-\ndard deviations across the experiments for all algorithms. Every algorithm was run on the same set of training and testing sets.\nFor each of these datasets, Figure 2 shows the comparison of our methods to LLE, ET, CODE, and a random embedding, as a function of the size of the training set. We also computed an “ideal embedding” value for τ . Because of ties, Kendall’s tau cannot always reach −1, so we calculate the lowest possible value for τ on the random dataset drawn. This takes nothing into account except ties and it is highly optimistic and probably not obtainable at such low dimensions. The optimal values for SAT, BGG, and MovieLens datasets are approximately −0.63, −0.87 and −0.90 respectively. We ran LLE algorithm with training size starting at 22 for SAT and 24 for MovieLens because of matrix inversion problems for smaller training sizes.\nFigure 3 shows another experiment with the same evaluation criteria. In this experiment, we fix the testing data as usual, and use all the remaining data for training. The plot shows Kendall’s tau as a function of the number of dimensions of the embedding space."
    }, {
      "heading" : "4.7 Analysis of the Results",
      "text" : "From the experiments above, we can see that when the rating matrix is denser, the embedding algorithm achieve better results. Our sampling method, with (MCMC-SA) and without (MCMC) simulated annealing, outperformed LLE, ET and CODE. None of them were designed with this type of data in mind, so we do not present these results to disparage those methods, but there were no other methods available to test against. Note that our MCMC algorithm on linear regression filled data (MCMC-REG) has similar performance to directly applying our MCMC method on data with missing ratings. This implies that it is not our regression that is causing the poor results from the other algorithms, but rather their misfit to this problem. We would also note that our method also seems more stable (smaller variance) than the other algorithms compared.\nOn the SAT dataset, which contains full density of ratings, our algorithms show strong negative correlations which indicate good visualization results. In most cases, using simulated annealing helps improve the quality of embedding (compared to “normal” MCMC). As the training size grows, we have more information on the relations between all the user and item points, and that leads to better performance for all the algorithms.\nThe BGG and MovieLens datasets have many missing ratings and the ratings values are more subjective and therefore noisier. Our algorithm is not as competitive with the “ideal” value for Kendall’s tau, but we feel that this ideal value is wildly optimistic in these settings. Our algorithm does perform better than random embeddings, LLE, and CODE."
    }, {
      "heading" : "5 Conclusions",
      "text" : "We formulated a new problem of visualizing collaborative data. This is a potentially very useful problem. Not only are on-line databases of user ratings growing, but personal databases are also becoming more common. We expect the collaborative visualization problem to be useful in organizing personal music or photography collections as well as on-line shopping.\nWe have not addressed the computational issues nor the stability of the resulting embedding in this work. Both are important problems for on-line deployment in changing databases. Because of the anytime nature of sampling methods and the ease of introducing constraints, we are hopeful that the solution presented here can be adapted to provide stable and adaptive solutions."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank Titus Winters for collecting and sharing the SAT dataset. This work was supported in part by a grant from Intel Research and the UC MICRO program."
    } ],
    "references" : [ {
      "title" : "Empirical analysis of predictive algorithms for collaborative filtering",
      "author" : [ "J. Breese", "D. Heckerman", "C. Kadie" ],
      "venue" : "UAI ’98",
      "citeRegEx" : "Breese et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Breese et al\\.",
      "year" : 1998
    }, {
      "title" : "An emperical study of memory and information retrieval with a spatial user interface",
      "author" : [ "C. Chennawasin", "J. Cole", "C. Chen" ],
      "venue" : "21st Annual BCS-IRSG Colloquium on IR",
      "citeRegEx" : "Chennawasin et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Chennawasin et al\\.",
      "year" : 1999
    }, {
      "title" : "Maximum likelihood from incomplete data via the EM algorithm",
      "author" : [ "A.P. Dempster", "N.M. Laird", "D.B. Rubin" ],
      "venue" : "J. of the Royal Stat. Society B,",
      "citeRegEx" : "Dempster et al\\.,? \\Q1977\\E",
      "shortCiteRegEx" : "Dempster et al\\.",
      "year" : 1977
    }, {
      "title" : "Euclidean embedding of co-occurrence data",
      "author" : [ "A. Globerson", "G. Chechik", "F. Pereira", "N. Tishby" ],
      "venue" : null,
      "citeRegEx" : "Globerson et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Globerson et al\\.",
      "year" : 2005
    }, {
      "title" : "Eigentaste: A constant time collaborative filtering algorithm",
      "author" : [ "K. Goldberg", "T. Roeder", "D. Gupta", "C. Perkins" ],
      "venue" : "Information Retrieval,",
      "citeRegEx" : "Goldberg et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Goldberg et al\\.",
      "year" : 2001
    }, {
      "title" : "Rank correlation methods",
      "author" : [ "M.G. Kendall" ],
      "venue" : null,
      "citeRegEx" : "Kendall,? \\Q1955\\E",
      "shortCiteRegEx" : "Kendall",
      "year" : 1955
    }, {
      "title" : "Optimization by simulated annealing",
      "author" : [ "S. Kirkpatrick", "C.D. Gelatt", "M.P. Vecchi" ],
      "venue" : "Science,",
      "citeRegEx" : "Kirkpatrick et al\\.,? \\Q1983\\E",
      "shortCiteRegEx" : "Kirkpatrick et al\\.",
      "year" : 1983
    }, {
      "title" : "Equation of state calculations by fast computing machines",
      "author" : [ "N. Metropolis", "A.W. Rosenbluth", "M.N. Rosenbluth", "A.H. Teller", "E. Teller" ],
      "venue" : "Journal of Chemical Physics,",
      "citeRegEx" : "Metropolis et al\\.,? \\Q1953\\E",
      "shortCiteRegEx" : "Metropolis et al\\.",
      "year" : 1953
    }, {
      "title" : "Collaborative filtering by personality diagnosis: A hybrid memory- and model-based approach",
      "author" : [ "D. Pennock", "E. Horvitz", "S. Lawrence", "C.L. Giles" ],
      "venue" : "UAI ’00",
      "citeRegEx" : "Pennock et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Pennock et al\\.",
      "year" : 2000
    }, {
      "title" : "Think globally, fit locally: Unsupervised learning of low dimensional manifolds",
      "author" : [ "L.K. Saul", "S.T. Roweis" ],
      "venue" : null,
      "citeRegEx" : "Saul and Roweis,? \\Q2003\\E",
      "shortCiteRegEx" : "Saul and Roweis",
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Spatial layouts have been shown in the past to increase interest in exploration and to aid in finding information (Chennawasin et al., 1999).",
      "startOffset" : 114,
      "endOffset" : 140
    }, {
      "referenceID" : 0,
      "context" : "Breese et al. (1998) classifies the approaches into two major categories: memory based and model based.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 8,
      "context" : "Pennock et al. (2000) propose a collaborative filter based on personality diagnosis.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 3,
      "context" : "CODE (Globerson et al., 2005) is one such recent example.",
      "startOffset" : 5,
      "endOffset" : 29
    }, {
      "referenceID" : 7,
      "context" : "In particular, we use the Metropolis-Hastings (MH) algorithm (Metropolis et al., 1953), which was extended to graphical models.",
      "startOffset" : 61,
      "endOffset" : 86
    }, {
      "referenceID" : 6,
      "context" : "To get the samples that maximize the posterior, we modify the standard MH algorithm along the lines of the simulated annealing (SA) algorithm (Kirkpatrick et al., 1983).",
      "startOffset" : 142,
      "endOffset" : 168
    }, {
      "referenceID" : 2,
      "context" : "We use expectation maximization (EM) algorithm (Dempster et al., 1977) to learn the rating function.",
      "startOffset" : 47,
      "endOffset" : 70
    }, {
      "referenceID" : 4,
      "context" : "We compare our results with Locally Linear Embedding (LLE) (Saul & Roweis, 2003), Eigentaste (ET) (Goldberg et al., 2001) and co-occurrence data embedding (CODE) (Globerson et al.",
      "startOffset" : 98,
      "endOffset" : 121
    }, {
      "referenceID" : 3,
      "context" : ", 2001) and co-occurrence data embedding (CODE) (Globerson et al., 2005).",
      "startOffset" : 48,
      "endOffset" : 72
    }, {
      "referenceID" : 5,
      "context" : "In this work, we introduce Kendall’s tau (Kendall, 1955) as a suitable evaluation criterion.",
      "startOffset" : 41,
      "endOffset" : 56
    } ],
    "year" : 2006,
    "abstractText" : "Collaborative data consist of ratings relating two distinct sets of objects: users and items. Much of the work with such data focuses on filtering: predicting unknown ratings for pairs of users and items. In this paper we focus on the problem of visualizing the information. Given all of the ratings, our task is to embed all of the users and items as points in the same Euclidean space. We would like to place users near items that they have rated (or would rate) high, and far away from those they would give low ratings. We pose this problem as a real-valued non-linear Bayesian network and employ Markov chain Monte Carlo and expectation maximization to find an embedding. We present a metric by which to judge the quality of a visualization and compare our results to Eigentaste, locally linear embedding and cooccurrence data embedding on three real-world datasets.",
    "creator" : "TeX"
  }
}