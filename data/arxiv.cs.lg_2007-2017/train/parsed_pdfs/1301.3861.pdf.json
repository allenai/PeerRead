{
  "name" : "1301.3861.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Inference for Belief Networks Using Coupling From the Past",
    "authors" : [ "Michael Harvey", "Radford M. Neal" ],
    "emails" : [ "mikeh@cs.", "radford@cs." ],
    "sections" : [ {
      "heading" : null,
      "text" : "Inference for belief networks using Gibbs sampling produces a distribution for unob served variables that differs from the correct distribution by a (usually) unknown error, since convergence to the right distribution occurs only asymptotically. The method of \"coupling from the past\" samples from ex actly the correct distribution by ( conceptu ally) running dependent Gibbs sampling sim ulations from every possible starting state from a time far enough in the past that all runs reach the same state at time t = 0. Ex plicitly considering every possible state is in tractable for large networks, however. We propose a method for layered noisy-or net works that uses a compact, but often impre cise, summary of a set of states. This method samples from exactly the correct distribution, and requires only about twice the time per step as ordinary Gibbs sampling, but it may require more simulation steps than would be needed if chains were tracked exactly.\n1 INTRODUCTION\nConditional probabilities for a Bayesian belief network can be calculated exactly only when the network can be transformed into a \"junction tree\" in which the number of states for each clique is manageable (see Cowell, et al 1999). For densely connected networks, clique sizes are large, and the time for exact calcula tions grows exponentially with the size of the network.\nStochastic simulation methods, in particular Markov chain Monte Carlo methods such as Gibbs sampling, are not limited by the density of edges in the network, but suffer from other problems. A Gibbs sampling simulation converges to the desired invariant distribu tion asymptotically, but the distribution after a finite\ntime differs from that desired by a generally uncertain amount. This error is greatest in the early part of the chain, which is generally thrown away in a burn in phase until it is felt that the error has dropped to within the desired tolerance. The burn-in time must usually be estimated because the rate of convergence of the Markov chain is not known theoretically. It is possible to overestimate and waste computing time, or underestimate and include states that are too far from the desired distribution. The conservative user might greatly overestimate the required burn-in time, to minimize the risk of getting the wrong answer.\nTo overcome this problem of initialization bias, Propp and Wilson (1997) introduced exact sampling, also known as perfect simulation, using the method of \"cou pling from the past\" to obtain states from exactly the desired distribution. Instead of starting a single chain in some arbitrary initial state at time t = 0, dependent chains are started in every possible state at some time T < 0, far enough back that all the chains coalesce to a single state by time t = 0. The state at t = 0 then comes from the correct distribution, and useful sam pling can begin at that time with zero bias (ie, with no systematic error). The final results are still approx imate due to ordinary sampling error, but this error is easily controlled.\nPropp and Wilson showed how coupling from the past can be implemented efficiently when the state space can be partially ordered, with unique maximal and minimal states, in a way that is preserved through transitions. Belief networks lack this monotonicity property, so some other way of keeping track of all possible chains is needed. We show that it is possi ble to represent a set of chains by a single summary chain, whose states are sets of states of the original chain. The set of chains may contain both those that are of interest and spurious chains added at intermedi ate stages of the simulation due to this representation being imperfect. When the summary chain reaches a state that represents a single state of the original chain,\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 257\ncoalescence of all the true chains (as well as the spu rious chains) will have occurred. The summary chain will then exactly represent the single coalesced chain, whose state at t = 0 comes from exactly the desired distribution.\nWe apply this idea to inference for conditional distri butions in layered noisy-or belief networks, in which variables that are siblings are not also directly con nected. For these networks, the summary chain tran sitions can be performed efficiently. The computa tion time required will depend on the coalescence time of the true chains, plus the overhead caused by any spurious chains. We give examples showing that this overhead can sometimes be large. Other tests on randomly-generated two-layer networks of \"dis eases\" and \"symptoms\" show that the summary chain method works well enough to be of practical interest. Interestingly, the relative overhead compared to simple Gibbs sampling appears to be smallest for the most dif ficult problems, for which uncertainty concerning the necessary burn-in time would be greatest.\n2 NOISY-OR BELIEF NETWORKS\nA belief network is a directed acyclic graph in which nodes represent random variables and edges describe how the joint distribution for these variables is ex pressed. A variable with an edge pointing to an other variable is a \"parent\" of that variable. For each variable A with parents B1, · · · , Bn, the condi tional probabilities P(A = a I B1 = b1, · · · , Bn = bn) are specified. Joint probabilities for all variables, P(A1 = a1, ··· , Am = am), are expressed in terms of the product of all forward conditional probabilities:\nII P(A; =a; I values, aj, for parents, Aj, of A;)\nThe noisy-OR scheme is a way of specifying these con ditional distributions without explicitly listing proba bilities for every combination of values for the parent variables. It applies when variables take on values of 0 and 1. Each parent variable influences the child vari able to be turned on (value = 1) when it is turned on. The degree of influence is determined by a weight, c;, on the link from parent X; to child W, giving the prob ability of turning on the child given that the parent X; is turned on. (If these weights are all one, the scheme is a deterministic OR-gate. ) Variable W is also caused to be on for some other reason with probability pw. The conditional probability for W to be on is therefore\nP(W = 1 I values, x;, for parents, X;) 1 - (1 - Pw) II (1 - c; )\ni : Xi=l\n3 GIBBS SAMPLING\nStochastic simulation using Markov chains was intro duced as a way of sampling from conditional distribu tions for belief networks by Pearl (1987, 1988). The method is now commonly known as Gibbs sampling.\n3.1 Sampling using Markov chains\nA Markov chain is specified by a sequence of dis crete random variables x<o), X(l), · · ·, a marginal dis tribution, Po, for the initial state X(o), and tran sition probabilities for state x<t+I) to follow state x<t): P(x<t+I) I x(t)). The joint distribution of x<o), X(l), · · · is then determined upon making the Markov assumption that x<tl is conditionally indepen dent of x<t-k) for k > 1 given x(t-l). Stationary Markov chains have transition probabilities that do not depend on time, which can be represented with a transition matrix M. The value in the i1th row and /th column of M is the probability of a transi tion to state j given that the system is in state i. The state probabilities at time t (i.e., P(X(t) = j)) can be represented as a row vector Pt, with Pt = p0Mt. A distribution 7f is invariant if 7f = 7f M. An er godic Markov chain has an invariant distribution that is reached asymptotically no matter what the initial distribution Po is, i.e. limt-+oo Pt = 7f.\nThe error in the distribution of the Markov chain at time t can be measured by the total variation distance between the state distribution at that time, Pt, and the invariant distribution, 7f. For a finite state space x, this is\n1 IIPt- 1rll = 2 L IPt(x)- 1r(x)l. xEx\nAsymptotically, the error decays exponentially as\nerror = ae-tfc where a and c are constants specific to the Markov chain. (See Rosenthal (1995) for further discussion.) If the user has an error tolerance E, the burn-in time for Gibbs sampling should be\nburn in time= -c ln(Eja)\nHowever, the convergence behaviour of the Markov chain is usually unknown (i.e., the constants a and c are not known). The conservative user will try to choose as large a burn-in time as is practical to lessen the chances of obtaining erroneous results.\n3.2 Gibbs sampling for belief networks\nWhen the state, X, consists of several variables, X1, ... , Xn, a transition matrix that leaves a desired\n258 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\ndistribution 1r invariant can be built from a sequence of matrices representing transitions that change only a single variable. The transition matrix M, on the state space of all possible combinations of values for X1, . . . , Xn, is written as M = B1Bz···Bn, with Bk representing an update that changes only xk (i.e. , en tries in Bk for transitions that change variables other than Xk are zero) . In the Gibbs sampling scheme, the entries in Bk corresponding to changes in Xk alone are the conditional probabilities under 1r of the vari able Xk taking on its various values, given the current values of the other variables. Each such Bk will leave the distribution 1r invariant, and hence so will M.\nPearl (1987) derives Gibbs sampling transition proba bilities for a belief network. When other variables are fixed, conditional probabilities for a particular variable depend only on its parents, its children, and its chil dren's parents. From the definition of belief networks,\nP(VkjV1, , · · ·, Vk-1, Vk+l, · · ·, Vn)\nex: P(VkjV1, · · ·, vk-d II P(Y;jV1, · · · , vk, · · ·, V;-d j>k\nHere V1, V2, · · ·, Vk_1 are possible parents of Vk, and Vj for j > k are possible children of vk' since v is ordered with parents before children. This expression can be evaluated using the specified conditional dis tributions for a variable given its parents. When Vj is not actually a child of Vk, the corresponding factor can be omitted.\nThe effect of sampling from the distribution of un known variables conditional on known values for other variables is achieved by simply keeping the values of the known variables fixed while the others are updated by Gibbs sampling, using the above probabilities.\nFor noisy-or belief networks, if a child of the variable being updated has the value 0, the other parents of that child can be ignored in the calculation - child variables instantiated to 0 cannot transmit informa tion between parents.\n4 EXACT SAMPLING\nPropp and Wilson (1997) proposed exact sampling us ing coupling from the past as a way to eliminate the error from using finite-length simulation runs. An er godic Markov chain will reach its equilibrium distribu tion if it is run for an infinite amount of time. There fore, if one were willing to wait forever, one could be sure that the correct distribution of the Markov chain had been reached. Propp and Wilson show that it is not necessary to wait forever to arrive at this result, however- that, at least when the state space is finite, there is a way to find the exact result with a finite num-\nber of computations. The state found in this way may be used as the starting state for a Gibbs sampling run that will be free of bias.\n4.1 The idea of coupling from the past\nPropp and Wilson's idea is to run many chains from some time, T < 0, in the past, starting from every pos sible state. These chains are \"coupled\", by introducing dependencies between their transitions, in an attempt to make them coalesce to the same state by t = 0. If by time t = 0 they have all coalesced into one chain, then it can be said that no matter what state was started from at time t = T, the same state at time t = 0 results. If coalescence of chains started at t = T does not oc cur by time t = 0, then the procedure is repeated from further back in the past.\nAt a minimum, the chains started from the various initial states must be dependent to the extent that two chains arriving at the same state henceforth use the same pseudo-random numbers for all their subse quent transitions. This causes chains to stay together once they first coalesce. Coalescence of all chains to a single state can be encouraged by introducing de pendencies beyond this. Such dependencies between chains do not invalidate the results as long as tran sitions made at different times remain independent. In this paper, we will consider only systematic Gibbs sampling, in which the variables of the network are updated in some sequence. Randomness is therefore required only for setting the value of the variable be ing updated according to its conditional distribution. A single real-valued pseudo-random number is suffi cient for making this random choice at each time step. Dependencies between chains are introduced by using the same such random number for all chains. If the chains are restarted from further back in the past, the same pseudo-random numbers as before are used at those times that were previously visited.\nPropp and Wilson show that if the chain is ergodic, this coupling from the past procedure will, with prob ability 1, lead to coalescence at t = 0 once the chains are started from sufficiently far in the past, and that the unique state of the coalesced chains at t = 0 is dis tributed exactly according to the chain's equilibrium distribution.\nWhen coalescence does not occur, it would be ineffi cient to try again with a chain started just one time step further back, since the new chain must be run all the way to time t = 0. It is more efficient to start runs at times t = -1, -2, -4, -8, - 16, ···until coalescence finally occurs. Propp and Wilson (1997) show that this scheme is not far from optimal, requiring no more than four times the total number of simulation steps\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 259\nas would be needed if the actual coalescence time were somehow guessed.\nNote that a coalesced chain must be continued to time t = 0 to obtain a state from the correct distribution. Usually, coalescence occurs before reaching time t = 0, but if the state at that time is used, there is a bias introduced toward conditions that favour coalescence of chains. By always selecting the state at time t = 0, there is no dependence on the time that coalescence occurs. Also note that when the chains do not all coalesce, it is not valid to just throw away these chains and start new chains from further back. Instead, the old chains must be extended backwards in time- i.e. , the old pseudo-random numbers generated at times that were already visited must be re-used. Otherwise, a bias is introduced by a preference for pseudo-random numbers that more easily allow coalescence.\n4.2 Using states obtained by exact sampling\nThe coupling from the past procedure just described can be performed a number of times, each time with new pseudo-random variables, thus obtaining multiple independent states from exactly the desired distribu tion. Each time the procedure is run, it must search for a starting time that allows the chains to coalesce. The procedure typically requires varying amounts of running time to complete (i.e. , one must go back vary ing amounts of time in the past to cause the chains to coalesce). In practice, if any of these independent at tempts result in coalescence, it is reasonable to expect all such attempts to succeed within a reasonable time period, since the chances of coalescing further back in the past are independent of the failure to coalesce later on. However, if none of the runs coalesce in a reasonable time, one must declare the results indeter minate, which is superior to getting a wrong answer using Gibbs sampling with the same number of steps.\nA state from the invariant distribution found using coupling from the past may be used to initialize an or dinary Gibbs sampling run, which continues forward from t = 0. The states at times t 2: 0 will all have exactly the correct distribution. However, it is desir able to run the coupling from the past procedure sev eral times, in order to find a number of initial states from the invariant distribution, and to take samples from each of the chains that follow them. These ini tial states are more valuable than the states that follow them, since they are completely independent of each other, but they come at the cost of coupling chains from the past. At the same time, the following states are less valuable because of their dependence on prior states, but they can be produced at the much lower cost of one Markov chain transition.\n4.3 Efficiently tracking chains\nKeeping track of every chain for every possible starting state is generally infeasible, since the size of the state space is exponential in the number of variables in the network. Propp and Wilson (1997) show that coupling from the past can be implemented efficiently when states can be given a partial order that is preserved through Markov chain transitions, by simulating just two chains, started from the minimal and maximal states. Furthermore, they show that for such mono tonic chains, coalescence cannot be much slower than convergence of the chain. This makes coupling from the past quite attractive for such problems.\nBelief network states usually cannot be ordered in a way that makes the Markov chain monotonic. An al ternative way of keeping track of every chain is there fore needed. There is also no known guarantee that the coalescence time will not be much greater than the time required for Gibbs sampling to converge to close to the desired distribution, though we know of no examples of this occurring.\nTo address the efficiency issue, a scheme is needed to simplify the tracking of all the chains. For noisy-or belief networks, we attempt to summarize the chains with one chain whose states are sets of states of the original chain. The amount of work for each transition of the summary chain is the same as for two transitions of the original chain, provided that no sibling vari ables in the network are directly connected. However, our method does not track the set of chains precisely, which may slow detection of coalescence, though the fi nal result is still from the exactly correct distribution. Similar techniques, applied to simulation of Markov random fields, have been independently developed by Huber (1998) and by Haggstrom and Nelander (1999).\n5 EXACT SAMPLING FOR\nNOISY-OR BELIEF NETWORKS\nWe now show how coalescence of a large number of Markov chains for a layered noisy-or belief net work can be determined by simulating one summary chain, whose states approximate sets of states of chains started in all possible initial states.\n5.1 Approximating a set of states\nThe state space S of noisy-or belief networks has vari ables that take the values 0 or 1. We approximate a set of states in S by a single state in a state space S(?), in which variables take the values 0, 1, or ?. The mapping, (3, from such a state to a set of states of the\n260 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\noriginal space is\nj3(V(?)) = { V E S : for all i, 11;,(?) = V;, or 11;,(?) = ? } That is, j3 selects all the states in S where every vari able matches the corresponding variable in S(?), with ? matching either 0 or 1. For example,\n{ ������ } 110010 110011 (3(1 ?001 ?) Not every set of states in S can be represented exactly by a state in S(?). For instance, there is no exact representation of { 100010 }\n100011 110010\nHence, to avoid losing true states, the approximation will sometimes have to include spurious states as well.\n5.2 Approximating a set of chains\nWe now show how a single summary chain on S(?) can be simulated so as to approximate a set of chains on S, with none of the true chains being lost, though spu rious chains may be introduced, delaying coalescence. Like the original Gibbs sampling chain, transitions in the summary chain change one variable at a time. For each state in S, the variable being updated has some conditional probability of changing to a 1, given the other variables in that state. Over the set of states that the state of the summary chain maps to, this condi tional probability will have some maximum and some minimum value, which can be used to determine the transition probabilities of the variable in s(?), using the fact that the chains are coupled by using the same pseudo-random variable, U. We that assume U is uni formly distributed over [0, 1)), and that transitions in the original chain are determined by setting the vari able being updated to 1 if U is less than the conditional probability of a 1, and to 0 otherwise. Transitions in the summary chain can then be determined as follows:\n• If U is less than the minimum probability, then all the original chains would set the variable to 1. Set the variable in the summary chain to 1 also.\n• Similarly, if U is greater then or equal to the max imum probability, set the variable in the summary chain to 0.\n• If U is between the minimum and maximum prob abilities, then some of the original chains would set the variable to 0 and some would set it to 1. To represent this, set the variable in the summary chain to ?.\nTransitions done this way do not lose track of any of the true chains, but spurious chains may be intro duced. For example, consider a transition in the sum mary chain that changes the last variable of 1 ?0010 (which j3 maps to {100010, 110010}), and suppose that in the original chains, the transitions from these two states are to states with different values for the last variable (e.g., 100010 --t 100010 and 110010 --t 110011). The transition in the summary chain will have to be to the state 1?001?, which maps to four states of S, rather than the previous two states. Hence two spurious chains have been introduced.\n5.3 Efficient simulation of the summary chain\nIt is possible to determine the minimum and maxi mum of the conditional probability used in updating vk, which is P(Vk = 1 I vl, . . . , Vk-1, vk+l, . .. , Vn), over all V in j3(S(7)), without exhaustive search, by examining just two judiciously chosen two states in j3(S(?)), for which this conditional probability will take on its minimum and maximum value. Let pa(V;) be parents of V;, and c(V;) be the children of V;. The required conditional probability is minimized or max imized by minimizing or maximizing the ratio\nP(Vk = 1[pa(Vk)) x TI P(l'J [pa(l'J), Vk = 1) VjEc(Vk) P(Vk=O[pa(Vk))x TI P(Vj[pa(Vj), Vk=O) VjEc(Vk)\nThe rules for selecting the appropriate states are summarized below, and justified in detail by Harvey (1999).\nFor the minimum probability that Vk = 1, look at the V E j3(V(7l) for which:\nIf l'J is a child or parent of Vk and l'i(?) = ? , then l'J = 0. If Vp is a parent of some child, l'J, of Vk, and l'i(?) = 1, and vPl = ?, then Vp = 1.\nFor the maximum probability that Vk = 1, look at the V E j3(V(?)) for which:\nIf l'J is a child or parent of Vk and l'i(?) = ? , then l'J = 1. If Vp is a parent of some child, l'J, of Vk, and l'i(?) E {1, ?}, and V�?) = ?,then Vp = 0.\nNote that variables not mentioned above are irrelevant to computing the conditional probability for vk. These rules assume that sibling variables are not di rectly connected. That is, a child does not share a\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 261\nparent with its parent vk 0 If this were so, the two pos sible settings for the value of that parent could have opposite effects on the conditional probability ratios for vk being 0 or 1 given its parents, and for the child of vk having its present value given that vk is 0 or 1. For layered networks, in which edges go only from variables in one layer to those in the layer immediately below, this problem does not arise.\n5.4 Time required\nThe summary chain is started at some time T < 0 in the state where all variables are set to ? , representing the set of all possible states. If none of the states of the network have zero probability, it is not hard to show that if T is early enough, then by time t = 0, the summary chain will have reached a state representing a single network state - i.e., one in which none of the variables are set to ? . Since no chains are lost in the approximation used, the true set of chains will have coalesced when this happens. However, because of spurious chains, the summary chain may need to be started at an earlier time than would be needed if all chains were tracked explicitly. The coalescence time for a simulation is the minimum time in the past sufficient to produce coalescence by t = 0. If simulation is done using start times of -T = 1, 2, 4, 8, ... , Propp and Wilson (1997) show that the expected total number of time steps simulated is around 2.89 times the coalescence time. Our noisy-or simulation scheme requires two calculations of condi tional probability at each time step, in order to obtain the minimum and the maximum. Therefore, the ex pected computational work, measured in terms of com putations similar to ordinary Gibbs sampling updates, is 5. 78 times the expected coalescence time.\n6 COALESCENCE TIMES\nWhen the summary chain method is used, the coales cence time may be greater than if chains were tracked explicitly. Here we look at the relationship between these two coalescence times for some small problems by calculating the convergence rates of the original and summary chains from the eigenvalues of their transi tion matrices. The convergence rate of the original chain can be bounded in terms of its expected coa lescence time, and though the reverse is not true in general, we will here take the convergence rate to be indicative of the chain's expected coalescence time.\n6.1 Transition matrix eigenvalues\nThe convergence rate of an ergodic Markov chain is re lated to the magnitudes of the eigenvalues of the tran-\nsition matrix M, as reviewed, for example, by Rosen thal (1995). There is a (left) eigenvector corresponding to the invariant distribution, 1r, of the Markov chain, with eigenvalue equal to 1. The magnitudes of the other (possibly complex) eigenvalues are less than 1. If the Markov chain is started in the initial distribu tion Po, the distribution at time n will be Pn = p0Mn. As n � oo, Pn � 1r for an ergodic chain, with the rate of convergence being determined by the magnitude of the eigenvalue whose magnitude is second-largest (i.e., the largest other than the eigenvalue associated with 1r, whose value is 1). This eigenvalue will have a mag nitude less than one, but the closer it is to one, the slower will be the convergence of the Markov chain. The transition matrix of a summary chain will have all the eigenvectors and eigenvalues of the original chain, including the eigenvector and eigenvalue corre sponding to the invariant distribution, to which both chains converge (if the original chain is ergodic). The summary chain will also have some eigenvectors and eigenvalues associated with states where some vari ables have ? as their value, and some of these eigen values may be larger in magnitude than the second largest eigenvalue of the original chain. Therefore, the summary chain cannot converge any faster than the chains it summarizes, and may converge more slowly. Below, we examine these eigenvectors and eigenvalues for some simple diagnostic networks in which a top layer of variables represent \"diseases\", which can cause various \"symptoms\", represented by variables in the bottom layer. Interest focuses on inference for what diseases are present, given certain observed symptoms.\n6.2 Perfectly summarized networks\nThe transitions of the chains being coupled from the past can be perfectly summarized in certain types of networks. A trivial example is a one-disease network with one or more symptoms that are known. When there is just one ? variable in the summary state, it is an exact representation of a set of two states. Any network with two unknowns can also be perfectly summarized. Although it is possible that the state of the summary chain does not exactly represent the set of states of the true chains, each variable of the summary chain summarizes the set of values for that variable correctly, even if some constraints between variables are missed. (This can be shown by induc tion, starting with the initial situation where all values are possible, and all summary variables are set to ? . ) When the true chains all coalesce, the summary chain will therefore show coalescence as well (i.e., it will have no ? values).\n262 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\nDisease apriori probability: 0.1 Symptom apriori probability: 0.0\nNoisy-or weight: 1.0\nto be absent does not produce interactions between the diseases that are its potential causes. Sections of the network can effectively be independent, with sub states whose transitions do not interact with those of other sections. If a large network consists only of inde pendent single-disease sub-networks, for example, ex act sampling will converge in one iteration. Such sit uations are discovered at run-time, depending on how the symptoms are instantiated.\n6.3 Imperfectly summarized networks\nIn general, network transitions cannot be perfectly summarized by a chain in which states consist of 0/1/? variables, and this can lead to worse convergence for such a summary chain than for the original chain. A moderate example of this is shown in Figure 2. The non-zero eigenvalues and associated eigenvectors of the summary chain transition matrix for this net work are shown in Table 2. Some of these are the same as for the transition matrix of the original chain. The additional eigenvector, which has non-zero com ponents for states with ?-valued variables, has an eigenvalue whose magnitude is greater than that of all the other eigenvalues less than 1. Its magnitude of .97, compared to the next largest, v'0.8512 + 0.0752 � 0.85, indicates moderately worse convergence for the summary chain. Experiments with this network show that detecting coalescence by t = 0 using the summary chain requires starting on average 53.9 time steps in the past, compared to only 17.6 time steps if every state is tracked explicitly. If the probabilities in the imperfectly summarized net work are more extreme, the summary chain is much worse than tracking every state. Figure 3 shows a net work that coalesces quickly when every state is tracked explicitly, and hence must also converge quickly when ordinary Gibbs sampling is done, but for which a very long time is needed (on average) for coalescence to be detected when using the summary chain. This sum mary chain's second-largest eigenvalue is .996, com pared to 0.352 for the original chain. The non-zero apriori symptom probability in this network helps the set of chains to coalesce quickly by allowing for other explanations of the evidence besides the diseases of the network - an effect that is lost when sets of states are approximately summarized.\n7 EMPIRICAL TESTS\nAlthough the above results show that coupling from the past using a summary chain can perform poorly, empirical testing shows that it performs reasonably well on simulated two-level diagnostic networks. These\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 263\nDisease apriori probability: 0.1 Symptom apriori probability: 0.0\nNoisy-or weight: 1.0\nFigure 2: An imperfectly-summarized network.\n0.973 0.851 + 0.075i 0.851 - 0.075i states\n000 0 0 0 0 00? 0 0 0 0 001 0 0 0 0 0?0 0 0 0 0 0?? 0 0 0 0 0?1 0 0 0 0 010 0 0 0 0 01? 0 0 0 0 011 0.321 -0.193 0.338 + Oi 0.338 + Oi ?00 0 0 0 0 ?0? 0 0 0 0 ?01 0 0 0 0 ??0 0 0 0 0 ??? 0 0.441 0 0 ??1 0 0.049 0 0 ?10 0 0 0 0 ?1? 0 0.049 0 0 ?11 0 0.005 0 0 100 0 0 0 0 10? 0 0 0 0 101 0.321 -0.191 -0.152 - 0.283i -0.152 + 0.283i 1 ?0 0 0 0 0 1 ?? 0 0.045 0 0 1 ?1 0 0.005 0 0 110 0.321 -0.189 -0.167 + 0.254i -0.167- 0.254i 11? 0 0 0 0 111 0.036 -0.021 -0.019 + 0.028i -0.019 - 0.028i\nTable 2: Eigenvectors and eigenvalues of the summary chain for the imperfectly-summarized network.\ntests are described in detail by Harvey (1999).\nTests were first performed using randomly-generated networks with 10 possible diseases and 10 symptoms whose structure was thought to be plausible for an actual application. The performance of coupling from the past using the summary chain was then compared to performance when every chain is tracked explicitly. For these problems, coalescence of the summary chain took at most 512 iterations, and using the summary chain was never hugely worse than tracking all chains.\nFurther tests were run on networks of the same size that were constructed randomly so as to produce a larger fraction of difficult problems. In these tests, the summary chain tended to work well on the problems for which coalescence was slowest (even tracking every chain explicitly), with coalescence typically detected using the summary chain in only about twice the time that was required when tracking every chain explicitly.\nWhen coalescence is slow even when tracking all states\nDisease apriori probability: Symptom apriori probability:\nNoisy-or weight: 0.001 0.001 1.000\nFigure 3: Extreme imperfectly-summarized network\nexplicitly, it is likely (though not guaranteed) that Gibbs sampling would also converge slowly. Neverthe less, calculations show that if the convergence prop erties of the chain for these problems were somehow known, use of coupling from the past would be com putationally favourable only for users with quite a low error tolerance. In practice, the convergence proper ties are not known, however, which makes relying on a \"burn-in\" period problematic. Coupling from the past eliminates all uncertainty about whether the right dis tribution has been reached, which may make it attrac tive whenever it is computationally feasible.\nReferences\nCowell, R. G., Dawid, A. P., Lauritzen, S. L., and Spiegelhalter, D. J. (1999) Probabilistic Networks and Expert Systems, Springer-Verlag.\nHaggstrom, 0. and Nelander, K. (1999) \"On exact simulation of Markov random fields using coupling from the past\", Scandinavian Journal of Statistics, vol. 26, pp. 395-411.\nHarvey, M. (1999) Monte Carlo Inference for Belief Networks Using Coupling From the Past, MSc The sis, Computer Science, University of Toronto.\nHuber, M. (1998) \"Exact sampling and approximate counting techniques\", Proceedings of the 30th ACM Symposium on the Theory of Computing, pp. 31-40.\nPropp, J. G. and Wilson, D. B. (1996) \"Exact Sam pling with Coupled Markov Chains and Applications to Statistical Mechanics\", Random Structures and Algorithms, vol. 9, pp. 223-252.\nPearl, J. (1987) \"Evidential Reasoning Using Stochas tic Simulation of Causal Models\", Artificial Intelli gence, vol. 32, pp. 245-257.\nPearl, J. (1988) Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, Morgan Kaufmann.\nRosenthal, J. S. (1995) \"Convergence rates of Markov chains\", SIAM Review, vol. 37, pp. 387-405."
    } ],
    "references" : [ {
      "title" : "On exact simulation of Markov random fields using coupling from the past",
      "author" : [ "Haggstrom", "K. Nelander" ],
      "venue" : "Scandinavian Journal of Statistics,",
      "citeRegEx" : "Haggstrom et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Haggstrom et al\\.",
      "year" : 1999
    }, {
      "title" : "Monte Carlo Inference for Belief Networks Using Coupling From the Past, MSc The­",
      "author" : [ "M. Harvey" ],
      "venue" : null,
      "citeRegEx" : "Harvey,? \\Q1999\\E",
      "shortCiteRegEx" : "Harvey",
      "year" : 1999
    }, {
      "title" : "Exact sampling and approximate counting techniques",
      "author" : [ "M. Huber" ],
      "venue" : "Proceedings of the 30th ACM Symposium on the Theory of Computing,",
      "citeRegEx" : "Huber,? \\Q1998\\E",
      "shortCiteRegEx" : "Huber",
      "year" : 1998
    }, {
      "title" : "Exact Sam­ pling with Coupled Markov Chains and Applications to Statistical Mechanics",
      "author" : [ "J.G. Propp", "D.B. Wilson" ],
      "venue" : "Random Structures and Algorithms,",
      "citeRegEx" : "Propp and Wilson,? \\Q1996\\E",
      "shortCiteRegEx" : "Propp and Wilson",
      "year" : 1996
    }, {
      "title" : "Evidential Reasoning Using Stochas­ tic Simulation of Causal Models",
      "author" : [ "J. Pearl" ],
      "venue" : "Artificial Intelli­ gence,",
      "citeRegEx" : "Pearl,? \\Q1987\\E",
      "shortCiteRegEx" : "Pearl",
      "year" : 1987
    }, {
      "title" : "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference",
      "author" : [ "J. Pearl" ],
      "venue" : null,
      "citeRegEx" : "Pearl,? \\Q1988\\E",
      "shortCiteRegEx" : "Pearl",
      "year" : 1988
    }, {
      "title" : "Convergence rates of Markov chains",
      "author" : [ "J.S. Rosenthal" ],
      "venue" : "SIAM Review,",
      "citeRegEx" : "Rosenthal,? \\Q1995\\E",
      "shortCiteRegEx" : "Rosenthal",
      "year" : 1995
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "To overcome this problem of initialization bias, Propp and Wilson (1997) introduced exact sampling, also known as perfect simulation, using the method of \"cou­ pling from the past\" to obtain states from exactly the desired distribution.",
      "startOffset" : 49,
      "endOffset" : 73
    }, {
      "referenceID" : 6,
      "context" : "(See Rosenthal (1995) for further discussion.",
      "startOffset" : 5,
      "endOffset" : 22
    }, {
      "referenceID" : 3,
      "context" : "Propp and Wilson (1997) show that this scheme is not far from optimal, requiring no more than four times the total number of simulation steps",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 3,
      "context" : "Propp and Wilson (1997) show that coupling from the past can be implemented efficiently when states can be given a partial order that is preserved through Markov chain transitions, by simulating just two chains, started from the minimal and maximal states.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 2,
      "context" : "Similar techniques, applied to simulation of Markov random fields, have been independently developed by Huber (1998) and by Haggstrom and Nelander (1999).",
      "startOffset" : 104,
      "endOffset" : 117
    }, {
      "referenceID" : 2,
      "context" : "Similar techniques, applied to simulation of Markov random fields, have been independently developed by Huber (1998) and by Haggstrom and Nelander (1999).",
      "startOffset" : 104,
      "endOffset" : 154
    }, {
      "referenceID" : 1,
      "context" : "The rules for selecting the appropriate states are summarized below, and justified in detail by Harvey (1999).",
      "startOffset" : 96,
      "endOffset" : 110
    }, {
      "referenceID" : 3,
      "context" : ", Propp and Wilson (1997) show that the expected total number of time steps simulated is around 2.",
      "startOffset" : 2,
      "endOffset" : 26
    }, {
      "referenceID" : 1,
      "context" : "tests are described in detail by Harvey (1999).",
      "startOffset" : 33,
      "endOffset" : 47
    } ],
    "year" : 2011,
    "abstractText" : "Inference for belief networks using Gibbs sampling produces a distribution for unob­ served variables that differs from the correct distribution by a (usually) unknown error, since convergence to the right distribution occurs only asymptotically. The method of \"coupling from the past\" samples from ex­ actly the correct distribution by ( conceptu­ ally) running dependent Gibbs sampling sim­ ulations from every possible starting state from a time far enough in the past that all runs reach the same state at time t = 0. Ex­ plicitly considering every possible state is in­ tractable for large networks, however. We propose a method for layered noisy-or net­ works that uses a compact, but often impre­ cise, summary of a set of states. This method samples from exactly the correct distribution, and requires only about twice the time per step as ordinary Gibbs sampling, but it may require more simulation steps than would be needed if chains were tracked exactly.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}