{
  "name" : "1206.4621.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Path Integral Policy Improvement with Covariance Matrix Adaptation",
    "authors" : [ "Freek Stulp", "Olivier Sigaud" ],
    "emails" : [ "freek.stulp@ensta-paristech.fr", "olivier.sigaud@upmc.fr" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Scaling reinforcement learning (RL) methods to continuous state-action problems, such as humanoid robotics tasks, has been the focus of numerous recent studies (Kober & Peters, 2011; Theodorou et al., 2010). Most of the progress in the domain comes from direct policy search methods based on trajectory rollouts. The recently proposed direct ‘Policy Improvement with Path Integrals’ algorithm (PI2) is derived\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nfrom first principles of stochastic optimal control, and is able to outperform gradient-based RL algorithms such as REINFORCE (Williams, 1992) and Natural Actor-Critic (Peters & Schaal, 2008) by an order of magnitude in terms of convergence speed and quality of the final solution (Theodorou et al., 2010).\nWhat sets PI2 apart from other direct policy improvement algorithms is its use of probability-weighted averaging to perform a parameter update, rather than using an estimate of the gradient. Interestingly enough, “Covariance Matrix Adaptation – Evolutionary Strategy (CMAES)” and “Cross-Entropy Methods (CEM)” are also based on this concept. It is striking that these algorithms, despite having been derived from very different principles, have converged to almost identical parameter update rules. To the best of our knowledge, this paper is the first to make this relationship between the three algorithms explicit (Section 2). This hinges on 1) re-interpreting CEM as performing probability-weighted averaging; 2) demonstrating that CEM is a special case of CMAES, by setting certain CMAES parameters to extreme values.\nA further contribution of this paper is that we conceptually and empirically investigate the differences and similarities between PI2, CEM, and CMAES (Section 3). These comparisons suggest a new algorithm, PI2-CMA, which has the algorithm structure of PI2, but uses covariance matrix adaptation as found in CEM and CMAES. A practical contribution of this paper is that we show how PI2-CMA automatically determines the exploration magnitude, the only parameter which is not straightforward to tune in PI2."
    }, {
      "heading" : "2. Background and Related Work",
      "text" : "We now describe the CEM, CMAES and PI2 algorithms, and their application to policy improvement."
    }, {
      "heading" : "2.1. Cross-Entropy Method (CEM)",
      "text" : "Given a n-dimensional parameter vector θ and a cost function J : Rn 7→ R, the Cross-Entropy Method (CEM) for optimization searches for the global minimum with the following steps: Sample – Take K samples θk=1...K from a distribution. Sort – Sort the samples in ascending order with respect to the evaluation of the cost function J(θk). Update – Recompute the distribution parameters, based only on the first Ke ‘elite’ samples in the sorted list. Iterate – return to the first step with the new distribution, until costs converge, or up to a certain number of iterations.\nA commonly used distribution is a multi-variate Gaussian distribution N (θ,Σ) with parameters θ (mean) and Σ (covariance matrix), such that these three steps are implemented as in (1)-(5). An example of one iteration of CEM is visualized in Figure 1, with a multivariate Gaussian distribution in a 2D search space1.\nCross-Entropy Method (one iteration)\nθk=1...K ∼ N (θ,Σ) sample (1) Jk = J(θk) eval. (2) θk=1...K ← sort θk=1...K w.r.t Jk=1...K sort (3)\nθnew = Ke∑ k=1 1 Ke θk update (4)\nΣnew = Ke∑ k=1 1 Ke (θk − θ)(θk − θ)ᵀ update (5)\nThroughout this paper, it is useful to think of CEM as performing probability-weighted averaging, where the elite samples have probability 1/Ke, and the non-elite have probability 0. With these values of Pk, (1)-(5) can be rewritten as in the left algorithm in Table 1. Here we use QKe/K to denote the Ke th quantile of the distribution Jk=1...K . This notation is chosen for brevity; it simply means that in the sorted array of ascending Jk, Pk is 1/Ke if K ≤ Ke, and 0 otherwise, as in (4). The resulting parameter updates are equivalent to those in (4) and (5), but this representation makes the relation to PI2 more obvious.\nCEM for Policy Improvement. Because CEM is a very general algorithm, it is used in many different contexts in robot planning and control. CEM for policy optimization was introduced by Mannor\n1Note that in (5), the unbiased estimate of the covariance is acquired by multiplying with 1\nKe , rather than 1 Ke−1 ,\nbecause we know the true mean to be θ.\net al. (2003). Although their focus is on solving finite small Markov Decision Processes (MDPs), they also propose to use CEM with parameterized policies to solve MDPs with large state spaces. Busoniu et al. (2011) extend this work, and use CEM to learn a mapping from continuous states to discrete actions, where the centers and widths of the basis functions are automatically adapted. The main difference with our work is that we use continuous action spaces of higher dimensionality, and compare CEM to PI2 and CMAES. CEM has also been used in combination with sampling-based motion planning (Kobilarov, 2011). An interesting aspect of this work is that it uses a mixture of Gaussians rather than a single distribution to avoid premature convergence to a local minimum. In (Marin et al., 2011), a CEM is extended to optimize a controller that generates trajectories to any point of the reachable space of the system."
    }, {
      "heading" : "2.2. Covariance Matrix Adaptation - Evolution Strategy",
      "text" : "The Covariance Matrix Adaptation - Evolution Strategy (Hansen & Ostermeier, 2001) algorithm is very similar to CEM, but uses a more sophisticated method to update the covariance matrix, as listed in Table 2.\nThere are three differences to CEM: • The probabilities in CMAES do not have to be Pk = 1/Ke as for CEM, but can be chosen by the user, as long as the constraints ∑Ke k=1 Pk = 1 and P1 ≥ · · · ≥ PKe are met. Here, we use the default suggested by Hansen & Ostermeier (2001), i.e. Pk = ln (0.5(K + 1)) − ln(k). • Sampling is done from a distribution N (θ, σ2Σ), i.e. the covariance matrix of the normal distribution is multiplied with a scalar step-size σ. These components govern the magnitude (σ) and shape (Σ) of the exploration, and are updated separately. • For both step-size and covariance matrix an ‘evolution path’ is maintained (pσ and pΣ respectively), which stores information about previous updates to θ. Using the information in the evolution path leads to significant improvements in terms of convergence speed, because it enables the algorithm to exploit correlations between consecutive steps. For a full explanation of the algorithm we refer to Hansen & Ostermeier (2001).\nReducing CMAES to CEM. This is done by setting certain parameters to extreme values: 1) set the time horizon cσ = 0. This makes (21) collapse to σnew = σ × exp(0), which means the step-size stays equal over time. Initially setting the step-size σinit = 1 means σ will always be 1, thus having no effect during sampling. 2) For the covariance matrix update, we set c1 = 0 and cµ = 1. The first two terms of (23) then\ndrop, and what remains is ∑Ke k=1 Pk(θk − θ)(θk − θ)ᵀ,\nwhich is equivalent to (16) in CEM, if Pk is chosen as in (12).\nCMAES for Policy Improvement. HeidrichMeisner and Igel (2008) use CMAES to directly learn a policy for a double pole-balancing task. Rückstiess et al. (2010) use Natural Evolution Strategies (NES), which has comparable results with CMAES, to di-\nrectly learn policies for pole balancing, robust standing, and ball catching. The results above are compared with various gradient-based methods, such as REINFORCE (Williams, 1992) and NAC (Peters & Schaal, 2008). To the best of our knowledge, our paper is the first to directly compare CMAES with CEM and PI2. Also, we use Dynamic Movement Primitives as the underlying policy representation, which 1) enables us to scale to higher-dimensional problems, as demonstrated by (Theodorou et al., 2010); 2) requires us to perform temporal averaging, cf. (18) and (19)."
    }, {
      "heading" : "2.3. Policy Improvement with Path Integrals",
      "text" : "A recent trend in reinforcement learning is to use parameterized policies in combination with probabilityweighted averaging ; the PI2 algorithm is a recent example of this approach. Using parameterized policies avoids the curse of dimensionality associated with (discrete) state-action spaces, and using probabilityweighted averaging avoids having to estimate a gradient, which can be difficult for noisy and discontinuous cost functions.\nPI2 is derived from first principles of optimal control, and gets its name from the application of the Feynman-Kac lemma to transform the HamiltonJacobi-Bellman equations into a so-called path integral, which can be approximated with Monte Carlo methods (Theodorou et al., 2010). The PI2 algorithm is listed to the right in Table 1. As in CEM, K samples θk=1...K are taken from a Gaussian distribution. In PI2, the vector θ represents the parameters of a policy, which, when executed, yields a trajectory τ i=1...N with N time steps. This multi-dimensional trajectory may represent the joint angles of a n-DOF arm, or the 3-D position of an end-effector.\nSo far, PI2 has mainly been applied to policies represented as Dynamic Movement Primitives (DMPs) (Ijspeert et al., 2002), where θ determines the shape of the movement. Although PI2 searches in the space of θ, the costs are defined in terms of the trajectory τ generated by the DMP when it is integrated over time. The cost of a trajectory is determined by evaluating J for every time step i, where the cost-to-go of a trajectory at time step i is defined as the sum over all future costs S(τ i,k) = ∑N j=i J(τ j,k), as in (11) 2.\nAnalogously, the parameter update is applied to every time step i with respect to the cost-to-go S(τ i). The probability of a trajectory at i is computed by exponentiating the cost, as in (13). This assigns high probability to low-cost trials, and vice versa. In prac-\n2For convenience, we abbreviate S(τ i,k) with Si,k.\ntice, − 1λSi,k is implemented with optimal baselining as −h(Si,k−min(Si,k)) max(Si,k)−min(Si,k) cf. (Theodorou et al., 2010).\nAs can be seen in (15), a different parameter update θnewi is computed for each time step i. To acquire the single parameter update θnew, the final step is therefore to average over all time steps (18). This average is weighted such that earlier parameter updates in the trajectory contribute more than later updates, i.e. the weight at time step i is Ti = (N − 1)/ ∑N j=1(N − 1). The intuition is that earlier updates affect a larger time horizon and have more influence on the trajectory cost.\nPoWeR is another recent policy improvement algorithm that uses probability-weighted averaging (Kober & Peters, 2011). In PoWeR, the immediate costs must behave like an improper probability, i.e. sum to a constant number and always be positive. This can make the design of cost functions difficult in practice; (24) for instance cannot be used with PoWeR. PI2 places no such constraint on the cost function, which may be discontinuous. When a cost function is compatible with both PoWeR and PI2, they perform essentially identical (Theodorou et al., 2010).\n3. Comparison of PI2, CEM and CMAES\nWhen comparing CEM, CMAES and PI2, there are some interesting similarities and differences. All sample from a Gaussian to explore parameter space – (6) and (7) are identical – and both use probabilityweighted averaging to update the parameters – (14) and (15). It is striking that these algorithms, which have been derived within very different frameworks, have converged towards the same principle of probability-weighted averaging.\nWe would like to emphasize that PI2’s properties follow directly from first principles of stochastic optimal control. For instance, the eliteness mapping follows from the application of the Feymann-Kac lemma to the (linearized) Hamilton Jacobi Bellmann equations, as does the concept of probability-weighted averaging. Whereas in other works the motivation for using CEM/CMAES for policy improvement is based on its empirical performance (Busoniu et al., 2011; HeidrichMeisner and Igel, 2008; Rückstiess et al., 2010) (e.g. it is shown to outperform a particular gradientbased method), the PI2 derivation (Theodorou et al., 2010) demonstrates that there is a theoretically sound motivation for using methods based on probabilityweighted averaging, as this principle follows directly from first principles of stochastic optimal control.\nWhereas Section 2 has mainly highlighted the similarities between the algorithms, this section focuses on the differences. Note that any differences between PI2 and CEM/CMAESin general also apply to the specific application of CEM/CMAES to policy improvement, as done for instance by Busoniu et al. (2011) or Heidrich-Meisner and Igel (2008). Before comparing the algorithms, we first present the evaluation task used in the paper."
    }, {
      "heading" : "3.1. Evaluation Task",
      "text" : "For evaluation purposes, we use a viapoint task with a 10-DOF arm. The task is visualized and described in Figure 2. This viapoint task is taken from (Theodorou et al., 2010), where it is used to compare PI2 with PoWeR (Kober & Peters, 2011), NAC (Peters & Schaal, 2008), and REINFORCE (Williams, 1992).\nThe goal of this task is expressed with the cost function in (24), where a represents the joint angles, x and y the coordinates of the end-effector, and D = 10 the number of DOF. The weighting term (D + 1− d) penalizes DOFs closer to the origin, the underlying motivation being that wrist movements are less costly than shoulder movements for humans, cf. (Theodorou et al., 2010).\nJ(τ ti) = δ(t− 0.3) · ((xt − 0.5) 2 + (yt − 0.5)2)\n+\n∑D d=1(D + 1− d)(ät)\n2∑D d=1(D + 1− d)\n(24)\nThe 10 joint angles trajectories are generated by a 10- dimensional DMP, where each dimension has B = 5 basis functions. The parameter vectors θ (one 1×5\nvector for each of the 10 dimensions), are initialized by training the DMP with a minimum-jerk movement. During learning, we run 10 trials per update K = 10, where the first of these 10 trials is a noise-free trial used for evaluation purposes. For PI2, the eliteness parameter is h = 10, and for CEM and CMAES it is Ke = K/2 = 5. The initial exploration noise is set to Σ = 104IB=5 for each dimension of the DMP."
    }, {
      "heading" : "3.2. Exploration Noise",
      "text" : "A first difference between CEM/CMAES and PI2 is the way exploration noise is generated. In CEM and CMAES, time does not play a role, so only one exploration vector θk is generated per trial. In stochastic optimal control, from which PI2 is derived, θi represents a motor command at time i, and the stochasticity θi+ i is caused by executing command in the environment. When applying PI2 to DMPs, this stochasticity rather represents controlled noise to foster exploration, which the algorithm samples from θi ∼ N (θ,Σ). We call this time-varying exploration noise. Since this exploration noise is under our control, we need not vary it at every time step. In the work by Theodorou et al. (2010) for instance, only one exploration vector θk is generated at the beginning of a trial, and exploration is only applied to the DMP basis function that has the highest activation. We call this per-basis exploration noise. In the most simple version, called constant exploration noise, we sample θk,i=0 once at the beginning for i = 0, and leave it unchanged throughout the execution of the movement, i.e. θk,i = θk,i=0.\nThe learning curves for these different variants are depicted in Figure 3. We conclude that time-varying exploration convergences substantially slower. Because constant exploration gives the fastest convergence, we use it throughout the rest of the paper."
    }, {
      "heading" : "3.3. Definition of Eliteness",
      "text" : "In each of the algorithms, the mapping from costs to probabilities is different. CEM implements a cut-off value for ‘eliteness’: you are either elite (Pk = 1/Ke) or not (Pk = 0). PI\n2 rather considers eliteness to be a continuous value that is inversely proportional to the cost of a trajectory. CMAES uses a hybrid eliteness measure where samples have zero probability if they are not elite, and a continuous value which is inverse proportional to the cost if they are elite. The probabilities in CMAES do not have to be Pk = 1/Ke as for CEM, but can be chosen by the user, as long as the constraints ∑Ke k=1 Pk = 1 and P1 ≥ · · · ≥ PKe are met. Here, we use the defaults suggested by Hansen & Ostermeier (2001), i.e. Pk = ln (0.5(K + 1))− ln(k).\nThese different mappings are visualized in Figure 4. An interesting similarity between the algorithms is that they each have a parameter – Ke in CEM/CMAES, and h in PI2 – that determines how ‘elitist’ the mapping from cost to probability is. Typical values are h = 10 and Ke = K/2. These and other values of h and Ke are depicted in Figure 4.\nThe average learning curves in Figure 5 are all very similar except for CEM with Ke = 5/7. This verifies the conclusion by Hansen & Ostermeier (2001) that choosing these weights is “relatively uncritical and can be chosen in a wide range without disturbing the adaptation procedure.” and choosing the optimal weights for a particular problem “only achieves speed-up factors of less than two” when compared with CEM-style weighting where all the weights are Pk = 1/Ke. Be-\ncause choosing the weights is uncritical, we use the PI2 weighting scheme with h = 10, the default suggested by Theodorou et al. (2010), throughout the rest of this paper."
    }, {
      "heading" : "3.4. Covariance Matrix Adaptation",
      "text" : "We now turn to the most interesting and relevant difference between the algorithms. In CEM/CMAES, both the mean and covariance of the distribution are updated, whereas PI2 only updates the mean. This is because in PI2 the shape of the covariance matrix is constrained by the relation Σ = λR−1, where R is the (fixed) command cost matrix, and λ is a parameter inversely proportional to the parameter h. This constraint is necessary to perform the derivation of PI2 (Theodorou et al., 2010).\nIn this paper, we choose to ignore the constraint Σ = λR−1, and apply covariance matrix updating to PI2. Because a covariance matrix update is computed for each time step i (17), we need to perform temporal averaging for the covariance matrix (19), just as we do for the mean θ. Temporal averaging over covariance matrices is possible, because 1) every positive-semidefinite matrix is a covariance matrix and vice versa 2) a weighted averaging over positivesemidefinite matrices yields a positive-semidefinite matrix (Dattorro, 2011).\nThus, rather than having a fixed covariance matrix, PI2 now adapts Σ based on the observed costs for the trials, as depicted in Figure 4. This novel algorithm, which we call PI2-CMA, for “Path Integral\nPolicy Improvement with Covariance Matrix Adaptation”, is listed in Table 1 (excluding the red indices i = 1 . . . N in (7), and including the green equations (17) and (19)). A second algorithm, PI2CMAES, is readily acquired by using the more sophisticated covariance matrix updating rule of CMAES. Our next evaluation highlights the main advantage of these algorithms, and compares their performance.\nIn Figure 6, we compare PI2 (where the covariance matrix is constant3) with PI2-CMA (CEM-style covariance matrix updating) and PI2-CMAES (covariance matrix updating with CMAES). Initially, the covariance matrix for each of the 10 DOFs is set to Σinit = λinitI5, where 5 is the number of basis functions, and λinit = {102, 104, 106} determines the initial exploration magnitude. All experiments are run for 200 updates, with K = 20 trials per update. We chose a higher K because we are now not only computing an update of the mean of the parameters (a 1 × 5 vector for each DOFs), but also its covariance matrix (a 5× 5 matrix), and thus more information is needed per trial to get a robust update (Hansen & Ostermeier, 2001). After each update, a small amount of base level exploration noise is added to the covariance matrix (Σnew ← Σnew + 102I5) to avoid premature convergence, as suggested by Kobilarov (2011).\nWhen the covariance matrices are not updated, the exploration magnitude remains the same during learning, i.e. λ = λinit (labels A in Figure 6), and the convergence behavior is different for the different exploration magnitudes λinit = {102, 104, 106}. For λinit = 104 we have nice convergence behavior B , which is not a coincidence – this value has been specifically tuned for this task, and it is the default we have used so far. However, when we set the exploration magnitude very low (λinit = 10\n2) convergence is much slower C . When the exploration magnitude is set very high λinit = 10\n6, we get quick convergence D . But due to the high stochasticity in sampling, we still have a lot of stochasticity in the cost after convergence in comparison to lower λinit. This can be seen in the inset, where the y-axis has been scaled ×20 for detail E .\nFor PI2-CMA, i.e. with covariance matrix updating, we see that the exploration magnitude λ changes over time (bottom graph), whereby λ is computed as the mean of the eigenvalues of the covariance matrix. For λinit = 10 2, λ rapidly increases F until a maximum\n3Please note the difference between 1) constant exploration as in Section 3.2, where a sampled parameter vector θk is not varied during the movement made in one trial; 2) constant covariance matrix, where Σ is not updated and thus constant during an entire learning session.\nvalue is reached, after which it decreases and converges to a value of 102.8 G . The same holds for λinit = 10\n4, but the initial increase is not so rapid H . For λinit = 10\n6, λ only decreases I , but converges to 102.8 as the others.\nFrom these results we derive three conclusions: 1) with PI2-CMA, the convergence speed does not depend as much on the initial exploration magnitude λinit, i.e. after 500 updates the µ± σ cost for PI2-CMA over all λinit is 10\n5 ·(8±7), whereas for PI2 without covariance matrix updating it is 105 · (35± 43) J . 2) PI2-CMA automatically increases λ if more exploration leads to quicker convergence F H . 3) PI2-CMA automatically decreases λ once the task has been learned G K . Note that 2) and 3) are emergent properties of covariance matrix updating, and has not been explicitly encoded in the algorithm. In summary, PI2-CMA is able to find a good exploration/exploitation trade-off, independent of the initial exploration magnitude.\nThis is an important property, because setting the exploration magnitude by hand is not straightforward, because it is highly task-dependent, and might require several evaluations to tune. One of the main contributions of this paper is that we demonstrate how\nusing probability-weighted averaging to update the covariance matrix (as is done in CEM) allows PI2 to autonomously tune the exploration magnitude – the user thus no longer needs to tune this parameter. The only remaining parameters of PI2 are K (number of trials per update) and h (eliteness parameter), but choosing them is not critical. Although an initial Σ must be given, Figure 6 shows that with an initial exploration magnitude two orders of magnitude higher/lower than a tuned value, PI2-CMA still converges to the same cost and exploration magnitude, with only slight differences in the initial speed of convergence.\nWhen comparing PI2-CMA and PI2-CMAES, we only see a very small difference in terms of convergence when the initial exploration is low λinit = 10 2 L . This is because the covariance update rule of CMAES is damped, (21) and (23), and it makes more conservative updates than CEM, cf. I and N . In our experiments, PI2-CMAES uses the default parameters suggested by Hansen & Ostermeier (2001). We have tried different parameters for PI2-CMAES, the conclusion being that the best parameters are those that reduce CMAES to CEM, cf. Section 2.2. In general, we do not claim that PI2-CMAES outperforms PI2, and Hansen & Ostermeier (2001) also conclude that there are tasks where CMAES has identical performance to simpler algorithms. Our results on comparing PI2CMAES and PI2-CMA are therefore not conclusive. An interesting question is whether typical cost functions found in robotics problems have properties that do not allow CMAES to leverage the advantages it has on benchmark problems used in optimization."
    }, {
      "heading" : "4. Conclusion",
      "text" : "In this paper, we have scrutinized the recent stateof-the-art direct policy improvement algorithm PI2 from the specific perspective of belonging to a family of methods based on the concept of probabilityweighted averaging. We have discussed similarities and differences between three algorithms in this family, being PI2, CMAES and CEM. In particular, we have demonstrated that using probability-weighted averaging to update the covariance matrix, as is done in CEM and CMAES, allows PI2 to autonomously tune the exploration magnitude. The resulting algorithm PI2-CMA shows more consistent convergence under varying initial conditions, and alleviates the user from having to tune the exploration magnitude parameter by hand. We are currently applying PI2-CMA to challenging tasks on a physical humanoid robot. Given the ability of PI2 to learn complex, high-dimensional tasks on real robots (Stulp et al., 2011), we are con-\nfident that PI2-CMA can also successfully be applied to such tasks."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank the reviewers for their constructive suggestions for improvement of the paper. This work is supported by the French ANR program (ANR 2010 BLAN 0216 01), more at http://macsi.isir.upmc.fr"
    } ],
    "references" : [ {
      "title" : "Cross-entropy optimization of control policies with adaptive basis functions",
      "author" : [ "L. Busoniu", "D. Ernst", "Schutter", "B. De", "R. Babuska" ],
      "venue" : "IEEE Transactions on Systems, Man, and Cybernetics,",
      "citeRegEx" : "Busoniu et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Busoniu et al\\.",
      "year" : 2011
    }, {
      "title" : "Convex Optimization & Euclidean Distance Geometry",
      "author" : [ "J. Dattorro" ],
      "venue" : "Meboo Publishing USA,",
      "citeRegEx" : "Dattorro,? \\Q2011\\E",
      "shortCiteRegEx" : "Dattorro",
      "year" : 2011
    }, {
      "title" : "Completely derandomized self-adaptation in evolution strategies",
      "author" : [ "N. Hansen", "A. Ostermeier" ],
      "venue" : "Evolutionary Computation,",
      "citeRegEx" : "Hansen and Ostermeier,? \\Q2001\\E",
      "shortCiteRegEx" : "Hansen and Ostermeier",
      "year" : 2001
    }, {
      "title" : "Evolution strategies for direct policy search",
      "author" : [ "Heidrich-Meisnerm V", "C. Igel" ],
      "venue" : "In Proc. of the Int’l Conference on Parallel Problem Solving from Nature,",
      "citeRegEx" : "V. and Igel,? \\Q2008\\E",
      "shortCiteRegEx" : "V. and Igel",
      "year" : 2008
    }, {
      "title" : "Movement imitation with nonlinear dynamical systems in humanoid robots",
      "author" : [ "A.J. Ijspeert", "J. Nakanishi", "S. Schaal" ],
      "venue" : "In Proc. of the IEEE Int’l Conference on Robotics and Automation (ICRA),",
      "citeRegEx" : "Ijspeert et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Ijspeert et al\\.",
      "year" : 2002
    }, {
      "title" : "Policy search for motor primitives in robotics",
      "author" : [ "J. Kober", "J. Peters" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Kober and Peters,? \\Q2011\\E",
      "shortCiteRegEx" : "Kober and Peters",
      "year" : 2011
    }, {
      "title" : "Cross-entropy randomized motion planning",
      "author" : [ "M. Kobilarov" ],
      "venue" : "In Proceedings of Robotics: Science and Systems,",
      "citeRegEx" : "Kobilarov,? \\Q2011\\E",
      "shortCiteRegEx" : "Kobilarov",
      "year" : 2011
    }, {
      "title" : "The CrossEntropy Method for fast policy search",
      "author" : [ "S. Mannor", "R.Y. Rubinstein", "Y. Gat" ],
      "venue" : "In Proceedings of the Int’l Conference on Machine Learning,",
      "citeRegEx" : "Mannor et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Mannor et al\\.",
      "year" : 2003
    }, {
      "title" : "Learning cost-efficient control policies with XCSF: Generalization capabilities and further improvement",
      "author" : [ "D. Marin", "J. Decock", "L. Rigoux", "O. Sigaud" ],
      "venue" : "In Proc. of Genetic and evolutionary computation,",
      "citeRegEx" : "Marin et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Marin et al\\.",
      "year" : 2011
    }, {
      "title" : "Exploring parameter space in reinforcement learning. Paladyn",
      "author" : [ "T. Rückstiess", "F. Sehnke", "T. Schaul", "D. Wierstra", "Y. Sun", "Schmidhuber" ],
      "venue" : "Journal of Behavioral Robotics,",
      "citeRegEx" : "Rückstiess et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Rückstiess et al\\.",
      "year" : 2010
    }, {
      "title" : "Learning to grasp under uncertainty",
      "author" : [ "F. Stulp", "E. Theodorou", "J. Buchli", "S. Schaal" ],
      "venue" : "In Proceedings of the Int’l Conference on Robotics and Automation,",
      "citeRegEx" : "Stulp et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Stulp et al\\.",
      "year" : 2011
    }, {
      "title" : "A generalized path integral control approach to reinforcement learning",
      "author" : [ "E. Theodorou", "J. Buchli", "Schaal" ],
      "venue" : "J. of Machine Learning Research,",
      "citeRegEx" : "Theodorou et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Theodorou et al\\.",
      "year" : 2010
    }, {
      "title" : "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
      "author" : [ "R.J. Williams" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Williams,? \\Q1992\\E",
      "shortCiteRegEx" : "Williams",
      "year" : 1992
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "Scaling reinforcement learning (RL) methods to continuous state-action problems, such as humanoid robotics tasks, has been the focus of numerous recent studies (Kober & Peters, 2011; Theodorou et al., 2010).",
      "startOffset" : 160,
      "endOffset" : 206
    }, {
      "referenceID" : 12,
      "context" : "from first principles of stochastic optimal control, and is able to outperform gradient-based RL algorithms such as REINFORCE (Williams, 1992) and Natural Actor-Critic (Peters & Schaal, 2008) by an order of magnitude in terms of convergence speed and quality of the final solution (Theodorou et al.",
      "startOffset" : 126,
      "endOffset" : 142
    }, {
      "referenceID" : 11,
      "context" : "from first principles of stochastic optimal control, and is able to outperform gradient-based RL algorithms such as REINFORCE (Williams, 1992) and Natural Actor-Critic (Peters & Schaal, 2008) by an order of magnitude in terms of convergence speed and quality of the final solution (Theodorou et al., 2010).",
      "startOffset" : 281,
      "endOffset" : 305
    }, {
      "referenceID" : 6,
      "context" : "CEM has also been used in combination with sampling-based motion planning (Kobilarov, 2011).",
      "startOffset" : 74,
      "endOffset" : 91
    }, {
      "referenceID" : 8,
      "context" : "In (Marin et al., 2011), a CEM is extended to optimize a controller that generates trajectories to any point of the reachable space of the system.",
      "startOffset" : 3,
      "endOffset" : 23
    }, {
      "referenceID" : 0,
      "context" : "Busoniu et al. (2011) extend this work, and use CEM to learn a mapping from continuous states to discrete actions, where the centers and widths of the basis functions are automatically adapted.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 9,
      "context" : "Rückstiess et al. (2010) use Natural Evolution Strategies (NES), which has comparable results with CMAES, to di-",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 12,
      "context" : "The results above are compared with various gradient-based methods, such as REINFORCE (Williams, 1992) and NAC (Peters & Schaal, 2008).",
      "startOffset" : 86,
      "endOffset" : 102
    }, {
      "referenceID" : 11,
      "context" : "Also, we use Dynamic Movement Primitives as the underlying policy representation, which 1) enables us to scale to higher-dimensional problems, as demonstrated by (Theodorou et al., 2010); 2) requires us to perform temporal averaging, cf.",
      "startOffset" : 162,
      "endOffset" : 186
    }, {
      "referenceID" : 11,
      "context" : "PI is derived from first principles of optimal control, and gets its name from the application of the Feynman-Kac lemma to transform the HamiltonJacobi-Bellman equations into a so-called path integral, which can be approximated with Monte Carlo methods (Theodorou et al., 2010).",
      "startOffset" : 253,
      "endOffset" : 277
    }, {
      "referenceID" : 4,
      "context" : "So far, PI has mainly been applied to policies represented as Dynamic Movement Primitives (DMPs) (Ijspeert et al., 2002), where θ determines the shape of the movement.",
      "startOffset" : 97,
      "endOffset" : 120
    }, {
      "referenceID" : 11,
      "context" : "(Theodorou et al., 2010).",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 11,
      "context" : "When a cost function is compatible with both PoWeR and PI, they perform essentially identical (Theodorou et al., 2010).",
      "startOffset" : 94,
      "endOffset" : 118
    }, {
      "referenceID" : 0,
      "context" : "Whereas in other works the motivation for using CEM/CMAES for policy improvement is based on its empirical performance (Busoniu et al., 2011; HeidrichMeisner and Igel, 2008; Rückstiess et al., 2010) (e.",
      "startOffset" : 119,
      "endOffset" : 198
    }, {
      "referenceID" : 9,
      "context" : "Whereas in other works the motivation for using CEM/CMAES for policy improvement is based on its empirical performance (Busoniu et al., 2011; HeidrichMeisner and Igel, 2008; Rückstiess et al., 2010) (e.",
      "startOffset" : 119,
      "endOffset" : 198
    }, {
      "referenceID" : 11,
      "context" : "it is shown to outperform a particular gradientbased method), the PI derivation (Theodorou et al., 2010) demonstrates that there is a theoretically sound motivation for using methods based on probabilityweighted averaging, as this principle follows directly from first principles of stochastic optimal control.",
      "startOffset" : 80,
      "endOffset" : 104
    }, {
      "referenceID" : 0,
      "context" : "Note that any differences between PI and CEM/CMAESin general also apply to the specific application of CEM/CMAES to policy improvement, as done for instance by Busoniu et al. (2011) or Heidrich-Meisner and Igel (2008).",
      "startOffset" : 160,
      "endOffset" : 182
    }, {
      "referenceID" : 0,
      "context" : "Note that any differences between PI and CEM/CMAESin general also apply to the specific application of CEM/CMAES to policy improvement, as done for instance by Busoniu et al. (2011) or Heidrich-Meisner and Igel (2008). Before comparing the algorithms, we first present the evaluation task used in the paper.",
      "startOffset" : 160,
      "endOffset" : 218
    }, {
      "referenceID" : 11,
      "context" : "This viapoint task is taken from (Theodorou et al., 2010), where it is used to compare PI with PoWeR (Kober & Peters, 2011), NAC (Peters & Schaal, 2008), and REINFORCE (Williams, 1992).",
      "startOffset" : 33,
      "endOffset" : 57
    }, {
      "referenceID" : 12,
      "context" : ", 2010), where it is used to compare PI with PoWeR (Kober & Peters, 2011), NAC (Peters & Schaal, 2008), and REINFORCE (Williams, 1992).",
      "startOffset" : 118,
      "endOffset" : 134
    }, {
      "referenceID" : 11,
      "context" : "(Theodorou et al., 2010).",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 11,
      "context" : "In the work by Theodorou et al. (2010) for instance, only one exploration vector θk is generated at the beginning of a trial, and exploration is only applied to the DMP basis function that has the highest activation.",
      "startOffset" : 15,
      "endOffset" : 39
    }, {
      "referenceID" : 11,
      "context" : "Because choosing the weights is uncritical, we use the PI weighting scheme with h = 10, the default suggested by Theodorou et al. (2010), throughout the rest of this paper.",
      "startOffset" : 113,
      "endOffset" : 137
    }, {
      "referenceID" : 11,
      "context" : "This constraint is necessary to perform the derivation of PI (Theodorou et al., 2010).",
      "startOffset" : 61,
      "endOffset" : 85
    }, {
      "referenceID" : 1,
      "context" : "Temporal averaging over covariance matrices is possible, because 1) every positive-semidefinite matrix is a covariance matrix and vice versa 2) a weighted averaging over positivesemidefinite matrices yields a positive-semidefinite matrix (Dattorro, 2011).",
      "startOffset" : 238,
      "endOffset" : 254
    }, {
      "referenceID" : 6,
      "context" : "After each update, a small amount of base level exploration noise is added to the covariance matrix (Σnew ← Σnew + 10I5) to avoid premature convergence, as suggested by Kobilarov (2011).",
      "startOffset" : 169,
      "endOffset" : 186
    }, {
      "referenceID" : 10,
      "context" : "Given the ability of PI to learn complex, high-dimensional tasks on real robots (Stulp et al., 2011), we are confident that PI-CMA can also successfully be applied to such tasks.",
      "startOffset" : 80,
      "endOffset" : 100
    } ],
    "year" : 2012,
    "abstractText" : "There has been a recent focus in reinforcement learning on addressing continuous state and action problems by optimizing parameterized policies. PI is a recent example of this approach. It combines a derivation from first principles of stochastic optimal control with tools from statistical estimation theory. In this paper, we consider PI as a member of the wider family of methods which share the concept of probability-weighted averaging to iteratively update parameters to optimize a cost function. We compare PI to other members of the same family – Cross-Entropy Methods and CMAES – at the conceptual level and in terms of performance. The comparison suggests the derivation of a novel algorithm which we call PI-CMA for “Path Integral Policy Improvement with Covariance Matrix Adaptation”. PI-CMA’s main advantage is that it determines the magnitude of the exploration noise automatically.",
    "creator" : "LaTeX with hyperref package"
  }
}