{
  "name" : "1406.1241.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "The Best Templates Match Technique For Example Based Machine Translation",
    "authors" : [ "T. El-Shishtawy" ],
    "emails" : [ "shishtawy@hotmail.com", "sammaka@ipa.edu.sa" ],
    "sections" : [ {
      "heading" : null,
      "text" : "applications require acquisition of huge knowledge about language and about the world. This knowledge is encoded in computational grammars, lexicons and domain models. Another approach – which avoids the need for collecting and analyzing massive knowledge- is the Example Based approach, which is the topic of this paper.\nWe show through the paper that using Example Based in its native form is not suitable for translating into Arabic. Therefore a modification to the basic approach is presented to improve the accuracy of the translation process. The basic idea of the new approach is to improve the technique by which template-based approaches select the appropriate templates. It relies on extracting, from a parallel Bilingual Corpus, all possible templates that could match parts of the source sentence. These templates are selected as suitable candidate chunks for the source sentence. The corresponding Arabic templates are also extracted and represented by a diredted graph. Each branch represents one possible string of templates candidate to represent the target sentence. The shortest continuous path or the most probable tree branch is selected to represent the target sentence. Finally the Arabic translation of the selected tree branch is generated.\nKeywords: Arabic Machine Translation, Arabic Example Based Machine Translation, Parallel Bilingual Corpus, Morphology Analysis, Template Chunk, Correspondence Matrix, Directed Graph.\nبولسأ لامنارثكلا ط قباطت ا ةلثملا ىلع ةينبملا ةيللآا ةمجرتلل حلا تاقيبطتلا نأ تبث دقلقيق ودح ةدفرعملا ندم ًادا ارددق بلطتت ةفرعملا ىلع ةينبملا ةيللآا ةمجرتلل ةي\nةدغللا اددطنلاو جاددعملاو درددةلاو وددحنلل ةيبادسح دددعاوق حروددة بدف دراددعملا دديمرت متدي مدديح ي دديف دبطت ت ددلا م مادختساب ةمجرتلل ىرخأ ةقيرط محبلا ا ا خ نم مدقن اننإف كل ل يسيماوقلاو ثملا باطتلا طامنلألي\nةغللا ىلإ ةمجرتلل حلةيلا ةيلةلا تروة بف ةلثملا ةطساوب ةمجرتلا بولسأ مادختسا نأ محبلا نيبيو نيدسحتل ةيدساسلا ةدقيرطلل ت يددعتلا يدعب ااردتقإ مدت كل دل يةديبرعلا و يةدمجرتلا ةديلمع حةادفكدت ةدقيرطلا صخلت\nحددبدجلا ندم ةا دجل ةدقباطملا ةدلمتحملا طادمنلا دك حارختدسلا ةدغللا بًادنث تودغل دنكمب ةناعتدسلاا بدف دحرتقملا ك توددغللا ددنكملا ددخ نددم ددباقملا بددبرعلا طمنددلا حارختددسا كل ددك متدديو ك اددععيطقت دددعب اددعتمجرت دارددملا ةددلمجلا\nتلا ةقيرطلا نيسحتلو طامنلا رايتخإ اعب متي ب اعليثمت متي دثمت عورف نم نوكتي ت لا \" جوملا ططخملا\" ةطساوب يةلمجلا نم ة جل ةمجرتلا طامنأ تلاامتحا دحأ اعنم حدحاو ك مث ندع مدحبلاب ةديًاعنلا ةدلمجلا طدمن حاتنتدسا متدي\nراسم ) ضفأ وأ( رةقأ ةتما ديلوت متي ةياعنلا بفو ك جوملا ططخملا خ يةمجرتملا ةلمجل"
    }, {
      "heading" : "1. Introduction",
      "text" : "Automatic translation between human languages (`Machine Translation') is a scientific dream of enormous social, political, and scientific importance. It was one of the earliest applications suggested for digital computers, but turning this dream into reality has turned out to be a much harder. And in spite of many problems, some degree of automatic translation is now available, and it is likely that during the next decade the bulk of routine technical and business translation will be done with some kind of automatic translation tools [1].\nUnfortunately, Arabic language is not involved in MT progress. Very few systems do exist, but they lack an open architecture of the underlying theoretical basics of Arabic language. This lack - hidden, or missed - do not give a good picture of the state of MT that involves Arabic language.\nIt has been proved that large-scale realistic Knowledge Based Machine Translation (KBMT) applications require acquisition of huge knowledge about language and about the world. This knowledge is encoded in computational grammars, lexicons and domain models [2]. Some researchers seek ways of bringing down the price of knowledge acquisition by applying ways of automatically or semi-automatically extracting relevant information from machine-readable dictionaries [3] or text corpora [4]. Another approach – which avoids the need for collecting and analyzing massive knowledge- is the Example Based approach."
    }, {
      "heading" : "2. Example Based Machine Translation (EBMT)",
      "text" : "Due to the increasing availability of large amounts of machine-readable textual material, a number of research groups investigate the possibilities for `empirical' approaches to machine translation.\nExample-based translation is essentially translation by analogy. An Example-Based Machine Translation (EBMT) system is given a set of sentences in the source language (from which one is translating) and their corresponding translations in the target language, and uses those examples to translate other, similar source-language sentences into the target language [5]. The basic premise is that, if a previously translated sentence occurs again, the same translation is likely to be correct again [6]. It takes the advantage of aligned parallel corpora with a large number of short aligned text structures (Scania Corpora), to produce translation equivalent between English and Swedish.\nIn the `translation by analogy', or `example-based' approach, there are no mapping rules, only procedure which involves matching against stored example translations. The basic idea is to collect a bilingual corpus of translation pairs and then use a best match algorithm to find the closest example to the source phrase in question. This gives a translation template, which can then be filled in by word-for-word translation [7].\nA main system which is based on EBMT is the Pangloss MT project [8] which translates from Spanish to English. The strategy is based on matching between chunks at word level. The chunks can be miss ordered or different in lengths which is not suitable for Arabic language.\nUsing EBMT in its native form, which is based on exact string matching, may include many complications when translating into Arabic. First, a huge parallel corpus is required to ensure matching of any input English sentence while translating at a sentence level. In this research, the translation is done at a smaller granularity level – which is called the chunk level. Second, mapping one-to-one to the sequence of words between the source language and Arabic as target language, will results in inaccurate translation. This is because the changes of the Arabic word (nouns, verbs, adjectives) according to the accompanied gender, count, and tense. Therefore, the aim of this paper is to seek for another solution which takes into account the features of Arabic language."
    }, {
      "heading" : "3. The Best-Template Match Approach",
      "text" : "Another approach to example-based translations is to use a template-based model instead of string-based one. In this approach the system depends on template-based syntactic matching of the input English sentence, and the templates stored in a database. Once a matching occurs, the system gradually modifies the corresponding Arabic part to produce correct translation. It has the advantage that they require little examples in the bilingual parallel corpus archive to generate good translations. The template-based translation approach suffers from the problem of inaccurate or bad selection of templates for the source sentence. This affects the accuracy of the translation process. The proposed new approach is motivated to improve the translation accuracy by improving the technique by which template-based approaches select the appropriate templates. .\nIn the current work, we make use of a pre-built parallel bilingual corpus [9]. This corpus is organized as set of chunks in English language and their corresponding Arabic ones. Organizing the parallel corpus at chunk level, which are smaller building blocks of sentence, will be more flexible in translating different combinations of chunks using the same training corpus. Also, the proposed system relies on a rich on-line bi-directional dictionary, which was designed and prepared previously for ACLP-Machine Translation [10]. The dictionary contains about 40,000 English words and their corresponding translations into Arabic.\nThe basic idea of the new approach is to extract, from a parallel bilingual corpus, all possible templates that could match parts of the source sentence. These templates are selected as suitable candidate chunks for the source sentence. The corresponding Arabic templates are also extracted. To overcome the problem of inaccurate or bad selection of templates, the found chunks are represented in a tree structure. Each tree branch represents one possible string of templates candidate to represent the target sentence. The shortest continuous path is selected to represent the target sentence. Finally the Arabic translation of the selected tree branch is generated."
    }, {
      "heading" : "4. The Proposed Model",
      "text" : "The algorithm of the proposed translation model is shown in figure (1). It starts with a morphological analysis (word category and lexical attributes) of the given English sentence. This valuable information will be used to extract all possible English chunk templates that could match possible tags of the given English sentence. The extracted templates are achieved by consulting the English-Arabic Parallel Corpus. Then the occurrence of each word of the given sentence within the candidate templates is tested. A correspondence matrix represents the result of this test is constructed. The corresponding matrix is tuned by removing useless templates and adding dummy templates. Set of template groups are extracted from the tuned correspondence matrix. Each group represents a possible path that covers all words of the given sentence. These paths are represented by a directed graph. The shortest path in the graph is selected as a string of chunk templates that best describes the given sentence. The corresponding string of Arabic chunk templates is extracted by consulting the English-Arabic corpus. Finally, the Arabic sentence of these chunks is generated, which represents the suggested translation of the given English sentence.\nDetailed description of the different phases of the proposed model is as follows:"
    }, {
      "heading" : "4.1 Analysis of Source Sentence",
      "text" : "The given English sentence is analyzed by the following algorithm:\nThe input sentence is segmented into word tags.\nFor each word\nSearch for the word in the English dictionary [10]\nIf it is found\nExtract the word lexical attributes, i.e. (Category, tense, count, gender, …. )\nElse strip a prefix from the word (if found) and repeat Else strip a suffix from the word (if found) and repeat\nEnd For"
    }, {
      "heading" : "4.2 Extracting all Possible Template Chunks",
      "text" : "The sequence of words (word category and its lexical attributes) constituting the\ninput English sentence (w1 w2 w3 … wn) is applied to the following algorithm to extract all the possible template chunks:\na. Start with the first word as one possible chunk. b. Search bilingual corpus for matching and extract its corresponding Arabic\nchunk\nc. Successively add next words –one by one- to the possible chunk, and repeat searching.(b) until the end of the sentence. d. Exclude the first word from the sentence and repeat (b) and (c).. e. Repeat (d) until the last word has been reached as the last chunk.\nThe output chunks consist of one or more consecutive words of the input English sentence and take the form: Ch_tem1 = {w1} Ch_tem2 = {w1, w2} Ch_tem3 = {w1, w2, w3} Ch_temn = {w1, w2, w3,…..,wn} Ch_temn+1 = {w2} Ch_temn+2 = {w2, w3} ……….. Ch_tem2n-1 = {w2, w3,…..,wn} ……….. Ch_temn(n+1)/2 = {wn}"
    }, {
      "heading" : "4.3 Searching for English chunks",
      "text" : "In this phase we will search for all possible sentence chunk templates in the bilingual corpus. This can be achieved by matching the extracted English chunk templates with those stored in the bilingual parallel corpus. If the corpus template exactly matches the current sentence chunk template, the English chunk and its corresponding Arabic template are stored."
    }, {
      "heading" : "4.4 Constructing the Correspondence Matrix",
      "text" : "The next step of our approach is to check the existence of the candidate English chunks within the English-Arabic parallel corpus. The result of the found chunks and their word coverage is then represented using a correspondence matrix. The correspondence matrix maps the coverage of the found English chunks, with the input sentence words. In other words, the contribution of each template chunk would be evaluated by describing the role of each template in representing part of the given sentence. This relation is best described by a correspondence matrix. The matrix rows represent the candidate chunks templates. The matrix columns represent the words patterns of the given sentence. The matrix cells take the values “1” or “0” according to the presence or absence of each word pattern within the found chunk template. The relation between chunks and words are many-to-many; i.e., chunk template can cover more than one word pattern. Also, the word pattern can belong to more than one chunk template.\nOur goal is then to search through the correspondence matrix for the best continuous chunks that cover the whole input English sentences. Table (1) shows different cases for an English sentence. On analyzing the correspondence matrix, one might find five different types of chunks: repeated, unreachable, dead-end, dummy and normal chunks. These chunks are treated as follows: 1- Repeated chunks are removed except one. For example Ch_tem1, Ch_tem4, and Ch_tem7 are repeated chunks that cover the same sequence of words 2- Useless template chunks which do not affect the process of selecting the proper templates are deleted such as unreachable and dead-end chunks. 3- Also one might detect empty columns, which means that there is a discontinuity between chunks. Dummy chunks are inserted to overcome this discontinuity.\nThe matrix shows several situations:\n1 Repeated chunks: Chunks which cover the same number and sequence of\nwords.\n2 Unreachable chunks: chunks which can not be reached since it starts where\nno other chunks end.\n3 Dummy chunks: Inserted chunks to overcome discontinuity\n4 Dead end chunks: chunks when reached lead to a dead end, since there is no\nother chunks start at its end.\n5 Normal Chunks."
    }, {
      "heading" : "4.5 Building the directed graph",
      "text" : "The best chunk's path can be detected by building a directed graph. In the graph, each node represents one word of the input sentence, and each branch represents a candidate chunk. The directed graph which represents the corresponding matrix is shown in Figure (2), where d1,d2,… define dummy chunks, 1,2,3,… represent words of the input sentence, and Ch_tem1, Ch_tem2,… are template chunks. The problem of complete translation is now reduced to finding an optimum path through the graph from first to final nodes (words). However in many real cases, there is discontinuity of the graph. For example, pronouns which are not found in parallel corpus can lead to such gap. To overcome this problem, we insert dummy chunks at all dead nodes, which are nodes without output branch (chunk). Dummy chunk connects dead nodes to their next neighborhood ones."
    }, {
      "heading" : "4.6 Selecting the optimum Path",
      "text" : "On examining directed graph, one might find that there is more than one possible path of chunk templates that can cover the whole word patterns of the given sentence. All the possible paths would be represented by an output table. Each row represents one possible path of chunk templates. Table (2) summarizes all possible paths for the previous graph.\nThe optimum path is selected as the shortest one with minimum dummy chunks. By\nexamining different experimental cases, we found the following criteria:\n- Longer chunks are preferred - Paths with minimum number of dummies are preferred.\nTo implement these criteria, we use a simple heuristic that gives advantages to complete paths without dummy chunks. If there is more than one path with no dummies, we select the shortest-which contains minimum number of chunks. In cases, where there are dummies in all paths, we select the one with the smallest number of dummies.\nIn the given example, all paths contain dummy chunks. Selecting paths (b), (c), (e), and (f) as candidates containing only one dummy chunks. Among these paths, we select the paths (b) and (e) as optimum paths. This means that there are two possible translations for the given sentence."
    }, {
      "heading" : "4.7 Extracting the corresponding Arabic templates",
      "text" : "The aim of this phase is to extract the Arabic templates corresponding to the English chunk templates of the selected path(s). This could be achieved by consulting the Bilingual Parallel Corpus. The extracted Arabic chunk templates are stored in the same sequence of the selected path. Table (3) shows the resultant Arabic chunk templates corresponding to English ones. During extraction, dummies are left unchanged in both sides. Actually, dummies represent unfound word(s) in parallel corpus, which may be due to pronouns or incomplete corpus. These unfound words are presented as English words inside the translated sentence."
    }, {
      "heading" : "4.8 Generating the Target Sentence",
      "text" : "A generation module is required to substitute Arabic templates with actual Arabic chunks. The inputs to this phase are the English chunks and their corresponding Arabic templates. Actually, Arabic chunks can’t produced by word-to-word substitution of English chunk word’s translation due to differences between English and Arabic surface structures. Therefore, Arabic template takes the form of command, which when applied to Arabic word, will produce the correct translation. Usually, the Arabic template is in the form of the following command:\n“add [xx] , class[yy] , add[zz] “\nWhere xx & zz are optional and represent any added words or prefixes and suffixes part of the word yy, where yy is mandatory and describe the Arabic word category. The following algorithm is used to generate the Arabic chunks:\nFor each English chunk do\nIf Chunk is dummy then\nCopy it to Arabic chunk\nElse\nFor each English word do\nGet its translation according to corresponding accompanied Arabic attributes\nFor each Arabic template do\nExecute the “add-class-add” command."
    }, {
      "heading" : "5. Example 1",
      "text" : "Assume the English sentence to be translated is “the Proteins are necessary for building our bodies”"
    }, {
      "heading" : "5.1 Analyzing the English sentence",
      "text" : "Word Lexical\nAttribute\nthe art\nproteins n [pl ,f]\nare be [p ,pl]\nnecessary adj\nfor prep\nbuilding v [ing]\nour poss [pl ,m ,1]\nbodies n [pl ,f]"
    }, {
      "heading" : "5.2 Extracting all Possible Template Chunks",
      "text" : "All Possible English Template Chunks\nart [def] art [def] n [pl ,f] art [def] n [pl ,f] be [p ,pl] art [def] n [pl ,f] be [p ,pl] adj art [def] n [pl ,f] be [p ,pl] adj prep art [def] n [pl ,f] be [p ,pl] adj prep v [ing] art [def] n [pl ,f] be [p ,pl] adj prep v [ing] poss [pl ,m ,1] art [def] n [pl ,f] be [p ,pl] adj prep v [ing] poss [pl ,m ,1] n [pl ,f] n [pl ,f] n [pl ,f] be [p ,pl]\nn [pl ,f] be [p ,pl] adj n [pl ,f] be [p ,pl] adj prep n [pl ,f] be [p ,pl] adj prep v [ing] n [pl ,f] be [p ,pl] adj prep v [ing] poss [pl ,m ,1] n [pl ,f] be [p ,pl] adj prep v [ing] poss [pl ,m ,1] n [pl ,f] be [p ,pl] be [p ,pl] adj be [p ,pl] adj prep be [p ,pl] adj prep v [ing] be [p ,pl] adj prep v [ing] poss [pl ,m ,1] be [p ,pl] adj prep v [ing] poss [pl ,m ,1] n [pl ,f] adj adj prep adj prep v [ing] adj prep v [ing] poss [pl ,m ,1] adj prep v [ing] poss [pl ,m ,1] n [pl ,f]\nPrep prep v [ing] prep v [ing] poss [pl ,m ,1] prep v [ing] poss [pl ,m ,1] n [pl ,f] v [ing] v [ing] poss [pl ,m ,1]\nv [ing] poss [pl ,m ,1] n [pl ,f] poss [pl ,m ,1] poss [pl ,m ,1] n [pl ,f] n [pl ,f]"
    }, {
      "heading" : "5.3 Searching for English chunks",
      "text" : "No English chunk English template\n1 for getting prep v [ing] 2 for eating prep v [ing] 3 in playing prep v [ing] 4 girls n [pl ,f] 5 the minerals art [def] n [pl ,f] 6 for feeding prep v [ing] 7 the proteins art [def] n [pl ,f] 8 necessary adj 9 for building prep v [ing]\n10 our bodies poss [pl ,m ,1] n [pl ,f] 11 the carbohydates art [def] n [pl ,f]\n12 the fats art [def] n [pl ,f] 13 necessary adj 14 the vitamins art [def] n [pl ,f]"
    }, {
      "heading" : "5.4 Constructing the Correspondence Matrix",
      "text" : "No English template w1 w2 w3 w4 w5 w6 w7 w8\nthe proteins are necessary for building our bodies art\n[def]\nn [pl ,f] be [p\n,pl]\nadj prep v [ing] poss\n[pl ,m ,1]\nn [pl\n,f] 1 prep v [ing] 2 prep v [ing] 3 prep v [ing] 4 n [pl ,f] 5 art [def] n [pl ,f] 6 prep v [ing] 7 art [def] n [pl ,f]\n8 adj 9 prep v [ing]\n10 poss [pl,m,1] n\n[pl ,f] 11 art [def] n [pl ,f] 12 art [def] n [pl ,f] 13 adj\n14 art [def] n [pl ,f]\nTuning the correspondence matrix is as follows:\nThe third word has no correspondence with any chunk, so dummy chunk (15) is added. The chunks (2, 3, 6, and 9) are repeated with chunk (1), so they are deleted. The chunks (14, 12, 11, and 7) are repeated with chunk (5), so they are deleted. The chunk (13) is repeated with chunk (8), so it is deleted. The chunk (4) is unreachable, so it is deleted.\nThe corresponding matrix after tuning is:\nN o English template w1 w2 w3 w4 w5 w6 w7 w8 the proteins are necessa\nry\nfor building our bodies\nart\n[def\n]\nn [pl ,f] be [p\n,pl]\nadj pre\np\nv [ing] poss [pl\n,m ,1]\nn [pl\n,f] 1 prep v [ing] 5 art [def] n [pl ,f] 8 adj\n10 poss [pl,m,1] n [pl\n,f] 15 dummy"
    }, {
      "heading" : "5.5 Selecting the Optimum Path",
      "text" : "Fortunately, in this example, only one possible path of chunk templates can cover the whole word patterns of the given sentence. This happened due to the positive impact of the pruning process applied to the correspondence matrix. This final path will be considered as the best template path that represents the given sentence.\nBest Template Path Chunk5 + chunk15 + chunk8 + chunk1 + chunk10"
    }, {
      "heading" : "5.6 Extracting the corresponding Arabic templates",
      "text" : "No English Chunk Template Path Arabic Chunk Templates 1 prep v [ing] (prep1) (v1 [source]) 5 art [def] n [pl ,f] (add [لا] n1 [pmean]) 8 adj (adj1 [s ,f]) 10 poss [pl,m,1] n [pl ,f] (n1 [pmean] add [ان]) 15 dummy dummy"
    }, {
      "heading" : "5.7 Generating the Target Sentence",
      "text" : "No Arabic template Arabic chunk\n1 (prep1) (v1 [source]) ءانبل 5 (add [لا] n1 [pmean]) تاينيتوربلا 8 (adj1 [s ,f]) ةيرورض\n10 (n1 [pmean] add [ان]) انماسجأ 15 dummy\nThe final translation is:\n\" انماسجأ ءانبل ةيرورض تاينيتوربلا \""
    }, {
      "heading" : "6. Example 2",
      "text" : "Another example is proposed to illustrate how correspondence matrix is pruned and the role of the directed graph in selecting the optimum chunk path.\nAssume the English sentence is “the good diet depends on the balance between the main elements for our bodies”.\nThe correspondence matrix is given by:\nNo w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 w11 w12 w13 w14 the good diet depends on the balance between the main elements for our bodies\nArt Adj N V Prep Def N Prep Art Adj N Prep Poss N\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\nRepeated chunks\nUnreachable\nDead End\nAfter pruning the matrix (removing repeated, unreachable and dead end chunks), the matrix is reduced to the following:\nNo w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 w11 w12 w13 w14 the good diet depends on the balance between the main elements for our bodies 1 2 3 6 7 8 11 12 13 14 15 16 19 20 21 27\nThe best path chunks\nThe directed graph representation can be summarized as:: Directed Graph Paths\nchunk1 + chunk6 + chunk8 + chunk11 + chunk14 + chunk15 + chunk19 + chunk21 chunk2 + chunk8 + chunk11 + chunk14 + chunk15 + chunk19 + chunk21 chunk3 + chunk13 + chunk14 + chunk15 + chunk19 + chunk21 chunk1 + chunk7 + chunk11 + chunk14 + chunk15 + chunk19 + chunk21 chunk1 + chunk6 + chunk8 + chunk12 + chunk15 + chunk19 + chunk21 chunk1 + chunk6 + chunk8 + chunk11 + chunk27 + chunk19 + chunk21 chunk1 + chunk6 + chunk8 + chunk11 + chunk14 + chunk16 + chunk21 chunk1 + chunk6 + chunk8 + chunk11 + chunk14 + chunk15 + chunk20 chunk2 + chunk8 + chunk12 + chunk15 + chunk19 + chunk21 chunk2 + chunk8 + chunk11 + chunk27 + chunk19 + chunk21 chunk2 + chunk8 + chunk11 + chunk14 + chunk16 + chunk21 chunk2 + chunk8 + chunk11 + chunk14 + chunk15 + chunk20 chunk3 + chunk13 + chunk27 + chunk19 + chunk21 chunk3 + chunk13 + chunk14 + chunk16 + chunk21 chunk3 + chunk13 + chunk14 + chunk15 + chunk20 chunk1 + chunk7 + chunk12 + chunk15 + chunk19 + chunk21 chunk1 + chunk7 + chunk11 + chunk27 + chunk19 + chunk21 chunk1 + chunk7 + chunk11 + chunk14 + chunk16 + chunk21 chunk1 + chunk7 + chunk11 + chunk14 + chunk15 + chunk20\nchunk1 + chunk6 + chunk8 + chunk12 + chunk16 + chunk21 chunk1 + chunk6 + chunk8 + chunk12 + chunk15 + chunk20 chunk1 + chunk6 + chunk8 + chunk11 + chunk27 + chunk20 chunk2 + chunk8 + chunk12 + chunk16 + chunk21 chunk2 + chunk8 + chunk12 + chunk15 + chunk20 chunk2 + chunk8 + chunk11 + chunk27 + chunk20 chunk3 + chunk13 + chunk27 + chunk20 chunk1 + chunk7 + chunk12 + chunk16 + chunk21 chunk1 + chunk7 + chunk12 + chunk15 + chunk20 chunk1 + chunk7 + chunk11 + chunk27 + chunk20\nSearching for the optimum path: Applying the heuristic that “fewest path's chunks and longer chunks are preferred leads to the selection of the path”\nchunk3 + chunk13 + chunk27 + chunk20\nConclusion\nIn this paper, we have presented a best template match technique for Example Based Machine Translation. The technique relies on using a tagged parallel corpus, aligned at chunk level. This allows much more flexibility in using the same chunk in many different sentences, and hence reduces the required corpus size. The technique tries all possible combinations of input words, and gets its corresponding chunks. A correspondence matrix is built, which maps the coverage of the found chunks to input English sentence. The matrix is then tuned to get the candidate chunks. The proposed technique then builds a directed graph that represents all suitable chunks which covers the whole input sentence (paths). Optimum path is selected to be a path with a minimum number of chunks, and avoids dummy chunks.\nIn this research, the translation is done at a smaller granularity chunk level, which\nallows the reduction of the required parallel corpus. More reduction in corpus size is also achieved by searching on word’s features rather than the word itself. Using tagged corpus is most suitable for Arabic language, which takes into account the different morphological forms, which enhances the quality of translation."
    } ],
    "references" : [ {
      "title" : "Multi- Purpose Development and Operation Environments for Natural Language Generation",
      "author" : [ "Nirenburg S", "P. Shell", "A. Cohen", "P. Cousseau", "D. Grannes", "McNeilly C" ],
      "venue" : "Proceedings of the Third Conference on Natural Language Applications. Trento,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1992
    }, {
      "title" : "Providing Machine Tractable Dictionary Tools",
      "author" : [ "Y. Wilks", "D. Fass", "C. Guo", "J. McDonald", "T. Plate", "Slator B" ],
      "venue" : "Machine Translation,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1990
    }, {
      "title" : "From N-Grams to Collocations: An Evaluation of Xtract",
      "author" : [ "F. Smadja" ],
      "venue" : "Proceedings of 29th ACL Meeting. Berkeley,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1991
    }, {
      "title" : "A framework of a mechanical translation between Japanese and English by analogy principle",
      "author" : [ "M Nagao" ],
      "venue" : "In: A. Elithorn and R. Banerji (eds.) Artificial and Human Intelligence. NATO Publications,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1984
    }, {
      "title" : "Extraction of Translation Equivalents from Parallel Corpora,",
      "author" : [ "J Tiedemann" ],
      "venue" : "The 11th Nordic Conference on Computational Linguistics,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1998
    }, {
      "title" : "Example-Based MT (EBMT)",
      "author" : [ "Bob Frederking" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2007
    }, {
      "title" : "A Full-Text Experiment in Example- Based Machine Translation",
      "author" : [ "Nirenburg S", "S. Beale", "C. Domashnev" ],
      "venue" : "Publications of School of Computer Science, Carnegie Mellon University, 1998.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "An Experiment for Automatic Alignment of Arabic-English Parallel Corpus",
      "author" : [ "T El-Shishtawy" ],
      "venue" : "Egyptian Computer Society Magazine,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2001
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "This knowledge is encoded in computational grammars, lexicons and domain models [2].",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 1,
      "context" : "Some researchers seek ways of bringing down the price of knowledge acquisition by applying ways of automatically or semi-automatically extracting relevant information from machine-readable dictionaries [3] or text corpora [4].",
      "startOffset" : 202,
      "endOffset" : 205
    }, {
      "referenceID" : 2,
      "context" : "Some researchers seek ways of bringing down the price of knowledge acquisition by applying ways of automatically or semi-automatically extracting relevant information from machine-readable dictionaries [3] or text corpora [4].",
      "startOffset" : 222,
      "endOffset" : 225
    }, {
      "referenceID" : 3,
      "context" : "An Example-Based Machine Translation (EBMT) system is given a set of sentences in the source language (from which one is translating) and their corresponding translations in the target language, and uses those examples to translate other, similar source-language sentences into the target language [5].",
      "startOffset" : 298,
      "endOffset" : 301
    }, {
      "referenceID" : 4,
      "context" : "The basic premise is that, if a previously translated sentence occurs again, the same translation is likely to be correct again [6].",
      "startOffset" : 128,
      "endOffset" : 131
    }, {
      "referenceID" : 5,
      "context" : "This gives a translation template, which can then be filled in by word-for-word translation [7].",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 6,
      "context" : "3 A main system which is based on EBMT is the Pangloss MT project [8] which translates from Spanish to English.",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 7,
      "context" : "In the current work, we make use of a pre-built parallel bilingual corpus [9].",
      "startOffset" : 74,
      "endOffset" : 77
    } ],
    "year" : 2014,
    "abstractText" : "It has been proved that large-scale realistic Knowledge Based Machine Translation (KBMT) applications require acquisition of huge knowledge about language and about the world. This knowledge is encoded in computational grammars, lexicons and domain models. Another approach – which avoids the need for collecting and analyzing massive knowledgeis the Example Based approach, which is the topic of this paper. We show through the paper that using Example Based in its native form is not suitable for translating into Arabic. Therefore a modification to the basic approach is presented to improve the accuracy of the translation process. The basic idea of the new approach is to improve the technique by which template-based approaches select the appropriate templates. It relies on extracting, from a parallel Bilingual Corpus, all possible templates that could match parts of the source sentence. These templates are selected as suitable candidate chunks for the source sentence. The corresponding Arabic templates are also extracted and represented by a diredted graph. Each branch represents one possible string of templates candidate to represent the target sentence. The shortest continuous path or the most probable tree branch is selected to represent the target sentence. Finally the Arabic translation of the selected tree branch is generated.",
    "creator" : "Microsoft® Word 2010"
  }
}