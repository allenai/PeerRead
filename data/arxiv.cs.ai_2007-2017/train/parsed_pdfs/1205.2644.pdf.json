{
  "name" : "1205.2644.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "First-Order Mixed Integer Linear Programming",
    "authors" : [ "Geoffrey J. Gordon" ],
    "emails" : [ "ggordon@cs.cmu.edu", "sahong@cs.cmu.edu", "mdudik@cs.cmu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Mixed integer linear programming (MILP) is a powerful representation often used to formulate decision-making problems under uncertainty. However, it lacks a natural mechanism to reason about objects, classes of objects, and relations. First-order logic (FOL), on the other hand, excels at reasoning about classes of objects, but lacks a rich representation of uncertainty. While representing propositional logic in MILP has been extensively explored, no theory exists yet for fully combining FOL with MILP. We propose a new representation, called first-order programming or FOP, which subsumes both FOL and MILP. We establish formal methods for reasoning about first order programs, including a sound and complete lifted inference procedure for integer first order programs. Since FOP can offer exponential savings in representation and proof size compared to FOL, and since representations and proofs are never significantly longer in FOP than in FOL, we anticipate that inference in FOP will be more tractable than inference in FOL for corresponding problems."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Mixed integer linear programming has established itself as a successful formalism for decision-making under uncertainty in operation research and cooperative control, and has attracted attention more recently in AI and machine learning. For example, in OR, a common approach to solving multi-stage planning problems under uncertainty is stochastic programming with recourse (e.g., [Powell, 1996]); or, in AI, we can use MILPs for planning [Vossen et al., 1999] or to reason about uncertainty due to the actions of other agents in a nonzero-sum game [Sandholm et al., 2005]; or, in ML, we can use MILPs for MAP inference in graphical models (e.g., [Roth and Yih, 2005]); or, in\ncooperative control, we can use MILPs for task allocation under uncertainty [Alighanbari and How, 2005].\nMany decision problems naturally contain objects, classes of objects, and relations among them. In such problems, there are many benefits to reasoning about entire classes of objects at once—so-called lifted reasoning. One benefit is representational: it is much simpler to state a single lifted constraint such as “all cars must follow the speed limit” than to state the constraint for each car separately. Another is computational: if we can derive a conclusion for all class members at once, then we don’t need to derive it separately for each individual object. Lifted inference may incur some initial overhead, but its cost is independent of the number of objects involved, even when this number is infinite. A final benefit is statistical: if we can share parameters among members of a class, we can often reduce the number of parameters we need to estimate, increasing the level of accuracy we can attain for a given amount of data.\nUnfortunately, MILPs lack an inherent mechanism to reason about classes of objects and relations. One might try to add this capability to MILPs using a “wrapper” or “compiler” such as AMPL [Fourer et al., 2002]: for example, to express a constraint that holds for all objects in a class, we can generate one copy of this constraint for every known object. However, in addition to losing the computational benefits of lifting, such a mechanism does not allow for reasoning when the number of objects is unknown or infinite, as is the case for example in entity resolution.\nFirst-order logic, on the other hand, is designed for lifted representation and reasoning: it uses unary predicates to refer to classes of objects, higher-arity predicates to refer to relations, and quantifiers to state properties that apply to many objects or tuples of objects at once. And, unification and resolution work on quantified statements directly, allowing us to reason efficiently about entire classes of objects. Unfortunately, FOL lacks a rich representation of uncertainty: in FOL, an atom’s truth value is either perfectly known or completely unknown.\nTo take advantage of the benefits of both languages, we\nintroduce a new representation, first-order programming, which subsumes both first order logic and mixed integer linear programming. FOP has syntax and semantics similar to FOL, but includes real and integer-valued predicates, analogous to MILP variables. We establish formal methods for reasoning in FOP, including a sound and complete lifted inference procedure for the integer fragment of FOP. (Our inference procedure works for mixed-integer FOP as well, but provides only weaker guarantees in this case.) To derive our procedure, we lift the well-known Gomory cutting plane algorithm; other families of cuts for MILPs could also lead to useful inference rules, but we focus on Gomory cuts for concreteness.\nWe proceed as follows. We first define a syntax and semantics for the FOP language, and give examples of problems represented in FOP. We then show how to transform FOP sentences to a normal form. Working from the normal form, we describe our inference procedure and prove its soundness and completeness. Finally, we discuss extensions, outline future work, and conclude."
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "It is well understood that we can combine propositional logic with integer programs, or translate one to the other. For example, Hooker and Osrio [1999] give an overview of the area and a general framework called mixed logical/linear programming. To handle the more general language of FOL, we can Herbrandize a sentence and translate it to a possibly-infinite MILP (cf. Sec. 5). Several authors have used this line of reasoning to bring optimization tools and theory to bear on FOL inference [Borkar et al., 2002, Chandru and Hooker, 1999]; results include compactness lemmas analogous to our Lemma 5.3 (but specialized to FOL), as well as ways to organize the search for a finite proof of infeasibility. Some of these search methods can be thought of as lifted inference procedures for FOL. However, none of these works extend the semantics of FOL, an essential contribution of FOP.\nEaves and Rothblum [1994] provide a syntax and semantics for mathematical programs with FOL-like quantifiers and connectives, called linear problems. However, the quantifiers are defined over assignments of numeric values to variables, not over assignments of objects as in FOL and FOP. Therefore, linear problems cannot reason about classes of objects, and do not generalize FOL. (In fact, Eaves and Rothblum show that linear problems are decidable in finite time, which implies that the language of linear problems is strictly less expressive than FOL.)\nRelational probabilistic languages (RPLs), such as plate models, probabilistic entity-relationship models [Heckerman et al., 2007], Markov logic networks [Richardson and Domingos, 2006], IBAL [Pfeffer, 2001], ICL [Poole, 2008], and BLOG [Milch et al., 2005], combine proba-\nbilistic and logical representations, and allow us to reason about uncertain statements about classes of objects. Hence, the goals of RPLs are in many ways similar to the goal of FOP. However, the focus of FOP is different from the focus of RPLs: while FOP is designed to capture uncertainty through nondeterministic choice and optimization (as in FOL and MILPs), RPLs are designed to capture uncertainty through marginalization and factorization (as in graphical models). Since FOP is Turing-complete, as are many RPLs, it is technically possible to translate any program from either type of representation to the other. But, for any given reasoning problem, we expect that the difference in representation styles means that either sort of language may lead to a much more natural and compact representation than the other.\nAdditionally, some RPLs do not provide semantics for unknown or infinite numbers of objects. (Exceptions include BLOG [Milch et al., 2005], which allows unknown numbers of objects; ICL [Poole, 2008], which allows unknown or infinite numbers of objects; and the work of Singla and Domingos [2007], which extends Markov logic to infinite domains, but makes strong assumptions in order to guarantee well-defined probability distributions.) Lifted inference algorithms have been proposed for RPLs, but many work only when the number of objects is finite and known; e.g., the work of Braz et al. [2005] on lifted variable elimination requires a known set of objects, and lifted belief propagation [Singla and Domingos, 2008] was described in terms of finite MLNs, leaving infinite domains explicitly for future work. In contrast, full lifted inference for firstorder logic is well understood (see, e.g., Russell and Norvig [2003]), and our inference procedure for FOP parallels the well-known resolution procedure for FOL."
    }, {
      "heading" : "3 DEFINITION OF FOP",
      "text" : ""
    }, {
      "heading" : "3.1 SYNTAX",
      "text" : "The syntax of FOP parallels that of FOL. Terms are expressions representing objects; formulas are those representing values. However, where there are only two values in FOL (true and false), values in FOP are bounded reals or integers.1 Just as FOL has the constant literals T and F , FOP has scalars, which are literals with predefined real values. Like FOL, FOP contains functions that map objects to objects, and predicates that map objects to numeric values. An atom is a predicate applied to a tuple of objects. A literal is a scalar multiple of an atom.\nA FOP formula is a string of literals combined using operators and quantifiers, as recursively defined in Fig. 1. Quantifiers for FOP are supremum and infimum, analoguous to\n1Boundedness is crucial to the completeness of our inference procedure; however, there is no limit on how large the bounds can be, so this requirement presents little practical concern.\n∃ and ∀ in FOL repectively. A variable with a matching quantifier is bound, while one without a matching quantifier is unbound or free. Binary addition, subtraction, min, and max are allowed between formulas. Scalar multiplication, i.e., multiplication of formulas by scalars, is allowed, but multiplication of two general formulas is not, in order to maintain linearity. Operators have the usual precedence order: scalar multiplication (including negation), binary operators {+,−},∧,∨, then quantifiers. As usual, a ground term is a term containing no unbound variables, and a sentence is a formula containing no unbound variables.\nFOP has two constructs analogous to FOL clauses, a sumclause (a sum of literals), and a max-clause (a maximum of literals). We use clause to refer to either one, when clear from context. A superclause is a maximum of sum-clauses; FOL has no corresponding construct."
    }, {
      "heading" : "3.2 Relationship to FOL and MILP",
      "text" : "We give a formal semantics for FOP below, in Sec. 3.3. For now, we informally say that a model in FOP is analogous to a model in FOL: a list of the objects in our world and a table of values for each function and predicate. Given a FOP sentence and model, we can evaluate the sentence by looking up its objects, functions, and predicates in the model, then combining the results using the standard definitions of operators like ∨ and +. A sentence’s value is the number to which it evaluates under the best (maximizing) model.\nWith this definition, the relationship between FOL and FOP is simple: we can translate any FOL sentence to FOP, preserving its value under all models, and therefore preserving its satisfiability (its maximal value under any model). Table 1 shows two possible translations. While Trans. A is more direct, Trans. B allows us to identify our lifted Gomory cuts (Sec. 5) as a direct generalization of FOL resolution (see Gordon et al. [2009]).\nAs an example, consider the following sentence in FOL:\n(bird(x)⇒ flies(x)) ∧ (eagle(y)⇒ bird(y)) ∧ (eagle(z)⇒ eagle(father(z))) ∧ eagle(Stanley)\nwhich can be written in FOP (using Trans. B and some further manipulations) as follows:\n(flies(x)− bird(x)) ∧ (bird(y)− eagle(y)) ∧ (eagle(father(z))− eagle(z)) ∧ (eagle(Stanley)− 1)\nIn the above sentences, we have used the convention that free variables are implicitly quantified, using ∀ (in FOL) or∧\n(in FOP)—e.g., the first conjunct in the FOP sentence can be read as ∧ x. (flies(x)−bird(x)). The FOP sentence is incomplete unless we specify the ranges of the predicates (flies, bird, and eagle); in this case, all take values in {0, 1}.\nIt is easy to see that every satisfying model for the FOL sentence corresponds to a model which makes the FOP sentence nonnegative. In FOL, we may ask whether, given our\nFigure 1 Definition of FOP Terms and Formulas\nOperators, Quantifiers:\n1. Negation −, scalar multiplication ∗ 2. Binary addition +, subtraction −, max ∨, min ∧ 3. Quantifiers: inf ∧ , sup ∨ Terms: A term represents an object.\n1. A variable representing objects 2. A function applied to 0 or more objects: fun(t1,...,tn)\nAtoms: An atom evaluates to a number. 1. A scalar, a real-valued constant 2. A predicate applied to 0 or more terms: pred(t1,...,tn).\nEach predicate is annotated with its range, a bounded interval of R or Z.\nFormulas (Statements): A formula is an expression that evaluates to a number, and is defined recursively:\n1. Any atom is a formula. 2. If f is a formula and c is a scalar, then the following\nare formulas: −f , f ∗ c, c ∗ f 3. If f , g are formulas, then so are the following: f + g, f − g, f ∨ g, f ∧ g. 4. If f is a formula and x is a variable, then the following are formulas: ∧ x. f , ∨ x. f\nassumptions, we can conclude flies(father(Stanley)), and use resolution to show that Stanley’s father flies. In FOP, we can use our lifted Gomory cut procedure to show that our assumptions imply flies(father(Stanley)) ≥ 1.\nTable 1 shows that it is never necessary to expand representation size when going from FOL to FOP. In fact, the reverse is true: we can sometimes achieve an exponentially smaller representation. For example, to describe the property “p(x) for at least k of a given n objects” in FOL, we need an exponentially-long formula: one such formula is the disjunction of ( n k ) clauses, each of which tests p(x) for one subset of k objects (e.g., p(x1) ∧ . . . ∧ p(xk)). In FOP, on the other hand, we can describe the same property with a formula of length linear in n: if p(x) ∈ {0, 1}, then p(x1) + p(x2) + . . . + p(xn) − k has value ≥ 0 exactly when p(x) = 1 for at least k of x1, . . . , xn.\nSimilarly, we can translate MILPs to FOP. To determine the feasibility of a MILP, we translate each of its constraints\ninto a sum-clause, and conjoin all of these clauses (using ∧). For example, consider the MILP schema\nmax x1 s.t. xi ≥ 2xS(i) ∀i ∈ I xi ∈ {0, 1, . . . , 8} ∀i ∈ I\n(3.1)\nHere I is an unspecified finite index set and S : I → I is an unspecified function. We instantiate the schema to get an ordinary MILP by fixing I and S; for example, if we set I = {1, 2, 3, 4} and S(i) = min(i + 1, 4), an optimal solution is (x1, x2, x3, x4) = (8, 4, 2, 0).\nWe can translate the constraints from (3.1) to the FOP sentence (x(i)−2∗x(S(i))), where x is a one-argument predicate with range {0, 1, . . . , 8}. We can also translate the objective [Gordon et al., 2009], or simply test whether there is a solution of value c by adding the clause (x(1)− c).\nJust like the MILP schema, our FOP sentence doesn’t specify the function S or the index set I . As we did above, we can specify S and I before inference. But, we have another choice: we can leave S and I unspecified or only partly specified, and ask our FOP inference algorithm to discover values for S and I . We can also ask our FOP inference algorithm to tell us facts about x, S, and I which must be true in any solution of (3.1); see Sec. 5.4 for an example.\nJust as for FOL, translating a MILP or MILP schema to FOP cannot increase representation size. On the other hand, it can be impossible to translate a FOP sentence to a MILP or MILP schema, since MILPs and MILP schemas are not Turing-complete, while FOP includes FOL.\nAs indicated by the above example, FOP variables and MILP variables do not correspond to one another. A MILP variable corresponds to a FOP ground atom. A FOP variable, on the other hand, can be thought of as an index into a family of related MILP variables, since a FOP expression like x(i) − 2 ∗ x(S(i)) expresses a whole family of constraints xi ≥ 2xS(i), one for each object i could refer to."
    }, {
      "heading" : "3.3 SEMANTICS",
      "text" : "A model M is a tuple (O,F, P ), where O is a list of objects, F is a table of function values, and P is a table of predicate values. A valuation I (under M ) is a mapping of syntactic variables to model objects. We write M for the set of possible models, and I(M) for the set of possible valuations underM . In the context of a model, each expression of FOP evaluates to a function mapping a valuation to a value (a model object or a number). (We need to use a function since in the inner stages of recursive evaluation we do not yet know how free variables will be quantified.) A ground expression then evaluates to a constant function, mapping every valuation to the same object or real number.\nIn particular, in the context of a model M , a sentence S evaluates to a constant function, mapping every valuation\nto the same real number. We will abuse notation slightly and write value(S,M) to mean this real number. Finally, we define value(S) to be supM∈M value(S,M).\nWe evaluate an expression under a model compositionally, based on its outermost syntactic operation:\n1. If the expression is a variable (say x), we return the function which looks that variable up in a valuation: λI. I(x). (Here and below, λI. [. . .] refers to a function taking a single argument I ∈ I(M), a valuation.)\n2. Scalar constants evaluate to constant functions: e.g., the formula 3 evaluates to λI. 3.\n3. If the outermost operation is an arithmetic operator, we compose by ordinary arithmetic: e.g, for A + B, if A evaluates to a function a : I(M) → O and B evaluates to a function b : I(M) → O, then A + B evaluates to λI. a(I) + b(I). Negation, scalar multiplication, and binary −, ∧, and ∨ are analogous.\n4. If the outermost operation is application of a function f , we first evaluate each argument expression, getting argument functions a1, . . . , ak, each in I(M) → O. We then return λI. F (f, a1(I), . . . , ak(I)): given a valuation I ∈ I(M), we find the value of each argument under I , then look up f and a1(I), . . . , ak(I) in our function table F . (Object constants are just zeroargument functions, and so evaluate by simple lookup in F , with no recursive evaluation of arguments.)\n5. Similarly, for application of a predicate p, we we first evaluate each argument expression, getting functions a1, . . . , ak, each in I(M) → O. We then return λI. P (p, a1(I), . . . , ak(I)), which looks up p and its arguments in the predicate table P . (Real or integer-valued variables are just zero-argument predicates, and therefore evaluate by simple lookup in P .)\n6. The quantifier expression ∧ x. Q works by taking an\ninfimum over model objects: write I[x → o] for the valuation which is the same as I except that it maps the variable x to the object o ∈ O. Then, if Q evaluates to q : I(M) → R, then ∧ x. Q evaluates to λI. info∈O q(I[x → o]). The quantifier expression∨ x. Q is similar, substituting sup for inf .\nVariable Substitution. We define a variable substitution V as a mapping from variables to terms, for example {x→ John, y → uncle(z)}. We write application of V to an expression S as S/V , with the usual meaning that each free variable in S appearing in the domain of V gets replaced by the corresponding term. (Bound variables are unaffected.) All variables appearing in the RHS of a substitution must be distinct from all variables on the LHS. Note that the process is purely syntactic: the result depends only on V and on whether a variable is syntactically bound in S.\nGiven an expression S, we can also define a variable substitution for S as a variable substitution whose range contains only the object constants and functions mentioned in\nS, along with the special constant nil if S contains no other object constants. With this definition, V will not introduce new functions or constants into S/V (except possibly nil). Finally, given two substitutions V and W , we write composition as V/W , so that S/(V/W ) = (S/V )/W ."
    }, {
      "heading" : "4 NORMAL FORMS",
      "text" : "To facilitate analysis, we translate sentences into one of two normal forms: min-normal form or reduced normal form. Min-normal form preserves the value of a sentence in all models, while reduced normal form preserves only the sign of the value. In both forms, we move negation and scalar multiplication inward, eliminate ∨ quantifiers, and rearrange binary operators to apply in the order ∧, ∨, + (outermost to innermost). In reduced normal form, we also eliminate ∨. Figs. 2 and 3 give translation procedures.\nOur translation to min-normal form is analogous to the well-known conjunctive normal form translation in FOL. We could also define a symmetric translation to a maxnormal form, isomorphic to the min-normal form of−S; as we will see, though, min-normal form lends itself to proofs of upper bounds on the value of a sentence, and therefore to proofs by contradiction.\nReduced normal form is the final form used by our inference procedure. It highlights the relationship of FOP to MILP: as mentioned above, conjoining sum-clauses with ∧ (as we do in reduced normal form) is analogous to intersecting linear constraints in a MILP. FOP as a whole assigns no special role to 0, but reduced normal form is designed for comparing a sentence’s value to 0. We could easily define an analogous normal form for comparing against any other threshold k, or simply translate S − k instead of S to reduced normal form.\nIn more detail, for min-normal form (Fig. 2), after step 1, each negation and scalar multiplication becomes part of a literal. Step 2 ensures that we do not quantify over the same variable more than once, in preparation for steps 3–4. Step 3 eliminates ∨ by using the fact that value(S) contains a supremum over models. Step 4 is purely syntactic, since quantifier ordering no longer matters after Steps 2– 3. Step 5 finally transforms the formula to normal form, using either distributive laws (such as (A ∧ B) ∨ C = (A∨C)∧(B∨C)) or the more-efficient Tseitin transformation. We show [Gordon et al., 2009] that the Tseitin transformation leads to at most a constant-factor growth in the length of our formula (compared to potentially-exponential growth for the simpler distributive procedure). Hence, the min-normal-form translation (with the Tseitin procedure) maintains the compactness of FOP.\n2We extend our semantics in the natural way: value(S), where S is in a normal form, fills in quantifiers using this convention.\n3It is enough to pick Bij ≥ range(Lij), but it is easier to pick Bij ≥ P P∈Pij range(P ) coeff(P, Lij) ≥ range(Lij). Here Pij\nFigure 2 Transformation to Min-Normal Form\nGiven a FOP sentence,\nGiven a sentence S = C1 ∧ . . . ∧ Cm in min-normal form, replace each superclause Ci = (Li1 ∨ . . . ∨ Lin) with the conjunction of sum-clauses L′i1 ∧ . . . ∧ L′in ∧Ri, where\n1. L′ij = Lij +Bij ∗ (1− zij(. . .)), where Bij is a sufficiently large3 scalar and zij(. . .) is a new 0-1 predicate whose arguments are the free variables in Ci 2. Ri = zi1(. . .) + ...+ zin(. . .)− 12 .\nIn reduced normal form (Fig. 3) we further replace each superclause with a conjunction of sum-clauses. To do so, we introduce new variables zij(x, y) to indicate which sumclause Lij gives the maximum value to each original superclause Ci, and enforce Lij iff its corresponding indicator is 1. The additional sum-clause Ri ensures that for every i and for every setting of the free variables in Ci there exists j s.t. zij(. . .) = 1, so that we always enforce at least one sum-clause out of each disjunction."
    }, {
      "heading" : "5 INFERENCE",
      "text" : "Given a sentence in reduced normal form, the most basic reasoning question is to determine whether it is feasible, that is, whether its value is at least zero. Using our feasibility test as a primitive, we can place bounds on the value of a general FOP sentence by binary search. Below, we also generalize the FOL concept of entailment, and use feasibility to test whether one FOP sentence is a logical conse-\nis the set of predicates mentioned in Lij , range(F ) is the difference between the largest and smallest possible values of F , and coeff(P, Lij) is the sum of absolute values of scalar multipliers in the literals based on P in Lij .\nquence of another. We will say that inference is the problem of generating entailed consequences of a given sentence.\nSince feasibility in FOP is analogous to satisfiability in FOL, we could try to design an inference procedure for FOP by reducing FOP to FOL. However, a good reduction is not immediately obvious; and in fact, we believe reducing to FOL would lose valuable structure in many sentences, making proofs longer and more difficult to find and interpret. Instead, we seek a procedure which works directly on the FOP representation.\nIn the following subsections, we first define entailment, soundness, and completeness. We then present a sound and complete, but non-lifted and therefore intractable, inference procedure. Finally, we give a lifted inference procedure, and show its soundness and completeness by comparing it to the first procedure."
    }, {
      "heading" : "5.1 ENTAILMENT",
      "text" : "In this subsection, we focus on the integer fragment of FOP; we discuss entailment for mixed-integer FOP in Sec. 6.\nIn FOL, sentence S entails sentence S′ (written S |= S′) iff S′ is true in every model which satisfies S. To extend this definition to FOP, we replace satisfiability by feasibility:\nDefinition 5.1 (FOP entailment). S |= S′ iff, for all models M , [value(S,M) ≥ 0]⇒ [value(S′,M) ≥ 0].\nUsing the translations from Table 1, together with an appropriate threshold (0 for Trans. A, 12 for Trans. B), it should be clear that FOP entailment generalizes FOL entailment.\nIn FOL, we can reduce entailment checking to satisfiability checking: S |= S′ iff S ∧ ¬S′ is satisfiable. (This proof strategy is called refutation.) Similarly, in integer FOP, we can reduce entailment to feasibility checking. To do so, we need the following lemmas, whose proofs can be found in Gordon et al. [2009].\nLemma 5.1. Given a sentence S of integer FOP, we can efficiently (in low-order polynomial time in the length of S) compute an (S) > 0 such that, for all models M , value(S,M) is an integer multiple of (S).\nLemma 5.2. Let S and S′ be sentences of integer FOP, and let = (S′). Then S |= S′ iff S ∧ (− 2 −S ′) is infeasible.\nSo, to search for a refutation proof of S |= S′ in FOP, we pass S ∧ (− (S\n′) 2 − S ′) to a feasibility checker.\nWe say that an inference procedure is sound if it generates only entailed sentences, and complete if it can generate all entailed sentences. Since refutation is based on feasibility checking, we say that a feasibility checker is sound if its reports of infeasibility are always correct, and complete if it can discover the infeasibility of all truly-infeasible sentences.\nAssuming that we have access to a practical, sound, and complete feasibility checker for FOP, lemma 5.1 shows that refutation inference is practical, and lemma 5.2 shows that refutation inference is sound and complete. So, we turn next to the problem of feasibility checking."
    }, {
      "heading" : "5.2 FEASIBILITY",
      "text" : "To try to prove value(S) < 0 for a mixed-integer sentence S in reduced normal form, we can use the following Naive Inference procedure. We give more detail on each step of this procedure below.\n1. Propositionalize (or Herbrandize) S. 2. For each finite subproblem of the Herbrandization:\n(a) Phrase the subproblem as a mixed-integer linear feasibility problem and pass it to a MILP solver.\n(b) If the MILP is infeasible, terminate and declare that S is infeasible; otherwise, continue.\nNaive Inference is not practical: we must generate and test all finite subproblems of the propositionalized S. So, in Sec. 5.3, we prove a “lifting lemma”, showing that we can avoid propositionalization, and use fully first-order reasoning instead. But first, we analyze Naive Inference.\nOur propositionalization procedure is analoguous to that in FOL: we build a Herbrand universe H containing all ground terms which could possibly be relevant to the value of S, and substitute them into S in all possible ways. For example, the sentence height(father(x)) becomes\nheight(father(nil)) ∧ height(father(father(nil))) ∧ . . .\nThe result will be a “propositional program” S(H), with no free variables, countably infinitely many distinct atoms, and countably infinitely many sum-clauses, such that value(S(H)) = value(S). A more detailed description of the procedure can be found in Gordon et al. [2009].\nFinding value(S(H)) is an optimization problem, supM value(S(H),M). It is sufficient to optimize over Herbrand models, i.e., models in which the set of objects is H , and in which the function tables implement the expected semantics (e.g., applying the function father to the object nil yields the object father(nil)). So, the only variable part of M is the assignment of (real or integer) values to syntactically-distinct atoms. Write D for the Cartesian product of the domains of all distinct atoms in S(H), and write M(d) for a model which implements an assignment d ∈ D. We then have value(S(H)) = supd∈D value(S(H),M(d)).\nGiven S(H), we formulate a subproblem by selecting a subset of sum-clauses: if S(H) = G1 ∧ G2 ∧ . . ., where theGi are ground clauses, and if J is a subset of the natural numbers, then subproblem SJ is the conjunction (using ∧) of all clauses Gj for j ∈ J . For any d ∈ D,\nvalue(S(H),M(d)) ≤ value(SJ ,M(d))\nas minimizing over fewer clauses only increases the value.\nWriteDJ for the subspace ofD corresponding to the atoms mentioned in SJ , and extend M(·) to apply to d ∈ DJ by assigning predicate values not mentioned in SJ arbitrarily. Since each Gj contains finitely many atoms, DJ has finitely many dimensions—that is, a subproblem has finitely many variables. Also note thatDJ can be described by finitely many linear constraints (upper and lower bounds on each dimension). Since\nvalue(SJ ,M(d)) ≥ 0 ⇔ ∀j ∈ J. value(Gj ,M(d)) ≥ 0\nand since each constraint value(Gj ,M(d)) ≥ 0 is linear in d, finding d ∈ DJ such that value(SJ ,M(d)) ≥ 0 is a MILP.\nNow that we have specified the Naive Inference procedure, we can show that it is sound and complete:\nLemma 5.3 (completeness). If value(S) < 0, Naive Inference will eventually find a subproblem SJ s.t. value(SJ) < 0 (and so declare that S is infeasible).\nProof. By the Tychonoff Product Theorem, D is a compact set under the product topology, since it is the product of bounded subsets of the reals or integers (each of which is compact). Write D[Gj<0] for the subset of D containing exactly those assignments d for which value(Gj ,M(d)) < 0. Crucially, D[Gj<0] is an open subset: value(Gj ,M(d)) is a linear function of d, and a strict linear inequality on finitely many variables defines an open set.\nIf indeed value(S) < 0, then the open sets D[Gj<0] for j = 1, 2, . . . form a cover of D: by the definition of value(S), for any assignment d ∈ D there exists a clause Gj s.t. value(Gj ,M(d)) < 0. Since D is compact, this open cover must have a finite subcover. Let J be the set of indices of any such subcover; then consider the subproblem value(SJ). Since D[Gj<0] for j ∈ J is also a cover of D, we have value(SJ) < 0, so we can use this subproblem to prove value(S) < 0 as claimed. Since Naive Inference iterates through all possible subproblems, it will eventually find SJ and declare that value(S) < 0.\nLemma 5.4 (Soundness). If value(S) ≥ 0, Naive Inference will never declare that value(S) < 0.\nProof. Naive Inference will only declare that value(S) < 0 if finds an infeasible subproblem. An infeasible subproblem SJ must satisfy value(SJ) < 0, which is not possible since value(SJ) ≥ value(S) ≥ 0.\nCombined with the proofs of correctness for our construction of the feasibility problem and our propositionalization process [Gordon et al., 2009], Lemmas 5.1–5.4 imply:\nTheorem 5.5. Naive Inference with entailment by refutation is a sound and complete inference method for the integer fragment of FOP.\nGomory cuts. Naive Inference uses a MILP solver to check the feasbility of each subproblem. For our purposes, a particularly convenient solver is based on Gomory cuts for integer linear programs [Gomory, 1958], along with their generalization to mixed-integer programs. Since we will also use a lifted version of Gomory cuts for our lifted inference procedure below, for clarity, we refer to the nonlifted cuts as propositional Gomory cuts.\nThe details of Gomory cuts are available in standard texts on optimization [Wolsey and Nemhauser, 1988]; here, we only require the following properties. First, Gomory cuts work on MILPs in equality form, that is, a set of m linear equality constraints on n > m nonnegative variables. Second, given a linear combination of the constraints, we get a unique Gomory cut, represented as a new linear inequality on the n variables.4,5 Third, this cut is valid, i.e., does not remove any (mixed) integer points from the feasible region. Finally, any valid inequality can be derived via a finite sequence of Gomory cuts, in which later cuts work from the expanded set of constraints containing both the original constraints and all previous cuts.\nThese properties together ensure that an inference procedure based on Gomory cuts terminates in finite time: generate all tableau-based Gomory cuts by breadth-first search, add them one at a time to our set of constraints, and check for mixed-integral basic feasible solutions after each one. Eventually, either we will find a feasible point which satisfies all integrality constraints, or we will generate a valid inequality which cuts away all (real as well as integer) points from our feasible region. In the latter case, the sequence of cuts constitutes a proof of infeasibility of the subproblem, and therefore of the original sentence S."
    }, {
      "heading" : "5.3 LIFTED INFERENCE",
      "text" : "While Naive Inference is sound and complete, it is impractical: it requires us to generate and test all finite subproblems of the Herbrandization of S until we find an infeasible one, and for each subproblem it requires us to generate and test all Gomory cuts until we prove feasibility or infeasibility. Instead, we would prefer an inference procedure more like the highly-successful resolution rule for first-order logic: since resolution is a lifted inference rule, a resolution proof in FOL can ignore irrelevant clauses, and never needs to blow up the size of its assumptions by Herbrandizing them.\nIn order to trust our lifted inference rule, we must demonstrate that it can form the basis of a sound and complete\n4To handle inequalities, we can turn them to equalities using slack variables. We can eliminate the slack variables once we find the cut: if our original constraint is Ax + b = s ≥ 0, with slack variables s, we can substitute Ax + b for s in the cut.\n5The linear combination is usually chosen as a row of a simplex tableau; this restriction can help focus search, but is not necessary.\ninference procedure (as resolution does for FOL); else, we might gain more-efficient inference, but lose the ability to prove some true statements. To do so, we will provide a lifting lemma: given a propositional proof (such as the one found by Naive Inference using propositional Gomory cuts), the lifting lemma will show that we can duplicate each step of the proof using our lifted inference rule. So, we can conclude that we do not lose the ability to prove any true statements when we move to lifted inference, and also that the lifted proofs are no longer than the original propositional proofs. Combined with the observations that the lifted inference rule allows us to consider large numbers of propositional proofs at once, and that lifting can significantly shorten proofs, this fact suggests that lifted inference will make it much more practical to search for proofs in FOP.\nWe start by describing our inference rule, which we call lifted Gomory cuts. This rule is a generalization of FOL’s resolution rule, in which we combine two clauses by eliminating a resolvent literal which appears positively in one and negatively in the other. (See Gordon et al. [2009] for a proof.) However, while resolution only combines pairs of clauses, our lifted Gomory cut rule can work from any number of clauses at once. Just as resolution provers unify expressions across clauses to find resolvents, we can use variable substitutions to simplify our clause set before cutting. And, just as a resolution proof ends by deriving the empty clause, a proof using lifted Gomory cuts ends when we can use simple linear algebra to prove that our sentence’s feasible region is empty.\nEach lifted Gomory cut tries to prove value(S) < 0 using a subset of the sum-clauses of S. Just as we did for Naive Inference, we build a MILP representing the feasibility of our chosen subset of sum-clauses, and perform a Gomory cut in this MILP. However, in contrast to Naive Inference, lifted cuts can work from non-ground clauses; in this case, the resulting cut holds for many subproblems simultaneously (corresponding to many ways to substitute ground terms into our clauses). We represent the cuts for all of these subproblems as a new, lifted sum-clause which has value < 0 in model M only if value(S,M) < 0. Because of this fact, conjoining this new clause to S does not change whether value(S) < 0.\nTo build a lifted Gomory cut, we start from a sentence S of FOP in reduced normal form. We then perform the following steps:\n1. Select sum-clauses L1, L2, . . . Lp from S, along with a variable substitution U . We may pick the same clause twice, in which case we standardize the copies apart. As a special case, to reason about predicate bounds, we allow some Li to be implicit clauses of the form p(x, y, . . .)− l or u− p(x, y, . . .), where [l, u] is the declared range for p, and x, y, . . . are standardized apart. Let L′i = Li/U for each i.\n2. Introduce a new, temporary MILP variable (zeroargument predicate) for each textually distinct atom in the clauses L′i, and substitute the variables into the L′i to get propositional sum-clauses L ′′ i . Write x for\nthe vector of temporary variables. 3. Convert L′′1 ∧ L′′2 ∧ . . . to a mixed integer feasibility\nproblem P as described for subproblems in Naive Inference. This step introduces slack variables si ≥ 0. 4. Pick a linear combination of constraints, and use the (propositional) Gomory method to derive a new, valid inequality C ≥ 0 for P , where C contains x and si. 5. Eliminate the slack variables, as described previously for Naive Inference, leaving an inequality C ′ ≥ 0 written in terms of x only. For convenience, we allow weakening the cut by increasing the coefficient of each si before eliminating it. (Weakening is never necessary, but can help interpretation.) 6. Replace each temporary variable in C ′ with its corresponding atom, yielding a lifted inequality L ≥ 0.\n7. Ensure no object variable in L conflicts with those in S by standardizing apart L. Call the result L′. Set S′ = S ∧ L′, and use S′ to continue generating lifted Gomory cuts.\nNote that, while there are well-studied heuristics for FOL which choose a subset of clauses and a variable substitution intelligently, so that unification and resolution are likely to result in refutation quickly, the corresponding search control problem for FOP is a subject of future research.\nDetailed proof sketches of the following two lemmas can be found in Gordon et al. [2009].\nLemma 5.6 (Soundness). Lifted Gomory cuts, with proof by refutation, are a sound inference procedure for FOP.\nLemma 5.7 (Lifting). Any propositional proof produced by Naive Inference using propositional Gomory cuts can be duplicated using lifted Gomory cuts.\nCombining the two lemmas, we obtain our desired result:\nTheorem 5.8. Lifted Gomory cuts, together with proof by refutation, form a sound and complete inference procedure for the integer fragment of FOP."
    }, {
      "heading" : "5.4 AN EXAMPLE PROOF",
      "text" : "For a simple demonstration of a proof by lifted Gomory cuts, consider the FOP sentence F = x(i)−2x(S(i)) from Sec. 3.2. An interesting fact about F is that, in every satisfying model, x(i) must “eventually” be zero: every time we move from i to S(i), the corresponding xmust decrease by at least a factor of two, reaching 0 in at most log(8) = 3 steps. In particular, for any i, x(S(S(S(i)))) must be zero.\nMore formally, we can show F |= −x(S(S(S(i)))) using a single lifted Gomory cut: pick four copies of F and one copy of the implicit clause 8−x(i). Standardize apart (say,\nusing variables j, k, l,m, n), and apply the substitution {j → i, k → S(i), l → S(S(i)),m → S(S(S(i))), n → i}. Use the linear combination (1, 2, 4, 8, 1)/16 to get the constraint 12 − x(S(S(S(i)))) ≥ 0, which leads to the cut −x(S(S(S(i)))) ≥ 0."
    }, {
      "heading" : "6 EXTENSIONS",
      "text" : "Equality. As defined so far, FOP cannot reason about object equality; that is, it cannot test whether two objects which appear syntactically different are in fact the same. Instead, object equality shows up only because all function and predicate expressions involving the two distinct names will evaluate to the same value. To represent equality, we could add a distinguished equality predicate equals(x, y) to FOP, along with appropriate new semantics. Such a treatment would be in many ways analogous to standard extensions of FOL to include equality; for example, it would require a new inference rule, analogous to paramodulation in FOL with equality.\nSums. Another useful extension to FOP would be a sum quantifier: if I is our context valuation, we define ∑ x. P as the sum, over all distinct objects y from our modelM , of the value of P under I[x→ y]. This extension makes most sense in conjunction with object equality, since an equality predicate gives us more control over which terms appear in our sum.\nSince the sum quantifier can represent a sum over infinitely many terms, we need additional restrictions to ensure convergence of the sum to a unique value. The best way to implement such restrictions is not immediately obvious, but is an interesting direction for future research.\nInference in mixed-integer FOP. Lifted Gomory cuts and refutation are sound and complete for integer FOP; unfortunately, the situation is more complicated for general mixedinteger sentences. Our lifted feasibility test (Sec. 5.3) works equally well for integer or mixed-integer sentences, and Lemmas 5.3 and 5.4 show that it is sound and complete in either case. However, Lemma 5.1 does not hold for mixed-integer sentences (see Gordon et al. [2009] for a counterexample), so we can no longer reduce entailment to feasibility checking by using a margin to convert from a < bound to a ≤ bound.\nTo get around this problem, we can strengthen or weaken the definition of entailment:\nDefinition 6.1 ( -entailment). Sentence S -entails S′, S |= S′, iff [value(S,M) ≥ 0]⇒ [value(S′,M) > ].\nFor ≥ 0, S |= S′ is stronger than S |= S′; for < 0, it is weaker. With this altered definition, S ∧ (− − S′) is feasible iff S |= S′, and so we can test -entailment using our lifted feasibility procedure from Sec. 5.3. If we wish to know whether S |= S′, a reasonable procedure might be to test S |= S′ for one or more values of ; however, this\nprocedure may give up either soundness (if we conclude entailment based on any efficiently computable < 0) or completeness (if we require ≥ 0). On the other hand, in some applications, “approximate entailment” might be a useful conclusion in its own right.\nDirect inference of values. Our feasibility checking procedure compares the value of a sentence to a threshold. While we can use this test together with binary search to discover tight bounds on a sentence’s value, we could also ask for a procedure which discovers a sentence’s value directly.\nWe can build such a procedure easily: given a sentence S, let z be a new zero-argument predicate which doesn’t appear in S. Then, the feasible region of S′ = S − z corresponds to the epigraph of S, that is, the set of pairs (M, z) where M is a model of S and z ≤ value(S,M).\nIf we now run our lifted Gomory cut procedure on S′, each new cut will place a new bound on the feasible region of S′ and therefore on the epigraph of S. Since we are interested in the maximal feasible value of z, we can focus our search to try to pin down this maximal value quickly: first note that z will appear in every clause of the reduced normal form of S′, and so will always appear in the MILP which we construct in Step 3 of the cut procedure. So, in Step 4 (where we pick a linear combination of constraints, thereby specifying a cut which removes a corner of our subproblem’s relaxed feasible region), we can make sure to cut off a corner which has maximal z: we can find such a corner by optimizing z over our relaxed feasible region, and then make a cut by choosing a row of the simplex tableau corresponding to this corner. A further benefit of this procedure is that the optimal z values for each of our subproblems will be upper bounds on value(S).\nIn applications where we care about the exact value of a sentence S, rather than merely its sign, we may be interested in the following extension of entailment:\nDefinition 6.2 (Strong entailment). Sentence S strongly entails sentence S′, S |=∗ S′, iff, for all models M , value(S′,M) ≥ value(S,M).\n(We could also define S |=∗ S′ to parallel Defn. 6.1.) Strong entailment implies ordinary FOP entailment, but not vice versa. If S |=∗ S′, then the conjunction S ∧S′ has the same value as S in any model. By contrast, if we only have S |= S′, the conjunction S ∧S′ only preserves whether the value is ≥ 0 in each model.\nConcrete FOP. One special case of FOP with equality is particularly useful in practice: if we assume unique names (all syntactically-distinct object constants refer to distinct objects), known functions (all function values are prespecified), and domain closure (there are no objects other than the ones mentioned in the sentence), we get a restricted version of FOP in which inference is much easier. We will call this version concrete FOP.\nIn particular, in concrete FOP, we can take the Herbrand universe for any sentence S to include all and only the named object constants, and so the concrete Herbrandization of S will be finite. There is then no need to consider finite subsets of the Herbrandized sentence: instead, we can convert the entire Herbrandized sentence to a MILP (as described in Sec. 5.2) and solve it directly. Since the MILP is finite, we can efficiently compute a threshold which allows us to reduce entailment to feasibility. And, no restrictions on the use of the ∑ quantifier are required, since we need not worry about convergence of sums.\nIn concrete FOP, quantifiers can be seen as macros: any expression containing quantifiers is equivalent to some finite, quantifier-free expression, and quantified statements merely prescribe a way to generate repetitive pieces of a ground formula. The representative power of concrete FOP is therefore no greater than that of ordinary MILPs, although equivalent problem statements may still be far more compact in concrete FOP than they are as MILPs.\nEven in concrete FOP, lifted inference can still be useful, since it allows us to perform many propositional inference steps simultaneously. Many papers about “lifted inference” consider only this sort of lifting, in which we operate on a lifted representation which is more compact than the corresponding propositional representation, but still only equivalently powerful. The true power of lifted inference, however, only becomes apparent when our lifted representation is capable of representing concepts which are impossible to specify using only propositional syntax (as is true both for FOL and (non-concrete) FOP)."
    }, {
      "heading" : "7 CONCLUSION AND FUTURE WORK",
      "text" : "We have defined first-order programming, and established formal methods for reasoning about first-order programs, including a sound and complete inference procedure for integer FOP. Future work includes extending FOP to include equality and a summation quantifier. We also plan to experiment with lifted Gomory cuts, demonstrating their ability to make proofs more compact than corresponding proofs in FOL or MILP. Finally, we plan to implement our lifted proof-search procedure, and test it on real-world examples such as first-order stochastic programs with recourse. Finding proofs quickly in real examples will require us to generalize standard FOL search-control heuristics to FOP."
    }, {
      "heading" : "Acknowledgements",
      "text" : "The authors gratefully acknowledge support from DARPA’s Computer Science Study Panel program (grant HR0011-07-10026, all authors), a Lucent-Alcatel Fellowship (author SAH), and ARO grant W911NF-08-1-0301 (authors GJG and MD)."
    } ],
    "references" : [ {
      "title" : "Cooperative task assignment of unmanned aerial vehicles in adversarial environments",
      "author" : [ "M. Alighanbari", "J.P. How" ],
      "venue" : "In Proc. IEEE American Control Conference (ACC),",
      "citeRegEx" : "Alighanbari and How.,? \\Q2005\\E",
      "shortCiteRegEx" : "Alighanbari and How.",
      "year" : 2005
    }, {
      "title" : "Mathematical programming embeddings of logic",
      "author" : [ "V. Borkar", "V. Chandru", "S. Mitter" ],
      "venue" : "Journal of Automatic Reasoning,",
      "citeRegEx" : "Borkar et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Borkar et al\\.",
      "year" : 2002
    }, {
      "title" : "Lifted first-order probabilistic inference",
      "author" : [ "R. Braz", "E. Amir", "D. Roth" ],
      "venue" : "In IJCAI-05,",
      "citeRegEx" : "Braz et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Braz et al\\.",
      "year" : 2005
    }, {
      "title" : "Optimization methods for logical inference",
      "author" : [ "Vijay Chandru", "John Hooker" ],
      "venue" : null,
      "citeRegEx" : "Chandru and Hooker.,? \\Q1999\\E",
      "shortCiteRegEx" : "Chandru and Hooker.",
      "year" : 1999
    }, {
      "title" : "Formulation of linear problems and solution by a universal machine",
      "author" : [ "B.C. Eaves", "U.G. Rothblum" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "Eaves and Rothblum.,? \\Q1994\\E",
      "shortCiteRegEx" : "Eaves and Rothblum.",
      "year" : 1994
    }, {
      "title" : "AMPL: A Modeling Language for Mathematical Programming",
      "author" : [ "R. Fourer", "D.M. Gay", "B.W. Kernighan" ],
      "venue" : "http://ampl.com",
      "citeRegEx" : "Fourer et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Fourer et al\\.",
      "year" : 2002
    }, {
      "title" : "Outline of an algorithm for integer solutions to linear programs",
      "author" : [ "R.E. Gomory" ],
      "venue" : "Bull. Amer. Math. Soc.,",
      "citeRegEx" : "Gomory.,? \\Q1958\\E",
      "shortCiteRegEx" : "Gomory.",
      "year" : 1958
    }, {
      "title" : "Dudı́k. First-order mixed integer linear programming",
      "author" : [ "G. Gordon", "S.A. Hong" ],
      "venue" : "Technical Report CMU-ML-09-108,",
      "citeRegEx" : "Gordon et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Gordon et al\\.",
      "year" : 2009
    }, {
      "title" : "Probabilistic entityrelationship models, PRMs, and plate models",
      "author" : [ "D. Heckerman", "C. Meek", "D. Koller" ],
      "venue" : "Introduction to statistical relational learning,",
      "citeRegEx" : "Heckerman et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Heckerman et al\\.",
      "year" : 2007
    }, {
      "title" : "Mixed logical/linear programming",
      "author" : [ "J.N. Hooker", "M.A. Osrio" ],
      "venue" : "Discrete Applied Mathematics,",
      "citeRegEx" : "Hooker and Osrio.,? \\Q1999\\E",
      "shortCiteRegEx" : "Hooker and Osrio.",
      "year" : 1999
    }, {
      "title" : "BLOG: Probabilistic models with unknown objects",
      "author" : [ "B. Milch", "B. Marthi", "D. Sontag", "S. Russell", "D.L. Ong" ],
      "venue" : "In IJCAI-05,",
      "citeRegEx" : "Milch et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Milch et al\\.",
      "year" : 2005
    }, {
      "title" : "IBAL: A probabilistic rational programming language",
      "author" : [ "A. Pfeffer" ],
      "venue" : "In IJCAI-01,",
      "citeRegEx" : "Pfeffer.,? \\Q2001\\E",
      "shortCiteRegEx" : "Pfeffer.",
      "year" : 2001
    }, {
      "title" : "The independent choice logic and beyond",
      "author" : [ "D. Poole" ],
      "venue" : "Probabilistic Inductive Logic Programming: Theory and Application,",
      "citeRegEx" : "Poole.,? \\Q2008\\E",
      "shortCiteRegEx" : "Poole.",
      "year" : 2008
    }, {
      "title" : "A stochastic formulation of the dynamic assignment problem, with an application to truckload motor carriers",
      "author" : [ "W.B. Powell" ],
      "venue" : "Transportation Sci.,",
      "citeRegEx" : "Powell.,? \\Q1996\\E",
      "shortCiteRegEx" : "Powell.",
      "year" : 1996
    }, {
      "title" : "Integer linear programming inference for conditional random fields",
      "author" : [ "D. Roth", "W. Yih" ],
      "venue" : "In ICML-05,",
      "citeRegEx" : "Roth and Yih.,? \\Q2005\\E",
      "shortCiteRegEx" : "Roth and Yih.",
      "year" : 2005
    }, {
      "title" : "Artificial Intelligence: A modern Approach",
      "author" : [ "S. Russell", "P. Norvig" ],
      "venue" : "Pearson Education, Inc.,",
      "citeRegEx" : "Russell and Norvig.,? \\Q2003\\E",
      "shortCiteRegEx" : "Russell and Norvig.",
      "year" : 2003
    }, {
      "title" : "Mixed-integer programming methods for finding Nash equilibria",
      "author" : [ "T. Sandholm", "A. Gilpin", "V. Conitzer" ],
      "venue" : "In AAAI-05,",
      "citeRegEx" : "Sandholm et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Sandholm et al\\.",
      "year" : 2005
    }, {
      "title" : "Markov logic in infinite domains",
      "author" : [ "P. Singla", "P. Domingos" ],
      "venue" : "In UAI-07,",
      "citeRegEx" : "Singla and Domingos.,? \\Q2007\\E",
      "shortCiteRegEx" : "Singla and Domingos.",
      "year" : 2007
    }, {
      "title" : "Lifted first-order belief propagation",
      "author" : [ "P. Singla", "P. Domingos" ],
      "venue" : "In AAAI-08,",
      "citeRegEx" : "Singla and Domingos.,? \\Q2008\\E",
      "shortCiteRegEx" : "Singla and Domingos.",
      "year" : 2008
    }, {
      "title" : "On the use of integer programming models in AI planning",
      "author" : [ "T. Vossen", "M. Ball", "R.H. Smith" ],
      "venue" : "In IJCAI-99,",
      "citeRegEx" : "Vossen et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Vossen et al\\.",
      "year" : 1999
    }, {
      "title" : "Integer and Combinatorial Optimization",
      "author" : [ "L. Wolsey", "G.L. Nemhauser" ],
      "venue" : null,
      "citeRegEx" : "Wolsey and Nemhauser.,? \\Q1988\\E",
      "shortCiteRegEx" : "Wolsey and Nemhauser.",
      "year" : 1988
    } ],
    "referenceMentions" : [ {
      "referenceID" : 19,
      "context" : "1996]); or, in AI, we can use MILPs for planning [Vossen et al., 1999] or to reason about uncertainty due to the actions of other agents in a nonzero-sum game [Sandholm et al.",
      "startOffset" : 49,
      "endOffset" : 70
    }, {
      "referenceID" : 16,
      "context" : ", 1999] or to reason about uncertainty due to the actions of other agents in a nonzero-sum game [Sandholm et al., 2005]; or, in ML, we can use MILPs for MAP inference in graphical models (e.",
      "startOffset" : 96,
      "endOffset" : 119
    }, {
      "referenceID" : 14,
      "context" : ", [Roth and Yih, 2005]); or, in cooperative control, we can use MILPs for task allocation under uncertainty [Alighanbari and How, 2005].",
      "startOffset" : 2,
      "endOffset" : 22
    }, {
      "referenceID" : 0,
      "context" : ", [Roth and Yih, 2005]); or, in cooperative control, we can use MILPs for task allocation under uncertainty [Alighanbari and How, 2005].",
      "startOffset" : 108,
      "endOffset" : 135
    }, {
      "referenceID" : 5,
      "context" : "One might try to add this capability to MILPs using a “wrapper” or “compiler” such as AMPL [Fourer et al., 2002]: for example, to express a constraint that holds for all objects in a class, we can generate one copy of this constraint for every known object.",
      "startOffset" : 91,
      "endOffset" : 112
    }, {
      "referenceID" : 7,
      "context" : "For example, Hooker and Osrio [1999] give an overview of the area and a general framework called mixed logical/linear programming.",
      "startOffset" : 13,
      "endOffset" : 37
    }, {
      "referenceID" : 8,
      "context" : "Relational probabilistic languages (RPLs), such as plate models, probabilistic entity-relationship models [Heckerman et al., 2007], Markov logic networks [Richardson and Domingos, 2006], IBAL [Pfeffer, 2001], ICL [Poole, 2008], and BLOG [Milch et al.",
      "startOffset" : 106,
      "endOffset" : 130
    }, {
      "referenceID" : 11,
      "context" : ", 2007], Markov logic networks [Richardson and Domingos, 2006], IBAL [Pfeffer, 2001], ICL [Poole, 2008], and BLOG [Milch et al.",
      "startOffset" : 69,
      "endOffset" : 84
    }, {
      "referenceID" : 12,
      "context" : ", 2007], Markov logic networks [Richardson and Domingos, 2006], IBAL [Pfeffer, 2001], ICL [Poole, 2008], and BLOG [Milch et al.",
      "startOffset" : 90,
      "endOffset" : 103
    }, {
      "referenceID" : 10,
      "context" : ", 2007], Markov logic networks [Richardson and Domingos, 2006], IBAL [Pfeffer, 2001], ICL [Poole, 2008], and BLOG [Milch et al., 2005], combine probabilistic and logical representations, and allow us to reason about uncertain statements about classes of objects.",
      "startOffset" : 114,
      "endOffset" : 134
    }, {
      "referenceID" : 10,
      "context" : "(Exceptions include BLOG [Milch et al., 2005], which allows unknown numbers of objects; ICL [Poole, 2008], which allows unknown or infinite numbers of objects; and the work of Singla and Domingos [2007], which extends Markov logic to infinite domains, but makes strong assumptions in order to guarantee well-defined probability distributions.",
      "startOffset" : 25,
      "endOffset" : 45
    }, {
      "referenceID" : 12,
      "context" : ", 2005], which allows unknown numbers of objects; ICL [Poole, 2008], which allows unknown or infinite numbers of objects; and the work of Singla and Domingos [2007], which extends Markov logic to infinite domains, but makes strong assumptions in order to guarantee well-defined probability distributions.",
      "startOffset" : 54,
      "endOffset" : 67
    }, {
      "referenceID" : 18,
      "context" : "[2005] on lifted variable elimination requires a known set of objects, and lifted belief propagation [Singla and Domingos, 2008] was described in terms of finite MLNs, leaving infinite domains explicitly for future work.",
      "startOffset" : 101,
      "endOffset" : 128
    }, {
      "referenceID" : 9,
      "context" : "(Exceptions include BLOG [Milch et al., 2005], which allows unknown numbers of objects; ICL [Poole, 2008], which allows unknown or infinite numbers of objects; and the work of Singla and Domingos [2007], which extends Markov logic to infinite domains, but makes strong assumptions in order to guarantee well-defined probability distributions.",
      "startOffset" : 26,
      "endOffset" : 203
    }, {
      "referenceID" : 2,
      "context" : ", the work of Braz et al. [2005] on lifted variable elimination requires a known set of objects, and lifted belief propagation [Singla and Domingos, 2008] was described in terms of finite MLNs, leaving infinite domains explicitly for future work.",
      "startOffset" : 14,
      "endOffset" : 33
    }, {
      "referenceID" : 2,
      "context" : ", the work of Braz et al. [2005] on lifted variable elimination requires a known set of objects, and lifted belief propagation [Singla and Domingos, 2008] was described in terms of finite MLNs, leaving infinite domains explicitly for future work. In contrast, full lifted inference for firstorder logic is well understood (see, e.g., Russell and Norvig [2003]), and our inference procedure for FOP parallels the well-known resolution procedure for FOL.",
      "startOffset" : 14,
      "endOffset" : 360
    }, {
      "referenceID" : 6,
      "context" : "B allows us to identify our lifted Gomory cuts (Sec. 5) as a direct generalization of FOL resolution (see Gordon et al. [2009]).",
      "startOffset" : 35,
      "endOffset" : 127
    }, {
      "referenceID" : 7,
      "context" : "We can also translate the objective [Gordon et al., 2009], or simply test whether there is a solution of value c by adding the clause (x(1)− c).",
      "startOffset" : 36,
      "endOffset" : 57
    }, {
      "referenceID" : 7,
      "context" : "We show [Gordon et al., 2009] that the Tseitin transformation leads to at most a constant-factor growth in the length of our formula (compared to potentially-exponential growth for the simpler distributive procedure).",
      "startOffset" : 8,
      "endOffset" : 29
    }, {
      "referenceID" : 7,
      "context" : "Ensure that binary operators are applied only in the order ∧, ∨, +, using either distributive laws or a procedure analogous to the Tseitin transformation for propositional logic [Gordon et al., 2009].",
      "startOffset" : 178,
      "endOffset" : 199
    }, {
      "referenceID" : 7,
      "context" : "To do so, we need the following lemmas, whose proofs can be found in Gordon et al. [2009].",
      "startOffset" : 69,
      "endOffset" : 90
    }, {
      "referenceID" : 7,
      "context" : "A more detailed description of the procedure can be found in Gordon et al. [2009].",
      "startOffset" : 61,
      "endOffset" : 82
    }, {
      "referenceID" : 7,
      "context" : "Combined with the proofs of correctness for our construction of the feasibility problem and our propositionalization process [Gordon et al., 2009], Lemmas 5.",
      "startOffset" : 125,
      "endOffset" : 146
    }, {
      "referenceID" : 6,
      "context" : "For our purposes, a particularly convenient solver is based on Gomory cuts for integer linear programs [Gomory, 1958], along with their generalization to mixed-integer programs.",
      "startOffset" : 103,
      "endOffset" : 117
    }, {
      "referenceID" : 20,
      "context" : "The details of Gomory cuts are available in standard texts on optimization [Wolsey and Nemhauser, 1988]; here, we only require the following properties.",
      "startOffset" : 75,
      "endOffset" : 103
    }, {
      "referenceID" : 6,
      "context" : "We start by describing our inference rule, which we call lifted Gomory cuts. This rule is a generalization of FOL’s resolution rule, in which we combine two clauses by eliminating a resolvent literal which appears positively in one and negatively in the other. (See Gordon et al. [2009] for a proof.",
      "startOffset" : 64,
      "endOffset" : 287
    }, {
      "referenceID" : 7,
      "context" : "Detailed proof sketches of the following two lemmas can be found in Gordon et al. [2009].",
      "startOffset" : 68,
      "endOffset" : 89
    }, {
      "referenceID" : 7,
      "context" : "1 does not hold for mixed-integer sentences (see Gordon et al. [2009] for a counterexample), so we can no longer reduce entailment to feasibility checking by using a margin to convert from a < bound to a ≤ bound.",
      "startOffset" : 49,
      "endOffset" : 70
    } ],
    "year" : 2009,
    "abstractText" : "Mixed integer linear programming (MILP) is a powerful representation often used to formulate decision-making problems under uncertainty. However, it lacks a natural mechanism to reason about objects, classes of objects, and relations. First-order logic (FOL), on the other hand, excels at reasoning about classes of objects, but lacks a rich representation of uncertainty. While representing propositional logic in MILP has been extensively explored, no theory exists yet for fully combining FOL with MILP. We propose a new representation, called first-order programming or FOP, which subsumes both FOL and MILP. We establish formal methods for reasoning about first order programs, including a sound and complete lifted inference procedure for integer first order programs. Since FOP can offer exponential savings in representation and proof size compared to FOL, and since representations and proofs are never significantly longer in FOP than in FOL, we anticipate that inference in FOP will be more tractable than inference in FOL for corresponding problems.",
    "creator" : "TeX"
  }
}