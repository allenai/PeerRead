{"conference": "ICLR 2017 conference submission", "title": "Learning Features of Music From Scratch", "abstract": "This paper introduces a new large-scale music dataset, MusicNet, to serve as a source  of supervision and evaluation of machine learning methods for music research.  MusicNet consists of hundreds of freely-licensed classical music recordings  by 10 composers, written for 11 instruments, together with instrument/note  annotations resulting in over 1 million temporal labels on 34 hours of chamber music performances under various studio and microphone conditions.   The paper defines a multi-label classification task to predict notes in musical recordings,  along with an evaluation protocol, and benchmarks several machine learning architectures for this task:  i) learning from spectrogram features;  ii) end-to-end learning with a neural net;  iii) end-to-end learning with a convolutional neural net.  These experiments show that end-to-end models trained for note prediction learn frequency selective filters as a low-level representation of audio.", "reviews": [{"is_meta_review": true, "comments": "The paper introduces a new dataset called MusicNet (presumably analogous to ImageNet), featuring dense ground truth labels for 30+ hours of classical music, which is provided as raw audio. Such a dataset is extremely valuable for music information retrieval (MIR) research and a dataset of this size has never before been publicly available. It has the potential to dramatically increase the impact of modern machine learning techniques (e.g. deep learning) in this field, whose adoption has previously been hampered by a lack of available datasets that are large enough. The paper is clear and well-written.\n\nThe paper also features some \"example\" experiments using the dataset, which I am somewhat less excited about. The authors decided to focus on one single task that is not particularly challenging: identifying pitches in isolated segments of audio. Pitch information is a fairly low-level characteristic of music. Considering that isolated fragments are used as input, this is a relatively simple problem that probably doesn't even require machine learning to solve adequately, e.g. peak picking on a spectral representation could already get you pretty far. It's not clear what value the machine learning component in the proposed approach actually adds, if any. I could be wrong about this as I haven't done the comparison myself, but I think the burden is on the authors to demonstrate that using ML here is actually useful.\n\nI would argue that one of the strenghts of the dataset is the variety of label information it provides, so a much more convincing setup would have been to demonstrate many different prediction tasks for both low-level (e.g. pitch, onsets) and high-level (e.g. composer) characteristics, perhaps with fewer and simpler models -- maybe even sticking to spectrogram input and forgoing raw audio input for the time being, as this comparison seems orthogonal to the introduction of the dataset itself. As it stands, I feel that the fact that the experiments are relatively uninteresting detracts from the main point of the paper, which is to introduce a new public dataset that is truly unique in terms of its scale and scope.\n\nThat said, the experiments seem to have been conducted in a rigorous fashion and the evaluation and analysis of the resulting models is properly executed.\n\nRe: Section 4.5, it is rather unsurprising to me that a pitch detector would learn filters that resemble pitches (i.e. sinusoids), although the observation that this requires a relatively large amount of data is interesting. However, it would be more interesting to demonstrate that this is also the case for higher-level tasks. The authors favourably compare the features learnt by their model with prior work on end-to-end learning from raw audio, but neglect that the tasks considered in this work were much more high-level.\n\nSome might also question whether ICLR is the appropriate venue to introduce a new dataset, but personally I think it's a great idea to submit it here, seeing as it will reach the right people. I suppose this is up to the organisers and the program committee, but I thought it important to mention this, because I don't think this paper merits acceptance based on its experimental results alone.", "IS_META_REVIEW": true}, {"TITLE": "ICLR committee final decision", "OTHER_KEYS": "ICLR 2017 pcs", "comments": "There was some question as to weather ICLR is the right venue for this sort of dataset paper, I tend to think it would be a good addition to ICLR as people from the ICLR community are likely to be among the most interested. The problem of note identification in music is indeed challenging and the authors revised the manuscript to provide more background on the problem. No major issues with the writing or clarity. Experiments and models explored are not extremely innovative, but help create a solid dataset introduction paper.", "IS_META_REVIEW": false, "DATE": "06 Feb 2017", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Final Review: valuable new dataset", "is_meta_review": false, "comments": "This paper introduces MusicNet, a new dataset. Application of ML techniques to music have been limited due to scarcity of exactly the kind of data that is provided here: meticulously annotated, carefully verified and organized, containing enough \"hours\" of music, and where genre has been well constrained in order to allow for sufficient homogeneity in the data to help ensure usefulness. This is great for the community.\n\nThe description of the validation of the dataset is interesting, and indicates a careful process was followed.\n\nThe authors provide just enough basic experiments to show that this dataset is big enough that good low-level features (i.e. expected sinusoidal variations) can indeed be learned in an end-to-end context.\n\nOne might argue that in terms of learning representations, the work presented here contributes more in the dataset than in the experiments or techniques used. However, given the challenges of acquiring good datasets, and given the essential role such datasets play for the community in moving research forward and providing baseline reference points, I feel that this contribution carries substantial weight in terms of expected future rewards. (If research groups were making great new datasets available on a regular basis, that would place this in a different context. But so far, that is not the case.) In otherwords, while the experiments/techniques are not necessarily in the top 50% of accepted papers (per the review criteria), I am guessing that the dataset is in the top 15% or better.", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "20 Dec 2016 (modified: 25 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Interesting corpus, what next?", "is_meta_review": false, "comments": "This paper describes the creation of a corpus of freely-licensed classical music recordings along with corresponding MIDI-scores aligned to the audio.  It also describes experiments in polyphonic transcription using various deep learning approaches, which show promising results.\n\nThe paper is a little disorganised and somewhat contradictory in parts. For example, I find the first sentence in section 2 (MusicNet) would better be pushed one paragraph below so that the section be allowed to begin with a survey of the tools available to researchers in music. Also, the description for Table 3 should probably appear somewhere in the Methods section. Last example: the abstract/intro says the purpose is note prediction; later (4th paragraph of intro) there's a claim that the focus is \"learning low-level features of music....\" I find this slightly disorienting.\n\nAlthough others (Uehara et al., 2016, for example) have discussed collection platforms and corpora, this work is interesting because of its size and the approach for generating features. I'm interested in what the authors will to do expand the offerings in the corpus, both in terms of volume and diversity.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "awesome new dataset, less interesting experiments", "is_meta_review": false, "comments": "The paper introduces a new dataset called MusicNet (presumably analogous to ImageNet), featuring dense ground truth labels for 30+ hours of classical music, which is provided as raw audio. Such a dataset is extremely valuable for music information retrieval (MIR) research and a dataset of this size has never before been publicly available. It has the potential to dramatically increase the impact of modern machine learning techniques (e.g. deep learning) in this field, whose adoption has previously been hampered by a lack of available datasets that are large enough. The paper is clear and well-written.\n\nThe paper also features some \"example\" experiments using the dataset, which I am somewhat less excited about. The authors decided to focus on one single task that is not particularly challenging: identifying pitches in isolated segments of audio. Pitch information is a fairly low-level characteristic of music. Considering that isolated fragments are used as input, this is a relatively simple problem that probably doesn't even require machine learning to solve adequately, e.g. peak picking on a spectral representation could already get you pretty far. It's not clear what value the machine learning component in the proposed approach actually adds, if any. I could be wrong about this as I haven't done the comparison myself, but I think the burden is on the authors to demonstrate that using ML here is actually useful.\n\nI would argue that one of the strenghts of the dataset is the variety of label information it provides, so a much more convincing setup would have been to demonstrate many different prediction tasks for both low-level (e.g. pitch, onsets) and high-level (e.g. composer) characteristics, perhaps with fewer and simpler models -- maybe even sticking to spectrogram input and forgoing raw audio input for the time being, as this comparison seems orthogonal to the introduction of the dataset itself. As it stands, I feel that the fact that the experiments are relatively uninteresting detracts from the main point of the paper, which is to introduce a new public dataset that is truly unique in terms of its scale and scope.\n\nThat said, the experiments seem to have been conducted in a rigorous fashion and the evaluation and analysis of the resulting models is properly executed.\n\nRe: Section 4.5, it is rather unsurprising to me that a pitch detector would learn filters that resemble pitches (i.e. sinusoids), although the observation that this requires a relatively large amount of data is interesting. However, it would be more interesting to demonstrate that this is also the case for higher-level tasks. The authors favourably compare the features learnt by their model with prior work on end-to-end learning from raw audio, but neglect that the tasks considered in this work were much more high-level.\n\nSome might also question whether ICLR is the appropriate venue to introduce a new dataset, but personally I think it's a great idea to submit it here, seeing as it will reach the right people. I suppose this is up to the organisers and the program committee, but I thought it important to mention this, because I don't think this paper merits acceptance based on its experimental results alone.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "detailed accuracy of the data set", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "IS_META_REVIEW": false, "DATE": "13 Dec 2016", "is_meta_review": false}, {"TITLE": "Clarifications on transcription experiments", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "IS_META_REVIEW": false, "DATE": "03 Dec 2016", "is_meta_review": false}, {"TITLE": "Choice of task", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "is_meta_review": false}], "SCORE": 6, "authors": "John Thickstun, Zaid Harchaoui, Sham Kakade", "KEYWORDS": "We introduce a new large-scale music dataset, define a multi-label classification task, and benchmark machine learning architectures on this task.", "accepted": true, "id": ""}
