{
  "name" : "1703.08862.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Socially Aware Motion Planning",
    "authors" : [ "Yu Fan Chen", "Michael Everett", "Miao Liu" ],
    "emails" : [ "jhow}@mit.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "I. INTRODUCTION\nRecent advances in sensing and computing technologies have spurred greater interest in various applications of autonomous ground vehicles. In particular, researchers have explored using robots to provide personal mobility services and luggage carrying support in complex, pedestrian-rich environments (e.g., airports and shopping malls) [1]. These tasks often require the robots to be capable of navigating efficiently and safely in close proximity of people, which is challenging because pedestrians tend to follow subtle social norms that are difficult to quantify, and pedestrians’ intents (i.e., goals) are usually not known [2].\nA common approach treats pedestrians as dynamic obstacles with simple kinematics, and employs specific reactive rules for avoiding collision [3]–[6]. Since these methods do not capture human behaviors, they sometimes generate unsafe/unnatural movements, particularly when the robot operates near human walking speed [2]. To address this issue, more sophisticated motion models have been proposed, which reason about the nearby pedestrians’ hidden intents to generate a set of predicted paths [7], [8]. Subsequently, classical path planning algorithms would be employed to generate a collision-free path for the robot. Yet, separating the navigation problem into disjoint prediction and planning steps can lead to the freezing robot problem, in which the robot fails to find any feasible action because the predicted paths could mark a large portion of the space untraversable [9]. A key to resolving this problem is to account for cooperation, that is, to model/anticipate the impact of the robot’s motion on the nearby pedestrians.\nLaboratory of Information and Decision Systems, Massachusetts Institute of Technology, 77 Massachusetts Ave., Cambridge, MA, USA {chenyuf2, mfe, miaoliu, jhow}@mit.edu\nExisting work on cooperative, socially compliant navigation can be broadly classified into two categories, namely model-based and learning-based. Model-based approaches are typically extensions of multiagent collision avoidance algorithms, with additional parameters introduced to account for social interactions [7], [10]–[13]. For instance, to distinguish between human–human and human–robot interactions, the extended social forces model [11], [12] augments the potential field algorithm with additional terms that specify the repulsive forces (e.g, strength and range) governing each type of interaction. Model-based methods are designed to be computationally efficient as they often correspond to intuitive geometric relations; yet, it is unclear whether humans do follow such precise geometric rules. In particular, the force parameters often need to be tuned individually, and can vary significantly for different pedestrians [12]. Also, it has been observed that model-based methods can lead to oscillatory paths [2], [14].\nIn comparison, learning-based approaches aim to develop a policy that emulates human behaviors by matching feature statistics, such as the minimum distance to pedestrians. In particular, Inverse Reinforcement Learning (IRL) [15] has been applied to learn a cost function from human demonstration (teleoperation) [16], and a probability distribution over the set of joint trajectories with nearby pedestrians [2], [17]. Compared with model-based approaches, learning-based methods have been shown to produce paths that more closely resemble human behaviors, but often at a much higher computational cost. For instance, computing/matching trajectory features often requires anticipating the joint path of all nearby pedestrians [2]. More importantly, since human behaviors are inherently stochastic, the feature statistics calculated on pedestrians’ paths can vary significantly from\nar X\niv :1\n70 3.\n08 86\n2v 1\n[ cs\n.R O\n] 2\n6 M\nar 2\n01 7\nperson to person, and even run to run for the same scenario [2], [16]. This raises concerns over whether such feature-matching methods are generalizable to different environments [13].\nIn short, existing works are mostly focused on modeling and replicating the detailed mechanisms of social compliance, which remains difficult to quantify due to the stochasticity in people’s behaviors. In comparison, humans can intuitively evaluate whether a behavior is acceptable. In particular, human navigation (or teleoperation) is time-efficient and generally respects a set of simple social norms (e.g., “passing on the right”) [2], [16], [18]. Building on a recent paper [14], we characterize these properties in a reinforcement learning framework, and show that human-like navigation conventions emerge from solving a cooperative collision avoidance problem.\nThe main contributions of this work are (i) introducing Socially Aware Collision Avoidance with Deep Reinforcement Learning (SA-CADRL) for explaining/inducing socially aware behaviors in a RL framework, (ii) generalizing to multiagent (n > 2) scenarios through developing a symmetrical neural network structure, and (iii) demonstrating on robotic hardware autonomous navigation at human walking speed in a pedestrian-rich environment."
    }, {
      "heading" : "II. BACKGROUND",
      "text" : ""
    }, {
      "heading" : "A. Collision Avoidance with Deep Reinforcement Learning",
      "text" : "A multiagent collision avoidance problem can be formulated as a sequential decision making problem in a reinforcement learning framework [14]. Let st, ut denote an agent’s state and action at time t, and let s̃t denote the state of a nearby agent. To capture the uncertainty in nearby agents’ intents, the state vector is partitioned into observable and unobservable parts, that is st = [sot , s h t ]. Let the observable states be the agent’s position, velocity, and radius (size), so = [px, py, vx, vy, r] ∈ R5; let the unobservable states be the agent’s intended goal position, preferred speed, and orientation, sh = [pgx, pgy, vpref , ψ] ∈ R4; and let the action be the agent’s velocity, ut = vt. It will be explained in Section IV that the observable states can be readily obtained from sensor measurements. The objective is to develop a policy, π : (st, s̃to) 7→ ut, that minimizes the expected time to goal E[tg] while avoiding collision with nearby agents,\nargmin π(s, s̃o)\nE [tg|s0, s̃o0, π] (1)\ns.t. ||pt − p̃t||2 ≥ r + r̃ ∀t (2) ptg = pg (3) pt = pt−1 + ∆t · π(st−1, s̃ot−1) p̃t = p̃t−1 + ∆t · π(s̃t−1, sot−1), (4)\nwhere (2) is the collision avoidance constraint, (3) is the goal constraint, (4) is the agents’ kinematics, and the expectation in (1) is with respect to the other agent’s unobservable states (intents) and policy.\nThis problem can be formulated in a reinforcement learning (RL) framework by considering the joint configuration of the agent with its neighbor, sjn = [s, s̃o]. In particular, a\nreward function, Rcol(sjn,u), can be specified to reward the agent for reaching its goal and penalize the agent for colliding with others. The unknown state-transition model, P (sjnt+1, s jn t |ut), takes into account the uncertainty in the other agent’s motion due to its hidden intents (s̃h). Solving the RL problem amounts to finding the optimal value function that encodes an estimate of the expected time to goal,\nV ∗(sjn0 ) = E [ T∑ t=0 γt·vpref Rcol(s jn t , π ∗(sjnt )) | s jn 0 ] , (5)\nwhere γ ∈ [0, 1) is a discount factor. The optimal policy can be retrieved from the value function, that is\nπ∗(sjnt+1) = argmax u Rcol(st,u)+ γ∆t·vpref ∫ sjnt+1 P (sjnt , s jn t+1|u)V ∗(s jn t+1)ds jn t+1. (6)\nA major challenge in finding the optimal value function is that the joint state sjn is a continuous, high-dimensional vector, making it impractical to discretize and enumerate the state space. Recent advances in reinforcement learning address this issue by using deep neural networks to represent value functions in high-dimensional spaces, and have demonstrated human-level performance on various complex tasks [19]– [21]. While several recent works have applied deep RL to motion planning [22], [23], they are mainly focused on single agent navigation in unknown static environments, and with an emphasis on computing control inputs directly from raw sensor data (e.g., camera images). In contrast, this work extends the Collision Avoidance with Deep Reinforcement Learning framework (CADRL) [14] to characterize and induce socially aware behaviors in multiagent systems."
    }, {
      "heading" : "B. Characterization of Social Norms",
      "text" : "It has been widely observed that humans tend to follow simple navigation norms to avoid colliding with each other, such as passing on the right and overtaking on the left [18]. Albeit intuitive, it remains difficult to quantify the precise mechanisms of social norms, such as when to turn and how much to turn when passing another pedestrian; and the problem exacerbates as the number of nearby pedestrians increases. This is largely due to the stochasticity in people’s\nmotion (e.g, speed, smoothness), which can vary significantly among different individuals [2].\nRather than trying to quantify human behaviors directly, this work notes that the complex normative motion patterns can be a consequence of simple local interactions. For instance, an intuitive pairwise collision avoidance rule [10] can cause simulated agents moving in the same direction to form lanes in long corridors. Similarly, rather than a set of precisely defined procedural rules, social norms could be the result of a timeefficient, reciprocal collision avoidance mechanism. Evidently, pedestrian navigation conventions are not unique, as the direction (e.g., left-handed vs. right-handed) and strength (e.g., separation distance) vary in different countries [24], as illustrated in Fig. 2.\nExisting works have reported that human navigation (or teleoperation of a robot) tend to be cooperative and timeefficient [2], [16]. This work notes that these two properties are encoded in the CADRL formulation through using the min-time reward function and the reciprocity assumption (π̃ = π). Furthermore, it was interesting to observe that while no behavior rules (e.g., function forms) were imposed in the problem formulation, CADRL policy exhibits certain navigation conventions, as illustrated in Fig. 3. In particular, Fig. 3a illustrates two CADRL agents passing on the right of each other, showing signs of conforming to mutually agreed rules. More importantly, this preference in passing direction is robust to small deviations in the initial condition, as shown in Fig. 3b. As the offset increases, the CADRL agents eventually change passing direction in favor of shorter, smoother paths (Fig. 3c). Recall no communication took place and each agent’s intent (e.g., goal) is not known to the other.\nHowever, the cooperative behaviors emerging from a CADRL solution are not consistent with human interpretation.\nFor instance, two CADRL agents with different sizes and preferred speeds show a preference to pass on the left of each other (Fig. 3d). This is because an agent’s state s is defined to be the concatenation of its position, velocity, size and goal, so a tie breaking navigation convention should not be solely dependent on relative position (as human social norms). Moreover, the cooperative behaviors of CADRL cannot be controlled – they are largely dependent on the initialization of the value network and set of randomly generated training test cases. The next section will address this issue and present a method to induce behaviors that respect human social norms."
    }, {
      "heading" : "III. APPROACH",
      "text" : "The following presents the socially aware multiagent collision avoidance with deep reinforcement learning algorithm (SA-CADRL). We first describe a strategy for shaping normative behaviors for a two agent system in the RL framework, and then generalize the method to multiagent scenarios.\nA. Inducing Social Norms\nRecall the RL training process seeks to find the optimal value function (5), which maps from the joint state of an agent with its neighbor, sjn = [s, s̃o], to a scalar value that encodes the expected time to goal. To reduce redundancy (up to a rotation and translation), this work uses a local coordinate frame with the x-axis pointing towards an agent’s goal, as shown in Fig. 4. Specifically, each agent’s state is parametrized as\ns = [dg, vpref , vx, vy, ψ, r] (7)\ns̃o = [p̃x, p̃y, ṽx, ṽy, r̃, d̃a, φ̃, b̃on] , (8)\nwhere dg = ||pg − p||2 is the agent’s distance to goal, d̃a = ||p−p̃||2 is the distance to the other agent, φ̃ = tan−1(ṽy/ṽx) is the other agent’s heading direction, and b̃on is a binary flag indicating whether the other agent is real or virtual (details will be provided in Section III-B).\nThis work notes that social norms are one of the many ways to resolve a symmetrical collision avoidance scenario, as illustrated in Fig. 2. To induce a particular norm, a small bias can be introduced in the RL training process in favor of one set of behaviors over others. For instance, to encourage passing on the right, states (configurations) with another agent approaching from the undesirable side can be\npenalized (green agent in Figure 4). The advantage of this approach is that violations of a particular social norm are usually easy to specify; and this specification need not be precise. This is because the addition of a penalty breaks the symmetry in the collision avoidance problem, thereby favoring behaviors respecting the desired social norm. This work uses the following specification of a reward function Rnorm for inducing the right-handed rules (Fig. 4),\nRnorm(s jn,u) = qnI(s jn ∈ Snorm) (9) s.t. Snorm = Spass ∪ Sovtk ∪ Scross\nSpass = { sjn | dg > 3, 1 < p̃x < 4, − 2 < p̃y < 0, |φ̃− ψ| > 3π/4 } (10) Sovtk = { sjn | dg > 3, 0 < p̃x < 3, |v| > |ṽ| 0 < p̃y < 1, |φ̃− ψ| < π/4 } (11)\nScross = { sjn | dg > 3, d̃a < 2, φ̃rot > 0, − 3π/4 < φ̃− ψ < −π/4 } , (12)\nwhere qn is a scalar penalty, I(·) is the indicator function, φ̃rot = tan\n−1((ṽx − vx)/(ṽy − vy)) is the relative rotation angle between the two agents, and the angle difference φ̃−ψ is wrapped between [−π, π]. An illustration of these three penalty sets is provided in Fig. 4.\nThe parameters defining the penalty set Snorm affect the rate of convergence. With (10)-(12), the SA-CADRL policy converged within 700 episodes (exhibiting the desired behaviors such as passing on the right on all validation test cases). With a 30% smaller penalty set (i.e., shrinking the shaded regions in Fig. 4), convergence occurred after 1250 episodes. Larger penalty sets, however, could lead to instability or divergence. Also, as long as training converges, the penalty sets’ size does not have a major effect on the learned policy. This is expected because the desired behaviors are not in the penalty set. Similarly, (9)-(12) can be modified to induce left-handed rules. We trained two SA-CADRL policies to learn left-handed and right-handed norms starting from the same initialization, the results of which are shown\nAlgorithm 1: Deep V-learning for SA-CADRL 1 initialize and duplicate a value net with n agents\nV (·; θ, n), V ′ ← V 2 initialize experience sets E ← ∅, Eb ← ∅ 3 for episode=1, . . . , Neps do 4 for m times do 5 p ∼ Uniform(2, n) 6 s10, s 2 0, . . . , s p 0 ← randomTestcase(p) 7 s10:tf , s 2 0:tf\n, . . . , sp0:tf ← SA-CADRL(V ) 8 with prob f , mirror every traj si0:tf in the x-axis 9 for every agent i do\n10 yi0:T ← findValues ( V ′, sjn,i0:tf ) 11 E, Eb ← assimilate ( E, Eb, (y i, sjn,i)0:tf )\n12 e ← randSubset(E) ∪ randSubset(Eb) 13 θ ← RMSprop(e) 14 for every C episodes do 15 Evaluate(V ), V ′ ← V\n16 return V\nin Fig. 6. The learned policies exhibited similar qualitative behaviors as shown in Fig. 2. Also note that training is performed on randomly generated test cases, and not on the validation test cases."
    }, {
      "heading" : "B. Training a Multiagent Value Network",
      "text" : "The CADRL work [14] trained a two-agent network with three fully connected hidden layers, and used a minimax scheme for scaling up to multiagent (n > 2) scenarios. Since training was solely performed on a two-agent system, it was difficult to encode/induce higher order behaviors (i.e., accounting for the relations between nearby agents). This work addresses this problem by developing a method that allows for training on multiagent scenarios directly.\nTo capture the multiagent system’s symmetrical structure, a neural network with weight-sharing and max-pooling layers is employed, as shown in Fig. 5. In particular, for a four-agent network shown in Fig. 5b, the three nearby agents’ observed states can be swapped (blue input blocks) without affecting the output value. This condition is enforced through weightsharing, as shown in Fig. 5a. Two of such symmetrical layers are used, followed by a max-pooling layer for aggregating features and two fully-connected layers for computing a scalar value. This works uses the rectified linear unit (ReLu) as the activation function in the hidden layers.\nThe input to the n-agent network is a generalization of (7)-(8), that is, sjn = [s, s̃o,1, . . . s̃o,n−1], where the superscripts enumerate the nearby agents. The norm-inducing reward function is defined similarly as (9), where a penalty is given if an agent’s joint configuration with the closest nearby agent belongs to the penalty set Snorm. The overall reward function is the sum of the original CADRL reward and the norm-inducing reward, that is, R(·) = Rcol(·) +Rnorm(·).\nThe procedure for training a multiagent SA-CADRL policy\nis outlined in Algorithm 1, which follows similarly as in [14], [19]. A value network is first initialized by training on an n-agent trajectory dataset through neural network regression (line 1). Using this value network (6) and following an - greedy policy, a set of trajectories can be generated on random test cases (line 5-7). The trajectories are then turned into statevalue pairs and assimilated into the experience sets E, Eb (line 10-11). A subset of state-value pairs is sampled from the experience sets, and subsequently used to update the value network through back-propagation (line 12-13). The process repeats for a pre-specified number of episodes (line 3-4).\nCompared with CADRL [14], two important modifications are introduced in the training process. First, two experience sets, E, Eb, are used to distinguish between trajectories that reached the goals and those that ended in a collision (line 2, 11). This is because a vast majority (≥ 90%) of the random generated test cases were fairly simple, requiring an agent to travel mostly straight towards its goal. The bad experience set Eb improves the rate of learning by focusing on the scenarios that fared poorly for the current policy. Second, during the training process, trajectories generated by SA-CADRL are reflected in the x-axis with probability f (line 8). By inspection of Fig. 2, this operation flips the paths’ topology (left-handedness vs right-handedness). Since a trajectory can be a few hundred steps long according to (6), it could take a long time for an -greedy policy to explore the state space and find an alternative topology. In particular, empirical results show that, without this procedure, policies can still exhibit the wrong passing side after 2000 training episodes. This procedure exploits symmetry in the problem to explore different topologies more efficiently.\nFurthermore, an n-agent network can be used to generate\ntrajectories for scenarios with fewer agents (line 5). In particular, when there are p ≤ n agents, the inputs in Fig. 5b corresponding to the non-existent agents1 can be filled by adding virtual agents – replicating the states of the closest nearby agent and set the binary bit b̃on to zero (8). The use of this parametrization avoids the need for training many different networks. A left-handed and a right-handed fouragent SA-CADRL policies are trained using the network structure shown in Fig. 5. Sample trajectories generated by these policies are shown in Fig. 7, which demonstrate the preferred behaviors of each respective set of social norms."
    }, {
      "heading" : "IV. RESULTS",
      "text" : ""
    }, {
      "heading" : "A. Computational Details",
      "text" : "The size and connections in the multiagent network shown in Fig. 5 are tuned to obtain good performance (ensure convergence and produce time-efficient paths) while achieving real-time performance. In particular, on a computer with an i7-5820K CPU, a Python implementation of a four-agent SA-CADRL policy takes on average 8.7ms for each query of the value network (finding an action). Furthermore, offline training (Algorithm 1) took approximately nine hours to complete 3,000 episodes. In comparison, a two-agent network took approximately two hours for 1,000 training episodes. The four-agent system took much longer to train because its state space is much larger (higher dimensional) than that of the two-agent system. The training process was repeated multiple times and all runs converged to a similar policy – exhibiting the respective desired social norms on all test cases in an evaluation set. Furthermore, this work generated\n1Consider an agent with two nearby agents using a four-agent network.\nrandom test cases with sizes and velocities similar to that of normal pedestrians [25], such that r ∈ [0.2, 0.5]m, and vpref ∈ [0.3, 1.8]m/s. Also, a desired minimum separation of 0.2m is specified through the collision reward Rcol, which penalizes an agent for getting too close to its neighbors."
    }, {
      "heading" : "B. Simulation Results",
      "text" : "Three copies of four-agent SA-CADRL policies were trained, one without the norm inducing reward Rnorm, one with the left-handed Rnorm, and the other with the right-handed Rnorm. On perfectly symmetrical test cases, such as those shown in Figs. 3 and 6, the left and righthanded SA-CADRL policies always select the correct passing side according to the respective norm. To demonstrate SACADRL can balance between finding time-efficient paths and respecting social norms, these policies are further evaluated on randomly generated test cases. In particular, we compute the average extra time to reach the goals2, the minimum separation distance, and the relative preference between left-handedness and right-handedness. Norm preference is calculated by counting the proportion of trajectories that violate the left-handed or the right-handed version of (10)-(12) for more than 0.5 second. To ensure the test set is left-right balanced, each random test case is duplicated and reflected in the x-axis. Evidently, the optimal reciprocal collision avoidance (ORCA) [6] algorithm – a reactive, rule-based method that computes a velocity vector based on an agent’s\n2 t̄e = 1 n ∑n i=1[t i g − ||pi0 − pig ||2 / vipref ], where t i g is the ith agent’s time to reach its goal, and the second term is a lower bound of tig (straight toward goal at the preferred speed).\njoint geometry with its neighbors – attains nearly 50-50 left/right-handedness on these test sets (first row of Table I).\nThe same four-agent SA-CADRL policies are used to generate trajectories for both the two-agent and the fouragent test sets. On the two-agent test-set, all RL-based methods produced more time-efficient paths than ORCA3. CADRL exhibited a slight preference to the right (40-60 split). The four-agent SA-CADRL (none) policy, in comparison, exhibited a stronger preference than ORCA and CADRL in each of the passing, crossing, and overtaking scenarios (third row in Table I). This observation suggests that (i) certain navigation conventions could emerge as a means of resolving symmetrical collision avoidance scenarios, and (ii) the conventions don’t always correspond to human social norms. For instance, SA-CADRL (none) prefers passing on the right but also overtaking on the right, which is a mix between right-handed and left-handed rules. In contrast, the SA-CADRL policies trained with Rnorm exhibited a strong preference (85-15 split) of the respective social norm. Recall that this ratio is not 1 because there is a tradeoff between time-optimality and social compliance, as illustrated in Fig. 3. This tradeoff can be controlled by tuning qn in (9). Evidently, SA-CADRL (lh/rh) achieves better social compliance at a cost of an approximately 20% larger t̄e, because satisfying the norms often requires traveling a longer path.\nSimilarly, the bottom rows of Table I show that in the fouragent test set, all RL-based methods outperformed ORCA, and SA-CADRL (lh/rh) exhibited behaviors that respect the social norms. CADRL produced paths that are closer to time-optimal than the other algorithms, but sometimes came very close (within 0.1m) to other agents. This close proximity occurred because CADRL was trained on a two-agent system, so its action choice is dominated by the single closest neighbor; possibly leading CADRL to select an action that avoids the closest neighbor but drives towards a third agent. In contrast, all SA-CADRL policies were trained on four-agent systems and they all maintained a larger average separation distance."
    }, {
      "heading" : "C. Hardware Experiment",
      "text" : "The SA-CADRL policy is implemented on a robotic vehicle for autonomous navigation in an indoor environment with many pedestrians, as shown in Fig. 1. The differential-drive vehicle is outfitted with a Lidar for localization, three Intel Realsenses for obstacle detection, and four webcams for pedestrian detection. Pedestrian detection and tracking is performed by combining Lidar’s pointcloud data with camera images [26]. The speed, velocity, and size (radius) of a pedestrian are estimated by clustering the pointcloud data [27]. The estimated radius includes a buffer (comfort) zone as reported in [2], [16]. Obstacles within a 10 x 10m square (perception range) centered at vehicle are detected and used to populate an occupancy map, shown as the white box in Fig. 8a.\n3ORCA specifies a reactive, geometric rule for computing a collision-free velocity vector, but it does not anticipate the evolution of an agent’s state with respect to other agents nearby. Thus, ORCA can generate shortsighted actions and oscillatory paths (see [14] for a detailed explanation).\nTABLE I: SA-CADRL’s performance statistics on randomly generated test cases. SA-CADRL policies were trained (i) without the norm inducing reward Rnorm, (ii) with left-handed Rnorm, and (iii) right-handed Rnorm, which are abbreviated as none, lh, rh, respectively. Results show that SA-CADRL policies produced time-efficient paths and exhibited behaviors that respect the corresponding social norm.\nNumber of Method Extra time to goal t̄e(s) Min separation dist. (m) Norm preference (%) [left-handed / right-handed] agents [Avg / 75th / 90th pctl] [10th pctl / avg] passing crossing overtaking\n2\nORCA [6] 0.46 / 0.49 / 0.82 0.108 / 0.131 45 / 55 51 / 49 50 / 50 CADRL [14] 0.25 / 0.30 / 0.47 0.153 / 0.189 37 / 63 38 / 62 43 / 57 SA-CADRL(none) 0.27 / 0.28 / 0.54 0.169 / 0.189 10 / 90 32 / 68 63 / 37 SA-CADRL(lh) 0.30 / 0.36 / 0.67 0.163 / 0.192 98 / 2 85 / 15 86 / 14 SA-CADRL(rh) 0.31 / 0.38 / 0.69 0.168 / 0.199 2 / 98 15 / 85 17 / 83\n4\nORCA [6] 0.86 / 1.14 / 1.80 0.106 / 0.125 46 / 54 50 / 50 48 / 52 CADRL(minimax) [14] 0.41 / 0.54 / 0.76 0.096 / 0.173 31 / 69 41 / 59 46 / 54\nSA-CADRL(none) 0.44 / 0.63 / 0.85 0.162 / 0.183 33 / 67 33 / 67 62 / 38 SA-CADRL(lh) 0.49 / 0.69 / 1.00 0.155 / 0.178 83 / 17 67 / 33 73 / 27 SA-CADRL(rh) 0.46 / 1.63 / 1.02 0.155 / 0.180 12 / 88 29 / 71 30 / 70\n(a) passing on the right (b) overtaking on the left\nFig. 8: Visualization of a ground vehicle’s sensor data. The vehicle (yellow box) uses the right-handed SA-CADRL policy to navigate autonomously in an indoor environment at a nominal speed of 1.2m/s. (a) shows the vehicle passing a pedestrian on the right, where the person in the front camera image is detected and shown as the blue cylinder in the rviz view. (b) shows the vehicle overtaking a pedestrian on the left, where the person in the right camera image corresponds to the teal cylinder in the rviz view.\nMotion planning uses an integration of a diffusion map [28] for finding global paths and SA-CADRL for local collision avoidance. In particular, the diffusion map algorithm considers static obstacles in the environment and computes a subgoal within 5m (static planning horizon) of the vehicle, and a set of feasible directions (heading and range). The subgoal is shown at the end of the green line in Fig. 8a, and the feasible directions are visualized as the blue lines emanating from the vehicle. SA-CADRL takes in the set of detected pedestrians, and chooses an action (velocity vector) from the feasible directions to move the vehicle toward the subgoal. SA-CADRL’s decision is shown as the blue arrow in Fig. 8a, which does not necessarily line up with the subgoal. Note that pedestrians can be detected beyond the 5m static planning horizon, thus allowing socially aware interaction at a longer range. This whole sense-plan-execute cycle is fast enough to operate in real-time at 10Hz on a Gigabyte Brix computer onboard the vehicle.\nUsing this motion planning strategy, the vehicle was able to navigate fully autonomously in a dynamic indoor environment.\nIn particular, the vehicle is issued randomly generated goals ten times, with an average distance between successive goals of more than 50 meters. During the experiment, an average of 10.2 persons came within 2m of the vehicle each minute, and all encountered pedestrians are part of the regular daily traffic4 in a public building. At a nominal speed of 1.2m/s, which is approximately the average human walking pace [25], the vehicle maintained safe distance to the pedestrians and generally respected social norms. While a safety driver was walking approximately five meters behind vehicle, he never had to intervene or take over control at any time during the ten runs. Since people in North America follow the righthanded rule, the SA-CADRL policy with right-handed norms is used for the hardware experiment, which causes the vehicle to generally pass pedestrians on the right and overtake on the left. For examples, snippets of the experiment are shown in Fig. 8. A hardware demonstration video can be found at https://youtu.be/PMTnUJWeEsA."
    }, {
      "heading" : "V. CONCLUSION",
      "text" : "This work presented SA-CADRL, a multiagent collision avoidance algorithm that considers and exhibits socially compliant behaviors. In particular, in a reinforcement learning framework, a pair of simulated agents navigate around each other to learn a policy that respect human navigation norms, such as passing on the right and overtaking on the left in a right-handed system. This approach is further generalized to multiagent (n > 2) scenarios through the use of a symmetrical neural network structure. Moreover, SA-CADRL is implemented on robotic hardware, which enabled fully autonomous navigation at human walking speed in a dynamic environment with many pedestrians. Future work will consider the relationships between nearby pedestrians, such as a group of people who walk together."
    }, {
      "heading" : "ACKNOWLEDGMENT",
      "text" : "This work is supported by Ford Motor Company.\n4Such as undergraduate students going between lectures or visitors on campus, not testers/personnel associated with this work."
    } ],
    "references" : [ {
      "title" : "Intention-aware online POMDP planning for autonomous driving in a crowd",
      "author" : [ "H. Bai", "S. Cai", "N. Ye", "D. Hsu", "W.S. Lee" ],
      "venue" : "Proceedings of the 2015 IEEE International Conference on Robotics and Automation (ICRA), May 2015, pp. 454–460.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Socially compliant mobile robot navigation via inverse reinforcement learning",
      "author" : [ "H. Kretzschmar", "M. Spies", "C. Sprunk", "W. Burgard" ],
      "venue" : "The International Journal of Robotics Research, Jan. 2016.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "The dynamic window approach to collision avoidance",
      "author" : [ "D. Fox", "W. Burgard", "S. Thrun" ],
      "venue" : "IEEE Robotics Automation Magazine, vol. 4, no. 1, pp. 23–33, Mar. 1997.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Reciprocal velocity obstacles for real-time multi-agent navigation",
      "author" : [ "J. Van den Berg", "M. Lin", "D. Manocha" ],
      "venue" : "Proceedings of the 2008 IEEE International Conference on Robotics and Automation (ICRA), 2008, pp. 1928–1935.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "SIPP: Safe interval path planning for dynamic environments",
      "author" : [ "M. Phillips", "M. Likhachev" ],
      "venue" : "Proceedings of the 2011 IEEE International Conference on Robotics and Automation (ICRA), May 2011, pp. 5628– 5635.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Reciprocal n-body collision avoidance",
      "author" : [ "J. van den Berg", "S.J. Guy", "M. Lin", "D. Manocha" ],
      "venue" : "Robotics Research, ser. Springer Tracts in Advanced Robotics, C. Pradalier, R. Siegwart, and G. Hirzinger, Eds. Springer Berlin Heidelberg, 2011, no. 70, pp. 3–19.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "BRVO: Predicting pedestrian trajectories using velocityspace reasoning",
      "author" : [ "S. Kim", "S.J. Guy", "W. Liu", "D. Wilkie", "R.W. Lau", "M.C. Lin", "D. Manocha" ],
      "venue" : "Int. J. Rob. Res., vol. 34, no. 2, pp. 201–217, Feb. 2015.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Humanrobot co-navigation using anticipatory indicators of human walking motion",
      "author" : [ "V.V. Unhelkar", "C. Pérez-D’Arpino", "L. Stirling", "J.A. Shah" ],
      "venue" : "Proceedings of the 2015 IEEE International Conference on Robotics and Automation (ICRA), May 2015, pp. 6183–6190.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Robot navigation in dense human crowds: Statistical models and experimental studies of human–robot cooperation",
      "author" : [ "P. Trautman", "J. Ma", "R.M. Murray", "A. Krause" ],
      "venue" : "The International Journal of Robotics Research, vol. 34, no. 3, pp. 335–356, Mar. 2015.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Social force model for pedestrian dynamics",
      "author" : [ "D. Helbing", "P. Molnár" ],
      "venue" : "Physical Review E, vol. 51, no. 5, pp. 4282–4286, May 1995.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Robot companion: A socialforce based approach with human awareness-navigation in crowded environments",
      "author" : [ "G. Ferrer", "A. Garrell", "A. Sanfeliu" ],
      "venue" : "2013 IEEE/RSJ International Conference on Intelligent Robots and Systems, Nov. 2013, pp. 1688–1694.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Behavior estimation for a complete framework for human motion prediction in crowded environments",
      "author" : [ "G. Ferrer", "A. Sanfeliu" ],
      "venue" : "Proceedings of the 2014 IEEE International Conference on Robotics and Automation (ICRA), May 2014, pp. 5940–5945.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Autonomous navigation in dynamic social environments using multi-policy decision making",
      "author" : [ "D. Mehta", "G. Ferrer", "E. Olson" ],
      "venue" : "Oct. 2016.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Decentralized, noncommunicating multiagent collision avoidance with deep reinforcement learning",
      "author" : [ "Y. Chen", "M. Liu", "M. Everett", "J.P. How" ],
      "venue" : "Proceedings of the 2017 IEEE International Conference on Robotics and Automation (ICRA), Singapore, 2017.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Apprenticeship learning via inverse reinforcement learning",
      "author" : [ "P. Abbeel", "A.Y. Ng" ],
      "venue" : "Proceedings of the Twenty-First International Conference on Machine Learning, ser. ICML ’04. New York, NY, USA: ACM, 2004, pp. 1–.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Socially adaptive path planning in human environments using inverse reinforcement learning",
      "author" : [ "B. Kim", "J. Pineau" ],
      "venue" : "International Journal of Social Robotics, vol. 8, no. 1, pp. 51–66, June 2015.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Featurebased prediction of trajectories for socially compliant navigation",
      "author" : [ "M. Kuderer", "H. Kretzschmar", "C. Sprunk", "W. Burgard" ],
      "venue" : "Robotics:Science and Systems, 2012.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Pedestrian-inspired sampling-based multi-robot collision avoidance",
      "author" : [ "R.A. Knepper", "D. Rus" ],
      "venue" : "2012 IEEE RO-MAN: The 21st IEEE International Symposium on Robot and Human Interactive Communication, Sept. 2012, pp. 94–100.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis" ],
      "venue" : "Nature, vol. 518, no. 7540, pp. 529–533, Feb. 2015.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Mastering the game of Go with deep neural networks and tree search",
      "author" : [ "D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. van den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M. Lanctot", "S. Dieleman", "D. Grewe", "J. Nham", "N. Kalchbrenner", "I. Sutskever", "T. Lillicrap", "M. Leach", "K. Kavukcuoglu", "T. Graepel", "D. Hassabis" ],
      "venue" : "Nature, vol. 529, no. 7587, pp. 484–489, Jan. 2016.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Asynchronous methods for deep reinforcement learning",
      "author" : [ "V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T.P. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu" ],
      "venue" : "arXiv:1602.01783 [cs], Feb. 2016.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "CAD)2RL: Real single-image flight without a single real image",
      "author" : [ "F. Sadeghi", "S. Levine" ],
      "venue" : "arXiv:1611.04201 [cs], Nov. 2016.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Learning deep control policies for autonomous aerial vehicles with MPC-guided policy search",
      "author" : [ "T. Zhang", "G. Kahn", "S. Levine", "P. Abbeel" ],
      "venue" : "2016 IEEE International Conference on Robotics and Automation (ICRA), May 2016, pp. 528–535.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Recommended walking speeds for timing of pedestrian clearance intervals based on characteristics of the pedestrian population",
      "author" : [ "T. Gates", "D. Noyce", "A. Bill", "N. Van Ee" ],
      "venue" : "Transportation Research Record: Journal of the Transportation Research Board, vol. 1982, pp. 38–47, Jan. 2006.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 1982
    }, {
      "title" : "Dynamic arrival rate estimation for campus Mobility On Demand network graphs",
      "author" : [ "J. Miller", "A. Hasfura", "S.Y. Liu", "J.P. How" ],
      "venue" : "2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Oct. 2016, pp. 2285–2292.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Dynamic clustering via asymptotics of the Dependent Dirichlet Process mixture",
      "author" : [ "T. Campbell", "M. Liu", "B. Kulis", "J.P. How", "L. Carin" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS), May 2013.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Motion planning with diffusion maps",
      "author" : [ "Y. Chen", "S.-Y. Liu", "M. Liu", "J. Miller", "J.P. How" ],
      "venue" : "2016 IEEE/RSJ International Conference on Intelligent Robots and Systems, Daejeon, Korea, Oct. 2016.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : ", airports and shopping malls) [1].",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 1,
      "context" : ", goals) are usually not known [2].",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 2,
      "context" : "A common approach treats pedestrians as dynamic obstacles with simple kinematics, and employs specific reactive rules for avoiding collision [3]–[6].",
      "startOffset" : 141,
      "endOffset" : 144
    }, {
      "referenceID" : 5,
      "context" : "A common approach treats pedestrians as dynamic obstacles with simple kinematics, and employs specific reactive rules for avoiding collision [3]–[6].",
      "startOffset" : 145,
      "endOffset" : 148
    }, {
      "referenceID" : 1,
      "context" : "Since these methods do not capture human behaviors, they sometimes generate unsafe/unnatural movements, particularly when the robot operates near human walking speed [2].",
      "startOffset" : 166,
      "endOffset" : 169
    }, {
      "referenceID" : 6,
      "context" : "To address this issue, more sophisticated motion models have been proposed, which reason about the nearby pedestrians’ hidden intents to generate a set of predicted paths [7], [8].",
      "startOffset" : 171,
      "endOffset" : 174
    }, {
      "referenceID" : 7,
      "context" : "To address this issue, more sophisticated motion models have been proposed, which reason about the nearby pedestrians’ hidden intents to generate a set of predicted paths [7], [8].",
      "startOffset" : 176,
      "endOffset" : 179
    }, {
      "referenceID" : 8,
      "context" : "robot fails to find any feasible action because the predicted paths could mark a large portion of the space untraversable [9].",
      "startOffset" : 122,
      "endOffset" : 125
    }, {
      "referenceID" : 6,
      "context" : "Model-based approaches are typically extensions of multiagent collision avoidance algorithms, with additional parameters introduced to account for social interactions [7], [10]–[13].",
      "startOffset" : 167,
      "endOffset" : 170
    }, {
      "referenceID" : 9,
      "context" : "Model-based approaches are typically extensions of multiagent collision avoidance algorithms, with additional parameters introduced to account for social interactions [7], [10]–[13].",
      "startOffset" : 172,
      "endOffset" : 176
    }, {
      "referenceID" : 12,
      "context" : "Model-based approaches are typically extensions of multiagent collision avoidance algorithms, with additional parameters introduced to account for social interactions [7], [10]–[13].",
      "startOffset" : 177,
      "endOffset" : 181
    }, {
      "referenceID" : 10,
      "context" : "For instance, to distinguish between human–human and human–robot interactions, the extended social forces model [11], [12] augments the potential field algorithm with additional terms that specify the repulsive forces (e.",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 11,
      "context" : "For instance, to distinguish between human–human and human–robot interactions, the extended social forces model [11], [12] augments the potential field algorithm with additional terms that specify the repulsive forces (e.",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 11,
      "context" : "In particular, the force parameters often need to be tuned individually, and can vary significantly for different pedestrians [12].",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 1,
      "context" : "Also, it has been observed that model-based methods can lead to oscillatory paths [2], [14].",
      "startOffset" : 82,
      "endOffset" : 85
    }, {
      "referenceID" : 13,
      "context" : "Also, it has been observed that model-based methods can lead to oscillatory paths [2], [14].",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 14,
      "context" : "In particular, Inverse Reinforcement Learning (IRL) [15] has been applied to learn a cost function from human demonstration (teleoperation) [16], and a probability distribution over the set of joint trajectories with nearby pedestrians [2],",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 15,
      "context" : "In particular, Inverse Reinforcement Learning (IRL) [15] has been applied to learn a cost function from human demonstration (teleoperation) [16], and a probability distribution over the set of joint trajectories with nearby pedestrians [2],",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 1,
      "context" : "In particular, Inverse Reinforcement Learning (IRL) [15] has been applied to learn a cost function from human demonstration (teleoperation) [16], and a probability distribution over the set of joint trajectories with nearby pedestrians [2],",
      "startOffset" : 236,
      "endOffset" : 239
    }, {
      "referenceID" : 16,
      "context" : "[17].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 1,
      "context" : "For instance, computing/matching trajectory features often requires anticipating the joint path of all nearby pedestrians [2].",
      "startOffset" : 122,
      "endOffset" : 125
    }, {
      "referenceID" : 1,
      "context" : "person to person, and even run to run for the same scenario [2],",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 15,
      "context" : "[16].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "This raises concerns over whether such feature-matching methods are generalizable to different environments [13].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 1,
      "context" : ", “passing on the right”) [2], [16], [18].",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 15,
      "context" : ", “passing on the right”) [2], [16], [18].",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 17,
      "context" : ", “passing on the right”) [2], [16], [18].",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 13,
      "context" : "Building on a recent paper [14], we characterize these properties in a reinforcement learning framework, and show that human-like navigation conventions emerge from solving a cooperative collision avoidance problem.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 13,
      "context" : "A multiagent collision avoidance problem can be formulated as a sequential decision making problem in a reinforcement learning framework [14].",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 18,
      "context" : "Recent advances in reinforcement learning address this issue by using deep neural networks to represent value functions in high-dimensional spaces, and have demonstrated human-level performance on various complex tasks [19]– [21].",
      "startOffset" : 219,
      "endOffset" : 223
    }, {
      "referenceID" : 20,
      "context" : "Recent advances in reinforcement learning address this issue by using deep neural networks to represent value functions in high-dimensional spaces, and have demonstrated human-level performance on various complex tasks [19]– [21].",
      "startOffset" : 225,
      "endOffset" : 229
    }, {
      "referenceID" : 21,
      "context" : "While several recent works have applied deep RL to motion planning [22], [23], they are mainly focused on single agent navigation in unknown static environments, and with an emphasis on computing control inputs directly from raw sensor data (e.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 22,
      "context" : "While several recent works have applied deep RL to motion planning [22], [23], they are mainly focused on single agent navigation in unknown static environments, and with an emphasis on computing control inputs directly from raw sensor data (e.",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 13,
      "context" : "extends the Collision Avoidance with Deep Reinforcement Learning framework (CADRL) [14] to characterize and induce socially aware behaviors in multiagent systems.",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 17,
      "context" : "It has been widely observed that humans tend to follow simple navigation norms to avoid colliding with each other, such as passing on the right and overtaking on the left [18].",
      "startOffset" : 171,
      "endOffset" : 175
    }, {
      "referenceID" : 1,
      "context" : "g, speed, smoothness), which can vary significantly among different individuals [2].",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 9,
      "context" : "For instance, an intuitive pairwise collision avoidance rule [10] can cause simulated agents moving in the same direction to form lanes in long corridors.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 1,
      "context" : "teleoperation of a robot) tend to be cooperative and timeefficient [2], [16].",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 15,
      "context" : "teleoperation of a robot) tend to be cooperative and timeefficient [2], [16].",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 13,
      "context" : "The CADRL work [14] trained a two-agent network with three fully connected hidden layers, and used a minimax scheme for scaling up to multiagent (n > 2) scenarios.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 13,
      "context" : "is outlined in Algorithm 1, which follows similarly as in [14], [19].",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 18,
      "context" : "is outlined in Algorithm 1, which follows similarly as in [14], [19].",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 13,
      "context" : "Compared with CADRL [14], two important modifications are introduced in the training process.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 23,
      "context" : "random test cases with sizes and velocities similar to that of normal pedestrians [25], such that r ∈ [0.",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 5,
      "context" : "avoidance (ORCA) [6] algorithm – a reactive, rule-based method that computes a velocity vector based on an agent’s",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 24,
      "context" : "Pedestrian detection and tracking is performed by combining Lidar’s pointcloud data with camera images [26].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 25,
      "context" : "pedestrian are estimated by clustering the pointcloud data [27].",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 1,
      "context" : "The estimated radius includes a buffer (comfort) zone as reported in [2], [16].",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 15,
      "context" : "The estimated radius includes a buffer (comfort) zone as reported in [2], [16].",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 13,
      "context" : "Thus, ORCA can generate shortsighted actions and oscillatory paths (see [14] for a detailed explanation).",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 5,
      "context" : "2 ORCA [6] 0.",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 13,
      "context" : "CADRL [14] 0.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 5,
      "context" : "4 ORCA [6] 0.",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 13,
      "context" : "CADRL(minimax) [14] 0.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 26,
      "context" : "Motion planning uses an integration of a diffusion map [28] for finding global paths and SA-CADRL for local collision avoidance.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 23,
      "context" : "2m/s, which is approximately the average human walking pace [25], the vehicle maintained safe distance to the pedestrians and generally respected social norms.",
      "startOffset" : 60,
      "endOffset" : 64
    } ],
    "year" : 2017,
    "abstractText" : "For robotic vehicles to navigate safely and efficiently in pedestrian-rich environments, it is important to model subtle human behaviors and navigation rules. However, while instinctive to humans, socially compliant navigation is still difficult to quantify due to the stochasticity in people’s behaviors. Existing works are mostly focused on using featurematching techniques to describe and imitate human paths, but often do not generalize well since the feature values can vary from person to person, and even run to run. This work notes that while it is challenging to directly specify the details of what to do (precise mechanisms of human navigation), it is straightforward to specify what not to do (violations of social norms). Specifically, using deep reinforcement learning, this work develops a time-efficient navigation policy that respects common social norms. The proposed method is shown to enable fully autonomous navigation of a robotic vehicle moving at human walking speed in an environment with many pedestrians.",
    "creator" : "LaTeX with hyperref package"
  }
}