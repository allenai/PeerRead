{
  "name" : "1511.01258.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "LEARN ON SOURCE, REFINE ON TARGET: A MODEL TRANSFER LEARNING FRAMEWORK WITH RANDOM FORESTS 1 Learn on Source, Refine on Target: A Model Transfer Learning Framework with Random Forests",
    "authors" : [ "Noam Segev", "Maayan Harel", "Shie Mannor", "Koby Crammer", "Ran El-Yaniv" ],
    "emails" : [ "rani]@cs.technion.ac.il", "maayanga@ee.technion.ac.il", "shie@ee.technion.ac.il", "koby@ee.technion.ac.il" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms—Transfer learning, model transfer, random forest, decision tree\nF"
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "CONSIDER a software company selling a trained pre-dictive model M to a community of consumers. The generic classifier M was constructed using a very large and expensive dataset D. While the generic classifier is very accurate over the “source” D, each of the individual consumers needs to apply M in a specific “target” context D′ with its own idiosyncrasies and noise parameters. The manufacturer can neither share its dataset D with its consumers nor afford to retrain an individual model for each of them (based on both D and D′). What would be a good approach to adapt the model M to each individual context using a relatively small training set?\nIn this paper we focus on the setting of model transfer (MT) whereby the adaptation of a given source model to a target domain relies on a relatively small training set from the target. In contrast to general transfer learning frameworks (such as instance transfer, see Sec. 4.3), in model transfer no training examples are available from the source domain during adaption for whatever reason, e.g., storage capacity or data privacy. This limitation makes model transfer a restrictive and more challenging type of transfer learning.\nThere are numerous practical scenarios where model transfer is essential, whereas the source/target data sharing required by standard transfer learning methods is impermissible. For example, Microsoft’s Kinect performs human pose recognition using random forests [1], which can be improved with user-specific training data to accommodate environmental changes (e.g., lighting and furniture) or mechanical ones. One of our experimental settings addresses\n• N. Segev and R. El-Yaniv are with the Department of Computer Sciences, Technion-Israel Institute of Technology, Israel E-mail: [nsegev, rani]@cs.technion.ac.il • M. Harel, S. Mannor and K. Crammer are with the Department of Electrical Engineering, Technion-Israel Institute of Technology, Israel E-mail: [maayanga,shie,koby]@ee.technion.ac.il\na conceptually similar case. In general, when the model manufacturer cannot send the data to the model consumer or the consumer cannot send the data to the manufacturer, be it due, for example, to memory limitations (at the consumer’s box) or computational/communication constraints (at/to the manufacturer’s site), model transfer is interesting as a potentially viable solution. It is certainly conceivable that model transfer for machine learning will be extremely widespread in the near future.\nOur own motivation to consider model transfer arose in a collaborative project with a leading cyber fraud detection company dealing with online bank transactions. The company created a powerful fraud detection model based on data collected from a number of banks (the “source” domain). However, new clients (banks) operate in different contexts (the “target” domains), where the type of fraud committed might differ from the generic frauds, due to variations in transaction protocols, geo-demographics and other factors. Due to regulations and secrecy requirements, the company is forbidden to share its dataset with its clients, and many of the clients were forbidden to share their own datasets with the company.\nWhile MT isn’t new, no single set of assumptions exist that define the model transfer setting, and thus existing model transfer techniques vary in their approaches. However, model transfer techniques typically resort to regularizing the learning of the target domain using the model learned for the source domain. This can be achieved, e.g., by using a biased regularizer [2], [3], [4], [5], or aggregating multiple source-target predictors [6], [7], [8]. A potential drawback of this regularization paradigm is its limited capacity to accommodate local changes between the source and target distributions, as these techniques typically focus on optimizing a global regularization.\nIn contrast, the techniques we developed emphasize simple model transformations based on local (and greedy) changes. We propose novel model transfer techniques that\nar X\niv :1\n51 1.\n01 25\n8v 2\n[ cs\n.L G\n] 8\nN ov\n2 01\n5\nrely on decision trees (DTs). As non-linear models, DTs can excel in learning non-linear decision rules, and their hierarchical structure enables detection and accommodation of non-linear transformations from source to target. Our methods are motivated by two frequently occurring transformations between the source and target domains: (1) translations (shifts) of the distributions of individual features, and (2) transformations in which a set of features needs to be refined or made coarser to fit the target problem. Each of the two DT techniques we propose is designed to capture one of these types.\nIn general, when dealing with DT learning, one has to carefully guard against overfitting. The two common techniques to regularize DTs are pruning and voting ensembles [9], [10]. Pruning is a technique to reduce the size of an existing decision tree by replacing internal nodes in the tree with leaf nodes. By reducing the tree size, one reduces the complexity of the classifier and hopefully removes sections of the tree that were based on a few noisy samples. However, we utilize voting ensembles, whereby multiple trees are generated and a forest is built by applying our DT induction algorithms.\nEach of these forests alone can be used for transfer learning, but we observed that the two methods tend to excel in different problems. While a judicious use of these algorithms based on prior knowledge of the problem at hand may suffice, we propose to create a diverse ensemble consisting of the union of the models generated by both methods. The resulting algorithm is capable of modeling complex, real world, source-to-target transformations, while performing better than, or almost as well as, its underlying constituents in most cases. This union algorithm is relatively easy to implement, requires modest hyper-parameter tuning, can effectively exploit intensive computational resources to handle large-scale problems, and improves stateof-the-art performance on a range of problems.\nAfter introducing our learning setup in Sec. 2, we present the two new algorithms in Sec. 3 and provide insights into the algorithms’ strengths and weaknesses using synthetic examples. In Sec. 4, we present extensive experiments over a number of datasets. The results demonstrate the effectiveness of our method, which often outperforms several stateof-the-art transfer learning methods. Related work is discussed in Sec. 6, followed by discussion about the advantage of our methods can be found in Sec. 5."
    }, {
      "heading" : "2 PRELIMINARY DEFINITIONS",
      "text" : "A domain D = (X ,Y, P ) consists of an r-dimensional feature space X , a label space Y , and a probability distribution P (x, y), where x ∈ X is the feature vector, and y ∈ Y . In supervised model transfer learning, we are given two domains: a source domain, DS = (XS ,YS , PS), and a target domain, DT = (XT ,YT , PT ). Given a loss function ` : Y × Y → R+, a source prediction function f : XS → YS and (typically limited) target training set ST drawn i.i.d. from DT , our objective is to learn a function f ∈ FT : XT → YT with low risk R(f) = EPT (x,y){`(f(x), y)} on the target domain.\nDifferent transfer learning models have different restrictions on the relationship between the source and the target domains. Our work focuses on inductive transfer learning,\nAlgorithm 1: Structure Expansion Reduction (SER) Input: Node v, labeled samples Sv Output: Node v % Expand leaves: if d(v) = 0 then v ← Build Tree (Sv) return v\nend if % Recurse over child nodes: for vi ∈ {v1, . . . , vn} do\nStructure Expansion Reduction (vi, Svi) end for % Reduce current node: if leafError(v, (Sv)) < subtreeError(v, (Sv)) then\nfor i ∈ d(v) do deleteNode(vi) end for d(v)← 0; y(v)← arg max\ny |{(·, y) ∈ Sv}|\nend if return v\na setting in which one assumes that both source and target tasks share the same features and label spaces, i.e., XS ,XT ⊆ X and Y = YS = YT . Clearly, the marginal distribution of the features may differ between the domains. This setting is quite common, both in research literature and in real world applications of transfer learning, but is only one of a few existing approaches [11]. The presented framework is suitable for both binary and multi-class classification tasks where ` is the zero-one loss function."
    }, {
      "heading" : "2.1 Random Forests Models:",
      "text" : "Our algorithms are based on standard Random Forests (RFs) [12]. We use the following notations for a tree in the forest: Each tree node v has an out-degree d(v) and its children are denoted v1, . . . , vd(v). A leaf node v is associated with a single decision value in Y , denoted y(v). An internal (nonleaf) node v is associated with a single feature φ(v), and for a numeric feature it is also associated with a numeric threshold τ(v). Classification of a sample is done based on the leaf that sample reaches, i.e., the leaf at the end of the path in the tree the sample will follow, and for each node u along this path we say that x “arrives” at u."
    }, {
      "heading" : "3 ALGORITHMS",
      "text" : "We now describe our two algorithms, SER and STRUT, for refitting trees to the target domain."
    }, {
      "heading" : "3.1 Structure Expansion/Reduction",
      "text" : "The structure expansion/reduction (SER) algorithm pairs two local transformations of a decision tree structure: expansion and reduction. In the expansion transformation, we specialize rules induced over the source data to the target data. In reduction we perform the opposite operation, i.e., generalize rules induced over the source data.\nInitially, a random forest is induced using the source data SS . The SER algorithm begins by calculating for each\nnode v the set STv of all labeled points in the target data S T that reaches v. Then, each leaf v is expanded to a full tree with respect to the sample set STv . Finally, for each internal node v, the algorithm — working bottom-up on the tree — attempts to perform a structure reduction as follows. Two types of errors are defined for node v with respect to STv . The first, the subtree error, is the empirical error of the subtree whose root is v. The second, the leaf error, is defined to be the empirical error on v if it were to be pruned into a leaf. If the leaf error is smaller than the subtree error, the subtree is pruned into a leaf node. The decision value at each leaf of the modified DT is obtained using the target (empirical) distribution.\nA pseudo-code of this algorithm is presented in Algorithm 1. Note that the recursive calls are equivalent to depthfirst traversal, with expansion performed whenever a leaf is reached, followed by a reduction step upon the completion of all recursive calls to an internal node’s children."
    }, {
      "heading" : "3.1.1 Visual Illustration of the SER Algorithm",
      "text" : "The SER algorithm applies two local transformations on a given decision tree. To gain some intuition about its operation, we exemplify these transformations. In Figures (1a,1b) we depict two simple domains (classification problems). The DTs learned for each domain are illustrated in Figure (1c). A standard DT induced over (1a) resulting in the LHS tree in Figure (1c) can be easily transferred to domain (1b) using the expansion operation (applied by SER), resulting in the RHS tree of Figure (1c) . Similarly, the tree induced for (1b) can be transferred to (1a) using the reverse, reduction operation.\nIt is obvious that a single classifier can describe two identical domains. Therefore, as one domains drifts, the changes can be captured via small modifications to the tree structure. The given example demonstrates this simple and intuitive observation, showing the high similarity between tree models. As the concepts drift further apart, iterative SER transformations can capture the new domain while maintaining a high degree of correlation between the unchanged similar sections of the domains."
    }, {
      "heading" : "3.1.2 Logical Regularization in SER",
      "text" : "The SER algorithm is especially designed to first consider expansions and then reductions. In this section we explain the rationale for this design and argue that it serves as a kind of regularization that keeps the resulting target model closer to the source model than it would if reduction were to precede expansion.\nIt is well known that a decision tree is equivalent to a disjunctive normal form (DNF) formula where a single rule τ , which constitutes a root-to-leaf path, is equivalent to a conjunction of literals [13]. Let u = u0, . . . , um be a root-toleaf path in a tree prior to running the SER algorithm, with rule τS corresponding to this path. Let u′ = u′0, . . . , u ′ n be a root-to-leaf path in a tree after running the SER algorithm, with rule τT corresponding to this path. If the path u′ was generated from the path u by a SER expansion step, then ui = u ′ i for 0 ≤ i ≤ m, and we say that rule τT expands rule τS . Similarly, if the path u′ was generated from the path u by a SER reduction step, then ui = u′i for 0 ≤ i ≤ n, and we say that rule τT reduces rule τS .\nFollowing these definitions, we make two observations on the relations between τT and τS .\nLemma 3.1. If rule τT expands rule τS , then τT satisfies τS (i.e., if x ∈ X satisfies τT then it also satisfies τS).\nProof. Let u = u0, . . . , un be the path corresponding to τT and u′ = u′0, . . . , u ′ m the path corresponding to τS . Rule τT is comprised of n literals and rule τS consists of m literals; each literal corresponds to a single node along a path. As τT is a conjunction of literals, x ∈ X satisfies τT if and only if x satisfies all of the n terms in τT . As ui = u′i for 0 ≤ i ≤ m, the m terms of rule τS are among the n terms in rule τT . Thus, if x satisfies all of the n terms of τT , it also satisfies the m terms which appear in both rules, and as x satisfies all of the m terms in τS , x satisfies τS .\nLemma 3.2. If rule τT reduces rule τS , then τS satisfies τT .\nProof. Similar to Proof 3.1.2, where the n terms in rule τT are among the m terms in rule τS .\nFollowing Lemma 3.1 and Lemma 3.2, the operation order of expansion followed by reduction has an interesting property:\nCorollary 3.3. For any rule τT in the transformed tree, there exists a rule τS in the original tree, such that either τT satisfies τS or τS satisfies τT .\nThe property presented in Corollary 3.3 is desirable in our context where we intend to perform local refinements and/or generalizations. In contrast, this property is violated when applying first reduction and then expansion, in which\nAlgorithm 2: Structure Transfer (STRUT) Input: Node v, labeled samples S Output: Node v % Prune unreachable subtree: if (|S| = 0) then\nfor ( i ∈ d(v) ) do deleteNode(vi) end for d(v)← 0 return v\nend if % Update leaf distribution: if (d(v) = 0) then y(v)← arg max\ny |{(·, y) ∈ S}|\nreturn v end if % Refit thresholds for numeric features: if (φ(v) is numeric) then τ1 ← Threshold Selection(S, φ(v), QL(v), QR(v)) DG1 = DG (S, φ(v), τ1, QL(v), QR(v)) τ2 ← Threshold Selection(S, φ(v), QR(v), QL(v)) DG2 = DG (S, φ(v), τ2, QR(v), QL(v)) if (DG1 ≥ DG2) then τ(v)← τ1\nelse τ(v)← τ2 swap(v1, v2)\nend if end if % Run STRUT on sons: for ( i ∈ d(v) ) do\nSTRUT (vi, Svi) end for return v\ncase the resulting model can drift further away from the source model."
    }, {
      "heading" : "3.2 Structure Transfer",
      "text" : "The structure transfer (STRUT) algorithm is motivated by the observation that decision trees for similar problems should exhibit structural similarity. Consider, for example, the similar problems of detecting online fraud in two big banks in two different geo-demographic environments (say one is in the USA and the other is in India). Both problems can be modeled such that they share many of the features and their dependencies (e.g., the feature ‘typical customer transaction size’ should appear in both models). However, the scale of such features and their associated decision thresholds are likely to differ between problems.\nThe STRUT algorithm adapts a DT trained on the source samples to the target samples by discarding all numeric threshold values in the tree and working top-down, selecting a new threshold τ(v) for a node v with a numeric feature using STv , the subset of target examples that reach v. If the algorithm encounters a node v for which STv is empty, v is considered unreachable in the target domain and is pruned. The final decision value at each leaf is computed on the target training data.\nA pseudo-code of the STRUT algorithm is presented in Algorithm 2. Threshold selection, as computed by the ThresholdSelection procedure, is posed as an optimization problem described bellow.\nAny feature φ and threshold τ split any set of (labeled) examples, S, into two subsets, denoted SL,τ and SR,τ . The label distributions over these subsets are denoted QL and QR, respectively. With respect to each decision node (with a corresponding feature φ), STRUT’s goal is to optimize a decision threshold with respect to the target training data. As an unconstrained optimization is not advisable in cases where the target training set is small, we require that the newly optimized threshold for decision node v result in label distributions Q′L and Q ′ R that are similar to QL and QR, the original distributions obtained when training v. To this end, we define the divergence gain (DG) measure, presented in Equation (1), that quantifies the similarity of the resulting distributions obtained for v, with respect to training set STv , to the original distributions, QL and QR. DG relies on the (symmetric) Jensen-Shannon divergence given in Equation (2), where DKL(·) is the familiar Kullback-Leibler divergence and M is the mean distribution, M = 12 (P +Q) [14]. The choice of the Jensen-Shannon divergence is justified by its frequent use as an effective statistic for the twosample problem.\nDG ( STv , φ(v), τ(v), QL, QR ) =\n1−|SL,τ | |STv | JSD(Q′L, QL)− |SR,τ | |STv | JSD(Q′R, QR). (1)\n2JSD (P,Q) = DKL (P ||M) +DKL (Q||M) . (2)\nTo perform threshold selection for feature φ, STRUT uses DG to quantify distributional similarity and the information gain (IG) criterion to measure a threshold’s informative value [15]. For feature φ, STRUT looks for a threshold yielding high similarity between the induced distributions and the original distributions calculated during the tree induction stage. This similarity is restricted to “informative” thresholds where, for any sufficiently small > 0, the IG of threshold x is larger than the IG of any other x′ in the - neighborhood of x, i.e., thresholds that are local maximums of IG. Thus, STRUT’s threshold selection can be formulated as an optimization problem (3).\nMaximize x\nDG ( STv , φ, x,QL, QR ) s.t. x ∈ R\n∀x′ ∈ (x− , x+ ) : IG ( STv , φ, x ) ≥ IG ( STv , φ, x ′) . (3)\nProblem (3) is not convex and we solve it using a line search on a limited number of possible thresholds. We note that the space/time complexities incurred by this optimization are very similar to the space/time complexities incurred when maximizing the IG value during standard tree induction. Note also that in order to calculate the DG value we require node v to retain the distributions QL and QR that were computed during construction.\nIt turns out that in some transfer learning problems features not only change threshold values but, as the concept drifts, they may also change their meaning. For example, in a fraud detection problem the “average transaction size”\nfeature occasionally changes meaning as attackers change strategies to fool the detector. The STRUT algorithm easily accommodates such changes between the source and the target by solving the optimization problem a second time but with the original distributions, QL and QR, reversed. If this switch improves the DG value, STRUT swaps the nodes’ subtrees in conjunction with optimizing the threshold."
    }, {
      "heading" : "3.2.1 On the Use of IG and DG",
      "text" : "As discussed above, the STRUT algorithm relies on both the DG and IG measures to optimize the adapted thresholds. Here we explain the motivation for using this combination of measures. The IG is effective in quantifying the “informativeness” of a threshold. However, IG is oblivious to dependencies enforced by the given structure. In contrast, DG is a global regularization measure that does not account for local gains. The following examples show that each of these measures on its own fails to select an appropriate threshold.\nConsider first the application of IG. Define two simple domains where the feature space, X , is the range [0, 1], and our label space is Y = {±1}. The source and target distributions, Ps and Pt, are taken to be\nPs (x) = { 1 0.4 < x < 0.7 −1 otherwise;\nPt (x) = { 1 0.3 < x < 0.6 −1 otherwise.\nThe induced tree for the source domain is given in Figure 2b along with a plot of the IG values for different thresholds in Figure 2a. Using a restricted variant of the STRUT algorithm on this problem, applied only with the IG measure, will result in a decision stump with a large error rate of ∼ 30%. The reason is that the root threshold is set by the algorithm to 0.6 and the left tree, which is a leaf, will\njust update the returned decision, while the right child will be pruned, because the samples that arrive at this node will all belong to a single label.\nNext we show a simple concept shift problem where DG over-regularizes unless it is mitigated by local considerations. We keep the same feature space as in the previous example (the range [0, 1]) as well as the same label space (Y = {±1}). However, now the source and target distributions, Ps and Pt, are\nPs (x) =  1 0 ≤ x < 0.5 −1 0.5 ≤ x < 0.75\n1 otherwise;\nPt (x) =  1 0 ≤ x < 0.6 −1 0.6 ≤ x < 0.85\n1 otherwise.\nThe induced tree for the source domain, as well as the induced distributions in each node v, are given in Figure 3. Using a variant of the STRUT algorithm now restricted to apply only the DG measure will result in a tree whose error rate is ∼ 10%. The reason is that the root threshold is set by the algorithm to 0.5. The left tree is a leaf, which will result in updating the returned decision of the leaf (i.e., no change actually occurs). However, for the right child, which is a stump, we are faced with a problem consisting of three distinct regions:\n1 0.5 ≤ x < 0.6 −1 0.6 ≤ x < 0.85\n1 otherwise .\nIf STRUT were to use the DG measure on its own, it would choose the threshold with the maximum DG value, which is 0.85, and as the node’s children, which are all leaves, it would simply update the returned decision (i.e., still no change occurs). In this case, it is easy to see that the new tree misclassifies the range 0.5 ≤ x < 0.6.\nIt is not hard to see that in both the above negative examples (for using IG or DG on their own), the transformed trees can achieve 100% accuracy if both the IG and DG measures are used in conjunction, as prescribed by the (unrestricted) STRUT algorithm."
    }, {
      "heading" : "3.3 A MIX Approach",
      "text" : "Our proposed solution is to generate two forests using both SER and STRUT and then define MIX as a voting ensemble\nwhose underlying model is the union of all the trees in these forests. Thus, MIX is a simple majority vote applied over all decision trees transferred by either SER or STRUT. As can be seen below, the resulting MIX ensemble often outperforms both of its constituents. An intuitive explanation for its excellent performance appears in Section 5."
    }, {
      "heading" : "3.4 Numerical Examples and Intuition",
      "text" : "To gain intuition about the relative strengths and weaknesses of the SER and STRUT algorithms, as well as the MIX solution, we consider a number of small synthetic transfer challenges, each representing a controlled transformation between the source and target domains. We present two of these challenges. Eight additional synthetic examples are available in Appendix A.\nEach synthetic example consists of 1, 000 independent trials. The “moving source” transformation demonstrates a source concept that is shifted in the target domain, but retains its geometry between domains. By design, we expect STRUT to excel in this case. In the “mixed boxes” transformation, the source concept is composed of a 50-50 mixture of slightly shifted boxes and the target concept consists of one of these boxes. This problem models a case where the target concept is a kind of refinement of the source concept. One can expect SER to excel in this problem.\nIn Table 2a we depict the concepts learned by STRUT, SER and MIX for the two transformations. The corresponding test errors are presented in Table 2b. Indeed, in these simple cases the algorithms perform as expected. The performance of MIX in these examples is clearly not the average performance of SER and STRUT; in the ‘moving source’ example MIX is a close runner up to the best algorithm, and in ‘mixed boxes’ it is better than both STRUT and SER.\nFrom the additional synthetic examples available in Appendix A, we can see that SER obviously outperforms STRUT in cases where feature correspondence is not maintained between source and target, such as OCR and domains of pixel based images. However, STRUT can easily outperform SER when feature correspondence is maintained, such as in the case of the inversion problem.\nAnother observation from the examples in Appendix A is that MIX can outperform its constituents or at worst be a close second. Furthermore, when MIX is only the second best algorithm, its error rates are not simply the average of both base algorithms but are much closer to the best algorithm. While MIX is a simple ensemble of different algorithms, it is capable of providing the desired beneficial\nresults. Further discussion on this behavior and its causes are found in Section 5."
    }, {
      "heading" : "4 EMPIRICAL EVALUATION",
      "text" : "We evaluated the SER, STRUT and MIX transfer learning algorithms over a number of challenges, first comparing our results with non-transfer learning techniques and trivial tree-based transfer learning baselines and finally competing against other model transfer algorithms.\nWe used the SrcOnly baseline as our first benchmark. Because it represents a trivial approach to transfer learning that utilizes no target data, the model was trained using only the source data. Our second benchmark was the TgtOnly baseline, where we create the target model using target only data. In general, any useful transfer learning method should surpass the SrcOnly baseline. The TgtOnly benchmark is traditionally viewed as a skyline, representing the best possible performance. However, effective transfer learning methods can sometimes surpass the skyline due to clever exploitation of both source and target examples, thus enjoying a larger training sample than that allotted to TgtOnly.\nIn addition, we compared performance to trivial treebased model transfer baselines on the same experimental setup. The relabeling classifier simply updates the leaves of a forest trained on the source examples using the target samples. In the bias approach, the weights in the original forest are changed from a uniform distribution to one which favors trees with lower error rates on the available target training samples. Pruning stands for using the target samples to perform pruning on the original forest, just like the reduction step in our SER algorithm or the pruning technique of the C4.5 algorithm [16].\nFinally, we compared performance to two well-known model transfer algorithms. The first is Adaptive SVM (ASVM) [4], which uses target examples to regularize an SVM model with a Gaussian kernel, trained using source examples only. While ASVM has several extensions, these usually rely on a large set of unlabeled target training data, without which these techniques are similar to ASVM [5], [17]. The second algorithm is consensus regularization [7], which attempts to decrease the classification error by minimizing an entropy based disagreement measure among a set of source-only and target-only models. While the original paper applied the algorithm with underlying logistic regression models, we used random forests, which outperformed the logistic regression application.\n∣∣SS∣∣"
    }, {
      "heading" : "4.1 Datasets",
      "text" : "The effectiveness of transfer learning techniques is of course expected to depend on the degree of relatedness between DS and DT . We generated the source and target sets based on either meaningful splits of existing datasets, or on a transformation of a subset of the dataset. These approaches are common practice in dataset construction for validating transfer models [18]. We used the following data sets, whose properties are presented in Table 2.\nMushroom: This is a publicly available dataset from the UCI Repository [19]. It contains samples of edible and poisonous mushrooms, and the value of the stalk-shape binary feature is used to partition the dataset into two. This technique was used by Dai et al. [18], and the resulting partition is such that the source mushrooms belong to different species than the target mushrooms.\nLetter: Also from the UCI Repository, the letter recognition dataset is partitioned according to the numeric feature x2bar, by thresholding on its median for each letter. This results in source and target distributions of different fonts.\nWine: Publicly available wine quality dataset [20], already partitioned into two; the source domain consists of white wines and the target domain consists of red wines.\nDigits1: The Digits dataset consists of images of handwritten digits. We considered the two binary problems of identifying ‘6’ and ‘9’, each of which is a 180O rotation of the other.\nInversion: The task concerns hand-written digit recognition from the MNIST digit database [21]. Source and target domains are generated from the MNIST database as follows. For the source domain we take 200 images of each digit, sampled uniformly at random, and invert the color of each image. The target data consists of images sampled at random without inversion. Test data was taken from the MNIST database. Examples of inverted digits are shown in Table 3.\nHigher Resolution: This challenge reflects a scenario where we have a lot of source data, from a low-resolution camera, and a small amount of target data, obtained with a high-resolution camera. The source examples are generated by an averaging process that creates lower resolution images, whereby the source image is partitioned into small\n1. http://tx.technion.ac.il/∼omerlevy/datasets/\n‘super-pixels’, each consisting of a disjoint 4 × 4 squares of pixels. The intensity of each super-pixel element is averaged. Test data was taken from the MNIST database. Examples of low resolution digits are shown in Table 3.\nUSPS: Another example of hand-written digit recognition collected under different conditions [22]. The USPS dataset was collected from scans of random letters in a US post office. We generate the domains using the same transformation as that used in the MNIST database, i.e., images are enlarged to 20X20 pixels and placed in a 28X28 image, centered on the center of mass. For this transfer experiment we treat MNIST as the source domain and utilize the MNIST training set as source data.\nLandmine2: The landmine dataset consists of information collected from 29 real mine fields. Each field is represented by 9 features collected using sonar images. 15 of these fields were dense in foliage, while the other 14 came from barren areas. We attempt to use the information from the foliage covered fields to improve mine detection in the barren areas.\nOffice-Caltech3: This dataset is a collection of images of 10 categories from 4 domains. We transfer information from larger domains with higher resolution images, Amazon.com product pages or the Caltech10 collection, and attempt to recognize lower resolution webcam images.\nActivity Recognition: This dataset, collected by Subramanya et al. [23], is a recording of a customized wearable sensor system. The system recorded measurements on 6 users doing various activities, such as walking or running, going up or down the stairs, or simply lingering. We performed the same preprocessing on the data as that performed by Harel and Mannor [24] and treated each ordered pair of users as a source-target pair, totaling 30 possible pairs."
    }, {
      "heading" : "4.2 Experiments and Results",
      "text" : "We set SS to be all available source data, and ST to be 5% (unless specified otherwise) of the target samples, stratified and randomly selected; the rest was used as test data, giving us around 20\n∣∣ST ∣∣ ∼= ∣∣SS∣∣ in almost all datasets. In all cases the models consisted of 50 decision trees. Following Breiman’s work on random forest learning, we consider only a log number of features, selected at random, when performing feature selection (see also Louppe et al. [25]).\n2. http://www.ee.duke.edu/∼lcarin/LandmineData.zip 3. http://www-scf.usc.edu/∼boqinggo/domain adaptation/GFK\nv1.zip\nThe landmine detection and activity recognition tasks exhibit special characteristics. Both tasks have class imbalance, where in the landmines problem only 6% of the examples were positive (mine found), and in the activity problem, running and going up or down the stairs totaled less than 10% of the examples. In these experiments we ascertained that the ratio of classes in the training data was similar to that of the entire target dataset. Moreover, with the severe class imbalance exhibited, error (or accuracy) is no longer an appropriate measurement and can lead to erroneous conclusions [26]. Therefore, in these cases we measured the balanced error rate (BER): BER = 1cΣ c i=1 ei ni\n, where ei and ni are the number of errors and the number of samples in class i respectively, and c is the number of classes.\nWe began by comparing the performance of our algorithms and the two benchmarks. Results of these tests for the SER, STRUT and MIX algorithms are presented in Table 4. For the “inversion” and “high-res” datasets, the table includes performance at 1%, 5% and 10% source-target ratios. For the “activity recognition” task, the table present the best case, worst case and median cases (minimum, maximum and median TGT error, respectively).\nSER often, but not always, outperformed STRUT, while MIX produced a classifier that was best, or close to best, among all the methods, even when one of the underlying forests performed poorly as happened, for example, in the three “inversion” sets. Such results indicate that MIX is robust to inferior performance of one of its constituents4. We note that MIX continuously outperformed the SrcOnly baseline and in many cases matched or outperformed the TgtOnly baseline.\nNext, we compared our algorithms to the previously described model transfer baselines and competing algorithms. We focus our discussion on the MIX algorithm which generally performed well. Following the results provided in Table 4, we note that our MIX algorithm constantly outperformed the relabeling and bias benchmarks. Similarly, with the exception of the wine and landmines experiments, our MIX algorithm outperformed the simple pruning approach.\n4. We ascertained that the good MIX results are not due to “unfair” model complexity conditions, as each base method contains 50 trees, while MIX is a union of them all (100 trees). To this end, we repeated all experiments with STRUT and SER containing 100 trees. No significant performance improvements were recorded due to this modification.\nFinally, we ascertained the superiority of our algorithms over the benchmarks using a t-test with p-value < 0.005 for the relabeling, bias and pruning benchmarks. Our algorithms also show success compared to ASVM and consensus regularization. These results were ascertained as statistically significant using a t-test (p-value < 0.005).\nFigure 4.2 presents the learning curves for the algorithms on the “inversion” and “high-res” datasets. The curves show error as a function of the ratio between source and target sizes. As seen before, our MIX algorithm yielded similar results to the better of the underlying algorithms, matching the error rates of SER in the “high-res” problem and coming close to STRUT in the case of “inversion”. In both cases the MIX algorithm outperformed the TgtOnly benchmark.\nFinally, we would like to comment on the time complexity recorded while performing these experiments. We note that each tree in the forest can be processed independently, allowing for easy parallelism. We have observed that the average model transformation time of the “letter” problem in a serial execution is 3.1s for MIX, 1.6s for consensus regularization, and 11s for ASVM, while on a 10-core machine we saw linear improvement, with MIX taking 0.31s (To the best of our knowledge, there is no parallel version of the consensus regularization algorithm). This advantage of our techniques is clearly visible in Table 5, where the average transfer runtime of our algorithms clearly superior to ASVM transfer. In today’s world of high throughput and massively parallel computing, a forest containing dozens or even hundreds of trees can be trained in almost the same time that it takes to build a single tree."
    }, {
      "heading" : "4.3 Comparing to Instance Transfer Algorithms",
      "text" : "Unlike model transfer, in the instance transfer approach to transfer learning, all source training examples are available during the adaptation to the target. At the outset, this additional information can lead to better performance. In this sense, a comparison of a model transfer algorithm that learns without source examples to an instance transfer algorithm is unfair. Nevertheless, it is interesting and important to understand the benefits and limitations of model transfer methods, and therefore, we conducted a comparative study of our model transfer methods to instance transfer algorithms.\nIn this section we briefly mention our comparison of the MIX algorithm versus instance transfer algorithms. The first is TradaBoost [18], which is applied with random decision trees as the weak learners. Our tests show that the use of random decision trees produces much better results linear SVMs, as suggested by TradaBoost’s authors. The second algorithm tested was TrBagg [27], which initially trains classifiers on bootstrapped bags sampled with replacements from TS ∪ TT and regularizes the ensemble by filtering out classifiers which are overly biased towards the target domain. TrBagg is also applied with random decision trees and for the filtering phase we use the MVT filtering technique, as suggested by the authors. In all experiments we applied TradaBoost and TrBagg with up to 50 iterations. The third algorithm we compared against is the Frustratingly Easy Domain Adaptation (FEDA) [28] meta-algorithm. FEDA generates a new middle-ground domain to train on by transferring the data from both source and target to the middle-ground domain. To compare apples-to-apples we also applied FEDA with a random forest as its underlying algorithm. Finally, we test the Mixed-Entropy (ME) [29] algorithm, a state-of-the-art forest-specific technique which combines source and target training samples using a weighted information gain measure. The results of these experiments are presented in Table 6.\nOur algorithms routinely outperform most other techniques and are competitive with FEDA. Surprisingly, our study shows that MIX is comparable and even competitive\nwith instance transfer algorithms, despite the unfair comparison. In particular, MIX often showed similar results to FEDA and Mixed-Entropy and consistently outperformed TradaBoost TrBagg. For example, the error rates of TradaBoost, TrBagg, FEDA and Mixed-Entropy for the “letter” dataset were 41.7, 29.3, 18.8 and 17.7, respectively. The advantage of MIX over TradaBoost and TrBagg was backed by t-tests with all p-values < 0.01. No statistically significant performance difference could be observed for FEDA and MIX or ME and MIX."
    }, {
      "heading" : "5 CAN WE EXPLAIN THE ADVANTAGE OF MIX?",
      "text" : "Our empirical results indicate that the MIX algorithm performs well even when just one of its constituents gives good results and can moreover outperform each of its constituents. We attribute this behavior to diversity and correlation among the ensemble members. A given tree transformed by the SER algorithm is likely to be different in size than the original tree, as the expansion step will add to the tree depth and the reduction step will reduce the size of some of the branches, while the same tree transformed by the STRUT algorithm is likely to retain its original size but with different thresholds. Thus, the pairwise correlation in the MIX forest between two trees transformed from the same original tree are expected to exhibit low correlation and result in a more diverse forest.\nLet yf(x) denote the classification margin of a soft binary classifier f with respect to a point x whose label is y, i.e., yf(x) > 0 iff f(·) is correct on (x, y). We consider\nthe expected error of an ensemble with weight distribution Q over its members via two risk functions commonly used in PAC-Bayesian literature, the Bayes risk R (BQ (f)), also called risk of the majority vote, and the Gibbs risk R (GQ (f)), found in Equations 4 and 5 respectively.\nR (BQ (f)) = E (x,y)∼D I\n( E\nf∼Q y · f (x) ≤ 0\n) , (4)\nR (GQ (f)) = E (x,y)∼D\n( E\nf∼Q I (f (x) 6= y)\n) . (5)\nWhile it is well known that R (BQ) ≤ 2R (GQ) (e.g., [30], [31], [32]), Germain et al. have shown that with more pairwise “non-correlated” ensemble members (those with a non-positive pairwise covariance of their risk between ensemble members), it is possible to provide the tighter bound found in Corollary 5.1 using the measure of expected disagreement, dQ.\nCorollary 5.1 (Corollary 16 [33]). Given n voters having non-positive pairwise covariance of their risk under a uniform distribution Q, we have R (BQ) ≤ CQ ≤ 1n·(1−2R(GQ))2 where CQ = 1− (1−2·R(GQ)) 2\n1−2·dQ\nand dQ = E (x,y)∼D\n( E\nf1∼Q E f2∼Q I (f1 (x) 6= f2 (x))\n) .\nThus, in the likely case where two trees transformed from the same original tree are “non-correlated”, the bound on the Bayes risk for the MIX forest is nearly halved as n doubles.\nWe now consider the ‘mixed boxes’ experiment presented Section 3.4, where SER is significantly better than STRUT and MIX is even better than SER (Table 2b). We calculated the empirical Gibbs risk of the forests, measuring 0.17 and 0.15 for the STRUT and SER forests respectively. Following the last inequality in Corollary 5.1 we get bounds of 0.14 and 0.13 on the Bayes risk of the STRUT and SER forests, respectively, and a much tighter bound of 0.07 for the MIX forest. These results indicate that the Bayes risk for the MIX algorithm are expected to be lower than those of its constituents, as are the actual test results.\nGermain et al. have also shown the following formulation to the C-bound:\nCQ = 1− (1− 2eQ − dQ)2\n1− 2 · dQ\nwhere\neQ = E (x,y)∼D\n( E\nf1∼Q E f2∼Q I (f1 (x) 6= y) I (f2 (x) 6= y) ) is a measure of expected joint error [33]. We measured the empirical joint error and disagreement, noting that the empirical joint error of the three algorithms was similar while the disagreement measure of MIX was much higher than that of SER or STRUT, which resulted in lower CQ bound for the MIX algorithm.\nWe also informally argue that the attractive property of the MIX advantage over its constituents is related to the distribution of empirical pointwise classification margins in cases where SER and STRUT disagree in their predictions. In Figure 5 we plot the cumulative distribution functions (CDFs) of empirical margins obtained by SER, STRUT and MIX for the same ‘mixed boxes’ experiment when SER and STRUT disagree. The advantage of MIX and SER here is evident by their lower CDF values at the origin in Figure. 5. Schapire et. al. addressed these circumstances and related better generalization to better empirical margin profiles, as given in Theorem 5.2:\nTheorem 5.2 (Theorem 1 [34]). let D be a domain over X × {−1, 1} with distribution , let S be a sample of m examples chosen independently at random according to P . Assume that the base-classifier space H is finite and let δ > 0. Then with probability at least 1 − δ over the random choice of the training set S, every weighted average function f satisfies the following bound for all θ > 0:\np (x,y)∼D\n[yf (x) ≤ 0] ≤\np (x,y)∼S\n[yf (x) ≤ θ]+O (\n1√ m\n( logm log|H| θ2 + log ( 1 δ ))1/2) .\nIntuitively, when an ensemble algorithm is correct, its underlying classification margins tend to be high and correlated, and when it is wrong, its underlying margins tend to be more dispersed as the result of low pairwise correlation. Combining STRUT and SER in MIX benefits from some correctly performing constituents within the erroneous ensemble. In other words, as STRUT and SER are only weakly correlated, MIX benefits when combining them."
    }, {
      "heading" : "6 RELATED WORK",
      "text" : "The generic title “transfer learning” encompasses quite a few different paradigms. As noted by Levy and Markovitch [35], such paradigms are motivated by (implicit or explicit) modeling or process assumptions. For example, some paradigms, such as “feature transfer”, are motivated by assumptions on the linkage between source and target domains (e.g., features at the target obtained by certain mappings applied on the source features). The survey by Pan et al. [11] identifies the following settings, which are not mutually exclusive.\nModel Transfer: This setting, within which the present work resides, assumes that a good predictor for the source has been learned, resulting in an attempt to adapt the model to the target problem using a training set from the target domain. Model transfer techniques are effective when a similar inductive bias performs well for the related tasks or\nwhen source examples are impossible to retain or distribute. Present model transfer model methods rely on a biased regularizer [3], [4], [36], [37], [38], on aggregating multiple source-target predictors [6], [7], [8], [39], utilizing model parameter transfer as priors [40], [41], [42], or by feature weight estimation [43], [44].\nInstance Transfer: In this setting one assumes certain instances of the source data can be used as examples in the target domain. Under this assumption, it is better to take some of the source data “as is”, and the problem reduces to identifying the relevant instances and ignoring the irrelevant ones, using a process of elimination or weighting. Boosting based instance weighing is common practice in this category [18], [45], [46], [47], as is instance elimination (and sub-sampling) [27], [48], but other techniques exist for utilizing the source information in different ways [28], [29], [49], [50].\nFeatures Transfer: Assuming some partial relation between the source and target features exists, algorithms working in this setting attempt to learn a feature mapping or weighing. These techniques represent an attempt to find the “common denominators” of the learning tasks, matching features, or combinations of features, to identify meaning in partial information. Standard techniques to address this problem include norm optimization [24], [51], [52] and manipulating and combining features [35], [42], [53]\nDomain Adaptation (DA) and Multi-Task Learning (MTL): In domain adaptation the difference between the domains is the result of different feature and labeling spaces; however, DA is typically considered within a semisupervised context where an abundance of unlabeled data is available as well [54], [55], [56], while in MTL the goal is to produce a good hypothesis for several related learning problems simultaneously [57]. Some notable approaches for these settings are based on similarity to a common predictor [28], [58], [59], [60], finding a shared representation [50], [61], [62], [63], [64] or a shared subspace [65], [66], [67], [68], as well as probabilistic approaches [69], [70], [71], [72].\nSemi-supervised Transfer: source and target domains are the same and source data includes only unlabeled examples [2], [5], [17], [51], [62], [73], [74], [75]. Semi-supervised transfer is very attractive in application areas such as machine vision, video event detection and text analysis.\nFor a comprehensive review of these fields the reader is referred to the works of Pan and Yang [11] and Jiang [56]."
    }, {
      "heading" : "6.1 Transfer Learning using Decision Trees",
      "text" : "Most early transfer learning methods were based on neural networks, and while SVMs and ensemble techniques have become prominent in this field, DT models are still underexplored in this setting. Of the few tree-based techniques researched, only the work by Won et al. operates in our model transfer setting. Specifically, Won et al. proposed a simple technique to update an existing tree trained only on source samples using target samples [76]. Their approach resembled a batch iterative learning technique which relies solely on iterative expansion steps. This technique does not consider any refitting of numeric feature thresholds.\nAdaptive DTs and stream sub-sampling have been used in data streams to handle massive, high-speed streams [77],\n[78], [79] as well as concept drift [80], [81], [82], modifying the DT as new samples arrive. However, there are no explicit source and target distributions in this setting, but a distribution that incrementally changes over time, requiring retaining of original samples and constant modifications to the DTs.\nFinally, in MTL, Faddoul et al. presented a variation of AdaBoost with decision stumps fitted to multiple tasks [83]. The same authors later applied boosting with DTs while using a modified information gain (IG) criterion [84]. Another approach builds an ensemble by combining multiple random DTs, where task-driven splits are added in each tree, in addition to ordinary feature splits [85]. In this manner, the trees may contain branches uniquely dedicated to particular subsets of tasks."
    }, {
      "heading" : "7 CONCLUDING REMARKS",
      "text" : "Exploiting the modularity and flexibility of decision trees, we designed two model transfer learning algorithms that utilize a model trained over the source domain and effectively adapted it to the target domain using local adjustments of the tree parameters and/or its architecture. Our experiments with synthetic data indicate that the effectiveness of the algorithms varies with the transformation type. Our final MIX algorithm combines the proposed base algorithms and often outperform their underlying constituents. It also achieve performance superior to leading model transfer algorithms and, moreover, are competitive with instance transfer algorithms and even outperform some of them.\nAn attractive feature of the proposed method (and any effective model transfer algorithm) is that the source data can be discarded after training over the source domain, and transferring the models to the target domain can be computed later on, whenever needed. A nice application of this property would be to devise a bank of models computed over a variety of source domains that can later be used to construct models for any related target domain.\nAn open issue is to capture formally and systematically the ramifications of possible source/target transformations over the tree structure. For example, we asserted above that some geometrical transformation of the support of the inputs density can be captured and modeled via threshold changes in a decision tree. Yet it would be interesting to formally map and relate these and other data transformations to the tree adaptation mechanisms.\nFurthermore, our work has not touched upon sample complexity and formal generalization ability. This problem of devising a comprehensive learning theory for transfer learning of decision trees might be contingent upon formally defining an effective model for possible relations between the source and the target."
    }, {
      "heading" : "APPENDIX A OTHER NUMERICAL EXAMPLES ON SYNTHETIC DATA",
      "text" : "In Section 3.4 we presented two small synthetic transfer learning challenges to illustrate the behavior of our algorithms. These two examples were selected from a set of 10 problems we synthetically designed to capture simple\ntransformations between the source and target problems. Here we define all 10 of these problems, present the test performance of the algorithms over these problems, and provide details on the experimental protocol used.\nEach challenge is defined via a fixed binary concept over the source domain and a transformation that maps the concept to the target domain. The concepts and transformations are defined below for each challenge. All challenges were defined such that X is the positive unit quadrant in R3 (using numeric features). In all experiments we maintained the relation\n∣∣SS∣∣ = 5 ∣∣ST ∣∣ and took |ST | = 64. In all cases, P (x), the marginal distribution over X , is the uniform distribution over X . Each challenge was randomly repeated 1, 000 times and each test error reported was computed as the averages over a test set consisting of 10, 000 random target domain points. This large number of trials ensured statistically significant comparisons of the results.\nEach challenge is defined via a binary “concept” and a transformation. The concept is the source region where points are labeled ’+’. In most cases the concept is a random 3D box in the source domain, XS , whose volume is 25% of vol(XS). Below we refer to such a box as a ”standard random box”. We define the following transformations:\n1) In the source/target mix, the source concept is a 50-50 mixture of two standard random boxes, and the target concept is a single random box among the two. 2) In source inversion the source concept is a standard random box. We represent this box by (α0, α1, α2) and (β0, β1, β2), such that a point x = (x0, x1, x2) is labeled as positive in the source domain (i.e., it is in the box) iff αi ≤ xi ≤ βi, i = 0, 1, 2. Then , the target concept is defined such that a target point, x = (x0, x1, x2), is labeled positive iff xi < αi or βi < xi, i = 0, 1, 2. 3) In moving source the source concept is a standard random box and the target concept is obtained by a\nrandom displacement of the source box along each of the axes that still keeps the displaced box within XT . 4) In the expanding and shrinking source challenges, the source concept is a standard random box. In the expanding challenge we expand the source box so that its volume is doubled in the target domain, and in the shrinking challenge its volume is halved. 5) In axis-swap the source concept is a standard random box and the target concept is obtained by swapping two randomly selected dimensions (among the three). 6) In noisy target the source concept is a standard random box and the target concept is the same box but each point inverts its label with probability 0.25. 7) The noisy source challenge is precisely the inverse of the noisy target challenge where the target concept is a standard random box and the source concept is its noisy distortion. 8) In the rotated source challenge, a standard random box in the source is rotated about a random vector by a random angle using a standard 3D rotation matrix. 9) In the fish-eye transformation challenge, each point is represented within a spherical coordinate system, namely, x = (r, θ, φ) where 0 ≤ θ ≤ π4 and 0 ≤ φ ≤ π2 . For every source point xs = (rs, θs, φs), there exists a maximum value rm such that (rm, θs, φs) is still inside the source feature space XS ; thus, we transfer the point xs from the source domain to the point xt = (rt, θs, φs) in the target domain, such that rt =\nrs rm . This is of course a deterministic transformation applied to each standard random concept (box) in the source.\n10) Our final challenge is the refined sine boundary, in which the source concept is defined by a sine wave manifold that changes frequency and amplitude in the target. A source point x = {x0, x1, x2} is labeled positive iff 0.5 + 0.05 · sin (4π (x0 + x1)) < x2. Our target domain is defined similarly, but with random frequency 0.25 ≤ φ ≤ 0.5, and a random amplitude 0 ≤ a ≤ 0.5. Thus, a point x = (x0, x1, x2) in the target domain is labeled positive iff 0.5 + a · sin ( 2π φ (x0 + x1) ) < x2.\nIn Table 7 we present the test error results of STRUT, SER and MIX for these challenges."
    } ],
    "references" : [ {
      "title" : "Real-time human pose recognition in parts from single depth images",
      "author" : [ "J. Shotton", "T. Sharp", "A. Kipman", "A. Fitzgibbon", "M. Finocchio" ],
      "venue" : "Communications of the ACM, vol. 56, no. 1, pp. 116–124, 2013.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Logistic regression with an auxiliary data source",
      "author" : [ "X. Liao", "Y. Xue", "L. Carin" ],
      "venue" : "Proceedings of the 22nd International Conference on Machine Learning. ACM, 2005, pp. 505–512.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Personalized handwriting recognition via biased regularization",
      "author" : [ "W. Kienzle", "K. Chellapilla" ],
      "venue" : "Proceedings of the 24th International Conference on Machine Learning (ICML). ACM, 2006, pp. 457–464.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Cross-domain video concept detection using adaptive svms",
      "author" : [ "J. Yang", "R. Yan", "A.G. Hauptmann" ],
      "venue" : "Proceedings of the 15th International Conference on Multimedia. ACM, 2007, pp. 188–197.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Domain adaptation from multiple sources via auxiliary classifiers",
      "author" : [ "L. Duan", "I.W. Tsang", "D. Xu", "T.S. Chua" ],
      "venue" : "Proceedings of the 26th Annual International Conference on Machine Learning, 2009.  LEARN ON SOURCE, REFINE ON TARGET: A MODEL TRANSFER LEARNING FRAMEWORK WITH RANDOM FORESTS  13",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Boosting expert ensembles for rapid concept recall",
      "author" : [ "A. Rettinger", "M. Zinkevich", "M. Bowling" ],
      "venue" : "Proceedings of the 21st National Conference on Artificial Intelligence, vol. 21, no. 1. AAAI, 2006, p. 464.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Transfer learning from multiple source domains via consensus regularization",
      "author" : [ "P. Luo", "F. Zhuang", "H. Xiong", "Y. Xiong", "Q. He" ],
      "venue" : "Proceedings of the 17th ACM Conference on Information and Knowledge Management. ACM, 2008, pp. 103–112.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Kernel-based inductive transfer",
      "author" : [ "U. Rückert", "S. Kramer" ],
      "venue" : "Machine Learning and Knowledge Discovery in Databases. Springer, 2008, pp. 220–233.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Mdl-based decision tree pruning",
      "author" : [ "M. Mehta", "J. Rissanen", "R. Agrawal" ],
      "venue" : "Proceedings of the 1st International Conference on Knowledge Discovery and Data Mining (KDD). AAAI, 1995, pp. 216–221.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "A survey on transfer learning",
      "author" : [ "S.J. Pan", "Q. Yang" ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering, vol. 22, no. 10, pp. 1345–1359, 2010.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Random forests",
      "author" : [ "L. Breiman" ],
      "venue" : "Machine Learning, vol. 45, no. 1, pp. 5–32, 2001.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Divergence measures based on the shannon entropy",
      "author" : [ "J. Lin" ],
      "venue" : "IEEE Transactions on Information Theory, vol. 37, no. 1, pp. 145–151, 1991.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "Programs for Machine Learning",
      "author" : [ "J.R. Quinlan" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1993
    }, {
      "title" : "Domain adaptation from multiple sources: A domain-dependent regularization approach",
      "author" : [ "L. Duan", "D. Xu", "I.W. Tsang" ],
      "venue" : "IEEE Transactions on Neural Networks and Learning Systems, vol. 23, no. 3, pp. 504–518, 2012.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Boosting for transfer learning",
      "author" : [ "W. Dai", "Q. Yang", "G.R. Xue", "Y. Yu" ],
      "venue" : "Proceedings of the 24th International Conference on Machine Learning, 2007.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "UCI machine learning repository",
      "author" : [ "K. Bache", "M. Lichman" ],
      "venue" : "2013. [Online]. Available: http://archive.ics.uci.edu/ml",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Modeling wine preferences by data mining from physicochemical properties",
      "author" : [ "P. Cortez", "A. Cerdeira", "F. Almeida", "T. Matos", "J. Reis" ],
      "venue" : "Decision Support Systems, vol. 47, no. 4, pp. 547–553, 2009.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "The MNIST database of handwritten digits",
      "author" : [ "Y. Lecun", "C. Cortes" ],
      "venue" : "1998. [Online]. Available: http://yann.lecun.com/exdb/ mnist/",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "A database for handwritten text recognition research",
      "author" : [ "J.J. Hull" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 16, no. 5, pp. 550–554, 1994.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Recognizing activities and spatial context using wearable sensors",
      "author" : [ "A. Subramanya", "A. Raj", "J.A. Bilmes", "D. Fox" ],
      "venue" : "Proceedings of the 22nd Annual Conference on Uncertainty in Artificial Intelligence (UAI). AUAI Press, 2006, pp. 494–502.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Learning from multiple outlooks",
      "author" : [ "M. Harel", "S. Mannor" ],
      "venue" : "Proceedings of the 28th International Conference on Machine Learning (ICML). ACM, 2011, pp. 401–408.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Understanding variable importances in forests of randomized trees",
      "author" : [ "G. Louppe", "L. Wehenkel", "A. Sutera", "P. Geurts" ],
      "venue" : "Advances in Neural Information Processing Systems 26, C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Weinberger, Eds. Curran Associates, Inc., 2013, pp. 431–439.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A review on ensembles for the class imbalance problem: bagging-, boosting-, and hybrid-based approaches",
      "author" : [ "M. Galar", "A. Fernandez", "E. Barrenechea", "H. Bustince", "F. Herrera" ],
      "venue" : "IEEE Transactions on Systems, Man, and Cybernetics, Part C: Applications and Reviews, vol. 42, no. 4, pp. 463–484, 2012.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Trbagg: A simple transfer learning method and its application to personalization in collaborative tagging",
      "author" : [ "T. Kamishima", "M. Hamasaki", "S. Akaho" ],
      "venue" : "Proceedings of the 9th International Conference on Data Mining (ICDM). IEEE, 2009, pp. 219–228.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Frustratingly easy domain adaptation",
      "author" : [ "III H. Daumé" ],
      "venue" : "Conference of the Association for Computational Linguistics (ACL), 2007.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Transfer learning decision forests for gesture recognition",
      "author" : [ "N.A. Goussies", "S. Ubalde", "M. Mejail" ],
      "venue" : "Journal of Machine Learning Research, vol. 15, pp. 3667–3690, 2014.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Pac-bayes & margins",
      "author" : [ "J. Shawe-Taylor", "J. Langford" ],
      "venue" : "Proceedings of the 17th Neural Information Processing Systems (NIPS). MIT Press, 2003, pp. 439–446.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Simplified pac-bayesian margin bounds",
      "author" : [ "D. McAllester" ],
      "venue" : "Learning Theory and Kernel Machines. Springer, 2003, pp. 203–215.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Pacbayesian learning of linear classifiers",
      "author" : [ "P. Germain", "A. Lacasse", "F. Laviolette", "M. Marchand" ],
      "venue" : "Proceedings of the 26th Annual International Conference on Machine Learning. ACM, 2009, pp. 353–360.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Risk bounds for the majority vote: From a pac-bayesian analysis to a learning algorithm",
      "author" : [ "P. Germain", "A. Lacasse", "F. Laviolette", "M. Marchand", "J.-F. Roy" ],
      "venue" : "Journal of Machine Learning Research, vol. 16, pp. 787–860, 2015.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Boosting the margin: A new explanation for the effectiveness of voting methods",
      "author" : [ "R.E. Schapire", "Y. Freund", "P. Bartlett", "W.S. Lee" ],
      "venue" : "The Annals of Statistics, vol. 26, no. 5, pp. 1651–1686, 1998.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Teaching machines to learn by metaphors",
      "author" : [ "O. Levy", "S. Markovitch" ],
      "venue" : "Proceedings of the 26th Conference on Artificial Intelligence. AAAI, 2012, pp. 991–997.",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Learning with few examples using a constrained gaussian prior on randomized trees",
      "author" : [ "E. Rodner", "J. Denzler" ],
      "venue" : "Proceedings of the Vision, Modeling, and Visualization Conference (VMV). Pro Universitate, 2008, pp. 159–168.",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Safety in numbers: Learning categories from few examples with multi model knowledge transfer",
      "author" : [ "T. Tommasi", "F. Orabona", "B. Caputo" ],
      "venue" : "Proceedings of the 23rd Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2010, pp. 3081–3088.",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Learning with few examples for binary and multiclass classification using regularization of randomized trees",
      "author" : [ "E. Rodner", "J. Denzler" ],
      "venue" : "Pattern Recognition Letters, vol. 32, no. 2, pp. 244–251, 2011.",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "A model of inductive bias learning",
      "author" : [ "J. Baxter" ],
      "venue" : "J. Artif. Intell. Res.(JAIR), vol. 12, pp. 149–198, 2000.",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Direct transfer of learned information among neural networks",
      "author" : [ "L.Y. Pratt", "J. Mostow", "C.A. Kamm", "A.A. Kamm" ],
      "venue" : "Proceedings of the 9th National Conference on Artificial Intelligence. AAAI, 1991, pp. 584–589.",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "Learning one more thing",
      "author" : [ "S. Thrun", "T.M. Mitchell" ],
      "venue" : "DTIC Document, Tech. Rep., 1994.",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Modeling transfer relationships between learning tasks for improved inductive transfer",
      "author" : [ "E. Eaton", "M. desJardins", "T. Lane" ],
      "venue" : "Machine Learning and Knowledge Discovery in Databases, pp. 317– 332, 2008.",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Learning with few examples by transferring feature relevance",
      "author" : [ "E. Rodner", "J. Denzler" ],
      "venue" : "Pattern Recognition. Springer, 2009, pp. 252–261.",
      "citeRegEx" : "44",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Boosting for regression transfer",
      "author" : [ "D. Pardoe", "P. Stone" ],
      "venue" : "Proceedings of the 27th International Conference on Machine learning (ICML). ACM, 2010, pp. 863–870.",
      "citeRegEx" : "45",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Boosting for transfer learning with multiple sources",
      "author" : [ "Y. Yao", "G. Doretto" ],
      "venue" : "Proceedings of the 23rd Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2010, pp. 1855–1862.",
      "citeRegEx" : "46",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Selective transfer learning for cross domain recommendation",
      "author" : [ "Z. Lu", "W. Pan", "E.W. Xiang", "Q. Yang", "L. Zhao", "E. Zhong" ],
      "venue" : "Proceedings of the 2013 SIAM International Conference on Data Mining. SDM, 2013, pp. 641–649.",
      "citeRegEx" : "47",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Instance weighting for domain adaptation in NLP",
      "author" : [ "J. Jiang", "C. Zhai" ],
      "venue" : "ACL, vol. 7, 2007, pp. 264–271.",
      "citeRegEx" : "48",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Improving SVM accuracy by training on auxiliary data sources",
      "author" : [ "P. Wu", "T.G. Dietterich" ],
      "venue" : "Proceedings of the 21st International Conference on Machine Learning (ICML). ACM, 2004, pp. 110–117.",
      "citeRegEx" : "49",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Adapting visual category models to new domains",
      "author" : [ "K. Saenko", "B. Kulis", "M. Fritz", "T. Darrell" ],
      "venue" : "Proceedings of the European Conference on Computer Vision (ECCV). Springer, 2010, pp. 213– 226.",
      "citeRegEx" : "50",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Self-taught learning: transfer learning from unlabeled data",
      "author" : [ "R. Raina", "A. Battle", "H. Lee", "B. Packer", "A.Y. Ng" ],
      "venue" : "Proceedings of the 24th International Conference on Machine Learning (ICML). ACM, 2007, pp. 759–766.",
      "citeRegEx" : "51",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Multi-task feature learning",
      "author" : [ "A. Evgeniou", "M. Pontil" ],
      "venue" : "Advances in Neural Information Processing Systems, vol. 19, p. 41, 2007.",
      "citeRegEx" : "52",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Transfer learning via dimensionality reduction",
      "author" : [ "S.J. Pan", "J.T. Kwok", "Q. Yang" ],
      "venue" : "Proceedings of the 23rd Conference on Artificial Intelligence. AAAI, 2008, pp. 677–682.",
      "citeRegEx" : "53",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Domain adaptation for statistical classifiers.",
      "author" : [ "H. Daumé III", "D. Marcu" ],
      "venue" : "J. Artif. Intell. Res.(JAIR),",
      "citeRegEx" : "54",
      "shortCiteRegEx" : "54",
      "year" : 2006
    }, {
      "title" : "A theory of learning from different domains",
      "author" : [ "S. Ben-David", "J. Blitzer", "K. Crammer", "A. Kulesza", "F. Pereira", "J.W. Vaughan" ],
      "venue" : "Machine Learning, vol. 79, no. 1-2.",
      "citeRegEx" : "55",
      "shortCiteRegEx" : null,
      "year" : 0
    }, {
      "title" : "A literature survey on domain adaptation of statistical classifiers",
      "author" : [ "J. Jiang" ],
      "venue" : "URL: http://sifaka.cs.uiuc.edu/jiang4/domainadaptation/survey, 2008.",
      "citeRegEx" : "56",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Regularized multi–task learning",
      "author" : [ "T. Evgeniou", "M. Pontil" ],
      "venue" : "Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2004.",
      "citeRegEx" : "58",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Multi-domain learning by confidence-weighted parameter combination",
      "author" : [ "M. Dredze", "A. Kulesza", "K. Crammer" ],
      "venue" : "Machine Learning, vol. 79, no. 1-2, pp. 123–149, 2010.",
      "citeRegEx" : "59",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Multiclass transfer learning from unconstrained priors",
      "author" : [ "L. Jie", "T. Tommasi", "B. Caputo" ],
      "venue" : "International Conference on Computer Vision (ICCV). IEEE, 2011, pp. 1863–1870.",
      "citeRegEx" : "60",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Domain adaptation with structural correspondence learning",
      "author" : [ "J. Blitzer", "R. McDonald", "F. Pereira" ],
      "venue" : "Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pp. 120–128.",
      "citeRegEx" : "61",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Cross-domain learning methods for high-level visual concept classification",
      "author" : [ "W. Jiang", "E. Zavesky", "S.-F. Chang", "A. Loui" ],
      "venue" : "Proceedings of the 15th IEEE International Conference on Image Processing (ICIP), 2008.",
      "citeRegEx" : "62",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Exploiting web images for event recognition in consumer videos: A multiple source domain adaptation approach",
      "author" : [ "L. Duan", "D. Xu", "S.-F. Chang" ],
      "venue" : "Computer Vision and Pattern Recognition (CVPR). IEEE, 2012, pp. 1338–1345.",
      "citeRegEx" : "63",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Unsupervised visual domain adaptation using subspace alignment",
      "author" : [ "B. Fernando", "A. Habrard", "M. Sebban", "T. Tuytelaars" ],
      "venue" : "International Conference on Computer Vision (ICCV). IEEE, 2013, pp. 2960–2967.",
      "citeRegEx" : "64",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Convex multi-task feature learning",
      "author" : [ "A. Argyriou", "T. Evgeniou", "M. Pontil" ],
      "venue" : "Machine Learning, vol. 73, no. 3, pp. 243–272, 2008.",
      "citeRegEx" : "65",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "A framework for learning predictive structures from multiple tasks and unlabeled data",
      "author" : [ "R.K. Ando", "T. Zhang" ],
      "venue" : "The Journal of Machine Learning Research, vol. 6, pp. 1817–1853, 2005.",
      "citeRegEx" : "66",
      "shortCiteRegEx" : null,
      "year" : 1817
    }, {
      "title" : "Taking advantage of sparsity in multi-task learning",
      "author" : [ "K. Lounici", "M. Pontil", "A. Tsybakov", "S. Geer" ],
      "venue" : "Proceedings of the 22nd Annual Conference on Learning Theory (COLT). ACL, 2009.",
      "citeRegEx" : "67",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Unsupervised domain adaptation by domain invariant projection",
      "author" : [ "M. Baktashmotlagh", "M.T. Harandi", "B.C. Lovell", "M. Salzmann" ],
      "venue" : "International Conference on Computer Vision (ICCV). IEEE, 2013, pp. 769–776.",
      "citeRegEx" : "68",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Learning to learn with the informative vector machine",
      "author" : [ "N.D. Lawrence", "J.C. Platt" ],
      "venue" : "Proceedings of the 21st International Conference on Machine Learning (ICML). ACM, 2004, p. 65.",
      "citeRegEx" : "69",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Hierarchical bayesian modelling with gaussian processes",
      "author" : [ "A. Schwaighofer", "V. Tresp", "K. Yu" ],
      "venue" : "Advances in Neural Information Processing Systems 17, 2005.",
      "citeRegEx" : "70",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Learning gaussian processes from multiple tasks",
      "author" : [ "K. Yu", "V. Tresp", "A. Schwaighofer" ],
      "venue" : "Proceedings of the 22nd International Conference on Machine Learning (ICML). ACM, 2005, pp. 1012– 1019.",
      "citeRegEx" : "71",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Multi-task Gaussian process prediction.",
      "author" : [ "E.V. Bonilla", "K.M. Chai", "C.K. Williams" ],
      "venue" : "in NIPS,",
      "citeRegEx" : "72",
      "shortCiteRegEx" : "72",
      "year" : 2007
    }, {
      "title" : "Domain transfer svm for video concept detection",
      "author" : [ "L. Duan", "I.W. Tsang", "D. Xu", "S.J. Maybank" ],
      "venue" : "Computer Vision and Pattern Recognition (CVPR). IEEE, 2009, pp. 1375–1381.",
      "citeRegEx" : "73",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Domain transfer multiple kernel learning",
      "author" : [ "L. Duan", "I.W. Tsang", "D. Xu" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 34, no. 3, pp. 465–479, 2012.",
      "citeRegEx" : "74",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Visual event recognition in videos by learning from web data",
      "author" : [ "L. Duan", "D. Xu", "I.-H. Tsang", "J. Luo" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 34, no. 9, pp. 1667–1680, 2012.",
      "citeRegEx" : "75",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Transfer learning in decision trees",
      "author" : [ "J. won Lee", "C. Giraud-Carrier" ],
      "venue" : "Proceedings of the International Joint Conference on Neural Networks (IJCNN). IEEE, 2007, pp. 726–731.",
      "citeRegEx" : "76",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Mining high-speed data streams",
      "author" : [ "P. Domingos", "G. Hulten" ],
      "venue" : "Proceedings of the 6th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2000, pp. 71–80.",
      "citeRegEx" : "77",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Efficient decision tree construction on streaming data",
      "author" : [ "R. Jin", "G. Agrawal" ],
      "venue" : "Proceedings of the 9th International Conference on Knowledge Discovery and Data Mining (SIGKDD). ACM, 2003, pp. 571–576.",
      "citeRegEx" : "78",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Mining massive data streams",
      "author" : [ "G. Hulten", "P. Domingos", "L. Spencer" ],
      "venue" : "The Journal of Machine Learning Research, 2005.",
      "citeRegEx" : "79",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Mining time-changing data streams",
      "author" : [ "G. Hulten", "L. Spencer", "P. Domingos" ],
      "venue" : "Proceedings of the 7th International Conference on Knowledge Discovery and Data Mining (SIGKDD). ACM, 2001, pp. 97–106.",
      "citeRegEx" : "80",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Decision trees for mining data streams",
      "author" : [ "J. Gama", "R. Fernandes", "R. Rocha" ],
      "venue" : "Intelligent Data Analysis, vol. 10, no. 1, pp. 23–45, 2006.",
      "citeRegEx" : "81",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Learning in environments with unknown dynamics: Towards more robust concept learners",
      "author" : [ "M. Núñez", "R. Fidalgo", "R. Morales" ],
      "venue" : "The Journal of Machine Learning Research, vol. 8, pp. 2595–2628, 2007.",
      "citeRegEx" : "82",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Boosting multi-task weak learners with applications to textual and social data",
      "author" : [ "J.B. Faddoul", "B. Chidlovskii", "F. Torre", "R. Gilleron" ],
      "venue" : "Proceedings of the 9th International Conference on Machine Learning and Applications (ICMLA). IEEE, 2010, pp. 367–372.",
      "citeRegEx" : "83",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Learning multiple tasks with boosted decision trees",
      "author" : [ "J.B. Faddoul", "B. Chidlovskii", "R. Gilleron", "F. Torre" ],
      "venue" : "Machine Learning and Knowledge Discovery in Databases. Springer, 2012, pp. 681–696.",
      "citeRegEx" : "84",
      "shortCiteRegEx" : null,
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "For example, Microsoft’s Kinect performs human pose recognition using random forests [1], which can be improved with user-specific training data to accommodate environmental changes (e.",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 1,
      "context" : ", by using a biased regularizer [2], [3], [4], [5], or aggregating multiple source-target predictors [6], [7], [8].",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 2,
      "context" : ", by using a biased regularizer [2], [3], [4], [5], or aggregating multiple source-target predictors [6], [7], [8].",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 3,
      "context" : ", by using a biased regularizer [2], [3], [4], [5], or aggregating multiple source-target predictors [6], [7], [8].",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 4,
      "context" : ", by using a biased regularizer [2], [3], [4], [5], or aggregating multiple source-target predictors [6], [7], [8].",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 5,
      "context" : ", by using a biased regularizer [2], [3], [4], [5], or aggregating multiple source-target predictors [6], [7], [8].",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 6,
      "context" : ", by using a biased regularizer [2], [3], [4], [5], or aggregating multiple source-target predictors [6], [7], [8].",
      "startOffset" : 106,
      "endOffset" : 109
    }, {
      "referenceID" : 7,
      "context" : ", by using a biased regularizer [2], [3], [4], [5], or aggregating multiple source-target predictors [6], [7], [8].",
      "startOffset" : 111,
      "endOffset" : 114
    }, {
      "referenceID" : 8,
      "context" : "The two common techniques to regularize DTs are pruning and voting ensembles [9], [10].",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 9,
      "context" : "This setting is quite common, both in research literature and in real world applications of transfer learning, but is only one of a few existing approaches [11].",
      "startOffset" : 156,
      "endOffset" : 160
    }, {
      "referenceID" : 10,
      "context" : "Our algorithms are based on standard Random Forests (RFs) [12].",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 11,
      "context" : "DG relies on the (symmetric) Jensen-Shannon divergence given in Equation (2), where DKL(·) is the familiar Kullback-Leibler divergence and M is the mean distribution, M = 1 2 (P +Q) [14].",
      "startOffset" : 182,
      "endOffset" : 186
    }, {
      "referenceID" : 0,
      "context" : "Define two simple domains where the feature space, X , is the range [0, 1], and our label space is Y = {±1}.",
      "startOffset" : 68,
      "endOffset" : 74
    }, {
      "referenceID" : 0,
      "context" : "We keep the same feature space as in the previous example (the range [0, 1]) as well as the same label space (Y = {±1}).",
      "startOffset" : 69,
      "endOffset" : 75
    }, {
      "referenceID" : 12,
      "context" : "5 algorithm [16].",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 3,
      "context" : "The first is Adaptive SVM (ASVM) [4], which uses target examples to regularize an SVM model with a Gaussian kernel, trained using source examples only.",
      "startOffset" : 33,
      "endOffset" : 36
    }, {
      "referenceID" : 4,
      "context" : "While ASVM has several extensions, these usually rely on a large set of unlabeled target training data, without which these techniques are similar to ASVM [5], [17].",
      "startOffset" : 155,
      "endOffset" : 158
    }, {
      "referenceID" : 13,
      "context" : "While ASVM has several extensions, these usually rely on a large set of unlabeled target training data, without which these techniques are similar to ASVM [5], [17].",
      "startOffset" : 160,
      "endOffset" : 164
    }, {
      "referenceID" : 6,
      "context" : "The second algorithm is consensus regularization [7], which attempts to decrease the classification error by minimizing an entropy based disagreement measure among a set of source-only and target-only models.",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 14,
      "context" : "These approaches are common practice in dataset construction for validating transfer models [18].",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 15,
      "context" : "Mushroom: This is a publicly available dataset from the UCI Repository [19].",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 14,
      "context" : "[18], and the resulting partition is such that the source mushrooms belong to different species than the target mushrooms.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "Wine: Publicly available wine quality dataset [20], already partitioned into two; the source domain consists of white wines and the target domain consists of red wines.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 17,
      "context" : "Inversion: The task concerns hand-written digit recognition from the MNIST digit database [21].",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 18,
      "context" : "USPS: Another example of hand-written digit recognition collected under different conditions [22].",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 19,
      "context" : "[23], is a recording of a customized wearable sensor system.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "We performed the same preprocessing on the data as that performed by Harel and Mannor [24] and treated each ordered pair of users as a source-target pair, totaling 30 possible pairs.",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 21,
      "context" : "[25]).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 22,
      "context" : "Moreover, with the severe class imbalance exhibited, error (or accuracy) is no longer an appropriate measurement and can lead to erroneous conclusions [26].",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 14,
      "context" : "The first is TradaBoost [18], which is applied with random decision trees as the weak learners.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 23,
      "context" : "The second algorithm tested was TrBagg [27], which initially trains classifiers on bootstrapped bags sampled with replacements from T ∪ T and regularizes the ensemble by filtering out classifiers which are overly biased towards the target domain.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 24,
      "context" : "The third algorithm we compared against is the Frustratingly Easy Domain Adaptation (FEDA) [28] meta-algorithm.",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 25,
      "context" : "Finally, we test the Mixed-Entropy (ME) [29] algorithm, a state-of-the-art forest-specific technique which combines source and target training samples using a weighted information gain measure.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 26,
      "context" : ", [30], [31], [32]), Germain et al.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 27,
      "context" : ", [30], [31], [32]), Germain et al.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 28,
      "context" : ", [30], [31], [32]), Germain et al.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 29,
      "context" : "1 (Corollary 16 [33]).",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 29,
      "context" : "is a measure of expected joint error [33].",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 30,
      "context" : "2 (Theorem 1 [34]).",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 31,
      "context" : "As noted by Levy and Markovitch [35], such paradigms are motivated by (implicit or explicit) modeling or process assumptions.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 9,
      "context" : "[11] identifies the following settings, which are not mutually exclusive.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 2,
      "context" : "Present model transfer model methods rely on a biased regularizer [3], [4], [36], [37], [38], on aggregating multiple source-target predictors [6], [7], [8], [39], utilizing model parameter transfer as priors [40], [41], [42], or by feature weight estimation [43], [44].",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 3,
      "context" : "Present model transfer model methods rely on a biased regularizer [3], [4], [36], [37], [38], on aggregating multiple source-target predictors [6], [7], [8], [39], utilizing model parameter transfer as priors [40], [41], [42], or by feature weight estimation [43], [44].",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 32,
      "context" : "Present model transfer model methods rely on a biased regularizer [3], [4], [36], [37], [38], on aggregating multiple source-target predictors [6], [7], [8], [39], utilizing model parameter transfer as priors [40], [41], [42], or by feature weight estimation [43], [44].",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 33,
      "context" : "Present model transfer model methods rely on a biased regularizer [3], [4], [36], [37], [38], on aggregating multiple source-target predictors [6], [7], [8], [39], utilizing model parameter transfer as priors [40], [41], [42], or by feature weight estimation [43], [44].",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 34,
      "context" : "Present model transfer model methods rely on a biased regularizer [3], [4], [36], [37], [38], on aggregating multiple source-target predictors [6], [7], [8], [39], utilizing model parameter transfer as priors [40], [41], [42], or by feature weight estimation [43], [44].",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 5,
      "context" : "Present model transfer model methods rely on a biased regularizer [3], [4], [36], [37], [38], on aggregating multiple source-target predictors [6], [7], [8], [39], utilizing model parameter transfer as priors [40], [41], [42], or by feature weight estimation [43], [44].",
      "startOffset" : 143,
      "endOffset" : 146
    }, {
      "referenceID" : 6,
      "context" : "Present model transfer model methods rely on a biased regularizer [3], [4], [36], [37], [38], on aggregating multiple source-target predictors [6], [7], [8], [39], utilizing model parameter transfer as priors [40], [41], [42], or by feature weight estimation [43], [44].",
      "startOffset" : 148,
      "endOffset" : 151
    }, {
      "referenceID" : 7,
      "context" : "Present model transfer model methods rely on a biased regularizer [3], [4], [36], [37], [38], on aggregating multiple source-target predictors [6], [7], [8], [39], utilizing model parameter transfer as priors [40], [41], [42], or by feature weight estimation [43], [44].",
      "startOffset" : 153,
      "endOffset" : 156
    }, {
      "referenceID" : 35,
      "context" : "Present model transfer model methods rely on a biased regularizer [3], [4], [36], [37], [38], on aggregating multiple source-target predictors [6], [7], [8], [39], utilizing model parameter transfer as priors [40], [41], [42], or by feature weight estimation [43], [44].",
      "startOffset" : 158,
      "endOffset" : 162
    }, {
      "referenceID" : 36,
      "context" : "Present model transfer model methods rely on a biased regularizer [3], [4], [36], [37], [38], on aggregating multiple source-target predictors [6], [7], [8], [39], utilizing model parameter transfer as priors [40], [41], [42], or by feature weight estimation [43], [44].",
      "startOffset" : 209,
      "endOffset" : 213
    }, {
      "referenceID" : 37,
      "context" : "Present model transfer model methods rely on a biased regularizer [3], [4], [36], [37], [38], on aggregating multiple source-target predictors [6], [7], [8], [39], utilizing model parameter transfer as priors [40], [41], [42], or by feature weight estimation [43], [44].",
      "startOffset" : 215,
      "endOffset" : 219
    }, {
      "referenceID" : 38,
      "context" : "Present model transfer model methods rely on a biased regularizer [3], [4], [36], [37], [38], on aggregating multiple source-target predictors [6], [7], [8], [39], utilizing model parameter transfer as priors [40], [41], [42], or by feature weight estimation [43], [44].",
      "startOffset" : 221,
      "endOffset" : 225
    }, {
      "referenceID" : 39,
      "context" : "Present model transfer model methods rely on a biased regularizer [3], [4], [36], [37], [38], on aggregating multiple source-target predictors [6], [7], [8], [39], utilizing model parameter transfer as priors [40], [41], [42], or by feature weight estimation [43], [44].",
      "startOffset" : 265,
      "endOffset" : 269
    }, {
      "referenceID" : 14,
      "context" : "Boosting based instance weighing is common practice in this category [18], [45], [46], [47], as is instance elimination (and sub-sampling) [27], [48], but other techniques exist for utilizing the source information in different ways [28], [29], [49], [50].",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 40,
      "context" : "Boosting based instance weighing is common practice in this category [18], [45], [46], [47], as is instance elimination (and sub-sampling) [27], [48], but other techniques exist for utilizing the source information in different ways [28], [29], [49], [50].",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 41,
      "context" : "Boosting based instance weighing is common practice in this category [18], [45], [46], [47], as is instance elimination (and sub-sampling) [27], [48], but other techniques exist for utilizing the source information in different ways [28], [29], [49], [50].",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 42,
      "context" : "Boosting based instance weighing is common practice in this category [18], [45], [46], [47], as is instance elimination (and sub-sampling) [27], [48], but other techniques exist for utilizing the source information in different ways [28], [29], [49], [50].",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 23,
      "context" : "Boosting based instance weighing is common practice in this category [18], [45], [46], [47], as is instance elimination (and sub-sampling) [27], [48], but other techniques exist for utilizing the source information in different ways [28], [29], [49], [50].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 43,
      "context" : "Boosting based instance weighing is common practice in this category [18], [45], [46], [47], as is instance elimination (and sub-sampling) [27], [48], but other techniques exist for utilizing the source information in different ways [28], [29], [49], [50].",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 24,
      "context" : "Boosting based instance weighing is common practice in this category [18], [45], [46], [47], as is instance elimination (and sub-sampling) [27], [48], but other techniques exist for utilizing the source information in different ways [28], [29], [49], [50].",
      "startOffset" : 233,
      "endOffset" : 237
    }, {
      "referenceID" : 25,
      "context" : "Boosting based instance weighing is common practice in this category [18], [45], [46], [47], as is instance elimination (and sub-sampling) [27], [48], but other techniques exist for utilizing the source information in different ways [28], [29], [49], [50].",
      "startOffset" : 239,
      "endOffset" : 243
    }, {
      "referenceID" : 44,
      "context" : "Boosting based instance weighing is common practice in this category [18], [45], [46], [47], as is instance elimination (and sub-sampling) [27], [48], but other techniques exist for utilizing the source information in different ways [28], [29], [49], [50].",
      "startOffset" : 245,
      "endOffset" : 249
    }, {
      "referenceID" : 45,
      "context" : "Boosting based instance weighing is common practice in this category [18], [45], [46], [47], as is instance elimination (and sub-sampling) [27], [48], but other techniques exist for utilizing the source information in different ways [28], [29], [49], [50].",
      "startOffset" : 251,
      "endOffset" : 255
    }, {
      "referenceID" : 20,
      "context" : "Standard techniques to address this problem include norm optimization [24], [51], [52] and manipulating and combining features [35], [42], [53]",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 46,
      "context" : "Standard techniques to address this problem include norm optimization [24], [51], [52] and manipulating and combining features [35], [42], [53]",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 47,
      "context" : "Standard techniques to address this problem include norm optimization [24], [51], [52] and manipulating and combining features [35], [42], [53]",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 31,
      "context" : "Standard techniques to address this problem include norm optimization [24], [51], [52] and manipulating and combining features [35], [42], [53]",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 38,
      "context" : "Standard techniques to address this problem include norm optimization [24], [51], [52] and manipulating and combining features [35], [42], [53]",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 48,
      "context" : "Standard techniques to address this problem include norm optimization [24], [51], [52] and manipulating and combining features [35], [42], [53]",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 49,
      "context" : "Domain Adaptation (DA) and Multi-Task Learning (MTL): In domain adaptation the difference between the domains is the result of different feature and labeling spaces; however, DA is typically considered within a semisupervised context where an abundance of unlabeled data is available as well [54], [55], [56], while in MTL the goal is to produce a good hypothesis for several related learning problems simultaneously [57].",
      "startOffset" : 292,
      "endOffset" : 296
    }, {
      "referenceID" : 50,
      "context" : "Domain Adaptation (DA) and Multi-Task Learning (MTL): In domain adaptation the difference between the domains is the result of different feature and labeling spaces; however, DA is typically considered within a semisupervised context where an abundance of unlabeled data is available as well [54], [55], [56], while in MTL the goal is to produce a good hypothesis for several related learning problems simultaneously [57].",
      "startOffset" : 298,
      "endOffset" : 302
    }, {
      "referenceID" : 51,
      "context" : "Domain Adaptation (DA) and Multi-Task Learning (MTL): In domain adaptation the difference between the domains is the result of different feature and labeling spaces; however, DA is typically considered within a semisupervised context where an abundance of unlabeled data is available as well [54], [55], [56], while in MTL the goal is to produce a good hypothesis for several related learning problems simultaneously [57].",
      "startOffset" : 304,
      "endOffset" : 308
    }, {
      "referenceID" : 24,
      "context" : "Some notable approaches for these settings are based on similarity to a common predictor [28], [58], [59], [60], finding a shared representation [50], [61], [62], [63], [64] or a shared subspace [65], [66], [67], [68], as well as probabilistic approaches [69], [70], [71], [72].",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 52,
      "context" : "Some notable approaches for these settings are based on similarity to a common predictor [28], [58], [59], [60], finding a shared representation [50], [61], [62], [63], [64] or a shared subspace [65], [66], [67], [68], as well as probabilistic approaches [69], [70], [71], [72].",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 53,
      "context" : "Some notable approaches for these settings are based on similarity to a common predictor [28], [58], [59], [60], finding a shared representation [50], [61], [62], [63], [64] or a shared subspace [65], [66], [67], [68], as well as probabilistic approaches [69], [70], [71], [72].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 54,
      "context" : "Some notable approaches for these settings are based on similarity to a common predictor [28], [58], [59], [60], finding a shared representation [50], [61], [62], [63], [64] or a shared subspace [65], [66], [67], [68], as well as probabilistic approaches [69], [70], [71], [72].",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 45,
      "context" : "Some notable approaches for these settings are based on similarity to a common predictor [28], [58], [59], [60], finding a shared representation [50], [61], [62], [63], [64] or a shared subspace [65], [66], [67], [68], as well as probabilistic approaches [69], [70], [71], [72].",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 55,
      "context" : "Some notable approaches for these settings are based on similarity to a common predictor [28], [58], [59], [60], finding a shared representation [50], [61], [62], [63], [64] or a shared subspace [65], [66], [67], [68], as well as probabilistic approaches [69], [70], [71], [72].",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 56,
      "context" : "Some notable approaches for these settings are based on similarity to a common predictor [28], [58], [59], [60], finding a shared representation [50], [61], [62], [63], [64] or a shared subspace [65], [66], [67], [68], as well as probabilistic approaches [69], [70], [71], [72].",
      "startOffset" : 157,
      "endOffset" : 161
    }, {
      "referenceID" : 57,
      "context" : "Some notable approaches for these settings are based on similarity to a common predictor [28], [58], [59], [60], finding a shared representation [50], [61], [62], [63], [64] or a shared subspace [65], [66], [67], [68], as well as probabilistic approaches [69], [70], [71], [72].",
      "startOffset" : 163,
      "endOffset" : 167
    }, {
      "referenceID" : 58,
      "context" : "Some notable approaches for these settings are based on similarity to a common predictor [28], [58], [59], [60], finding a shared representation [50], [61], [62], [63], [64] or a shared subspace [65], [66], [67], [68], as well as probabilistic approaches [69], [70], [71], [72].",
      "startOffset" : 169,
      "endOffset" : 173
    }, {
      "referenceID" : 59,
      "context" : "Some notable approaches for these settings are based on similarity to a common predictor [28], [58], [59], [60], finding a shared representation [50], [61], [62], [63], [64] or a shared subspace [65], [66], [67], [68], as well as probabilistic approaches [69], [70], [71], [72].",
      "startOffset" : 195,
      "endOffset" : 199
    }, {
      "referenceID" : 60,
      "context" : "Some notable approaches for these settings are based on similarity to a common predictor [28], [58], [59], [60], finding a shared representation [50], [61], [62], [63], [64] or a shared subspace [65], [66], [67], [68], as well as probabilistic approaches [69], [70], [71], [72].",
      "startOffset" : 201,
      "endOffset" : 205
    }, {
      "referenceID" : 61,
      "context" : "Some notable approaches for these settings are based on similarity to a common predictor [28], [58], [59], [60], finding a shared representation [50], [61], [62], [63], [64] or a shared subspace [65], [66], [67], [68], as well as probabilistic approaches [69], [70], [71], [72].",
      "startOffset" : 207,
      "endOffset" : 211
    }, {
      "referenceID" : 62,
      "context" : "Some notable approaches for these settings are based on similarity to a common predictor [28], [58], [59], [60], finding a shared representation [50], [61], [62], [63], [64] or a shared subspace [65], [66], [67], [68], as well as probabilistic approaches [69], [70], [71], [72].",
      "startOffset" : 213,
      "endOffset" : 217
    }, {
      "referenceID" : 63,
      "context" : "Some notable approaches for these settings are based on similarity to a common predictor [28], [58], [59], [60], finding a shared representation [50], [61], [62], [63], [64] or a shared subspace [65], [66], [67], [68], as well as probabilistic approaches [69], [70], [71], [72].",
      "startOffset" : 255,
      "endOffset" : 259
    }, {
      "referenceID" : 64,
      "context" : "Some notable approaches for these settings are based on similarity to a common predictor [28], [58], [59], [60], finding a shared representation [50], [61], [62], [63], [64] or a shared subspace [65], [66], [67], [68], as well as probabilistic approaches [69], [70], [71], [72].",
      "startOffset" : 261,
      "endOffset" : 265
    }, {
      "referenceID" : 65,
      "context" : "Some notable approaches for these settings are based on similarity to a common predictor [28], [58], [59], [60], finding a shared representation [50], [61], [62], [63], [64] or a shared subspace [65], [66], [67], [68], as well as probabilistic approaches [69], [70], [71], [72].",
      "startOffset" : 267,
      "endOffset" : 271
    }, {
      "referenceID" : 66,
      "context" : "Some notable approaches for these settings are based on similarity to a common predictor [28], [58], [59], [60], finding a shared representation [50], [61], [62], [63], [64] or a shared subspace [65], [66], [67], [68], as well as probabilistic approaches [69], [70], [71], [72].",
      "startOffset" : 273,
      "endOffset" : 277
    }, {
      "referenceID" : 1,
      "context" : "Semi-supervised Transfer: source and target domains are the same and source data includes only unlabeled examples [2], [5], [17], [51], [62], [73], [74], [75].",
      "startOffset" : 114,
      "endOffset" : 117
    }, {
      "referenceID" : 4,
      "context" : "Semi-supervised Transfer: source and target domains are the same and source data includes only unlabeled examples [2], [5], [17], [51], [62], [73], [74], [75].",
      "startOffset" : 119,
      "endOffset" : 122
    }, {
      "referenceID" : 13,
      "context" : "Semi-supervised Transfer: source and target domains are the same and source data includes only unlabeled examples [2], [5], [17], [51], [62], [73], [74], [75].",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 46,
      "context" : "Semi-supervised Transfer: source and target domains are the same and source data includes only unlabeled examples [2], [5], [17], [51], [62], [73], [74], [75].",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 56,
      "context" : "Semi-supervised Transfer: source and target domains are the same and source data includes only unlabeled examples [2], [5], [17], [51], [62], [73], [74], [75].",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 67,
      "context" : "Semi-supervised Transfer: source and target domains are the same and source data includes only unlabeled examples [2], [5], [17], [51], [62], [73], [74], [75].",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 68,
      "context" : "Semi-supervised Transfer: source and target domains are the same and source data includes only unlabeled examples [2], [5], [17], [51], [62], [73], [74], [75].",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 69,
      "context" : "Semi-supervised Transfer: source and target domains are the same and source data includes only unlabeled examples [2], [5], [17], [51], [62], [73], [74], [75].",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 9,
      "context" : "For a comprehensive review of these fields the reader is referred to the works of Pan and Yang [11] and Jiang [56].",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 51,
      "context" : "For a comprehensive review of these fields the reader is referred to the works of Pan and Yang [11] and Jiang [56].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 70,
      "context" : "proposed a simple technique to update an existing tree trained only on source samples using target samples [76].",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 71,
      "context" : "Adaptive DTs and stream sub-sampling have been used in data streams to handle massive, high-speed streams [77], [78], [79] as well as concept drift [80], [81], [82], modifying the DT as new samples arrive.",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 72,
      "context" : "Adaptive DTs and stream sub-sampling have been used in data streams to handle massive, high-speed streams [77], [78], [79] as well as concept drift [80], [81], [82], modifying the DT as new samples arrive.",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 73,
      "context" : "Adaptive DTs and stream sub-sampling have been used in data streams to handle massive, high-speed streams [77], [78], [79] as well as concept drift [80], [81], [82], modifying the DT as new samples arrive.",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 74,
      "context" : "Adaptive DTs and stream sub-sampling have been used in data streams to handle massive, high-speed streams [77], [78], [79] as well as concept drift [80], [81], [82], modifying the DT as new samples arrive.",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 75,
      "context" : "Adaptive DTs and stream sub-sampling have been used in data streams to handle massive, high-speed streams [77], [78], [79] as well as concept drift [80], [81], [82], modifying the DT as new samples arrive.",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 76,
      "context" : "Adaptive DTs and stream sub-sampling have been used in data streams to handle massive, high-speed streams [77], [78], [79] as well as concept drift [80], [81], [82], modifying the DT as new samples arrive.",
      "startOffset" : 160,
      "endOffset" : 164
    }, {
      "referenceID" : 77,
      "context" : "presented a variation of AdaBoost with decision stumps fitted to multiple tasks [83].",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 78,
      "context" : "The same authors later applied boosting with DTs while using a modified information gain (IG) criterion [84].",
      "startOffset" : 104,
      "endOffset" : 108
    } ],
    "year" : 2015,
    "abstractText" : "We propose novel model transfer-learning methods that refine a decision forest model M learned within a “source” domain using a training set sampled from a “target” domain, assumed to be a variation of the source. We present two random forest transfer algorithms. The first algorithm searches greedily for locally optimal modifications of each tree structure by trying to locally expand or reduce the tree around individual nodes. The second algorithm does not modify structure, but only the parameter (thresholds) associated with decision nodes. We also propose to combine both methods by considering an ensemble that contains the union of the two forests. The proposed methods exhibit impressive experimental results over a range of problems.",
    "creator" : "TeX"
  }
}