{
  "name" : "1506.02190.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Thresholding for Top-k Recommendation with Temporal Dynamics",
    "authors" : [ "Lei Tang" ],
    "emails" : [ "leitang@acm.org" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Categories and Subject Descriptors H.2.8 [Information Technology and Systems ]: Database Applications—Data Mining ; H.3.3 [Information Storage and Retrieval]: Information Filtering\nGeneral Terms Algorithms, Experimentation, Performance\nKeywords bias learning, temporal dynamics, top-k recommendation"
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "Recommender systems have been extensively studied in different domains including eCommerce [13], movie/music ratings [11], news personalization [4], content recommendation at web portals [2], etc. And all sorts of methods have been proposed for recommendation [1], including contentbased methods, neighborhood based approaches [5], latent\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00.\nfactor models like SVD or matrix factorization [11]. We notice that many methods assume data in recommender systems is static or follows the same distribution. Hence, a significant body of existing works evaluate their proposed models following a cross-validation procedure by randomly hiding some entries in a user-item matrix for testing.\nHowever, this is not the case for recommender systems where user feedbacks are collected over time. In this paper, we take one real-world eCommerce website (denoted as XYZ) as an example. The fundamental task is to recommend relevant items to customers, hopefully to increase purchases and thus revenues and profits for the company in the long run. It is observed that the trending products change week by week, or even day by day, due to user interest change, demand shift ignited by some external events, or simply because a product is out of inventory or just on shelf. Figure 1 demonstrates the fluctuation of relative ranking (determined by sales volume) of the top 10 popular products at XYZ identified at the first week of September, 2013.1 Notice that some portion of the lines are missing for certain weeks in the figure, indicating no transaction at the corresponding moments. Among the 10 products, only 6 were\n1All privacy-sensitive items and services oriented products and warranties are removed for analysis.\nar X\niv :1\n50 6.\n02 19\n0v 2\n[ cs\n.I R\n] 9\nN ov\n2 01\n5\nsold one year ago, The other 4 products were not even sold at the website yet. The variance is huge as observed in the figure. For instance, the 3rd popular product were ranked after 10, 000 even just few weeks ago. Majority of the select products, did not enter top-10 mostly in the past.\nThe temporal dynamics described above pose thorny challenges for recommendation, because it violates the fundamental assumption in most collaborative filtering that training and test data share the same distribution. Simply ignoring this discrepancy will lead to bad user experience . For instance, we may recommend a product that is popular one year ago, but now has been replaced by new models. This is particular common for products on promotion during holiday season. Another näıve solution to deal with the discrepancy is to restrict the model training to consider transactions only occurring the past few days so that the test distribution would not change too much from that of training. However, that essentially discards plenty of past transaction information for model training, leading to a more severe data sparsity problem.\nData sparsity have been widely observed in many different domains. At XYZ, for example, majority of customers purchase only few items across a whole year, and majority of products have relatively small number of transactions. It is not a wise choice to discard data for model training. With fewer data samples, the estimation of related parameters in the model will be coupled with high variance. Moreover, a shorter time-window typically results in a smaller coverage of items that can be leveraged for recommendation since some items may not appear in the select window. In Figure 2, we plot the relative performance improvement of a Markov model2 when we expand the time window of training to past 1 month, 3 months, 6 months and 15 months respectively. The baseline is the model trained using the past 1-week transactions only. Apparently, the larger the time window (and thus the more data) we use for training, the better the performance is, implying that we should collect as much data for training as possible.\nHence we have the following dilemma: on one hand, we want to exploit all possible transactions to overcome sparsity; on the other hand, we wish to capture the trend change in recommendation by examining recent transactions only. How can we capture temporal dynamics without discarding data for training? In this work, we propose to learn itemspecific biases in order to capture the temporal trend change for recommendation, while the recommendation model itself is still constructed using as much data as possible. We define\n2Details and evaluation criteria are described later.\na bias learning problem and then propose one algorithm to optimize biases for given evaluation criteria of top-k recommendation. Its convergence properties and time complexity are carefully examined. Experiments demonstrate the bias learning process will almost always improve the performance of base recommendation model, by keeping pace with the temporal dynamics of user-item interactions.\n2. TOP-K SELECTION AND EVALUATION Before proceeding to the bias learning problem, we need to review the commonly used procedure in top-k recommendation and corresponding evaluation measures. Without loss of generality, we will adopt standard terms user and item to speak about the recommendation problem throughout the paper . We assume there are in total n users, m items. u, v are used to denote the index for users, i, j for items, and p, q the rank positions of items in top-k. More symbols and their meanings can be referred in Table 1.\n2.1 Top-k Selection In recommender systems, the number of items to recommend is typically bounded by certain number. For example, in email marketing, the number of items can be recommended is limited by the email template. Assuming that we need to select k items for a given user u, a common practice is to have an recommendation engine yielding a prediction score for each item as follows:\nfu∗ = (fu1, fu2, · · · , fup)T (1)\nwhere fui denotes the prediction score of item i for user u. Note that some recommendation models yield scores only for a select set of candidate items, with others default to 0. Then items are ranked according to their prediction scores and the top ranking ones will be selected3. Equivalently, we find an ordered set of k items with maximal scores:\nR(fu∗) = {I(1), I(2), · · · , I(k)} (2) s.t. f (1)u ≥ f (2)u ≥ · · · ≥ f (k)u ,\nf (p)u ≥ fuj , ∀p ∈ {1, 2, · · · , k}, j /∈ R(fu∗)\nwhere f (p) u = fuI(p) is the score of the top p-th item."
    }, {
      "heading" : "2.2 Evaluation Criteria",
      "text" : "As for evaluation, the top ranking items R(fu∗) and the relevance of items have to be provided. Different from typical ratings in collaborative filtering, we focus on recommendations where there are only binary responses: relevant or irrelevant. In our application, we deem one item to be relevant for a user when the user purchases the item. Let yui ∈ {0, 1} denote the relevance of item i for a particular user u, with 0 being irrelevant, and 1 relevant. For presentation convenience, we use y (p) u to denote the relevance of the p-th top ranking item for u. For top-k recommendation, standard evaluation criteria include accuracy (ACC), mean average precision (MAP), and normalized discounted cumulative gain (NDCG). Since all of them compute performance with respect to top-k recommendation of individual user and then average across all users, we shall just describe them with respect to one user.\n3We assume items are selected solely based on prediction scores, while researchers have been considering other factors like diversity [30], which is beyond the scope of this work.\nAccuracy measures how many items in the top-k recommendation are indeed relevant.\nACC@k(u) = 1\nk k∑ p=1 y(p)u . (3)\nAccuracy does not care about position of items once they enter into top-k recommendation. By contrast, the other two measures average precision (AP@k) and normalized discounted cumulative gain (NDCG@k) take into account item position in the top-k recommendation as well.\nThe precision up to rank p (denoted as Precision(p, u)) is essentially ACC@p(u). AP is the average of precision at positions of those relevant items:\nAP@k(u) =\n∑k p=1 [ y (p) u · Precision(p, u) ] min{k,# relevant items for u} . (4)\nThe denominator in Eq. (4) is to account for cases when users are associated with different numbers of relevant items. Average precision is 1 when all relevant items are ranked at the top. When averaging AP@k for all users, we obtain mean average precision (MAP@k).\nDiscounted cumulative gain (DCG) is initially proposed for rankings with different degree of relevance:\nDCG@k(u) = k∑ p=1\n[ 2y (p) u − 1\nlog(1 + p)\n] =\nk∑ p=1\n[ y (p) u\nlog(1 + j)\n] (5)\nEq (5) follows because y (p) u ∈ {0, 1} in our case. However, DCG generally varies with respect to k and number of relevant items for u , making it difficult for comparison. Hence, normalized discounted cumulative gain (NDCG) is proposed to normalize the DCG into [0, 1]:\nNDCG@k(u) =\n[ y (p) u\nlog(1 + j)\n] /Zku, (6)\nwhere Zku is the maximal DCG, i.e., the DCG when all relevant items are ranked at the top.\nOnce we have the performance measure (ACC@k, AP@k, or NDCG@k) for each individual user, the overall performance for a set users U can be computed as the mean:\nperf@k(U) = 1 n n∑ u=1 perf@k(u). (7)\nSince the ranking of items are determined based on score predictions, and so are the corresponding performance, for presentation convenience we can rewrite Eq. (7) as\nperf(f1∗, f2∗, · · · , fn∗) = perf@k(U),\nwhere fu∗ denotes the prediction scores for user u."
    }, {
      "heading" : "3. THE BIAS LEARNING PROBLEM",
      "text" : "As mentioned in introduction, recommendation in eCommerce, on one hand, suffers from data sparsity, hence it is imperative to train the recommendation model with transaction history of as long as possible. On the other hand, the temporal dynamics of consumer purchase lead us to weigh more for those items that are recently trending. Such fluctuation can be attributed to all kinds of factors. For example, retailer itself occasionally may post products with huge discount for promotion. External events like a nation-wide\nsnow storm is very likely to lead to booming purchases of heaters. While capturing all varieties of external factors can be one way to demystify the temporal dynamics, it is challenging to encompass them all. Alternatively, we take one data-based approach to learn a bias for each item. In particular, we collect transactions in recent few days or weeks, and attempt to determine item-specific biases such that certain evaluation measure is maximized. The problem can be formally stated as follows:\nGiven:\n• a set of users U ; • evaluation criterion perf ; • recent relevance information Y of items for\nusers in U , with yui denoting the relevance of item i for user u;\n• prediction scores F for the set of users U , with fui being the score of item i for u;\nFind: a vector of biases b = (b1, b2, · · · , bm)T with bi indicating the bias of item i so that\n• the top-k recommendation are selected as R(fu∗ + b) according to Eq. (2); • the corresponding performance\nperf(f1∗ + b, f2∗ + b, · · · , fn∗ + b) (8)\nis maximized.\nBecause the biases are learned with recent transactions only, it would capture the recent trend change. The resultant biases can be different depending on the evaluation criterion. Note that all the three measures ACC@k, AP@k and NDCG@k are not smooth with respect to prediction scores due to the top-k selection and dichotomy of relevance, making the problem above intractable to solve analytically. However, we shall show that the problem can be solved iteratively, which is guaranteed to terminate in finite steps and converge to a coordinate-wise (local) optimal."
    }, {
      "heading" : "4. ALGORITHM",
      "text" : "Since the bias learning aims to find a bias for each item, we can rewrite the objective in Eq. (8) with respect to items:\nmax b1,b2,··· ,bm\nperf(f∗1 + b1, f∗2 + b2, · · · , f∗m + bm). (9)\nThe problem is difficult to resolve because of the ranking hidden in calculating in perf . Yet, the problem is solvable if we optimize one bi at a time. We propose to adopt an alternating optimization approach. That is, we fix the scores and biases for all other items while optimizing the bias for one particular item. We can cycle through all items until the objective function is stabilized. Next, we first describe the case of finding the optimal bias for one single item. We’ll use ACC@k as an example to derive the algorithm and then generalize it to handle other types of evaluation criteria like MAP@k and NDCG@k."
    }, {
      "heading" : "4.1 Finding optimal bias for single item",
      "text" : "Keep in mind the accuracy contribution of one item solely depends on whether the item enters into top-k. We start with the simplest case: assuming for a given user u, item i is not in top-k recommendation, how will the objective in Eq. (9) change accordingly if we increase the score of fui to push the item i into top-k? Once item i is pushed into top-k, naturally the top k-th item in the original recommendation will be discarded. This swap of items has four cases when considering the relevance of each item, which is shown below:\nyui y (k) u δui = (yui − y(k)u )/k\n0 0 0 1 1 0 0 1 −1/k 1 0 1/k\n(10)\nApparently, when both items share the same relevance, that is, either both relevant or irrelevant, the accuracy of user u would not change according to Eq. (3). The accuracy alters only if these two items are associated with different relevance information. In particular, when the item discarded from top-k is relevant while item i is not, the accuracy would decrease by 1/k. By contrast, if item i is relevant but the item discarded is not, the accuracy increases by 1/k. We can summarize the accuracy change as (yui − y(k)u )/k if we push item i into the top-k recommendation.\nIn order to push item i into top-k recommendation, the add-on bias should be at least f (k) u − fui. Note that the bias value can be negative. Once the item enters top-k recommendation, the accuracy does not change even if we increase the bias more. We can record the necessary add-on bias of item i to enter top-k recommendation for each user, and compute its corresponding accuracy change. Then we can determine the overall accuracy change for all users at different bias values and pick the optimal bias with maximal utility increase. For presentation convenience, we define utility as the accuracy change after item i enters into top-k recommendation. The utility is 0 if the item does not appear in the top-k recommendation of any user, This serves as the reference point (origin) in comparing utility values of different bias candidate values.\nAlgorithm 1 presents the procedure to find the optimal bias for one single item i with respect to ACC@k. Line 2-11 computes all possible bias values which may lead to a utility change. Note that line 7-9 computes the current utility score when bias is set to 0. We’ll prefer a 0 bias if no utility improvement is possible. Line 12 considers the extreme case that item i does not enter into top-k recommendation for any user, i.e., we set the bias to negative infinity (say, a huge negative constant), and the corresponding utility is zero. This is included due to cases that item i entering\nAlgorithm 1: Find optimal bias for item i\nInput: item i, scores f∗i, f (k) ∗ , and relevance y∗i, y (k) ∗ ;\nOutput: optimal bias bi and utility change ∆i; 1 init candidate bias values S = φ; cur util = 0; 2 for each u do 3 if yui 6= y (k) u then 4 compute utility change δui = (yui − y (k) u )/k; 5 compute score difference sui = f (k) u − fui; 6 append (sui, δui) to S; 7 if sui < 0 then 8 cur util = cur util + δui; 9 end\n10 end 11 end 12 push (−inf, 0) to S; 13 find bi and max util via subroutine in Algorithm 2; 14 update ∆i = max util − cur util; 15 return bi, ∆i\nAlgorithm 2: Subroutine to find optimal bias given candidate <bias, utility change> pairs\nInput: cur util, candidate pairs S = {(s, δ)}; Output: optimal bias b and max util;\n1 S = sort S by score difference s; 2 init b = 0, max util = cur util; 3 init util = 0; 4 for each (s, δ) in S do 5 util = util + δ; 6 if util > max util then 7 b = s; 8 max util = util; 9 end\n10 end\ninto top-k recommendation leads to a negative utility. Then it is better to exclude item i from top-k recommendation. Once we have all the potential bias values that may result in a different utility score, the subroutine in Algorithm 2 sorts the bias values in ascending order, and figure out the optimal bias which leads to the maximal utility. Then in line 14-15 of Algorithm 1, we report the optimal bias and corresponding utility change comparing with current utility.\nThe time complexity for such a procedure is O(n logn) where n is the number of users. Line 2-11 in Algorithm 1 cost linear time. Algorithm 2 costs O(n logn) because of the sorting in line 1. Therefore, the total computational time to find the optimal bias for one single item is O(n logn)."
    }, {
      "heading" : "4.2 Finding optimal biases for all items",
      "text" : "In the previous subsection, we have present an algorithm to learn the optimal bias of one single item when fixing the scores for all other items. In order to find optimal biases for all items, we propose to cycle through all items to update biases, tantamount to the well-known coordinate descent method in optimization. The detailed algorithm in shown in Algorithm 15.\nThe algorithm consists of two loops: the inner loop cycles through all items to find the optimal bias individually. The outer loop stops if no utility increase can be found. The related update in lines 8-12 after changing the bias of one item is straightforward except line 12. In order to save computational cost, we can record the top-k recommendation given\nAlgorithm 3: Learning Biases for All Items\nInput: score predictions {f1∗, f2∗, · · · , fn∗}, relevance information{y1∗,y2∗, · · · ,yn∗};\nOutput: optimal biases b∗ = {b∗1, b∗2, · · · , b∗m} 1 construct candidate item set C to learn optimal bias; 2 init b∗i = 0 for all i ∈ C; 3 init ∆utility = 1; 4 while ∆utility > 0 do 5 reset ∆utility = 0; 6 for each item i in C do 7 compute potential utility change ∆i and corresponding bias bi following algorithm 1; 8 if ∆i > 0 then 9 update ∆utility = ∆utility + ∆i ;\n10 update b∗i = b ∗ i + bi; 11 update prediction scores f∗i for item i; 12 update top k-th recommendation for each user if\nnecessary;\n13 end 14 end 15 end\ncurrent scores, and the minimum score in the top-k recommendation for each user. Based on the minimum score, we can immediately decide whether the item i is in the top-k recommendation before and after bias is updated. If the item i is not among top-k recommendation before and after bias update, then nothing needs to be done. We can take care of other cases in a similar vein so that we do not have to recompute top-k recommendation for each user, and thus speed up the computation significantly.\nBased on the algorithm above, we can derive theoretical properties below:\nTheorem 4.1. Algorithm 15 is guaranteed to terminate in finte steps.\nProof. The ∆utility is upper-bounded by the maximum accuracy of top-k recommendation among all the possible rankings. Each cycle (lines 4 - 15) in the algorithm will increase utility by at least 1/k if not zero. Therefore, the algorithm must terminate in finite iterations.\nThe number of cycles tends to be very small in reality. In most cases, we just need to cycle through all items a couple of times. Nevertheless, we show that based on the algorithm, we are able to filter out certain items so that each iteration scans smaller number of items, which is stated below:\nTheorem 4.2. If one item i satisfies yui = 0 for all u , then the item i can be removed from consideration of recommendation.\nProof. Let’s revisit the utility change table as shown in Eq. (10). Note that when y (k) u is 0, swapping in a different item in the top-k recommendation will always increase or keep current utility. Because item i satisfies yui = 0 for all u, it follows that setting a bias of negative infinity for item i would lead to no utility loss overall (if not positive utility change), no matter whether item i is in the top-k recommendation. It is essentially equivalent to removing item i from consideration of recommendation.\nThe theorem above suggests that we may remove items without positive relevance from the input score predictions\nin Algorithm 15. In the context of eCommerce, those products which does not appear in recent transactions can be removed from recommendation directly and thus reduce computational cost substantially.\nThe proposed algorithm is sequential in nature, and thus difficult to parallel. The time complexity for the proposed algorithm is at least O(mn logn) where m is the size of candidate item set C, and n is the number of users. Even though with few cycles, the time and space complexity will be scary when both numbers of users and items are huge. Next we discuss a couple of heuristics we might consider in practice. All these methods have been adopted in our experiments and have been shown to save tremendous time and space."
    }, {
      "heading" : "4.3 Scaling up for Practical Implementation",
      "text" : "First of all, it is not necessary to store the prediction scores for all items. Considering recommendation for even just 100K users with 100K items, which is medium size in the era of big data. Assuming each score takes 4 bytes as a float number, it requires 100K× 100K× 4 = 40G memory space. Therefore, we suggest exploiting a sparse representation of prediction scores by keeping only select number of top recommendations while defaulting others to zero. The number of recommendations to keep is typically a multiplier of k. For instance, when we need to optimize for performance@top-10, we can keep the top-50 recommendations from a model. For the remaining items, their predict scores are set to 0. This is valid because we often observe a fast decay of the scores no matter what the base recommendation model is.\nFor another, we can shrink the candidate item set for bias tuning to reduce time complexity. Two types of items should be considered with higher priority for bias tuning. One set are those items which are recommended frequently based on raw prediction scores, which tend to be past popular items. The other set are those appearing frequently in recent transactions, which are the recently popular ones. The former is likely to lead to a negative bias, while the latter a positive bias. For other items outside the candidate set, we set their bias value to zero without changing their prediction scores.\nMoreover, we notice that the number of items with updated bias scores is dramatically decreased with iterations of cycling through items. The algorithm reaches a cooridatewise local optimal and stops after few cycles. Yet, the scanning of all items are expensive, and thus early stopping criteria can be used. For example, we may set a percentage threshold such that if fewer than the percentage of items need to update bias, then we terminate the cycling. Or we can simply set the upper-bound of cycles. In our experiments, we just set it to 2 to reduce computational time, yet found no performance loss."
    }, {
      "heading" : "4.4 Extensions to optimize MAP or NDCG",
      "text" : "We have described how we optimize biases with respect to ACC@k. Now we extend the algorithm 1 to optimize MAP@k or NDCG@k. Different from accuracy, for which the position of one item in the top-k recommendation does not matter, MAP and NDCG is rank -related metric. The position of one item in the top-k recommendation plays an import role. A relevant item ranked as top-1 will results in a different score than that when the item is ranked as the top k-th. In order to compute the potential utility change with different bias scores, we have to consider all the possible positions rather than just the top k-th item. Nevertheless,\nthe basic idea remains the same. We compute out the score difference and corresponding utility change with respect to each position in top-k.\nTake average precision in Eq. (4) as an example. Given a user, assume the average precision of top-k recommendation without item i is AP0, serving as our reference point. We can gradually increase the score of item i to be ranked at position k, k−1, · · · , 1, and compute out the corresponding performance. For each position, its utility change should be computed as follows:\nPosition Performance utility change k APk δk = APk −AP0\nk − 1 APk−1 δk−1 = APk−1 −APk ... ... ...\n1 AP1 δ1 = AP1 −AP2\nSuppose item i is currently at position p+1, and we increase its bias to position p. Note that only items at position p and p+ 1 are swapped, while the other items remain unchanged. Therefore, we only need to examine the utility change because of the swap of the two items. If item i and the item at position p have the same relevance, i.e. yui = y (p) u , the AP would not change, and hence leading to no utility change. If yui = 1 and y (p) u = 0, then it leads to a utility increase when we push item i into position p; If yui = 0 and y p u = 1, then a reduction of utility. Let k̃ = min{k,#relevant items for u}, the utility change can be derived below:\nδ (p) ui = yui ·\n( 1 + ∑p−1 q=1 y (q) u\np −\n1 + ∑p−1\nq=1 y (q) u\np+ 1\n) /k̃ (11)\n+ y(p)u ·\n( 1 + ∑p−1 q=1 y (q) u\np+ 1 −\n1 + ∑p−1\nq=1 y (q) u p\n) /k̃ (12)\n= ( yui − y(p)u )( 1 +\np−1∑ q=1 y(q)u\n)( 1\np − 1 p+ 1\n) /k̃ (13)\nwith 1\np+ 1 = 0 if p+ 1 > k.\nIn the equation, the first term in Eq. (11) is the utility change that item i moves from position p+ 1 to position p, and the second term in Eq. (12) is the utility change when the original item at position p downgrades to position p+ 1. There is one special case when p + 1 > k, i.e., item i has not entered into top-k yet at the beginning. In that case, we replace 1/(p+ 1) by 0. Similarly for NDCG, it follows that\nδ (p) ui = ( yui − y(p)u )( 1 log(1 + p) − 1 log(1 + (p+ 1)) ) /Zuk (14)\nwith 1\nlog(1 + (p+ 1)) = 0 if p+ 1 > k.\nNote that when yui and y (p) u are the same, both utility changes following Eqs (13) and (14) would be zero. Hence, rather than checking every possible position in top-k, we just need to check those positions with different relevance to item i. The algorithm to find optimal bias for one single item is summarized in Algorithm 4. It is almost the same as Algorithm 1, except line 3-5. Line 3-4 are supposed to check each potential position in the top-k, and line 5 is to update the utility change based on corresponding performance metric. Apparently, such a change leads to an increase of time\nAlgorithm 4: Find optimal bias wrt. MAP/NDCG\nInput: item i, scores f∗i, f (k) ∗ , and relevance y∗i, y (k) ∗ ; rank-related performance metric Output: optimal bias bi and utility change ∆i;\n1 initialize candidate set S = φ, cur util = 0; 2 for each u do 3 for each position p do 4 if yui 6= y (k) u then 5 compute utility change wrt. performance metric following Eq. (13) or (14); 6 compute score difference sui = f (k) u − fui; 7 append (sui, δui) to S; 8 if sui < 0 then 9 cur util = cur util + δui;\n10 end 11 end 12 end 13 end 14 push (−inf, 0) to S; 15 find bi and max util via subroutine in Algorithm 2; 16 update ∆i = max util − cur util; 17 return bi, ∆i\ncomplexity, reaching O(kn log kn) to find optimal bias for single item. This is still fine if k is reasonably small, which is mostly true in practice. Plugging Algorithm 4 into Algorithm 15, we obtain the bias values for all items with respect to select evaluation criterion."
    }, {
      "heading" : "5. EXPERIMENT SETUP",
      "text" : "In this section, we mainly describe the basic setup for our experiments, including preparation of benchmark data sets, base recommendation model and other methods considering temporal dynamics for comparison."
    }, {
      "heading" : "5.1 Benchmark Data Sets",
      "text" : "We collect customer transactions of XYZ and construct benchmark data sets via a split based on date. User activities before the date are used for training, and the transactions in the subsequent week are used to evaluate recommendation performance. Corresponding performance measures include ACC@k, MAP@k, and NDCG@k as described in Section 2.2. In our experiments, k is set to 10. For easy interpretation, we report all numbers in terms of lift (relative improvement) with respect to one baseline:\nlift =\n( perf perfbaseline − 1 ) ∗ 100%.\nWe prepare two benchmark data sets: one is during regular season (Nov. 1, 2013); and the other is during holiday season (Dec. 11, 2013). User shopping behavior in holiday season tends to be quite different from regular season, both in terms of quantity and trending products. We aim to verify the efficacy of our proposed method under both settings."
    }, {
      "heading" : "5.2 Base Recommendation Model",
      "text" : "One commonly used approach for recommendation in eCommerce is to model user actions as a Markov chain [19, 18]. It is tantamount to computing item similarity [13], but keeping the metric directional. That is, one’s current action depends only on his most recent action. The transition probability\nfrom one action to another can be estimated below:\nP (buy i|bought j) = # users who bought j then i # users who bought j . (15)\nWe also take into consideration of those highly associated purchase patterns by computing co-purchase probability of multiple items beyond 2. For instance, the transition of two actions leading to one purchase can be computed as:\nP (buy i|bought j1, j2) = # users who bought j1, j2 then i\n# users who bought j1, j2 .\n(16) However, considering those higher-order purchase patterns leading to explosion of state spaces. Therefore, we consider only the state spaces containing up to 2 actions. As for prediction, we pick the products with highest probability given user’s most recent few transactions.\nThis Markov model is exploited because it has been validated to work quite well in eCommerce. Later in experiments, we shall show that our proposed bias learning can be applied to other base recommendation models as well."
    }, {
      "heading" : "5.3 Methods Considering Temporal Change",
      "text" : "For our proposed bias learning method (denoted asMbias), we use most recent 3-day transactions to fine tune the bias of items. All the biases, unless specified, are learned via optimizing ACC@k. As for comparison, we also include one baseline method without considering temporal change.\nMlong: This method utilizes as long history as possible for training. As already shown in Figure 2, the longer time window we use, the better the recommendation model performs. In our experiments, we use up to 15 months of user activity history for training.\nBesides the baseline, there are several other approaches to take into account temporal dynamics.\nMtruncate: According to Theorem 4.2, if one item does not appear in recent transactions, we can remove it from recommendation. This method trains the recommendation model using 15-month data, but for prediction it concentrates only on those items that are recently attracting user attentions. It truncates the item set for recommendation but does not tune biases of items.\nMdistrdiff : This method directly computes a bias term for each item, rather than optimizing biases with respect to certain metric. In particular, we compute two distributions of items, one from the 15-month transactions(denoted as d(long)), and the other from recent\ntransactions (denoted as d(short)). So the item-specific bias is b = d(short) − d(long). Biases are added to the normalized probabilistic output of recommendation engine to promote recent trending products while suppressing past popular ones.\nMdecay: Another option is to train a model that already incorporates temporal dynamics, by assigning lower weight to those remote events. An exponential decay function is used: w = exp (−∆t/β) where ∆t is the time gap of the purchase in Eqs (15) and (16) to current date of recommendation, and β is a decay factor. β is set to 60 in our experiments.\nNote that there has been some work to consider temporal dynamics for recommendation. For example, Koren et al. [10, 9] proposed to have a time-dependent bias in matrix factorization for movie/music ratings. The model minizes the root mean squared error and adopts stochastic gradient descent to find biases and latent factors. However, in our application, the responses are binary (either purchase or not purchase), and only positive responses are collected. A trivial solution would set bias to 1 for all, which is meaningless. We may randomly sample negative entries for our one-class collaborative filtering [16] problem, but that would essentially connect bias to the sampling rate, which is not acceptable either."
    }, {
      "heading" : "6. EXPERIMENTS",
      "text" : "In this section, we conduct a series of experiments over the constructed benchmark datasets to study the performance, sensitivity to base recommendation models and performance metrics. At the end, we report results by applying our method to online A/B test."
    }, {
      "heading" : "6.1 Performance Comparison",
      "text" : "The performance of various methods are shown in Tables 2 and 3. For easy comparison, we deem Mlong as the baseline and show the lift of other methods. The numbers in bold face indicates the one with best performance. For both data sets at regular season and holiday season, our proposed method Mbias is the winner. The lift is more observable at holiday season because of the strong shopping pattern change thanks to Black Friday, Cyber Monday and other promotion campaigns. The numbers might look small, but keep in mind it took nearly 3 years and thousands of teams worldwide to improve 10% over a trivial baseline in Netflix prize competition4. It is noticed that Mtruncate and Mdistrdiff both yield some improvement, suggesting that it is always helpful to incorporate recent trend. However, neither of them is comparable to learning a bias for each item as we proposed. As shown in both tables, adding a weighted decay based on recency for training does not help. In short, learning biases to capture the temporal dynamics can model individual interests and preferences more accurately, and thus improve performance of the base recommendation model. This is especially helpful when the temporal fluctuation is huge, as shown during the holiday season.\n4http://en.wikipedia.org/wiki/Netflix_prize\n0  \n0.2  \n0.4  \n0.6  \n0.8  \n1  \n1.2  \n1.4  \nACC@10   MAP@10   NDCG@10  \nLi # (%\n)  o ve\nr  b as el in e  \nRegular  Season  \nop3mize  ACC  \nop3mize  MAP  \nop3mize  NDCG  \nFigure 3: Lift at Regular Season\n0  \n1  \n2  \n3  \n4  \n5  \n6  \n7  \nACC@10   MAP@10   NDCG@10  \nLi # (%\n)  o ve\nr  b as el in e  \nHoliday  Season  \nop4mize  ACC  \nop4mize  MAP  \nop4mize  NDCG  \nFigure 4: Lift at Holiday Season\n0 50 100 150 200 250 300 350 400 450 item\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0\n0.2\nb ia\ns v a lu\ne\noptimize ACC optimize NDCG\nFigure 5: Bias Values\nAt macro level, we observe that bias learning tends to shift the item distribution in our top-k recommendation towards the ground truth. We sort all items based on their frequency in reverse order in test data and predictions respectively, and then check the overlap of top popular items between the two. The result over the holiday season data is shown in Table 4. A similar trend is observed during regular season. Apparently, adding the bias helps the prediction to capture those recently trending products. For example, only 3 out of the top-10 popular products in testing period is covered by the raw recommendation. However, adding a bias immediately increases the coverage to 7. This pattern is consistent for a wide range of values as shown in the Table.\nWe also compute the KL-divergence between transactions in testing period and predictions. As some items appear only in transactions or predictions, we smooth the distributions by considering all zero frequency as 0.01 to avoid an unbounded divergence value. For the data in regular season, the KL-divergence between predictions and true transactions is 0.9413, and decreases to 0.9406 once biases are added. Similarly for data in holiday season, the divergences reduces from 0.7263 to 0.6757 owning to learned biases.\nIn summary, our proposed method is able to learn biases that incorporate temporal dynamics of items, both at individual-level and macro-level, and hence improving recommendation performance. This improvement is more observable when there is a big difference between data distribution for training and recommendation time."
    }, {
      "heading" : "6.2 Optimizing Different Performance Metrics",
      "text" : "Here we apply the proposed bias learning algorithm with respect to rank-related metrics like MAP or NDCG. Figures 3 and 4 plot the lift of Mbias over baseline Mlong at both data sets. First of all, all methods lead to a positive lift, implying the effectiveness of learning biases. Nevertheless, the optimization criteria results in final performance difference. Initially, we conjecture that optimizing one metric would lead to higher numbers in terms of that particular metric, e.g., optimizing MAP should result in higher MAP in test performance. In reality, this is not the case. As seen\nin Figure 4, optimizing MAP actually results in lower performance in terms of all three metrics, and optimizing NDCG, on the contrary, yields comparable performance to ACC.\nWe also compare the difference of learned biases of optimizing ACC or NDCG. It turns out we learned 386 non-zero biases for ACC, 402 non-zero biases for NDCG and 384 are shared between both. The corresponding bias values are shown in Figure 5, with items sorted based on bias values of optimizing ACC. It is noticed that majority of them have close-to-zero values. Though some values are positive, more bias values are towards negative, suggesting many item biases tries to discount the popularity in the training data. The scale of negative values tend to be much larger. Moreover, optimizing NDCG gives less extreme values, because it considers all the possible ranking positions of top-k.\nRecall that the time complexity of each iteration in optimizing NDCG is k times larger than that of optimizing ACC. Therefore, we have to strike a balance between the performance and computational time. In practice, we can have one trained recommendation model, and just need to update the biases daily. Faster convergence can be accomplished via warm start, i.e., adopting the learned biases yesterday for initialization. For our experiment purpose, we just report findings of optimizing ACC thereafter."
    }, {
      "heading" : "6.3 Bias Learning for Different Base Models",
      "text" : "In previous subsections, we have mainly studied properties of bias learning with the Markov model. Here we explore other base recommendation models. Our proposed bias learning is kind of orthogonal to the base model being used and it is applicable to a wide array of models. Two models are considered here: matrix factorization (MF) and category-based recommendation (CBR).\nMatrix factorization (MF) gained momentum thanks to the Netflix prize competition[11, 21]. It is shown to be one of the start-of-the-art methods for collaborative filtering. Standard matrix factorization aims to approximate a user-item matrix as the product of two low-rank matrices: An×m ≈ Pn×`Q`×m. where P and Q are the latent factors of users and items, respectively. Typical matrix factorization (either through alternating least squares or stochastic gradient descent) requires an iterative process, which involves too much overhead when implemented in MapReduce in order to deal with large-scale data sets. Alternatively, we implemented a randomized version of matrix factorization as described in [22]. It utilizes a randomized SVD [7] to compute approximate Q and then determines P given Q.\nCategory-based recommendation (CBR) assumes a user is more likely to purchase a product within the same category\nif he has already indicated an interest in a category. P (buy i|user u) = ∑ c P (buy i|buy in c) ·P (buy in c|user u).\nThe interest categories of one user P (buy in c|user u) is computed through his past actions. Each user is represented as a multinomial distribution of interest categories, by mapping each of his actions to its corresponding category in a carefully curated product taxonomy. To estimate P (buy i|buy in c), we examine the popularity of each product among existing transactions. In order to capture the recent trend, we apply Mtruncate, that is, we restrict ourselves to look at transactions only within the past few days/weeks at recommendation time. Therefore, this category-based recommendation tends to pick those recent best-selling products given one’s personal interest categories.\nWe apply bias learning to both models. For brevity, we just report the lift in terms of ACC@10 over different base models in Table 5. For all base models, we observe a positive lift, suggesting that our bias learning is able to capture the temporal dynamics no matter which model is being used. The lift over MF is substantial, partly because of MF’s poor performance itself. To our surprise, among all three methods, matrix factorization performs the worst. Such a poor performance of matrix factorization is also observed in other domains with binary responses [3]. One factor is that matrix construction based on transactions is critical yet not well defined. It is difficult to incorporate both frequency and recency information simultaneously into one matrix."
    }, {
      "heading" : "6.4 Online A/B Tests",
      "text" : "Here we run online A/B test through email campaigns to further examine the impact of added bias for recommendation. Each email contains 8 item recommendations. As mentioned earlier, the Markov model works quite well in our domain and is adopted for base recommendation. We sample a small percentage of XYZ customers and randomly split them into two buckets, one with bias learning and the other without. We run two tests, on 2013/11/26 and 2013/12/07, respectively. Both tests sent around 800K marketing emails. Three widely-used metrics are recorded: the click-through rate (CTR), average number of orders and revenue per email-open. We attribute one order/revenue to be from marketing emails only if customers receiving emails click on one link in the email and place order(s) within the same session. The lifts after bias learning are shown in Table 6. For all metrics, we see a positive lift, though only the CTR is shown to be significant. Because the number of orders was extremely small, it is difficult to reject the null hypothesis given limited impressions. These online tests confirm our hypothesis that adding a bias to capture temporal dynamics intrigue more customers to click and place orders subsequently, suggesting more effective recommendation."
    }, {
      "heading" : "7. RELATED WORK",
      "text" : "Mining concept-drifting data streams [23] for classification and pattern mining has been studied extensively. Yet considering temporal changes for recommendation is gaining some attention recently. Koren et al. [10, 9] proposed to have a time-dependent bias in matrix factorization for movie/music ratings. But the proposed method is not applicable for one-class collaborative filtering [16] problem. Moreover, it aims to minimize the root mean squared error (which is differentiable) rather than ranking metrics for top-k recommendation. On the other hand, Xiong et al. [28] formulate temporal collaborative filtering as a tensor factorization by treating time as one additional dimension. Wang et al. [24, 25] consider the time gap between purchases and propose an opportunity model to identify not only the items to recommend, but also the best timing to recommend a particular product. Meanwhile, improving temporal diversity of recommendation across time [12, 31] is also considered.\nAnother related domain is learning to rank [15], which is initially motivated for the problem of information retrieval given queries. Making recommendations by learning to rank has attracted lots of attentions recently [17, 27, 8]. EigenRank [14] extends memory-based (or similarity-based) methods by considering the ranking (rather than rating) of items in computing user similarities. Matrix factorization has been extended to optimize for ranking-oriented loss as well. But most ranking-related metrics are non-smooth or non-convex. Hence, majority of the methods either approximate the loss via a smooth function or find a smooth lower/upper bound for the loss function. For instance, CofiRank [26] extends matrix factorization to optimize ranking measures like NDCG instead of rating measures. Because NDCG is non-convex, the authors propose a couple of steps to find a a convex upper-bound for the non-convex problem and adopt bundle method for optimization. CLiMF [20] instead optimizes a lower bound of smooth reciprocal rank. Our proposed method differs because we explicitly optimizes for the exact ranking measure. This is viable because we are learning only biases, rather than latent factors, with the ranking loss.\nOur proposed bias learning method in collaborative filtering is partly inspired from the thresholding problem in multi-class/label classification [29, 6]. For large-scale multiclass/label classification problem, one-vs-rest is still widely used. That is, for each class we construct a binary classifier by treating the class as positive, and the remaining classes as negative. Since each binary classifier is constructed independently, researchers propose to learns a threshold (bias) for each class mainly to optimize classification accuracy, precision/recall or F-measure. However, in top-k recommendation, the score difference and ranking of items matter, making all the items dependent on each other. Also, the motivation of this work is mainly to capture temporal dynamics rather than calibrating the classifier prediction scores."
    }, {
      "heading" : "8. CONCLUSIONS AND FUTURE WORK",
      "text" : "This work attempts to take into account temporal dynamics for top-k recommendation. It is motivated from the observation that certain domains, e.g. eCommerce, are highly dynamic. Since user feedbacks are likely to be rare in most recommender systems, we suggest keeping as much data as possible for training recommendation model to avoid sparsity problem. On the other hand, we propose to learn a\ntime-dependent bias for each item based on recent user feedback only to capture the temporal trend change. We define the bias learning problem and present a coordinate-descent like algorithm to optimize ranking-based measures like ACC, MAP or NDCG. We prove that the algorithm is guaranteed to terminate in finite steps with reasonable time complexity. Empirical results via both offline and online experiments demonstrate that the proposed bias learning method is able to boost the performance of base recommendation models, and capture the temporal shift in user feedback. As the bias learning works independently of base recommendation model being used, we encourage other practitioners to add it as a standard module in recommender systems where temporal dynamics are a norm.\nA couple of problems remain open. Even though we have provided some guidelines to reduce computational cost for bias learning, the proposed algorithm is sequential in nature and thus difficult to harness the power of parallel/distributed computing. Its scalability needs to be improved. For another, it has been shown that one item can be removed from recommendation if it does not appear in the recent transactions. So bias learning would not pick new items. This seems to be against the initial purpose of recommendation, to encourage users to discover more items in the long tail. It is pressing to understand more about the balance between relevance, popularity and serendipity in recommendation."
    }, {
      "heading" : "9. REFERENCES",
      "text" : "[1] G. Adomavicius and A. Tuzhilin. Toward the next\ngeneration of recommender systems: A survey of the state-of-the-art and possible extensions. IEEE Trans. on Knowl. and Data Eng., 17(6):734–749, June 2005.\n[2] D. Agarwal, B.-C. Chen, P. Elango, and R. Ramakrishnan. Content recommendation on web portals. Commun. ACM, 56(6):92–101, June 2013.\n[3] F. Aiolli. Efficient top-n recommendation for very large scale binary rated datasets. In RecSys, 2013.\n[4] A. S. Das, M. Datar, A. Garg, and S. Rajaram. Google news personalization: scalable online collaborative filtering. In WWW, pages 271–280, 2007.\n[5] C. Desrosiers and G. Karypis. A comprehensive survey of neighborhood-based recommendation methods. In Recommender systems handbook, pages 107–144. 2011.\n[6] R.-E. Fan and C.-J. Lin. A study on threshold selection for multi-label classication, 2007.\n[7] N. Halko, P. G. Martinsson, and J. A. Tropp. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. SIAM Rev., 53(2):217–288, May 2011.\n[8] A. Karatzoglou, L. Baltrunas, and Y. Shi. Learning to rank for recommender systems. In RecSys, pages 493–494, 2013.\n[9] N. Koenigstein, G. Dror, and Y. Koren. Yahoo! music recommendations: modeling music ratings with temporal dynamics and item taxonomy. In RecSys, pages 165–172, 2011.\n[10] Y. Koren. Collaborative filtering with temporal dynamics. In KDD, pages 447–456, 2009.\n[11] Y. Koren, R. Bell, and C. Volinsky. Matrix factorization techniques for recommender systems. Computer, 42(8):30–37, 2009.\n[12] N. Lathia, S. Hailes, L. Capra, and X. Amatriain.\nTemporal diversity in recommender systems. In SIGIR pages 210–217, 2010.\n[13] G. Linden, B. Smith, and J. York. Amazon. com recommendations: Item-to-item collaborative filtering. Internet Computing, IEEE, 7(1):76–80, 2003.\n[14] N. N. Liu and Q. Yang. Eigenrank: A ranking-oriented approach to collaborative filtering. In SIGIR pages 83–90, 2008.\n[15] T.-Y. Liu. Learning to rank for information retrieval. Foundations and Trends in Information Retrieval, 3(3):225–331, 2009.\n[16] R. Pan, Y. Zhou, B. Cao, N. N. Liu, R. Lukose, M. Scholz, and Q. Yang. One-class collaborative filtering. In ICDM, pages 502–511, 2008.\n[17] S. Rendle, C. Freudenthaler, Z. Gantner, and L. Schmidt-Thieme. Bpr: Bayesian personalized ranking from implicit feedback. In UAI, 2009.\n[18] S. Rendle, C. Freudenthaler, and L. Schmidt-Thieme. Factorizing personalized markov chains for next-basket recommendation. In WWW, pages 811–820, 2010.\n[19] G. Shani, D. Heckerman, and R. I. Brafman. An mdp-based recommender system. J. Mach. Learn. Res., 6:1265–1295, Dec. 2005.\n[20] Y. Shi, A. Karatzoglou, L. Baltrunas, M. Larson, N. Oliver, and A. Hanjalic. Climf: Learning to maximize reciprocal rank with collaborative less-is-more filtering. In RecSys pages 139–146, 2012.\n[21] G. Takács, I. Pilászy, B. Németh, and D. Tikk. Scalable collaborative filtering approaches for large recommender systems. J. Mach. Learn. Res., 10:623–656, June 2009.\n[22] L. Tang and P. Harrington. Scaling matrix factorization for recommendation with randomness. In WWW Companion, pages 39–40, 2013.\n[23] H. Wang, W. Fan, P. S. Yu, and J. Han. Mining concept-drifting data streams using ensemble classifiers. In KDD, pages 226–235, 2003.\n[24] J. Wang, B. Sarwar, and N. Sundaresan. Utilizing related products for post-purchase recommendation in e-commerce. In RecSys pages 329–332, 2011.\n[25] J. Wang and Y. Zhang. Opportunity model for e-commerce recommendation: right product; right time. In SIGIR pages 303–312, 2013.\n[26] M. Weimer, A. Karatzoglou, Q. V. Le, and A. J. Smola. Cofi rank-maximum margin matrix factorization for collaborative ranking. In NIPS, 2007.\n[27] J. Weston, H. Yee, and R. J. Weiss. Learning to rank recommendations with the k-order statistic loss. In RecSys, pages 245–248. 2013.\n[28] L. Xiong, X. Chen, T.-K. Huang, J. G. Schneider, and J. G. Carbonell. Temporal collaborative filtering with bayesian probabilistic tensor factorization. In SDM, volume 10, pages 211–222, 2010.\n[29] Y. Yang. A study of thresholding strategies for text categorization. In SIGIR pages 137–145, 2001.\n[30] C. Yu, L. Lakshmanan, and S. Amer-Yahia. It takes variety to make a world: diversification in recommender systems. In EDBT pages 368–378, 2009.\n[31] G. Zhao, M. L. Lee, W. Hsu, and W. Chen. Increasing temporal diversity with purchase intervals. In SIGIR, pages 165–174, 2012."
    } ],
    "references" : [ {
      "title" : "Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions",
      "author" : [ "G. Adomavicius", "A. Tuzhilin" ],
      "venue" : "IEEE Trans. on Knowl. and Data Eng.,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2005
    }, {
      "title" : "Efficient top-n recommendation for very large scale binary rated datasets",
      "author" : [ "F. Aiolli" ],
      "venue" : "In RecSys,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2013
    }, {
      "title" : "Google news personalization: scalable online collaborative filtering",
      "author" : [ "A.S. Das", "M. Datar", "A. Garg", "S. Rajaram" ],
      "venue" : "In WWW,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2007
    }, {
      "title" : "A comprehensive survey of neighborhood-based recommendation methods",
      "author" : [ "C. Desrosiers", "G. Karypis" ],
      "venue" : "In Recommender systems handbook,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2011
    }, {
      "title" : "A study on threshold selection for multi-label classication",
      "author" : [ "R.-E. Fan", "C.-J. Lin" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2007
    }, {
      "title" : "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions",
      "author" : [ "N. Halko", "P.G. Martinsson", "J.A. Tropp" ],
      "venue" : "SIAM Rev.,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2011
    }, {
      "title" : "Learning to rank for recommender systems",
      "author" : [ "A. Karatzoglou", "L. Baltrunas", "Y. Shi" ],
      "venue" : "In RecSys,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2013
    }, {
      "title" : "Yahoo! music recommendations: modeling music ratings with temporal dynamics and item taxonomy",
      "author" : [ "N. Koenigstein", "G. Dror", "Y. Koren" ],
      "venue" : "In RecSys,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2011
    }, {
      "title" : "Collaborative filtering with temporal dynamics",
      "author" : [ "Y. Koren" ],
      "venue" : "In KDD, pages 447–456,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2009
    }, {
      "title" : "Matrix factorization techniques for recommender systems",
      "author" : [ "Y. Koren", "R. Bell", "C. Volinsky" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2009
    }, {
      "title" : "Temporal diversity in recommender systems",
      "author" : [ "N. Lathia", "S. Hailes", "L. Capra", "X. Amatriain" ],
      "venue" : "In SIGIR pages 210–217,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2010
    }, {
      "title" : "Amazon. com recommendations: Item-to-item collaborative filtering",
      "author" : [ "G. Linden", "B. Smith", "J. York" ],
      "venue" : "Internet Computing, IEEE,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2003
    }, {
      "title" : "Eigenrank: A ranking-oriented approach to collaborative filtering",
      "author" : [ "N.N. Liu", "Q. Yang" ],
      "venue" : "In SIGIR pages",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2008
    }, {
      "title" : "Learning to rank for information retrieval",
      "author" : [ "T.-Y. Liu" ],
      "venue" : "Foundations and Trends in Information Retrieval,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2009
    }, {
      "title" : "One-class collaborative filtering",
      "author" : [ "R. Pan", "Y. Zhou", "B. Cao", "N.N. Liu", "R. Lukose", "M. Scholz", "Q. Yang" ],
      "venue" : "In ICDM,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2008
    }, {
      "title" : "Bpr: Bayesian personalized ranking from implicit feedback",
      "author" : [ "S. Rendle", "C. Freudenthaler", "Z. Gantner", "L. Schmidt-Thieme" ],
      "venue" : "In UAI,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2009
    }, {
      "title" : "Factorizing personalized markov chains for next-basket recommendation",
      "author" : [ "S. Rendle", "C. Freudenthaler", "L. Schmidt-Thieme" ],
      "venue" : "In WWW,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2010
    }, {
      "title" : "An mdp-based recommender system",
      "author" : [ "G. Shani", "D. Heckerman", "R.I. Brafman" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2005
    }, {
      "title" : "Climf: Learning to maximize reciprocal rank with collaborative less-is-more filtering",
      "author" : [ "Y. Shi", "A. Karatzoglou", "L. Baltrunas", "M. Larson", "N. Oliver", "A. Hanjalic" ],
      "venue" : "In RecSys pages 139–146,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2012
    }, {
      "title" : "Scalable collaborative filtering approaches for large recommender systems",
      "author" : [ "G. Takács", "I. Pilászy", "B. Németh", "D. Tikk" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2009
    }, {
      "title" : "Scaling matrix factorization for recommendation with randomness",
      "author" : [ "L. Tang", "P. Harrington" ],
      "venue" : "In WWW Companion,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2013
    }, {
      "title" : "Mining concept-drifting data streams using ensemble classifiers",
      "author" : [ "H. Wang", "W. Fan", "P.S. Yu", "J. Han" ],
      "venue" : "In KDD,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2003
    }, {
      "title" : "Utilizing related products for post-purchase recommendation in e-commerce",
      "author" : [ "J. Wang", "B. Sarwar", "N. Sundaresan" ],
      "venue" : "In RecSys pages 329–332,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2011
    }, {
      "title" : "Opportunity model for e-commerce recommendation: right product; right time",
      "author" : [ "J. Wang", "Y. Zhang" ],
      "venue" : "In SIGIR pages 303–312,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2013
    }, {
      "title" : "Cofi rank-maximum margin matrix factorization for collaborative ranking",
      "author" : [ "M. Weimer", "A. Karatzoglou", "Q.V. Le", "A.J. Smola" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2007
    }, {
      "title" : "Learning to rank recommendations with the k-order statistic loss",
      "author" : [ "J. Weston", "H. Yee", "R.J. Weiss" ],
      "venue" : "In RecSys,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2013
    }, {
      "title" : "Temporal collaborative filtering with bayesian probabilistic tensor factorization",
      "author" : [ "L. Xiong", "X. Chen", "T.-K. Huang", "J.G. Schneider", "J.G. Carbonell" ],
      "venue" : "In SDM,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2010
    }, {
      "title" : "A study of thresholding strategies for text categorization",
      "author" : [ "Y. Yang" ],
      "venue" : "In SIGIR pages 137–145,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2001
    }, {
      "title" : "It takes variety to make a world: diversification in recommender systems",
      "author" : [ "C. Yu", "L. Lakshmanan", "S. Amer-Yahia" ],
      "venue" : "In EDBT pages 368–378,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2009
    }, {
      "title" : "Increasing temporal diversity with purchase intervals",
      "author" : [ "G. Zhao", "M.L. Lee", "W. Hsu", "W. Chen" ],
      "venue" : "In SIGIR,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "Recommender systems have been extensively studied in different domains including eCommerce [13], movie/music ratings [11], news personalization [4], content recommendation at web portals [2], etc.",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 9,
      "context" : "Recommender systems have been extensively studied in different domains including eCommerce [13], movie/music ratings [11], news personalization [4], content recommendation at web portals [2], etc.",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 2,
      "context" : "Recommender systems have been extensively studied in different domains including eCommerce [13], movie/music ratings [11], news personalization [4], content recommendation at web portals [2], etc.",
      "startOffset" : 144,
      "endOffset" : 147
    }, {
      "referenceID" : 0,
      "context" : "And all sorts of methods have been proposed for recommendation [1], including contentbased methods, neighborhood based approaches [5], latent",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 3,
      "context" : "And all sorts of methods have been proposed for recommendation [1], including contentbased methods, neighborhood based approaches [5], latent",
      "startOffset" : 130,
      "endOffset" : 133
    }, {
      "referenceID" : 9,
      "context" : "factor models like SVD or matrix factorization [11].",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 28,
      "context" : "We assume items are selected solely based on prediction scores, while researchers have been considering other factors like diversity [30], which is beyond the scope of this work.",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 0,
      "context" : "Hence, normalized discounted cumulative gain (NDCG) is proposed to normalize the DCG into [0, 1]:",
      "startOffset" : 90,
      "endOffset" : 96
    }, {
      "referenceID" : 17,
      "context" : "One commonly used approach for recommendation in eCommerce is to model user actions as a Markov chain [19, 18].",
      "startOffset" : 102,
      "endOffset" : 110
    }, {
      "referenceID" : 16,
      "context" : "One commonly used approach for recommendation in eCommerce is to model user actions as a Markov chain [19, 18].",
      "startOffset" : 102,
      "endOffset" : 110
    }, {
      "referenceID" : 11,
      "context" : "It is tantamount to computing item similarity [13], but keeping the metric directional.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 8,
      "context" : "[10, 9] proposed to have a time-dependent bias in matrix factorization for movie/music ratings.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 7,
      "context" : "[10, 9] proposed to have a time-dependent bias in matrix factorization for movie/music ratings.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 14,
      "context" : "We may randomly sample negative entries for our one-class collaborative filtering [16] problem, but that would essentially connect bias to the sampling rate, which is not acceptable either.",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 9,
      "context" : "Matrix factorization (MF) gained momentum thanks to the Netflix prize competition[11, 21].",
      "startOffset" : 81,
      "endOffset" : 89
    }, {
      "referenceID" : 19,
      "context" : "Matrix factorization (MF) gained momentum thanks to the Netflix prize competition[11, 21].",
      "startOffset" : 81,
      "endOffset" : 89
    }, {
      "referenceID" : 20,
      "context" : "Alternatively, we implemented a randomized version of matrix factorization as described in [22].",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 5,
      "context" : "It utilizes a randomized SVD [7] to compute approximate Q and then determines P given Q.",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 1,
      "context" : "Such a poor performance of matrix factorization is also observed in other domains with binary responses [3].",
      "startOffset" : 104,
      "endOffset" : 107
    }, {
      "referenceID" : 21,
      "context" : "Mining concept-drifting data streams [23] for classification and pattern mining has been studied extensively.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 8,
      "context" : "[10, 9] proposed to have a time-dependent bias in matrix factorization for movie/music ratings.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 7,
      "context" : "[10, 9] proposed to have a time-dependent bias in matrix factorization for movie/music ratings.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 14,
      "context" : "But the proposed method is not applicable for one-class collaborative filtering [16] problem.",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 26,
      "context" : "[28] formulate temporal collaborative filtering as a tensor factorization by treating time as one additional dimension.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 22,
      "context" : "[24, 25] consider the time gap between purchases and propose an opportunity model to identify not only the items to recommend, but also the best timing to recommend a particular product.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 23,
      "context" : "[24, 25] consider the time gap between purchases and propose an opportunity model to identify not only the items to recommend, but also the best timing to recommend a particular product.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 10,
      "context" : "Meanwhile, improving temporal diversity of recommendation across time [12, 31] is also considered.",
      "startOffset" : 70,
      "endOffset" : 78
    }, {
      "referenceID" : 29,
      "context" : "Meanwhile, improving temporal diversity of recommendation across time [12, 31] is also considered.",
      "startOffset" : 70,
      "endOffset" : 78
    }, {
      "referenceID" : 13,
      "context" : "Another related domain is learning to rank [15], which is initially motivated for the problem of information retrieval given queries.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 15,
      "context" : "Making recommendations by learning to rank has attracted lots of attentions recently [17, 27, 8].",
      "startOffset" : 85,
      "endOffset" : 96
    }, {
      "referenceID" : 25,
      "context" : "Making recommendations by learning to rank has attracted lots of attentions recently [17, 27, 8].",
      "startOffset" : 85,
      "endOffset" : 96
    }, {
      "referenceID" : 6,
      "context" : "Making recommendations by learning to rank has attracted lots of attentions recently [17, 27, 8].",
      "startOffset" : 85,
      "endOffset" : 96
    }, {
      "referenceID" : 12,
      "context" : "EigenRank [14] extends memory-based (or similarity-based) methods by considering the ranking (rather than rating) of items in computing user similarities.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 24,
      "context" : "For instance, CofiRank [26] extends matrix factorization to optimize ranking measures like NDCG instead of rating measures.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 18,
      "context" : "CLiMF [20] instead optimizes a lower bound of smooth reciprocal rank.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 27,
      "context" : "Our proposed bias learning method in collaborative filtering is partly inspired from the thresholding problem in multi-class/label classification [29, 6].",
      "startOffset" : 146,
      "endOffset" : 153
    }, {
      "referenceID" : 4,
      "context" : "Our proposed bias learning method in collaborative filtering is partly inspired from the thresholding problem in multi-class/label classification [29, 6].",
      "startOffset" : 146,
      "endOffset" : 153
    } ],
    "year" : 2015,
    "abstractText" : "This work focuses on top-k recommendation in domains where underlying data distribution shifts overtime. We propose to learn a time-dependent bias for each item over whatever existing recommendation engine. Such a bias learning process alleviates data sparsity in constructing the engine, and at the same time captures recent trend shift observed in data. We present an alternating optimization framework to resolve the bias learning problem, and develop methods to handle a variety of commonly used recommendation evaluation criteria, as well as large number of items and users in practice. The proposed algorithm is examined, both offline and online, using real world data sets collected from a retailer website. Empirical results demonstrate that the bias learning can almost always boost recommendation performance. We encourage other practitioners to adopt it as a standard component in recommender systems where temporal dynamics are a norm.",
    "creator" : "LaTeX with hyperref package"
  }
}