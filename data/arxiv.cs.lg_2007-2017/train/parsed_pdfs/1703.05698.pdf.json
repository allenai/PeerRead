{
  "name" : "1703.05698.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Bayesian Sketch Learning for Program Synthesis",
    "authors" : [ "Vijayaraghavan Murali", "Swarat Chaudhuri", "Chris Jermaine" ],
    "emails" : [ "<vijay@rice.edu>,", "<swarat@rice.edu>,", "<cmj4@rice.edu>." ],
    "sections" : [ {
      "heading" : null,
      "text" : "The key technical innovation of our approach — embodied in a system called BAYOU— is utilizing user-supplied evidence as to the program’s desired behavior, along with a Bayesian update, to obtain a posterior distribution over the program’s true, latent specification (indicating user intent), which in turn produces a posterior over possible sketches. As we show experimentally, explicitly modeling uncertainty in specification significantly increases the accuracy of the synthesis algorithm. We evaluate BAYOU’s ability to synthesize Java and Android methods. We find that using just a few example API sequences to communicate user intent, BAYOU can synthesize complex method bodies, some implementing tasks never encountered during training."
    }, {
      "heading" : "1. Introduction",
      "text" : "Automating computer programming is a long-standing goal in artificial intelligence. An important instance of this vision is (inductive) program synthesis (Biermann, 1978; Summers, 1977), the problem of learning a program automatically from examples of its behavior. There has been a recent resurgence of interest in this problem (Alur et al., 2013; Solar-Lezama et al., 2006; Gulwani, 2011; Feser et al., 2015), and tools solving versions of the problem have begun to see industrial adoption (Gulwani et al., 2015).\n1Rice University, USA. Correspondence to: Vijayaraghavan Murali <vijay@rice.edu>, Swarat Chaudhuri <swarat@rice.edu>, Chris Jermaine <cmj4@rice.edu>.\nExisting approaches to the problem commonly frame it as search over a space of programs (Alur et al., 2013). The goal is to find a program that can generate the example behaviors. In spite of much recent progress, these methods continue to struggle with two basic challenges.\nThe first challenge is generalizing from examples. In synthesis, one typically seeks to generate programs in an expressive language given a small number of examples. This means that it is often easy to find programs that are permitted by the language, fit the examples, but are meaningless. As an extreme example, the synthesizer could generate a monolithic if-statement that simply replays the example behaviors on appropriate inputs. Clearly, such a statement does not match user intent, does not generalize beyond the given examples, and has no tolerance for error in user input.\nThe second challenge is scalability. The space of programs relevant in a typical synthesis problem is vast, and only a few of these programs might fit a given set of examples. Finding such a program is a fundamentally hard problem.\nA popular way of addressing these challenges is to assume some syntactic constraints on the program that is to be synthesized (Alur et al., 2013). These constraints are imposed either by having a incomplete syntactic model, known as a program sketch (Solar-Lezama et al., 2006; Solar-Lezama, 2009), as part of the problem instance, or by restricting synthesis to a domain-specific language (DSL) (Gulwani, 2011; Barowy et al., 2015; Polozov & Gulwani, 2015).\nSuch syntactic constraints rule out programs that obviously violate user intent, making it easier to generalize from small example sets. They also reduce the computational difficulty of synthesis by limiting the search space of programs. At the same time, these constraints represent lowlevel knowledge and domain-specific assumptions, and compromise the goal of building a general-purpose automatic programming system. Finding alternatives to such restrictions is therefore a natural research question.\nIn this paper, we present such an alternative, in the form of a Bayesian statistical model that can automatically infer program sketches relevant to a given synthesis task. Our model is trained on a corpus of real-world programs, and during training, learns the relationship between the form and function of “typical” programs. During synthesis, the\nar X\niv :1\n70 3.\n05 69\n8v 1\n[ cs\n.P L\n] 1\n6 M\nar 2\n01 7\nmodel treats the user-supplied example set Θ as evidence as to the nature of the synthesis task. This evidence is used to infer a probabilistic process that can iteratively “grow” sketches, with a higher likelihood of producing sketches that are likely to solve the task. Synthesis now amounts to drawing samples from this process and fleshing them out into complete programs using combinatorial techniques.\nAn essential feature of our model is that it explicitly models the user intent or specification that lies behind an example set and completely defines the task at hand, as a latent variable Ψ. The process of inferring the likelihood P (X|Θ) of a sketch X given evidence is divided into two stages, one inferring P (Ψ|Θ) and the other inferring P (X|Ψ). As we show experimentally, explicitly taking into account uncertainty-in-specification via the posterior P (Ψ|Θ)—as opposed to simply utilizing a point estimate—significantly increases the ability of the synthesis algorithm to synthesize an appropriate code.\nWe realize this model using a neural architecture we call a Bayesian encoder-decoder (BED), trained using variational bounds (Kingma & Welling, 2014). A BED consists of an encoder network and a decoder network, linked via a latent random variable. Together, the networks enable a simple inference procedure for generalizing a set of user-given examples into an uncertain specification, then generating sketches that implement this specification.\nWe have implemented a prototype program synthesizer, called BAYOU, based on our approach. We evaluate BAYOU in the synthesis of API-heavy Java and Android methods from sequences of API calls made in the methods. Our experiments show that BAYOU can generate complex programs that are far out of reach of state-of-the-art approaches to synthesis, from a very small number (as low\nas one) of examples. Many of these programs implement tasks that are never encountered during training.\nThe rest of the paper is organized as follows. Sec. 2 presents a few motivating examples for our approach. Sec. 3 presents the methodology of Bayesian sketch learning; Sec. 4 discusses BEDs. The next few sections give more details on some of the key components of BAYOU, specifically the language of sketches (Sec. 6), the neural decoder (Sec. 6), and the techniques used to generate Java programs given sketches (Sec. 7). Experiments are presented in Sec. 8; Sec. 9 discusses related work; and Sec. 10 presents our conclusions."
    }, {
      "heading" : "2. Motivating Examples",
      "text" : "In this section, we illustrate some motivating use cases of our approach. Suppose a user wants to use the BAYOU system to synthesize a Java method that reads from a file. To achieve this, the user first writes an unfinished draft\nimport java.io.File; public class Program {\nvoid foo(String file){ ??\n} }\nand then provides some evidence in the form of an example set Θ. In BAYOU, example behaviors are sequences of API calls the synthesized program ought to make. Intuitively, these sequences capture “scenarios” that arise during the execution of the program.\nConcretely, suppose the user provides the sequences\nnew FR(..), new BR(..), BR.readLine(), BR.readLine(), BR.close()\nnew FR(..), new BR(..), BR.readLine(), BR.readLine(), BR.close(), java.lang.Throwable.printStackTrace().\n(We use BR and FR as abbreviations of FileReader and BufferedReader, respectively.) The first sequence captures the typical behavior of a program that reads lines from a buffer. The second captures the case where an exception is thrown; here the user wants to see a stack trace.\nOn such inputs, BAYOU can stochastically generate possible Java codes based upon the evidence, but in this case there is little uncertainty as to the desired code and BAYOU repeatedly generates the code shown in Fig. 1. Note that the program indeed reads from a file, and that it imports relevant packages and uses try-catch blocks, even though these were not directly present in the examples.\nNow suppose the user only gave the first example. The programs returned in this case still resemble the program in Fig. 1; however, the two catch-blocks are now typically empty. Uncertainty in the output continues to be low.\nFinally, consider a more complex scenario where the user wants to read input from an Android Bluetooth socket and write this input to a file. To do so, the user modifies the interface of the method foo as follows.\npublic void foo (File file, FileWriter writer, BluetoothSocket socket){ ?? }\nThey also provide the following example:\nnew ISR(...), new BR(...), new BW(...), BR.read(), BW.write(int), BR.read(), BR.close(), BR.close().\n(ISR and BW represent InputStreamReader and BufferedWriter respectively.) We note that this sequence does not directly mention a Bluetooth socket. All the synthesizer has to go by is that the constructor for ISR takes in an InputStream type as argument, and that a BluetoothSocket variable (some of whose methods return this type) is available as an input to foo.\nGiven the gross underspecification, there is more variance in the output of BAYOU this time. However, a number of programs in the top few results look like the program in Fig. 2, which achieve the desired functionality by calling BW.write with an input read from the socket.\nWe note that our training corpus did not contain an implementation of a program that uses a Bluetooth socket and a file in this manner. This is an illustration of how BAYOU\ncan generate solutions to programming tasks that it never encountered during training."
    }, {
      "heading" : "3. Bayesian Sketch Learning",
      "text" : "In this section, we formalize the methodology of Bayesian sketch learning. Given an example set Θ, this approach learns a probabilistic model that can iteratively derive a sketch using rules from a grammar. Concretely, the learned model lets us sample from a distribution P (Xi|xi−1,Θ), where Xi is a random variable capturing the i-th derivation step, and xi−1 is the list of steps derived up to (and including) step i − 1. To sample X ∼ P (X|Θ), corresponding to a complete sketch, the model first sets x0 = 〈〉. Now it samples X1 ∼ P (X1|x0,Θ), then X2 ∼ P (X2|x1,Θ), and so on, proceeding recursively until a terminal symbol is sampled.\nThe resulting abstraction is then passed to a synthesizer based on type-directed search (Feser et al., 2015), which attempts to use the abstraction to create a Java program that could have produced the user-supplied sequences. If the enumerative search fails for a particular sketch, we generate alternative, candidate sketches, until one is generated that can be used as the basis for an acceptable Java code.\nTo correctly generate candidate abstractions, our approach takes into account two separate sources of uncertainty:\n1. The uncertainty incurred due to under- (or incorrect) specification in the user-specified evidence Θ.\n2. The uncertainty associated with the identity of the correct sketch for a program, even if the user intent is clear.\nAs we will show experimentally, unless both sources of uncertainly are explicitly taken into account, it can be difficult to generate a sketch that can be converted into a correct program. To take into account both sources, we introduce the notion of a latent specification, or user intent—denoted by Ψ—with prior P (Ψ). Ψ is latent since, when learning a model from data, it is unrealistic to assume access to the actual specification.\nWhile it is unreasonable to assume access to Ψ directly, it is reasonable to assume that the generation of the usersupplied evidence Θ is conditional upon the latent specification, so P (Θ,Ψ) = P (Θ|Ψ)P (Ψ). That is, in some sense the user relies upon the intended specification to generate the evidence s/he presents to the system. The posterior distribution quantifying uncertainty-in-specification, P (Ψ|Θ), can then be obtained using Bayes’ rule.\nWe also assume that the synthesis of an abstraction is generated conditionally upon the latent specification, so P (X,Ψ) = P (X|Ψ)P (Ψ).\nPutting all of this together gives us: P (X|Θ) = ∫ Ψ P (Ψ)P (Θ|Ψ)P (X|Ψ)dΨ\nP (Θ) .\nUltimately, we are interested in drawing samples from this distribution to perform synthesis. This is difficult, due to the requirement that we integrate over all possible Ψ values. However, if we can instead sample from P (Ψ|Θ) ∝ P (Θ|Ψ)P (Ψ), our algorithm for synthesizing an abstraction given evidence Θ becomes:\n1. Sample Ψ ∼ P (Ψ|Θ) 2. Let x0 be 〈〉 3. For i ∈ {1...n} do:\n(a) Xi ∼ P (Xi|xi−1,Ψ)"
    }, {
      "heading" : "4. A Bayesian Encoder-Decoder",
      "text" : "To realize the algorithm to learn and sample sketches, we propose a Bayesian encoder-decoder (BED) architecture, where a latent variable with a Normal prior links an encoder and a decoder, and which is trained using a variational bound (Kingma & Welling, 2014). The approach we propose is Bayesian in the sense that it utilizes a Bayesian update to produce the posterior distribution P (Ψ|Θ).\nWe begin our development of the Bayesian encoderdecoder by noting that for a particular (X,Θ) pair, the KL divergence between P (Ψ|Θ) and P (Ψ|X) is:\nDKL[P (Ψ|Θ)||P (Ψ|X)] =EΨ∼P (Ψ|Θ)[logP (Ψ|Θ)− logP (Ψ|X)]\n=EΨ∼P (Ψ|Θ)\n[ logP (Ψ|Θ)− log P (X|Ψ)P (Ψ)\nP (X) ] =EΨ∼P (Ψ|Θ)[logP (Ψ|Θ)− logP (X|Ψ)−\nlogP (Ψ)] + logP (X)\nAnd so:\nlogP (X)−DKL[P (Ψ|Θ)||P (Ψ|X)] =− EΨ∼P (Ψ|Θ)[logP (Ψ|Θ)− logP (X|Ψ)− logP (Ψ)] =EΨ∼P (Ψ|Θ) logP (X|Ψ)−DKL[P (Ψ|Θ)||P (Ψ)]\n(1)\nThen by maximizing, via stochastic gradient ascent, the expected value of the right-hand-side of the above equation over (X,Θ) pairs obtained from a training set, we in turn maximize the expected value of logP (X) − DKL[P (Ψ|Θ)||P (Ψ|X)] over the training data.\nThe resulting maximization problem makes sense intuitively as it simultaneously (a) attempts to maximize the log-likelihood of each observed sketch in the training data,\nwhile (b) minimizing the divergence between the two posteriors P (Ψ|Θ) and P (Ψ|X). The latter minimization ensures that for each training pair (Θ0, X0), the distribution P (Ψ|Θ0) can be used to be used to encode Θ0 in such a way that it is likely to be decoded into X0. Specifically, let Q(X|X0) = ∫ P (Ψ|X0)P (X|Ψ)dΨ; this is the distribution that would be obtained by autoencoding/decoding X0 (that is, sampling Ψ ∼ P (Ψ|X0) and then sampling X ∼ P (X|Ψ)). If DKL[P (Ψ|Θ0)||P (Ψ|X0)] = 0, it ensures that P (X|Θ0) = Q(X|X0), and so the learned distribution will encode/decode Θ0 as efficiently as it will autoencode/decode X0.\nSo far, this resembles a variational autoencoder, with the obvious difference that we are learning to encode/decode. However, there is a more important difference. Specifically, we posit the following, hierarchical generative process for Ψ and Θ:\nΨ ∼ Normal(~0, I) f(θi) ∼ Normal(Ψ, σ2I) for i ∈ {1...m}\nHere, f(.) is a differentiable function that encodes θi ∈ Θ. For example, our implementation BAYOU, which takes API call sequences as input, utilizes an RNN to realize f(.). σ2 is an appropriately-chosen hyperparameter.\nFurther, since a multivariate Normal is conjugate for the mean of a multivariate Normal, if f(.) is invertible, we have\n(Ψ|Θ) = Normal\n( σ−2\n1 +mσ−2 ∑ θ∈Θ f(θ), 1 1 +mσ−2 I\n) .\n(2)\nAfter encoding each θ ∈ Θ separately, the Bayesian encoder-decoder utilizes Bayes’ rule to take into account the multiple pieces of evidence present in Θ, producing a proper posterior on Ψ given Θ.\nSince there is a simple closed-form for P (Ψ|Θ), DKL[P (Ψ|Θ)||P (Ψ)] in Equation 1 can easily be computed as\nk 2 + 2mσ−2 − k 2\n( 1 + log\n1\n1 +mσ−2\n) +\n1\n2\n( 1\n1 +mσ−2 ∑ θ∈Θ f(θ)\n)T ( 1\n1 +mσ−2 ∑ θ∈Θ f(θ)\n) .\nwhere k is the dimensionality of the latent space.\nNote that, strictly speaking, Equation 2 need only hold if f(.) is invertible; if f(.) is not invertible, then P (θ) need not equal P (f(θ)); in fact, P (θ) = ∑ θ′|f(θ′)=f(θ) P (f(θ\n′)). In the case of a non-invertible f(.), a slightly different formulation of the problem is required that maintains the distinction between P (θ) and P (f(θ)), though the resulting algorithm remains the same.\nDiscussion. The BED architecture is parameterized on σ2, which controls the relative strength of the evidence Θ relative to the prior. Interestingly, we found that a variance of σ2 = 1 works well in practice. One might expect that a smaller variance is required (given the unit normal prior on Ψ) or else each evidential point in Θ will only weakly update on the prior for Ψ. This is not what our experiments show. One reason may be that in a high dimensional space, even a small change in the posterior mean along each dimension can add up to a significant shift of the distribution overall. Further, a smaller variance may not be desirable, as it is important to retain an appropriate amount of uncertainty as to the position of Ψ. Otherwise, synthesis may suffer as a reasonable value of Ψ is never sampled.\nAnother issue is the precise form of the distribution P (X|Ψ). In practice, this is implemented as a neural decoder that takes as input Ψ and then uses this to generate a vector of probabilities corresponding to the possible productions that could fire in our abstraction language (see Sec. 5). Once the first production X1 is sampled using this vector, the decoder uses Ψ and x1 to generate a vector of probabilities corresponding to X2. After sampling X2 we use x2 to generate the vector of probabilities corresponding toX3. This process is continued until the entire abstraction is generated. See Sec. 6 for more details."
    }, {
      "heading" : "5. Language Abstractions",
      "text" : "Now we present some key details of the sketches used in BAYOU. Formally, a sketch is a statement S generated by the grammar in Fig. 3. The grammar allows syntax for calls to a predefined set of API methods, branches, loops, no-ops (skip), statement sequencing, and exception handling. The abstraction does not record details like variable names or local assignments. A call to an API method is abstracted by the method’s name a and the types τ1, . . . , τk of its arguments. A boolean guard Cond for a loop or a branch is abstracted by the set of API calls that appear in the guard.\nA behavior θ is a sequence of symbols Call that each abstracts an API call.\nWhile choosing a language for sketches, one faces two competing goals. For faster combinatorial synthesis, we want sketches to have sufficient detail about a program’s structure. On the other hand, to enable statistical learning,\n(a)\ntry ((while {BR.readline()} do skip);BR.close()) catch (IOException) JLT .printStackTrace() catch (FileNotFoundException)\nJLT .printStackTrace()\nwe want sketches to consist of features, such as API names and types, that are broadly shared across programs. Our grammar offers a compromise between these goals.\nWe abstract Java programs into sketches using a standard syntax-directed translation that transforms the abstract syntax tree (AST) of a Java program node by node. Any statement that falls outside the scope of our sketches — for example updates to local variables — is replaced by skip. As a concrete example, consider again the program in Fig. 1. The sketch for the method foo is shown in Fig. 4(a). To generate Θ, we construct a standard control flow graph (CFG) representation of F , where nodes are locations in the program and edges are operations (such as API calls) that are executed between two locations. Behaviors in Θ are now generated through exploring all paths in the CFG."
    }, {
      "heading" : "6. Neural Decoder",
      "text" : "The task of the neural decoder is to implement the sampler for X ∼ P (X|Ψ). This is implemented recursively via repeated samples drawn as Xi ∼ P (Xi|xi−1,Ψ). Since a sketch is tree-structured, we use a top-down tree-structured RNN similar to (Zhang et al., 2016). The generation of each Xi requires the generation of a new “path” from a series of previous “paths”, where each path corresponds to a series of production rules fired in the grammar.\nSimilar to the notion of a “dependency path” in (Zhang et al., 2016), we define a production path as a sequence of pairs 〈(v1, e1), (v2, e2), . . . , (vk, ek)〉 where vi is a node in the sketch (i.e., a term in the grammar) and ei is the type of edge that connects vi with vi+1. Our representation has\ntwo types of edges: sibling and child. A sibling edge connects two nodes at the same level of the tree that are under the same parent node (i.e., RHS of the same rule), whereas a child edge connects a node with another that is one level deeper in the tree (i.e., the LHS and RHS of a rule).\nThe root of the entire tree is a special node named root, and so the first pair in all production paths is (root, child). Also, the last edge in a production path is irrelevant (·) as it does not connect the node to any subsequent nodes.\nAs an example, consider the sketch of a program in Figure 4(b). There are five production paths in this tree (for brevity, the first pair (root, c) is not shown, and assume s and c stand for sibling and child edges respectively):\n• 〈(try, c), (while, c), (Cond, c), (readLine(), ·)〉 • 〈(try, c), (while, c), (Cond, s), (do, c), (skip, ·)〉 • 〈(try, c), (while, s), (close(), ·)〉 • 〈(try, s), (catch, c), (IOException, s),\n(printStackTrace(), ·)〉 • 〈(try, s), (catch, s), (catch, c), (FNFException, s),\n(printStackTrace(), ·)〉\nGiven a specification Ψ and a sequence of pairs xi = 〈(v1, e1), . . . , (vi, ei)〉 along a production path, the next node in the path is assumed to be dependent solely on Ψ and xi. Therefore, a single inference step of the decoder computes the probability P (vi+1|xi,Ψ). To do this, the decoder uses two RNNs, one for each type of edge, that act on the production pairs in xi. First, as is usual, all nodes vi are converted into their one-hot vector encoding, v′i.\nLet Weh ∈ Rd×d and beh ∈ Rd be the hidden state weight and bias matrices of the RNN, Wev ∈ R|G|×d and bev ∈ Rd be the input weight and bias matrices, and Wey ∈ Rd×|G| and bey ∈ R|G| be the output weight and bias matrices, where (i) e is the type of edge: either c (child) or s (sibling), (ii) |G| is the size of the output vocabulary – the number of terminals and non-terminals in the grammar, and (iii) d is the size of the RNN’s hidden state. Note that the two RNNs have different parameters, but their hidden state is shared. We also use “lifting” matrices Wl ∈ Rk×d and bl ∈ Rd, where k is the dimensionality of the latent space, to lift the latent specification onto the (typically) higher-dimensional hidden state space d of the RNNs.\nLet hi and yi be the hidden state and output of the network at time point i. We compute these quantities as follows:\nh0 = Wl.Ψ + bl (3)\nhci = W c h . hi−1 + b c h + W c v . v ′ i + b c v hsi = W s h . hi−1 + b s h + W s v . v ′ i + b s v\nhi = { tanh(hci ) if ei = c\ntanh(hsi ) if ei = s (4)\nyi = { softmax(Wcy . hi + bcy) if ei = c\nsoftmax(Wsy . hi + b s y) if ei = s\n(5)\nThe two RNNs take turns to update their (shared) hidden state and output depending on the type of edge at time i. During training, v′i, ei and the target output are known from the data point, and so we optimize a cross-entropy loss function (over all i) between the output yi of the RNN and the target output. During inference, P (vi+1|xi,Ψ) is the distribution given by the softmax output of yi.\nA sketch (tree) is obtained by starting with the root node pair (v1, e1) = (root, child), recursively applying Equation 5 to get the output yi, sampling vi+1 from yi, and growing the tree by adding the sampled node. The edge ei+1 is provided as c or s depending on the vi+1 that was sampled. If only one type of edge is feasible (for instance, if the node is a terminal in the grammar, only a sibling edge is possible with the next node), then only that edge is provided. If both edges are feasible, then both possibilities are recursively explored, growing the tree in both directions.\nRemarks. In our implementation, we generate trees in a depth-first fashion, by exploring a child edge before a sibling edge if both are possible. If a node has two children, a neural encoding of the nodes that were generated on the left is carried onto the right sub-tree so that the generation of this tree can leverage additional information about its previously generated sibling. We refer the reader to Section 2.4 of (Zhang et al., 2016) for more details."
    }, {
      "heading" : "7. Program Synthesis",
      "text" : "Now we describe the main elements of the combinatorial search used to synthesize a Java programs from a given sketch S. A key feature of this “concretization” step is that it does not alter the syntactic structure of S. Instead, it replaces types and abstract guards of loops and branches in S by concrete Java expressions.\nMore precisely, our concretization procedure recursively transforms the syntax tree of a sketch. In each recursive invocation, its parameters include a sketch S and an environment E that defines the names and types of local variables and formal parameters in scope. Now we highlight three important cases of this transformation.\n• Suppose S is “a(τ1, . . . , τk)”, where a is an API method name and each τi is a type. If the return type of a is void, the algorithm concretizes the sketch into a Java method invocation a(e1, . . . , ek), where each ei is a Java expression of type τi. Otherwise, it generates the statement z = a(e1, . . . , ek), where z is a fresh local variable. The expressions ei can refer to API methods and variables defined in E . These expressions are identified through type-directed enumeration (Feser et al., 2015).\n• Suppose S is “while ({a1(. . . ), . . . , ak(. . . )}) do S′”. Then S is transformed to a Java program while e {J ′}. Here, e is a boolean expression, found by type-guided enumeration, that can make calls to the API methods ai as well as variables in the environment E . The statement J ′ is obtained by transforming S′ under a new environment, obtained by adding variables declared within e to E .\n• Suppose S is “try S′ catch(τ1) S1”. This sketch is concretized into a program “try { J ′ } catch (τ1 z) { J1 }”, where J ′ is a Java program obtained by recursively concretizing S′ under environment E , z is a fresh variable name for an exception, and J1 is obtained by concretizing S1 under an environment that augments E with the name z."
    }, {
      "heading" : "8. Experiments",
      "text" : "In evaluating our method, we seek answers to the following questions:\n• How does BAYOU perform in terms of its ability to synthesize programs?\n• Does the central technical idea in this paper—that using a Bayesian framework to explicitly model the uncertainty incurred mapping evidence to specification—actually result in greater accuracy in synthesized programs?\n• How good is BAYOU at synthesizing solutions to programming tasks that were not encountered at training time?\nData sets. To answer these questions, we utilized two repositories (Linstead et al., 2009; Androiddrawer), which together provided a total of 37,000 Java and Android projects. Recall that our grammar of sketches is parameterized on a set of API methods. These methods were picked from classes in several commonly used Java APIs such as java.io, java.lang.Throwable (the interface for exceptions) and javax.crypto, and Android APIs such as AlertDialog, BluetoothSocket and Camera.\nSince the projects used are stored at the bytecode and APK level, we used well-known off-the-shelf decompilers CFR (Java) and JADX (Android) to generate source code for these projects. Applied to methods (henceforth called “programs”) in this code, our data extraction process was able to extract 15,300 sketches in the abstract DSL. 12,000 of these were randomly selected and used for training, and the remaining 3,300 for testing, so that there was no overlap. On average, there were 5 sequences of API calls extracted from each program (median 4, maximum 16).\nImplementation and training. BAYOU uses TensorFlow to learn and employ the Bayesian encoder-decoder, and the Eclipse Java AST model to implement the abstract DSL. Our implementation uses 8 hidden units in the encoder, 128 hidden units in the decoder and an 8-dimensional latent space.\nThe training was performed on a quad-core machine with 12GB of memory and an NVIDIA K80 GPU. As each sketch was broken down into a set of production paths, the total number of data points fed to the neural network was 17,900 per epoch. Training for 150 epochs took around 5 hours for the Bayesian model, and around 4 hours for the non-Bayesian model because of its fewer parameters.\nA 2-dimensional visualization of the latent space learned by BAYOU is shown in Figure 5. 1,000 Ψ values are sampled from a unit normal, and for each Ψ value, a sketch is generated. The class of the initial API call made by the sketch is color-coded in the plot. The evident clustering indicates that the model has learned to distinguish different specifications from each other.\nExperiments. After training, for each of the 3,300 test programs, we hid the actual program and instead generated a set of sequences of API calls from it. A subset of the sequences were provided to BAYOU, which was used to stochastically synthesize ten candidate programs. The average time taken to synthesize a program was 2 seconds (median 2s, maximum 9s). On average, each synthesized program contained 14 lines of code (median 15, maximum 34), including import and variable declarations. We then compared the syntax and semantics of the original and synthesized programs using four metrics:\n(a) the minimum Jaccard distance between the set of sequences generated by the original program (P ) and those generated by the synthesized programs (P ′),\n(b) the minimum Jaccard distance between the set of API calls made in the original program and those made in the synthesized programs,\n(c) minimum of the difference between the number of statements in the original program and the synthesized programs, proportional to that of the original program\n(d) minimum of the difference between the number of control structures (branches, loops and exception handlers) in the original program and the synthesized programs, proportional to that of the original program.\nOur goal was to evaluate each synthesizer’s ability to generate a code that correctly generalizes from a small, incomplete set of sequences. Hence, we computed each metric as a function of the fraction of the sequences generated from the test code. This fraction was decreased from 100% down to 10%. To evaluate whether the explicit use of uncertainty matters, we constructed a synthesizer based on a non-Bayesian, non-probabilistic encoder-decoder, where the encoder concatenates the hidden representation of each sequence to form the (deterministic) encoding for the set. This encoder can be seen to implement the high-level ideas of the encoder used in (Parisotto et al., 2016), although low-level details may differ as their inputs are I/O exam-\nples. The decoder then uses the decoder of Sec. 6 to map the encoding into a sequence of production rule firings. The statistics described above were also computed for this “non-Bayesian” synthesizer.\nWe did not compare our method with existing non-datadriven synthesizers for a simple reason: our benchmarks are significantly beyond the scope of these tools. For example, JSKETCH (Jeon et al., 2015) synthesizes Java code, but expects the input to include a detailed program sketch. While tools like λ2 (Feser et al., 2015) do not require sketches, they assume restricted programming languages and cannot synthesize programs that use complex APIs.\nThe four plots in Figure 6 show the average scores obtained by BAYOU and the non-Bayesian synthesizer across the ten codes synthesized for each of the 3,300 test programs, plotted versus the percentage of sequences from the original program that are provided as input to the model.\nWe note that when the provided fraction of sequences from the original program is close to 100%, both synthesizers perform similarly. However, when this fraction is reduced, BAYOU is much more robust. For example, when only 10% of the sequences extracted from the test code (we round up if this number is not integer) are provided to the synthesizer, the Jaccard distance between the sets of sequences generated by the synthesized code compared to the original code is close to 0.6 for BAYOU, but 0.9 for the nonBayesian synthesizer. The Jaccard distances for the two methods over synthesized and original sets of API calls are 0.2 and 0.7 respectively. Even the much simpler metrics — the number of synthesized program statements and the number of synthesized control flow structures show a significant difference.\nTo evaluate the ability of BAYOU to synthesize programs that it has never seen before, we measured the minimum distance between the sketch of each test program and the entire training corpus using the Zhang-Shasha tree-edit distance metric (Zhang & Shasha, 1989), and using string edit distance for node labels. We then collected the top-5% (i.e., 165 out of 3300) test programs that were most distant from the corpus. These programs can be seen as “outliers” in a traditional outlier-detection sense, and their distances are plotted in Figure 7.\nWe then repeated the same experiment as before, collecting the four metrics, but only over these 165 programs. These metrics, plotted in Figure 8, are worse than before for both models as expected, since we are specifically considering the worst 5% of cases. However, it can still be seen that BAYOU outperforms the non-Bayesian synthesizer.\nWe picked a particular example to highlight the ability of BAYOU to deal with completely new scenarios that it never encountered before. Consider the program in Figure 9(a), picked from these top-5% outliers (we omitted the import declarations for brevity), whose particular programming task did not appear in the training data, evident by its non-zero distance (specifically, 40) from the corpus. Nevertheless, given just the sequences from this program, BAYOU was able to synthesize the program in Figure 9(b), that closely resembles the original. Most of the differences were harmless, for instance, calling close on one reader/writer object is sufficient, and there are no exceptions other than the ones caught that can be thrown by the code. This shows that even when provided with a task that BAYOU has never seen before, it is able to synthesize a program reasonably close to the desired one."
    }, {
      "heading" : "9. Related work",
      "text" : "Many recent efforts in the programming language community (Solar-Lezama et al., 2006; Gulwani, 2011; Albarghouthi et al., 2013; Feser et al., 2015; Osera & Zdancewic, 2015) target synthesis using search and constraint solving. A consensus here is that to scale synthesis and generalize well from examples, one must syntactically limit the space of feasible programs (Alur et al., 2013). This is done either by adding a human-provided sketch to a problem instance (Solar-Lezama et al., 2006), or by restricting synthesis a priori to a narrow DSL (Gulwani, 2011; Barowy et al., 2015; Polozov & Gulwani, 2015). In contrast, our approach starts with a general-purpose language and infers the syntactic constraints relevant for a synthesis task dynamically. Also, unlike prior approaches, we directly model uncertainty in user input. At the same time, because our method completes inferred sketches using tools from syntax-guided synthesis, it can leverage advances in this area. For example, the BAYOU system uses type-guided search techniques developed in prior work (Feser et al., 2015).\nThere is also an emerging body of work on program synthesis in the deep learning community. One category of methods frame synthesis as the problem of learning an endto-end differentiable, neural model of a program (Graves et al., 2014; Kurach et al., 2015; Neelakantan et al., 2015; Reed & De Freitas, 2015; Joulin & Mikolov, 2015). A fundamental difference between these methods and ours is that our programs are in a high-level, interpretable representation, as opposed to neural nets. A second category of methods learn interpretable programs (Gaunt et al., 2016; Riedel et al., 2016; Bunel et al., 2016), but requires new learning for each synthesis task. In contrast, our method learns a\nbody of general knowledge only once.\nA third category of methods seek to learn reusable synthesizers that can produce programs in an interpretable DSL. DeepCoder (Balog et al., 2016) uses a neural network to predict binary attributes of the solution to a synthesis problem, and uses these predictions to set goals for a combinatorial synthesizer. In contrast, our neural net generates an unbounded sketch of a program. In neuro-symbolic synthesis (Parisotto et al., 2016), a neural architecture is used to encode a set of input examples and decode the resulting representation into a program in a DSL. This method is similar to the non-Bayesian method from in Sec. 8.\nAnother body of related work learns hidden specifications (Raychev et al., 2015) and syntactic patterns (Allamanis & Sutton, 2014; Nguyen et al., 2013; Nguyen & Nguyen, 2015; Raychev et al., 2015; Bielik et al., 2016) from code in large software repositories. In general, these methods do not learn the sort of association between syntax and behavior that is key to our approach. The two most relevant efforts (Raychev et al., 2014; 2016) use statistical language models for code completion. However, unlike in the present paper, there is no end-to-end probabilistic interpretation of synthesis, and no direct model of user intent."
    }, {
      "heading" : "10. Conclusion",
      "text" : "We have given a data-driven, probabilistic method for program synthesis from example behaviors. Our central contribution is a Bayesian probabilistic model, trained on realworld code, that treats user-given examples as evidence towards the specification and structure of a program. This information can be used to infer sketches of likely solutions\nto a synthesis problem, which can then be used to drive combinatorial methods for synthesis. We have shown that the model has an efficient realization in the form of BEDs. Finally, we have presented an implementation, BAYOU, for the concrete context of synthesizing API-heavy Android and Java code, and shown that it can often generate nontrivial programs from a very small number of examples.\nAcknowledgements This research was supported by DARPA MUSE award #FA8750-14-2-0270 and a Google Research Award. The views, opinions, and/or findings contained in this article are those of the authors and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government."
    } ],
    "references" : [ {
      "title" : "Recursive program synthesis",
      "author" : [ "Albarghouthi", "Aws", "Gulwani", "Sumit", "Kincaid", "Zachary" ],
      "venue" : "In CAV,",
      "citeRegEx" : "Albarghouthi et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Albarghouthi et al\\.",
      "year" : 2013
    }, {
      "title" : "Mining idioms from source code",
      "author" : [ "Allamanis", "Miltiadis", "Sutton", "Charles" ],
      "venue" : "In Proceedings of the 22Nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,",
      "citeRegEx" : "Allamanis et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Allamanis et al\\.",
      "year" : 2014
    }, {
      "title" : "Syntax-guided synthesis",
      "author" : [ "Alur", "Rajeev", "Bodı́k", "Rastislav", "Juniwal", "Garvit", "Martin", "Milo M. K", "Raghothaman", "Mukund", "Seshia", "Sanjit A", "Singh", "Rishabh", "Solar-Lezama", "Armando", "Torlak", "Emina", "Udupa", "Abhishek" ],
      "venue" : "In FMCAD, pp",
      "citeRegEx" : "Alur et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Alur et al\\.",
      "year" : 2013
    }, {
      "title" : "Deepcoder: Learning to write programs",
      "author" : [ "Balog", "Matej", "Gaunt", "Alexander L", "Brockschmidt", "Marc", "Nowozin", "Sebastian", "Tarlow", "Daniel" ],
      "venue" : "arXiv preprint arXiv:1611.01989,",
      "citeRegEx" : "Balog et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Balog et al\\.",
      "year" : 2016
    }, {
      "title" : "PHOG: probabilistic model for code",
      "author" : [ "Bielik", "Pavol", "Raychev", "Veselin", "Vechev", "Martin T" ],
      "venue" : "In ICML, pp",
      "citeRegEx" : "Bielik et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Bielik et al\\.",
      "year" : 2016
    }, {
      "title" : "The inference of regular LISP programs from examples",
      "author" : [ "Biermann", "Alan W" ],
      "venue" : "IEEE transactions on Systems, Man, and Cybernetics,",
      "citeRegEx" : "Biermann and W.,? \\Q1978\\E",
      "shortCiteRegEx" : "Biermann and W.",
      "year" : 1978
    }, {
      "title" : "Adaptive neural compilation",
      "author" : [ "Bunel", "Rudy R", "Desmaison", "Alban", "Mudigonda", "Pawan K", "Kohli", "Pushmeet", "Torr", "Philip" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Bunel et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Bunel et al\\.",
      "year" : 2016
    }, {
      "title" : "Synthesizing data structure transformations from input-output examples",
      "author" : [ "Feser", "John K", "Chaudhuri", "Swarat", "Dillig", "Isil" ],
      "venue" : "In PLDI,",
      "citeRegEx" : "Feser et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Feser et al\\.",
      "year" : 2015
    }, {
      "title" : "Terpret: A probabilistic programming language for program induction",
      "author" : [ "Gaunt", "Alexander L", "Brockschmidt", "Marc", "Singh", "Rishabh", "Kushman", "Nate", "Kohli", "Pushmeet", "Taylor", "Jonathan", "Tarlow", "Daniel" ],
      "venue" : "arXiv preprint arXiv:1608.04428,",
      "citeRegEx" : "Gaunt et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Gaunt et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural turing machines",
      "author" : [ "Graves", "Alex", "Wayne", "Greg", "Danihelka", "Ivo" ],
      "venue" : "arXiv preprint arXiv:1410.5401,",
      "citeRegEx" : "Graves et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2014
    }, {
      "title" : "Automating string processing in spreadsheets using input-output examples",
      "author" : [ "Gulwani", "Sumit" ],
      "venue" : "In POPL,",
      "citeRegEx" : "Gulwani and Sumit.,? \\Q2011\\E",
      "shortCiteRegEx" : "Gulwani and Sumit.",
      "year" : 2011
    }, {
      "title" : "Inductive programming meets the real world",
      "author" : [ "Gulwani", "Sumit", "Hernández-Orallo", "José", "Kitzelmann", "Emanuel", "Muggleton", "Stephen H", "Schmid", "Ute", "Zorn", "Benjamin" ],
      "venue" : "Commun. ACM,",
      "citeRegEx" : "Gulwani et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gulwani et al\\.",
      "year" : 2015
    }, {
      "title" : "Jsketch: Sketching for java",
      "author" : [ "Jeon", "Jinseong", "Qiu", "Xiaokang", "Foster", "Jeffrey S", "Solar-Lezama", "Armando" ],
      "venue" : "In Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,",
      "citeRegEx" : "Jeon et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Jeon et al\\.",
      "year" : 2015
    }, {
      "title" : "Inferring algorithmic patterns with stack-augmented recurrent nets",
      "author" : [ "Joulin", "Armand", "Mikolov", "Tomas" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Joulin et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Joulin et al\\.",
      "year" : 2015
    }, {
      "title" : "Auto-encoding variational bayes",
      "author" : [ "Kingma", "Diederik P", "Welling", "Max" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Kingma et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2014
    }, {
      "title" : "Neural random-access machines",
      "author" : [ "Kurach", "Karol", "Andrychowicz", "Marcin", "Sutskever", "Ilya" ],
      "venue" : "arXiv preprint arXiv:1511.06392,",
      "citeRegEx" : "Kurach et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kurach et al\\.",
      "year" : 2015
    }, {
      "title" : "Sourcerer: Mining and searching internet-scale software repositories",
      "author" : [ "Linstead", "Erik", "Bajracharya", "Sushil", "Ngo", "Trung", "Rigor", "Paul", "Lopes", "Cristina", "Baldi", "Pierre" ],
      "venue" : "Data Min. Knowl. Discov.,",
      "citeRegEx" : "Linstead et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Linstead et al\\.",
      "year" : 2009
    }, {
      "title" : "Neural programmer: Inducing latent programs with gradient descent",
      "author" : [ "Neelakantan", "Arvind", "Le", "Quoc V", "Sutskever", "Ilya" ],
      "venue" : "arXiv preprint arXiv:1511.04834,",
      "citeRegEx" : "Neelakantan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Neelakantan et al\\.",
      "year" : 2015
    }, {
      "title" : "Graph-based statistical language model for code",
      "author" : [ "Nguyen", "Anh Tuan", "Tien N" ],
      "venue" : "In Proceedings of the 37th International Conference on Software Engineering - Volume 1,",
      "citeRegEx" : "Nguyen et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2015
    }, {
      "title" : "A statistical semantic language model for source code",
      "author" : [ "Nguyen", "Tung Thanh", "Anh Tuan", "Hoan Anh", "Tien N" ],
      "venue" : "In Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,",
      "citeRegEx" : "Nguyen et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2013
    }, {
      "title" : "Type-andexample-directed program synthesis",
      "author" : [ "Osera", "Peter-Michael", "Zdancewic", "Steve" ],
      "venue" : "In PLDI,",
      "citeRegEx" : "Osera et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Osera et al\\.",
      "year" : 2015
    }, {
      "title" : "Neuro-symbolic program synthesis",
      "author" : [ "Parisotto", "Emilio", "Mohamed", "Abdel-rahman", "Singh", "Rishabh", "Li", "Lihong", "Zhou", "Dengyong", "Kohli", "Pushmeet" ],
      "venue" : "arXiv preprint arXiv:1611.01855,",
      "citeRegEx" : "Parisotto et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Parisotto et al\\.",
      "year" : 2016
    }, {
      "title" : "Flashmeta: A framework for inductive program synthesis",
      "author" : [ "Polozov", "Oleksandr", "Gulwani", "Sumit" ],
      "venue" : "ACM SIGPLAN Notices,",
      "citeRegEx" : "Polozov et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Polozov et al\\.",
      "year" : 2015
    }, {
      "title" : "Code completion with statistical language models",
      "author" : [ "Raychev", "Veselin", "Vechev", "Martin", "Yahav", "Eran" ],
      "venue" : "In Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation,",
      "citeRegEx" : "Raychev et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Raychev et al\\.",
      "year" : 2014
    }, {
      "title" : "Predicting program properties from big code",
      "author" : [ "Raychev", "Veselin", "Vechev", "Martin", "Krause", "Andreas" ],
      "venue" : "In POPL,",
      "citeRegEx" : "Raychev et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Raychev et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning programs from noisy data",
      "author" : [ "Raychev", "Veselin", "Bielik", "Pavol", "Vechev", "Martin T", "Krause", "Andreas" ],
      "venue" : "In Proceedings of the 43rd Annual ACM SIGPLANSIGACT Symposium on Principles of Programming Languages,",
      "citeRegEx" : "Raychev et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Raychev et al\\.",
      "year" : 2016
    }, {
      "title" : "Programming with a differentiable forth",
      "author" : [ "Riedel", "Sebastian", "Bosnjak", "Matko", "Rocktäschel", "Tim" ],
      "venue" : "interpreter. CoRR,",
      "citeRegEx" : "Riedel et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Riedel et al\\.",
      "year" : 2016
    }, {
      "title" : "The sketching approach to program synthesis",
      "author" : [ "Solar-Lezama", "Armando" ],
      "venue" : "In Asian Symposium on Programming Languages and Systems,",
      "citeRegEx" : "Solar.Lezama and Armando.,? \\Q2009\\E",
      "shortCiteRegEx" : "Solar.Lezama and Armando.",
      "year" : 2009
    }, {
      "title" : "Combinatorial sketching for finite programs",
      "author" : [ "Solar-Lezama", "Armando", "Tancau", "Liviu", "Bodı́k", "Rastislav", "Seshia", "Sanjit A", "Saraswat", "Vijay A" ],
      "venue" : "In ASPLOS,",
      "citeRegEx" : "Solar.Lezama et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Solar.Lezama et al\\.",
      "year" : 2006
    }, {
      "title" : "A methodology for LISP program construction from examples",
      "author" : [ "Summers", "Phillip D" ],
      "venue" : "Journal of the ACM (JACM),",
      "citeRegEx" : "Summers and D.,? \\Q1977\\E",
      "shortCiteRegEx" : "Summers and D.",
      "year" : 1977
    }, {
      "title" : "Simple fast algorithms for the editing distance between trees and related problems",
      "author" : [ "Zhang", "Kaizhong", "Shasha", "Dennis" ],
      "venue" : "SIAM journal on computing,",
      "citeRegEx" : "Zhang et al\\.,? \\Q1989\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 1989
    }, {
      "title" : "Topdown tree long short-term memory networks. In NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
      "author" : [ "Zhang", "Xingxing", "Lu", "Liang", "Lapata", "Mirella" ],
      "venue" : null,
      "citeRegEx" : "Zhang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "There has been a recent resurgence of interest in this problem (Alur et al., 2013; Solar-Lezama et al., 2006; Gulwani, 2011; Feser et al., 2015), and tools solving versions of the problem have begun to see industrial adoption (Gulwani et al.",
      "startOffset" : 63,
      "endOffset" : 144
    }, {
      "referenceID" : 28,
      "context" : "There has been a recent resurgence of interest in this problem (Alur et al., 2013; Solar-Lezama et al., 2006; Gulwani, 2011; Feser et al., 2015), and tools solving versions of the problem have begun to see industrial adoption (Gulwani et al.",
      "startOffset" : 63,
      "endOffset" : 144
    }, {
      "referenceID" : 7,
      "context" : "There has been a recent resurgence of interest in this problem (Alur et al., 2013; Solar-Lezama et al., 2006; Gulwani, 2011; Feser et al., 2015), and tools solving versions of the problem have begun to see industrial adoption (Gulwani et al.",
      "startOffset" : 63,
      "endOffset" : 144
    }, {
      "referenceID" : 11,
      "context" : ", 2015), and tools solving versions of the problem have begun to see industrial adoption (Gulwani et al., 2015).",
      "startOffset" : 89,
      "endOffset" : 111
    }, {
      "referenceID" : 2,
      "context" : "Existing approaches to the problem commonly frame it as search over a space of programs (Alur et al., 2013).",
      "startOffset" : 88,
      "endOffset" : 107
    }, {
      "referenceID" : 2,
      "context" : "A popular way of addressing these challenges is to assume some syntactic constraints on the program that is to be synthesized (Alur et al., 2013).",
      "startOffset" : 126,
      "endOffset" : 145
    }, {
      "referenceID" : 28,
      "context" : "These constraints are imposed either by having a incomplete syntactic model, known as a program sketch (Solar-Lezama et al., 2006; Solar-Lezama, 2009), as part of the problem instance, or by restricting synthesis to a domain-specific language (DSL) (Gulwani, 2011; Barowy et al.",
      "startOffset" : 103,
      "endOffset" : 150
    }, {
      "referenceID" : 7,
      "context" : "The resulting abstraction is then passed to a synthesizer based on type-directed search (Feser et al., 2015), which attempts to use the abstraction to create a Java program that could have produced the user-supplied sequences.",
      "startOffset" : 88,
      "endOffset" : 108
    }, {
      "referenceID" : 31,
      "context" : "Since a sketch is tree-structured, we use a top-down tree-structured RNN similar to (Zhang et al., 2016).",
      "startOffset" : 84,
      "endOffset" : 104
    }, {
      "referenceID" : 31,
      "context" : "Similar to the notion of a “dependency path” in (Zhang et al., 2016), we define a production path as a sequence of pairs 〈(v1, e1), (v2, e2), .",
      "startOffset" : 48,
      "endOffset" : 68
    }, {
      "referenceID" : 31,
      "context" : "4 of (Zhang et al., 2016) for more details.",
      "startOffset" : 5,
      "endOffset" : 25
    }, {
      "referenceID" : 7,
      "context" : "These expressions are identified through type-directed enumeration (Feser et al., 2015).",
      "startOffset" : 67,
      "endOffset" : 87
    }, {
      "referenceID" : 21,
      "context" : "This encoder can be seen to implement the high-level ideas of the encoder used in (Parisotto et al., 2016), although low-level details may differ as their inputs are I/O examples.",
      "startOffset" : 82,
      "endOffset" : 106
    }, {
      "referenceID" : 12,
      "context" : "For example, JSKETCH (Jeon et al., 2015) synthesizes Java code, but expects the input to include a detailed program sketch.",
      "startOffset" : 21,
      "endOffset" : 40
    }, {
      "referenceID" : 7,
      "context" : "While tools like λ (Feser et al., 2015) do not require sketches, they assume restricted programming languages and cannot synthesize programs that use complex APIs.",
      "startOffset" : 19,
      "endOffset" : 39
    }, {
      "referenceID" : 28,
      "context" : "Many recent efforts in the programming language community (Solar-Lezama et al., 2006; Gulwani, 2011; Albarghouthi et al., 2013; Feser et al., 2015; Osera & Zdancewic, 2015) target synthesis using search and constraint solving.",
      "startOffset" : 58,
      "endOffset" : 172
    }, {
      "referenceID" : 0,
      "context" : "Many recent efforts in the programming language community (Solar-Lezama et al., 2006; Gulwani, 2011; Albarghouthi et al., 2013; Feser et al., 2015; Osera & Zdancewic, 2015) target synthesis using search and constraint solving.",
      "startOffset" : 58,
      "endOffset" : 172
    }, {
      "referenceID" : 7,
      "context" : "Many recent efforts in the programming language community (Solar-Lezama et al., 2006; Gulwani, 2011; Albarghouthi et al., 2013; Feser et al., 2015; Osera & Zdancewic, 2015) target synthesis using search and constraint solving.",
      "startOffset" : 58,
      "endOffset" : 172
    }, {
      "referenceID" : 2,
      "context" : "A consensus here is that to scale synthesis and generalize well from examples, one must syntactically limit the space of feasible programs (Alur et al., 2013).",
      "startOffset" : 139,
      "endOffset" : 158
    }, {
      "referenceID" : 28,
      "context" : "This is done either by adding a human-provided sketch to a problem instance (Solar-Lezama et al., 2006), or by restricting synthesis a priori to a narrow DSL (Gulwani, 2011; Barowy et al.",
      "startOffset" : 76,
      "endOffset" : 103
    }, {
      "referenceID" : 7,
      "context" : "For example, the BAYOU system uses type-guided search techniques developed in prior work (Feser et al., 2015).",
      "startOffset" : 89,
      "endOffset" : 109
    }, {
      "referenceID" : 9,
      "context" : "One category of methods frame synthesis as the problem of learning an endto-end differentiable, neural model of a program (Graves et al., 2014; Kurach et al., 2015; Neelakantan et al., 2015; Reed & De Freitas, 2015; Joulin & Mikolov, 2015).",
      "startOffset" : 122,
      "endOffset" : 239
    }, {
      "referenceID" : 15,
      "context" : "One category of methods frame synthesis as the problem of learning an endto-end differentiable, neural model of a program (Graves et al., 2014; Kurach et al., 2015; Neelakantan et al., 2015; Reed & De Freitas, 2015; Joulin & Mikolov, 2015).",
      "startOffset" : 122,
      "endOffset" : 239
    }, {
      "referenceID" : 17,
      "context" : "One category of methods frame synthesis as the problem of learning an endto-end differentiable, neural model of a program (Graves et al., 2014; Kurach et al., 2015; Neelakantan et al., 2015; Reed & De Freitas, 2015; Joulin & Mikolov, 2015).",
      "startOffset" : 122,
      "endOffset" : 239
    }, {
      "referenceID" : 8,
      "context" : "A second category of methods learn interpretable programs (Gaunt et al., 2016; Riedel et al., 2016; Bunel et al., 2016), but requires new learning for each synthesis task.",
      "startOffset" : 58,
      "endOffset" : 119
    }, {
      "referenceID" : 26,
      "context" : "A second category of methods learn interpretable programs (Gaunt et al., 2016; Riedel et al., 2016; Bunel et al., 2016), but requires new learning for each synthesis task.",
      "startOffset" : 58,
      "endOffset" : 119
    }, {
      "referenceID" : 6,
      "context" : "A second category of methods learn interpretable programs (Gaunt et al., 2016; Riedel et al., 2016; Bunel et al., 2016), but requires new learning for each synthesis task.",
      "startOffset" : 58,
      "endOffset" : 119
    }, {
      "referenceID" : 3,
      "context" : "DeepCoder (Balog et al., 2016) uses a neural network to predict binary attributes of the solution to a synthesis problem, and uses these predictions to set goals for a combinatorial synthesizer.",
      "startOffset" : 10,
      "endOffset" : 30
    }, {
      "referenceID" : 21,
      "context" : "In neuro-symbolic synthesis (Parisotto et al., 2016), a neural architecture is used to encode a set of input examples and decode the resulting representation into a program in a DSL.",
      "startOffset" : 28,
      "endOffset" : 52
    }, {
      "referenceID" : 24,
      "context" : "Another body of related work learns hidden specifications (Raychev et al., 2015) and syntactic patterns (Allamanis & Sutton, 2014; Nguyen et al.",
      "startOffset" : 58,
      "endOffset" : 80
    }, {
      "referenceID" : 19,
      "context" : ", 2015) and syntactic patterns (Allamanis & Sutton, 2014; Nguyen et al., 2013; Nguyen & Nguyen, 2015; Raychev et al., 2015; Bielik et al., 2016) from code in large software repositories.",
      "startOffset" : 31,
      "endOffset" : 144
    }, {
      "referenceID" : 24,
      "context" : ", 2015) and syntactic patterns (Allamanis & Sutton, 2014; Nguyen et al., 2013; Nguyen & Nguyen, 2015; Raychev et al., 2015; Bielik et al., 2016) from code in large software repositories.",
      "startOffset" : 31,
      "endOffset" : 144
    }, {
      "referenceID" : 4,
      "context" : ", 2015) and syntactic patterns (Allamanis & Sutton, 2014; Nguyen et al., 2013; Nguyen & Nguyen, 2015; Raychev et al., 2015; Bielik et al., 2016) from code in large software repositories.",
      "startOffset" : 31,
      "endOffset" : 144
    }, {
      "referenceID" : 23,
      "context" : "The two most relevant efforts (Raychev et al., 2014; 2016) use statistical language models for code completion.",
      "startOffset" : 30,
      "endOffset" : 58
    } ],
    "year" : 2017,
    "abstractText" : "We present a data-driven approach to the problem of inductive computer program synthesis. Our method learns a probabilistic model for realworld programs from a corpus of existing code. It uses this model during synthesis to automatically infer a posterior distribution over sketches, or syntactic models of the problem to be synthesized. Sketches sampled from this posterior are then used to drive combinatorial synthesis of a program in a high-level programming language. The key technical innovation of our approach — embodied in a system called BAYOU— is utilizing user-supplied evidence as to the program’s desired behavior, along with a Bayesian update, to obtain a posterior distribution over the program’s true, latent specification (indicating user intent), which in turn produces a posterior over possible sketches. As we show experimentally, explicitly modeling uncertainty in specification significantly increases the accuracy of the synthesis algorithm. We evaluate BAYOU’s ability to synthesize Java and Android methods. We find that using just a few example API sequences to communicate user intent, BAYOU can synthesize complex method bodies, some implementing tasks never encountered during training.",
    "creator" : "TeX"
  }
}