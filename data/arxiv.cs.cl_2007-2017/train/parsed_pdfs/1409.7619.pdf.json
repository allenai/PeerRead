{
  "name" : "1409.7619.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Generating Conceptual Metaphors from Proposition Stores",
    "authors" : [ "Ekaterina Ovchinnikova", "Vladimir Zaytsev", "Suzanne Wertheim" ],
    "emails" : [ "katya@isi.edu", "vzaytsev@isi.edu", "israel@isi.edu", "worthwhileresearch@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n40 9.\n76 19\nv1 [\ncs .C\nL ]\n2 5\nSe p\n20 14"
    }, {
      "heading" : "1 Introduction",
      "text" : "The term conceptual metaphor refers to the understanding of one concept or conceptual domain in terms of the properties of another (Lakoff and Johnson, 1980; Lakoff, 1987). For example, development can be understood as movement (e.g., the economy moves forward, the engine of the economy). In other words, a conceptual metaphor consists in mapping source conceptual domain (e.g., vehicle) properties to a target domain (e.g., economy). For example, an economy develops like a vehicle moves. In language, conceptual metaphors are expressed by linguistic metaphors, i.e. natural language phrases expressing the implied mapping of two domains.\nContemporary research on computational processing of linguistic metaphors is divided into two main branches: 1) metaphor recognition, i.e. distinguishing between literal and metaphorical use of language, and 2) metaphor interpretation, inferring the intended literal meaning of a metaphorical expression; see (Shutova, 2010b) for an overview.\nIn this paper, we take a different line of research and present an automated method for generating conceptual metaphors from linguistic data. Given a proposition store, i.e. a collection of weighted\ntuples of words that have a determined pattern of syntactic relations among them, for each lexically expressed concept we collect a weighted list of propositions that represent properties of this concept. For each target concept, we generate a weighted list of possible sources having properties similar to the properties of the target. Given a generated conceptual metaphor, we automatically find linguistic metaphors that are realizations of this conceptual metaphor in a corpus of texts. The proposed method does not crucially rely on manually coded lexical-semantic resources and does not require an annotated training set. It provides a mechanism that can be used for both metaphor recognition and interpretation.\nWe test our approach using corpora in two languages: English and Russian. We select target concepts and generate potential sources for them. For top-ranked sources, we automatically find corresponding linguistic metaphors. These linguistic metaphors are then validated by expert linguists. In addition, we evaluate proposed conceptual metaphors generated for English against the Master Metaphor List (Lakoff et al., 1991)"
    }, {
      "heading" : "2 Related Work",
      "text" : "Contemporary research on computational processing of linguistic metaphors (LMs) mainly focuses on metaphor recognition and metaphor interpretation. Automatic metaphor recognition is performed by checking selectional preference violation (Fass, 1991; Krishnakumaran and Zhu, 2007; Baumer et al., 2010), relying on existing lexical-semantic resources (Peters and Peters, 2000; Wilks et al., 2013), mapping phrases to predefined target and source concepts constituting a conceptual metaphor (Heintz et al., 2013; Mohler et al., 2013). mapping linguistic phrases to predefined concepts constituting a conceptual metaphor (Heintz et al., 2013; Mohler et al., 2013), and employing a classifier (Birke and Sarkar, 2006;\nGedigian et al., 2006; Baumer et al., 2010; Hovy et al., 2013; Mohler et al., 2013). Approaches based on selectional preferences are known to overgeneralize and miss conventional metaphors (Shutova, 2010b). Approaches relying on handcoded knowledge and manually annotated training sets have coverage limitations.\nAutomatic metaphor interpretation is performed using two principal approaches: 1) reasoning with manually coded knowledge of source domains (Narayanan, 1999; Barnden and Lee, 2002; Agerri et al., 2007; Ovchinnikova et al., 2014) and 2) deriving literal paraphrases for metaphorical expressions from corpora (Shutova, 2010a). The first approach has yielded only limited scale, while the second approach does not explain the target– source mapping.\nThere has been considerably less work on generating conceptual metaphors (CMs). The CorMet system (Mason, 2004) generates CMs given predefined target and source domains. Given two domain-specific corpora and two sets of characteristic keywords for each domain, CorMet finds domain-characteristic verbs with the highest relative frequency that have the keywords as their arguments. For each verb, the system learns domain-specific selectional preferences represented by WordNet synsets expressing domain concepts. All possible mappings between the top target and source domain concepts are then scored according to polarity (i.e., structure transfer between domains), the number of verbs instantiating the mapping, and the systematic co-occurrence of that mapping with different mappings. For example, given the LAB and FINANCE domains, CorMet finds a CM \"Money is a Liquid\". In contrast to CorMet, our method does not require target and source domains to be predefined and does not rely on selectional preferences.\nGandy et al. (2013) propose another method for generating CMs. Given predefined target nouns, the method classifies verb and adjective phrases (called facets) containing these nouns as being either metaphorical or not based on the assumption that a metaphor usually involves a mapping from a concrete to an abstract domain. For generating source candidates, the method finds nouns that are non-metaphorically associated with the same facets in the corpus. Source and target nouns are clustered into concepts represented by WordNet synsets. The obtained target-source concept pairs\nare filtered and scored according to several heuristics. This method is restricted to metaphorical mappings from concrete to abstract domains, e.g. \"Power is a Building\", whereas our method does not imply such limitation.\nOur method is similar to (Mason, 2004) and (Gandy et al., 2013) in that we rely on characteristic predicate, but we use general propositions instead of verb and adjective phrases, see Sec. 3."
    }, {
      "heading" : "3 Building Proposition Stores",
      "text" : "We build upon the idea that by parsing a sentence and abstracting from its syntactic structure (e.g., dropping modifiers) we can obtain common sense knowledge, see (Schubert, 2002; Clark and Harrison, 2009). For example, the sentence Powerful summer storms left extensive damage in California reveals common sense knowledge about storms possibly leaving damage and being powerful. This knowledge can be captured by propositions, i.e. tuples of words that have a determined pattern of syntactic relations among them (Clark and Harrison, 2009; Peñas and Hovy, 2010; Tsao and Wible, 2013). While many of such tuples can be erroneous due to parse errors, statistically higher frequency tuples can be considered more reliable.\nWe generated propositions from parsed English and Russian corpora. We parsed English Gigaword (Parker et al., 2011) with Boxer (Bos et al., 2004). As one of the possible formats, Boxer outputs logical forms of sentences in the style of (Hobbs, 1985), generalizes over some syntactic constructions (e.g., passive/active), and performs binding of arguments. For example, Boxer represents the sentence John decided to go to school as:\nJohn(e1, x1) ∧ decide(e2, x1, e3) ∧ go(e3, x1) ∧to(e4, e3, x2) ∧ school(e5, x2)\nThe following propositions can be extracted from this output:\n(NV John decide) (NV John go) (NVV John decide go) (VPN go to school) (NVPN John go to school) (NVVPN John decide go to school)\nA proposition store is a collection of such tuples such that each tuple is assigned its frequency in a corpus. For Russian, we used the ruWac corpus (Sharoff and Nivre, 2011) parsed with the Malt dependency parser (Nivre et al., 2006). We then con-\nvert these dependency graphs into logical forms in the style of (Hobbs, 1985)."
    }, {
      "heading" : "4 Generalizing over Propositions",
      "text" : "A significant amount of the propositions can be further generalized if we abstract from named entities, synonyms, and sister terms. Consider the following tuples:\n(Guardian publish interview with Stevens) (newspaper publish interview with John) (journal publish interview with Dr. Crick)\nThe first two propositions above provide evidence for generating the proposition (newspaper publish interview with person). All three propositions above can be generalized into (periodical publish interview with person). Such generalizations help to refine frequencies assigned to propositions containing abstract nouns, infer new propositions, and cluster propositions.\nIn order to obtain the generalizations, we first map nouns contained in the propositions into WordNet and Wikipedia semantic nodes using the YAGO ontology (Suchanek et al., 2007). YAGO links lexical items to WordNet and Wikipedia for all the languages presented in Wikipedia including English and Russian. Given a single noun n being an argument in a tuple, the mapping procedure works as follows.\n1. Find semantic nodes N corresponding to n - If n is a given name or a surname, then N is <wordnet_person>. - If n is equal to a YAGO lexical item, then N is the corresponding YAGO semantic node. - If n is a substring of several YAGO lexical items, then N is a union of all corresponding YAGO semantic nodes.\n2. Filter semantic nodes N - If N contains class nodes only, the class nodes are returned. We do not perform disambiguation and map ambiguous nouns to all possible semantic nodes. - If N contains both class and instance nodes, the class nodes are returned (e.g., nirvana can be mapped to the class <wordnet_nirvana> and to the instance <Nirvana_(band)>). - If N contains instance nodes only, then the instance nodes are mapped to the corresponding classes using the YAGO hierarchy.\nNoun compounds require special treatment. We map the longest sequence of lexemes in each\ncompound to semantic nodes. For example, for the New York Times we obtain the node <The_New_York_Times> instead of the nodes <New_York> and <wordnet_time>. If different parts of the compound are mapped to different nodes, we prefer class nodes over instances. For example, parts of the compound musician Peter Gabriel are mapped to <wordnet_musician> as well as to <Peter_Gabriel>. We prefer the node <wordnet_musician>, because it refers to a class. If several classes can be produced for a noun compound, the mapping procedure returns all of them. Given the obtained mappings to YAGO classes, we merge identical propositions and sum their frequencies. For example, given two propositions (live in city, 10) and (live in New York, 5), we obtain (live in <wordnet_city>, 15).\nThe described simple mapping procedure has obvious limitations, especially concerning the preference of classes over instances. However, the procedure is computationally cheap and does not require a training dataset. In future work we plan to compare its performance with the performance of the Wikifier system (Ratinov et al., 2011) that identifies important entities and concepts in text, disambiguates them and links them to Wikipedia."
    }, {
      "heading" : "5 Generating Salient Concept Properties",
      "text" : "In a metaphor, properties of a source domain are mapped to a target domain. In natural language, concepts and properties are represented by words and phrases. There is a long-standing tradition for considering computational models derived from word co-occurrence statistics as being capable of producing reasonable property-based descriptions of concepts; see (Baroni and Lenci, 2008) for an overview. We use the proposition stores to derive salient properties of concepts that can be potentially mapped in a metaphor.\nGiven a seed lexeme l, we extract all tuples from the proposition store that contain that lexeme. For each tuple t = 〈x1, .., xn〉 there is a set of patterns of the form pi(t) = 〈x1, .., xi−1, _, xi+1, .., xn〉, i.e. a pattern pi of tuple t is obtained by replacing the ith argument of t with a blank value. The weight of tuple t relative to lexeme l occurring at position i in t is computed as follows:\nweightl(t) = freq(t)∑\nt′∈T :pi(t)=pi(t′) freq(t′)\n, (1)\nwhere T is a set of all tuples, and freq(t) is a frequency of tuple t. For example, the following are the top-ranked tuples containing the English lexeme poverty:\n(NVPN majority live in poverty) (NVAdv poverty affect increasingly) (NPN lift out of poverty) (VN fight poverty) (VN eliminate poverty) (NN poverty eradication) (AdvPN deep in poverty)"
    }, {
      "heading" : "6 Generating Source Lexemes",
      "text" : "Some property tuples presented in Sec. 5 already suggest conceptual metaphors: \"Poverty is a Location\" (live in poverty), \"Poverty is an Enemy\" (fight poverty), \"Poverty is an Abyss\" (deep in poverty). We generate potential source lexemes for a seed target lexeme l in three steps:\n1. Find all tuples Tl containing l.\n2. Find all potential source lexemes S such that for each s ∈ S there are tuples t, t′ in the proposition store such that l occurs at position i in t and s occurs at position i in t′. The set of tuples containing l and s at the same positions is denoted by Tl,s.\n3. Weight potential source lexemes s ∈ S using the following equation:\nweightl(s) = ∑\nt∈Tl,s\nweightl(t), (2)\nWe generated potential source lexemes for the target domains of Poverty, Wealth, and Friendship. For each target domain, we selected two seed target lexemes with different parts of speech. In English, we selected poverty and poor, wealth and buy, friendship and friendly. In Russian, we selected бедность (poverty) and бедный (poor), богатство (wealth) and покупать (buy), дружба (friendship) and дружеский (friendly). The top-ranked source lexemes for each seed target are presented in Tables 1 and 2.\nNouns seem to be better seeds as compared to adjectives and verbs. Looking at the source lexemes for nouns, we find a) semantically related words (e.g., poverty: emission, recession, 2) abstract supercategories (e.g., poverty: situation), or 3) real potential sources (e.g., poverty: disease).\nThe list of source lexemes for verbs and adjectives contains many random frequent words (e.g., buy: set, poor: first). One possible explanation is that the selected seed adjectives and verbs occur in more contexts and combine more freely than selected nouns. A larger study over a wide range of targets is needed to draw further conclusions.\nObviously, many of the found source lexemes are semantically related to the target lexemes. In order to find sources that share patterns with the target, but are not closely semantically related, we need to compute \"anti-relatedness\". For doing so, we use the Latent Dirichlet allocation (LDA) model (Blei et al., 2003).1 LDA is a probabilistic topic model, in which documents are viewed as distributions over topics and topics are viewed as distributions over words. We generated English and Russian LDA models using the Gensim toolkit2 applied to lemmatized English Gigaword and ruWac corpora with stop words removed; the number of topics was equal to 50.\nFollowing (Rus et al., 2013), we define the LDA-based relatedness of words as follows:\nrel(w1, w2) =\nT∑\nt=1\nφt(w1)φt(w2), (3)\nwhere T is the number of topics and φt(w) is the probability of word w in topic t. We filter out source lexemes such that their relatedness to the target lexeme is above a threshold. The value of the threshold defines the number of final source lexemes. The results in Tables 3, 4, 5, 6, 7 were obtained with a threshold of 0.04.\nThe results in the tables show that some of the original source lexemes (Tables 1 and 2) that are semantically close to the target were filtered out, e.g., poverty: corruption, recession, situation, problem. At the same time, some of the filtered out sources that are semantically similar to the target still have potential to constitute a CM. For example, \"Poverty is a Crime\" has realizations in the corpus, e.g., a powerful indictment of the iniquities of racial discrimination and the crime of poverty."
    }, {
      "heading" : "7 Generating Conceptual Metaphors",
      "text" : "A conceptual metaphor is a triple 〈Ct, Cs, P 〉 consisting of a target concept Ct, a source concept Cs,\n1We also experimented with the Latent Semantic Analysis model (Dumais, 2004), but the LDA model proved to provide more relevant results.\n2http://radimrehurek.com/gensim/\nand a set P of properties transferred from Cs to Ct. Each concept is a set of words and phrases. In order to obtain potential source concepts, we cluster generated source lexemes.\nBoth (Mason, 2004) and (Gandy et al., 2013) employ the WordNet hierarchy for clustering. In this paper, we take a similar approach and use the YAGO hierarchy based on WordNet and Wikipedia.3 Source lexemes belong to the same concept if they are all hyponyms of the same YAGO node in the hierarchy and share k or more patterns. The value of k defines the number of final source concepts. The results in Tables 3, 4, 5, 6, 7 were obtained with k = 5. The weight of the potential source concept is equal to the sum of the weights of the corresponding source lexemes. For example, area, room, appartment, city, region were clustered together based on the patterns (live in X), (reside in X), etc.\n3Note that a preliminary clustering of synonyms was done at the proposition generalization step (Sec. 4).\nTables 3 and 4 contain detailed results for the targets Wealth and Бедность (Poverty) showing some of the patterns common for the targets and the sources that can be used to explain the conceptual metaphor. For English, \"Wealth is Blood\", because one can donate and pump both money and blood. For Russian, \"Бедность это Враг\" (\"Poverty is an Enemy\"), is based on the idea that one can fight against poverty, defeat it, etc.\nThe usage of the YAGO hierarchy is just one out of many options. In the future, we will investigate different word clustering algorithms (including distributional models, n-gram-based language models, etc.) and their effect on the obtained sources."
    }, {
      "heading" : "8 Finding Linguistic Metaphors",
      "text" : "For each potential conceptual metaphor, we look for supporting linguistic metaphors in corpora. A large number of LMs supporting a particular CM suggests that this CM might be cognitively plausi-\nble. However, it should be noted that if a CM is not supported by any LMs it does not mean that this CM is wrong. The target-source mapping can be still cognitively relevant, but not yet conventional enough to be represented linguistically.\nWe use a simple method for finding LMs. If a target lexeme and a source lexeme are connected by a dependency relation in a sentence, then we assume that this dependency structure contains a LM. For example, in the phrases medicine against poverty, chronic poverty, бедность – это болезнь, которую надо лечить (\"poverty is a disease that should be treated\"), болезнь хронической бедности (\"disease of chronic poverty\") target words (poverty, бедность) are related by a dependency with source words (medicine, chronic, болезнь, хронический). Heintz et al. (2013) present a similar approach mapping sentences to LDA topic models for target and source domains. Our method allows us to exploit dependency links and output only sentences containing target words being modified by source words or vice versa.\nThis method has limitations with respect to both precision and recall. First, not all LMs are accommodated in a dependency structure. For example, in the text fragment There is no \"magic bullet\" for poverty, no cure-all the target word poverty is not related to the source word cure-all by a dependency link. Second, this method overgenerates. In the previous section, there was an example of the conceptual metaphor \"Poverty is a Location\". While poor is a target lexeme and country is a source lexeme for this CM, the phrase poor country is not metaphorical. One more problem concerns ambiguity. The phrase friendship is a two-way deal instantiating the \"trade/business\" meaning of deal seems to contain a LM, whereas personal friendship is a big deal instantiating the meaning \"important\" is not metaphorical.\nTables 3 and 4 contain examples of the extracted sentences potentially containing LMs for the CMs generated for the targets Wealth and Бедность (Poverty) along with the patterns explaining the target-source mapping."
    }, {
      "heading" : "9 Evaluation",
      "text" : "In this section, we describe an evaluation of the proposed approach. First, we present a validation of the generated CMs. As mentioned in Sec. 7, nouns proved to be the best seeds for generating potential sources. For two of the selected\nnoun targets per language (poverty, wealth, бедность (poverty), богатство (wealth)) we generate source lexemes, select 100 top-ranked lexemes, cluster them into source concepts, and select 10 top-ranked CM proposals (Tables 5 and 6).\nIn order to obtain a lexically richer representation of the domains, we expand the sets of these target and source lexemes with semantically related lexemes using English and Russian ConceptNet resources (Speer and Havasi, 2013) and top ranked patterns from the proposition stores.4 For example, the expansion of the lexeme disease results in {disease, symptom, syndrome, illness, unwellness, sickness, sick, medicine, treatment, treat, cure, doctor, ... }.\nGiven parsed English Gigaword and ruWac corpora, we extracted sentences that contain dependency structures relating target and source lexemes. For each language, we randomly selected at most 10 sentences per target-source pair. For some pairs, less than 10 sentences were retrieved. In total, we obtained 197 sentences for English and 186 for Russian. Each sentence was validated by three linguist experts. The experts were asked if the sentence contains a metaphor mapping indicated target and source domains. The Fleiss’ kappa (Fleiss, 1971) is 0.69 for English and 0.68 for Russian.\nTables 5 and 6 show potential sources with numbers of the corresponding sentences containing a LM. Column ALL contains the number of sentences per a proposed CM such that all three experts agreed that there is a metaphor in these sentences. Column TWO contains the number of sentences such that any two experts agreed that there is a metaphor in them. Column ONE contains the number of sentences such that only one expert thought there is a metaphor.\nWe consider a CM to be approved if at least one of the associated LMs was positively validated by all three experts. According to the validation results in Tables 5 and 6, 18 out of 20 top-ranked conceptual metaphors proved to be promising for English (90%). For Russian, the experts approved 15 of 20 proposed CMs (75%).\nIn several cases, experts validated given sentences as containing LMs, but disagreed with the label of the corresponding source domain. For ex-\n4ConceptNet combines several resources developed manually (e.g., WordNet, Wiktionary) and thus provides high quality semantic relations. The usage of it is optional. ConceptNet semantic relations can be replaced by semantically similar words provided by a distributional model.\nample, the experts complained about \"Poverty is a Slump\" as being a label for the examples like Getting a college degree does not assure one will lift out of poverty. The source lexeme slump generates many relevant patterns, e.g., lift out of X, deep X, pull out of X, etc., but Abyss seems to be a better label for this domain.\nIn some cases two concepts constituting a CM are mapped to the same source domain. For example, both Poverty and Terrorism are mapped to Enemy. Our method produces the CM \"Poverty is Terrorism\" and many \"enemy\"-patterns as overlapping properties (fight against X, war on X). Does it mean that the method overgenerates and \"Poverty is Terrorism\" is not a valid CM? In our experimental study we find that sentences like Poverty is a form of terrorism, causing its victims to live in fear are validated by the experts as metaphorical. Thus, even if D1 and D2 are mapped to the same domain, 〈D1,D2, P 〉 might still be a valid CM.\nIn order to compare our system results to the CorMet system (Mason, 2004), we present an evaluation against the Master Metaphor List (Lakoff et al., 1991).The Master Metaphor List (MML) is composed of manually verified metaphors common in English. We restrict our evaluation to the elements of MML used for evaluating CorMet (Table 7). For each target and source domain in Table 7, we select English seed lexemes and expand the sets using ConceptNet and topranked patterns from the proposition store. For target nouns, we generate potential sources. For each target lexeme, we take a set of 100 top-ranked potential sources and check if the set contains MML source lexemes. For example, for the CM \"Fighting a War is Treating Illness\", we obtain the lexeme sets T={war, fight, combat, battle, attack,..} and S={disease, treatment, medicine, doctor,..}. Mappings found between T and S are shown in Table 7. For example, treatment was mapped to attack with weight 0.51.5 The table also contains the original CorMet mappings and scores. Similar to CorMet, our system found reasonable CMs in 10 of 13 cases (77%).6\nWe used LMs associated with CMs in the Master Metaphor List for error analysis. We found that\n5Weights were scaled between 0 and 1. 6Note that Mason (2004) evaluates the correspondences between the CorMet mappings and the MML mappings by hand which introduces subjectivity, whereas our evaluation is done automatically.\nfor two missing CMs, our system produced mappings, but they were assigned low weights because of the low frequencies of the corresponding propositions in the corpus. Therefore they were not included into 100 top-ranked source proposals. For example, for the CM \"Investments as Containers for Money\" (The bottom of the economy dropped out, I’m down to my bottom dollar), the system found the pattern (NPN bottom of X) for both economy and container. For the CM \"People as Containers for Emotions\" (I was filled with rage, She could hardly contain her anger), it found the propositions (VPN fill with emotion/feeling), (NVN people contain feeling) and the patterns (VPN fill with X) and (NVN X contain Y) for both people and container. For \"People as Machines\" (He had a breakdown, what makes him tick, Fuel up with a good breakfast), no mappings were found.\nThe presented evaluation shows how many of the proposed top-ranked CMs are approved by experts and how many of the MML CMs are found by the system, but we do not learn much about the quality of the CM ranking measure. In the future we plan to evaluate how well our ranking of CMs correlates with human ranking."
    }, {
      "heading" : "10 Conclusion and Future Work",
      "text" : "Contemporary research on computational processing of linguistic metaphors is divided into two main branches: metaphor recognition and\nmetaphor interpretation. In this paper, we take a different line of research and present an automated method for generating conceptual metaphors from linguistic data. The proposed approach does not crucially rely on manually coded resources and does not require an annotated training set. It provides a mechanism that can be used for metaphor recognition given predefined target and source domains (see Sec. 8). It also enables metaphor interpretation through patterns shared by target and source lexemes (cf. Tables 3 and 4). All developed tools, generated resources, and validation data are freely available for the community as an open source project at http://ovchinnikova.me/proj/metaphor.html.\nIn the future, we will investigate different word clustering algorithms and their effect on the obtained CMs. We also aim at providing a more solid evaluation of the CM ranking score and compute how well it correlates with human ranking. Furthermore, we will study how well salient properties shared by targets and sources help to explain conceptual metaphors. For doing so, we will need to create a gold standard by asking human subjects to explain given conceptual metaphors and provide properties mapped from source to target domains."
    } ],
    "references" : [ {
      "title" : "An artificial intelligence approach to metaphor understanding",
      "author" : [ "J.A. Barnden", "M.G. Lee." ],
      "venue" : "Theoria et Historia Scientiarum, 6(1):399–412.",
      "citeRegEx" : "Barnden and Lee.,? 2002",
      "shortCiteRegEx" : "Barnden and Lee.",
      "year" : 2002
    }, {
      "title" : "Concepts and properties in word spaces",
      "author" : [ "M. Baroni", "A. Lenci." ],
      "venue" : "Italian Journal of Linguistics, 20(1):55–88.",
      "citeRegEx" : "Baroni and Lenci.,? 2008",
      "shortCiteRegEx" : "Baroni and Lenci.",
      "year" : 2008
    }, {
      "title" : "Comparing semantic role labeling with typed dependency parsing in computational metaphor identification",
      "author" : [ "E. PS. Baumer", "J.P. White", "B. Tomlinson." ],
      "venue" : "Proc. of the NAACL HLT 2010 Second Workshop on Computational Approaches to Linguis-",
      "citeRegEx" : "Baumer et al\\.,? 2010",
      "shortCiteRegEx" : "Baumer et al\\.",
      "year" : 2010
    }, {
      "title" : "A clustering approach for nearly unsupervised recognition of nonliteral language",
      "author" : [ "J. Birke", "A. Sarkar." ],
      "venue" : "Proc. of EACL’06.",
      "citeRegEx" : "Birke and Sarkar.,? 2006",
      "shortCiteRegEx" : "Birke and Sarkar.",
      "year" : 2006
    }, {
      "title" : "Latent dirichlet allocation",
      "author" : [ "D.M. Blei", "A. Ng", "M. Jordan." ],
      "venue" : "JMLR, 3:993–1022.",
      "citeRegEx" : "Blei et al\\.,? 2003",
      "shortCiteRegEx" : "Blei et al\\.",
      "year" : 2003
    }, {
      "title" : "Wide-coverage semantic representations from a ccg parser",
      "author" : [ "J. Bos", "S. Clark", "M. Steedman", "J.R. Curran", "J. Hockenmaier." ],
      "venue" : "Proc. of COLING’04, pages 1240–1246.",
      "citeRegEx" : "Bos et al\\.,? 2004",
      "shortCiteRegEx" : "Bos et al\\.",
      "year" : 2004
    }, {
      "title" : "Large-scale extraction and use of knowledge from text",
      "author" : [ "P. Clark", "P. Harrison." ],
      "venue" : "Proc. of the 5th international conference on Knowledge capture, pages 153–160. ACM.",
      "citeRegEx" : "Clark and Harrison.,? 2009",
      "shortCiteRegEx" : "Clark and Harrison.",
      "year" : 2009
    }, {
      "title" : "Latent semantic analysis",
      "author" : [ "S.T. Dumais." ],
      "venue" : "ARIST, 38(1):188–230.",
      "citeRegEx" : "Dumais.,? 2004",
      "shortCiteRegEx" : "Dumais.",
      "year" : 2004
    }, {
      "title" : "met*: A method for discriminating metonymy and metaphor by computer",
      "author" : [ "D. Fass." ],
      "venue" : "Computational Linguistics, 17(1):49–90.",
      "citeRegEx" : "Fass.,? 1991",
      "shortCiteRegEx" : "Fass.",
      "year" : 1991
    }, {
      "title" : "Measuring nominal scale agreement among many raters",
      "author" : [ "J.L. Fleiss." ],
      "venue" : "Psychological Bulletin, 76(5):378–382.",
      "citeRegEx" : "Fleiss.,? 1971",
      "shortCiteRegEx" : "Fleiss.",
      "year" : 1971
    }, {
      "title" : "Automatic Identification of Conceptual MetaphorsWith Limited Knowledge",
      "author" : [ "L. Gandy", "N. Allan", "M. Atallah", "O. Frieder", "N. Howard", "S. Kanareykin", "M. Koppel", "M. Last", "Y. Neuman", "S. Argamon." ],
      "venue" : "Proc. of AAAI’13.",
      "citeRegEx" : "Gandy et al\\.,? 2013",
      "shortCiteRegEx" : "Gandy et al\\.",
      "year" : 2013
    }, {
      "title" : "Catching metaphors",
      "author" : [ "M. Gedigian", "J. Bryant", "S. Narayanan", "B. Ciric." ],
      "venue" : "Proc. of the Third Workshop on Scalable Natural Language Understanding, pages 41–48. ACL.",
      "citeRegEx" : "Gedigian et al\\.,? 2006",
      "shortCiteRegEx" : "Gedigian et al\\.",
      "year" : 2006
    }, {
      "title" : "Automatic extraction of linguistic metaphors with LDA topic modeling",
      "author" : [ "I. Heintz", "R. Gabbard", "M. Srinivasan", "D. Barner", "D.S. Black", "M. Freedman", "R. Weischedel." ],
      "venue" : "Proc. of the First Workshop on Metaphor in NLP, pages 58–66. ACL.",
      "citeRegEx" : "Heintz et al\\.,? 2013",
      "shortCiteRegEx" : "Heintz et al\\.",
      "year" : 2013
    }, {
      "title" : "Ontological promiscuity",
      "author" : [ "J.R. Hobbs." ],
      "venue" : "Proceedings of ACL, pages 61–69, Chicago, Illinois.",
      "citeRegEx" : "Hobbs.,? 1985",
      "shortCiteRegEx" : "Hobbs.",
      "year" : 1985
    }, {
      "title" : "Identifying Metaphorical Word Use with Tree Kernels",
      "author" : [ "D. Hovy", "S. Srivastava", "S.K. Jauhar", "M. Sachan", "K. Goyal", "H. Li", "W. Sanders", "Eduard Hovy." ],
      "venue" : "Proc. of the First Workshop on Metaphor in NLP, pages 52–57. ACL.",
      "citeRegEx" : "Hovy et al\\.,? 2013",
      "shortCiteRegEx" : "Hovy et al\\.",
      "year" : 2013
    }, {
      "title" : "Hunting elusive metaphors using lexical resources",
      "author" : [ "S. Krishnakumaran", "X. Zhu." ],
      "venue" : "Proc. of the Workshop on Computational approaches to Figurative Language, pages 13–20. ACL.",
      "citeRegEx" : "Krishnakumaran and Zhu.,? 2007",
      "shortCiteRegEx" : "Krishnakumaran and Zhu.",
      "year" : 2007
    }, {
      "title" : "Metaphors we Live by",
      "author" : [ "G. Lakoff", "M. Johnson." ],
      "venue" : "University of Chicago Press.",
      "citeRegEx" : "Lakoff and Johnson.,? 1980",
      "shortCiteRegEx" : "Lakoff and Johnson.",
      "year" : 1980
    }, {
      "title" : "The Master Metaphor List",
      "author" : [ "G. Lakoff", "J. Espenson", "A. Schwartz." ],
      "venue" : "Draft 2nd ed. Technical report, University of California at Berkeley.",
      "citeRegEx" : "Lakoff et al\\.,? 1991",
      "shortCiteRegEx" : "Lakoff et al\\.",
      "year" : 1991
    }, {
      "title" : "Women, fire, and dangerous things: what categories reveal about the mind",
      "author" : [ "G. Lakoff." ],
      "venue" : "University of Chicago Press.",
      "citeRegEx" : "Lakoff.,? 1987",
      "shortCiteRegEx" : "Lakoff.",
      "year" : 1987
    }, {
      "title" : "CorMet: a computational, corpus-based conventional metaphor extraction system",
      "author" : [ "Z.J. Mason." ],
      "venue" : "Computational Linguistics, 30(1):23–44.",
      "citeRegEx" : "Mason.,? 2004",
      "shortCiteRegEx" : "Mason.",
      "year" : 2004
    }, {
      "title" : "Semantic Signatures for Example-Based Linguistic Metaphor Detection",
      "author" : [ "M. Mohler", "D. Bracewell", "D. Hinote", "M. Tomlinson." ],
      "venue" : "Proc. of the First Workshop on Metaphor in NLP, pages 27–35. ACL.",
      "citeRegEx" : "Mohler et al\\.,? 2013",
      "shortCiteRegEx" : "Mohler et al\\.",
      "year" : 2013
    }, {
      "title" : "Moving right along: A computational model of metaphoric reasoning about events",
      "author" : [ "S. Narayanan." ],
      "venue" : "Proc. of AAAI/IAAI, pages 121–127.",
      "citeRegEx" : "Narayanan.,? 1999",
      "shortCiteRegEx" : "Narayanan.",
      "year" : 1999
    }, {
      "title" : "Maltparser: A data-driven parser-generator for dependency parsing",
      "author" : [ "J. Nivre", "J. Hall", "J. Nilsson." ],
      "venue" : "Proc. of LREC’06, volume 6, pages 2216– 2219.",
      "citeRegEx" : "Nivre et al\\.,? 2006",
      "shortCiteRegEx" : "Nivre et al\\.",
      "year" : 2006
    }, {
      "title" : "Abductive Inference for Interpretation of Metaphors",
      "author" : [ "E. Ovchinnikova", "R. Israel", "S. Wertheim", "V. Zaytsev", "N. Montazeri", "J. Hobbs." ],
      "venue" : "Proc. of the Second Workshop on Metaphor in NLP, page to appear. ACL.",
      "citeRegEx" : "Ovchinnikova et al\\.,? 2014",
      "shortCiteRegEx" : "Ovchinnikova et al\\.",
      "year" : 2014
    }, {
      "title" : "English gigaword fifth edition",
      "author" : [ "R. Parker", "D. Graff", "J. Kong", "K. Chen", "K. Maeda." ],
      "venue" : "LDC.",
      "citeRegEx" : "Parker et al\\.,? 2011",
      "shortCiteRegEx" : "Parker et al\\.",
      "year" : 2011
    }, {
      "title" : "Filling knowledge gaps in text for machine reading",
      "author" : [ "A. Peñas", "E.H. Hovy." ],
      "venue" : "Proc. of COLING’10, pages 979–987.",
      "citeRegEx" : "Peñas and Hovy.,? 2010",
      "shortCiteRegEx" : "Peñas and Hovy.",
      "year" : 2010
    }, {
      "title" : "Lexicalised systematic polysemy in wordnet",
      "author" : [ "W. Peters", "I. Peters." ],
      "venue" : "LREC’00.",
      "citeRegEx" : "Peters and Peters.,? 2000",
      "shortCiteRegEx" : "Peters and Peters.",
      "year" : 2000
    }, {
      "title" : "Local and global algorithms for disambiguation to wikipedia",
      "author" : [ "L. Ratinov", "D. Roth", "D. Downey", "M. Anderson." ],
      "venue" : "Proc. of ACL’11.",
      "citeRegEx" : "Ratinov et al\\.,? 2011",
      "shortCiteRegEx" : "Ratinov et al\\.",
      "year" : 2011
    }, {
      "title" : "SEMILAR: The Semantic Similarity Toolkit",
      "author" : [ "V. Rus", "M. Lintean", "R. Banjade", "N. Niraula", "Stefanescu D." ],
      "venue" : "Proc. of ACL’13.",
      "citeRegEx" : "Rus et al\\.,? 2013",
      "shortCiteRegEx" : "Rus et al\\.",
      "year" : 2013
    }, {
      "title" : "Can we derive general world knowledge from texts? In Proc",
      "author" : [ "L. Schubert." ],
      "venue" : "of HLT’03, pages 94–97. Morgan Kaufmann Publishers Inc.",
      "citeRegEx" : "Schubert.,? 2002",
      "shortCiteRegEx" : "Schubert.",
      "year" : 2002
    }, {
      "title" : "The proper place of men and machines in language technology: Processing Russian without any linguistic knowledge",
      "author" : [ "S. Sharoff", "J. Nivre." ],
      "venue" : "Proc. Dialogue 2011, Russian Conference on Computational Linguistics.",
      "citeRegEx" : "Sharoff and Nivre.,? 2011",
      "shortCiteRegEx" : "Sharoff and Nivre.",
      "year" : 2011
    }, {
      "title" : "Automatic metaphor interpretation as a paraphrasing task",
      "author" : [ "E. Shutova." ],
      "venue" : "Proc. of NAACL’10.",
      "citeRegEx" : "Shutova.,? 2010a",
      "shortCiteRegEx" : "Shutova.",
      "year" : 2010
    }, {
      "title" : "Models of metaphor in NLP",
      "author" : [ "E. Shutova." ],
      "venue" : "Proc. of ACL’10, pages 688–697. ACL.",
      "citeRegEx" : "Shutova.,? 2010b",
      "shortCiteRegEx" : "Shutova.",
      "year" : 2010
    }, {
      "title" : "Conceptnet 5: A large semantic network for relational knowledge",
      "author" : [ "R. Speer", "C. Havasi." ],
      "venue" : "The People’s Web Meets NLP, pages 161–176. Springer.",
      "citeRegEx" : "Speer and Havasi.,? 2013",
      "shortCiteRegEx" : "Speer and Havasi.",
      "year" : 2013
    }, {
      "title" : "Yago: A Core of Semantic Knowledge",
      "author" : [ "F.M. Suchanek", "G. Kasneci", "G. Weikum." ],
      "venue" : "Proc. of the 16th international World Wide Web conference, NY, USA. ACM Press.",
      "citeRegEx" : "Suchanek et al\\.,? 2007",
      "shortCiteRegEx" : "Suchanek et al\\.",
      "year" : 2007
    }, {
      "title" : "Word similarity using constructions as contextual features",
      "author" : [ "N. Tsao", "D. Wible." ],
      "venue" : "Proc. JSSP’13, pages 51–59.",
      "citeRegEx" : "Tsao and Wible.,? 2013",
      "shortCiteRegEx" : "Tsao and Wible.",
      "year" : 2013
    }, {
      "title" : "Automatic Metaphor Detection using Large-Scale Lexical Resources and Conventional Metaphor Extraction",
      "author" : [ "Y. Wilks", "L. Galescu", "J. Allen", "Adam Dalton." ],
      "venue" : "Proc. of the First Workshop on Metaphor in NLP, pages 36–44. ACL.",
      "citeRegEx" : "Wilks et al\\.,? 2013",
      "shortCiteRegEx" : "Wilks et al\\.",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "The term conceptual metaphor refers to the understanding of one concept or conceptual domain in terms of the properties of another (Lakoff and Johnson, 1980; Lakoff, 1987).",
      "startOffset" : 131,
      "endOffset" : 171
    }, {
      "referenceID" : 18,
      "context" : "The term conceptual metaphor refers to the understanding of one concept or conceptual domain in terms of the properties of another (Lakoff and Johnson, 1980; Lakoff, 1987).",
      "startOffset" : 131,
      "endOffset" : 171
    }, {
      "referenceID" : 32,
      "context" : "distinguishing between literal and metaphorical use of language, and 2) metaphor interpretation, inferring the intended literal meaning of a metaphorical expression; see (Shutova, 2010b) for an overview.",
      "startOffset" : 170,
      "endOffset" : 186
    }, {
      "referenceID" : 17,
      "context" : "Master Metaphor List (Lakoff et al., 1991)",
      "startOffset" : 21,
      "endOffset" : 42
    }, {
      "referenceID" : 26,
      "context" : ", 2010), relying on existing lexical-semantic resources (Peters and Peters, 2000; Wilks et al., 2013), mapping phrases to predefined target and source concepts constituting a conceptual metaphor (Heintz et al.",
      "startOffset" : 56,
      "endOffset" : 101
    }, {
      "referenceID" : 36,
      "context" : ", 2010), relying on existing lexical-semantic resources (Peters and Peters, 2000; Wilks et al., 2013), mapping phrases to predefined target and source concepts constituting a conceptual metaphor (Heintz et al.",
      "startOffset" : 56,
      "endOffset" : 101
    }, {
      "referenceID" : 12,
      "context" : ", 2013), mapping phrases to predefined target and source concepts constituting a conceptual metaphor (Heintz et al., 2013; Mohler et al., 2013).",
      "startOffset" : 101,
      "endOffset" : 143
    }, {
      "referenceID" : 20,
      "context" : ", 2013), mapping phrases to predefined target and source concepts constituting a conceptual metaphor (Heintz et al., 2013; Mohler et al., 2013).",
      "startOffset" : 101,
      "endOffset" : 143
    }, {
      "referenceID" : 12,
      "context" : "fined concepts constituting a conceptual metaphor (Heintz et al., 2013; Mohler et al., 2013), and employing a classifier (Birke and Sarkar, 2006;",
      "startOffset" : 50,
      "endOffset" : 92
    }, {
      "referenceID" : 20,
      "context" : "fined concepts constituting a conceptual metaphor (Heintz et al., 2013; Mohler et al., 2013), and employing a classifier (Birke and Sarkar, 2006;",
      "startOffset" : 50,
      "endOffset" : 92
    }, {
      "referenceID" : 32,
      "context" : "Approaches based on selectional preferences are known to overgeneralize and miss conventional metaphors (Shutova, 2010b).",
      "startOffset" : 104,
      "endOffset" : 120
    }, {
      "referenceID" : 21,
      "context" : "Automatic metaphor interpretation is performed using two principal approaches: 1) reasoning with manually coded knowledge of source domains (Narayanan, 1999; Barnden and Lee, 2002; Agerri et al., 2007; Ovchinnikova et al., 2014) and 2) deriving literal paraphrases for metaphorical expressions from corpora (Shutova, 2010a).",
      "startOffset" : 140,
      "endOffset" : 228
    }, {
      "referenceID" : 0,
      "context" : "Automatic metaphor interpretation is performed using two principal approaches: 1) reasoning with manually coded knowledge of source domains (Narayanan, 1999; Barnden and Lee, 2002; Agerri et al., 2007; Ovchinnikova et al., 2014) and 2) deriving literal paraphrases for metaphorical expressions from corpora (Shutova, 2010a).",
      "startOffset" : 140,
      "endOffset" : 228
    }, {
      "referenceID" : 23,
      "context" : "Automatic metaphor interpretation is performed using two principal approaches: 1) reasoning with manually coded knowledge of source domains (Narayanan, 1999; Barnden and Lee, 2002; Agerri et al., 2007; Ovchinnikova et al., 2014) and 2) deriving literal paraphrases for metaphorical expressions from corpora (Shutova, 2010a).",
      "startOffset" : 140,
      "endOffset" : 228
    }, {
      "referenceID" : 31,
      "context" : ", 2014) and 2) deriving literal paraphrases for metaphorical expressions from corpora (Shutova, 2010a).",
      "startOffset" : 86,
      "endOffset" : 102
    }, {
      "referenceID" : 19,
      "context" : "The CorMet system (Mason, 2004) generates CMs given predefined target and source domains.",
      "startOffset" : 18,
      "endOffset" : 31
    }, {
      "referenceID" : 19,
      "context" : "Our method is similar to (Mason, 2004) and (Gandy et al.",
      "startOffset" : 25,
      "endOffset" : 38
    }, {
      "referenceID" : 10,
      "context" : "Our method is similar to (Mason, 2004) and (Gandy et al., 2013) in that we rely on characteristic predicate, but we use general propositions instead of verb and adjective phrases, see Sec.",
      "startOffset" : 43,
      "endOffset" : 63
    }, {
      "referenceID" : 29,
      "context" : ", dropping modifiers) we can obtain common sense knowledge, see (Schubert, 2002; Clark and Harrison, 2009).",
      "startOffset" : 64,
      "endOffset" : 106
    }, {
      "referenceID" : 6,
      "context" : ", dropping modifiers) we can obtain common sense knowledge, see (Schubert, 2002; Clark and Harrison, 2009).",
      "startOffset" : 64,
      "endOffset" : 106
    }, {
      "referenceID" : 6,
      "context" : "tuples of words that have a determined pattern of syntactic relations among them (Clark and Harrison, 2009; Peñas and Hovy, 2010; Tsao and Wible, 2013).",
      "startOffset" : 81,
      "endOffset" : 151
    }, {
      "referenceID" : 25,
      "context" : "tuples of words that have a determined pattern of syntactic relations among them (Clark and Harrison, 2009; Peñas and Hovy, 2010; Tsao and Wible, 2013).",
      "startOffset" : 81,
      "endOffset" : 151
    }, {
      "referenceID" : 35,
      "context" : "tuples of words that have a determined pattern of syntactic relations among them (Clark and Harrison, 2009; Peñas and Hovy, 2010; Tsao and Wible, 2013).",
      "startOffset" : 81,
      "endOffset" : 151
    }, {
      "referenceID" : 24,
      "context" : "We parsed English Gigaword (Parker et al., 2011) with Boxer (Bos et al.",
      "startOffset" : 27,
      "endOffset" : 48
    }, {
      "referenceID" : 5,
      "context" : ", 2011) with Boxer (Bos et al., 2004).",
      "startOffset" : 19,
      "endOffset" : 37
    }, {
      "referenceID" : 13,
      "context" : "As one of the possible formats, Boxer outputs logical forms of sentences in the style of (Hobbs, 1985), generalizes over some syntactic",
      "startOffset" : 89,
      "endOffset" : 102
    }, {
      "referenceID" : 30,
      "context" : "For Russian, we used the ruWac corpus (Sharoff and Nivre, 2011) parsed with the Malt dependency parser (Nivre et al.",
      "startOffset" : 38,
      "endOffset" : 63
    }, {
      "referenceID" : 22,
      "context" : "For Russian, we used the ruWac corpus (Sharoff and Nivre, 2011) parsed with the Malt dependency parser (Nivre et al., 2006).",
      "startOffset" : 103,
      "endOffset" : 123
    }, {
      "referenceID" : 13,
      "context" : "vert these dependency graphs into logical forms in the style of (Hobbs, 1985).",
      "startOffset" : 64,
      "endOffset" : 77
    }, {
      "referenceID" : 34,
      "context" : "In order to obtain the generalizations, we first map nouns contained in the propositions into WordNet and Wikipedia semantic nodes using the YAGO ontology (Suchanek et al., 2007).",
      "startOffset" : 155,
      "endOffset" : 178
    }, {
      "referenceID" : 27,
      "context" : "In future work we plan to compare its performance with the performance of the Wikifier system (Ratinov et al., 2011) that identifies important entities and concepts in text, disambiguates them and links them to Wikipedia.",
      "startOffset" : 94,
      "endOffset" : 116
    }, {
      "referenceID" : 1,
      "context" : "There is a long-standing tradition for considering computational models derived from word co-occurrence statistics as being capable of producing reasonable property-based descriptions of concepts; see (Baroni and Lenci, 2008) for an overview.",
      "startOffset" : 201,
      "endOffset" : 225
    }, {
      "referenceID" : 4,
      "context" : "For doing so, we use the Latent Dirichlet allocation (LDA) model (Blei et al., 2003).",
      "startOffset" : 65,
      "endOffset" : 84
    }, {
      "referenceID" : 28,
      "context" : "Following (Rus et al., 2013), we define the LDA-based relatedness of words as follows:",
      "startOffset" : 10,
      "endOffset" : 28
    }, {
      "referenceID" : 7,
      "context" : "We also experimented with the Latent Semantic Analysis model (Dumais, 2004), but the LDA model proved to provide more relevant results.",
      "startOffset" : 61,
      "endOffset" : 75
    }, {
      "referenceID" : 19,
      "context" : "Both (Mason, 2004) and (Gandy et al.",
      "startOffset" : 5,
      "endOffset" : 18
    }, {
      "referenceID" : 10,
      "context" : "Both (Mason, 2004) and (Gandy et al., 2013) employ the WordNet hierarchy for clustering.",
      "startOffset" : 23,
      "endOffset" : 43
    }, {
      "referenceID" : 12,
      "context" : "Heintz et al. (2013) present a similar approach mapping sentences to LDA topic models for target and source domains.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 33,
      "context" : "In order to obtain a lexically richer representation of the domains, we expand the sets of these target and source lexemes with semantically related lexemes using English and Russian ConceptNet resources (Speer and Havasi, 2013) and top ranked patterns from the proposition stores.",
      "startOffset" : 204,
      "endOffset" : 228
    }, {
      "referenceID" : 9,
      "context" : "The Fleiss’ kappa (Fleiss, 1971) is 0.",
      "startOffset" : 18,
      "endOffset" : 32
    }, {
      "referenceID" : 19,
      "context" : "In order to compare our system results to the CorMet system (Mason, 2004), we present an evaluation against the Master Metaphor List (Lakoff et al.",
      "startOffset" : 60,
      "endOffset" : 73
    }, {
      "referenceID" : 17,
      "context" : "In order to compare our system results to the CorMet system (Mason, 2004), we present an evaluation against the Master Metaphor List (Lakoff et al., 1991).",
      "startOffset" : 133,
      "endOffset" : 154
    }, {
      "referenceID" : 19,
      "context" : "Note that Mason (2004) evaluates the correspondences between the CorMet mappings and the MML mappings by hand which introduces subjectivity, whereas our evaluation is done automatically.",
      "startOffset" : 10,
      "endOffset" : 23
    } ],
    "year" : 2014,
    "abstractText" : "Contemporary research on computational processing of linguistic metaphors is divided into two main branches: metaphor recognition and metaphor interpretation. We take a different line of research and present an automated method for generating conceptual metaphors from linguistic data. Given the generated conceptual metaphors, we find corresponding linguistic metaphors in corpora. In this paper, we describe our approach and its evaluation using English and Russian data.",
    "creator" : "dvips(k) 5.991 Copyright 2011 Radical Eye Software"
  }
}