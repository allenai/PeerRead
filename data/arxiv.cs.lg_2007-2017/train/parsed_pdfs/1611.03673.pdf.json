{
  "name" : "1611.03673.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Piotr Mirowski", "Razvan Pascanu", "Fabio Viola", "Hubert Soyer", "Andy Ballard", "Andrea Banino", "Misha Denil", "Ross Goroshin", "Laurent Sifre", "Koray Kavukcuoglu", "Dharshan Kumaran", "Raia Hadsell" ],
    "emails" : [ "@google.com" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "The ability to navigate efficiently within an environment is fundamental to intelligent animal behavior. Whilst conventional robotics methods, such as Simultaneous Localisation and Mapping (SLAM), tackle navigation through an explicit focus on position inference and mapping (Dissanayake et al., 2001), here we follow recent work in deep reinforcement learning (Mnih et al., 2015; 2016) and propose that navigational abilities will emerge as the by-product of an agent learning a policy that maximizes reward. One advantage of an intrinsic, end-to-end approach is that actions are not divorced from representation, but rather learnt together, thus ensuring that task-relevant features are present in the representation. Learning to navigate from reinforcement learning in partially observable environments, however, poses several challenges.\nFirst, rewards are often sparsely distributed in the environment, where there may be only one goal location. Second, environments often comprise dynamic elements, requiring the agent to use memory at different timescales: rapid one-shot memory for the goal location (Section 4.1), together with short term memory subserving temporal integration of velocity signals and visual observations, and longer term memory for constant aspects of the environment (e.g. boundaries, cues).\nWe tackle the problem of sparse rewards by augmenting our loss with auxiliary tasks that provide denser training signals that support navigation-relevant representation learning. We consider two additional losses: the first involves reconstruction of a low-dimensional depth map at each time step, and can be seen as inferring depth from monocular imagery (Eigen et al., 2014). This unsupervised task concerns the 3D geometry of the environment, and is aimed to encourage the learning of representations that aid obstacle avoidance and short-term trajectory planning. The second task is\n∗Denotes equal contribution\nar X\niv :1\n61 1.\n03 67\n3v 1\n[ cs\n.A I]\n1 1\nN ov\nself-supervised, and directly invokes loop closure from SLAM: the agent is trained to predict if the current location has been previously visited within a local trajectory. We show that the addition of these auxiliary tasks bootstraps the learning process and considerably increases data efficiency.\nTo address the requirement for memory over different timescales, we incorporate a variant of a stacked LSTM architecture (Graves et al., 2013; Pascanu et al., 2013): we believe that this allows one LSTM, which receives the reward signal and the representation constructed by the convolutional encoder as input, to rapidly update and maintain the currently relevant goal location (i.e. in environments where the goal changes frequently) and provide this as contextual input to a separate LSTM that also receives input from the encoder and additional velocity information, which dictates the policy.\nTo evaluate our approach, we use five 3D maze environments and demonstrate the accelerated learning and increased performance of the proposed agent architecture. These environments feature complex geometry, random start position and orientation, dynamic goal locations, and long episodes that require thousands of agent steps (see Figure 1). We also provide detailed analysis of the trained agent to show that critical navigation skills are acquired. This is important as neither position inference nor mapping are directly part of the loss; therefore, raw performance on the goal finding task is not necessarily a good indication that these skills are acquired. In particular, we show that the proposed agent resolves ambiguous observations and quickly localizes itself in a complex maze, and that this localization capability is correlated with higher task reward."
    }, {
      "heading" : "2 APPROACH",
      "text" : "Our approach is an online, end-to-end learning framework that incorporates multiple objectives to train a neural network consisting of a convolutional encoder that feeds representations of the visual input to a stacked Long-Short Term Memory (LSTM) recurrent neural network. Specifically, the agent is trained jointly on three losses. Firstly it tries to maximize cumulative reward using reinforcement learning. Secondly it solves an unsupervised loss of inferring the depth map from the RGB observation. Finally, the agent is trained to detect loop closures as a self-supervised task that uses velocity integration.\nThe reinforcement learning problem is addressed with the Asynchronous Advantage Actor-Critic (A3C) algorithm (Mnih et al., 2016) that relies on learning both a policy π(at|st; θ) and value function V (st; θV ) given a state observation st. Both the policy and value function share all intermediate representations, both being computed using a separate linear layer from the top most layer of the model. The advantage function A is used to estimate the policy gradient which updates model parameters θ and θV :\nA(st, at; θV ) = k−1∑ i=0 γirt+i + γ kV (st+k; θV )− V (st; θV ). (1)\nThe agent setup closely follows the work of (Mnih et al., 2016) and we refer to this work for the details (e.g. the use of a convolutional encoder followed by either a Multi-Layer Perceptron (MLP) or an LSTM, the use of action repetition, entropy regularization to prevent the policy saturation, etc.). These details can be found in the Appendix. We note that the main model used in our work is recurrent, though, for comparison, we also run a feedforward version of the agent.\nThe baseline that we consider in this work is an A3C agent that receives only RGB input from the environment, using either a recurrent or a purely feed-forward model. To support the navigation capability of our approach, we expand the observations of the agents to include agent-relative velocity measurements. Additionally, the agent is provided with the action sampled from the stochastic policy and the immediate reward, from the previous time step. Thus, the observation st may include an image xt ∈ R3×W×H (where W and H are the width and height of the image), the agent-relative lateral and rotational velocity vt ∈ R6, the previous action at−1 ∈ RNA , and the previous reward rt−1 ∈ R. Figure 2 depicts four agent architectures that we consider. All use a three-layer convolutional encoder. Figure 2a shows a purely feedforward model, while 2b replaces the last linear layer with an LSTM. Figure 2c (Nav A3C) shows the A3C agent with augmented inputs and a stacked LSTM, where reward is input to the first layer, while the velocity and action are input to the second LSTM. Figure 2d (Nav A3C+D+L) shows additional losses, where depth may be predicted from the encoder features and loop closures may be predicted from the LSTM hidden units. The additional losses are computed on the current frame via an MLP from the last hidden state of the model, similarly to the policy and value functions, and from the output of the convolutional encoder. The agent is trained online by applying both the advantage actor-critic gradient update and the gradient updates from the depth and loop prediction, with the predictors scaled by respective coefficients βd and βl. More details of the online learning algorithm are given in Appendix B."
    }, {
      "heading" : "2.1 DEPTH PREDICTION",
      "text" : "The primary input to the agent is in the form of RGB images. However, depth information, covering the field of view of the agent, might supply valuable information about the 3D structure of the environment. While depth could be directly used as an input, it has been shown that depth can be successfully predicted from single frames using a convolutional neural network (Eigen et al., 2014). Furthermore, the depth prediction loss provides more consistent gradients than those obtained from reward-based updates of the RL loss.\nA mean square loss Ld is used, scaled by a hyper-parameter βd when combined with the RL update and the loop closure loss term. The predicted depth is a function of the convolutional output (d̂t = gd(ft)), where gd is an MLP. To ensure that we converge quickly on the unsupervised loss and hence drive meaningful representation for the RL task, we use a coarse resolution for the depth map (8× 16). The MSE depth loss is expressed as Ld = 12 ∑ t ||d̂t − dt||22."
    }, {
      "heading" : "2.2 LOOP CLOSURE PREDICTION",
      "text" : "Loop closure, like depth, is valuable for a navigating agent, since it signals that the agent has returned to an already visited location, and can be used for efficient exploration and spatial reasoning. As with depth, we hypothesize that loop closure prediction could be a valuable auxiliary loss to augment the reward-based training signal. Unlike depth prediction, the loop predictor is an MLP that operates on the output of the LSTM, since memory is required to predict the loop closure events.\nTo produce the training targets, we detect loop closures based on the similarity of local position information during an episode, which is obtained by integrating 2D velocity over time. Specifically, in a trajectory noted {p0, p1, . . . , pT }, where pt is the position of the agent at time t, we define a loop closure label lt that is equal to 1 if the position pt of the agent is close to the position pt′ at an earlier time t′. In order to avoid trivial loop closures on consecutive points of the trajectory, we add an extra condition on an intermediary position pt′′ being far from pt. Thresholds η1 and η2 provide these two limits. Learning to predict the binary loop label is done by minimizing the Bernoulli loss Ll between lt and the output of a single-layer output from the hidden representation ht of the last hidden layer of the model, followed by a sigmoid activation. We note gl the MLP function applied to ht. The scale of this loss is a hyper-parameter of the model, noted βl. The loop closure loss is expressed as Ll = ∑ t ltgl(ht) + (1− lt)(1− gl(ht))."
    }, {
      "heading" : "3 RELATED WORK",
      "text" : "There is a rich literature on navigation, primarily in the robotics literature. However, here we focus on related work in deep RL. Deep Q-networks (DQN) have had breakthroughs in extremely challenging domains such as Atari (Mnih et al., 2015).Recent work has developed on-policy RL methods such as advantage actor-critic that use asynchronous training of multiple agents in parallel (Mnih et al., 2016). Recurrent networks have also been successfully incorporated to enable state disambiguation in partially observable environments (Koutnk et al., 2013; Hausknecht & Stone, 2015; Mnih et al., 2016; Narasimhan et al., 2015).\nDeep RL has recently been used in the navigation domain. Kulkarni et al. (2016) used a feedforward architecture to learn deep successor representations that enabled behavioral flexibility to reward changes in the MazeBase gridworld, and provided a means to detect bottlenecks in 3D VizDoom. Zhu et al. (2016) used a feedforward siamese actor-critic architecture incorporating a pretrained ResNet to support navigation to a target in a discretised 3D environment. Oh et al. (2016) investigated the performance of a variety of networks with external memory (Weston et al., 2014) on simple navigation tasks in the Minecraft 3D block world environment. Tessler et al. (2016) also used the Minecraft domain to show the benefit of combining feedforward deep-Q networks with the learning of resuable skill modules (cf options: (Sutton et al., 1999)) to transfer between navigation tasks.\nAuxiliary tasks have often been used to facilitate representation learning (Suddarth & Kergosien, 1990). Recently, the incorporation of additional objectives, designed to augment representation learning through auxiliary reconstructive decoding pathways (Zhang et al., 2016; Rasmus et al., 2015; Zhao et al., 2015; Mirowski et al., 2010), has yielded benefits in large scale classification tasks. In deep RL settings, however, only one previous paper has examined the benefit of auxiliary tasks. Specifically, Lample & Chaplot (2016) show that the performance of a DQN agent in a first-person shooter game in the VizDoom environment can be substantially enhanced by the addition of a supervised auxiliary task, whereby the convolutional network was trained on an enemy-detection task, with information about the presence of enemies, weapons, etc., provided by the game engine.\nIn contrast, our contribution addresses fundamental questions of how to learn an intrinsic representation of space, geometry, and movement while simultaneously maximising rewards through reinforcement learning. Our method is validated in challenging maze domains with random start and goal locations."
    }, {
      "heading" : "4 EXPERIMENTS",
      "text" : "We consider a set of first-person 3D mazes called Labyrinth and based on OpenArena (see Fig. 1) that are visually rich, with additional observations available to the agent such as inertial information\nand local depth information.1 The action space is discrete, yet allows finegrained control, comprising 8 actions: the agent can rotate in small increments, accelerate forward or backward or sideways, or induce rotational acceleration while moving. Reward is achieved in these environments by reaching a goal from a random start location and orientation. If the goal is reached, the agent is respawned to a new start location and must return to the goal. The episode terminates when a fixed amount of time expires, typically affording the agent enough time to find the goal several times. There are additional ‘fruit’ rewards which are very sparse and serve to encourage exploration. Apples are worth 1 point, strawberries 2 points and goals are 10 points. Videos of the agent solving the maze are linked in Appendix A.\nIn the static variant of the maze, the goal and fruit locations are fixed and only the agent start location changes. In the dynamic (Random Goal) variant, the goal and fruits are randomly placed on every episode, and agent starts randomly, but the maze layout itself is static. Within an episode, the goal and apple locations stay fixed until the episode ends. This allows an explore-exploit strategy, where the agent should initially explore the maze to find the goal, then remember the location and quickly reacquire the goal after each respawn. For both variants (static and random goal) we consider a small and large map. The small mazes are 5× 10 and episodes last for 3600 timesteps, and the large mazes are 9× 15 with 10800 steps (see Figure 1). The RGB observation is 84× 84. The I-Maze environment (see Figure 1, right) is inspired by the classic T-maze used to investigate navigation in rodents (Olton et al., 1979): the layout remains fixed throughout, the agent spawns in the central corridor where there are apple rewards and has to locate the goal which is placed in the alcove of one of the four arms. Because the goal is hidden in the alcove, the optimal agent behaviour must rely on memory of the goal location in order to return to the goal using the most direct route. Goal location is constant within an episode but varies randomly across episodes.\nThe different agent architectures described in Section 2 are evaluated by training on five mazes. Figure 3 shows these learning curves. In each case we ran 64 experiments with randomly sampled hyper-parameters (for ranges and details please see the appendix). The mean over the top 5 runs as well as the top 5 curves are plotted. Expert human scores, established by a professional game player, are compared to these results in Table 1. The Nav A3C+D+L agents reach human-level performance on Static 1 and 2, and attain about 80% and 50% of human scores on Random Goal 1 and 2.\n1The environments described in this paper will be publicly available before the conference.\nWe note some particular results from these learning curves. In Figure 3 (a and b), consider the feedforward A3C model (red curve) versus the LSTM version (pink curve). Even though navigation seems to intrinsically require memory, as single observations could often be ambiguous, the feedforward model achieves competitive performance on static mazes. This suggest that there might be good strategies that do not involve temporal memory and give good results, namely a reactive policy held by the weights of the encoder, or learning a wall-following strategy. We therefore introduce dynamic environments that encourage the use of memory and more general navigation strategies.\nFigure 3 also shows the advantage of adding velocity, reward and action as an input, as well as the impact of using a two layer LSTM (orange curve vs red and pink). Though this agent (Nav A3C) is better than the simple architectures, it is still relatively slow to train on all of the mazes. We believe that this is mainly due to the slower, data inefficient learning that is generally seen in pure RL approaches. Supporting this we see that adding the auxiliary prediction targets of depth and loop closure (Nav A3C+D+L, blue curve) speeds up learning dramatically on most of the mazes (see Table 1: AUC metric). It has the strongest effect on the static mazes because of the accelerated learning, but also gives a substantial and lasting performance increase on the random goal mazes.\nAlthough we place more value on the task performance than on the auxiliary losses, we report the results from the loop closure prediction task. Over 100 test episodes of 2250 steps each, within a large maze (random goal 2), the Nav A3C+D+L agent demonstrated very successful loop detection, reaching an F-1 score of 0.83. A sample trajectory can be seen in Figure 4 (right).\nIn order to disambiguate the effect of a depth prediction loss vs. simply adding depth as an input to the agent, we compare the performance of the Nav A3C+D agent to a Nav A3C where the visual input is RGBD instead of RGB. The comparison, in Figure 3f, shows that depth is much more useful for self supervision than as input to the agent."
    }, {
      "heading" : "5 ANALYSIS",
      "text" : ""
    }, {
      "heading" : "5.1 POSITION DECODING",
      "text" : "In order to evaluate the internal representation of location within the agent (either in the hidden units ht of the last LSTM, or, in the case of the FF A3C agent, in the features ft on the last layer of the conv-net), we train a position decoder that takes that representation as input, consisting of a linear classifier with multinomial probability distribution over the discretized maze locations. Small mazes (5× 10) have 50 locations, large mazes (9× 15) have 135 locations, and the I-maze has 77 locations. Note that we do not backpropagate the gradients from the position decoder through the rest of the network. The position decoder can only see the representation exposed by the model, not change it.\nAn example of position decoding by the Nav A3C+D+L agent is shown in Figure 6, where the initial uncertainty in position is improved to near perfect position prediction as more observations are acquired by the agent. We observe that position entropy spikes after a respawn, then decreases once the agent acquires certainty about its location. Additionally, videos of the agent’s position decoding are linked in Appendix A. In these complex mazes, where localization is important for the purpose of reaching the goal, it seems that position accuracy and final score are correlated, as shown in Table 1. In Static 1, the best position decoding is obtained by the plain A3C agent (88.6% accuracy), whereas\nNav A3C+D+L follow at 86.0% accuracy. A pure feed-forward architecture still achieves 64.3% accuracy in a static maze with static goal, suggesting that the encoder memorizes the position in the weights and that this small maze is solvable by all the agents, with sufficient training time. In Random Goal 1, it is Nav A3C+D+L that achieves the best position decoding performance (78.7% accuracy), whereas the FF A3C and the LSTM A3C architectures are at approximately 50%.\nIn the I-maze, the opposite branches of the maze are nearly identical, with the exception of very sparse visual cues. We observe that once the goal is first found, the Nav A3C+D+L agent is capable of directly returning to the correct branch in order to achieve the maximal score. However, the linear position decoder for this agent is only 68.5% accurate, whereas it is 87.8% in the plain LSTM A3C agent. We hypothesize that the symmetry of the I-maze will induce a symmetric policy that need not be sensitive to the exact position of the agent (see analysis below).\nA desired property of navigation agents in our Random Goal tasks is to be able to first find the goal, and reliably return to the goal via an efficient route after subsequent re-spawns. The latency column in Table 1 shows that the Nav A3C+D+L agents achieve the lowest latency to goal once the goal has been discovered (the first number shows the time in seconds to find the goal the first time, and the second number is the average time for subsequent finds). Figure 5 shows clearly how the agent finds the goal, and directly returns to that goal for the rest of the episode. For Random Goal 2, none of the agents achieve lower latency after initial goal acquisition; this is presumably due to the larger, more challenging environment."
    }, {
      "heading" : "5.2 STACKED LSTM GOAL ANALYSIS",
      "text" : "Figure 7(a) shows shows the trajectories traversed by an agent for each of the four goal locations. After an initial exploratory phase to find the goal, the agent consistently returns to the goal location. We visualize the agent’s policy by applying tSNE dimension reduction (Maaten & Hinton, 2008) to the cell activations at each step of the agent for each of the four goal locations. Whilst clusters corresponding to each of the four goal locations are clearly distinct in the LSTM A3C agent, there are 2 main clusters in the Nav A3C agent – with trajectories to diagonally opposite arms of the maze represented similarly. Given that the action sequence to opposite arms is equivalent (e.g. straight, turn left twice for top left and bottom right goal locations), this suggests that the Nav A3C policy-dictating LSTM maintains an efficient representation of 2 sub-policies (i.e. rather than 4 independent policies) – with critical information about the currently relevant goal provided by the additional LSTM."
    }, {
      "heading" : "6 CONCLUSION",
      "text" : "We proposed a deep RL method, augmented with memory and auxiliary learning targets, for training agents to navigate within large and visually rich environments that include frequently changing start and goal locations. Our results and analysis highlight the utility of un/self-supervised auxiliary objectives, namely depth prediction and loop closure, in providing richer training signals that bootstrap learning and enhance data efficiency. Further, we examine the behavior of trained agents, their ability to localise, and their network activity dynamics, in order to analyse their navigational abilities.\nOur approach of augmenting deep RL with auxiliary objectives allows end-end learning and may encourage the development of more general navigation strategies. Notably, our work is related to (Jaderberg et al., 2017) which focuses on data efficiency by exploiting different auxiliary losses that are applicable in many RL settings. Our focus is on the navigation domain and understanding if navigation emerges as a bi-product of solving an RL problem.\nWhilst our best performing agents are relatively successful at navigation, their abilities would be stretched if larger demands were placed on rapid memory (e.g. in procedurally generated mazes), due to the limited capacity of the stacked LSTM in this regard. It will be important for future work\nto combine visually complex environments with architectures that make use of external memory (Graves et al., 2016; Weston et al., 2014; Olton et al., 1979) to enhance the navigational abilities of agents."
    }, {
      "heading" : "ACKNOWLEDGEMENTS",
      "text" : "We would like to thank Thomas Degris and Joseph Modayil for useful discussions, Charles Beattie, Julian Schrittwieser, Marcus Wainwright, and Stig Petersen for environment design and development, and Amir Sadik and Sarah York for expert human game testing."
    }, {
      "heading" : "B NETWORK ARCHITECTURE AND TRAINING",
      "text" : "B.1 THE ONLINE MULTI-LEARNER ALGORITHM FOR MULTI-TASK LEARNING\nWe introduce a class of neural network-based agents that have modular structures and that are trained on multiple tasks, with inputs coming from different modalities (vision, depth, past rewards and past actions). Implementing our agent architecture is simplified by its modular nature. Essentially, we construct multiple networks, one per task, using shared building blocks, and optimise these networks jointly. Some modules, such as the conv-net used for perceiving visual inputs, or the LSTMs used for learning the navigation policy, are shared among multiple tasks, while other modules, such as depth predictor gd or loop closure predictor gl, are task-specific. The navigation network that outputs the policy and the value function is trained using reinforcement learning, while the depth prediction and loop closure prediction networks are trained using self-supervised learning.\nWithin each thread of the asynchronous training environment, the agent plays on its own episode of the game environment, and therefore sees observation and reward pairs {(st, rt)} and takes actions that are different from those experienced by agents from the other, parallel threads. Within a thread, the multiple tasks (navigation, depth and loop closure prediction) can be trained at their own schedule, and they add gradients to the shared parameter vector as they arrive. Within each thread, we use a flag-based system to subordinate gradient updates to the A3C reinforcement learning procedure.\nB.2 NETWORK AND TRAINING DETAILS\nFor all the experiments we use an encoder model with 2 convolutional layers followed by a fully connected layer, or recurrent layer(s), from which we predict the policy and value function. The architecture is similar to the one in (Mnih et al., 2016). The convolutional layers are as follows. The first convolutional layer has a kernel of size 8x8 and a stride of 4x4, and 16 feature maps. The second layer has a kernel of size 4x4 and a stride of 2x2, and 32 feature maps. The fully connected layer, in the FF A3C architecture in Figure 2a has 256 hidden units (and outputs visual features ft). The LSTM in the LSTM A3C architecture has 256 hidden units (and outputs LSTM hidden activations ht). The LSTMs in Figure 2c and 2d are fed extra inputs (past reward rt−1, previous action at expressed as a one-hot vector of dimension 8 and agent-relative lateral and rotational velocity vt encoded by a 6-dimensional vector), which are all concatenated to vector ft. The Nav A3C architectures (Figure 2c,d) have a first LSTM with 64 or 128 hiddens and a second LSTM with 256 hiddens. The depth predictor modules gd and the loop closure detection modules gl are both single-layer MLPs with 128 hidden units, respectively followed by 8× 16 = 128 linear outputs and by 2 softmax outputs. We illustrate on Figure 8 the architecture of the Nav A3C+D+L agent.\nThe parameters of each of the modules point to a subset of a common vector of parameters. We optimise these parameters using an asynchronous version of RMSProp (Tieleman & Hinton, 2012). (Nair et al., 2015) was a recent example of asynchronous and parallel gradient updates in deep\n2Video of the Nav A3C+D+L agent on the I-maze: https://youtu.be/PS4iJ7Hk_BU 3Video of the Nav A3C+D+L agent on static maze 1: https://youtu.be/-HsjQoIou_c 4Video of the Nav A3C+D+L agent on static maze 2: https://youtu.be/kH1AvRAYkbI 5Video of the Nav A3C+D+L agent on random goal maze 1: https://youtu.be/5IBT2UADJY0 6Video of the Nav A3C+D+L agent on random goal maze 2: https://youtu.be/e10mXgBG9yo\nreinforcement learning; in our case, we focus on the specific Asynchronous Advantage Actor Critic (A3C) reinforcement learning procedure in (Mnih et al., 2016).\nLearning follows closely the paradigm described in (Mnih et al., 2016). We use 16 workers and thesame RMSProp algorithm without momentum or centering of the variance. Gradients are computed over non-overlaping chunks of 100 steps of the episode. The score for each point of a training curve is the average over all the episodes the model gets to finish in 5e4 environment steps.\nThe whole experiments are run for a maximum of 1e8 environment step. The agent has an action repeat of 4 as in (Mnih et al., 2016), which means that for 4 consecutive steps the agent will use the same action picked at the beginning of the series. For this reason through out the paper we actually report results in terms of agent perceived steps rather than environment steps. That is, the maximal number of agent perceived step that we do for any particular run is 2.5e7.\nIn our grid we sample hyper-parameters from categorical distributions:\n• Learning rate was sampled from [10−4, 5 · 10−3]. • Strength of the entropy regularization from [10−4, 10−2]. • Rewards were scaled by a factor from {0.3, 0.5} and clipped to 1 prior to back-propagation\nin the Advantage Actor-Critic algorithm.\nThe auxiliary tasks, when used, have hyperparameters sampled from:\n• Coefficient βd of the depth prediction loss Ld sampled from {0.1, 0.33, 1, 3.33, 10, 33, 100}. • Coefficient βl of the loop closure prediction loss Ll sampled from {0.1, 0.33, 1, 3.33, 10}.\nLoop closure uses the following thresholds: maximum distance for position similarity η1 = 1 square and minimum distance for removing trivial loop-closures η2 = 2 squares."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "Learning to navigate in complex environments with dynamic elements is an impor-<lb>tant milestone in developing AI agents. In this work we formulate the navigation<lb>question as a reinforcement learning problem and show that data efficiency and task<lb>performance can be dramatically improved by relying on additional auxiliary tasks<lb>to bootstrap learning. In particular we consider jointly learning the goal-driven<lb>reinforcement learning problem with an unsupervised depth prediction task and<lb>a self-supervised loop closure classification task. Using this approach we can<lb>learn to navigate from raw sensory input in complicated 3D mazes, approaching<lb>human-level performance even under conditions where the goal location changes<lb>frequently. We provide detailed analysis of the agent behaviour, its ability to<lb>localise, and its network activity dynamics. We then show that the agent implicitly<lb>learns key navigation abilities, through reinforcement learning with sparse rewards<lb>and without direct supervision.",
    "creator" : "LaTeX with hyperref package"
  }
}