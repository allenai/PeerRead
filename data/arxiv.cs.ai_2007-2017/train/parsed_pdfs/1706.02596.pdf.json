{
  "name" : "1706.02596.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Reading Twice for Natural Language Understanding",
    "authors" : [ "Dirk Weissenborn" ],
    "emails" : [ "dirk.weissenborn@dfki.de" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Words can be interpreted in many ways depending on the context they appear in. However, in natural language understanding (NLU) applications, the appropriate interpretation and function of a word is generally not ambiguous to humans (if it were, the task would be ill-posed). Understanding words in context is therefore a fundamental challenge to all NLU applications. For example, in document question answering (QA) [19], the “context” consists of both the question being asked and the document where the answer is expected to be found, and both are necessary to capture the function and meaning of the words they are composed of.\nNeural NLU systems typically represent a word by a single, context-agnostic n-dimensional word embedding [15, 18]. Following studies [17, 5], however, quickly realized that a single representation can hardly account for lexical ambiguity and proposed to model words by multiple representations instead. Recently, Hu et al. [8] show that general-purpose contextual representations can also be computed from the local context instead of keeping multiple representations for single words. Melamud et al. [14] showed that bidirectional recurrent neural networks (BiRNNs) can compute contextual representations which help in disambiguating tokens in their local context.\nAlthough these studies show progress in disambiguating tokens in local context, there hasn’t been an attempt to model the broader contextual meaning of words directly as part of the NLU task. We argue that the meaning and function of individual words highly depends on the broader context they appear in, including the task. For instance, recent work on QA [26, 3] shows that providing simple, task-specific word level information, in particular a feature determining whether a word occurs in the question or not, in addition to the words themselves can have a dramatic impact on the performance\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nar X\niv :1\n70 6.\n02 59\n6v 1\n[ cs\n.C L\n] 8\nJ un\n2 01\nof a previously non-competitive neural QA model. Such hand-crafted, contextual features, however, should be easy to catch by a model that reads the context first.\nThis work improves natural language understanding models by refining generic (i.e., context-agnostic) word representations with the full context they are encountered in. We compute contextual (i.e., context-dependent) word representations by “reading” the provided context and updating the initial, non-contextual representations incrementally.\nAfter the word representations have been refined using the context, they are utilized by a task-specific NLU model which solves the task at hand. Since our refinement model produces new representations for words conditioned on text, it can be used as a mechanism for incorporating contextually relevant information as well. We explore this possibility by retrieving relevant supporting information (in the form of free text) from external data sources by constructing queries from the initial context. This process is illustrated in Figure 1.\nOur experiments show that our incremental, context-conditional word representation refinement strategy leads to substantially improved performance across different NLU tasks. Moreover, with contextually refined embeddings and supplementary information from a simple retrieval process, we can significantly close the gap between basic, general-purpose reading architectures and the stateof-the-art performance obtained with highly fine-tuned, task-specific architectures.\nThe contributions of this work are the following: i) the specification of a task-agnostic refinement strategy for the computation of contextual word representations; ii) detailed experiments on 3 competitive benchmarks revealing that performance of previously non-competitive, neural baseline models greatly benefit from employing contextual representations approaching the performances of highly tuned and complex neural architectures; iii) we show that increasing the amount of provided context during refinement leads to higher performance and iv) that our models are able to exploit external knowledge from ConceptNet while demonstrating their sensitivity towards the semantics of the provided knowledge; v) we provide an in-depth analysis of the contextual representations created by our model wrt. to the QA task and their non-contextual counterparts."
    }, {
      "heading" : "2 Contextual Word Representations",
      "text" : "In this work, we propose to compute contextually refined word representations prior to processing the NLU task at hand. An illustration of our (incremental) refinement strategy can be found in Figure 1. In the following, we define this procedure formally. We denote the hidden dimensionality of our model by n and a fully-connected layer by FC(u) =Wu+ b, W ∈ Rn×m, b ∈ Rn,u ∈ Rm."
    }, {
      "heading" : "2.1 Non-contextual Word Representations",
      "text" : "We compute non-contextual word representations ew by combining pre-trained word vectors epw ∈ Rn ′ with computed character-based embeddings echarw ∈ Rn. The formal definition of this combination is given in Eq. 1.\nep+w = ReLU(Ue p w) , U ∈ Rn×n\n′\ngw = σ\n( FC ([ ep+w echarw ])) ew = gw · ep+w + (1− gw) · echarw (1)\necharw is computed via a single-layer convolutional neural network using n convolutional filters of width 5 followed by a max-pooling operation over time. Combining pre-trained with character based word embeddings in such a way is common practice. Our approach follows [21, 26]."
    }, {
      "heading" : "2.2 Contextual Word Representations",
      "text" : "For the creation of contextual word representations eXw we assume a given context which we formally define as a set of texts X . Each text X ∈ X represents a sequence of tokens X = (X1, ..., XLX ) which are embedded using the computed, non-contextual word representations to X = (X1, ...,XLX ). The embedded texts are individually processed by a bidirectional recurrent neural network, a BiLSTM [1] in this work. The resulting output X̃ ∈ RLX×2n is further projected to X̂ ∈ RLX×n by a fully-connected layer followed by a ReLU non-linearity (Eq. 2).\nX̃ = BiLSTM(X) X̂i = ReLU(FC(X̃i)) (2)\nTo update the non-contextual representation of w, we initially max-pool all representations of occurrences matching w in every X ∈ X to compute êw (Eq. 3). Finally, we combine the contextindependent representation ew with êw to form a context-sensitive representation eXw via a gated addition (Eq. 5).\nêw = max { X̂i|X ∈ X , lemma(Xi) = lemma(w) } (3)\nĝw = σ\n( FC ([ ew êw ])) (4)\neXw = ĝw · ew + (1− ĝw) · êw (5)\nNote that we soften the matching condition forw using lemmatization, lemma(w), during the pooling operation of Eq. 3 because contextual information about certain words is mostly independent of the current word form w they appear in. As a consequence, this minor linguistic pre-processing step allows for additional interaction between word forms of the same lemma."
    }, {
      "heading" : "2.3 Incremental Reading",
      "text" : "Instead of reading the entire context given by the NLU task at once, we propose to read the context X incrementally and update word representations after each reading step. To do this we need to divide X into K ordered partitions Xk ⊆ X , ⋃ k Xk = X , Xk ∩ Xk′ = ∅. The model reads X starting with X1 and updates the non-contextual word representations ew = e0w as described in Eq. 5. This process is repeated K times resulting in intermediate representations ekw for each partition k. The subsequent, task-specific model utilizes the final representations eXw = e K w for word w as its respective word embedding. This allows for interaction between different parts of X already during the computation of the contextual representations.\nIn case of document QA X1 might contain the question q while X2 contains the supporting document p used to extract the answer. Thus, the system first reads q and updates its word-representations for each w to e1w followed by reading p using the newly computed e 1 w and another update on the word-embeddings resulting in e2w = e X w . Because such a partitioning scheme gives a meaning to k (it encodes the type of input), we feed k represented by a 1-in-K feature vector additionally for each token to the BiLSTM of Eq. 2."
    }, {
      "heading" : "2.4 Integrating External Knowledge",
      "text" : "Different kinds of external knowledge could potentially be leveraged to enhance the performance of a NLU model. However, these come in different formats, such as (subject, predicate, object)-triples, a very prominent format, but we can also imagine to exploit encyclopedia definitions in free text form. Since we do not want to restrict ourselves by using a specific format or ontology, we consider external knowledge to be simple assertions in free text form. For instance, given a triple (monkey, typeOf, animal) we can derive the free text assertion “monkey type of animal\". Crucially, this allows for a seamless integration of external knowledge into our contextual, refinement process.\nConceptNet ConceptNet1 [23] (CN) is freely-available, multi-lingual semantic network that originated from the Open Mind Common Sense project and incorporates selected knowledge from various other knowledge sources, such as Wiktionary2, Open Multilingual WordNet3, OpenCyc or DBpedia4.\nRetrieval We view an assertion a as a statement about a certain subject sa. They are therefore indexed and retrieved by sa and by a set of objects Oa. The only pre-processing we employ is lemmatization, i.e., sa and Oa refer to the lemmatized subject and objects of a, respectively. For instance, the assertion a =“the Greek classical elements are fire, air, water, and earth\" would be indexed with sa = Greek_classical_element and Oa = {fire, air, water, earth}. In this work we use ConceptNet5 as our only source of external knowledge. It comes in form of triples which we view as the simplest form of assertions containing only a single object, i.e., |Oa| = 1. However, our definition of assertions and their retrieval easily extends to free form assertions, such as word definitions or encyclopedic entries."
    }, {
      "heading" : "3 Experimental Setup",
      "text" : "Task-specific Models One objective of our study is to allow simple, general-purpose models to approach state-of-the-art results by operating on contextual word representations wX explained in §2. Therefore, we chose single-layer BiLSTM based encoders with a task-specific, feed-forward neural network on top for prediction. Such models are common baselines for NLU tasks and can be considered general reading architectures as opposed to highly tuned, task-specific NLU systems that achieve current state-of-the art results. All models are trained end-to-end jointly with our refinement architecture. We provide the exact model implementations and training details in Appendix A.\nQuestion Answering We apply our QA models on 2 recent QA benchmark datasets, namely SQuAD[19] and NewsQA[24]. The task is to predict an answer span within a provided document p given a question q. Both datasets are very large, containing on the order of 105 examples. See Appendix A.1 for model details.\nRecognizing Textual Entailment We test on the frequently used SNLI dataset [2], a collection of 570k sentence pairs. Given two sentences, the premise p and the hypothesis q, a model should predict whether p either entails, contradicts or is neutral to q. See Appendix A.2 for model details.\nAssertion Retrieval To restrict the number of assertions and increase the connections between q and p in our two tasks, we retrieve only assertions a for which the subject sa appears in q and at least one object o ∈ Oa appears in p, or vice versa. Because still too many such assertions might be retrieved for a task instance, we rank all retrievals based on their respective subject s and object o using an idf -based score rs,o = 1/ ∑ a′ I(sa′ = s) · 1/ ∑ a′ I(o ∈ Oa′). During training and application we only retain the top-k assertions.\n1http://conceptnet.io/ 2http://wiktionary.org/ 3http://compling.hss.ntu.edu.sg/omw/ 4http://dbpedia.org/ 5We exclude instances from ConceptNet 4 created by only one contributor and from Verbosity to reduce\nnoise"
    }, {
      "heading" : "4 Results",
      "text" : ""
    }, {
      "heading" : "4.1 To Read or not to Read",
      "text" : "Table 1 shows results on our 3 benchmark datasets for different context-reading configurations. On all benchmarks we see that our task-specific models perform rather poorly without contextual word representations. This is especially pronounced for the QA datasets. Reading only p (support for QA/premise in RTE) improves results on all tasks, however, only with marginal improvements on NewsQA. Reading only q (the question or hypothesis, respectively) is similarly helpful to reading p for SNLI, but there is a large improvement for the QA task. However, including both q and p into the context for refinement improves performance even further on all tasks whereas the order of incremental reading doesn’t seem to be important.\nDiscussion The most important finding of this experiment is that reading the context prior to processing the task helps a lot and that reading all context is better than only reading parts of it. The most notable improvement when reading q in the QA task can be explained by the fact that knowing about words appearing in the question while processing the document improves performance dramatically. This kind of feature can easily be captured in our refinement process when including the question [26, 3]."
    }, {
      "heading" : "4.2 Integrating external knowledge",
      "text" : "In addition to providing only input provided by the task itself we additionally provided access to external knowledge from ConceptNet as explained in §2.4. We trained the same model as in the other experiment only that the system is allowed to read the top-10 additional assertions during training and the top-0/10/100 assertions, respectively, during testing. The first thing to notice is that reading CN helps consistently across the datasets with notable improvements on SNLI and slight improvements for QA. Allowing the model access to no additional assertions during reading (CN(0)) decreases\nperformance significantly whereas allowing 100 assertions, i.e., more than 10 which the model was trained on, does not improve performance further. When changing our simple retrieval heuristic to an oracle that either provides no assertions or the best possible to the model we can improve results on SNLI even further to 87.9% accuracy for the 150-dimensional model, a remarkable 2.2 point improvement over our the model running on our heuristic, which is very close to state-of-the-art. This shows that there is room for improvement without changing the model but only the assertion retrieval process.\nIn a qualitative analysis we manually changed assertions for wrongly predicted examples and found that slight changes can already have a large impact on the prediction. Two RTE examples are presented in Table 2. While both are predicted wrongly by the model without assertions it correctly changes predictions when providing it with crucial information. It is important to note that the model is sensitive to the semantics of the assertions which is demonstrated by the changing scores and predictions when changing the assertions slightly. For instance, simply changing synonym to antonym in the provided assertion has a very large impact on prediction such that the model can be fooled.\nDiscussion It is striking to see that the introduction of ConceptNet assertions indeed helps. The fact that the models perform significantly worse when not providing the assertions it was trained on ([p → q → CN(0)]) shows that improvements are not due to potential regularization effects during training but that the model learns to exploit external knowledge. This can be observed even more clearly in the examples of Table 2 which offer another very interesting insight: the model is sensitive to the semantics of the provided assertions. Another interesting finding is the performance gap between [p→ q → CN(0)] and [p→ q] which reveals that [p→ q] can make up for some of the information provided by CN by learning it implicitly from the data, which is not necessary when training the model with CN assertions. The larger relative improvements on SNLI can be explained by the fact that the instances of the QA task should be self-contained by design whereas additional conceptual knowledge is required for RTE."
    }, {
      "heading" : "4.3 Comparing to State-of-the-Art",
      "text" : "The comparison of our approach to state-of-the-art results in Table 3 reveals that our simple models show very strong performances without any hyper-parameter tuning when operating on contextual word representations, especially when comparing only against comparable task-specific architectures. For these models, only the Document Reader [3] performs better on SQuAD but in contrast to our single layer BiLSTM it uses a 3-layer BiLSTM and relies on hand-crafted features. However, this is notable considering that state-of-the-art results are typically obtained by extensive fine-tuning on the task-specific architectures and hyper-parameters."
    }, {
      "heading" : "4.4 Analysis of Contextual Word Representations",
      "text" : "In the following evaluations we examine the characteristics of our contextual word representations in general and wrt. the non-contextual representations. All the results presented in this section are based\n.\non our 150-dimensional [p→ q → CN(10)] model and a subset of 2067 instances of the SQuAD development set with mutually exclusive supporting paragraphs6.\nWhat words are adapted by the context and how much? To answer this question we measure different statistics in Table 4. 1) The mean distance between non-contextual and contextual representations shows that very specific nouns, infrequent nouns get updated the most, whereas more frequent words get adapted the least. 2) The mean difference of norms demonstrates that words carrying little meaning exhibit smaller norms after reading compared, i.e., less pronounced representations when considering that representations are non-negative by design. The norms tend to become slightly smaller on average (−0.5). 3) The variance within the different contextual representations for a word seems to be highest for locations and named entities in general. From these observations we conclude that rather infrequent nouns and especially named entities are subject to changing representations.\nDo contextual representations encode task-specific information? We try to answer this by example and ask whether we can predict words that are part of the question or not in the QA task using only their respective contextual representations. We construct a set of contextual representations for words in our QA analysis dataset and label them with 0 if they appear in the question of the respective context and with 1 if not while excluding stop words and punctuations. We train a logistic-regression classifier on 5000 labelled examples where labels are balanced and evaluate on 2000 (balanced) test examples. Our classifier is able to classify about 90% of test examples correctly. Compared to about 60% of correct classifications for non-contextual representations this demonstrates that there is a clear signal within the contextual word representations for whether a word has appeared in the question. This is a crucial finding because it shows that contextual representations can be enriched with task-specific information.\n6There are more than one questions per paragraph but we only use one.\nA look at the embedding space. In order to get further insights into the characteristics of our embedding space we looked at PCA plots of contextual (red) vs. non-contextual (blue) representations presented in Figures 2a and 2b. For these plots, we restrict contexts to include the high-variance target words “france\" or “abc\", respectively. The plot show that there is a clear axis on which the non-contextual word representations mostly spread (see non-contextual variance axis in Figure 2a) whereas their respective contextual counterparts cover the most of the space. Looking at the contextual updates illustrated by the green arrows in Figure 2a we see that the contextual representations of a single word are spread almost orthogonal to the non-contextual axis, which indicates that contextual updates occur mostly orthogonal to the manifold of the non-contextual representations. Figure 2c shows a TSNE-plot [13] of contextual updates, i.e., the differences between the contextual and the respective non-contextual representations. We find that the contextual updates form different clusters. Interestingly, the updates to the respective target words (orange) are part of different clusters which demonstrates that contextual updates for a single word apart from being orthogonal to the non-contextual representation manifold also vary across contexts."
    }, {
      "heading" : "5 Related Work",
      "text" : "There has been a plethora of work and studies on general-purpose word embeddings [15, 18, 17, 11] in the recent past that are evaluated against rather artificial word relatedness or similarity benchmarks. Only little progress has been made in creating and studying contextual word representations wrt. “real world” tasks and the integration of context-sensitive external knowledge.\nContextual Word Representations Context2Vec [14] learns unsupervised context representations along word embeddings. The authors show that context representations are especially helpful for word-sense disambiguation. Hu et al. [8] compute contextual representations for individual context tokens by summing a non-contextual representation with a computed representation of the local context. Their model ultimately aims at improving general-purpose word representations by utilizing local contexts of appearances whereas our goal is to adapt word representations wrt. a given instance of an NLP task. These two lines of work can therefore be considered orthogonal.\nIntegrating External Knowledge In the area of visual question answering Wu et al. [27] utilize external knowledge in form of DBpedia comments (short abstracts/definitions) to improve the answering ability of a model. In a prior step they encode comments for DBpedia entities via Doc2Vec[10]. Xu et al. [28] incorporate a recall mechanism into a standard LSTM cell and similarly encode external pieces of knowledge by a single representation. Dhingra et al. [6] exploit linguistic knowledge using MAGE-GRUs, an adapation of GRUs to handle graphs, however, external knowledge has to be present in form of triples. The main differences to our approach are that we incorporate external knowledge on the word level prior to processing the task at hand.\nWe like to note that our instantiation of incorporating contextual information explicitly on word representations is only one way of dealing with contextual information. For instance, neural semantic encoders (NSE) [16] utilize neural memory with soft addressing to incrementally read and write information from. We could leverage such ideas as well in our architecture and exchange matching using lemmatization in Eq. 3 by a softer matching function in future work."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this work we propose a refinement strategy for incorporating context information into contextagnostic word representations. Our approach is applicable prior to any kind of neural NLU model that operates on word representations, and lends itself to the introduction of external knowledge in the form of free text. Detailed analyses reveal that our refinement models are sensitive to the provided context and construct contextual word representations that vary across contexts and include task-specific information. We show that utilizing contextual word representations instead of their non-contextual counterparts greatly boosts performance of otherwise low-performing baselines on three competitive benchmarks, closing the gap between general-purpose reading architectures and highly tuned, state-of-the-art models. We believe that our refinement strategy can offer an important general-purpose building block for the construction of NLU architectures."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank Chris Dyer and Nils Rethmeier for comments on an early draft of this work. This research was supported by the German Federal Ministry of Education and Research (BMBF) through the projects ALL SIDES (01IW14002), BBDC (01IS14013E), and Software Campus (01IS12050, subproject GeNIE)."
    } ],
    "references" : [ {
      "title" : "A large annotated corpus for learning natural language inference",
      "author" : [ "Samuel R Bowman", "Gabor Angeli", "Potts Christopher", "Christopher D Manning" ],
      "venue" : "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2015
    }, {
      "title" : "Reading Wikipedia to Answer Open-Domain Questions",
      "author" : [ "Danqi Chen", "Adam Fisch", "Jason Weston", "Antoine Bordes" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2017
    }, {
      "title" : "Enhancing and Combining Sequential and Tree LSTM for Natural Language Inference",
      "author" : [ "Qian Chen", "Xiaodan Zhu", "Zhenhua Ling", "Si Wei", "Hui Jiang" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2016
    }, {
      "title" : "A unified model for word sense representation and disambiguation",
      "author" : [ "Xinxiong Chen", "Zhiyuan Liu", "Maosong Sun" ],
      "venue" : "In EMNLP,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2014
    }, {
      "title" : "Linguistic Knowledge as Memory for Recurrent Neural Networks",
      "author" : [ "Bhuwan Dhingra", "Zhilin Yang", "William W Cohen", "Ruslan Salakhutdinov" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2017
    }, {
      "title" : "Dropout as a Bayesian Approximation : Representing Model Uncertainty in Deep Learning",
      "author" : [ "Yarin Gal", "Zoubin Ghahramani" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2015
    }, {
      "title" : "Different contexts lead to different word embeddings",
      "author" : [ "Wenpeng Hu", "Jiajun Zhang", "Nan Zheng" ],
      "venue" : "In COLING,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2016
    }, {
      "title" : "Adam: A Method for Stochastic Optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba" ],
      "venue" : "ICLR,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2015
    }, {
      "title" : "Distributed representations of sentences and documents",
      "author" : [ "Quoc V. Le", "Tomas Mikolov" ],
      "venue" : "In ICML,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2014
    }, {
      "title" : "Improving distributional similarity with lessons learned from word embeddings",
      "author" : [ "Omer Levy", "Yoav Goldberg", "Ido Dagan" ],
      "venue" : "TACL, 3:211–225,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2015
    }, {
      "title" : "Learning Natural Language Inference using Bidirectional LSTM model and Inner-Attention",
      "author" : [ "Yang Liu", "Chengjie Sun", "Lei Lin", "Xiaolong Wang" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2016
    }, {
      "title" : "Visualizing data using t-sne",
      "author" : [ "Laurens van der Maaten", "Geoffrey Hinton" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2008
    }, {
      "title" : "context2vec: Learning generic context embedding with bidirectional lstm",
      "author" : [ "Oren Melamud", "Jacob Goldberger", "Ido Dagan" ],
      "venue" : "In CoNLL,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2016
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean" ],
      "venue" : "CoRR, abs/1310.4546,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2013
    }, {
      "title" : "Neural Semantic Encoders",
      "author" : [ "Tsendsuren Munkhdalai", "Hong Yu" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2017
    }, {
      "title" : "Efficient non-parametric estimation of multiple embeddings per word in vector space",
      "author" : [ "Arvind Neelakantan", "Jeevan Shankar", "Alexandre Passos", "Andrew McCallum" ],
      "venue" : "In EMNLP,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2014
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D. Manning" ],
      "venue" : "In EMNLP,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2014
    }, {
      "title" : "SQuAD: 100,000+ Questions for Machine Comprehension of Text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang" ],
      "venue" : null,
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2016
    }, {
      "title" : "Reasoning about entailment with neural attention",
      "author" : [ "Tim Rocktäschel", "Edward Grefenstette", "Karl Moritz Hermann", "Tomás Kociský", "Phil Blunsom" ],
      "venue" : "ICLR, abs/1509.06664,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2015
    }, {
      "title" : "Bi-Directional Attention Flow for Machine Comprehension",
      "author" : [ "Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hananneh Hajishirzi" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2017
    }, {
      "title" : "Reading and Thinking : Re-read LSTM Unit for Textual Entailment Recognition",
      "author" : [ "Lei Sha", "Baobao Chang", "Zhifang Sui", "Sujian Li" ],
      "venue" : null,
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2016
    }, {
      "title" : "Representing General Relational Knowledge in ConceptNet 5",
      "author" : [ "Robert Speer", "Catherine Havasi" ],
      "venue" : "In LREC,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2012
    }, {
      "title" : "Machine Comprehension Using Match-LSTM and Answer Pointer",
      "author" : [ "Shuohang Wang", "Jing Jiang" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2017
    }, {
      "title" : "Making Neural QA as Simple as Possible but not Simpler",
      "author" : [ "Dirk Weissenborn", "Georg Wiese", "Laura Seiffe" ],
      "venue" : null,
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2017
    }, {
      "title" : "Ask Me Anything: Free-form Visual Question Answering Based on Knowledge from External Sources",
      "author" : [ "Qi Wu", "Peng Wang", "Chunhua Shen", "Anton van den Hengel", "Anthony Dick" ],
      "venue" : null,
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2016
    }, {
      "title" : "Incorporating Loose-Structured Knowledge into LSTM with Recall Gate for Conversation Modeling",
      "author" : [ "Zhen Xu", "Bingquan Liu", "Baoxun Wang", "Chengjie Sun", "Xiaolong Wang" ],
      "venue" : null,
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 17,
      "context" : "For example, in document question answering (QA) [19], the “context” consists of both the question being asked and the document where the answer is expected to be found, and both are necessary to capture the function and meaning of the words they are composed of.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 13,
      "context" : "Neural NLU systems typically represent a word by a single, context-agnostic n-dimensional word embedding [15, 18].",
      "startOffset" : 105,
      "endOffset" : 113
    }, {
      "referenceID" : 16,
      "context" : "Neural NLU systems typically represent a word by a single, context-agnostic n-dimensional word embedding [15, 18].",
      "startOffset" : 105,
      "endOffset" : 113
    }, {
      "referenceID" : 15,
      "context" : "Following studies [17, 5], however, quickly realized that a single representation can hardly account for lexical ambiguity and proposed to model words by multiple representations instead.",
      "startOffset" : 18,
      "endOffset" : 25
    }, {
      "referenceID" : 3,
      "context" : "Following studies [17, 5], however, quickly realized that a single representation can hardly account for lexical ambiguity and proposed to model words by multiple representations instead.",
      "startOffset" : 18,
      "endOffset" : 25
    }, {
      "referenceID" : 6,
      "context" : "[8] show that general-purpose contextual representations can also be computed from the local context instead of keeping multiple representations for single words.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 12,
      "context" : "[14] showed that bidirectional recurrent neural networks (BiRNNs) can compute contextual representations which help in disambiguating tokens in their local context.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "For instance, recent work on QA [26, 3] shows that providing simple, task-specific word level information, in particular a feature determining whether a word occurs in the question or not, in addition to the words themselves can have a dramatic impact on the performance",
      "startOffset" : 32,
      "endOffset" : 39
    }, {
      "referenceID" : 1,
      "context" : "For instance, recent work on QA [26, 3] shows that providing simple, task-specific word level information, in particular a feature determining whether a word occurs in the question or not, in addition to the words themselves can have a dramatic impact on the performance",
      "startOffset" : 32,
      "endOffset" : 39
    }, {
      "referenceID" : 19,
      "context" : "Our approach follows [21, 26].",
      "startOffset" : 21,
      "endOffset" : 29
    }, {
      "referenceID" : 23,
      "context" : "Our approach follows [21, 26].",
      "startOffset" : 21,
      "endOffset" : 29
    }, {
      "referenceID" : 21,
      "context" : "ConceptNet ConceptNet1 [23] (CN) is freely-available, multi-lingual semantic network that originated from the Open Mind Common Sense project and incorporates selected knowledge from various other knowledge sources, such as Wiktionary2, Open Multilingual WordNet3, OpenCyc or DBpedia4.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 17,
      "context" : "Question Answering We apply our QA models on 2 recent QA benchmark datasets, namely SQuAD[19] and NewsQA[24].",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 0,
      "context" : "Recognizing Textual Entailment We test on the frequently used SNLI dataset [2], a collection of 570k sentence pairs.",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 23,
      "context" : "This kind of feature can easily be captured in our refinement process when including the question [26, 3].",
      "startOffset" : 98,
      "endOffset" : 105
    }, {
      "referenceID" : 1,
      "context" : "This kind of feature can easily be captured in our refinement process when including the question [26, 3].",
      "startOffset" : 98,
      "endOffset" : 105
    }, {
      "referenceID" : 23,
      "context" : "FastQA [26] 77.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 1,
      "context" : "[3] 79.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 23,
      "context" : "1 FastQA [26] 55.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 22,
      "context" : "M-LSTM [24, 25] 48.",
      "startOffset" : 7,
      "endOffset" : 15
    }, {
      "referenceID" : 23,
      "context" : "4 FastQAExt [26] 56.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 10,
      "context" : "[12] 84.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "2 NSE [16] 84.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 20,
      "context" : "Re-read LSTM [22] 87.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 2,
      "context" : "5 TreeLSTM [4] 88.",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 1,
      "context" : "For these models, only the Document Reader [3] performs better on SQuAD but in contrast to our single layer BiLSTM it uses a 3-layer BiLSTM and relies on hand-crafted features.",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 11,
      "context" : "Figure 2c shows a TSNE-plot [13] of contextual updates, i.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 13,
      "context" : "There has been a plethora of work and studies on general-purpose word embeddings [15, 18, 17, 11] in the recent past that are evaluated against rather artificial word relatedness or similarity benchmarks.",
      "startOffset" : 81,
      "endOffset" : 97
    }, {
      "referenceID" : 16,
      "context" : "There has been a plethora of work and studies on general-purpose word embeddings [15, 18, 17, 11] in the recent past that are evaluated against rather artificial word relatedness or similarity benchmarks.",
      "startOffset" : 81,
      "endOffset" : 97
    }, {
      "referenceID" : 15,
      "context" : "There has been a plethora of work and studies on general-purpose word embeddings [15, 18, 17, 11] in the recent past that are evaluated against rather artificial word relatedness or similarity benchmarks.",
      "startOffset" : 81,
      "endOffset" : 97
    }, {
      "referenceID" : 9,
      "context" : "There has been a plethora of work and studies on general-purpose word embeddings [15, 18, 17, 11] in the recent past that are evaluated against rather artificial word relatedness or similarity benchmarks.",
      "startOffset" : 81,
      "endOffset" : 97
    }, {
      "referenceID" : 12,
      "context" : "Contextual Word Representations Context2Vec [14] learns unsupervised context representations along word embeddings.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 6,
      "context" : "[8] compute contextual representations for individual context tokens by summing a non-contextual representation with a computed representation of the local context.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 24,
      "context" : "[27] utilize external knowledge in form of DBpedia comments (short abstracts/definitions) to improve the answering ability of a model.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 8,
      "context" : "In a prior step they encode comments for DBpedia entities via Doc2Vec[10].",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 25,
      "context" : "[28] incorporate a recall mechanism into a standard LSTM cell and similarly encode external pieces of knowledge by a single representation.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 4,
      "context" : "[6] exploit linguistic knowledge using MAGE-GRUs, an adapation of GRUs to handle graphs, however, external knowledge has to be present in form of triples.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 14,
      "context" : "For instance, neural semantic encoders (NSE) [16] utilize neural memory with soft addressing to incrementally read and write information from.",
      "startOffset" : 45,
      "endOffset" : 49
    } ],
    "year" : 2017,
    "abstractText" : "Despite the recent success of neural networks in tasks involving natural language understanding (NLU) there has only been limited progress in some of the fundamental challenges of NLU, such as the disambiguation of the meaning and function of words in context. This work approaches this problem by incorporating contextual information into word representations prior to processing the task at hand. To this end we propose a general-purpose reading architecture that is employed prior to a task-specific NLU model. It is responsible for refining context-agnostic word representations with contextual information and lends itself to the introduction of additional, context-relevant information from external knowledge sources. We demonstrate that previously non-competitive models benefit dramatically from employing contextual representations, closing the gap between general-purpose reading architectures and the state-of-the-art performance obtained with fine-tuned, task-specific architectures. Apart from our empirical results we present a comprehensive analysis of the computed representations which gives insights into the kind of information added during the refinement process.",
    "creator" : "LaTeX with hyperref package"
  }
}