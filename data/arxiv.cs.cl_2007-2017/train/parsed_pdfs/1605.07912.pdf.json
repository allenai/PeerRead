{
  "name" : "1605.07912.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Encode, Review, and Decode: Reviewer Module for Caption Generation",
    "authors" : [ "Zhilin Yang", "Ye Yuan", "Yuexin Wu", "Ruslan Salakhutdinov", "William W. Cohen" ],
    "emails" : [ "zhiliny@cs.cmu.edu", "yey1@cs.cmu.edu", "yuexinw@cs.cmu.edu", "rsalakhu@cs.cmu.edu", "wcohen@cs.cmu.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Encoder-decoder is a framework for learning a transformation from one representation to another. In this framework, an encoder network first encodes the input into a context vector, and then a decoder network decodes the context vector to generate the output. The encoder-decoder framework was recently introduced for sequence-to-sequence learning based on recurrent neural networks (RNNs) with applications to machine translation [3, 15], where the input is a text sequence in one language and the output is a text sequence in the other language. More generally, the encoder-decoder framework is not restricted to RNNs and text; e.g., encoders based on convolutional neural networks (CNNs) are used for image captioning [17]. Since it is often difficult to encode all the necessary information in a single context vector (often regarded as the bottleneck), attentive encoder-decoder introduces attention mechanism to the encoder-decoder framework. Attention mechanism breaks down the encoder-decoder bottleneck by conditioning the generative process in the decoder on the encoder hidden states, rather than on one single context vector only. Improvements by the attention mechanism are shown on various tasks, including machine translation [1], image captioning [19], and text summarization [12].\nHowever, there remain two important issues to address for the attentive encoder-decoder models. First, the attention mechanism proceeds in a sequential manner and thus lacks global modeling abilities. More specifically, at the generation step t, the decoded token is conditioned on the attention results at the current time step h̃t, but has no information about future attention results h̃t′ with t′ > t. For example, when there are multiple objects in the image, the caption tokens generated at the beginning focuses on the first one or two objects and is unaware of the other objects, which is potentially suboptimal due to the lack of global modeling abilities. Second, previous works show that discriminative supervision (e.g., predicting word occurrences in the caption) is beneficial for generative models [5], but it is not clear how to integrate discriminative supervision into the encoder-decoder framework in an end-to-end manner.\nTo address the above questions, we propose a novel module, the reviewer module, which can be integrated into existing (attentive) encoder-decoder models. The reviewer module is plugged into\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n60 5.\n07 91\n2v 1\n[ cs\n.L G\n] 2\n5 M\nay 2\nthe middle of the encoder and the decoder. The reviewer module performs a given number of review steps with attention mechanism on the encoder hidden states and outputs a fact vector after each step, where the fact vectors are expected to capture the global facts in compact vector representation and are usable by the attention mechanism in the decoder. The intuition behind the reviewer module is to review all the information encoded by the encoder and learn fact vectors that are more compact, abstractive, and global representation than the original encoder hidden states. Our novel model with the reviewer module is referred to as encoder-reviewer-decoder models in the following sections.\nThe basic reviewer module does not have step-wise supervision as in the decoder RNNs. However, we can leverage discriminative supervision signals by imposing loss on the fact vectors to guide the review process. More specifically, we apply a linear transformation upon the fact vectors, and then employ a multi-label margin loss following a max pooling layer, to impose discriminative supervision, such as predicting word occurrences in the caption. We also study the effects of weight tying between the reviewer units.\nWe show that our model can be reduced to a conventional attentive encoder-decoder in a special case, which indicates that our model is strictly more expressive than the attentive encoder-decoders. We experiment with two different tasks, image captioning and source code captioning, using CNNs and RNNs as the encoders respectively. Our results show that the reviewer module can consistently improve the performance over attentive encoder-decoders on both datasets and obtain state-of-the-art performance."
    }, {
      "heading" : "2 Related Work",
      "text" : "The encoder-decoder framework in the context of sequence-to-sequence learning was recently introduced for learning transformation between text sequences [3, 15], where RNNs are used for both encoding and decoding. Encoder-decoders, in general, can refer to models that learn representation transformation using two network components, an encoder and a decoder. Besides RNNs, convolutional encoders are developed to address multi-modal tasks such as image captioning [17]. Attention mechanism was later introduced to the encoder-decoder framework for machine translation, with an explanation of explicit token-level alignment between input and output sequences [1]. Different from vanilla encoder-decoders, attentive encoder-decoders condition the decoder on the encoder hidden states. At each generation step, the decoder pays attention to a specific part (token) in the encoder, and generates the next token based on both the current hidden state in the decoder and the attended hidden states in the encoder. Attention mechanism demonstrates considerable success in other applcations as well, including image captioning [19] and text summarization [12].\nOur work is also related to memory networks [18, 14]. Memory networks take a question embedding as input, and perform multiple computational steps with attention on the memory, which is usually formed by the embeddings of a group of sentences. Dynamic memory networks extend memory networks to model sequential memories [8]. Variants of memory networks are mainly developed in the context of question answering; the reviewer module, on the other hand, is a generic module that can be integrated into existing encoder-decoder models. Moreover, the reviewer module learns fact vectors using multiple review steps, while (embedded) facts are provided as input to the memory networks. The reviewer module outputs a sequence of fact vectors, while memory networks only use the last hidden state to generate the answer."
    }, {
      "heading" : "3 Model",
      "text" : "Given the input representation x and the output representation y, the goal is to learn a function mapping from x to y. For example, image captioning aims to learn a mapping from an image x to a caption y. For notation simplicity, we use x and y to denote both a tensor and a sequence of tensors. For example, x can be a 3d-tensor that represents an image with RGB channels in image captioning, or can be a sequence of 1d-tensors (i.e., vectors) x = (x1, · · · ,xTx) in machine translation, where xt denotes the one-of-K embedding of the t-th word in the input sequence of length Tx.\nIn contrast to conventional (attentive) encoder-decoder models, our model consists of three components, encoder, reviewer, and decoder. The comparison of architectures is shown in Figure 1. Now we describe the three components in detail."
    }, {
      "heading" : "3.1 Encoder",
      "text" : "The encoder encodes the input x into a context vector c and a set of hidden states H = {ht}t. We discuss two types of encoders, RNN encoders and CNN encoders.\nRNN Encoder: Let Tx = |H| be the length of the input sequence. An RNN encoder processes the input sequence x = (x1, · · · ,xTx) sequentially. At time step t, the RNN encoder updates the hidden state by\nht = f(xt,ht−1).\nIn this work, we implement f using an LSTM unit. The context vector is defined as the final hidden state c = hTx . The cell state and hidden state h0 of the first LSTM unit are initialized as zero.\nCNN Encoder: We take a widely-used CNN architecture—VGGNet [13]—as an example to describe how we use CNNs as encoders. Given a VGGNet, we use the output of the last fully connected layer fc7 as the context vector c = fc7(x), and use 14× 14 = 196 columns of 512d convolutional output conv5 as hidden states H = conv5(x). In this case Tx = |H| = 196."
    }, {
      "heading" : "3.2 Reviewer",
      "text" : "Let Tr be a hyperparameter that specifies the number of review steps. The intuition behind the reviewer module is to review all the information encoded by the encoder and learn fact vectors that are more compact, abstractive, and global representation than the original encoder hidden states. The reviewer performs Tr review steps on the encoder hidden states H and outputs a fact vector ft after each step. More specifically,\nft = gt(H, ft−1),\nwhere gt is a modified LSTM unit with attention mechanism at review step t. We study two variants of gt, attentive input reviewer and attentive output reviewer."
    }, {
      "heading" : "3.2.1 Attentive Input Reviewer",
      "text" : "At each review step t, the attentive input reviewer first applies an attention mechanism on H and use the attention result as the input to an LSTM unit (Cf. Figure 2a). Let f̃t = att(H, ft−1) be the\nattention result at step t. The attentive input reviewer is formulated as\nf̃t = att(H, ft−1) = |H|∑ i=1 α(hi, ft−1)∑|H| i′=1 α(hi′ , ft−1) hi, gt(H, ft−1) = f ′ t(f̃t, ft−1), (1)\nwhere α(hi, ft−1) is a function that determines the weight for the i-th hidden state. α(x1,x2) can be implemented as a dot product between x1 and x2 or a multi-layer perceptron (MLP) that takes the concatenation of x1 and x2 as input [9]. f ′t is an LSTM unit at step t."
    }, {
      "heading" : "3.2.2 Attentive Output Reviewer",
      "text" : "In contrast to the attentive input reviewer, the attentive output reviewer uses a zero vector as input to the LSTM unit, and the fact vector is computed as the weighted sum of the attention results and the output of the LSTM unit (Cf. Figure 2b). More specifically, the attentive output reviewer is formulated as f̃t = att(H, ft−1), gt(H, ft−1) = f ′t(0, ft−1) +Wf̃t, where the attention mechanism att follows the definition in Eq. (1), 0 denotes a zero vector, W is a model parameter matrix, and f ′t is an LSTM unit at step t. We note that performing attention on top of an RNN unit is commonly used in sequence-to-sequence learning [1, 9, 12]. We apply a linear transformation with a matrix W since the dimensions of f ′t(·, ·) and f̃t can be different."
    }, {
      "heading" : "3.2.3 Weight Tying",
      "text" : "We study two variants of weight tying for the reviewer units. Let wt denote the parameters for the unit f ′t . The first variant follows the common setting in RNNs, where weights are shared among all the units; i.e., w1 = · · · = wTr . We also observe that the reviewer unit does not have sequential input, so we experiment with the second variant where weights are untied; i.e. wi 6= wj ,∀i 6= j. The cell state and hidden state of the first unit f ′1 are initialized as the context vector c. The cell states and hidden states are passed through all the reviewer units in both cases of weight tying."
    }, {
      "heading" : "3.3 Decoder",
      "text" : "Let F = {ft}t be the set of fact vectors output by the reviewer. The decoder is formulated as an LSTM network with attention on the fact vectors F (Cf. Figure 2c). Let st be the hidden state of the t-th LSTM unit in the decoder. The decoder is formulated as follows:\ns̃t = att(F, st−1), st = f ′′([s̃t;yt−1], st−1), yt = argmax y softmaxy(st), (2)\nwhere [·; ·] denotes the concatenation of two vectors, f ′′ denotes the decoder LSTM, softmaxy is the probability of word y given by a softmax layer, yt is the t-th decoded token, and yt is the word embedding of yt. The attention mechanism att follows the definition in Eq. (1). The initial cell state and hidden state s0 of the decoder LSTM are both set to the review vector r = W′[fTr ; c], where W′ is a model parameter matrix."
    }, {
      "heading" : "3.4 Discriminative Supervision",
      "text" : "In conventional encoder-decoders, supervision is provided in a generative manner; i.e., the model aims to maximize the conditional probability of generating the sequential output p(y|x). However, discriminative supervision has been shown to be useful in [5], where the model is guided to predict discriminative objectives, such as the word occurrences in the output y.\nWe argue that the reviewer module provides a natural way of incorporating discriminative supervision into the model. Here we take word occurrence prediction for example to describe how to incorporate discriminative supervision. As shown in the blue components in Figure 1b, we first apply a linear layer on top of the fact vector to compute a score for each word at each review step. We then apply a max-pooling layer over all the review units to extract the most salient signal for each word, and add a multi-label margin loss as discriminative supervision. Let si be the score of word i after the max pooling layer, and W be the set of all words that occur in y. The discriminative loss can be written as\nLd = 1\nZ ∑ j∈W ∑ i6=j max(0, 1− (sj − si)), (3)\nwhere Z is a normalizer that counts all the valid i, j pairs. We note that when the discriminative supervision is derived from the given data (i.e., predicting word occurrences in captions), we are not using extra information."
    }, {
      "heading" : "3.5 Training",
      "text" : "The training loss for a single training instance (x,y) is defined as a weighted sum of the negative conditional log likelihood and the discriminative loss. Let Ty be the length of the output sequence y. The loss can be written as\nL(x,y) = 1 Ty Ty∑ t=1 − log softmaxyt(st) + λLd,\nwhere the definition of softmaxy and st follows Eq. (2), and the formulation of Ld follows Eq. (3). λ is a constant weighting factor. Since Ld is optional, we set Ld = 0 when unused. We adopt adaptive stochastic gradient descent (AdaGrad) [4] to train the model in an end-to-end manner. The loss of a training batch is averaged over all instances in the batch."
    }, {
      "heading" : "3.6 Connection to Encoder-Decoders",
      "text" : "We now show that our model can be reduced to the conventional (attentive) encoder-decoders in a special case. In attentive encoder-decoders, the decoder takes the context vector c and the set of encoder hidden states H = {ht}t as input, while in our encoder-reviewer-decoder model, the input of the decoder is instead the review vector r and the set of fact vectors F = {ft}t. To show that our model can be reduced to attentive encoder-decoders, we only need to construct a case where H = F and c = r.\nSince r = W′[fTr ; c], it can be reduced to r = c with a specific setting of W ′. We further set Tr = Tx, and define each reviewer unit as an identity mapping gt(H, ft−1) = ht, which can fit into the definition of both the attentive input reviewer and the attentive output reviewer with untied weights. With the above setting, we have ht = ft,∀t = 1, · · · , Tx; i.e., H = F . Thus our model can be reduced to attentive encoder-decoders in a special case. Similarly we can show that our model can be reduced to vanilla encoder-decoders (without attention) by constructing a case where r = c and ft = 0. Therefore, our model is more expressive than (attentive) encoder-decoders.\nThough we set Tr = Tx in the above construction, in practice, we set the number of review steps Tr to be much smaller compared to Tx, since we find that the reviewer module can learn more compact and effective representation."
    }, {
      "heading" : "4 Experiments",
      "text" : "We experiment with two datasets of different tasks, image captioning and source code captioning. Since these two tasks are essentially different, we can use them to test the robustness and generalizability of our model."
    }, {
      "heading" : "4.1 Image Captioning",
      "text" : ""
    }, {
      "heading" : "4.1.1 Data and Settings",
      "text" : "We evaluate our model on the MSCOCO benchmark dataset [2] for image captioning. The dataset contains 123,000 images with at least 5 captions for each image. We use the same data split as in [7, 19, 20], where we reserve 5,000 images for development and test respectively and use the rest for training. The models are evaluated using the official MSCOCO evaluation scripts. We report three widely used automatic evaluation metrics, BLEU-4, METEOR, and CIDEr.\nWe remove all the non-alphabetic characters in the captions, transform all letters to lowercase, and tokenize the captions using white space. We replace all words occurring less than 5 times with an unknown token <UNK> and obtain a vocabulary of 9,520 words. We truncate all the captions longer than 30 tokens.\nWe set the number of review steps Tr = 8, the weighting factor λ = 10.0, the dimension of word embeddings to be 100, the learning rate to be 1e−2, and the dimension of LSTM hidden states to be 1, 024. These hyperparameters are tuned on the development set. We also use early stopping strategies to prevent overfitting. More specifically, we stop the training procedure when the BLEU-4 score on the development set reaches the maximum. We use an MLP with one hidden layer of size 512 to define the function α(·, ·) in the attention mechanism, and use an attentive input reviewer in our experiments to be consistent with visual attention models [19]. We use beam search with beam size 3 for decoding. We guide the model to predict the words occurring in the caption through the discriminative supervision Ld without introducing extra information. We fix the parameters of the CNN encoders during training."
    }, {
      "heading" : "4.1.2 Results",
      "text" : "We compare our model with encoder-decoders to study the effectiveness of the reviewer module. We also compare different variants of our model to evaluate the effects of different weight tying strategies and discriminative supervision. Results are reported in Table 1. All the results in Table 1 are obtained using VGGNet [13] as encoders as described in Section 3.1.\nFrom Table 1, we can see that the reviewer module can improve the performance over conventional attentive encoder-decoders consistently on all the three metrics We also observe that adding discriminative supervision can boost the model performance, which indicates that the reviewer module provides an effective way of incorporating discriminative supervision in an end-to-end manner. Untying the weights between the reviewer units can further improve the performance. Our conjecture is that the models with untied weights are more expressive than shared-weight models since each unit can have its own parametric function to compute the fact vector.\nWe also compare our model with state-of-the-art results on the MSCOCO dataset. Since VGGNet and GoogLeNet [16] are both used as the image encoders in previous work, we additionally report the performance of our model using GoogLeNet as the encoder for fair comparison. The results are reported in Table 2. Different from some of the other approaches that use model ensembles, the results of our encoder-reviewer-decoder models are obtained with a single model. Table 2 shows that our models outperform all of the other approaches except for Semantic Attention [20]. Though Semantic Attention performs slightly better than our models, we note that they use model ensembles, while the results of a single model are not published. Since model ensembles can usually boost the\nperformance dramatically (by a few BLEU-4 points [17]), our results are competitive and possibly better. Moreover, we note that Semantic Attention makes use of task-specific attributes, while our model does not."
    }, {
      "heading" : "4.1.3 Case Study and Visualization",
      "text" : "To better understand the reviewer module, we visualize the attention weights α in the reviewer module in Figure 3. The visualization is based on the encoder-reviewer-decoder model with untied weights and discriminative supervision. We also list the top-5 words with highest scores (computed based on the fact vectors) at each reviewer unit.\nWe find that the top words with highest scores can uncover the reasoning procedure underlying the reviewer module. For example, in the first image (a giraffe in a zoo), the first reviewer focuses on the motion of the giraffe and the tree around, the second reviewer analyzes the relative position between the giraffe and the tree, and the third reviewer looks at the big picture and infers that the scene is in a zoo based on recognizing the fences and enclosures. All the above information is stored in the fact vectors and decoded as natural language by the decoder.\nDifferent from attentive encoder-decoders [19] that attend to a single object at a time during generation, it can be clearly seen from Figure 3 that the reviewer module captures more global signals, usually combining multiple objects into one fact, including objects not finally shown in the caption (e.g., “traffic light” and “motorcycles”). The facts are sometimes abstractive, such as motion (“standing”), relative position (“near”, “by”, “up”), quantity (“bunch”, “group”), and scene (“city”, “zoo”). Also, the order of review is not restricted by the order in natural language."
    }, {
      "heading" : "4.2 Source Code Captioning",
      "text" : ""
    }, {
      "heading" : "4.2.1 Data and Settings",
      "text" : "The task of source code captioning is to predict the code comment given the source code, which can be framed under the problem of sequence-to-sequence learning. We experiment with a benchmark dataset for source code captioning, HabeasCorpus [11]. HabeasCorpus collects nine popular opensource Java code repositories, such as Apache Ant and Lucene. The dataset contains 6, 734 Java source code files with 7, 903, 872 source code tokens and 251, 565 comment word tokens. We randomly sample 10% of the files as the test set, and use the rest for training. We randomly sample a different extra validation split for tuning hyperparameters.\nOur evaluation follows previous works on source code language modeling [10] and captioning [11]. We report the log-likelihood of generating the actual code captions based on the learned models. We also evaluate the approaches from the perspective of code comment completion, where we compute the percentage of characters that can be saved by applying the models to predict the next token. More specifically, we use a metric of top-k character savings [11] (CS-k). Let n be the minimum number of prefix characters needed to be filtered such that the actual word ranks among the top-k based on the given model. Let L be the length of the actual word. The number of saved characters is then L− n. We compute the average percentage of saved characters per comment to obtain the metric CS-k.\nWe follow the tokenization used in [11], where we transform camel case identifiers into multiple separate words (e.g., “binaryClassifierEnsemble” to “binary classifier ensemble”), and remove all non-alphabetic characters. We truncate code sequences and comment sequences longer than 300 tokens. We use an RNN encoder and an attentive output reviewer with tied weights. We set the number of review steps Tr = 8, the dimension of word embeddings to be 50, and the dimension of the LSTM hidden states to be 256."
    }, {
      "heading" : "4.2.2 Results",
      "text" : "We report the log-likelihood and top-k character savings of different model variants in Table 3. The baseline model “Language Model” is an LSTM decoder whose output is not sensitive to the input code sequence. Our preliminary experiment shows that the LSTM decoder significantly outperforms the Ngram models used in [11] (+3% in CS-2), so we use the LSTM decoder as a baseline for comparison. We also compare with different variants of encoder-decoders, including incorporating bidirectional RNN encoders and attention mechanism. It can be seen from Table 3 that both bidirectional encoders and attention mechanism can improve over vanilla encoder-decoders. Encoder-reviewer-decoder model can improve attentive encoder-decoders consistently in all the metrics, which indicates that the reviewer module is effective at learning useful representation."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We present a novel module, the reviewer module, to improve the encoder-decoder learning framework. The reviewer module perform multiple review steps with attention on the encoder hidden states, and computes a set of fact vectors that summarize the global information in the input. We empirically show consistent improvement over conventional encoder-decoders on the tasks of image captioning and source code captioning. In the future, it will be interesting to apply our model to more tasks that can be modeled under the encoder-decoder framework."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2015
    }, {
      "title" : "Microsoft coco captions: Data collection and evaluation",
      "author" : [ "Xinlei Chen", "Hao Fang", "Tsung-Yi Lin", "Ramakrishna Vedantam", "Saurabh Gupta", "Piotr Dollár", "C Lawrence Zitnick" ],
      "venue" : "server. arXiv preprint arXiv:1504.00325,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2015
    }, {
      "title" : "Learning phrase representations using rnn encoderdecoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart Van Merriënboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio" ],
      "venue" : "In ACL,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2014
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "John Duchi", "Elad Hazan", "Yoram Singer" ],
      "venue" : "JMLR, 12:2121–2159,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2011
    }, {
      "title" : "From captions to visual concepts and back",
      "author" : [ "Hao Fang", "Saurabh Gupta", "Forrest Iandola", "Rupesh K Srivastava", "Li Deng", "Piotr Dollár", "Jianfeng Gao", "Xiaodong He", "Margaret Mitchell", "John C Platt" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2015
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1997
    }, {
      "title" : "Deep visual-semantic alignments for generating image descriptions",
      "author" : [ "Andrej Karpathy", "Li Fei-Fei" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2015
    }, {
      "title" : "Ask me anything: Dynamic memory networks for natural language processing",
      "author" : [ "Ankit Kumar", "Ozan Irsoy", "Jonathan Su", "James Bradbury", "Robert English", "Brian Pierce", "Peter Ondruska", "Ishaan Gulrajani", "Richard Socher" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2016
    }, {
      "title" : "Effective approaches to attentionbased neural machine translation",
      "author" : [ "Minh-Thang Luong", "Hieu Pham", "Christopher D Manning" ],
      "venue" : "In ACL,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2015
    }, {
      "title" : "Structured generative models of natural source code",
      "author" : [ "Chris J Maddison", "Daniel Tarlow" ],
      "venue" : "In ICML,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2014
    }, {
      "title" : "Natural language models for predicting programming comments",
      "author" : [ "Dana Movshovitz-Attias", "William W Cohen" ],
      "venue" : "In ACL,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2013
    }, {
      "title" : "A neural attention model for abstractive sentence summarization",
      "author" : [ "Alexander M Rush", "Sumit Chopra", "Jason Weston" ],
      "venue" : "In EMNLP,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2015
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "Karen Simonyan", "Andrew Zisserman" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2015
    }, {
      "title" : "End-to-end memory networks",
      "author" : [ "Sainbayar Sukhbaatar", "Jason Weston", "Rob Fergus" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2015
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2014
    }, {
      "title" : "Going deeper with convolutions",
      "author" : [ "Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2015
    }, {
      "title" : "Show and tell: A neural image caption generator",
      "author" : [ "Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2015
    }, {
      "title" : "Show, attend and tell: Neural image caption generation with visual attention",
      "author" : [ "Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio" ],
      "venue" : "In ICML,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2015
    }, {
      "title" : "Image captioning with semantic attention",
      "author" : [ "Quanzeng You", "Hailin Jin", "Zhaowen Wang", "Chen Fang", "Jiebo Luo" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "The encoder-decoder framework was recently introduced for sequence-to-sequence learning based on recurrent neural networks (RNNs) with applications to machine translation [3, 15], where the input is a text sequence in one language and the output is a text sequence in the other language.",
      "startOffset" : 171,
      "endOffset" : 178
    }, {
      "referenceID" : 14,
      "context" : "The encoder-decoder framework was recently introduced for sequence-to-sequence learning based on recurrent neural networks (RNNs) with applications to machine translation [3, 15], where the input is a text sequence in one language and the output is a text sequence in the other language.",
      "startOffset" : 171,
      "endOffset" : 178
    }, {
      "referenceID" : 16,
      "context" : ", encoders based on convolutional neural networks (CNNs) are used for image captioning [17].",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 0,
      "context" : "Improvements by the attention mechanism are shown on various tasks, including machine translation [1], image captioning [19], and text summarization [12].",
      "startOffset" : 98,
      "endOffset" : 101
    }, {
      "referenceID" : 17,
      "context" : "Improvements by the attention mechanism are shown on various tasks, including machine translation [1], image captioning [19], and text summarization [12].",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 11,
      "context" : "Improvements by the attention mechanism are shown on various tasks, including machine translation [1], image captioning [19], and text summarization [12].",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 4,
      "context" : ", predicting word occurrences in the caption) is beneficial for generative models [5], but it is not clear how to integrate discriminative supervision into the encoder-decoder framework in an end-to-end manner.",
      "startOffset" : 82,
      "endOffset" : 85
    }, {
      "referenceID" : 2,
      "context" : "The encoder-decoder framework in the context of sequence-to-sequence learning was recently introduced for learning transformation between text sequences [3, 15], where RNNs are used for both encoding and decoding.",
      "startOffset" : 153,
      "endOffset" : 160
    }, {
      "referenceID" : 14,
      "context" : "The encoder-decoder framework in the context of sequence-to-sequence learning was recently introduced for learning transformation between text sequences [3, 15], where RNNs are used for both encoding and decoding.",
      "startOffset" : 153,
      "endOffset" : 160
    }, {
      "referenceID" : 16,
      "context" : "Besides RNNs, convolutional encoders are developed to address multi-modal tasks such as image captioning [17].",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 0,
      "context" : "Attention mechanism was later introduced to the encoder-decoder framework for machine translation, with an explanation of explicit token-level alignment between input and output sequences [1].",
      "startOffset" : 188,
      "endOffset" : 191
    }, {
      "referenceID" : 17,
      "context" : "Attention mechanism demonstrates considerable success in other applcations as well, including image captioning [19] and text summarization [12].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 11,
      "context" : "Attention mechanism demonstrates considerable success in other applcations as well, including image captioning [19] and text summarization [12].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 13,
      "context" : "Our work is also related to memory networks [18, 14].",
      "startOffset" : 44,
      "endOffset" : 52
    }, {
      "referenceID" : 7,
      "context" : "Dynamic memory networks extend memory networks to model sequential memories [8].",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 12,
      "context" : "CNN Encoder: We take a widely-used CNN architecture—VGGNet [13]—as an example to describe how we use CNNs as encoders.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 8,
      "context" : "α(x1,x2) can be implemented as a dot product between x1 and x2 or a multi-layer perceptron (MLP) that takes the concatenation of x1 and x2 as input [9].",
      "startOffset" : 148,
      "endOffset" : 151
    }, {
      "referenceID" : 0,
      "context" : "We note that performing attention on top of an RNN unit is commonly used in sequence-to-sequence learning [1, 9, 12].",
      "startOffset" : 106,
      "endOffset" : 116
    }, {
      "referenceID" : 8,
      "context" : "We note that performing attention on top of an RNN unit is commonly used in sequence-to-sequence learning [1, 9, 12].",
      "startOffset" : 106,
      "endOffset" : 116
    }, {
      "referenceID" : 11,
      "context" : "We note that performing attention on top of an RNN unit is commonly used in sequence-to-sequence learning [1, 9, 12].",
      "startOffset" : 106,
      "endOffset" : 116
    }, {
      "referenceID" : 4,
      "context" : "However, discriminative supervision has been shown to be useful in [5], where the model is guided to predict discriminative objectives, such as the word occurrences in the output y.",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 3,
      "context" : "We adopt adaptive stochastic gradient descent (AdaGrad) [4] to train the model in an end-to-end manner.",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 1,
      "context" : "We evaluate our model on the MSCOCO benchmark dataset [2] for image captioning.",
      "startOffset" : 54,
      "endOffset" : 57
    }, {
      "referenceID" : 6,
      "context" : "We use the same data split as in [7, 19, 20], where we reserve 5,000 images for development and test respectively and use the rest for training.",
      "startOffset" : 33,
      "endOffset" : 44
    }, {
      "referenceID" : 17,
      "context" : "We use the same data split as in [7, 19, 20], where we reserve 5,000 images for development and test respectively and use the rest for training.",
      "startOffset" : 33,
      "endOffset" : 44
    }, {
      "referenceID" : 18,
      "context" : "We use the same data split as in [7, 19, 20], where we reserve 5,000 images for development and test respectively and use the rest for training.",
      "startOffset" : 33,
      "endOffset" : 44
    }, {
      "referenceID" : 6,
      "context" : "Model BLEU-4 METEOR CIDEr BRNN [7] 23.",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 17,
      "context" : "0 Soft Attention [19] 24.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 17,
      "context" : "9 — Hard Attention [19] 25.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 4,
      "context" : "0 — MS Research [5]v†∗ 25.",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 16,
      "context" : "6 — Google NIC [17]g‡∗ 27.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 18,
      "context" : "5 Semantic Attention [20]g†‡ 30.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 17,
      "context" : "We use an MLP with one hidden layer of size 512 to define the function α(·, ·) in the attention mechanism, and use an attentive input reviewer in our experiments to be consistent with visual attention models [19].",
      "startOffset" : 208,
      "endOffset" : 212
    }, {
      "referenceID" : 12,
      "context" : "All the results in Table 1 are obtained using VGGNet [13] as encoders as described in Section 3.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 15,
      "context" : "Since VGGNet and GoogLeNet [16] are both used as the image encoders in previous work, we additionally report the performance of our model using GoogLeNet as the encoder for fair comparison.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 18,
      "context" : "Table 2 shows that our models outperform all of the other approaches except for Semantic Attention [20].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 16,
      "context" : "performance dramatically (by a few BLEU-4 points [17]), our results are competitive and possibly better.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 17,
      "context" : "Different from attentive encoder-decoders [19] that attend to a single object at a time during generation, it can be clearly seen from Figure 3 that the reviewer module captures more global signals, usually combining multiple objects into one fact, including objects not finally shown in the caption (e.",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 10,
      "context" : "We experiment with a benchmark dataset for source code captioning, HabeasCorpus [11].",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 9,
      "context" : "Our evaluation follows previous works on source code language modeling [10] and captioning [11].",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 10,
      "context" : "Our evaluation follows previous works on source code language modeling [10] and captioning [11].",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 10,
      "context" : "More specifically, we use a metric of top-k character savings [11] (CS-k).",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 10,
      "context" : "We follow the tokenization used in [11], where we transform camel case identifiers into multiple separate words (e.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 10,
      "context" : "Our preliminary experiment shows that the LSTM decoder significantly outperforms the Ngram models used in [11] (+3% in CS-2), so we use the LSTM decoder as a baseline for comparison.",
      "startOffset" : 106,
      "endOffset" : 110
    } ],
    "year" : 2017,
    "abstractText" : "We propose a novel module, the reviewer module, to improve the encoder-decoder learning framework. The reviewer module is generic, and can be plugged into an existing encoder-decoder model. The reviewer module performs a number of review steps with attention mechanism on the encoder hidden states, and outputs a fact vector after each review step; the fact vectors are used as the input of the attention mechanism in the decoder. We show that the conventional encoderdecoders are a special case of our framework. Empirically, we show that our framework can improve over state-of-the-art encoder-decoder systems on the tasks of image captioning and source code captioning.",
    "creator" : "LaTeX with hyperref package"
  }
}