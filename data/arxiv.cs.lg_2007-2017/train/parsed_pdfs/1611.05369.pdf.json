{
  "name" : "1611.05369.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Fast On-Line Kernel Density Estimation for Active Object Localization",
    "authors" : [ "Anthony D. Rhodes", "Max H. Quinn", "Melanie Mitchell" ],
    "emails" : [ "arhodespdx@gmail.com", "quinn.max@gmail.com", "mm@pdx.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "More specifically, at any given time in a run on a test image, our system uses image features plus contextual information it has discovered to identify a small subset of training images— an importance cluster—that is deemed most similar to the given test image, given the context. This subset is used to generate an updated situation model in an on-line fashion, using an efficient multipole expansion technique.\nAs a proof of concept, we apply our algorithm to a highly varied and challenging dataset consisting of instances of a “dog-walking” situation. Our results support the hypothesis that dynamically-rendered, context-based probability models can support efficient object localization in visual situations. Moreover, our approach is general enough to be applied to diverse machine learning paradigms requiring interpretable, probabilistic representations generated from partially observed data.\nIndex Terms—Computer vision; object localization; online learning; kernel density estimation; multipole method; data clustering\nI. INTRODUCTION\nRecent advances in computer vision have enabled significant progress on tasks such as object detection, scene classification, and automated scene captioning. However, these advances depend crucially on large sets of labeled training data as well as deep multilayer networks that require extensive training and whose learned models are hard, if not impossible, to interpret.\nThe work we report here is motivated by the need for more efficient, active learning procedures that utilize small (yet information-rich) sets of training examples, and that yield interpretable models. We propose a novel, general method for learning probabilistic models that capture and use context in a dynamic, on-line fashion.\nFor the current study, we apply this method to the task of efficiently locating objects in an image that depicts a known visual situation. In general, a visual situation defines a space of visual instances (e.g., images) that are linked by an abstract concept rather than any particular low-level visual similarity. For example, consider the situation “walking a dog.” Figure 1 illustrates varied instances of this situation. Different instances can be visually dissimilar, but conceptually analogous, and can even require “conceptual slippage” from a prototype [1] (e.g., in the fifth image the people are running, not walking; in the sixth image the “dog-walker” is biking, and there are multiple dogs.\nWhile the term situation can be applied to any abstract concept [3], most people would consider a visual situation category to be—like Dog-Walking—a named concept that invokes a collection of objects, regions, attributes, actions, and goals with particular spatial, temporal, and/or semantic relationships to one another. For humans, recognizing a visual situation—and localizing its components—is an active process that unfolds over time, in which prior knowledge interacts with visual information as it is perceived, in order to guide subsequent eye movements. This interaction enables a human viewer to very quickly locate relevant aspects of the situation [4]–[8].\nSimilarly, we hypothesize that a computer vision system that uses prior knowledge of a situation’s expected structure, as well as situation-relevant context as it is dynamically perceived, will allow the system to be accurate and efficient at localizing relevant objects, even when training data is sparse, or when object localization is otherwise difficult due to image clutter or small, blurred, or occluded objects.\nThe subsequent sections give some background on object localization, the details of the specific task we address, the dataset and methods we use, results and discussion of experiments, and plans for future work."
    }, {
      "heading" : "II. BACKGROUND",
      "text" : "Object localization is the task of locating an instance of a particular object category in an image, typically by specifying a tightly cropped bounding box centered on the instance. An object proposal specifies a candidate bounding box, and an object proposal is said to be a correct localization if it\nar X\niv :1\n61 1.\n05 36\n9v 1\n[ cs\n.C V\n] 1\n6 N\nov 2\n01 6\nsufficiently overlaps a human-labeled “ground-truth” bounding box for the given object. In the computer vision literature, overlap is measured via the intersection over union (IOU) of the two bounding boxes, and the threshold for successful localization is typically set to 0.5 [9]. In the literature, the “object localization” task is to locate one instance of an object category, whereas “object detection” focuses on locating all instances of a category in a given image.\nMost popular object-localization or detection algorithms in computer vision do not exploit prior knowledge or dynamic perception of context. The current state-of-the-art methods employ feedforward deep networks that test a fixed number of object proposals in a given image (e.g., [10]–[12]).\nPopular benchmark datasets for object-localization and detection include Pascal VOC [9] and ILSVRC [13]. Algorithms are typically rated on their mean average precision (mAP) on the object-detection task. On both Pascal VOC and ILSVRC, the best algorithms to date have mAP in the range of about 0.5 to 0.80; in practice this means that they are quite good at detecting some kinds of objects, and very poor at others. In fact, state-of-the-art methods are still susceptible to several problems, including difficulty with cluttered images, small or obscured objects, and inevitable false positives resulting from large numbers of object-proposal classifications. Moreover, such methods require large training sets for learning, and potential scaling issues as the number of possible categories increases.\nFor these reasons, several groups have pursued the more human-like approach of “active object localization,” in which a search for objects unfolds over time, with each subsequent time step using information gained in previous time steps (e.g., [14]–[17]).\nIn particular, in prior work our group showed that, on the “dog-walking” situation, an active object localization method that combines learned situation structure and active contextdirected search requires dramatically fewer object proposals than methods that do not use such information [17].\nOur system, called “Situate,” learns the expected structure of a situation from training images by inferring a set of joint probability distributions—a situation model—linking aspects of the relevant objects. Situate then uses these learned distributions to iteratively sample and score object proposals on a test image. At each time step, information from earlier sampled object proposals is used to adaptively modify the situation model, based on what the system has detected. That is, during a search for relevant objects, evidence gathered during the\nsearch continually serves as context that influences the future direction of the search.\nWhile this approach—active search with dynamically updated situation models—shows promise for efficient object localization, in the work reported in [17] it was limited by our use of low-dimensional parametric distributions to represent prior knowledge and perceived context. While efficient to compute, these simple distributions are not flexible enough to reliably serve as a basis for probabilistic knowledge representation in a general setting.\nIn contrast to parametric models, kernel-based density estimation can serve as a powerful and versatile tool for modeling complex data, and is potentially a better approach for probabilistic knowledge representation in computer vision. However, kernel density estimation methods are typically computationally expensive, which has limited their use for active, on-line search of the kind performed by Situate. In this study we present an efficient algorithm for performing online conditional kernel density estimation based on multipole expansions. We report preliminary experiments testing this algorithm on the dataset of [17] and assess its potential for more general applications in knowledge-based computer vision tasks."
    }, {
      "heading" : "III. DATASET AND SPECIFIC TASK",
      "text" : "Following [17], in this study we use the “Portland State Dog-Walking Images” [2]. This dataset currently contains 700 photographs, taken in different locations. Each image is an instance of a “Dog-Walking” situation in a natural setting. (Figure 1 gives some examples from this dataset.) In each image, the dog-walker(s), dog(s), and leash(es) have been labeled with tightly enclosing bounding boxes and object category labels.\nFor the purposes of this paper, we focus on a simplified subset of the Dog-Walking situation: photographs in which there is exactly one (human) dog-walker, one dog, one leash, along with unlabeled “clutter” (such as non-dog-walking people, buildings, etc) as in Figure 1. There are 500 such images in this subset.\nSituate’s task is to locate the objects defining the situation— dog-walker, dog, and leash—in a test image using as few object proposals as possible. Here, an object proposal comprises an object category (e.g., “dog”), coordinates of a bounding box center, and the bounding box’s width and height. As described above, an object is said to be localized by an object proposal’s bounding box if the intersection over union (IOU) with the\ntarget object’s ground-truth bounding box is greater than or equal to 0.5. Our main performance metric is the median number of object-proposal evaluations per image needed in order to locate all the relevant objects."
    }, {
      "heading" : "IV. SITUATE’S ACTIVE OBJECT LOCALIZATION ALGORITHM",
      "text" : ""
    }, {
      "heading" : "A. Learned Situation Models",
      "text" : "Situate learns a probabilistic model of situation structure— a situation model—by inferring two joint distributions over ground-truth bounding boxes in the training data. Joint Location is the joint distribution over the location (boundingbox center) of the dog-walker, dog, and leash in an image. Joint Dimensions is the joint distribution of the bounding-box width and height of these three objects within an image. In short, these two joint distributions encode expectations about the spatial and scale relationships among the relevant objects in the situation: when the system locates one object in a test image, the learned joint distributions can be conditioned on the features of that object to predict where, and what size, the other objects are likely to be.\nThe system also learns prior distributions over boundingbox width and height for each object category. The prior distribution over locations is uniform for each category since we do not want the system to learn photographers’ biases to put relevant objects near the center of the image.\nIn the version of Situate described in [17], the joint distributions (and prior distributions over bounding-box dimensions) were modeled as multivariate Gaussians. Gaussians are efficient to learn and to update on-line. However, as we will describe below, these low-dimensional parametric distributions are in general too inflexible to capture important patterns in visual situations.\nThe following subsection describes how Situate uses these learned distributions in its localization algorithm."
    }, {
      "heading" : "B. Running Situate on a Test Image",
      "text" : "1) Workspace: Situate’s main data structure is the Workspace, which is initialized with the input image. Situate uses its learned probability distributions to select and score object proposals in the Workspace, one at a time. If an object proposal for a given category scores above a threshold, that proposal is added to the Workspace as a detection.\n2) Category-Specific Probability Distributions: At each time step during a run, each relevant object category (here, dog-walker, dog, leash) is associated with a location distribution and a dimensions distribution. If there are no object proposals currently in the Workspace, these distributions are set to the priors described in Section IV-A. Otherwise, these distributions are derived by conditioning the learned situation model on the object proposals in the Workspace. (This will be illustrated in more detail below.)\n3) Main Loop of Situate: Given a test image, Situate iterates over a series of time steps, ending when it has localized each of the three relevant objects, or when a maximum number of iterations has occurred. At each time step in a run, Situate\nrandomly chooses an object category that has not yet been localized, and samples from that category’s current location and dimensions distributions in order to create a new object proposal. The resulting proposal is then given a score for that object category, as described below.\n4) Scoring Object Proposals: In the experiments reported here, during a run of Situate, each object proposal is scored by an “oracle” that returns the intersection over union (IOU) of the object proposal with the ground-truth bounding box for the target object. This oracle can be thought of as an idealized “classifier” whose scores reflect the amount of partial localization of a target object. Why do we use this idealized oracle rather than an actual object classifier? The goal of this paper is not to determine the quality of any particular object classifier, but to assess the benefit of using prior situation knowledge and active context-directed search on the efficiency of locating relevant objects. Thus, in this study, we do not use trained object classifiers to score object proposals. In future work we will experiment with object classifiers that can predict not only on the object category of a proposal but also the amount and type of overlap with ground truth.\n5) Provisional and Final Detections: An object proposal’s score determines whether it is added to the Workspace. For this purpose, Situate has two user-defined thresholds: a provisional detection threshold and a final detection threshold. If an object proposal’s score is greater than or equal to the final detection threshold, the system marks the object proposal as “final,” adds the proposal to the Workspace, and stops searching for that object category. Otherwise, if an object proposal’s score is greater than or equal to the provisional detection threshold, it is marked as “provisional.” If its score is greater than any provisional proposal for this object category already in the Workspace, it replaces that earlier proposal in the Workspace. The system will continue searching for better proposals for this object category. Whenever the Workspace is updated with a new object proposal, the system modifies the current situation model to be conditioned on all of the object proposals currently in the Workspace.\nThe purpose of provisional detections in our system is to use information the system has discovered even if the system is not yet confident that the information is correct or complete. For the experiments described in this paper, we used a provisional detection threshold of 0.25 and a final detection threshold of 0.5."
    }, {
      "heading" : "C. A Sample Run of Situate; Prior Results",
      "text" : "Figure 2 illustrates Situate’s context-driven active search with visualizations of the Workspace and probability distributions from a run on a sample test image. Prior to this run, the program has learned a situation model from training images, as was described in Section IV-A. The prior and joint distributions were learned as multivariate Gaussians. The caption of Figure 2 describes the dynamics of this run.\nIn [17] we compared Situate’s performance with that of several variations, as well as a recently published categoryindependent object detection system [18]. Our results sup-\nported the hypothesis that Situate’s active, context-directed search method was able to localize the three relevant objects with dramatically fewer object proposals than the comparison systems that did not use active search or contextual information."
    }, {
      "heading" : "V. FAST KERNEL DENSITY ESTIMATION WITH",
      "text" : "CONTEXT-BASED IMPORTANCE CLUSTERING\nAs we described above, the Joint Location and Joint BBDimensions distributions used in [17] were computed as multivariate Gaussian distributions, learned from a set of training images. On the one hand, this model restriction is computationally efficient, which makes it desirable for realtime probability density estimates. However, any parametric assumptions also necessarily restrict the expressiveness—and hence the general utility—of a model.\nIn this section we present an efficient algorithm for computing non-parametric probability density estimates. Unlike parametric methods, non-parametric methods make no global a priori assumptions about the shape of a distribution function. These models are consequently highly flexible and capable of representing useful patterns in diverse datasets."
    }, {
      "heading" : "A. Overview of Kernel Density Estimation",
      "text" : "Kernel density estimation (KDE) is a widely used method for computing non-parametric proability density estimates from data. Suppose our data lives in a d dimensional space. We are given a set S of training examples x, with x ∈ Rd. Now suppose we want to compute the probability density of an unobserved point z ∈ Rd, given S. The idea of KDE is to use a kernel function, which measures similarity between data points, so that points in S that are most similar to z contribute the most weight to the density estimate at point z.\nThis concept is formalized as follows. Using a kernel function K and bandwidth parameter σ, we estimate the density f at a point z ∈ Rd due to N local points, x1, . . . ,xN , with the following formula:\nf̂(z) = 1\nσdN N∑ i=1 K(z− xi),with ∫ K(z)dz = 1.\nIntuitively, a kernel estimate aggregates normalized distances over the local (i.e., similar) points for the point z. The bandwidth parameter σ controls the smoothness of the estimate, which determines the bias/variance trade-off for the model.\nIn the experiments we will describe below, we will generate two-dimensional (i.e., d = 2) densities for z = (width,height) for each object category. In particular, at training time, we will use KDE to compute prior distributions over z for each object category independently, and we will also use KDE to compute joint distributions over z values for the three categories. As before, during a run on a test image, the joint distributions will be conditioned on object proposals that have been added to the Workspace. Our hypothesis is that these prior and joint non-parametric distributions will be able to capture likely bounding box widths and heights more flexibly\nthan our original multivariate Gaussian distributions for these values. To simplify our focus, we retain the original uniform and multivariate Gaussian distributions for the prior location and joint locations models, respectively.\nA commonly used kernel function is the Gaussian kernel:\nK(u) = (2π) − d2 exp ( −‖u‖ 2\n2σ2\n) , (1)\nwhich yields the following form for the kernel density estimate of f , due to N points:\nf̂(z) = Z N∑ i=1 exp ( −‖z− xi‖ 2 2σ2 ) , (2)\nwith Z = 1\nσdN (2π)\n− d2 .\nWe can now express the conditional density estimate for a point z, given observed data {y} and kernel K, as follows:\nf̂(z|y) = ∑N i=1K (z− xzi )K (y − x y i )∑N\ni=1K (y − x y i )\n, (3)\nFor example, if we are estimating the width and height distribtuions of “dog” bounding boxes conditioned on a detected “dog-walker”, y would be the width and height of the detected dog-walker, z would be the expected “dog” width and height densities we are trying to estimate, and xzi and x y i are the width and height values of ground truth dogs and dog-walkers (respectively) observed in the training images.\nSuppose we wish to directly compute a density estimate f̂ at M discrete values of z, each time using N neighboring points. Equation 2 shows that the complexity of this computation is O(M ·N), which is is frequently prohibitive for on-line density approximations with large images and/or large values of N .\nThus, in order to efficiently employ non-parametric models for our active object localization procedure, we need to solve two related problems. First, we need to choose—from our training data—a small number N of points that gives us the most useful information for our estimate. Seond, even with a small N , it can still be expensive to compute the estimates using Equation 2, due to the multiplicative O(M ·N) complexity, so we need a way to compute an accurate and fast density approximation method that scales well with the number of variables on which we will be conditioning our distributions.\nTowards these ends we developed (1) a novel method to use the context of the detections discovered so far in the Workspace to determine an importance cluster—an appropriate, information-rich subset of the training data to use to create conditional distributions; and (2) a fast approximation technique for estimating distributions based on the method of multipole expansions."
    }, {
      "heading" : "B. Context-Based Importance Clustering",
      "text" : "Our first innovation addresses the problem of determining an appropriate subset of the data to use to compute conditional\ndistributions. Because a dataset of images depicting a particular, sometimes complex, visual situation is likely to exhibit high variability, we would like to optimally leverage contextual cues as our algorithm discovers them, in order to assist in object localization. As such, we employ a novel context-based importance clustering (CBIC) procedure, which our system uses during its active search for objects.\nConsider, for example, Figure 2(b), where the system has added a provisional “dog-walker” proposal with width w and height h. Our goal is to estimate the expected width and height distributions for “dog” and “leash”, conditioned on this proposal. In the system described in [17], this was done by conditioning the learned joint multivariate Gaussian width/height distributions on the detected dog-walker in order to form updated Gaussian distributions for “dog” and “leash”. The joint distributions were learned from the entire set of training data. But what if these learned distributions do not give a good conditional fit, given this data?\nOur novel procedure instead computes a flexible nonparametric conditional estimate, not from the entire training set, but from a subset of the training images—those that are deemed to be most similar to the test image, given the object proposals currently in the Workspace.\nThe motivation for this method is that we wish to focus our density estimation procedure on data that is most contextually relevant to a given test image, as it is perceived at a given time in a run.\nMore specifically, during a run of Situate on a test image, whenever a new object proposal has been added to the Workspace (i.e., the proposal’s score is above one of the detection thresholds), we determine a subset of the training data to use to update conditional distributions for the other object categories. To do this, we cluster the training dataset, using a k-means algorithm, based on the following features. (1) In the case where a single object has been localized, we cluster based on the normalized size of that object category’s ground-truth bounding boxes. For example, when the “dogwalker” proposal of Figure 2(b) is added to the Workspace, we update the “dog” and “leash” bounding-box distributions based on training data with similar size dog-walkers. (“Normalized size” is calculated as bounding-box area divided by the image area.) (2) When multiple objects have been localized, we again use the normalized sizes of the located object-categories, but we also use the normalized distance between the localized objects. For example, consider Figure 2(c), where the Workspace has “dog-walker” and “dog” proposals. We update the bounding box distribution for “leash” based on training-set images with similar “dog” and “dog-walker” bounding boxes, and similar normalized distance between the dog and dogwalker (measured center to center).\nOne reason for using these particular features is that they are strongly associated with both the depth of an object in an image as well as the spatial configurations of objects in a visual situation. Together, these data provide us with useful information about the size of the bounding-box of a target\nobject. The number of clusters we use for k-means is rendered optimally from a range of possible values, according to a conventional internal clustering validation measure based on a variance ratio criterion (Calinski-Harabasz index) [19].\nOnce the training data has been clustered, the test image is then assigned to a particular cluster—the importance cluster— with the nearest centroid.\nNote that importance clusters change dynamically as Situate adds new proposals to the Workspace."
    }, {
      "heading" : "C. Kernel Density Estimation with Multipole Expansions",
      "text" : "Our second innovation is to employ a fast approximation technique for estimating distributions: the method of multipole expansions. In short, multipole expansions are a physicsinspired method [20] for estimating probability densities with Taylor expansions.\nLet K denote the Gaussian kernel (Equation 1). We apply the multipole method to estimate Equation 2 by forming the multivariate Taylor series for K(z− xi).\nThe key advantage of this method is that, following the scheme of the factorized Gaussians presented in [21], the kernel estimate about the centroid x∗ (i.e., the center of the Taylor series expansion) can be expressed in factored form (we omit the details here for brevity, see [21] for a detailed treatment). The multipole form of this factorization [20] is the following expression:\nN∑ i=1 K(z− xi) = G(z) N∑ i=1 wiF (xi). (4)\nHere, the symbol connotes the multiplication of two Taylor series with vector components; G(z) is the Taylor series representing the points z at which we are estimating densities, and F (xi) is the Taylor series representing the elements of the importance cluster being used to estimate these densities. The value wi weights the point xi by how similar it is to the test image, using the features described in Section V-B.\nNote that the sum over the weighted F terms needs to be performed only once in order to estimate M point-wise densities.\nNow, suppose we wish to compute a density estimate f̂ at M discrete values of z, each time using N neighboring points. As we discussed in Section V-A, doing this directly with KDE is O(M · N) complexity (Equation 2). What the multipole method allows is a reasonable approximation to KDE, but with O(M+N) complexity, where N is the size of our importance cluster. This is potentially a huge gain in efficiency; in fact it allows us to use this method in an on-line fashion while our system performs its active search.\nIn order to use the multipole method in our Situate architecture, we need to extend Equation 4 to approximate conditional proability densities (e.g., the expected distribution of “dog” widths / heights given a detected “dog-walker”).\nRecall that conditional density esitmation for KDE involves multiplying two kernel functions (numerator of Equation 3).\nThe product of (Gaussian) kernels is a (Gaussian) kernel [22], [23], with asymptotic convergence properties (subject to choice of bandwidth). To generate an efficient multipole conditional density estimation, we use a common bandwidth for each kernel in the numerator of Equation 3. Because the product of Gaussian kernels with shared bandwidths yields a single Gaussian kernel function (in a higher dimension), this transforms Equation 3’s numerator into a sum of Gaussian kernels (as opposed to a sum of products). We can subsequently apply the multipole expansion method from Equation 4 to obtain an expression for conditional density estimation with multipole expansion:\nf̂(z|y) ∝ G(K(z− x∗)) N∑ i=1 wiF (xi). (5)\nHere we have omitted the normalization constant for the conditional density estimate, which gives the proportionality result indicated. In this equation, x∗ is a stochastically determined centroid for the estimate (as will be explained in the next subsection); G(z), F (xi), and wi are all defined analogously to Equation 4.\nEquation 5 still gives us a complexity of O(M + N). By comparison, other conventional conditional density estimation procedures, such as the least-squares method, require O(MN3) computations [24]."
    }, {
      "heading" : "D. Stochastic Filtering",
      "text" : "A significant issue arises when we consider performing this density approximation for a large M (i.e., for many different point-wise approximations), which might be required in cases for which comprehensive, interpretable models are desired. The issue is that the inevitable errors in the approximation can accumulate.\nAlthough the overall error in our density approximation can be improved by choosing a sufficiently large order for the Taylor expansions (such as a multivariate quadratic, cubic, etc.), the error margin can nonetheless potentially become excessive when aggregated over points that are a great distance from the center of each Gaussian kernel; naturally, this issue is compounded further as the size of the set of sample points, N , grows.\nThere have been a few proposed remedies in the literature to this issue of aggregated errors. The authors in [20] simply suggest limiting the points over which the density estimation is performed to a small subset of the space, but this is a fairly weak and impractical compromise for a general problem setting. Alternatively, the authors in [21] suggest performing a constrained clustering of the density space and then estimating each point-wise density by its nearest centroid. However, finding an appropriate clustering needed for this scheme turns out to be very expensive to achieve. Various approximate solutions exist, including an adaptive, greedy algorithm called “farthest point clustering” [25] and a more computationallyefficient version given by [26].\nAs the third innovation of this paper, we introduce a new approach, termed stochastic filtering—that obviates the need\nfor such clustering of the density space. For each target pointdensity approximation f̂(z), we simply choose one element of the current importance cluster at random, and use this element to be the center of our Taylor expansion G(z).\nNote that our proposed stochastic filtering method will produce a sparse density estimate since the stochastic choice of cluster center coupled with the Gaussian kernel will render many of the approximate values zero. The sparsity of the estimate is therefore the penalty we pay for using this filter. Nevertheless, so long as M >> N (a very natural assumption for most practical applications of density estimation), then f̂ → f(z) as M → ∞, which follows from the convergence of the Taylor series. From a sparse estimate, one can additionally apply a simple Gaussian smoothing process to achieve a low-cost, yet high-fidelity density estimate. Figure 3 compares results of density estimation using KDE, multipole with stochastic filtering, and multipole with stochastic filtering and Gaussian smoothing, all with respect to the same sample test image and a small importance cluster. This shows how close our method can come to a full KDE method, but with a very significant speed-up.\nIt should also be noted that perfect density estimation is not at all required for practical use in our object localization task. Instead we desire an efficient localization process which is capable of dynamically leveraging visual-contextual cues for active object localization."
    }, {
      "heading" : "E. MIC-Situate Algorithm",
      "text" : "The following are the steps in our algorithm, Multipole Density Estimation with Importance Clustering (MIC-Situate). Assume that we have a training set S, and Situate is running on a test image T . As was described in Section IV at each time step in a run, Situate chooses an object category at random, samples a location and a bounding-box width and height from its current distributions for the given object category in order to form an object proposal, and scores that object proposal to determine if it should be added to the Workspace.\nSuppose that L object proposals have added to the Workspace, with values {l1, . . . , lL}. (E.g., l1 might be the (width,height) values of a detected dog-walker bounding box, and l2 might be the (width,height) values of a detected dog bounding box.)\nWhenever a new object proposal is added to the Workspace, do the following: For each object category c:\n1) Perform k-means clustering of the training data, as described in Section V-B. 2) Determine which cluster the test image belongs to (the importance cluster). 3) Using this importance cluster, compute the fast multipole conditional density estimation (Equation 5), conditioning on the L detected objects. 4) Update the size (width/height) distribution for object category c."
    }, {
      "heading" : "VI. EXPERIMENTAL RESULTS",
      "text" : "In this section we present results from running the methods described above for the MIC-Situate algorithm which utilizes both our novel importance clustering technique as well as our fast non-parametric, multipole method for learning a flexible knowledge representation of bounding-box sizes of objects for active object localization. In reporting results, we use the term completed situation detection to refer to a run on an image for which a method successfully located all three relevant objects within a maximum number iterations; we use the term failed situation detection to refer to a run on an image that did not result in a completed situation detection within the maximum allotted iterations.\nAltogether, we tested four distinct methods for object localization in the dog-walking situations: (1) Multipole (with IC): non-parametric multipole method with importance clustering, as described above (2) Multipole (no IC): non-parametric multipole method without importance clustering (where density approximations are generated using the entire training dataset), (3) MVN:distributions learned as multivariate Gaussian methods and (4) Uniform: a baseline uniform distribution. In the case of (1) and (2) we used a multipole-based non-parametric density estimate for target object width/height priors, utilizing the entire training dataset; we similarly used conditional multipole density estimates for our conditional width/height size distributions. With method (3) we employed multivariate Gaussian distributions as priors using the entire training dataset. For methods (1)-(3) we used multivariate Gaussian (normal) distributions (MVNs) for our prior distributions for object location, and conditioned MVNs for conditioned distributions for location. For method (4) a uniform distribution is used for priors and conditioned distributions alike.\nAs described above, our dataset contains 500 images. For each method, we performed 10-fold cross-validation: at each fold, 450 images were used for training and 50 images for testing. Each fold used a different set of 50 test images. We ran the algorithm on the test images, with final-detection-threshold set to 0.5, provisional-detection-threshold set to 0.25, and maximum number of iterations set to 1,000. Throughout, our density estimations used the following conventional “rule of thumb” bandwidth [27]:\nσ = σ̂D\n( 4\n(d+ 2)n\n)1/(d+4) ,\nwhere σ̂D is the standard deviation of the data set.\nIn reporting the results, we combine results on the 50 test images from each of the 10 folds and report statistics over the total set of 500 test images.\nFigure 4 gives, for each method, the median number of iterations per image in order to reach a completed situation detection. The medians are over the union of test images from all 10 folds—that is for 500 images total. The median value is given as 1,000 (i.e., “failure”) for methods on which a majority of test image runs resulted in failed situation detections. We used the median instead of the mean to allow us to give a statistic that includes the “failure” runs.\nThe percentages below each bar are the percentage of images on which the method reached a completed situation (i.e., correct final bounding boxes for all three objects). For example, the “Multipole (no IC)” method reached completed situations on 58.6% of the 500 images.\nThe most effective method for our experiments was the multipole with importance clustering procedure (“Multipole (with IC)”), which demonstrated a 24% reduction over MVN in the median completed situation detection time. These results confirm the benefit of using both importance clustering and flexible, non-parametric probabilistic models in our active, knowledge-driven situation detection task. Perhaps even more impressive was the ability of the multipole method with importance clusters to outperform the other procedures while explicitly using a much smaller dataset for model-building. For comparison, in the “Multipole (with IC)” method, the importance clusters used on average only 25 images for density estimation (cluster size is variable in our simulations), whereas the three other methods utilize 450 images. This outcome serves as a strong indication of the significant promise and potential of our novel importance clustering process."
    }, {
      "heading" : "VII. CONCLUSIONS AND FUTURE WORK",
      "text" : "Our work has provided the following contributions: (1) We have proposed a new approach to actively localizing objects in visual situations using a knowledge-driven search with adaptable probabilistic models. (2) We devised an innovative, general-purpose machine learning process that uses\nobserved/contextual data to generate a refined, informationrich training set (an importance cluster) applicable to problems with high situational specificity. (3) We developed a novel, fast kernel density estimation procedure capable of producing flexible models efficiently, in a challenging on-line setting; furthermore, when applied in conjunction with importance clustering, this estimation procedure scales well with even a large number of observed variables. (4) We employed these techniques to the problem of conditional density estimation. (5) As a proof of concept, we applied our algorithm to a highly varied and challenging dataset.\nThe work described in this paper is an early step in our broader research goal: to develop a system that integrates cognitive-level symbolic knowledge with lower-level vision in order to exhibit a deep understanding of specific visual situations. This is a long-term and open-ended project. In the near-term, we plan to improve our current system in several ways, including chiefly applying Bayesian optimization techniques to enrich our active learning algorithm.\nIn the longer term, our goal is to extend Situate to incorporate important aspects of Hofstadter and Mitchells Copycat architecture [1] in order to give it the ability to quickly and flexibly recognize visual actions, object groupings, relationships, and to be able to make analogies (with appropriate conceptual slippages) between a given image and situation prototypes. In Copycat, the process of mapping one (idealized) situation to another was interleaved with the process of building up a representation of a situation. This interleaving was shown to be essential to the ability to create appropriate, and even creative analogies [28]. Our long-term goal is to build Situate into a system that bridges the levels of symbolic knowledge and low-level perception in order to more deeply understand visual situationsa core component of general intelligence."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "This material is based upon work supported by the National Science Foundation under Grant Number IIS-1423651. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation."
    } ],
    "references" : [ {
      "title" : "The Copycat project: A model of mental fluidity and analogy-making",
      "author" : [ "D.R. Hofstadter", "M. Mitchell" ],
      "venue" : "Advances in Connectionist and Neural Computation Theory, K. Holyoak and J. Barnden, Eds. Ablex Publishing Corporation, 1994, vol. 2, pp. 31–112.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Surfaces and Essences",
      "author" : [ "D. Hofstadter", "E. Sander" ],
      "venue" : "Basic Books,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2013
    }, {
      "title" : "Visual objects in context",
      "author" : [ "M. Bar" ],
      "venue" : "Nature Reviews Neuroscience, vol. 5, no. 8, pp. 617–629, 2004.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "More than meets the eye: The active selection of diagnostic information across spatial locations and scales during scene categorization",
      "author" : [ "G.L. Malcolm", "P.G. Schyns" ],
      "venue" : "Scene Vision: Making Sense of What We See, K. Kveraga and M. Bar, Eds. MIT Press, 2014, pp. 27–44.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Scene context guides eye movements during visual search",
      "author" : [ "M. Neider", "G. Zelinsky" ],
      "venue" : "Vision Research, vol. 46, no. 5, pp. 614–621, 2006.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Meaning in visual search",
      "author" : [ "M.C. Potter" ],
      "venue" : "Science, vol. 187, pp. 965–966, 1975.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1975
    }, {
      "title" : "Expectation (and attention) in visual cognition.",
      "author" : [ "C. Summerfield", "T. Egner" ],
      "venue" : "Trends in Cognitive Sciences,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2009
    }, {
      "title" : "The Pascal visual object classes (VOC) challenge",
      "author" : [ "M. Everingham", "L. Gool", "C.K.I. Williams", "J. Winn", "A. Zisserman" ],
      "venue" : "International Journal of Computer Vision, vol. 88, no. 2, pp. 303–338, 2010.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "K. He", "X. Zhang", "S. Ren", "J. Sun" ],
      "venue" : "arXiv:1512.03385, 2015.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Fast R-CNN",
      "author" : [ "R. Girshick" ],
      "venue" : "International Conference on Computer Vision (ICCV). IEEE, 2015, pp. 1440–1448.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "You only look once: Unified, real-time object detection",
      "author" : [ "J. Redmon", "S. Divvala", "R. Girshick", "A. Farhadi" ],
      "venue" : "arXiv:1506.02640, 2015.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "ImageNet large scale visual recognition challenge",
      "author" : [ "O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei" ],
      "venue" : "International Journal of Computer Vision, vol. 115, no. 3, pp. 211–252, 2015.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Adaptive gaze control for object detection.",
      "author" : [ "G.C.H.E. de Croon", "E.O. Postma", "H.J. van den Herik" ],
      "venue" : "Cognitive Computation,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2011
    }, {
      "title" : "An active search strategy for efficient object class detection",
      "author" : [ "A. Gonzalez-Garcia", "A. Vezhnevets", "V. Ferrari" ],
      "venue" : "Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2015, pp. 3022–3031.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Adaptive object detection using adjacency and zoom prediction",
      "author" : [ "Y. Lu", "T. Javidi", "S. Lazebnik" ],
      "venue" : "arXiv:1512.07711, 2015.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Active object localization in visual situations",
      "author" : [ "M.H. Quinn", "A.D. Rhodes", "M. Mitchell" ],
      "venue" : "arXiv:1607.00548, 2016.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Prime object proposals with randomized Prim’s algorithm",
      "author" : [ "S. Manen", "M. Guillaumin", "L.V. Gool" ],
      "venue" : "International Conference on Computer Vision (ICCV). IEEE, 2013, pp. 2536–2543.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Understanding of internal clustering validation measures",
      "author" : [ "Y. Liu", "Z. Li", "H. Xiong", "X. Gao", "J. Wu" ],
      "venue" : "2010 IEEE International Conference on Data Mining. IEEE, 2010, pp. 911–916.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Efficient on-line nonparametric kernel density estimation",
      "author" : [ "C.G. Lambert", "S.E. Harrington", "C.R. Harvey", "A. Glodjo" ],
      "venue" : "Algorithmica, vol. 25, no. 1, pp. 37–57, 1999.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Efficient kernel machines using the improved fast Gauss transform",
      "author" : [ "C. Yang", "R. Duraiswami", "L.S. Davis" ],
      "venue" : "Advances in neural information processing systems, 2004, pp. 1561–1568.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Positive definite kernels: past, present and future",
      "author" : [ "G. Fasshauer" ],
      "venue" : "Dolomite Research Notes on Approximation, 2011.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Classes of Kernels for Machine Learning: A Statistics Perspective",
      "author" : [ "M.G. Genton" ],
      "venue" : "Journal of Machine Learning Research, vol. 2, no. Dec, pp. 299–312, 2001.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Conditional Density Estimation via Least-Squares Density Ratio Estimation",
      "author" : [ "M. Sugiyama", "I. Takeuchi", "T. Suzuki", "T. Kanamori" ],
      "venue" : "AIS- TATS, pp. 781–788, 2010.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Clustering to minimize the maximum intercluster distance",
      "author" : [ "T.F. Gonzalez" ],
      "venue" : "Theoretical Computer Science, vol. 38, pp. 293–306, 1985.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 1985
    }, {
      "title" : "Optimal algorithms for approximate clustering",
      "author" : [ "T. Feder", "D. Greene" ],
      "venue" : "Proceedings of the twentieth annual ACM symposium on Theory of computing - STOC ’88. New York, New York, USA: ACM Press, 1988, pp. 434–444.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "A Bayesian Approach to Parameter Estimation for Kernel Density Estimation via Transformations",
      "author" : [ "Q. Liu", "D. Pitt", "X. Zhang", "X. Wu" ],
      "venue" : "Annals of Actuarial Science, vol. 5, no. 2, pp. 181–193, 2011.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Analogy-Making as Perception: A Computer Model",
      "author" : [ "M. Mitchell" ],
      "venue" : null,
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 1993
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Different instances can be visually dissimilar, but conceptually analogous, and can even require “conceptual slippage” from a prototype [1] (e.",
      "startOffset" : 136,
      "endOffset" : 139
    }, {
      "referenceID" : 1,
      "context" : "While the term situation can be applied to any abstract concept [3], most people would consider a visual situation category to be—like Dog-Walking—a named concept that invokes a collection of objects, regions, attributes, actions, and goals with particular spatial, temporal, and/or semantic relationships to one another.",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 2,
      "context" : "This interaction enables a human viewer to very quickly locate relevant aspects of the situation [4]–[8].",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 6,
      "context" : "This interaction enables a human viewer to very quickly locate relevant aspects of the situation [4]–[8].",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 7,
      "context" : "5 [9].",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 8,
      "context" : ", [10]–[12]).",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 10,
      "context" : ", [10]–[12]).",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 7,
      "context" : "Popular benchmark datasets for object-localization and detection include Pascal VOC [9] and ILSVRC [13].",
      "startOffset" : 84,
      "endOffset" : 87
    }, {
      "referenceID" : 11,
      "context" : "Popular benchmark datasets for object-localization and detection include Pascal VOC [9] and ILSVRC [13].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 12,
      "context" : ", [14]–[17]).",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 15,
      "context" : ", [14]–[17]).",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 15,
      "context" : "directed search requires dramatically fewer object proposals than methods that do not use such information [17].",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 15,
      "context" : "While this approach—active search with dynamically updated situation models—shows promise for efficient object localization, in the work reported in [17] it was limited by our use of low-dimensional parametric distributions to represent prior knowledge and perceived context.",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 15,
      "context" : "We report preliminary experiments testing this algorithm on the dataset of [17] and assess its potential for more general applications in knowledge-based computer vision tasks.",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 15,
      "context" : "Following [17], in this study we use the “Portland State Dog-Walking Images” [2].",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 15,
      "context" : "In the version of Situate described in [17], the joint distributions (and prior distributions over bounding-box dimensions) were modeled as multivariate Gaussians.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 15,
      "context" : "In [17] we compared Situate’s performance with that of several variations, as well as a recently published categoryindependent object detection system [18].",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 16,
      "context" : "In [17] we compared Situate’s performance with that of several variations, as well as a recently published categoryindependent object detection system [18].",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 15,
      "context" : "As we described above, the Joint Location and Joint BBDimensions distributions used in [17] were computed as multivariate Gaussian distributions, learned from a set of training images.",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 15,
      "context" : "In the system described in [17], this was done by conditioning the learned joint multivariate Gaussian width/height distributions on the detected dog-walker in order to form updated Gaussian distributions for “dog” and “leash”.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 17,
      "context" : "The number of clusters we use for k-means is rendered optimally from a range of possible values, according to a conventional internal clustering validation measure based on a variance ratio criterion (Calinski-Harabasz index) [19].",
      "startOffset" : 226,
      "endOffset" : 230
    }, {
      "referenceID" : 18,
      "context" : "In short, multipole expansions are a physicsinspired method [20] for estimating probability densities with Taylor expansions.",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 19,
      "context" : "The key advantage of this method is that, following the scheme of the factorized Gaussians presented in [21], the kernel estimate about the centroid x∗ (i.",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 19,
      "context" : ", the center of the Taylor series expansion) can be expressed in factored form (we omit the details here for brevity, see [21] for a detailed treatment).",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 18,
      "context" : "The multipole form of this factorization [20] is the following expression:",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 20,
      "context" : "[22], [23], with asymptotic convergence properties (subject to choice of bandwidth).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "[22], [23], with asymptotic convergence properties (subject to choice of bandwidth).",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 22,
      "context" : "By comparison, other conventional conditional density estimation procedures, such as the least-squares method, require O(MN) computations [24].",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 18,
      "context" : "The authors in [20] simply suggest limiting the points over which the density estimation is performed to a small subset of the space, but this is a fairly weak and impractical compromise for a general problem setting.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 19,
      "context" : "Alternatively, the authors in [21] suggest performing a constrained clustering of the density space and then estimating",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 23,
      "context" : "Various approximate solutions exist, including an adaptive, greedy algorithm called “farthest point clustering” [25] and a more computationally-",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 24,
      "context" : "efficient version given by [26].",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 25,
      "context" : "density estimations used the following conventional “rule of thumb” bandwidth [27]:",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 0,
      "context" : "In the longer term, our goal is to extend Situate to incorporate important aspects of Hofstadter and Mitchells Copycat architecture [1] in order to give it the ability to quickly and flexibly recognize visual actions, object groupings, relationships, and to be able to make analogies (with appropriate conceptual slippages) between a given image and situation prototypes.",
      "startOffset" : 132,
      "endOffset" : 135
    }, {
      "referenceID" : 26,
      "context" : "This interleaving was shown to be essential to the ability to create appropriate, and even creative analogies [28].",
      "startOffset" : 110,
      "endOffset" : 114
    } ],
    "year" : 2016,
    "abstractText" : "A major goal of computer vision is to enable computers to interpret visual situations—abstract concepts (e.g., “a person walking a dog,” “a crowd waiting for a bus,” “a picnic”) whose image instantiations are linked more by their common spatial and semantic structure than by low-level visual similarity. In this paper, we propose a novel method for prior learning and active object localization for this kind of knowledge-driven search in static images. In our system, prior situation knowledge is captured by a set of flexible, kernel-based density estimations— a situation model—that represent the expected spatial structure of the given situation. These estimations are efficiently updated by information gained as the system searches for relevant objects, allowing the system to use context as it is discovered to narrow the search. More specifically, at any given time in a run on a test image, our system uses image features plus contextual information it has discovered to identify a small subset of training images— an importance cluster—that is deemed most similar to the given test image, given the context. This subset is used to generate an updated situation model in an on-line fashion, using an efficient multipole expansion technique. As a proof of concept, we apply our algorithm to a highly varied and challenging dataset consisting of instances of a “dog-walking” situation. Our results support the hypothesis that dynamically-rendered, context-based probability models can support efficient object localization in visual situations. Moreover, our approach is general enough to be applied to diverse machine learning paradigms requiring interpretable, probabilistic representations generated from partially observed data.",
    "creator" : "LaTeX with hyperref package"
  }
}