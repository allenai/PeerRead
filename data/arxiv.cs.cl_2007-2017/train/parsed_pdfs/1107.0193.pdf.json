{
  "name" : "1107.0193.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "On ambiguity. Its locus in the architecture of Language and its origin in efficient communication",
    "authors" : [ "Jordi Fortuny", "Bernat Corominas-Murtra" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Contents"
    }, {
      "heading" : "1 Introduction 4",
      "text" : ""
    }, {
      "heading" : "2 The locus of ambiguity in the architecture of Language 6",
      "text" : "2.1 Lexical ambiguity . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n2.2 Syntactic ambiguity . . . . . . . . . . . . . . . . . . . . . . . . . 11\n2.3 Quantifier scope ambiguity . . . . . . . . . . . . . . . . . . . . . 16\n2.4 Garden path sentences . . . . . . . . . . . . . . . . . . . . . . . . 18\n2.5 Concluding remarks on the locus of ambiguity . . . . . . . . . . . 20\n3 Ambiguity and logical reversibility: a rigorous treatment 23\n3.1 Logical irreversibility . . . . . . . . . . . . . . . . . . . . . . . . . 23\n3.2 Logical (ir)reversibility in terms of Turing machines . . . . . . . 25\n3.3 Logical reversibility and ambiguous codes . . . . . . . . . . . . . 28\n3.3.1 Noise: quantifying the degree of ambiguity . . . . . . . . 30\n3.3.2 Ambiguity and logical irreversibility . . . . . . . . . . . . 31"
    }, {
      "heading" : "4 The emergence of ambiguity in natural communication 32",
      "text" : "4.1 Zipf’s hypothesis . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n4.2 Symmetry in coding/decoding complexities . . . . . . . . . . . . 34"
    }, {
      "heading" : "5 Discussion 37",
      "text" : "A Appendix: Ambiguity and physical irreversibility 38"
    }, {
      "heading" : "1 Introduction",
      "text" : "It is a common observation that natural languages are ambiguous, namely, that linguistic utterances can potentially be assigned more than one interpretation and that receivers of linguistic utterances need to resort to supplementary information (i.e., the linguistic or the communicative context) to choose one among the available interpretations.\nBoth linguists and logicians have been interested in this observation. On the one hand, a traditional tasks of grammar is to illustrate and classify ambiguity, which may be of different types, as well as to determine how apparently ambiguous utterances are disambiguated at the relevant level of representation; indeed, the search for a parsimonious treatment of certain ambiguities such as scope ambiguities has been one of the most powerful motors in the development of the formal inquiry of the syntax-semantics interface, since its modern inception in Montague’s semiotic program [24]. It is no exaggeration at all, in our opinion, to say that the presence of ambiguity (particularly, scope ambiguity) and the apparent mismatch between the form and the alleged semantic structure of quantified expressions in natural languages have been the two major guiding problems in the development of a rigorous theory of the syntax-semantics interface of natural languages.\nOn the other hand, logicians in general would not be as interested in describing or characterizing the phenomenon of ambiguity, as in the construction of unambiguous artificial languages whose primitive symbols have a univocal interpretation and whose formulae are constructed by the appropriate recursive syntactic definitions and unambiguously interpreted by the relevant compositional semantic rules, formulated as recursive definitions that trace back the syntactic construction of the formulae. Not surprisingly, some philosophers, as [35, 3.323-3.325], identified the ambiguity of ordinary language as the source of philosophical confusion, and aspired to construct a language whose signs were univocal and whose propositions mirrored the logical structure of reality itself.\nIt is a rather common view that the presence of phenomena such as ambiguity and garden path sentences suggests that Language is poorly designed for communication [8]. In fact, there are at least two opposite starting hypotheses about the nature and emergence of ambiguity in natural communication sys-\ntems. It could well be that ambiguity is an intrinsic imperfection, since natural, self-organized codes of communication are not perfect, but coevolved through a fluctuating medium and no-one designed them. But it may be as well that ambiguity is the result of an optimization process, inasmuch as natural, self-organized codes of communication must satisfy certain constraints that a non-ambiguous artificial language can afford to neglect. Logical languages, for instance, are constructed to study relations such as logical consequence and equivalence among well-formed formulae, for which it is desirable to define syntactic rules that do not generate syntactically ambiguous expressions.1 However, in the design of logical languages that provide the appropriate tools for that particular purpose, certain features that may be crucial in the emergence of natural communication systems are neglected, such as the importance of the cost in generating expressions and the role of cooperation between the coder and the decoder in the process of communicating those expressions, a factor that is completely extraneous to the design of logical languages.\nDespite the indisputability of ambiguity in natural languages and the attention that this observation has received among linguists, philosophers and logicians, it is fair to conclude that the emergence of ambiguity has not come yet under serious theoretical scrutiny.\nIn this article we shall capitalize on the two above-mentioned factors (the importance of the cost in generating expressions and the role of cooperation between the coder and the decoder) in order to provide a rigorous argument for the idea that ambiguity is an unavoidable consequence of the following efficiency factor in natural communication systems: interacting communicative agents must attain a code that tends to minimize the complexities of both the coding and the decoding processes.\n1It is desirable, but not mandatory. As noted by Thomason in his ’Introduction’ [24, Chapter 1], “[A] by-product of Montague’s work (...) is a theory of how logical consequence can be defined for languages admitting syntactic ambiguity. For those logicians concerned only with artificial languages this generalization will be of little interest, since there is no serious point to constructing an artificial language that is not disambiguated (p.4, note 5)” if the objective is to characterize logical notions such as consequence. However, it is important for the development of ’Universal Grammar’ in Montague’s sense, i.e., for the development of a general and uniform mathematical theory valid for the syntax and semantics of both artificial and natural languages. We refer the reader interested on the treatment of ambiguity in Montague Grammar to Thomason’s ’Introduction’ [24, Chapter 1] and to Montague’s ’Universal Grammar’ [24, Chapter 7].\nThe remaining of this article is organized as follows. In section 2 we illustrate several well-known types of ambiguity and we articulate the idea that ambiguity appears because the phonetic form (which feeds the Articulatory-Perceptual system) dispenses with part of the information present in the logical form (which feeds the Conceptual-Intentional system). After locating the emergence of ambiguity in the architecture of Language, we proceed to develop our theoretical argument for the emergence of ambiguity from communicative efficiency factors. In section 3 we introduce Landauer’s concept of logical (ir)reversibility of a given computational device and we quantify the degree of ambiguity of a code as the amount of uncertainty in reversing the coding process. In section 4 we introduce Zipf’s vocabulary balance condition, a particular instance of Zipf’s Least Effort Principle [36], and we show how it can be properly generalized and accomodated to the information-theoretic framework adopted in section 3. We conclude our reasoning by showing that if the coding and the decoding processes are performed in a cooperative regime expressed in terms of a symmetry equation between coding and decoding complexities, a certain amount of logical uncertainty or ambiguity is unavoidable. In section 5 we recapitulate our derivation of the presence of ambiguity and we stress a further important result intimately related to our development: Zipf’s law, another well-known and ubiquitous feature of natural language products, is the sole expected outcome of an evolving communicative system that satisfies the symmetry equation between coding and decoding complexities, as argued in [11]. A emphasizes the relationship between logical irreversibility and thermodynamical irreversibility."
    }, {
      "heading" : "2 The locus of ambiguity in the architecture of",
      "text" : "Language\nIn this preliminary section we would like to reflect about the locus of ambiguity in Language. We shall adapt Chomsky’s general assumptions about the architecture of Language [6]. Let a particular language L be constituted of a lexicon Lex and a generative procedure Γ that generates a set ∆ of structural descriptions of the form (π, λ), where π is an element of the set Π of phonetic forms and λ an element of the set Λ of logical forms. The set ∆ of structural descriptions of L generated by Γ is the set of expressions of L. We thus view a\nlanguage L as a tuple L = (Lex,Γ,∆).\nThe Articulatory-Perceptual (A-P) system and the Conceptual-Intentional (C-I) system are the external systems of Language. The elements of Π are interpreted at the A-P system and provide it with the necessary instructions to generate acoustic signs in an oral language and patterns of visual signs in a sign language, and the elements of Λ are interpreted at the C-I system, where an expression is semantically interpreted.\nΓ is assumed to have an initial branch Γinit responsible for generating a set Σ of hierarchical representations whose terminals are lexical items that provide constituency and chain relationships; once Σ has been generated, Γ splits into two branches: (i) the externalization branch Γext, which generates Π from Σ, and (ii) the abstract branch Γabs, which generates Λ from Σ. We shall refer to this splitting model as the inverted-Y model.\nA couple of caveats are in order. Although the existence of a lexicon that provides the basic units for computations and the existence of a generative procedure that constructs semantic and phonetic representations seem to us\nvirtual conceptual necessities of Language, we must emphasize that multiple details about the architecture of Language are controversial, perhaps especially (but of course not exclusively) those concerning the nature and organization of the C-I system and the lexicon. We must also remark that we do not see any advantage or disadvantage for the standard inverted-Y model just sketched with respect to a model where phonetic forms were generated from logical forms, that means, a model where Λ would feed both the C-I system and the externalization branch, which is responsible for generating Π, the input of the A-P system. It is not the objective of this article, though, to review current assumptions about the architecture of Language, and indeed the alternative options we could explore do not affect the correctness of the course we are about to develop. We shall thus assume the standard inverted-Y model 1 as a framework and without further discussion.\nWe shall immediately argue that ambiguity appears because Π dispenses with part of the information that is present in Λ. In other words, the branch Γext eliminates or does not contain some information that is preserved or contained at the branch Γabs and necessary for the C-I system. Once we have identified the loss of resolution in Γext as the locus of ambiguity in Language, we shall investigate in the following sections whether the low resolution of Π is a communicative imperfection or an avoidable byproduct of certain efficiency considerations in natural communication systems."
    }, {
      "heading" : "2.1 Lexical ambiguity",
      "text" : "Let us first briefly consider lexical ambiguity. As is well known, we can distinguish two types of lexical ambiguity: homonymy, which can be partial or absolut [22], and polysemy. Following Lyons, absolut homonyms can be defined in the following terms.\nTwo lexemes are absolut homonyms iff:\n1. they are unrelated in meaning\n2. all their forms are identical\n3. the identical forms will be grammatically equivalent\nLyons (1995: 55)\nTwo lexemes are partial homonyms if there is identity of at least one form and not all three conditions are satisfied; accordingly, the two lexical entries bank1 (“financial institution”) and bank2 (“sloppy side of a river”) constitute classical examples of absolut homonymy [22, p.55], but the verbs find and found illustrate partial homonymy, since they share the form found, but not the forms find, findings, or founds, founding (they do not satisfy condition (2)).\nWhereas homonymy is a relation between lexemes, polysemy is a property of single lexemes. A lexeme is said to be polysemic if it possesses several meanings. The criteria to distinguish polysemy and homonymy are etymology and relatedness of meaning in such a way that etymology generally suports the intuitions of speakers about relatedness of meaning, but not always. Indeed, as observed by Lyons [22, p.59]:\n“It sometimes happens that lexemes which the average speaker of the language thinks of as being semantically unrelated have come from the same source”, as in the case of sole1 (“bottom of foot or shoe”) and sole2 (“kind of fish”).\nIt is not surprising that etymology tends to support the intuitions of native speakers, provided that metaphorical extension is both an important factor in semantic change and a synchronic process at work in defining semantic relatedness. Although the distinction between polysemy (also called complementary ambiguity) and homonymy (or contrastive ambiguity) may be fuzzy and vary among speakers, it is usually assumed to reflect central properties of the lexicon. We can view, for concreteness, a lexical entry as a set whose single element is a pair constituted of a signifier and a signified; in the case that two lexemes lexeme1, lexeme2 are absolut homonyms, their respective signifiers signifier1, signifier2, turn out to be phonologically identical, although they are etymologically unrelated and their siginifieds are unconnected.\nlexeme1 = {< signifier1, signified1 >}\nlexeme2 = {< signifier2, signified2 >}\nFollowing what we can call a static conception of the lexicon, a polysemic lexemen could be listed as a single entry with multiple signifieds associated to a single signifier, that is as a set of several pairs all constituted of the signifiern and a different signified. (1) lexemen = { < signifiern, signified j n >,< signifiern, signified k n >, ... }\nHowever, as argued by Pustejovsky [30, chapter 4], this static conception of the lexicon (or Sense Enumerative Lexicon, in Pustejovsky’s terminology) is not adequate to provide an explicative treatment of polysemy in natural languages. Polysemy, as part of the creative use of words (“how words can take an infinite number of meanings in novel contexts” [30, p. 42]), contributes to the expressive power of natural languages, but the static conception is unable to address the question of what the logical relationship among the multiple meanings of a lexeme is. Acording to Pustejovsky’s generative model of the lexicon, there is one basic meaning for an apparently polysemic verb as bake, and “any other readings are derived through generative mechanisms in composition with its arguments”. In this case, the change of state sense of bake is enumerated in the lexicon, but not the creation sense, which is generated when the verb is combined with a DP that denotes an artifact.\n(2) a. John baked a potato [change of state] b. John baked a cake [creation sense]\nTherefore, apparently polysemic lexemes would not be listed as in (1) but as in (3).\n(3) lexemen = {< signifiern, signifiedn >}\nAccordingly, all lexical entries would be a singleton of a pair of a signifier and a signified.\nWe must make a clarification: the lexicon, as a list of lexical entries, is not properly ambiguous or inambiguous. What may be ambiguous or not is a signifier, but not a list of pairs of signifiers and signifieds. According to our reasoning, when two lexemes are homonyms we postulate two different signifiers that are related to two different signifieds, and when a single lexeme is apparently polysemic the lexicon contains a regular entry compounded of a single signifier and a single signified, other signifieds being derived from the basic signified and generative mechanisms. Therefore, following Puestejovsky’s approach, there would be no truly ambiguous lexical entry, i.e., no lexical entry where a single signified is associated to multiple signifieds. Semantic representations generated during the C-I system putatively keep track of the lexical meaning, and build the meaning of complex expressions on the basis of the meaning of lexical items and the syntactic operations that generate them; consequently there is no lexical ambiguity generated during Γinit and Γabs. Lexical ambiguity appears at the externalization system (Γext), more precisely, when a signifier is transferred and a hearer can potentially assign it more than a signified and thus needs to resort to the communicative or to the linguistic context to hypothesize the signified that the speaker intended to convey."
    }, {
      "heading" : "2.2 Syntactic ambiguity",
      "text" : "Ambiguity is not only lexical (when a hearer can potentially assign more than one signified to a signifier), but also syntactic2. Syntactic ambiguity can be relative to syntactic constituency or to chain-formation, as illustrated in (4) and (5) respectively. In (4-a), the PP with Mary modifies the VP talked about your story, whereas in (4-b) it modifies the DP your story. In (5), the copies of\n2For the sake of simplicity, we deliberately omit in this paper the mathematical distinction between ambiguous grammars and ambiguous languages. It is worth noting, however, that the set of well-formed strings of a given language can be generated, a priori, by more than one system of syntactic rules. Ambiguous grammars lead to the so-called structural ambiguity, which refers, roughly speaking, to the possibility of attributing two or more syntactic structures to a given string. Since a given language can be generated by more than one grammar system, one can argue that the presence of structural ambiguity is contingent, because one could hypothetically find another grammar accounting for the studied language without structural ambiguity. This is not, however, the general case. Indeed, it can be shown that some languages are inherently ambiguous, i.e., that all grammars accounting for them are ambiguous. We refer the interested reader to the excellent discussion on this topic provided by [18](pp. 99-103).\nthe lexical item when in the base-position are in bold; in (5-a), when is basegenerated in the VP headed by said and in (5-b) in the VP headed by leave. Intermediate copies of when as well as internal merge operations of syntactic categories other than where are not represented in these very partial analyses.\n(4) I talked about your story with Mary\na. [TP I [V P [V P talked [PP about your story] [PP with Mary] ] ] ] b. [TP I [V P talked [PP about [DP your story [PP with Mary] ] ] ] ]\n(5) I wonder when Mary said John would leave\na. [TP I wonder [CP when Alice [V P said when [CP John would leave] ] ] ] b. [TP I wonder [CP when Alice said [CP John would [V P leave when] ] ] ]\nFollowing standard conceptions, the initial branch Γinit is responsible for the external merge of lexical items and for overt applications of internal merge. Therefore, immediately before the generative procedure Γ splits into Γext and Γabs, there is no ambiguity as for consituency and chain-formation. In (4) and (5) concretely, either objects (a) or objects (b) have been generated by Γinit. There is, though, an apparent asymmetry between Γabs and Γext: whereas in the former the information relative to constituency and chain-formation is preserved, it is lost in the latter, since constituent structures are transformed into strings of morphemes, which do not display any discrete morphophonological correlate to syntactic nodes (or, say, parentheses in first order logic) and, typically, all copies but the head of the chain are deleted in Γext (they are not pronounced). This triggers the appearance of syntactic ambiguities, as those illustrated in (4) and (5), or in (6), which contains an object that can be both a disjunction of a term and a coordination ((6-a)), and a conjunction of a disjunction and a term ((6-b)).\n(6) I’ll call Peter or Mary and John\na. I’ll call [Peter or [Mary and John] ] b. I’ll call [ [Peter or Mary] and John]\nThe apparent asymmetry just noted requires further consideration. Although, typically, there is no morphophonological discrete symbol representing non-final copies of a chain and no discrete symbol representing constituency, experimental research has shown that the presence of syntactic boundaries may be signaled through non-discrete prosodic features such as duration, amplitude and frequency peaks (cfr. [29] and references therein). Let us consider, for concreteness, some evidence in Catalan. It is relatively easy to construct in this language sentences that are both lexically and syntactically ambiguous. In (7), for instance, jove (“young”) can be either a noun or an adjective; if it is a noun ((7)), then veu is a verb (“sees”), l’ an article (“the”) and amenaça a noun (“threat”); if jove is an adjective, then veu is a noun (“voice”), l’ an object clitic and amenaça a verb (“threaten”). The written sentence under discussion can thus have the two different syntactic analysis indicated.\n(7) La jove veu l’amenaça [14]\na. jove: noun veu: verb l’ : article amenaça: noun b. [V P [DP la [N jove]] [V P [V veu] [DP l’amenaça]]] “The young lady sees the threat”\nc. jove: adjective veu: noun l’ : clitic amenaça: verb d. [V P [DP la [AdjP jove [N veu]]] [V l’[V amenaça]]] “The young voice threatens him/her”\nHowever, as formerly noted by Ferrater [14, p. 80], when the sentence illustrated in (7) is pronounced the ambiguity disappears. As observed by Bonet [5], the two different interpretations have different intonational patterns. More precisely, as shown by [29], an intermediate phrase boundary is phonetically marked by a high phrase accent H− placed at the end of a phonological constituent; accordingly, the boundary tone H− is placed after jove when jove is a noun, and thus the\nsubject of the sentence is the DP la jove (“the young lady”), and after veu when jove is an adjective and the subject is la jove veu (“the young voice”).\nPrieto’s perceptual results reveal an interesting fact: when the phonological phrasing is [la jove] [veu l’amenaça], with a phrase boundary after jove, hearers assign it a sole interpretation, namely that where the subject is la jove, as expected, in contrast with the phonological phrasing [la jove veu] [l’amenaça], which is ambiguous: the absence of a phrase boundary between jove and veu is compatible with the two possible syntactic analysis, whereas the presence of such boundary is compatible only with one. This suggests that “linguistic context is the key disambiguation factor in natural speech, and that phrasing decisions, subject to certain constraints, are highly optional” [29, p. 187-188].\nThe following sentences are also brought into consideration [29]:\n(8) La vella llança l’amenaça\na. The old lady threatens him/her b. The old lance threatens him/her\n(9) La vella escolta la veu\na. The old lady listens to the voice b. The old girl scout sees her\n(10) La poderosa crema la casa\na. The powerful lady sets the house on fire b. The powerful cream helps her get married\n(11) El vell guarda la porta\na. The old man guards the door b. The old guard is wearing it (the scarf)\nLet us consider a further complication brought up by (12), an example owed to J. Mascaró (p.c.). In this case, el can be either an article or a clitic; if it is a an article, then pot is a noun (“can”), dur an adjective (“hard”) and cap a verb (“fits”) ((12-a)); if el is a clitic, then pot is an existential modal verb (“can”),\ndur an infinitive verb (“bring”), and cap a a complex preposition (“towards”) ((12-b)). But in (12), the two possible syntactic analysis are mapped into the same intonational phrases((12-c)). Further work should investigate if any other prosodic factors are at work in producing differentiated phonetic forms and to what extent these features are perceived.\n(12) El pot dur cap a l’armari\na. [TP [DP el [NP pot [AdjP [Adj dur]]] [TP cap [PP a l’armari] ] ] “The hard can fits in the cupboard” b. [modalP [modal el pot] [V P [V dur] [PP cap a l’armari] ] ] “He can bring it to the cupboard” c. [IntP2 el pot dur] [IntP1 cap a l’armari]\nTherefore, prosody seems to play an auxiliary role in indicating how words are combined into phrases. The amount of constituency ambiguities is drastically reduced if not only discrete features but also non-discrete prosodic features are taken into consideration. As a consequence, produced phonetic forms may display a higher resolution to this respect than usually thought. Although prosodic patterns are available to avoid the appearence of constituency ambiguities in phonetic forms, they may be used only as optional, and not mandatory, strategies amd perhaps in a gradual way, depending on pragmatic factors. For instance, speakers may take advantage of the continuous nature of prosodic features to minimize their efforts in externalizing an expression by resorting to relatively marked prosodic patterns only to avoid ambiguities that could not be easily solved by taking into consideration the communicative context. In the end, a phonetic form with a relatively high resolution may not be an optimal strategy to avoid the appearence of an ambiguity that could be easily solved on pragmatic grounds. It is reasonable to conjecture that a speaker will measure its effort in externalizing an expression by considering the efforts that the hearer will have to make in order to interpret that expression. It is also important to recall the presence of noise: as noted, produced phonetic forms with an unambiguous phonological phrasing can indeed be perceived as ambiguous by hearers.\nFinally, we must keep in mind that the match between intonational phrases and syntactic phrases is far from trivial, as illustrated in the following well-known\nexample ([9], [32]), where the intonational phrases do not correspond to the syntactic phrases.\n(13) This is the cat that caught the rat that stole the cheese\na. [IP This [V P is [DP the cat [CP that [V P caught [DP the rat [CP that [V P stole [DP the cheese]]]]]]]]] b. [IntP3 This is the cat] [IntP2 that caught the rat] [IntP1 that stole the cheese]\nNeedless to say, more experimental work is still necessary to have a more general and solid understanding of how prosody is used in production and perception to disambiguate otherwise ambiguous sentences, such as those in (4) and (6-b), as well as those in (14), for instance, where a postnominal adjective can be both a noun complement and a secondary predication, thereby differing from languages like English where the postnominal adjective would introduce a secondary predication.\n(14) Ha trobat el llibre interessant\na. He has found the interesting book b. He has found the book interesting"
    }, {
      "heading" : "2.3 Quantifier scope ambiguity",
      "text" : "The existence of quantifier scope ambiguities in natural languages has been an interesting conundrum in the development of a formal theory of the syntaxsemantics interface ([24], [10], [23], among many others). Consider the following two examples.\n(15) Every linguist knows two languages\n(16) Peter believes that one of his friends has betrayed him\nThe sentence in (15) illustrates that in English, as in many other languages, given an object numeral phrase headed by two and a subject distributive phrase headed by every, it is possible that the subject distributive phrase scopes over the object numeral phrase (a scope relation that reflects the apparent constituent ordering), but also that the object numeral phrase scopes over the subject distributive phrase (a scope relation that disagrees with the apparent constituent ordering). The sentence in (16) illustrates a similar phenomenon: the DP his friend can have wide or narrow scope with respect to the propositional attitude verb believe, a lexically intensional element, thereby yielding, respectively, a de re or a de dicto interpretation.\nScope ambiguities in languages like English contain no lexical ambiguity and apparently there are no independent arguments to postulate different syntactic representations and derivations for different scope relations. This poses an obvious difficulty to mantain the Compositionality Principle, understood as follows:\nCompositionality Principle. The meaning of a complex expression is a function of the meaning of its parts and of the syntactic rules by which they are combined. [27, p. 318]\nAs observed by Pelletier [28]:\n”In order to mantain the Compositionality Principle, theorists have resorted to a number of devices which are all more or less unmotivated (except to mantain the Principle): Motagovian “quantifying-in rules”, “traces”, “gaps”, “Quantifier Raising”... features, and many more”.\nAlthough Pelletier’s criticism to Montagovian and Government and Binding approaches may be fair, it seems to us that it does not apply to current minimalist transformational proposals. As observed by Chomsky [7], if we allow Γ to generate hierarchical objects by externally merging elements from Lex into the syntactic workspace but also by internally merging elements that have already been introduced into the syntactic workspace, we can constructively capture the property of displacement, which is apparently ubiquitous in natural languages.\nTraces do not need to be generated, but simply multiple copies. Moreover, the postulation of internal merge operations is not only a matter of applicability but also of conceptual necessity: if external merge is available, only by stipulation could we ban the syntactic procedure to perform internal merge operations, a stipulation that would be unmotivated given our current understanding of syntactic patterns.3\nIf we allow internal merge operations to take place not solely at Γinit but also during Γabs, we obtain the minimalist correlate of quantifier raising. As a consequence, different syntactic derivations yield now different scope relations and the syntax-semantics interface is kept as simple as possible, governed by the Compositionality Principle but without stipulating any unmotivated elements. Internal merge operations responsible for defining quantifier scope are covert in a language like English, which means that they take place during Γabs and that they have no correlate at Γext. Therefore, Π displays also a lower resolution than Λ as for quantifier scope."
    }, {
      "heading" : "2.4 Garden path sentences",
      "text" : "Garden path sentences appear when a substring of words of a given string is in principle ambiguous, but one of its interpretations is more likely than the others. In parsing the whole string, the most likely interpretation of the substring must be abandoned for interpretive reasons, and an alternative interpretation must be found. English sentences (17) and (18) are well-known illustrations of this phenomenon.\n(17) The government plans to raise taxes were defeated\n(18) The horse raced past the barn fell\nIt is relatively easy to construct garden path sentences in English, arguably because of the type of ambiguities allowed in this language, such as those derived\n3We refer the reader to [16] for a set-theoretical definition of internal and external merge as well as of the basic structural relationships.\nfrom the omission of the relativizer, as in (18). In Romance languages, which lack this type of ambiguities, it is not so easy. In (19) we provide an instance of garden path sentence in Catalan [31].\n(19) Hem de redirigir les tendncies cap a la dreta cap a l’esquerra We must redirect the tendencies towards the right towards the left (literal translation)\na. First interpretation or syntactic analysis: Hem de [V P redirigir [DP les tendncies]] [PP cap a l’esquerra] *[PP cap a la dreta] b. Second interpretation or syntactic analysis: Hem de [V P redirigir [DP les tendncies [PP cap a l’esquerra] ] [PP cap a la dreta] ]\nIn the first bracket analysis the PP cap a l’esquerra (“towards the left”) is analyzed as an adjunct of the VP; when the next PP cap a la dreta (“towards the right”) is parsed, the syntactic analysis crashes, since it is not possible to assign this PP any adjunct or argument relationship. In the second brackett analysis the first PP cap a la dreta is an adjunct of the DP les tendncies, and thus the second PP cap a l’esquerra can now be properly interpreted as an adjunct of the VP.\nThe source of these garden path sentences is syntactical. It is also possible to construct garden path sentences in Catalan whose source is phonological (Joan Mascaró, p.c.). Consider the following sentence:\n(20) Els reis Joan Carles i Sofia The kings Joan Carles and Sofia.\nDue to the contextual deletion of the two occurrences of the plurality morpheme -s attached to the article and the noun, (20) is pronounced as\n(21) El rei Joan Carles i Sofia.\nAs a consequence, the string (21) is morphophonologically ambiguous: the hearer can interpret it as the coordination of a singular DP el rei Joan Carles and the noun Sofia, which does not correspond to the intended meaning of the expression before the deletion of the two final occurrences of the plurality morpheme -s. In order to arrive at the intented meaning, the hearer needs to reconstruct the two deleted -s, thereby obtaining a plural DP that contains two coordinated nouns (i.e., Els reis Joan Carles i Sofia).\nThe existence of garden path sentences could be viewed as a sign of the poor design of Language for communication, provided that it poses parsing difficulties. However, garden path sentences derive from the possibility of having ambiguous strings of words and from the non-uniform probability among the interpretations of a string. That probabilities among interpretations may be non-uniform comes for free; if the property of strings of being ambiguous can be derived from efficient communication considerations -as we shall argue-, then we can safely conclude that garden path sentences are not a sign of the poor design of Language for communication."
    }, {
      "heading" : "2.5 Concluding remarks on the locus of ambiguity",
      "text" : "Before concluding this section we want to raise a further issue. It is important to distinguish ambiguous expressions from vague expressions. Whereas an ambiguous expression is an expression with more than one meaning, a vague expression is an expression whose meaning is imprecise or vague. Some clear examples of vague predicates in natural languages are the English adjectives tall, young, cold, rich and smart. We consider them vague for there are some objects that clearly display these properties, others that do not display them whatsoever, and others that display them or that do not display them ’to a certain degree’. In general, a predicate P is vague iff there is some object x for which it is impossible to determine whether or not x satisfies P.\nMany predicates in natural languages are vague, and do not adhere to the tertium non datur law, according to which, for a given proposition p, either the formula ’p’ or the formula ’not p’ is true. Whereas mathematical reasoning characteristically adheres to the tertium non datur law, most non-mathematical reasonings are full of items that denote vague predicates for which it is not\ndetermined whether or not some objects satisfy them.4\nFor its ubiquity in natural languages and human reasoning, it is crucial to have a good mathematical characterization of vagueness, not only for linguistics (how should we represent the lexical meaning of vague terms and their contribution in the meaning of complex expressions?), but also for logic (how should we characterize a correct reasoning involving vague terms?) and artificial intelligence (how can we efficiently implement vague reasonings in artificial intelligence systems?).5 However, we emphasize that vagueness is not under the scope of the communicative argument we are about to develop, which is concerned strictly with ambiguity. The existence of vague terms is not related at all to the factors of efficient communication we will present. We also want to note that the deictic words such as the noun today, the adverb here or the pronoun I are not instances of ambiguity, and thus are unrelated to the course we shall develop. The meaning of these words is unambiguosly connected to the speech act in such a way that they always mean, respectively, “the day of the speech act”, “the place of the speech act” and “the logophoric agent of the speech act”.\nSo far we have reviewed certain types of ambiguity that are familiar to grammarians: lexical ambiguity, syntactic ambiguity, and scope ambiguity. We have considered further elementary distinctions: lexical ambiguity can be divided into (total and partial) homonymy and polysemy, and syntactic ambiguity into constituency ambiguity and chain-formation ambiguity; we have also analyzed several Catalan constructions that display at the same lexical ambiguities and constituency ambiguities. The general claim of this section is that ambiguity appears at the externalization branch (Γext) of the generative procedure. More precisely:\n4A well-known and interesting property of vague predicates is that they give rise to the Sorites paradox, which can be illustrated by the following reasoning:\n• p0: A man who has no euro is poor • pi → pi+1: A man who is poor and earns one euro is still poor • p1000000: Therefore, a man who has one million euros is poor.\nThis reasoning is compounded of the atomic proposition p0 and the implication pi → pi+1, which states that for all situations pi in which a man is poor, he remains poor in pi+1 after earning one euro. This reasoning is thus the iterated application of a modus ponens 1000000 times. Although the argument is correct and the two premises are true, the conclusion p1000000 is admittedly false.\n5We refer to [25] for a very interesting presentation of vagueness and fuzzy logic.\n1. Lexical ambiguity. Ambiguity relative to lexemes appears at the externalization branch, when a hearer needs to resort to the communicative or the linguistic context to choose one meaning among multiple meanings that can potentially be associated to a morphophonological form.\n2. Syntactic ambiguity and prosody. Prosodic patterns play an ancillary role in avoiding the emergence of constituency ambiguities in the production of phonetic forms by indicating the limits between constituents; although some phonetic forms are unambiguous as for phrasing in production, they are ambiguous in perception. Chain-formation ambiguities are triggered by the deletion of base-generated copies at Γext.\n3. Quantifier scope ambiguity. Internal merge, a virtual conceptual necessity that is constructively used to analyze displacement patterns in general, can be naturally applied to represent inverse scope relations, without needing to stipulate any unmotivated elements. Scope ambiguities appear because internal merge operations responsible for defining inverse scope relations take place at Γabs, and thus have no effect on Γext.\nAfter locating the emergence of ambiguity in the architecture of Language, we are now ready to investigate what factors relative to the externalization of linguistic expressions require a certain loss of that information relevant for the C-I system, in order to understand why the generative procedure opts for reducing the resolution of Π with respect to Λ in connecting the C-I system and the A-P system. This leads us to move from grammar-internal considerations to the study of general conditions on how two agents communicate efficiently. Accordingly, we shall present in the following section a rigorous and general framework based on fundamental concepts of computation theory and information theory.6\n6Both the computation-theoretic and information-theoretic concepts used in the following section are standard and can be found in any basic textbook on these subjects. We refer the reader interested in computation-theoretic concepts to the classical references [18, 13, 21, 26]; and to [1, 12] for an introduction to information theory."
    }, {
      "heading" : "3 Ambiguity and logical reversibility: a rigorous",
      "text" : "treatment\nIn this section we begin by presenting the concept of Logical (ir)reversibility of a given computation. Subsequently we formally explain how a code generated through logically irreversible computations is necessarily ambiguous and we quantify the degree of ambiguity as the minimum amount of information needed to properly reconstruct a given message."
    }, {
      "heading" : "3.1 Logical irreversibility",
      "text" : "The concept of computation is theoretically studied as an abstract process of data manipulation and only its logical properties are taken into consideration; however, if we want to investigate how an abstract computation is realized by a physical system, such as an electronic machinery, an abacus or a biological system as the brain, it becomes important to consider the connections between the logical properties of (abstract) computations and the physical -or more precisely, thermodynamical- properties of the system that perform those computations.\nThe fundamental examination of the physical constraints that computations must satisfy when they are performed by a physical system was started by [20], and continued in several other works [2, 4, 19, 3, 33]. The general objective of these approaches is to determine the physical limits of the process of computing, the “general laws that must govern all information processing no matter how it is accomplished” [4, pg.48]. Therefore, the concept of computation is subject to the same questions that apply to other physical processes, and thus the following questions become central to the physical study of computational devices [4, p. 48]:\n1. How much energy must be expended to perform a particular computation?\n2. How long must it take?\n3. How large must the computing device be?\nA central objective is to study the reversibility/irreversibility of a computational process.7 Let us thus introduce the concept of logical reversibility/irreversibility, which is crucial to our concerns.\nAs remarked by [4], no computation ever generates information since the output is implicit in the input. For instance, if we consider the operation of adding (+)\n7The problem of reversibility/irreversibility of a computational process was first proposed in relation to the problem of heat generation during such a process. In this way, it was postulated that any irreversible computation generates an amount of heat -the so-called Landauer’s principle. We refer the interested reader to [4]. See also A.\ndefined on the set of natural numbers N, then the expression +(2, 3) contains its output, 5; accordingly, we say that the output 5 is implicit in the input (2, 3) of the operation + defined on N, in which case all the information contained in the output is contained in the input. However many operations destroy information whenever two previously distinct situations become indistinguishable, in which case the input contains more information than the output. If we consider again the operation + defined on N, the output 5 can be obtained from the following inputs: (0, 5), (1, 4), (2, 3), (3, 2), (4, 1), (5, 0). The concept of logical irreversibility is introduced in Landauer [20, pg. 264] in order to study those computations for which its input cannot be unequivocally determined from its output:\n“We shall call a device logically irreversible if the output of a device does not uniquely define the inputs”.\nConversely, a device is logically reversible if its output can be unequivocally defined from the inputs -see figure (2)."
    }, {
      "heading" : "3.2 Logical (ir)reversibility in terms of Turing machines",
      "text" : "In this section we rigorously define logically (ir)reversible computations. To this end, we must present an abstract computing device, i.e., a Turing machine. A Turing machine is an ideal calculus algorithm defined by [34]. Turing machines seem to constitute a stable and maximal class of computational devices in terms of the computations they can perform. Although Turing machines are rather primitive and simple, they are capable of expressing any algorithm and of simulating any programming language, and indeed, any operation that a modern computer can perform can be simulated by a Turing machine. In fact, it is widely accepted that any way of formalizing the intuitive idea of what is computable must be equivalent to a Turing machine, a conjecture commonly known as Church-Turing thesis [13].\nWe informally describe the action of a Turing machine on an infinite tape divided into squares as a sequence of simple operations that take place after an initial moment. At each step, the machine is at a particular internal state and its head\nexamines a square of the tape (i.e., reads the symbol written on a square). The machine subsequently writes a symbol on that square, changes its internal state and moves the head one square leftwards or rightwards or remains at the same square.\nFormally, a Turing machine T is compounded of a finite set of internal states Q, a finite set of symbols Σ (an alphabet) and a transition function δ:\nT = (Q,Σ, δ).\nThere is an initial state s belonging to Q and in general Q∩Σ = ∅. Two special symbols, the blank t and the initial symbol B, belong to Σ. Additionally δ is a transition function that takes as input an ordered pair and yields as output a triple. The first component of the ordered pair taken as input can be any member of Q and the second component any element of Σ; therefore, the domain of δ is the Cartesian product Q× Σ. The first component of the triple yielded as output is any member of Q, the second component any member of Σ and the third component any member of {L,R, }, where ’L’ and ’R’ mean, respectively, “move the head one square leftwards or rightwards”, and ’ ’ means “stay at the square just examined”. Accordingly, the range of δ is the Cartesian product Q× Σ× {L,R, }. We succintly define δ as usual:\nδ : Q× Σ→ Q× Σ× {L,R, } .\nThus, δ is the program of the machine; it specifies, for each combination of current state σi ∈ Q and current symbol rk ∈ Σ, a triple\n〈σj , r`,D〉,\nwhere σj is the step immediately after σi, r` is the symbol to be overwritten on rk, and D ∈ {L,R, }. A schema of how a computation is performed in a Turing machine is shown in figure (3).\nIn general, we say that T performs logically reversible computations if the inverse function of δ, δ−1, defined as\nδ−1 : Q× Σ× {L,R, } → Q× Σ,\nexists. This implies that for any input we have a different output and, therefore, we can invert the process for every element of the input set. The non-existence\nof δ−1 is due to the fact that\n(∃α, β ∈ Q× Σ) : ((α 6= β) ∧ (δ(α) = δ(β) ∈ Q× Σ× {L,R, })).\nTherefore, from the knowledge of γ = δ(α) = δ(β) we cannot determine with certainty the actual value of the input, either α or β -see figure (2) for a simple example. In these cases, we say that T performs logically irreversible computations.\nAfter these general definitions, we shall provide a particular definition of a Turing machine suitable for the study of the coding process. This coding machine will be compounded of a set of internal states Q, a transition function δ and two alphabets -an input alphabet Ω = {m1, ...,mn} and an output alphabet S = {s1, ..., sm}: T = (Q,Ω, S, δ). We shall call the elements of Ω referents and the elements of S signs. We assume that Q ∩ S = ∅ and that Q ∩ Ω = ∅. For simplicity, we also assume that, in a coding process, the two alphabets are disjoint,\nΩ ⋂ S = ∅,\ni.e., an object is either a sign or a referent, but not both. Technically, this implies that T can never reexamine a square where a sign has been printed, which\nmeans that T must move always in the same direction; assume for concreteness that it must move rightwards. In order to clearly identify input and output configurations, we express the applications of δ in the following terms:\nσkmi → σjs`R,\nwhere σkmi and σjs`R, (σk, σj ∈ Q, mi ∈ Ω, s` ∈ S) are respectively input and output configurations.\nFor the sake of simplicity, we will consider only deterministic Turing machines, i.e., those machines for which there is a different input configuration for each different output configuration. That a coding machine is deterministic means that there are no properly synonimous signs that encode the same referent. A determistic Turing machine will be logically reversible if each output configuration is obtained from only one input configuration, and irreversible if more than one input configurations yields a single output configuration. In other words, T is ambiguous iff its δ is not injective. A logically irreversible coding machine generates, thus, ambiguous signs, for which there is not a unique referent.\nAs shown by [2], a logically irreversible Turing machine can always be made logically reversible at every step. Thus, logical irreversibility is not an essential property of computation. It is crucial for our concerns that a logically reversible machine need not be much more complicated than the irreversible machine it is associated with: computations on a reversible machine take about twice as many steps as on an irreversible machine and require a particular amount of temporary storage [2, 4]. Therefore, the study of the complexity of the computations of the coding device alone does not seem to offer a necessity argument for the emergence of ambiguity but only a relatively weak plausibility argument, provided that reversible computations are not significantly more complex than irreversible computations. Here we shall show that ambiguity must appear when a coding machine interacts with a decoding machine in an optimal way."
    }, {
      "heading" : "3.3 Logical reversibility and ambiguous codes",
      "text" : "Before proceeding further in studying the concepts of logical (ir)reversibility in a communication system formed by two agents (a coder and a decoder) and the channel, some clarifications are in order. Firstly, we note that logical reversibil-\nity refers to the potential existence of a reconstruction or decoding algorithm, which does not entail that, in a real scenario, such algorithm is at work; in other words, logical (ir)reversibility is a feature of the computations alone. Secondly, in the process of transmitting a signal through a channel, the presence of noise in the channel through which the output is received may be responsible for the emergence of logical irreversibility. This implies that, although the coder agent could in principle compute in a reversible regime, the noise of the channel makes the cascade system coder agent + channel analogue to a single computation device working in an irreversible regime. And finally, whereas logical (ir)reversibility is a property of the computational device (or coding algorithm) related to the potential existence of a reconstruction algorithm (or decoding algorithm), ambiguity is a property referred to signs: we say that a sign (an output of a coding computation) is ambiguous when the decoder can associate it with more than one referent (or input). A sign transmitted through a channel is ambiguous if the cascade coder agent + channel is logically irreversible, which may be due to the computations of the coding agent itself or due to the noise of the channel."
    }, {
      "heading" : "3.3.1 Noise: quantifying the degree of ambiguity",
      "text" : "The minimal amount of additional information needed to properly reconstruct the input from the knowledge of the output is identified as the quantitative estimator of ambiguity. The more additional information we need, the more ambiguous the code is. This minimal amount of dissipated information is known as noise in standard information theory, and its formulation in terms of the problem we are dealing with is the objective of the following two subsections.\nTo study logical irreversibility in information-theoretical terms, we choose a simple version of the transition function δ\nδ : Ω→ S.\nThis choice puts aside the role of the states but is justified for the sake of clarity and because the qualitative nature of the results does not change: the only changes are the sizes of the input and output sets. Let δij be a matrix by which\nδij = { 1⇔ δ(mi) = sj 0 otherwise.\nSince the machine is deterministic, there is no possibility to have two outputs for a given input, therefore\n(∀k ≤ n)(∃!i ≤ m) : [(δki = 1) ∧ (∀j 6= i)(δkj = 0)] .\nTo properly study the reversibility of the above coding machine, let us define two random variables, XΩ, Xs. XΩ takes values on the set Ω following the probability measure p, being p(mk) the probability to have symbol mk as the input in a given computation. Essentially, Ω describes the behavior of a fluctuating environment. Xs takes values on S and follows the probability distribution q, which for a given si ∈ S, takes the following value:\nq(si) = ∑ k≤n p(mk)δki,\ni.e., the probability to obtain symbol si as the output of a computation. The amount of uncertainty in recovering the inputs from the knowledge of the outputs of the computations performed by T is related to the logical irreversibility.\nIn fact, this amount of uncertainty is precisely the amount of extra information we need to introduce to have a non-ambiguous code. This amount of conditional uncertainty or extra information needed is well defined by the uncertainty function or Shannon’s conditional entropy8:\nH(XΩ|Xs) = − ∑ k≤m q(sk) ∑ i≤n P(mi|sk) logP(mi|sk), (1)\nwhere, by virtue of Bayes’ theorem,\nP(mi|sk) = ∑ i≤n δik −1 δik. Equation (1) is the amount of noise, i.e., the information that is dissipated during the communicative exchange or, conversely, the (minimum) amount of information we need to externally provide to the system in order to perfectly reconstruct the input."
    }, {
      "heading" : "3.3.2 Ambiguity and logical irreversibility",
      "text" : "The interpretation we provided for the noise equation enables us to rigorously connect ambiguity and logical (ir)reversibility. First, we emphasize a crucial fact: by the properties of Shannon’s entropy,\nH(XΩ|Xs) ≥ 0, which explicitly states that information can be either destroyed or mantained but never created in the course of a given computation -as pointed out in [4].\nIf there is no uncertainty in defining the input signals by the only knowledge of the outputs, then H(XΩ|Xs) = 0, i.e., there is certainty when reversing the computations performed by the coding machine. Therefore, the computations performed by T to define the code are logically reversible and the code is not ambiguous. Otherwise, if\nH(XΩ|Xs) > 0, 8Throughout the paper, log ≡ log2.\nthen, we need extra information (at least H(XΩ|Xs)) to properly reverse the process, which indicates that the computations defining the code are logically irreversible and, thus, that the code is ambiguous.\nWe therefore identified in a quantitative and rigorous way the ambiguity of the code with the amount of uncertainty of the reversal of the coding process or the minimal amount of additional information we need to properly reverse the coding process. Furthermore, we clearly identified the source of uncertainty through a rigorous concept, namely, logical irreversibility, which is a feature of the computations generating the code. In this way, we establish the following correspondences:\nlogically reversible computations ⇔ No Ambiguity ⇔ H(XΩ|Xs) = 0\nlogically irreversible computations ⇔ Ambiguity ⇔ H(XΩ|Xs) > 0 Amount of Ambiguity = H(XΩ|Xs).\nNow that we rigorously defined ambiguity on solid theoretical grounds of computation theory and information theory, we are ready to explain why it appears in natural communication systems. As we shall see in the following section, the reason is that natural systems must satisfy certain constraints that generate a communicative tension whose solution implies the emergence of a certain amount of ambiguity."
    }, {
      "heading" : "4 The emergence of ambiguity in natural com-",
      "text" : "munication\nThe tension we referred to at the end of the last section was postulated by the linguist G. K. Zipf [36] as the origin of the widespread scaling behavior of word appearance having his name. Such a communicative tension was conceived in terms of a balance between two opposite forces: “the speaker’s economy force” and the “auditor’s economy force”."
    }, {
      "heading" : "4.1 Zipf’s hypothesis",
      "text" : "Let us thus informally present Zipf’s vocabulary balance between two opposite forces, “the speaker’s economy force” and the “auditor’s economy force” [36, pp. 19-31]. The speaker’s economy force (also called Unification Force) is conceived as a tendency “to reduce the size of the vocabulary to a single word by unifying all meanings”, whereas the auditor’s economy force (or Diversification Foce) “will tend to increase the size of a vocabulary to a point where there will be a distinctly different word for each different meaning”. Therefore, a conflict will be present while trying to simultaneously minimize these two theoretical opposite forces, and the resulting vocabulary will emerge from a cooperative solution to that conflict. In Zipf’s words,\n“whenever a person uses words to convey meanings he will automatically try to get his ideas across most efficiently by seeking a balance between the economy of a small wieldy vocabulary of more general reference on the one hand, and the economy of a larger one of more precise reference on the other, with the result that the vocabulary of n different words in his resulting flow of speech will represent a vocabulary balance between our theoretical Forces of Unification and Diversification” [36, p. 22].\nObviously the Unification Force ensures a minimal amount of lexical ambiguity, since it will require some words to convey more than one meaning, and the Diversification Force constrains such amount. Thus, lexical ambiguity can be viewed as a consequence of the vocabulary balance. Although Zipf’s vocabulary balance, as stated, provides a useful intuition to understand the emergence of lexical ambiguity by emphasizing the cooperative strategy between communicative agents, it lacks the necessary generality to provide a principled account for the origins of ambiguity beyond the particular case of lexical ambiguity. In the following sections we shall present several well-known concepts in order to generalize Zipf’s informal condition and provide solid fundations for it.\nWe remark that Zipf conceived the vocabulary balance as a particular case of a more general principle, the Least Effort Principle, “the primary principle that governs our entire individual and collective behaviour of all sorts, including the behaviour of our language and preconceptions” [36, p. 22]. In Zipf’s terms,\n“the Principle of Least Effort means, for example, that a person in solving his immediate problems will view these against the background of his probable future problems as estimated by himself. Moreover he will strive to solve his problems in such a way as to minimize the total work that must be expend in solving both his immediate problems and his probable future problems. That in turn means that the person will strive to minimize the probable average rate of his work-expenditure (over time). And in so doing he will be minimizing his effort, by our definition of effort. Least effort, therefore, is a variant of least work.”\nHence, we consider the symmetry equation between the complexities of the coder and the decoder we shall arrive at to be a particular instance of the Least Effort Principle."
    }, {
      "heading" : "4.2 Symmetry in coding/decoding complexities",
      "text" : "How can we accommodate the previous intuitions to the rigorous framework proposed in section 3? The auditor’s economy force leads to a one-to-one mapping between Ω and S. In this case, the computations performed by T to generate the code are logically reversible and thus generate an unambiguous code, and no supplementary amount of information to successfully reconstruct XΩ is required. However, The speaker’s economy force conspires exactly in the opposite direction. In these latter terms, the best option is an all-to-one mapping, i.e., a coding process where any realization of XΩ is coded through a single signal. Such a coding is logically irreversible and therefore totally ambiguous, for it is clear that the knowledge of the output tells us nothing about the input. In order to characterize this conflict, let us properly formalize the above intuitive statement: The auditor’s force pushes the code in such a way that it is possible to reconstruct XΩ through the intermediation of the coding performed by T . Therefore, the amount of bits the decoder of Xs needs to unambiguously reconstruct XΩ is\nH(XΩ, Xs) = − ∑ i≤n ∑ k≤n P(mi, sk) logP(mi, sk),\nwhich is the joint Shannon entropy or, simply, joint entropy of the two random variables XΩ, Xs [12]. From the codification process, the auditor receives H(Xs) bits, and thus, the remaining uncertainty it must face will be\nH(XΩ, Xs)−H(Xs) = H(XΩ|Xs),\nwhere H(Xs) = − ∑ i≤n q(si) log q(si),\n(i.e, the entropy of the random variable Xs) and H(XΩ|Xs) = − ∑ i≤n q(si) ∑ k≤n P(mk|si) logP(mk|si),\nthe conditional entropy of the random variable XΩ conditioned to the random variable Xs. At this point Zipf’s hypothesis becomes crucial. Under this interpretation, the tension between the auditor’s force and the speaker’s force is cooperatively solved by imposing a symmetric balance between the efforts associated to each communicative agent: the coder sends as many bits as the additional bits the decoder needs to perfectly reconstruct XΩ:\nH(Xs) = H(XΩ|Xs). (2)\nThis is the symmetry equation governing the communication among cooperative agents when we take into account computational efforts -which have been associated here with the entropy or complexity of the code9. Selective pressures will push H(Xs) and, at the same time, by equation (2), the amount of ambiguity will also grow, as a consequence of the cooperative nature of communication.10\nEquation (2) specifies that a certain amount of information must be lost (or equivalently, a certain amount of ambiguity must appear) if coder and decoder\n9We shall be aware of the use of the word complexity. In the context of this section, complexity has to be understood in the sense of Kolmogorov complexity. Given an abstract object, such a general complexity measure is the length, in bits, of the minimal program whose execution in a Universal Turing machine generates a complete description of the object. In the case of codes where the presence of a given signal is governed by a probabilistic process, it can be shown that Kolmogorov complexity equals (up to an additive constant factor) the entropy of the code [12].\n10Equations of this kind have been obtained in the past through different approraches [17, 15].\nmiminize their effors in a symmetric scenario. A further question is how much information is lost due to equation (2). In order to measure these amount of information, we must take into consideration the properties of the so-called Shannon Information or Mutual Information among the two random variables Xs, XΩ, to be written as I(Xs : XΩ). In our particular case, such measure quantifies the amount of information of the input set by the only knowledge of the output set after the computations. Consistently [12, 1],\nI(Xs : XΩ) = H(XΩ)−H(XΩ|Xs). (3)\nAn interesting property of Shannon information is its symmetrical behavior, i.e., I(Xs : XΩ) = I(XΩ, Xs). Thus, by equation (3),\nH(XΩ)−H(XΩ|Xs) = H(Xs)−H(Xs|XΩ),\nwhere H(Xs|XΩ) = 0, because the Turing machine is deterministic11. Therefore, by applying directly equation (2) to the above equation we reach the following identity:\nH(Xs) = 1\n2 H(XΩ). (4)\nThus,\nI(Xs : XΩ) = H(XΩ)−H(XΩ|Xs) by eq. (2) = H(XΩ)−H(Xs) by eq. (4) = 1\n2 H(XΩ).\nThe above derivation shows that half of the information is dissipated during the communicative exchange if coding and decoding computations are symmetrically or cooperatively optimized.12 Accordingly, an amount of ambiguity must\n11Notice that, if the Turing machine is deterministic, every input generates one and only one output. The problem may arise during the reversion process, if the computations are logically irreversible.\n12This derivation has been performed by assuming that there is no noise affecting the process of output set observation. If we assume the more realistic situation in which there is noise in the process of output observation, the situation is even worse, and, actually, I(Xs : XΩ) = 1 2 H(XΩ) would be considered as an upper bound; therefore, in presence of noise in the process of output observation, this equation must be replaced by:\nI(Xs : XΩ) < 1\n2 H(XΩ).\nemerge. Ambiguity is not an inherent imperfection of a communication system or a footprint of poor design, but rather a property emerging from conditions on efficient computation: coding and decoding computations have a cost when they are performed by physical agents and thereby it becomes crucial to minimize the costs of coding and decoding processes. Whereas studying the process of an isolated coding agent would not provide a necessity argument for the emergence of ambiguous codes (as noted in section 3.2, following [2]), a formalization of an appropriately general version of Zipf’s intuitions along the course we developed provides a solid and general necessity argument for the emergence of ambiguity."
    }, {
      "heading" : "5 Discussion",
      "text" : "In this article we have argued that ambiguity appears at the externalization branch of Language. We have substantiated this claim by reviewing certain familiar types of ambiguity: lexical ambiguity, syntactic ambiguity and quantifier scope ambiguity. We have constructed a communicative argument based on fundamental concepts from computation theory and information theory in order to understand the emergence of ambiguity in the externalization branch, or in other words, why the generative procedure opts for reducing the resolution of phonetic forms with respect to logical forms in connecting the A-P system and the C-I system.\nWe have rigorously identified the source of ambiguity in a code with the concept of logical irreversibility in such a way that a code is ambiguous when the coding process performs logically irreversible computations. Provided that logical irreversibility is not an essential property of computations and that a logically reversible machine need not be much more complicated than the logically irreversible machine it simulates, we have inquired into how a coding machine interacts with a decoding machine in an optimal way in order to identify the source of ambiguity. We have rigorously quantified the ambiguity of a code in terms of the amount of uncertainty of the reversal of the coding process, and we have subsequently formulated the intuition that coder and decoder cooperate in order to minimize their efforts in terms of a symmetry equation that forces the coder to send only as many bits as the additional bits the decoder needs to perfectly reconstruct the coding process. Given the symmetric behaviour of\nShannon information it has been possible to quantify the amount of ambiguity that must emerge from the symmetry equation regardless the presence of noise in the channel: at least a half of the information is dissipated during the communicative process if both the coding and the decoding computations are cooperatively minimized. As noted explicitly in A the presence of ambiguity asociated to a computational process realized by a physical system seems as necessary as the generation of heat during a thermodynamical process.\nThe interest of the symmetry equation from which we derive a certain amount of ambiguity in natural languages is further corroborated in [11]. In this study it is shown that Zipf’s law emerges from two factors: a static symmetry equation that solves the tension between coder and decoder (namely, our symmetry equation 2) and the path-dependence of the code evolution through time, which is mathematically stated by imposing a variational principle between successive states of the code (namely, kullback’s Minimum Discrimination of Information Principle). We thus conclude this study by emphasizing the importance of the symmetry equation for the understanding of how communicative efficiency considerations shape linguistic productions."
    }, {
      "heading" : "A Appendix: Ambiguity and physical irreversibil-",
      "text" : "ity\nThroughout the paper we highlighted the strict relation between the logical irreversibility of the computations generating a given code and the ambiguity of the latter. Now we highlight the formal equivalence of the mathematical treatment we proposed to deal with ambiguity at the theoretical level with the mathematical formulation of physical/thermodynamical irreversibility. The strict relation of thermodynamic irreversibility and logical irreversibility is a hot topic of debate since the definition of the equivalence of heat and bits by R. Landauer [20]. This equivalence, known as Landauer’s principle, states that, for any erased bit of information, a quantity of\nkT ln 2\njoules are dissipated in terms of heat, being k the Boltzmann constant and T the temperature of the system.\nThis principle relates logical irreversibility and thermodynamical irreversibility. Thermodynamical irreversibility is a property of abstract processes. Almost all processes taking place in our everyday life are irreversible. One of the consequences of this irreversibility is the degradation of the energy. The common property of such processes is that they generate thermodynamical entropy. The second law of thermodynamics states that any physical process generates a nonnegative amount of entropy, i.e., for the process P,\n∆S(P) ≥ 0.\nThe units of physical entropy are nats instead of bits. Now suppose that we face the problem of reversing the process P -for example, a gas expansion- by which ∆S(P) > 0. Without further help, the reversion of this process is forbidden by the second law: It would generate a net amount of negative entropy. Therefore, we will need external energy to reverse the process. In the same way, we have seen that H(XΩ|Xs) ≥ 0, which means that information cannot be created during an information process. A negative amount of H(XΩ|Xs) would imply, by virtue of equation (3), a net creation of information. Therefore, we face the same problem. Indeed, if we have a computational process C by which HC(XΩ|Xs) > 0, the reversion of such a process, with no further external help, would be a process by which the computations would generate information. The reversion, as we have discussed above, is only possible by the external addition of information.\nThus the information flux can only be mantained (in the case where all computations are logically reversible) or degraded, and the same applies for the energy flux: by the second law, the energy flux can only be mantained (in the case of thermodynamically reversible processes) or degraded. We can go further. If Q(P) is the heat generated during the physical process, physical entropy is defined as\n∆S(P) = Q(P)\nT .\nFurthermore, if we consider an ideal computational process C we know, from Landauer’s principle, that\nQ(C) = kT ln 2× erased bits.\nAnd we actually know how many bits have been erased -or dissipated. Exactly H(XΩ|Xs) bits. Therefore, the physical entropy generated by this ideal, irreversible computing process will be:\n∆S(C) = k ln 2HC(XΩ|Xs).\nTherefore, logically irreversible computations are thermodynamically irreversible.\nWith this short exposition we emphasize the general character of logical irreversibility and ambiguity in natural communication systems. More than an imperfection, ambiguity seems to be, for natural communication systems, a feature as unavoidable as the generation of heat during a thermodynamical process."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We would like to thank the members of the Centre de Lingǘıstica Teòrica that attended the course on ambiguity for postgraduate students we taught within The PhD Program on Cognitive Scicence and Language (fall semester, 2010). The development of many of the ideas contained in this article has benefited from the multiple questions, observations and critical remarks they raised. We are especially grateful to J. Mascaró and to E. Bonet for their help with the phonological aspects of ambiguity, and to M. T. Espinal for many interesting discussions during the elaboration process of this study. We finally express our gratitude to Adriana Fasanella, Carlos Rubio, Francesc Josep Torres and Ricard Solé for carefully reading a first version of this article and providing us with multiple improvements. This work has been supported by the Spanish MCIN Theoretical Linguistics 2009SGR1079 (JF) and the James S. McDonnell Foundation (BCM)."
    } ],
    "references" : [ {
      "title" : "Least effort and the origins of scaling in human language",
      "author" : [ "R. Ferrer-i-Cancho", "R.V. Solé" ],
      "venue" : "Proc. Natl. Acad. Sci. USA",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2003
    }, {
      "title" : "Some formal considerations on the generation of hierarchically structured expressions",
      "author" : [ "J. Fortuny", "B. Corominas-Murtra" ],
      "venue" : "Catalan Journal of Linguistics",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2009
    }, {
      "title" : "Maximum entropy fundamentals, Entropy",
      "author" : [ "P. Harremoës", "F. Topsøe" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2001
    }, {
      "title" : "Introduction to Automata Theory, Languages and Computation, Addison-Wesley",
      "author" : [ "J. Hopcroft", "J. Ullman" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1979
    }, {
      "title" : "The connection between logical and thermodynamic irreversibility, Studies In History and Philosophy of Science Part B: Studies",
      "author" : [ "J. Ladyman", "S. Presnell", "A. Short", "B. Groisman" ],
      "venue" : "In History and Philosophy of Modern Physics",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2007
    }, {
      "title" : "Irreversibility and heat generation in the computing process, IBM",
      "author" : [ "R. Landauer" ],
      "venue" : "Journal of Research and Development",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1961
    }, {
      "title" : "Elements of the Theory of Computation, Prentice Hall PTR Upper Saddle River, NJ, USA",
      "author" : [ "H. Lewis", "C. Papadimitriou" ],
      "venue" : null,
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1997
    }, {
      "title" : "Linguistic semantics: an introduction, Cambridge Univ Pr",
      "author" : [ "J. Lyons" ],
      "venue" : null,
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1995
    }, {
      "title" : "Logical Form: Its structure and derivation",
      "author" : [ "R. May" ],
      "venue" : null,
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1985
    }, {
      "title" : "Formal philosophy: selected papers of richard montague, edited and with an introduction by Richmond",
      "author" : [ "R. Montague" ],
      "venue" : null,
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 1974
    }, {
      "title" : "Un apropament matemtic al problema de la vaguetat",
      "author" : [ "C. Noguera i Clofent" ],
      "venue" : "Butllet de la Societat Catalana de Matemtiques",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2008
    }, {
      "title" : "Mathematical methods in linguistics, Springer",
      "author" : [ "B. Partee", "A. Ter Meulen", "R. Wall" ],
      "venue" : null,
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 1990
    }, {
      "title" : "Some issues involving internal and external semantics, The logical foundations of cognition",
      "author" : [ "F. Pelletier" ],
      "venue" : null,
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 1993
    }, {
      "title" : "Prosodic manifestation of syntactic structure in Catalan, Issues in the phonology and morphology of the major Iberian languages",
      "author" : [ "P. Prieto" ],
      "venue" : null,
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 1997
    }, {
      "title" : "The generative lexicon, MIT press",
      "author" : [ "J. Pustejovsky" ],
      "venue" : null,
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 1995
    }, {
      "title" : "Introducció a la sintaxi, Ms",
      "author" : [ "J. Rosselló" ],
      "venue" : "Universitat de Barcelona",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2008
    }, {
      "title" : "On derived domains in sentence phonology",
      "author" : [ "E. Selkirk" ],
      "venue" : "Phonology yearbook",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 1986
    }, {
      "title" : "Reversible computing, Automata, Languages and Programming",
      "author" : [ "T. Toffoli" ],
      "venue" : null,
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 1980
    }, {
      "title" : "On computable numbers, with an application to the Entscheidungsproblem",
      "author" : [ "A. Turing" ],
      "venue" : "Proceedings of the London Mathematical Society",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 1937
    }, {
      "title" : "Human behavior and the principle of least effort: An introduction to human ecology",
      "author" : [ "G. Zipf" ],
      "venue" : null,
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 1965
    } ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "Ambiguity is an indisputable and ubiquitous feature of linguistic products. In this article we investigate both the locus of ambiguity in the architecture of Language and the origin of ambiguity in natural communication systems. We locate ambiguity at the externalization branch of Language and we study the emergence of ambiguity in communication through the concept of logical irreversibility and within the framework of Shannon’s information theory. This leads us to a precise and general expression of the intuition behind Zipf’s vocabulary balance in terms of a symmetry equation between the complexities of the coding and the decoding processes that imposes an unavoidable amount of logical uncertainty in natural communication. Accordingly, the emergence of irreversible computations is required if the complexities of the coding and the decoding processes are balanced in a symmetric scenario, which means that the emergence of ambiguous codes is a necessary condition for natural communication to succeed.",
    "creator" : "LaTeX with hyperref package"
  }
}