{
  "name" : "1609.05632.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "On the adoption of abductive reasoning for time series interpretation",
    "authors" : [ "T. Teijeiro", "P. Félix" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : ""
    }, {
      "heading" : "1 Introduction",
      "text" : "The interpretation and understanding of the behavior of a complex system involves the deployment of a cognitive apparatus aimed at guessing the processes and mechanisms underlying what is observed. The human ability to recognize patterns plays a paramount role as an instrument for pointing at the evidence that should require an explanation, by matching information from observations with information retrieved from memory. Classification naturally arises as a pattern recognition task defined as the assignment of observations to categories.\nLet us first state precisely what is the problem under consideration: we wish to interpret the behavior of a complex system by measuring a physical quantity along time. This quantity is represented as a time series.\nArtificial Intelligence community has devoted a lot of effort on different paradigms, strategies, methodologies and techniques for time series classification. Nonetheless, in spite of the wide range of proposals for building classifiers, either by eliciting domain knowledge or by induction from a set of observations, the resulting classifier behaves as a deductive system. The present work is premised on the assumption that some of the important weaknesses of this approach lie in its deductive nature, and that an abductive approach can address these shortcomings.\nLet us remember that a deduction contains in its conclusions an information that is already implicitly contained in the premises, and thus is truth preserving. In this sense, a classifier ultimately assigns a label or a set of labels to each observation. This label can designate a process or a mechanism of the system being observed, but it is not more than a term that summarizes the premises satisfied by the observations. Conversely, abduction, or inference to the best explanation, is a form of inference that goes from data to a hypotheses that best explains or accounts for the data [15]. Abductive conclusions contain new information, not contained in the premises, capable of predicting new evidence, although they are fallible. Abductions are thus truth widening, and they can make the leap from the language of observations to the language of the underlying processes and mechanisms, responding to the aforementioned problem in a natural way [17]. For example, consider\na simple rule stating that if a patient experiences a sudden tachycardia and a decrease in blood pressure, then we can conclude that is suffering a shock due to the loss of blood volume. From a deductive perspective, loss of blood volume is just a name provided by the rule for the satisfaction of the two premises. However, from an abductive perspective, loss of blood volume is an explanatory hypothesis that expands the truth contained in the premises, enabling to predict additional consequences like for example skin paleness, faintness, dizziness or thirst.\nA classifier is based on the assumption that the underlying processes or mechanisms are mutually exclusive. Superpositions and interactions of two or more processes, modifying their respective observation patterns, are excluded, and they must be represented by a new process, corresponding to a new category, different and usually unrelated to previous ones. Therefore, an artificial casuistry-based heuristics is adopted, increasing the complexity of the interpretation and reducing its adaptability to the variability of observations. In contrast, abduction infers a set of explanations as far as the available evidence does not allow us to identify the best one, and they are not incompatible with each other.\nA better interpretation and understanding of a complex system should involve a larger number of observations, even future ones. Such interpretation is usually organized into a set of abstraction layers, where at each layer the temporal granularity of representation is reduced from below. A classification strategy provides an interpretation as the result of connecting a set of classifiers in cascade. Monotonicity of deduction entails a propagation of errors from the first abstraction layers upwards, narrowing the capability of making a proper interpretation as new abstraction layers are successively added. Following an abductive process instead, at each abstraction layer a best explanation hypothesis is conjectured from the data by the layer or layers below, within the context of information from above. Every observation made by abstracting from raw data upwards is a conjecture, and non monotonicity of abduction supports retracting any observation at any abstraction layer in search for the best global explanation. Thus, bottom-up and top-down processing complement one another and jointly provide a result.\nIn a classifier the truth of the conclusion follows from the truth of all the premises, and missing data usually demand for some imputation strategy that results in a conjecture: a sort of abducing to go on deducing. In contrast, an abductive interpretation is posed as a hypothesize-and-test cycle, where the evidence is incrementally added to reasoning as soon as it is available. In this sense, an abductive interpretation adapts to time-varying properties of the complex system, providing the current result of the interpretation process at any given time.\nAbduction has been primarily proposed for diagnostic tasks [24, 6], but also for question answering [11], language understanding [16], story comprehension [3], image understanding [27] or plan recognition [20], amongst others. Some researches have proposed that perception could rely on some form of abduction. Even though abductive reasoning has been proved to be NP-complete, a compiled form of abduction, based on a set of prestored hypotheses could narrow the generation of hypotheses [17]. The present work takes this assumption as a starting point and proposes a model-based abductive framework for time series interpretation supported on a set of temporal abstraction patterns. An abstraction pattern represents a set of constraints that must be satisfied by some evidence for being interpreted as the hypothetical observation of a certain process, together with an observation procedure providing a set of measurements for the features of the conjectured observation.\nA set of algorithms is devised in order to achieve the best explanation through a process of successive abstraction from raw data, by means of a hypothesize-and-test strategy.\nThe present work includes some examples from electrocardiography, the recording on the body surface of the electrical activity of the heart as it changes with time. The electrocardiogram (ECG) is the primary method for the study and diagnosis of the cardiac disease, since the processes involved in cardiac physiology manifest in characteristic temporal patterns on the ECG tracing. In other words, a correct reading of the ECG has the potential to provide valuable insight into cardiac phenomena. Learning to interpret the ECG involves the acquisition of perceptual skills from an extensive bibliography with interpretation criteria and worked examples. In particular, pattern recognition is specially important in order to build a bottom-up representation of the cardiac phenomena in multiple abstraction levels. This has encouraged extensive research on classification techniques for interpreting the ECG but in spite of all the efforts, it is considered an open problem. We shall try to demonstrate that the problem is in the nature of deduction itself.\nThe rest of this paper is outlined as follows: Section 2 introduces the main concepts and terminology used in the paper in an informal and intuitive way. Afterwards, in sections 3, 4 and 5 we formally describe all the components of the interpretation framework, including the knowledge representation model and the algorithms to obtain effective interpretations in an affordable time. Section 6 illustrates the capabilities of the framework to overcome some the most important shortcomings of deductive classifiers. Finally, in section 7 we discuss the properties of the model compared with other related approaches, and we draw some conclusions."
    }, {
      "heading" : "2 Interpretation as a process-guessing task",
      "text" : "We propose a knowledge-based interpretation framework upon the principles of abductive reasoning, on the basis of a strategy of hypothesis formation and testing. Taking as starting point a time series of physical measurements, a set of observations are guessed as conjectures of the underlying processes, through successive levels of abstraction. Each new observation will be generated from previous levels, as underlying processes aggregate, superimpose or concatenate to form more complex processes with greater duration and scope, being organized in an abstraction hierarchy.\nThe knowledge of the domain is described as a set of abstraction patterns as follows:\nhψ((Ah, Th) = π(A1, T1, ...,An, Tn)) abstracts m1(A1, T1), ...,mn(An, Tn)\n{C(Ah, Th,A1, T1, ...,An, Tn)}\nwhere hψ(Ah, Th) is an observable of the domain playing the role of a hypothesis on the observation of an underlying process ψ, where Ah represents a tuple of attributes, and its temporal support is represented as an instant Th for the sake of simplicity, but it can also be an interval; m1(A1, T1),...,mn(An, Tn) is a set of observables of the domain playing the role of the evidence that suggests the observation of hψ; C is a set of constraints among the variables involved in the abstraction pattern; constraints in C are interpreted as necessary conditions in order for the evidence m1(A1, T1),...,mn(An, Tn) to be abstracted into hψ(Ah, Th); π(A1, T1, ...,An, Tn) is an abstraction procedure that gives as a result an observation of hψ(Ah, Th) from a set of observations for the set m1(A1, T1),...,mn(An, Tn).\nTo illustrate this concept, consider the sequence of observations in figure 1. Each one of these observations is an instance of an observable we call point (p), that is represented as p(A = 〈V 〉, T ), where T determines the temporal location of the observation and V is a value attribute.\nIf we visually analyze these observations, we may hypothesize the presence of an underlying sinusoidal process. Let us define an observable sinus for such a sinusoidal process, with two attributes: the amplitude of the process (α) and its frequency (ω). The necessary knowledge to conjecture this hypothesis is collected in the following abstraction pattern:\nhsinus(Ah = 〈α, ω〉 = π(V1, T1, . . . , Vn, Tn)) abstracts p(V1, T1), ..., p(Vn, Tn) {C(V1, T1, ..., Vn, Tn)}\nWe can estimate the characteristics 〈α, ω〉 of this process by a simple abstraction procedure π, such that α = max(|Vi|), for 1 ≤ i ≤ n, that is, the amplitude α is obtained as the maximum absolute value of the observations; and ω = π/mean(T peakj − T peak j−1 ), where (V peakj = Vk, T peak j = Tk) ∧ sign(Vk − Vk−1) 6= sign(Vk+1 − Vk), that is, the frequency ω is obtained as the inverse of the mean temporal separation between consecutive peaks in the sequence of observations.\nWe can impose the following constraint C(V1, T1, ..., Vn, Tn) for every pair (Vi, Ti) in the sequence:\nmax(|α · sin(ω · Ti)− Vi|) ≤ ,\nThis constraint provides a model of a sinusoidal process and a measure of how well it fits a set of observations, by means of a maximum error . Figure 2 shows the continuous representation of the abstracted process and its relation with the original observations. A value of α/3 has been chosen for .\nOf course, different abstraction methods can be devised in order to estimate the same or different characteristics of the process being guessed. These methods can provide one or multiple valid estimations in terms of their consistency with the above mentioned necessary constraints. And different processes can be guessed from the same set of observations, all of them being valid in terms of their consistency. Hence, we can need further criteria in order to rank the set of interpretations.\nThis simple example summarizes the common approach to interpretation of experimental results in science and technology, when the knowledge is available as a model or a set of\nmodels. The challenge is to assume that knowledge is not available in an analytical but in a declarative form, as a pattern or a set of patterns, and the interpretation task is expected to mimic some mechanisms of human perception."
    }, {
      "heading" : "3 Definitions",
      "text" : "In this section we formally define the main pieces of our interpretation framework: observables and observations for representing the behavior of the system under study, and abstraction patterns for representing the knowledge about this system."
    }, {
      "heading" : "3.1 Representation entities",
      "text" : "An observation is the result of applying an observation procedure. Such procedure makes it possible to observe something with the quality of being observable. We call Q = {q0, q1, ..., qn} to the set of observables of a particular domain.\nDefinition 1. We define an observable as a tuple q = 〈ψ,A, T b, T e〉, where ψ is a name representing the underlying process being observable, A = (A1, ..., Anq) is a set of attributes to be valued, and T b and T e are two temporal variables representing the beginning and the end of the observable.\nWe call V (Ai) to the domain of possible values for the attribute Ai. For any observable, we implicitly assume the constraint T b < T e. In case of an instantaneous observable, then it is represented as q = 〈ψ,A, T 〉. Some observables can be dually represented from the temporal perspective, as an observable supported by a temporal interval or as an observable supported by a temporal instant, according to the task to be carried out. A paradigmatic example is found in representing the heart beat, since it can be represented as a domain entity with a temporal extension comprising its constituent waves, and it can also be represented as an instantaneous entity for measuring the heart rate.\nExample 3.1. In the ECG signal several distinctive waveforms can be identified, corresponding to the electrical activation-recovery cycle of the different heart chambers. The so-called P wave represents the activation of the atria, and is the first wave of the cardiac cycle. The next group of waves recorded is the QRS complex, representing the simultaneous activation of the right and left ventricles. Finally, the wave that represents the ventricular\nrecovery is called the T wave. Together, these waveforms devise the characteristic pattern of the heart cycle, which is repeated in a normal situation with every beat [31]. An example of a common ECG strip is shown in figure 3.\nAccording to this description, the observable qPw = 〈atrial activation, amplitude, T b, T e〉 represents a P wave resulting from an atrial activation process with an unknown amplitude localized in a still unknown temporal interval.\nDefinition 2. We define an observation as a tuple o = 〈q,v, tb, te〉, the result of applying an observation procedure over an observable q, where v = (v1, ..., vnq) is a set of attribute values, such that A1 = v1,...,Anq = vnq ; and t\nb and te are two precise instants, such that T b = tb and T e = te, limiting the beginning and the end of the observation.\nExample 3.2. The tuple o = 〈qPw, amplitude = 0.17mV, 12:16.977, 12:17.094〉 represents the particular occurrence of the P wave observable highlighted in figure 3.\nWe call O to the set of observations available for the observables in Q. Now we define some relations involving observables and observations, that will be useful to describe some properties and constraints of the domain concepts, as well as to temporally arrange the interpretation process.\nDefinition 3. Given a set of observables Q, a generalization relation can be defined between two different observables q = 〈ψ,A, T b, T e〉 and q′ = 〈ψ′,A′, T ′b, T ′e〉, denoted by q′ is a q, meaning that q generalizes q′. Every generalization satisfies that V (A′i) ⊆ V (Ai) ∀i ∈ [1, nq], and that A ⊆ A′.\nThe generalization relation is reflexive, antisymmetric and transitive. The inverse of a generalization relation is a specification relation. From a logical perspective, a generalization relation can be read as an implication q′ → q, meaning that q′ is more specific than q. It holds that every observation o = 〈q′,v, tb, te〉 of the observable q′ is also an observation of q.\nExample 3.3. A common example of a generalization relation can be defined from a domain partition of an attribute. For example, the observable q1 = 〈Sinus Rhythm, (RR ∈ [200ms, 4000ms]), T b, T e〉 is a generalization of the observables q2 = 〈Sinus Tachycardia, (RR ∈ [200ms, 600ms]), T b, T e〉, q3 = 〈Normal Rhythm, (RR ∈ [600ms, 1000ms]), T b, T e〉 and q4 = 〈Sinus Bradycardia, (RR ∈ [1000ms, 4000ms]), T b, T e〉. The RR attribute represents the measure, in milliseconds, of the mean distance between consecutive beats, while q2, q3 and q4 represent the normal cardiac rhythm denominations according to the heart rate [31].\nDefinition 4. Given a set of observables Q, an exclusion relation can be defined between two different observables q = 〈ψ,A, T b, T e〉 and q′ = 〈ψ′,A′, T ′b, T ′e〉, denoted by q excludes q′, meaning that two different observables are mutually exclusive if and only if they cannot be observed over two properly overlapping intervals, that is, T e < T ′b ∨ T ′e < T b.\nThe exclusion relation is reflexive, symmetric and transitive. Hence, every observation oi = 〈qi,vi, tbi , tei 〉 excludes a different observation oj = 〈qj,vj, tbj, tej〉 of the same observable at the same time, that is, no attribute may take different values simultaneously: ∀oi, oj ∈ O, (qi = qj) ∧ (vi 6= vj) ⇒ (tei < tbj) ∨ (tej < tbi). Furthermore, the observation of the same values is also excluded over two properly overlapping intervals, assuming that the evidence explained by the later one will be subsumed in the first one, which will be temporally extended. A subsumption procedure will be described later in this paper. The generalization relation may hidden the exclusion relation, since the observation of multiple non exclusive observables over properly overlapping intervals can appear as the multiple observation of a single observable which is a generalization of the above: ∀oi = 〈q,v, tbi , tei 〉, oj = 〈q,v, tbj, tej〉 ∈ O | (tei > tbj) ∧ (tei < tej) ⇒ ∃ q′i, q′j ∈ Q | oi = 〈q′i,v, tbi , tei 〉 ∧ oj = 〈q′j,v, tbj, tej〉 ∧ q′i is a q ∧ q′j is a q ∧ q′i¬excludes q′j.\nExample 3.4. The R-on-T phenomenon is the superimposition of an ectopic beat on the T wave of a preceding beat [31]. There is an overlapping of two observations of the observable wave as a generalization of the R and T waves. An example is shown in figure 4.\nWe call O to the set of observations requiring interpretation. In order to index the set of observations they will be represented as a sequence by defining an order relation between them. This ordering aims to prioritize the interpretation of the observations as they appear.\nDefinition 5. Let < be an order relation between two observations oi = 〈qi,vi, tbi , tei 〉 and oj = 〈qj,vj, tbj, tej〉 such that (oi < oj) ⇔ (tbi < tbj) ∨ ((tbi = tbj) ∧ (tei < tej)) ∨ ((tbi = tbj) ∧ (tei = tej) ∧ (qi < qj)), assuming a lexicographical order between observable names.\nA sequence of observations is an ordered set of observations O = (o1, ..., oi, ...) where for all i < j then oi < oj. Every subset of a sequence of observations is also a sequence. The temporal arrangement of observations in a sequence is restricted when the observations correspond to the same observable:\nDefinition 6. We define a q-sequence of observations as a sequence of observations for the observable q, denoted as O(q), where any pair of observations oi = 〈q,vi, tbi , tei 〉 and oj = 〈q,vj, tbj, tej〉 holds that tei < tbj or tej < tbi .\nBy succ(oi) we denote the successor of the observation oi in the sequence O, according to the precedence ordering <. By q-succ(oi) we denote the successor of the observation oi ∈ O(q) in its q-sequence O(q). Conversely to this notation, we denote by q(oi) the observable corresponding to the oi observation."
    }, {
      "heading" : "3.2 Abstraction Patterns",
      "text" : "Abstraction patterns are the knowledge representation primitives of this interpretation framework. An abstraction pattern allows to conjecture the occurrence of an observable from some other observables that appear as findings occurring in a particular temporal arrangement. Formally, an abstraction pattern is defined as follows:\nDefinition 7. An abstraction pattern P = 〈qh,MP , CP , πP 〉 consists of a hypothesis observable qh, a set of findings MP = {m1, . . . ,mn}, a set of constraints CP (Ah, Th,A1, T1, ...,An, Tn) checked according to an ordering compatible with their dependencies, and an observation procedure πP : (A1, T1, ...,An, Tn)→ O(qh).\nAn abstraction pattern establishes the set of conditions and calculations for producing a new observation oh ∈ O(qh) as a conjecture from the observation of a set of findings. Each finding numbers the multiple occurrences of a given observable in the pattern. We call M qP = {m q 1,m q 2, ...,m q i = 〈ψ,Ai, T bi , T ei 〉, ...} to the set of findings of the observable q in P . The interpretation procedure will choose, as we will see later, among the available observations for the observable q satisfying the constraints CP , those to be assigned to the findings in M qP in order to calculate oh.\nThe set of findings MP is divided into two sets AP and EP , being AP ∩EP = ∅, where AP is the set of findings that is said to be abstracted in oh, and EP is the set of findings that constitute the observation environment of oh, that is, the set of findings needed to properly conjecture oh, but which are not synthesized in oh.\nA temporal covering assumption can be made as a default assumption on a hypothesis with respect to those findings appearing in an abstraction pattern:\nDefault Assumption 1. (Temporal covering) Given an abstraction pattern P , it holds that T bh ≤ T bm and T em ≤ T eh , for all m ∈ AP ⊆MP .\nThe temporal covering assumption will allow us to define the exclusiveness of an interpretation, as the impossibility of including competing abstractions in the same interpretation.\nExample 3.5. In electrocardiography, a wave is a discernible deviation from a horizontal reference line called baseline, where at least two opposite slopes can be identified. The term discernible means that both the amplitude and the duration of the deviation must exceed some minimum values, that are agreed to be 20 µV and 6 ms, respectively [7]. A wave can be completely described by its amplitude (A), duration (T e−T b), voltage polarity (V ∈ {+,−}) and its turning point T tp ∈ [T b, T e], resulting in the following observable:\nqwave = 〈electrical activity, (A, V, T tp), T b, T e〉\nConsider the following abstraction pattern:\nPwave = 〈qwave,MP = {mECG0 , . . . ,mECGn }, CPwave , wave observation()〉\nwhere a set of temporal constraints can be established between the temporal variables, C1 ≡ 〈T e − T b ≥ 6ms〉, C2 ≡ 〈T b = Tm1〉 and C3 ≡ 〈T e = Tmn−1〉 , and another set of constraints limit the amplitude and slope changes of the samples included in a wave, C4 ≡ 〈sign(m1−m0) 6= sign(m2−m1)〉, C5 ≡ 〈sign(mn−mn−1) 6= sign(mn−1−mn−2)〉, C6 ≡ 〈sign(mtp − mtp−1) = −sign(mtp+1 − mtp)〉 and C7 ≡ 〈min{|mtp − m1|, |mtp − mn−1|} ≥ 20µV 〉.\nOnce a set of ECG samples has satisfied these constraints, they support the observation of a wave, and the observation procedure provides a value for its parameters, for example, V = sign(mtp−m1), A = max{|mtp−m1|, |mtp−mn−1|}, and T tp = arg mink{mk|1 ≤ k ≤ n− 1}, if m1 < m0, or T tp = arg maxk{mk|1 ≤ k ≤ n− 1}, if m1 > m0.\nThe onset and end of a wave are set to the time of the second and second last samples, since m0 and mn are environment observations used to check the presence of a slope change just before and after the wave, so EPwave = {m0,mn}, and APwave = {m1, . . . ,mn−1}.\nAccording to the definition, an abstraction pattern is defined over a fixed set of evidence findings MP . In general, however, an abstraction involves an undetermined number of pieces of evidence -in the case of an ECG wave the number of samples-. Hence we provide a procedure for dynamically generating abstraction patterns, based on the formal language theory. The set Q of observables can be considered as an alphabet. Given an alphabet Q, a couple of special symbols ∅ (empty set), and λ (empty string), and operators | (union), · (concatenation), and ∗ (kleene closure), a formal grammar G denotes a pattern of symbols of the alphabet, describing a language L(G) ⊆ Q∗, as a subset of the set of possible strings of symbols of the alphabet.\nLet Gap be the class of formal attributed grammars of abstraction patterns. An abstraction grammar G ∈ Gap is syntactically defined as a tuple (VN , VT , H,R). We adopt a simple expressiveness for the production rules in R:\nH = qh → q[c]D D → q[c]F | q[c] | λ\nH = qh is the initial symbol of the grammar, and it plays the role of the hypothesis guessed by the patterns generated by G. A single grammar allows us to describe all the different ways of abstracting an observable qh. VN is the set of non-terminal symbols of the grammar, satisfying H /∈ VN since an observable cannot explain itself. VT is the set of terminal symbols of the grammar, gathering together: a set of observables QG ⊆ Q that can be abstracted by the hypothesis, and a set of constraint descriptions [c] between all the observables.\nGiven a grammar G ∈ Gap, we provide a constructive method for representing a set of abstraction patterns PG = {P1, ..., Pi, ...}. An abstraction pattern aims to describe a set of observables as findings to be abstracted. When these findings appear, possibly repeatedly, under certain constraints they can be abstracted in a new observable qh. PG gathers the set of abstraction patterns that share the same observable qh as hypothesis, so they represent the different ways to conjecture the presence of qh.\nBesides describing all the possible sets of findings that may be abstracted by qh, grammars in Gap must also define the set of constraints C, the observation procedure π, and the membership of every finding to AP or EP . For this, every grammar G ∈ Gap is semantically extended with a set of attributes, so G = ((VN , VT , H,R), B,BR), where B associates\neach grammar symbol α ∈ VN ∪ VT with a set of attributes, and BR associates each rule r ∈ R with a set of attribute computation rules. The set of attribute computation rules comprises two types of rules: 1) the semantic expression of the constraint descriptions [c] between all the observables, leading to the set of constraints C that must be satisfied by those findings involved in an abstraction pattern; and 2) the observation procedure π tied to the symbol qh, that establishes a calculation for producing an observation oh ∈ O(qh) from observations of the findings MP involved in the abstraction pattern.\nIn the following, we devise a constructive method to build abstraction patterns from a grammar description G ∈ Gap. Using this method, the application of every production incrementally adds a new observable as a finding and a set of constraints among this finding and those generated previously:\n1. The start symbol H entails initializing an abstraction pattern with the hypothesis:\nP ← 〈qh,MP = ∅, CP = C(T bh, T eh ,Ah), πP (Ah)〉\n2. All those productions of the form H = qh → q[c]D entail:\nP ← 〈qh,MP = {mq1}, CP ∪ C(T b1 , T e1 ,A1), πP (Ah,A1)〉\n3. All those productions of the form D → q[c]F | q[c] entail:\nP ← 〈qh,MP ∪ {mqk}, CP ∪ C(T b k , T e k ,Ak), πP (Ah,A1, . . . ,Ak)〉\n4. All those productions of the form D → λ entail:\nP ← P\nThis constructive method enables the incremental addition of new constraints as new findings are included in the representation of the abstraction pattern, providing a dynamic mechanism of knowledge assembling by language generation. Moreover, it is possible to design an adaptive observation procedure as new evidence becomes available, since πP (Ah,A1, . . . ,An) can be different at each step.\nIn case the constraints description [c] is omitted in a production, it is assumed the ’after’ temporal relationship between the new finding and the set of previous findings. For instance, all those productions of the form C → qF | q entail:\nP ← 〈qH ,MP ∪ {mqk}, CP ∪ {Ti ≤ T b k | mi ∈MP}, πP (Ah, ...,Ak)〉\nHence, in the absence of any temporal description, an increasing temporal order among consecutive findings in every abstraction pattern is assumed. Moreover, every temporal description must be consistent with this temporal order.\nAccording to the limitation imposed on observations of the same observable, preventing two different observations from occurring at the same time, an additional constraint is added on any two findings of the same observable, and thus ∀mqi ,m q j ∈MP , (T ei < T bj ∨T ej < T bi ).\nBelow we show some examples of abstraction pattern grammars modeling common knowledge in electrocardiography, in order to illustrate the expressiveness of the Gap grammars.\nExample 3.6. The grammar GN = (VN , VT , H = qN , R) is designed to generate an abstraction pattern for a normal cardiac cycle, including the descriptions of common durations and intervals [31]. In this grammar, VN = {D,E}, VT = {qPw, qQRS, qTw}, and R is given by:\nH = qN → qPw[c1]D D → qQRS[c2]E E → qTw[c3]\nwhere a set of temporal constraints can be established among the constituent waves: c11 ≡ 〈T bN = T bPw〉 and c12 ≡ 〈50ms ≤ T ePw − T bPw ≤ 120ms〉 for the first production rule; c21 ≡ 〈50ms ≤ T eQRS − T bQRS ≤ 150ms〉 and c22 ≡ 〈100ms ≤ T bQRS − T bPw ≤ 210ms〉 for the second production rule; and c31 ≡ 〈80ms ≤ T bTw − T eQRS ≤ 120ms〉, c32 ≡ 〈T eTw − T bQRS ≤ 520ms〉 and c33 ≡ 〈T eN = T eTw〉 for the last production rule.\nAlong the execution of these production rules an abstraction pattern is generated:\n1. The start symbol H entails initializing the observable pattern:\nP ← 〈qN ,MP = ∅, CP = ∅, πP = ∅〉\n2. The production H = qN → qPw[c1]D entails:\nP ← 〈qN ,MP = {m1 = qPw}, CP = {c11, c12}, πP 〉\n3. The production D → qQRS[c2]E entails:\nP ← 〈qN ,MP ∪ {m2 = qQRS}, CP ∪ {c21, c22}, πP 〉\n4. The production E → qTw[c3] entails:\nP ← 〈qN ,MP ∪ {m3 = qTw}, CP ∪ {c31, c32, c33}, πP 〉\nThis grammar generates a single abstraction pattern, which allows us to interpret a sequence of a P wave, a QRS complex, and a T wave as a normal cardiac cycle, where some additional temporal constraints are required. The abstraction pattern allows us to specify the temporal location of the hypothesis, a normal cardiac cycle, from the location of the findings. In this case, an observation procedure πP is not necessary since the attributes of the hypothesis are completely determined by the constraints in the grammar, not requiring any additional calculus.\nThe next example shows the ability of an abstraction grammar to dynamically generate abstraction patterns with an undefined number of findings.\nExample 3.7. A bigeminy is a heart arrhythmia in which there is a continuous alternation of long and short heart beats. Most often this is due to ectopic heart beats occurring so frequently that there is one after each normal beat, typically premature ventricular contractions (PVCs) [31]. For example, a normal beat is shortly followed by a PVC, then followed by a pause. The normal beat then returns, only to be followed by another PVC.\nThe grammar GV B = (VN , VT , H = qV B, R) is designed to generate a set of abstraction patterns for ventricular bigeminy, where VN = {D,E, F}, VT = {qN , qV }, and R is given by:\nH = qV B → qN [c1]D D → qV [c2]E E → qN [c3]F F → qV [c4]E | qV [c4, c5]\nThe set of constraints is defined as following: c1 ≡ 〈T bV B = T1〉, c2 ≡ 〈200ms ≤ T2 − T1 ≤ 800ms〉, c3 ≡ 〈1.5 · 200ms ≤ Tk − Tk−1 ≤ 4 · 800ms〉, c4 ≡ 〈200ms ≤ Tk+1 − Tk ≤ 800ms〉 and c5 ≡ 〈T eV B = Tn〉.\nFor simplicity, we have referenced each N and V heart beats with a single temporal variable, so Ti represents the time point of the i-th heart beat, being a normal beat if i is odd, and a PVC if i is even. Along the execution of these production rules an unbounded sequence of alternating normal and premature ventricular QRS complexes is generated, what is just described as ventricular bigeminy. Note that in terms of the {N, V } symbols the GV B grammar is equivalent to the regular expression NV (NV )+. The iterative building of abstraction patterns according to this grammar is as follows:\n1. The start symbol H entails initializing the pattern:\nP ← 〈qV B,MP = ∅, CP = ∅, πP = ∅〉\n2. The production H = qV B → qN [c1]D entails:\nP ← 〈qV B,MP = {m1 = qN}, CP ∪ {c1}, πP 〉\n3. The production D → qV [c2]E entails:\nP ← 〈qV B,MP ∪ {m2 = qV }, CP ∪ {c2}, πP 〉\n4. The production E → qN [c3]F entails, in the k repetition:\nP ← 〈qV B,MP ∪ {mk = qN}, CP ∪ {c3}, πP 〉\n5. The production F → qV [c4]E entails, in the k repetition:\nP ← 〈qV B,MP ∪ {mk+1 = qV }, CP ∪ {c4}, πP 〉\n6. The production F → qV [c4, c5] entails finishing the pattern with n observations:\nP ← 〈qV B,MP ∪ {mn = qV }, CP ∪ {c4, c5}, πP 〉\nIn this example, as in 3.6, an observation procedure πP is not necessary, since the constraints in the grammar completely determine the temporal endpoints of the hypothesis and there are no more attributes to be valued. Figure 5 shows an example of a ventricular bigeminy pattern."
    }, {
      "heading" : "4 An interpretation framework",
      "text" : "In this section, we define and characterize an interpretation problem. Informally, an interpretation problem arises from the availability of a set of initial observations from a given system, and of domain knowledge formalized as a set G = {G1, ..., Gn} of Gap grammars. Every abstraction grammar Gi ∈ G generates a set of abstraction patterns that share the same hypothesis qh. The whole set of abstraction patterns that can be generated by G will be denoted by P .\nDefinition 8. Let Q be a set of observables and G a set of abstraction grammars. We say G induces an abstraction relation in Q×Q, denoted by qi qj if and only if there exists an abstraction pattern P generated by some G ∈ G such that:\n1. qj = qhP\n2. M qiP ∩ AP 6= ∅\n3. qi +qi, where + is the transitive closure of\n4. image( ) = Q− q0\nThe relation qi qj can be read as ’the observation of qi allows us to conjecture the presence of qj’. The transitive closure of the abstraction relation is an strict partial order relation between the domain observables, so that qi < qj ⇔ qi +qj, that is, if and only if ∃qk0 , ..., qkn ∈ Q such that qk0 = qi, qkn = qj and for all h, with 0 ≤ h < n, it holds that qkh qkh+1 . We denote by qi = qk0 qk1 ... qkn = qj an abstraction sequence in n steps that allows to conjecture qj from qi. This order relation defines an abstraction hierarchy among the observables in Q.\nExample 4.1. Let Q = {qPw, qQRS, qTw, qN , qV , qV B} and G = {GN , GV B}, containing the knowledge represented in examples 3.6 and 3.7. The derived abstraction relation states that qPw, qQRS, qTw qN , and qN , qV qV B. Intuitively, we can see that this relation splits the observables in the wave, heartbeat, and rhythm levels commonly used in electrocardiogram analysis.\nIt is important to note that the abstraction relation is only established between observables in the AP set. This provides flexibility to define the evidence that form the context of a pattern, as it can belong to the same or even higher abstraction level.\nDefinition 9. We define an abstraction model as a tuple M = 〈Q, ,G〉, where Q is the set of domain observables, is an abstraction relation between such observables, and G is the available knowledge as a set of abstraction grammars.\nThe successive application of the available abstraction grammars result in a series of observations organized in a hierarchy of abstraction, according to the order relation between observables as described above. We are able to define an interpretation problem:\nDefinition 10. We define an interpretation problem as a tuple IP = 〈O,M〉, where O = (o1, o2, . . . , oi, . . .) is a sequence of observations requiring interpretation andM is an abstraction model of the domain.\nIt is worth mentioning that this definition of an abductive interpretation problem differs from the common definition of abductive diagnosis problem, where the difference between normal and faulty behaviors is explicit, leading to the role of faulty manifestations. Only when some faulty manifestation is detected the abductive process of diagnostic is started. In contrast, in the present framework all the observations have the same status, and the objective of the interpretation process is to provide an interpretation of what is observed at the highest possible abstraction level in terms of the underlying processes. As we will see later, some observables can stand out amongst others regarding the efficiency of the interpretation process, as salient features that can draw some sort of perceptual attention.\nAs we have said before, any observable q ∈ QP can appear multiple times as different pieces of evidence for an abstraction pattern P , in the form of findings collected in the set MP . As a consequence, P can predict multiple observations of the set O for a given observable q ∈ QP , each one of them corresponding to one of the findings of the set MP through a matching relation. This matching relation is a matter of choice for the agent in charge of the interpretation task, by selecting the observation corresponding to each finding in a given pattern.\nDefinition 11. Given an interpretation problem IP , a matching relation for a pattern P ∈ P is an injective relation in MP × O, defined by mq ← o, if and only if mq = 〈ψ,A, T b, T e〉 ∈ MP and o = 〈q,v, tb, te〉 ∈ O(q) ⊆ O, such that A = v, T b = tb and T e = te.\nThe assignment included in a matching relation leads us to understand the interpretation problem as a search for a solution for the constraints represented in an abstraction pattern. In this sense, the interpretation problem can be seen as a consistency problem.\nFrom the notion of matching relation we can design a mechanism for abductively interpreting a subset of observations in O through the use of abstraction patterns. Thus, a matching relation for a given pattern allows us to hypothesize new observations from previous ones, and to iteratively incorporate new evidence to the interpretation by means of a hypothesis generation-and-test cycle. The notion of abstraction hypothesis defines those conditions that a subset of observations must satisfy in order to be abstracted by a new observation, and makes it possible to incrementally build an interpretation by the incorporation of new evidence.\nDefinition 12. Given an interpretation problem IP , we define an abstraction hypothesis as a tuple h = 〈oh, Ph,←h〉, where Ph = 〈qh,Mh, Ch, πh〉 ∈ P , ←h⊆ Mh × O, and we denote Oh = image(←h), satisfying:\n1. oh ∈ O(qh).\n2. oh = πh(Oh).\n3. Ch(Ah, Th,A1, T1, ..., An, Tn) is consistent.\nThese conditions entail: (1) an abstraction hypothesis guesses an observation of the observable hypothesized by the pattern; (2) a new observation is obtained from the application of the observation procedure to those observations being assigned to the set of findings Mh by the matching relation; and (3) observations involved in an abstraction hypothesis must satisfy temporal and value consistency.\nOn the other hand, an abstraction hypothesis assigns a set of observations to the findings of the pattern, giving them the role of evidence for the hypothesis. Even though the matching relation is a matter of choice, and therefore a conjecture by itself, some additional constraints could be considered as default assumptions. An important default assumption in the abstraction of periodic processes states that consecutive observations are related by taking part of the same hypothesis, defining the basic period of the process. This assumption behaves as a sort of operative hypothesis of the abstraction task:\nDefault Assumption 2. (Basic periodicity) Periodic findings in an abstraction pattern must be assigned consecutive observations by any matching relation:\n∀mqi ,m q i+1 ∈M q h,m q i ←h oj ∧ q−succ(oj) ∈ Oh ⇒ m q i+1 ←h q−succ(oj)\nThis default assumption allows us to avoid certain combinations of abstraction hypotheses that, being formally correct, are senseless from an interpretation point of view. For example, without the basic periodicity assumption, a normal rhythm fragment might be abstracted by two alternating bradycardia hypotheses, as shown in figure 6.\nThe set of observations that may be abstracted in an interpretation problem IP is O(domain( )), that is, observations corresponding to observables involved in the set of findings to be abstracted by some abstraction pattern. An abstraction hypothesis defines in the set of observations O a counterpart of the subsets AP and EP of the set of findings MP of a pattern P , resulted from selecting a set of observations Oh ⊆ O by means of a matching relation, satisfying those requirements shown on definition 12.\nDefinition 13. Given an interpretation problem IP and an abstraction hypothesis h = 〈oh, Ph,←h〉, we define the following sets of observations:\n• abstracted by(oh) = {o ∈ Oh | mqi ←h o ∧m q i ∈ APh}.\n• environment of(oh) = {o ∈ Oh | mqi ←h o ∧m q i ∈ EPh}.\n• evidence of(oh) = abstracted by(oh) ∪ environment of(oh).\nWe denote by abstracted by(oh) the set of observations abstracted by oh and which are somehow its constituents, while environment of(oh) denotes the evidential context of oh. We denote by evidence of(oh) the set of all observations supporting a specific hypothesis. Since the matching relation is injective, it follows that abstracted by(oh) ∩ environment of(oh) = ∅.\nDefinition of these sets can be generalized to include as arguments a set of observations O = {oh1 , ..., ohm} from a set of abstraction hypotheses h1, ..., hm:\n• abstracted by(O) = ⋃ oh∈O abstracted by(oh)\n• environment of(O) = ⋃ oh∈O environment of(oh).\n• evidence of(O) = ⋃ oh∈O evidence of(oh).\nAs a result of an abstraction hypothesis, a new observation oh is generated, that can be included in the set of domain observations, so O = O∪{oh}. In this way, an interpretation can be incrementally built from the observations, by means of the consistent aggregation of abstraction hypotheses.\nDefinition 14. Given an interpretation problem IP , we define an interpretation as a consistent set of abstraction hypotheses I = {h1, ..., hm}.\nAn interpretation can be rewritten as I = 〈OI , PI ,←I〉, where: OI = {oh1 , ..., ohm} is the set of observations guessed by performing multiple abstraction hypotheses; PI = {P1, ..., Pm} is the set of abstraction patterns used in the interpretation; and ←I=←1 ∪...∪ ←m ⊆ (M1 ∪ ... ∪ Mm) × O is the global matching relation. We should note that the global matching relation ←I is not necessarily injective, since some observations could simultaneously belong to abstracted by() and environment of() sets of different observations.\nFrom a given interpretation problem IP , different interpretations through different sets of abstraction hypothesis can be abductively proposed. Indeed, the definition of interpretation is actually weak, since even an empty set I = ∅ is formally a valid interpretation. Thus, we need additional criteria in order to select the solution to the interpretation problem as the best choice among different possibilities [24].\nDefinition 15. Given an interpretation problem IP , an interpretation I is a cover of IP if the set of observations to be interpreted O(domain( )) ⊆ O is included in the set of observations abstracted by I, that is, O(domain( )) ⊆ abstracted by(OI).\nDefinition 16. Given an interpretation problem IP , two different abstraction hypothesis h and h′ on mutually exclusive observables qh and qh′ are alternative hypotheses if and only if abstracted by(oh) ∩ abstracted by(oh′) 6= ∅.\nExample 4.2. A ventricular trigeminy is an infrequent arrhythmia very similar to ventricular bigeminy, except that the ectopic heart beats occur after every pair of normal beats instead of after each one. The grammar to hypothesize a ventricular trigeminy qV T would therefore\nbe very similar to that described in example 3.7, with the difference that each qV finding would appear after every pair of qN findings. These two processes are mutually exclusive, in so far as the activation pattern of the heart can be only one in a certain interval. For this reason, in the event of an observation of qV , it may be abstracted by a qV B or a qV T hypothesis, but never by both simultaneously.\nDefinition 17. Given an interpretation problem IP , a cover I for IP is exclusive if and only if it contains no alternative hypotheses.\nThus, two or more different hypotheses of mutually exclusive observables abstracted from the same observation will be incompatible in the same interpretation, since inferring a statement and its negation is logically prevented, and therefore only one of them can be selected.\nOn the other hand, a parsimony criterion is required, in order to disambiguate among possible interpretations to select the most plausible ones as those whose complexity is minimum [24]. We translate this minimum complexity in terms of minimal cardinality.\nDefinition 18. Given an interpretation problem IP , a cover I for IP is minimal, if and only if its cardinality is smallest among all covers for IP .\nMinimality introduces a parsimony criterion on hypothesis generation, promoting temporally maximal hypotheses, that is, those hypothesis of a larger scope instead of multiple equivalent hypotheses of smaller scope. For example, consider an abstraction pattern that allows to conjecture regular cardiac rhythm from the presence of three or more consecutive heart beats. Without a parsimony criterion, a sequence of nine consecutive beats could be abstracted by up to three consecutive rhythm observations, even when a single rhythm observation would be sufficient and better.\nDefinition 19. The solution of an interpretation problem IP , is the set of all minimal and exclusive covers of IP .\nThis definition of solution is very conservative and with a limited practical value, since the usual objective is to obtain a small set of interpretations explaining what has been observed (ideally only one). However, it allows us to characterize the problem in terms of complexity. Abduction has been formulated under different frameworks according to the task to be addressed, but it has always shown as an intractable problem in the general case. The next theorem proves that an interpretation problem is also an intractable problem.\nTheorem 1. Finding the solution to an interpretation problem is NP-hard.\nProof: We will provide a polynomial-time reduction of the well-known set covering problem to an interpretation problem. Given a set of elements U = {u1, ..., um} and a set S of subsets of U , a cover is a set C ⊆ S of subsets of S whose union is U . Regarding complexity analysis two different problems of interest are identified:\n• A set covering decision problem, stating that given a pair (U, S) and an integer k the question is whether there is a set covering of size k or less. This decision version of set covering is NP-complete.\n• A set covering optimization problem, stating that given a pair (U, S) the task is to find a set covering that uses the fewest sets. This optimization version of set covering is NP-hard.\nWe will therefore reduce the set covering problem to an interpretation problem by means of a polynomial-time function ϕ. Thus, we shall prove that ϕ(U, S) is an interpretation problem, and there is a set covering of ϕ(U, S) of size k or less if and only if there is a set covering of U in S of size k or less.\nGiven a pair (U, S), let ϕ(U, S) = 〈O,M〉 where:\n1. O = U = {u1, ..., um}, such that ui = 〈q, true, i〉 and q = 〈ψ, present, T 〉.\n2. M = 〈Q, ,P〉, such that domain( )= q.\n3. ∀s = {ui1 , ..., uin} ∈ S, ∃P ∈ P , being P = 〈qP ,MP , CP , πP 〉, where:\n• q qP and P 6= P ′ ⇒ qP 6= qP ′ . • MP = AP =M q P = {m q 1 = 〈ψ, present, T1〉, ...,mqn}. • CP = u1.present ∧ ... ∧ un.present = true. • πP = ∧n k=1(uk.t = ik).\nThus, ϕ(U, S) is an interpretation problem according to this definition. On the other hand, ϕ(U, S) can be built in polynomial time. In addition, for all s ∈ S there exists an abstraction hypothesis h = 〈oh, Ph,←h〉 such that:\n1. oh = 〈qPh , πPh = true,minui∈s{i},maxui∈s{i}〉.\n2. ui ∈ s⇒ ui ∈ image(←h).\n3. ←h is consistent, since the set of observations satisfying πPh = true also satisfies the constraints in CP .\nSince any abstraction hypothesis involves a different abstraction pattern there are no alternative hypothesis in any interpretation of ϕ(U, S). In addition, any combination of temporal constraints from abstraction patterns in ϕ(U, S) is, by definition, consistent.\nSuppose there is a set covering C ⊆ S of U of size k or less. For all u ∈ U there exists ci ∈ C − {∅} such that u ∈ ci and, by the above construction, there exists hi ∈ I such that abstracted by(ohi) = {u ∈ image(←hi)} = {u ∈ ci} = ci, and therefore, O(domain( )) ⊆ ⋃ hi∈I abstracted by(ohi) = ⋃ i ci = C, that is, the set of abstraction hypotheses I is an exclusive cover of the interpretation problem ϕ(U, S) of size k or less. Following the same reasoning as for the set covering optimization problem, finding a minimal and a exclusive cover of an interpretation problem ϕ(U, S) is NP-hard, since we can use the solution of this problem to check if there is an exclusive cover of the interpretation problem of size k or less, and this has just been proved to be NP-complete."
    }, {
      "heading" : "5 Solving an interpretation problem: A heuristic search",
      "text" : "approach\nThe solution set for an interpretation problem IP consists of all exclusive covers of IP having the minimum possible number of abstraction hypotheses. Obtaining this solution set can be stated as a search on the set of interpretations of IP . The major source of complexity of searching for a solution is the local election, from the available evidence in O, of the most appropriate matching relation for a number of abstraction hypotheses that can globally shape a minimal and exclusive cover of IP .\nNevertheless, the whole notion of solution must be revised in practical terms, due to the intractability of the task and the incompleteness of the abstraction model, that is, of the available knowledge. Indeed, we assume that any realistic abstraction model can hardly provide a cover for every possible interpretation problem. Hence the objective should move from searching for a solution to searching for an approximate solution.\nCertain principles applicable to the interpretation problem can be exploited in order to approach a solution in an iterative way, bounding the combinatorial complexity of the search. These principles can be stated as a set of heuristics that make it possible to evaluate and discriminate some interpretations against others from the same base evidence:\n• A coverage principle, which states the preference for interpretations explaining more initial observations.\n• A simplicity principle, which states the preference for interpretations with fewer abstraction hypotheses.\n• An abstraction principle, which states the preference for interpretations involving higher abstraction levels.\n• A predictability principle, which states the preference for interpretations that properly predict future evidence.\nThe coverage and simplicity principles are used to define a cost measure for the heuristic search process [9], while the abstraction and predictability principles are used to guide the reasoning process, trying to emulate the same shortcuts used by humans.\nGiven an interpretation problem IP , a heuristic vector for a certain interpretation I can be defined to guide the search, as (I) = (1− ς(I), κ(I)), where ς(I) is the covering ratio of I, defined as ς(I) = |abstracted by(OI)|/|O(domain( ))|, and κ(I) = |OI | is the complexity of I. The main goal of the search strategy is to approach a solution with a maximum covering ratio and a minimum complexity, which is equivalent to the minimization of the heuristic vector. Covering ratio will be considered the primary heuristic, and complexity will be considered for ranking those interpretations with the same covering ratio. The (I) heuristic is intuitive and very easy to calculate, but as a counterpart it is a non-admissible heuristic, since it is not monotone and can alternatively underestimate and overestimate the true goal covering. Therefore optimality cannot be guaranteed and we require an algorithm efficient with this type of heuristic. We propose the CONSTRUE() algorithm, whose pseudocode is shown in algorithm 1. This algorithm is a small variation of the K-Best First Search algorithm [10], with partial expansion to reduce the number of explored nodes.\nAlgorithm 1 CONSTRUE search algorithm.\n1: function CONSTRUE(IP ) 2: var I0 = ∅ 3: var K = max(|{qj ∈ Q | qi qj , qi ∈ Q}|) 4: set focus(I0, o1) 5: var open = sorted([〈 (I0), I0〉]) 6: var closed = sorted([]) 7: while open 6= ∅ do 8: for all I ∈ open[0 . . .K] do 9: I ′ = next(get descendants(I))\n10: if I ′ is null then 11: open = open− {〈 (I), I〉} 12: closed = closed ∪ {〈 (I), I〉} 13: else if ς(I ′) = 1.0 then 14: return I ′ 15: else 16: open = open ∪ {〈 (I ′), I ′〉} 17: end if 18: end for 19: end while 20: return min(closed) 21: end function\nThe CONSTRUE() algorithm takes as input an interpretation problem IP , and returns the first found interpretation with full coverage, or the interpretation with the maximum covering ratio and minimum complexity if no covers are found, using the abstraction and predictability principles along the searching process as well. For this, it manages two ordered lists of interpretations, named open and closed. Every interpretation is annotated with the computed values of the heuristic vector. The open list contains those partial interpretations that can further evolve by (1) appending new hypotheses or (2) extending previously conjectured hypotheses to subsume or predict new evidence. This open list is initialized with the trivial interpretation I0 = ∅. The closed list contains those interpretations that cannot explain more evidence.\nAt each iteration, the algorithm selects the K most promising interpretations according to the heuristic vector (line 8), and partially expands each one of them to obtain the next descendant node I ′. If this node is a solution, then the process finishes by returning it (line 13), otherwise it is added to the open list. The partial expansion warrants that the open list grows at each iteration at most K new nodes, in order to save memory. When a node cannot expand more, it is added to the closed list (line 12), from which the solution is taken if no full coverages are found (line 20).\nThe selection of a value for the K parameter depends on the problem at hand. We select its value as K = max(|{qj ∈ Q | qi qj, qi ∈ Q}|), that is, as the maximum number of observables that can be abstracted from any observable qi. The intuition behind this choice is that at any point in the interpretation process, and with the same heuristic values, we will give the same chance to any plausible abstraction hypothesis in order to explain a certain observation.\nIn order to expand the current set of interpretations, the GET DESCENDANTS() function relies on different reasoning modes, that is, different forms of abduction and deduction, which are brought into play under the guidance of an attentional mechanism. Since searching for a solution involves at the end the election of a matching relation, both observations and findings should be included in the scope of this mechanism. Hence, a focus of attention can be defined to answer the following question: which is the next observation or finding to be processed? Answering this question develops a hypothesis-and-test cycle: if the attention focuses on an observation, then an abstraction hypothesis explaining such observation should be generated (hypothesize); but if the attention focuses on a finding predicted by some hypothesis, an observation should be sought to match such finding (test).\nTo illustrate and motivate the reasoning modes implemented to build interpretations and support the execution of the CONSTRUE() algorithm, we use a simple, but complete, interpretation problem.\nExample 5.1. Let Q = {qwave, qPw, qQRS, qTw, qN},G = {Gw, GN , GTw}, where Gw models the example 3.5, GN is described in example 3.6, and GTw describes the knowledge to conjecture a T wave as follows:\nGTw = (({D}, {qQRS, qwave}, {H = qTw → qQRS[c1]D D → qwave[c2]}), ∅, Tw delin())\nwhere the set of constraints is defined as following: c11 ≡ 〈80ms ≤ T bTw − T eQRS ≤ 120ms〉, c12 ≡ 〈T eTw − T bQRS ≤ 520ms〉, c21 ≡ 〈T bTw = T bwave〉, c22 ≡ 〈T eTw = T ewave〉 and c23 ≡ 〈max(diff(sig[mwave]) ≤ 0.7 ·max(diff(sig[mQRS]))〉.\nThis grammar hypothesizes the observation of a T wave shortly after the observation of a QRS complex, requiring a significant decrease in the maximum slope of the signal. Let us note that the observation of a T wave is not deduced from the observation of a QRS complex since it is not a necessary condition for the observation of a QRS complex. Attribute computation rules are denoted as Tw delin(), and represent a procedure to obtain the time limits of a T wave based on the observation of a leading QRS complex. This method may be any of those described in the literature, such as [19].\nIn addition to the Pwave pattern generated by Gw and detailed in example 3.5, GN and GTw generate the following abstraction patterns:\nPN = 〈qN , APN = {mPw,mQRS,mTw} ∪ EPN = ∅, CPN , ∅〉 PTw = 〈qTw, APTw = {mwave} ∪ EPTw = {mQRS}, c1 ∪ c2, Tw delin()〉\nFinally, letO = {owave1 = 〈qwave, ∅, 0.300, 0.403〉, owave2 = 〈qwave, ∅, 0.463, 0.549〉, oPw = 〈qPw, ∅, 0.300, 0.403〉, oQRS = 〈qQRS, ∅, 0.463, 0.549〉} be a set of initial observations including a P wave and a QRS complex abstracting two wave observations located in specific time points.\nGiven this interpretation problem, figure 7 shows the starting point for the interpretation: where the root of the interpretation process is the trivial interpretation I0, and the attention is focused on the first observation. The sequence of reasoning steps towards the resolution of this interpretation problem will be explained in the next subsections."
    }, {
      "heading" : "5.1 Focus of attention",
      "text" : "The focus of attention is modeled as a stack, so once the focus is set on a particular observation (or finding), any observation that was previously under the focus will not return to be focused on until the reasoning process on the current observation is finished. Algorithm 2 shows how the different reasoning modes are invoked based on the content of the focus of attention, resulting in a hypothesize-and-test cycle.\nAlgorithm 2 Method for obtaining the descendants of an interpretation using different reasoning modes based on the content of the focus of attention.\n1: function get descendants(I) 2: var focus = get focus(I).top() 3: var desc = ∅ 4: if is observation(focus) then 5: if focus = oh | h ∈ I then 6: desc = deduce(I, focus) 7: end if 8: desc = desc ∪ abduce(I, focus) ∪ advance(I, focus) 9: else if is finding(focus) then\n10: desc = subsume(I, focus) ∪ predict(I, focus) 11: end if 12: return desc 13: end function\nLines 4-8 generate the descendants of an interpretation I when at the top of the stack there is an observation. These descendants are the result of two possible reasoning modes: the deduction of new findings, performed by the DEDUCE() function provided that the observation being focused on is an abstraction hypothesis; and the abduction of a new hypothesis explaining the observation being focused on, performed by the ABDUCE() function. A last descendant is obtained using the ADVANCE() function, which simply restores the previous focus of attention by means of a POP() operation. If the focus is then empty, ADVANCE() inserts the following observation to explain, that may be selected by temporal order in the general case, or by some domain-dependent saliency criterion to prioritize certain observations over others. By removing an observation at the top of the focus of attention, the ADVANCE() function sets aside that observation as unintelligible to the current interpretation, according to the available knowledge.\nIf the top of the stack contains a finding, then algorithm 2 obtains the descendants of the interpretation from the SUBSUME() and PREDICT() functions (line 10). The first one looks for an existing observation satisfying the constraints on the finding being focused on, while the second makes predictions about observables that have not yet been observed. Below we detail all these reasoning modes separately, and we will illustrate how the CONSTRUE() algorithm combines them in order to solve the interpretation problem described in example 5.1."
    }, {
      "heading" : "5.2 Building an interpretation: Abduction",
      "text" : "Algorithm 3 enables the abductive generation of new abstraction hypotheses. It is applied when the attention is focused on an observation that can be abstracted by some abstraction pattern, producing a new observation at a higher level of abstraction.\nAlgorithm 3 Moving forward an interpretation through abduction.\n1: function abduce(I, oi) 2: var desc = ∅ 3: for all G = 〈VN , VT , qH , R〉 ∈ G | q(oi) qH do 4: for all (D → q[c]F ) ∈ R | q(oi) is a q ∧ [c]→ mq ∈ AP do 5: Ph ← 〈qH , {mq}, [c], πh〉 6: h = 〈oh, Ph, {mq ← oi}〉 7: Bh = D;Eh = F 8: I ′ = 〈OI ∪ {oh}, PI ∪ {Ph},←I ∪ ←h〉 9: O = O ∪ {oh}\n10: get focus(I ′).pop() 11: get focus(I ′).push(oh) 12: desc = desc ∪ {I ′} 13: end for 14: end for 15: return desc 16: end function\nThe result of ABDUCE() is a set of interpretations I ′, each one adding a new abstraction hypothesis with respect to the parent interpretation I. To generate these abstraction hypotheses, we iterate through those grammars that can make a conjecture from the observation oi being focused on (line 3). Then, for each grammar, each rule including the corresponding observable q(oi) (line 4) makes use of the above mentioned constructive method to initialize an abstraction pattern with a single finding of this observable (line 5), and a new hypothesis is conjectured with a matching relation involving both the observation being focused on and the finding (line 6). Two additional variables, Bh and Eh, allow us to mark the involved rule and they will play an important role in subsequent steps (line 7). Then, the set of constraints concerning this finding are checked, and finally the new hypothesis opens a new interpretation (lines 8-9).\nIn this way, the ABDUCE() function implements, from a single piece of evidence, the hypothesize step of the hypothesize-and-test cycle. Below we explain the reasoning modes involved in the test step of the cycle.\nExample 5.2. Consider the interpretation problem set out in example 5.1 and the interpretation I0 shown in figure 7. According to algorithm 2, the ABDUCE() function is used to move forward the interpretation, since the focus of attention points to an observation oPw. The abstraction pattern that supports this operation is PN , and a matching relation is established with the mPw finding. As a result, the following hypothesis is generated:\nh1 = 〈oN , PN , {mPw ← oPw}〉\nFigure 7 shows the result of this reasoning process, in a new interpretation called I1. Note that the focus of attention has been moved to the newly created hypothesis (lines 10- 11 of the ABDUCE() function)."
    }, {
      "heading" : "5.3 Building an interpretation: Deduction",
      "text" : "This reasoning mode is applied when the attention is focused on an observation oh previously conjectured as part of an abstraction hypothesis h = 〈oh, Ph,←h〉 (see algorithm 4). The DEDUCE() function takes the evidence that has led to conjecture oh and tries to extend it with new findings that can be expected, that is, deduced, from the abstraction grammar used to guess the observation. The key point is that this deduction process follows an iterative procedure, as the corresponding abstraction pattern is dynamically generated from the grammar. Hence the DEDUCE() function aims to extend a partial matching relation by providing the next finding to be tested, as part of the test step of the hypothesize-and-test cycle.\nAlgorithm 4 Moving forward an interpretation through deduction of new findings.\n1: function deduce(I, oh) 2: var desc = ∅ 3: if Bh 6= H then 4: for all (X → q′[c]Bh) ∈ R | X ∈ Vn do 5: P ′h ← 〈qh,Mh ∪ {m q′\n0 }, Ch ∪ Cc, πh〉 6: Bh = X 7: I ′ = 〈OI , PI ∪ {P ′h},←I〉 8: get focus(I ′).push(mq ′\n0 ) 9: desc = desc ∪ {I ′}\n10: end for 11: else 12: for all (Eh → q′[c]X) ∈ R | X ∈ Vn do 13: P ′h ← 〈qh,Mh ∪ {m q′\nn+1}, Ch ∪ Cc, πh〉 14: Eh = X 15: I ′ = 〈OI , PI ∪ {P ′h},←I〉 16: get focus(I ′).push(mq ′\nn+1) 17: desc = desc ∪ {I ′} 18: end for 19: end if 20: return desc 21: end function\nSince the first finding that has led to conjecture oh does not have to appear at the beginning of the grammar description, the corresponding abstraction pattern will not be, in general, generated incrementally from the first rule of the grammar. Taking as a starting point the rule used to conjecture oh (line 4 in algorithm 3), the idea is to add a new finding by adding a new rule at both sides, towards the beginning and the end of the grammar, using the Bh(egin) and Eh(nd) variables in order to track the next finding to be tested. In this way, the matching relation can be incrementally built from a sequence of findings. Bh represents the non-terminal at the left side of the rule that generated the first finding in the matching relation, while Eh represents the non-terminal at the right side of the rule that generated the last finding in the matching relation. With this information we are able to extend the current abstraction pattern Ph in two opposite directions:\n• Towards the beginning of the grammar (lines 3-10). We explore the set of observables that may occur before the first finding according to the rules of the grammar (line 4), and we deduce a new finding for each one of them in different descendant interpretations. An update of the current abstraction pattern is performed (line 5), and, consequently, an update of the current interpretation (line 7). The newly deduced finding is the next to be focused on (line 8).\n• Towards the end of the grammar (lines 11-18). For each one of the observables that may occur after the last finding, a new finding is deduced, updating the abstraction pattern and the interpretation. This new finding is the next to be focused on (lines 12- 18).\nExample 5.3. Consider the interpretation problem set out in example 5.1 and the interpretation I1 shown in figure 7. Remember that the grammar used to generate the hypothesis in the focus of attention, GN , has the following form:\nH = qN → qPw[c1]D D → qQRS[c2]E E → qTw[c3]\nIn this situation, it is possible to deduce new findings from the oN hypothesis. Following algorithm 3 we can check that Bh = H and Eh = D, since the only finding in the matching relation is mPw. Deduction has then to be performed after this last finding, using the rule D → qQRS[c2]E. After [c2] constraint checking, the resulting finding is the following:\nmq ′\nn+1 = m QRS = 〈qQRS, ∅, T bQRS ∈ [0.400, 0.520], T eQRS ∈ [0.450, 0.660]〉\nFigure 7 illustrates the outcome of this reasoning process and the uncertainty in the temporal limits of the predicted finding, that is now being focused on in the interpretation I2."
    }, {
      "heading" : "5.4 Building an interpretation: Subsumption",
      "text" : "Subsumption is performed when the attention is focused on a finding previously deduced from some abstraction grammar (see algorithm 5). This reasoning mode avoids to generate a new hypothesis for every piece of available evidence if it can be explained by a previous hypothesis. The SUBSUME() function explores the set of observations O and selects those consistent with the constraints of the finding in the focus of attention (line 3), expanding the matching relation of the corresponding hypothesis in different descendant interpretations (line 4). The focus of attention is then restored to its previous state (line 5), allowing to deduce new findings from the same hypothesis. The SUBSUME() function clearly enforces the simplicity principle.\nExample 5.4. Consider the interpretation I2 shown in figure 7. If we apply the subsumption procedure, it is possible to set a matching relation between oQRS and mQRS, since this observation satisfies all the constraints on the finding. The result is shown in the interpretation I3. Note that the uncertainty in the end time of the o\nN hypothesis is now reduced after the matching, having T eN ∈ [0.631, 1.030]. Then the attention focuses once again on this hypothesis, and therefore a new deduction operation may be performed.\nAlgorithm 5 Moving forward an interpretation through subsumption.\n1: function subsume(I,mi) 2: var desc = ∅ 3: for all oj ∈ O | mi ← oj do 4: I ′ = 〈OI , PI ,←I ∪ {mi ← oj}〉 5: get focus(I ′).pop(mi) 6: desc = desc ∪ {I ′} 7: end for 8: return desc 9: end function"
    }, {
      "heading" : "5.5 Building an interpretation: Prediction",
      "text" : "This reasoning mode is also performed when the attention is focused on a finding previously deduced from some abstraction grammar (see algorithm 6). In this case, if a finding previously deduced has not been observed yet it will be predicted.\nAlgorithm 6 Moving forward an interpretation through prediction of non-available evidence.\n1: function predict(I,mi) 2: var desc = ∅ 3: for all 〈VN , VT , qH , R〉 ∈ G | qH is a q(mi) do 4: Ph ← 〈qH , ∅, Ch, πh〉 5: h = 〈oh, Ph, ∅〉 6: Bh = Eh = H 7: I ′ = 〈OI ∪ {oh}, PI ∪ {Ph},←I ∪ {mi ← oh}〉 8: O = O ∪ {oh} 9: get focus(I ′).pop(mi)\n10: get focus(I ′).push(oh) 11: desc = desc ∪ {I ′} 12: end for 13: return desc 14: end function\nThe goal of the PREDICT() function is to conjecture a new observation for the finding being focused on. To do this, the knowledge base is explored and those grammars whose hypothesized observable is more specific than the predicted observable are selected (line 3). Then, a new pattern is initialized with no evidence supporting it, and a new abstraction hypothesis with an empty matching relation is generated (lines 4-5). Finally, the attention focuses on the observation being guessed (lines 9-10) to enable the DEDUCE() function to start a new iteration of the test step at a lower abstraction level. Since the Bh and Eh variables point to the first non-terminal symbol of the grammar (line 6), the corresponding abstraction pattern is generated only towards the end of the grammar.\nExample 5.5. Starting from the I3 interpretation shown in figure 7, the next step we can take to move forward the interpretation is a new deduction on the oN hypothesis, generating a new finding mTw, leading to the I4 interpretation. Since there is no available observation of the T wave, a matching with this new finding mTw cannot be made by the SUBSUME() function, so the only option to move forward this interpretation is through\nprediction. Following the PREDICT() function, the GTw grammar can be selected, and a new observation oTw can be conjectured, generating the I5 interpretation.\nFrom I5 we can continue the deduction on the o Tw hypothesis. If we apply the DE-\nDUCE() function we obtain the mQRS ′\nfinding from the environment, shown in figure 7 as I6. To move on, we can apply the SUBSUME() function, establishing the matching relation {mQRS′ ← oQRS}. This leads to the I7 interpretation, in which the uncertainty on the oTw observation is reduced, but the evidence for the PTw pattern is not yet complete. A new DEDUCE() step is necessary, which deduces the mwave necessary finding in the I8 interpretation. This finding is also absent, so another PREDICT() step is required. In this last step, the Pwave pattern can be applied to observe the deviation in the raw ECG signal, generating the owave3 observation and completing the necessary evidence for the o\nTw observation and thus also for oN . Constraint solving assigns the value of tbTw, t e Tw and t e N , so the result is a cover of the initial interpretation problem in which all the hypotheses have a necessary and sufficient set of evidence. This solution is depicted in I9.\nIt is worth noting that in this example the global matching relation ←I is not injective, since mQRS ← oQRS and mQRS′ ← oQRS. Also note that each interpretation only generates one descendant, but in a more complex scenario the possibilities are numerous, and the responsibility to find the proper sequence of reasoning steps lies with the CONSTRUE() algorithm."
    }, {
      "heading" : "5.6 Improving the efficiency of interpretation through saliency",
      "text" : "Starting a hypothesize-and-test cycle for every single sample is not feasible for most of the time series interpretation problems. Still, many problems may benefit from certain saliency features that can guide the attention focus to some limited temporal fragments that can be easily interpretable. Thus, the interpretation of the whole time series can pivot on a reduced number of initial observations, therefore speeding-up the interpretation process.\nA saliency-based attentional strategy can be devised from the definition of abstraction patterns, by using a subset of their constraints as a coarse filter to identify a set of plausible observations. For example, in the ECG interpretation problem, the minimum slope constraint defined by the QRS abstraction pattern is a reliable feature to select more promising signal segments to focus the interpretation on, in the same way that a cardiologist focuses on the prominent peaks in the signal to start the analysis [31].\nBy using some of the constraints of the abstraction patterns we are taking advantage of the necessary nature of these constraints. If there are multiple alternative constraints in the description of an abstraction pattern, those that are easier to compute can be preferable, even when some of the potential observations could be lost, as they can be predicted later as part of the hypothesize-and-test cycle. This is frequent when a cardiologist makes a fast read of an ECG. Additionally, the non-sufficient nature of these constraints makes it necessary to insert the resulting set of plausible observations in the hypothesize-and-test cycle where they can be refuted later."
    }, {
      "heading" : "6 Strengths of the model: Overcoming some limita-",
      "text" : "tions of classifiers\nIn this section we provide some practical examples to illustrate some of the strengths of the proposed interpretation framework, and its ability to tackle with typical weaknesses of classification-based approaches."
    }, {
      "heading" : "6.1 Avoiding a casuistry-based interpretation",
      "text" : "In the time domain, a classification-based recognition of multiple processes occurring concurrently usually lead to a casuistry-based proliferation of classes, where it is usually needed a new class for each possible superposition of processes, in order to properly identify all situations. It is common to use a representation in transform domain, where certain regular processes are easily separable, though at the expense of a cumbersome representation of the temporal information [21]. In contrast, in the present framework, the hypothesize-and-test cycle aims to conjecture those hypotheses that best explain the available evidence, including in a natural way simultaneous hypotheses as long as they are consistent and they are not mutually exclusive.\nECG interpretation provides some interesting examples of this sort of problems. Atrial fibrillation, a common heart arrhythmia caused by the independent and erratic contractions of the atrial muscle fibers, is characterized by an irregularly irregular heart rhythm [31]. Most of the classification techniques for the identification of atrial fibrillation are based on the analysis of the time interval between consecutive beats, trying to detect this irregularity [25]. These techniques offer good results in those situations in which atrial fibrillation is the only anomaly, but they fail to properly identify complex scenarios which go beyond the distinction between atrial fibrillation and normal rhythm. In the strip of figure 8, obtained during a pilot study for the home follow-up of patients with cardiac diseases [28], such a classifier would wrongly identify this segment as an atrial fibrillation episode, since the observed rhythm variability is consistent with the description of this arrhythmia. In contrast, the present interpretation framework properly explains the first 5 beats as a sinus bradycardia, compatible with the presence of a premature ectopic beat in the second position, followed by a trigeminy pattern during 6 beats, and finally another ectopic beat with a morphology change. The reason to choose this interpretation, despite being more complex than the atrial fibrillation explanation, is that it is able to abstract some of the small P waves before the QRS complexes, increasing the interpretation coverage."
    }, {
      "heading" : "6.2 Coping with ignorance",
      "text" : "Most of the classifiers solve a separability problem among classes, whether by learning from a training set or by eliciting prior knowledge, and they are implicitly based on the closed-world assumption, i.e., every new instance to be classified is assigned to one of the predefined classes. Such classifiers may additionally include a reject option for all those instances that could be misclassified as they appear too close to the classification boundaries [4, 13]. This reject option is added as another possible answer expressing doubt. However, such classifiers fail to classify new instances of unknown classes, since they cannot express ignorance. An approach to this problem can be found in novelty detection proposals [26], which can detect when a new instance does not fit any of the predefined classes as it substantially differs from those instances available during training. Still, they are limited to a common feature representation for every instance, hindering to identify what is unintelligible from the available knowledge.\nThe present framework provides an expression of ignorance as a common result of the interpretation problem. As long as the abstraction model may be incomplete, a non coverage of some piece of evidence by any interpretation is an expression of partial ignorance. In the extreme case, the trivial interpretation I0 can be a proper solution for an interpretation problem, expressing total ignorance. Furthermore, abduction naturally includes the notion of ignorance in the reasoning process, since any single piece of evidence can be sufficient to guess an interpretation, and the hypothesize-and-test cycle can be understood as a process of incremental addition of evidence against an initial state of ignorance, being able to provide an interpretation at any time based on the available evidence.\nAs an example, consider the interpretation problem illustrated in figure 9. Let the initial evidence be the set of QRS annotations obtained by a state-of-the art detection algorithm [32]. In this short strip, the eighth and ninth annotations correspond to false positives caused by the presence of noise. A classification-based strategy processes these two annotations as true QRS complexes, and the monotone nature of the reasoning prevents their possible refutation, probably leading to beat misclassification and false arrhythmia detection, propagating the errors onwards to the end of the processing. In contrast, the present framework provides a single normal rhythm as the best interpretation, which explains all but those two aforementioned annotations, being ignored and considered unintelligible from the available model. It is also worth noting the ability of this framework to integrate the results of an available classifier -as a sort of constraint specification- in the interpretation cycle."
    }, {
      "heading" : "6.3 Looking for missing evidence",
      "text" : "Application of the classification paradigm to pattern detection also entails the potential risk of providing false negative results. In the worst case, a false negative result may be interpreted by a decision maker as an evidence of absence, leading to interpretation errors with their subsequent costs; or in the best case, as an absence of evidence caused by the lack of a proper detection instrument.\nEven though abduction is fallible, and false negative results persist, the hypothesizeand-test cycle involves a prediction mechanism that points to that missing evidence that should be expected, and moreover, estimates when it should appear. Both the bottom-up and the top-down processing performed along this cycle reinforces the confidence in the interpretation, since the semantics of any conclusion is widened according to its explanatory power.\nAs an example, consider the interpretation problem illustrated in figure 10. The initial evidence is again a set of QRS annotations obtained by a state-of-the art detection algorithm [32]. Note that the eighth beat has not been annotated, due to a sudden decrease in the signal amplitude. This error can be amended along the hypothesis-and-test cycle, since the normal rhythm hypothesis that abstracts the first seven QRS annotations predicts the following QRS to be in the position of the missing annotation, and the PREDICT() procedure can look for it (e.g. checking an alternative set of constraints).\nThe capability of abduction to ignore or look for new evidence has been tested with a simplified version of the present framework in the QRS detection problem [30], leading to a statistically significant improvement over a state-of-the art algorithm."
    }, {
      "heading" : "6.4 Interpretability of the reasoning process and the results",
      "text" : "Interpretability of a reasoning formalism, defined as the ability to understand and evaluate its conclusions, is an essential feature to achieve an adequate confidence for decision making [22]. In this sense, there is a number of classification methods with good interpretability, but methods that typically offer the best performance belong to the so-called black box approaches.\nThe present interpretation framework is able to provide a justification of any result in relation to the available model. Given any solution or partial solution of an interpretation problem, the searching path up to I0 gives us a full detail of all those reasoning steps taken to this end, and any abstraction hypothesis can be traced back to the information supporting it.\nThis interpretation framework is also able to answer why a certain hypothesis has been rejected or neglected at any reasoning step. This is done by exploring those branches\noutside the path between I0 and the solution. Since the K exploration parameter within the CONSTRUE() algorithm has been chosen as the maximum number of hypotheses that may explain a given observable, it is possible to reproduce the reasoning steps to conjecture any abstraction hypothesis, and check why it did not succeed (non-satisfaction of pattern constraints, lower coverage, etc.). This can be useful to build and refine the knowledge base."
    }, {
      "heading" : "7 Discussion",
      "text" : "A new model-based framework for time series interpretation is proposed. This framework relies on some basic assumptions: (i) interpreting the behavior of a system from the set of available observations is a sort of conjecturing, and as such follows the logic of abduction; (ii) the interpretation task involves both bottom-up and top-down processing of information along a set of abstraction levels; (iii) at the lower levels of abstraction, the interpretation task is a form of precompiled knowledge-based pattern recognition; (iv) the interpretation task involves both the representation of time and the reasoning about time and along time.\nModel-based representation in the present framework is based on the notion of abstraction pattern, which defines an abstraction relation between observables, and provides the knowledge and methods to conjecture new observations from previous ones. Let us deepen in both the backward and forward logical meaning of an abstraction pattern, following a reasoning similar to that of [2]:\n• Backward meaning. From a backward reading of an abstraction pattern, a hypothesis h is a possible abstraction of m1, . . . ,mn, provided that the constraints in C hold. An abstraction pattern satisfies the compositionality principle of abductive reasoning, and hence an abstraction hypothesis can be conjectured from a single piece of evidence, and new pieces of evidence can be later added [12]. On the other hand, the abductive observation of h must satisfy the falsification principle, that is, if any of the constraints in C is inconsistent with the evidence, then h is refuted assuming this pattern is the only knowledge available about observing h. If there are multiple ways of observing h by means of multiple patterns, and all of their constraints are inconsistent with evidence, we do not conclude ¬h, interpreted as failure to prove h; we will conclude ¬h in all those interpretations conjecturing an observation of a different h′, being h and h′ mutually exclusive.\n• Forward meaning. An abductive observation is built upon an archetypical representation of a hypothesis h, creating an observation as an instance of h by estimating, from the available evidence, its attribute values Ah and its temporal location Th by means of π(). From a forward reading, assuming h is true there is an observation for each observable of the set m1,...,mn such that the constraints in C hold. However, the estimated nature of abstraction does not usually allow us to infer, from the observation of h, the same observations of m1,...,mn that have been abstracted into h. We must presume instead that assuming h is true entails the occurrence of an observation for each observable of m1, ...,mn, without necessarily entailing its attribute values and its temporal location.\nAn abstraction model, built on a set of abstraction patterns, establishes a causal responsibility for the behavior observed in a complex system [17]. This responsibility is expressed in the language of processes: a process is said to be observable if it is assumed that it causes a recognizable trace in the physical quantity to be interpreted. The successive application of abstraction hypotheses provides successive reinterpretations of the observations, as more processes are included, expanding its scale and scope as we go up the abstraction hierarchy. This notion of causality is behind perception, concerned at the explanation of sensory data, in contrast with the notion of causality in diagnosis, concerned at the explanation of abnormality [6].\nRepresenting and reasoning about context is a relevant issue in model-based diagnosis [2, 6, 24, 29]. A contextual observation is nothing more than another observation that needs not be explained by a diagnosis. In most of the works, the distinction between this two roles must be defined beforehand. Some other works enable the same observation to play different roles in different causal patterns, providing some general operations for expressing common changes made by the context in a diagnostic pattern [18, 23]. In the present interpretation framework an observation can either be part of the evidence to be explained in a certain abstraction pattern, or can be part of the environment in another abstraction pattern. Both sort of observations take part of the hypothesize-and-test cycle, with the only difference that observations from the environment of an abstraction pattern are not expected to be abstracted by this pattern. Hence, observations from the environment are naturally included in the deduction, subsumption and prediction modes of reasoning.\nThere are some issues where our efforts are focusing on at present. An important one is the representation of negated evidence and its processing by the CONSTRUE() algorithm. There can be certain observations that are incompatible with a certain interpretation, so their absence is a necessary condition. This can help us to model an inhibitory relation between processes, that allows us to represent that a certain process prevents other to occur under some temporal constraints.\nFurther efforts are being made to improve the efficiency of the interpretation process. To this end, two main strategies are being explored. On the one hand, the exploitation of the model structure to identify necessary and sufficient conditions for every hypotheses to be conjectured; necessary conditions avoid the expansion of those hypotheses that can be ruled out because they are inconsistent with observations, while sufficient conditions avoid the construction of redundant interpretations [5]. Another different strategy entails additional restrictions in the amount of computer memory and time needed to run the algorithm, resulting in a selective pruning of the node expansion while sacrificing optimality; this strategy is similar to the one used in the K-Beam algorithm [9].\nThe CONSTRUE() algorithm is based on the assumption that all the evidence to be explained is available at the beginning of the interpretation task. A new version of the algorithm should be provided to cope with a wide range of problems where the interpretation must be updated as new evidence becomes available over time. Examples of such problems are continuous biosignal monitoring or plan execution monitoring [1]. At the emergence of a new piece of evidence, two reasoning modes may come into play triggered by the CONSTRUE() algorithm: a new explanatory hypothesis can be conjectured by means of the ABDUCE() procedure, or the evidence can be enclosed in an existing hypothesis by means of the SUBSUME() procedure. In this way, the incorporation of new evidence over time is seamlessly integrated into the hypothesize-and-test cycle. Furthermore, to properly address\nthese interpretation scenarios, the heuristics used to guide the search must be updated to account for the timing of the interpretation process, which will lead to the definition of a covering ratio until time t, and the complexity until time t.\nOn the other hand, the present framework has to be tested in different interpretation problems. An example of a problem that fits this approach is chronicle recognition [1]. In this case, the initial evidence is a potentially large collection of time-stamped data, and the objective is to summarize it in a set of chronicles, describing more abstract events and relations. Notions like the focus of attention or the use of abstraction levels have already been used by other authors [8], albeit from a different, non abductive perspective. Currently, in the ECG domain, we are working on some significant open problems, like beat labeling, with some promising results which show a better performance than the previous state-of-the-art automatic machine learning classifiers, and a performance comparable to the assisted machine learning classifiers that benefit from the aid of an expert cardiologist.\nImplementation\nWith the aim of supporting reproducible research, the full source code of the algorithms presented in this paper has been published under an open source license1, along with a knowledge base for the interpretation of the ECG signal strips of all examples in this paper."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work was supported by the Spanish Ministry of Economy and Competitiveness under project TIN2014-55183-R. T. Teijeiro was funded by an FPU grant from the Spanish Ministry of Education (MEC) (ref. AP2010-1012)."
    } ],
    "references" : [ {
      "title" : "An Introduction to Constraint-Based Temporal Reasoning",
      "author" : [ "R. Barták", "R.A. Morris", "K.B. Venable" ],
      "venue" : "Synthesis Lectures on Artificial Intelligence and Machine Learning,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2014
    }, {
      "title" : "A spectrum of definitions for temporal model-based diagnosis",
      "author" : [ "V. Brusoni", "L. Console", "P. Terenziani", "D. Theseider Dupré" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1998
    }, {
      "title" : "Motivation analysis, abductive unification and nonmonotonic equality",
      "author" : [ "E. Charniak" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1989
    }, {
      "title" : "On optimum recognition error and reject tradeoff",
      "author" : [ "C.K. Chow" ],
      "venue" : "IEEE Transaction on Information Theory,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1970
    }, {
      "title" : "Using compiled knowledge to guide and focus abductive diagnosis",
      "author" : [ "L. Console", "L. Portinale", "D. Theseider Dupré" ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1996
    }, {
      "title" : "A spectrum of logical definitions of model-based diagnosis",
      "author" : [ "L. Console", "P. Torasso" ],
      "venue" : "Computational Intelligence,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1991
    }, {
      "title" : "Chronicle recognition improvement using temporal focusing and hierarchization",
      "author" : [ "C. Dousson", "P. Le Maigat" ],
      "venue" : "In The International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2007
    }, {
      "title" : "Heuristic Search: Theory and Applications",
      "author" : [ "S. Edelkamp", "S. Schrödl" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2011
    }, {
      "title" : "KBFS: K-best-first search",
      "author" : [ "A Felner", "S Kraus", "RE Korf" ],
      "venue" : "Annals of Mathematics and Artificial Intelligence,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2003
    }, {
      "title" : "Watson: Beyond Jeopardy",
      "author" : [ "D. Ferrucci", "A. Levas", "S. Bagchi", "D. Gondek", "E.T. Mueller" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2012
    }, {
      "title" : "Abduction and induction: Syllogistic and inferential perspectives",
      "author" : [ "P. Flach" ],
      "venue" : "University of Bristol,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1996
    }, {
      "title" : "Reject option with multiple thresholds",
      "author" : [ "G. Fumera", "F. Roli", "G. Giacinto" ],
      "venue" : "Pattern Recognition,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2000
    }, {
      "title" : "PhysioBank, PhysioToolkit, and PhysioNet: Components of a New Research Resource for Complex",
      "author" : [ "A.L. Goldberger" ],
      "venue" : "Physiologic Signals. Circulation,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2000
    }, {
      "title" : "Collected papers of Charles Sanders Peirce",
      "author" : [ "C. Hartshorn" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1931
    }, {
      "title" : "Interpretation as abduction",
      "author" : [ "J.R. Hobbs", "M. Stickel", "P. Martin" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1993
    }, {
      "title" : "Abductive inference. Computation, philosophy, technology",
      "author" : [ "J.R. Josephson", "S.G. Josephson" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1994
    }, {
      "title" : "Computing context-dependent temporal diagnosis in complex domains",
      "author" : [ "J.M. Juarez", "M. Campos", "J. Palma", "R. Marin" ],
      "venue" : "Expert Systems with Applications,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2008
    }, {
      "title" : "Automatic detection of wave boundaries in multilead ECG signals: validation with the CSE database",
      "author" : [ "P. Laguna", "R. Jané", "P. Caminal" ],
      "venue" : "Computers and Biomedical Research,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1994
    }, {
      "title" : "A plan recognition model for subdialogues in conversation",
      "author" : [ "D. Litman", "J. Allen" ],
      "venue" : "Cognitive Science,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1987
    }, {
      "title" : "Time series feature extraction for data mining using DWT and DFT",
      "author" : [ "F. Mörchen" ],
      "venue" : "Technical Report no. 33,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2003
    }, {
      "title" : "Obtaining interpretable fuzzy classification rules from medical data",
      "author" : [ "D. Nauck", "R. Kruse" ],
      "venue" : "Artificial Intelligence in Medicine,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1999
    }, {
      "title" : "Fuzzy theory approach for temporal model-based diagnosis: An application to medical domains",
      "author" : [ "J. Palma", "J.M. Juarez", "M. Campos", "R. Marin" ],
      "venue" : "Artificial Intelligence in Medicine,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2006
    }, {
      "title" : "Abductive inference models for diagnostic problem-solving",
      "author" : [ "Y. Peng", "J.A. Reggia" ],
      "venue" : null,
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 1990
    }, {
      "title" : "Low-complexity detection of atrial fibrillation in continuous long-term monitoring",
      "author" : [ "A. Petrenas", "V. Marozas", "L. Sörnmo" ],
      "venue" : "Computers in biology and medicine,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2015
    }, {
      "title" : "A review of novelty detection",
      "author" : [ "M.A.F. Pimentel", "D.A. Clifton", "L. Clifton", "L. Tarassenko" ],
      "venue" : "Signal Processing,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2014
    }, {
      "title" : "A methodology for using a default and abductive reasoning system",
      "author" : [ "D. Poole" ],
      "venue" : "International Journal of Intelligent Systems,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 1990
    }, {
      "title" : "Combining Decision Support System-Generated Recommendations with Interactive Guideline Visualization for Better Informed Decisions",
      "author" : [ "L. Sacchi", "E. Parimbelli", "S. Panzarasa", "N. Viani", "E. Rizzo", "C. Napolitano", "R. Ioana Budasu", "S. Quaglini" ],
      "venue" : "In Artificial Intelligence in Medicine,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2015
    }, {
      "title" : "A framework for knowledge-based temporal abstraction",
      "author" : [ "Y Shahar" ],
      "venue" : "Artificial intelligence,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 1997
    }, {
      "title" : "Using Temporal Abduction for Biosignal Interpretation: A Case Study on QRS Detection",
      "author" : [ "T. Teijeiro", "P. Felix", "J. Presedo" ],
      "venue" : "IEEE International Conference on Healthcare Informatics,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2014
    }, {
      "title" : "Marriott’s Practical Electrocardiography",
      "author" : [ "Galen S. Wagner" ],
      "venue" : null,
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2008
    }, {
      "title" : "A robust open-source algorithm to detect onset and duration of QRS complexes",
      "author" : [ "W. Zong", "G.B. Moody", "D. Jiang" ],
      "venue" : "In Computers in Cardiology,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 13,
      "context" : "Conversely, abduction, or inference to the best explanation, is a form of inference that goes from data to a hypotheses that best explains or accounts for the data [15].",
      "startOffset" : 164,
      "endOffset" : 168
    }, {
      "referenceID" : 15,
      "context" : "Abductions are thus truth widening, and they can make the leap from the language of observations to the language of the underlying processes and mechanisms, responding to the aforementioned problem in a natural way [17].",
      "startOffset" : 215,
      "endOffset" : 219
    }, {
      "referenceID" : 22,
      "context" : "Abduction has been primarily proposed for diagnostic tasks [24, 6], but also for question answering [11], language understanding [16], story comprehension [3], image understanding [27] or plan recognition [20], amongst others.",
      "startOffset" : 59,
      "endOffset" : 66
    }, {
      "referenceID" : 5,
      "context" : "Abduction has been primarily proposed for diagnostic tasks [24, 6], but also for question answering [11], language understanding [16], story comprehension [3], image understanding [27] or plan recognition [20], amongst others.",
      "startOffset" : 59,
      "endOffset" : 66
    }, {
      "referenceID" : 9,
      "context" : "Abduction has been primarily proposed for diagnostic tasks [24, 6], but also for question answering [11], language understanding [16], story comprehension [3], image understanding [27] or plan recognition [20], amongst others.",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 14,
      "context" : "Abduction has been primarily proposed for diagnostic tasks [24, 6], but also for question answering [11], language understanding [16], story comprehension [3], image understanding [27] or plan recognition [20], amongst others.",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 2,
      "context" : "Abduction has been primarily proposed for diagnostic tasks [24, 6], but also for question answering [11], language understanding [16], story comprehension [3], image understanding [27] or plan recognition [20], amongst others.",
      "startOffset" : 155,
      "endOffset" : 158
    }, {
      "referenceID" : 25,
      "context" : "Abduction has been primarily proposed for diagnostic tasks [24, 6], but also for question answering [11], language understanding [16], story comprehension [3], image understanding [27] or plan recognition [20], amongst others.",
      "startOffset" : 180,
      "endOffset" : 184
    }, {
      "referenceID" : 18,
      "context" : "Abduction has been primarily proposed for diagnostic tasks [24, 6], but also for question answering [11], language understanding [16], story comprehension [3], image understanding [27] or plan recognition [20], amongst others.",
      "startOffset" : 205,
      "endOffset" : 209
    }, {
      "referenceID" : 15,
      "context" : "Even though abductive reasoning has been proved to be NP-complete, a compiled form of abduction, based on a set of prestored hypotheses could narrow the generation of hypotheses [17].",
      "startOffset" : 178,
      "endOffset" : 182
    }, {
      "referenceID" : 29,
      "context" : "Together, these waveforms devise the characteristic pattern of the heart cycle, which is repeated in a normal situation with every beat [31].",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 12,
      "context" : "[Source: MIT-BIH arrhythmia DB [14], recording: 123, between 12:11.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 29,
      "context" : "The RR attribute represents the measure, in milliseconds, of the mean distance between consecutive beats, while q2, q3 and q4 represent the normal cardiac rhythm denominations according to the heart rate [31].",
      "startOffset" : 204,
      "endOffset" : 208
    }, {
      "referenceID" : 29,
      "context" : "The R-on-T phenomenon is the superimposition of an ectopic beat on the T wave of a preceding beat [31].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 29,
      "context" : "The grammar GN = (VN , VT , H = qN , R) is designed to generate an abstraction pattern for a normal cardiac cycle, including the descriptions of common durations and intervals [31].",
      "startOffset" : 176,
      "endOffset" : 180
    }, {
      "referenceID" : 29,
      "context" : "Most often this is due to ectopic heart beats occurring so frequently that there is one after each normal beat, typically premature ventricular contractions (PVCs) [31].",
      "startOffset" : 164,
      "endOffset" : 168
    }, {
      "referenceID" : 22,
      "context" : "Thus, we need additional criteria in order to select the solution to the interpretation problem as the best choice among different possibilities [24].",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 22,
      "context" : "On the other hand, a parsimony criterion is required, in order to disambiguate among possible interpretations to select the most plausible ones as those whose complexity is minimum [24].",
      "startOffset" : 181,
      "endOffset" : 185
    }, {
      "referenceID" : 7,
      "context" : "The coverage and simplicity principles are used to define a cost measure for the heuristic search process [9], while the abstraction and predictability principles are used to guide the reasoning process, trying to emulate the same shortcuts used by humans.",
      "startOffset" : 106,
      "endOffset" : 109
    }, {
      "referenceID" : 8,
      "context" : "This algorithm is a small variation of the K-Best First Search algorithm [10], with partial expansion to reduce the number of explored nodes.",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 17,
      "context" : "This method may be any of those described in the literature, such as [19].",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 29,
      "context" : "For example, in the ECG interpretation problem, the minimum slope constraint defined by the QRS abstraction pattern is a reliable feature to select more promising signal segments to focus the interpretation on, in the same way that a cardiologist focuses on the prominent peaks in the signal to start the analysis [31].",
      "startOffset" : 314,
      "endOffset" : 318
    }, {
      "referenceID" : 19,
      "context" : "It is common to use a representation in transform domain, where certain regular processes are easily separable, though at the expense of a cumbersome representation of the temporal information [21].",
      "startOffset" : 193,
      "endOffset" : 197
    }, {
      "referenceID" : 29,
      "context" : "Atrial fibrillation, a common heart arrhythmia caused by the independent and erratic contractions of the atrial muscle fibers, is characterized by an irregularly irregular heart rhythm [31].",
      "startOffset" : 185,
      "endOffset" : 189
    }, {
      "referenceID" : 23,
      "context" : "Most of the classification techniques for the identification of atrial fibrillation are based on the analysis of the time interval between consecutive beats, trying to detect this irregularity [25].",
      "startOffset" : 193,
      "endOffset" : 197
    }, {
      "referenceID" : 26,
      "context" : "In the strip of figure 8, obtained during a pilot study for the home follow-up of patients with cardiac diseases [28], such a classifier would wrongly identify this segment as an atrial fibrillation episode, since the observed rhythm variability is consistent with the description of this arrhythmia.",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 26,
      "context" : "[Source: Mobiguide Project [28], private recording]",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 3,
      "context" : "Such classifiers may additionally include a reject option for all those instances that could be misclassified as they appear too close to the classification boundaries [4, 13].",
      "startOffset" : 168,
      "endOffset" : 175
    }, {
      "referenceID" : 11,
      "context" : "Such classifiers may additionally include a reject option for all those instances that could be misclassified as they appear too close to the classification boundaries [4, 13].",
      "startOffset" : 168,
      "endOffset" : 175
    }, {
      "referenceID" : 24,
      "context" : "An approach to this problem can be found in novelty detection proposals [26], which can detect when a new instance does not fit any of the predefined classes as it substantially differs from those instances available during training.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 30,
      "context" : "Let the initial evidence be the set of QRS annotations obtained by a state-of-the art detection algorithm [32].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 30,
      "context" : "The initial evidence is again a set of QRS annotations obtained by a state-of-the art detection algorithm [32].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 28,
      "context" : "The capability of abduction to ignore or look for new evidence has been tested with a simplified version of the present framework in the QRS detection problem [30], leading to a statistically significant improvement over a state-of-the art algorithm.",
      "startOffset" : 159,
      "endOffset" : 163
    }, {
      "referenceID" : 20,
      "context" : "Interpretability of a reasoning formalism, defined as the ability to understand and evaluate its conclusions, is an essential feature to achieve an adequate confidence for decision making [22].",
      "startOffset" : 188,
      "endOffset" : 192
    }, {
      "referenceID" : 1,
      "context" : "Let us deepen in both the backward and forward logical meaning of an abstraction pattern, following a reasoning similar to that of [2]:",
      "startOffset" : 131,
      "endOffset" : 134
    }, {
      "referenceID" : 10,
      "context" : "An abstraction pattern satisfies the compositionality principle of abductive reasoning, and hence an abstraction hypothesis can be conjectured from a single piece of evidence, and new pieces of evidence can be later added [12].",
      "startOffset" : 222,
      "endOffset" : 226
    }, {
      "referenceID" : 15,
      "context" : "An abstraction model, built on a set of abstraction patterns, establishes a causal responsibility for the behavior observed in a complex system [17].",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 5,
      "context" : "This notion of causality is behind perception, concerned at the explanation of sensory data, in contrast with the notion of causality in diagnosis, concerned at the explanation of abnormality [6].",
      "startOffset" : 192,
      "endOffset" : 195
    }, {
      "referenceID" : 1,
      "context" : "Representing and reasoning about context is a relevant issue in model-based diagnosis [2, 6, 24, 29].",
      "startOffset" : 86,
      "endOffset" : 100
    }, {
      "referenceID" : 5,
      "context" : "Representing and reasoning about context is a relevant issue in model-based diagnosis [2, 6, 24, 29].",
      "startOffset" : 86,
      "endOffset" : 100
    }, {
      "referenceID" : 22,
      "context" : "Representing and reasoning about context is a relevant issue in model-based diagnosis [2, 6, 24, 29].",
      "startOffset" : 86,
      "endOffset" : 100
    }, {
      "referenceID" : 27,
      "context" : "Representing and reasoning about context is a relevant issue in model-based diagnosis [2, 6, 24, 29].",
      "startOffset" : 86,
      "endOffset" : 100
    }, {
      "referenceID" : 16,
      "context" : "Some other works enable the same observation to play different roles in different causal patterns, providing some general operations for expressing common changes made by the context in a diagnostic pattern [18, 23].",
      "startOffset" : 207,
      "endOffset" : 215
    }, {
      "referenceID" : 21,
      "context" : "Some other works enable the same observation to play different roles in different causal patterns, providing some general operations for expressing common changes made by the context in a diagnostic pattern [18, 23].",
      "startOffset" : 207,
      "endOffset" : 215
    }, {
      "referenceID" : 4,
      "context" : "On the one hand, the exploitation of the model structure to identify necessary and sufficient conditions for every hypotheses to be conjectured; necessary conditions avoid the expansion of those hypotheses that can be ruled out because they are inconsistent with observations, while sufficient conditions avoid the construction of redundant interpretations [5].",
      "startOffset" : 357,
      "endOffset" : 360
    }, {
      "referenceID" : 7,
      "context" : "Another different strategy entails additional restrictions in the amount of computer memory and time needed to run the algorithm, resulting in a selective pruning of the node expansion while sacrificing optimality; this strategy is similar to the one used in the K-Beam algorithm [9].",
      "startOffset" : 280,
      "endOffset" : 283
    }, {
      "referenceID" : 0,
      "context" : "Examples of such problems are continuous biosignal monitoring or plan execution monitoring [1].",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 0,
      "context" : "An example of a problem that fits this approach is chronicle recognition [1].",
      "startOffset" : 73,
      "endOffset" : 76
    }, {
      "referenceID" : 6,
      "context" : "Notions like the focus of attention or the use of abstraction levels have already been used by other authors [8], albeit from a different, non abductive perspective.",
      "startOffset" : 109,
      "endOffset" : 112
    } ],
    "year" : 2016,
    "abstractText" : "Time series interpretation aims to provide an explanation of what is observed in terms of its underlying processes. The present work is based on the assumption that common classification-based approaches to time series interpretation suffer from a set of inherent weaknesses whose ultimate cause lies in the monotonic nature of the deductive reasoning paradigm. In this document we propose a new approach to this problem based on the initial hypothesis that abductive reasoning properly accounts for the human ability to identify and characterize patterns appearing in a time series. The result of the interpretation is a set of conjectures in the form of observations, organized into an abstraction hierarchy, and explaining what has been observed. A knowledge-based framework and a set of algorithms for the interpretation task are provided, implementing a hypothesize-and-test cycle guided by an attentional mechanism. As a promising application domain, the interpretation of the electrocardiogram allows us to highlight the strengths of the present approach in comparison with traditional classification-based approaches.",
    "creator" : "LaTeX with hyperref package"
  }
}