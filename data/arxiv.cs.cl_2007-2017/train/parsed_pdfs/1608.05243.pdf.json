{
  "name" : "1608.05243.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "marasovic@cl.uni-heidelberg.de", "frank@cl.uni-heidelberg.de" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 8.\n05 24\n3v 1\n[ cs\n.C L\n] 1\n8 A\nug 2\n01 6"
    }, {
      "heading" : "1 Introduction",
      "text" : "Factuality recognition (de Marneffe et al., 2012; Lee et al., 2015) is a subtask in information extraction that differentiates facts from hypotheses and speculation, expressed through signals of modality, most prominently, modal verbs and adverbs. Modal verbs are, however, ambiguous between an epistemic sense (possibility) as opposed to non-epistemic deontic (permission/obligation) or dynamic (capability) senses, as in: He could be at home (epistemic), You can enter now (deontic) and Only John can solve this problem (capability).\nModal sense classification (MSC) is a special case of sense disambiguation that is also relevant in areas of dialogue act and plan recognition in AI, as well as novel tasks such as argumentation mining. Prior work (Ruppenhofer and Rehbein, 2012; Zhou et al., 2015) addressed the task with featurebased classification. However, even with carefully designed semantic features the models have difficulties beating the majority sense baseline in cases of difficult sense distinctions and when applying the models to heterogenous text genres.\nWe cast modal sense classification as a novel semantic sentence classification task using a convolutional neural network (CNN) architecture. Our contributions are: (i) our experiments on MSC confirm the adequacy of CNNs for modeling propositions in semantic sentence classification tasks (cf. Kim (2014)); (ii) we show that automatically learned features in a CNN outperform manually designed features for difficult modal verbs and novel genres; (iii) we demonstrate that the CNN approach can be generalized across languages, by adapting the model to German. (iv) We offer insights into the linguistic properties captured by the learned feature maps. Finally, (v) we benchmark the CNN on a standard WSD task, comparing it to a WSD model using rich sense-disambiguated embeddings and obtain comparable results."
    }, {
      "heading" : "2 Prior and related work",
      "text" : "Modal sense classification (MSC). We focus on disambiguation of modal verbs, adopting the sense inventory established in formal semantics: epistemic, deontic/bouletic and circumstantial/dynamic.1 We compare to prior work in Ruppenhofer and Rehbein (2012) and followup work in Zhou et al. (2015) (henceforth, R&R and Z+). R&R induced modal sense classifiers from manual annotations on the MPQA corpus (Wiebe et al., 2005) using word-based and syntactic features. Z+ propose an extended semantically informed model that significantly outperforms R&R’s results. Z+ also create heuristically sense-annotated training data from parallel corpora, to overcome sparsity and bias in the MPQA corpus. However, their models do not beat the majority sense baseline for the difficult modal verbs,\n1These senses correspond to (Baker et al., 2010)’s modal categories (with deontic split into requirement and permissive), and R&Rs inventory, with regrouping of concessive, conditional and circumstantial, cf. Zhou et al. (2015).\nmay, can and could. Modal sense classification interacts with genre and domain differences. Prabhakaran et al. (2012) observe strong cross-genre effects and missing generalization capacities when applying their modality classifier to out-of-domain genres.\nWord Embeddings and Sense Disambiguation. Taghipour and Ng (2015) investigate the impact of word embeddings on classical WSD, using pretrained embeddings and tuning them to the task using a NN. Both variants, integrated into the stateof-the-art system IMS (Zhong and Ng, 2012), improve WSD performance on benchmark tasks.\nOrdinary word embeddings do not differentiate word senses. Rothe and Schütze (2015) explore supervised WSD using sense-specific embeddings, which they induce by exploiting sense encodings and constraints given by a lexical resource.2 Integrating the sense-specific vectors into IMS yields significant improvements and small gains relative to Taghipour and Ng (2015). Hence, word embeddings – tuned to the task or sensespecific – prove beneficial for supervised WSD.\nThe CNN approach we investigate in our work does not employ a fixed feature space or a predefined window around the target word. It flexibly learns feature maps for variable window sizes over the embedding matrix for the full sentence. In contrast to Rothe and Schütze (2015), embeddings used by our CNNs models are knowledgelean and do not encode senses of the target words.\nSentence classification using CNNs. Recent work investigates NN architectures and their ability to capture the semantics of sentences for various classification tasks. Kalchbrenner et al. (2014) construct a dynamic CNN that builds on unparsed input and achieves performance beyond strong baselines for sentiment and question type classification. By contrast, recursive neural networks (Socher et al., 2013) take parsed input, recursively generate representations for intermediate phrases, and perform classification on the basis of the full sentence representation.\nKim (2014) evaluates a one-layer CNN on various benchmark tasks for sentence classification. CNNs trained on pre-trained (static) embeddings\n2 Modal verbs are not or not systematically covered in WordNets or VerbNet; FrameNet relates modal verbs to their predominant sense only. Also, FrameNet’s frame-to-frame relations are known to lack coverage (Burchardt et al., 2009).\nperform well and can be further improved by tuning them to the task (non-static). Using two channels did not significantly improve results. Overall, the CNNs show consistently strong performance, improving on state-of-the-art results in 4 out of 7 tasks, i.a., sentiment and opinion classification."
    }, {
      "heading" : "3 A CNN for modal sense classification",
      "text" : "We aim at a NN approach to MSC that (i) improves over existing feature-based classifiers, (ii) alleviates manual crafting of features, (iii) generalizes over various text genres, and (iv) is easily portable to novel languages. Besides this, MSC is a special kind of WSD, in that modal verbs have a restricted sense inventory shared across languages, and act as operators that take a full proposition as argument. We thus cast MSC as a semantic sentence classification task in a CNN architecture, adopting the one-layer CNN model of Kim (2014), a variant of Collobert et al. (2011). Unlike Kim (2014) we will use only one channel, but experiment with various types of word vectors.\nA CNN represents a sentence with a fixed size vector, passed to classifier to classify the sentence into task-specific target categories. In our case, it will classify sentences into three modal sense categories. The input layer is a matrix x ∈ Rs×d, with each row corresponding to a d-dimensional word embedding xi ∈ Rd of a word in the sentence of length s. Word embeddings can be randomly initialized or pre-trained vectors, e.g. word2vec (Mikolov et al., 2013) or dependency-based (Levy and Goldberg, 2014) embeddings. Based on the input layer, a CNN builds up one or more convolutional layers. A convolution is an operation between sub-matrices of the input matrix x ∈ Rs×d and a filter parametrised by a weight matrix w ∈ Rn×d, that returns a vector usually referred to as a feature map. Formally, let xi−n+1:i be the sub-matrix of the input matrix x from the (i−n+1)-th row to the i-th row and let 〈. , .〉F denote the sum of elements of the component-wise inner product of two matrices, known as Frobenius inner product. The i-th component of the feature map c is obtained by taking the Frobenius inner product of the sub-matrix xi−n+1:i with the filter matrix w\nci = 〈xi−n+1:i,w〉F , (1)\nfor i ∈ {n, . . . , s}3. Afterwards, we add a bias 3We apply the narrow type of convolution.\nterm, b ∈ R to every component of the feature map and apply an activation function f ,\nc̃i = f(ci + b) . (2)\nFinally, max-over-time pooling (Collobert et al., 2011) is applied over a single feature map that extracts the maximum value ĉ = max{c̃}, which represents the chosen feature for this feature map. Like Kim (2014) we don’t use just one filter as described, but multiple filters with different region sizes n, resulting in multiple feature maps. Features obtained through maxpooling from each feature map are concatenated to a vector representation of the input sentence that is passed to the softmax layer. Parameters to learn are elements of the filter matrices and the input matrix when word vectors are tuned.\nFilters are trained to be especially active when they encounter a sequence of words relevant for the given classification task. Kalchbrenner et al. (2014) present n-grams of different feature detectors that capture positive or negative sentiment phrases, and also more abstract semantic categories, such as negation or degree particles (’too’) that are relevant in compositional sentiment detection. In the modal sense classification task, we expect the feature maps to capture semantic categories found to be relevant in prior work, such as tense, aspectual classes, negation and semantic properties of verbs and phrases. Moreover, prior work has shown that MSC profits from features that model the wider syntactic context, esp. subject and embedded verb and their semantics (abstractness, semantic class, aspect, tense). Explicit modeling of these features as in Z+ improves performance, but requires feature design for each new language. Also, modeling semantic features through lexical resources is subject to sparsity, and relying on parsed input leads to lack of robustness.\nGiven that MSC profits from semantic features in the wider syntactic context, we expect that a CNN that applies filters of variable sizes to various regions of the sentence to learn feature maps can capture diverse linguistic features, and offers greater flexibility compared to a conventional WSD model with a fixed window size centered around the target word. To investigate these special properties of the CNN model, we test it on English and German data. While in English, subject, modal and embedded verb are in a close syntactic\ncontext, in German, they can be distributed over wider distances, and the feature maps are expected to capture properties over wider distances.\nWe perform experiments for MSC for English and German, using various data sets. Section 4 presents the data, experimental settings and the model variations we investigate. We perform detailed quantitative and qualitative evaluation of our experimental results. In Section 5, we evaluate the CNN approach in a lexical sample WSD task, to benchmark its performance on a well-studied data set, and to investigate the potential advantage of learning feature maps based on flexible window sizes. To our knowledge, this constitutes the first attempt to apply a CNN model in a WSD task."
    }, {
      "heading" : "4 Modal sense classification",
      "text" : ""
    }, {
      "heading" : "4.1 Data",
      "text" : "Our experiments are based on three data sets. Their basic composition is given in Table 1.4\n1) MPQA + EPOSE The English benchmark data set MPQA from R&R was further enriched through balanced heuristically tagged training data, EPOSE, by Z+. The EPOSE data set was obtained using a cross-lingual sense projection approach. Z+ identified paraphrases for modal senses (e.g. brauchen-need; erlauben-permit for deontic, schaffen-able to for dynamic sense), extracted sentences from a parallel corpus with a modal verb aligned to a sense-identifying paraphrase, and tagged them with the identified modal sense. Z+ measured 0.92 accuracy on 420 instances of the heuristically tagged corpora. To alleviate distributional bias stemming from the MPQA dataset, Z+ balanced the blend of MPQA with EPOSE using under- and oversampling. We experiment with both versions (± balanced).5\n2) MASC A subset of the multi-genre corpus MASC (Ide et al., 2008), consisting of 19 genres was manually annotated (Anonymous) with modal senses for the same modal verbs. The annotated data consists of ≈100 instances for each genre.6\n3) EPOSG Following the method of Z+, we constructed a German data set EPOSG from the\n4More detailed information will be provided through accompanying material with the final version. The annotated MASC and EPOSG data sets will be made publicly available.\n5Their data is publicly available through their website. We omit shall from MPQA, due to low number of occurrences.\n6Exceptions with less than 100 instances are journal, newspaper, technical, travel guides, and telephone.\nEuroparl and OpenSubtitles corpora of OPUS (Tiedemann, 2012) by projecting modal sense categories from English to German, using selected modal sense identifying English paraphrases. The resulting corpus with sense-tagged German modal verbs können (can), müssen (must), sollen (should), dürfen (may) consists of a manually validated test section consisting of up to 100 instances for each sense. Annotation was done by two independent judges and one adjudicator. Balanced training data of 1000 instances per sense for each modal verb was constructed from heuristically tagged sentences that were judged highquality by validating 20 instances for each paraphrase. For modal verbs with rare extractions, we added training data from modal verbs of shared senses, changing their verb forms to the verb form of the target verb.7"
    }, {
      "heading" : "4.2 Experimental settings",
      "text" : "MSC on MPQA using CNN-EB and CNN-EU, CV For MSC we benchmark the CNN approach against the latest state-of-the-art results in Z+. We reimplemented their maximum entropy classifier (henceforth, MaxEnt) and trained it on their balanced and unbalanced blend of MPQA and EPOS.8 As in Z+ we train independent classifiers for each modal verb on their respective training data.9 For evaluation, we perform 5-fold cross validation as in Z+. Each fold for training holds a stratified 80% section of the MPQA data together\n7Replacing e.g. könnte with dürfte in Es könnte Dir gefallen extracted from You might get a taste for it.\n8We omit shall with a small number of instances. 9This holds for all our experiments.\nwith the full EPOSE data set, and we use the remaining 20% of MPQA data for testing. We refer to the CNN models trained on the ±balanced versions of this data as CNN-EB and CNN-EU.\nMSC on MASC using CNN-EB and CNN-EU Besides MPQA, we evaluate the CNN on the multi-genre MASC (sub)corpus. For comparability with Z+, for training we use one training fold from the previous setting,10 and evaluate on MASC as test. We analyze the performance of the CNN model overall and on different genre subcorpora (not reported here).\nBoth English data sets are characterized by modest training set sizes and involve a considerable distributional biases, with high most frequent sense majority baselines (cf. Tables 3 and 4).\nMSC on EPOSG using CNN-G In constrast to the English data sets, the German EPOSG data set provides larger training set sizes of 1000 instances for all modal verbs and senses. This eliminates distributional bias from the data, so that the discriminating power of the classifier model is not masqued by distributional information."
    }, {
      "heading" : "4.3 Model variations",
      "text" : "Hyperparameters Model-specific hyperparameters of the CNN are the number of filters, filter region size, and the depth of the network. We restrict our model to a one-dimensional CNN architecture.\nFollowing the advices in Zhang and Wallace (2015), we used following setting: ReLU (rectified linear unit) as activation function, filter region sizes of 3, 4, and 5 with 100 feature maps each, dropout keep probability of 0.5, l2 regularisation coefficient of 10−3, number of iterations of 100111 and minibatch size of 50. Training is done with the Adam optimisation algorithm (Kingma and Ba, 2014) with learning rate of 10−4. Filter weights are initialized using Glorot-Bengio strategy (Glorot and Bengio, 2010). We experimented with some parameter variations (using nested CV), but found no consistently better results. In all following MSC experiments we thus used this hyperparameter setting for CNN training.\n10Hence, one 80% fold of MPQA plus EPOSE. Despite this small difference, we refer to the CNN models as above, as CNN-EB and CNN-EU.\n11We did not perform early stopping.\nWord embeddings In the first and third experimental setting we investigate the impact of static and tuned versions of different word vectors: word2vec (Mikolov et al., 2013), dependencybased (Levy and Goldberg, 2014) and randomly initialized embeddings.\nWe used publicly available word2vec vectors that were trained on Google News for English12 and various datasets for German (Reimers et al., 2014)13, as well as English dependency-based vectors trained on Wikipedia14 . The German dependency-based embeddings were trained on the SdeWaC corpus (Faaß and Eckart, 2013), parsed with Malt parser. We used 300 dimensions for English embeddings and 100 for German.\nFor words without a pre-trained vector and in the random initialization setting, each dimension of the random vector was sampled from U ∼ [−a, a] with parameter a picked such that the variance of the uniform distribution equals the variance of the available pre-trained vectors.\nBaselines For MPQA and MASC, the classifiers are compared against strong majority sense baselines, BLmaj , due to skewed sense distributions in the training data. Further, we compare the CNN results to the reconstructed MaxEnt classifier from Z+, trained on the blend of MPQA and EPOS with R&R’s shallow lexical and syntactic path features and the newly designed semantic features of Z+.\nTo our knowledge, there is no work on modal sense classification using a neural network. We thus compare our CNN models with a simple, onelayer neural network NN to investigate the impact offered by the more complex CNN architecture.\nInput to the NN is the sum of all vectors of the words in the sentence. As for the CNN, we experimented with different types of word vectors.\nThe hyperparameter setting for the NN is: ReLU as activation function, l2 regularisation coefficient of 10−3, hidden layer size of 1024, number of iterations of 3001, dropout keep probability of 0.5, and mini-batch size of 50. Training is again done with the Adam optimisation algorithm (Kingma and Ba, 2014) with learning rate of 10−4. Weights are initialized using Glorot-Bengio\n12https://code.google.com/archive/p/word2vec 13https://www.ukp.tu-darmstadt.de/research/ukp-in-\nchallenges/germeval-2014 14https://levyomer.wordpress.com/2014/04/25/dependencybased-word-embeddings\nstrategy (Glorot and Bengio, 2010).15"
    }, {
      "heading" : "4.4 Results",
      "text" : "English\nIn Table 2 we report results for CNN-EB and CNN-EU with diverse input representations. For balanced training, dependency based vectors yield the best (can, could) or equally good results (may, must, should). Could is the only case with large performance differences depending on the choice of embeddings. For can and could choosing either static or tuned versions of vectors is beneficial. With unbalanced training, dependency-based vectors are outperformed by word2vec for must and by randomly initialized vectors for can. Large differences in the results for could w.r.t. the choice of embeddings, are no longer present.\nIn Table 3 we report overall results for CNNEB and CNN-EU on MPQA compared to the baselines. As representations for the NN and CNN we selected, for each modal verb, the embedding type that yielded the best results (Table 2)16.\nFor each training data set, scores of the CNN which are significantly better17 than the next lower\n15This is clearly not shown to be the best hyperparameter setting, as we chose it heuristically without tuning.\n16For NN the impact of word vectors was investigated as well.\n17By conducting the mid-p-value McNemar test\nscore among the baselines are underlined. If CNN does not yield the best results, significance between the baseline with the best score and CNN is reported. Overlining is used if CNN with unbalanced training performs significantly better than CNN with balanced training, and vice versa.\nWith balanced training, CNN outperforms all baselines for every modal verb and in terms of micro average. However, differences between CNN and MaxEnt are significant only for can, could and micro average. Moving to unbalanced training, CNN has difficulties beating the baselines (cf. may, should), but yields the best micro average. Unbalanced training for CNN outperforms balanced training in terms of micro averages, however the difference is not significant.\nTable 4 summarizes the evaluation of CNN-EB and CNN-EU on the MASC corpus. Note that CNN with unbalanced training, CNN-EU, does not have enough generalization capability when applied to different genres. This behavior coincides with changes of the predominant sense between training and test. CNN-EU, as well as MaxEnt, is highly sensitive to such distributional changes. Even though balanced training for CNN leads to a slightly worse micro average when evaluated on MPQA, on MASC CNN–EB yields a +3pp gain in micro average compared to unbalanced training.18\nIn sum, our evaluation shows that the CNN model is able to outperform strong baselines in most configurations. Balanced training shows more consistent results beyond the baselines and is competitive with unbalanced training, without significant difference except for can. In view of genre differences in MASC, the CNN–EB model is more robust against sense changes, and yields overall better results. The strong behaviour on balanced training data shows that the CNN model is able to learn meaningful structure from the data.\nGerman\nIn Table 2 we report results for CNN-G with diverse input representations. Reasons for the slightly weaker performance of dependency-based vectors compared to word2vec (1-2 pp.) can be seen in the smaller size of the training corpus, and possibly greater noise due to parsing errors.\nIn Table 5 we report overall results for CNN-G\n(Fagerland et al., 2013) with p <0.05. 18In contrast to MaxEnt, which does not profit from balanced training.\ncompared to the NN baseline.19 The CNN outperforms both baselines by large margins, per modal verb and in terms of micro average. Given we employed perfectly balanced training data, the classifier performances reflect their ability to learn characteristic information for the classes. Indeed, the NN has great difficulties distinguishing the senses for können (3 senses) and sollen, and is outperformed by CNN-G by +35.6 and +24.4 pp. gains. The confusion matrices for CNN-G show a clear separation of these classes, in contrast to the NN.\nWhile German is a more difficult language than English due to its syntactic properties (word order, degree of inflection), CNN-G reaches overall higher performance levels compared to English, especially for difficult cases.20 One reason can be the morphological distinction between indicative and subjunctive (Konjunktiv), which – in interaction with tense and other factors – can ease the distinction of epistemic vs. deontic/dynamic sense. For sollen this morphological division is masqued, and this can explain the weaker results compared to other binary classes. Generally, CNN-G profits\n19We did not construct a MaxEnt classifier for German. For NN and CNN-G we chose the best performing embedding types per modal verb.\n20Clearly, we cannot draw any strict comparison here.\nfrom larger and perfectly balanced training data."
    }, {
      "heading" : "4.5 Semantic feature detectors",
      "text" : "Z+ provided a thorough analysis of the impact of semantic features by ablating individual feature groups. Their ablation analysis confirmed that feature groups relating to tense and aspect of the embedded verb, negation, abstractness of the subject and semantic features of the embedded verb yield significant effects on classification performance.\nFor must, Z+ found clear patterns for the occurrence of specific features and the ability to properly classify a specific sense. However, they did not identify precise features that differentiate epistemic and dynamic readings with can. We speficically investigated whether the learned filters for must can be related to the semantic categories Z+ found to be important for distinguishing its senses. In addition, we investigated whether the CNN is able to capture unattested features that differentiate epistemic and dynamic readings with can.\nFor every modal verb and every filter, we sort sentences in the training data by the maximum value obtained by applying 1-max pooling to the feature map acquired by applying the respective filter to a sentence. For each filter and each of the top-ranked 15 sentences, we extract the ngram that corresponds to the maximum value w.r.t. the filter, i.e. the argmax of the feature map. The ngram vector is the sum of all vectors of words in the ngram. The obtained ngram vectors were plotted using the t-SNE algorithm (Van der Maaten and Hinton, 2008) and textually displayed with their surrounding context.\nFor must we found many feature detectors that relate to observations in Z+. Many filters detect past (you must have been out last night; ep) vs. non-past (we must make further efforts; de) and a dynamic event (we must develop a policy; de) vs. stative (you must think me a perfect fool; ep) reading of the embedded verb. Among others the feature detectors capture passive constructions (actual steps must be taken; de) and negation (we must not fear; de). Some filters were trained to\ncapture domain vocabulary which intuitively goes along with deontic sense (European parliament; present regulation; fisheries policy). One filter captures telic clauses (to address these problems; to prevent both forum; to exert maximum influence), identifying deontic sense. Novel features not considered in Z+ are discourse markers (but; and (then)) that correlate with deontic sense. All in all, the CNN learns meaningful features that are known to be important for differentiating senses for must, and in contrast to manual feature design, it detects relevant unattested features by itself.\nFor can many filters recognise accomplishments which go along with dynamic sense, e.g. You can do it/make it to NY. Others detect words indicating possibility (ep), negation (de), discourse markers, animate subject (de and dy), passive construction (de and dy). However, without a systematic classification of these features it remains unclear how important they are for differentiating the senses of can. Also, similar to Z+ we did not find clear-cut features that recognize epistemic sense.\nWe performed a corresponding analysis of feature maps for German, following the same extraction procedure. We found the typical state (ep) vs. event (de) contrast for the embedded verb, negation and tense, and again previously unattested factors such as discourse relation markers21 (but; without; thereby; in order to (dy)). For German we identified various indicators for epistemic sense (for müssen and können): attitude predicates (believe, not know; tell me; have an idea, be afraid), adverbials (possibly), conditionals (if); counterfactual and negative polarity contexts (not be the case; how; ever). Further detectors for epistemic sense are abstract subjects: placeholders for propositions (it), abstract concepts (idea; music; grades; application); indefinite subjects (one). We find a tendency for 1st or 2nd person subjects to co-occur with de/dy and 3rd person pronouns with ep. For können (dy) we find achievements (present report; move mountains; find compromise). For deontic readings, next to negation with 1st and 2nd person we find typical verb-object combinations for actions that can be granted: use telephone; communicate with third parties.\nWe extracted statistics about the distance of the extracted ngrams from the modal verb (distance overall; to the left/right and ngrams starting with the modal). There are no greater overall distances\n21For reasons of space we provide translations to English.\nfor German compared to English. However, for German we find significantly more ngrams that include the modal verb, especially for epistemic readings of können, müssen, dürfen that clearly mark subjunctive mood, whereas for sollen, with ambiguous forms for subjunctive and past tense, no such tendency is observed. Thus, the feature maps identify subjunctive marking (in conjunction with other factors) as relevant for classifying epistemic sense, whereas for sollen the lack of this indicator goes along with lower performance. Finally, we observe, for English and German, strikingly larger distances to the left of the modal verb for epistemic readings compared to non-epistemic readings. This can be traced back to indicators in the wider left-embedding context: embedding predicates, subjects, if clauses, etc."
    }, {
      "heading" : "5 Word sense disambiguation",
      "text" : "Next to modal sense classification, we evaluate our CNN model in a classical WSD task. As benchmark corpus we chose the SensEval-3 lexical sample data set (Mihalcea et al., 2004), which was recently applied in Rothe and Schütze (2015) (henceforth R&S) and Taghipour and Ng (2015), using sense-specific embeddings and a NN architecture, respectively (cf. Section 2).\nThe training data size for the 57 target word types ranges from 14 to 263 instances. Sense labels of test instances of a given target word are predicted using the CNN model trained on the training instances for the respective word type.22 We set the CNN hyperparameters to be the same as for MSC, except for mini-batch size and region sizes. Since the training data for some words is below 50 instances, mini-batch size was set to 10. For tuning of the region sizes, we split the training data for each word (80:20 for training and validation) and used static word2vec for the input representation. Among {(1, 2, 3), (2, 3, 4), (3, 4, 5), (4, 5, 6), (5, 6, 7)} the best results were obtained for (5, 6, 7).23\nThe final hyperparameter setting was used to investigate the impact of representations. Among word2vec, dependency-based and randomly initialised, word2vec performed the best, the tuned version being slightly better than static vectors.\n22Training instances in the SensEval-3 dataset can have more than one sense label. For training we randomly picked one of possible labels. Instances which contain more than one marked target word were omitted.\n23However, the differences in the results were minor.\nWe report results for tuned word2vec vectors. We compare our results to the results R&S obtained when using only sense-specific embeddings. These are not the state-of-the-art WSD results they obtain with additional features, namely POS tags of words in a small window around the target word, their discrete representation and local collocations. For sentence representation, R&S used every word in the target word sentence. For sense prediction, they used the following feature vectors that are fed into a linear SVM classifier:\nS-cosine = 〈cos(c, s(1)), . . . , cos(c, s(k))〉 ,\nS-product = 〈c1s (1) 1 , . . . , cns (1) n , . . . , c1s (k) 1 , . . . , cns (k) n 〉 ,\nS-raw = 〈c1, . . . , cn, . . . , s (k) 1 , . . . , s (k) n 〉 ,\nwhere w is a target word with k senses, c is the centroid defined as the sum of all word2vec vectors of words in the sentence and s(j) is the embedding of the j-th synset of w.24 They propose a variant of the S-prod feature vector, Snaive-prod, for which the synset embeddings are the sum of the word2vec vectors of all words in that sysnet.\nThe results are summarised in Table 6. The CNN model compares favorably to the competitor models of R&S using AutoExtend embeddings for WSD. It achieves slightly higher results without explicitly marking the target word, whereas the AutoExtend embeddings encode much richer information: what is the target word, how many possible sense it has, and knowledge-intense sense embeddings for each of its synsets. The CNN is able to compete with the rich AutoExtend model, and future work needs to investigate whether – similar to the S-product setting in R&S – the CNN model can achieve competitive state-of-the-art results by incorporating features corresponding to those of the IMS system of Zhong and Ng (2010)."
    }, {
      "heading" : "6 Conclusion and future work",
      "text" : "We presented an account for multilingual modal sense classification using a CNN architecture. We apply the same architecture in a standard WSD task and achieve competitive results compared to a system using richer embedding information.\n24Obtained using the AutoExtend method of R&S.\nOur one-layer CNN architecture outperforms strong baselines and prior art for MSC in English, including a NN and MaxEnt model, and proves particularly robust in cross-genre classification.\nWe applied the CNN model to German, on a data set of modest size, obtained using crosslingual projection techniques. The CNN-G classifier outperforms a NN model by large margins.\nOur approach can be easily generalized to novel languages without tedious and resource-intensive feature engineering. Through analysis of learned feature maps we gave evidence that the CNN learns both known and novel features for MSC.\nThe attractiveness of the CNN framework lies in its ability to learn (semantic) features from flexible window regions without syntactic processing, and the ensuing robustness on difficult text genres and its ease in generalizing to novel languages."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank Mengfei Zhou for her support with the German corpus construction. This work has been supported by the German Research Foundation as part of the Research Training Group ”Adaptive Preparation of Information from Heterogeneous Sources” (AIPHES) under grant No. GRK 1994/1."
    } ],
    "references" : [ {
      "title" : "A Modality Lexicon and its use in Automatic Tagging",
      "author" : [ "Baker et al.2010] Kathryn Baker", "Michael Bloodgood", "Bonnie J Dorr", "Nathaniel W Filardo", "Lori Levin", "Christine Piatko" ],
      "venue" : "In Proceedings of LREC,",
      "citeRegEx" : "Baker et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Baker et al\\.",
      "year" : 2010
    }, {
      "title" : "Assessing the impact of frame semantics on textual entailment",
      "author" : [ "Marco Pennacchiotti", "Stefan Thater", "Manfred Pinkal" ],
      "venue" : "Natural Langugae Engineering,",
      "citeRegEx" : "Burchardt et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Burchardt et al\\.",
      "year" : 2009
    }, {
      "title" : "Natural language processing (almost) from scratch",
      "author" : [ "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Collobert et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2011
    }, {
      "title" : "Did It Happen? The Pragmatic Complexity of Veridicality Assessment",
      "author" : [ "Christopher D. Manning", "Christopher Potts" ],
      "venue" : "Computational Linguistics,",
      "citeRegEx" : "Marneffe et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Marneffe et al\\.",
      "year" : 2012
    }, {
      "title" : "The mcnemar test for binary matched-pairs data: mid-p and asymptotic are better than exact conditional",
      "author" : [ "Stian Lydersen", "Petter Laake" ],
      "venue" : "BMC medical research methodology,",
      "citeRegEx" : "Fagerland et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Fagerland et al\\.",
      "year" : 2013
    }, {
      "title" : "Understanding the difficulty of training deep feedforward neural networks",
      "author" : [ "Glorot", "Bengio2010] Xavier Glorot", "Yoshua Bengio" ],
      "venue" : "In International conference on artificial intelligence and statistics,",
      "citeRegEx" : "Glorot et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Glorot et al\\.",
      "year" : 2010
    }, {
      "title" : "MASC: The manually annotated sub-corpus of American English",
      "author" : [ "Ide et al.2008] Nancy Ide", "Collin Baker", "Christiane Fellbaum", "Charles Fillmore" ],
      "venue" : "In Proceedings of the Sixth International Conference on Language Resources and Evaluation",
      "citeRegEx" : "Ide et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Ide et al\\.",
      "year" : 2008
    }, {
      "title" : "A convolutional neural network for modelling sentences",
      "author" : [ "Edward Grefenstette", "Phil Blunsom" ],
      "venue" : "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Kalchbrenner et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kalchbrenner et al\\.",
      "year" : 2014
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Yoon Kim" ],
      "venue" : "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Kim.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kim.",
      "year" : 2014
    }, {
      "title" : "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980",
      "author" : [ "Kingma", "Ba2014] Diederik Kingma", "Jimmy Ba" ],
      "venue" : null,
      "citeRegEx" : "Kingma et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2014
    }, {
      "title" : "Event detection and factuality assessment with non-expert supervision",
      "author" : [ "Lee et al.2015] Kenton Lee", "Yoav Artzi", "Yejin Choi", "Luke Zettlemoyer" ],
      "venue" : "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Lee et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2015
    }, {
      "title" : "Dependency-based word embeddings",
      "author" : [ "Levy", "Goldberg2014] Omer Levy", "Yoav Goldberg" ],
      "venue" : "ACL",
      "citeRegEx" : "Levy et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Levy et al\\.",
      "year" : 2014
    }, {
      "title" : "The Senseval-3 English lexical sample task",
      "author" : [ "Mihalcea et al.2004] R. Mihalcea", "T. Chklovski", "A. Kilgarriff" ],
      "venue" : "In Proceedings of SENSEVAL-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text [CD-ROM],",
      "citeRegEx" : "Mihalcea et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Mihalcea et al\\.",
      "year" : 2004
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Statistical modality tagging from rule-based annotations",
      "author" : [ "Michael Bloodgood", "Mona Diab", "Bonnie Dorr", "Lori Levin", "Christine D. Piatko", "Owen Rambow", "Benjamin Van Durme" ],
      "venue" : null,
      "citeRegEx" : "Prabhakaran et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Prabhakaran et al\\.",
      "year" : 2012
    }, {
      "title" : "Germeval-2014: Nested named entity recognition with neural networks",
      "author" : [ "Reimers et al.2014] Nils Reimers", "Judith EckleKohler", "Carsten Schnober", "Jungi Kim", "Iryna Gurevych" ],
      "venue" : "In Gertrud Faaß and Josef Ruppenhofer,",
      "citeRegEx" : "Reimers et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Reimers et al\\.",
      "year" : 2014
    }, {
      "title" : "Autoextend: Extending word embeddings to embeddings for synsets and lexemes",
      "author" : [ "Rothe", "Schütze2015] Sascha Rothe", "Hinrich Schütze" ],
      "venue" : "In Proceedings of the 53rd Annual Meeting of the Association",
      "citeRegEx" : "Rothe et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Rothe et al\\.",
      "year" : 2015
    }, {
      "title" : "Yes we can !? Annotating the senses of English modal verbs",
      "author" : [ "Ruppenhofer", "Rehbein2012] Josef Ruppenhofer", "Ines Rehbein" ],
      "venue" : "In Proceedings of the LREC 2012 Conference,",
      "citeRegEx" : "Ruppenhofer et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Ruppenhofer et al\\.",
      "year" : 2012
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts" ],
      "venue" : null,
      "citeRegEx" : "Socher et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Semi-supervised word sense disambiguation using word embeddings in general and specific domains",
      "author" : [ "Taghipour", "Ng2015] Kaveh Taghipour", "Hwee Tou Ng" ],
      "venue" : "In The 2015 Annual Conference of the North American Chapter",
      "citeRegEx" : "Taghipour et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Taghipour et al\\.",
      "year" : 2015
    }, {
      "title" : "Parallel Data, Tools and Interfaces in OPUS",
      "author" : [ "Jörg Tiedemann" ],
      "venue" : null,
      "citeRegEx" : "Tiedemann.,? \\Q2012\\E",
      "shortCiteRegEx" : "Tiedemann.",
      "year" : 2012
    }, {
      "title" : "Visualizing data using t-sne",
      "author" : [ "Van der Maaten", "Geoffrey Hinton" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Maaten et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Maaten et al\\.",
      "year" : 2008
    }, {
      "title" : "Annotating expressions",
      "author" : [ "Wiebe et al.2005] Janyce Wiebe", "Theresa Wilson", "Claire Cardie" ],
      "venue" : null,
      "citeRegEx" : "Wiebe et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Wiebe et al\\.",
      "year" : 2005
    }, {
      "title" : "A sensitivity analysis of (and practitioners’ guide to) convolutional neural networks for sentence classification",
      "author" : [ "Zhang", "Wallace2015] Ye Zhang", "Byron C. Wallace" ],
      "venue" : "Technical report, University of Texas",
      "citeRegEx" : "Zhang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2015
    }, {
      "title" : "It makes sense: A wide-coverage word sense disambiguation system for free text",
      "author" : [ "Zhong", "Ng2010] Zhi Zhong", "Hwee Tou Ng" ],
      "venue" : "In Proceedings of the ACL 2010 System Demonstrations,",
      "citeRegEx" : "Zhong et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Zhong et al\\.",
      "year" : 2010
    }, {
      "title" : "Word sense disambiguation improves information retrieval",
      "author" : [ "Zhong", "Ng2012] Zhi Zhong", "Hwee Tou Ng" ],
      "venue" : "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
      "citeRegEx" : "Zhong et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Zhong et al\\.",
      "year" : 2012
    }, {
      "title" : "Semantically Enriched Models for Modal Sense Classification",
      "author" : [ "Zhou et al.2015] Mengfei Zhou", "Anette Frank", "Annemarie Friedrich", "Alexis Palmer" ],
      "venue" : "In Proceedings of the EMNLP 2015 Workshop LSDSem: Linking Models of Lexical,",
      "citeRegEx" : "Zhou et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "Factuality recognition (de Marneffe et al., 2012; Lee et al., 2015) is a subtask in information extraction that differentiates facts from hypotheses and speculation, expressed through signals of",
      "startOffset" : 23,
      "endOffset" : 67
    }, {
      "referenceID" : 26,
      "context" : "Prior work (Ruppenhofer and Rehbein, 2012; Zhou et al., 2015) addressed the task with featurebased classification.",
      "startOffset" : 11,
      "endOffset" : 61
    }, {
      "referenceID" : 8,
      "context" : "Kim (2014)); (ii) we show that automatically learned features in a CNN outperform manually designed features for difficult modal verbs and novel genres; (iii) we demonstrate that the CNN approach can be generalized across languages, by adapting the model to German.",
      "startOffset" : 0,
      "endOffset" : 11
    }, {
      "referenceID" : 22,
      "context" : "R&R induced modal sense classifiers from manual annotations on the MPQA corpus (Wiebe et al., 2005) using word-based and syntactic features.",
      "startOffset" : 79,
      "endOffset" : 99
    }, {
      "referenceID" : 25,
      "context" : "1 We compare to prior work in Ruppenhofer and Rehbein (2012) and followup work in Zhou et al. (2015) (henceforth, R&R and Z+).",
      "startOffset" : 82,
      "endOffset" : 101
    }, {
      "referenceID" : 0,
      "context" : "These senses correspond to (Baker et al., 2010)’s modal categories (with deontic split into requirement and permissive), and R&Rs inventory, with regrouping of concessive, conditional and circumstantial, cf.",
      "startOffset" : 27,
      "endOffset" : 47
    }, {
      "referenceID" : 0,
      "context" : "These senses correspond to (Baker et al., 2010)’s modal categories (with deontic split into requirement and permissive), and R&Rs inventory, with regrouping of concessive, conditional and circumstantial, cf. Zhou et al. (2015).",
      "startOffset" : 28,
      "endOffset" : 227
    }, {
      "referenceID" : 14,
      "context" : "Prabhakaran et al. (2012) observe strong cross-genre effects and missing generalization capacities when applying their modality classifier to out-of-domain genres.",
      "startOffset" : 0,
      "endOffset" : 26
    }, {
      "referenceID" : 18,
      "context" : "By contrast, recursive neural networks (Socher et al., 2013) take parsed input, recursively generate representations for intermediate phrases, and perform classification on the basis of the full sentence representation.",
      "startOffset" : 39,
      "endOffset" : 60
    }, {
      "referenceID" : 7,
      "context" : "Kalchbrenner et al. (2014) construct a dynamic CNN that builds on unparsed input and achieves performance beyond strong baselines for sentiment and question type classification.",
      "startOffset" : 0,
      "endOffset" : 27
    }, {
      "referenceID" : 1,
      "context" : "Also, FrameNet’s frame-to-frame relations are known to lack coverage (Burchardt et al., 2009).",
      "startOffset" : 69,
      "endOffset" : 93
    }, {
      "referenceID" : 13,
      "context" : "word2vec (Mikolov et al., 2013) or dependency-based (Levy and Goldberg, 2014) embeddings.",
      "startOffset" : 9,
      "endOffset" : 31
    }, {
      "referenceID" : 7,
      "context" : "We thus cast MSC as a semantic sentence classification task in a CNN architecture, adopting the one-layer CNN model of Kim (2014), a variant of Collobert et al.",
      "startOffset" : 119,
      "endOffset" : 130
    }, {
      "referenceID" : 2,
      "context" : "We thus cast MSC as a semantic sentence classification task in a CNN architecture, adopting the one-layer CNN model of Kim (2014), a variant of Collobert et al. (2011). Unlike Kim (2014) we will use only one channel, but experiment with various types of word vectors.",
      "startOffset" : 144,
      "endOffset" : 168
    }, {
      "referenceID" : 2,
      "context" : "We thus cast MSC as a semantic sentence classification task in a CNN architecture, adopting the one-layer CNN model of Kim (2014), a variant of Collobert et al. (2011). Unlike Kim (2014) we will use only one channel, but experiment with various types of word vectors.",
      "startOffset" : 144,
      "endOffset" : 187
    }, {
      "referenceID" : 2,
      "context" : "(Collobert et al., 2011) is applied over a single feature map that extracts the maximum value ĉ = max{c̃}, which represents the chosen feature for this feature map.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 2,
      "context" : "(Collobert et al., 2011) is applied over a single feature map that extracts the maximum value ĉ = max{c̃}, which represents the chosen feature for this feature map. Like Kim (2014) we don’t use just one filter as described, but multiple filters with different region sizes n, resulting in multiple feature maps.",
      "startOffset" : 1,
      "endOffset" : 181
    }, {
      "referenceID" : 7,
      "context" : "Kalchbrenner et al. (2014) present n-grams of different feature detectors that capture positive or negative sentiment phrases, and also more abstract semantic categories, such as negation or degree particles (’too’) that are relevant in compositional sentiment detection.",
      "startOffset" : 0,
      "endOffset" : 27
    }, {
      "referenceID" : 6,
      "context" : "2) MASC A subset of the multi-genre corpus MASC (Ide et al., 2008), consisting of 19 genres",
      "startOffset" : 48,
      "endOffset" : 66
    }, {
      "referenceID" : 20,
      "context" : "Europarl and OpenSubtitles corpora of OPUS (Tiedemann, 2012) by projecting modal sense categories from English to German, using selected modal sense identifying English paraphrases.",
      "startOffset" : 43,
      "endOffset" : 60
    }, {
      "referenceID" : 13,
      "context" : "Word embeddings In the first and third experimental setting we investigate the impact of static and tuned versions of different word vectors: word2vec (Mikolov et al., 2013), dependencybased (Levy and Goldberg, 2014) and randomly initialized embeddings.",
      "startOffset" : 151,
      "endOffset" : 173
    }, {
      "referenceID" : 15,
      "context" : "We used publicly available word2vec vectors that were trained on Google News for English12 and various datasets for German (Reimers et al., 2014)13, as well as English dependency-based vectors trained on Wikipedia14 .",
      "startOffset" : 123,
      "endOffset" : 145
    }, {
      "referenceID" : 4,
      "context" : "(Fagerland et al., 2013) with p <0.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 12,
      "context" : "As benchmark corpus we chose the SensEval-3 lexical sample data set (Mihalcea et al., 2004), which was recently applied in Rothe and Schütze (2015) (henceforth R&S) and Taghipour and Ng (2015), using sense-specific embeddings and a NN architecture, respectively (cf.",
      "startOffset" : 68,
      "endOffset" : 91
    }, {
      "referenceID" : 12,
      "context" : "As benchmark corpus we chose the SensEval-3 lexical sample data set (Mihalcea et al., 2004), which was recently applied in Rothe and Schütze (2015) (henceforth R&S) and Taghipour and Ng (2015), using sense-specific embeddings and a NN architecture, respectively (cf.",
      "startOffset" : 69,
      "endOffset" : 148
    }, {
      "referenceID" : 12,
      "context" : "As benchmark corpus we chose the SensEval-3 lexical sample data set (Mihalcea et al., 2004), which was recently applied in Rothe and Schütze (2015) (henceforth R&S) and Taghipour and Ng (2015), using sense-specific embeddings and a NN architecture, respectively (cf.",
      "startOffset" : 69,
      "endOffset" : 193
    } ],
    "year" : 2016,
    "abstractText" : "Modal sense classification (MSC) is a special WSD task that depends on the meaning of the proposition in the modal’s scope. We explore a CNN architecture for classifying modal sense in English and German. We show that CNNs are superior to manually designed feature-based classifiers and a standard NN classifier. We analyze the feature maps learned by the CNN and identify known and previously unattested linguistic features. We benchmark the CNN on a standard WSD task, where it compares favorably to models using sense-disambiguated target vectors.",
    "creator" : "LaTeX with hyperref package"
  }
}