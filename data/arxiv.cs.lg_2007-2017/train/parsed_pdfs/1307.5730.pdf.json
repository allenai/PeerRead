{
  "name" : "1307.5730.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A New Strategy of Cost-Free Learning in the Class Imbalance Problem",
    "authors" : [ "Xiaowan Zhang" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n30 7.\n57 30\nv1 [\ncs .L\nG ]\n2 2\nJu l 2\n01 3\nIndex Terms—Classification, class imbalance, cost-free learning, cost-sensitive learning, abstaining, mutual information, ROC.\n✦"
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "IMBALANCED data sets [1], [2] arise frequently in avariety of real-world applications, such as medicine, biology, finance, and computer vision. Generally, users focus more on the minority class and consider the cost of misclassifying a minority class to be more expensive. Unfortunately, most conventional classification algorithms assume that the class distributions are balanced or the misclassification costs are equal. They seek to maximize the overall accuracy which yet cannot distinguish the error types. Therefore, they may neglect the significance of the minority class and tend toward the majority class. Learning in the class imbalance is thus of high importance in data mining and machine learning. From the background of this problem, various meth-\nods are developed within a category called cost-sensitive learning (CSL), such as costs to test [3], to relabel training instances [4], to sample [5], to weight instances [6], and to find a decision threshold [7], [8]. These methods use unequal costs to make a bias toward the minority class. Generally, when the costs are not given, these methods can not work properly. A comprehensive review of learning in the class imbalance problem is provided by He and Garcia [9]. When there exist some uncertainties in the decision,\nit may be better to apply abstaining classification [10] to\n• X. Zhang is with the National Laboratory of Pattern Recognition, Institute of Automation Chinese Academy of Sciences, Beijing 100190, P.R. China. E-mail: xwzhang@nlpr.ia.ac.cn. • B.-G. Hu is with the National Laboratory of Pattern Recognition, Institute of Automation Chinese Academy of Sciences, 95 ZhongGuanCun East Road, Beijing 100190, P.R. China. E-mail: hubg@nlpr.ia.ac.cn.\nreduce the chance of a potential misclassification. Significant benefits have been obtained from abstaining classification, particularly in very critical applications [11], [12]. The optimal rejection thresholds could be found through minimizing a loss function in a cost-sensitive setting [13], [14], [15]. The possibility of designing loss functions for classifiers with a reject option is also explored [16]. In the context of abstaining classifications, the existing CSL approaches require the cost terms associated to the rejects. However, one often fails to provide such information. Up to now, there seems no proper guideline to give the information in terms of the skew ratio. Obviously, a reject option adds another degree of complexity in classifications over the non-abstaining approaches. For advancing the technology and being compatible with human intelligence, we consider the abstaining strategy will become a common option for most learning machines in future. In the class imbalance problem, CSL is an important\nresearch direction. Based on the definition in [17], we extend it below by including the situation of abstaining.\nDefinition 1. Cost-Sensitive Learning (CSL) is a type of learning that takes the misclassification costs and/or rejection costs into consideration. The goal of this type of learning is to minimize the total cost.\nCSL generally requires modelers or users to specify cost terms for reaching the goal. However, this work addresses one open issue which is mostly overlooked: “How to conduct a learning in the class imbalance problem\nwhen costs are unknown for errors and rejects”? In fact, the issue is not unusual in real-world applica-\ntions. Therefore, we propose another category of learning\n2\nbelow for distinguishing the differences between the present work and the existing studies in CSL.\nDefinition 2. Cost-Free Learning (CFL) is a type of learning that does not require the cost terms associated with the misclassifications and/or rejects as the inputs. The goal of this type of learning is to get optimal classification results without using any cost information.\nIt is understandable that CFL may face a bigger challenge which is shown by the fact that most existing approaches may fail to present reasonable solutions to the open issue. This work attempts to provide an applicable learning strategy in CFL.\nWe extend Hu’s [18] study on mutual information classifiers. While Hu presents the theoretical formulas, no learning approaches and results are shown for the realworld data sets. Hence, this work focuses on learning and presents main contributions as follows.\n• We propose a CFL strategy in the class imbalance problem. Using normalized mutual information (NI) as the learning target, we conduct the learning from cost-insensitive classifiers. Therefore, we are able to adopt conventional classifiers for simple and direct implementations. The most advantage of this strategy is its unique feature in classification scenarios where one has no knowledge of costs.\n• We study the relations between the strategy and some existing approaches. First, we derive the “equivalent” costs and the rejection thresholds for binary classifications by using the strategy. The costs, being “objective” for the reason of purely determined by the distributions of the given data sets, can be a useful reference for “subjective” cost specifications in CSL (Fig. 1). Second, we present graphical interpretations of ROC curve plots for both nonabstaining and abstaining classifiers. From the plots, the intrinsic differences between the strategy and other existing approaches are explained in the cases when one class becomes extremely rare.\n• We conduct empirical studies on binary class and multi-class problems. Specific investigation is made on abstaining classifications, and we obtain several results from the benchmark data sets which have not been reported before in literature. The results\nconfirm the advantages of the strategy and show the promising perspective of CFL in imbalanced data sets."
    }, {
      "heading" : "1.1 Related Work",
      "text" : "When costs are unequal and unknown, Maloof [20] uses ROC curve to show the performance of binary classifications under different cost settings. The study can be viewed as comparing classifiers rather than finding an optimal operating point. Cost curve [21], [14] can be used to visualize optimal expected costs over a range of cost settings, but it does not suit multi-class problem. Zadrozny and Elkan [22] apply least-squares multiple linear regression to estimate the costs. The method requires cost information of the training sets to be known. Cross validation [23] is proposed to choose from a limited set of cost values, and the final decisions are made by users. There exists some CFL approaches in the class im-\nbalance problem. Various sampling strategies [24], [25], [26] try to modify the imbalanced class distributions. Active learning [27] is also investigated to select desired instances and the feature selection techniques [28], [29] are applied to combat the class imbalance problem for high-dimensional data sets. Besides, ensemble learning methods [30], [31] are used to improve the generalization of predicting the minority class. The recognitionbased methods [32], [33] that train on a single class are proposed as alternatives to the discrimination-based methods to avoid the influence of imbalanced distributions. Sun et al. [34] can get costs for multi-class data sets through maximizing the geometric mean (Gmean) or F-measure which has the ability of balancing the performance of each class. However, all CFL methods above do not take abstaining into consideration and may fail to process the abstaining classifications. In regards to abstaining classification, some strategies\nhave been proposed for defining optimal reject rules. Pietraszek [35] proposes a bounded-abstaintion model with ROC analysis, and Fumera et al. [36] seek to maximize accuracy while keeping the reject rate below a given value. However, the bound information and the targeted reject rate are required to be specified respectively. When there is no prior knowledge of these settings, it is hard to determine the values. Li and Sethi [37] restrict the maximum error rate of each class, but the rates may conflict when they are arbitrarily given."
    }, {
      "heading" : "1.2 Paper Organization",
      "text" : "The remainder of this paper is organized as follows: In Section 2, a brief review of NI is provided. We present our CFL strategy in Section 3. Section 4 analyzes the relations between the optimal parameters and the cost terms, and presents the graphical interpretations of ROC curve plots. The experimental results are presented in Section 5. Finally, we conclude this work in Section 6.\n3"
    }, {
      "heading" : "2 REVIEW: NORMALIZED MUTUAL INFORMATION",
      "text" : "Normalized mutual information (NI) has been used as an evaluation criterion to measure the degree of dependence between the targets T and the decision outputs Y , and it is denoted as\nNI(T, Y ) = I(T, Y )\nH(T ) ,\nwhere I(T, Y ) is the mutual information of two random variables T and Y , H(T ) is the Shannon’s entropy of T . Note that NI(T, Y ) is in the range [0, 1]. Suppose anm-class abstaining classification, with each\nclass denoted as 1, 2, . . . ,m, and the rejected class denoted as m+1. The value of the target variable T ranges from 1 to m, while the decision output variable Y ranges from 1 to m+ 1. Then we have\nI(T, Y )=\nm ∑\ni=1\nm+1 ∑\nj=1\nP (T = i, Y = j) log2 P (T = i, Y = j)\nP (T = i)P (Y = j) ,\nH(T )=− m ∑\ni=1\nP (T = i) log2 P (T = i).\nIn general, as the exact probability distribution functions of T and Y are hard to derive, Hu et al. [38] apply empirical estimations to compute NI based on the confusion matrix. Table 1 illustrates an augmented confusion matrix C in anm-class abstaining classification by adding the last column as a rejected class m+1. The rows correspond to the states of the targets T , and the columns correspond to the states of the decision outputs Y . cij represents the number of the instances that belong to the i-th class classified as the j-th class, i = 1, 2, . . . ,m, j = 1, 2, . . . ,m + 1. To avoid unchanged value of NI if rejections are made within only one class, the formula of NI is proposed as [38]\nNI(T, Y ) =\nm ∑\ni=1\nm ∑\nj=1\nPe(T = i, Y = j) Pe(T=i,Y =j)\nPe(T=i)Pe(Y=j)\n− m ∑\ni=1\nPe(T = i) log2 Pe(T = i)\n= −\nm ∑\ni=1\nm ∑\nj=1\ncij log2\n\n\ncij\nCi m∑\ni=1 (\ncij\nn )\n\n\nm ∑\ni=1\nCi log2( Ci n )\n, (1)\nwhere Y is counted from 1 to m rather than to m + 1. The subscript “e” is given for denoting empirical terms, Ci = ∑m+1 j=1 cij is the total number of instances in the i-th class, i = 1, 2, . . . ,m, and n = ∑m\ni=1 ∑m+1 j=1 cij is the\ntotal number in the confusion matrix. In non-abstaining classification, i.e. classification without rejection, Y ranges from 1 to m and (1) is actually the formula of the original NI. Then (1) is applicable for both non-abstaining and abstaining classifications in the present work. Principe et al. [40] present a schematic diagram of\ninformation theory learning (ITL) and they mention that maximizing mutual information as the target function makes the decision outputs correlate with the targets as much as possible. Recently, a study [18] confirms that ITL opens a new perspective for classifier design. MacKay [39] recommends mutual information for its single rankable value which makes more sense than error rate. Hu et al. [38] study theoretically for the first time on both error types and reject types in binary classifications. They consider information-theoretic measures most promising in providing “objectivity” to classification evaluations in class imbalance problems. The above viewpoints of mutual information motivate our following NI-based strategy for CFL in the class imbalance problem."
    }, {
      "heading" : "3 NI-BASED CLASSIFICATION",
      "text" : "In this work, we distinguish two types of classificaitons, namely, “non-abstaining classification” for no rejection and “abstaining classification” for rejection. From the phenomenon that different error types and reject types produce different effects on NI, one can derive a conclusion that NI considers the costs to be unequal, unlike accuracy. In fact, the cost information is hiding in NI, and we take advantage of its bias toward the minority class. The bias can be changed through moving the decision thresholds, and the value of NI is changed accordingly. We focus our study on the probabilistic classifiers in the present work, although it can also be applied to nonprobabilistic classifiers [19]. Let x = [x1,x2, . . . ,xn]\nT denote a data matrix with n instances to be classified, xl ∈ R\nd is the input feature vector, l = 1, 2, . . . , n. The target vector is denoted as t = [t1, t2, . . . , tn]\nT , tl ∈ T = {1, 2, . . . ,m}. The decision output vector is denoted as y = [y1, y2, . . . , yn]\nT , yl ∈ Y = {1, 2, . . . ,m} for non-abstaining classification while yl ∈ Y = {1, 2, . . . ,m + 1} for abstaining classification. Then for both non-abstaining and abstaining classifications, we have a generalized formula with NI being a function of the data set and the decision thresholds:\nNI = NI ( t,y = f(ϕ(x), τ ) ) ,\nyl =\n\n\n\nargmax i\n(ϕi(xl)\nτi\n) if max (ϕi(xl)\nτi\n)\n≥ 1,\nm+ 1 otherwise,\n(2)\n0 < τi ≤ 1, i = 1, 2, . . . ,m, l = 1, 2, . . . , n,\nwhere ϕ(x) ∈ Rn×m denotes the real-value output matrix of a probabilistic classifier for n instances, ϕi(xl)\n4 is the probabilistic output of class i for xl, ∑m\ni=1 ϕi(xl)=1 and 0 ≤ ϕi(xl)≤1. τ=[τ1, τ2, . . . , τm]\nT ∈Rm is the vector parameter of the decision thresholds. The decision rule of yl is proposed in this form to avoid classifying an instance xl into more than one class."
    }, {
      "heading" : "3.1 Non-Abstaining Classification",
      "text" : "In non-abstaining classification, the first condition for deriving yl in (2) should only be satisfied, i.e.\nyl = argmax i\n(ϕi(xl)\nτi\n)\n, 0 < τi ≤ 1,\ni = 1, 2, . . . ,m, l = 1, 2, . . . , n.\nLet φi(xl) = αiϕi(xl), αi is denoted as the weight parameter for ϕi(xl), αi =\nτm τi and αm = 1. Then we have the following:\nφi(xl) = αiϕi(xl)\n= τm ϕi(xl)\nτi .\nIt is obvious that argmaxi φi(xl)=argmaxi ( ϕi(xl) τi )\n, and the optimal decision for yl remains the same. The effect of assigning weights to the probabilistic outputs is the same as setting decision thresholds. Therefore, we denote α=[α1, α2, . . . , 1]\nT∈Rm as the weight parameter vector, and the class assignment rule for yl=f(ϕ(xl),α) is based on the highest weighted probabilistic outputs. For non-abstaining classification, we propose\nmaximize NI ( t,y = f(ϕ(x),α) ) ,\nsubject to\nyl = argmax i αiϕi(xl), αi > 0, i = 1, 2, . . . ,m, l = 1, 2, . . . , n. (3)\nIn order to maximize NI, the optimal weight parameter α∗ should be\nα∗ = argmax α\nNI ( t,y = f(ϕ(x),α) ) . (4)"
    }, {
      "heading" : "3.2 Abstaining Classification",
      "text" : "We denote Tr=[Tr1, Tr2, . . . , Trm] T ∈Rm as the rejection threshold vector in dealing with abstaining classificaiton. Let 1− Tri = τi, Tri is in the range [0, 1), i = 1, 2, . . . ,m. The decision output for yl = f(ϕ(xl),Tr) lies within m+ 1 classes. Then we propose\nmaximize NI ( t,y = f(ϕ(x),Tr) ) ,\nsubject to\nyl =\n\n\n\nargmax i\n( ϕi(xl)\n1− Tri\n) ifmax ( ϕi(xl)\n1− Tri\n)\n≥ 1,\nm+ 1 otherwise,\n(5)\n0 ≤ Tri < 1, 0 ≤ m ∑\ni=1\nTri < m− 1,\ni = 1, 2, . . . ,m, l = 1, 2, . . . , n.\nNote that m − 1 is the loose upper bound for the summation\n∑m i=1 Tri. Assume a situation that all instances\nsatisfy the first condition in (5), and ϕi(xl)1−Tri ≥ 1 for all probabilistic outputs, i.e. ∀i, l, ϕi(xl) ≥ 1−Tri. Then we get the following:\nm ∑\ni=1\nϕi(xl) ≥\nm ∑\ni=1\n(1− Tri),\nm ∑\ni=1\nTri ≥ m− 1.\nIf ∑m\ni=1 Tri falls in this interval, the condition of rejection would never be satisfied and the proposal of abstaining classification is ineffective. Reversely, this extreme situation would not happen if\n∑m i=1 Tri < m− 1.\nIn order to maximize NI, the optimal rejection threshold vector T ∗r should be\nT ∗r = argmax Tr\nNI ( t,y = f(ϕ(x),Tr) ) . (6)"
    }, {
      "heading" : "3.3 Optimization Algorithm",
      "text" : "The present framework is proposed based on the confusion matrix from which we compute NI, but it is not differentiable. We apply a general optimization algorithm called “Powell Algorithm” which is a direct method for nonlinear optimization without calculating the derivatives [41]. It is also widely used in image registration to find optimal registration parameters.\nAlgorithm 1 Learning algorithm\nInput: Probabilistic outputs ϕ(x), target labels t, D as the degree of freedom in τ . Output: τ∗\n1: Initialize τ1 as a random vector in the range of τ , d1,d2, . . . ,dD as linear independent vectors, number of iterations W = 0, ε ≥ 0.\n2: Iterative Search Phase: 3: repeat 4: W = W + 1. Let τ (1) W\n= τW . 5: for each direction di, i = 1 to D do 6: η̄(i) = argmaxη∈R NI ( t,y = f(ϕ(x), τ (i) W + ηdi) ) ; 7: Update τW in the current direction: τ (i+1) W = τ (i) W\n+ η̄(i)di; 8: end for 9: Update the directions: di = di+1, i = 1, 2, . . . ,D − 1;\ndD = τ (D+1) W\n− τW ; 10: η∗\nW = argmaxη∈R NI\n( t,y = f(ϕ(x), τW + ηdD) )\n; 11: Update τ after the current iteration: τW+1 = τW + η ∗ W dD ; 12: until ||τW+1 − τW ||2 ≤ ε 13: Return τ∗ = τW+1.\nThe algorithm is given in Algorithm 1, which we apply to find τ∗ for demonstration. We can also apply it to both α and Tr. For Step 6 and Step 10, we use bracketing method to find three starting points and use Brent’s Method to realize one-dimensional optimization. W iterations of the basic procedure lead toW (D+1) onedimensional optimizations. One disadvantage of this algorithm is that it may find a local extrema. Hence, we randomly choose the starting points several times and then pick the best one. In non-abstaining binary\n5 classification, D = 1, so we just work from Step 4 to Step 7 once and assign the value of τ (2) W to τ ∗."
    }, {
      "heading" : "4 RELATIONS IN BINARY CLASSIFICATION",
      "text" : "The previous section completes the essence of the present framework. It can be regarded as a generic way to make the conventional learning algorithms informationtheoretic based. The optimal parameters reflect the degree of bias\nimplied by NI, and may reveal the cost information to some extent. In this section, we focus on binary classification and analyze the relations between the optimal parameters and the cost terms. Moreover, we discover some graphical interpretations of performance measures on ROC curve, which allows the users to adjust the parameters more conveniently using ROC curve."
    }, {
      "heading" : "4.1 Normalized Cost Matrix",
      "text" : "Friedel et al. [14] derive normalized cost matrix based on the overall risk which is written as\nRisk = ∑\ni,j\nλijp(j|i)p(i), (7)\nwhere λij is the original cost in the common cost matrix that assigns an instance of class i to class j, p(j|i) is the true probability in such situation, and p(i) is the true prior probability of class i. The conditional risk of assigning an instance xl to class j is\nRisk(j|xl) =\nm ∑\ni=1\nλijp(i|xl), (8)\nwhere p(i|xl) is the true posterior probability of class i given xl. By applying the way of transforming costs [14], we find that the normalization way for the overall risk is also applicable for the conditional risk. In binary classification, we refer to class 1 and class 2\nas negative class (N ) and positive class (P ), respectively. We denote λFN , λFP , λTN , λTP , λRN and λRP to be the costs of false negative, false positive, true negative, true positive, reject negative, and reject positive, respectively. Therefore, the normalized cost matrix for non-abstaining binary classification can be denoted as\nλ̄no rej =\n[\nλ̄TN λ̄FP λ̄FN λ̄TP\n]\n=\n[\n0 λ̄FP 1 0\n]\n(9)\nwith β = λFN − λTP , then λ̄TN = λTN−λTN\nβ = 0, λ̄FP =\nλFP−λTN β , λ̄FN = λFN−λTP β = 1, λ̄TP = λTP−λTP β\n= 0. Similarly, the normalized cost matrix for abstaining\nbinary classification can be denoted as\nλ̄rej =\n[\nλ̄TN λ̄FP λ̄RN λ̄FN λ̄TP λ̄RP\n]\n=\n[\n0 λ̄FP λ̄RN 1 0 λ̄RP\n]\n(10)\nwith λ̄TN = 0, λ̄FP = λFP−λTN\nβ , λ̄RN = λRN−λTN β , λ̄FN =\n1, λ̄TP = 0, λ̄RP = λRP−λTP\nβ , β = λFN − λTP . The first\ntwo columns contain the misclassification costs, while the last column indicates the rejection costs.\nIt is reasonable to assume that the values of the original correct classification costs and misclassification costs in the common cost matrix are not affected by introducing a reject option. Therefore, what is noteworthy is that λ̄FP in (9) is consistent with that in (10)."
    }, {
      "heading" : "4.2 Optimal Weight and Misclassification Cost",
      "text" : "In non-abstaining binary classification, it is feasible to set the decision thresholds as τ = [1 − τP , τP ]\nT , which has one degree of freedom. The relation between the decision thresholds and the\ncosts has been derived by Elkan [7] through minimizing the conditional risk. Considering the normalized cost matrix in (9), the decision threshold τ∗P of the positive class for making optimal decision can be represented as\nτ∗P = λ̄FP\n1 + λ̄FP , (11)\nwith λ̄FP be the variable. It is required that the value of λ̄FP be given and be reasonable. Otherwise, τ ∗\nP can not be derived or not be proper. In our present work, the optimal weight vector is α∗ = [α∗N , 1] T . We apply it in the decision rule of maximum weighted posterior probability. Then the optimal prediction is the positive class if and only if α∗Np(N |xl) ≤ p(P |xl). Hence, the decision threshold τ ∗∗\nP of the positive class for making optimal decision is\nτ∗∗P = α∗N\n1 + α∗N . (12)\nSuppose that the minimum conditional risk rule shares the same decision thresholds with the maximum weighted posterior probability rule, then (11) and (12) should be equal. And we give the following definition:\nDefinition 3. Given the optimal weight α∗N , the “equivalent” misclassification cost is defined as\nλ̄FP = α ∗ N . (13)\nIn general, it is assumed that λ̄FP < λ̄FN , i.e. λ̄FP < 1. In this case, it is required that α∗N < 1."
    }, {
      "heading" : "4.3 Optimal Rejection Thresholds and Costs",
      "text" : "In abstaining binary classification, the relations between the rejection thresholds and the costs can be presented in a form of explicit formulae [18]. With the optimal rejection threshold vector T∗r = [T ∗ rN , T ∗ rP ] T and the normalized cost matrix in (10), these relations are\nT ∗rN = λ̄RN\n1 + λ̄RN − λ̄RP ,\nT ∗rP = λ̄RP\nλ̄FP − λ̄RN + λ̄RP ,\nwhich imply a parameter redundancy. In addition, the value of λ̄FP derived from (13) can be utilized as a prior knowledge under the assumption of cost consistency.\nDefinition 4. Given the “equivalent” misclassification cost λ̄FP = α ∗ N , the “equivalent” rejection costs are defined as\nλ̄RN = T ∗rN(1− T ∗ rP )− T ∗ rNT ∗ rP λ̄FP\n1− T ∗rN − T ∗ rP\n,\nλ̄RP = −T ∗rNT ∗ rP + (1− T ∗ rN)T ∗ rP λ̄FP\n1− T ∗rN − T ∗ rP\n. (14)\nBased on [18], one can have the relations λ̄TN < λ̄RN < λ̄FP and λ̄TP < λ̄RP < λ̄FN . Then we can obtain the following properties from (14):\nP1. If 0 < λ̄RN < λ̄FP , we have 0 < T ∗ rN < α∗N\n1+α∗ N\nand\nT ∗rP < 1\n1+α∗ N\n;\nP2. If 0 < λ̄RP < 1, we have T ∗ rN < α∗N\n1+α∗ N\nand 0 <\nT ∗rP < 1\n1+α∗ N\n;\nP3. If 0 < T ∗rN < α∗N\n1+α∗ N\nand 0 < T ∗rP < 1\n1+α∗ N\n, then\n0 < λ̄RN < λ̄FP and 0 < λ̄RP < 1."
    }, {
      "heading" : "4.4 Graphical Interpretations of ROC Curve Plots with/without Abstaining",
      "text" : "In binary classification, an ROC curve plot presents complete information about the performance of each class [42], so that an overall performance measure [43], such as AUC, can be formed. This is a preferred feature in processing class imbalance problems [44]. Furthermore, an ROC curve can also provide the graphical interpretations for non-abstaining and abstaining classifications in Fig. 2, where TPR and FPR are true positive rate and false positive rate. We denote A, CR, E and Rej to be accuracy, correct recognition rate, error rate, and reject rate, respectively. CN and CP are the total numbers of the negatives and positives, respectively. CFN , CFP , CTN , CTP , CRN and CRP are the numbers of the false negatives, false positives, true negatives, true positives, reject negatives, and reject positives, respectively. Their relations are shown as follows: Non-abstaining:\nAN + EN = 1, and AP + EP = 1, AN = CTN\nCN , EN =\nCFP\nCN , AP =\nCTP\nCP , EP =\nCFN\nCP ; (15a)\nAbstaining:\nCRN + EN +RejN = 1, and CRP + EP +RejP = 1, CRN = CTN\nCN , EN =\nCFP\nCN , RejN =\nCRN\nCN ,\nCRP = CTP\nCP , EP =\nCFN\nCP , RejP =\nCRP\nCP . (15b)\nSeveral observations are summarized below for understanding the features of ROC plots. To begin with, we discuss an ROC curve in a non-abstaining classification, as shown in Fig. 2a. For a theoretical ROC curve which is concave, the decision is made by K, the slope of ROC curve, in the form of [45]:\nK = p(N)\np(P) λFP − λTN λFN − λTP = p(N) p(P) λ̄FP , (16)\nwhich is also equivalent to the likelihood ratio [46]:\nL = p(x|P)\np(x|N) =\np(N)\np(P) λFP − λTN λFN − λTP = p(N) p(P) λ̄FP . (17)\nFrom (16), one can observe that:\nif p(P) → 0, then K → ∞, (18a)\nand EP = 1, AP = 0, EN = 0, AN = 1, (18b)\nfor general cost terms. (18a) indicates that the tangent point on the ROC curve will be located at the origin in Fig. 2a, and (18b) demonstrates a graphical interpretation why conventional classifiers fail to process minority class (herein the positive class) properly. However, the situation in (18) can never appear from using the present strategy, because it will result in a zero value of mutual information [39], [43]. Different with the non-abstaining classification, Fig.\n2b shows the abstaining classification graphically on an ROC curve. Two abstaining slopes, KN and KP , are generally given in the forms of [15]:\nKN = p(N)\np(P ) λRN − λTN λFN − λRP = p(N) p(P ) λ̄RN 1− λ̄RP ,\nKP = p(N)\np(P ) λFP − λRN λRP − λTP = p(N) p(P ) λ̄FP − λ̄RN λ̄RP . (19)\n7\n(#Inst: number of instances, #Attr: number of attributes, #C: number of classes)\nWhenever KN 6= KP , one can observe the non-zero results of rejection rates. (19) confirms the finding in [18] that at most two independent parameters will determine the rejection range in binary classifications. Sometimes, one can still apply a single independent parameter, such as KP = 2KN , for abstaining decisions. There exist relations between rejection thresholds in\nthe posterior curve plot [18] and abstaining slopes in the ROC curve plot. Their relations and the associated constraint are derived from [18]:\nKN = p(N)\np(P )\nTrN\n1− TrN , KP =\np(N)\np(P ) 1− TrP TrP ,\nKN < KP . (20)"
    }, {
      "heading" : "5 EXPERIMENTS",
      "text" : ""
    }, {
      "heading" : "5.1 Configuration",
      "text" : "Table 2 lists twelve binary class and four multi-class data sets with imbalanced class distributions. On Pageblock, the maximum ratio between the majority class and the minority class is 175. Most of the data sets are obtained from the UCI Machine Learning Repository1, Ism is from [25], Rooftop is from [20], and Phoneme is from KEEL Datasets2. All of them have continuous attributes and are rescaled to be in the range [0, 1]. We perform 3-fold cross validation and all experiments are repeated ten times to get the average results. In addition, Table 3 lists the procedure of our NI based experiments for each run. We call our NI based non-abstaining classification\nand NI based abstaining classification “NI no rej” and “NI rej” respectively. To illustrate the effectiveness of our strategy, we adopt kNN and Bayes classifier as the conventional classifiers, and we compare our methods with SMOTE, Cost-sensitive learning, Chow’s reject [10] methods and the G-mean based methods (“Gmean no rej” and “Gmean rej”) besides two conventional classifications. In kNN classifier, we apply Euclidean distance and\nuse the confidence values [47], [19] as the probabilistic\n1. http://archive.ics.uci.edu/ml/ 2. http://sci2s.ugr.es/keel/datasets.php\noutputs. The class assignment is decided by the highest confidence. For brevity, we just list the results of 11- NN on all data sets except 5-NN on Pageblock. In Bayes classifier, we derive the estimated class-conditional density from the Parzen-window estimation with Gaussian kernel [48] and apply Bayes rule to classification. The smooth parameter is chosen as the average value of the distance from one instance to its rth nearest neighborhood (r=10 empirically), and the empirical probability of the occurrence of class is chosen as the prior probability. In SMOTE, the average results are presented with the amount from 1 to 5, and it performs simultaneously on the minority classes of the multi-class data sets with the same amount. In Cost-sensitive learning, we simply assign the inverse of the class distribution ratio to the misclassification cost λij for i 6=j, and λii=0. We do not consider abstaining for it because the rejection costs would be hard to give. In Chow’s reject, we simply assign 0.3 to the rejection thresholds for all classes. In G-mean based methods, we apply our way of parameter settings and optimization to maximize G-mean."
    }, {
      "heading" : "5.2 Evaluation Criterion",
      "text" : "In order to show the changes of each class clearly, Ei and Reji are applied as the error rate and the reject rate within its ith class respectively. The total error rate (“E”) and the total reject rate (“Rej”) are also applied. “A” is short for the total accuracy. “G” is short for G-mean with the formula G−mean=(\n∏m i=1 Ai) 1\nm , where Ai represents the accuracy within its ith class. In binary class tasks, we also evaluate F-measure (“F” for short)."
    }, {
      "heading" : "5.3 Binary Class Tasks",
      "text" : "The results on the binary class data sets are shown in Table 4. Both conventional classifiers have high accuracies and low error rates of the negative class, but the error rates of the positive class are high. SMOTE is an effective method with low error rate of the positive class. However, it does not have the ability to reject instances. Cost-sensitive learning performs well under the current cost settings, but its accuracy is the lowest when the class distribution differs greatly. On Nursery and Letter, the error rate of the positive class is zero with Cost-sensitive learning at the price of high error rate of the negative class. Besides, Gmean no rej and NI no rej perform well on balancing the classification of two classes. When a\n8\n9 (Continued from previous page) Data set Method EN (%) EP (%) E(%) A(%) RejN (%) RejP (%) Rej(%) G(%) F (%) NI\nOptdigits\nk N N kNN classifier 0.10 4.80 0.56 99.44 — — — 97.52 97.08 0.8938 SMOTE 0.59 1.65 0.69 99.31 — — — 98.88 96.57 0.9019 Cost-sensitive 1.09 1.19 1.10 98.90 — — — 98.86 94.65 0.8709 Gmean no rej 0.84 1.34 0.89 99.11 — — — 98.91 95.64 0.8868 NI no rej 0.28 2.26 0.48 99.52 — — — 98.72 97.59 0.9160 Chow’s reject 0.06 2.35 0.28 99.72 0.21 7.29 0.90 98.70 98.45 0.8898 Gmean rej 0.07 0.76 0.14 99.86 1.85 10.06 2.66 99.54 99.25 0.9221 NI rej 0.15 1.41 0.27 99.72 0.74 2.04 0.87 99.21 98.58 0.9307\nB a y e s\nBayes classifier 0.00 100.00 9.86 90.14 — — — 0.00 0.00 0.0000 SMOTE 1.29 60.59 7.14 92.86 — — — 46.68 40.41 0.2741 Cost-sensitive 52.14 0.00 47.00 53.00 — — — 69.18 29.56 0.1855 Gmean no rej 6.19 3.90 5.96 94.04 — — — 94.94 76.13 0.6109 NI no rej 4.39 8.26 4.77 95.23 — — — 93.62 79.18 0.6134 Chow’s reject 0.00 100.00 9.86 90.14 0.00 0.00 0.00 0.00 0.00 0.0000 Gmean rej 0.08 0.18 0.09 99.86 39.84 59.48 41.78 99.70 99.02 0.4653 NI rej 2.50 1.53 2.40 97.31 10.44 15.21 10.91 97.69 87.48 0.6626\nVehicle\nk N N kNN classifier 5.81 68.30 21.47 78.53 — — — 54.46 42.35 0.0915 SMOTE 30.69 24.93 29.25 70.75 — — — 70.47 55.69 0.1542 Cost-sensitive 21.33 34.90 24.73 75.27 — — — 71.45 56.95 0.1442 Gmean no rej 29.47 24.50 28.23 71.77 — — — 72.79 57.28 0.1523 NI no rej 39.57 14.94 33.40 66.60 — — — 71.14 56.21 0.1579 Chow’s reject 0.79 36.43 9.72 86.61 19.09 52.44 27.44 47.72 36.05 0.1039 Gmean rej 3.97 3.49 3.85 90.71 55.62 74.26 60.29 88.49 74.85 0.1764 NI rej 14.67 6.86 12.71 80.69 34.63 42.54 36.61 82.46 68.11 0.1982\nB a y e s\nBayes classifier 0.62 93.13 23.80 76.20 — — — 23.22 11.94 0.0288 SMOTE 30.04 41.96 33.03 66.97 — — — 57.28 42.86 0.0755 Cost-sensitive 26.97 40.42 30.33 69.67 — — — 65.86 49.54 0.0790 Gmean no rej 36.91 25.71 34.09 65.91 — — — 68.24 52.08 0.1034 NI no rej 45.48 17.48 38.46 61.54 — — — 66.52 51.85 0.1072 Chow’s reject 0.00 48.42 12.13 82.85 21.86 51.58 29.31 0.00 0.00 0.0236 Gmean rej 2.40 1.60 2.20 91.40 73.51 82.10 75.66 84.71 73.28 0.1223 NI rej 9.64 8.30 9.30 81.23 50.04 58.16 52.07 79.80 65.67 0.1322\nYeast\nk N N kNN classifier 9.44 58.16 23.52 76.48 — — — 61.50 50.64 0.1086 SMOTE 36.11 25.29 32.98 67.02 — — — 67.48 56.39 0.1180 Cost-sensitive 25.08 32.82 27.32 72.68 — — — 70.92 58.70 0.1289 Gmean no rej 30.99 26.62 29.73 70.27 — — — 71.02 58.84 0.1289 NI no rej 31.52 26.25 30.00 70.00 — — — 70.35 58.72 0.1340 Chow’s reject 3.07 34.17 12.06 83.31 20.68 45.41 27.83 59.93 49.45 0.1108 Gmean rej 3.45 3.45 3.45 89.39 65.17 74.78 67.95 87.59 77.92 0.1421 NI rej 13.20 9.86 12.23 79.35 41.55 41.96 41.67 80.30 69.91 0.1607\nB a y e s\nBayes classifier 1.60 89.32 26.96 73.04 — — — 32.23 18.55 0.0313 SMOTE 50.67 26.09 43.57 56.43 — — — 50.16 45.92 0.0719 Cost-sensitive 51.30 10.35 39.46 60.54 — — — 65.99 56.81 0.1234 Gmean no rej 30.48 30.12 30.38 69.62 — — — 69.38 57.19 0.1138 NI no rej 35.56 26.50 32.93 67.07 — — — 66.49 55.84 0.1202 Chow’s reject 0.19 20.33 6.01 88.02 38.44 78.60 50.05 19.78 8.77 0.0742 Gmean rej 2.67 3.12 2.80 86.14 77.69 81.72 78.86 83.67 73.48 0.0822 NI rej 12.15 9.46 11.37 80.98 40.91 41.45 41.06 81.53 71.72 0.1786\nPhoneme\nk N N kNN classifier 6.44 23.03 11.31 88.69 — — — 84.86 79.98 0.4261 SMOTE 17.40 10.46 15.36 84.64 — — — 85.81 77.51 0.4106 Cost-sensitive 13.69 11.55 13.06 86.94 — — — 87.37 79.90 0.4372 Gmean no rej 14.09 11.00 13.18 86.82 — — — 87.43 79.86 0.4388 NI no rej 12.53 12.77 12.60 87.40 — — — 87.32 80.28 0.4406 Chow’s reject 2.27 11.80 5.07 93.94 11.08 29.17 16.39 90.10 87.21 0.4683 Gmean rej 1.20 1.94 1.42 97.62 36.01 52.78 40.93 96.93 94.98 0.4509 NI rej 5.35 4.62 5.14 93.54 20.22 21.79 20.68 93.69 89.41 0.5086\nB a y e s\nBayes classifier 10.44 31.25 16.55 83.45 — — — 78.46 70.92 0.2816 SMOTE 24.61 12.53 21.06 78.94 — — — 80.70 71.02 0.3122 Cost-sensitive 21.89 12.41 19.10 80.90 — — — 82.71 72.91 0.3248 Gmean no rej 23.81 9.96 19.74 80.26 — — — 82.81 72.81 0.3331 NI no rej 26.03 7.94 20.72 79.28 — — — 82.49 72.29 0.3359 Chow’s reject 1.17 13.14 4.69 93.05 20.36 62.33 32.68 80.01 75.37 0.2679 Gmean rej 0.17 0.14 0.16 99.25 78.47 91.71 82.36 96.97 95.24 0.1292 NI rej 10.05 4.35 8.37 88.92 23.50 28.33 24.92 90.32 82.69 0.3845\nGerman\nk N N kNN classifier 9.67 69.03 27.48 72.52 — — — 52.75 40.24 0.0554 SMOTE 49.80 21.39 41.28 58.72 — — — 60.30 53.23 0.0746 Cost-sensitive 29.57 35.53 31.36 68.64 — — — 67.32 55.19 0.0888 Gmean no rej 32.92 29.47 31.88 68.12 — — — 68.67 57.03 0.1017 NI no rej 34.40 28.00 32.48 67.52 — — — 68.38 57.12 0.1037 Chow’s reject 2.17 35.53 12.18 81.03 27.40 55.60 35.86 43.97 30.40 0.0582 Gmean rej 6.20 6.07 6.16 80.66 68.63 72.53 69.80 78.48 67.65 0.0797 NI rej 17.30 16.83 17.16 73.89 34.85 33.50 34.45 73.63 63.57 0.1139\nB a y e s\nBayes classifier 0.00 100.00 30.03 69.97 — — — 0.00 0.00 0.0000 SMOTE 61.84 32.33 52.99 47.01 — — — 13.06 37.16 0.0183 Cost-sensitive 21.43 39.27 26.78 73.22 — — — 69.06 57.64 0.1179 Gmean no rej 32.23 26.73 30.58 69.42 — — — 70.23 58.96 0.1144 NI no rej 30.80 29.50 30.41 69.59 — — — 69.12 57.87 0.1189 Chow’s reject 0.00 30.60 9.18 84.62 28.51 69.40 40.78 0.00 0.00 0.0567 Gmean rej 1.77 2.07 1.86 89.16 82.11 88.33 83.98 79.02 68.39 0.0689 NI rej 15.40 12.23 14.45 76.33 40.20 39.50 39.99 75.88 66.92 0.1334\n(Continued on next page)\n10\n(a) kNN Classifier Based\nData set α∗N ( λ̄FP ) T∗rN T ∗ rP λ̄RN λ̄RP Ism 0.2312(0.0408) 0.0743(0.0085) 0.7643(0.0296) 0.0272 0.6616 Nursery 0.3482(0.0328) 0.1215(0.0565) 0.7125(0.0403) 0.0288 0.7914 Letter 0.3802(0.0321) 0.1284(0.0343) 0.6140(0.0517) 0.0760 0.4838 Rooftop 0.1372(0.0302) 0.0705(0.0610) 0.7733(0.0404) 0.0544 0.2823 Pendigits 0.4714(0.0771) 0.1745(0.0851) 0.3890(0.0555) 0.1710 0.1913 Optdigits 0.4061(0.0667) 0.1487(0.0372) 0.5913(0.0486) 0.0964 0.4481 Vehicle 0.1972(0.0769) 0.1101(0.0297) 0.6266(0.0719) 0.1045 0.1556 Yeast 0.3610(0.1333) 0.1245(0.0335) 0.5608(0.0536) 0.0937 0.3414 Phoneme 0.4651(0.0835) 0.1319(0.0180) 0.4543(0.0409) 0.1066 0.2985 German 0.3848(0.0785) 0.1915(0.0336) 0.6021(0.0368) 0.1542 0.3489 Diabetes 0.3725(0.0796) 0.1725(0.0455) 0.5284(0.0672) 0.1585 0.2398 Gamma 0.5682(0.0207) 0.1663(0.0225) 0.4188(0.0319) 0.1376 0.3103\n(b) Bayes Classifier Based\nData set α∗N ( λ̄FP ) T∗rN T ∗ rP λ̄RN λ̄RP Ism 0.1420(0.0230) 0.0222(0.0168) 0.8560(0.0212) 0.0041 0.8198 Nursery 0.1052(0.0117) 0.0593(0.0076) 0.8845(0.0090) 0.0237 0.6242 Letter 0.1155(0.0066) 0.0687(0.0163) 0.8772(0.0128) 0.0273 0.6302 Rooftop 0.0786(0.0299) 0.0242(0.0080) 0.8363(0.0242) 0.0170 0.3147 Pendigits 0.4283(0.0472) 0.1521(0.0488) 0.6170(0.0394) 0.0782 0.5640 Optdigits 0.1883(0.0087) 0.1339(0.0047) 0.8274(0.0078) 0.0581 0.6240 Vehicle 0.2490(0.0262) 0.1301(0.0149) 0.5369(0.0384) 0.1287 0.1395 Yeast 0.4469(0.0734) 0.2668(0.0147) 0.6413(0.0137) 0.2093 0.4247 Phoneme 0.3364(0.0374) 0.1723(0.0266) 0.5511(0.0363) 0.1641 0.2115 German 0.4265(0.0137) 0.2802(0.0081) 0.6866(0.0085) 0.1736 0.5541 Diabetes 0.3808(0.0460) 0.2461(0.0174) 0.6638(0.0327) 0.2279 0.3020 Gamma 0.5859(0.0483) 0.1723(0.0212) 0.4660(0.0272) 0.1243 0.4028\nOptimal values are listed as mean(standard deviation). (a) Derived based on kNN classifier. (b) Derived based on Bayes classifier.\n11\nreject option is added, the error rate may be reduced and the accuracy may be increased. But it is difficult to decide the rejection costs and the rejection thresholds for lack of information about the rejections. Regarding to Chow’s reject, it is usually wasteful to reject lots of instances from the positive class with arbitrary settings on the rejection thresholds. On most data sets, Gmean rej achieves the highest accuracy and the lowest error rate of the positive class, at the price of considerably high reject rate. However, the accuracy of Gmean rej is lower than the conventional classifications on Ism and Rooftop. One explanation is that the goal of the G-mean based methods is to maximize the geometric mean of the accuracy within each class rather than the total accuracy. Compared with Gmean rej and Chow’s reject, our NI rej performs best on the whole with low error rate of the positive class, high accuracy, a certain amount of reject rate, high G-mean, high F-measure and the highest NI.\nTable 5 lists the values of the optimal weight α∗N and rejection thresholds T ∗r . The last two columns represent the “equivalent” rejection costs computed with the mean values of α∗N and T ∗\nr . Moreover, these values are purely determined by the data sets besides the conventional classification algorithms. They can be adopted as “objective” references while the cost information is unknown. In addition, the “equivalent” costs of these data sets are consistent with human assumption, which also reflects the effectiveness of our NI based strategy.\nFig. 3 shows the ROC convex hull (ROCCH) of kNN for Diabetes generated from 90 validation sets by threshold averaging [44]. We just list some of the vertices in Table 6. And we use them to approximatively locate the parameters [15]. In Fig. 3a, we use equal and “equivalent” misclassification costs to compute the slopes, respectively. The slopes computed with costs are the same as those computed with rejection thresholds on ROCCH, so we only plot the latter in Fig. 3b. Point B in Fig. 3a lies between D and F. Due to approximation, points D and F that the slopes find are not cohere with the optimal\nthreshold points C and E. In addition, the parameters can be adjusted with the graphical interpretations in Fig. 2 under the property P3 by the users."
    }, {
      "heading" : "5.4 Multi-Class Tasks",
      "text" : "The detailed results on the multi-class data sets are shown from Table 7 to Table 9, including the performance evaluations and the values of the optimal parameters. Compared with the conventional classifications that have high error rates of the minority classes, SMOTE is effective in reducing these errors. Cost-sensitive learning classifies all instances to the class that has the minimum number of instances. Both Gmean no rej and NI no rej perform well with low error rates of the minority classes, high G-mean and high NI. Chow’s reject and Gmean rej reject lots of instances from the minority classes; besides, Gmean rej rejects lots of instances from the majority class. On the whole, our NI rej performs best with low error rates of the minority classes, a certain amount of reject rate, high G-mean, and the highest NI. In summary, the observations above suggest that:\n12\n(a) Evaluation of the methods, “–”: not available, the best performance in each cell is bolded. (b) Optimal weights and rejection thresholds are listed as mean(standard deviation).\n1. Within the CFL category, both SMOTE and G-mean based methods are effective in the class imbalance problem. However, they are unable to process abstaining classifications.\n2. Regarding to Cost-sensitive learning, it is feasible to apply the inverses of the class distribution ratios as the misclassification costs on binary class tasks. But on multi-class tasks, it may be ineffective. Moreover, the rejection costs are always hard to get.\n3. Chow’s reject would perform poorly if the rejection thresholds are arbitrarily given.\n4. NI based strategy is a good choice for both nonabstaining and abstaining classifications. It can produce reasonable solutions on the minority classes."
    }, {
      "heading" : "6 CONCLUSION",
      "text" : "In this paper, we propose a new strategy of CFL to deal with the class imbalance problem. Based on the specific property of mutual information that can distinguish different error types and reject types, we seek to maximize it as a general rule for dealing with binary/multiclass classifications with/without abstaining. A unique feature is gained in abstaining classifications when information is unknown about errors and rejects. To our best knowledge, no other existing approach is applicable to this scenario. Moreover, we can derive the “equivalent” costs for binary classifications. Generally, the “equivalent” costs will be changed accordingly to the distributions of the given data sets. Therefore, the present strategy provides an “objective” reference for CSL if users want\n13\n(a) Evaluation of the methods, “–”: not available, the best performance in each cell is bolded. (b) Optimal weights and rejection thresholds are listed as mean(standard deviation).\nto adjust the costs. For better understanding ROC curves in binary classifications, graphical interpretations of the theoretical ROC curve plots are explained in terms of the related parameters, such as cost terms and rejection thresholds. Empirical study confirms the advantages of the proposed strategy in solving class imbalance problems. At the same time, we recognize the disadvantage of the work that it will add an extra computational cost over the existing approaches. This difficulty will form a future work for advancing the study."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "This work is supported in part by NSFC (No. 61075051) and the SPRP-CAS (No. XDA06030300)."
    } ],
    "references" : [ {
      "title" : "The Class Imbalance Problem: A Systematic Study",
      "author" : [ "N. Japkowicz", "S. Stephen" ],
      "venue" : "Intelligent Data Analysis, vol. 6, pp. 429-449, 2002.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Learning from Imbalanced Data: Evaluation Matters",
      "author" : [ "T. Raeder", "G. Forman", "N.V. Chawla" ],
      "venue" : "Data Mining: Foundations and Intelligent Paradigms, D. E. Holmes and L. C. Jain, Eds., New York: Springer, pp. 315-331, 2012.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Test-Cost Sensitive Naive Bayes Classification",
      "author" : [ "X. Chai", "L. Deng", "Q. Yang", "C.X. Ling" ],
      "venue" : "Proc. IEEE Int’l Conf. Data Mining, pp. 51-58, 2004.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "MetaCost: A General Method for Making Classifiers Cost-Sensitive",
      "author" : [ "P. Domingos" ],
      "venue" : "Proc. ACM SIGKDD Int’l Conf. Knowledge Discovery and Data Mining, pp. 155-164, 1999.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Cost-Sensitive Learning by Cost-Proportionate Example Weighting",
      "author" : [ "B. Zadrozny", "J. Langford", "N. Abe" ],
      "venue" : "Proc. IEEE Int’l Conf. Data Mining, pp. 435-442, 2003.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "An Instance-Weighting Method to Induce Cost- Sensitive Trees",
      "author" : [ "K.M. Ting" ],
      "venue" : "IEEE Trans. Knowledge and Data Eng., vol. 14, no. 13, pp. 659-665, May, 2002.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "The Foundations of Cost-Sensitive Learning",
      "author" : [ "C. Elkan" ],
      "venue" : "Proc. Int’l Joint Conf. Artificial Intelligence, pp. 973-978, 2001.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Thresholding for Making Classifiers Cost-Sensitive",
      "author" : [ "V.S. Sheng", "C.X. Ling" ],
      "venue" : "Proc. AAAI National Conf. Artificial Intelligence, 2006.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Learning from Imbalanced Data",
      "author" : [ "H. He", "E.A. Garcia" ],
      "venue" : "IEEE Trans. Knowledge and Data Eng., vol. 21, no. 9, pp. 1263-1284, Sept. 2009.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "On Optimum Recognition Error and Reject Tradeoff",
      "author" : [ "C. Chow" ],
      "venue" : "IEEE Trans. Information Theory, vol. 16, no. 1, pp. 41-46, 1970.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1970
    }, {
      "title" : "Classification of Intrusion Detection Alerts Using Abstaining Classifiers",
      "author" : [ "T. Pietraszek" ],
      "venue" : "Intelligent Data Analysis, vol. 11, no. 3, pp. 293-316, 2007.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Aggregating Abstaining and Delegating Classiers for Improving Classication Performance: An Application to Lung Cancer Survival Prediction",
      "author" : [ "M.-R. Temanni", "S.-A. Nadeem", "D. Berrar", "J.-D. Zucker" ],
      "venue" : "[Online]. Available: http://camda.bioinfo.cipf.es/camda07/agenda/detailed.html",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 0
    }, {
      "title" : "The Interaction between Classification and Reject Performance for Distance- Based Reject-Option Classifiers",
      "author" : [ "T.C. Landgrebe", "D.M. Tax", "P. Paclı́k", "R.P. Duin" ],
      "venue" : "Pattern Recognition Letters, vol. 27, no. 8, pp. 908-917, 2006.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Cost Curves for Abstaining Classifiers",
      "author" : [ "C.C. Friedel", "U. Rückert", "S. Kramer" ],
      "venue" : "Proc. Workshop on ROC Analysis in Machine Learning. Int’l Conf. Machine Learning, 2006.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Reducing the Classification Cost of Support Vector Classifiers Through an ROC-Based Reject Rule",
      "author" : [ "F. Tortorella" ],
      "venue" : "Pattern Anal. Applic., vol. 7, pp. 128-143, 2004.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Local Estimation of Posterior Class Probabilities to Minimize Classification Errors",
      "author" : [ "A. Guerrero-Curieses", "J. Cid-Sueiro", "R. Alaiz-Rodrı́guez", "A.R. Figueiras-Vidal" ],
      "venue" : "IEEE Trans. Neural Networks, vol. 15, no. 2, pp. 309-317, Mar. 2004.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Cost-Sensitive Learning and the Class Imbalance Problem",
      "author" : [ "C.X. Ling", "V.S. Sheng" ],
      "venue" : "Encyclopedia of Machine Learning, 2008.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "What are the Differences between Bayesian Classifiers and Mutual-Information Classifiers",
      "author" : [ "B.-G. Hu" ],
      "venue" : "arXiv:1105.0051v2 [cs.IT], 2012.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Learning When Data Sets are Imbalanced and When Costs are Unequal and Unknown",
      "author" : [ "M.A. Maloof" ],
      "venue" : "Proc. Workshop on Learning from Imbalanced Data Sets II. Int’l Conf. Machine Learning, 2003.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Explicitly Representing Expected Cost: An Alternative to ROC Representation",
      "author" : [ "C. Drummond", "R.C. Holte" ],
      "venue" : "Proc. ACM SIGKDD Int’l Conf. Knowledge Discovery and Data Mining, pp. 198-207, 2000.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Learning and Making Decisions When Costs and Probabilities are Both Unknown",
      "author" : [ "B. Zadrozny", "C. Elkan" ],
      "venue" : "Proc. ACM SIGKDD Int’l Conf. Knowledge Discovery and Data Mining, pp. 204- 213, 2001.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Cost-Sensitive Face Recognition",
      "author" : [ "Y. Zhang", "Z.-H. Zhou" ],
      "venue" : "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 32, no. 10, pp. 1758-1769, Oct. 2010.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Matwin,“Addressing the Curse of Imbalanced Training Sets: One-Sided Selection,",
      "author" : [ "S.M. Kubat" ],
      "venue" : "Proc. Int’l Conf. Machine Learning,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 1997
    }, {
      "title" : "SMOTE: Synthetic Minority Over-Sampling Technique",
      "author" : [ "N.V. Chawla", "K.W. Bowyer", "W.P. Kegelmeyer" ],
      "venue" : "Journal Of Artificial Intelligence Research, vol. 16, pp. 321-357, 2002.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Yao,“Dynamic Sampling Approach to Training Neural Networks for Multiclass Imbalance Classification,",
      "author" : [ "M. Lin", "K. Tang" ],
      "venue" : "IEEE Trans. Neural Networks and Learning Syst.,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2013
    }, {
      "title" : "Learning on the Border: Active Learning in Imbalanced Data Classification",
      "author" : [ "S. Ertekin", "J. Huang", "L. Bottou", "L. Giles" ],
      "venue" : "Proc. ACM Conf. Information and Knowledge Management, pp. 127-136, 2007.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Feature Selection for Text Categorization on Imbalanced Data",
      "author" : [ "Z. Zheng", "X. Wu", "R. Srihari" ],
      "venue" : "ACM SIGKDD Explor. Newsl., vol. 6, pp. 80-89, 2004.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Combating the Small Sample Class Imbalance Problem Using Feature Selection",
      "author" : [ "M. Wasikowski", "X.-W. Chen" ],
      "venue" : "IEEE Trans. Knowledge and Data Eng., vol. 22, no. 10, pp. 1388-1400, Oct. 2010.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Relationships between Diversity of Classification Ensembles and Single-Class Performance Measures",
      "author" : [ "S. Wang", "X. Yao" ],
      "venue" : "IEEE Trans. Knowledge and Data Eng., vol. 25, no. 1, pp. 206-219, Jan. 2013.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Ensembles of α-Trees for Imbalanced Classification Problems",
      "author" : [ "Y. Park", "J. Ghosh" ],
      "venue" : "IEEE Trans. Knowledge and Data Eng., preprint, Dec. 2012.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "One-Class Svms for Document  Classification",
      "author" : [ "L.M. Manevitz", "M. Yousef" ],
      "venue" : "Journal of Machine Learning Research, vol. 2, pp. 139- 154, 2002.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Supervised Versus Unsupervised Binary-Learning by Feedforward Neural Networks",
      "author" : [ "N. Japkowicz" ],
      "venue" : "Machine Learning, vol. 42, pp. 97-122, 2001.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Boosting for Learning Multiple Classes with Imbalanced Class Distribution",
      "author" : [ "Y. Sun", "M.S. Kamel", "Y. Wang" ],
      "venue" : "Proc. IEEE Int’l Conf. Data Mining, pp. 592-602, 2006.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Optimizing Abstaining Classifiers using ROC Analysis",
      "author" : [ "T. Pietraszek" ],
      "venue" : "Proc. Int’l Conf. Machine Learning, pp. 665-672, 2005.",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Reject Option with Multiple Thresholds",
      "author" : [ "G. Fumera", "F. Roli", "G. Giacinto" ],
      "venue" : "Pattern Recognition, vol. 33, no. 12, pp. 2099-2101, 2000.",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Confidence-Based Classifier Design",
      "author" : [ "M. Li", "I.K. Sethi" ],
      "venue" : "Pattern Recognition, vol. 39, no. 7, pp. 1230-1240, 2006.",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Information-Theoretic Measures for Objective Evaluation of Classifications",
      "author" : [ "B.-G. Hu", "R. He", "X.-T. Yuan" ],
      "venue" : "Acta Automatica Sinica, vol. 38, pp. 1170-1182, 2012.",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Information Theory, Inference and Learning Algorithms",
      "author" : [ "D. MacKay" ],
      "venue" : null,
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2003
    }, {
      "title" : "Learning from Examples with Information-Theoretic Criteria",
      "author" : [ "J. Principe", "D. Xu", "Q. Zhao", "J. Fisher" ],
      "venue" : "Journal of VLSI Signal Processing Systems, vol. 26, no. 1/2, pp. 61-77, Aug. 2000.",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "An Efficient Method for Finding the Minimum of a Function of Several Variables without Calculating Derivatives",
      "author" : [ "M.J.D. Powell" ],
      "venue" : "Computer Journal, vol. 7, no. 2, pp. 155-162, 1964.",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 1964
    }, {
      "title" : "A Survey on Graphical Methods for Classification Predictive Performance Evaluation",
      "author" : [ "R.C. Prati", "G.E.A.P.A. Batista", "M.C. Monard" ],
      "venue" : "IEEE Trans. Knowledge and Data Eng., vol. 23, no. 11, pp. 1601-1618, Nov. 2011.",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Evaluation Criteria Based on Mutual Information for Classifications Including Rejected Class",
      "author" : [ "B.-G. Hu", "Y. Wang" ],
      "venue" : "Acta Automatica Sinica, vol. 34, no. 11, pp. 1396-1403, Nov. 2008.",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "An Introduction to ROC Analysis",
      "author" : [ "T. Fawcett" ],
      "venue" : "Pattern Recognition Letters, vol. 27, pp. 861-874, 2006.",
      "citeRegEx" : "44",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Robust Classification for Imprecise Environments",
      "author" : [ "F. Provost", "T. Fawcett" ],
      "venue" : "Machine Learning, vol. 42, no. 3, pp. 203-231, 2001.",
      "citeRegEx" : "45",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Rejection Strategies 15 and Confidence Measures for a k-NN Classifier in an OCR Task",
      "author" : [ "J. Arlandis", "J.C. Perez-Cortes", "J. Cano" ],
      "venue" : "Proc. Int’l Conf. Pattern Recognition, pp. 576-579, 2002.",
      "citeRegEx" : "47",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Restricted Bayes Optimal Classifiers",
      "author" : [ "S. Tong" ],
      "venue" : "Proc. AAAI National Conf. Artificial Intelligence, pp. 658-664, 2000.",
      "citeRegEx" : "48",
      "shortCiteRegEx" : null,
      "year" : 2000
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "1 INTRODUCTION IMBALANCED data sets [1], [2] arise frequently in a variety of real-world applications, such as medicine, biology, finance, and computer vision.",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 1,
      "context" : "1 INTRODUCTION IMBALANCED data sets [1], [2] arise frequently in a variety of real-world applications, such as medicine, biology, finance, and computer vision.",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 2,
      "context" : "From the background of this problem, various methods are developed within a category called cost-sensitive learning (CSL), such as costs to test [3], to relabel training instances [4], to sample [5], to weight instances [6], and to find a decision threshold [7], [8].",
      "startOffset" : 145,
      "endOffset" : 148
    }, {
      "referenceID" : 3,
      "context" : "From the background of this problem, various methods are developed within a category called cost-sensitive learning (CSL), such as costs to test [3], to relabel training instances [4], to sample [5], to weight instances [6], and to find a decision threshold [7], [8].",
      "startOffset" : 180,
      "endOffset" : 183
    }, {
      "referenceID" : 4,
      "context" : "From the background of this problem, various methods are developed within a category called cost-sensitive learning (CSL), such as costs to test [3], to relabel training instances [4], to sample [5], to weight instances [6], and to find a decision threshold [7], [8].",
      "startOffset" : 195,
      "endOffset" : 198
    }, {
      "referenceID" : 5,
      "context" : "From the background of this problem, various methods are developed within a category called cost-sensitive learning (CSL), such as costs to test [3], to relabel training instances [4], to sample [5], to weight instances [6], and to find a decision threshold [7], [8].",
      "startOffset" : 220,
      "endOffset" : 223
    }, {
      "referenceID" : 6,
      "context" : "From the background of this problem, various methods are developed within a category called cost-sensitive learning (CSL), such as costs to test [3], to relabel training instances [4], to sample [5], to weight instances [6], and to find a decision threshold [7], [8].",
      "startOffset" : 258,
      "endOffset" : 261
    }, {
      "referenceID" : 7,
      "context" : "From the background of this problem, various methods are developed within a category called cost-sensitive learning (CSL), such as costs to test [3], to relabel training instances [4], to sample [5], to weight instances [6], and to find a decision threshold [7], [8].",
      "startOffset" : 263,
      "endOffset" : 266
    }, {
      "referenceID" : 8,
      "context" : "A comprehensive review of learning in the class imbalance problem is provided by He and Garcia [9].",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 9,
      "context" : "When there exist some uncertainties in the decision, it may be better to apply abstaining classification [10] to",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 10,
      "context" : "Significant benefits have been obtained from abstaining classification, particularly in very critical applications [11], [12].",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 11,
      "context" : "Significant benefits have been obtained from abstaining classification, particularly in very critical applications [11], [12].",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 12,
      "context" : "The optimal rejection thresholds could be found through minimizing a loss function in a cost-sensitive setting [13], [14], [15].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 13,
      "context" : "The optimal rejection thresholds could be found through minimizing a loss function in a cost-sensitive setting [13], [14], [15].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 14,
      "context" : "The optimal rejection thresholds could be found through minimizing a loss function in a cost-sensitive setting [13], [14], [15].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 15,
      "context" : "The possibility of designing loss functions for classifiers with a reject option is also explored [16].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 16,
      "context" : "Based on the definition in [17], we extend it below by including the situation of abstaining.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 17,
      "context" : "We extend Hu’s [18] study on mutual information classifiers.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 18,
      "context" : "When costs are unequal and unknown, Maloof [20] uses ROC curve to show the performance of binary classifications under different cost settings.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 19,
      "context" : "Cost curve [21], [14] can be used to visualize optimal expected costs over a range of cost settings, but it does not suit multi-class problem.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 13,
      "context" : "Cost curve [21], [14] can be used to visualize optimal expected costs over a range of cost settings, but it does not suit multi-class problem.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 20,
      "context" : "Zadrozny and Elkan [22] apply least-squares multiple linear regression to estimate the costs.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 21,
      "context" : "Cross validation [23] is proposed to choose from a limited set of cost values, and the final decisions are made by users.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 22,
      "context" : "Various sampling strategies [24], [25], [26] try to modify the imbalanced class distributions.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 23,
      "context" : "Various sampling strategies [24], [25], [26] try to modify the imbalanced class distributions.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 24,
      "context" : "Various sampling strategies [24], [25], [26] try to modify the imbalanced class distributions.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 25,
      "context" : "Active learning [27] is also investigated to select desired instances and the feature selection techniques [28], [29] are applied to combat the class imbalance problem for high-dimensional data sets.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 26,
      "context" : "Active learning [27] is also investigated to select desired instances and the feature selection techniques [28], [29] are applied to combat the class imbalance problem for high-dimensional data sets.",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 27,
      "context" : "Active learning [27] is also investigated to select desired instances and the feature selection techniques [28], [29] are applied to combat the class imbalance problem for high-dimensional data sets.",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 28,
      "context" : "Besides, ensemble learning methods [30], [31] are used to improve the generalization of predicting the minority class.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 29,
      "context" : "Besides, ensemble learning methods [30], [31] are used to improve the generalization of predicting the minority class.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 30,
      "context" : "The recognitionbased methods [32], [33] that train on a single class are proposed as alternatives to the discrimination-based methods to avoid the influence of imbalanced distributions.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 31,
      "context" : "The recognitionbased methods [32], [33] that train on a single class are proposed as alternatives to the discrimination-based methods to avoid the influence of imbalanced distributions.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 32,
      "context" : "[34] can get costs for multi-class data sets through maximizing the geometric mean (Gmean) or F-measure which has the ability of balancing the performance of each class.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 33,
      "context" : "Pietraszek [35] proposes a bounded-abstaintion model with ROC analysis, and Fumera et al.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 34,
      "context" : "[36] seek to maximize accuracy while keeping the reject rate below a given value.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 35,
      "context" : "Li and Sethi [37] restrict the maximum error rate of each class, but the rates may conflict when they are arbitrarily given.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 0,
      "context" : "Note that NI(T, Y ) is in the range [0, 1].",
      "startOffset" : 36,
      "endOffset" : 42
    }, {
      "referenceID" : 36,
      "context" : "[38] apply empirical estimations to compute NI based on the confusion matrix.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 36,
      "context" : "To avoid unchanged value of NI if rejections are made within only one class, the formula of NI is proposed as [38]",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 38,
      "context" : "[40] present a schematic diagram of information theory learning (ITL) and they mention that maximizing mutual information as the target function makes the decision outputs correlate with the targets as much as possible.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "Recently, a study [18] confirms that ITL opens a new perspective for classifier design.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 37,
      "context" : "MacKay [39] recommends mutual information for its single rankable value which makes more sense than error rate.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 36,
      "context" : "[38] study theoretically for the first time on both error types and reject types in binary classifications.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 39,
      "context" : "We apply a general optimization algorithm called “Powell Algorithm” which is a direct method for nonlinear optimization without calculating the derivatives [41].",
      "startOffset" : 156,
      "endOffset" : 160
    }, {
      "referenceID" : 13,
      "context" : "[14] derive normalized cost matrix based on the overall risk which is written as",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "By applying the way of transforming costs [14], we find that the normalization way for the overall risk is also applicable for the conditional risk.",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 6,
      "context" : "The relation between the decision thresholds and the costs has been derived by Elkan [7] through minimizing the conditional risk.",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 17,
      "context" : "In abstaining binary classification, the relations between the rejection thresholds and the costs can be presented in a form of explicit formulae [18].",
      "startOffset" : 146,
      "endOffset" : 150
    }, {
      "referenceID" : 17,
      "context" : "Based on [18], one can have the relations λ̄TN < λ̄RN < λ̄FP and λ̄TP < λ̄RP < λ̄FN .",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 40,
      "context" : "In binary classification, an ROC curve plot presents complete information about the performance of each class [42], so that an overall performance measure [43], such as AUC, can be formed.",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 41,
      "context" : "In binary classification, an ROC curve plot presents complete information about the performance of each class [42], so that an overall performance measure [43], such as AUC, can be formed.",
      "startOffset" : 155,
      "endOffset" : 159
    }, {
      "referenceID" : 42,
      "context" : "This is a preferred feature in processing class imbalance problems [44].",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 43,
      "context" : "For a theoretical ROC curve which is concave, the decision is made by K, the slope of ROC curve, in the form of [45]:",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 37,
      "context" : "However, the situation in (18) can never appear from using the present strategy, because it will result in a zero value of mutual information [39], [43].",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 41,
      "context" : "However, the situation in (18) can never appear from using the present strategy, because it will result in a zero value of mutual information [39], [43].",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 14,
      "context" : "Two abstaining slopes, KN and KP , are generally given in the forms of [15]:",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 17,
      "context" : "(19) confirms the finding in [18] that at most two independent parameters will determine the rejection range in binary classifications.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 17,
      "context" : "There exist relations between rejection thresholds in the posterior curve plot [18] and abstaining slopes in the ROC curve plot.",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 17,
      "context" : "Their relations and the associated constraint are derived from [18]:",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 23,
      "context" : "Most of the data sets are obtained from the UCI Machine Learning Repository, Ism is from [25], Rooftop is from [20], and Phoneme is from KEEL Datasets.",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 18,
      "context" : "Most of the data sets are obtained from the UCI Machine Learning Repository, Ism is from [25], Rooftop is from [20], and Phoneme is from KEEL Datasets.",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 0,
      "context" : "All of them have continuous attributes and are rescaled to be in the range [0, 1].",
      "startOffset" : 75,
      "endOffset" : 81
    }, {
      "referenceID" : 9,
      "context" : "To illustrate the effectiveness of our strategy, we adopt kNN and Bayes classifier as the conventional classifiers, and we compare our methods with SMOTE, Cost-sensitive learning, Chow’s reject [10] methods and the G-mean based methods (“Gmean no rej” and “Gmean rej”) besides two conventional classifications.",
      "startOffset" : 194,
      "endOffset" : 198
    }, {
      "referenceID" : 44,
      "context" : "In kNN classifier, we apply Euclidean distance and use the confidence values [47], [19] as the probabilistic",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 45,
      "context" : "In Bayes classifier, we derive the estimated class-conditional density from the Parzen-window estimation with Gaussian kernel [48] and apply Bayes rule to classification.",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 42,
      "context" : "3 shows the ROC convex hull (ROCCH) of kNN for Diabetes generated from 90 validation sets by threshold averaging [44].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 14,
      "context" : "And we use them to approximatively locate the parameters [15].",
      "startOffset" : 57,
      "endOffset" : 61
    } ],
    "year" : 2013,
    "abstractText" : "In this work, we define cost-free learning (CFL) formally in comparison with cost-sensitive learning (CSL). The main difference between them is that a CFL approach seeks optimal classification results without requiring any cost information, even in the class imbalance problem. In fact, several CFL approaches exist in the related studies, such as sampling and some criteria-based approaches. However, to our best knowledge, none of the existing CFL and CSL approaches are able to process the abstaining classifications properly when no information is given about errors and rejects. Based on information theory, we propose a novel CFL which seeks to maximize normalized mutual information of the targets and the decision outputs of classifiers. Using the strategy, we can deal with binary/multi-class classifications with/without abstaining. Significant features are observed from the new strategy. While the degree of class imbalance is changing, the proposed strategy is able to balance the errors and rejects accordingly and automatically. Another advantage of the strategy is its ability of deriving optimal rejection thresholds for abstaining classifications and the “equivalent” costs in binary classifications. The connection between rejection thresholds and ROC curve is explored. Empirical investigation is made on several benchmark data sets in comparison with other existing approaches. The classification results demonstrate a promising perspective of the strategy in machine learning.",
    "creator" : "LaTeX with hyperref package"
  }
}