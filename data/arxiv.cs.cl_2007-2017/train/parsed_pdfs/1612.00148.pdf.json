{
  "name" : "1612.00148.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Domain Adaptation for Named Entity Recognition in Online Media with Word Embeddings",
    "authors" : [ "Vivek Kulkarni", "Yashar Mehdad", "Troy Chevalier" ],
    "emails" : [ "{vvkulkarni@cs.stonybrook.edu,", "ymehdad@yahoo-inc.com,", "troyc@yahoo-inc.com}" ],
    "sections" : [ {
      "heading" : null,
      "text" : "In this paper, we propose methods to effectively adapt models learned on one domain onto other domains using distributed word representations. First we analyze the linguistic variation present across domains to identify key linguistic insights that can boost performance across domains. We propose methods to capture domain specific semantics of word usage in addition to global semantics. We then demonstrate how to effectively use such domain specific knowledge to learn NER models that outperform previous baselines in the domain adaptation setting.\n∗This work was done when the author was a research intern at Yahoo. ∗© 2016 This is the authors draft of the work. It is posted here for your\npersonal use. Not for redistribution."
    }, {
      "heading" : "1 Introduction",
      "text" : "Named Entity Recognition (NER) is a critical task for understanding textual content. While most NER systems demonstrate very good performance, this performance is typically measured on test data drawn from the same domain as the training data.\nFor example, most competitive Named Entity Recognition systems are trained on large amounts of labeled data from a given domain (like CoNLL or MUC) and evaluated on a held out test set drawn from the same domain (Florian et al., 2003; Chieu and Ng, 2002; Ando and Zhang, 2005; Collobert et al., 2011; Huang et al., 2015). While such systems demonstrate high performance in-domain, content on the Internet can originate from multiple domains like Finance and Sports over which these systems perform quite poorly. Moreover one typically does not have access to large amounts of labeled examples on these domains to train robust domain specific models. This challenge is typically addressed through domain adaptation techniques (Blitzer et al., 2006; Jiang and Zhai, 2007; Satpal ar X\niv :1\n61 2.\n00 14\n8v 1\n[ cs\n.C L\n] 1\nD ec\n2 01\n6\nand Sarawagi, 2007; Jiang, 2008; Li, 2012). Most existing work on domain adaptation like Feature Sub-setting (Satpal and Sarawagi, 2007), Structural Correspondence Learning (Blitzer et al., 2006; Chen et al., 2012), learn a subset of features or learn dense representations of features that are more suited for domain adaptation. Different from these works, we explore word embeddings that explicitly capture domain specific differences while still capturing shared semantics across domains, and show that our proposed methods outperform several competitive baselines on domain adaptation for NER.\nWith recent advances in representation learning, word embeddings have been shown to be very useful features for several NLP tasks like POS Tagging, NER, and Sentiment Analysis (Chen et al., 2013; Al-Rfou et al., 2015; Collobert et al., 2011). One drawback of using generic word embeddings is that these word vectors do not capture domain specific differences in word semantics and usage. To illustrate this, consider articles from two distinct domains: (a) Sports and (b) Finance. The word tackle in the Sports domain is generally associated with moves in football and used as “A defensive tackle”. However in the domain of Finance, tackle is used to indicate problem solving as in “The company needs to tackle the rising costs immediately”.\nExplicitly modeling such domain specific differences allows us to capture linguistic variation between domains that serve as distinctive features to boost performance of a machine learning model on NLP tasks. In this work we propose methods to effectively model such domain specific differences of language. We then apply our methods to analyze domain specific differences in word semantics. Finally, we demonstrate the effectiveness of using domain specific word embeddings for the task of Named Entity Recognition in the domain adaptation setting. Figure 1 shows the domain specific differences captured by our method across two domains (a) Sports and (b) Finance. Observe how the domain specific embeddings that our method learns can easily capture the distinct usages of a word (in this case as a Person or an Organization). As we will show in Section 4 such distinctive representations can improve performance of Named Entity Recognition in different domains outperforming competitive baselines.\nIn a nut shell, our contributions are as follows:\n• Linguistic Variation across Domains: Given a word w how does its usage differ across different domains? We analyze variation in word usage (semantics) across different domains like Finance and Sports using distributed word representations (Section 2.1).\n• NER systems for Sports and Finance: We propose methods to effectively use such domain specific knowledge captured by word embeddings towards the task of Named Entity Recognition. In particular we show how to build state of the art NER systems for domains with scarce amount of annotated training data by adapting NER models learned primarily on domains with large amounts of annotated training data (Section 2.2)."
    }, {
      "heading" : "2 Methods",
      "text" : "In this section we propose (a) Two methods to model domain specific word semantics in order to explicitly capture linguistic differences between domains and (b) Two methods that use domain specific word embeddings to learn robust Named Entity Recognition models for different domains using domain adaptation."
    }, {
      "heading" : "2.1 Domain Specific Linguistic Variation",
      "text" : ""
    }, {
      "heading" : "DOMAINDIST",
      "text" : "Given a corpus C with K domains and vocabulary V , we seek to learn a domain specific word embedding φk : V 7→ Rd using a neural language model where k ∈ {1 · · ·K}. We apply the method discussed in (Bamman et al., 2014; Kulkarni et al., 2015b) to learn domain specific word embeddings. 1 We briefly describe this approach below as pertaining to learning domain specific embeddings. For each word w ∈ V the model learns (1) A global embedding δMAIN(w) for the word ignoring all domain specific cues and (2) A differential embedding δk(w) that encodes deviations from the global embedding for w specific to domain k. The domain specific embedding φk(w) is computed as: φk(w) = δMAIN(w) + δk(w). The global word embeddings are randomly initialized, while the differential word embeddings are initialized to 0. We use the Skip-gram objective function with hierarchical soft-max to learn the global and the differential embeddings. We set the learning\n1 In Section A.1 we differentiate ourselves from (Bamman et al., 2014; Kulkarni et al., 2015b) by outlining a probabilistic method that uses this model to disambiguate the domain given a phrase that outlines the usage of a word w.\nrate α = 0.025, context window size m to 10 and word embedding size d to be 100. An example of the domain specific linguistic variation captured by DOMAINDIST is illustrated in Figure 1."
    }, {
      "heading" : "DOMAINSENSE",
      "text" : "Here we outline yet another method to capture semantic variation in word usage across domains. We model the problem as follows:\n• Sense Specific Embeddings We assume each word w has potentially S senses where we seek to learn not only an embedding for each sense of w but also infer what these senses are from the corpus C. • Sense Proportions in Domains The usage of w in each domain d can be characterized by a probability distribution πd(w) over the inferred senses of w.\nTo learn sense specific embeddings, we use the Adaptive Skip-gram model proposed by (Bartunov et al., 2015) to automatically infer (a) the different senses a word w exhibits (b) a probability distribution π(w) over the the different senses a word exhibits in the corpus and (c) an embedding for each sense of the word. Specifically, we combine the sub-corpora of different domains to form a single corpus C. We then learn sense specific embeddings for each word w in C using the Adaptive Skip-gram model. We set the number of dimensions d of the embedding to 100, the maximum number of senses a word has S = 5 and restrict the vocabulary to only words that occur more than 100 times.\nFinally, given a word w we quantify the difference in the sense usage of w between two domains di and dj as follows:\n1. Disambiguate each occurrence of w in di and dj using the method described by (Bartunov et al., 2015). We can then estimate the sense distribution of word w in domain di, πdi(w)\nas Pr(πdi(w) = s) = #di (Sense(w)=s)\n#di (w) where\n#di(X) represents the count of number of times X is true in domain di.\n2. We then compute the Jennsen-Shannon Divergence (JSD) between the sense distributions of the wordw between the two domains di and dj to quantify the difference in sense usage of w between these domains.\nTable 1 shows a small sample of words along with their inferred senses using this method. Figure 2 then depicts the domain specific difference in the\nsense usages of goal as computed by DOMAINSENSE.\nWhile both DOMAINDIST and DOMAINSENSE explicitly capture domain specific differences in word semantics, they differ in their underlying models. DOMAINDIST captures domain specific word semantic/usage by directly learning domain specific word representations. DOMAINSENSE on the other hand infers different senses of a word and learns an embedding for each sense. Domain specific differences are then modeled by differences in sense usage of the word across domains. To illustrate this difference, consider the word goal. DOMAINDIST will capture the fact that goal is associated with match, winning in Sports and capture this sense of goal in the Sports Specific Embedding. DOMAINSENSE in contrast will infer that goal has two senses overall (see Table 1) and then capture that in Sports both these senses are used. Moreover, the sense related to score is used 70% of the time while the sense associated with objective is estimated to be used 30% of the time in Sports. Finally, we empirically evaluate the effectiveness of both DOMAINDIST and DOMAINSENSE for the task of NER (Section 4)."
    }, {
      "heading" : "Error Bounds on Estimated JS Divergence",
      "text" : "Note that for any given word w, the empirical probability estimate computed as Pr(πdi(w) = s) is estimated from its usage in the sample corpus and is hence a random variable. Since these estimates of probabilities are further used to compute the JS Divergence between the sense distributions over two domains (di, dj) this estimate is also a random\nvariable and demonstrates variance. We now provide theoretical bounds on the standard deviation of the computed Jennsen Shannon Divergence.\nLemma 1. Let na(w) and nb(w) be the total number of occurrences of a word w in domains da and db respectively. The standard deviation in the JS divergence of the sense distribution of w across this domain pair is O( 1√\nna(w) + 1√ nb(w) )\nProof. We provide the proof in the supplemental material.\nThe bound above implies the following: (a) We can quantify the uncertainty in our estimates based on the frequency of the words in the corpus and interpret our results with greater confidence. Very rare words would have larger deviations. (b) Depending on an applications sensitivity to error, we can estimate the appropriate sample complexity needed. 2"
    }, {
      "heading" : "2.2 Domain Adaptation for NER",
      "text" : "In the previous section, we described methods to capture domain specific linguistic variation in word semantics/usage by learning word embeddings that are domain specific. In this section, we outline how to learn NER models for the various domains using such word embeddings as features.\nAs in previous works, we treat NER as a sequence labeling problem. To train, we use CRFsuite (Okazaki, 2007) with L-BFGS algorithm. We use a BILOU label encoding scheme. The features we use are listed in Table 2. Our main features are tokens and word embeddings, within a small window of the target token. We investigate using different kinds of embeddings listed below:\n• Generic Word2vec embeddings: We learn generic Skipgram embeddings using English Wikipedia.\n2Our reported results (see Figure 3) computing JS Divergence all have counts >= 1000 in both domains and hence have low errors.\n• Domain/Sense Specific Word Embeddings: We experiment by using the embeddings learned using DOMAINDIST and DOMAINSENSE."
    }, {
      "heading" : "DOMAINEMBNER",
      "text" : "Here, we outline the supervised domain adaptation method that uses domain specific word embeddings to learn NER models that significantly outperform other baselines on NER task in the domain adaptation setting. In this setting, we are interested in a Named Entity Recognition system for domain T . However training data available for domain T is scarce but we have access to a source domain S for which we have large number of training examples. We would like to perform domain adaptation by learning a model using the large amount of training data in source domain S and adapt it to work well on the target domain T . There exist a number of methods for the task of supervised domain adaptation (Jiang, 2008). We use a simple method for this task outlined below:\n1. Combine the training data from S and T . Note again that |S|>> |T | in our setting.\n2. Extract the features outlined for training the CRF model as out-lined in Table 2. Note that we experiment with different kinds of word embeddings and baselines.\n3. Learn a CRF model using this training data.\n4. Evaluate the learned CRF model on the domain specific test data set and report the performance.\nAs we will show in Section 4, using domain specific word embeddings improves the performance of NER on these target domains significantly and outperforms previous baselines for this task."
    }, {
      "heading" : "ACTIVEDOMAINEMBNER",
      "text" : "In this section, we describe how we can learn a Named Entity Recognition system, assuming we\nAlgorithm 1 ACTIVEDOMAINEMBNER (S, T , B, k) Input: S: Training data for NER in the source domain, T :\nUnlabeled data for the task of NER in the target domain which is separate and distinct from the final test set. B: Number of actively labeled examples, k: Batch size of actively labeled examples. Output: M : NER model 1: C ← S 2: repeat 3: Learn a model M using C 4: Evaluate M on T . 5: E ← Sort the evaluated phrases of T in ascending\norder of model confidence (probability) and remove top k least confident examples. 6: Ask an expert to label each example in E and add them to C. 7: C ← C ∪ E 8: until |C|≥ |S|+B 9: return M\nhave no labeled training data in the target domain. We can however request for a small number of examples B to be labeled by annotators. In such a setting, one can actively choose the set of examples that need to be labeled which will be most useful to learn a good model. We propose a method to actively label examples for the purpose of domain adaptation which we describe succinctly in Algorithm 1. In Section 4 we show that by merely asking for an editorial to label 1500 sentences, we can achieve performance close to state of art in this setting."
    }, {
      "heading" : "3 Datasets",
      "text" : "In this section, we outline details of the datasets we consider for our experiments.\nOur datasets can be classified into 2 categories (a) Unlabeled data for learning word embeddings and (b) Labeled data for the task of NER, each of which we describe below."
    }, {
      "heading" : "3.1 Unlabeled Data",
      "text" : "We use the following unlabeled data sets for the purpose of learning word embeddings. We consider (a) all sentences of English Wikipedia (b) a random sample of 1 Million articles from Yahoo! Finance restricting our language to only English and (c) a random sample of 1 Million articles from Yahoo! Sports restricting our language to only English."
    }, {
      "heading" : "3.2 Labeled Data",
      "text" : "We also use labeled data sets for the task of learning NER models which we summarize in Table 3."
    }, {
      "heading" : "4 Experiments",
      "text" : "Here, we briefly describe the results of our experiments on (a) Domain Specific Linguistic Variation and (b) Domain Adaptation for Named Entity Recognition."
    }, {
      "heading" : "4.1 Domain Specific Linguistic Variation",
      "text" : "Table 4 shows some of the semantic differences in word usage captured by DOMAINDIST. Observe that the method is able to capture words like quote, overtime, hurdles that have alternative meanings (semantics) in a domain. For example, the word hurdles means challenges in Finance but a kind of athletic race in Sports. In addition to capturing words that differ in semantics, note that DOMAINDIST also uncovers differing semantic usages of entities as well, as depicted in Figure 1. In the domain of Finance, Anthem refers to a health insurance company but Anthem in Sports dominantly refers to a song like a team anthem. In Figure 3 a sample set of words detected by DOMAINSENSE are shown. Note once again, that we are able to capture domain specific differences between words (both entities and nonentities). Furthermore, DOMAINSENSE is able to quantify the proportion of each word sense usage in various domains. For example, the word tackle is used exclusively in Finance as a verb that means to solve, whereas in Sports tackle is dominantly used to refer to an American football move. Note that in Sports, the sense of tackle that means to solve is only used 30% of the time.\nThis ability to capture differing entity roles (like\nOrganizations and Persons) provides an insight into the effectiveness of domain specific embeddings for improved performance on Named Entity Recognition."
    }, {
      "heading" : "4.2 Domain Adaptation for Named Entity Recognition",
      "text" : "In this section, we report the results of using our DOMAINDIST and DOMAINSENSE word embeddings for the task of Named Entity Recognition on Finance and Sports Domains in the domain adaptation setting as described in Section 2. We also outline the baseline methods we compare to below:"
    }, {
      "heading" : "Baseline methods",
      "text" : "Since our setting is the setting of domain adaptation for Named Entity Recognition, we consider several competitive baselines for this task:\n• CoNLL-only Model: We consider a simple baseline where we train a NER model only using CoNLL data and generic Wikipedia Embeddings without any adaptation to the target domain. • Feature Subsetting: This domain adaptation method tries to penalize features which demonstrate large divergence between source and target domains (Satpal and Sarawagi, 2007). It is worth noting that this models the task of NER as a classification problem and not a structured prediction problem. 3 • Online-FLORS: FLORS learns robust representations of each word based on distributional features and counts, to boost performance across domains and treats the tagging problem as a classification problem. We also use a random sample of 100K unlabeled sentences from each domain which FLORS uses to enrich the robustness of representations learned. We consider a scalable version of FLORS (Yin et al., 2016). • FEMA: FEMA (Yang and Eisenstein, 2015) learns low dimensional embeddings of the features used in a CRF model by using a variant of the Skipgram Model (Mikolov et al., 2013). These features can be used to learn a model for sequence tagging. While they demonstrate their method on Part of Speech tagging, the method itself is general and can be applied to other tasks like Named Entity Recognition as well and provide a nice replacement for word\n3We use the implementation of feature sub-setting for Named Entity Recognition provided by https://github.com/siqil/udaner.\nembeddings as features in a CRF model. We learned 100 dimensional FEMA embeddings in our experiment. 4"
    }, {
      "heading" : "Results and Discussion",
      "text" : "Table 5 shows the performance of our methods and other baselines on Finance and Sports. First note that a CoNLL-only model without any domain adaptation results in poor performance. Domain Adaptation methods like Feature Subsetting and FLORS that model Named Entity Recognition as a classification problem, rather than a sequence prediction problem perform even worse. In contrast FEMA which learns dense representations of CRF features which can then be used to learn a more robust CRF model (that is more suited to domain adaptation) yields an significantly improved F1 score of 67.70 on Finance and 82.48 on Sports respectively. Empirically we observe that using DOMAINSENSE embeddings improves the performance over the ConLL only model, but does not perform as well on this task (especially in Finance). We hypothesize while decomposing a word into multiple fine-grained senses is useful to capture semantic variation, using fine-grained sense embeddings for every word results in a overly complex decision space when used for tasks like NER. Finally observe that DOMAINDIST which learns domain specific embeddings (without explicitly decomposing words into their senses) outperforms all these methods in both domains. This superior performance results from the ability to capture useful broad domain specific differences more effectively.\nIn Table 6, we evaluate the performance of using domain specific word embeddings 5 against using just generic Wikipedia based word embeddings as a function of available training data α. First observe that on an average, using DOMAINDIST word embeddings improves the performance over using generic Wikipedia based embeddings. Observe that in general, as the amount of training data in the target domain increases, the advantage (gain) of using domain specific embeddings reduces. For example, when only 10% of training data is available for Finance, using domain specific word embeddings results in F1 Score gain of 1.21. However when 90% of training data is available in Finance we get a small but still significant boost of 0.42 in the F1 score on using domain specific word embeddings.\n4We use the open source implementation provided at https://github. com/yiyang-gt/feat2vec\n5For brevity. we present results here only DOMAINDIST embeddings as DOMAINDIST embeddings are the best performing embeddings.\nWe explain this by noting that as the proportion of training data in the target domain increases, the model is able to pick up on domain specific cues and fine-tune its decision boundary better without needing to rely too much on the domain specific cues captured by the domain specific word embed-\ndings.\nTable 7 shows the performance of ACTIVEDOMAINEMBNER as a function of number of sentences we sought to be actively labeled. Note that merely requiring 1500 sentences to be manually annotated, we are able to achieve close to state of\nart F1 performance (69.79 on Finance and 86.78 on Sports respectively) outperforming competitive baselines."
    }, {
      "heading" : "5 Related Work",
      "text" : "Related work can be organized into two areas: (a) Socio-variational linguistics and (b) Domain Adaptation. Socio-variational linguistics Several works study how language varies according to geography and time (Eisenstein et al., 2010; Eisenstein et al., 2011; Bamman and others, 2014; Bamman et al., 2014; Kim et al., 2014; Kulkarni et al., 2015a; Kenter et al., 2015; Gonçalves and Sánchez, 2014; Kulkarni et al., 2015b; Cook et al., 2014; Frermann and Lapata, 2016; Hamilton et al., 2016). Different from these studies, our work seeks to identify semantic changes in word meaning (usage) across domains with a focus on improving performance on an NLP task like NER. The methods outlined in (Kulkarni et al., 2015b; Bamman et al., 2014) are most closely related to our work. While we directly build on methods outlined by them we differentiate ourselves from their work in two ways. First, we show that the model proposed in (Bamman et al., 2014) is not only useful for learning domain specific word embeddings but the model itself can be utilized for the task of domain disambiguation by using an often neglected set of model parameters (the output vectors of the model).\nSecond, we also present a method to quantify the semantic variation in word usage by explicitly modeling differences in the usage of different senses of words. While the methods outlined in (Kulkarni et al., 2015b; Bamman et al., 2014) capture domain specific differences, they do not explicitly model the fact that words have multiple senses and their usage in a domain is a mixture of different proportions over these senses which can be explicitly quantified. Finally we apply these methods to identify and analyze semantic variation in word usage across domains like Sports and Finance, highlight interesting examples of such variation prevalent across these domains with applications to Named Entity Recognition.\nDomain Adaptation There is a long line of work on domain adaptation (Evgeniou and Pontil, 2004; Ando and Zhang, 2005; Blitzer et al., 2006; Jiang and Zhai, 2007; Satpal and Sarawagi, 2007; Daumé III, 2009; Chen et al., 2012; Schnabel and Schütze, 2014; Yang and Eisenstein, 2015). Most of these works can be classified based on the strategies they use as follows: (a) Instance Weighting Methods (Satpal and Sarawagi, 2007; Jiang and Zhai, 2007) (b) Regularization based methods (Evgeniou and Pontil, 2004; Daumé III, 2009) and (c) Representation Induction (Blitzer et al., 2006; Chen et al., 2012; Schnabel and Schütze, 2014; Yang and Eisenstein, 2015). Our method of learning domain specific word embeddings in an unsupervised manner can be placed into this final category. Finally an excellent survey of various domain adaptation algorithms for NLP is provided by (Jiang, 2008; Li, 2012)."
    }, {
      "heading" : "6 Conclusions",
      "text" : "In this paper, we proposed methods to detect and analyze semantic differences in word usage across multiple domains. Our methods explicitly capture domain specific cues by learning word embeddings from unlabeled text and scale well to large web scale data sets. Furthermore, we outline methods that leverage such domain specific linguistic variation and knowledge effectively to boost performance on NLP tasks like Named Entity Recognition on domains with scarce training data and requiring domain adaptation. Our methods not only out-perform previous competitive baselines but also require a very small number of manually annotated sentences in the target domain to achieve competitive performance. We believe our work sets\nthe stage for new directions and further research into applications that effectively model linguistic variation across domains to improve the performance, applicability and usability of NLP systems analyzing the diverse content on the Internet."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank Akshay Soni, Swayambhoo Jain, Aasish Pappu and\nKapil Thadani for valuable insights and discussions."
    }, {
      "heading" : "A Supplemental Material",
      "text" : ""
    }, {
      "heading" : "A.1 Domain Disambiguation",
      "text" : "In this section, we outline a method DOMAINDIST++ that builds on DOMAINDIST to infer the domain given a phrase highlighting a word’s usage is likely to belong to. Specifically, given a finite set of domainsD, a wordw and a set of context words T , we would like to infer the most likely domain d which reflects the given usage of w. As an illustration, suppose we have two potential domains D = {SPORTS, FINANCE} and given the usage of word loss as loss of the dividend we would like to infer that this usage is most likely from the Finance domain.\nFirst note that DOMAINDIST models the probability of a word o given a context word c as follows:\nPr(o|c,D = d) = exp(uo Tvc)∑\nv∈V exp(uo Tv)\n(1)\nwhere D is discrete random variable that represents the particular domain used. The vector uo corresponds to the output vector for o while vc corresponds to the input vector (word embedding) for word c in domain d.\nGiven a word o and its context word c (where c is a context word in the usage of o), we seek to compute the probability Pr(D = d|o, c). We decompose this as follows:\n(2)Pr(D = d|o, c) ∝ Pr(o|c,D = d) Pr(c|D = d)Pr(D = d)\nThe first term in Equation 2 is modeled by Equation 1 which can be efficiently computed in O(log|V|) using hierarchical soft-max (Mikolov et al., 2013). The second term in Equation 2 is easily estimated by computing the relative frequency of c in the corpus specific to domain d. The final term is just a prior on the domains (which can be computed by relative sizes of the domain specific corpora or set to uniform).\nTo conclude, given a word-context pair, we can estimate the domain that characterizes this wordusage by\nd̂ = argmax d\nPr(D = d|o, c). (3)\nWhile we discuss how to disambiguate the domain given word-context pair the above can be trivially extended when multiple context words are given. In such a case, we make an independence assumption: Each word-context pair is independent. This decomposes the joint into a product of probabilities for each word context-pair (each of which can be computed by Equation 1).\nIn order to evaluate our method for domain disambiguation, we consider the following three competitive approaches:\n• Unigram Model One simple method to disambiguate the domain reflecting the usage of a word w along with its context words T , is to estimate the probability of this phrase under a unigram language model specific to the domain. Specifically we estimate the domain as follows:\nd̂ = argmax d Pr(w|D = d) ∏ c∈T Pr(c|D = d)\n(4)\n• DistanceMean (DM) We consider a simple nearest neighbor based method. For each domain d, we compute a score, the mean cosine similarity between the word w and the context words c ∈ T and choose the domain with the higher score. In summary, we estimate the domain by:\nd̂ = argmax d Score(d), (5)\nwhere Score(d) is given by:\nScore(d) = 1 |T | ∑ c∈T CosineSim(vw,vc),\n(6) . Here vw and vc are the word embeddings for w and c specific to the considered domain d.\n• Context Vector Mean (CVM) We consider yet another nearest neighbor based method. For each domain d, we first compute the mean context embedding vT̃ by averaging the domain specific word embeddings for each context word c. We then compute a score Score(d) which is the cosine similarity between the domain specific word embedding for w and the mean context vector. Therefore, we estimate the domain as follows:\nd̂ = argmax d Score(d) (7)\nwhere Score(d) = CosineSim(vw,vT̃) and vT̃ = 1 |T | ∑ c∈T vc\nTo highlight the differences between these methods and our method, we evaluate these methods on a small but insightful dataset of 20 phrases which we show in Table 8. We describe our observations and conclusions briefly below:\n• The Unigram model fails when the context words are not distinctive of domains and occur with similar frequencies in both domains (as illustrated by the first eight examples in Table 8).\n• DOMAINDIST++ is superior to Unigram and is competitive with other nearest neighbor baselines. One drawback of the nearest neighbor methods is that they do not explicitly and interpret-ably capture strength of domain membership which DOMAINDIST++\ncaptures. To illustrate, consider disambiguating the usage of the word on in we are focused on the heats. The word on does not really have distinctive semantic usages between the two domains SPORTS and FINANCE. DOMAINDIST++ will assign close to equal probabilities of membership of on to both these domains thus naturally capturing the graded membership, which is not possible using nearest neighbor methods (DM and CVM).\nA.2 Error bounds on computation of JS Divergence for DOMAINSENSE\nLemma 1 (Lemma 1). Let na(w) and nb(w) be the total number of occurrences of a word w in domains da and db respectively. The standard deviation in the JS divergence of the sense distribution of w across this domain pair isO( 1√\nna(w) + 1√ nb(w) )\nProof. Assume that a word w has S senses where the probability distribution over the senses in domain da is given by p = (p1, p2, . . . pS) and that in domain db is given by q = (q1, q2, . . . qS).\nThe JS Divergence between the probability distributions p and q is given by:\nJS(p,q) = ∑ pi log pi + ∑ qi log qi\n2 −∑ pi + qi\n2 log\n(pi + qi)\n2\n(8)\nNow note that each of pi and qi are sample MLE estimates of multinomial distribution. The standard deviations of each of these sample MLE estimates denoted by σpi and σqi are as follows:\nσpi = √ p∗i (1− p∗i ) na(w)\n(9)\nσqi = √ q∗i (1− q∗i ) nb(w)\n(10)\nwhere p∗i , q ∗ i are the true values of the particular probabilities. In order to quantify the standard deviation in the resulting computation of JS(p,q) which we denote by σJS(p,q), we now apply the rule for propagation of uncertainty6, which yields:\n6We ignore covariance terms as each parameter is estimated independently.\nσJS(p,q) = √√√√√√√ ∑ ( ∂JS(p,q) ∂pi )2σp2i\n+∑ ( ∂JS(p,q)\n∂qi )2σq2i\n(11)\nThe coefficients ∂JS(p,q)∂pi and ∂JS(p,q) ∂qi are called the sensitivity coefficients. Substituting the computations for σpi and σqi in Equation 11 completes the proof. This fact that the uncertainty in the JS Divergence is inversely proportional to the square root of the sample size enables us to get reasonably accurate estimates by choosing an appropriate sample size."
    } ],
    "references" : [ {
      "title" : "Polyglotner: Massive multilingual named entity recognition",
      "author" : [ "Al-Rfou et al.2015] Rami Al-Rfou", "Vivek Kulkarni", "Bryan Perozzi", "Steven Skiena" ],
      "venue" : null,
      "citeRegEx" : "Al.Rfou et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Al.Rfou et al\\.",
      "year" : 2015
    }, {
      "title" : "A framework for learning predictive structures from multiple tasks and unlabeled data",
      "author" : [ "Ando", "Zhang2005] Rie Kubota Ando", "Tong Zhang" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Ando et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Ando et al\\.",
      "year" : 2005
    }, {
      "title" : "Gender identity and lexical variation in social media",
      "author" : [ "David Bamman" ],
      "venue" : "Journal of Sociolinguistics",
      "citeRegEx" : "Bamman,? \\Q2014\\E",
      "shortCiteRegEx" : "Bamman",
      "year" : 2014
    }, {
      "title" : "Distributed representations of geographically situated language",
      "author" : [ "Bamman et al.2014] David Bamman", "Chris Dyer", "Noah A. Smith" ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Bamman et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bamman et al\\.",
      "year" : 2014
    }, {
      "title" : "Breaking sticks and ambiguities with adaptive skipgram",
      "author" : [ "Dmitry Kondrashkin", "Anton Osokin", "Dmitry Vetrov" ],
      "venue" : "arXiv preprint arXiv:1502.07257",
      "citeRegEx" : "Bartunov et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bartunov et al\\.",
      "year" : 2015
    }, {
      "title" : "Domain adaptation with structural correspondence learning",
      "author" : [ "Blitzer et al.2006] John Blitzer", "Ryan McDonald" ],
      "venue" : "In Proceedings of the 2006 conference on empirical methods in natural language processing,",
      "citeRegEx" : "Blitzer and McDonald,? \\Q2006\\E",
      "shortCiteRegEx" : "Blitzer and McDonald",
      "year" : 2006
    }, {
      "title" : "Marginalized denoising autoencoders for domain adaptation",
      "author" : [ "Chen et al.2012] Minmin Chen", "Zhixiang Xu" ],
      "venue" : "arXiv preprint arXiv:1206.4683",
      "citeRegEx" : "Chen and Xu,? \\Q2012\\E",
      "shortCiteRegEx" : "Chen and Xu",
      "year" : 2012
    }, {
      "title" : "The expressive power of word embeddings",
      "author" : [ "Chen et al.2013] Yanqing Chen", "Bryan Perozzi", "Rami Al-Rfou", "Steven Skiena" ],
      "venue" : "arXiv preprint arXiv:1301.3226",
      "citeRegEx" : "Chen et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2013
    }, {
      "title" : "Named entity recognition: a maximum entropy approach using global information",
      "author" : [ "Chieu", "Ng2002] Hai Leong Chieu", "Hwee Tou Ng" ],
      "venue" : "In Proceedings of the 19th international conference on Computational linguistics-Volume",
      "citeRegEx" : "Chieu et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Chieu et al\\.",
      "year" : 2002
    }, {
      "title" : "Natural language processing (almost) from scratch. JMLR",
      "author" : [ "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa" ],
      "venue" : null,
      "citeRegEx" : "Collobert et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2011
    }, {
      "title" : "Novel wordsense identification",
      "author" : [ "Cook et al.2014] Paul Cook", "Jey Han Lau", "Diana McCarthy", "Timothy Baldwin" ],
      "venue" : null,
      "citeRegEx" : "Cook et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Cook et al\\.",
      "year" : 2014
    }, {
      "title" : "A latent variable model for geographic lexical variation",
      "author" : [ "Brendan O’Connor", "Noah A Smith", "Eric P Xing" ],
      "venue" : null,
      "citeRegEx" : "Eisenstein et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Eisenstein et al\\.",
      "year" : 2010
    }, {
      "title" : "Discovering sociolinguistic associations with structured sparsity",
      "author" : [ "Noah A Smith" ],
      "venue" : "ACL-HLT",
      "citeRegEx" : "Eisenstein and Smith,? \\Q2011\\E",
      "shortCiteRegEx" : "Eisenstein and Smith",
      "year" : 2011
    }, {
      "title" : "Regularized multi–task learning",
      "author" : [ "Evgeniou", "Pontil2004] Theodoros Evgeniou", "Massimiliano Pontil" ],
      "venue" : "In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "citeRegEx" : "Evgeniou et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Evgeniou et al\\.",
      "year" : 2004
    }, {
      "title" : "Named entity recognition through classifier combination",
      "author" : [ "Florian et al.2003] Radu Florian", "Abe Ittycheriah" ],
      "venue" : "In Proceedings of the seventh conference on Natural language learning at HLTNAACL 2003-Volume",
      "citeRegEx" : "Florian and Ittycheriah,? \\Q2003\\E",
      "shortCiteRegEx" : "Florian and Ittycheriah",
      "year" : 2003
    }, {
      "title" : "A bayesian model of diachronic meaning change. Transactions of the Association for Computational Linguistics",
      "author" : [ "Frermann", "Lapata2016] Lea Frermann", "Mirella Lapata" ],
      "venue" : null,
      "citeRegEx" : "Frermann et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Frermann et al\\.",
      "year" : 2016
    }, {
      "title" : "Crowdsourcing dialect characterization through twitter",
      "author" : [ "Gonçalves", "Sánchez2014] Bruno Gonçalves", "David Sánchez" ],
      "venue" : null,
      "citeRegEx" : "Gonçalves et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Gonçalves et al\\.",
      "year" : 2014
    }, {
      "title" : "Diachronic word embeddings reveal statistical laws of semantic change",
      "author" : [ "Jure Leskovec", "Dan Jurafsky" ],
      "venue" : "arXiv preprint arXiv:1605.09096",
      "citeRegEx" : "Hamilton et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Hamilton et al\\.",
      "year" : 2016
    }, {
      "title" : "Bidirectional LSTM-CRF models for sequence tagging. CoRR, abs/1508.01991",
      "author" : [ "Huang et al.2015] Zhiheng Huang", "Wei Xu", "Kai Yu" ],
      "venue" : null,
      "citeRegEx" : "Huang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2015
    }, {
      "title" : "Instance weighting for domain adaptation in nlp",
      "author" : [ "Jiang", "Zhai2007] Jing Jiang", "ChengXiang Zhai" ],
      "venue" : "In ACL,",
      "citeRegEx" : "Jiang et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2007
    }, {
      "title" : "Domain adaptation in natural language processing. ProQuest",
      "author" : [ "Jing Jiang" ],
      "venue" : null,
      "citeRegEx" : "Jiang.,? \\Q2008\\E",
      "shortCiteRegEx" : "Jiang.",
      "year" : 2008
    }, {
      "title" : "Ad hoc monitoring of vocabulary shifts over time",
      "author" : [ "Kenter et al.2015] Tom Kenter", "Melvin Wevers", "Pim Huijnen" ],
      "venue" : "In CIKM. ACM",
      "citeRegEx" : "Kenter et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kenter et al\\.",
      "year" : 2015
    }, {
      "title" : "Temporal analysis of language through neural language models",
      "author" : [ "Kim et al.2014] Yoon Kim", "Yi-I Chiu", "Kentaro Hanaki" ],
      "venue" : null,
      "citeRegEx" : "Kim et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2014
    }, {
      "title" : "Statistically significant detection of linguistic change",
      "author" : [ "Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena" ],
      "venue" : null,
      "citeRegEx" : "Kulkarni et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kulkarni et al\\.",
      "year" : 2015
    }, {
      "title" : "2015b. Freshman or fresher? quantifying the geographic variation of internet",
      "author" : [ "Bryan Perozzi", "Steven Skiena" ],
      "venue" : null,
      "citeRegEx" : "Kulkarni et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kulkarni et al\\.",
      "year" : 2015
    }, {
      "title" : "Literature survey: domain adaptation algorithms for natural language processing",
      "author" : [ "Qi Li" ],
      "venue" : null,
      "citeRegEx" : "Li.,? \\Q2012\\E",
      "shortCiteRegEx" : "Li.",
      "year" : 2012
    }, {
      "title" : "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781",
      "author" : [ "Kai Chen", "Greg Corrado", "Jeffrey Dean" ],
      "venue" : null,
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Crfsuite: a fast implementation of conditional random fields (crfs)",
      "author" : [ "Naoaki Okazaki" ],
      "venue" : null,
      "citeRegEx" : "Okazaki.,? \\Q2007\\E",
      "shortCiteRegEx" : "Okazaki.",
      "year" : 2007
    }, {
      "title" : "Domain adaptation of conditional probability models via feature subsetting",
      "author" : [ "Satpal", "Sunita Sarawagi" ],
      "venue" : "In European Conference on Principles of Data Mining and Knowledge Discovery,",
      "citeRegEx" : "Satpal et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Satpal et al\\.",
      "year" : 2007
    }, {
      "title" : "Flors: Fast and simple domain adaptation for part-of-speech tagging. Transactions of the Association for Computational Linguistics, 2:15–26",
      "author" : [ "Schnabel", "Schütze2014] Tobias Schnabel", "Hinrich Schütze" ],
      "venue" : null,
      "citeRegEx" : "Schnabel et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Schnabel et al\\.",
      "year" : 2014
    }, {
      "title" : "Unsupervised multi-domain adaptation with feature embeddings",
      "author" : [ "Yang", "Eisenstein2015] Yi Yang", "Jacob Eisenstein" ],
      "venue" : null,
      "citeRegEx" : "Yang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2015
    }, {
      "title" : "Online updating of word representations for part-of-speech tagging",
      "author" : [ "Yin et al.2016] Wenpeng Yin", "Tobias Schnabel", "Hinrich Schütze" ],
      "venue" : "arXiv preprint arXiv:1604.00502",
      "citeRegEx" : "Yin et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Yin et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "For example, most competitive Named Entity Recognition systems are trained on large amounts of labeled data from a given domain (like CoNLL or MUC) and evaluated on a held out test set drawn from the same domain (Florian et al., 2003; Chieu and Ng, 2002; Ando and Zhang, 2005; Collobert et al., 2011; Huang et al., 2015).",
      "startOffset" : 212,
      "endOffset" : 320
    }, {
      "referenceID" : 18,
      "context" : "For example, most competitive Named Entity Recognition systems are trained on large amounts of labeled data from a given domain (like CoNLL or MUC) and evaluated on a held out test set drawn from the same domain (Florian et al., 2003; Chieu and Ng, 2002; Ando and Zhang, 2005; Collobert et al., 2011; Huang et al., 2015).",
      "startOffset" : 212,
      "endOffset" : 320
    }, {
      "referenceID" : 7,
      "context" : "With recent advances in representation learning, word embeddings have been shown to be very useful features for several NLP tasks like POS Tagging, NER, and Sentiment Analysis (Chen et al., 2013; Al-Rfou et al., 2015; Collobert et al., 2011).",
      "startOffset" : 176,
      "endOffset" : 241
    }, {
      "referenceID" : 0,
      "context" : "With recent advances in representation learning, word embeddings have been shown to be very useful features for several NLP tasks like POS Tagging, NER, and Sentiment Analysis (Chen et al., 2013; Al-Rfou et al., 2015; Collobert et al., 2011).",
      "startOffset" : 176,
      "endOffset" : 241
    }, {
      "referenceID" : 9,
      "context" : "With recent advances in representation learning, word embeddings have been shown to be very useful features for several NLP tasks like POS Tagging, NER, and Sentiment Analysis (Chen et al., 2013; Al-Rfou et al., 2015; Collobert et al., 2011).",
      "startOffset" : 176,
      "endOffset" : 241
    }, {
      "referenceID" : 3,
      "context" : "We apply the method discussed in (Bamman et al., 2014; Kulkarni et al., 2015b) to learn domain specific word embeddings.",
      "startOffset" : 33,
      "endOffset" : 78
    }, {
      "referenceID" : 3,
      "context" : "1 we differentiate ourselves from (Bamman et al., 2014; Kulkarni et al., 2015b) by outlining a probabilistic method that uses this model to disambiguate the domain given a phrase that outlines the usage of a word w.",
      "startOffset" : 34,
      "endOffset" : 79
    }, {
      "referenceID" : 4,
      "context" : "To learn sense specific embeddings, we use the Adaptive Skip-gram model proposed by (Bartunov et al., 2015) to automatically infer (a) the different senses a word w exhibits (b) a probability distribution π(w) over the the different senses a word exhibits in the corpus and (c) an embedding for each sense of the word.",
      "startOffset" : 84,
      "endOffset" : 107
    }, {
      "referenceID" : 4,
      "context" : "Disambiguate each occurrence of w in di and dj using the method described by (Bartunov et al., 2015).",
      "startOffset" : 77,
      "endOffset" : 100
    }, {
      "referenceID" : 27,
      "context" : "To train, we use CRFsuite (Okazaki, 2007) with L-BFGS algorithm.",
      "startOffset" : 26,
      "endOffset" : 41
    }, {
      "referenceID" : 20,
      "context" : "There exist a number of methods for the task of supervised domain adaptation (Jiang, 2008).",
      "startOffset" : 77,
      "endOffset" : 90
    }, {
      "referenceID" : 31,
      "context" : "We consider a scalable version of FLORS (Yin et al., 2016).",
      "startOffset" : 40,
      "endOffset" : 58
    }, {
      "referenceID" : 26,
      "context" : "• FEMA: FEMA (Yang and Eisenstein, 2015) learns low dimensional embeddings of the features used in a CRF model by using a variant of the Skipgram Model (Mikolov et al., 2013).",
      "startOffset" : 152,
      "endOffset" : 174
    }, {
      "referenceID" : 11,
      "context" : "Socio-variational linguistics Several works study how language varies according to geography and time (Eisenstein et al., 2010; Eisenstein et al., 2011; Bamman and others, 2014; Bamman et al., 2014; Kim et al., 2014; Kulkarni et al., 2015a; Kenter et al., 2015; Gonçalves and Sánchez, 2014; Kulkarni et al., 2015b; Cook et al., 2014; Frermann and Lapata, 2016; Hamilton et al., 2016).",
      "startOffset" : 102,
      "endOffset" : 383
    }, {
      "referenceID" : 3,
      "context" : "Socio-variational linguistics Several works study how language varies according to geography and time (Eisenstein et al., 2010; Eisenstein et al., 2011; Bamman and others, 2014; Bamman et al., 2014; Kim et al., 2014; Kulkarni et al., 2015a; Kenter et al., 2015; Gonçalves and Sánchez, 2014; Kulkarni et al., 2015b; Cook et al., 2014; Frermann and Lapata, 2016; Hamilton et al., 2016).",
      "startOffset" : 102,
      "endOffset" : 383
    }, {
      "referenceID" : 22,
      "context" : "Socio-variational linguistics Several works study how language varies according to geography and time (Eisenstein et al., 2010; Eisenstein et al., 2011; Bamman and others, 2014; Bamman et al., 2014; Kim et al., 2014; Kulkarni et al., 2015a; Kenter et al., 2015; Gonçalves and Sánchez, 2014; Kulkarni et al., 2015b; Cook et al., 2014; Frermann and Lapata, 2016; Hamilton et al., 2016).",
      "startOffset" : 102,
      "endOffset" : 383
    }, {
      "referenceID" : 21,
      "context" : "Socio-variational linguistics Several works study how language varies according to geography and time (Eisenstein et al., 2010; Eisenstein et al., 2011; Bamman and others, 2014; Bamman et al., 2014; Kim et al., 2014; Kulkarni et al., 2015a; Kenter et al., 2015; Gonçalves and Sánchez, 2014; Kulkarni et al., 2015b; Cook et al., 2014; Frermann and Lapata, 2016; Hamilton et al., 2016).",
      "startOffset" : 102,
      "endOffset" : 383
    }, {
      "referenceID" : 10,
      "context" : "Socio-variational linguistics Several works study how language varies according to geography and time (Eisenstein et al., 2010; Eisenstein et al., 2011; Bamman and others, 2014; Bamman et al., 2014; Kim et al., 2014; Kulkarni et al., 2015a; Kenter et al., 2015; Gonçalves and Sánchez, 2014; Kulkarni et al., 2015b; Cook et al., 2014; Frermann and Lapata, 2016; Hamilton et al., 2016).",
      "startOffset" : 102,
      "endOffset" : 383
    }, {
      "referenceID" : 17,
      "context" : "Socio-variational linguistics Several works study how language varies according to geography and time (Eisenstein et al., 2010; Eisenstein et al., 2011; Bamman and others, 2014; Bamman et al., 2014; Kim et al., 2014; Kulkarni et al., 2015a; Kenter et al., 2015; Gonçalves and Sánchez, 2014; Kulkarni et al., 2015b; Cook et al., 2014; Frermann and Lapata, 2016; Hamilton et al., 2016).",
      "startOffset" : 102,
      "endOffset" : 383
    }, {
      "referenceID" : 3,
      "context" : "The methods outlined in (Kulkarni et al., 2015b; Bamman et al., 2014) are most closely related to our work.",
      "startOffset" : 24,
      "endOffset" : 69
    }, {
      "referenceID" : 3,
      "context" : "First, we show that the model proposed in (Bamman et al., 2014) is not only useful for learning domain specific word embeddings but the model itself can be utilized for the task of domain disambiguation by using an often neglected set of model parameters (the output vectors of the model).",
      "startOffset" : 42,
      "endOffset" : 63
    }, {
      "referenceID" : 3,
      "context" : "While the methods outlined in (Kulkarni et al., 2015b; Bamman et al., 2014) capture domain specific differences, they do not explicitly model the fact that words have multiple senses and their usage in a domain is a mixture of different proportions over these senses which can be explicitly quantified.",
      "startOffset" : 30,
      "endOffset" : 75
    }, {
      "referenceID" : 20,
      "context" : "Finally an excellent survey of various domain adaptation algorithms for NLP is provided by (Jiang, 2008; Li, 2012).",
      "startOffset" : 91,
      "endOffset" : 114
    }, {
      "referenceID" : 25,
      "context" : "Finally an excellent survey of various domain adaptation algorithms for NLP is provided by (Jiang, 2008; Li, 2012).",
      "startOffset" : 91,
      "endOffset" : 114
    } ],
    "year" : 2016,
    "abstractText" : "Content on the Internet is heterogeneous and arises from various domains like News, Entertainment, Finance and Technology. Understanding such content requires identifying named entities (persons, places and organizations) as one of the key steps. Traditionally Named Entity Recognition (NER) systems have been built using available annotated datasets (like CoNLL, MUC) and demonstrate excellent performance. However, these models fail to generalize onto other domains like Sports and Finance where conventions and language use can differ significantly. Furthermore, several domains do not have large amounts of annotated labeled data for training robust Named Entity Recognition models. A key step towards this challenge is to adapt models learned on domains where large amounts of annotated training data are available to domains with scarce annotated data. In this paper, we propose methods to effectively adapt models learned on one domain onto other domains using distributed word representations. First we analyze the linguistic variation present across domains to identify key linguistic insights that can boost performance across domains. We propose methods to capture domain specific semantics of word usage in addition to global semantics. We then demonstrate how to effectively use such domain specific knowledge to learn NER models that outperform previous baselines in the domain adaptation setting. ∗This work was done when the author was a research intern at Yahoo. ∗© 2016 This is the authors draft of the work. It is posted here for your personal use. Not for redistribution.",
    "creator" : "LaTeX with hyperref package"
  }
}