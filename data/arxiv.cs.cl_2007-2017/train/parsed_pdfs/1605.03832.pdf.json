{
  "name" : "1605.03832.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Polyglot Neural Language Models: A Case Study in Cross-Lingual Phonetic Representation Learning",
    "authors" : [ "Yulia Tsvetkov", "Sunayana Sitaram", "Manaal Faruqui", "Guillaume Lample", "Patrick Littell", "David Mortensen", "Alan W Black", "Lori Levin", "Chris Dyer" ],
    "emails" : [ "ytsvetko@cs.cmu.edu", "ssitaram@cs.cmu.edu", "mfaruqui@cs.cmu.edu", "glample@cs.cmu.edu", "plittell@cs.cmu.edu", "dmortens@cs.cmu.edu", "awb@cs.cmu.edu", "lsl@cs.cmu.edu", "cdyer@cs.cmu.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Nearly all existing language model (LM) architectures are designed to model one language at a time. This is unsurprising considering the historical importance of count-based models in which every surface form of a word is a separately modeled entity (English cat and Spanish gato would not likely benefit from sharing counts). However, recent models that use distributed representations—in particular models that share representations across languages (Hermann and Blunsom, 2014; Faruqui and Dyer, 2014; Huang et al., 2015; Lu et al., 2015, inter alia)—suggest universal models applicable to multiple languages are a possibility. This paper takes a\nstep in this direction.\nWe introduce polyglot language models: neural network language models that are trained on and applied to any number of languages. Our goals with these models are the following. First, to facilitate data and parameter sharing, providing more training resources to languages, which is especially valuable in low-resource settings. Second, models trained on diverse languages with diverse linguistic properties will better be able to learn naturalistic representations that are less likely to “overfit” to a single linguistic outlier. Finally, polyglot models offer convenience in a multilingual world: a single model replaces dozens of different models.\nExploration of polyglot language models at the sentence level—the traditional domain of language modeling—requires dealing with a massive event space (i.e., the union of words across many languages). To work in a more tractable domain, we evaluate our model on phone-based language modeling, the modeling sequences of sounds, rather than words. We choose this domain since a common assumption of many theories of phonology is that all spoken languages construct words from a finite inventory of phonetic symbols (represented conveniently as the elements of the the International Phonetic Alphabet; IPA) which are distinguished by language-universal features (e.g., place and manner of articulation, voicing status, etc.). Although our focus is on sound sequences, our solution can be ported to the semantic/syntactic problem as resulting from adaptation to constraints on semantic/syntactic structure.\nThis paper makes two primary contributions: in\nar X\niv :1\n60 5.\n03 83\n2v 1\n[ cs\n.C L\n] 1\n2 M\nay 2\nmodeling and in applications. In §2, we introduce a novel polyglot neural language model (NLM) architecture. Despite being trained on multiple languages, the multilingual model is more effective (9.5% lower perplexity) than individual models, and substantially more effective than naive baselines (over 25% lower perplexity). Our most effective polyglot architecture conditions not only on the identity of the language being predicted in each sequence, but also on a vector representation of its phono-typological properties. In addition to learning representations of phones as part of the polyglot language modeling objective, the model incorporates features about linguistic typology to improve generalization performance (§3). Our second primary contribution is to show that downstream applications are improved by using polyglotlearned phone representations. We focus on two tasks: predicting adapted word forms in models of cross-lingual lexical borrowing and speech synthesis (§4). Our experimental results (§5) show that in borrowing, we improve over the current stateof-the-art, and in speech synthesis, our features are more effective than manually-designed phonetic features. Finally, we analyze the phonological content of learned representations, finding that our polyglot models discover standard phonological categories such as length and nasalization, and that these are grouped correctly across languages with different phonetic inventories and contrastive features."
    }, {
      "heading" : "2 Model",
      "text" : "In this section, we first describe in §2.1 the underlying framework of our model—RNNLM—a standard recurrent neural network based language model (Mikolov et al., 2010; Sundermeyer et al., 2012). Then, in §2.2, we define a Polyglot LM—a modification of RNNLM to incorporate language information, both learned and hand-crafted.\nProblem definition. In the phonological LM, phones (sounds) are the basic units. Mapping from words to phones is defined in pronunciation dictionaries. For example, “cats” [kæts] is a sequence of four phones. Given a prefix of phones φ1, φ2, . . . , φt−1, the task of the LM is to estimate the conditional probability of the next phone p(φt | φ1, φ2, . . . , φt−1)."
    }, {
      "heading" : "2.1 RNNLM",
      "text" : "In NLMs, a vocabulary V (here, a set of phones composing all word types in the language) is represented as a matrix of parameters X ∈ Rd×|V |, with |V | phone types represented as d-dimensional vectors. X is often denoted as lookup table. Phones in the input sequence are first converted to phone vectors, where φi is represented by xi by multiplying the phone indicator (one-hot vector of length |V |) and the lookup table.\nAt each time step t, most recent phone prefix vector1 xt and hidden state ht−1 are transformed to compute a new hidden representation:\nht = f(xt,ht−1),\nwhere f is a non-linear transformation. In the original RNNLMs (Mikolov et al., 2010), the transformation is such that:\nht = tanh(Whxxt + Whhht−1 + bh).\nTo overcome the notorious problem in recurrent neural networks of vanishing gradients (Bengio et al., 1994), following Sundermeyer et al. (2012), in recurrent layer we use long short-term memory (LSTM) units (Hochreiter and Schmidhuber, 1997):2\nht = LSTM(xt,ht−1).\nGiven the hidden sequence ht, the output sequence is then computed as follows:\np(φt = i | φ1, . . . , φt−1) = softmax(Woutht + bout)i,\nwhere softmax(xi) = e xi∑ j e xj ensures a valid probability distribution over output phones.\n1 We are reading at each time step the most recent n-gram context rather than—as is more common in RNNLMs—a single phone context. Empirically, this works better for phone sequences, and we hypothesize that this lets the learner rely on direct connections for local phenomena (which are abundant in phonology) and minimally use the recurrent state to model longer-range effects.\n2For brevity, we omit the equations describing the LSTM cells; they can be found in (Graves, 2013, eq. 7–11)."
    }, {
      "heading" : "2.2 Polyglot LM",
      "text" : "We now describe our modifications to RNNLM to account for multilinguality. The architecture is depicted in figure 1. Our task is to estimate the conditional probability of the next phone given the preceding phones and the language (`): p(φt | φ1, . . . , φt−1, `).\nIn a multilingual NLM, we define a vocabulary V ∗ to be the union of vocabularies of all training languages, assuming that all language vocabularies are mapped to a shared representation (here, IPA). In addition, we maintain V` with a special symbol for each language (e.g., φenglish, φarabic). Language symbol vectors are parameters in the new lookup table X` ∈ Rd×|#langs| (e.g., xenglish, xarabic). The inputs to the Polyglot LM are the phone vectors xt, the language character vector x`, and the typological feature vector constructed externally t`. The typological feature vector will be discussed in the following section.\nThe input layer is passed to the hidden localcontext layer:\nct = Wcxxt + Wclangxlang + bc.\nThe local-context vector is then passed to the hidden LSTM global-context layer, similarly to the previously described RNNLM:\ngt = LSTM(ct,gt−1).\nIn the next step, the global-context vector gt is “factored” by the typology of the training language, to integrate manually-defined language features. To obtain this, we first project the (potentially highdimensional) t` into a low-dimensional vector, and apply non-linearity. Then, we multiply the gt and the projected language layer, to obtain a globalcontext-language matrix:\nf` = tanh(W`t` + b`),\nG`t = gt ⊗ f>` .\nFinally, we vectorize the resulting matrix into a column vector and compute the output sequence as follows:\np(φt = i | φ1, . . . , φt−1, `) = softmax(Woutvec(G ` t) + bout)i.\nModel training. Parameters of the models are the lookup tables X and X`, weight matrices Wi, and bias vectors bi. Parameter optimization is performed using stochastic updates to minimize the categorical cross-entropy loss (which is equivalent to minimizing perplexity and maximizing likelihood): H(φ, φ̂) = −Σiφ̂i log φi, where φ is predicted and φ̂ is the gold label."
    }, {
      "heading" : "3 Typological features",
      "text" : "Typological information is fed to the model via vectors of 190 binary typological features, all of which are phonological (related to sound structure) in their nature. These feature vectors are derived from data from the WALS (Dryer and Haspelmath, 2013), PHOIBLE (Moran et al., 2014), and Ethnologue (Lewis et al., 2015) typological databases via extensive post-processing and analysis.3 The features primarily concern properties of sound inventories (i.e., the set of phones or phonemes occurring in a language) and are mostly of one of four types:\n1. Single segment represented in an inventory; 3This data resource, which provides standardized phono-typological information for 2,273 languages, is available at https://github.com/dmort27/ uriel-phonology/tarball/0.1. It is a subset of the URIEL database, a comprehensive database of typological features encoding syntactic and morphological (as well as phonological) properties of languages. It is available at http://cs.cmu.edu/~dmortens/uriel.html.\ne.g., does language `’s sound inventory include /g/, a voiced velar stop? 2. Class of segments represented in an inventory; e.g., does language `’s sound inventory include voiced fricatives like /z/ and /v/? 3. Minimal contrast represented in an inventory; e.g., does language `’s sound inventory include two sounds that differ only in voicing, such as /t/ and /d/? 4. Number of sounds representative of a class that are present in an inventory; e.g., does language `’s sound inventory include exactly five vowels?\nThe motivation and criteria for coding each individual feature required extensive linguistic knowledge and analysis. Consider the case of tense vowels like /i/ and /u/ in “beet” and “boot” in contrast with lax vowels like /I/ and /U/ in “bit” and “book.” Only through linguistic analysis does it become evident that (1) all languages have tense vowels—a feature based on the presence of tense vowels is uninformative and that (2) a significant minority of languages make a distinction between tense and lax vowels—a feature based on whether languages display a minimal difference of this kind would be more useful."
    }, {
      "heading" : "4 Applications of Phonetic Vectors",
      "text" : "Learned continuous word representations—word vectors—are an important by-product of neural LMs, and these are used as features in numerous NLP applications, including chunking (Turian et al., 2010), part-of-speech tagging (Ling et al., 2015), dependency parsing (Lazaridou et al., 2013; Bansal et al., 2014; Dyer et al., 2015; Watanabe and Sumita, 2015), named entity recognition (Guo et al., 2014), and sentiment analysis (Socher et al., 2013; Wang et al., 2015). We evaluate phone vectors learned by Polyglot LMs in two downstream applications that rely on phonology: modeling lexical borrowing (§4.1) and speech synthesis (§4.2)."
    }, {
      "heading" : "4.1 Lexical borrowing",
      "text" : "Lexical borrowing is the adoption of words from another language, that inevitably happens when speakers of different languages communicate for a long period of time (Thomason and Kaufman, 2001). Borrowed words—also called loan-\nwords—constitute 10–70% of most language lexicons (Haspelmath, 2009); these are content words of foreign origin that are adapted in the language and are not perceived as foreign by language speakers. Computational modeling of cross-lingual transformations of loanwords is effective for inferring lexical correspondences across languages with limited parallel data, benefiting applications such as machine translation (Tsvetkov and Dyer, 2015; Tsvetkov and Dyer, 2016).\nIn the process of their nativization in a foreign language, loanwords undergo primarily phonological adaptation, namely insertion/deletion/substitution of phones to adapt to the phonotactic constraints of the recipient language. If a foreign phone is not present in the recipient language, it is usually replaced with its closest native equivalent—we thus hypothesize that cross-lingual phonological features learned by the Polyglot LM can be useful in models of borrowing to quantify cross-lingual similarities of sounds.\nTo test this hypothesis, we augment the handengineered models proposed by Tsvetkov and Dyer (2016) with features from phone vectors learned by our model. Inputs to the borrowing framework are loanwords (in Swahili, Romanian, Maltese), and outputs are their corresponding “donor” words in the donor language (Arabic, French, Italian, resp.). The framework is implemented as a cascade of finitestate transducers with insertion/deletion/substitution operations on sounds, weighted by high-level conceptual linguistic constraints that are learned in a supervised manner. Given a loanword, the system produces a candidate donor word with lower ranked violations than other candidates, using the shortest path algorithm. In the original borrowing model, insertion/deletion/substitution operations are unweighted. In this work, we integrate transition weights in the phone substitution transducers, which are cosine distances between phone vectors learned by our model. Our intuition is that similar sounds appear in similar contexts, even if they are not present in the same language (e.g., /sQ/ in Arabic is adapted to /s/ in Swahili). Thus, if our model effectively captures cross-lingual signals, similar sounds should have smaller distances in the vector space, which can improve the shortest path results. Figure 2 illustrates our modifications to the\noriginal framework."
    }, {
      "heading" : "4.2 Speech synthesis",
      "text" : "Speech synthesis is the process of converting text into speech. It has various applications, such as screen readers for the visually impaired and handsfree voice based systems. Text-to-speech (TTS) systems are also used as part of speech-to-speech translation systems and spoken dialog systems, such as personal digital assistants. Natural and intelligible TTS systems exist for a number of languages in the world today. However, building TTS systems remains prohibitive for many languages due to the lack of linguistic resources and data.\nThe language-specific resources that are traditionally used for building TTS systems in a new language are: (1) audio recordings with transcripts; (2) pronunciation lexicon or letter to sound rules; and (3) a phone set definition. Standard TTS systems today use phone sets designed by experts. Typically, these phone sets also contain phonetic features for each phoneme, which are used as features in models of the spectrum and prosody. The phonetic features available in standard TTS systems are multidimensional vectors indicating various properties of each phoneme, such as whether it is a vowel or consonant, vowel length and height, place of articulation of a consonant, etc. Constructing these features by hand can be labor intensive, and coming up with such features automatically may be useful in low-resource scenarios.\nIn this work, we replace manually engineered phonetic features with phone vectors, which are then\nused by classification and regression trees for modeling the spectrum. Each phoneme in our phone set is assigned an automatically constructed phone vector, and each member of the phone vector is treated as a phoneme-level feature which is used in place of the manually engineered phonetic features. While prior work has explored TTS augmented with acoustic features (Watts et al., 2015), to the best of our knowledge, we are the first to replace manually engineered phonetic features in TTS systems with automatically constructed phone vectors."
    }, {
      "heading" : "5 Experiments",
      "text" : "Our experimental evaluation of our proposed polyglot models consists of two parts: (i) an intrinsic evaluation where phone sequences are modeled with independent models and (ii) an extrinsic evaluation of the learned phonetic representations. Before discussing these results, we provide details of the data resources we used."
    }, {
      "heading" : "5.1 Resources and experimental setup",
      "text" : "Resources. We experiment with the following languages: Arabic (AR), French (FR), Hindi (HI), Italian (IT), Maltese (MT), Romanian (RO), Swahili (SW), Tamil (TA), and Telugu (TE). In our language modeling experiments, two main sources of data are pronunciation dictionaries and typological features described in §3. The dictionaries for AR, FR, HI, TA, and TE are taken from in-house speech recognition/synthesis systems. For remaining languages, the dictionaries are automatically constructed using the Omniglot grapheme-to-IPA conversion rules.4\nWe use two types of pronunciation dictionaries: (1) AR, FR, HI, IT, MT, RO, and SW dictionaries used in experiments with lexical borrowing; and (2) EN, HI, TA, and TE dictionaries used in experiments with speech synthesis. The former are mapped to IPA, with the resulting phone vocabulary size—the number of distinct phones across IPA dictionaries—of 127 phones. The latter are encoded using the UniTran universal transliteration resource (Qian et al., 2010), with a vocabulary of 79 phone types.\nFrom the (word-type) pronunciation dictionaries, we remove 15% of the words for development, and a further 10% for testing; the rest of the data is\n4http://omniglot.com/writing/\nused to train the models. In tables 1 and 2 we list—for both types of pronunciation dictionaries— train/dev/test data statistics for words (phone sequences) and phone tokens. We concatenate each phone sequence with beginning and end symbols (<s>, </s>).\nHyperparameters. We used the following network architecture: 100-dimensional phone vectors, with hidden local-context and LSTM layers of size 100, and hidden language layer of size 20. All language models were trained using the left context of 3 phones (4-gram LMs). Across all language modeling experiments, parameter optimization was performed on the dev set using the Adam algorithm (Kingma and Ba, 2014) with mini-batches of size 100 to train the models for 5 epochs."
    }, {
      "heading" : "5.2 Intrinsic perplexity evaluation",
      "text" : "Perplexity is the standard evaluation measure for language models, which has been shown to correlate strongly with error rates in downstream applications (Klakow and Peters, 2002). We evaluated perplexities across several architectures, and several monolingual and multilingual setups. We kept the same hyper-parameters across all setups, as detailed in §5. Perplexities of LMs trained on the two types of pronunciation dictionaries were evaluated separately; table 3 summarizes perplexities of the models trained on IPA dictionaries, and table 4 summarizes perplexities of the UniTran LMs.\nIn columns, we compare three model architectures: baseline denotes the standard RNNLM archi-\ntecture described in §2.1; +lang denotes the Polyglot LM architecture described in §2.2 with input language vector but without typological features and language layer; finally, +typology denotes the full Polyglot LM architecture. This setup lets us separately evaluate the contribution of modified architecture and the contribution of auxiliary set of features introduced via the language layer.\nTest languages are IT in table 3, and HI in table 4. The rows correspond to different sets of training languages for the models: monolingual is for training and testing on the same language; +similar denotes training on three typologically similar languages: IT, FR, RO in table 3, and HI, TA, TE in table 4; +dissimilar denotes training on four languages, three similar and one typologically dissimilar language, to evaluate robustness of multilingual systems to diverse types of data. The final sets of training languages are IT, FR, RO, HI in table 3, and HI, TA, TE, EN in table 4.\nWe see several patterns of results. First, polyglot models require, unsurprisingly, information about\nwhat language they are predicting to obtain good modeling performance. Second, typological information is more valuable than letting the model learn representations of the language along with the characters. Finally, typology-augmented polyglot models outperform their monolingual baseline, providing evidence in support of the hypothesis that cross-lingual evidence is useful not only for learning cross-lingual representations and models, but monolingual ones as well."
    }, {
      "heading" : "5.3 Lexical borrowing experiments",
      "text" : "We fully reproduced lexical borrowing models described in (Tsvetkov and Dyer, 2016) for three language pairs: AR–SW, FR–RO, and IT–MT. Train and test corpora are donor–loanword pairs in the language pairs. Corpora statistics are given in table 5 (note that these are extremely small data sets; thus small numbers of highly informative features a necessary for good generalization). We use the reproduced systems as the baselines, and compare these to the corresponding systems augmented with phone vectors, as described in §4.1.\nIntegrated vectors were obtained from a single polyglot model with typology, trained on all languages with IPA dictionaries. For comparison with the results in table 3, perplexity of the model on the IT dataset (used for evaluation is §5.2) is 4.16, even lower than in the model trained on four languages. To retrain the high-level conceptual linguistic features learned by the borrowing models, we initialized the augmented systems with feature weights learned by the baselines, and retrained. Final weights were established using cross-validation. Then, we evaluated the accuracy of the augmented borrowing systems on the held-out test data.\nAccuracies are shown in table 6. We observe improvements of up to 5% in accuracies of FR–RO and IT–MT pairs. Effectiveness of the same polyglot model trained on multiple languages and integrated in different downstream systems supports our as-\nsumption that the model remains stable and effective with addition of languages. Our model is less effective for the AR–SW language pair. We speculate that the results are worse, because this is a pair of (typologically) more distant languages; consequently, the phonological adaptation processes that happen in loanword assimilation are more complex than mere substitutions of similar phones that we are targeting via the integration of phone vectors."
    }, {
      "heading" : "5.4 Speech synthesis experiments",
      "text" : "A popular objective metric for measuring the quality of synthetic speech is the Mel Cepstral Distortion (MCD) (Hu and Loizou, 2008). The MCD metric calculates an L2 norm of the Mel Frequency Cepstral Coefficients (MFCCs) of natural speech from a held out test set, and synthetic speech generated from the same test set. Since this is a distance metric, a lower value of MCD suggests better synthesis. The MCD is a database-specific metric, but experiments by Kominek et al. (Kominek et al., 2008) have shown that a decrease in MCD of 0.08 is perceptually significant, and a decrease of 0.12 is equivalent to doubling the size of the TTS database. In our experiments, we use MCD to measure the relative improvement obtained by our techniques.\nWe conducted experiments on the IIIT-H Hindi voice database (Prahallad et al., 2012), a 2 hour single speaker database recorded by a professional male speaker. We used the same front end (UniTran) to build all the Hindi TTS systems, with the only difference between the systems being the presence or absence of phonetic features and our vectors. For all our voice-based experiments, we built CLUSTERGEN Statistical Parametric Synthesis voices (Black, 2006) using the Festvox voice building tools (Black and Lenzo, 2003) and the Festival speech synthesis engine (Black and Taylor, 1997).\nThe baseline TTS system was built using no phonetic features. We also built a TTS system with standard hand-crafted phonetic features. Table 7 shows the MCD for the HI baseline, the standard TTS with hand-crafted features, and augmented TTS systems built using monolingual and multilingual phone vectors constructed with Polyglot LMs.\nOur multilingual vectors outperform the baseline, with a significant decrease of 0.19 in MCD. Crucially, TTS systems augmented with the Polyglot LM phone vectors outperform also the standard TTS with hand-crafted features. We found that using both feature sets added no value, suggesting that learned phone vectors are capturing information that is equivalent to the hand-engineered vectors."
    }, {
      "heading" : "5.5 Qualitative analysis of vectors",
      "text" : "Phone vectors learned by Polyglot LMs are mere sequences of real numbers. An interesting question is whether these vectors capture linguistic (phonological) qualities of phones they are encoding. To analyze to what extent our vectors capture linguistic properties of phones, we use the QVEC—a tool to quantify and interpret linguistic content of vector space models (Tsvetkov et al., 2015). The tool aligns dimensions in a matrix of learned distributed representations with dimensions of a hand-crafted linguistic matrix. Alignments are induced via correlating columns in the distributed and the linguistic matrices. To analyze the content of the distributed matrix, annotations from the linguistic matrix are projected via the maximally-correlated alignments.\nWe constructed a phonological matrix in which 5,059 rows are IPA phones and 21 columns are boolean indicators of universal phonological properties, e.g. consonant, voiced, labial.5 We the projected annotations from the linguistic matrix and\n5This matrix is described in Littell et al. (2016) and is available at https://github.com/dmort27/panphon/.\nmanually examined aligned dimensions in the phone vectors from §5.3 (trained on six languages). In the maximally-correlated columns—corresponding to linguistic features long, consonant, nasalized—we examined phones with highest coefficients. These were: [5:, U:, i:, O:, E:] for long; [v, ñ, > dZ, d, f, j, > ts, N] for consonant; and [Õ, Ẽ, Ã, œ̃] for nasalized. Clearly, the learned representation discover standard phonological features. Moreover, these top-ranked sounds are not grouped by a single language, e.g., / > dZ/ is present in Arabic but not in French, and /ñ, N/ are present in French but not in Arabic. From this analysis, we conclude that (1) the model discovers linguistically meaningful phonetic features; (2) the model induces meaningful related groupings across languages."
    }, {
      "heading" : "6 Related Work",
      "text" : "Multilingual language models. Interpolation of monolingual LMs is an alternative to obtain a multilingual model (Harbeck et al., 1997; Weng et al., 1997). However, interpolated models still require a trained model per language, and do not allow parameter sharing at training time. Bilingual language models trained on concatenated corpora were explored mainly in speech recognition (Ward et al., 1998; Wang et al., 2002; Fügen et al., 2003). Adaptations have been proposed to apply language models in bilingual settings in machine translation (Niehues et al., 2011) and code switching (Adel et al., 2013). These approaches, however, require adaptation to every pair of languages, and an adapted model cannot be applied to more than two languages.\nIndependently, Ammar et al. (2016) used a different polyglot architecture for multilingual dependency parsing. This work has also confirmed the utility of polyglot architectures in leveraging multilinguality.\nMultimodal neural language models. Multimodal language modeling is integrating image/video modalities in text LMs. Our work is inspired by the neural multimodal LMs (Kiros and Salakhutdinov, 2013; Kiros et al., 2015), which defined language models conditional on visual contexts, although we use a different language model architecture (recurrent vs. log-bilinear) and a different approach to gat-\ning modality."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We presented a novel multilingual language model architecture. The model obtains substantial gains in perplexity, and improves downstream text and speech applications. Although we focus on phonology, our approach is general, and can be applied in problems that integrate divergent modalities, e.g., topic modeling, and multilingual tagging and parsing."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was supported by the National Science Foundation through award IIS-1526745 and in part by the Defense Advanced Research Projects Agency (DARPA) Information Innovation Office (I2O). Program: Low Resource Languages for Emergent Incidents (LORELEI). Issued by DARPA/I2O under Contract No. HR0011-15-C-0114."
    } ],
    "references" : [ {
      "title" : "Combination of recurrent neural networks and factored language models for codeswitching language modeling",
      "author" : [ "Adel et al.2013] Heike Adel", "Ngoc Thang Vu", "Tanja Schultz" ],
      "venue" : "In Proc. ACL,",
      "citeRegEx" : "Adel et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Adel et al\\.",
      "year" : 2013
    }, {
      "title" : "Tailoring continuous word representations for dependency parsing",
      "author" : [ "Bansal et al.2014] Mohit Bansal", "Kevin Gimpel", "Karen Livescu" ],
      "venue" : "In Proc. ACL",
      "citeRegEx" : "Bansal et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bansal et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning long-term dependencies with gradient descent is difficult",
      "author" : [ "Bengio et al.1994] Yoshua Bengio", "Patrice Simard", "Paolo Frasconi" ],
      "venue" : "IEEE Transactions on Neural Networks,",
      "citeRegEx" : "Bengio et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 1994
    }, {
      "title" : "Building synthetic voices. http: //festvox.org/bsv",
      "author" : [ "Black", "Lenzo2003] Alan W Black", "Kevin A Lenzo" ],
      "venue" : null,
      "citeRegEx" : "Black et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Black et al\\.",
      "year" : 2003
    }, {
      "title" : "The Festival speech synthesis system: system documentation",
      "author" : [ "Black", "Taylor1997] Alan W Black", "Paul Taylor" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Black et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Black et al\\.",
      "year" : 1997
    }, {
      "title" : "CLUSTERGEN: a statistical parametric synthesizer using trajectory modeling",
      "author" : [ "Alan W Black" ],
      "venue" : "In Proc. Interspeech",
      "citeRegEx" : "Black.,? \\Q2006\\E",
      "shortCiteRegEx" : "Black.",
      "year" : 2006
    }, {
      "title" : "Transition-based dependency parsing with stack long short-term memory",
      "author" : [ "Dyer et al.2015] Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A Smith" ],
      "venue" : "In Proc. ACL",
      "citeRegEx" : "Dyer et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Dyer et al\\.",
      "year" : 2015
    }, {
      "title" : "Improving vector space word representations using multilingual correlation",
      "author" : [ "Faruqui", "Dyer2014] Manaal Faruqui", "Chris Dyer" ],
      "venue" : "In Proc. EACL",
      "citeRegEx" : "Faruqui et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Faruqui et al\\.",
      "year" : 2014
    }, {
      "title" : "Efficient handling of multilingual language models",
      "author" : [ "Sebastian Stuker", "Hagen Soltau", "Florian Metze", "Tanja Schultz" ],
      "venue" : "In Proc. ASRU,",
      "citeRegEx" : "Fügen et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Fügen et al\\.",
      "year" : 2003
    }, {
      "title" : "Generating sequences with recurrent neural networks. CoRR, abs/1308.0850",
      "author" : [ "Alex Graves" ],
      "venue" : null,
      "citeRegEx" : "Graves.,? \\Q2013\\E",
      "shortCiteRegEx" : "Graves.",
      "year" : 2013
    }, {
      "title" : "Revisiting embedding features for simple semi-supervised learning",
      "author" : [ "Jiang Guo", "Wanxiang Che", "Haifeng Wang", "Ting Liu" ],
      "venue" : "In Proc. EMNLP",
      "citeRegEx" : "Guo et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2014
    }, {
      "title" : "Multilingual speech recognition",
      "author" : [ "Elmar Nöth", "Heinrich Niemann" ],
      "venue" : "In Proc. 2nd SQEL Workshop on Multi-Lingual Information Retrieval Dialogs,",
      "citeRegEx" : "Harbeck et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Harbeck et al\\.",
      "year" : 1997
    }, {
      "title" : "Multilingual Models for Compositional Distributional Semantics",
      "author" : [ "Hermann", "Blunsom2014] Karl Moritz Hermann", "Phil Blunsom" ],
      "venue" : "In Proc. ACL",
      "citeRegEx" : "Hermann et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hermann et al\\.",
      "year" : 2014
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Hochreiter et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter et al\\.",
      "year" : 1997
    }, {
      "title" : "Evaluation of objective quality measures for speech enhancement",
      "author" : [ "Hu", "Loizou2008] Yi Hu", "Philipos C Loizou" ],
      "venue" : "Audio, Speech, & Language Processing,",
      "citeRegEx" : "Hu et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2008
    }, {
      "title" : "Translation invariant word embeddings",
      "author" : [ "Huang et al.2015] Kejun Huang", "Matt Gardner", "Evangelos Papalexakis", "Christos Faloutsos", "Nikos Sidiropoulos", "Tom Mitchell", "Partha P. Talukdar", "Xiao Fu" ],
      "venue" : "In Proc. EMNLP,",
      "citeRegEx" : "Huang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2015
    }, {
      "title" : "Adam: A method for stochastic optimization. CoRR, abs/1412.6980",
      "author" : [ "Kingma", "Ba2014] Diederik Kingma", "Jimmy Ba" ],
      "venue" : null,
      "citeRegEx" : "Kingma et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2014
    }, {
      "title" : "Multimodal neural language models",
      "author" : [ "Kiros", "Salakhutdinov2013] Ryan Kiros", "Ruslan Salakhutdinov" ],
      "venue" : "In Proc. NIPS Deep Learning Workshop",
      "citeRegEx" : "Kiros et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Kiros et al\\.",
      "year" : 2013
    }, {
      "title" : "Unifying visual-semantic embeddings with multimodal neural language models. TACL",
      "author" : [ "Kiros et al.2015] Ryan Kiros", "Ruslan Salakhutdinov", "Richard Zemel" ],
      "venue" : null,
      "citeRegEx" : "Kiros et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kiros et al\\.",
      "year" : 2015
    }, {
      "title" : "Testing the correlation of word error rate and perplexity",
      "author" : [ "Klakow", "Peters2002] Dietrich Klakow", "Jochen Peters" ],
      "venue" : "Speech Communication,",
      "citeRegEx" : "Klakow et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Klakow et al\\.",
      "year" : 2002
    }, {
      "title" : "Synthesizer voice quality of new languages calibrated with mean Mel Cepstral Distortion",
      "author" : [ "Kominek et al.2008] John Kominek", "Tanja Schultz", "Alan W Black" ],
      "venue" : "In Proc. SLTU,",
      "citeRegEx" : "Kominek et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Kominek et al\\.",
      "year" : 2008
    }, {
      "title" : "Fish transporters and miracle homes: How compositional distributional semantics can help NP parsing",
      "author" : [ "Eva Maria Vecchi", "Marco Baroni" ],
      "venue" : "In Proc. EMNLP",
      "citeRegEx" : "Lazaridou et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Lazaridou et al\\.",
      "year" : 2013
    }, {
      "title" : "Ethnologue: Languages of the world. Texas: SIL International",
      "author" : [ "Lewis et al.2015] M Paul Lewis", "Gary F Simons", "Charles D Fennig" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2015
    }, {
      "title" : "Finding function in form: Compositional character models for open vocabulary word representation",
      "author" : [ "Ling et al.2015] Wang Ling", "Tiago Luís", "Luís Marujo", "Ramón Fernandez Astudillo", "Silvio Amir", "Chris Dyer", "Alan W Black", "Isabel Trancoso" ],
      "venue" : null,
      "citeRegEx" : "Ling et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ling et al\\.",
      "year" : 2015
    }, {
      "title" : "Bridgelanguage capitalization inference in Western Iranian: Sorani, Kurmanji, Zazaki, and Tajik",
      "author" : [ "David Mortensen", "Kartik Goyal", "Chris Dyer", "Lori Levin" ],
      "venue" : "In Proceedings of the Eleventh International Conference on Language",
      "citeRegEx" : "Littell et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Littell et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep multilingual correlation for improved word embeddings",
      "author" : [ "Lu et al.2015] Ang Lu", "Weiran Wang", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu" ],
      "venue" : "In Proc. NAACL",
      "citeRegEx" : "Lu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2015
    }, {
      "title" : "Recurrent neural network based language model",
      "author" : [ "Martin Karafiát", "Lukas Burget", "Jan Cernockỳ", "Sanjeev Khudanpur" ],
      "venue" : "In Proc. Interspeech,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2010
    }, {
      "title" : "Wider context by using bilingual language models in machine translation",
      "author" : [ "Niehues et al.2011] Jan Niehues", "Teresa Herrmann", "Stephan Vogel", "Alex Waibel" ],
      "venue" : "In Proc. WMT,",
      "citeRegEx" : "Niehues et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Niehues et al\\.",
      "year" : 2011
    }, {
      "title" : "The IIIT-H Indic speech databases",
      "author" : [ "E. Naresh Kumar", "Venkatesh Keri", "S. Rajendran", "Alan W Black" ],
      "venue" : "In Proc. Interspeech",
      "citeRegEx" : "Prahallad et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Prahallad et al\\.",
      "year" : 2012
    }, {
      "title" : "A Python toolkit for universal transliteration",
      "author" : [ "Malta LREC." ],
      "venue" : "Proc. LREC.",
      "citeRegEx" : "LREC.,? 2010",
      "shortCiteRegEx" : "LREC.",
      "year" : 2010
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Y. Ng", "Christopher Potts" ],
      "venue" : "In Proc. EMNLP",
      "citeRegEx" : "Socher et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "LSTM neural networks for language modeling",
      "author" : [ "Ralf Schlüter", "Hermann Ney" ],
      "venue" : "In Proc. Interspeech",
      "citeRegEx" : "Sundermeyer et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Sundermeyer et al\\.",
      "year" : 2012
    }, {
      "title" : "Language contact",
      "author" : [ "Thomason", "Kaufman2001] Sarah Grey Thomason", "Terrence Kaufman" ],
      "venue" : null,
      "citeRegEx" : "Thomason et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Thomason et al\\.",
      "year" : 2001
    }, {
      "title" : "Lexicon stratification for translating out-ofvocabulary words",
      "author" : [ "Tsvetkov", "Dyer2015] Yulia Tsvetkov", "Chris Dyer" ],
      "venue" : "In Proc. ACL,",
      "citeRegEx" : "Tsvetkov et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Tsvetkov et al\\.",
      "year" : 2015
    }, {
      "title" : "Cross-lingual bridges with models of lexical borrowing",
      "author" : [ "Tsvetkov", "Dyer2016] Yulia Tsvetkov", "Chris Dyer" ],
      "venue" : null,
      "citeRegEx" : "Tsvetkov et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Tsvetkov et al\\.",
      "year" : 2016
    }, {
      "title" : "Evaluation of word vector representations by subspace alignment",
      "author" : [ "Manaal Faruqui", "Wang Ling", "Guillaume Lample", "Chris Dyer" ],
      "venue" : "In Proc. EMNLP. https://github. com/ytsvetko/qvec",
      "citeRegEx" : "Tsvetkov et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Tsvetkov et al\\.",
      "year" : 2015
    }, {
      "title" : "Word representations: a simple and general method for semi-supervised learning",
      "author" : [ "Turian et al.2010] Joseph Turian", "Lev Ratinov", "Yoshua Bengio" ],
      "venue" : "In Proc. ACL",
      "citeRegEx" : "Turian et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Turian et al\\.",
      "year" : 2010
    }, {
      "title" : "Towards universal speech recognition",
      "author" : [ "Wang et al.2002] Zhirong Wang", "Umut Topkara", "Tanja Schultz", "Alex Waibel" ],
      "venue" : "In Proc. ICMI,",
      "citeRegEx" : "Wang et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2002
    }, {
      "title" : "Predicting polarities of tweets by composing word embeddings with long short-term memory",
      "author" : [ "Wang et al.2015] Xin Wang", "Yuanchao Liu", "Chengjie Sun", "Baoxun Wang", "Xiaolong Wang" ],
      "venue" : "In Proc. ACL,",
      "citeRegEx" : "Wang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2015
    }, {
      "title" : "Towards speech understanding across multiple languages",
      "author" : [ "Ward et al.1998] Todd Ward", "Salim Roukos", "Chalapathy Neti", "Jerome Gros", "Mark Epstein", "Satya Dharanipragada" ],
      "venue" : "In Proc. ICSLP",
      "citeRegEx" : "Ward et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Ward et al\\.",
      "year" : 1998
    }, {
      "title" : "Transition-based neural constituent parsing",
      "author" : [ "Watanabe", "Sumita2015] Taro Watanabe", "Eiichiro Sumita" ],
      "venue" : "In Proc. ACL",
      "citeRegEx" : "Watanabe et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Watanabe et al\\.",
      "year" : 2015
    }, {
      "title" : "Sentence-level control vectors for deep neural network speech synthesis",
      "author" : [ "Watts et al.2015] Oliver Watts", "Zhizheng Wu", "Simon King" ],
      "venue" : "In Proc. Interspeech",
      "citeRegEx" : "Watts et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Watts et al\\.",
      "year" : 2015
    }, {
      "title" : "A study of multilingual speech recognition",
      "author" : [ "Weng et al.1997] Fuliang Weng", "Harry Bratt", "Leonardo Neumeyer", "Andreas Stolcke" ],
      "venue" : "In Proc. EUROSPEECH,",
      "citeRegEx" : "Weng et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Weng et al\\.",
      "year" : 1997
    } ],
    "referenceMentions" : [ {
      "referenceID" : 26,
      "context" : "1 the underlying framework of our model—RNNLM—a standard recurrent neural network based language model (Mikolov et al., 2010; Sundermeyer et al., 2012).",
      "startOffset" : 103,
      "endOffset" : 151
    }, {
      "referenceID" : 31,
      "context" : "1 the underlying framework of our model—RNNLM—a standard recurrent neural network based language model (Mikolov et al., 2010; Sundermeyer et al., 2012).",
      "startOffset" : 103,
      "endOffset" : 151
    }, {
      "referenceID" : 26,
      "context" : "In the original RNNLMs (Mikolov et al., 2010), the transformation is such that:",
      "startOffset" : 23,
      "endOffset" : 45
    }, {
      "referenceID" : 2,
      "context" : "To overcome the notorious problem in recurrent neural networks of vanishing gradients (Bengio et al., 1994), following Sundermeyer et al.",
      "startOffset" : 86,
      "endOffset" : 107
    }, {
      "referenceID" : 2,
      "context" : "To overcome the notorious problem in recurrent neural networks of vanishing gradients (Bengio et al., 1994), following Sundermeyer et al. (2012), in recurrent layer we use long short-term memory (LSTM) units (Hochreiter and Schmidhuber, 1997):2",
      "startOffset" : 87,
      "endOffset" : 145
    }, {
      "referenceID" : 22,
      "context" : ", 2014), and Ethnologue (Lewis et al., 2015) typological databases via extensive post-processing and analysis.",
      "startOffset" : 24,
      "endOffset" : 44
    }, {
      "referenceID" : 36,
      "context" : "Learned continuous word representations—word vectors—are an important by-product of neural LMs, and these are used as features in numerous NLP applications, including chunking (Turian et al., 2010), part-of-speech tagging (Ling et al.",
      "startOffset" : 176,
      "endOffset" : 197
    }, {
      "referenceID" : 23,
      "context" : ", 2010), part-of-speech tagging (Ling et al., 2015), dependency parsing (Lazaridou et al.",
      "startOffset" : 32,
      "endOffset" : 51
    }, {
      "referenceID" : 21,
      "context" : ", 2015), dependency parsing (Lazaridou et al., 2013; Bansal et al., 2014; Dyer et al., 2015; Watanabe and Sumita, 2015), named entity recognition (Guo et al.",
      "startOffset" : 28,
      "endOffset" : 119
    }, {
      "referenceID" : 1,
      "context" : ", 2015), dependency parsing (Lazaridou et al., 2013; Bansal et al., 2014; Dyer et al., 2015; Watanabe and Sumita, 2015), named entity recognition (Guo et al.",
      "startOffset" : 28,
      "endOffset" : 119
    }, {
      "referenceID" : 6,
      "context" : ", 2015), dependency parsing (Lazaridou et al., 2013; Bansal et al., 2014; Dyer et al., 2015; Watanabe and Sumita, 2015), named entity recognition (Guo et al.",
      "startOffset" : 28,
      "endOffset" : 119
    }, {
      "referenceID" : 10,
      "context" : ", 2015; Watanabe and Sumita, 2015), named entity recognition (Guo et al., 2014), and sentiment analysis (Socher et al.",
      "startOffset" : 61,
      "endOffset" : 79
    }, {
      "referenceID" : 30,
      "context" : ", 2014), and sentiment analysis (Socher et al., 2013; Wang et al., 2015).",
      "startOffset" : 32,
      "endOffset" : 72
    }, {
      "referenceID" : 38,
      "context" : ", 2014), and sentiment analysis (Socher et al., 2013; Wang et al., 2015).",
      "startOffset" : 32,
      "endOffset" : 72
    }, {
      "referenceID" : 41,
      "context" : "While prior work has explored TTS augmented with acoustic features (Watts et al., 2015), to the best of our knowledge, we are the first to replace manually engineered phonetic features in TTS systems with automatically constructed phone vectors.",
      "startOffset" : 67,
      "endOffset" : 87
    }, {
      "referenceID" : 20,
      "context" : "(Kominek et al., 2008) have shown that a decrease in MCD of 0.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 28,
      "context" : "We conducted experiments on the IIIT-H Hindi voice database (Prahallad et al., 2012), a 2 hour single speaker database recorded by a professional male speaker.",
      "startOffset" : 60,
      "endOffset" : 84
    }, {
      "referenceID" : 5,
      "context" : "For all our voice-based experiments, we built CLUSTERGEN Statistical Parametric Synthesis voices (Black, 2006) using the Festvox voice building tools (Black and Lenzo, 2003) and the Festival speech synthesis engine (Black and Taylor, 1997).",
      "startOffset" : 97,
      "endOffset" : 110
    }, {
      "referenceID" : 33,
      "context" : "to quantify and interpret linguistic content of vector space models (Tsvetkov et al., 2015).",
      "startOffset" : 68,
      "endOffset" : 91
    }, {
      "referenceID" : 24,
      "context" : "This matrix is described in Littell et al. (2016) and is available at https://github.",
      "startOffset" : 28,
      "endOffset" : 50
    }, {
      "referenceID" : 11,
      "context" : "Interpolation of monolingual LMs is an alternative to obtain a multilingual model (Harbeck et al., 1997; Weng et al., 1997).",
      "startOffset" : 82,
      "endOffset" : 123
    }, {
      "referenceID" : 42,
      "context" : "Interpolation of monolingual LMs is an alternative to obtain a multilingual model (Harbeck et al., 1997; Weng et al., 1997).",
      "startOffset" : 82,
      "endOffset" : 123
    }, {
      "referenceID" : 39,
      "context" : "Bilingual language models trained on concatenated corpora were explored mainly in speech recognition (Ward et al., 1998; Wang et al., 2002; Fügen et al., 2003).",
      "startOffset" : 101,
      "endOffset" : 159
    }, {
      "referenceID" : 37,
      "context" : "Bilingual language models trained on concatenated corpora were explored mainly in speech recognition (Ward et al., 1998; Wang et al., 2002; Fügen et al., 2003).",
      "startOffset" : 101,
      "endOffset" : 159
    }, {
      "referenceID" : 8,
      "context" : "Bilingual language models trained on concatenated corpora were explored mainly in speech recognition (Ward et al., 1998; Wang et al., 2002; Fügen et al., 2003).",
      "startOffset" : 101,
      "endOffset" : 159
    }, {
      "referenceID" : 27,
      "context" : "Adaptations have been proposed to apply language models in bilingual settings in machine translation (Niehues et al., 2011) and code switching (Adel et al.",
      "startOffset" : 101,
      "endOffset" : 123
    }, {
      "referenceID" : 0,
      "context" : ", 2011) and code switching (Adel et al., 2013).",
      "startOffset" : 27,
      "endOffset" : 46
    }, {
      "referenceID" : 18,
      "context" : "Our work is inspired by the neural multimodal LMs (Kiros and Salakhutdinov, 2013; Kiros et al., 2015), which defined language models conditional on visual contexts, although we use a different language model architecture (recurrent vs.",
      "startOffset" : 50,
      "endOffset" : 101
    } ],
    "year" : 2016,
    "abstractText" : "We introduce polyglot language models, recurrent neural network models trained to predict symbol sequences in many different languages using shared representations of symbols and conditioning on typological information about the language to be predicted. We apply these to the problem of modeling phone sequences—a domain in which universal symbol inventories and cross-linguistically shared feature representations are a natural fit. Intrinsic evaluation on held-out perplexity, qualitative analysis of the learned representations, and extrinsic evaluation in two downstream applications that make use of phonetic features show (i) that polyglot models better generalize to held-out data than comparable monolingual models and (ii) that polyglot phonetic feature representations are of higher quality than those learned monolingually.",
    "creator" : "LaTeX with hyperref package"
  }
}