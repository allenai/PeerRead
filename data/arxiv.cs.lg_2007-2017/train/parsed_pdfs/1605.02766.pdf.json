{
  "name" : "1605.02766.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "LightNet: A Versatile, Standalone Matlab-based Environment for Deep Learning [Simplify Deep Learning in Hundreds of Lines of Code]",
    "authors" : [ "Chengxi Ye", "Chen Zhao", "Yezhou Yang", "Cornelia Fermüller", "Yiannis Aloimonos" ],
    "emails" : [ "yiannis}@umiacs.umd.edu", "*chenzhao@umd.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Availability: the source code and data is available at: https://github.com/yechengxi/LightNet\nCategories and Subject Descriptors D.0 [Software]: General; I.2.10 [Artificial Intelligence]: Vision and Scene Understanding\nKeywords Computer vision; natural language processing; image understanding; machine learning; deep learning; convolutional neural networks; multilayer perceptrons; recurrent neural networks; reinforcement learning"
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "Deep neural networks [8] have given rise to major advancements in many problems of machine intelligence. Most current implementations of neural network models primarily emphasize efficiency. These pipelines (Table 1) can consist of a quarter to half a million lines of code and often involve multiple programming languages [5, 13, 2]. It requires extensive efforts to thoroughly understand and modify the models. A straightforward and self-explanatory deep learning framework is highly anticipated to accelerate the understanding and application of deep neural network models.\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). MM ’16 October 15-19, 2016, Amsterdam, Netherlands c© 2016 Copyright held by the owner/author(s).\nACM ISBN 978-1-4503-3603-1/16/10.\nDOI: http://dx.doi.org/10.1145/2964284.2973791\nWe present LightNet, a lightweight, versatile, purely Matlab-based implementation of modern deep neural network models. Succinct and efficient Matlab programming techniques have been used to implement all the computational modules. Many popular types of neural networks, such as multilayer perceptrons, convolutional neural networks, and recurrent neural networks are implemented in LightNet, together with several variations of stochastic gradient descent (SDG) based optimization algorithms.\nSince LightNet is implemented solely with Matlab, the major computations are vectorized and implemented in hundreds of lines of code, orders of magnitude more succinct than existing pipelines. All fundamental operations can be easily customized, only basic knowledge of Matlab programming is required. Mathematically oriented researchers can focus on the mathematical modeling part rather than the engineering part. Application oriented users can easily understand and modify any part of the framework to develop new network architectures and adapt them to new applications. Aside from its simplicity, LightNet has the following features: 1. LightNet contains the most modern network architectures. 2. Applications in computer vision, natural language processing and reinforcement learning are demonstrated. 3. LightNet provides a comprehensive collection of optimization algorithms. 4. LightNet supports straightforward switching between CPU and GPU computing. 5. Fast Fourier transforms are used to efficiently compute convolutions, and thus large convolution kernels are supported. 6. LightNet automates hyper-parameter tuning with a novel Selective-SGD algorithm."
    }, {
      "heading" : "2. USING THE PACKAGE",
      "text" : "An example of using LightNet can be found in (Fig. 1): a simple template is provided to start the training process. The user is required to fill in some critical training parameters, such as the number of training epochs, or the training method. A Selective-SGD algorithm is provided to facilitate the selection of an optimal learning rate. The learning rate is\nar X\niv :1\n60 5.\n02 76\n6v 3\n[ cs\n.L G\n] 2\nA ug\n2 01\n6\nselected automatically, and can optionally be adjusted during the training. The framework supports both GPU and CPU computation, through the opts.use gpu option. Two additional functions are provided to prepare the training data and initialize the network structure. Every experiment in this paper can reproduced by running the related script file. More details can be found on the project webpage."
    }, {
      "heading" : "3. BUILDING BLOCKS",
      "text" : "The primary computational module includes a feed forward process and a backward/back propagation process. The feed forward process evaluates the model, and the back propagation reports the network gradients. Stochastic gradient descent based algorithms are used to optimize the model parameters."
    }, {
      "heading" : "3.1 Core Computational Modules",
      "text" : "LightNet allows us to focus on the mathematical modeling of the network, rather than low-level engineering details. To make this paper self-contained, we explain the main computational modules of LightNet. All networks ( and related experiments) in this paper are built with these modules. The notations below are chosen for simplicity. Readers can easily extend the derivations to the mini-batch setting.\n3.1.1 Linear Perceptron Layer A linear perceptron layer can be expressed as: y = Wx+b.\nHere, x denotes the input data of size input dim × 1, W denotes the weight matrix of size output dim × input dim, b is a bias vector of size output dim× 1, and y denotes the linear layer output of size output dim× 1.\nThe mapping from the input of the linear perceptron to the final network output can be expressed as: z = f(y) = f(Wx+ b), where f is a non-linear function that represents the network’s computation in the deeper layers, and z is the network output, which is usually a loss value.\nThe backward process calculates the derivative ∂z ∂x , which\nis the derivative passing to the shallower layers, and ∂z ∂W ,\n∂z ∂b\n, which are the gradients that guide the gradient descent process.\n∂z ∂x = ∂z ∂y · ∂y ∂x = f ′(y)T ·W (1)\n∂z ∂W = ∂z ∂y · ∂y ∂W = f ′(y) · xT (2)\n∂z ∂b = ∂z ∂y · ∂y ∂b = f ′(y) (3)\nThe module adopts extensively optimized Matlab matrix operations to calculate the matrix-vector products.\n3.1.2 Convolutional Layer A convolutional layer maps Nmap in input feature maps to\nNmap out output feature maps with a multidimensional filter bank kio. Each input feature map xi is convolved with the corresponding filter bank kio. The convolution results are summed, and a bias value bo is added, to generate the o-th output map: yo = ∑ 1≤i≤Nmap in kio ∗ xi+bo. To allow using large convolution kernels, fast Fourier transforms (FFT) are used for computing convolutions (and correlations). According to the convolution theorem [10], convolution in the spatial domain is equivalent to point-wise multiplication in the frequency domain. Therefore, ki ∗xi can be calculated using the Fourier transform as: ki ∗ xi = F−1{F{ki} · F{xi}}. Here, F denotes the Fourier transform and · denotes the point-wise multiplication operation. The convolution layer supports both padding and striding.\nThe mapping from the o-th output feature map to the network output can be expressed as: z = f(yo). Here f is the non-linear mapping from the o-th output feature map yo to the final network output. As before (in Sec. 3.1.1), ∂z\n∂xi ,\n∂z ∂ki , and ∂z ∂bo need to be calculated in the backward process, as follows:\n∂z ∂xi = ∂z ∂yo · ∂yo ∂xi = f ′(yo) ? ki, (4)\nwhere ? denotes the correlation operation. Denoting the complex conjugate as conj, this correlation is calculated in the frequency domain using the Fourier transform as: x?k = F−1{F{x} · conj(F{k})}.\n∂z\n∂k∗io =\n∂z ∂yo · ∂yo ∂k∗io = f ′(yo) ? xi, (5)\nwhere k∗ represents the flipped kernel k. Thus, the gradient ∂z ∂kio is calculated by flipping the correlation output. Finally,\n∂z ∂bo = ∂z ∂yo · ∂yo ∂bo = 1T · vec(f ′(yo)) (6)\nIn words, the gradient ∂z ∂bo can be calculated by point-wise summation of the values in f ′(yo).\n3.1.3 Max-pooling Layer The max pooling layer calculates the largest element in\nPr × Pc windows, with stride size Sr × Sc. A customized im2col ln function is implemented to convert the stridden pooling patches into column vectors, to vectorize the pooling computation in Matlab. The built-in max function is called on these column vectors to return the pooling result and the indices of these maximum values. Then, the indices in the\noriginal batched data are recovered accordingly. Also, zero padding can be applied to the input data.\nWithout the loss of generality, the mapping from the maxpooling layer input to the final network output can be expressed as: z = f(y) = f(Sx), where S is a selection matrix, and x is a column vector which denotes the input data in this layer.\nIn the backward process, ∂z ∂x is calculated and passed to\nthe shallower layers: ∂z ∂x = ∂z ∂y · S = f ′(y)TS.\nWhen the pooling range is less than or equal to the stride size, ∂z\n∂x can be calculated with simple matrix indexing tech-\nniques in Matlab. Specifically, an empty tensor dzdx of the same size with the input data is created. dzdx(from) = dzdy, where from is the pooling indices, and dzdy is a tensor recording the pooling results. When the pooling range is larger than the stride size, each entry in x can be pooled multiple times, and the back propagation gradients need to be accumulated for each of these multiple-pooled entries. In this case, the ∂z\n∂x is calculated using the Matlab function:\naccumarray().\n3.1.4 Rectified Linear Unit The rectified linear unit (ReLU) is implemented as a ma-\njor non-linear mapping function, some other functions including sigmoid and tanh are omitted from the discussion here. The ReLU function is the identity function if the input is larger than 0 and outputs 0 otherwise: y = relu(x) = x · ind(x > 0). In the backward process, the gradient is passed to the shallower layer if the input data is non-negative. Otherwise, the gradient is ignored."
    }, {
      "heading" : "3.2 Loss function",
      "text" : "Usually, a loss function is connected to the outputs of the deepest core computation module. Currently, LightNet supports the softmax log-loss function for classification tasks."
    }, {
      "heading" : "3.3 Optimization Algorithms",
      "text" : "Stochastic gradient descent (SGD) algorithm based optimization algorithms are the primary tools to train deep neural networks. The standard SGD algorithm and several of its popular variants such as Adagrad [3], RMSProp [12] and Adam [6] are also implemented for deep learning research. It is worth mentioning that we implement a novel Selective-SGD algorithm to facilitate the selection of hyperparameters, especially the learning rate. This algorithm selects the most efficient learning rate by running the SGD process for a few iterations using each learning rate from a discrete candidate set. During the middle of the neural net training, the Selective-SGD algorithm can also be applied to select different learning rates to accelerate the energy decay."
    }, {
      "heading" : "4. EXPERIMENTS",
      "text" : ""
    }, {
      "heading" : "4.1 Multilayer Perceptron Network",
      "text" : "A multilayer perceptron network is constructed to test the performance of LightNet on MNIST data [9]. The network takes 28 × 28 inputs from the MNIST image dataset and has 128 nodes respectively in the next two layers. The 128-dimensional features are then connected to 10 nodes to calculate the softmax output. See Fig. 2 for the experiment results.\n4.2 Convolutional Neural Network\nLightNet supports using state-of-the-art convolutional network models pretrained on the ImageNet dataset. It also supports training novel network models from scratch. A convolutional network with 4 convolution layers is constructed to test the performance of LightNet on CIFAR-10 data [7]. There are 32, 32, 64, 64 convolution kernels of size 5 × 5 in the first three layers, the last layer has kernel size 4 × 4. relu functions are applied after each convolution layer as the non-linear mapping function. LightNet automatically selects and adjusts the learning rate and can achieve stateof-the-art accuracy with this architecture. Selective-SGD leads to better accuracy compared with standard SGD with a fixed learning rate. Most importantly, using Selective-SGD avoids manual tuning of the learning rate. See Fig. 3 for the experiment results. The computations are carried out on a desktop computer with an Intel i5 6600K CPU and a Nvidia Titan X GPU with 12GB memory. The current version of LightNet can process 750 images per second with this network structure on the GPU, around 5× faster than using CPU."
    }, {
      "heading" : "4.3 LSTM Network",
      "text" : "The Long Short Term Memory (LSTM) [4] is a popular recurrent neural network model. Because of LightNet’s versatility, the LSTM network can be implemented in the LightNet package as a particular application. Notably, the core computational modules in LightNet are used to perform time domain forward process and back propagation for LSTM.\nThe forward process in an LSTM model can be formulated\nas:\nit = sigmoid(Wihht−1 +Wixxt + bi), (7)\not = sigmoid(Wohht−1 +Woxxt + bo), (8)\nft = sigmoid(Wfhht−1 +Wfxxt + bf ), (9)\ngt = tanh(Wghht−1 +Wgxxt + bg), (10)\nct = ft ct−1 + it gt, ht = ot tanh(ct), (11)\nzt = f(ht), z = T∑ t=1 zt. (12)\nWhere it/ot/ft denotes the response of the input/output/forget gate at time t. gt denotes the distorted input to the memory cell at time t. ct denotes the content of the memory cell at time t. ht denotes the hidden node value. f maps the hidden nodes to the network loss zt at time t. The full network loss is calculated by summing the loss at each individual time frame in Eq. 12.\nTo optimize the LSTM model, back propagation through time is implemented and the most critical value to calculate in LSTM is: ∂z\n∂cs = ∑T t=s ∂zt ∂cs\n. A critical iterative property is adopted to calculate the\nabove value:\n∂z\n∂cs−1 =\n∂z\n∂cs ∂cs ∂cs−1 + ∂zs−1 ∂cs−1 . (13)\nA few other gradients can be calculated through the chain rule using the above calculation output:\n∂zt ∂ot = ∂zt ∂ht ∂ht ∂ot , ∂z ∂{i, f, g}t = ∂z ∂ct ∂ct ∂{i, f, g}t . (14)\nThe LSTM network is tested on a character language modeling task. The dataset consists of 20, 000 sentences selected from works of Shakespeare. Each sentence is broken into 67 characters (and punctuation marks), and the LSTM model is deployed to predict the next character based on the characters before. 30 hidden nodes are used in the network model and RMSProp is used for the training. After 10 epochs, the prediction accuracy of the next character is improved to 70%."
    }, {
      "heading" : "4.4 Q-Network",
      "text" : "As an application in reinforcement learning, We created a Q-Network [11] with the MLP network. The Q-Network is then applied to the classic Cart-Pole problem [1]. The dynamics of the Cart-Pole system can be learned with a twolayer network in hundreds of iterations. One iteration of the update process of the Q-Network is:\nQnew(stateold, act) = reward+γQcurrent(statenew, actbest)\n= reward+ γmaxaQcurrent(statenew, a)\n= reward+ γV (statenew). (15)\nThe action is randomly selected with probability epsilon, otherwise the action leading to the highest score is selected. The desired network output Qnew is calculated using the observed reward and the discounted value γV (statenew) of the resulting state, predicted by the current network through Eq. 15.\nBy using a least squared loss function:\nz = (y −Qcurrent(stateold, act))2\n= (Qnew(stateold, act)−Qcurrent(stateold, act))2, (16)\nthe Q-Network can be optimized using the gradient:\n∂z ∂θ =\n∂z\n∂Qcurrent ∂Qcurrent ∂θ . (17)\nHere θ denotes the parameters in the Q-Network."
    }, {
      "heading" : "5. CONCLUSION",
      "text" : "LightNet provides an easy-to-expand ecosystem for the understanding and development of deep neural network models. Thanks to its user-friendly Matlab based environment, the whole computational process can be easily tracked and visualized. This set of the main features can provide unique convenience to the deep learning research community."
    }, {
      "heading" : "6. REFERENCES",
      "text" : "[1] Barto, A. G., Sutton, R. S., and Anderson, C. W.\nNeuronlike adaptive elements that can solve difficult learning control problems. Systems, Man and Cybernetics, IEEE Transactions on, 5 (1983), 834–846.\n[2] Bastien, F., Lamblin, P., Pascanu, R., Bergstra, J., Goodfellow, I., Bergeron, A., Bouchard, N., Warde-Farley, D., and Bengio, Y. Theano: new features and speed improvements. arXiv preprint arXiv:1211.5590 (2012). [3] Duchi, J., Hazan, E., and Singer, Y. Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research 12 (2011), 2121–2159. [4] Hochreiter, S., and Schmidhuber, J. Long short-term memory. Neural computation 9, 8 (1997), 1735–1780. [5] Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadarrama, S., and Darrell, T. Caffe: Convolutional architecture for fast feature embedding. In Proceedings of the ACM International Conference on Multimedia (2014), ACM, pp. 675–678. [6] Kingma, D., and Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014). [7] Krizhevsky, A., and Hinton, G. Learning multiple layers of features from tiny images, 2009. [8] Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems (2012), pp. 1097–1105. [9] LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document recognition. Proceedings of the IEEE 86, 11 (1998), 2278–2324.\n[10] Mallat, S. A wavelet tour of signal processing: the sparse way. Academic press, 2008. [11] Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., et al. Human-level control through deep reinforcement learning. Nature 518, 7540 (2015), 529–533. [12] Tieleman, T., and Hinton, G. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning 4 (2012), 2. [13] Vedaldi, A., and Lenc, K. Matconvnet: Convolutional neural networks for matlab. In Proceedings of the 23rd Annual ACM Conference on Multimedia Conference (2015), ACM, pp. 689–692."
    } ],
    "references" : [ {
      "title" : "Neuronlike adaptive elements that can solve difficult learning control problems",
      "author" : [ "A.G. Barto", "R.S. Sutton", "C.W. Anderson" ],
      "venue" : "Systems, Man and Cybernetics, IEEE Transactions on,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1983
    }, {
      "title" : "Theano: new features and speed improvements",
      "author" : [ "F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I. Goodfellow", "A. Bergeron", "N. Bouchard", "D. Warde-Farley", "Y. Bengio" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2012
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "J. Duchi", "E. Hazan", "Y. Singer" ],
      "venue" : "The Journal of Machine Learning Research",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2011
    }, {
      "title" : "Long short-term memory",
      "author" : [ "S. Hochreiter", "J. Schmidhuber" ],
      "venue" : "Neural computation 9,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1997
    }, {
      "title" : "Caffe: Convolutional architecture for fast feature embedding",
      "author" : [ "Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell" ],
      "venue" : "In Proceedings of the ACM International Conference on Multimedia (2014),",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2014
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "D. Kingma", "J. Ba" ],
      "venue" : "arXiv preprint arXiv:1412.6980",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2014
    }, {
      "title" : "Learning multiple layers of features from tiny",
      "author" : [ "A. Krizhevsky", "G. Hinton" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2009
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2012
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner" ],
      "venue" : "Proceedings of the IEEE 86,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1998
    }, {
      "title" : "A wavelet tour of signal processing: the sparse way",
      "author" : [ "S. Mallat" ],
      "venue" : "Academic press,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2008
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G Ostrovski" ],
      "venue" : "Nature 518,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2015
    }, {
      "title" : "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude",
      "author" : [ "T. Tieleman", "G. Hinton" ],
      "venue" : "COURSERA: Neural Networks for Machine Learning",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2012
    }, {
      "title" : "Matconvnet: Convolutional neural networks for matlab",
      "author" : [ "A. Vedaldi", "K. Lenc" ],
      "venue" : "In Proceedings of the 23rd Annual ACM Conference on Multimedia Conference (2015),",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "Deep neural networks [8] have given rise to major advancements in many problems of machine intelligence.",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 4,
      "context" : "These pipelines (Table 1) can consist of a quarter to half a million lines of code and often involve multiple programming languages [5, 13, 2].",
      "startOffset" : 132,
      "endOffset" : 142
    }, {
      "referenceID" : 12,
      "context" : "These pipelines (Table 1) can consist of a quarter to half a million lines of code and often involve multiple programming languages [5, 13, 2].",
      "startOffset" : 132,
      "endOffset" : 142
    }, {
      "referenceID" : 1,
      "context" : "These pipelines (Table 1) can consist of a quarter to half a million lines of code and often involve multiple programming languages [5, 13, 2].",
      "startOffset" : 132,
      "endOffset" : 142
    }, {
      "referenceID" : 9,
      "context" : "According to the convolution theorem [10], convolution in the spatial domain is equivalent to point-wise multiplication in the frequency domain.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 2,
      "context" : "The standard SGD algorithm and several of its popular variants such as Adagrad [3], RMSProp [12] and Adam [6] are also implemented for deep learning research.",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 11,
      "context" : "The standard SGD algorithm and several of its popular variants such as Adagrad [3], RMSProp [12] and Adam [6] are also implemented for deep learning research.",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 5,
      "context" : "The standard SGD algorithm and several of its popular variants such as Adagrad [3], RMSProp [12] and Adam [6] are also implemented for deep learning research.",
      "startOffset" : 106,
      "endOffset" : 109
    }, {
      "referenceID" : 8,
      "context" : "A multilayer perceptron network is constructed to test the performance of LightNet on MNIST data [9].",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 6,
      "context" : "A convolutional network with 4 convolution layers is constructed to test the performance of LightNet on CIFAR-10 data [7].",
      "startOffset" : 118,
      "endOffset" : 121
    }, {
      "referenceID" : 3,
      "context" : "The Long Short Term Memory (LSTM) [4] is a popular recurrent neural network model.",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 10,
      "context" : "As an application in reinforcement learning, We created a Q-Network [11] with the MLP network.",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 0,
      "context" : "The Q-Network is then applied to the classic Cart-Pole problem [1].",
      "startOffset" : 63,
      "endOffset" : 66
    } ],
    "year" : 2016,
    "abstractText" : "LightNet is a lightweight, versatile, purely Matlabbased deep learning framework. The idea underlying its design is to provide an easy-to-understand, easy-to-use and efficient computational platform for deep learning research. The implemented framework supports major deep learning architectures such as Multilayer Perceptron Networks (MLP), Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN). The framework also supports both CPU and GPU computation, and the switch between them is straightforward. Different applications in computer vision, natural language processing and robotics are demonstrated as experiments. Availability: the source code and data is available at: https://github.com/yechengxi/LightNet",
    "creator" : "LaTeX with hyperref package"
  }
}