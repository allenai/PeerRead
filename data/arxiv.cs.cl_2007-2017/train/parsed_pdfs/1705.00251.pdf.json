{
  "name" : "1705.00251.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "liub}@uic.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 5.\n00 25\n1v 1\n[ cs\n.C L\n] 2\n9 A\npr 2\n01 7\nThis paper makes a focused contribution to supervised aspect extraction. It shows that if the system has performed aspect extraction from many past domains and retained their results as knowledge, Conditional Random Fields (CRF) can leverage this knowledge in a lifelong learning manner to extract in a new domain markedly better than the traditional CRF without using this prior knowledge. The key innovation is that even after CRF training, the model can still improve its extraction with experiences in its applications."
    }, {
      "heading" : "1 Introduction",
      "text" : "Aspect extraction is a key task of opinion mining (Liu, 2012). It extracts opinion targets from opinion text. For example, from the sentence “The battery is good”, it aims to extract “battery”, which is a product feature, also called an aspect.\nAspect extraction is commonly done using a supervised or an unsupervised approach. The unsupervised approach includes methods such as frequent pattern mining (Hu and Liu, 2004; Popescu and Etzioni, 2005; Zhu et al., 2009), syntactic rules-based extraction (Zhuang et al., 2006; Wang and Wang, 2008; Wu et al., 2009; Zhang et al., 2010; Qiu et al., 2011; Poria et al., 2014), topic modeling (Mei et al., 2007; Titov and McDonald, 2008; Li et al., 2010; Brody and Elhadad, 2010; Wang et al., 2010; Moghaddam and Ester, 2011; Mukherjee and Liu, 2012; Lin and He, 2009; Zhao et al., 2010; Jo and Oh, 2011; Fang and Huang, 2012; Wang et al., 2016), word alignment (Liu et al., 2013), label propagation (Zhou et al., 2013; Shu et al., 2016), and others (Zhao et al., 2015).\nThis paper focuses on the supervised approach (Jakob and Gurevych, 2010; Choi and Cardie, 2010; Mitchell et al., 2013) using Conditional Random Fields (CRF) (Lafferty et al., 2001). It shows that the results of CRF can be significantly improved by leveraging some prior knowledge automatically mined from the extraction results of previous domains, including domains without labeled data. The improvement is possible because although every product (domain) is different, there is a fair amount of aspects sharing across domains (Chen and Liu, 2014). For example, every review domain has the aspect price and reviews of many products have the aspect battery life or screen. Those shared aspects may not appear in the training data but appear in unlabeled data and the test data. We can exploit such sharing to help CRF perform much better.\nDue to leveraging the knowledge gained from the past to help the new domain extraction, we are using the idea of lifelong machine learning (LML) (Chen and Liu, 2016; Thrun, 1998; Silver et al., 2013), which is a continuous learning paradigm that retains the knowledge learned in the past and uses it to help future learning and problem solving with possible adaptations.\nThe setting of the proposed approach L-CRF (Lifelong CRF) is as follows: A CRF model M has been trained with a labeled training review dataset. At a particular point in time, M has extracted aspects from data in n previous domains D1, . . . ,Dn (which are unlabeled) and the extracted sets of aspects are A1, . . . , An. Now, the system is faced with a new domain data Dn+1. M can leverage some reliable prior knowledge in A1, . . . , An to make a better extraction fromDn+1 than without leveraging this prior knowledge.\nThe key innovation of L-CRF is that even after supervised training, the model can still improve its extraction in testing or its applications with expe-\nriences. Note that L-CRF is different from semisupervised learning (Zhu, 2005) as the n previous (unlabeled) domain data used in extraction are not used or not available during model training.\nThere are prior LML works for aspect extraction (Chen et al., 2014; Liu et al., 2016), but they were all unsupervised methods. Supervised LML methods exist (Chen et al., 2015; Ruvolo and Eaton, 2013), but they are for classification rather than for sequence learning or labeling like CRF. A semi-supervised LML method is used in NELL(Mitchell et al., 2015), but it is heuristic pattern-based. It doesn’t use sequence learning and is not for aspect extraction. LML is related to transfer learning and multi-task learning (Pan and Yang, 2010), but they are also quite different (see (Chen and Liu, 2016) for details).\nTo the best of our knowledge, this is the first paper that uses LML to help a supervised extraction method to markedly improve its results."
    }, {
      "heading" : "2 Conditional Random Fields",
      "text" : "CRF learns from an observation sequence x to estimate a label sequence y: p(y|x;θ), where θ is a set of weights. Let l be the l-th position in the sequence. The core parts of CRF are a set of feature functions F = {fh(yl, yl−1,xl)} H h=1 and their corresponding weights θ = {θh} H h=1. Feature Functions: We use two types of feature functions (FF). One is Label-Label (LL) FF:\nfLLij (yl, yl−1) = 1{yl = i}1{yl−1 = j},∀i, j ∈ Y, (1)\nwhere Y is the set of labels, and 1{·} an indicator function. The other is Label-Word (LW) FF:\nfLWiv (yl,xl) = 1{yl = i}1{xl = v},∀i ∈ Y,∀v ∈ V, (2)\nwhere V is the vocabulary. This FF returns 1when the l-th word is v and the l-th label is v’s specific label i; otherwise 0. xl is the current word, and is represented as a multi-dimensional vector. Each dimension in the vector is a feature of xl.\nFollowing the previous work in (Jakob and Gurevych, 2010), we use the feature set {W, -1W, +1W, P, -1P, +1P, G}, where W is the word and P is its POS-tag, -1W is the previous word, -1P is its POS-tag, +1W is the next word, +1P is its POS-tag, and G is the generalized dependency feature.\nUnder the Label-Word FF type, we have two sub-types of FF: Label-dimension FF and Label-G\nFF. Label-dimension FF is for the first 6 features, and Label-G is for the G feature.\nThe Label-dimension (Ld) FF is defined as\nfLd ivd (yl,xl) = 1{yl = i}1{x d l = v d},∀i ∈ Y,∀vd ∈ Vd, (3)\nwhere Vd is the set of observed values in feature d ∈ {W,−1W,+1W,P,−1P,+1P} and we call Vd feature d’s feature values. Eq. (3) is a FF that returns 1 when xl’s feature d equals to the feature value vd and the variable yl (lth label) equals to the label value i; otherwise 0.\nWe describe G and its feature function next,\nwhich also holds the key to the proposed L-CRF."
    }, {
      "heading" : "3 General Dependency Feature (G)",
      "text" : "Feature G uses generalized dependency relations. What is interesting about this feature is that it enables L-CRF to use past knowledge in its sequence prediction at the test time in order to perform much better. This will become clear shortly. This feature takes a dependency pattern as its value, which is generalized from dependency relations.\nThe general dependency feature (G) of the variable xl takes a set of feature values V G. Each feature value vG is a dependency pattern. The LabelG (LG) FF is defined as:\nfLG ivG (yl,xl) = 1{yl = i}1{x G l = v G},∀i ∈ Y,∀vG ∈ VG. (4)\nSuch a FF returns 1 when the dependency feature of the variable xl equals to a dependency pattern vG and the variable yl equals to the label value i."
    }, {
      "heading" : "3.1 Dependency Relation",
      "text" : "Dependency relations have been shown useful in many sentiment analysis applications (Johansson and Moschitti, 2010; Jakob and Gurevych, 2010). A dependency relation 1 is a quintuple-tuple: (type, gov, govpos, dep, deppos), where type is the type of the dependency relation, gov is the governor word, govpos is the POS tag of the governor word, dep is the dependent word, and deppos is the POS tag of the dependent word. The l-th word can either be the governor word or the dependent word in a dependency relation."
    }, {
      "heading" : "3.2 Dependency Pattern",
      "text" : "We generalize dependency relations into dependency patterns using the following steps:\n1We obtain dependency relations using Stanford CoreNLP: http://stanfordnlp.github.io/CoreNLP/.\n1. For each dependency relation, replace the\ncurrent word (governor word or dependent word) and its POS tag with a wildcard since we already have the word (W) and the POS tag (P) features.\n2. Replace the context word (the word other\nthan the l-th word) in each dependency relation with a knowledge label to form a more general dependency pattern. Let the set of aspects annotated in the training data be Kt. If the context word in the dependency relation appears in Kt, we replace it with a knowledge label ‘A’ (aspect); otherwise ‘O’ (other).\nFor example, we work on the sentence “The battery of this camera is great.” The dependency relations are given in Table 1. Assume the current word is “battery,” and “camera” is annotated as an aspect. The original dependency relation between “camera” and “battery” produced by a parser is (nmod, battery, NN, camera, NN). Note that we do not use the word positions in the relations in Table 1. Since the current word’s information (the word itself and its POS-tag) in the dependency relation is redundant, we replace it with a wild-card. The relation becomes (nmod, *, camera, NN). Secondly, since “camera” is in Kt, we replace “camera” with a general label ‘A’. The final dependency pattern becomes (nmod,*, A, NN).\nWe now explain why dependency patterns can enable a CRF model to leverage the past knowledge. The key is the knowledge label ‘A’ above, which indicates a likely aspect. Recall that our problem setting is that when we need to extract from the new domain Dn+1 using a trained CRF model M , we have already extracted from many previous domains D1, . . . ,Dn and retained their extracted sets of aspects A1, . . . , An. Then, we can mine reliable aspects from A1, . . . , An and add them in Kt, which enables many knowledge labels in the dependency patterns of the new data An+1 due to sharing of aspects across domains. This enriches the dependency pattern features, which consequently allows more aspects to\nbe extracted from the new domain Dn+1.\nAlgorithm 1 Lifelong Extraction of L-CRF\n1: Kp ← ∅ 2: loop 3: F ← FeatureGeneration(Dn+1,K) 4: An+1 ← Apply-CRF-Model(M,F ) 5: S ← S ∪ {An+1} 6: Kn+1 ← Frequent-Aspects-Mining(S, λ) 7: if Kp = Kn+1 then 8: break 9: else\n10: K ← Kt ∪Kn+1 11: Kp ← Kn+1 12: S ← S − {An+1} 13: end if 14: end loop"
    }, {
      "heading" : "4 The Proposed L-CRF Algorithm",
      "text" : "We now present the L-CRF algorithm. As the dependency patterns for the general dependency feature do not use any actual words and they can also use the prior knowledge, they are quite powerful for cross-domain extraction (the test domain is not used in training).\nLet K be a set of reliable aspects mined from the aspects extracted in past domain datasets using the CRF model M . Note that we assume that M has already been trained using some labeled training data Dt. Initially, K is Kt (the set of all annotated aspects in the training data Dt). The more domainsM has worked on, the more aspects it extracts, and the larger the set K gets. When faced with a new domain Dn+1, K allows the general dependency feature to generate more dependency patterns related to aspects due to more knowledge labels ‘A’ as we explained in the previous section. Consequently, CRF has more informed features to produce better extraction results.\nL-CRF works in two phases: training phase and lifelong extraction phase. The training phase trains a CRF model M using the training data Dt, which is the same as normal CRF training, and\nwill not be discussed further. In the lifelong extraction phase, M is used to extract aspects from coming domains (M does not change and the domain data are unlabeled). All the results from the domains are retained in past aspect store S. At a particular time, it is assumed M has been applied to n past domains, and is now faced with the n + 1 domain. L-CRF uses M and reliable aspects (denoted Kn+1) mined from S and K t (K = Kt ∪ Kn+1) to extract from Dn+1. Note that aspects Kt from the training data are considered always reliable as they are manually labeled, thus a subset ofK . We cannot use all extracted aspects from past domains as reliable aspects due to many extraction errors. But those aspects that appear in multiple past domains are more likely to be correct. ThusK contains those frequent aspects in S. The lifelong extraction phase is in Algorithm 1.\nLifelong Extraction Phase: Algorithm 1 per-\nforms extraction onDn+1 iteratively.\n1. It generates features (F ) on the data Dn+1 (line 3), and applies the CRF model M on F\nto produce a set of aspects An+1 (line 4).\n2. An+1 is added to S, the past aspect store.\nFrom S, we mine a set of frequent aspects Kn+1. The frequency threshold is λ.\n3. If Kn+1 is the same as Kp from the previ-\nous iteration, the algorithm exits as no new aspects can be found. We use an iterative process because each extraction gives new results, which may increase the size of K , the reliable past aspects or past knowledge. The increased K may produce more dependency patterns, which can enable more extractions.\n4. Else: some additional reliable aspects are\nfound. M may extract additional aspects in the next iteration. Lines 10 and 11 update the two sets for the next iteration."
    }, {
      "heading" : "5 Experiments",
      "text" : "We now evaluate the proposed L-CRFmethod and compare with baselines."
    }, {
      "heading" : "5.1 Evaluation Datasets",
      "text" : "We use two types of data for our experiments. The first type consists of seven (7) annotated benchmark review datasets from 7 domains (types of products). Since they are annotated, they are used in training and testing. The first 4 datasets are from (Hu and Liu, 2004), which actually has 5 datasets from 4 domains. Since we are mainly interested in results at the domain level, we did not use one of the domain-repeated datasets. The last 3 datasets of three domains (products) are from (Liu et al., 2016). These datasets are used to make up our CRF training data Dt and test data Dn+1. The annotation details are given in Table 2.\nThe second type has 50 unlabeled review datasets from 50 domains or types of products (Chen and Liu, 2014). Each dataset has 1000 reviews. They are used as the past domain data, i.e., D1, . . . ,Dn (n = 50). Since they are not labeled, they cannot be used for training or testing."
    }, {
      "heading" : "5.2 Baseline Methods",
      "text" : "We compare L-CRF with CRF. We will not compare with unsupervised methods, which have been shown improvable by lifelong learning (Chen et al., 2014; Liu et al., 2016). The frequency threshold λ in Algorithm 1 used in our experiment to judge which extracted aspects are considered reliable is empirically set to 2. CRF: We use the linear chain CRF from 2. Note that CRF uses all features including dependency features as the proposed L-CRF but does not employ the 50 domains unlabeled data used for lifelong learning\nCRF+R: It treats the reliable aspect set K as a dictionary. It adds those reliable aspects in K that are not extracted by CRF but are in the test data to the final results. We want to see whether incorporating K into the CRF extraction through dependency patterns in L-CRF is actually needed.\nWe do not compare with domain adaptation or transfer learning because domain adaption basically uses the source domain labeled data to help learning in the target domain with few or no labeled data. Our 50 domains used in lifelong learning have no labels. So they cannot help in transfer\n2https://github.com/huangzhengsjtu/pcrf/\nlearning. Although in transfer learning, the target domain usually has a large quantity of unlabeled data, but the 50 domains are not used as the target domains in our experiments."
    }, {
      "heading" : "5.3 Experiment Setting",
      "text" : "To compare the systems using the same training and test data, for each dataset we use 200 sentences for training and 200 sentences for testing to avoid bias towards any dataset or domain because we will combine multiple domain datasets for CRF training. We conducted both cross-domain and in-domain tests. Our problem setting is crossdomain. In-domain is used for completeness. In both cases, we assume that extraction has been done for the 50 domains.\nCross-domain experiments: We combine 6 labeled domain datasets for training (1200 sentences) and test on the 7th domain (not used in training). This gives us 7 cross-domain results. This set of tests is particularly interesting as it is desirable to have the trained model used in crossdomain situations to save manual labeling effort.\nIn-domain experiments: We train and test on the same 6 domains (1200 sentences for training and 1200 sentences for testing). This also gives us 7 in-domain results.\nEvaluating Measures: We use the popular pre-\ncision P, recall R, and F1-score."
    }, {
      "heading" : "5.4 Results and Analysis",
      "text" : "All the experiment results are given in Table 3.\nCross-domain: Each −X in column 1 means that domain X is not used in training. X in column 2 means that domain X is used in testing. We can see that L-CRF is markedly better than CRF and CRF+R in F1. CRF+R is very poor due to poor precisions, which shows treating the reliable aspects set K as a dictionary isn’t a good idea.\nIn-domain: −X in training and test columns means that the other 6 domains are used in both training and testing (thus in-domain). We again see that L-CRF is consistently better than CRF and CRF+R in F1. The amount of gain is smaller. This is expected because most aspects appeared in training probably also appear in the test data as they are reviews from the same 6 products."
    }, {
      "heading" : "6 Conclusion",
      "text" : "This paper proposed a lifelong learning method to enable CRF to leverage the knowledge gained from extraction results of previous domains (unlabeled) to improve its extraction. Experimental results showed the effectiveness of L-CRF. The current approach does not change the CRF model itself. In our future work, we plan to modify CRF so that it can consider previous extraction results as well as the knowledge in previous CRF models."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was supported in part by grants from National Science Foundation (NSF) under grant no. IIS-1407927 and IIS-1650900."
    } ],
    "references" : [ {
      "title" : "An unsupervised aspect-sentiment model for online reviews",
      "author" : [ "Samuel Brody", "Noemie Elhadad." ],
      "venue" : "NAACL ’10. pages 804–812.",
      "citeRegEx" : "Brody and Elhadad.,? 2010",
      "shortCiteRegEx" : "Brody and Elhadad.",
      "year" : 2010
    }, {
      "title" : "Topic modeling using topics from many domains, lifelong learning and big data",
      "author" : [ "Zhiyuan Chen", "Bing Liu." ],
      "venue" : "ICML ’14. pages 703–711.",
      "citeRegEx" : "Chen and Liu.,? 2014",
      "shortCiteRegEx" : "Chen and Liu.",
      "year" : 2014
    }, {
      "title" : "Lifelong Machine Learning",
      "author" : [ "Zhiyuan Chen", "Bing Liu." ],
      "venue" : "Morgan & Claypool Publishers.",
      "citeRegEx" : "Chen and Liu.,? 2016",
      "shortCiteRegEx" : "Chen and Liu.",
      "year" : 2016
    }, {
      "title" : "Lifelong learning for sentiment classification",
      "author" : [ "Zhiyuan Chen", "Nianzu Ma", "Bing Liu." ],
      "venue" : "Volume 2: Short Papers page 750.",
      "citeRegEx" : "Chen et al\\.,? 2015",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2015
    }, {
      "title" : "Aspect extraction with automated prior knowledge learning",
      "author" : [ "Zhiyuan Chen", "Arjun Mukherjee", "Bing Liu." ],
      "venue" : "ACL ’14. pages 347–358.",
      "citeRegEx" : "Chen et al\\.,? 2014",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2014
    }, {
      "title" : "Hierarchical sequential learning for extracting opinions and their attributes",
      "author" : [ "Yejin Choi", "Claire Cardie." ],
      "venue" : "ACL ’10. pages 269–274.",
      "citeRegEx" : "Choi and Cardie.,? 2010",
      "shortCiteRegEx" : "Choi and Cardie.",
      "year" : 2010
    }, {
      "title" : "Fine granular aspect analysis using latent structural models",
      "author" : [ "Lei Fang andMinlie Huang." ],
      "venue" : "ACL ’12. pages 333–337.",
      "citeRegEx" : "Huang.,? 2012",
      "shortCiteRegEx" : "Huang.",
      "year" : 2012
    }, {
      "title" : "Mining and summarizing customer reviews",
      "author" : [ "Minqing Hu", "Bing Liu." ],
      "venue" : "KDD ’04. pages 168– 177.",
      "citeRegEx" : "Hu and Liu.,? 2004",
      "shortCiteRegEx" : "Hu and Liu.",
      "year" : 2004
    }, {
      "title" : "Extracting opinion targets in a single- and cross-domain setting with conditional random fields",
      "author" : [ "Niklas Jakob", "Iryna Gurevych." ],
      "venue" : "EMNLP ’10. pages 1035–1045.",
      "citeRegEx" : "Jakob and Gurevych.,? 2010",
      "shortCiteRegEx" : "Jakob and Gurevych.",
      "year" : 2010
    }, {
      "title" : "Aspect and sentiment unification model for online review analysis",
      "author" : [ "Yohan Jo", "Alice H. Oh." ],
      "venue" : "InWSDM ’11. pages 815–824.",
      "citeRegEx" : "Jo and Oh.,? 2011",
      "shortCiteRegEx" : "Jo and Oh.",
      "year" : 2011
    }, {
      "title" : "Syntactic and semantic structure for opinion expression detection",
      "author" : [ "Richard Johansson", "Alessandro Moschitti." ],
      "venue" : "Proceedings of the Fourteenth Conference on Computational Natural Language Learning. pages 67–76.",
      "citeRegEx" : "Johansson and Moschitti.,? 2010",
      "shortCiteRegEx" : "Johansson and Moschitti.",
      "year" : 2010
    }, {
      "title" : "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "author" : [ "John Lafferty", "Andrew McCallum", "Fernando CN Pereira." ],
      "venue" : "ICML ’01. pages 282–289.",
      "citeRegEx" : "Lafferty et al\\.,? 2001",
      "shortCiteRegEx" : "Lafferty et al\\.",
      "year" : 2001
    }, {
      "title" : "Sentiment analysis with global topics and local dependency",
      "author" : [ "Fangtao Li", "Minlie Huang", "Xiaoyan Zhu." ],
      "venue" : "AAAI ’10. pages 1371–1376.",
      "citeRegEx" : "Li et al\\.,? 2010",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2010
    }, {
      "title" : "Joint sentiment/topic model for sentiment analysis",
      "author" : [ "Chenghua Lin", "Yulan He." ],
      "venue" : "CIKM ’09. pages 375–384.",
      "citeRegEx" : "Lin and He.,? 2009",
      "shortCiteRegEx" : "Lin and He.",
      "year" : 2009
    }, {
      "title" : "Sentiment Analysis and Opinion Mining",
      "author" : [ "Bing Liu." ],
      "venue" : "Morgan & Claypool Publishers.",
      "citeRegEx" : "Liu.,? 2012",
      "shortCiteRegEx" : "Liu.",
      "year" : 2012
    }, {
      "title" : "Opinion target extraction using partially-supervised word alignment model",
      "author" : [ "Kang Liu", "Liheng Xu", "Yang Liu", "Jun Zhao." ],
      "venue" : "IJCAI ’13. pages 2134– 2140.",
      "citeRegEx" : "Liu et al\\.,? 2013",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2013
    }, {
      "title" : "Improving opinion aspect extraction using semantic similarity and aspect associations",
      "author" : [ "Qian Liu", "Bing Liu", "Yuanlin Zhang", "Doo Soon Kim", "Zhiqiang Gao." ],
      "venue" : "Thirtieth AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Liu et al\\.,? 2016",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2016
    }, {
      "title" : "Topic sentiment mixture: Modeling facets and opinions in weblogs",
      "author" : [ "Qiaozhu Mei", "Xu Ling", "Matthew Wondra", "Hang Su", "ChengXiang Zhai." ],
      "venue" : "WWW ’07. pages 171–180.",
      "citeRegEx" : "Mei et al\\.,? 2007",
      "shortCiteRegEx" : "Mei et al\\.",
      "year" : 2007
    }, {
      "title" : "Open domain targeted sentiment",
      "author" : [ "Margaret Mitchell", "Jacqui Aguilar", "Theresa Wilson", "Benjamin Van Durme." ],
      "venue" : "ACL ’13. pages 1643–1654.",
      "citeRegEx" : "Mitchell et al\\.,? 2013",
      "shortCiteRegEx" : "Mitchell et al\\.",
      "year" : 2013
    }, {
      "title" : "Neverending learning",
      "author" : [ "A Saparov", "M Greaves", "J Welling." ],
      "venue" : "AAAI’2015.",
      "citeRegEx" : "Saparov et al\\.,? 2015",
      "shortCiteRegEx" : "Saparov et al\\.",
      "year" : 2015
    }, {
      "title" : "ILDA: interdependent lda model for learning latent aspects and their ratings from online product reviews",
      "author" : [ "Samaneh Moghaddam", "Martin Ester." ],
      "venue" : "SIGIR ’11. pages 665–674.",
      "citeRegEx" : "Moghaddam and Ester.,? 2011",
      "shortCiteRegEx" : "Moghaddam and Ester.",
      "year" : 2011
    }, {
      "title" : "Aspect extraction through semi-supervised modeling",
      "author" : [ "Arjun Mukherjee", "Bing Liu." ],
      "venue" : "ACL ’12. volume 1, pages 339–348.",
      "citeRegEx" : "Mukherjee and Liu.,? 2012",
      "shortCiteRegEx" : "Mukherjee and Liu.",
      "year" : 2012
    }, {
      "title" : "A survey on transfer learning",
      "author" : [ "Sinno Jialin Pan", "Qiang Yang." ],
      "venue" : "IEEE Transactions on knowledge and data engineering 22(10):1345–1359.",
      "citeRegEx" : "Pan and Yang.,? 2010",
      "shortCiteRegEx" : "Pan and Yang.",
      "year" : 2010
    }, {
      "title" : "Extracting product features and opinions from reviews",
      "author" : [ "Ana-Maria Popescu", "Oren Etzioni." ],
      "venue" : "HLT-EMNLP ’05. pages 339–346.",
      "citeRegEx" : "Popescu and Etzioni.,? 2005",
      "shortCiteRegEx" : "Popescu and Etzioni.",
      "year" : 2005
    }, {
      "title" : "A rule-based approach to aspect extraction from product reviews",
      "author" : [ "Soujanya Poria", "Erik Cambria", "Lun-Wei Ku", "Chen Gui", "Alexander Gelbukh." ],
      "venue" : "SocialNLP ’14. pages 28–37.",
      "citeRegEx" : "Poria et al\\.,? 2014",
      "shortCiteRegEx" : "Poria et al\\.",
      "year" : 2014
    }, {
      "title" : "Opinion word expansion and target extraction through double propagation",
      "author" : [ "Guang Qiu", "Bing Liu", "Jiajun Bu", "Chun Chen." ],
      "venue" : "Computational Linguistics 37(1):9–27.",
      "citeRegEx" : "Qiu et al\\.,? 2011",
      "shortCiteRegEx" : "Qiu et al\\.",
      "year" : 2011
    }, {
      "title" : "Ella: An efficient lifelong learning algorithm",
      "author" : [ "Paul Ruvolo", "Eric Eaton." ],
      "venue" : "ICML (1) 28:507–515.",
      "citeRegEx" : "Ruvolo and Eaton.,? 2013",
      "shortCiteRegEx" : "Ruvolo and Eaton.",
      "year" : 2013
    }, {
      "title" : "Lifelong-rl: Lifelong relaxation labeling for separating entities and aspects in opinion targets",
      "author" : [ "Lei Shu", "Bing Liu", "Hu Xu", "Annice Kim." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Shu et al\\.,? 2016",
      "shortCiteRegEx" : "Shu et al\\.",
      "year" : 2016
    }, {
      "title" : "Lifelong learning algorithms",
      "author" : [ "Sebastian Thrun" ],
      "venue" : "Machine Learning. Citeseer,",
      "citeRegEx" : "Thrun.,? \\Q1998\\E",
      "shortCiteRegEx" : "Thrun.",
      "year" : 1998
    }, {
      "title" : "Jointly modeling aspects and opinions",
      "author" : [ "ing Li" ],
      "venue" : null,
      "citeRegEx" : "Li.,? \\Q2010\\E",
      "shortCiteRegEx" : "Li.",
      "year" : 2010
    }, {
      "title" : "Multi-aspect opinion polling",
      "author" : [ "Muhua Zhu" ],
      "venue" : null,
      "citeRegEx" : "Zhu.,? \\Q2009\\E",
      "shortCiteRegEx" : "Zhu.",
      "year" : 2009
    }, {
      "title" : "Semi-supervised learning literature",
      "author" : [ "Xiaojin Zhu" ],
      "venue" : null,
      "citeRegEx" : "1802",
      "shortCiteRegEx" : "1802",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : "Aspect extraction is a key task of opinion mining (Liu, 2012).",
      "startOffset" : 50,
      "endOffset" : 61
    }, {
      "referenceID" : 7,
      "context" : "The unsupervised approach includes methods such as frequent pattern mining (Hu and Liu, 2004; Popescu and Etzioni, 2005; Zhu et al., 2009), syntactic rules-based extraction (Zhuang et al.",
      "startOffset" : 75,
      "endOffset" : 138
    }, {
      "referenceID" : 23,
      "context" : "The unsupervised approach includes methods such as frequent pattern mining (Hu and Liu, 2004; Popescu and Etzioni, 2005; Zhu et al., 2009), syntactic rules-based extraction (Zhuang et al.",
      "startOffset" : 75,
      "endOffset" : 138
    }, {
      "referenceID" : 25,
      "context" : ", 2009), syntactic rules-based extraction (Zhuang et al., 2006; Wang and Wang, 2008; Wu et al., 2009; Zhang et al., 2010; Qiu et al., 2011; Poria et al., 2014), topic modeling (Mei et al.",
      "startOffset" : 42,
      "endOffset" : 159
    }, {
      "referenceID" : 24,
      "context" : ", 2009), syntactic rules-based extraction (Zhuang et al., 2006; Wang and Wang, 2008; Wu et al., 2009; Zhang et al., 2010; Qiu et al., 2011; Poria et al., 2014), topic modeling (Mei et al.",
      "startOffset" : 42,
      "endOffset" : 159
    }, {
      "referenceID" : 17,
      "context" : ", 2014), topic modeling (Mei et al., 2007; Titov and McDonald, 2008; Li et al., 2010; Brody and Elhadad, 2010; Wang et al., 2010; Moghaddam and Ester, 2011; Mukherjee and Liu, 2012; Lin and He, 2009; Zhao et al., 2010; Jo and Oh, 2011; Fang and Huang, 2012; Wang et al., 2016), word alignment (Liu et al.",
      "startOffset" : 24,
      "endOffset" : 276
    }, {
      "referenceID" : 12,
      "context" : ", 2014), topic modeling (Mei et al., 2007; Titov and McDonald, 2008; Li et al., 2010; Brody and Elhadad, 2010; Wang et al., 2010; Moghaddam and Ester, 2011; Mukherjee and Liu, 2012; Lin and He, 2009; Zhao et al., 2010; Jo and Oh, 2011; Fang and Huang, 2012; Wang et al., 2016), word alignment (Liu et al.",
      "startOffset" : 24,
      "endOffset" : 276
    }, {
      "referenceID" : 0,
      "context" : ", 2014), topic modeling (Mei et al., 2007; Titov and McDonald, 2008; Li et al., 2010; Brody and Elhadad, 2010; Wang et al., 2010; Moghaddam and Ester, 2011; Mukherjee and Liu, 2012; Lin and He, 2009; Zhao et al., 2010; Jo and Oh, 2011; Fang and Huang, 2012; Wang et al., 2016), word alignment (Liu et al.",
      "startOffset" : 24,
      "endOffset" : 276
    }, {
      "referenceID" : 20,
      "context" : ", 2014), topic modeling (Mei et al., 2007; Titov and McDonald, 2008; Li et al., 2010; Brody and Elhadad, 2010; Wang et al., 2010; Moghaddam and Ester, 2011; Mukherjee and Liu, 2012; Lin and He, 2009; Zhao et al., 2010; Jo and Oh, 2011; Fang and Huang, 2012; Wang et al., 2016), word alignment (Liu et al.",
      "startOffset" : 24,
      "endOffset" : 276
    }, {
      "referenceID" : 21,
      "context" : ", 2014), topic modeling (Mei et al., 2007; Titov and McDonald, 2008; Li et al., 2010; Brody and Elhadad, 2010; Wang et al., 2010; Moghaddam and Ester, 2011; Mukherjee and Liu, 2012; Lin and He, 2009; Zhao et al., 2010; Jo and Oh, 2011; Fang and Huang, 2012; Wang et al., 2016), word alignment (Liu et al.",
      "startOffset" : 24,
      "endOffset" : 276
    }, {
      "referenceID" : 13,
      "context" : ", 2014), topic modeling (Mei et al., 2007; Titov and McDonald, 2008; Li et al., 2010; Brody and Elhadad, 2010; Wang et al., 2010; Moghaddam and Ester, 2011; Mukherjee and Liu, 2012; Lin and He, 2009; Zhao et al., 2010; Jo and Oh, 2011; Fang and Huang, 2012; Wang et al., 2016), word alignment (Liu et al.",
      "startOffset" : 24,
      "endOffset" : 276
    }, {
      "referenceID" : 9,
      "context" : ", 2014), topic modeling (Mei et al., 2007; Titov and McDonald, 2008; Li et al., 2010; Brody and Elhadad, 2010; Wang et al., 2010; Moghaddam and Ester, 2011; Mukherjee and Liu, 2012; Lin and He, 2009; Zhao et al., 2010; Jo and Oh, 2011; Fang and Huang, 2012; Wang et al., 2016), word alignment (Liu et al.",
      "startOffset" : 24,
      "endOffset" : 276
    }, {
      "referenceID" : 15,
      "context" : ", 2016), word alignment (Liu et al., 2013), label propagation (Zhou et al.",
      "startOffset" : 24,
      "endOffset" : 42
    }, {
      "referenceID" : 27,
      "context" : ", 2013), label propagation (Zhou et al., 2013; Shu et al., 2016), and others (Zhao et al.",
      "startOffset" : 27,
      "endOffset" : 64
    }, {
      "referenceID" : 8,
      "context" : "This paper focuses on the supervised approach (Jakob and Gurevych, 2010; Choi and Cardie, 2010; Mitchell et al., 2013) using Conditional Random Fields (CRF) (Lafferty et al.",
      "startOffset" : 46,
      "endOffset" : 118
    }, {
      "referenceID" : 5,
      "context" : "This paper focuses on the supervised approach (Jakob and Gurevych, 2010; Choi and Cardie, 2010; Mitchell et al., 2013) using Conditional Random Fields (CRF) (Lafferty et al.",
      "startOffset" : 46,
      "endOffset" : 118
    }, {
      "referenceID" : 18,
      "context" : "This paper focuses on the supervised approach (Jakob and Gurevych, 2010; Choi and Cardie, 2010; Mitchell et al., 2013) using Conditional Random Fields (CRF) (Lafferty et al.",
      "startOffset" : 46,
      "endOffset" : 118
    }, {
      "referenceID" : 11,
      "context" : ", 2013) using Conditional Random Fields (CRF) (Lafferty et al., 2001).",
      "startOffset" : 46,
      "endOffset" : 69
    }, {
      "referenceID" : 1,
      "context" : "The improvement is possible because although every product (domain) is different, there is a fair amount of aspects sharing across domains (Chen and Liu, 2014).",
      "startOffset" : 139,
      "endOffset" : 159
    }, {
      "referenceID" : 2,
      "context" : "Due to leveraging the knowledge gained from the past to help the new domain extraction, we are using the idea of lifelong machine learning (LML) (Chen and Liu, 2016; Thrun, 1998; Silver et al., 2013), which is a continuous learning paradigm that retains the knowledge learned in the past and uses it to help future learning and problem solving with possible adaptations.",
      "startOffset" : 145,
      "endOffset" : 199
    }, {
      "referenceID" : 28,
      "context" : "Due to leveraging the knowledge gained from the past to help the new domain extraction, we are using the idea of lifelong machine learning (LML) (Chen and Liu, 2016; Thrun, 1998; Silver et al., 2013), which is a continuous learning paradigm that retains the knowledge learned in the past and uses it to help future learning and problem solving with possible adaptations.",
      "startOffset" : 145,
      "endOffset" : 199
    }, {
      "referenceID" : 4,
      "context" : "There are prior LML works for aspect extraction (Chen et al., 2014; Liu et al., 2016), but they were all unsupervised methods.",
      "startOffset" : 48,
      "endOffset" : 85
    }, {
      "referenceID" : 16,
      "context" : "There are prior LML works for aspect extraction (Chen et al., 2014; Liu et al., 2016), but they were all unsupervised methods.",
      "startOffset" : 48,
      "endOffset" : 85
    }, {
      "referenceID" : 3,
      "context" : "Supervised LML methods exist (Chen et al., 2015; Ruvolo and Eaton, 2013), but they are for classification rather than for sequence learning or labeling like CRF.",
      "startOffset" : 29,
      "endOffset" : 72
    }, {
      "referenceID" : 26,
      "context" : "Supervised LML methods exist (Chen et al., 2015; Ruvolo and Eaton, 2013), but they are for classification rather than for sequence learning or labeling like CRF.",
      "startOffset" : 29,
      "endOffset" : 72
    }, {
      "referenceID" : 22,
      "context" : "LML is related to transfer learning and multi-task learning (Pan and Yang, 2010), but they are also quite different (see (Chen and Liu, 2016) for details).",
      "startOffset" : 60,
      "endOffset" : 80
    }, {
      "referenceID" : 2,
      "context" : "LML is related to transfer learning and multi-task learning (Pan and Yang, 2010), but they are also quite different (see (Chen and Liu, 2016) for details).",
      "startOffset" : 121,
      "endOffset" : 141
    }, {
      "referenceID" : 8,
      "context" : "Following the previous work in (Jakob and Gurevych, 2010), we use the feature set {W, -1W, +1W, P, -1P, +1P, G}, where W is the word and P is its POS-tag, -1W is the previous word, -1P is its POS-tag, +1W is the next word, +1P is its POS-tag, and G is the generalized dependency feature.",
      "startOffset" : 31,
      "endOffset" : 57
    }, {
      "referenceID" : 10,
      "context" : "Dependency relations have been shown useful in many sentiment analysis applications (Johansson and Moschitti, 2010; Jakob and Gurevych, 2010).",
      "startOffset" : 84,
      "endOffset" : 141
    }, {
      "referenceID" : 8,
      "context" : "Dependency relations have been shown useful in many sentiment analysis applications (Johansson and Moschitti, 2010; Jakob and Gurevych, 2010).",
      "startOffset" : 84,
      "endOffset" : 141
    }, {
      "referenceID" : 7,
      "context" : "The first 4 datasets are from (Hu and Liu, 2004), which actually has 5 datasets from 4 domains.",
      "startOffset" : 30,
      "endOffset" : 48
    }, {
      "referenceID" : 16,
      "context" : "The last 3 datasets of three domains (products) are from (Liu et al., 2016).",
      "startOffset" : 57,
      "endOffset" : 75
    }, {
      "referenceID" : 1,
      "context" : "The second type has 50 unlabeled review datasets from 50 domains or types of products (Chen and Liu, 2014).",
      "startOffset" : 86,
      "endOffset" : 106
    }, {
      "referenceID" : 4,
      "context" : "We will not compare with unsupervised methods, which have been shown improvable by lifelong learning (Chen et al., 2014; Liu et al., 2016).",
      "startOffset" : 101,
      "endOffset" : 138
    }, {
      "referenceID" : 16,
      "context" : "We will not compare with unsupervised methods, which have been shown improvable by lifelong learning (Chen et al., 2014; Liu et al., 2016).",
      "startOffset" : 101,
      "endOffset" : 138
    } ],
    "year" : 2017,
    "abstractText" : "This paper makes a focused contribution to supervised aspect extraction. It shows that if the system has performed aspect extraction from many past domains and retained their results as knowledge, Conditional Random Fields (CRF) can leverage this knowledge in a lifelong learning manner to extract in a new domain markedly better than the traditional CRF without using this prior knowledge. The key innovation is that even after CRF training, the model can still improve its extraction with experiences in its applications.",
    "creator" : "LaTeX with hyperref package"
  }
}