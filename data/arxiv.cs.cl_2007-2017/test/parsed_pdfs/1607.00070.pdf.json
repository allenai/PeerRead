{
  "name" : "1607.00070.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Sequence-to-Sequence Model for User Simulation in Spoken Dialogue Systems",
    "authors" : [ "Layla El Asri", "Jing He", "Kaheer Suleman" ],
    "emails" : [ "first.last@maluuba.com" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Statistical Spoken Dialogue Systems (SDS) [1, 2, 3] typically require several thousands of dialogues to learn a good dialogue strategy [4, 5]. It is costly to collect this quantity of dialogues; therefore, research has turned to user simulation [6, 7, 8, 9]. A user simulator is expected to have the following properties: to be statistically consistent with real users, to generate coherent sequences of actions, and to generalize to new contexts [10]. User simulation can be either at the intention level, i.e., generating dialogue acts [11, 9], or at the utterance level [12]. In this work, we focus on the intention level.\nMany models have been designed in order to meet the requirements cited above [6, 13, 14, 15, 11, 9]. These models typically suffer from important drawbacks, which include the inability to take dialogue history into account [6], the need of rigid structure to ensure coherent user behaviour [16], heavy dependence on a specific domain [13], the inability to output several user intentions during one dialogue turn [12], or the requirement of a summarized action space for tractability [11].\nIn this paper, we introduce a sequence-to-sequence model for user simulation. The simulator is modelled with an encoder Recurrent Neural Network (RNN) and a decoder RNN [17]. The encoder takes as input the entire history of the dialogue, encoded as a sequence of dialogue contexts. It outputs an internal representation of this sequence. This representation is passed\nas input to the decoder. The decoder generates a sequence of dialogue acts corresponding to user intentions.\nWe train this model on the Dialogue State Tracking Challenge 2 (DSTC2) [18] dataset. This corpus consists of dialogues between real users and an SDS in the domain of restaurantseeking. We compare the sequence-to-sequence model to an agenda-based simulator [16, 11], an n-gram simulator, and a sequence-to-one RNN which takes the same input as the sequence-to-sequence model but chooses an output from among a list of predefined sequences of acts. We show that the RNNbased models outperform the other two simulators on the Fscore measure. We also show on the DSTC3 dataset [19] that the RNN-based models generalize best to new domains.\nIn the next section, we discuss previous models for user simulation. Then, in Section 3, we describe the sequence-tosequence model. Section 4 presents the DSTC2 and DSTC3 corpora, the models and the metrics used for comparison, and the results of our experiments."
    }, {
      "heading" : "2. Background",
      "text" : "User simulation, at the intention level, consists of predicting the next user dialogue act depending on the dialogue history and the user goal. The first user simulator was proposed by Eckert et al. [6] who used a simple bi-gram modelP (au|am) to predict the next user act au given the last system act am. This model does not produce coherent behaviours from the user because the user only reacts to the latest system action. This issue can be overcome by restricting the types of actions that the user can draw from according to dialogue history, which requires more engineering effort. Scheffler and Young [13] proposed a graphbased model. Therein, all possible paths for user behaviour are mapped into a network. The main difficulty of this approach is that it requires extensive domain knowledge and engineering. Pietquin and Dutoit [20] suggested a Bayesian model for user behaviour. They added an explicit representation of the user goal and memory to the probabilistic bi-gram model. The user’s action was then conditioned on her goal and memory. Georgila et al. [14] proposed a richer model of the user with the information state approach [21]. The information state carries information on the current state, the dialogue history and ongoing actions. The authors investigated learning user behaviour by using a 4-gram representation and a linear combination to map each state to a vector of features. Cuayáhuitl et al. [15] used a hidden Markov model (HMM) for user simulation. The model generated both user and system actions. Schatzmann et al. [16] proposed a new agenda-based approach that did not necessarily need training data but could be trained in case such data was available [11]. Chandramohan et al. [9] proposed to model the ar X iv :1 60 7. 00 07 0v 1\n[ cs\n.C L\n] 3\n0 Ju\nn 20\n16\nuser as a decision-making agent and to model user behaviour with reinforcement learning.\nAn important feature for user simulation, which encourages coherent behaviour throughout a dialogue, is the ability to take into account the dialogue history. For tractability reasons, previous models do not account for a long dialogue history. Another important consideration is that users who interact with SDS often utter several dialogue acts during a single dialogue turn. This feature is not often represented in user simulators, as it would quickly become inefficient to compute a model with an output space containing all possible sequences of dialogue acts. To deal with this, in the agenda-based approach, a stacklike structure is added to the model to provide a coherent set of dialogue acts that can be output at the same time. In the next section, we propose a model that takes into account the entire dialogue history and outputs a sequence of dialogue acts without relying on any external structure."
    }, {
      "heading" : "3. The Sequence-to-sequence Model",
      "text" : "Figure 1 represents the sequence-to-sequence user simulation model. The model takes as input a sequence of dialogue contexts (c1, c2, ..., ck) and outputs a sequence of actions (a1, a2, ..., al).\nSimilarly to Schatzmann et al. [16, 11], at the beginning of each dialogue, we uniformly draw a goal G = (C,R) where C is a set of constraints and R is a set of requests. For a restaurant-seeking system, constraints are typically expressed over the type of food, the price range, and the area where the restaurant is located. Requests can include these slots as well as the restaurant’s name, its address, its phone number, etc.\nA context ct at turn t is defined by the following components:\n• the most recent machine acts am,t,\n• the inconsistency between the most recent information provided by the machine and the user goal inconsistt,\n• the constraints status (informed or not) constt, and\n• the requests status (informed or not) reqt.\nThe machine acts are encoded as a vector am,t of size nma 1. The vector am,t has ones for the current machine acts\n1where nma is the number of possible machine acts.\nand zeros everywhere else. The inconsistency is composed of two vectors whose size equals the number of possible constraints nc. Both vectors are initialized at 0 and reset after each turn. After the machine makes a proposition to the user (e.g., a proposition of restaurant), all of the constraint slots which are in the user goal but which were not mentioned by the machine are set to 1 in the first vector. Every time the machine mentions a slot provided by the user (e.g., in a confirmation or a proposition), all of the constraint slots which have been misunderstood are set to 1 in the second vector. The inconsistency vector is thus a turn-level vector which models the system’s understanding of the user goal. The constraints status vector is of size nc and keeps track of what the user has said to the machine. The constraints which are not in C are set to 1 and those in C are set to 0. Every time the user provides a constraint to the SDS, this constraint is set to 1. A constraint is reset to 0 every time it is set to 1 in the inconsistency vector or if the machine requests this slot. The requests status vector is of size nr where nr is the number of possible requests. This vector has ones for all slots which are not in the user goal and zeros for the slots in R. A request slot is set to 1 every time the SDS mentions it in a proposition. The requests status vector is reset after each new proposition from the system. Examples of updates are given in Table 1. At time t, the sequence-to-sequence model takes as input the entire sequence of contexts that have been observed\nso far, which models dialogue history. This input is passed to an RNN which outputs a single vector vt corresponding to the model’s internal representation of dialogue history. The encoder and the decoder have similar structures. They are both based on a Long Short-Term Memory (LSTM) [22]. In both cases, the LSTM is followed by a fully connected layer. The input of the encoder is a sequence of contexts ct:\nct = am,t inconsistt constt reqt,\nwhere is concatenation. The LSTM are implemented following these equations:\nit = σ(Wict + Uiht−1)\nft = σ(Wfct + Ufht−1)\nCt = it ∗ tanh(Wcct + Ucht−1) + ft ∗ Ct−1 ot = σ(Woxt + Uoht−1)\nht = ot ∗ tanh(Ct), (1)\nwhere it is the input gate, σ is the sigmoid function, ft is the forget gate, ot is the output gate, Ct is the cell gate and ht is the hidden state. The last output of the LSTM is passed to one layer fully connected which outputs vt. Then, vt is used to initialize the decoder LSTM at each time step [23]. During training, the decoder is fed with ground truth, i.e., the sequences of user acts observed in the dataset given the history of contexts. During runtime, the only input to the decoder is the null action. The decoder is implemented according to the same equations as the encoder. It is followed by softmax activation in order to compute a distribution of probabilities over the actions. The first action at,1 is drawn according to the output distribution of the first step of the LSTM. Then it is fed as input to the second step. This process is repeated until a sequence of l actions (at,1, ..., at,l) (including one or more null actions at the end of the sequence) has been generated. We train this model with a categorical cross-entropy loss function.\nEach sequence output by the simulator is a sequence of dialogue acts, e.g., (inform, request). We map these dialogue acts to actions such as inform(type of food = Chinese), request(price range) by looking at the current user goal and uniformly drawing among the constraints left to inform and the requests left to ask. In the case of a confirmation asked by the system or if the system misunderstood a slot, we map the inform dialogue act to the slot in question. We show in the following section that it is also possible to train the model on original actions directly, e.g., request-area, which removes this post-processing step and models user behaviour at a finer level."
    }, {
      "heading" : "4. Experiments",
      "text" : "In this section, we compare the sequence-to-sequence simulator to an agenda-based simulator, a sequence-to-one model, and an n-gram model. We train these models on the training set of DSTC2.\nWe define a user compound act ãut as a sequence of dialogue acts (aut,1, ..., aut,l), where l ≥ 1. All the models compared in this section output user compound acts. Similarly, we define machine compound acts as ãmt ."
    }, {
      "heading" : "4.1. User Simulation Models",
      "text" : "The first baseline for comparison is a simple bi-gram model, which outputs a compound act ãut given the last machine com-\npound act ãmt . We compute probabilities for the 54 possible user compound acts in the DSTC2 dataset.\nIn the agenda-based model, the user is modelled with a pair (G,A), where G is the goal and A is the agenda. As explained in Section 3, the goal is a pair (C,R), where C is a set of constraints and R is a set of requests. The agenda A is a stack-like structure which contains all of the inform and request acts needed by the user in order to perform her goal.2 At each dialogue turn t, the user simulator samples a single act aut based on the current dialogue context dt3. Then, based on the chosen act aut , the user simulator samples the number n of acts to pop from the stack. The compound act ãut is then formed by aut and the acts that are popped from the stack. The dialogue context dt does not only include the latest dialogue acts spoken by the system, it also includes information on the dialogue history. For instance, if the SDS proposes a restaurant to the user and, in another dialogue turn, answers one of the user’s requests regarding this restaurant, dt will include an indication over the goal status for this restaurant. The dialogue contexts combined with the agenda guarantee coherent user behaviour throughout the dialogue. This feature, as well as the fact that the model outputs one or several dialogue acts at each turn, makes this a good model for comparison with the sequence-to-sequence approach.\nThe third simulator is a sequence-to-one model. This model takes the same input as the sequence-to-sequence model but only outputs a probability distribution over a predefined set of compound acts. This set of size 54 contains all of the compound acts in DSTC2."
    }, {
      "heading" : "4.2. F-score",
      "text" : "We compare the 4 models based on F-score. The F-score is the geometric mean of the precision and the recall, which are computed as follows:\nprecision = number of correctly predicted dialogue acts\nnumber of predicted dialogue acts\nrecall = number of correctly predicted dialogue acts\nnumber of dialogue acts in the corpus\nF-score = 2× precision× recall precision + recall ."
    }, {
      "heading" : "4.3. The DSTC2 dataset",
      "text" : "DSTC2 is a publicly available dataset composed of a training set of 1612 dialogues, a validation set of 506 dialogues and a test set of 1117 dialogues. The training and validation sets were collected with two handcrafted policy managers whereas a statistical policy manager was used for the test set. The dialogues were collected with real users who had been given a goal consisting of a set of constraints and a set of requests. Each user interacted with the system in order to find a restaurant matching all of the constraints and then to collect the information in the requests. The user dialogue acts tagged in this dataset are as follows: deny, null (empty act), request more, confirm, acknowledge, affirm, request, inform, thank, repeat, request alternative (ask for an-\n2If the user is looking for an Indian restaurant downtown and wants to know the price range, the agenda will be: inform(food = Indian), inform(area = downtown), request(price range).\n3Since the dialogue contexts are not expressed in the same way for the sequence-to-sequence model and the agenda-based model, we use different notations.\nother option), negate, goodbye, hello and restart (ask the system to restart the dialogue).\nThis dataset offers an interesting setting since we can use both the validation and test sets in order to evaluate the user simulators. In general, a user simulator is designed for a given policy manager: data is collected with this manager then the user simulator model is trained on this data and evaluated with the same policy manager. With this dataset, we have the possibility to follow this methodology (on the validation set) but we are also able to evaluate on a set of dialogues on the same domain but collected with a different policy manager (the test set). Therefore, we can evaluate the extent to which each model captures the behaviour of real users in unseen settings for the same task."
    }, {
      "heading" : "4.4. Results",
      "text" : "Table 2 presents results on the validation and test sets of DSTC2. The first observation is that, as expected, the bi-gram model performs relatively poorly. On both the validation and test sets, the RNN-based models significantly outperform the agenda-based model in terms of F-score. The sequence-to-one model performs slightly better than the sequence-to-sequence model because it is a simpler problem to learn a distribution over a given set of sequences than to output each sequence step by step. However, the sequence-to-sequence model performs very closely to the sequence-to-one simulator, demonstrating that this model can achieve good performance. In addition, a considerable advantage of this model concerns scalability. In particular, the number of possible compound acts might grow considerably if the sets of constraint and request slots were of larger size and/or if the number of dialogue acts was larger. The output space would rapidly become too large for training the sequence-to-one model on a small dataset and it would likely be more efficient to use the sequence-to-sequence model. A further advantage is that the sequence-to-sequence model can be used on the original act space.\nWe illustrated this property with a second experiment, in which we modify the sequence-to-sequence model to train it on the original action space. The dialogue acts generated by the simulator are uniformly mapped to the user goal as discussed in Section 3. In this experiment, we circumvent this random mapping by increasing the number of possible acts. Instead of having one inform dialogue act, we define three separate acts: inform food, inform pricerange, inform area. The advantage of this format is that a mapper is no longer needed and users can be modelled at finer granularity. Indeed, as shown in Table 3, it is possible to learn the order in which constraint and request slots are provided to the system by users. For instance, in the case that the user goal includes food, area and price range, the encoder-decoder model learns, in proportions commensurate with those found in the corpus, that the food slot is most often preferred as the first slot (72% in the corpus, 48% for the simulator), then the price range (16% vs. 31%), and then the area (12% vs. 21%).\nThe last experiment involves evaluating the simulators on the DSTC3 test set [19]. DSTC3 is a dataset of 2264 dialogues with a system that can search for restaurants, pubs and coffee shops. Compared to DSTC2, in this dataset, the number of possible constraints is increased with the following slots: children allowed, has internet, has tv, near (e.g., nearby Queens college) and type (restaurant, pub or coffee shop). The user and system dialogue acts can easily be mapped to those in DSTC2. We use this dataset in order to evaluate the user simulators on a new, larger domain. We train the models on DSTC2 as before, and evaluate them on the DSTC3 test set based on F-score. The results are presented in Table 2. These show that the sequenceto-one and sequence-to-sequence models significantly outperform the agenda-based model. Compared to DSTC2, there is a degradation in F-score which can be explained by the fact that this new domain has a larger set of compound acts (we found 40 compound acts which never occurred in DSTC2). The degradation concerns mostly the recall. Notably, the F-score for these models is similar to the F-score of the agenda-based model on the test set of DSTC2."
    }, {
      "heading" : "5. Conclusions",
      "text" : "We proposed a new sequence-to-sequence model for user simulation in spoken dialogue systems. Compared to previous models, this simulator takes into account the entire dialogue history, it does not rely on any external data structure to ensure coherent user behaviour, and it does not require mapping to a summarized action space, which makes it able to model user behaviour with finer granularity. We showed that this model outperforms a state-of-the-art simulator based on the F-score measure. We also showed that it can be efficiently transferred to a new information-seeking domain. In future work, we will use the model to train a statistical spoken dialogue system and further explore the potential of this architecture."
    }, {
      "heading" : "6. References",
      "text" : "[1] E. Levin, R. Pieraccini, and W. Eckert, “Learning dialogue strate-\ngies within the markov decision process framework,” in Proc. of IEEE ASRU, 1997.\n[2] M. Gašić, F. Jurčı́ček, B. Thomson, K. Yu, and S. Young, “On-line policy optimisation of spoken dialogue systems via live interaction with human subjects,” in Proc. of IEEE ASRU, 2011.\n[3] L. Daubigney, M. Geist, S. Chandramohan, and O. Pietquin, “A Comprehensive Reinforcement Learning Framework for Dialogue Management Optimisation,” IEEE Journal of Selected Topics in Signal Processing, vol. 6, no. 8, pp. 891–902, 2012.\n[4] O. Pietquin, M. Geist, S. Chandramohan, and H. Frezza-Buet, “Sample-efficient batch reinforcement learning for dialogue management optimization,” ACM Transaction on Speech and Language Processing, vol. 7, no. 3, pp. 1–21, 2011.\n[5] M. Gašić, M. Henderson, B. Thomson, P. Tsiakoulis, and S. Young, “Policy optimisation of pomdp-based dialogue systems without state space compression,” in Proc. of SLT, 2012.\n[6] W. Eckert, E. Levin, and R. Pieraccini, “User modeling for spoken dialogue system evaluation,” in Proc. of IEEE ASRU, 1997, pp. 80–87.\n[7] K. Georgila, J. Henderson, and O. Lemon, “User simulation for spoken dialogue systems: Learning and evaluation,” in Proc. of Interspeech, 2006.\n[8] J. Schatzmann, K. Weilhammer, M. Stuttle, and S. Young, “A survey of statistical user simulation techniques for reinforcementlearning of dialogue management strategies,” The Knowledge Engineering Review, vol. 21, no. 2, 2006.\n[9] S. Chandramohan, M. Geist, F. Lefèvre, and O. Pietquin, “User simulation in dialogue systems using inverse reinforcement learning,” in Proc. of Interspeech, 2011.\n[10] O. Pietquin and H. Hastie, “A survey on metrics for the evaluation of user simulations,” Knowledge Engineering Review, vol. 28, no. 01, pp. 59–73, 2013.\n[11] J. Schatzmann, B. Thomson, and S. Young, “Statistical user simulation with a hidden agenda,” in Proc. of SIGDIAL, 2007.\n[12] S. Jung, C. Lee, K. Kim, M. Jeong, and G. G. Lee, “Data-driven user simulation for automated evaluation of spoken dialog systems,” Computer Speech and Language, vol. 23, no. 4, pp. 479– 509, 2009.\n[13] K. Scheffler and S. J. Young, “Automatic learning of dialogue strategy using dialogue simulation and reinforcement learning,” in Proc. of HLT, 2002, pp. 12–18.\n[14] K. Georgila, J. Henderson, and O. Lemon, “Learning user simulations for information state update dialogue systems,” in Proc. of Eurospeech, 2005.\n[15] H. Cuayáhuitl, S. Renals, O. Lemon, and H. Shimodaira, “Human-computer dialogue simulation using hidden markov models,” in Proc. of ASRU, 2005, pp. 290–295.\n[16] J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye, and S. Young, “Agenda-based user simulation for bootstrapping a POMDP dialogue system,” in Proc. of HLT, 2007.\n[17] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning with neural networks,” CoRR, 2014.\n[18] M. Henderson, B. Thomson, and J. Williams, “The Second Dialog State Tracking Challenge,” in Proceedings of SIGDIAL, 2014.\n[19] ——, “The Third Dialog State Tracking Challenge,” in Proc. of IEEE SLT, 2014.\n[20] O. Pietquin and T. Dutoit, “A Probabilistic Framework for Dialog Simulation and Optimal Strategy Learning,” IEEE Transactions on Audio, Speech and Language, vol. 14, no. 2, pp. 589–599, 2006.\n[21] S. Larsson and D. Traum, “Information state and dialogue management in the trindi dialogue move engine toolkit,” Natural Language Engineering, vol. 6, pp. 323–340, 2000.\n[22] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural Computation, vol. 9, no. 8, pp. 1735–1780, 1997.\n[23] K. Cho, B. van Merrienboer, Ç. Gülçehre, F. Bougares, H. Schwenk, and Y. Bengio, “Learning phrase representations using RNN encoder-decoder for statistical machine translation,” CoRR, 2014."
    } ],
    "references" : [ {
      "title" : "Learning dialogue strategies within the markov decision process framework",
      "author" : [ "E. Levin", "R. Pieraccini", "W. Eckert" ],
      "venue" : "Proc. of IEEE ASRU, 1997.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "On-line policy optimisation of spoken dialogue systems via live interaction with human subjects",
      "author" : [ "M. Gašić", "F. Jurčı́ček", "B. Thomson", "K. Yu", "S. Young" ],
      "venue" : "Proc. of IEEE ASRU, 2011.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "A Comprehensive Reinforcement Learning Framework for Dialogue Management Optimisation",
      "author" : [ "L. Daubigney", "M. Geist", "S. Chandramohan", "O. Pietquin" ],
      "venue" : "IEEE Journal of Selected Topics in Signal Processing, vol. 6, no. 8, pp. 891–902, 2012.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Sample-efficient batch reinforcement learning for dialogue management optimization",
      "author" : [ "O. Pietquin", "M. Geist", "S. Chandramohan", "H. Frezza-Buet" ],
      "venue" : "ACM Transaction on Speech and Language Processing, vol. 7, no. 3, pp. 1–21, 2011.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Policy optimisation of pomdp-based dialogue systems without state space compression",
      "author" : [ "M. Gašić", "M. Henderson", "B. Thomson", "P. Tsiakoulis", "S. Young" ],
      "venue" : "Proc. of SLT, 2012.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "User modeling for spoken dialogue system evaluation",
      "author" : [ "W. Eckert", "E. Levin", "R. Pieraccini" ],
      "venue" : "Proc. of IEEE ASRU, 1997, pp. 80–87.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "User simulation for spoken dialogue systems: Learning and evaluation",
      "author" : [ "K. Georgila", "J. Henderson", "O. Lemon" ],
      "venue" : "Proc. of Interspeech, 2006.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "A survey of statistical user simulation techniques for reinforcementlearning of dialogue management strategies",
      "author" : [ "J. Schatzmann", "K. Weilhammer", "M. Stuttle", "S. Young" ],
      "venue" : "The Knowledge Engineering Review, vol. 21, no. 2, 2006.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "User simulation in dialogue systems using inverse reinforcement learning",
      "author" : [ "S. Chandramohan", "M. Geist", "F. Lefèvre", "O. Pietquin" ],
      "venue" : "Proc. of Interspeech, 2011.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "A survey on metrics for the evaluation of user simulations",
      "author" : [ "O. Pietquin", "H. Hastie" ],
      "venue" : "Knowledge Engineering Review, vol. 28, no. 01, pp. 59–73, 2013.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Statistical user simulation with a hidden agenda",
      "author" : [ "J. Schatzmann", "B. Thomson", "S. Young" ],
      "venue" : "Proc. of SIGDIAL, 2007.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Data-driven user simulation for automated evaluation of spoken dialog systems",
      "author" : [ "S. Jung", "C. Lee", "K. Kim", "M. Jeong", "G.G. Lee" ],
      "venue" : "Computer Speech and Language, vol. 23, no. 4, pp. 479– 509, 2009.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Automatic learning of dialogue strategy using dialogue simulation and reinforcement learning",
      "author" : [ "K. Scheffler", "S.J. Young" ],
      "venue" : "Proc. of HLT, 2002, pp. 12–18.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Learning user simulations for information state update dialogue systems",
      "author" : [ "K. Georgila", "J. Henderson", "O. Lemon" ],
      "venue" : "Proc. of Eurospeech, 2005.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Human-computer dialogue simulation using hidden markov models",
      "author" : [ "H. Cuayáhuitl", "S. Renals", "O. Lemon", "H. Shimodaira" ],
      "venue" : "Proc. of ASRU, 2005, pp. 290–295.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Agenda-based user simulation for bootstrapping a POMDP dialogue system",
      "author" : [ "J. Schatzmann", "B. Thomson", "K. Weilhammer", "H. Ye", "S. Young" ],
      "venue" : "Proc. of HLT, 2007.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "I. Sutskever", "O. Vinyals", "Q.V. Le" ],
      "venue" : "CoRR, 2014.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "The Second Dialog State Tracking Challenge",
      "author" : [ "M. Henderson", "B. Thomson", "J. Williams" ],
      "venue" : "Proceedings of SIGDIAL, 2014.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "The Third Dialog State Tracking Challenge",
      "author" : [ "——" ],
      "venue" : "Proc. of IEEE SLT, 2014.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A Probabilistic Framework for Dialog Simulation and Optimal Strategy Learning",
      "author" : [ "O. Pietquin", "T. Dutoit" ],
      "venue" : "IEEE Transactions on Audio, Speech and Language, vol. 14, no. 2, pp. 589–599, 2006.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Information state and dialogue management in the trindi dialogue move engine toolkit",
      "author" : [ "S. Larsson", "D. Traum" ],
      "venue" : "Natural Language Engineering, vol. 6, pp. 323–340, 2000.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Long short-term memory",
      "author" : [ "S. Hochreiter", "J. Schmidhuber" ],
      "venue" : "Neural Computation, vol. 9, no. 8, pp. 1735–1780, 1997.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Learning phrase representations using RNN encoder-decoder for statistical machine translation",
      "author" : [ "K. Cho", "B. van Merrienboer", "Ç. Gülçehre", "F. Bougares", "H. Schwenk", "Y. Bengio" ],
      "venue" : "CoRR, 2014.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Statistical Spoken Dialogue Systems (SDS) [1, 2, 3] typically require several thousands of dialogues to learn a good dialogue strategy [4, 5].",
      "startOffset" : 42,
      "endOffset" : 51
    }, {
      "referenceID" : 1,
      "context" : "Statistical Spoken Dialogue Systems (SDS) [1, 2, 3] typically require several thousands of dialogues to learn a good dialogue strategy [4, 5].",
      "startOffset" : 42,
      "endOffset" : 51
    }, {
      "referenceID" : 2,
      "context" : "Statistical Spoken Dialogue Systems (SDS) [1, 2, 3] typically require several thousands of dialogues to learn a good dialogue strategy [4, 5].",
      "startOffset" : 42,
      "endOffset" : 51
    }, {
      "referenceID" : 3,
      "context" : "Statistical Spoken Dialogue Systems (SDS) [1, 2, 3] typically require several thousands of dialogues to learn a good dialogue strategy [4, 5].",
      "startOffset" : 135,
      "endOffset" : 141
    }, {
      "referenceID" : 4,
      "context" : "Statistical Spoken Dialogue Systems (SDS) [1, 2, 3] typically require several thousands of dialogues to learn a good dialogue strategy [4, 5].",
      "startOffset" : 135,
      "endOffset" : 141
    }, {
      "referenceID" : 5,
      "context" : "It is costly to collect this quantity of dialogues; therefore, research has turned to user simulation [6, 7, 8, 9].",
      "startOffset" : 102,
      "endOffset" : 114
    }, {
      "referenceID" : 6,
      "context" : "It is costly to collect this quantity of dialogues; therefore, research has turned to user simulation [6, 7, 8, 9].",
      "startOffset" : 102,
      "endOffset" : 114
    }, {
      "referenceID" : 7,
      "context" : "It is costly to collect this quantity of dialogues; therefore, research has turned to user simulation [6, 7, 8, 9].",
      "startOffset" : 102,
      "endOffset" : 114
    }, {
      "referenceID" : 8,
      "context" : "It is costly to collect this quantity of dialogues; therefore, research has turned to user simulation [6, 7, 8, 9].",
      "startOffset" : 102,
      "endOffset" : 114
    }, {
      "referenceID" : 9,
      "context" : "A user simulator is expected to have the following properties: to be statistically consistent with real users, to generate coherent sequences of actions, and to generalize to new contexts [10].",
      "startOffset" : 188,
      "endOffset" : 192
    }, {
      "referenceID" : 10,
      "context" : ", generating dialogue acts [11, 9], or at the utterance level [12].",
      "startOffset" : 27,
      "endOffset" : 34
    }, {
      "referenceID" : 8,
      "context" : ", generating dialogue acts [11, 9], or at the utterance level [12].",
      "startOffset" : 27,
      "endOffset" : 34
    }, {
      "referenceID" : 11,
      "context" : ", generating dialogue acts [11, 9], or at the utterance level [12].",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 5,
      "context" : "Many models have been designed in order to meet the requirements cited above [6, 13, 14, 15, 11, 9].",
      "startOffset" : 77,
      "endOffset" : 99
    }, {
      "referenceID" : 12,
      "context" : "Many models have been designed in order to meet the requirements cited above [6, 13, 14, 15, 11, 9].",
      "startOffset" : 77,
      "endOffset" : 99
    }, {
      "referenceID" : 13,
      "context" : "Many models have been designed in order to meet the requirements cited above [6, 13, 14, 15, 11, 9].",
      "startOffset" : 77,
      "endOffset" : 99
    }, {
      "referenceID" : 14,
      "context" : "Many models have been designed in order to meet the requirements cited above [6, 13, 14, 15, 11, 9].",
      "startOffset" : 77,
      "endOffset" : 99
    }, {
      "referenceID" : 10,
      "context" : "Many models have been designed in order to meet the requirements cited above [6, 13, 14, 15, 11, 9].",
      "startOffset" : 77,
      "endOffset" : 99
    }, {
      "referenceID" : 8,
      "context" : "Many models have been designed in order to meet the requirements cited above [6, 13, 14, 15, 11, 9].",
      "startOffset" : 77,
      "endOffset" : 99
    }, {
      "referenceID" : 5,
      "context" : "These models typically suffer from important drawbacks, which include the inability to take dialogue history into account [6], the need of rigid structure to ensure coherent user behaviour [16], heavy dependence on a specific domain [13], the inability to output several user intentions during one dialogue turn [12], or the requirement of a summarized action space for tractability [11].",
      "startOffset" : 122,
      "endOffset" : 125
    }, {
      "referenceID" : 15,
      "context" : "These models typically suffer from important drawbacks, which include the inability to take dialogue history into account [6], the need of rigid structure to ensure coherent user behaviour [16], heavy dependence on a specific domain [13], the inability to output several user intentions during one dialogue turn [12], or the requirement of a summarized action space for tractability [11].",
      "startOffset" : 189,
      "endOffset" : 193
    }, {
      "referenceID" : 12,
      "context" : "These models typically suffer from important drawbacks, which include the inability to take dialogue history into account [6], the need of rigid structure to ensure coherent user behaviour [16], heavy dependence on a specific domain [13], the inability to output several user intentions during one dialogue turn [12], or the requirement of a summarized action space for tractability [11].",
      "startOffset" : 233,
      "endOffset" : 237
    }, {
      "referenceID" : 11,
      "context" : "These models typically suffer from important drawbacks, which include the inability to take dialogue history into account [6], the need of rigid structure to ensure coherent user behaviour [16], heavy dependence on a specific domain [13], the inability to output several user intentions during one dialogue turn [12], or the requirement of a summarized action space for tractability [11].",
      "startOffset" : 312,
      "endOffset" : 316
    }, {
      "referenceID" : 10,
      "context" : "These models typically suffer from important drawbacks, which include the inability to take dialogue history into account [6], the need of rigid structure to ensure coherent user behaviour [16], heavy dependence on a specific domain [13], the inability to output several user intentions during one dialogue turn [12], or the requirement of a summarized action space for tractability [11].",
      "startOffset" : 383,
      "endOffset" : 387
    }, {
      "referenceID" : 16,
      "context" : "The simulator is modelled with an encoder Recurrent Neural Network (RNN) and a decoder RNN [17].",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 17,
      "context" : "We train this model on the Dialogue State Tracking Challenge 2 (DSTC2) [18] dataset.",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 15,
      "context" : "We compare the sequence-to-sequence model to an agenda-based simulator [16, 11], an n-gram simulator, and a sequence-to-one RNN which takes the same input as the sequence-to-sequence model but chooses an output from among a list of predefined sequences of acts.",
      "startOffset" : 71,
      "endOffset" : 79
    }, {
      "referenceID" : 10,
      "context" : "We compare the sequence-to-sequence model to an agenda-based simulator [16, 11], an n-gram simulator, and a sequence-to-one RNN which takes the same input as the sequence-to-sequence model but chooses an output from among a list of predefined sequences of acts.",
      "startOffset" : 71,
      "endOffset" : 79
    }, {
      "referenceID" : 18,
      "context" : "We also show on the DSTC3 dataset [19] that the RNN-based models generalize best to new domains.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 5,
      "context" : "[6] who used a simple bi-gram modelP (au|am) to predict the next user act au given the last system act am.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 12,
      "context" : "Scheffler and Young [13] proposed a graphbased model.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 19,
      "context" : "Pietquin and Dutoit [20] suggested a Bayesian model for user behaviour.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 13,
      "context" : "[14] proposed a richer model of the user with the information state approach [21].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "[14] proposed a richer model of the user with the information state approach [21].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 14,
      "context" : "[15] used a hidden Markov model (HMM) for user simulation.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "[16] proposed a new agenda-based approach that did not necessarily need training data but could be trained in case such data was available [11].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[16] proposed a new agenda-based approach that did not necessarily need training data but could be trained in case such data was available [11].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 8,
      "context" : "[9] proposed to model the ar X iv :1 60 7.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 15,
      "context" : "[16, 11], at the beginning of each dialogue, we uniformly draw a goal G = (C,R) where C is a set of constraints and R is a set of requests.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 10,
      "context" : "[16, 11], at the beginning of each dialogue, we uniformly draw a goal G = (C,R) where C is a set of constraints and R is a set of requests.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 21,
      "context" : "They are both based on a Long Short-Term Memory (LSTM) [22].",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 22,
      "context" : "Then, vt is used to initialize the decoder LSTM at each time step [23].",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 18,
      "context" : "The last experiment involves evaluating the simulators on the DSTC3 test set [19].",
      "startOffset" : 77,
      "endOffset" : 81
    } ],
    "year" : 2016,
    "abstractText" : "User simulation is essential for generating enough data to train a statistical spoken dialogue system. Previous models for user simulation suffer from several drawbacks, such as the inability to take dialogue history into account, the need of rigid structure to ensure coherent user behaviour, heavy dependence on a specific domain, the inability to output several user intentions during one dialogue turn, or the requirement of a summarized action space for tractability. This paper introduces a data-driven user simulator based on an encoder-decoder recurrent neural network. The model takes as input a sequence of dialogue contexts and outputs a sequence of dialogue acts corresponding to user intentions. The dialogue contexts include information about the machine acts and the status of the user goal. We show on the Dialogue State Tracking Challenge 2 (DSTC2) dataset that the sequence-to-sequence model outperforms an agendabased simulator and an n-gram simulator, according to F-score. Furthermore, we show how this model can be used on the original action space and thereby models user behaviour with finer granularity.",
    "creator" : "LaTeX with hyperref package"
  }
}