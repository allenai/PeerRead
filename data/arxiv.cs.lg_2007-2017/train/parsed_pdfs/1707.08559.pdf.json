{
  "name" : "1707.08559.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Video Highlight Prediction Using Audience Chat Reactions",
    "authors" : [ "Cheng-Yang Fu", "Joon Lee", "Mohit Bansal", "Alexander C. Berg" ],
    "emails" : [ "aberg}@cs.unc.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "On-line eSports events provide a new setting for observing large-scale social interaction focused on a visual story that evolves over time—a video game. While watching sporting competitions has been a major source of entertainment for millennia, and is a significant part of today’s culture, eSports brings this to a new level on several fronts. One is the global reach, the same games are played around the world and across cultures by speakers of several languages. Another is the scale of on-line text-based discourse during matches that is public and amendable to analysis. One of the most popular games, League of Legends, drew 43 million views for the 2016 world series final matches (broadcast in 18 languages) and a peak concurrent viewership of 14.7 million1. Finally, players interact through what they see on screen while fans (and researchers) can see exactly the same views.\n1 http://www.lolesports.com/en_US/articles/\n2016-league-legends-world-championship-numbers\nThis paper builds on the wealth of interaction around eSports to develop predictive models for match video highlights based on the audience’s online chat discourse as well as the visual recordings of matches themselves. ESports journalists and fans create highlight videos of important moments in matches. Using these as ground truth, we explore automatic prediction of highlights via multimodal CNN+RNN models for multiple languages. Appealingly this task is natural, as the community already produces the ground truth and is global, allowing multilingual multimodal grounding.\nHighlight prediction is about capturing the exciting moments in a specific video (a game match in this case), and depends on the context, the state of play, and the players. This task of predicting the exciting moments is hence different from summarizing the entire match into a story summary. Hence, highlight prediction can benefit from the available real-time text commentary from fans, which is valuable in exposing more abstract background context, that may not be accessible with\nar X\niv :1\n70 7.\n08 55\n9v 1\n[ cs\n.C L\n] 2\n6 Ju\nl 2 01\n7\ncomputer vision techniques that can easily identify some aspects of the state of play. As an example, computer vision may not understand why Michael Jordan’s dunk is a highlight over that of another player, but concurrent fan commentary might reveal this.\nWe collect our dataset from Twitch.tv, one of the live-streaming platforms that integrates comments (see Fig. 1), and the largest live-streaming platform for video games. We record matches of the game League of Legends (LOL), one of the largest eSports game in two subsets, 1) the spring season of the North American League of Legends Championship Series (NALCS), and 2) the League of Legends Master Series (LMS) hosted in Taiwan/Macau/HongKong, with chat comments in English and traditional Chinese respectively. We use the community created highlights to label each frame of a match as highlight or not.\nIn addition to our new dataset, we present several experiments with multilingual characterbased models, deep-learning based vision models either per-frame or tied together with a videosequence LSTM-RNN, and combinations of language and vision models. Our results indicate that while surprisingly the visual models generally outperform language-based models, we can still build reasonably useful language models that help disambiguate difficult cases for vision models, and that combining the two sources is the most effective model (across multiple languages)."
    }, {
      "heading" : "2 Related Work",
      "text" : "We briefly discuss a small sample of the related work on language and vision datasets, summarization, and highlight prediction. There has been a surge of vision and language datasets focusing on captions over the last few years, (Rashtchian et al., 2010; Ordonez et al., 2011; Lin et al., 2014), followed by efforts to focus on more specific parts of images (Krishna et al., 2016), or referring expressions (Kazemzadeh et al., 2014), or on the broader context (Huang et al., 2016). For video, similar efforts have collected descriptions (Chen and Dolan, 2011), while others use existing descriptive video service (DVS) sources (Rohrbach et al., 2015; Torabi et al., 2015). Beyond descriptions, other datasets use questions to relate images and language (Antol et al., 2015; Yu et al., 2015). This approach is extended to movies in Tapaswi et al. (2016).\nThe related problem of visually summarizing videos (as opposed to finding the highlights) has produced datasets of holiday and sports events with multiple users making summary videos (Gygli et al., 2014) and multiple users selecting summary key-frames (de Avila et al., 2011) from short videos. For language-based summarization, Extractive models (Filippova and Altun, 2013; Filippova et al., 2015) generate summaries by selecting important sentences and then assembling these, while Abstractive models (Chopra et al., 2016; Mei et al., 2016; Nallapati et al., 2016; See et al., 2017) generate/rewrite the summaries from scratch.\nCloser to our setting, there has been work on highlight prediction in football (soccer) and basketball based on audio of broadcasts (Cheng and Hsu, 2006) (Wang et al., 2004) where commentators may have an outsized impact or visual features (Bertini et al., 2005). In the spirit of our study, there has been work looking at tweets during sporting events (Hsieh et al., 2012), but the tweets are not as immediate or as well aligned with the games as the eSports comments. More closely related to our work, Song (2016) collects videos for Heroes of the Storm, League of Legends, and Dota2 on online broadcasting websites of around 327 hours total. They also provide highlight labeling annotated by four annotators. Our method, on the other hand, has a similar scale of data, but we use existing highlights, and we also employ textual audience chat commentary, thus providing a new resource and task for Language and Vision research. In summary, we present the first languagevision dataset for video highlighting that contains audience reactions in chat format, in multiple languages. The community produced ground truth provides labels for each frame and can be used for supervised learning. The language side of this new dataset presents interesting challenges related to real-world Internet-style slang."
    }, {
      "heading" : "3 Data Collection",
      "text" : "Our dataset covers 218 videos from NALCS and 103 from LMS for a total of 321 videos from week 1 to week 9 in 2017 spring series from each tournament. Each week there are 10 matches for NALCS and 6 matches for LMS. Matches are best of 3, so consist of two games or three games. The first and third games are used for training. The second games in the first 4 weeks are used as valida-\ntion and the remainder of second games are used as test. Table 1 lists the numbers of videos in train, validation, and test subsets.\nEach game’s video ranges from 30 to 50 minutes in length which contains image and chat data linked to the specific timestamp of the game. The average number of chats per video is 7490 with a standard deviation of 4922. The high value of standard deviation is mostly due to the fact that NALCS simultaneously broadcasts matches in two different channels (nalcs12 and nalcs23) which often leads to the majority of users watching the channel with a relatively more popular team causing an imbalance in the number of chats. If we only consider LMS which broadcasts with a single channel, the average number of chats are 7210 with standard deviation of 2719. The number of viewers for each game averages about 21526, and the number of unique users who type in chat is on average 2185, i.e., roughly 10% of the viewers.\nHighlight Labeling For each game, we collected community generated highlights ranging from 5 minutes to 7 minutes in length. For the purpose of consistency within our data, we collected the highlights from a single Youtube channel,\n2 https://www.twitch.tv/nalcs1 3 https://www.twitch.tv/nalcs2\nOnivia,4 which provided highlights for both championship tournaments in a consistent arrangement. We expect such consistency will aid our model to better pick up characteristics for determining highlights. We next need to align the position of the frames from the highlight video to frames in the full game video. For this, we adopted a template matching approach. For each frame in the video and the highlight, we divide it into 16 regions of 4 by 4 and use the average value of each color channel in each region as the feature. The feature representation of each frame ends up as a 48-dim vector as shown in Figure 2a. For each frame in the highlight, we can find the most similar frame in the video by calculating distance between these two vectors. However, matching a single frame to another suffers from noise. Therefore, we alternatively concatenate the following frames to form a window and use template matching to find the best matching location in the video. We found out that when the window size is 60 frames, it gives consistent and high quality results. For each frame, the result contains not only the best matching score but also the location of that match in the video.5 Figure 2b illustrates this matching process."
    }, {
      "heading" : "4 Model",
      "text" : "In this section, we explain the proposed models and components. We first describe the notation and definition of the problem, plus the evaluation metric used. Next, we explain our vision model VCNN-LSTM and language model L-Char-LSTM. Finally, we describe the joint multimodal model lv-LSTM.\nProblem Definition Our basic task is to determine if a frame of the full input video should be labeled as being part of the output highlight or not. To simplify our notation, we use X = {x1, x2, ..., xt} to denote a sequence of features for frames. Chats are expressed as C = {(c1, ts1), ..., (cn, tsn)}. where each chat c comes with a timestamp ts. Methods take the image features and/or chats and predict labels for the frames, Y = {y1, y2, ..., yt}. Evaluation Metric: We refer to the set of frames with positive ground truth label as Sgt and the set\n4 https://www.youtube.com/channel/\nUCPhab209KEicqPJFAk9IZEA 5When the window contains a moment of clip transition in highlights, the best matching score appears low. This is used to separate all clips in the highlight. Then we can use the starting and end locations of each clip to label the video.\nof predicted frames with a positive label as Spred. Following (Gygli et al., 2014; Song et al., 2015), we use the harmonic mean F-score in Eq.2 widely used in video summarization task for evaluation:\nP = Sgt ∩ Spred |Spred| , R = Sgt ∩ Spred |Sgt|\n(1)\nF = 2PR\nP +R × 100% (2)\nV-CNN We use the ResNet-34 model (He et al., 2016) to represent frames, motivated by its strong results on the ImageNet Challenge (Russakovsky et al., 2015). Our naive V-CNN model (Figure 3a) uses features from the pre-trained version of this network 6 directly to make prediction at each frame (which are resized to 224x224).\nV-CNN-LSTM In order to exploit visual video information sequentially over time, we use a memory-based LSTM-RNN on top of the image features, so as to model long-term dependencies. All of our videos are 30FPS. As the difference between consecutive frames is usually minor, we run prediction every 10th frame during evaluation and interpolate predictions between these frames. During training, due to the GPU memory constraints, we unfold the LSTM cell 16 times. Therefore the image window size is around 5-seconds (16 samples every 10th frame from 30fps video). The hidden state from the last cell is used as the V-CNNLSTM feature. This process is shown in Figure 3b.\nL-Word-LSTM and L-Char-LSTM Next, we discuss our language-based models using the audience chat text. Word-level LSTM-RNN models (Sutskever et al., 2014) are a common approach to embedding sentences. Unfortunately, this does not fit our Internet-slang style language with irregularities, “mispelled” words (hapy, happppppy), emojis (ˆ ˆ), abbreviations (LOL), marks (?!?!?!?!), or onomatopoeic cases\n6 https://github.com/pytorch/pytorch\n(e.g., 4 which sounds like yes in traditional Chinese). People may type variant length of 4, e.g.,, 4444444 to express their remarks.\nTherefore, alternatively, we model the audience chat with a character-level LSTM-RNN model (Graves, 2013). Characters of the language, Chinese, English, or Emojis, are expanded to multiple ASCII characters according to the two-character Unicode or other representations used on the chat servers. We encode a 1-hot vector for each ASCII input character. For each frame we use all chats that occur in the next Wt seconds which are called text window size to form the input for L-CharLSTM. We concatenate all the chats in a window, separating them by a special stop character, and then fed to a 3-layer L-Char-LSTM model.7 This model is shown in Figure 3c. Following the setting in Sec. 5, we evaluate the text window size from 5 seconds to 9 seconds, and got the following accuracies:32.1%, 29.6%, 41.5%, 28.2%, 34.4%. We achieved best results with text window size as 7 seconds, and used this in rest of the experiments.\nJoint lv-LSTM Model Our final lv-LSTM model combines the best vision and language models: V-CNN-LSTM and L-Char-LSTM. For the vision and language models, we can extract features Fv and Fl from V-CNN-LSTN and LChar-LSTM, respectively. Then we concatenate Fv and Fl, and feed it into a 2-layer MLP. The completed model is shown in Figure 3d. We expect there is room to improve this approach, by using more involved representations, e.g., Bilinear Pooling (Fukui et al., 2016), Memory Networks (Xiong et al., 2016), and Attention Models (Lu et al., 2016); this is future work.\n7The number of these stop characters is then an encoding of the number of chats in the window. Therefore, the L-Char-LSTM could learn to use this #chats information, if it is a useful feature. Also, some content has been deleted by Twitch.tv or the channel itself due to the usage of improper words. We use symbol ”\\n” to replace such cases."
    }, {
      "heading" : "5 Experiments and Results",
      "text" : "Training Details In development and ablation studies, we use train and val splits of the data from NALCS to evaluate models in Section 3. For the final results, models are retrained on the combination of train and val data (following major vision benchmarks e.g. PASCAL-VOC and COCO), and performance is measured on the test set. We separate the highlight prediction to three different tasks based on using different input data: videos, chats, and videos+chats. The details of dataset split are in Section 3. Our code is implemented in PyTorch.\nTo deal with the large number of frames total, we sample only 5k positive and 5k negative examples in each epoch. We use batch size of 32 and run 60 epochs in all experiments. Weight decay is 10−4 and learning rate is set as 10−2 in the first 20 epochs and 10−3 after that. Cross entropy loss is used. Highlights are generated by fans and consist of clips. We match each clip to when it happened in the full match and call this the highlight clip (non-overlapping). The action of interest (kill, objective control, etc.) often happens in the later part of a highlight clip, while the clip contains some additional context before that action that may help set the stage. For some of our experimental settings (Table 2), we used a heuristic of only including the last 25% frames in every highlight clip as positive training examples. During evaluation, we used all frames in the highlight clip.\nAblation Study Table 2 shows the performance of each module separately on the dev set. For the basic L-Char-LSTM and V-CNN models, using only the last 25% of frames in highlight clips in training works best. In order to evaluate the performance of L-Char-LSTM model, we also train a Word-LSTM model by tokenizing all the chats and\nonly considering the words that appeared more than 10 times, which results in 10019 words. We use this vocabulary to encode the words to 1-hot vectors. The L-Char-LSTM outperforms L-WordLSTM by 22.3%.\nTest Results Test results are shown in Table 3. Somewhat surprisingly, the vision only model is more accurate than the language only model, despite the real-time nature of the comment stream. This is perhaps due to the visual form of the game, where highlight events may have similar animations. However, including language with vision in the lv-LSTM model significantly improves over vision alone, as the comments may exhibit additional contextual information. Comparing results between ablation and the final test, it seems more data contributes to higher accuracy. This effect is more apparent in the vision models, perhaps due to complexity. Moreover, L-Char-LSTM performs better in English compared to traditional Chinese. From the numbers given in Section 3, variation in the number of chats in NALCS was much higher than LMS, which one may expect to have a critical effect in the language model. However, our results seem to suggest that the L-Char-LSTM model can pickup other factors of the chat data (e.g. content) instead of just counting the number of chats. We expect a different language model more suitable for the traditional Chinese language should be able to improve the results for the LMS data."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We presented a new dataset and multimodal methods for highlight prediction, based on visual cues and textual audience chat reactions in multiple languages. We hope our new dataset can encourage further multilingual, multimodal research."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank Tamara Berg, Phil Ammirato, and the reviewers for their helpful suggestions, and we acknowledge support from NSF 1533771."
    } ],
    "references" : [ {
      "title" : "VQA: Visual Question Answering",
      "author" : [ "Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh." ],
      "venue" : "ICCV.",
      "citeRegEx" : "Antol et al\\.,? 2015",
      "shortCiteRegEx" : "Antol et al\\.",
      "year" : 2015
    }, {
      "title" : "Vsumm: A mechanism designed to produce static video summaries and a novel evaluation method",
      "author" : [ "Sandra E.F. de Avila", "Ana P.B. Lopes", "Antonio da Luz Jr.", "Arnaldo de A. Arajo." ],
      "venue" : "Pattern Recognition Letters.",
      "citeRegEx" : "Avila et al\\.,? 2011",
      "shortCiteRegEx" : "Avila et al\\.",
      "year" : 2011
    }, {
      "title" : "Soccer videos highlight prediction and annotation in real time",
      "author" : [ "M. Bertini", "A. Del Bimbo", "W. Nunziati." ],
      "venue" : "ICIAP.",
      "citeRegEx" : "Bertini et al\\.,? 2005",
      "shortCiteRegEx" : "Bertini et al\\.",
      "year" : 2005
    }, {
      "title" : "Collecting highly parallel data for paraphrase evaluation",
      "author" : [ "David L. Chen", "William B. Dolan." ],
      "venue" : "ACL.",
      "citeRegEx" : "Chen and Dolan.,? 2011",
      "shortCiteRegEx" : "Chen and Dolan.",
      "year" : 2011
    }, {
      "title" : "Fusion of audio and motion information on hmmbased highlight extraction for baseball games",
      "author" : [ "Chih-Chieh Cheng", "Chiou-Ting Hsu." ],
      "venue" : "IEEE Trans. Multimedia.",
      "citeRegEx" : "Cheng and Hsu.,? 2006",
      "shortCiteRegEx" : "Cheng and Hsu.",
      "year" : 2006
    }, {
      "title" : "Abstractive sentence summarization with attentive recurrent neural networks",
      "author" : [ "Sumit Chopra", "Michael Auli", "Alexander M. Rush." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Chopra et al\\.,? 2016",
      "shortCiteRegEx" : "Chopra et al\\.",
      "year" : 2016
    }, {
      "title" : "Sentence compression by deletion with lstms",
      "author" : [ "Katja Filippova", "Enrique Alfonseca", "Carlos A Colmenares", "Lukasz Kaiser", "Oriol Vinyals." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Filippova et al\\.,? 2015",
      "shortCiteRegEx" : "Filippova et al\\.",
      "year" : 2015
    }, {
      "title" : "The lack of parallel data in sentence compression",
      "author" : [ "Katja Filippova", "Yasemin Altun." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Filippova and Altun.,? 2013",
      "shortCiteRegEx" : "Filippova and Altun.",
      "year" : 2013
    }, {
      "title" : "Multimodal compact bilinear pooling for visual question answering and visual grounding",
      "author" : [ "Akira Fukui", "Dong Huk Park", "Daylen Yang", "Anna Rohrbach", "Trevor Darrell", "Marcus Rohrbach." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Fukui et al\\.,? 2016",
      "shortCiteRegEx" : "Fukui et al\\.",
      "year" : 2016
    }, {
      "title" : "Generating sequences with recurrent neural networks",
      "author" : [ "Alex Graves." ],
      "venue" : "Neural computation.",
      "citeRegEx" : "Graves.,? 2013",
      "shortCiteRegEx" : "Graves.",
      "year" : 2013
    }, {
      "title" : "Creating summaries from user videos",
      "author" : [ "Michael Gygli", "Helmut Grabner", "Hayko Riemenschneider", "Luc Van Gool." ],
      "venue" : "ECCV.",
      "citeRegEx" : "Gygli et al\\.,? 2014",
      "shortCiteRegEx" : "Gygli et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun." ],
      "venue" : "CVPR.",
      "citeRegEx" : "He et al\\.,? 2016",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Live semantic sport highlight detection based on analyzing tweets of twitter",
      "author" : [ "Liang-Chi Hsieh", "Ching-Wei Lee", "Tzu-Hsuan Chiu", "Winston Hsu." ],
      "venue" : "ICME.",
      "citeRegEx" : "Hsieh et al\\.,? 2012",
      "shortCiteRegEx" : "Hsieh et al\\.",
      "year" : 2012
    }, {
      "title" : "Referitgame: Referring to objects in photographs of natural scenes",
      "author" : [ "Sahar Kazemzadeh", "Vicente Ordonez", "Mark Matten", "Tamara Berg." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Kazemzadeh et al\\.,? 2014",
      "shortCiteRegEx" : "Kazemzadeh et al\\.",
      "year" : 2014
    }, {
      "title" : "Visual genome: Connecting language and vision",
      "author" : [ "Ranjay Krishna", "Yuke Zhu", "Oliver Groth", "Justin Johnson", "Kenji Hata", "Joshua Kravitz", "Stephanie Chen", "Yannis Kalantidis", "Li-Jia Li", "David A Shamma", "Michael Bernstein", "Li Fei-Fei" ],
      "venue" : null,
      "citeRegEx" : "Krishna et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Krishna et al\\.",
      "year" : 2016
    }, {
      "title" : "Microsoft coco: Common objects in context",
      "author" : [ "Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Dollr", "C. Lawrence Zitnick." ],
      "venue" : "ECCV.",
      "citeRegEx" : "Lin et al\\.,? 2014",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2014
    }, {
      "title" : "Hierarchical question-image coattention for visual question answering",
      "author" : [ "Jiasen Lu", "Jianwei Yang", "Dhruv Batra", "Devi Parikh." ],
      "venue" : "NIPS.",
      "citeRegEx" : "Lu et al\\.,? 2016",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2016
    }, {
      "title" : "What to talk about and how? selective generation using lstms with coarse-to-fine alignment",
      "author" : [ "Hongyuan Mei", "Mohit Bansal", "Matthew R. Walter." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Mei et al\\.,? 2016",
      "shortCiteRegEx" : "Mei et al\\.",
      "year" : 2016
    }, {
      "title" : "Abstractive text summarization using sequence-to-sequence rnns and beyond",
      "author" : [ "Ramesh Nallapati", "Bowen Zhou", "Caglar Gulcehre", "Bing Xiang" ],
      "venue" : "In CoNLL",
      "citeRegEx" : "Nallapati et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Nallapati et al\\.",
      "year" : 2016
    }, {
      "title" : "Im2text: Describing images using 1 million captioned photographs",
      "author" : [ "Vicente Ordonez", "Girish Kulkarni", "Tamara L Berg." ],
      "venue" : "NIPS.",
      "citeRegEx" : "Ordonez et al\\.,? 2011",
      "shortCiteRegEx" : "Ordonez et al\\.",
      "year" : 2011
    }, {
      "title" : "Collecting image annotations using amazon’s mechanical turk",
      "author" : [ "Cyrus Rashtchian", "Peter Young", "Micah Hodosh", "Julia Hockenmaier." ],
      "venue" : "NAACL HLT workshop.",
      "citeRegEx" : "Rashtchian et al\\.,? 2010",
      "shortCiteRegEx" : "Rashtchian et al\\.",
      "year" : 2010
    }, {
      "title" : "A dataset for movie description",
      "author" : [ "Anna Rohrbach", "Marcus Rohrbach", "Niket Tandon", "Bernt Schiele." ],
      "venue" : "CVPR.",
      "citeRegEx" : "Rohrbach et al\\.,? 2015",
      "shortCiteRegEx" : "Rohrbach et al\\.",
      "year" : 2015
    }, {
      "title" : "Imagenet large scale visual recognition challenge",
      "author" : [ "Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein", "Alexander C. Berg", "Li Fei-Fei." ],
      "venue" : "IJCV.",
      "citeRegEx" : "Russakovsky et al\\.,? 2015",
      "shortCiteRegEx" : "Russakovsky et al\\.",
      "year" : 2015
    }, {
      "title" : "Get to the point: Summarization with pointergenerator networks",
      "author" : [ "Abigail See", "Peter J Liu", "Christopher D Manning." ],
      "venue" : "ACL.",
      "citeRegEx" : "See et al\\.,? 2017",
      "shortCiteRegEx" : "See et al\\.",
      "year" : 2017
    }, {
      "title" : "Real-time video highlights for yahoo esports",
      "author" : [ "Yale Song." ],
      "venue" : "arXiv:1611.08780.",
      "citeRegEx" : "Song.,? 2016",
      "shortCiteRegEx" : "Song.",
      "year" : 2016
    }, {
      "title" : "Tvsum: Summarizing web videos using titles",
      "author" : [ "Yale Song", "Jordi Vallmitjana", "Amanda Stent", "Alejandro Jaimes." ],
      "venue" : "CVPR.",
      "citeRegEx" : "Song et al\\.,? 2015",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2015
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le." ],
      "venue" : "NIPS.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Movieqa: Understanding stories in movies through question-answering",
      "author" : [ "Makarand Tapaswi", "Yukun Zhu", "Rainer Stiefelhagen", "Antonio Torralba", "Raquel Urtasun", "Sanja Fidler." ],
      "venue" : "CVPR.",
      "citeRegEx" : "Tapaswi et al\\.,? 2016",
      "shortCiteRegEx" : "Tapaswi et al\\.",
      "year" : 2016
    }, {
      "title" : "Using descriptive video services to create a large data source for video annotation research",
      "author" : [ "Atousa Torabi", "Christopher Pal", "Hugo Larochelle", "Aaron Courville." ],
      "venue" : "arXiv:1503.01070v1.",
      "citeRegEx" : "Torabi et al\\.,? 2015",
      "shortCiteRegEx" : "Torabi et al\\.",
      "year" : 2015
    }, {
      "title" : "Sports highlight detection from keyword sequences using hmm",
      "author" : [ "Jinjun Wang", "Changsheng Xu", "Engsiong Chng", "Qi Tian." ],
      "venue" : "ICME.",
      "citeRegEx" : "Wang et al\\.,? 2004",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2004
    }, {
      "title" : "Dynamic memory networks for visual and textual question answering",
      "author" : [ "Caiming Xiong", "Stephen Merity", "Richard Socher." ],
      "venue" : "ICML.",
      "citeRegEx" : "Xiong et al\\.,? 2016",
      "shortCiteRegEx" : "Xiong et al\\.",
      "year" : 2016
    }, {
      "title" : "Visual madlibs: Fill-in-theblank image description and question answering",
      "author" : [ "Licheng Yu", "Eunbyung Park", "Alexander C. Berg", "Tamara L. Berg." ],
      "venue" : "ICCV.",
      "citeRegEx" : "Yu et al\\.,? 2015",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 20,
      "context" : "There has been a surge of vision and language datasets focusing on captions over the last few years, (Rashtchian et al., 2010; Ordonez et al., 2011; Lin et al., 2014), followed by efforts to focus on more specific parts of images (Krishna et al.",
      "startOffset" : 101,
      "endOffset" : 166
    }, {
      "referenceID" : 19,
      "context" : "There has been a surge of vision and language datasets focusing on captions over the last few years, (Rashtchian et al., 2010; Ordonez et al., 2011; Lin et al., 2014), followed by efforts to focus on more specific parts of images (Krishna et al.",
      "startOffset" : 101,
      "endOffset" : 166
    }, {
      "referenceID" : 15,
      "context" : "There has been a surge of vision and language datasets focusing on captions over the last few years, (Rashtchian et al., 2010; Ordonez et al., 2011; Lin et al., 2014), followed by efforts to focus on more specific parts of images (Krishna et al.",
      "startOffset" : 101,
      "endOffset" : 166
    }, {
      "referenceID" : 14,
      "context" : ", 2014), followed by efforts to focus on more specific parts of images (Krishna et al., 2016), or referring expressions (Kazemzadeh et al.",
      "startOffset" : 71,
      "endOffset" : 93
    }, {
      "referenceID" : 13,
      "context" : ", 2016), or referring expressions (Kazemzadeh et al., 2014), or on the broader context (Huang et al.",
      "startOffset" : 34,
      "endOffset" : 59
    }, {
      "referenceID" : 3,
      "context" : "For video, similar efforts have collected descriptions (Chen and Dolan, 2011), while others use existing descriptive video service (DVS) sources (Rohrbach et al.",
      "startOffset" : 55,
      "endOffset" : 77
    }, {
      "referenceID" : 21,
      "context" : "For video, similar efforts have collected descriptions (Chen and Dolan, 2011), while others use existing descriptive video service (DVS) sources (Rohrbach et al., 2015; Torabi et al., 2015).",
      "startOffset" : 145,
      "endOffset" : 189
    }, {
      "referenceID" : 28,
      "context" : "For video, similar efforts have collected descriptions (Chen and Dolan, 2011), while others use existing descriptive video service (DVS) sources (Rohrbach et al., 2015; Torabi et al., 2015).",
      "startOffset" : 145,
      "endOffset" : 189
    }, {
      "referenceID" : 0,
      "context" : "Beyond descriptions, other datasets use questions to relate images and language (Antol et al., 2015; Yu et al., 2015).",
      "startOffset" : 80,
      "endOffset" : 117
    }, {
      "referenceID" : 31,
      "context" : "Beyond descriptions, other datasets use questions to relate images and language (Antol et al., 2015; Yu et al., 2015).",
      "startOffset" : 80,
      "endOffset" : 117
    }, {
      "referenceID" : 10,
      "context" : "The related problem of visually summarizing videos (as opposed to finding the highlights) has produced datasets of holiday and sports events with multiple users making summary videos (Gygli et al., 2014) and multiple users selecting summary key-frames (de Avila et al.",
      "startOffset" : 183,
      "endOffset" : 203
    }, {
      "referenceID" : 7,
      "context" : "For language-based summarization, Extractive models (Filippova and Altun, 2013; Filippova et al., 2015) generate summaries by selecting important sentences and then assembling these, while Abstractive models (Chopra et al.",
      "startOffset" : 52,
      "endOffset" : 103
    }, {
      "referenceID" : 6,
      "context" : "For language-based summarization, Extractive models (Filippova and Altun, 2013; Filippova et al., 2015) generate summaries by selecting important sentences and then assembling these, while Abstractive models (Chopra et al.",
      "startOffset" : 52,
      "endOffset" : 103
    }, {
      "referenceID" : 5,
      "context" : ", 2015) generate summaries by selecting important sentences and then assembling these, while Abstractive models (Chopra et al., 2016; Mei et al., 2016; Nallapati et al., 2016; See et al., 2017) generate/rewrite the summaries from scratch.",
      "startOffset" : 112,
      "endOffset" : 193
    }, {
      "referenceID" : 17,
      "context" : ", 2015) generate summaries by selecting important sentences and then assembling these, while Abstractive models (Chopra et al., 2016; Mei et al., 2016; Nallapati et al., 2016; See et al., 2017) generate/rewrite the summaries from scratch.",
      "startOffset" : 112,
      "endOffset" : 193
    }, {
      "referenceID" : 18,
      "context" : ", 2015) generate summaries by selecting important sentences and then assembling these, while Abstractive models (Chopra et al., 2016; Mei et al., 2016; Nallapati et al., 2016; See et al., 2017) generate/rewrite the summaries from scratch.",
      "startOffset" : 112,
      "endOffset" : 193
    }, {
      "referenceID" : 23,
      "context" : ", 2015) generate summaries by selecting important sentences and then assembling these, while Abstractive models (Chopra et al., 2016; Mei et al., 2016; Nallapati et al., 2016; See et al., 2017) generate/rewrite the summaries from scratch.",
      "startOffset" : 112,
      "endOffset" : 193
    }, {
      "referenceID" : 0,
      "context" : "Beyond descriptions, other datasets use questions to relate images and language (Antol et al., 2015; Yu et al., 2015). This approach is extended to movies in Tapaswi et al. (2016). The related problem of visually summarizing videos (as opposed to finding the highlights) has produced datasets of holiday and sports events with multiple users making summary videos (Gygli et al.",
      "startOffset" : 81,
      "endOffset" : 180
    }, {
      "referenceID" : 4,
      "context" : "Closer to our setting, there has been work on highlight prediction in football (soccer) and basketball based on audio of broadcasts (Cheng and Hsu, 2006) (Wang et al.",
      "startOffset" : 132,
      "endOffset" : 153
    }, {
      "referenceID" : 29,
      "context" : "Closer to our setting, there has been work on highlight prediction in football (soccer) and basketball based on audio of broadcasts (Cheng and Hsu, 2006) (Wang et al., 2004) where commentators may have an outsized impact or visual features (Bertini et al.",
      "startOffset" : 154,
      "endOffset" : 173
    }, {
      "referenceID" : 2,
      "context" : ", 2004) where commentators may have an outsized impact or visual features (Bertini et al., 2005).",
      "startOffset" : 74,
      "endOffset" : 96
    }, {
      "referenceID" : 12,
      "context" : "In the spirit of our study, there has been work looking at tweets during sporting events (Hsieh et al., 2012), but the tweets are not as immediate or as well aligned with the games as the eSports comments.",
      "startOffset" : 89,
      "endOffset" : 109
    }, {
      "referenceID" : 2,
      "context" : ", 2004) where commentators may have an outsized impact or visual features (Bertini et al., 2005). In the spirit of our study, there has been work looking at tweets during sporting events (Hsieh et al., 2012), but the tweets are not as immediate or as well aligned with the games as the eSports comments. More closely related to our work, Song (2016) collects videos for Heroes of the Storm, League of Legends, and Dota2 on online broadcasting websites of around 327 hours total.",
      "startOffset" : 75,
      "endOffset" : 350
    }, {
      "referenceID" : 10,
      "context" : "Following (Gygli et al., 2014; Song et al., 2015), we use the harmonic mean F-score in Eq.",
      "startOffset" : 10,
      "endOffset" : 49
    }, {
      "referenceID" : 25,
      "context" : "Following (Gygli et al., 2014; Song et al., 2015), we use the harmonic mean F-score in Eq.",
      "startOffset" : 10,
      "endOffset" : 49
    }, {
      "referenceID" : 11,
      "context" : "V-CNN We use the ResNet-34 model (He et al., 2016) to represent frames, motivated by its strong results on the ImageNet Challenge (Russakovsky et al.",
      "startOffset" : 33,
      "endOffset" : 50
    }, {
      "referenceID" : 22,
      "context" : ", 2016) to represent frames, motivated by its strong results on the ImageNet Challenge (Russakovsky et al., 2015).",
      "startOffset" : 87,
      "endOffset" : 113
    }, {
      "referenceID" : 26,
      "context" : "Word-level LSTM-RNN models (Sutskever et al., 2014) are a common approach to embedding sentences.",
      "startOffset" : 27,
      "endOffset" : 51
    }, {
      "referenceID" : 9,
      "context" : "Therefore, alternatively, we model the audience chat with a character-level LSTM-RNN model (Graves, 2013).",
      "startOffset" : 91,
      "endOffset" : 105
    }, {
      "referenceID" : 8,
      "context" : ", Bilinear Pooling (Fukui et al., 2016), Memory Networks (Xiong et al.",
      "startOffset" : 19,
      "endOffset" : 39
    }, {
      "referenceID" : 30,
      "context" : ", 2016), Memory Networks (Xiong et al., 2016), and Attention Models (Lu et al.",
      "startOffset" : 25,
      "endOffset" : 45
    }, {
      "referenceID" : 16,
      "context" : ", 2016), and Attention Models (Lu et al., 2016); this is future work.",
      "startOffset" : 30,
      "endOffset" : 47
    } ],
    "year" : 2017,
    "abstractText" : "Sports channel video portals offer an exciting domain for research on multimodal, multilingual analysis. We present methods addressing the problem of automatic video highlight prediction based on joint visual features and textual analysis of the real-world audience discourse with complex slang, in both English and traditional Chinese. We present a novel dataset based on League of Legends championships recorded from North American and Taiwanese Twitch.tv channels (will be released for further research), and demonstrate strong results on these using multimodal, character-level CNN-RNN model architectures.",
    "creator" : "LaTeX with hyperref package"
  }
}