{
  "name" : "1610.06210.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Theme-Rewriting Approach for Generating Algebra Word Problems",
    "authors" : [ "Rik Koncel-Kedziorski", "Ioannis Konstas", "Luke Zettlemoyer", "Hannaneh Hajishirzi" ],
    "emails" : [ "kedzior@uw.edu,", "hannaneh}@cs.washington.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Storytelling is the complex activity of expressing a plot, its events and participants in words meaningful to an audience. Automatic storytelling systems can be used for customized sport commentaries, enriching video games with personalized or dynamic plot-lines (Barros and Musse, 2007), or providing customized learning materials which meet each individual student’s needs and interests (Bartlett, 2004). In this paper, we focus on generating narrative-style\nmath word problems (Figure 1) and demonstrate that it is possible to design an algorithm that can automatically change the overall theme of a text without changing its underlying story, for example to create more engaging homework that is in the theme of a student’s favorite movie.\nA math word problem is a coherent story that provides the student with good clues to the correct mathematical operations between the numerical quantities described therein. However, the particular theme of a problem, whether it be about collecting apples or traveling distances through space, can vary significantly so long as the correlation between the story and underlying equation is maintained. Students’ success at solving a word problem is tied to their interest in the problem’s theme (Renninger\nar X\niv :1\n61 0.\n06 21\n0v 1\n[ cs\n.C L\n] 1\n9 O\nct 2\net al., 2002), and personalizing word problems increases student understanding, engagement, and performance in the problem solving process (Hart, 1996; Davis-Dorsey et al., 1991).\nMotivated by this need for thematically diverse, highly coherent stories, we address the problem of story rewriting, or transforming human-authored stories into novel, coherent stories in a new theme. Rather than synthesizing first a story plot (McIntyre and Lapata, 2009; McIntyre and Lapata, 2010) or script (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2016; Granroth-Wilding and Clark, 2016) from scratch, we instead begin from an existing story and iteratively edit it towards a thematically novel but –most crucially– semantically compatible story. This approach allows us to reuse much, but not all, of the syntactic and semantic structure of the original text, resulting in the creation of more coherent and solvable math word problems.\nWe define a theme to be a collection of reference texts, such as a movie script or series of books. Given a theme, the rewrite algorithm constructs new texts by substituting thematically appropriate words and phrases, as measured with automatic metrics over the theme text collection, for parts of the original texts. This process optimizes for a number of metrics of overall text quality, including syntactic, semantics, and discourse scores. It uses no hand crafted templates and requires no theme-specific tuning data, making it easy to apply for new themes in practice. Tables 4–6 show example stories generated from the rewrite system.\nTo evaluate performance, we collected a corpus of 450 rewrites of math word problems in Star Wars and Children’s Cartoon themes via crowdsourcing.1 Experiments with automated metrics and human evaluations demonstrate that the approach described here outperforms a number of baselines and can produce solvable problems in multiple different themes, even with no in-domain tuning."
    }, {
      "heading" : "2 Related Work",
      "text" : "Our approach is related to the previous work in story generation (e.g., McIntyre and Lapata (2010)) and sentence rewriting (e.g., text simplification (Xu et\n1Data and code available at https://gitlab.cs. washington.edu/kedzior/Rewriter/.\nal., 2016)), as reviewed in this section. It has three major differences from all these approaches: First, we focus on multi-sentence stories where preserving the coherence, discourse relations, and solvability is essential. Previous work mainly focuses on rewriting single sentences. Second, we build a theme from a text corpus and show how the stories can be adapted to new themes. Third, our method leverages the human-authored story to capture the semantic skeleton and the plot of the current story, rather than synthesizing the story plot. To our knowledge, we are the first to introduce a text rewriting formulation for story generation.\nStory generation has been of long interest to AI researchers (Meehan, 1976; Lebowitz, 1987; Turner, 1993; Liu and Singh, 2002; Mostafazadeh et al., 2016). Recent methods in story generation first synthesize candidate plots for a story and then compile those plots into text. Li et al. (2013) use crowdsourcing to build plot graphs. McIntyre and Lapata (2009; 2010) address story generation through the automatic deduction and reassembly of scripts (Schank and Abelson, 1977), or structured representations of events and their participants, and causal relationships involved. Leveraging the automatic script learning methods of Chambers and Jurafsky (2009), McIntyre and Lapata (2010) learn candidate entity-centered plot graphs, or possible events involving the entity and an ordering between these events, with the use of a genetic algorithm. Then plots are compiled into stories through the use of a rule-based text surface realizer (Lavoie and Rambow, 1997) and reranked using a language model.\nPolozov et al. (2015) automatically generate math word problems tailored to a student’s interest using Answer Set Programming to satisfy a collection of pedagogical and narrative requirements. This method naturally produces highly coherent, personalized story problems that meet pedagogical requirements, at the expense of building the thematic ontologies and discourse constraints by hand.2\nAdditionally, there is related work in text simplification (Wubben et al., 2012; Kauchak, 2013; Zhu et al., 2010; Vanderwende et al., 2007; Woodsend and Lapata, 2011b; Hwang et al., 2015), sentence\n2According to Polozov et al. (2015) building small thematic ontologies of types, relations, and discourse tropes (100-200 entries) for each of only 3 literary settings took 1-2 person months.\ncompression (Filippova and Strube, 2008; Rush et al., 2015), and paraphrasing (Ganitkevitch et al., 2013; Chen and Dolan, 2011; Ganitkevitch et al., 2011). All these tasks are focused on rewriting sentences under a predefined set of constraints, such as simplicity. Different rule-based and data-driven approaches are introduced by Petersen and Ostendorf (2007), Vickrey and Koller (2008), and Siddharthan (2004). Most data-driven approaches take advantage of machine translation techniques, use source-target sentence pairs, and learn rewrite operations (Yatskar et al., 2010; Woodsend and Lapata, 2011a), or use additional external paraphrasing resources (Xu et al., 2016).\nFinally, this work is related to those on automatically solving math word problems. Specific topics include number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015; Koncel-Kedziorski et al., 2015a; Roy et al., 2016), and geometry word problems (Seo et al., 2015; Seo et al., 2014). Several datasets of word problems are available (Koncel-Kedziorski et al., 2016; Huang et al., 2016), though none address the need for thematic text."
    }, {
      "heading" : "3 Problem Formulation",
      "text" : "Our system takes as input a story s and a theme t, and outputs the best rewrite s∗ from generated candidates S.\nA theme t is defined as a textual corpus that describes a topic or a domain. This is an intentionally broad definition that allows a variety of textual resources to serve as themes. For example, the collection of all Science Fiction stories from the Project Gutenberg can be a theme, or the script of a single movie, or a sampling of fan fiction from the Internet. This flexibility adds to the utility of our work, as varying amounts of thematic text may be available.\nThe generated candidate s∗ is the most thematically fit problem that is syntactically and semantically coherent given the original problem s and the new theme t. We represent a story in terms of the words it contains, so that s = {w1, w2, . . . wn} and\n|s| = n. The new story s′ is defined as:\ns′ = { f(w1), f(w2), . . . f(wn) } where the function f(w) : Vo → VKt ∪∅, rewrites a word from the vocabulary of the original problem Vo to either a word, a trivial noun compound of length K (e.g., multi-word named entity) from the vocabulary of the the thematic vocabulary Vt, or reduces to the empty symbol, i.e., omits the input word entirely; hence the length of s′ can differ from that of the original problem.\nFormally, our goal is to select the candidate s′ ∈ S by maximizing a scoring functionR over thematic, syntactic and semantic constraints, subject to a set of parameters θ:\ns∗ = argmax s′∈S R(s′|s, t; θ) (1)\nIn order to find the best story s∗, our problem reduces to generating candidate stories s′ from the space of possible rewrites of the human-authored story s in a new theme t (Section 5). Since there are exponentially many rewrites, we follow a twostage decoding approach: first we identify only the content words wi in the input problem, and provide for each a list of the top-k most salient thematic candidate words and trivial noun compounds. We then search the space by progressively introducing more rewrites in the beam, and scoring them according to R (Section 4). Figure 2 shows the overview of the scoring function for a candidate sentence s′."
    }, {
      "heading" : "4 Scoring Stories",
      "text" : "The scoring functionR decomposes into three components, capturing aspects of syntactic compatibility, semantic coherence, and thematicity:\nR(s′|s, t; θ) =α× Sem(s′|s) + β × Syn(s′|s) (2) + γ × Th(s′|s, t)\nThe syntactic (Syn) and semantic (Sem) coherence components measure the coherence of the words in the new story s′, as well as their compatibility to the syntactic and semantic relations in the original story s. On the other hand, thematicity (Th) scores the relevance and importance of words in the new story with respect to theme t.\nWe describe each of these components and the decoding process in the following sections."
    }, {
      "heading" : "4.1 Thematicity",
      "text" : "Recall that a theme t is defined as a collection of documents that share a common topic, such as books in the science fiction genre, or scripts of horror movies. We define thematicity of a word w′ as the measure of salience, or how discriminative that word is to a given theme.3 For example, robot and spaceship are expected to be highly thematic with respect to Star Wars. In our setting we extend this definition to a candidate problem s′ given s and t as:\nTh(s′|s, t) = |s|∑ i Sal(w′i, t) (3)\nwhere w′i is a word from the candidate problem, and Sal is its salience score with respect to the theme. In the context of this work we argue that the thematic adaptation of the content words, i.e., nouns, verbs, named entities, and adjectives, plays the most important role in forming a new thematic problem. Therefore, we define their salience (except named entities) based on their tf-idf score over the theme t, and set it to zero for function words. Since named entities have relatively low frequencies in the theme corpus we set their salience to 1− 1\nc(w′i) , where c(w′i)\n3We will be interchangeably referring to w′ as either the word or the head of the multi-word noun compound that rewrites the equivalent word w in the original problem.\nis the number of times w′i occurs in the theme. In the example story in Figure 2 the thematicity score is derived as Sal(Luke Skywalker) + Sal(ships) + Sal(droids)."
    }, {
      "heading" : "4.2 Syntactic compatibility",
      "text" : "This work offers a new method for syntactic and discourse coherence based on preserving humanauthored syntactic structure in generated text (hence our use of the term rewriting). The syntactic constructs in a document play a distinctive role in maintaining cohesion across sentences. We consider the human-authored syntax of the original story s as gold standard, and use it to score a candidate problem s′ by considering how well the syntactic relations of s apply to s′.\nFormally, given a dependency triple (wi, wj , l) from a parse of a sentence in s, we compute the likelihood for the corresponding triple (w′i, w ′ j , l) for w′i, w ′ j in s\n′. We define the syntactic score for all sentences in s′ as:\nSyn(s′|s) = ∑\ni,j,l|(wi,wj ,l)∈Dep(s)\nLDep(w′i, w′j , l) (4)\nwhere Dep(s) are the dependency parse trees for all sentences in s; LDep is a 3-gram language model over dependency triples which gives the likelihood of an arc label l being used between a pair of words (w′i, w ′ j). For example in Figure 2, the syntactic compatibility score includes dependency likelihoods of LDep(ship, 2, num), LDep(had, ship, dobj).\nTherefore, the Syn function prefers stories s′ that (a) have similar dependency structure to the original story s and (b) make use of a common syntactic configuration."
    }, {
      "heading" : "4.3 Semantic Coherence",
      "text" : "The semantic coherence component expresses how well a candidate s′ rewrites individual words and realizes the semantic relationships that exist in the human-authored story s. Ideally, we would like to preserve enough of the semantics of s in order to produce a coherent story s′, yet we are populating s′ with words taken from an unrelated theme. Therefore, we model the semantics of a story s′ in terms of the lexical semantics contributed by individual words as well as semantic relationships that exist between its elements. Note that the relationships can\ncross the sentence boundaries, promoting discourse coherence.\nWe decompose semantic relations in a story into a set of local, lexical relationships between pairs of words. Specifically, we consider semantic relations for noun-noun and verb-verb pairs as provided by WordNet (Miller, 1995). Since some relations are not directly outlined in these resources (e.g., the selectional preferences of nouns with regard to their adjectival modifiers), we also consider the wordembedding similarity between words. For example in Figure 2 the semantic relationships are denoted with blue arrows between pairs of content words in the story (e.g., {Sam, dogs}, {dogs, puppies}, etc).\nMore formally, we define the semantic coherence of s′ with respect to s as:\nSem(s′|s) = |s′|∑ i SemLex(wi, w′i) (5)\n+ ∑\ni,j∈CW SemPair({wi, wj}, {w′i, w′j})\nwhere CW is the set of pairs of indices of content words (nouns, verbs, adjectives, and named entities) from s. We focus on the content words of the original problem, as they carry most of the semantic information. SemLex and SemPair functions are semantic adaptation scores for individual words and semantic relations respectively, described below.\nSemantic Compatibility between words (SemLex) is defined as:\nSemLex(wi, w′i) = cos(wi, w ′ i) +Resnik(wi, w ′ i)\n(6)\nwhere cos(wi, w′i) denotes the cosine similarity between the vector space embeddings of two words wi andw′i\n4, andResnik(wi, w′i) expresses the information content of the lowest subsumer of {wi, w′i} in WordNet. For example in Figure 2, the semantic compatibility score incorporates lexical similarities SemLex(dog, ship), etc.\nCompatibility score between semantic relations (SemPair) is defined by adding two components: PairSim and Analogy that compute how semantic relations between pairs of words are preserved in\n4For the ease of notation, we represent the embedding of the words with wi as well.\nthe new story:\nPairSim =cos(wi, wj) ∗ cos(w′i, w′j) (7) Analogy =cos(w′i + wj − wi, w′j) (8)\nPairSim preserves the similarity between pairs of words {wi, wj} in s and the corresponding pair {w′i, w′j} in the new story s′. Intuitively, ifwi andwj are semantically close to each other, we would like the corresponding words to be close in the new story as well. For example in Figure 2, ‘dog’ and ‘puppy’ are similar in the original story, we expect the corresponding words ‘ship’ and ‘droid’ to be similar in the new story. The Analogy function, inspired by Mikolov et al. (2013), computes the analogy of w′j fromw′i given the relationship that holds betweenwi and wj in the vector space. For example in Figure 2, the relation between ‘Sam’ and ‘dog’ is similar to the relation between ‘Luke Skywalker’ and ‘ship’."
    }, {
      "heading" : "5 Decoding",
      "text" : "Our decoding process begins by first identifying the content words wi (nouns, verbs, adjectives and named entities) in the original problem s that will be considered as initial points for rewriting. For each of these lexical classes we extract the top-k most thematic words and trivial noun compounds from the theme t. For example, in Figure 2, candidate nouns are: ‘ships’, ‘robots’, ‘droids’, etc., and for verbs: ‘blast’, ‘soar’, ‘command’, etc. Recall that the space of candidate rewrites is large, prohibiting an exhaustive enumeration. We therefore do approximate search with a beam by considering simultaneously all possible paths that start at the different initial points. At each step the decoder considers an additional rewrite from the list of candidates, adds it to the existing hypothesis path, and scores it according to functionR (Equation 2).\nAll the counterpart scores are locally optimal, as they factor over each new word w′i or pair of {w′i, w′j}, where w′j is a rewrite already existing in the hypothesis path. At any given step we may recombine hypotheses that share the same prefix hypothesis path, and keep the top scoring one. The process terminates when there are no more rewrites left. We also experimented decoding with a variety of orderings of the text in the original problem s, including left-to-right, and head-first following the\ndependency tree of each sentence and then concatenating these linearizations; we observed that considering multiple paths achieves the best performance."
    }, {
      "heading" : "6 Data Collection",
      "text" : "For the set of human-authored stories {s}, we use a corpus of math word problems described in KoncelKedziorski et al. (2016). We select a subset of 150 problems targeting 5th and 6th grade levels, all of which involve a single equation in one variable. These problems have 2.7 sentences and 29.4 words on average, 12.6 of which are considered content words by our system. In order to tune and evaluate our model, we collect a corpus of human-authored rewrites produced by workers from Amazon Mechanical Turk based on two themes: Star Wars, and Adventure Time (a children’s cartoon).\nWe experimented with different ways of helping to define the theme for the workers, including offering automatically generated word clouds or enforcing that a response includes one of several keywords. In practice, we have found that using specific cultural elements as themes (such as famous movie or cartoon franchises) attracts workers who already have a strong knowledge of the theme, resulting in higher quality work.\nTo help explain the rewriting process, we show workers three examples of thematic rewrites with varying degrees of correlation to the original problems. We then show workers a random problem from the original set {s} and a corresponding equation for that problem. We instruct the workers to “rewrite” the problem according to the theme, ensuring that their rewritten problem can be solved by the provided equation. The final dataset collection comprises of 450 human-authored rewrites. We collect 3 rewrites for 100 of the original problems for the Star Wars theme (based on the popular Star Wars sequel movies), and 3 rewrites for the rest of the 50 original problems, for the Children Cartoons Theme (CARTOON), based on the Adventure Time TV show. We keep 150 examples from the Star Wars theme for development (STARdev), and the rest 150 for testing (STARtest).\nWe collected the STARdev and CARTOON data based on workers with the “master” designation and at least 95% approval rating. Then we pro-\nceeded collecting STARtest by a subset of the authors of STARdev who self-identify as theme experts and whose quality of work is manually confirmed."
    }, {
      "heading" : "7 Experiments",
      "text" : ""
    }, {
      "heading" : "7.1 Setup",
      "text" : "Implementation Details We pre-process the themes using the Stanford CoreNLP tools (Manning et al., 2014) for tokenization, Named Entity Recognition (Finkel et al., 2005), and dependency parsing (Chen and Manning, 2014). For calculating salience scores, we use the ScriptBase dataset of movie scripts (Gorinski and Lapata, 2015). The Star Wars theme is constructed from the available script, roughly 7300 words. The Cartoon theme is constructed from fan-authored scripts of the first 10 episodes of the show (Springfield, 2016) totaling 1370 words. Since our thematic options are taken from arbitrary text, we use the lists of offensive terms published by The Racial Slur database (Database, 2016) and FrontGate Media (Media, 2016) to filter out offensive content. To prohibit overgeneration, we forbid the transformation of stop words or math-specific words (Survivors, 2013; Koncel-Kedziorski et al., 2015b).\nFor syntactic compatibility score Syn (Equation 4) we use the English Fiction subset of the Google Syntactic N-grams corpus (Goldberg and Orwant, 2013) and train a 3-gram language model using KenLM (Heafield, 2011). For SemLex, PairSim and Analogy (Equations 6-8) we use the pretrained word embeddings of Levy and Goldberg (2014). These embeddings are trained using dependency contexts rather than windows of adjacent words, allowing them to capture functional word similarity. Finally, we tune the parameters of our model (Equation 2) on the development set STARdev and pick those values5 that maximize METEOR score (Denkowski and Lavie, 2014) against 3 human references.\nEvaluation We compare two ablated configurations of our method against our full model (FULL): -SYN that only uses semantic and thematicity components and does not incorporate the syntactic compatibility score, -SEM replaces the semantic coher-\n5We set α = 0.1, β = 0.1 and γ = 1\nence score with the simpler cos(wi, w′i), effectively rewriting only single words, and not pairs. We refrained from ablating the thematicity score as it is the core part of our model that drives the rewriting process into a new theme.\nWe evaluate our method using an automatic metric, and via eliciting human judgments on Amazon Mechanical Turk. For automatic evaluation, we compute the METEOR score, comparing the output of each model for a given problem and theme to the 3 human rewrites we collected, on STARdev, STARtest and CARTOON. METEOR is a recalloriented metric, widely used in the MT community; the additional stemming, synonym and paraphrase matching modules make it more applicable for our use, given the nature of our rewriting task.6\nFor human evaluation, we conduct pairwise comparison tests, pairing FULL against a human rewrite (HUMAN), FULL against -SYN, and FULL against -SEM. Participants were given a short description of the theme, and the output of each system. For each test we asked 40 subjects to select which problem they preferred over 5 pairs of outputs; we obtained a total of 200 (5x40) responses for STARtest and CARTOON.\nIn order to better understand the strengths and weaknesses of the generated stories, we conducted a more detailed human evaluation. 8 participants were presented with the output of the three automatic systems, human rewrites (HUMAN), and a theme. The participants were asked to rate the stories across three dimensions: coherence (how coherent is the text of the problem?), solvability (can elementary school students solve it?), and thematicity (how well does the problem express them?) on a scale from 1 to 5. We collected ratings over 16 outputs from\n6The average METEOR score comparing 1 annotator against the other 2 is 0.26, indicating that there are diverse correct strategies for solving the rewriting problem.\nSTARtest, resulting in 128 responses."
    }, {
      "heading" : "7.2 Results",
      "text" : "Table 1 reports METEOR; we notice that removing the semantic coherence scores in -SEM hurts the performance compared to FULL; this confirms our claim that semantic compatibility is crucial for building coherent stories. On the other hand, -SYN performs similarly to FULL. Closer inspection of the -SYN system’s output reveals a greater diversity in thematic elements as a result of the relaxed syntactic compatibility constraints. Hence it is more likely to have greater overlap with any of the reference rewrites, resulting in higher METEOR scores.\nHowever, a pairwise comparison between FULL and -SYN (Table 2) reveals that human subjects consistently prefer the output of FULL instead of -SYN both for STARtest and CARTOON. Table 2 also reports that HUMAN outperforms the output of the FULL model, and a pairwise comparison of FULL and -SEM which yields a result in line with the METEOR scores.\nTable 3 shows the results of the detailed comparison of Thematicity, Coherence, and Solvability. This table clearly shows the strong contribution of the semantic component of our system. The specific contribution of the syntactic component is to pro-\nduce overall more solvable and thematically satisfying problems, although it can slightly affect coherence especially when automatic parses fail. Finally, the overall high ratings for human-authored stories across all three dimensions, confirm the high quality of the crowd-sourced stories."
    }, {
      "heading" : "7.3 Qualitative Examples",
      "text" : "Table 4–6 shows some problems generated by our method. Recall that since our system needs no annotated thematic training data, we can easily generate from any theme where thematic text is available. To demonstrate this fact, we include generated examples in a Western theme from novels from the Project Gutenberg corpus. Many of the results of our system are very legible, with only minor agreement errors. Coherent, thematic semantic relations are evident in problems such as s′1, where ships, guns, and weapons combine to effect the Star Wars theme; this is also evident in s′5, where people with western sounding names like Kurt and Madeline trade in cigarettes, an old-fashioned pre-cursor to e-cigarettes.\nIn some cases, semantic inconsistencies result in weird sounding problems, such as in s′6 where the main character receives “wheat of grub”. But because of the syntactic compatibility component, our model scores this candidate higher because of the\nconnection between “wheat” and “graze”. Semantic incoherence is less of a problem in the cartoon theme, where absurd interactions between characters are expected. However, a difficulty for our system is demonstrated in s′7, where the physical entity “swords” is substituted for the nominalization of an event “trips”. Improvements to the semantic coherence component could resolve such issues.\nTable 7 shows some instances where the rewrite algorithm produces unusable results. An example of under-generation is s′10. Here, too many words are left untouched, resulting in both ungrammaticality and semantic incoherence. In s′11, we witness some limitations of using word vectors. The rare word “Ferris” is not close to anything in the Star Wars theme, and is thus mapped almost arbitrarily to “int” (movie script shorthand for an interior shot). Better treatment of noun compounds and the use of phrase vectors would reduce such errors."
    }, {
      "heading" : "8 Conclusion",
      "text" : "We formalized the problem of story rewriting as automatically changing the theme of a text without\naltering the underlying story and developed an approach for rewriting algebra word problems where the rewriting model optimized for a number of measures of overall text coherence. Experiments on a newly gathered dataset demonstrated our model can produce themed texts that are usually solvable.\nFuture work could improve the thematicity and solvability components by incorporating domainspecific and commonsense knowledge, leveraging information extraction. Additionally, neural network architectures (e.g., LSTMs, seq2seq) can be trained to rewrite coherently with less reliance on brittle syntactic parses. Additionally, we plan to study rewriting in other domains such as children’s short stories and extend the model to generate math word problems directly from equations. Finally, we intend to incorporate the generated problems in educational technology and tutoring systems."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This research was supported by the NSF (IIS 1616112), Allen Institute for AI (66-9175), Allen Distinguished Investigator Award, DARPA (FA8750-13-2-0008) and a Google research faculty\naward. We thank the anonymous reviewers for their helpful comments."
    } ],
    "references" : [ {
      "title" : "Planning algorithms for interactive storytelling",
      "author" : [ "Leandro Motta Barros", "Soraia Raupp Musse." ],
      "venue" : "Computers in Entertainment (CIE), 5(1):4.",
      "citeRegEx" : "Barros and Musse.,? 2007",
      "shortCiteRegEx" : "Barros and Musse.",
      "year" : 2007
    }, {
      "title" : "Expanding teacher work roles: a resource for retention or a recipe for overwork",
      "author" : [ "Lora Bartlett" ],
      "venue" : "Journal of Education",
      "citeRegEx" : "Bartlett.,? \\Q2004\\E",
      "shortCiteRegEx" : "Bartlett.",
      "year" : 2004
    }, {
      "title" : "Unsupervised Learning of Narrative Schemas and Their Participants",
      "author" : [ "Nathanael Chambers", "Dan Jurafsky." ],
      "venue" : "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Pro-",
      "citeRegEx" : "Chambers and Jurafsky.,? 2009",
      "shortCiteRegEx" : "Chambers and Jurafsky.",
      "year" : 2009
    }, {
      "title" : "Collecting highly parallel data for paraphrase evaluation",
      "author" : [ "David Chen", "William B. Dolan." ],
      "venue" : "Proceedings of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Chen and Dolan.,? 2011",
      "shortCiteRegEx" : "Chen and Dolan.",
      "year" : 2011
    }, {
      "title" : "A fast and accurate dependency parser using neural networks",
      "author" : [ "Danqi Chen", "Christopher Manning." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 740–750, Doha, Qatar, October. Association for",
      "citeRegEx" : "Chen and Manning.,? 2014",
      "shortCiteRegEx" : "Chen and Manning.",
      "year" : 2014
    }, {
      "title" : "The role of rewording and context personalization in the solving of mathematical word problems",
      "author" : [ "Judy Davis-Dorsey", "Steven M Ross", "Gary R Morrison." ],
      "venue" : "Journal of Educational Psychology, 83(1):61.",
      "citeRegEx" : "Davis.Dorsey et al\\.,? 1991",
      "shortCiteRegEx" : "Davis.Dorsey et al\\.",
      "year" : 1991
    }, {
      "title" : "Meteor Universal: Language Specific Translation Evaluation",
      "author" : [ "Michael Denkowski", "Alon Lavie" ],
      "venue" : null,
      "citeRegEx" : "Denkowski and Lavie.,? \\Q2014\\E",
      "shortCiteRegEx" : "Denkowski and Lavie.",
      "year" : 2014
    }, {
      "title" : "Dependency tree based sentence compression",
      "author" : [ "K. Filippova", "M. Strube." ],
      "venue" : "Proceedings of the Fifth International Natural Language Generation Conference (INLG)).",
      "citeRegEx" : "Filippova and Strube.,? 2008",
      "shortCiteRegEx" : "Filippova and Strube.",
      "year" : 2008
    }, {
      "title" : "Incorporating non-local information into information extraction systems by gibbs sampling",
      "author" : [ "Jenny Rose Finkel", "Trond Grenager", "Christopher Manning." ],
      "venue" : "Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05,",
      "citeRegEx" : "Finkel et al\\.,? 2005",
      "shortCiteRegEx" : "Finkel et al\\.",
      "year" : 2005
    }, {
      "title" : "Learning sentential paraphrases from bilingual parallel corpora for text-to-text generation",
      "author" : [ "J. Ganitkevitch", "C. Callison-Burch", "C. Napoles", "B. Van Durme." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Ganitkevitch et al\\.,? 2011",
      "shortCiteRegEx" : "Ganitkevitch et al\\.",
      "year" : 2011
    }, {
      "title" : "PPDB: The paraphrase database",
      "author" : [ "Juri Ganitkevitch", "Benjamin Van Durme", "Chris Callison-Burch." ],
      "venue" : "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technolo-",
      "citeRegEx" : "Ganitkevitch et al\\.,? 2013",
      "shortCiteRegEx" : "Ganitkevitch et al\\.",
      "year" : 2013
    }, {
      "title" : "A Dataset of syntactic-Ngrams over Time from a Very Large Corpus of English Books",
      "author" : [ "Yoav Goldberg", "Jon Orwant." ],
      "venue" : "Second Joint Conference on Lexical and Computational Semantics (* SEM), volume 1, pages 241–247.",
      "citeRegEx" : "Goldberg and Orwant.,? 2013",
      "shortCiteRegEx" : "Goldberg and Orwant.",
      "year" : 2013
    }, {
      "title" : "Movie script summarization as graph-based scene extraction",
      "author" : [ "Philip John Gorinski", "Mirella Lapata." ],
      "venue" : "Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL.",
      "citeRegEx" : "Gorinski and Lapata.,? 2015",
      "shortCiteRegEx" : "Gorinski and Lapata.",
      "year" : 2015
    }, {
      "title" : "What happens next? event prediction using a compositional neural network model",
      "author" : [ "Mark Granroth-Wilding", "Stephen Clark." ],
      "venue" : "Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI-16), Phoenix, Arizona.",
      "citeRegEx" : "Granroth.Wilding and Clark.,? 2016",
      "shortCiteRegEx" : "Granroth.Wilding and Clark.",
      "year" : 2016
    }, {
      "title" : "The Effect of Personalized Word Problems",
      "author" : [ "Janis M Hart." ],
      "venue" : "Teaching Children Mathematics, 2(8):504– 505.",
      "citeRegEx" : "Hart.,? 1996",
      "shortCiteRegEx" : "Hart.",
      "year" : 1996
    }, {
      "title" : "KenLM: Faster and smaller language model queries",
      "author" : [ "Kenneth Heafield." ],
      "venue" : "Proceedings of the Sixth Workshop on Statistical Machine Translation, WMT ’11, pages 187–197, Stroudsburg, PA, USA. Association for Computational Linguistics.",
      "citeRegEx" : "Heafield.,? 2011",
      "shortCiteRegEx" : "Heafield.",
      "year" : 2011
    }, {
      "title" : "Learning to Solve Arithmetic Word Problems with Verb Categorization",
      "author" : [ "Mohammad Javad Hosseini", "Hannaneh Hajishirzi", "Oren Etzioni", "Nate Kushman." ],
      "venue" : "EMNLP, pages 523–533.",
      "citeRegEx" : "Hosseini et al\\.,? 2014",
      "shortCiteRegEx" : "Hosseini et al\\.",
      "year" : 2014
    }, {
      "title" : "How well do computers",
      "author" : [ "Danqing Huang", "Shuming Shi", "Chin-Yew Lin", "Jian Yin", "Wei-Ying Ma" ],
      "venue" : null,
      "citeRegEx" : "Huang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2016
    }, {
      "title" : "Aligning Sentences from Standard Wikipedia to Simple Wikipedia",
      "author" : [ "William Hwang", "Hannaneh Hajishirzi", "Mari Ostendorf", "Wei Wu." ],
      "venue" : "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).",
      "citeRegEx" : "Hwang et al\\.,? 2015",
      "shortCiteRegEx" : "Hwang et al\\.",
      "year" : 2015
    }, {
      "title" : "Improving text simplification language modeling using unsimplified text data",
      "author" : [ "David Kauchak." ],
      "venue" : "Proceedings of the Conference of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Kauchak.,? 2013",
      "shortCiteRegEx" : "Kauchak.",
      "year" : 2013
    }, {
      "title" : "Parsing Algebraic Word Problems into Equations",
      "author" : [ "Rik Koncel-Kedziorski", "Hannaneh Hajishirzi", "Ashish Sabharwal", "Oren Etzioni", "Siena Ang." ],
      "venue" : "TACL, 3.",
      "citeRegEx" : "Koncel.Kedziorski et al\\.,? 2015a",
      "shortCiteRegEx" : "Koncel.Kedziorski et al\\.",
      "year" : 2015
    }, {
      "title" : "Parsing algebraic word problems into equations",
      "author" : [ "Rik Koncel-Kedziorski", "Hannaneh Hajishirzi", "Ashish Sabharwal", "Oren Etzioni", "Siena Ang." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 3:585–597.",
      "citeRegEx" : "Koncel.Kedziorski et al\\.,? 2015b",
      "shortCiteRegEx" : "Koncel.Kedziorski et al\\.",
      "year" : 2015
    }, {
      "title" : "MAWPS: A Math Word Problem Repository",
      "author" : [ "Rik Koncel-Kedziorski", "Subhro Roy", "Aida Aimini", "Nate Kushman", "Hannaneh Hajishirzi." ],
      "venue" : "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
      "citeRegEx" : "Koncel.Kedziorski et al\\.,? 2016",
      "shortCiteRegEx" : "Koncel.Kedziorski et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning to Automatically Solve Algebra Word Problems",
      "author" : [ "Nate Kushman", "Yoav Artzi", "Luke Zettlemoyer", "Regina Barzilay." ],
      "venue" : "ACL, pages 271– 281.",
      "citeRegEx" : "Kushman et al\\.,? 2014",
      "shortCiteRegEx" : "Kushman et al\\.",
      "year" : 2014
    }, {
      "title" : "A Fast and Portable Realizer for Text Generation Systems",
      "author" : [ "Benoit Lavoie", "Owen Rambow." ],
      "venue" : "Proceedings of the fifth conference on Applied natural language processing, pages 265–268. Association for Computational Linguistics.",
      "citeRegEx" : "Lavoie and Rambow.,? 1997",
      "shortCiteRegEx" : "Lavoie and Rambow.",
      "year" : 1997
    }, {
      "title" : "Planning Stories",
      "author" : [ "Michael Lebowitz." ],
      "venue" : "Proceedings of the cognitive science society, Hillsdale, pages 234–242.",
      "citeRegEx" : "Lebowitz.,? 1987",
      "shortCiteRegEx" : "Lebowitz.",
      "year" : 1987
    }, {
      "title" : "DependencyBased Word Embeddings",
      "author" : [ "Omer Levy", "Yoav Goldberg." ],
      "venue" : "ACL, pages 302–308.",
      "citeRegEx" : "Levy and Goldberg.,? 2014",
      "shortCiteRegEx" : "Levy and Goldberg.",
      "year" : 2014
    }, {
      "title" : "Story generation with crowdsourced plot graphs",
      "author" : [ "Boyang Li", "Stephen Lee-Urban", "George Johnston", "Mark O. Riedl." ],
      "venue" : "Proceedings of AAAI Conferece on Artificial Intelligence (AAAI).",
      "citeRegEx" : "Li et al\\.,? 2013",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2013
    }, {
      "title" : "MAKEBELIEVE: Using Commonsense Knowledge to Generate Stories",
      "author" : [ "Hugo Liu", "Push Singh." ],
      "venue" : "AAAI/IAAI, pages 957–958.",
      "citeRegEx" : "Liu and Singh.,? 2002",
      "shortCiteRegEx" : "Liu and Singh.",
      "year" : 2002
    }, {
      "title" : "The Stanford CoreNLP natural language processing toolkit",
      "author" : [ "Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky." ],
      "venue" : "Proceedings of the Conference of the Association for Computational Linguistics: System",
      "citeRegEx" : "Manning et al\\.,? 2014",
      "shortCiteRegEx" : "Manning et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning to Tell Tales: A Data-driven Approach to Story Generation",
      "author" : [ "Neil McIntyre", "Mirella Lapata." ],
      "venue" : "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Pro-",
      "citeRegEx" : "McIntyre and Lapata.,? 2009",
      "shortCiteRegEx" : "McIntyre and Lapata.",
      "year" : 2009
    }, {
      "title" : "Plot Induction and Evolutionary Search for Story Generation",
      "author" : [ "Neil McIntyre", "Mirella Lapata." ],
      "venue" : "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1562–1572. Association for Computational Linguistics.",
      "citeRegEx" : "McIntyre and Lapata.,? 2010",
      "shortCiteRegEx" : "McIntyre and Lapata.",
      "year" : 2010
    }, {
      "title" : "The Metanovel: Writing Stories by Computer",
      "author" : [ "James Richard Meehan." ],
      "venue" : "Technical report, DTIC Document.",
      "citeRegEx" : "Meehan.,? 1976",
      "shortCiteRegEx" : "Meehan.",
      "year" : 1976
    }, {
      "title" : "Efficient Estimation of Word Representations in Vector Space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "arXiv preprint arXiv:1301.3781.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "WordNet: A Lexical Database for English",
      "author" : [ "George A Miller." ],
      "venue" : "Communications of the ACM, 38(11):39–",
      "citeRegEx" : "Miller.,? 1995",
      "shortCiteRegEx" : "Miller.",
      "year" : 1995
    }, {
      "title" : "Learning to automatically solve logic grid puzzles",
      "author" : [ "Arindam Mitra", "Chitta Baral." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Mitra and Baral.,? 2015",
      "shortCiteRegEx" : "Mitra and Baral.",
      "year" : 2015
    }, {
      "title" : "A corpus and evaluation framework for deeper understanding of commonsense stories",
      "author" : [ "Nasrin Mostafazadeh", "Nathanael Chambers", "Xiaodong He", "Devi Parikh", "Dhruv Batra", "Lucy Vanderwende", "Pushmeet Kohli", "James Allen." ],
      "venue" : "Proceedings of the 2016",
      "citeRegEx" : "Mostafazadeh et al\\.,? 2016",
      "shortCiteRegEx" : "Mostafazadeh et al\\.",
      "year" : 2016
    }, {
      "title" : "Text simplification for langauge learners: A corpus analysis",
      "author" : [ "Sarah Petersen", "Mari Ostendorf." ],
      "venue" : "Proceedings of the Speech and Language Technology in Education Workshop (SLaTE).",
      "citeRegEx" : "Petersen and Ostendorf.,? 2007",
      "shortCiteRegEx" : "Petersen and Ostendorf.",
      "year" : 2007
    }, {
      "title" : "Learning statistical scripts with LSTM recurrent neural networks",
      "author" : [ "Karl Pichotta", "Raymond J. Mooney." ],
      "venue" : "Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI-16), Phoenix, Arizona.",
      "citeRegEx" : "Pichotta and Mooney.,? 2016",
      "shortCiteRegEx" : "Pichotta and Mooney.",
      "year" : 2016
    }, {
      "title" : "Personalized Mathematical Word Problem Generation",
      "author" : [ "Oleksandr Polozov", "Eleanor ORourke", "Adam M Smith", "Luke Zettlemoyer", "Sumit Gulwani", "Zoran Popovic." ],
      "venue" : "Proceedings of the 24th International Joint Conference on Artificial Intelligence",
      "citeRegEx" : "Polozov et al\\.,? 2015",
      "shortCiteRegEx" : "Polozov et al\\.",
      "year" : 2015
    }, {
      "title" : "Individual interest as context in expository text and mathematical word problems",
      "author" : [ "KA Renninger", "L Ewen", "AK Lasher." ],
      "venue" : "Learning and Instruction, 12(4):467–490.",
      "citeRegEx" : "Renninger et al\\.,? 2002",
      "shortCiteRegEx" : "Renninger et al\\.",
      "year" : 2002
    }, {
      "title" : "Solving General Arithmetic Word Problems",
      "author" : [ "Subhro Roy", "Dan Roth." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Roy and Roth.,? 2015",
      "shortCiteRegEx" : "Roy and Roth.",
      "year" : 2015
    }, {
      "title" : "2016. Equation parsing : Mapping sentences to grounded",
      "author" : [ "Subhro Roy", "Shyam Upadhyay", "Dan Roth" ],
      "venue" : null,
      "citeRegEx" : "Roy et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Roy et al\\.",
      "year" : 2016
    }, {
      "title" : "A neural attention model for abstractive sentence summarization",
      "author" : [ "Alexander M Rush", "Sumit Chopra", "Jason Weston." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Rush et al\\.,? 2015",
      "shortCiteRegEx" : "Rush et al\\.",
      "year" : 2015
    }, {
      "title" : "Scripts, Plans, Goals, and Understanding: An Inquiry into Human Knowledge Structures",
      "author" : [ "Roger C Schank", "Robert P Abelson." ],
      "venue" : "Hillsdale, NJ: Lawrence Erlbaum.",
      "citeRegEx" : "Schank and Abelson.,? 1977",
      "shortCiteRegEx" : "Schank and Abelson.",
      "year" : 1977
    }, {
      "title" : "Diagram Understanding in Geometry Questions",
      "author" : [ "Min Joon Seo", "Hannaneh Hajishirzi", "Ali Farhadi", "Oren Etzioni." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Seo et al\\.,? 2014",
      "shortCiteRegEx" : "Seo et al\\.",
      "year" : 2014
    }, {
      "title" : "Solving Geometry Problems: Combining Text and Diagram Interpretation",
      "author" : [ "Minjoon Seo", "Hannaneh Hajishirzi", "Ali Farhadi", "Oren Etzioni", "Clint Malcolm." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Seo et al\\.,? 2015",
      "shortCiteRegEx" : "Seo et al\\.",
      "year" : 2015
    }, {
      "title" : "Automatically Solving Number Word Problems by Semantic Parsing and Reasoning",
      "author" : [ "Shuming Shi", "Yuehui Wang", "Chin-Yew Lin", "Xiaojiang Liu", "Yong Rui." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Shi et al\\.,? 2015",
      "shortCiteRegEx" : "Shi et al\\.",
      "year" : 2015
    }, {
      "title" : "Syntactic simplification and text cohesion",
      "author" : [ "Advaith Siddharthan." ],
      "venue" : "Research on Language and Computation, 4(1):77–109.",
      "citeRegEx" : "Siddharthan.,? 2004",
      "shortCiteRegEx" : "Siddharthan.",
      "year" : 2004
    }, {
      "title" : "Minstrel: A Computer Model of Creativity and Storytelling",
      "author" : [ "Scott R Turner" ],
      "venue" : null,
      "citeRegEx" : "Turner.,? \\Q1993\\E",
      "shortCiteRegEx" : "Turner.",
      "year" : 1993
    }, {
      "title" : "Beyond sumbasic: Task-focused summarization with sentence simplification and lexical expansion",
      "author" : [ "Lucy Vanderwende", "Hisami Suzuki", "Chris Brockett", "Ani Nenkova." ],
      "venue" : "Information Processing and Management.",
      "citeRegEx" : "Vanderwende et al\\.,? 2007",
      "shortCiteRegEx" : "Vanderwende et al\\.",
      "year" : 2007
    }, {
      "title" : "Sentence simplification for semantic role labeling",
      "author" : [ "David Vickrey", "Daphne Koller." ],
      "venue" : "Proceedings of the Conference of the Association for Computational Linguistics (ACL), pages 344–352.",
      "citeRegEx" : "Vickrey and Koller.,? 2008",
      "shortCiteRegEx" : "Vickrey and Koller.",
      "year" : 2008
    }, {
      "title" : "Learning to simplify sentences with quasi-synchronous grammar and integer programming",
      "author" : [ "Kristian Woodsend", "Mirella Lapata." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Woodsend and Lapata.,? 2011a",
      "shortCiteRegEx" : "Woodsend and Lapata.",
      "year" : 2011
    }, {
      "title" : "Wikisimple: Automatic simplification of wikipedia articles",
      "author" : [ "Kristian Woodsend", "Mirella Lapata." ],
      "venue" : "Proceedings of the Association for Advancement of Artificial Intelligence Conference on Artificial Intelligence (AAAI), pages 927–932, San Francisco, CA.",
      "citeRegEx" : "Woodsend and Lapata.,? 2011b",
      "shortCiteRegEx" : "Woodsend and Lapata.",
      "year" : 2011
    }, {
      "title" : "Sentence simplification by monolingual",
      "author" : [ "Sander Wubben", "Antal Van Den Bosch", "Emiel Krahmer" ],
      "venue" : null,
      "citeRegEx" : "Wubben et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Wubben et al\\.",
      "year" : 2012
    }, {
      "title" : "Optimizing statistical machine translation for text simplification",
      "author" : [ "Wei Xu", "Courtney Napoles", "Ellie Pavlick", "Quanze Chen", "Chris Callison-Burch." ],
      "venue" : "Transactions of Association of Computational Linguistics.",
      "citeRegEx" : "Xu et al\\.,? 2016",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2016
    }, {
      "title" : "For the sake of simplicity: Unsupervised extraction of lexical simplifications from wikipedia",
      "author" : [ "Mark Yatskar", "Bo Pang", "Cristian Danescu-NiculescuMizil", "Lillian Lee." ],
      "venue" : "Proceedings of the Conference of the North American Chapter of the Associa-",
      "citeRegEx" : "Yatskar et al\\.,? 2010",
      "shortCiteRegEx" : "Yatskar et al\\.",
      "year" : 2010
    }, {
      "title" : "Learn to Solve Algebra Word Problems Using Quadratic Programming",
      "author" : [ "Lipu Zhou", "Shuaixiang Dai", "Liwei Chen." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Zhou et al\\.,? 2015",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2015
    }, {
      "title" : "A monolingual tree-based translation model for sentence simplification",
      "author" : [ "Zhemin Zhu", "Delphine Bernhard", "Iryna Gurevych." ],
      "venue" : "Proceedings of the International Conference on Computational Linguistics (COLING).",
      "citeRegEx" : "Zhu et al\\.,? 2010",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Automatic storytelling systems can be used for customized sport commentaries, enriching video games with personalized or dynamic plot-lines (Barros and Musse, 2007), or providing customized learning materials which meet each individual student’s needs and interests (Bartlett, 2004).",
      "startOffset" : 140,
      "endOffset" : 164
    }, {
      "referenceID" : 1,
      "context" : "Automatic storytelling systems can be used for customized sport commentaries, enriching video games with personalized or dynamic plot-lines (Barros and Musse, 2007), or providing customized learning materials which meet each individual student’s needs and interests (Bartlett, 2004).",
      "startOffset" : 266,
      "endOffset" : 282
    }, {
      "referenceID" : 14,
      "context" : ", 2002), and personalizing word problems increases student understanding, engagement, and performance in the problem solving process (Hart, 1996; Davis-Dorsey et al., 1991).",
      "startOffset" : 133,
      "endOffset" : 172
    }, {
      "referenceID" : 5,
      "context" : ", 2002), and personalizing word problems increases student understanding, engagement, and performance in the problem solving process (Hart, 1996; Davis-Dorsey et al., 1991).",
      "startOffset" : 133,
      "endOffset" : 172
    }, {
      "referenceID" : 30,
      "context" : "Rather than synthesizing first a story plot (McIntyre and Lapata, 2009; McIntyre and Lapata, 2010) or script (Chambers and Jurafsky, 2009; Pichotta and",
      "startOffset" : 44,
      "endOffset" : 98
    }, {
      "referenceID" : 31,
      "context" : "Rather than synthesizing first a story plot (McIntyre and Lapata, 2009; McIntyre and Lapata, 2010) or script (Chambers and Jurafsky, 2009; Pichotta and",
      "startOffset" : 44,
      "endOffset" : 98
    }, {
      "referenceID" : 30,
      "context" : ", McIntyre and Lapata (2010)) and sentence rewriting (e.",
      "startOffset" : 2,
      "endOffset" : 29
    }, {
      "referenceID" : 32,
      "context" : "researchers (Meehan, 1976; Lebowitz, 1987; Turner, 1993; Liu and Singh, 2002; Mostafazadeh et al., 2016).",
      "startOffset" : 12,
      "endOffset" : 104
    }, {
      "referenceID" : 25,
      "context" : "researchers (Meehan, 1976; Lebowitz, 1987; Turner, 1993; Liu and Singh, 2002; Mostafazadeh et al., 2016).",
      "startOffset" : 12,
      "endOffset" : 104
    }, {
      "referenceID" : 49,
      "context" : "researchers (Meehan, 1976; Lebowitz, 1987; Turner, 1993; Liu and Singh, 2002; Mostafazadeh et al., 2016).",
      "startOffset" : 12,
      "endOffset" : 104
    }, {
      "referenceID" : 28,
      "context" : "researchers (Meehan, 1976; Lebowitz, 1987; Turner, 1993; Liu and Singh, 2002; Mostafazadeh et al., 2016).",
      "startOffset" : 12,
      "endOffset" : 104
    }, {
      "referenceID" : 36,
      "context" : "researchers (Meehan, 1976; Lebowitz, 1987; Turner, 1993; Liu and Singh, 2002; Mostafazadeh et al., 2016).",
      "startOffset" : 12,
      "endOffset" : 104
    }, {
      "referenceID" : 25,
      "context" : "researchers (Meehan, 1976; Lebowitz, 1987; Turner, 1993; Liu and Singh, 2002; Mostafazadeh et al., 2016). Recent methods in story generation first synthesize candidate plots for a story and then compile those plots into text. Li et al. (2013) use crowd-",
      "startOffset" : 27,
      "endOffset" : 243
    }, {
      "referenceID" : 44,
      "context" : "McIntyre and Lapata (2009; 2010) address story generation through the automatic deduction and reassembly of scripts (Schank and Abelson, 1977), or structured representations of events and their participants, and causal",
      "startOffset" : 116,
      "endOffset" : 142
    }, {
      "referenceID" : 2,
      "context" : "Leveraging the automatic script learning methods of Chambers and Jurafsky (2009), McIntyre and Lapata (2010) learn candidate entity-centered plot graphs, or possible events involving the entity and an ordering between these events, with the use of a genetic algorithm.",
      "startOffset" : 52,
      "endOffset" : 81
    }, {
      "referenceID" : 2,
      "context" : "Leveraging the automatic script learning methods of Chambers and Jurafsky (2009), McIntyre and Lapata (2010) learn candidate entity-centered plot graphs, or possible events involving the entity and an ordering between these events, with the use of a genetic algorithm.",
      "startOffset" : 52,
      "endOffset" : 109
    }, {
      "referenceID" : 24,
      "context" : "plots are compiled into stories through the use of a rule-based text surface realizer (Lavoie and Rambow, 1997) and reranked using a language model.",
      "startOffset" : 86,
      "endOffset" : 111
    }, {
      "referenceID" : 54,
      "context" : "2 Additionally, there is related work in text simplification (Wubben et al., 2012; Kauchak, 2013; Zhu et al., 2010; Vanderwende et al., 2007; Woodsend and Lapata, 2011b; Hwang et al., 2015), sentence",
      "startOffset" : 61,
      "endOffset" : 189
    }, {
      "referenceID" : 19,
      "context" : "2 Additionally, there is related work in text simplification (Wubben et al., 2012; Kauchak, 2013; Zhu et al., 2010; Vanderwende et al., 2007; Woodsend and Lapata, 2011b; Hwang et al., 2015), sentence",
      "startOffset" : 61,
      "endOffset" : 189
    }, {
      "referenceID" : 58,
      "context" : "2 Additionally, there is related work in text simplification (Wubben et al., 2012; Kauchak, 2013; Zhu et al., 2010; Vanderwende et al., 2007; Woodsend and Lapata, 2011b; Hwang et al., 2015), sentence",
      "startOffset" : 61,
      "endOffset" : 189
    }, {
      "referenceID" : 50,
      "context" : "2 Additionally, there is related work in text simplification (Wubben et al., 2012; Kauchak, 2013; Zhu et al., 2010; Vanderwende et al., 2007; Woodsend and Lapata, 2011b; Hwang et al., 2015), sentence",
      "startOffset" : 61,
      "endOffset" : 189
    }, {
      "referenceID" : 53,
      "context" : "2 Additionally, there is related work in text simplification (Wubben et al., 2012; Kauchak, 2013; Zhu et al., 2010; Vanderwende et al., 2007; Woodsend and Lapata, 2011b; Hwang et al., 2015), sentence",
      "startOffset" : 61,
      "endOffset" : 189
    }, {
      "referenceID" : 18,
      "context" : "2 Additionally, there is related work in text simplification (Wubben et al., 2012; Kauchak, 2013; Zhu et al., 2010; Vanderwende et al., 2007; Woodsend and Lapata, 2011b; Hwang et al., 2015), sentence",
      "startOffset" : 61,
      "endOffset" : 189
    }, {
      "referenceID" : 22,
      "context" : "plots are compiled into stories through the use of a rule-based text surface realizer (Lavoie and Rambow, 1997) and reranked using a language model. Polozov et al. (2015) automatically generate math word problems tailored to a student’s interest using Answer Set Programming to satisfy a collection of pedagogical and narrative requirements.",
      "startOffset" : 87,
      "endOffset" : 171
    }, {
      "referenceID" : 39,
      "context" : "According to Polozov et al. (2015) building small thematic ontologies of types, relations, and discourse tropes (100-200 entries) for each of only 3 literary settings took 1-2 person months.",
      "startOffset" : 13,
      "endOffset" : 35
    }, {
      "referenceID" : 7,
      "context" : "compression (Filippova and Strube, 2008; Rush et al., 2015), and paraphrasing (Ganitkevitch et al.",
      "startOffset" : 12,
      "endOffset" : 59
    }, {
      "referenceID" : 43,
      "context" : "compression (Filippova and Strube, 2008; Rush et al., 2015), and paraphrasing (Ganitkevitch et al.",
      "startOffset" : 12,
      "endOffset" : 59
    }, {
      "referenceID" : 10,
      "context" : ", 2015), and paraphrasing (Ganitkevitch et al., 2013; Chen and Dolan, 2011; Ganitkevitch et al., 2011).",
      "startOffset" : 26,
      "endOffset" : 102
    }, {
      "referenceID" : 3,
      "context" : ", 2015), and paraphrasing (Ganitkevitch et al., 2013; Chen and Dolan, 2011; Ganitkevitch et al., 2011).",
      "startOffset" : 26,
      "endOffset" : 102
    }, {
      "referenceID" : 9,
      "context" : ", 2015), and paraphrasing (Ganitkevitch et al., 2013; Chen and Dolan, 2011; Ganitkevitch et al., 2011).",
      "startOffset" : 26,
      "endOffset" : 102
    }, {
      "referenceID" : 36,
      "context" : "Different rule-based and data-driven approaches are introduced by Petersen and Ostendorf (2007), Vickrey and Koller (2008), and Siddharthan (2004).",
      "startOffset" : 66,
      "endOffset" : 96
    }, {
      "referenceID" : 36,
      "context" : "Different rule-based and data-driven approaches are introduced by Petersen and Ostendorf (2007), Vickrey and Koller (2008), and Siddharthan (2004).",
      "startOffset" : 66,
      "endOffset" : 123
    }, {
      "referenceID" : 14,
      "context" : "Different rule-based and data-driven approaches are introduced by Petersen and Ostendorf (2007), Vickrey and Koller (2008), and Siddharthan (2004). Most data-driven approaches take advantage",
      "startOffset" : 132,
      "endOffset" : 147
    }, {
      "referenceID" : 56,
      "context" : "of machine translation techniques, use source-target sentence pairs, and learn rewrite operations (Yatskar et al., 2010; Woodsend and Lapata, 2011a), or use additional external paraphrasing resources (Xu et al.",
      "startOffset" : 98,
      "endOffset" : 148
    }, {
      "referenceID" : 52,
      "context" : "of machine translation techniques, use source-target sentence pairs, and learn rewrite operations (Yatskar et al., 2010; Woodsend and Lapata, 2011a), or use additional external paraphrasing resources (Xu et al.",
      "startOffset" : 98,
      "endOffset" : 148
    }, {
      "referenceID" : 55,
      "context" : ", 2010; Woodsend and Lapata, 2011a), or use additional external paraphrasing resources (Xu et al., 2016).",
      "startOffset" : 87,
      "endOffset" : 104
    }, {
      "referenceID" : 47,
      "context" : "Specific topics include number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al.",
      "startOffset" : 45,
      "endOffset" : 63
    }, {
      "referenceID" : 35,
      "context" : ", 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al.",
      "startOffset" : 31,
      "endOffset" : 54
    }, {
      "referenceID" : 23,
      "context" : "2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015; Koncel-Kedziorski et al., 2015a; Roy et al., 2016), and geometry word problems (Seo et al.",
      "startOffset" : 49,
      "endOffset" : 141
    }, {
      "referenceID" : 57,
      "context" : "2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015; Koncel-Kedziorski et al., 2015a; Roy et al., 2016), and geometry word problems (Seo et al.",
      "startOffset" : 49,
      "endOffset" : 141
    }, {
      "referenceID" : 20,
      "context" : "2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015; Koncel-Kedziorski et al., 2015a; Roy et al., 2016), and geometry word problems (Seo et al.",
      "startOffset" : 49,
      "endOffset" : 141
    }, {
      "referenceID" : 42,
      "context" : "2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015; Koncel-Kedziorski et al., 2015a; Roy et al., 2016), and geometry word problems (Seo et al.",
      "startOffset" : 49,
      "endOffset" : 141
    }, {
      "referenceID" : 46,
      "context" : ", 2016), and geometry word problems (Seo et al., 2015; Seo et al., 2014).",
      "startOffset" : 36,
      "endOffset" : 72
    }, {
      "referenceID" : 45,
      "context" : ", 2016), and geometry word problems (Seo et al., 2015; Seo et al., 2014).",
      "startOffset" : 36,
      "endOffset" : 72
    }, {
      "referenceID" : 22,
      "context" : "available (Koncel-Kedziorski et al., 2016; Huang et al., 2016), though none address the need for thematic text.",
      "startOffset" : 10,
      "endOffset" : 62
    }, {
      "referenceID" : 17,
      "context" : "available (Koncel-Kedziorski et al., 2016; Huang et al., 2016), though none address the need for thematic text.",
      "startOffset" : 10,
      "endOffset" : 62
    }, {
      "referenceID" : 34,
      "context" : "Specifically, we consider semantic relations for noun-noun and verb-verb pairs as provided by WordNet (Miller, 1995).",
      "startOffset" : 102,
      "endOffset" : 116
    }, {
      "referenceID" : 33,
      "context" : "The Analogy function, inspired by Mikolov et al. (2013), computes the analogy of w′ j fromw′ i given the relationship that holds betweenwi and wj in the vector space.",
      "startOffset" : 34,
      "endOffset" : 56
    }, {
      "referenceID" : 29,
      "context" : "Implementation Details We pre-process the themes using the Stanford CoreNLP tools (Manning et al., 2014) for tokenization, Named Entity Recognition (Finkel et al.",
      "startOffset" : 82,
      "endOffset" : 104
    }, {
      "referenceID" : 8,
      "context" : ", 2014) for tokenization, Named Entity Recognition (Finkel et al., 2005), and dependency",
      "startOffset" : 51,
      "endOffset" : 72
    }, {
      "referenceID" : 4,
      "context" : "parsing (Chen and Manning, 2014).",
      "startOffset" : 8,
      "endOffset" : 32
    }, {
      "referenceID" : 12,
      "context" : "For calculating salience scores, we use the ScriptBase dataset of movie scripts (Gorinski and Lapata, 2015).",
      "startOffset" : 80,
      "endOffset" : 107
    }, {
      "referenceID" : 21,
      "context" : "To prohibit overgeneration, we forbid the transformation of stop words or math-specific words (Survivors, 2013; Koncel-Kedziorski et al., 2015b).",
      "startOffset" : 94,
      "endOffset" : 144
    }, {
      "referenceID" : 11,
      "context" : "tion 4) we use the English Fiction subset of the Google Syntactic N-grams corpus (Goldberg and Orwant, 2013) and train a 3-gram language model using KenLM (Heafield, 2011).",
      "startOffset" : 81,
      "endOffset" : 108
    }, {
      "referenceID" : 15,
      "context" : "tion 4) we use the English Fiction subset of the Google Syntactic N-grams corpus (Goldberg and Orwant, 2013) and train a 3-gram language model using KenLM (Heafield, 2011).",
      "startOffset" : 155,
      "endOffset" : 171
    }, {
      "referenceID" : 6,
      "context" : "Finally, we tune the parameters of our model (Equation 2) on the development set STARdev and pick those values5 that maximize METEOR score (Denkowski and Lavie, 2014) against 3 human references.",
      "startOffset" : 139,
      "endOffset" : 166
    }, {
      "referenceID" : 25,
      "context" : "the pretrained word embeddings of Levy and Goldberg (2014). These embeddings are trained using dependency contexts rather than windows of adjacent words, allowing them to capture functional word similarity.",
      "startOffset" : 34,
      "endOffset" : 59
    } ],
    "year" : 2016,
    "abstractText" : "Texts present coherent stories that have a particular theme or overall setting, for example science fiction or western. In this paper, we present a text generation method called rewriting that edits existing human-authored narratives to change their theme without changing the underlying story. We apply the approach to math word problems, where it might help students stay more engaged by quickly transforming all of their homework assignments to the theme of their favorite movie without changing the math concepts that are being taught. Our rewriting method uses a twostage decoding process, which proposes new words from the target theme and scores the resulting stories according to a number of factors defining aspects of syntactic, semantic, and thematic coherence. Experiments demonstrate that the final stories typically represent the new theme well while still testing the original math concepts, outperforming a number of baselines. We also release a new dataset of human-authored rewrites of math word problems in several themes.",
    "creator" : "LaTeX with hyperref package"
  }
}