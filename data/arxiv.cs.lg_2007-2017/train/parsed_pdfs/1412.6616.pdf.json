{
  "name" : "1412.6616.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "demski@usc.edu", "ustun@ict.usc.edu", "rosenbloom@ict.usc.edu", "cydeko@ucla.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n41 2.\n66 16\nv1 [\nWe present a distributed vector representation based on a simplification of the BEAGLE system, designed in the context of the Sigma cognitive architecture. Our method does not require gradient-based training of neural networks, matrix decompositions as with LSA, or convolutions as with BEAGLE. All that is involved is a sum of random vectors and their pointwise products. Despite the simplicity of this technique, it gives state-of-the-art results on analogy problems, in most cases better than Word2Vec. To explain this success, we interpret it as a dimension reduction via random projection."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "DVRS is a distributed representation of words designed for the Sigma cognitive architecture (Ustun et al., 2014). Inspired by the design of BEAGLE (Jones & Mewhort, 2007), DVRS constructs vector representations for words by summing up vectors representing its experiences. The details of DVRS were influenced strongly by the Sigma cognitive architecture. Implementing distributed representations in a cognitive architecture such as Sigma provides design constraints, due to the underlying theory of cognition which the architecture embodies. Unlike pure machine learning, performance is not the only goal; we also hope to find cognitive principles. Nonetheless, the performance turned out to be quite good.\nDVRS does not rely on the gradient-based training of neural embeddings (Mikolov et al., 2013; Mnih & Kavukcuoglu, 2013), or optimization of any other objective as with GloVe (Pennington et al., 2014). It does not involve a matrix factorization as with latent semantic analysis and similar techniques (Baroni et al., 2014). It does not even require the circular convolutions of BEAGLE and earlier holographic representations (Jones & Mewhort, 2007) (Plate, 1995). The representations are built up from random vectors using only vector sum and pointwise product. As a result, it is especially simple to implement.\nThis paper presents further work on the DVRS system. The performance of DVRS and Word2Vec is examined side-by-side on analogy problems, and DVRS has a higher accuracy in a majority of cases.\nIn order to explain the success of the algorithm, a theoretical account is provided, discussing its interpretation as a random projection method. The analysis shows that DVRS can handle a vocabulary size exponential in the vector size."
    }, {
      "heading" : "2 DISTRIBUTED REPRESENTATIONS OF WORDS",
      "text" : "Vector representations put words in a linear space, allowing for geometric computations and often capturing useful semantic information. Each word is associated with a vector of numbers which define its “location” in the semantic space. If the representation is good, words representing similar concepts will end up near to each other. Directions in the space can also have meaning, which are useful in the solution of analogy problems (Sections 3.1 and 3.2).\nDistributed representations of various kinds have been experimented with since the 60s, according to the technical report Hinton (1984). Already in that survey, distributed representations of words were a topic. Hinton (1984) also discussed the binding problem, which is the question of how to build up a representation of a pair of objects.\nPlate Plate (1995) describes a system of distributed representations which solves the binding problem using a method called circular convolution. This is a modification of standard convolution in order to keep the size of the representations from growing as a result of binding. The binding operation and a set of other operations on the vectors form a kind of algebra over concepts. 1\nThe BEAGLE system Jones & Mewhort (2007) uses the circular convolution binding to build up distributed representations of words. BEAGLE reads text and constructs vector representations in an unsupervised way based on co-occurrence and ordering information. Each word wi is assigned a random vector which does not change, called the environment vector (which we will denote as ~ei). These are used to build up the semantic representation, which in BEAGLE is called the lexical vector. (We will write ~li.) This terminology will be carried over to the discussion of DVRS for the sake of standardization.\nThe lexical vector for a word is constructed as the sum of context information and ordering information. Context information refers to simple co-occurrence: when word i occurs in the same sentence as word j, we add the environment vector ~ej into the lexical vector~li. Ordering information consists of vectors ~oi which capture information about n-tuples within which the word occurs. BEAGLE constructs these using a formula involving circular convolution, and then adds them into the lexical vector.\nDVRS, BEAGLE, and their precursors were motivated partially by performance, but largely by cognitive-modeling considerations. This includes a concern for how human memory works, and a desire to find a set of cognitive mechanisms which can model it.\nThis is very different from the machine learning approach, which focuses on performance first and puts unifying principles second. Latent Semantic Analysis (LSA) is a well-established and widelyused algorithm in this category, but rapid progress has recently been made in neural-network based embeddings. Neural models are compared extensively to LSA-style approaches by Baroni et al. (2014). Neural models, and in particular the Word2Vec model, are declared the winner. We will be using Word2Vec as our comparison.\nIn terms of a cognitive algebra, BEAGLE can be seen as a result of just two operations: vector addition and circular convolution. Vector addition gathers things together like a multiset or bag, while circular convolution (as the binding operation) establishes relationship between things, building ordered pairs.\n1The algebraic view seems common in the field. Different systems have different basic operations. Algebraic features of the operations such as commutativity, associativity, and the existence of an inverse are of interest.\nThe design of DVRS is similar to this, but does not use the circular convolution operation from Plate (1995). Circular convolution is O(n logn), and this operation is not conveniently supported within the Sigma architecture. A more natural approach was pointwise multiplication: combining two vectors by multiplying their corresponding elements. This fit well with the existing operations in Sigma, which uses the summary-product algorithm Ustun et al. (2014), and is less expensive, taking O(n) time."
    }, {
      "heading" : "3 THE DVRS MODEL",
      "text" : "Like BEAGLE, for each word wi DVRS has an associated environment vector ~ei and we will build a lexical vector ~li. The environment vectors are random unit vectors which remain fixed. They are formed by sampling their elements uniformly from [−1, 1] and normalizing the result to length 1. All vectors in the system are of a set dimensionality d. When we need to indicate accessing individual dimensions of a vector ~v, it will be notated by vi with no arrow above, so ~v = 〈v1, v2, . . . , vd〉. An indexed vector variable like ~vi, on the other hand, represents one of a collection of vectors ~v1, ~v2, . . . .\nThe lexical vectors are constructed from environment vectors and ordering vectors ~o. The term feature (indicated as ~fi) will be used to refer generically to either an environment vector or an ordering vector; these play the role of individual features in the representation. Note that the number of ~ei and ~li are both equal to the number of words in the vocabulary, but the number of ~oi will be larger, and ~fi (being the union of the ~ei and ~oi) larger still.\nThe lexical vector~li is built incrementally by summing the features relevant to each instance of word wi. We will refer to the current instance being calculated as the target word. For each target word, we find co-occurrence information and ordering information.\nThe co-occurrence information is very similar to BEAGLE, but the ordering information differs significantly. The information stored in these features is more similar to the skip-gram architecture in Mikolov et al. (2013), rather than the n-gram architecture of BEAGLE. DVRS uses the binding operation only to make pairwise bindings of words and word locations, whereas BEAGLE applies it successively to bind short strings of words together.\nAdditionally, the choice of binding operation in DVRS is different. BEAGLE used circular convolution to bind two vectors together into a third. DVRS instead uses pointwise product: for two vectors ~v and ~w, the pointwise product is ~v .∗ ~w = 〈v1w1, ..., vdwd〉. We performed limited tests showing no advantage to circular convolution, detailed in Ustun et al. (2014). A similar formula to ours is mention in Mnih & Kavukcuoglu (2013).\nThe co-occurrence features consist of the environment vectors of all other words in the current paragraph.2 Ordering information, on the other hand, is collected from words immediately surrounding the target word in the sentence. For each position in a window around the word, we have a sequence vector which encodes that location. The window is currently set to include four words in either direction, so we have eight sequence vectors, ~s−4, ~s−3, ~s−2, ~s−1, ~s1, ~s2, ~s3, ~s4. Like the environment vectors, these eight vectors remain constant, and are formed by uniformly sampling their entries from the range [−1, 1] and normalizing. To compute the ordering vector ~o which will be used as a feature, the binding operation is applied to combine the environment vector of the context-word in location j (relative to the current target word) with the sequence vector ~sj . This gives a vector representing a specific word at a specific location (relative to the target word). These features are not collected across sentence boundaries, however; for example, when the target word is the last in a sentence, only four ordering vectors are collected.\nThe co-occurrence information and the ordering information are separately summed together and normalized, so that we have two vectors of length 1, representing the different kinds of information.\n2A typical set of stop-words was excluded from the co-occurrence information. Stop-words were not excluded from the ordering information, however.\nThis allows us to re-weight the information before adding them together. We found that a weight of 1 for co-occurrence and 0.6 for ordering information worked well.\nThe combined vector is then added to the lexical vector, and we set the target word to the next word in the corpus.\nPutting it all together, define occ(wi, wj) to be the set of co-occurences between wi and wj in the paragraphs, and let occ(wi, wj , n) be the subset where wj occurs exactly n words after wi (with negative n indicating an occurrence before wi). The lexical vector for a word is obtained as follows:\n~li = ∑\nocc(wi,wj)\n~ej + 0.6 ∑\nn\n∑\nocc(wi,wj ,n)\n~ej .∗ ~sn\nwhere n ranges from -4 to 4, excluding zero.\nThis simple sum of features performs surprisingly well."
    }, {
      "heading" : "3.1 GOOGLE WORD ANALOGY TEST",
      "text" : "Mikolov et al. (2013) showed that distributed vector representations capture various relational similarities and these similarities can be recovered and used to solve word analogy tasks. These tasks simply ask, for example, the question “What is the word that is similar to small in the same way that biggest is similar to big?”. We would like to answer this question by first inferring the relation and then applying it to the first word, small in this case. Such questions can be answered by performing simple algebraic operations over the vector representations. To find the word that is most similar to small in the same way that biggest is similar to big, one can compute the vector ~V = (~lbiggest − ~lbig ) + ~lsmall and determine which word’s lexical vector is closest to this vector according to cosine similarity. This is interpreted as finding the direction (in the embedding space space) from big to biggest, and then looking the same direction from small to find smallest.\nThe results reported in this section use the Google dataset introduced in Mikolov et al. (2013), GOOGLEfull and a variant of it, generated by randomly selecting approximately 10% of the questions in the Google dataset, GOOGLEreduced. The GOOGLEfull dataset contains 19544 questions and the GOOGLEreduced contains 1986 questions. The results obtained using the GOOGLEfull dataset are reported in parentheses in the tables. The GOOGLEreduced dataset is representative of the GOOGLEfull dataset as pointed out by the results in the last column of Table 3. The reduced set was therefore used for most of the experiments.\nThree different datasets are used in training DVRS models: (1) Enwik8 is the first 108 bytes of the English wikipedia dump on March 3, 2006, (2) Enwik9 is the first 109 bytes of the same dump and (3) Enwik2013 is the full English Wikipedia dump on August 5, 2013. Each training set is preprocessed to contain only lowercase characters and spaces, using the script in (Mahoney).\nTable 1 show the accuracy of the DVRS model trained on Enwik8 and Enwik9 with various vector dimensions. The accuracy significantly improves with Enwik9 over Enwik8 and bigger vector sizes result in better accuracy with diminishing returns; the accuracy plateaus with vector sizes over 1000.\nIt is apparent from Table 2 that co-occurrence information is the main contributor to the accuracy achieved in the Google Word Analogy Test. The impact of the ordering information by itself is limited, however, the composition of co-occurrence and ordering information works substantially better than the sum of their individual performance.\nTable 3 provides a comparison between DVRS and Word2Vec.3 The Word2Vec code is run with standard parameter set that is defined in the download of the Word2Vec. The accuracy number for Word2Vec trained on the large Enwik2013 dataset is taken from Levy & Goldberg (2014). DVRS performs better than Word2Vec on Enwik8 and Enwik9 for vector sizes of 200 and 512. Surprisingly, however, the DVRS model did not show any improvement with the very large Enwik2013 dataset\n3The results in this paper use 2013 Word2Vec. A new version of Word2Vec was released September 2014, which our initial tests show to perform significantly better, outperforming DVRS on the Google analogy task. No publication yet explains the algorithmic changes involved, so it is difficult to draw a meaningful conclusion from this as of yet."
    }, {
      "heading" : "100 15.5% 32.5%",
      "text" : ""
    }, {
      "heading" : "200 22.9% 41.7%",
      "text" : ""
    }, {
      "heading" : "512 28.3% 46.5%",
      "text" : ""
    }, {
      "heading" : "1024 30.1% (30.5%) 47.1%",
      "text" : ""
    }, {
      "heading" : "100 25.4% 3.3% 32.5%",
      "text" : ""
    }, {
      "heading" : "200 32.4% 4.6% 41.7%",
      "text" : ""
    }, {
      "heading" : "512 35.2% 5.5% 46.5%",
      "text" : ""
    }, {
      "heading" : "1024 36.2% 5.8% 47.1%",
      "text" : "over Enwik9, unlike Word2Vec. This causes DVRS to trail significantly behind Word2Vec. It seems possible that DVRS is hitting a plateau, unable to improve with more data.\nTable 4 presents a breakdown of the accuracy between DVRS and Word2Vec trained on Enwik9 with a vector size of 512. The DVRS model captures the semantic information better, whereas the original Word2Vec captures the syntactic information better.\nA straightforward implementation of the DVRS mode in Common Lisp without any optimized parallel processing takes 18.5 minutes to train on Enwik8 with a vector size of 100 on a MacBookAir4,2. It takes 4.5 minutes for Word2Vec to train with the same data on the same machine using the standard configuration, which utilizes 20 threads."
    }, {
      "heading" : "3.2 MILLER ANALOGIES TEST",
      "text" : "The Miller Analogies Test (MAT) is an assessment of analytical skills used for graduate school admissions, similar to the Graduate Record Examinations (GRE). Performance on the MAT predicts individual graduate school grade point average better than GRE scores or undergraduate GPA. As a rigorous test of cognitive abilities for individuals of above average intelligence, competitive results on the MAT indicate sophisticated reasoning and representation similar to a well educated human.\nWe selected 150 MAT example questions from an free online test preparation website to compare performance of DVRS and Word2Vec. These analogies require sophisticated vocabulary and complex relational representation. Each analogy takes the form A:B::C:(a,b,c,d). ”A is to B as C is to which of the four options?” For example, germinal is to senescent as nascent is to (a) sophomoric, (b) covetous, (c) moribund, or (d) shrewd? (Answer: moribund). Better is to worse as ameliorate is to (a) elucidate, (b) engender, (c) accredit, or (d) exacerbate? (Answer: exacerbate).\nIn our tests, Word2Vec did a little better than DVRS, at 46.9% for DVRS and 50.0% for Word2Vec.\nAnswering correctly on these analogies requires complex representations of words that are seen relatively few times in the training data. Based on a conversion chart from California State University, Stanislaus, a raw MAT score of 46% converts to a scaled score of 408. Although percentile ranking depends on the group with whom the test is taken, 400 tends to be near the 50th percentile because the test is scored from 200 to 600. Therefore, DVRS and Word2Vec appear to be on par with the average human graduate school applicant."
    }, {
      "heading" : "4 RANDOM PROJECTIONS",
      "text" : "It would be helpful to explain the performance of DVRS with a mathematical model of what it is computing. Why does adding random vectors together give us anything of value? Why is it possible to solve analogy problems with vector addition and subtraction, and why is the cosine similarity semantically meaningful? The analysis of this section can’t completely answer these questions, but useful things can be said by viewing DVRS as a random projection method.\nRandom projections are a very simple dimensionality reduction technique, which applies a randomly chosen linear transformation from a high-dimensional space to a lower-dimensional one (a random matrix). The method is justified by the Johnson-Lindenstrauss lemma, which established that relative distances are preserved with high probability under random projection (Indyk & Motwani, 1998). This shows that almost all dimension reductions are good (in the sense of preserving relative distance), so it suffices to choose one randomly.\nRandom projections typically compare well to more traditional techniques of dimensionality reduction like singular value decomposition and principle component analysis (Bingham & Mannila, 2001). An early application to computational learning theory was via the notion of robust concepts: classes which have a large distance between their closest data points (that is, a large margin) (Arriaga & Vempala, 1999).\nOther useful results about random vectors include an application to clustering. When clustering very high-dimensional data, it is useful to reduce computational cost by first applying dimensionreduction to the data. For learning Gaussian clusters, it was shown that random projections can perform well with a number of dimensions logarithmic in the number of clusters, whereas principle component analysis may require a number of dimensions linear in the number of clusters Dasgupta (2000). Another result, of great relevance to this paper, is the ability to fit exponentially many\nnearly-orthogonal vectors into a space. This will be discussed in more detail in section 4.1. We will see that there is a similar logarithmic requirement for distributed representations of words, which means that we can represent a large vocabulary with reasonably small vector sizes."
    }, {
      "heading" : "4.1 PROPERTIES OF RANDOM VECTORS",
      "text" : "The most important property for the system is the meaning of the cosine similarity. Why does this measure of similarity work well? What can we say about the numbers which come out of it? The argument of this section is that cosine similarity counts the fraction of features that two lexical vectors have in common.\nThe cosine similarity of two vectors ~v and ~w is their dot product divided by their lengths, ~v· ~w/|~v||~w|. (DVRS normalizes all vectors to length 1, so that there is no difference between the cosine similaroty and the dot product.) The dot product of two independent random vectors of unit length is very probably very close to zero. In other words, random vectors are nearly orthogonal. This has been known for some time, but was studied in detail recently in Cai et al. (2013). Due to this, it is possible to sum random vectors ~s = ∑\ni ci~vi and recover the counts ci by taking the dot product ~vi · ~s. The dot product distributes over the sum, and ~vi · ~vi = 1. Because the other vectors contribute almost zero to the result, the result is close to ci. This allows us to recover feature frequencies from cosine similarity.\nThe near-orthogonality of random vectors is the key A basic statistical analysis of the phenomenon is presented here to develop an insight into why this happens.\nEach random vector ~v ∈ Rd is created by sampling d times from a random variable X symmetric about zero and standard deviation σ The cosine similarity of two such vectors sim(~v, ~w) can be written as:\nd ∑\ni=1\nviwi |~v||~w| (1)\nIt is easy to see that the expected value is zero, since the symmetry about zero is preserved from the distribution of X . To show that it is in fact quite close to zero, we examine the variance. The variance of a sum of random variables is the sum of their pairwise covariances. The variance of (1) becomes:\nV ar[sim(~v, ~w)] =\nd ∑\ni=1\nd ∑\nj=1\nE ( viwivjwj ∑d\nk=1 v 2 k\n∑d\nl=1 w 2 l\n)\n(2)\n=\nd ∑\ni=1\nd ∑\nj=1\nE ( vivj ∑d\nk=1 v 2 k\n) E ( wiwj ∑d\nl=1 w 2 l\n)\n(3)\nAll the expected values in this are zero except when i = j, because whenever vivj is positive, it’s equally likely for it to have been negative, with the same denominator (and similarly for the expression involving ~w).\n= d ∑\ni=1\nE ( v2i ∑d\nk=1 v 2 k\n) E ( w2i ∑d\nl=1 w 2 l\n) = d ∑\ni=1\nE ( v2i ∑d\nk=1 v 2 k\n)\n2 (4)\nThe fraction inside the expectation is just the result of taking d samples from X2 and dividing the ith by the sum of them all. By symmetry in i, this must have expected value 1/d. Thus we have:\n= d ∑\ni=1\n1\nd2 = 1/d (5)\nIntuitively, what’s going on in this computation is that while the length normalization stops us from assuming independence of the different entries in the vector, they nonetheless have no covariance thanks to the symmetry of the distribution. As a result, the variance of the dot product can be computed from the variance of the individual products. These are 1/d2 as a result of the normalization, so their sum is 1/d.\nApplying Chebyshev’s inequality,\nP (|sim(~v, ~w)| ≥ ǫ) ≤ 1 ǫ2d\n(6)\nThus, for large enough d, the chance of similarity above any desired ǫ becomes arbitrarily small. This proof is similar to a typical proof of the law of large numbers. If we were dividing by d rather than dividing by |~v||~w|, the law of large numbers would apply directly: we would be taking the mean of the pointwise product of vectors. Since the expected value is zero, we would then conclude that the mean will converge to zero. What has been shown here is that the vector normalization serves the same role, driving the result closer to zero as the size of the vectors is increased.\nCai et al. (2013) examine the properties of a set of n random vectors of dimensionality d.4 Their results go beyond simple pairwise properties of two random vectors. Since a distributed vector representation is likely to use a large number of vectors, this is quite useful to the current subject. They show (Theorem 6) that as d → ∞, if log(n)/d → 0, then the minimum and maximum angles between any of the n vectors approaches 90◦. This is an extremely strong result; it means that for large d, we would have to be using exponentially many random vectors before we will notice significant collisions between any two random vectors. Furthermore, they show (Theorem 8) that we can put a bound on accidental similarity with high probability: given a particular ratio β = n/ed, the minimum and maximum similarities converge to ± √ 1− e−4β . Therefore, for a fixed tolerance ǫ > 0 for erroneous similarity, the maximum number of random vectors we can use is about:\nn = −e d\n4 ln(1 − ǫ2) (7)\nAgain, this is based on the worst accidental similarity between any two random vectors. This indicates that the cosine similarity will likely be less than ǫ for all pairs of random vectors in the system, until we exceed this number. (The rule is still probabilistic, however!)\nTo understand the vectors used in DVRS, it is also necessary to examine the properties of pointwise products (the binding operation). Recall, pointwise product is given by the formula ~v . ∗ ~w = 〈v1w1, ..., vdwd〉. In order for the pointwise product to act as if it were a new random vector, this should be nearly orthogonal to its constituent vectors. Consider the dot product ~v · (~v .∗ ~w) = ∑di=1 v2i wi z2vzw\n. The expected value of this is zero as before, and similarly, the covariance of different terms is zero. So, we compute the variance as:\nV ar [~v · (~v .∗ ~w)] = d ∑\ni=1\nE [ ( v2iwi z2vzw )2 ] = d ∑\ni=1\nE( v4i z4v )E( w2i z2w ) = d ∑\ni=1\nE( v4i z4v ) 1 d ≤ 1 d (8)\nTo justify the final inequality, let Yi be d random variables distributed as X2. The expression ∑d\ni=1 E(v 4 i /z 4 v) = E(\n∑d\ni=1 v 4 i /(\n∑d\ni=1 v 2)2) is then E((\n∑d\ni=1(Yi) 2)/(\n∑d\ni=1 Yi) 2). This is the\nsum of the square of positive numbers divided by the square of their sum, so it must be less than or equal to 1.\nThis allows us to conclude that the variance of ~v · (~v .∗ ~w) goes to zero at least as fast as the variance of ~v · ~w. As a result, we conclude that ~v .∗ ~w acts as if it is a new random vector, nearly orthogonal to both its constituent vectors.\n4They make the assumption that X is normally distributed, which we do not make in DVRS. This is an elegant assumption, because it is equivalent to sampling points uniformly from the unit hypersphere. However, the properties of random vectors are not very sensitive to this assumption, since the sum of iid random numbers approximates a normal distribution quickly. This can be verified with very simple experiments."
    }, {
      "heading" : "4.2 DVRS AS A RANDOM PROJECTION",
      "text" : "DVRS assigns random vectors to each word, and through the binding operation, assigns vectors which act random to the ordering information as well.5 This amounts to a F × d projection matrix, where F is the number of features. We compute the matrix multiplication by iterating through the data and summing up the vectors for features we observe.\nIn Table 5, we see a very small example projection of 5 features down to a 3-dimensional space. The features could be co-occurrence with words, for example. The numbers in the columns correspond to the environment vectors for each word. In order to generate the lexical vector for one of the words, we count co-occurrences in the data. This gives us a co-occurrence vector with 5 elements. Table 5 is the projection matrix which transforms this co-occurrence count vector into the lexical vector for the word.\nThe set of feature vectors can be thought of as a pseudo-basis for the reduced space. A true basis for a vector space is a set of vectors that are linearly independent and span the space. In an orthogonal basis, we additionally have that each basis vector is orthogonal to all others. Unfortunately, there is only room for d basis vectors, so it is not possible to assign independent vectors to each feature when projecting down from a higher-dimensional feature space. The surprising fact from the previous section is that although we can’t make them linearly independent, we can find a set that is very nearly orthogonal, accommodating F that grows exponentially in d. Furthermore, we can find such a pseudo-basis with almost no effort.\nConsider the dot product of a feature vector and a lexical vector, ~fi · ~lj . The lexical vector is the (normalized) sum of many feature vectors; we can write it as (1/z)\n∑F k=1 ck ~fk where ck is the number of occurrences of each feature ~fk and z is the normalizing constant. The dot product is a linear function in each argument, so ~fi ·~lj = ∑F k=1 ck ~fk · ~fi. When i = k, the dot product is exactly 1. The rest of the sum constitutes noise from the random projection, which is equally likely to be positive or negative. Treating the normalization constant as fixed, the expected value of ~fi · ~lj is ci/z, the normalized number of occurrences of feature i. This shows that the dot product will be proportional to the feature count.6 The lexical vectors are approximately encoding a much larger feature-count table.\nThe noise from random projections will be less than ∑F\nk=1 ckǫ/z, the sum of the maximum error for all vectors over each component of the sum. Since the feature vectors are nearly orthogonal, z will be close to √ ∑F\nk=0 c 2 k = |~c|, the length of the features before projection. This must be greater\nthan or equal to ∑F\nk=1 ck, establishing that the noise will be less than ǫ. (Since ǫ is the worst error in the system, it is likely to be much smaller than this.)\nSimilarly, the dot product of two lexical vectors ~li ·~lj will approximate the agreement between their features, ~li · ~lj = (1/zi) ∑F k=1 c i k ~fi · ~lj ≈ (1/zizj) ∑F k=1 c i kc j k, where c i k and c j k represent the value of the feature k for words i and j respectively, and zi and zj are the normalizing terms for their lexical vectors. This is related to the common observation that a random projection tends to\n5We would not want to generate new random vectors for all possible pieces of ordering information, as well, because this would require a large table to store. So, it is better to generate something deterministically which behaves as if it is random. The pointwise product achieves this. BEAGLE achieves something similar, but with more expensive operations.\n6Although we cannot treat z as fixed when computing the expected value of this dot product, z is fixed across different choice of ~fi; so, the ratios of the dot products are as desired, modulo the noise from the projection.\npreserve distance. The contribution of noise to this will be less than (1/zi) ∑F k=1 c i kǫ ≤ ǫ, by the same argument made in the case of ~fi ·~lj . What all of this indicates is the vocabulary that DVRS can handle is exponential in the vector length. The ability of sums of random vectors to approximately store so much information is rather striking."
    }, {
      "heading" : "4.3 DISCUSSION",
      "text" : "We presented DVRS, a novel distributed representation which compares well to Word2Vec in analogy tests. DVRS seems to perform well on especially semantic analogies, whereas Word2Vec tends to do better on grammar-based analogies. DVRS did not do better than Word2Vec on the Miller analogy test, and more worryingly, it failed to improve significantly when fed significantly more data, causing it to do worse than Word2Vec when compared on that dataset. This was a surprise, and\nDVRS does not require any gradient-descent training or other optimization process, nor does it use a matrix factorization or any probabilistic topic model. Instead, it constructs meaningful representations from sums and pointwise products of random vectors. The result can be described as a random projection, indicating that cosine similarity of the lexical vectors can effectively measure the similarity of the feature counts.\nFor machine learning applications, DVRS demonstrates the viability of random projections for competitive-quality word embeddings. For cognitive modeling, DVRS shows that circular convolution can be replaced by pointwise product as a binding operation, achieving very good performance."
    } ],
    "references" : [ {
      "title" : "An algorithmic theory of learning: Robust concepts and random projection",
      "author" : [ "Arriaga", "Rosa I", "Vempala", "Santosh" ],
      "venue" : "In Foundations of Computer Science,",
      "citeRegEx" : "Arriaga et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Arriaga et al\\.",
      "year" : 1999
    }, {
      "title" : "Don’t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors",
      "author" : [ "Baroni", "Marco", "Dinu", "Georgiana", "Kruszewski", "Germán" ],
      "venue" : "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Baroni et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Baroni et al\\.",
      "year" : 2014
    }, {
      "title" : "Random projection in dimensionality reduction: applications to image and text data",
      "author" : [ "Bingham", "Ella", "Mannila", "Heikki" ],
      "venue" : "In Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "citeRegEx" : "Bingham et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Bingham et al\\.",
      "year" : 2001
    }, {
      "title" : "Distributions of angles in random packing on spheres",
      "author" : [ "Cai", "Tony", "Fan", "Jianqing", "Jiang", "Tiefeng" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Cai et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Cai et al\\.",
      "year" : 2013
    }, {
      "title" : "Experiments with random projection",
      "author" : [ "Dasgupta", "Sanjoy" ],
      "venue" : "In Proceedings of the Sixteenth conference on Uncertainty in artificial intelligence,",
      "citeRegEx" : "Dasgupta and Sanjoy.,? \\Q2000\\E",
      "shortCiteRegEx" : "Dasgupta and Sanjoy.",
      "year" : 2000
    }, {
      "title" : "Approximate nearest neighbors: towards removing the curse of dimensionality",
      "author" : [ "Indyk", "Piotr", "Motwani", "Rajeev" ],
      "venue" : "In Proceedings of the thirtieth annual ACM symposium on Theory of computing,",
      "citeRegEx" : "Indyk et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Indyk et al\\.",
      "year" : 1998
    }, {
      "title" : "Representing word meaning and order information in a composite holographic lexicon",
      "author" : [ "Jones", "Michael N", "Mewhort", "Douglas JK" ],
      "venue" : "Psychological review,",
      "citeRegEx" : "Jones et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Jones et al\\.",
      "year" : 2007
    }, {
      "title" : "Linguistic regularities in sparse and explicit word representations",
      "author" : [ "Levy", "Omer", "Goldberg", "Yoav" ],
      "venue" : "In Proceedings of the Eighteenth Conference on Computational Natural Language Learning,",
      "citeRegEx" : "Levy et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Levy et al\\.",
      "year" : 2014
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Mikolov", "Tomas", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey" ],
      "venue" : "arXiv preprint arXiv:1301.3781,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Learning word embeddings efficiently with noisecontrastive estimation",
      "author" : [ "Mnih", "Andriy", "Kavukcuoglu", "Koray" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Mnih et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2013
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Pennington", "Jeffrey", "Socher", "Richard", "Manning", "Christopher D" ],
      "venue" : "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014),",
      "citeRegEx" : "Pennington et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Holographic reduced representations",
      "author" : [ "Plate", "Tony A" ],
      "venue" : "Neural networks, IEEE transactions on,",
      "citeRegEx" : "Plate and A.,? \\Q1995\\E",
      "shortCiteRegEx" : "Plate and A.",
      "year" : 1995
    }, {
      "title" : "Distributed vector representations of words in the sigma cognitive architecture",
      "author" : [ "Ustun", "Volkan", "Rosenbloom", "Paul S", "Sagae", "Kenji", "Demski", "Abram" ],
      "venue" : "In Proceedings of the 7th conference on Artificial General Intelligence,",
      "citeRegEx" : "Ustun et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Ustun et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "DVRS is a distributed representation of words designed for the Sigma cognitive architecture (Ustun et al., 2014).",
      "startOffset" : 92,
      "endOffset" : 112
    }, {
      "referenceID" : 8,
      "context" : "DVRS does not rely on the gradient-based training of neural embeddings (Mikolov et al., 2013; Mnih & Kavukcuoglu, 2013), or optimization of any other objective as with GloVe (Pennington et al.",
      "startOffset" : 71,
      "endOffset" : 119
    }, {
      "referenceID" : 10,
      "context" : ", 2013; Mnih & Kavukcuoglu, 2013), or optimization of any other objective as with GloVe (Pennington et al., 2014).",
      "startOffset" : 88,
      "endOffset" : 113
    }, {
      "referenceID" : 1,
      "context" : "It does not involve a matrix factorization as with latent semantic analysis and similar techniques (Baroni et al., 2014).",
      "startOffset" : 99,
      "endOffset" : 120
    }, {
      "referenceID" : 1,
      "context" : "Neural models are compared extensively to LSA-style approaches by Baroni et al. (2014). Neural models, and in particular the Word2Vec model, are declared the winner.",
      "startOffset" : 66,
      "endOffset" : 87
    }, {
      "referenceID" : 12,
      "context" : "This fit well with the existing operations in Sigma, which uses the summary-product algorithm Ustun et al. (2014), and is less expensive, taking O(n) time.",
      "startOffset" : 94,
      "endOffset" : 114
    }, {
      "referenceID" : 8,
      "context" : "The information stored in these features is more similar to the skip-gram architecture in Mikolov et al. (2013), rather than the n-gram architecture of BEAGLE.",
      "startOffset" : 90,
      "endOffset" : 112
    }, {
      "referenceID" : 8,
      "context" : "The information stored in these features is more similar to the skip-gram architecture in Mikolov et al. (2013), rather than the n-gram architecture of BEAGLE. DVRS uses the binding operation only to make pairwise bindings of words and word locations, whereas BEAGLE applies it successively to bind short strings of words together. Additionally, the choice of binding operation in DVRS is different. BEAGLE used circular convolution to bind two vectors together into a third. DVRS instead uses pointwise product: for two vectors ~v and ~ w, the pointwise product is ~v .∗ ~ w = 〈v1w1, ..., vdwd〉. We performed limited tests showing no advantage to circular convolution, detailed in Ustun et al. (2014). A similar formula to ours is mention in Mnih & Kavukcuoglu (2013).",
      "startOffset" : 90,
      "endOffset" : 702
    }, {
      "referenceID" : 8,
      "context" : "The information stored in these features is more similar to the skip-gram architecture in Mikolov et al. (2013), rather than the n-gram architecture of BEAGLE. DVRS uses the binding operation only to make pairwise bindings of words and word locations, whereas BEAGLE applies it successively to bind short strings of words together. Additionally, the choice of binding operation in DVRS is different. BEAGLE used circular convolution to bind two vectors together into a third. DVRS instead uses pointwise product: for two vectors ~v and ~ w, the pointwise product is ~v .∗ ~ w = 〈v1w1, ..., vdwd〉. We performed limited tests showing no advantage to circular convolution, detailed in Ustun et al. (2014). A similar formula to ours is mention in Mnih & Kavukcuoglu (2013). The co-occurrence features consist of the environment vectors of all other words in the current paragraph.",
      "startOffset" : 90,
      "endOffset" : 769
    }, {
      "referenceID" : 3,
      "context" : "This has been known for some time, but was studied in detail recently in Cai et al. (2013). Due to this, it is possible to sum random vectors ~s = ∑",
      "startOffset" : 73,
      "endOffset" : 91
    }, {
      "referenceID" : 3,
      "context" : "Cai et al. (2013) examine the properties of a set of n random vectors of dimensionality d.",
      "startOffset" : 0,
      "endOffset" : 18
    } ],
    "year" : 2014,
    "abstractText" : "We present a distributed vector representation based on a simplification of the BEAGLE system, designed in the context of the Sigma cognitive architecture. Our method does not require gradient-based training of neural networks, matrix decompositions as with LSA, or convolutions as with BEAGLE. All that is involved is a sum of random vectors and their pointwise products. Despite the simplicity of this technique, it gives state-of-the-art results on analogy problems, in most cases better than Word2Vec. To explain this success, we interpret it as a dimension reduction via random projection.",
    "creator" : "LaTeX with hyperref package"
  }
}