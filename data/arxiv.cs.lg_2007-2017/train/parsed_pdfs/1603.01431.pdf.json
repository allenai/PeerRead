{
  "name" : "1603.01431.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Normalization Propagation: A Parametric Technique for Removing Internal Covariate Shift in Deep Networks ",
    "authors" : [ "Devansh Arpit", "Yingbo Zhou", "Bhargava U. Kota", "Venu Govindaraju {DEVANSHA" ],
    "emails" : [ "GOVIND}@DEVANSHA.EDU" ],
    "sections" : [ {
      "heading" : "1. Introduction and Motivation",
      "text" : "Ioffe & Szegedy (2015) identified an important problem involved in training deep networks, viz., Internal Covariate Shift. It refers to the problem of shifting distribution of the input of every hidden layer in a deep neural network. This idea is borrowed from the concept of covariate shift\n(Shimodaira, 2000), where this problem is faced by a single input-output learning system. Consider the last layer of a deep network being used for classification; this layer essentially tries to learn P (Y |X), where Y is the class label random variable (r.v.) and X is the layer input r.v. However, learning a fixed P (Y |X) becomes a problem if P (X) changes continuously. As a result, this slows down training convergence.\nBatch Normalization (BN) addresses this problem by normalizing the distribution of every hidden layers’ input. In order to do so, it approximates the pre-activation mean and standard deviation (std) using mini-batch statistics at each iteration of training and uses these estimates to normalize the input to the next layer. While this approach leads to a significant performance jump by addressing internal covariate shift, it’s estimates of mean and std of hidden layer input rely on mini-batch statistics which makes these estimates an approximation (specially during initial training iterations). This is because the mini-batch statistics of input to hidden layers depends on the output from previous layers, which in turn depend on the previous layer parameters that keep shifting during training. Validation and testing get affected for the same reason since they use a running average estimate of these statistics. Finally, due to involvement of batch statistics, BN is inapplicable with batch-size 1.\nIn this paper, we propose a parametric normalization technique for addressing internal covariate shift that does not depend on batch statistics for normalizing the input to hidden layers and is less severely affected by the problem of shifting parameters during training. In fact, we show that it is unnecessary to explicitly calculate mean and standarddeviation from mini-batches for normalizing the input to hidden layers. Instead, a data independent estimate of these normalization components are available in closed form for every hidden layer, assuming the pre-activation values follow Gaussian distribution and that the weight matrix of hidden layers are roughly incoherent. We show how to forward propagate the normalization property (of the data distribution) to all hidden layers by exploiting the knowledge of\nar X\niv :1\n60 3.\n01 43\n1v 1\n[ st\nat .M\nL ]\n4 M\nar 2\n01 6\nthe distribution of the pre-activation values (Gaussian) and some algebraic manipulations. Hence we call our approach Normalization Propagation."
    }, {
      "heading" : "2. Background",
      "text" : "It has long been known in Deep Learning community that input whitening and decorrelation helps in speeding up the training process (LeCun et al., 2012). In fact, it is explicitly mentioned in (LeCun et al., 2012) that this whitening should be performed before every layer so that the input to the next layer has zero mean. From the perspective of Internal Covariate Shift, what is required for the network to learn a fixed hypothesis P (Y |X) at any given layer at every point in time during training, is for the distribution P (X) of the input to that layer to be fixed. While whitening could be used for achieving this task at every layer, it would be a very expensive choice (cubic order of input size) since whitening dictates computing the Singular Value Decomposition (SVD) of the input data matrix. However, Desjardins et al. (2015) suggest to overcome this problem by approximating this SVD by: a) using training mini-batch data to compute this SVD; b) computing it every few number of iteration and relying on the assumption that this SVD approximately holds for the iterations in between. In addition, each hidden layer’s input is then whitened by re-parametrizing a subset of network parameters that are involved in gradient descent. As mentioned in Ioffe & Szegedy (2015), this re-parametrizing may lead to effectively cancelling/attenuating the effect of the gradient update step since these two operations are done independently.\nBatch Normalization (Ioffe & Szegedy, 2015) addresses both the above problems. First, they propose a strategy for normalizing the data at hidden layers such that the gradient update step accounts for this normalization. Secondly, this normalization is performed for units of each hidden layer independently (thus avoiding whitening) using mini-batch statistics. Specifically, this is achieved by normalizing the pre-activation u = WTx of all hidden layers as,\nûi = ui − EB[ui]√\nvarB(ui) (1)\nwhere ui denotes the ith element of u and the expectation/variance is calculated over the training mini-batch B. Notice since W is a part of this normalization, it becomes a part of the gradient descent step as well. However, a problem common to both the above approaches is that of shifting network parameters and mini-batch statistics upon which their approximation of input normalization for hidden layers depends.\n3. Normalization Propagation (NormProp) Derivation\nWe will now describe the idea behind NormProp. At a glance the problem at hand seems cyclic because estimating the mean and standard deviation of the input distribution to any hidden layer requires the input distribution of it’s previous layer (and hence it’s parameters) to be fixed to the optimal value before hand. However, as we will now show, we can side-step this naive approach and get an approximation of the true unbiased estimate using the knowledge that the pre-activation to every hidden layer follows a Gaussian distribution and some algebraic manipulation over the properties of the weight matrix. For the derivation below, we will focus on networks with ReLU activation, and later discuss how to extend our algorithm to other activation functions."
    }, {
      "heading" : "3.1. Data Normalization",
      "text" : "Real world data generally follows Gaussian like distribution. Therefore, consider a data distribution X in Rn such that all the samples are normalized, i.e.,\nEx∈X [x] = 0 Ex∈X [x2j ] = 1 ∀ j ∈ {1, . . . , n}\n(2)\nThen our goal is to find a way to propagate this normalization to all the hidden layers without the need of explicit data dependent normalization. Depending on whether this input is passed through a convolutional layer or a fully connected layer, a part of the input or the entire input gets multiplied to a weight matrix. Irrespective of the case, lets use x to denote this input for ease of notation; which can thus be the entire data vector or a subset of it’s element depending on the case. The pre-activation is given by u , Wx, where W ∈ Rm×n and m is the number of filters (we will ignore bias for now). As also claimed by Ioffe & Szegedy (2015); Hyvärinen & Oja (2000), we assume the pre-activation (u) has a Gaussian form. So if we ensure that the pre-activation of the first hidden layer is a normalized Gaussian, then the hidden layer’s output (ReLU(u)) will be a Rectified Gaussian distribution. As mentioned in section 2, we can choose to directly normalize the post-activation output ReLU(u). However, as we will now show, it is easier to find closed form estimates for normalizing u instead."
    }, {
      "heading" : "3.2. Mean and Standard-deviation Normalization for First Hidden Layer",
      "text" : "Clearly, since the input data x is mean subtracted, the preactivation to the first hidden layer u also has zero mean from linearity,i.e., Ex∈X [u] = 0. Now we want to ensure the variance of u is 1; this is the tricky part. Let the covariance matrix of u be denoted by Σ. Then the following\nproposition bounds how far Σ is from a canonical distribution.\nProposition 1. (Canonical Error Bound) Let u = Wx where x ∈ Rn and W ∈ Rm×n such that Ex[x] = 0 and Ex[xxT ] = σ2I (I is the identity matrix) . Then the covariance matrix of u is approximately canonical satisfying,\nmin α ‖Σ− diag (α)‖F ≤\nσ2 ( m∑ i=1 ( ‖Wi‖22 ( 1− ‖Wi‖22 ))\n+ m∑ i,j=1;i 6=j m(m− 1)µ2‖Wi‖22‖Wj‖22\n1/2 (3)\nwhere Σ = Eu[(u−Eu[u])(u−Eu[u])T ] is the covariance matrix of u, µ is the coherence of the rows of W, α ∈ Rm is the closest approximation of the covariance matrix to a canonical ellipsoid and diag(.) diagonalizes a vector to a diagonal matrix. The corresponding optimal α∗i = σ2‖W‖22 ∀i ∈ {1, . . . ,m}.\nThe above proposition tells us two things. First, the covariance matrix of the pre-activation Σ is approximately canonical (diagonal covariance matrix) with a bounded error and that this error can be controlled by certain properties of the weight matrix W. Second, if we want to normalize each element of the vector u to have unit standarddeviation, then our best bet is to divide each ui by the corresponding weight length ‖Wi‖2. This is because the closest estimate of a diagonal variance for Σ is α∗i = ‖Wi‖22 (σ = 1 in our case).\nNotice the error bound has two distinct terms, while the second term depends on both length and coherence between weight vectors, the first term depends only on the weight lengths. For any dictionary (weight matrix W), while the second term in the above error bound only takes non-negative values, the first term can take both negative as well as positive values depending on the weight length. However, due to complex interaction between these two terms during training, it is hard to choose a fixed value that optimally minimizes the error bound. In our approach, we need to normalize each element of the vector u to have unit standard-deviation which is achieved by dividing each ui by ‖Wi‖2. Notice this automatically makes each hidden weight vector to effectively have unit `2 length. As a result, the first term in the above error bound automatically becomes zero and the error bound only depends on the coherence of W. On the other hand, it is generally observed that useful filters that constitute a good representation of real world data are roughly incoherent (Wright et al., 2010; Makhzani & Frey, 2013); thus ensuring the second term is also small thereby minimizing the overall error bound.\nAt this point, we have normalized the pre-activation u to have zero mean and unit variance (divide each preactivation element by corresponding ‖Wi‖2). As a result, the output of the first hidden layer (ReLU(u)) is Rectified Gaussian distribution. Notice that the above bound ensures the dimensions of u and hence (ReLU(u)) are roughly uncorrelated. Thus, if we subtract the distribution mean from ReLU(u) and divide by it’s standard deviation, we will have reduced the dynamics of the second layer to be identical to that of the first layer. The mean and standard deviation of the aforementioned Rectified Gaussian is, Remark 1. (Post-ReLU distribution) Let X ∼ N (0, 1) and Y = max(0, X). Then E[Y ] = 1√\n2π and var(Y ) =\n1 2 ( 1− 1π ) Hence in order to normalize the post-activation ReLU(u) to have zero mean and unit standard, the above calculated values can be used. Finally, in the case of Pooling (in ConvNets), we essentially take a block of post-activated units and take average or maximum of these values. If we consider each such unit to be independent then the distribution after pooling, will have a different mean and standard deviation. However, in reality, each of these units are highly correlated since they involve computation over either overlapping or spatially close patches. Therefore, we found that the distribution statistics do not get affected significantly and hence we do not recompute mean and standard deviation post-pooling."
    }, {
      "heading" : "3.3. Propagation to Higher Layers",
      "text" : "With the above two operations, the dynamics of the second hidden layer become identical to that of the first hidden layer. By induction, repeating these two operations for every layer, viz.–1) divide every hidden layer’s pre-ReLUactivation by it’s corresponding ‖Wi‖2, where W is the corresponding layer’s weight matrix, 2) subtract and divide√\n1/2π and √\n1 2 ( 1− 1π ) (respectively) from every hidden\nlayer’s post-ReLU-activation– we ensure that the input to every layer is a canonical distribution. While training, all these normalization operations are back-propagated."
    }, {
      "heading" : "3.4. Effect of NormProp on Jacobian of Hidden Layers",
      "text" : "It has been discussed in Saxe et al. (2013); Ioffe & Szegedy (2015) that Jacobian of hidden layers with singular values close to one improves training speed in deep networks. Although the authors of BN suggest that their approach intuitively achieves this condition, we will now show more rigorously that NormProp (approximately) indeed achieves this condition.\nLet l ∈ Rm be a vector such that the ith element of l is given by li = 1/‖Wi‖2. The output of a hidden unit using NormProp is given by o = 1c1 ReLU((Wx) l) − c2/c1\nwhere W ∈ Rm×n, x ∈ Rn, c1 = √ 1 2 ( 1− 1π ) and c2 = 1√ 2π\n. Let W̃ be such that each row of W̃ equals Wi/‖Wi‖2. Thus the output can be rewritten as o = 1 c1\nReLU(W̃x) − c2/c1. Let J denote the Jacobian of this output with respect to the previous layer input x. Then the ith row of J is given by\nJi , 1\nc1 ∂ReLU(W̃ix) ∂W̃ix ∂W̃ix ∂x\n= 1\nc1 ∂ReLU(W̃ix) ∂W̃ix W̃i\n(4)\nwhere W̃i denotes the ith row of W̃. Let 1x ∈ Rn be an indicator vector such that the ith element of 1x is given by\n1xi , ∂ReLU(W̃ix)\n∂W̃ix = 1(W̃ix > 0) (5)\nwhere 1(.) is the indicator operator. Let M1x ∈ Rm×n be a matrix such that every column of M1x is occupied by the vector 1x. Then the entire Jacobian matrix can be written as J = 1c1 (M1x W̃). In order to analyze the singular values of J, we want to calculate JJT . From proposition 1, the covariance matrix Σ of the pre-activation W̃x is given by Σ = σW̃W̃T , where σ = 1. Since the length of each W̃i is 1, Σ, and (therefore) W̃W̃T is approximately an identity matrix if the rows of W̃ are incoherent. Thus,\nJJT = 1\nc21 (M1x W̃)(W̃T MT1x)\n≈ 1 c21 diag(1x 1x) = 1 c21 diag(1x)\n(6)\nFinally taking an expectation of JJT over the distribution of x which is Normal, we get,\nEx[JJT ] ≈ 1\nc21 Ex[diag(1x)]\n= 1\nc21\n∫ diag(1x)p(x)d(x)\n(7)\nwhere p(.) denotes the density of x– Normal distribution. From the definition of 1x, it is straight forward to see that the result of the integral is a matrix with it’s diagonal equal to a vector of 0.5, hence, Ex[JJT ] ≈ 0.5c21 I ≈ 1.47I, where I is the identity matrix. Thus the singular values of the Jacobian are √ (1.47) = 1.21 which, being close to 1, approximately achieves dynamical isometry (Saxe et al., 2013) and should thus prevent the problem of exploding or diminishing gradients while training deep networks suggesting faster convergence. In the next section, we will use this value during the practical implementation of NormProp for improving the Jacobian to be approximately 1."
    }, {
      "heading" : "4. NormProp: Implementation Details",
      "text" : "We have all the ingredients required to filter out the steps for Normalization Propagation for training any deep neural network with ReLU activation though out it’s hidden layers. Like BN, NormProp can be used alongside any optimization algorithm (eg. Stochastic Gradient Descent with/without momentum) for training deep networks."
    }, {
      "heading" : "4.1. Normalize Data",
      "text" : "Since the core idea of NormProp is to propagate the data normalization through hidden layers, we offer two alternative choices either one of which can be used for normalizing the input to a NormProp network. As we will describe, both options are justified in their respective scenario.\n1. Global Data Normalization: In cases when the entire dataset – approximately representing the true data distribution – is available at hand, we compute the global mean and standard deviation for each feature element. Then the first step for NormProp is to subtract element-wise mean calculated over the entire dataset from each sample. Similarly divide each feature element by the element-wise standard-deviation. Ideally it is required by NormProp that all input dimensions be statistically uncorrelated, a property achieved by whitening for instance, but we suggest element-wise normalization as an approximation since it is computationally cheaper.Notice this precludes the dilemma of what range the input should be scaled to before passing through the network.\n2. Batch Data Normalization: In many real world scenario, streaming data is available and thus it is not possible to compute an unbiased estimate of global mean and standard deviation at any given point in time. In such cases, we propose to instead batch-normalize every mini-batch training data fed to the network. Again, we perform the normalization of each feature element independently for computational purposes. Notice this normalization is only performed at the data level, all hidden layers are still normalized by the NormProp strategy which is both more accurate and computationally cheaper compared to the Batch Normalization strategy (Ioffe & Szegedy, 2015) because NormProp is not affected by shifting model parameters as compared to BN. Moreover, Batch Data Normalization also serves as a regularization since each data sample gets a different representation each time depending on the minibatch it comes with. Thus by using the Batch Data Normalization strategy we actually benefit from the regularization aspect of BN but also overcome it’s drawbacks by computing the hidden layer mean and standard-deviation more accurately. Notice this strategy is most effective when the incoming data is well shuffled."
    }, {
      "heading" : "4.2. Initialize Network Parameters",
      "text" : "We use Normalized Initialization (Glorot & Bengio, 2010) for setting the initial values of all the weight matrices, both fully connected and convolutional."
    }, {
      "heading" : "4.3. Propagate Normalization",
      "text" : "Similar to BN, we also make use of gradient-basedlearnable scaling and bias parameters γ and β during implementation. We will now describe our normalization in detail for both fully connected and convolutional layers."
    }, {
      "heading" : "4.3.1. FULLY CONNECTED LAYERS",
      "text" : "Consider any fully connected layer characterized by a weight matrix W ∈ Rm×n, bias β ∈ Rm, scaling γ ∈ Rm, input x ∈ Rn and activation ReLU. Here m denotes the number of filters and n denotes the input dimension. Then without NormProp, the ith output unit oi of the hidden layer would traditionally be:\noi = ReLU(WTi x + βi) (8)\nNow in the case of NormProp, the output oi becomes,\noi = 1√\n1 2 ( 1− 1π\n) [ ReLU ( γi(W T i x)\n1.21‖Wi‖2 + βi\n) − √ 1\n2π ] (9)\nHere we divide the pre-activation by 1.21 in order to make the Jacobian close to one as suggested by our analysis in section 3.4. Thus we call this number the Jacobian factor. We found dividing the pre-activation by Jacobian factor helps significantly while training with larger learning rates without diverging."
    }, {
      "heading" : "4.3.2. CONVOLUTIONAL LAYERS",
      "text" : "Consider any convolutional layer characterized by a filter matrix W ∈ Rm×d×h×w, bias β ∈ Rm, scaling γ ∈ Rm, input x ∈ Rd×L×B and activation ReLU along with any arbitrary choice of stride-size. Here,m denotes the number of filters, d–depth, h–height, w– width for input/filters and L,B– height and width of image. Then without NormProp, the ith output feature map oi of the hidden layer using the ith filter Wi ∈ Rd×h×w would traditionally be:\noi = ReLU(Wi∗x + βi) (10)\nwhere ∗ denotes the convolution operation. Now in the case of NormProp, the output feature map oi becomes,\noi = 1√\n1 2 ( 1− 1π\n) [ ReLU ( γi(Wi∗x) 1.21‖Wi‖F + βi ) − √ 1 2π ]\n(11)\nwhere each element of γi is multiplied to all outputs from the same corresponding filter and similarly all the scalars as well as the bias vector are broadcasted to all the dimensions. Pooling is done after this normalization process the same way as done traditionally."
    }, {
      "heading" : "4.4. Training",
      "text" : "The network is trained using Back Propagation. While doing so, all the normalizations also get back-propagated at every layer.\nOptimization: We use Stochastic Gradient Descent with momentum (set to 0.9) for training. Data shuffling also leads to performance improvement (this however, is true in general while training deep networks).\nLearning Rate: We found learning speeds up by reducing the learning rate by half whenever the training error starts saturating. Also, we found larger initial learning rate for larger batch size improves performance.\nConstraints: After every training iteration, we scale each hidden weight-vector/filter-map to have unit `2 length, i.e., we use `2 constraint on all hidden weights, both convolutional and fully connected. This is done because the scale of weight vectors do not affect network representation, so constraining the weights reduces the parameter search space.\nRegularizations: We use weight decay along with the loss function; we found a small coefficient value of 0.0005 − 0.005 is necessary during training. We found Dropout does not help during training; we believe this might be because Dropout changes the distribution of output of the layer it is applied, which affects NormProp. We believe this is also the reason Dropout doesn’t improve performance for BN ."
    }, {
      "heading" : "4.5. Validation and Testing",
      "text" : "Validation and test procedures are identical in our case in contrast with BN because of parametric normalization.\nWhile validation/testing, each sample is first normalized using mean and standard deviation which are calculated depending on how the train data is normalized during training. In case we use Global Data Normalization during training, we simply use the same global estimate of mean and standard deviation to normalize each test/validation sample. On the other hand, if Batch Data Normalization is used during training, a running estimate of mean and standard deviation is maintained during training which is then used to normalize every test/validation sample. Finally, the input is forward propagated though the network with learned parameters using the same strategy described in section 4.3."
    }, {
      "heading" : "4.6. Extension to other Activation Functions",
      "text" : "Even though our paper shows how to overcome the problem of Internal Covariate Shift specifically for networks with ReLU activation throughout, we have in essence proposed a general framework for propagating normalization done at data level to all hidden layers. All that is needed for extending NormProp to other activation functions is to compute the distribution mean and standard deviation of output after the activation function of choice, similar to what is shown in remark 1. This activation can be both parameter based or fixed. For instance, a parameter based activation is Parametric ReLU (PReLU, He et al. (2015)) (with parameter a) given by,\nPReLUa(x) = { x ifx > 0 ax ifx ≤ 0 (12)\nThen the post PReLU distribution statistics is given by, Remark 2. LetX ∼ N (0, 1) and Y = PReLUa(X). Then E[Y ] = (1−a) 1√\n2π and var(Y ) = 12\n( (1 + a2)− (1−a) 2\nπ ) Notice the distribution mean and standard deviation depends on the parameter a and thus will be involved in the normalization process. In case of non-parameter based activations (eg. Tanh, Sigmoid), one can either choose to analytically compute the statistics (like we did for ReLU) or compute these values empirically by simulation since the input distribution to the activation is a fixed Normal distribution. Thus NormProp is a general framework which can be extended to any activation function of choice."
    }, {
      "heading" : "5. Empirical Results and Observations",
      "text" : "We want to verify the following: a) performance comparison of NormProp when using Global Data Normalization vs. Batch Data Normalization; b) NormProp alleviates the problem of Internal Covariate Shift more accurately compared to BN; c) thus, convergence stability of NormProp is better than BN; d) effect of batch-size on the behaviour of NormProp, specially batch-size 1 (BN not applicable). Finally we report classification result on various datasets using NormProp and BN.\nDatasets: We use the following datasets, 1) CIFAR-10 (Krizhevsky, 2009)– It consists of 60, 000 32 × 32 real world color images in 10 classes split into 50, 000 train and 10, 000 test images. We use 5000 images from train set for validation and remaining for training. 2) CIFAR-100– It has the same number of train and test samples as CIFAR-10 but it has 100 classes. For training, we use hyperparameters same as those for CIFAR-10. 3) SVHN (Netzer et al., 2011)– It consists of 32× 32 color images of house numbers collected by Google Street View. It has 73, 257 train images, 26, 032 test images and an additional 5, 31, 131 train images. Similar to the protocol in (Goodfellow et al., 2013), we select 400 samples per class from the train set and 200 samples per class from the extra set as validation and use the remaining images of the train and extra sets for training.\nExperimental Protocols (For experiments in sections 5.1 through 5.4): We use CIFAR-10 with the following Network in Network (Lin et al., 2014) architecture1 C(192, 5, 1, 2) − C(160, 1, 1, 0) − P (3, 2, 1,max) − C(96, 1, 1, 0) − C(192, 5, 1, 2) − C(192, 1, 1, 0) − P (3, 2, 1, avg) − C(192, 1, 1, 0) − C(192, 5, 1, 0) − C(192, 1, 1, 2) − C(10, 1, 1, 0) − P (8, 8, 0, avg). For any specified initial learning rate, we reduce it by half every 10 epochs. We use Stochastic Gradient Descent with momentum 0.9. We use test set during validation for convergence analysis.\n1We use the following shorthand for a) conv layer: C(number of filters, filter size, stride size, padding); b) pooling: P(kernel size, stride, padding, pool mode)"
    }, {
      "heading" : "5.1. Global vs. Batch Data Normalization",
      "text" : "Since we offer two alternate ways to normalize data (section 4.1) fed to a NormProp network, we evaluate both strategies with different batch sizes to see the difference in performance. We use batch sizes2 50 and 100 using initial learning rates 0.05 and 0.08 respectively. The results are shown in figure 1. The performance 3 using both strategies is very similar for both batch sizes, converging in only 30 epochs. This shows the robustness and applicability of NormProp in both streaming data as well as block data scenario. However, since Batch Data Normalization strategy is a more practical choice, we stick to this strategy for the rest of the experiments when using NormProp.\n5.2. NormProp vs. BN– Internal Covariate Shift\nThe fundamental goal of our paper (as well as that of Batch Normalization Ioffe & Szegedy, 2015) is to alleviate the problem of Internal Covariate Shift. This implies preventing the distribution of hidden layer inputs from shifting while the network is being trained. In deep networks, the features generated by higher layers are completely dependent on the lower features since all the information in data is propagated from lower to higher layers. Thus the problem of Internal Covariate Shift in lower hidden layers is expected to affect the overall performance more severely as compared to the same problem in higher layers.\nIn order to study the effect of normalization by NormProp and BN on hidden layers, we train two separate networks using each strategy and an additional network without any normalization as our baseline. After every training epoch, we record the mean of the input distribution (over the validation set) to a single randomly chosen (but fixed) unit in each hidden layer. We use batch size 50 and an initial learning rate of 0.05 for NormProp and BN, and 0.0001\n2Notice this batch size has nothing to do with the data normalization strategies in discussion. Different batch sizes are used only for adding more variation in experiments.\n3Even though the numbers are very close, the best accuracy of 90.35% is achieved by Batch Data Normalization using batch size 50.\nfor training the network without any normalization (larger learning rates cause divergence). For the 9 layer convolutional networks we train, the input mean to the last 8 layers against training epoch are shown in figure 2. There are three important observations in these figures: a) NormProp achieves significantly more stable input distribution for lower hidden layers compared to BN, thus facilitating the learning of good lower level representation; b) the input distribution for all hidden layers converge after 32 epochs for NormProp. On the other hand, the input distribution to the second layer for BN remains un-converged even after 50 epochs; c) on an average, input distribution to all layers converge closer to zero for NormProp (avg. 0.19) as compared to BN (avg. 0.33). Finally the performance of the network trained without any normalization is incomparable to the normalized ones due to large variations in the hidden layer input distribution (especially the lower layers). This experiment also serves to show the Canonical Error Bound (proposition 1) is small since the input statistics to hidden layers are roughly preserved."
    }, {
      "heading" : "5.3. Convergence Stability of NormProp vs. BN",
      "text" : "As a result of alleviating Internal Covariate Shift more accurately as compared to BN, NormProp is expected to achieve a more stable convergence. We confirm this intuition by recording the validation accuracy while the network is being trained. We use batch size 50 and initial learning rates 0.05. The plot is shown in figure 34. Clearly NormProp achieves a more stable convergence in general, but specially during initial training. This is because of more stable input distribution (specially to lower hidden layers) achieved by NormProp as well as a more accurate estimate of layer input distribution statistics resulting from it during initial training."
    }, {
      "heading" : "5.4. Effect of Batch-size on NormProp",
      "text" : "We want to see the effect of batch-size used during training with NormProp. Since it is also possible to train with batch size 1 (using Global Data Normalization at data layer), we\n4We observed identical trends on SVHN and CIFAR-100.\ncompare the validation performance of NormProp during training for various batch sizes including 1. The plots are shown in figure 4. The performance of NormProp is largely unaffected by batch size although lower batch sizes seem to yield better performance."
    }, {
      "heading" : "5.5. Results on various Datasets",
      "text" : "We evaluate NormProp and BN on CIFAR-10, CIFAR-100 and SVHN datasets, but also report existing state-of-the-art (SOTA) results. For all the datasets and both methods, we use the same architecture as mentioned in the experimental protocol above except for CIFAR-100, the last convolutional layer is C(100, 1, 1, 0) instead of C(10, 1, 1, 0). For CIFAR datasets we use batch size 50 and an initial learning rate of 0.05 and reduce it by half after every 25 epochs and train for 200 epochs. Since SVHN is a much larger dataset, we only train for 25 epochs with batch size 100 and an initial learning rate of 0.08 and reduce it by half after every 5 epochs. We use Stochastic gradient descent with momentum (0.9). For CIFAR-10 and CIFAR-100, we train using both without data augmentation and with data augmentation (horizontal flipping only); and no data augmentation for SVHN. We did not pre-process any of the datasets. The results are shown in table 1. We find NormProp consistently achieves either better or competitive performance compared to BN, but also beats existing SOTA results."
    }, {
      "heading" : "6. Conclusion",
      "text" : "We have proposed a novel algorithm for addressing the problem of Internal Covariate Shift involved during training deep neural networks that overcomes several drawbacks of Batch Normalization (BN). Specifically, we propose a parametric approach (NormProp) that avoids estimating the mean and standard deviation of hidden layers’ input distribution using input data mini-batch statistics (that involve shifting network parameters), which make it inaccurate (specially during initial training period when parameters change drastically). Instead, NormProp relies on normalizing the statistics of the given dataset and conditioning the weight matrix which ensures normalization done for the dataset is propagated to all hidden layers. Thus NormProp does not need to compute and maintain a moving average estimate of batch statistics of hidden layer inputs for test phase. This also enables the use of batch size 1 for training, which overcomes another fundamental drawback of BN. Although we have shown how to apply NormProp in detail for networks with ReLU activation, we have discussed (section 4.6) how to extend it for other activations as well. We have empirically shown NormProp achieves more stable convergence and hidden layer input distribution during training, and better/competitive classification performance\ncompared with BN. In conclusion, our approach is applicable alongside any activation function and cost objectives for improving training convergence."
    }, {
      "heading" : "A. Proofs",
      "text" : "Proposition 1. Let u = Wx where x ∈ Rn and W ∈ Rm×n such that Ex[x] = 0 and Ex[xxT ] = σ2I (I is the identity matrix) . Then the covariance matrix of u is approximately canonical satisfying,\nmin α ‖Σ− diag (α)‖F ≤ σ2 √√√√ m∑ i=1 (‖Wi‖22 (1− ‖Wi‖22)) + m∑ i,j=1;i 6=j m(m− 1)µ2‖Wi‖22‖Wj‖22 (13)\nwhere Σ = Eu[(u− Eu[u])(u− Eu[u])T ] is the covariance matrix of u, µ is the coherence of the rows of W, α ∈ Rm is the closest approximation of the covariance matrix to a canonical ellipsoid and diag(.) diagonalizes a vector to a diagonal matrix. The corresponding optimal α∗i = σ 2‖W‖22 ∀i ∈ {1, . . . ,m}.\nProof. Notice that,\nEu[u] = WEx[x] = 0 (14)\nOn the other hand, the covariance of u is given by,\nΣ = Eu[(u− Eu[u])(u− Eu[u])T ] = Ex[(Wx−WEx[x])(Wx−WEx[x])T ] = Ex[W(x− Ex[x])(x− Ex[x])TWT ] = WEx[(x− Ex[x])(x− Ex[x])T ]WT\n(15)\nSince x has spherical covariance, the off-diagonal elements of Ex[(x − Ex[x])(x − Ex[x])T ] are zero and the diagonal elements are the variance of any individual unit, since all units are identical. Thus,\nEu[(u− Eu[u])(u− Eu[u])T ] = σ2WWT (16)\nThus,\n‖Σ− diag (α)‖2F = tr ( (σ2WWT − diag (α))(σ2WWT − diag (α))T ) = tr ( σ4WWTWWT + diag (α2)− 2σ2 diag (α)WWT\n) = σ4‖WWT ‖2F +\nm∑ i=1 ( α2i − 2σ2αi‖Wi‖22 ) ≤ σ4\nm∑ i=1 ( ‖Wi‖22 ) + m∑ i,j=1;i 6=j m(m− 1)µ2‖Wi‖22‖Wj‖22 + m∑ i=1 ( α2i − 2σ2αi‖Wi‖22\n) (17)\nα2 in the above equation denotes element-wise square of elements of α. Finally minimizing w.r.t αi ∀i ∈ {1, . . . ,m}, leads to α∗i = σ 2‖W‖22. Substituting this into equation 17, we get,\n‖Σ− diag (α)‖2F ≤ σ4  m∑ i=1 ( ‖Wi‖22 ( 1− ‖Wi‖22 )) + m∑ i,j=1;i 6=j m(m− 1)µ2‖Wi‖22‖Wj‖22  (18)\nRemark 1. Let X ∼ N (0, 1) and Y = max(0, X). Then E[Y ] = 1√ 2π and var(Y ) = 12 ( 1− 1π ) Proof. For the definition of X and Y , we have,\nE[Y ] = 1\n2 .0 +\n1 2 E[Z] = 1 2 E[Z] (19)\nwhere Z is sampled from a Half-Normal distribution such that Z = |X|; thus E[Z] = √\n2 π leading to the claimed result.\nIn order to compute variance, notice that E[Y 2] = 0.5E[Z2]. Then,\nvar(Y ) = E[Y 2]− E[Y ]2 = 0.5E[Z2]− 1 4 E[Z]2 = 0.5(var(Z) + E[Z]2)− 1 4 E[Z]2 (20)\nSubstituting var(Z) = 1− 2π yields the claimed result.\nRemark 2. Let X ∼ N (0, 1) and Y = PReLUa(X). Then E[Y ] = (1− a) 1√2π and var(Y ) = 1 2\n( (1 + a2)− (1−a) 2\nπ ) Proof. For the definition of X and Y , half the mass of Y is concentrated on R+ with Half-Normal distribution, while the other half of the mass is concentrated on R−sign(a)with Half-Normal distribution scaled with |a|. Thus,\nE[Y ] = −a 2 E[Z] + 1 2 E[Z] = (1− a)1 2 E[Z] (21)\nwhere Z is sampled from a Half-Normal distribution such that Z = |X|; thus E[Z] = √\n2 π leading to the claimed result.\nSimilarly in order to compute variance, notice that E[Y 2] = 0.5E[(aZ)2] + 0.5E[Z2] = 0.5E[Z2](1 + a2). Then,\nvar(Y ) = E[Y 2]− E[Y ]2 = 0.5E[Z2](1 + a2)− (1− a)2 1 4 E[Z]2\n= 0.5(1 + a2)(var(Z) + E[Z]2)− (1− a)2 1 4 E[Z]2\n(22)\nSubstituting var(Z) = 1− 2π yields the claimed result."
    } ],
    "references" : [ {
      "title" : "Learning activation functions to improve deep neural networks",
      "author" : [ "Agostinelli", "Forest", "Hoffman", "Matthew", "Sadowski", "Peter", "Baldi", "Pierre" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Agostinelli et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Agostinelli et al\\.",
      "year" : 2015
    }, {
      "title" : "Natural neural networks",
      "author" : [ "Desjardins", "Guillaume", "Simonyan", "Karen", "Pascanu", "Razvan" ],
      "venue" : "In Advances in Neural Information Processing Systems, pp. 2062–2070,",
      "citeRegEx" : "Desjardins et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Desjardins et al\\.",
      "year" : 2015
    }, {
      "title" : "Understanding the difficulty of training deep feedforward neural networks",
      "author" : [ "Glorot", "Xavier", "Bengio", "Yoshua" ],
      "venue" : "In International conference on artificial intelligence and statistics,",
      "citeRegEx" : "Glorot et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Glorot et al\\.",
      "year" : 2010
    }, {
      "title" : "Reducing the dimensionality of data with neural networks",
      "author" : [ "Hinton", "Geoffrey E", "Salakhutdinov", "Ruslan R" ],
      "venue" : null,
      "citeRegEx" : "Hinton et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2006
    }, {
      "title" : "Independent component analysis: algorithms and applications",
      "author" : [ "Hyvärinen", "Aapo", "Oja", "Erkki" ],
      "venue" : "Neural networks,",
      "citeRegEx" : "Hyvärinen et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Hyvärinen et al\\.",
      "year" : 2000
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "Ioffe", "Sergey", "Szegedy", "Christian" ],
      "venue" : "ICML, volume 37 of JMLR Proceedings,",
      "citeRegEx" : "Ioffe et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ioffe et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning Multiple Layers of Features from Tiny Images",
      "author" : [ "Krizhevsky", "Alex" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Krizhevsky and Alex.,? \\Q2009\\E",
      "shortCiteRegEx" : "Krizhevsky and Alex.",
      "year" : 2009
    }, {
      "title" : "Efficient backprop",
      "author" : [ "LeCun", "Yann A", "Bottou", "Léon", "Orr", "Genevieve B", "Müller", "Klaus-Robert" ],
      "venue" : "In Neural networks: Tricks of the trade,",
      "citeRegEx" : "LeCun et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 2012
    }, {
      "title" : "Reading digits in natural images with unsupervised feature learning",
      "author" : [ "Netzer", "Yuval", "Wang", "Tao", "Coates", "Adam", "Bissacco", "Alessandro", "Wu", "Bo", "Ng", "Andrew Y" ],
      "venue" : "In NIPS workshop on deep learning and unsupervised feature learning,",
      "citeRegEx" : "Netzer et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Netzer et al\\.",
      "year" : 2011
    }, {
      "title" : "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "author" : [ "Saxe", "Andrew M", "McClelland", "James L", "Ganguli", "Surya" ],
      "venue" : "arXiv preprint arXiv:1312.6120,",
      "citeRegEx" : "Saxe et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Saxe et al\\.",
      "year" : 2013
    }, {
      "title" : "Improving predictive inference under covariate shift by weighting the log-likelihood function",
      "author" : [ "Shimodaira", "Hidetoshi" ],
      "venue" : "Journal of statistical planning and inference,",
      "citeRegEx" : "Shimodaira and Hidetoshi.,? \\Q2000\\E",
      "shortCiteRegEx" : "Shimodaira and Hidetoshi.",
      "year" : 2000
    }, {
      "title" : "Dropout: A simple way to prevent neural networks from overfitting",
      "author" : [ "Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Srivastava et al\\.,? \\Q1929\\E",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 1929
    }, {
      "title" : "Sparse representation for computer vision and pattern recognition",
      "author" : [ "Wright", "John", "Ma", "Yi", "Mairal", "Julien", "Sapiro", "Guillermo", "Huang", "Thomas S", "Yan", "Shuicheng" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "Wright et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Wright et al\\.",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "It has long been known in Deep Learning community that input whitening and decorrelation helps in speeding up the training process (LeCun et al., 2012).",
      "startOffset" : 131,
      "endOffset" : 151
    }, {
      "referenceID" : 7,
      "context" : "In fact, it is explicitly mentioned in (LeCun et al., 2012) that this whitening should be performed before every layer so that the input to the next layer has zero mean.",
      "startOffset" : 39,
      "endOffset" : 59
    }, {
      "referenceID" : 1,
      "context" : "However, Desjardins et al. (2015) suggest to overcome this problem by approximating this SVD by: a) using training mini-batch data to compute this SVD; b) computing it every few number of iteration and relying on the assumption that this SVD approximately holds for the iterations in between.",
      "startOffset" : 9,
      "endOffset" : 34
    }, {
      "referenceID" : 1,
      "context" : "However, Desjardins et al. (2015) suggest to overcome this problem by approximating this SVD by: a) using training mini-batch data to compute this SVD; b) computing it every few number of iteration and relying on the assumption that this SVD approximately holds for the iterations in between. In addition, each hidden layer’s input is then whitened by re-parametrizing a subset of network parameters that are involved in gradient descent. As mentioned in Ioffe & Szegedy (2015), this re-parametrizing may lead to effectively cancelling/attenuating the effect of the gradient update step since these two operations are done independently.",
      "startOffset" : 9,
      "endOffset" : 478
    }, {
      "referenceID" : 12,
      "context" : "On the other hand, it is generally observed that useful filters that constitute a good representation of real world data are roughly incoherent (Wright et al., 2010; Makhzani & Frey, 2013); thus ensuring the second term is also small thereby minimizing the overall error bound.",
      "startOffset" : 144,
      "endOffset" : 188
    }, {
      "referenceID" : 9,
      "context" : "It has been discussed in Saxe et al. (2013); Ioffe & Szegedy (2015) that Jacobian of hidden layers with singular values close to one improves training speed in deep networks.",
      "startOffset" : 25,
      "endOffset" : 44
    }, {
      "referenceID" : 9,
      "context" : "It has been discussed in Saxe et al. (2013); Ioffe & Szegedy (2015) that Jacobian of hidden layers with singular values close to one improves training speed in deep networks.",
      "startOffset" : 25,
      "endOffset" : 68
    }, {
      "referenceID" : 9,
      "context" : "21 which, being close to 1, approximately achieves dynamical isometry (Saxe et al., 2013) and should thus prevent the problem of exploding or diminishing gradients while training deep networks suggesting faster convergence.",
      "startOffset" : 70,
      "endOffset" : 89
    }, {
      "referenceID" : 8,
      "context" : "3) SVHN (Netzer et al., 2011)– It consists of 32× 32 color images of house numbers collected by Google Street View.",
      "startOffset" : 8,
      "endOffset" : 29
    }, {
      "referenceID" : 0,
      "context" : "41 NIN + ALP units (Agostinelli et al., 2015) 9.",
      "startOffset" : 19,
      "endOffset" : 45
    }, {
      "referenceID" : 0,
      "context" : "25 NIN + ALP units (Agostinelli et al., 2015) 7.",
      "startOffset" : 19,
      "endOffset" : 45
    }, {
      "referenceID" : 0,
      "context" : "32 NIN + ALP units (Agostinelli et al., 2015) 34.",
      "startOffset" : 19,
      "endOffset" : 45
    }, {
      "referenceID" : 0,
      "context" : "26 NIN + ALP units (Agostinelli et al., 2015) 30.",
      "startOffset" : 19,
      "endOffset" : 45
    }, {
      "referenceID" : 0,
      "context" : "25 NIN + ALP units (Agostinelli et al., 2015) NIN (Lin et al.",
      "startOffset" : 19,
      "endOffset" : 45
    } ],
    "year" : 2017,
    "abstractText" : "While the authors of Batch Normalization (BN) identify and address an important problem involved in training deep networks– Internal Covariate Shift– the current solution has multiple drawbacks. For instance, BN depends on batch statistics for layerwise input normalization during training which makes the estimates of mean and standard deviation of input (distribution) to hidden layers inaccurate due to shifting parameter values (specially during initial training epochs). Another fundamental problem with BN is that it cannot be used with batch-size 1 during training. We address these (and other) drawbacks of BN by proposing a non-adaptive normalization technique for removing covariate shift, that we call Normalization Propagation. Our approach does not depend on batch statistics, but rather uses a data-independent parametric estimate of mean and standard-deviation in every layer We exploit the observation that the pre-activation before Rectified Linear Units follow a Gaussian distribution in deep networks, and that once the first and second order statistics of any given dataset are normalized, we can forward propagate this normalization without the need for recalculating the approximate statistics (using data) for any of the hidden layers.",
    "creator" : "LaTeX with hyperref package"
  }
}