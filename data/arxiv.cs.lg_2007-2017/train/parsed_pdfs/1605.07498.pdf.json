{
  "name" : "1605.07498.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Leveraging over priors for boosting control of prosthetic hands",
    "authors" : [ "Giovanni B. Bachelet", "Relatrice Esterna", "Barbara Caputo" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Università degli studi di Roma “La Sapienza”\nFacoltà di Scienze Matematiche Fisiche e Naturali\nCorso di Laurea in Fisica\nDissertazione di Laurea Magistrale\nLeveraging over priors for boosting control of prosthetic hands\nCandidata Relatore Interno\nValentina Gregori Prof. Giovanni B. Bachelet\nRelatrice Esterna\nProf.ssa Barbara Caputo\nMatricola 1387986\nAnno Accademico 2015/2016\nar X\niv :1\n60 5.\n07 49\n8v 1\n[ cs\n.L G\n] 2\n4 M\nay 2\n01 6\nA Simona che, nonostante la lontananza,\nè sempre vicina.\nA mamma e papà, senza i quali tutto questo\nnon sarebbe iniziato.\nContents\nAbstract 5"
    }, {
      "heading" : "1 Introduction 7",
      "text" : "1.1 Contributions of this thesis . . . . . . . . . . . . . . . . . . . . 9 1.2 Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10"
    }, {
      "heading" : "2 A few landmarks 11",
      "text" : "2.1 Prosthesis and control . . . . . . . . . . . . . . . . . . . . . . 11 2.2 Path in learning problems . . . . . . . . . . . . . . . . . . . . 13 2.3 Experimental framework . . . . . . . . . . . . . . . . . . . . . 14 2.4 Previous works . . . . . . . . . . . . . . . . . . . . . . . . . . 15"
    }, {
      "heading" : "3 Data description and representation 19",
      "text" : "3.1 Data description: the NinaPro database . . . . . . . . . . . . 20\n3.1.1 Acquisition setup . . . . . . . . . . . . . . . . . . . . . 20 3.1.2 Acquisition protocol . . . . . . . . . . . . . . . . . . . 22 3.1.3 Data collections . . . . . . . . . . . . . . . . . . . . . . 24\n3.2 Data representation: features extraction . . . . . . . . . . . . 26 3.2.1 Preprocessing . . . . . . . . . . . . . . . . . . . . . . . 26 3.2.2 Feature extraction . . . . . . . . . . . . . . . . . . . . 28"
    }, {
      "heading" : "4 From theory to learning algorithms 33",
      "text" : "4.1 Mathematical framework and background . . . . . . . . . . . 34 4.2 Supervised learning on a single domain . . . . . . . . . . . . . 35 4.3 Support Vector Machines . . . . . . . . . . . . . . . . . . . . . 35\n4.3.1 Linear SVMs . . . . . . . . . . . . . . . . . . . . . . . 36 4.3.2 Non-linear SVMs . . . . . . . . . . . . . . . . . . . . . 40\n4.4 Supervised learning on a different domain . . . . . . . . . . . 42 4.5 Domain adaptation algorithms . . . . . . . . . . . . . . . . . . 43\n4.5.1 Multi Adapt algorithm . . . . . . . . . . . . . . . . . . 46 4.5.2 Multi Kernel Adaptive Learning algorithm . . . . . . . 51\n3"
    }, {
      "heading" : "4 CONTENTS",
      "text" : "4.5.3 High Level-Learning2Learn algorithm . . . . . . . . . . 55"
    }, {
      "heading" : "5 Results 57",
      "text" : "5.1 Experimental protocol . . . . . . . . . . . . . . . . . . . . . . 57 5.2 Intact-Intact . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59 5.3 Amputees-Amputees . . . . . . . . . . . . . . . . . . . . . . . 65 5.4 Amputees-Intact . . . . . . . . . . . . . . . . . . . . . . . . . 70 5.5 Discussion and comparison . . . . . . . . . . . . . . . . . . . . 75\nConclusions 79"
    }, {
      "heading" : "A Histogram analysis 83",
      "text" : "A.1 Histogram: first experiment . . . . . . . . . . . . . . . . . . . 83 A.2 Histogram: second experiment . . . . . . . . . . . . . . . . . . 88 A.3 Histogram: third experiment . . . . . . . . . . . . . . . . . . . 92\nBibliography 97\nAbstract\nThe Electromyography (EMG) signal is the electrical activity produced by cells of skeletal muscles in order to provide a movement. The non-invasive prosthetic hand works with several electrodes, placed on the stump of an amputee, that record this signal. In order to favour the control of prosthesis, the EMG signal is analysed with algorithms based on machine learning theory to decide the movement that the subject is going to do. In order to obtain a significant control of the prosthesis and avoid mismatch between desired and performed movements, a long training period is needed when we use the traditional algorithm of machine learning (i.e. Support Vector Machines). An actual challenge in this field concerns the reduction of the time necessary for an amputee to learn how to use the prosthesis. Recently, several algorithms that exploit a form of prior knowledge have been proposed. In general, we refer to prior knowledge as a past experience available in the form of models. In our case an amputee, that attempts to perform some movements with the prosthesis, could use experience from different subjects that are already able to perform those movements. The aim of this work is to verify, with a computational investigation, if for an amputee this kind of previous experience is useful in order to reduce the training time and boost the prosthetic control. Furthermore, we want to understand if and how the final results change when the previous knowledge of intact or amputated subjects is used for a new amputee. Our experiments indicate that: (1) the use of experience, from other subjects already trained to perform a task, makes us able to reduce the training time of about an order of magnitude; (2) it seems that an amputee that tries to learn to use the prosthesis doesn’t reach different results when he/she exploits previous experience of amputees or intact.\n5\nChapter 1\nIntroduction\nThe problem of amputations is very actual. According to data from the National Center for Health Statistics, there are 50,000 new amputations every year in the USA. The statistics of COPC (Center for Orthotic & Prosthetic Care) show that the level of amputation concerns the upper limb in 86 % of cases and lower limb in 14 %. Among upper limb amputees, the trans-radial ones make up the 60 % of total wrist and hand amputations. Starting from this statistics we focus our attention on upper limb transradial amputees. Reasons for amputations include cardiovascular disease, traumatic accidents, infection, cancer, nerve injury and congenital anomalies (in according to ISHN statistics). In the Figure 1.1 statistic for upper limb amputees is shown.\nNowadays there exists two types of prosthetic hands: invasive and noninvasive. The first ones provide for direct installation in the arm of the\n7\nCHAPTER 1. INTRODUCTION\namputee with the surgery. The second ones can be put on during day and removed in the night or when an amputee wants. In this work we focus on myoelectric prosthetic hands that belong to the second type of prosthesis. These are very advanced from the hardware point of view thanks to the exploitation of several portable sensors used to gather the electrical signal from the stump of an amputee. From a software view of point, the prosthesis is controlled with several algorithms that analyse the input signal and provide the best output movement. Examples of prosthesis are shown in Figure 1.2.\nNowadays robotics has reached great heights and there exist robotic hands designed to perform any desired task or a predetermined sequence of these. Also the prosthesis, built for the amputees, are very advanced: these have five fingers and can potentially perform all the possible configurations, like a human hand. Despite the rapid progress that robotics has had in last years, for an amputee the control of non-invasive robotic prosthetic hand is still far ([19]). The real challenge in the world of robotic hands is to achieve the total control of prosthesis. In fact, in most cases the tasks that an amputee can perform with this are limited to opening and closing. The control of prosthetic hand is far from simulation of natural movements and the training process to make amputees familiar with them is still long and sometime painful ([19] and references therein). The open challenge in the world of bio-robotics is to try to reduce the time required for learning of use of prosthesis and to make this control as natural as possible.\n8\n1.1. CONTRIBUTIONS OF THIS THESIS"
    }, {
      "heading" : "1.1 Contributions of this thesis",
      "text" : "This thesis tackles the control of prosthesis with an approach based on machine learning, as previous works in this field suggest ([8], [9], [10]). Our purpose is to increase the level of dexterity in the use of prosthesis, in order to favour a natural form of control. The prosthesis to which we refer in this work are composed by several electrodes that gather the Electromyography (EMG) signal, i.e. the electrical manifestation of muscular activation. With machine learning techniques, we analyse this signal in order to decide the most probable movement that the subject wants to perform. Currently, machine learning algorithms have success in this field, and not only, for their generality and for the possibility to enlarge the traditional theory. One of the most famous definition of machine learning is provided by Tom M. Mitchell and can be summarized as follow: a computer program is said to learn if its performance at a given task is improving with experience. The given definition is enough intuitive because it is not very far from the traditional human learning and skill. Human beings don’t learn new things in isolation but with the help of what it is already known: i.e. their previous experience ([4]). This intuition can be extended also for the control of prosthesis: for an amputee could be simpler to learn a movement using experience from other subjects that are already able to perform this. If this statement is true, which previous experience to exploit could play a key role and could change the final learning result. Thus, the questions that this thesis aims to answer are essentially two:\n• Is it possible to speed up the process of control of prosthesis with the help of prior experience?\n• What does it change when we use for an amputee the experience from other amputees, as opposed to that from intact subjects?\nThe contribution of this work is to answer to these queries following an experimental approach. In order to achieve this goal we performed a thorough series of simulations exploiting data from a public database. Our results show that the help of previous experiences boost the control of prosthetic hands and speed up the learning process. For an amputee it is not achieved a different result when experience from amputated or intact subjects is exploited.\n9\nCHAPTER 1. INTRODUCTION"
    }, {
      "heading" : "1.2 Outline",
      "text" : "The content of this thesis is organized as follows:\nIn Chapter 2 we give some landmarks useful to have an introduction of the problems tackled in the following and a general vision of the entire work. We begin with an overview of different kind of prosthesis and the general problems and challenges related to their usage and control. In the end of the chapter we enumerate all the contributions given by previous works in this field about data, analysis and algorithms.\nIn Chapter 3 we describe the experiment NinaPro that gathered kinematic, dynamic and electromyographic data from intact and amputated subjects and built the database that we used in our experiments. In the first part of the chapter we describe in detail the acquisition protocol, used devices and the data collection. In the second part we describe the path generally followed in this field for processing and representation of raw data acquired.\nChapter 4 focuses on the learning algorithms used in this work to solve the problem of recognition and classification of different hand movements. First we present an introduction about the traditional and innovative theories on which these methods are based. In the second part we introduce the used algorithms and their implementation.\nIn Chapter 5 we present our experiments and results obtained. We have worked with three different settings using data described in Chapter 3 and algorithms introduced in Chapter 4. First we present each experimental protocol and the results obtained in the three experiments done. We conclude with an overall discussion and comparison of our findings.\nThe thesis ends with a conclusive summary and possible direction of research and future works.\n10\nChapter 2"
    }, {
      "heading" : "A few landmarks",
      "text" : "This chapter gives a general vision of the entire works: it aims to focus the problems linked to prosthesis, the best path to tackle them and the related works useful in this task.\nSection 2.1 focuses on different prosthesis currently used by amputees. After a general introduction, we list the problems that an amputee tackles when he/she learns how to use a prosthetic hand. Section 2.2 is about challenges and questions that this work aims to solve. We explain the general path taken and, briefly, the results obtained. In section 2.3 we report a general vision and a summary of all the work developed. In section 2.4 we list the principal works that have dealt with related problems."
    }, {
      "heading" : "2.1 Prosthesis and control",
      "text" : "As said in the introduction, our study is referred to upper limb amputated, that are the majority in the community of amputees, and to non-invasive prosthesis. The most common non-invasive prosthesis are: cosmetics, body-powered and myoelectrics (Figure 2.1).\n11\nCHAPTER 2. A FEW LANDMARKS\nThe cosmetic ones are only aesthetic hands, they are comfortable but any type of grasping or movement is forbidden. The body-powered are mechanical prosthesis, they work by using cables to link the movement of the body to the prosthesis and to control it. When an amputee moves the body in a certain way he/she drives the cables, consequently the prosthetic hand is opened, closed, or bended. Obviously only few movements can be performed. The myoelectric prosthesis use several electrodes placed in contact with the skin of the stump. These kind of electrodes are called electromyography surface and they detect electrical activity produced by skeletal muscles when a movement is performed. The recorded electromyographic signal (EMG) varies in: ∼ 10µV ÷ 10mV ([19] and references therein). The electrodes record the muscular activity, the signal is analysed in order to decide the intentional movement and, theoretically, infinite positions can be performed.\nIn this work we study the control of non-invasive myoelectric prosthesis. They could potentially improve the quality of life of an amputee, but the control system is difficult. The open challenge in the world of myoelectric prosthesis is to try to reduce the training time, i.e. the time required for learning how to use them. Nowadays, this is still a very long process, often with great mismatch between desired and performed movements. Moreover this is generally perceived as very tiring and sometimes painful by the users. These reasons make the use of myoelectric prosthesis still limited in practice. Often the amputees stop using this kind of prosthetic hands and replace them with a cosmetic ones ([19]).\n12\n2.2. PATH IN LEARNING PROBLEMS"
    }, {
      "heading" : "2.2 Path in learning problems",
      "text" : "The control of robotic prosthetic hands using non-invasive techniques, like myoelectrical surface, is still a challenge nowadays. In bio-robotics and rehabilitation community it is clear that the success of myoelectric prosthesis is linked to the creation of an accurate control system to make them easy to use by the patient (see [8] and references therein). The path generally chosen to tackle the problem is machine learning ([8], [9], [10] and references therein). It makes it possible to analyse the electromyography signal with modern statistical techniques like support vector machines, neural networks and linear discriminant, in order to guess the movement that the subject wants to perform ([8]). Generally with these techniques a subject can choose among a finite number of hand postures. Thus, the final configurations are not all the possible ones but only a selection of these. In a machine learning problem a learner system tries to perform a task with the help of previous experience. In our work the learner is the subject that wants to learn to perform several movements with a robotic hand. Evidently, the choice of experience influences the whole problem. Most of the existing machine learning methods build a new learning model directly over the data from the learner itself. In our case, starting from the EMG signals collected from the user, it is built a function that, for any future unknown EMG signal, chooses the most probable hand posture associated. This method gives good results only if a large amount of training samples of the subject is available. Most recent algorithms suggest to exploit a source of knowledge external to the subject studied: i.e. experience from other subjects. If this previous experience consists in robust statistical models built in the past, these can be reused when a new patient trains the prosthesis. Obviously a form of adaptation is needed when we use experience from a subject to boost the learning of a new patient because the two domains could differ. The EMG signal recorded from different subjects can vary due to characteristics of the forearm (like size and shape), personal characteristics of subjects (like gender, age, use of the arm), electrode displacement and muscular fatigue ([25]). By using state of the art adaptive learning algorithms, we want to understand if knowledge from other subjects can boost the control of prosthesis and reduce the learning time for a new amputee. Previous experience that an amputee exploits to achieve the goal of a natural use of prosthesis could come from amputees or from intact subjects. Our task is to study how the final result might change in the two cases.\n13\nCHAPTER 2. A FEW LANDMARKS"
    }, {
      "heading" : "2.3 Experimental framework",
      "text" : "We clarify now all the passages to perform an experiment that aims at the control of non-invasive myoelectric prosthesis. We consider intact and amputated subjects from which we gather electromyographic data. These data are collected by using, as sensors, surface electrodes placed on the last part of the arm for an amputee, or on the forearm for an intact subject. These electrodes detect the electromyographic signal generated by muscular contractions, where each signal corresponds to a different movement. The aim of prosthetic control is to establish the best output movement for each given input signal. In order to achieve this goal the input data are processed to remove a component of noise. In a second step, we extract useful information from the data, in order to determine the discriminant characteristics of signal that we want to classify. This process is called features extraction. Data appropriately processed are used as input of different recognition algorithms. These algorithms solve in different ways a classification problem: given an input signal their aim is to find the right output movement (i.e. the right class) between the ones proposed. In this kind of problem is not possible to select other classes than the initial ones, thus the final hand movements are not all those that are the possible. Each algorithm works by exploiting differently the previous experience coming from source subjects. All the described steps are reported schematically in Figure 2.2.\nIn this work we exploited data acquired during the experiment NinaPro (2011-2013) and online available. After the choice of data with which to work, we were dedicated to their processing and representations, following what the literature proposes. To solve the final classification problem we used the adaptive learning algorithms that constitute the state of the art in the field of machine learning.\n14\n2.4. PREVIOUS WORKS"
    }, {
      "heading" : "2.4 Previous works",
      "text" : "Database and EMG signal. Data used in this work have been collected in the experiment NinaPro, where 10 or 12 sEMG electrodes (it depends on the chosen configuration) gather the electical signal from the arm of the users. In [19], [20], [21] all the details about used devices, the acquisition protocol and any information on database are present. NinaPro represents an important contribution in this field because it is, in according to our knowledge, the only existing public database that gathers data of a consistent number of subjects and postures. As [19] and references therein underline, previous studies usually include too few subjects and too few tasks: the maximum is represented by 11 intact subjects and 6 amputees with a maximum of 12 tasks. The NinaPro database collects data and clinical information of 78 subjects, 67 intact and 11 trans-radial amputees, that perform 53 or 50 postures (it depends on the chosen experiment). This large amount of data makes results obtained with this database statistically relevant.\nData acquired need several preprocessing steps in order to make them available for a recognition process or for movements classification. The preprocessing of EMG data and their final representation, called features extraction, could have a profound impact on the final performance of the algorithm chosen to solve the classification problem of EMG signal. In [24] the interested reader can find all the details about feature extraction methods used in order to analyse sEMG signals. The methods described in that work can take into account amplitude or spectral properties of the signal. The first ones are the algorithms that work in the time domain: Mean Absolute Value, Variance, Waveform Length or Cepstral Coefficients. The second ones are the algorithms that operate in the frequency domain: Frequency Ratio or Mean Frequency. Considering time and frequency domain at once we obtain the time-frequency domain features: Short Time Fourier Transform, Wavelet Transform, or Wavelet Packet Transform. These are richer of information about the signal but the computational cost increases. Authors of [23] studied and compared different methods in order to understand which are the ones that give the best ratio between the final performance and computational cost. Their results show that Mean Absolute Value and Waveform Length, despite their simplicity, can reach similar performance to the computationally more demanding marginal Discrete Wavelet\n15\nCHAPTER 2. A FEW LANDMARKS\nTransform.\nDomain Adaptation. It is a field associated with machine learning used to overcome the distribution mismatch between different domains. In general, in order to solve a new target problem with few labelled data, we can exploit solid models from sources, built with a large amount of data. The adaptive learning methods transfer useful information from the source domain to the target domain, although these are different, when the task to solve is the same. With this technique we avoid the collection of new samples from our target and the building of a little robust model based only on the few data gathered.\nHow to exploit the source prior knowledge and how to adapt it to the new target model depends on the algorithm used. Over the last years different directions have been proposed on how to tackle the problem. Each one suggests a different adaptive method between sources and target. There are methods that aim to approach directly data from target and sources using some mathematical tricks. In [13] a process based on two stage of weighting for each source sample is used in order to overcome the distribution mismatch. The authors of [28] take into account the fact that source and target live in different space; in order to overcome this problem it is exploited a function that builds a path from source to target in a dimensional reduced space. Another possible solution is to leverage over source models exploiting their parameters. The basic idea is that the new parameters of the target models must be close to the source ones and that they must be found solving an optimization problem. The first algorithm proposed in this direction is presented in [10], in this case only a single source model is exploited (the best one). In [8] this approach is enlarged by exploiting different linear combinations of source models. In the last method proposed in domain adaptation the sources are considered as experts. The judgement given by them for each target sample consists in extra features. We can distinguish three different levels in this approach, depending on the processing of the extra features. A middle level method based on this theory is proposed in [14] and [15]. An high level method is instead exploited in [18]. All these different approaches evaluate autonomously the importance of each previous knowledge and decide individually from where and how much to transfer.\n16\n2.4. PREVIOUS WORKS\nThe traditional fields in which domain adaptation is applied are: language processing for speech recognition ([12]) and computer vision for image classification ([11], [18]).\nApplication of domain adaptation to biological signal. The problem that we tackle in the prosthetic control is a domain adaptation problem due to the differences of EMG signal distribution among different subjects. In fact, the direct use of sources data for the solution of a target problem could result poorly in performance due to the many differences between the two domains. EMG signal from sources and target can differ for the user’s age, gender, height, weight, dominant hand, different exercise of the arm muscle and placement of electrodes ([25]). Thus an adaptation process is necessary.\nThis kind of studies and application are very recent. One of the first work, according to our knowledge, proposed in this environment is [10]. This proposes a method that chooses automatically the best source that the target problem can exploit. It uses directly its parameters in order to build the new target model. This algorithm is generally called Best-Adapt. Previous method is resumed and revisited in [8], where two adaptive algorithms able to exploit many prior knowledge models are proposed and compared with the previous one. The two methods are called Multi-Adapt and Multi-perclass-Adap. The first exploits a weighted combination of sources, the second assigns a different weight to each class of each source. In [8] these algorithms are tested on sEMG data from 10 and 20 intact subjects with 6 postures. The algorithm that gives the best result in performance is Multiperclass-Adap. In [9] several existing algorithms are tested. The adaptive methods that are compared come from [13], [28], [14] and [8] (see previous paragraph). The experiments of this work involve 10 intact subjects with 9 postures and 27 intact subjects with 12, 17, 23 and 52 postures. The results show that the method proposed by authors of [13] need a running time much longer than the others. Generally the method that achieve the best performance is the one proposed by authors of [14]. In both cases ([8],[9]) experiments showed that for intact subjects the postures recognition and classification can be improved and boosted by the use of prior knowledge of other intact subjects.\n17\nChapter 3\nData description and representation\nResearch in the area of hand prosthetics suffered from a number of problems. First, previous studies in this field are based on few subjects with few hand postures: according to our knowledge, up to 11 intact subjects and 6 amputees that perform a maximum of 12 movements (see [19] and references therein). It makes it hard to obtain statistically relevant results. Second, it is necessary to establish a data analysis method accepted in order to compare final results. Currently, the best public database is NinaPro (Non Invasive Adaptive Prosthetics). Its data have been collected during the experiment NinaPro (2011- 2013) and, according to our knowledge, represents the state of the art among public database in this field. In this work, we decided to use data coming from NinaPro for our experiments in order to overcome the first problem described above. Regarding data processing, we decide to use the algorithms of features extraction proposed in literature that, currently, represent the state of the art in the field of analysis of raw data.\nIn section 3.1 it is introduced the NinaPro database. We describe the acquisition set-up with particular attention to the experimental protocol and used devices to collect data from subjects. In the last part of this section we focus on the organization and structure of the dataset. In section 3.2 we explain all the procedures for data processing. Data and all the reference papers are available in http://ninapro.hevs. ch/.\n19\nCHAPTER 3. DATA DESCRIPTION AND REPRESENTATION"
    }, {
      "heading" : "3.1 Data description: the NinaPro database",
      "text" : "The NinaPro (Non Invasive Adaptive Prosthetics) database holds data and clinical information about 78 subjects: 67 intact and 11 trans-radial amputees. According to our knowledge, it is the only existing public database with a consistent number of subjects. Data have been acquired from 2011 to 2013. All the information about data acquirement and database can be found in [19],[20] and [21]."
    }, {
      "heading" : "3.1.1 Acquisition setup",
      "text" : "The acquisition setup ([19]) is composed by several devices for the recording of hand kinematics, dynamics and muscular activity. All the sensors are connected to the laptop to make the data recording possible. The CyberGlove II (CyberGlove Systems LLC, www.cyberglovesystems. com) is a motion capture data glove that takes information about hand kinematic using 22 sensors (see Figure 3.1). These sensors register hand and fingers motions and return 22 8-bit values of resistance. That resistance is proportional to the angles between pairs of hand joints of interest (for example metacarpum-phalanx, inter-finger, palm arch angles and so on). The average resolution is less than one degree and it depends on the size of the subject’s hand.\nA standard commercially available 2-axis IS40 inclinometer (Fritz Kübler GmbH, www.kuebler.com) is fixed to the subject’s wrist to measure the wrist orientation. This device covers a range of 120◦and it has a resolution of 0.15◦. Hand dynamics is measured by a strain gauge sensor: Finger-Force Linear\n20\n3.1. DATA DESCRIPTION: THE NINAPRO DATABASE\nSensor (FFLS). It records flexion and extension forces of all fingers and abduction and adduction of the thumb. Muscular activity is measured using sEMG electrodes. In the first configuration of the experiment 10 MyoBock 13E200-50 electrodes (Otto Bock HealthCare GmbH, www.ottobock.com) were used. These electrodes are set to amplify the signal of about 14.000 times. They have also a shielding and filtering system in order to avoid low and high frequency interferences, for example from 50–60 Hz power sources, mobile phones or security systems. sEMG signals are sampled at a rate of 100 Hz. In the second configuration of the experiment 12 Trigno Wireless electrodes (Delsys Inc, www.delsys.com) are used. sEMG signals are sampled at a rate of 2 kHz. In both cases the equipment is fixed on the forearm using a hypoallergenic elastic latex–free band. Figure 3.2 shows how electrodes are placed. In the first configuration, with 10 electrodes, eight of these are equally spaced around the forearm, at the height of the radio-humeral joint. Two are placed on the main activity spots of the flexor digitorum superficialis and of the extensor digitorum superficialis. In the second configuration two electrodes are added on the main activity spots of the biceps brachii and of the triceps brachii.\nIn the case of intact subjects, the recording of sEMG and kinematic data is from the same arm. In the case of amputees, sEMG signals are recorded from the missing limb while kinematic and dynamic data from the intact limb.\n21\nCHAPTER 3. DATA DESCRIPTION AND REPRESENTATION"
    }, {
      "heading" : "3.1.2 Acquisition protocol",
      "text" : "Before the beginning of data acquisition, each subject has a written and an oral explanation about the experiment and he/she has to sign an informed consent form. All the experiments are approved by the Ethics Commission of the Canton Valais (Switzerland). As first step, several clinical data like age, gender, height, weight and laterality are annotated. For amputated subjects, information about date, type and reason of the amputation, remaining forearm percentage, use of prostheses and phantom limb sensation are collected. The remaining forearm percentage is the ratio between the length of the amputated forearm and the length of the intact forearm from the elbow to the wrist, rounded to the tens. As second step, subjects are seated on a office chair. A laptop in front of the subjects shows with a movie the exercises that they have to perform. The intact subjects are asked to mimic movies on the screen with their dominant hand. Amputees are asked to mimic as naturally as possible the same movement with their missing limb. For amputees, generally, it is very difficult to reproduce an action with missing limb. Thus, they can simulate a task bilaterally or they can follow a visual stimulus, that could be the movie on the screen or the experimenter that performs the same movement. Before starting to record data, there is a training phase. This consists of a mix of the future exercises, it is important to make the subject practical with the experiment. All the performed movements are divided in four different group: A, B, C and D (see Figure 3.3). The first group (Exercise A) is about 12 basic movements of fingers. The second (Exercise B) is about 8 hand configuration and 9 basic movements of the wrist. The third (Exercise C) is about 23 grasping movements and it involves everyday objects that are presented to the subject to mimic daily-life actions. The fourth (Exercise D) is about 9 force pattern, it consists in a press combinations of fingers with an increasing force. The movements are chosen from the literature about taxonomy, robotics and rehabilitation. Each movement is alternated by rest posture to avoid muscular fatigue. The expected execution time of each movement is about 5 s, of rest posture is about 3 s. The sequence of movements is not randomized in order to favour unconscious movements.\n22\n23\nCHAPTER 3. DATA DESCRIPTION AND REPRESENTATION"
    }, {
      "heading" : "3.1.3 Data collections",
      "text" : "The NinaPro database, acquired with the setup and procedure explained in the previous section, consists of three sub-datasets, according to the devices used and the subjects’ characteristics. The first database contains data from 27 intact subjects: 7 females and 20 males, 2 left handed and 25 right handed with 28 ± 3.4 years. The electrodes used for sEMG data acquisition are the 10 Otto Bock. The subjects perform the exercise A,B and C described in the previous section. Each movement has been repeated 10 times and each repetition is interspaced by the rest posture. The second database contains data obtained from 40 intact subjects: 11 females and 29 males, 5 left handed and 35 right handed with 29.9 ± 3.9 years. The electrodes used for sEMG data acquisition are the 12 Delsys. The subjects perform the exercise B,C and D described in the previous section. Each movement has been repeated 6 times and each repetition is interspaced by the rest posture. The third database contains data obtained from 11 trans-radial amputated subject: 11 male, 1 left handed and 10 right handed with 42.36 ± 11.96 years. The subjects perform the exercise B,C and D described in the previous section. Each movement has been repeated 6 times and each repetition is interspaced by the rest posture. All the details about the datasets are reported in Table 3.1.\nBefore the raw data could be used for classification, several steps are necessary. The first step is the filtering: the Delsys sEMG signals, that are not shielded against power line interferences, are cleaned from 50 Hz power-line\n24\n3.1. DATA DESCRIPTION: THE NINAPRO DATABASE\ninterference using a Hampel filter ([23]). The second step is the synchronization: it is a linear interpolation or nearest-neighbour interpolation of data. The third step is the relabelling: it is a correction of movements label by maximizing the likelihood of a rest-movement-rest sequence. In fact the movements performed by the subjects may not perfectly match with the stimuli proposed, due to human reaction times and experimental conditions. The raw data are not online but are available upon request. For each subject and exercise one can find online (http://ninapro.hevs. ch/) a Matlab file with the following data:\n• subject: subject number;\n• exercise: exercise number;\n• emg (10 or 12 columns): sEMG signal of the 10 or 12 electrodes. Columns from 1 to 8 include the signal from 8 electrodes around the forearm, columns 9 and 10 include the signal of the muscle Flexor/Extensor Digitorum Superficialis, columns 11 and 12 include signal of the muscle Biceps/Triceps Brachii;\n• acc (36 columns): three-axes acceleration values of the 12 electrodes (only for database 1 and 2);\n• glove (22 columns): signal from the 22 sensors of the Cyberglove;\n• inclin (2 columns): signal from the 2 axes inclinometer (of the wrist);\n• stimulus (1 column): the original label of the movements repeated by the subject (before the relabelling phase);\n• restimulus (1 column): label of the movements a-posteriori (after the relabelling phase);\n• repetition (1 column): repetition of the stimulus;\n• rerepetition (1 column): repetition of restimulus;\n• force (6 columns): force recorded during the third exercise of database 2 and 3;\n• forcecal (2 × 6 values): minimal and maximal force values for each sensor;\n25\nCHAPTER 3. DATA DESCRIPTION AND REPRESENTATION\nThe database 3 presents some exceptions. The amputated subjects 7 and 8 had only 10 electrodes instead of 12 due to insufficient space. The amputated subjects 1, 3 and 10 asked to interrupt the experiment before its end due to fatigue or pain. They performed respectively 39, 49 and 43 postures (including rest)."
    }, {
      "heading" : "3.2 Data representation: features extraction",
      "text" : "Online sEMG data require some processes to make them available for classification. Thus, we want to extract from them useful information to achieve this goal. This process is called features extraction. It consists in several steps of preprocessing in order to clean up data from the component of noise and in choosing of the algorithm for data representation. For a complete reading about this approach we refer to [22] and [23]. The described approach and processes are universally accepted in this field. In this thesis we work with data of Exercise 1 (17 hand configuration and movements of the wrist) of the second and third database. Thus the reported values of parameters are referred to this analysis."
    }, {
      "heading" : "3.2.1 Preprocessing",
      "text" : "The initial data consists of a single continuous signal (one for each electrode) that contains the information about the sequence that goes from the first movement to the last, interspaced by rest posture. The first step consists of the division of a movement from another. Signals with different labels are separated. Each resulting matrix identifies different movements or rest, the columns of this matrix are the channels (10 for first database, 12 for second and third). In Figure 3.4 is shown the movements division process.\n26\n3.2. DATA REPRESENTATION: FEATURES EXTRACTION\nThe second operation is called windowing. Each matrix from the previous point is divided in overlapping windows of fixed length. The windows length used in the literature is of 100 ms, 200 ms and 400 ms, the shift considered is of 10 ms ([23]). Thus, between windows there is an overlap of N − 10ms, where N is the window length considered. All the parameters are analysed in the literature. The experiments of [23] show that a window length of 200 ms or 400 ms results in higher accuracy. Indeed, if a window is long enough, the error of label on the edge of a movement can be reduced. In this work the used windows length is N = 200ms (i.e. 400 samples) and the increment of the sliding window is 10 ms (i.e. 20 samples). An example of windowing is shown in Figure 3.5.\n27\nCHAPTER 3. DATA DESCRIPTION AND REPRESENTATION\nAs said before, each movement is repeated a fixed number of times. The third passage consists of the split of the signal in test and training part, depending on number of repetitions considered. For the second and third database the repetitions { 1, 3, 4, 6 } are used as training, the others ({ 2, 5 }) as test. At this point the training part can be sub-sampled to reduce the amount of data and to achieve a computationally feasible problem. The training set is reduced by keeping every 10th sample."
    }, {
      "heading" : "3.2.2 Feature extraction",
      "text" : "The last step is the choice of the algorithm for the data representation. There are some experimental evidence that we can consider in sEMG processing operation. First, there is a quasi-linear relation between Root Mean\n28\n3.2. DATA REPRESENTATION: FEATURES EXTRACTION\nSquare (RMS) amplitude of sEMG signal and force exerted by a muscle ([23], [26]). Second, sEMG spectral characteristics might be related to conduction velocity of muscle fibers ([23], [27]). If a time domain algorithm is used the first aspect is privileged. If a frequency domain algorithm is used the second aspect is favoured. The most important algorithms are reported in Table 3.2. We refer to x̂ as a feature computed from a signal x of length T , where its elements are indexed as xt.\nThe authors of [23] compared the previous different algorithms for features extraction. From these studies emerge that, in the time domain, Mean Absolute Value and Waveform Length have similar performance to the computationally more demanding marginal Discrete Wavelet Transform. In the frequency domain Short Time Fourier Transform is more robust than simple Fourier Transform when dealing with non-stationary signal. In this thesis we work in time domain with the mean between Mean Absolute\n29\nCHAPTER 3. DATA DESCRIPTION AND REPRESENTATION\nValue, Variance and Waveform Length to be as independent as possible from the method. In Figure 3.6 is shown the representation of a movement after features extraction.\nIn Figure 3.7 we show the total signal before and after processing and feature extraction.\n30\n3.2. DATA REPRESENTATION: FEATURES EXTRACTION\nAt the end of the feature extraction step, we have a set of vectors with dimension equal to the number of channels. The output of each vector is one of the possible movements inside the three exercises with the adding of rest. The number of training vectors and test vectors for each subject are respectively of the order of 3 · 103 and 2 · 104.\n31\nChapter 4\nFrom theory to learning algorithms\nAfter extracting features, we have a set of data to classify. In this chapter we present the state of art learning algorithms that solve classification problems and the theory on which they are based.\nThe section 4.1 is an introduction about the mathematical framework and background required to understand the rest of the work. In section 4.2 we focus on the traditional form of supervised learning, where we exploit only the information from the target domain. It is the supervised learning on a single domain. In section 4.3 we introduce Support Vector Machines (SVMs). It is one of the most used and theoretically well motivated methods in machine learning scenario. It represents the state of the art of supervised learning on a single domain. In section 4.4 we introduce a new form of supervised learning, where we exploit information from target domain and source domains. It is the supervised learning on different domains. Section 4.5 begins with a general introduction about different proposed directions to tackle the problem of domain adaptation. In the last part of the section we introduce the state of the art algorithms for supervised learning on different domains. We explain the theory on which each algorithm is based and the main structure of the code with the help of flow charts. All the mentioned algorithms are implemented with Matlabr.\n33\nCHAPTER 4. FROM THEORY TO LEARNING ALGORITHMS"
    }, {
      "heading" : "4.1 Mathematical framework and background",
      "text" : "In this section we present mathematical tools necessary to tackle the rest of the work. In the following we indicate column vectors with small bold letters, e.g. a = [a1, a2, ..., aN ] and matrices with capital bold letters, e.g. A ∈ RM×N. With Ai,j and Ai we denote respectively the element (i, j) and the i -th column of matrix A. In the rest of the work we refer to xi ∈ X as an input vector of a learning algorithm and to yi ∈ Y as its associated output. X ⊆ R and Y = R are respectively the input space and the output space. We denote with D = {xi, yi}Ni=1 a set of data that come from an unknown probability distribution. The goal of a learning algorithm is to find a function that, for any future input vector xi, can determine the best corresponding output yi. In a classification problem, like the one analysed in this work, yi can assume values from a finite set that are the possible output classes. If the possible outputs are only two we have a binary problem, if these are more we have a multiclass problem. In this thesis we have the second type of problem and classes are equal to the possible hand postures. In the following we refer to G as the total number of classes, or possible output label.\nLoss function. In general we can define a loss function as a function that associates a real number to a given event in order to represent some “cost” associated with the event. In a classification problem it is the function that represents the price paid for a misclassification. For a binary problem let us refer to yi and ỹi as, respectively, the true and the predicted label of a vector xi. The loss function can be defined as:\nl(ỹi, yi) = max{0, (1− ỹiyi)p}, (4.1)\nwhere different choices of parameter p lead to different penalties for misclassified vectors. In a multiclass problem, let us define with Ỹi the column of the confidences for each class referred to a given vector xi. The loss function is modified as follow:\nl(Yi, Ỹi) = max{0, 1− Ỹyi,i + max g 6=yi {Ỹg,i}}. (4.2)\nThe loss is zero if the confidence value for the correct class is at least greater\n34"
    }, {
      "heading" : "4.2. SUPERVISED LEARNING ON A SINGLE DOMAIN",
      "text" : "than one with respect to the other confidence values. Otherwise, the loss is linearly proportional to the differences between the confidence values of real class and the maximum confidence referred to another class. The goal of a learning problem is to minimize this risk."
    }, {
      "heading" : "4.2 Supervised learning on a single domain",
      "text" : "We refer to training samples and test samples as a set of vectors, or items, for which we know the output classification, i.e. the label value. In a supervised learning problem the first ones are used in the training phase to build a classification model. The second ones are exploited in the test phase to evaluate the performance of a learning algorithm from the agreement between the predicted labels and their real values. Thus, the learner receives a set of labelled examples as training data and makes predictions for all unseen points. In the following we refer to N as the total number of training vectors. We define the target problem as the new classification problem that we aim to solve. In our case it is the new subject that learns to perform some hand postures. In the simplest case, in order to build a classification model for the new target subject, we can exploit only the training data of the target himself. Thus, data used to train the model and data used to test it come from the same domain, i.e. the target domain. Taking this into account, it is reasonable to assume that the data present the same distribution. We refer to this process as a supervised learning on a single domain: in fact in order to build the model, we use available labelled data (i.e. supervised process) from only the target subject (i.e. single domain). In section 4.3 we present the Support Vector Machines (SVMs) method. It represents the state of art of supervised learning on a single domain for classification problem."
    }, {
      "heading" : "4.3 Support Vector Machines",
      "text" : "In this section we introduce one of the most effective classification algorithms in machine learning: Support Vector Machines (SVMs). The goal of SVMs is to produce a model, based on the training data, able to predict the output of a given test vector. For a complete reading about topics, one can refer to [2], [3] and [7].\n35\nCHAPTER 4. FROM THEORY TO LEARNING ALGORITHMS"
    }, {
      "heading" : "4.3.1 Linear SVMs",
      "text" : "We start with the introduction of linear SVMs for the solution of a binary problem. We refer to the two types of vectors as positive and negative ones, i.e. yi ∈ Y = {1;−1}. The aim of a classification problem is to find an hyperplane which separates positive from negative vectors. For a point that lies on this hyperplane the following equation holds:\nw · x + b = 0, (4.3)\nwhere w identifies the hyperplane (it is the direction vector of the plane) and |b| ‖w‖ is the normal distance from the hyperplane to the origin. Usually there are many hyperplanes that a learning algorithm can choose to solve a problem. The SVMs algorithm returns the hyperplane with the maximum margin m. The margin is the maximum distance between the separating hyperplane and the closest points of each sets. All these concepts are reported in Figure 4.1.\nIf we define marginal hyperplanes as w · x + b = ±1, the following inequalities hold :\nw · xi + b ≥ 1 for yi = 1, w · xi + b ≤ −1 for yi = −1.\n(4.4)\nThe vectors that lie on marginal hyperplanes are called support vectors. The equations 4.4 can be combined in order to obtain a unique inequality:\n36\n4.3. SUPPORT VECTOR MACHINES\nyi(w · xi + b)− 1 ≥ 0. (4.5)\nUsing equations of marginal hyperplanes we can find that m = 1‖w‖ , thus it is clear that searching the hyperplane that maximizes the margin is equivalent to minimize ‖ w ‖ with constrain (4.5).\nIn most of classification problems, data present a component of noise, an example is shown in Figure 4.2.\nTo take into account the possible noise in the data, we can introduce the slack variables ξi. This variable measures the distance by which vector xi violates the desired inequality: yi(w · xi + b) ≥ 1. At this point we can define the minimization problem to solve as:\nmin w,b,ξ\n1 2 ‖ w ‖2 +C p N∑ i=1 ξpi\nsubject to: yi(w · xi + b) ≥ 1− ξi ∧ ξi ≥ 0, ∀i ∈ [1, N ]. (4.6)\nThis can be formulated as a Lagrangian problem as follows:\nL(w, b,α, ξ,β) = 1 2 ‖ w ‖2 +C p N∑ i=1 ξpi − N∑ i=1 αi[yi(xi ·wi+b)−1+ξi]− N∑ i=1 βiξi.\n(4.7)\n37\nCHAPTER 4. FROM THEORY TO LEARNING ALGORITHMS\nFirst and second terms of sum are the objective function that we want to minimize. The others are the constraints multiplied by different Lagrangian multipliers α and β. In this optimization problem, we minimize the amount of slack variables and ‖ w ‖ . Generally, these two requests are conflicting. The parameter that sets this trade-off is C. The parameter p describes different penalties for misclassified vectors as we explain in the end of this paragraph. Until objective function and constrains are convex and differentiable, the KKT conditions can be applied at the optimum:\n5wLp = 0 ⇒ w = N∑ i=1 αiyixi, (4.8)\n5bLp = 0 ⇒ N∑ i=1 αiyi = 0, (4.9) 5ξiLp = 0 ⇒ αi + βi = Cξ p−1 i , (4.10)\n∀i, αi[yi(w · xi + b)− 1 + ξi] = 0 ⇒ αi = 0 ∨ yi(w · xi + b) = 1− ξi, (4.11)\n∀i, βiξi = 0 ⇒ βi = 0 ∨ ξi = 0. (4.12)\nThe first equation shows that w is a liner combination of training vectors. The forth equation indicates that the vectors that really appear in that combination are only the support vectors. Indeed for other vectors the Lagrangian multiplier is zero. In this case we have two different types of support vectors. The first ones are vectors that lie on margin hyperplane (they have ξi = 0). The second ones are called outlier and are the misclassified vectors for which ξi 6= 0. The task of determine a classifier is equivalent to find the parameters w and b of the model. In the test phase, given a new vector x with unknown label, the output hypothesis is:\nh(x) = sgn(w · x + b) = sgn( N∑ i=1 αiyi(xi · x) + b). (4.13)\nThe second equality can be simply obtained using equation (4.8). The extension of the theory to the multiclass case, i.e. yi ∈ Y = {1, 2, ..., G}, is straightforward. We solve different optimization problems, one for each\n38\n4.3. SUPPORT VECTOR MACHINES\nclass g = [1, 2, ..., G]. Thus, during the training phase, a pair (wg, bg) is found for each class. The considered class takes the label y = 1 and the others y = −1 (one-vs-all approach), in this way we solve G different binary problems. In the test phase, given a vector x, we choose the class g that gives the maximum output according to:\nh(x) = argmax g∈G (wgx + bg) = argmax g∈G ( N∑ i=1 αi,gyixix + bg ) . (4.14)\nLoss Function. As anticipated in section 4.1 the loss function is the penalty associated to a misclassified vector. For a binary problem the most common loss functions are the hinge loss and the quadratic hinge loss. These are respectively associated to p = 1 and p = 2 in equation (4.1). As shown Figure 4.3, penalties are different depending on the value of p chosen.\nWe have no loss if the prediction falls in the right part of the hypersurface (i.e. distance between point and hyperplane is greater than 1). When the prediction falls into the margin we have a loss 0 ≤ l ≤ 1. If the loss is greater than one the prediction is in the wrong part of the hypersurface.\n39\nCHAPTER 4. FROM THEORY TO LEARNING ALGORITHMS"
    }, {
      "heading" : "4.3.2 Non-linear SVMs",
      "text" : "Until now, a linear decision boundary is used to solve a classification problem. However there are techniques that extend SVM algorithms in order to define non-linear decision boundaries. Kernel methods are an example of these. A way to define a non-linear decision boundary is to use a non-linear map function defined as: Φ(x) : X 7−→ H. X ⊆ R is the input space of training vectors and H is a high-dimensional (possibly infinite-dimensional) Hilbert space called feature space. In other words, if the training vectors are not linearly separable in the input space we can replace each of them with Φ(xi) to make the problem solvable by changing the metric. An example is shown in Figure 4.4.\nHowever, determining the hyperplane solution requires multiple computations of inner products in high-dimensional spaces: 〈Φ(x) · Φ(x)〉 instead of the simple product between training vectors. A solution to this problem is to use kernel methods. A kernel over H is a function K : X × X 7−→ R. The idea is to define a kernel K such that:\n∀x,x′ ∈ X , K(x,x′) = 〈Φ(x) · Φ(x′)〉. (4.15)\nAn inner product is a measure of the similarity between two vectors, thus K can be interpreted as a similarity measure between elements of the input space X . K is often significantly less complex to compute than Φ(x) and its inner product. But there is another bigger advantage: under precise conditions (Mercer’s conditions) we can work directly with K without knowing\n40\n4.3. SUPPORT VECTOR MACHINES\nΦ(x). Now given a vector x with unknown label, the output hypothesis is:\nh(x) = sgn( N∑ i=1 αiyiK(xi,x) + b). (4.16)\nOne of the most used kernel is the Gaussian one, or Radial Basis Function (RBF):\nK(x ′ ,x) = e\n− ‖ x′ − x ‖2\n2σ2 = e−γ‖x ′−x‖2 . (4.17)\nIt is also the kernel chosen in the computer’s implementation of algorithms used in this work. For completeness, we underline that there are many kinds of kernel like: linear, polynomial and sigmoid.\nLeast Square Support Vector Machines (LS-SVM). The LS-SVM ([6]) problem is a new formulation of equation (4.6) in order to avoid the high computational burden of the original SVM problem. In this formulation we replace the inequality constraints with equality. The quadratic hinge loss (p = 2) and the kernel method are used. The optimization problem is written as follow:\nmin w,b\n1 2 ‖ w ‖2 +C 2 N∑ i=1 ξ2i\nsubject to: yi = wφ(xi) + b+ ξi, ∀i ∈ [1, N ], (4.18)\nThe solution of the reformulated Lagrangian problem is characterized by a linear system which takes a similar form as the linear system of standard SVM (see equations from 4.8 to 4.12). Let us indicate with I the identity matrix and with K the kernel matrix defined as:\nK = K(x1,x1) K(x1,x2) . . .. . . . . . K(xN ,xN)  (4.19) The solution of the problem is given by:\n41\nCHAPTER 4. FROM THEORY TO LEARNING ALGORITHMS\n[ α b ] = P [ y 0 ] , (4.20)\nwhere P =\n[ K + 1\nc I I IT 0\n]−1 and y is the vector of outputs.\nThe LS-SVM solution that we obtain now presents a difference with respect to the original SVMs. Sparsity is lost and all data contribute to the model."
    }, {
      "heading" : "4.4 Supervised learning on a different domain",
      "text" : "Up to now we have dealt with the traditional learning. One of the main assumptions on which it is based is that the training data, used to learn the target model, and the test data, used to test this model, come from the same distribution. However in many real problems this assumption is not true: this happens because data can be dependent on dynamic factors like time, acquisition devices and space ([4]). We tackle a similar problem when we want to learn a model with few available labelled data. To avoid the creation of a not very solid model we can exploit also data coming from different domains (for example from different data acquisitions or databases, gathered with different devices and so on). For example, we can have a lot of labelled data on a source problem and the need to solve the same problem for a different target domain with few labelled data. If source and target present a distribution mismatch the source data can’t be used directly to solve the target problem, hence a form of adaptation is needed.\nLet us define the concept of source as a known classification model built with a great number of training vectors. In a domain adaptation problem we aim to solve a new target problem exploiting information from a source domain. The classification problem of target and source is the same. In particular we have the same output label set, Y s = Y t. However, the target domain Dt and the source domain Ds are different in terms of marginal data distribution, P s(X) 6= P t(X). Thus, the conditional probability distribution P (y|x) might be slightly different for source and target ([4]). When we attempt to leverage over existing source knowledge to solve a target problem, we are combining information from different domains: the target domain and all the available sources domains. We can consider the source as a type of prior knowledge that, after an appropriate adaptation, can be used to learn something for new target problem.\n42\n4.5. DOMAIN ADAPTATION ALGORITHMS\nThe application on prosthetic hands treated in this work is an example of domain adaptation problem. In particular, the task is to catalogue different hand’s postures of new target subject using, as sources, subjects that already performed the same movements. We refer about domain adaptation problem because biological signals of the same movement can be different for different users in terms of marginal data distribution. In fact, the EMG signal from different subjects varies depending on gender, age, muscular activity, dominant hand, position of the electrodes during the data collection and so on (see Figure 4.5).\nThe goal of a domain adaptation algorithm is to find the best way to exploit useful informations from the available sources, in order to solve the new target problem. In the following we refer to K as the total number of sources."
    }, {
      "heading" : "4.5 Domain adaptation algorithms",
      "text" : "We present here several different algorithms that use adaptive learning in order to boost the training phase of a prosthetic hand with the exploitation of previous experience. Each methods try to combine in different ways informations from target domain (represented by training labelled vectors) and from domains of sources (represented by source classification models). In the following it is explained how informations coming from different sources can be combined. Each algorithm adds in different ways informations given\n43\nCHAPTER 4. FROM THEORY TO LEARNING ALGORITHMS\nby training vectors of target and this particular aspect is treated in the sessions dedicated to individual algorithms. Some methods exploit the prior knowledge directly. These use a combination of the parameters (w, b) of source models in order to obtain the new parameters of new model solving an optimization problem (see algorithm in section 4.5.1). In other methods the sources are seen like experts, or feature extractors. The outputs obtained using source’s models for the classification of training data are considered as a descriptor, or as an extra-features. At this point there are different methods to combine informations derived from different sources. We can discern three different levels of integration: low, mid and high. In the low level the extra-features, coming from each source, are combined into a single new vector. For this feature vector a classifier (for example a linear or non-liner SVMs) is trained. The different information from different sources influence in the same way the final decision. In fact, no weight factor is used. In the middle level the extra-features of each source are kept separately, but a single classifier is used for the final hypothesis. This classifier is based on SVM algorithm with a new kernel that is a liner combination of kernels of sources appropriately weighed (see algorithm in section 4.5.2). In the high level a classifier is trained for each extra-feature vectors. Each classifier produces a final hypothesis about training vectors. All these assumptions are combined together to produce the final output (see algorithm in section 4.5.3). The different ways of integration explained above are called cue integration methods ([5] and [17]) and are summarised in Figure 4.6.\n44\n4.5. DOMAIN ADAPTATION ALGORITHMS\nWith respect to the domain adaptation algorithms chosen, there are two baseline. The first is given by the solution of a classification problem using only the training data of the target. This method is called No Transfer and it solves non-linear LS-SVM problem trained on the target data only (see section 4.3.2). The second is represented by the exploitation of training data of the source only. This method is called Prior Features and it solves a linear LS-SVM problem trained on the source data only (see section 4.3.1).\nIn the rest of the section we present the algorithms used in the simulations of this work for the classification problem of hand movements. The main structure of the code is the same for all of them. We have a training phase and a test phase. In the first one, the classification problem is tackled for each target subject and models are built. For each subject the models are calculated several times for an increasing number of training vectors. In the second phase models are tested using new vectors. In the following we explain the training phase of each algorithms. It is the point that distinguish one adaptive methods from the others.\n45\nCHAPTER 4. FROM THEORY TO LEARNING ALGORITHMS"
    }, {
      "heading" : "4.5.1 Multi Adapt algorithm",
      "text" : "The first form of Multi Adapt algorithm was proposed in [10] and after extended in [8]. It solves a classification problem exploiting a combination of source models already known and properly weighted. An optimization problem based on non-linear SVMs with Gaussian kernel is solved for every source. The result is a vector ŵ for each of these. We begin to treat a binary problem to simplify the notation, in the end we extend the results to a multiclass problem. As a starting point, it is possible to use one of the pre-trained models to build the new model for the new subject. The basic idea is to search a solution for the new problem that is close to the pre-trained one. The optimization problem that we tackle is the following:\nmax w,b\n1 2 ‖ w− βŵ ‖2 +C 2 N∑ i=1 ξ2i\nsubject to: yi = w · φ(xi) + b+ ξi. (4.21)\nAs described in Section 4.3 we can use the Lagrangian formulation that leads to the following optimality conditions:\n5w L = 0 ⇒ w = βŵ + N∑ i=1 αiφ(xi), (4.22)\n5b L = 0 ⇒ N∑ i=1 αi = 0, (4.23) 5ξi L = 0 ⇒ αi = Cξi, (4.24) 5αi L = 0 ⇒ w · φ(xi) + b+ ξi − yi = 0, (4.25)\nwhere α = [α1, α2, ..., αN ] T is the vector of Lagrangian multipliers. As in non-linear SVM algorithm, the new model w is given by a combination of feature functions φ(xi). In addition, it appears the pre-trained model ŵ weighted by β. Previous equations can be combined and written in matrix form:[\nK + 1 c I I\nIT 0 ] [ α b ] = [ y− βŷ 0 ] , (4.26)\nwhere y = [y1, y2, ..., yN ] T is the vector that contains the real label of each\n46\n4.5. DOMAIN ADAPTATION ALGORITHMS\ntraining vector and ŷ = [ŵφ(x1), ŵφ(x2), ..., ŵφ(xN)] is the vector of labels predicted using the model ‖ ŵ ‖. K is the kernel matrix introduced in 4.19 and I is the identity matrix. Inverting equation (4.26) the solution is:[\nα b\n] = P [ y− βŷ\n0\n] , (4.27)\nwhere P is the inverse of the first matrix on the left of equation (4.26). In order to evaluate β, the leave-one-out error is introduced. Let us define with ỹi the prediction obtained on i -th sample when it is removed from the training set. Starting from equation (4.27) and writing the Lagrangian multiplier as α = α ′ + βα ′′ , it can be shown that the prediction vector has the following equation:\nỹi = yi − α ′ i\nPi,i + β\nα ′′ i Pi,i . (4.28)\nThus, given a set of training vectors, the weight β, that gives the minimum distance between true and predicted label, is chosen in agreement with:\nmin β N∑ i=1 l(ỹi, yi), (4.29)\nwhere l(ỹi, yi) is a loss function. The method described above exploits only one source, although many of them are available. The best source is selected by evaluating the minimal leave-one-out error. The previous approach can be extended in case of multiple sources by defining the following learning problem:\nmax w,b\n1 2 ‖ w− K∑ k=1 βkŵk ‖2 +C 2 N∑ i=1 ξ2i\nsubject to: yi = w · φ(xi) + b+ ξi, (4.30)\nwhere K are the number of sources and β = [β1, β2, ..., βK ] T is the vector of the importance weight of each prior. Now the optimal solution is composed by a linear weighted combination of previous models:\nw = K∑ k=1 βkwk + N∑ i=1 αiφ(xi). (4.31)\n47\nCHAPTER 4. FROM THEORY TO LEARNING ALGORITHMS\nThe parameters of the new model are given by:\n[ α b ] = P y− K∑ k=1 βkŷk\n0  , (4.32) where ŷk = [ŵkφ(x1), ŵ kφ(x2), ..., ŵ kφ(xN)]\nT is the vector of labels predicted using the k-th source model. As in the previous case, βk can be evaluated with live-one-out error, the only difference is that now the minimization problem (4.29) is with respect to a vector (β). The last case is the more general and it is an extension to the multiclass problem. It consists in the use of different weights for different classes of the same source. Indeed, in a problem of domain adaptation, it can be reasonable to think that a subject learns better a task from a source and another task from on different source. Starting from the previous cases, the new implementation is straightforward. Now the weight parameters are inside a matrix B = RK×G. K and G are respectively the number of sources and classes, thus βk,g is the weight associated to class g of source k. Given a sample xi we refer respectively to Yi, Ŷi and Ỹi as the column vector with real output (in one-vs-all approach), output predicted by prior and output predicted with live-one-out. The loss function used to evaluate the live-one-out prediction is reported in equation (4.2). The three different cases are shown in Figure 4.7.\n48\n4.5. DOMAIN ADAPTATION ALGORITHMS\nIn this work we use the case of different weights for different classes. During the test phase, for each test vector x, we choose the class g that gives the maximum output in according with:\nh(x) = argmax g\n( wgφ(x) + bg +\nK∑ k=1 Bk,gŶk,g\n) . (4.33)\nThe flow char of the code that implements Multi Adapt method is shown in Figure 4.8. For each subject and class, during the training phase, is solved the optimization problem with different weights for different classes in order to find the parameters of new models.\n49\nCHAPTER 4. FROM THEORY TO LEARNING ALGORITHMS\n4.5. DOMAIN ADAPTATION ALGORITHMS"
    }, {
      "heading" : "4.5.2 Multi Kernel Adaptive Learning algorithm",
      "text" : "There are several types of algorithms that exploit a kernel combination to obtain a better performance in test phase. The authors of [14], [15] present the Multi Kernel Adaptive Learning (MKAL) algorithm for a multiclass problem. This is described in detail also in [5]. The MKAL algorithm belongs to middle level of features integration, it is based on a linear combination of kernels. Kernels may be different for type, or these may be of the same type with different parameters. The weights of this linear combination reflect the importance of each kernel. In order to introduce this algorithm, we define its final prediction as:\nh(x) = argmax g K∑ k=1 βkg s k(x, y), (4.34)\nwhere the index g runs over classes and the index k over sources. βkg are the weights of the score function sk(x, y) defined as:\nsk(x, y) = wkφk(x, y) = wkgφ ′k g (x). (4.35)\nIn the previous equation wk = [wk1,w k 2, ...,w k G] is the hyperplane referred to source k. It is composed by G blocks, one for each class. The feature map φk(x, y) = [0, .., 0, φ\n′k g (x), 0, ..., 0] is represented as a vector with all the\nelements null except that for the g-th position, that corresponds to the output class of vector x. The kernel corresponding to the source k is defined as: Kk((x, y), (x ′ , y ′ )). Let us define with:\nw̄ = [w0,w1,w2, ...,wK ] and φ̄(x, y) = [φ0(x, y), φ1(x, y), φ2(x, y), ..., φK(x, y)] (4.36)\nrespectively the concatenated vectors of hyperplanes and feature maps. These are both composed by (K + 1) blocks: one for each source plus the block labelled with 0 that is referred to the original training vectors. Let as define also the (2,p) group norm ‖ w̄ ‖2,p of vector w̄ as:\n‖ w̄ ‖2,p=‖ [‖ w0 ‖2, ‖ w1 ‖2, ..., ‖ wK ‖2] ‖p, (4.37)\nthat is the p-norm of a vector with K + 1 elements, where each element is given by the 2-norm of the vector wk composed by G elements. The dual\n51\nCHAPTER 4. FROM THEORY TO LEARNING ALGORITHMS\nnorm of ‖ · ‖p is defined as ‖ · ‖q where p and q satisfy 1p + 1 q\n= 1. At this point we introduce the optimization problem in order to find the vector w̄:\nmin w̄\nλ 2 ‖ w̄ ‖22,p + 1 N N∑ i=1 ξi\nsubject to: w̄ · (φ̄(xi, yi)− φ̄(xi, y)) ≥ 1− ξi, ∀i, y 6= yi, (4.38)\nwhere λ = 1 CN\nis a regularization term and y is the predicted class of the vector xi. The previous equation can be written as:\nmin w̄\nλ 2 ‖ w̄ ‖22,p + 1 N N∑ i=1 l(w̄,xi, yi). (4.39)\nThe function that appears in the second term is the loss function for a multiclass problem. It is defined as:\nl(w,xi, yi) = max y 6=yi {0, 1− w̄ · (φ̄(xi, yi)− φ̄(xi, y)).} (4.40)\nThe element p that appears in the first term of equation (4.39) can vary in the range [1, 2]. By changing the value of p, the level of sparsity of the solution changes. With p = 1 we have only the sum of all the 2-norm of each hyperplane wk of each source. Thus a solution with few hyperplanes is favoured. The l1 norm introduces sparsity in the solution by reducing the complexity, but often it produces a non-convex problem. To select p = 2 is equivalent to choose an unweighted sum of kernels. To overcame these problems usually a 1 < p < 2 is chosen. Summarizing, the task of finding the vector w̄ that minimize equation (4.39) is equivalent to search for the best kernels combination that minimizes the loss function for a given set of training vectors. There are several algorithms that solve computationally the minimal problem (4.39). The one used in our implementation is called Obscure ([14], [15]). The described process is iterated for T times for all the training vectors. At each step the algorithm takes a random sample of the training set. The output of the training vector is evaluated using equation (4.34). At the first step all the parameters are null, in the others the parameters of previous iteration are taken. For each training vector the loss function is evaluated with equation (4.40). The minimization problem that is tackled if the loss function is greater than\n52\n4.5. DOMAIN ADAPTATION ALGORITHMS\nzero is slightly different to the one earlier presented. It is formulated as follows:\nw̄t+1 = argmin w̄ [h(w̄) + w̄ t∑ i=1 ηi∂l(w̄i,xi, yi)]. (4.41)\nThe loss function is replaced with its sub-gradient, in order to linearise the problem. The first term of the equation is the regularization term h(x) = λ\n2 ‖ w̄ ‖22,p. In the second term the ηi are a set of trade-off parameters\nthat balance the speed of convergence and the precision. The solution of above equation is:\nw̄t+1 = Oh ∗ ( −\nt∑ i=1 ηi∂l(w̄i,xi, yi)\n) , (4.42)\nwhere h∗ is the Fenchel conjugate of h. In the considered case the earlier equation is written as follows:\nwkt+1 = 1\nq ( ‖ θkt+1 ‖2 ‖ θ̄t+1 ‖2,q )q−2 θkt+1, ∀k (4.43)\nwhere θ̄ = (− t∑ i=1 ηi∂l(w̄i,xi, yi)) is the dual weight of w̄ and q is the dual coefficient of p. If we have a positive loss function the vector w̄ is updated, if the loss is null the vector doesn’t change. In order to exploit the kernel definition in final prediction (4.34) we can express as follow the vector w̄:\nwkt+1 ∝ θkt+1 = − t∑ i=1 ηi((φ k(xi, yi)− φk(xi, y))). (4.44)\nThe flow char of the code that implements Multi Kernel Adaptive Learning method is shown in Figure 4.9. For each subject, during the training phase, is solved the optimization problem (4.39) in order to find the parameters of the new models.\n53\nCHAPTER 4. FROM THEORY TO LEARNING ALGORITHMS\n54\n4.5. DOMAIN ADAPTATION ALGORITHMS"
    }, {
      "heading" : "4.5.3 High Level-Learning2Learn algorithm",
      "text" : "The algorithm High Level-Learning2Learn (H-L2L) belongs to the high level cue integration methods ([17] and [18]). This algorithm is composed by two layers. In the first one, it is calculated the confidence score on each source model and on a target model built with a multiclass LS-SVM. The vectors used to train the target model are a part of the original training vectors (63% for each class). The ones exploited in the calculation of confidence are all the test vectors and the remaining training vectors (37% for each class). Thus, given a vector x, for each output class g we have the score starget(x, g) and K different scores ssource(x, g) in according to:\ns(x, g) = wgφ(x) + bg. (4.45)\nThe vectors used to train the new model in the second layer are the concatenation of confidence scores of target and sources for the 37% of the original training vectors. These are exploited to solve a multiclass LS-SVM problem and the final score is given by:\ns(x, g) = w0φ(starget(x, g)) + K∑ k=1 wkφ(ssources(x, g)), (4.46)\nwhere the the index 0 refers to the target model. The concatenation of confidence scores of target and sources of the original test vectors are used to test the model of second layer. The flow chart of the code that implements High Level-Learning2Learn algorithm method is shown in Figure 4.10. In the two training phases two different LS-SVM problems are solved, the first one exploiting original vectors and the second one using the output scores obtained previously.\n55\nCHAPTER 4. FROM THEORY TO LEARNING ALGORITHMS\n56\nChapter 5\nResults\nIn this chapter we evaluate and compare the performance and the results obtained using the state of the art algorithms described in chapter 4 for data processed as explained in chapter 3. Our goal is to understand how and if, for a new target subject, prior knowledge coming from other subjects can boost the control of prosthesis and reduce the time needed to learn how to control the device. Furthermore, when we consider a new amputee that is learning how to control the prosthesis, it is interesting to investigate which sources he/she exploits. In particular our task is to understand how the learning curve for an user might change when using intact or amputees as sources.\nWe present in Section 5.1 the experimental protocol used for each experiment. Sections 5.2, 5.3 and 5.4 focus on results obtained in the three different experiments done. In the first we consider an intact subject that exploits prior knowledge from other intact subjects. In the second we have an amputated subject that uses prior knowledge from other amputated subjects. In the third we consider an amputated subject that exploits prior knowledge from intact subjects. We conclude with an overall discussion of our findings in Section 5.5."
    }, {
      "heading" : "5.1 Experimental protocol",
      "text" : "As said above, in this work we ran three different experiments. In this section we describe the experimental settings that are common to all of them. We consider a target subject that exploits prior knowledge of source subjects in order to learn how to perform hand postures. Each experiment is repeated\n57\nCHAPTER 5. RESULTS\nfor several targets: 20 in the first and 9 in the second and third. The considered source models are 19, 8 and 20 in, respectively, the first, second and third set-up. Each protocol will be explained in detail in the following sections. The goal that a target subjects aims to achieve is to learn to perform the same movements that source subjects are already able to do. For all the experiments we consider the postures of Exercise B, i.e. 8 hand configurations and 9 basic movements of the wrist (see section 3.1.3). Thus, we are solving for each target a classification problem with 18 classes: 17 movements and rest posture. Each experiment for each target subject has two phases: training and test. During the first, starting from few training vectors and prior models, each new target builds a classification model. During the second the model is tested with new vectors unused during the training phase.\nThe classification models of sources are built with a non-linear SVM with Gaussian kernel using the library LIBSVM available online (https: //www.csie.ntu.edu.tw/~cjlin/libsvm/). In all settings the target model is built with an increasing number of training vectors, up to a maximum of 2160. For each subject the experiment is repeated 18 times where at each step we increase the training vectors of 120. The algorithms used in the training phase of each experiment are the same and represent the state of the art in adaptive learning. These are described in section 4.5 and are: Multi Kernel Adaptive Learning (MKAL), High LevelLearning2Learn (H-L2L) and Multi Adapt. We consider as reference two baseline: No Transfer and Prior Features (see section 4.5). In the test phase, the final performance is evaluated using the formula of balanced accuracy that takes into account a possible imbalance in the number of vectors in different classes. We report the formulation for a binary problem:\n0.5TP\nTP + FN +\n0.5TN\nTN + FP . (5.1)\nWe refer to the positive or negative samples classified correctly with TP (True Positive) and TN (True Negative). With FP (False Positive) and FN (False Negative) we indicate the negative or positive samples misclassified. This formulation can be straightforwardly extended to multiclass case. An average performance is calculated for each algorithm, it comes from the mean between the performance obtained for all the target subjects. For each subject the performance is evaluated for each considered set of training vectors.\n58\n5.2. INTACT-INTACT\nThus, we evaluate the trend of mean performance as the number of training vectors increases.\nAll the classification models used are based on non-linear SVM with Gaussian kernel (see equation 4.17). These need the setting of the hyperparameters C and γ. The method generally used to determine them is called crossvalidation. A grid with the following values for parameters is taken into account: C = (0.01, 0.1, 1, 10, 100, 1000) and γ = (0.01, 0.1, 1, 10, 100, 1000). For each subject and for each possible combination of C and γ, a classification model is built using the library LIBSVM. The couple of parameters that gives the best result in performance is chosen.\nAll the obtained results for each experiment are available in https: //sites.google.com/site/noninvasiveprosthetichand/."
    }, {
      "heading" : "5.2 Intact-Intact",
      "text" : "Setup. The first experiment that we tackle is close to the experiments already performed in the literature ([9] and [8]). It involves 20 intact random subjects from the second sub-database of NinaPro (see Table in section 3.1.3). In this case we consider intact subjects as target and source. In particular, one by one, each of the 20 subjects is the new target problem. The remaining 19 intact subjects are considered as sources. The data taken into account are those of the Exercise B: 8 hand configurations and 9 basic movements of the wrist. During the training phase the classification models are built for each target subject and for each of the five algorithms. We repeat the process for an increasing number of training vectors with steps of 120 up to a maximum of 2160. In the test phase we evaluate the performance of each model. In this work we performed the same experiment for 10, 20 and 30 intact random subjects. Taking into account that the final performance slightly increases from 10 to 20 but doesn’t change from 20 to 30, in this section we report only the results and analysis for 20 subjects. The interested readers can find all results in https://sites.google.com/site/noninvasiveprosthetichand/. In the Tables 5.1 and 5.2 we report the principal characteristics of the first experiment.\n59\nCHAPTER 5. RESULTS\nResults: Recognition Rate. For all the algorithms, the trend of the mean performance as a function of the training vectors of the target problem is reported in Figure 5.1. It comes from the mean between performance obtained for all the target subjects.\nIn Figure 5.2 the best and worst cases are reported. These are respectively the subjects for which each algorithm gives the best and worst result in performance.\n60\n5.2. INTACT-INTACT\nMKAL and H-L2L achieve the best performance followed by Multi Adapt, Prior Features and No Transfer. The same order is preserved in the best and worst case, but a component of noise is present for the single subject and the trend appears not very smooth. As shown in Figure 5.1 H-L2L outperforms MKAL for more than 1000 training samples of about 3 % (p < 0.05). The difference between MKAL and Multi Adapt shows an average advantage in recognition rate of around 10 % (p < 0.05). Multi Adapt has an average gain of 2 % with respect to Prior Features (p < 0.05). No Transfer is the method that shows the lowest performance. Multi Adapt, MKAL and H-L2L outperform No Transfer with an average of about 12 %, 23 % and 22 % respectively. At the last step, i.e. 2160 training vectors, No Transfer achieves a performance in recognition of about 47 %. The adaptive methods reach before this goal: 57 % for MKAL at 240 training samples, 49 % for H-L2L at 360 and 47 % for Multi Adapt at 600. This result means that the use of prior knowledge allows us to reduce by one order of magnitude the training time. The adaptive methods achieve faster than No Transfer the asymptotic performance. In fact, passing from 600 training samples to 2160 the performance of No Transfer, Multi Adapt, MKAL and H-L2L increases of respectively: 16 %, 5 %, 5 % and 12 %.\nResults: Confusion Matrices. Let us introduce the confusion matrix. This matrix contains information about real labels and labels predicted by a classification model. Each row represents the predicted values associated to a class for each real label. Each column represents the prediction given for\n61\nCHAPTER 5. RESULTS\neach real class, so the cumulative is 1. The analysis of confusion matrices shows several aspects of recognition in a classification problem that the single performance analysis hides. We can check if a set of classes are better recognize than others or if there are differences in recognition of a single posture changing algorithm. Thus we obtain a statistic of recognition about single class, changing algorithm and number of training samples. We report the confusion matrices for No Transfer (Figure 5.3), Prior Features (Figure 5.4) and MKAL (Figure 5.5) for 120 (i.e. initial step), 1080 (i.e. middle step) and 2160 (i.e. final step) training vectors. The label 1 is associated to rest posture. We can find the confusion matrices for all algorithms in https://sites. google.com/site/noninvasiveprosthetichand/.\n62\n5.2. INTACT-INTACT\nAs we can evaluate from colorbar, the warm colors are associated to high probability and cool colors to low probability. The ideal case is represented\n63\nCHAPTER 5. RESULTS\nby red colour into the diagonal of matrix (i.e. the prediction labels are equal to the true ones) and blue colours outside (i.e. wrong prediction is equal to zero). In all three cases, augmenting the number of training vectors, the warm colours move towards the diagonal. It is interesting to analyse the case of No Transfer. It doesn’t exploit any type of source and predictions are based only on training vectors of the target used at each step. At first one the classes with higher predictions are 8, 10, 11 and 12. It means that the 120 training vectors of first step in majority belong to these classes. The situation at first step is different for Prior Features and MKAL, it means that the recognition task is helped by the presence of the source knowledge. With No Transfer, at the first step, each class has as output the predicted value of 8. With Prior Features all the classes, except 1 and 14, have the highest prediction associated to the true label. With MKAL for each true label the highest prediction is always on the diagonal. The increasing of training samples gives stability in all cases. At the last step there aren’t attractors, i.e. there are no rows with higher cumulative probability than others. For all methods the highest predictions are inside the diagonal except one class for No Transfer. In all cases we can note that the posture with lowest accuracy on the diagonal is the 1, i.e. the rest. We recall that, in data acquisition process, each movement is inter-spaced by rest. It makes this posture different than others and easily confused.\nAnother analysis can be done to evaluate if there is an adaptive method that recognizes a class better than others and if this statistic changes increasing the number of training vectors. It is based on the results obtained with confusion matrices. For each column (i.e. each real class) we consider the four classes with highest predictions. The details of this analysis are reported in the Appendix A.1. From this analysis we can conclude that, given a class, different methods, generally, misclassify it with the same wrong classes. The situation doesn’t change as we increase the number of training vectors. Thus, different algorithms have different mean performances but, paying our attention to a single class, the type of misclassification are the same.\n64\n5.3. AMPUTEES-AMPUTEES"
    }, {
      "heading" : "5.3 Amputees-Amputees",
      "text" : "Setup. The second experiment involves 9 amputated subjects from the third sub-database of NinaPro (see Table in section 3.1.3). In this set-up we consider amputees as target and sources. In particular, one by one, each of the 9 subjects is the new target problem. The remaining 8 amputated subjects are considered as sources. Thus, we have an amputee that learns to use the prosthetic hand with the help of prior knowledge from other amputees. The data taken into account are those of the Exercise B: 8 hand configurations and 9 basic movements of the wrist. In the training phase a model, using each of the five algorithms, is built for each target subject. The process is repeated for an increasing number of training vectors with steps of 120 up to a maximum of 2160. In the test phase we evaluate the performance of each model. The 9 subjects of this experiment are not chosen randomly. As explained in section 3.1, 2 of the 11 amputees had only 10 electrodes instead of 12 because of insufficient space in the stump. At the beginning we tested all the 11 subjects, but those with less electrodes had a lower performance than others. Thus, in the following we report only the results for 9 subjects; the results for 11 subjects are available in https://sites.google.com/site/ noninvasiveprosthetichand/. In Tables 5.3 and 5.4 we report the principal characteristics of second experiment.\nResults: Recognition Rate. The trend of the performance averaged over all the subjects as a function of the training vectors of the target problem\n65\nCHAPTER 5. RESULTS\nis reported in Figure 5.6 for all the algorithms. It comes from the mean between performance obtained for all the target subjects.\nIn Figure 5.7 the best and worst cases are reported. These are respectively the subjects for which each algorithm gives the best and worst result in performance.\nAsymptotically MKAL and H-L2L achieve the best performance followed by Multi Adapt and Prior Features with No Transfer. A similar order is\n66\n5.3. AMPUTEES-AMPUTEES\npreserved in the best and worst case but a component of noise is present for single subject and the trend appears not very smooth. The only difference is that Prior Features outperforms No Transfer of about 2 % in the worst case (p < 0.05). As shown in Figure 5.6 MKAL performs better than H-L2L with an average of 4 % until 1080 training vectors (p < 0.05), after these curves give the same result in performance. The difference between MKAL and Multi Adapt shows an average advantage in recognition rate of around 10 % for first curve (p < 0.05). Multi Adapt has an average gain of 6 % with respect to Prior Features (p < 0.05). No Transfer and Prior show the lowest performance and their difference is not statistically significant (p > 0.05). Multi Adapt, MKAL and H-L2L outperform No Transfer with an average of about 6 %, 16 % and 14 % respectively. At 2160 training samples, i.e. the last step, No Transfer achieves a performance of 35 %. The same performance is reached by Multi Adapt, MKAL and H-L2L at only 840, 240 and 480 training vectors, respectively. This result shows that the use of prior knowledge allows us to reduce by one order of magnitude the training time. The adaptive methods achieve faster than No Transfer the asymptotic performance. Passing from 600 training samples to 2160 the performance of No Transfer, Multi Adapt, MKAL and H-L2L increases of respectively: 9 %, 6 %, 5 % and 8 %.\nResults: Confusion Matrices. As done for intact subjects in previous section we can analyse the confusion matrices in order to evaluate the level of recognition and misclassification of a single class changing the number of training vectors and the algorithm. We report the confusion matrices for No Transfer (Figure 5.8), Prior Features (Figure 5.9) and MKAL (Figure 5.10) for 120 (i.e. initial step), 1080 (i.e. middle step) and 2160 (i.e. final step) training vectors. The label 1 is associated to rest posture. One can find the confusion matrices for all algorithms in https://sites. google.com/site/noninvasiveprosthetichand/.\n67\nCHAPTER 5. RESULTS\n68\n5.3. AMPUTEES-AMPUTEES\nAs explained previously the warm colors are associated to high probability and the cool colors to low probability. In ideal case the prediction labels are equal to the true ones with the maximum probability and, graphically, we have red cells on the diagonal and blue outside. In all the three reported cases, augmenting the number of training vectors, the warm colours move towards the diagonal. No Transfer makes prediction exploiting only the training vectors. At the first step there is an imbalance due to the training samples considered. From first image of Figure 5.8 it is evident that the majority of first 120 training vectors belong to classes 2, 3, 7 and 8. At the last step, with 2160 training vectors, this displacement is solved. Only the predictions associated to class 8 are higher than others out of the diagonal, but the highest prediction is always associated to the right classes except two cases. With Prior, using only 120 training vectors, the highest prediction is associated to classes 8 and 9. Passing to 1080 training vectors the highest prediction is inside the diagonal for all classes except three. At the last step only a class suffers of this problem. With MKAL only a class has the highest prediction outside the diagonal using only 120 training vectors, but the problem is solved at the next steps.\nAlso in this case we evaluate for each real class the first four classes with\n69\nCHAPTER 5. RESULTS\nhighest predictions. This analysis is done in order to understand if there is an adaptive method that recognizes a class better than others, or if the misclassification changes increasing the number of training vectors. The details of this analysis are reported in the Appendix A.2. The obtained results show that, generally, a class is always misclassified with the same wrong classes and it is independent from the number of training vectors and the adaptive algorithm used."
    }, {
      "heading" : "5.4 Amputees-Intact",
      "text" : "Setup. The third experiment involves 9 amputated subjects and 20 intact subjects from respectively the third and the second sub-database of NinaPro (see Table in section 3.1.3). Considering previous results we used the same amputated and intact subjects exploited in the first and second experiment. In this set-up we consider amputees as target and intact subjects as sources. In particular the sources are fixed and, one by one, we consider as target an amputated subject. Thus, we have an amputee that learns to use the prosthetic hand with the help from the prior knowledge of intact subjects. The data taken into account are those of the Exercise B: 8 hand configurations and 9 basic movements of the wrist. In the training phase we build a classification model with each of the five algorithms for each target subject. The process is repeated for an increasing number of training vectors with steps of 120 up to a maximum of 2160. In the test phase we evaluate the performance of each model. In the Tables 5.5 and 5.6 we report the principal characteristics of third experiment.\n70\n5.4. AMPUTEES-INTACT\nResults: Recognition Rate. The trend of performance averaged over all the subjects as a function of the training vectors of the target problem is reported in Figure 5.11 for all the algorithms. It comes from the mean between performance obtained for all the target subjects.\nIn Figure 5.12 the best and worst cases are reported. These are respectively the subjects for which each algorithm gives the best and worst result in performance.\n71\nCHAPTER 5. RESULTS\nMKAL achieves the best performance asymptotically and considering the whole trend. It is followed by H-L2L, Multi Adapt, Prior Features and No Transfer. A similar order is preserved in the best and worst case but a component of noise is present for single subject and the trend appears not very smooth. We can note that No Transfer and Prior Features perform in the same way and their differences are not statistically significant (p > 0.05). As shown in Figure 5.11 MKAL outperforms H-L2L with an average of 4 % (p < 0.05). The difference between H-L2L and Multi Adapt shows an average advantage in recognition rate of around 5 % (p < 0.05). Multi Adapt has an average gain of 8 % with respect to Prior Features (p < 0.05). Until 720 training vectors Prior Features outperforms No Transfer with an average of 6 %, after the two curves keep the same performance. Multi Adapt, MKAL and H-L2L outperform No Transfer with an average of about 11 %, 20 % and 15 % respectively. At 2160 training samples, i.e. the last step, No Transfer achieves a performance of 31 %. The same performance is reached by Multi Adapt, MKAL and H-L2L with only 480, 240 and 360 training vectors, respectively. Also in this case, the use of prior knowledge allows us to reduce by one order of magnitude the training time. The adaptive methods achieve faster than No Transfer the asymptotic performance. In fact, passing from 600 training samples to 2160 the performance of No Transfer, Multi Adapt, MKAL and H-L2L increases of respectively: 10 %, 7 %, 4 % and 7 %.\nResults: Confusion Matrices. As done in previous sections we evaluate the level of recognition and misclassification of all classes from analysis of confusion matrices for each algorithm changing the number of training vec-\n72\n5.4. AMPUTEES-INTACT\ntors. We report the confusion matrices for No Transfer (Figure 5.13), Prior Features (Figure 5.14) and MKAL (Figure 5.15) for 120 (i.e. initial step), 1080 (i.e. middle step) and 2160 (i.e. final step) training vectors. The label 1 is associated to rest posture. One can find the confusion matrices for all algorithms in https://sites. google.com/site/noninvasiveprosthetichand/.\n73\nCHAPTER 5. RESULTS\nIn the matrices the warm and cool colors represent respectively an high and low probability. In the case of a perfect prevision we have red cells on\n74\n5.5. DISCUSSION AND COMPARISON\ndiagonal and blue outside. In all three reported cases, augmenting the number of training vectors, the warm colours move towards the diagonal. No Transfer at first step presents an imbalance in predictions because the models are built with few training vectors. From first image of Figure 5.8 it is evident that the majority of first 120 training vectors belong to classes 3, 4, 5 and 16. With 120 training samples the predictions associated to classes 2 and 10 are higher than others out of the diagonal. In this case the highest prediction is always associated to right classes except three cases. With Prior Features the highest prediction is associated to classes 10, 11 and 18 at the first step. Passing to 1080 and after to 2160 training vectors the highest prediction is inside the diagonal for all classes except one. With MKAL only a class has the highest prediction outside the diagonal using only 120 training vectors and the problem is solved at next steps.\nAlso in this case we evaluate for each real class the first four classes with highest predictions. From this analysis we can evaluate if there is an adaptive method than in the recognition of a particular class is is better than others. The details of this analysis are reported in the Appendix A.3. As in previous cases, the obtained results show that, generally, a class is always misclassified with the same wrong classes and it is independent from the number of training vectors and the adaptive algorithm used."
    }, {
      "heading" : "5.5 Discussion and comparison",
      "text" : "Comparison: Recognition Rate. In this section we compare results obtained in different experiments. For the comparison in performance of all the algorithms in the three experiments we report in Figure 5.16 the asymptotic classification rate (i.e. 2160 training vectors) obtained averaging over all the subjects, in the best and in the worst case.\n75\nCHAPTER 5. RESULTS\nThe order in performance of the different algorithms is the same in the three experiments, thus the average behaviours are maintained. Only H-L2L presents some differences: it outperforms MKAL in the first case, instead in the second and third case the two algorithms reach the same final performance. The performance achieved for intact target (first experiment) is higher than the performance obtained for amputated target (second and third experiment). It means that an intact is able to adapt his domain on domain of other intact subjects successfully (MKAL and H-L2L reach a performance of respectively 63 % and 67 % at 2160 training samples). The same performance is not reached by amputees, in fact MKAL and H-L2L achieve an asymptote of about 50 %. It means that, with respect to intact, for an amputee is more difficult to extract useful informations from different domains. The most interesting point concerns the comparison between the results from the second and the third experiment. The obtained performance appears unchanged in the two cases for all the algorithms. It means that an amputee\n76\n5.5. DISCUSSION AND COMPARISON\nlearns in the same way from intact and amputated sources. It is a very important result, in fact as it is difficult to recruit amputees and to perform an experiment with them. Amputees are obviously outnumbered with respect to intact subjects and, for them, the proposed exercises are usually difficult and painful to perform.\nComparison: Correlation Matrix. We study now if there are movements learned with ease or difficulty by intact subjects and vice versa by amputees. In order to achieve this goal we consider the recognition percentage of each movement for a given number of training vectors. We normalize each recognition with respect to the maximum in order to compare results from different settings and algorithms without considering the absolute performance. For these values (one for each algorithm and experiment) the correlation matrix is calculated and reported in Figure 5.17.\nIn previous matrix the warm colors and cool colors represent respectively high and low correlation between algorithms. As Figure 5.17 shows, we obtain a block matrix. The two diagonal blocks contain respectively the correlations between intact and amputees. The block outside the diagonal holds\n77\nCHAPTER 5. RESULTS\nthe correlation between intact and amputees. The correlations between different methods of the same experiment is high. It means that there aren’t movements learned simply with a method and hardly with another. The correlation between second and third experiment is high, thus an amputee learns a movement with the same simplicity or difficulty from intact and amputated subjects. Instead the correlation between amputees and intact is low. It means that for amputees and intact simple and difficult movements are different.\n78\nConclusion\nIn this work we have dealt with the control of non-invasive myoelectric prosthesis in order to overcome the most common problems that an amputee tackles: fatigue and pain due to the long learning process and the discouragement due to the mismatch between desired and performed movements. In particular we have investigated the variation of the learning curve when a form of prior knowledge is used and if the type of knowledge (from amputees or intact) is significant with respect to the final learning performance for an amputee. Our approach has been computational. We worked with public data from database NinaPro to make the experiment reproducible and results statistically relevant (considering the large number of subjects in the database). The algorithms used for data representation, and to solve the movements classification problem represent, respectively, the state of the art in the field of features extraction and domain adaptation. We have done three experiments in order to answer the previous questions. In the first we have considered intact that exploit the knowledge from other intact subjects. In the second we have studied amputated subjects that use other amputees as prior. In the last experiment we looked at amputees as target and intact subjects as their sources.\nOur findings are very interesting and can contribute to improve this field of research. Our results show that the prior knowledge positively affects the trend of recognition curve. With respect to the case in which no kind of knowledge is transferred, with adaptive algorithms the asymptotic performance is reached before and with an higher value. The performance achieved without transfer considering the maximum number of training vectors (i.e. 2160) is typically reached by adaptive methods exploiting an order of magnitude less of training vectors. These results allow us to reduce the training time, speeding up the learning. The last two experiments have been done in order to answer the question about previous knowledge for an amputee target: our findings show that\n79\nCHAPTER 5. RESULTS\nthere is not any change in the trend and in the asymptotic performance of each algorithms when amputees or intact subjects are used as source. This means that an amputee, in order to boost the control learning of her prosthetic hand, can exploit the knowledge of intact subjects that are simpler to recruit than amputees. Furthermore, for an intact user to complete the experiment requires less time and effort with respect to an amputee, thus the source models are created with ease.\nFuture work. The asymptotic performance reached by amputees is not very high (50 % at the best). A possible direction for future works could be to try to improve this result focusing on deep learning theory that has had very interesting results in the field of machine learning.\nFrom analysis of confusion matrices we have seen that, given different algorithms and number of training vectors, a posture (i.e. a class) is, in most cases, misclassified with the same wrong classes. In order to solve the problem a possible direction for future research could be to insert a sort of classes selector that, a priori, cuts out on classification for some postures.\nLastly, we have made in this work the hypothesis that all source subjects had performed the same exercises and are therefore able to perform the same postures, both among them and with respect to the target subject. This in general will not be the case. Different people might require different functionalities from their prostheses. As of today, there are no domain adaptation algorithms able to deal with this scenario. Future work will focus on these research threads.\n80\nAllegati\n81"
    }, {
      "heading" : "Appendix A",
      "text" : "Histogram analysis\nThis analysis is done in order to evaluate if there is an adaptive method that recognizes a class better than others and if this statistic changes increasing the number of training vectors. For each real class we consider the four classes with highest predictions, in order to evaluate the differences in misclassification. Generally the fifth predicted class has a recognition lower than 5 %, for this reason it and the following classes are not taken into account. In the following we analyse the results obtained for each experiment. We can find the complete results in https://sites.google.com/site/noninvasiveprosthetichand/.\nA.1 Histogram: first experiment\nWe report the histogram of the first four classes predicted for each true label. Methods involved in this analysis are Multi Adapt (Figure A.1), MKAL (Figure A.2) and H-L2L (Figure A.3) for 120 (i.e. initial step), 1080 (i.e. middle step) and 2160 (i.e. final step) training vectors.\n83"
    }, {
      "heading" : "APPENDIX A. HISTOGRAM ANALYSIS",
      "text" : "84\nA.1. HISTOGRAM: FIRST EXPERIMENT\n85"
    }, {
      "heading" : "APPENDIX A. HISTOGRAM ANALYSIS",
      "text" : "We study the change in misclassification of each class when the number of training vectors increases. For each algorithm we calculate the percentage of classes that have at least the 3\n4 of predicted labels equal, changing the\nnumber of training vectors. The results are reported in Table A.2.\n86\nFor adaptive methods, increasing the number of training vectors, a posture is misclassified always with the same wrong classes. The same analysis can be done considering different algorithms with the same number of training vectors. Results of percentage are reported in Table A.2.\nFrom this analysis we can conclude that, given a class, different methods, generally, misclassify it with the same wrong classes. Thus the misclassification is due to the similarity between the classes and it is independent from\n87\nAPPENDIX A. HISTOGRAM ANALYSIS\nthe algorithm. We must remember that the problem that we tackle is the recognition and classification of 8 fingers movements and 9 wrist postures (see Figure 3.3). These movements are very similar, thus it is reasonable to expect an high degree of confusion between different postures.\nA.2 Histogram: second experiment\nWe report the histogram of the first four classes predicted for each true label. Methods involved in this analysis are Multi Adapt (Figure A.4), MKAL (Figure A.5) and H-L2L (Figure A.6) for 120 (i.e. initial step), 1080 (i.e. middle step) and 2160 (i.e. final step) training vectors.\n88\nA.2. HISTOGRAM: SECOND EXPERIMENT\n89"
    }, {
      "heading" : "APPENDIX A. HISTOGRAM ANALYSIS",
      "text" : "We study the change in misclassification of each class when the number of training vectors increases. For each algorithm we calculate the percentage of classes that have at least the 3\n4 of predicted labels equal, changing the\nnumber of training vectors. The results are reported in Table A.4.\n90\nFor adaptive methods, increasing the number of training vectors, a posture is misclassified always with the same wrong classes. Only Multi Adapt shows a low percentage passing from 120 to 1080 training vectors. Probably this model is not very reliable at first step, in fact from first image of Figure A.4 we can see that the bins with highest probability have not the label equal to the right one. The same analysis can be done considering different algorithms with the same number of training vectors. Results of percentage are reported in Table A.4.\n91"
    }, {
      "heading" : "APPENDIX A. HISTOGRAM ANALYSIS",
      "text" : "From values of previous Table we conclude that, given a class, different methods, generally, misclassify it with the same wrong classes. We remember that the problem that we tackle is the recognition and classification of 8 fingers movements and 9 wrist postures (see Figure 3.3). These movements are very similar, thus it is reasonable to expect an high degree of confusion between different postures.\nA.3 Histogram: third experiment\nWe report the histogram of the first four classes predicted for each true label. Methods involved in this analysis are Multi Adapt (Figure A.7), MKAL (Figure A.8) and H-L2L (Figure A.9) for 120 (i.e. initial step), 1080 (i.e. middle step) and 2160 (i.e. final step) training vectors.\n92\nA.3. HISTOGRAM: THIRD EXPERIMENT\n93"
    }, {
      "heading" : "APPENDIX A. HISTOGRAM ANALYSIS",
      "text" : "We study the change in misclassification of each class when the number of training vectors increases. As done in previous sections, for each algorithm we calculate the percentage of classes that have at least the 3\n4 of predicted labels\nequal, changing the number of training vectors. The results are reported in Table A.6.\n94\nFor adaptive methods, increasing the number of training vectors, a posture is misclassified always with the same wrong classes. It means that the increment of training samples helps in term of average recognition, but the classifier mistakes in the same way different postures. The same analysis can be done considering different algorithms with the same number of training vectors. Results of percentage are reported in Table A.6.\nFrom values of previous Table we conclude that, given a class, different\n95"
    }, {
      "heading" : "APPENDIX A. HISTOGRAM ANALYSIS",
      "text" : "methods, generally, misclassify it with the same wrong classes. As done in previously we underline that our classification problem involves very similar classes (8 finger movements and 9 wrist postures, see Figure 3.3), thus it is reasonable to expect an high degree of confusion between similar postures.\n96"
    } ],
    "references" : [ {
      "title" : "Foundations of Machine Learning",
      "author" : [ "Mehryar Mohri", "Afshin Rostamizadeh", "Ameet Talwalkar" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2012
    }, {
      "title" : "Understanding Machine Learning, From Theory to Algorithms",
      "author" : [ "Shai Shalev-Shwartz", "Shai Ben-David" ],
      "venue" : "Avenue of the Americas,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2014
    }, {
      "title" : "Learning to Learn by Exploiting Prior Knowledge",
      "author" : [ "Tatiana Tommasi" ],
      "venue" : "Lausanne, EPFL,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2012
    }, {
      "title" : "Open-ended Learning of Visual and Multi-modal Patterns",
      "author" : [ "Jie Luo" ],
      "venue" : "Lausanne, EPFL,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2011
    }, {
      "title" : "Weighted least squares support vector machines: robustness and sparse approximation",
      "author" : [ "J.A.K. Suykens", "J. De Brabanter", "L. Lukas", "J. Vandewalle" ],
      "venue" : "Neurocomputing, Vol",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2002
    }, {
      "title" : "A Tutorial on Support Vector Machine for Pattern Recognition”, Data Mining and Knowledge Discovery",
      "author" : [ "Christopher J.C. Burgers" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1998
    }, {
      "title" : "Improving control of dexterous hand prostheses using adaptive learning",
      "author" : [ "Tatiana Tommasi", "Francesco Orabona", "Claudio Castellini", "B. Caputo" ],
      "venue" : "IEEE, Transactions on Robotics,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2013
    }, {
      "title" : "Model adaptation with least-squares SVM for hand prosthetics",
      "author" : [ "F. Orabona", "C. Castellini", "B. Caputo", "E. Fiorilla", "G. Sandini" ],
      "venue" : "Proceedings of ICRA - International Conference on Robotics and Automation",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Adapting visual category models to new domains",
      "author" : [ "K. Saenko", "B. Kulis", "M. Fritz", "T. Darrell" ],
      "venue" : "European Conference on Computer Vision (ECCV), pp. 213–226",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A theory of learning from different domains",
      "author" : [ "S.B. David", "J. Blitzer", "K. Crammer", "A. Kulesza", "F. Pereira", "J.W. Vaughan" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2010
    }, {
      "title" : "A Two-Stage Weighting Framework for Multi-Source Domain Adaptation",
      "author" : [ "Qian Sun", "Rita Chattopadhyay", "Sethuraman Panchanathan", "Jieping Ye" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2011
    }, {
      "title" : "Multi Kernel Learning with Online-Batch Optimization",
      "author" : [ "Francesco Orabona", "Luo Jie", "Barbara Caputo" ],
      "venue" : "Journal of Machine Learning Research",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2012
    }, {
      "title" : "Online-Batch Strongly Convex Multi Kernel Learning",
      "author" : [ "Francesco Orabona", "Luo Jie", "Barbara Caputo" ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2010
    }, {
      "title" : "Multiclass Transfer Learning from Unconstrained Priors",
      "author" : [ "Luo Jie", "Tatiana Tommasi", "Barbara Caputo" ],
      "venue" : "IEEE International Conference on Computer Vision (ICCV),",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2011
    }, {
      "title" : "Discriminative cue integration for medical image annotation",
      "author" : [ "Tatiana Tommasi", "Francesco Orabona", "Barbara Caputo" ],
      "venue" : "Pattern Recognition Letters,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2008
    }, {
      "title" : "Learning to Learn, from Transfer Learning to Domain Adaptation: A Unifying Perspective",
      "author" : [ "Novi Patricia", "Barbara Caputo" ],
      "venue" : "IEEE, Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2014
    }, {
      "title" : "Building the NINAPRO Database: A Resource for the Biorobotics Community",
      "author" : [ "Manfredo Atzori", "Arjan Gijsberts", "Simone Heynen", "Anne-Gabrielle Mittaz Hager", "Olivier Deriaz", "Patrick van der Smagt", "Claudio Castellini", "Barbara Caputo", "Henning Müller" ],
      "venue" : "IEEE RAS & EMBS International Conference on Biomedical Robotics and Biomechatronics (BioRob),",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2012
    }, {
      "title" : "A Benchmark Database for Myoelectric Movement Classification",
      "author" : [ "Manfredo Atzori", "Arjan Gijsberts", "Ilja Kuzborskij", "Simone Heynen", "Anne-Gabrielle Mittaz Hager", "Olivier Deriaz", "Claudio Castellini", "Henning Müller", "Barbara Caputo" ],
      "venue" : "Transactions on Neural Systems & Rehabilitation Engineering,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2013
    }, {
      "title" : "The Movement Error Rate for Evaluation of Machine Learning Methods for sEMG-based Hand Movement Classification",
      "author" : [ "Arjan Gijsberts", "Manfredo Atzori", "Claudio Castellini", "Henning Müller", "Barbara Caputo" ],
      "venue" : "IEEE Trans. neural Syst. Rehabil. Eng",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2014
    }, {
      "title" : "On the Challenge of Classifying 52 Hand Movements from Surface Electromyography",
      "author" : [ "Ilja Kuzborskij", "Arjan Gijsberts", "Barbara Caputo" ],
      "venue" : "34th Annu. Conf. IEEE Eng. Med. Biol. Soc.,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2012
    }, {
      "title" : "Control of multifunctional prosthetic hands by processing the electromyographic signal",
      "author" : [ "M. Zecca", "S. Micera", "M. Carrozza", "P. Dario" ],
      "venue" : "Critical Reviews in Biomedical Engineering, vol. 30, no. 4-6, p. 459",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Surface EMG in advanced hand prosthetics",
      "author" : [ "Claudio Castellini", "Patrick van der Smagt" ],
      "venue" : "Biol Cybern,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2009
    }, {
      "title" : "The use of surface electromyography in biomechanics",
      "author" : [ "C. De Luca" ],
      "venue" : "Journal of applied biomechanics, vol. 13, pp. 135–163",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "The extraction of neural strategies from the surface EMG",
      "author" : [ "D. Farina", "R. Merletti", "R.M. Enoka" ],
      "venue" : "Journal of Applied Physiology, vol. 96, no. 4, pp. 1486–1495",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Geodesic Flow Kernel for Unsupervised Domain Adaptation",
      "author" : [ "B. Gong", "Y. Shi", "F. Sha", "K. Grauman" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition ",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "Source: [8].",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 6,
      "context" : "1 Contributions of this thesis This thesis tackles the control of prosthesis with an approach based on machine learning, as previous works in this field suggest ([8], [9], [10]).",
      "startOffset" : 162,
      "endOffset" : 165
    }, {
      "referenceID" : 7,
      "context" : "1 Contributions of this thesis This thesis tackles the control of prosthesis with an approach based on machine learning, as previous works in this field suggest ([8], [9], [10]).",
      "startOffset" : 172,
      "endOffset" : 176
    }, {
      "referenceID" : 2,
      "context" : "their previous experience ([4]).",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 6,
      "context" : "In bio-robotics and rehabilitation community it is clear that the success of myoelectric prosthesis is linked to the creation of an accurate control system to make them easy to use by the patient (see [8] and references therein).",
      "startOffset" : 201,
      "endOffset" : 204
    }, {
      "referenceID" : 6,
      "context" : "The path generally chosen to tackle the problem is machine learning ([8], [9], [10] and references therein).",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 7,
      "context" : "The path generally chosen to tackle the problem is machine learning ([8], [9], [10] and references therein).",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 6,
      "context" : "It makes it possible to analyse the electromyography signal with modern statistical techniques like support vector machines, neural networks and linear discriminant, in order to guess the movement that the subject wants to perform ([8]).",
      "startOffset" : 232,
      "endOffset" : 235
    }, {
      "referenceID" : 21,
      "context" : "The EMG signal recorded from different subjects can vary due to characteristics of the forearm (like size and shape), personal characteristics of subjects (like gender, age, use of the arm), electrode displacement and muscular fatigue ([25]).",
      "startOffset" : 236,
      "endOffset" : 240
    }, {
      "referenceID" : 16,
      "context" : "In [19], [20], [21] all the details about used devices, the acquisition protocol and any information on database are present.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 17,
      "context" : "In [19], [20], [21] all the details about used devices, the acquisition protocol and any information on database are present.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 20,
      "context" : "In [24] the interested reader can find all the details about feature extraction methods used in order to analyse sEMG signals.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 19,
      "context" : "Authors of [23] studied and compared different methods in order to understand which are the ones that give the best ratio between the final performance and computational cost.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 10,
      "context" : "In [13] a process based on two stage of weighting for each source sample is used in order to overcome the distribution mismatch.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 24,
      "context" : "The authors of [28] take into account the fact that source and target live in different space; in order to overcome this problem it is exploited a function that builds a path from source to target in a dimensional reduced space.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 7,
      "context" : "The first algorithm proposed in this direction is presented in [10], in this case only a single source model is exploited (the best one).",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 6,
      "context" : "In [8] this approach is enlarged by exploiting different linear combinations of source models.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 11,
      "context" : "A middle level method based on this theory is proposed in [14] and [15].",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 12,
      "context" : "A middle level method based on this theory is proposed in [14] and [15].",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 15,
      "context" : "An high level method is instead exploited in [18].",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 9,
      "context" : "The traditional fields in which domain adaptation is applied are: language processing for speech recognition ([12]) and computer vision for image classification ([11], [18]).",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 8,
      "context" : "The traditional fields in which domain adaptation is applied are: language processing for speech recognition ([12]) and computer vision for image classification ([11], [18]).",
      "startOffset" : 162,
      "endOffset" : 166
    }, {
      "referenceID" : 15,
      "context" : "The traditional fields in which domain adaptation is applied are: language processing for speech recognition ([12]) and computer vision for image classification ([11], [18]).",
      "startOffset" : 168,
      "endOffset" : 172
    }, {
      "referenceID" : 21,
      "context" : "EMG signal from sources and target can differ for the user’s age, gender, height, weight, dominant hand, different exercise of the arm muscle and placement of electrodes ([25]).",
      "startOffset" : 171,
      "endOffset" : 175
    }, {
      "referenceID" : 7,
      "context" : "One of the first work, according to our knowledge, proposed in this environment is [10].",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 6,
      "context" : "Previous method is resumed and revisited in [8], where two adaptive algorithms able to exploit many prior knowledge models are proposed and compared with the previous one.",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 6,
      "context" : "In [8] these algorithms are tested on sEMG data from 10 and 20 intact subjects with 6 postures.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 10,
      "context" : "The adaptive methods that are compared come from [13], [28], [14] and [8] (see previous paragraph).",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 24,
      "context" : "The adaptive methods that are compared come from [13], [28], [14] and [8] (see previous paragraph).",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 11,
      "context" : "The adaptive methods that are compared come from [13], [28], [14] and [8] (see previous paragraph).",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 6,
      "context" : "The adaptive methods that are compared come from [13], [28], [14] and [8] (see previous paragraph).",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 10,
      "context" : "The results show that the method proposed by authors of [13] need a running time much longer than the others.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 11,
      "context" : "Generally the method that achieve the best performance is the one proposed by authors of [14].",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 6,
      "context" : "In both cases ([8],[9]) experiments showed that for intact subjects the postures recognition and classification can be improved and boosted by the use of prior knowledge of other intact subjects.",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 16,
      "context" : "All the information about data acquirement and database can be found in [19],[20] and [21].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 17,
      "context" : "All the information about data acquirement and database can be found in [19],[20] and [21].",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 18,
      "context" : "Source: [22].",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 19,
      "context" : "DATA DESCRIPTION: THE NINAPRO DATABASE interference using a Hampel filter ([23]).",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 18,
      "context" : "For a complete reading about this approach we refer to [22] and [23].",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 19,
      "context" : "For a complete reading about this approach we refer to [22] and [23].",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 19,
      "context" : "The windows length used in the literature is of 100 ms, 200 ms and 400 ms, the shift considered is of 10 ms ([23]).",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 19,
      "context" : "The experiments of [23] show that a window length of 200 ms or 400 ms results in higher accuracy.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 19,
      "context" : "DATA REPRESENTATION: FEATURES EXTRACTION Square (RMS) amplitude of sEMG signal and force exerted by a muscle ([23], [26]).",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 22,
      "context" : "DATA REPRESENTATION: FEATURES EXTRACTION Square (RMS) amplitude of sEMG signal and force exerted by a muscle ([23], [26]).",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 19,
      "context" : "Second, sEMG spectral characteristics might be related to conduction velocity of muscle fibers ([23], [27]).",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 23,
      "context" : "Second, sEMG spectral characteristics might be related to conduction velocity of muscle fibers ([23], [27]).",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 19,
      "context" : "Source: [23].",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 19,
      "context" : "The authors of [23] compared the previous different algorithms for features extraction.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 0,
      "context" : "For a complete reading about topics, one can refer to [2], [3] and [7].",
      "startOffset" : 54,
      "endOffset" : 57
    }, {
      "referenceID" : 1,
      "context" : "For a complete reading about topics, one can refer to [2], [3] and [7].",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 5,
      "context" : "For a complete reading about topics, one can refer to [2], [3] and [7].",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 0,
      "context" : "Source: [2].",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 0,
      "context" : "Source: [2].",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 0,
      "context" : "Source: [2].",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 4,
      "context" : "The LS-SVM ([6]) problem is a new formulation of equation (4.",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 2,
      "context" : "However in many real problems this assumption is not true: this happens because data can be dependent on dynamic factors like time, acquisition devices and space ([4]).",
      "startOffset" : 163,
      "endOffset" : 166
    }, {
      "referenceID" : 2,
      "context" : "Thus, the conditional probability distribution P (y|x) might be slightly different for source and target ([4]).",
      "startOffset" : 106,
      "endOffset" : 109
    }, {
      "referenceID" : 3,
      "context" : "The different ways of integration explained above are called cue integration methods ([5] and [17]) and are summarised in Figure 4.",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 14,
      "context" : "The different ways of integration explained above are called cue integration methods ([5] and [17]) and are summarised in Figure 4.",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 14,
      "context" : "Source: [17].",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 7,
      "context" : "1 Multi Adapt algorithm The first form of Multi Adapt algorithm was proposed in [10] and after extended in [8].",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 6,
      "context" : "1 Multi Adapt algorithm The first form of Multi Adapt algorithm was proposed in [10] and after extended in [8].",
      "startOffset" : 107,
      "endOffset" : 110
    }, {
      "referenceID" : 2,
      "context" : "Source: [4].",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 11,
      "context" : "The authors of [14], [15] present the Multi Kernel Adaptive Learning (MKAL) algorithm for a multiclass problem.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 12,
      "context" : "The authors of [14], [15] present the Multi Kernel Adaptive Learning (MKAL) algorithm for a multiclass problem.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 3,
      "context" : "This is described in detail also in [5].",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 0,
      "context" : "39) can vary in the range [1, 2].",
      "startOffset" : 26,
      "endOffset" : 32
    }, {
      "referenceID" : 11,
      "context" : "The one used in our implementation is called Obscure ([14], [15]).",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 12,
      "context" : "The one used in our implementation is called Obscure ([14], [15]).",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 14,
      "context" : "3 High Level-Learning2Learn algorithm The algorithm High Level-Learning2Learn (H-L2L) belongs to the high level cue integration methods ([17] and [18]).",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 15,
      "context" : "3 High Level-Learning2Learn algorithm The algorithm High Level-Learning2Learn (H-L2L) belongs to the high level cue integration methods ([17] and [18]).",
      "startOffset" : 146,
      "endOffset" : 150
    }, {
      "referenceID" : 6,
      "context" : "The first experiment that we tackle is close to the experiments already performed in the literature ([9] and [8]).",
      "startOffset" : 109,
      "endOffset" : 112
    } ],
    "year" : 2017,
    "abstractText" : "5",
    "creator" : "LaTeX with hyperref package"
  }
}