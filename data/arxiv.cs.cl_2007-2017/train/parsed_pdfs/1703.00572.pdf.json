{
  "name" : "1703.00572.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Structural Embedding of Syntactic Trees for Machine Comprehension",
    "authors" : [ "Rui Liu", "Junjie Hu", "Wei Wei", "Zi Yang", "Eric Nyberg" ],
    "emails" : [ "ehn}@cs.cmu.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Machine comprehension on question answering is a problem that requires machines to generate answers by locating a phase inside a context paragraph [14, 17], which is especially challenging since the algorithm has to come up with an answer that matches exactly with what human would do.\nMany works have been proposed to leverage deep neural networks for the machine comprehension task, most of which involves learning the query-aware context representation in order to locate the answer within the context. [4, 15, 19, 21, 22]. However, none of them take syntactic information of the sentences into consideration. Here we are particularly interested in two types of syntactic information, Constituency Tree and Dependency Tree, which have been reported to be some of the most important features for modeling question answering problem using a logistic regression model [14]. We adopt similar ideas but apply them to a neural attention model for question answering.\n∗Authors contributed equally to this work.\nar X\niv :1\n70 3.\n00 57\n2v 1\n[ cs\n.C L\n] 2\nM ar\nIn a constituency tree [9], a sentence is decomposed to a tree structure diagram with internal nodes representing phrase structure grammars and terminal nodes representing the actual words presented. Figure 1 illustrates the constituency tree of the sentence “the architect or engineer acts as the project coordinator”. Here, “the architect or engineer” and “the project coordinator” are labeled as noun phrase, or “NP”. This information is critical for questions such as the following1\nWhose role is to design the works, prepare the specifications and produce construction drawings, administer the contract, tender the works, and manage the works from inception to completion?\nwhich asks for the name of certain occupation that can be best answered using an NP. Constituency relations can reduce the size of the candidate space and help identify the actual answer.\nOn the other hand, a dependency tree [9] is constructed based on the dependency structure of a sentence. Figure 2 displays the dependency tree for sentence\n1SQuAD QID: 572753335951b619008f8854, https://rajpurkar.github.io/SQuAD-explorer/\nThe Annual Conference, roughly the equivalent of a diocese in the Anglican Communion and the Roman Catholic Church or a synod in some Lutheran denominations such as the Evangelical Lutheran Church in America, is the basic unit of organization within the UMC.\nThe link here annotating “The Annual Conference” being the subject of “the basic unit of organization within the UMC” provided a critical clue for the learning algorithm to skip over a large portion of the text when answering the question “What is the basic unit of organization within the UMC”. As we show in the analysis section, adding dependency information dramatically helped the models identify dependency structures within the sentence, which is otherwise fairly difficult to learn.\nIn this paper, we propose Structural Embedding of Syntactic Trees (SEST) models that embed syntactic information structured by constituency tree and dependency tree into neural attention models for the question answering task. Our analysis shows that syntactic information improved the quality of the answers compared to the ones that are not aware of such structures. When multiple candidates exist for a single question, syntactic information helped the algorithm to choose the ones that are both succinct and grammatically coherent, which is natural to how human would answer questions. We performed experiments on a large scale question answer data set and our approaches outperform baseline method by a significant amount."
    }, {
      "heading" : "2 Methodology",
      "text" : "The general framework of our model is illustrated in Figure 3. Here the input of the model is the embedding of the context and question while the output are two indices begin and end which indicates the begin and end indices of the answer in the context space.\nThe input of the model contains two parts: the word/character model and the syntactic model. The shaded portion of our model in Figure 3 represents the encoded syntactic information of both context and question that are feeded into the model. To gain an insight of how the encoding works, consider a sentence which syntactic tree consists of four nodes (o1, o2, o3, o4). A specific word is represented to be a sequence of nodes from its leave all the way to the root. We cover how this process work in detail in Section 3.1.1 and 3.1.2. Another input that will be feeded into deep learning model is the embedding information for words and characters respectively. There are many ways to convert words in a sentence into a high dimensional embedding. We choose GloVe [13] to obtain a pre-tained and fixed vector for each word. There are other alternatives exist such as word2vec [11]. Instead of using a fixed embedding, we use Convolutional Neural Networks (CNN) to model character level embedding, which values can be changed during training [6]. To integrate both embeddings into the deep neural model, we feed the concatenation of them for the question and the context to be the input of the model,\nnamely [xw;xs]. Here xw and xs represent the word/character embedding and the syntactic embedding respectively.\nThe inputs are processed in the embedding layer to form more abstraction representation. Similar to other parts of the model, the exact techniques remain high flexible. Here we choose a multi-layer bi-directional Long Short Term Memory (LSTM) [5] to obtain more abstract representations for words in the contexts and questions.\nAfter we obtain the representations of words in the contexts and questions, we employ an attention layer to fuse information from both the contexts and the questions. Various matching mechanisms using attentions have been extensively studied for machine comprehension tasks. Examples are Dynamic Coattention Network [21], Bi-directional Attention flow [15], Multi-Perspective Context Matching [20] and match-LSTM [19]. In this paper, we use Bi-directional Attention flow. We note that our proposed structural embedding of syntactic trees can be easily applied to any attention approaches mentioned above.\nFinally, in the output layer we use two softmax layers to generate predictions of begin index and end indices to form an answer by extracting a phrase from the context."
    }, {
      "heading" : "3 Structural Embedding of Syntactic Tree",
      "text" : "We detail the procedures to generate embeddings from syntactic information. There are two models we consider in this section, e.g., the Structural Embedding of Constituency Trees model (SECT) and the Structural Embedding of Dependency Trees model (SEDT). We assume that the syntactic information have already been generated in the preprocessing step using tools such as the Stanford CoreNLP [10] and this section only concerns with the problem of converting syntactic information into embeddings that can be used in the deep neural models."
    }, {
      "heading" : "3.1 Syntactic Sequence Extraction",
      "text" : "We first extract a syntactic collection C(p) for each word p, which consists of a set of nodes {o1, o2, . . . , od−1, od} in the syntactic parse tree T . Each node oi can be a word, a grammatical category (e.g., part-of-speech tagging), or a dependency link label, depending on the type of syntactic tree we use. To construct syntactic embeddings, the first thing we need to do is to define a specific processing order A over the syntactic collection C(p), in which way we can extract a syntactic sequence S(p) for the word p."
    }, {
      "heading" : "3.1.1 Structural Embedding of Constituency Trees (SECT)",
      "text" : "The constituency tree is a syntactic parse tree constructed by phrase structure grammars [9], which defines the way to hierarchically construct a sentence from words in a bottom-up manner based on constituency relations. Words in the contexts or the questions are represented by leaf nodes in the constituency tree while the non-terminal nodes are labeled by categories of the grammar. Non-terminal node summarizes the grammatical function of the sub-tree. Figure 1 shows an example of the constituency tree with “the architect or engineer” being annotated as a noun phrase (NP).\nA path originating from the leaf node to the root node captures the syntactic information in the constituency tree in a hierarchical way. The higher the node is, the longer span of words the sub-tree of this node covers. Hence, to extract the syntactic sequence S(p) for a leaf node p, it is reasonable to define the processing order A(p) from the leaf p all the way to its root. For example, the syntactic sequence for the word “acts” in Figure 1 is detected as (VBZ, VP, S). In practice, we choose a window size to limit the amount of information that is used in our models. For example, if we choose the window size as 2, then the syntactic sequence becomes (VBZ, VP). This process is introduced for both performance and memory utilization consideration, which is discussed in detail in Section 4.5.\nIn addition, a non-terminal node at a particular position in the syntactic sequence defines the begin and end indices of a phrase in the context. By measuring the similarity between syntactic sequences S(p) extracted for each word p of both\nquestion and context, we are able to locate the boundaries of the answer span. This is done in the attention layer shown in Figure 3."
    }, {
      "heading" : "3.1.2 Structural Embedding of Dependency Trees (SEDT)",
      "text" : "The dependency tree is a syntactic tree constructed by dependency grammars [9], which defines the way to connect words by directed links that represent dependencies. A dependency link is able to capture both long and short distance dependencies of words. Relations on links vary in their functions and are labeled with different categories. For example, in the dependency tree plotted in Figure 2, the link from “unit” to “Conference” indicates that the target node is a nominal subject (i.e. NSUBJ) of the source node.\nThe syntactic collection C(p) for dependency tree is defined as p’s children, each represented by its word embedding concatenated with a vector that uniquely identifies the dependency label. The processing order A(p) for dependency tree is then defined to be the dependent’s original order in the sentence. Similar to SECT, we use a window of size l to limit the amount of syntactic information for the learning models by choosing only the l-nearest dependents, which is again reported in Section 4.5."
    }, {
      "heading" : "3.2 Syntactic Sequence Encoding",
      "text" : "Similar to previous work [1, 7], we use a neural network to encode a variable-length syntactic sequence into a fixed-length vector representation. The encoder can be a Recurrent Neural Network (RNN) or a Convolutional Neural Network (CNN) that learns a structural embedding for each node such that embedding of nodes under similar syntactic trees are close in their embedding space.\nWe can use a Bi-directional LSTM as our RNN encoder, where the hidden state hpt is updated according to Eq. 1. Here x p t is the t\nth node in the syntactic sequence of word p, which is a vector that uniquely identifies each syntactic node. We obtain the structural embedding of the given word p, upBi-LSTM = h p T to be the final hidden state of the Bi-directional LSTM at time T .\nhpt = Bi-LSTM(h p t−1,x p t ) (1)\nAlternatively, we can also use CNN to obtain embeddings from a sequence of syntactic nodes. We denote l as the length of the filter of the CNN encoder. We define xpi:i+l as the concatenation of the vectors from x p i to x p i+l−1 within the filter. The ith element in the jth feature map can be obtained in Eq. 2. Finally we obtain the structural embedding of the given word p by upCNN in Eq. 3.\ncpi,j = f(wj · xi:i+l−1 + bj) (2) upCNN = maxrow(c p) (3)\nwhere wj and bj are the weight and bias of the jth filter respectively, f is a nonlinear activation function and maxrow(·) takes the maximum value along rows in a matrix."
    }, {
      "heading" : "4 Experiments",
      "text" : "We conducted systematic experiments on the SQuAD dataset [14], a massive question answer dataset to evaluate the performance of our proposed SEST methods. We compared our methods against Bi-Directional Attention Flow (BiDAF), a stateof-the-art model for machine comprehension as well as the SEST models described in Section 3. Our analyses show that SEST methods outperform baseline methods on predictive performance on both development and testing sets of SQuAD. Detailed description of the SQuAD dataset can be found in [14]."
    }, {
      "heading" : "4.1 Preprocessing",
      "text" : "A couple of preprocessing steps is in place to ensure that the deep neural models get the correct input. We segmented context and questions into sentences by using NLTK’s Punkt sentence segmenter2. Words in the sentences were then converted into symbols by using PTB Tokenizer3. Syntactic information including POS tags and syntactic trees were acquired by Stanford CoreNLP utilities [10]. For the parser, we collected constituent relations and dependency relations for each word by using tree annotation and enhanced dependencies annotation respectively. To generate syntactic sequence, we removed sequences whose first node is a punctuation (“$”, “:”, “#”, “.”, “ ” ”, “ “ ”, “,”). To use dependency labels, we removed all the subcategories (e.g., “nmod:poss”⇒ “nmod”)."
    }, {
      "heading" : "4.2 Experiment Setting",
      "text" : "We experimented our models on a machine that contains a single GTX 1080 GPU with 8GB VRAM. We use both SECT and SEDT models to compare against the baseline models, which are BiDAF model [15] and the POS model introduced in Section 3.1.1. All of the four models have the same settings on character embedding and word embedding. As introduced in Section 2, we use a variable character embedding along with a fixed pre-trained word embedding to serve as part of the input into the model. The character embedding is implemented using CNN with a one-dimensional layer consists of 100 units with a channel size of 5. It has an input depth of 8. The max length of the SQuAD is 16 which means there are a maximum 16 words in a sentence. The fixed word embedding has a dimension of 100, which is provided by the GloVe data set [12]. The settings for syntactic embedding are slightly different for each model. The BiDAF model does not deal\n2http://www.nltk.org/api/nltk.tokenize.html 3http://nlp.stanford.edu/software/tokenizer.shtml\nwith syntactic information. The POS model contains syntactic information with 39 different POS tags that serve as both input and output. For SECT and SEDT the input of the model has a size of 8 with 30 units to be output. Both of them has a maximum length size that is set to be 10 and 20 respectively, which values will be further discussed in Section 4.5. They also have two different ways to encode the syntactic information as indicated in Section 3: LSTM and CNN. We apply the same sets of parameters when we experiment them with the two models."
    }, {
      "heading" : "4.3 Predictive Performance",
      "text" : "We first compared the performance of single models between the baseline approach BiDAF and the proposed SEST approaches, including SEPOS, SECT-LSTM, SECTCNN, SEDT-LSTM, and SEDT-CNN, on the development dataset of SQuAD. For each model, we conducted 5 different single experiments and evaluated them using two metrics: “Exact match” (EM), which calculates the ratio of questions that are answered correctly by strict string comparison, and the F1 score, which calculates the harmonic mean of the precision and recall between predicted answers and ground true answers at the character level. As shown in Table 1, we reported the maximum, the mean, and the standard deviation of EM and F1 scores across all single runs for each approach, and highlighted the best model using bold font. SEDT-LSTM is the second best method, which confirms the predictive powers of different types of syntactic information. We could see that SECT-LSTM model outperforms the baseline method and other proposed methods in terms of both EM and F1. Another observation is that our propose models achieve higher relative improvements in EM scores than F1 scores over the baseline methods, providing the evidence that syntactic information can accurately locate the boundaries of the answer.\nMoreover, we found that both SECT-LSTM and SEDT-LSTM have better performance than their CNN counterparts, which suggests that LSTM can more effectively preserve the syntactic information. As a result, we conduct further analysis of only SECT-LSTM and SEDT-LSTM models in the subsequent subsections and drop the suffix “-LSTM” for abbreviation. We built an ensemble model from the 5 single models for the baseline method BiDAF and our proposed methods SEPOS, SECT-LSTM, and SEDT-LSTM. We compared these models on both the development set and official test set and reported the results in Table 2. Although underperformed to SECT in the single model comparison, SEDT outperformed all the other models in the ensemble setting. We believe that it can be attributed to (1) that a relatively low variance of SEDT-LSTM single models made its learners consistently strong, and (2) that the dependency tree provided hints for long distance inference, which is illustrated in Figure 2."
    }, {
      "heading" : "4.4 Contribution of Syntactic Sequence",
      "text" : "To take a closer look at how syntactic sequences affect the performance, we removed the character/word embedding from our model seen in Figure 3 and conducted experiments based on the syntactic input along. In particular, we are interested in two aspects related to syntactic sequences: First, the ability to predict answers of questions of syntactic sequences compared to complete random sequences. Second, the amount of impacts brought by our proposed ordering introduced in Section 3.1.1 and Section 3.1.2 compared to random ordering.\nWe compared the performance of the models using syntactic information along in their original order (i.e. SECT-Only and SEDT-Only) against their counterparts with the same syntactic tree nodes but with randomly shuffled order (i.e. SECTRandom-Order and SEDT-Random-Order) as well as the baselines with randomly generated tree nodes (i.e. SECT-Random and SEDT-Random). Here we choose the length of window size to be 10. The predictive results in terms of EM and F1 metrics are reported in Table 3. From the table we see that both the ordering and the contents of the syntactic tree are important for the models to work properly: constituency and dependency trees achieved over 20% boost on perfor-\nmance compared to the randomly generated ones and our proposed ordering also out-performed the random ordering. It also worth mentioning that the ordering of dependency trees seems to have less impacts to the performance compared to that of the constituency trees. This is because sequences extracted from constituency trees contain hierarchical information, which ordering will affect the output of the model significantly. However, sequences extracted from dependency trees are all children nodes, which are often interchangeable and don’t seem to be affected by ordering much."
    }, {
      "heading" : "4.5 Window Size Analysis",
      "text" : "As we have mentioned in the earlier sections, limiting the window size is an important technique to prevent excessive usage on VRAM. In practice, we found that limiting the window size also benefits the performance of our models. In Table 4 we compared the predictive performance of SECT and SEDT models by varying the length of their window sizes from 1 to maximum on the development set. In general the results illustrate that performances of the models increase with the length of the window. However, we found that for SECT model, its mean performance reached the peak while standard deviations narrowed when window size reaches 10. We also observed that larger window size does not generate predictive results that is as good as the one with window size set to 10. This suggests that there exists an optimal window size for the constituency tree. One possible explanation is increasing the window size leads to the increase in the number of syntactic nodes in the extracted syntactic sequence. Although sub-trees might be similar between context and question, it is very unlikely that the complete trees are the same. Because of that, allowing the syntactic sequence to extend beyond the certain heights will introduce unnecessary noise into the learned representation, which will compromise the performance of the models. Similar conclusion holds for the SEDT model, which has an improved performance and decreased variance with the window size is set to 10. We did not perform experiments with window size beyond 10 for SEDT since it will consume VRAM that exceeds the capacity\nof our computing device."
    }, {
      "heading" : "4.6 Overlapping Analysis",
      "text" : "To further understand the performance benefits of incorporating syntactic information into the question answering problem, we can take a look at the questions that models disagree. Figure 4 is the Venn Diagram on the questions that have been corrected identified by SECT, SEDT and the baseline BiDAF model. Here we see that the vast majority of the correctly answered questions are shared across all three models. The rest of them indicates questions that models disagree and are distributed fairly evenly.\nTo understand the types of the questions that syntactic models can do better, we extracted three questions that was correctly answered by SECT and SEDT but not the baseline model. In Table 5, all of the three questions are “Wh-questions” and expect an answer of noun phrase (NP). Without knowing the syntactic information, BiDAF answered questions with unnecessary structures such as verb phrases (vp) (e.g. “acts as · · · ”, “represented · · · ”) or prepositional phrases (pp) (e.g. “in · · · ”) in addition to NPs (e.g. “the architect engineer”, “uncertainty” and “powerful high frequency currents”) that normal human would answer. For that reason, answers provided by BiDAF failed the exact match although its answers are semantically\nequivalent to the ones provided by SECT. Having incorporated constituency information provided an huge advantage in inferring the answers that are most natural for human.\nThe advantages of using dependency tree in our model can be illustrated using the questions in Table 6. Here again we listed the ones that are correctly identified by SEDT but not BiDAF. As we can see that the answer provided by BiDAF for first question broke the parenthesis incorrectly, a problem that can be easily solved by utilizing dependency information. In the second example, BiDAF failed to identify the dependency structures between “50%” and the keyword being asked “wheat”, which resulted in an incorrect answer that has nothing to do with the question. SEDT, on the other hand, answered the question correctly. In the third question, the key to the answer is to correctly identify the subject of question phrase “is the basic unit of organization”. Using dependency tree as illustrated in Figure 2, SEDT is able to identify the subject phrase correctly, namely “The Annual Conference”. However, BiDAF failed to anwer the question correctly and selected a noun phrase as the answer."
    }, {
      "heading" : "5 Related Work",
      "text" : "Reading Comprehension. Reading comprehension is a challenging task in NLP research. Since the release of the SQuAD data set, many works have been done to construct models on this massive question answering data set. Rajpurkar et. al. are among the first authors to explore the SQuAD. They use logistic regression with pos tagging information [14] and provided a strong baseline for all subsequent models. A steep improvement is given by the RaSoR model [8] which utilizes recurrent neural networks which consider all possible subphrases of the context and evaluate them one by one. To avoid comparing all possible candidates and to improve the performance, Match-LSTM [19] is proposed by using a pointer network [18] to extract the answer span from the context. The same idea was taken to the BiDAF [15] model by introducing a bi-directional attention mechanism. We\nQuestion Context BiDAF SEDT In the layered model of the Earth, the mantle has two layers below it. What are they? These advances led to the development of a layered model of the Earth, with a crust and lithosphere on top, the mantle below (separated within itself by seismic discontinuities at 410 and 660 kilometers), and the outer core and inner core below that. seismic discontinuities at 410 and 660 kilometers), and the outer core and inner core the outer core and inner core What percentage of farmland grows wheat? More than 50% of this area is sown for wheat, 33% for barley and 7% for oats. 33% 50% What is the basic unit of organization within the UMC?\nThe Annual Conference, roughly the equivalent of a diocese in the Anglican Communion and the Roman Catholic Church or a synod in some Lutheran denominations such as the Evangelical Lutheran Church in America, is the basic unit of organization within the UMC. Evangelical Lutheran Church in America The Annual Conference\nTable 6: Questions that are correctly answered by SEDT but not BiDAF\nnote that although the above models are quite strong models for the machine comprehension task, none of them considers syntactic information into their prediction models.\nRepresentations of Texts and Words. One of the main issues in reading comprehension is to identify the latent representations of texts and words [3, 8, 20, 21, 23]. Many pre-trained libraries such as word2vec [11] and Glove[12] exists that can be used to map words into a high dimensional embedding space. Another approach is to generate embeddings by using neural networks models such as Character Embedding [6] and Tree-LSTM [16]. One thing that worth mentioning is that although Tree-LSTM does utilize syntactic information, it targets at phrases or sentences level embedding other than the word level embedding we have discussed in this paper. Many machine comprehension models include both pre-trained embeddings and variable embeddings that can be changed through training stage [15, 22]."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we proposed methods to embed syntactic information into the deep neural models to improve the accuracy of our prediction model in the machine comprehension task. We started our paper with definitions of SEST framework and proposed two instances to it: the structural embedding of constituency trees (SECT) and the structural embedding of dependency trees (SEDT). Experimental results on SQuAD data set showed that our proposed approaches outperform the state-of-the-art BiDAF model, proofing that the proposed embeddings play a sig-\nnificant part in correctly identifying answers for the machine comprehension task. In particular, we found that our model can perform especially good on exact match metrics, which requires syntactic information to accurately locate the boundaries of the answers. Future work involves integrating our proposed model SEST with deeper neural networks such as VD-CNN [2] to improve learning capacity for syntactic embedding."
    } ],
    "references" : [ {
      "title" : "Learning phrase representations using RNN encoder-decoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart van Merrienboer", "Çaglar Gülçehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio" ],
      "venue" : "In EMNLP,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2014
    }, {
      "title" : "Very deep convolutional networks for text classification",
      "author" : [ "Alexis Conneau", "Holger Schwenk", "Loı̈c Barrault", "Yann Lecun" ],
      "venue" : "In European Chapter of the Association for Computational Linguistics",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2017
    }, {
      "title" : "Attention-over-attention neural networks for reading comprehension",
      "author" : [ "Yiming Cui", "Zhipeng Chen", "Si Wei", "Shijin Wang", "Ting Liu", "Guoping Hu" ],
      "venue" : "arXiv preprint arXiv:1607.04423,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2016
    }, {
      "title" : "Gated-attention readers for text comprehension",
      "author" : [ "Bhuwan Dhingra", "Hanxiao Liu", "William W Cohen", "Ruslan Salakhutdinov" ],
      "venue" : "arXiv preprint arXiv:1606.01549,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2016
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1997
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Yoon Kim" ],
      "venue" : "arXiv preprint arXiv:1408.5882,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2014
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Yoon Kim" ],
      "venue" : "In EMNLP, pages 1746–1751,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2014
    }, {
      "title" : "Learning recurrent span representations for extractive question answering",
      "author" : [ "Kenton Lee", "Tom Kwiatkowski", "Ankur Parikh", "Dipanjan Das" ],
      "venue" : "arXiv preprint arXiv:1611.01436,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2016
    }, {
      "title" : "Foundations of statistical natural language processing, volume 999",
      "author" : [ "Christopher D Manning", "Hinrich Schütze" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1999
    }, {
      "title" : "The Stanford CoreNLP natural language processing toolkit",
      "author" : [ "Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky" ],
      "venue" : "In Association for Computational Linguistics (ACL) System Demonstrations,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2014
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2013
    }, {
      "title" : "Glove: Global vectors for word representation. In EMNLP, pages 1532–1543",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher Manning" ],
      "venue" : "Association for Computational Linguistics,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2014
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D. Manning" ],
      "venue" : "In Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2014
    }, {
      "title" : "Squad: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang" ],
      "venue" : "In EMNLP,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2016
    }, {
      "title" : "Bidirectional attention flow for machine comprehension",
      "author" : [ "Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi" ],
      "venue" : "arXiv preprint arXiv:1611.01603,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2016
    }, {
      "title" : "Improved semantic representations from tree-structured long short-term memory networks",
      "author" : [ "Kai Sheng Tai", "Richard Socher", "Christopher D. Manning" ],
      "venue" : "In IN PROC. ACL,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2015
    }, {
      "title" : "Newsqa: A machine comprehension dataset",
      "author" : [ "Adam Trischler", "Tong Wang", "Xingdi Yuan", "Justin Harris", "Alessandro Sordoni", "Philip Bachman", "Kaheer Suleman" ],
      "venue" : "arXiv preprint arXiv:1611.09830,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2016
    }, {
      "title" : "Machine comprehension using match-lstm and answer pointer",
      "author" : [ "Shuohang Wang", "Jing Jiang" ],
      "venue" : "arXiv preprint arXiv:1608.07905,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2016
    }, {
      "title" : "Multiperspective context matching for machine comprehension",
      "author" : [ "Zhiguo Wang", "Haitao Mi", "Wael Hamza", "Radu Florian" ],
      "venue" : "arXiv preprint arXiv:1612.04211,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2016
    }, {
      "title" : "Dynamic coattention networks for question answering",
      "author" : [ "Caiming Xiong", "Victor Zhong", "Richard Socher" ],
      "venue" : "arXiv preprint arXiv:1611.01604,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2016
    }, {
      "title" : "Words or characters? fine-grained gating for reading comprehension",
      "author" : [ "Zhilin Yang", "Bhuwan Dhingra", "Ye Yuan", "Junjie Hu", "William W Cohen", "Ruslan Salakhutdinov" ],
      "venue" : "arXiv preprint arXiv:1611.01724,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2016
    }, {
      "title" : "End-to-end answer chunk extraction and ranking for reading comprehension",
      "author" : [ "Yang Yu", "Wei Zhang", "Kazi Hasan", "Mo Yu", "Bing Xiang", "Bowen Zhou" ],
      "venue" : "arXiv preprint arXiv:1610.09996,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 13,
      "context" : "Machine comprehension on question answering is a problem that requires machines to generate answers by locating a phase inside a context paragraph [14, 17], which is especially challenging since the algorithm has to come up with an answer that matches exactly with what human would do.",
      "startOffset" : 147,
      "endOffset" : 155
    }, {
      "referenceID" : 16,
      "context" : "Machine comprehension on question answering is a problem that requires machines to generate answers by locating a phase inside a context paragraph [14, 17], which is especially challenging since the algorithm has to come up with an answer that matches exactly with what human would do.",
      "startOffset" : 147,
      "endOffset" : 155
    }, {
      "referenceID" : 3,
      "context" : "[4, 15, 19, 21, 22].",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 14,
      "context" : "[4, 15, 19, 21, 22].",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 17,
      "context" : "[4, 15, 19, 21, 22].",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 19,
      "context" : "[4, 15, 19, 21, 22].",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 20,
      "context" : "[4, 15, 19, 21, 22].",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 13,
      "context" : "Here we are particularly interested in two types of syntactic information, Constituency Tree and Dependency Tree, which have been reported to be some of the most important features for modeling question answering problem using a logistic regression model [14].",
      "startOffset" : 255,
      "endOffset" : 259
    }, {
      "referenceID" : 8,
      "context" : "In a constituency tree [9], a sentence is decomposed to a tree structure diagram with internal nodes representing phrase structure grammars and terminal nodes representing the actual words presented.",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 8,
      "context" : "On the other hand, a dependency tree [9] is constructed based on the dependency structure of a sentence.",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 12,
      "context" : "We choose GloVe [13] to obtain a pre-tained and fixed vector for each word.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 10,
      "context" : "There are other alternatives exist such as word2vec [11].",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 5,
      "context" : "Instead of using a fixed embedding, we use Convolutional Neural Networks (CNN) to model character level embedding, which values can be changed during training [6].",
      "startOffset" : 159,
      "endOffset" : 162
    }, {
      "referenceID" : 4,
      "context" : "Here we choose a multi-layer bi-directional Long Short Term Memory (LSTM) [5] to obtain more abstract representations for words in the contexts and questions.",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 19,
      "context" : "Examples are Dynamic Coattention Network [21], Bi-directional Attention flow [15], Multi-Perspective Context Matching [20] and match-LSTM [19].",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 14,
      "context" : "Examples are Dynamic Coattention Network [21], Bi-directional Attention flow [15], Multi-Perspective Context Matching [20] and match-LSTM [19].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 18,
      "context" : "Examples are Dynamic Coattention Network [21], Bi-directional Attention flow [15], Multi-Perspective Context Matching [20] and match-LSTM [19].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 17,
      "context" : "Examples are Dynamic Coattention Network [21], Bi-directional Attention flow [15], Multi-Perspective Context Matching [20] and match-LSTM [19].",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 9,
      "context" : "We assume that the syntactic information have already been generated in the preprocessing step using tools such as the Stanford CoreNLP [10] and this section only concerns with the problem of converting syntactic information into embeddings that can be used in the deep neural models.",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 8,
      "context" : "The constituency tree is a syntactic parse tree constructed by phrase structure grammars [9], which defines the way to hierarchically construct a sentence from words in a bottom-up manner based on constituency relations.",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 8,
      "context" : "The dependency tree is a syntactic tree constructed by dependency grammars [9], which defines the way to connect words by directed links that represent dependencies.",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 0,
      "context" : "Similar to previous work [1, 7], we use a neural network to encode a variable-length syntactic sequence into a fixed-length vector representation.",
      "startOffset" : 25,
      "endOffset" : 31
    }, {
      "referenceID" : 6,
      "context" : "Similar to previous work [1, 7], we use a neural network to encode a variable-length syntactic sequence into a fixed-length vector representation.",
      "startOffset" : 25,
      "endOffset" : 31
    }, {
      "referenceID" : 13,
      "context" : "We conducted systematic experiments on the SQuAD dataset [14], a massive question answer dataset to evaluate the performance of our proposed SEST methods.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 13,
      "context" : "Detailed description of the SQuAD dataset can be found in [14].",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 9,
      "context" : "Syntactic information including POS tags and syntactic trees were acquired by Stanford CoreNLP utilities [10].",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 14,
      "context" : "We use both SECT and SEDT models to compare against the baseline models, which are BiDAF model [15] and the POS model introduced in Section 3.",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 11,
      "context" : "The fixed word embedding has a dimension of 100, which is provided by the GloVe data set [12].",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 13,
      "context" : "They use logistic regression with pos tagging information [14] and provided a strong baseline for all subsequent models.",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 7,
      "context" : "A steep improvement is given by the RaSoR model [8] which utilizes recurrent neural networks which consider all possible subphrases of the context and evaluate them one by one.",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 17,
      "context" : "To avoid comparing all possible candidates and to improve the performance, Match-LSTM [19] is proposed by using a pointer network [18] to extract the answer span from the context.",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 14,
      "context" : "The same idea was taken to the BiDAF [15] model by introducing a bi-directional attention mechanism.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 2,
      "context" : "One of the main issues in reading comprehension is to identify the latent representations of texts and words [3, 8, 20, 21, 23].",
      "startOffset" : 109,
      "endOffset" : 127
    }, {
      "referenceID" : 7,
      "context" : "One of the main issues in reading comprehension is to identify the latent representations of texts and words [3, 8, 20, 21, 23].",
      "startOffset" : 109,
      "endOffset" : 127
    }, {
      "referenceID" : 18,
      "context" : "One of the main issues in reading comprehension is to identify the latent representations of texts and words [3, 8, 20, 21, 23].",
      "startOffset" : 109,
      "endOffset" : 127
    }, {
      "referenceID" : 19,
      "context" : "One of the main issues in reading comprehension is to identify the latent representations of texts and words [3, 8, 20, 21, 23].",
      "startOffset" : 109,
      "endOffset" : 127
    }, {
      "referenceID" : 21,
      "context" : "One of the main issues in reading comprehension is to identify the latent representations of texts and words [3, 8, 20, 21, 23].",
      "startOffset" : 109,
      "endOffset" : 127
    }, {
      "referenceID" : 10,
      "context" : "Many pre-trained libraries such as word2vec [11] and Glove[12] exists that can be used to map words into a high dimensional embedding space.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 11,
      "context" : "Many pre-trained libraries such as word2vec [11] and Glove[12] exists that can be used to map words into a high dimensional embedding space.",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 5,
      "context" : "Another approach is to generate embeddings by using neural networks models such as Character Embedding [6] and Tree-LSTM [16].",
      "startOffset" : 103,
      "endOffset" : 106
    }, {
      "referenceID" : 15,
      "context" : "Another approach is to generate embeddings by using neural networks models such as Character Embedding [6] and Tree-LSTM [16].",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 14,
      "context" : "Many machine comprehension models include both pre-trained embeddings and variable embeddings that can be changed through training stage [15, 22].",
      "startOffset" : 137,
      "endOffset" : 145
    }, {
      "referenceID" : 20,
      "context" : "Many machine comprehension models include both pre-trained embeddings and variable embeddings that can be changed through training stage [15, 22].",
      "startOffset" : 137,
      "endOffset" : 145
    }, {
      "referenceID" : 1,
      "context" : "Future work involves integrating our proposed model SEST with deeper neural networks such as VD-CNN [2] to improve learning capacity for syntactic embedding.",
      "startOffset" : 100,
      "endOffset" : 103
    } ],
    "year" : 2017,
    "abstractText" : "This paper develops a model that addresses syntactic embedding for machine comprehension, a key task of natural language understanding. Our proposed model, structural embedding of syntactic trees (SEST), takes each word in a sentence, constructs a sequence of syntactic nodes extracted from syntactic parse trees, and encodes the sequence into a vector representation. The learned vector is then incorporated into neural attention models, which allows learning the mapping of syntactic structures between question and context pairs. We evaluate our approach on SQuAD dataset and demonstrate that our model can accurately identify the syntactic boundaries of the sentences and to extract answers that are syntactically coherent over the baseline methods.",
    "creator" : "LaTeX with hyperref package"
  }
}