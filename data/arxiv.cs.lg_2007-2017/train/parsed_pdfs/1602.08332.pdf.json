{
  "name" : "1602.08332.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Bounded Rational Decision-Making in Feedforward Neural Networks",
    "authors" : [ "Felix Leibfried", "Daniel A. Braun" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Bounded rational decision-makers transform sensory input into motor output under limited computational resources. Mathematically, such decision-makers can be modeled as informationtheoretic channels with limited transmission rate. Here, we apply this formalism for the first time to multilayer feedforward neural networks. We derive synaptic weight update rules for two scenarios, where either each neuron is considered as a bounded rational decision-maker or the network as a whole. In the update rules, bounded rationality translates into information-theoretically motivated types of regularization in weight space. In experiments on the MNIST benchmark classification task for handwritten digits, we show that such information-theoretic regularization successfully prevents overfitting across different architectures and attains results that are competitive with other recent techniques like dropout, dropconnect and Bayes by backprop, for both ordinary and convolutional neural networks."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Intelligent systems in biology excel through their ability to flexibly adapt their behavior to changing environments so as to maximize their (expected) benefit. In order to understand such biological intelligence and to design artificial intelligent systems, a central goal is to analyze adaptive behavior from a theoretical point of view. A formal framework to achieve this goal is decision theory. An important idea, originating from the foundations of decision theory, is the principle of maximum expected utility [1]. According to the principle of maximum expected utility, an intelligent agent is formalized as a decision-maker that chooses optimal actions that maximize the expected benefit of an outcome, where the agent’s benefit is quantified by a utility function.\nA fundamental problem of the maximum expected utility principle is that it does not take into account computational resources that are necessary to identify optimal actions— it is for example computationally prohibitive to compute an optimal chess move because of the vast amount of potential board configurations. One way of taking computational resources into account is to study optimal decisionmaking under information-processing constraints [2, 3]. In this study, we use an information-theoretic model of bounded rational decision-making [4, 5, 6] that has precursors in the economic literature [7, 8, 9] and that is closely related to recent advances harnessing information theory for machine learning and perception-action systems [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20].\nPreviously, this information-theoretic bounded rationality model was applied to derive a synaptic weight update rule for a single reward-maximizing spiking neuron [21]. It was shown that such a neuron tries to keep its firing rate close to its average firing rate, which ultimately leads to economizing of synaptic weights. Mathematically, such economizing is equivalent to a regularization that prevents synaptic weights from growing without bounds. The bounded rational weight update rule furthermore generalizes the synaptic weight update rule for an ordinary reward-maximizing spiking neuron as presented for example in [22]. In our current work, we extend the framework of informationtheoretic bounded rationality to networks of neurons, but restrict ourselves for a start to deterministic settings. In particular, we investigate two scenarios, where either each single neuron is considered as a bounded rational decisionmaker or the network as a whole.\nThe remainder of this manuscript is organized as follows. In Section 2, we explain the information-theoretic bounded rationality model that we use. In Section 3, we apply this model to derive bounded rational synaptic weight update rules for single neurons and networks of neurons. In Section 4, we demonstrate the regularizing effect of these bounded rational weight update rules on the MNIST benchmark classification task. In Section 5, we conclude.\nProceedings of the 32nd Conference on Uncertainty in Artificial Intelligence, New York City, NY, USA, 2016.\nar X\niv :1\n60 2.\n08 33\n2v 2\n[ cs\n.A I]\n2 3\nM ay\n2 01"
    }, {
      "heading" : "2 BACKGROUND ON BOUNDED RATIONAL DECISION-MAKING",
      "text" : ""
    }, {
      "heading" : "2.1 A FREE ENERGY PRINCIPLE FOR BOUNDED RATIONALITY",
      "text" : "A decision-maker is faced with the task to choose an optimal action out of a set of actions. Each action y is associated with a given task-specific utility value U(y). A fully rational decision-maker picks the action y∗ that globally maximizes the utility function, where y∗ = arg maxy U(y), assuming for notational simplicity that the global maximum is unique. Under limited computational resources however, the decision-maker may not be able to identify the globally optimal action y∗ which leads to the question of how limited computational resources should be quantified. In general, the decision-maker’s behavior can be expressed as a probability distribution over actions p(y). The basic idea of information-theoretic bounded rationality is that changes in such probability distributions are costly and necessitate computational resources. More precisely, computational resources are quantified as informational cost evoked by changing from a prior probabilistic strategy p0(y) to a posterior probabilistic strategy p(y) during the deliberation process preceding the choice. Mathematically, this informational cost is given by the Kullback-Leibler divergence DKL(p(y)||p0(y)) ≤ B between prior and posterior strategy, where computational resources are modeled as an upper bound B ≥ 0 [10, 5, 6, 13, 14, 15, 17, 18, 19, 20, 8, 9]. Accordingly, bounded rational decision-making can be formalized by the following free energy objective\np∗(y)\n= arg max p(y) (1− β) 〈U(y)〉p(y) − βDKL(p(y)||p0(y))\n= arg max p(y)\n〈 (1− β)U(y)− β ln p(y)\np0(y) 〉 p(y) ,\n(1)\nwhere β ∈ (0; 1) controls the trade-off between expected utility and informational cost. Note that the upper bound B imposed on the Kullback-Leibler divergence determines the value of β. Choosing the value ofB is hence equivalent to choosing the value of β.\nThe free energy objective in Equation (1) is concave with respect to p(y) and the optimal solution p∗(y) can be expressed in closed analytic form:\np∗(y) = p0(y) exp( 1−β β U(y))∑\ny′ p0(y ′) exp( 1−ββ U(y\n′)) . (2)\nIn the limit cases of none (β → 1) and infinite (β → 0)\nresources, the optimal strategy from Equation (2) becomes\nlim β→1\np∗(y) = p0(y), (3)\nlim β→0\np∗(y) = δyy∗ , (4)\nrespectively, where y∗ = arg maxy U(y) represents an action that globally maximizes the utility function. A decision-maker without any computational resources (β → 1) sticks to its prior strategy p0(y), whereas a decisionmaker that can access an arbitrarily large amount of resources (β → 0) always picks a globally optimal action and recovers thus the fully rational decision-maker."
    }, {
      "heading" : "2.2 A RATE DISTORTION PRINCIPLE FOR CONTEXT-DEPENDENT DECISION-MAKING",
      "text" : "In the face of multiple contexts, fully rational decisionmaking requires to find an optimal action y for each environment x, where optimality is defined through a utility function U(x, y). Bounded rational decision-making in multiple contexts means to compute multiple strategies, expressed as conditional probability distributions p(y|x), under limited computational resources. Limited computational resources are modeled through an upper bound B ≥ 0 on the expected Kullback-Leibler divergence 〈DKL(p(y|x)||p0(y))〉p(x) ≤ B between the strategies p(y|x) and a common prior p0(y), averaged over all possible environments described by the distribution p(x) [4, 15]. The resulting optimization problem may be formalized as\np∗(y|x) = arg max p(y|x) (1− β) 〈U(x, y)〉p(x,y)\n− β 〈DKL (p(y|x)||p0(y))〉p(x) , (5)\nwhere β ∈ (0; 1) governs the trade-off between expected utility and informational cost. It can be shown that the most economic prior p0(y) is given by the marginal distribution p0(y) = p(y) = ∑ x p(y|x)p(x), because the marginal distribution minimizes the expected KullbackLeibler divergence for a given set of conditional distributions p(y|x)—see [23]. In this case, the expected KullbackLeibler divergence becomes identical to the mutual information I(x, y) between the environment x and the action y [4, 21, 11, 12, 7, 16]. Accordingly, bounded rational decision-making can be formalized through the following objective\np∗(y|x) = arg max\np(y|x) (1− β) 〈U(x, y)〉p(x,y) − βI(x, y)\n= arg max p(y|x)\n〈 (1− β)U(x, y)− β ln p(y|x)\np(y) 〉 p(x,y) ,\n(6)\nwhich is mathematically equivalent to the rate distortion problem from information theory [24].\nThe rate distortion objective in Equation (6) is concave with respect to p(y|x) but there is unfortunately no closed analytic form solution. It is however possible to express the optimal solution as a set of self-consistent equations:\np∗(y|x) = p(y) exp( 1−ββ U(x, y))∑ y′ p(y ′) exp( 1−ββ U(x, y ′)) , (7)\np(y) = ∑ x p∗(y|x)p(x). (8)\nThese self-consistent equations are solved by replacing p(y) with an initial arbitrary distribution q(y) and iterating through Equations (7) and (8) in an alternating fashion. This procedure is known as Blahut-Arimoto algorithm [25, 26] and is guaranteed to converge to a global optimum [27] presupposed that q(y) does not assign zero probability mass to any y.\nIn the limit cases of none (β → 1) and infinite (β → 0) resources, the optimal strategy from Equations (7) and (8) may be expressed in closed analytic form\nlim β→1\np∗(y|x) = p(y) = δyy∗ , (9)\nlim β→0\np∗(y|x) = δyy∗x , (10)\nwhere y∗ = arg maxy 〈U(x, y)〉p(x) refers to an action that globally maximizes the expected utility averaged over all possible environments, and y∗x refers to an action that globally maximizes the utility for one particular environment x—assuming for notational simplicity that global maxima are unique in both cases. In the absence of any computational resources (β → 1), the decision-maker chooses the same strategy no matter which environment is encountered in order to minimize the deviation between the conditional strategies p(y|x) and the average strategy p(y). The decision-maker chooses however a strategy that maximizes the average expected utility. In case of access to an arbitrarily large amount of computational resources (β → 0), the decision-maker picks the best action for each environment and recovers thus the fully rational decision-maker."
    }, {
      "heading" : "3 THEORETICAL RESULTS: SYNAPTIC WEIGHT UPDATE RULES",
      "text" : ""
    }, {
      "heading" : "3.1 PARAMETERIZED STRATEGIES AND ONLINE RULES",
      "text" : "Computing the optimal solution to the rate distortion problem in Equation (6) with help of Equations (7) and (8) through the Blahut-Arimoto algorithm has two severe drawbacks. First, it requires to compute and store the conditional strategies p(y|x) and the marginal strategy p(y) explicitly, which is prohibitive for large environment and action spaces. And second, it requires that the decisionmaker is able to evaluate the utility function for arbitrary\nenvironment-action pairs (x, y), which is a plausible assumption in planning, but not in reinforcement learning where samples from the utility function can only be obtained from interactions with the environment.\nWe therefore assume a parameterized form of the strategy pw(y|x), from which the decision-maker can draw samples y for a given sample of the environment x, and optimize the rate distortion objective from Equation (6) with help of gradient ascent [21]—also referred to as policy gradient in the reinforcement learning literature [22]. Gradient ascent requires to compute the derivative of the objective function L(w) with respect to the strategy parameters w and to update the parameters according to the rule w ← w + α · ∂∂wL(w) in each time step, where α > 0 denotes the learning rate and ∂∂wL(w) is defined as\n∂\n∂w L(w) =〈( ∂\n∂w ln pw(y|x)\n) (1− β)U(x, y) 〉 pw(x,y)\n− 〈( ∂\n∂w ln pw(y|x)\n) β ln\npw(y|x) pw(y) 〉 pw(x,y) . (11)\nNote that the update rule from Equation (11) requires the computation of an expected value over pw(x, y). This expected value can be approximated through environmentaction samples (x, y) in either a batch or an online manner. For the rest of this paper, we assume an online update rule where the agent adapts its behavior instantaneously after each interaction with the environment in response to an immediate reward signal U(x, y) as is typical for reinforcement learning.\nInformally, the rate distortion model for bounded rational decision-making translates into a specific form of regularization that penalizes deviations of the decision-maker’s instantaneous strategy pw(y|x), given the current environment x, from the decision-maker’s mean strategy pw(y) =∑ x pw(y|x)p(x), averaged over all possible environments. Previously, Equation (11) was applied to a single spiking neuron that was stochastic [21]. Here, we generalize this approach to deterministic networks of neurons that have neural input (environmental context x), neural output (action y) and a reward signal (utility U ). We derive parameter update rules in the style of Equation (11) that allow to adjust synaptic weights in an online fashion. In particular, we investigate two scenarios where either each single neuron is considered as a bounded rational decision-maker or the network as a whole."
    }, {
      "heading" : "3.2 A STOCHASTIC NEURON AS A BOUNDED RATIONAL DECISION-MAKER",
      "text" : "A stochastic neuron may be considered as a bounded rational decision-maker [21]: the neuron’s presynaptic input is\ninterpreted as environmental context and the neuron’s output is interpreted as action variable. The neuron’s parameterized strategy corresponds to its firing behavior and is given by\npw(y|x) = y · ρ(w>x) + (1− y) · (1− ρ(w>x)), (12)\nwhere y ∈ {0, 1} is a binary variable reflecting the neuron’s current firing state, x is a binary column vector representing the neuron’s current presynaptic input and w is a real-valued column vector representing the strength of presynaptic weights. ρ ∈ (0; 1) is a monotonically increasing function denoting the neuron’s current firing probability. In a similar way, the neuron’s mean firing behavior can be expressed as:\npw(y) = y · ρ̄(w) + (1− y) · (1− ρ̄(w)), (13)\nwhere ρ̄(w) = ∑\nx ρ(w >x)p(x) denotes the neuron’s\nmean firing probability averaged over all possible inputs x. The mean firing probability ρ̄(w) can be easily estimated with help of an exponential window in an online manner according to\nρ̄(w)← (1− 1 τ )ρ̄(w) + 1 τ ρ(w>x), (14)\nwhere τ is a constant defining the time horizon [21].\nAssuming a task-specific utility function U(x, y) determining the neuron’s instantaneous reward and assuming furthermore that the neuron’s output y does not impact the presynaptic input x of the next time step, the bounded rational neuron may be thought of as optimizing a rate distortion objective according to Equation (6) with gradient ascent as outlined in Section 3.1 [21]. Equation (11) is then applicable by using the quantities\n∂\n∂wi ln pw(y|x) =\nxiρ ′(w>x)\n( y\nρ(w>x) − 1− y 1− ρ(w>x)\n) ,\n(15)\nand\nln pw(y|x) pw(y) = y ln ρ(w>x)\nρ̄(w) + (1− y) ln 1− ρ(w >x) 1− ρ̄(w) .\n(16)\nBy averaging over the binary quantity y, a more concise weight update rule is derived [21]:\n∂\n∂wi L(w) =〈\nxiρ ′(w>x)(1− β)∆U(x) 〉 p(x)\n− 〈 xiρ ′(w>x)β ln\nρ(w>x)(1− ρ̄(w)) ρ̄(w)(1− ρ(w>x)) 〉 p(x) , (17)\nwhere ∆U(x) = U(x, y = 1) − U(x, y = 0) denotes the difference in utility between firing (y = 1) and not firing (y = 0) for a given x. If the conditional and marginal strategies are initialized to be roughly equal pw0(y) ≈ pw0(y|x), where w0 ≈ 0 refers to the initial value of w, the hyperparameter β determines how fast the decisionmaker’s strategy converges. A high value of β implies little computational resources and quick convergence due to the fact that conditional and marginal strategies are initially almost equal. On the opposite, a low value of β indicating vast computational resources allows the decision-maker to find an optimal strategy for each environment where conditional and marginal strategies may deviate substantially."
    }, {
      "heading" : "3.3 A DETERMINISTIC NEURON AS A BOUNDED RATIONAL DECISION-MAKER",
      "text" : "In a deterministic setup, the neuron’s parameterized firing behavior in a small time window ∆t may be expressed through its firing rate φ(w>ξ) as:\npw(y|ξ) = y·φ(w>ξ)∆t+(1−y)·(1−φ(w>ξ)∆t), (18)\nwhere ξ is a real-valued column vector indicating the presynaptic firing rates and φ > 0 is a monotonically increasing function. In a similar fashion, the neuron’s mean firing behavior is given by\npw(y) = y · φ̄(w)∆t+ (1− y) · (1− φ̄(w)∆t), (19)\nwhere φ̄(w) = ∑\nξ φ(w >ξ)p(ξ) refers to the neuron’s\nmean firing rate averaged over all possible presynaptic firing rates ξ. In accordance with the previous section, the mean firing rate φ̄(w) can be conveniently approximated in an online manner through an exponential window with a time constant τ as:\nφ̄(w)← (1− 1 τ )φ̄(w) + 1 τ φ(w>ξ). (20)\nUsing the quantities introduced above, we can define a mutual information rate between the presynaptic firing rates ξ and the instantaneous firing state of the neuron y ∈ {0; 1}:\nlim ∆t→0\n1\n∆t I(ξ, y)\n= lim ∆t→0\n1\n∆t 〈∑ y pw(y|ξ) ln pw(y|ξ) pw(y) 〉 p(ξ)\n= 〈 φ(w>ξ) ln φ(w>ξ)\nφ̄(w) 〉 p(ξ) .\n(21)\nA derivation of Equation (21) can be found in Section A.1. Assuming a rate-dependent utility function U(ξ, φ(w>ξ)), a deterministic neuron can be interpreted as a bounded rational decision-maker similar to Equation (6) with the fol-\nlowing rate distortion objective\nw∗ = arg max w\n(1− β) 〈 U(ξ, φ(w>ξ)) 〉 p(ξ)\n− β lim ∆t→0\n1\n∆t I(ξ, y)\n= arg max w\n〈 (1− β)U(ξ, φ(w>ξ)) 〉 p(ξ)\n− 〈 βφ(w>ξ) ln φ(w>ξ)\nφ̄(w) 〉 p(ξ) .\n(22)\nOptimizing the neuron’s weights with gradient ascent, a similar weight update rule as in Equation (17) is derived for the deterministic case:\n∂\n∂wi L(w) =〈\nξiφ ′(w>ξ)(1− β) ∂\n∂φ U(ξ, φ(w>ξ)) 〉 p(ξ)\n− 〈 ξiφ ′(w>ξ)β ln φ(w>ξ)\nφ̄(w) 〉 p(ξ) ,\n(23)\nwhere ∂∂φU(ξ, φ(w >ξ)) denotes the derivative of the utility function with respect to the neuron’s firing rate. The solution in Equation (23) requires the derivative of two terms with respect to wi. The derivative of the expected utility 〈 U(ξ, φ(w>ξ)) 〉 p(ξ)\nis straightforward, whereas the derivative of the mutual information rate lim∆t→0 1 ∆tI(ξ, y) is not so trivial and explained in more detail in Section A.2."
    }, {
      "heading" : "3.4 A NEURAL NETWORK OF BOUNDED RATIONAL DETERMINISTIC NEURONS",
      "text" : "Here, we consider a feedforward multilayer perceptron that can be imagined to consist of individual bounded rational deterministic neurons as described in the previous section. Assuming that all neurons aim at maximizing a global utility function while at the same time minimizing their local mutual information rate, each neuron n may be interpreted as solving a deterministic rate distortion objective where the utility function is shared among all neurons but the mutual information cost is neuron-specific:\nwn∗ = arg max wn\n(1− β) 〈 U(ξin, f(W, ξin)) 〉 p(ξin)\n− β lim ∆t→0\n1\n∆t I(ξn, yn),\n(24)\nwhere wn, ξn and yn refer to the presynaptic weight vector, the presynaptic firing rates and the current firing state of neuron n respectively and whereW denotes the entirety of all weights in the whole neural network. The global utility U(ξin, f(W, ξin)) is expressed as a function of the network’s input rates ξin and the network’s output rates f(W, ξin).\nThe corresponding synaptic weight update rule for gradient ascent is similar to Equation (23) and given by\n∂\n∂wni Ln(W) =〈\n(1− β) ∂ ∂wni U(ξin, f(W, ξin)) 〉 p(ξin)\n− 〈 βξni φ ′(wn>ξn) ln φ(wn>ξn)\nφ̄(wn) 〉 p(ξin) ,\n(25)\nwhere Ln(W) refers to the rate distortion objective of neuron n. The derivative of the utility function with respect to the weight ∂∂wni U(ξ\nin, f(W, ξin)) can be straightforwardly derived via ordinary backpropagation [28]."
    }, {
      "heading" : "3.5 A DETERMINISTIC NEURAL NETWORK AS",
      "text" : "A BOUNDED RATIONAL DECISION-MAKER\nWhile focusing on individual neurons as bounded rational decision-makers in the previous section, it is also possible to interpret an entire feedforward multilayer perceptron as one bounded rational decision-maker. To allow for this interpretation, we consider in the following the network’s output rates fj(W, ξ) ∈ (0; 1) as the event probabilities of a categorical distribution (for example, by using a softmax activation function in the last layer). Importantly, the categorical distribution is considered as a bounded rational strategy\npW(y|ξ) = ∑ j yjfj(W, ξ), (26)\nthat generates a binary unit output vector y given the input rates ξ and the set of all weights in the entire network denoted byW . The average bounded rational strategy is then given by\npW(y) = ∑ j yj f̄j(W), (27)\nwhere f̄j(W) is the mean rate of output unit j that can again be approximated in an online manner according to\nf̄j(W)← (1− 1\nτ )f̄j(W) +\n1 τ fj(W, ξ), (28)\nby use of an exponential window with a time constant τ in line with previous sections.\nAccordingly, the informational cost can be quantified by the mutual information between ξ and y:\nI(ξ,y) = 〈∑ y pW(y|ξ) ln pW(y|ξ) pW(y) 〉 p(ξ)\n= 〈∑ j fj(W, ξ) ln fj(W, ξ) f̄j(W) 〉 p(ξ) .\n(29)\nPresupposing again a rate dependent utility function U(ξ, f(W, ξ)), the entire deterministic network may be interpreted to solve the subsequent rate distortion objective\nW∗ = arg max W (1− β) 〈U(ξ, f(W, ξ))〉p(ξ)\n− βI(ξ,y) = arg max\nW 〈(1− β)U(ξ, f(W, ξ))〉p(ξ)\n− 〈 β ∑ j fj(W, ξ) ln fj(W, ξ) f̄j(W) 〉 p(ξ) ,\n(30)\nAssuming that synaptic weights are updated via gradient ascent, the following weight update rule can be derived\n∂\n∂wni L(W) =〈\n(1− β) ∑ j ( ∂ ∂wni fj(W, ξ) )( ∂ ∂fj U(ξ, f(W, ξ)) )〉 p(ξ)\n− 〈 β ∑ j ( ∂ ∂wni fj(W, ξ) ) ln fj(W, ξ) f̄j(W) 〉 p(ξ) ,\n(31)\nwhere ∂∂wni denotes the derivative with respect to the ith weight of neuron n, and ∂∂fjU(ξ, f(W, ξ)) denotes the derivative of the utility function with respect to the firing rate of the jth output neuron. Equation (31) requires to differentiate two terms with respect to wni . The derivative of the expected utility is straightforward while the derivative of the mutual information is explained in Section A.3.\nNote that the derivative of the rate distortion objective ∂ ∂wni L(W) takes a convenient form which can be easily computed by extending ordinary backpropagation [28]. In ordinary backpropagation, the quantity ∂∂fjU(ξ, f(W, ξ)) is propagated backwards through the network. The core algorithm of ordinary backpropagation can be employed for computing ∂∂wni L(W) by simply replacing the derivative of the utility function ∂∂fjU(ξ, f(W, ξ)) with the more general quantity (1− β) ∂∂fjU(ξ, f(W, ξ))− β ln fj(W,ξ) f̄j(W) ."
    }, {
      "heading" : "4 EXPERIMENTAL RESULTS: MNIST CLASSIFICATION",
      "text" : "In our simulations, we applied both types of rate distortion regularization (the local type from Section 3.4 and the global type from Section 3.5) on the MNIST benchmark classification task. In particular, we investigated in how far this information-theoretically motivated regularization subserves generalization. To this end, we trained classification on the MNIST training set, consisting of 60, 000 grayscale images of handwritten digits, and tested generalization on\nthe MNIST test set, consisting of 10, 000 examples. For all our simulations, we used a network with two hidden layers of rectified linear units [29] and a top layer of 10 softmax units implemented in Lua with Torch [30]. We chose as optimization criterion the negative cross entropy between the class labels and the network output [31]\nU(ξ, f(W, ξ)) = ∑ j δjl(ξ) ln fj(W, ξ), (32)\nwhere δ denotes the Kronecker delta and ξ the vectorized input image—note that pixels were normalized to lie in the range [0; 1]. The variable j ∈ {1, 10} is an index over the network’s output units and l(ξ) ∈ {1, 10} denotes the label of image ξ.\nIn order to assess the robustness of our regularizers, we performed our experiments with networks of different architectures. In particular, we used network architectures with two hidden layers and varied the number of neurons #neu ∈ {529, 1024, 2025, 4096} per hidden layer. We performed gradient ascent with a learning rate α = 0.01 updating weights online after each training example. We trained the networks for 70 epochs where one epoch corresponded to one sweep through the entire training set. After each epoch, the learning rate decayed according to α ← α1+t·η where t denotes the current epoch and η = 0.002 is a decay parameter. Weights were updated by use of a momentum γ = 0.9 according to ∆wni ← γ∆wni +(1−γ) ∂∂wni L(W) and were randomly initialized in the range (−(#in(n))−0.5; (#in(n))−0.5) with help of a uniform distribution at the beginning of the simulation where #in(n) denotes the number of inputs to neuron n. Each non-input neuron had an additional bias weight that was initialized in the same way as the presynaptic weights of that neuron. Rate distortion regularization required furthermore to compute the mean firing rate φ̄(wn) of individual neurons n through an exponential window in an online fashion with a time constant τ = 1000. In order to ensure numerical stability when using rate distortion regularization, terms of the form ln φ(w n>ξn)\nφ̄(wn) in the weight update rules were computed according to ln max{φ(wn>ξn), ε} − ln max{φ̄(wn), ε} with ε = 2.22 · 10−16.\nTo find optimal values for the rate distortion trade-off parameter β, we conducted pilot studies with small networks comprising 529 neurons per hidden layer that were trained for only 50 epochs on the MNIST training set according to the aforementioned training scheme and subsequently evaluated on the MNIST test set. While this might induce overfitting of β on the test set in the small networks, we used the same β-values as a heuristic for all larger architectures and did not tune the hyperparameter any further. In global rate distortion regularization (Grdi), the best test error was achieved around β = 0.2 although Grdi seems to behave rather robust in the range β ∈ [0; 0.8]—see middle\nleft panel in Figure 1. In local rate distortion regularization (Lrdi), the best test error was achieved for β = 0 with ordinary utility maximization without regularization—see middle right panel. However, when measuring the performance in terms of expected utility on the test set, Lrdi achieved a significant performance increase compared to ordinary utility maximization in the range β ∈ [10−5, 5·10−4]—see upper right panel. In our final studies, we could furthermore ascertain that Lrdi performs reasonably well on larger architectures as it achieved a test error of 1.26% compared to 1.43% in ordinary utility maximization when increasing the number of units per hidden layer to 4096.\nThe results of our final studies where we trained networks for 70 epochs are illustrated in Table 1 which compares rate distortion regularization to other techniques from the literature for different network architectures comprising two hidden layers. It can be seen that both local and global rate distortion regularization (Lrdi and Grdi respectively) attain results in the permutation invariant setting (Lrdi: 1,26%, Grdi: 1.11%) that are competitive with other recent techniques like dropout (1.01% [33] and 1.28% [32]), dropconnect (1.20% [32]) and Bayes by backprop (1.32% [10]). It is furthermore shown that both rate distortion regularizers lead to a decreasing generalization error when increasing the number of neurons in hidden layers which demonstrates successful prevention of overfitting. Successful prevention of overfitting is additionally demonstrated by applying global rate distortion regularization (Grid, β = 0.2) to a convolutional neural network with an architecture according to [32]—see Section B.2 in [32]—attaining an error of 0.61% without tuning any hyperparameters (see Table 2). This result is also competitive with other recent techniques in the permutation non-invariant setting— compare to dropout (0.59% [32]) and dropconnect (0.63% [32]). In line with [33], we preprocessed the input with ZCA whitening and added a max-norm regularizer to limit the size of presynaptic weight vectors to at most 3.5.\nThe lower panels of Figure 1 show the development of the test set error over epochs for both rate distortion regularizers (red) compared to ordinary utility maximization without regularization (Umax, black) for the different network architectures that we used in the permutation invariant setting. It can be seen that the global variant of our regularizer (Grdi with β = 0.2, see lower left panel in Figure 1) leads to a significant increase in performance across different architectures as demonstrated by the two separate clusters of trajectories. In addition, Grdi also leads to faster learning as the red trajectories in the lower left panel of Figure 1 decrease significantly faster then the black trajectories during the first ten epochs of training. For the local variant of our regularizer (Lrdi with β = 10−5, see lower right panel in Figure 1), the performance improvements are less prominent when compared to the global variant."
    }, {
      "heading" : "5 CONCLUSION",
      "text" : "Previously, a synaptic weight update rule for a single reward-maximizing spiking neuron was devised, where the neuron was interpreted as a bounded rational decisionmaker under limited computational resources with help of rate distortion theory [21]. It was shown that such a bounded rational weight update rule leads to an efficient regularization by preventing synaptic weights from growing without bounds. In our current work, we extend these results to deterministic neurons and neural networks. On the MNIST benchmark classification task, we have demonstrated the regularizing effect of our approach as networks were successfully prevented from overfitting. These results are robust as we conducted experiments with different network architectures achieving performance competitive with other recent techniques like dropout [33], dropconnect [32] and Bayes by backprop [10] for both ordinary and convolutional networks. The strength of rate distortion regularization is that it is a more principled approach than for example dropout and dropconnect as it may be applied to general artificial agents with parameterized policies and not only to neural networks. Parameterized policies that optimize the rate distortion objective have been previously applied to unsupervised density estimation tasks with autoencoder networks [12]. Our current work extends this kind of approach to the theory of reinforcement and supervised learning with feedforward neural networks, and also provides evidence that this approach scales well on large data sets."
    }, {
      "heading" : "A APPENDIX",
      "text" : "A.1 MUTUAL INFORMATION RATE OF A DETERMINISTIC NEURON\nlim ∆t→0\n1\n∆t I(ξ, y)\n= lim ∆t→0\n1\n∆t 〈∑ y pw(y|ξ) ln pw(y|ξ) pw(y) 〉 p(ξ)\n= lim ∆t→0\n1\n∆t\n〈 φ(w>ξ)∆t ln φ(w>ξ)\nφ̄(w) 〉 p(ξ)\n+ lim ∆t→0\n1\n∆t\n〈 (1− φ(w>ξ)∆t) ln 1− φ(w >ξ)∆t\n1− φ̄(w)∆t︸ ︷︷ ︸ →0 〉 p(ξ)\n= 〈 φ(w>ξ) ln φ(w>ξ)\nφ̄(w) 〉 p(ξ) .\n(33)\nA.2 DERIVATIVE OF THE MUTUAL INFORMATION RATE\n∂\n∂wi lim ∆t→0\n1\n∆t I(ξ, y)\n= lim ∆t→0\n1\n∆t\n∂\n∂wi 〈∑ y pw(y|ξ) ln pw(y|ξ) pw(y) 〉 p(ξ)\n= lim ∆t→0\n1\n∆t 〈∑ y ( ∂ ∂wi pw(y|ξ) ) ln pw(y|ξ) pw(y) 〉 p(ξ)\n+ lim ∆t→0\n1\n∆t 〈∑ y pw(y|ξ) ( ∂ ∂wi ln pw(y|ξ) )〉 p(ξ)︸ ︷︷ ︸\n= 〈∑\ny ∂ ∂wi pw(y|ξ) 〉 p(ξ) = ∂∂wi 1=0\n− lim ∆t→0\n1\n∆t 〈∑ y pw(y|ξ) ( ∂ ∂wi ln pw(y) )〉 p(ξ)︸ ︷︷ ︸\n= ∑\ny ∂\n∂wi pw(y)= ∂ ∂wi 1=0\n= lim ∆t→0\n1\n∆t\n〈 ξiφ ′(w>ξ)∆t ln φ(w>ξ)\nφ̄(w) 〉 p(ξ)\n− lim ∆t→0\n1\n∆t\n〈 ξiφ ′(w>ξ)∆t ln\n1− φ(w>ξ)∆t 1− φ̄(w)∆t︸ ︷︷ ︸ →0 〉 p(ξ)\n= 〈 ξiφ ′(w>ξ) ln φ(w>ξ)\nφ̄(w) 〉 p(ξ) .\n(34)\nA.3 DERIVATIVE OF THE GLOBAL MUTUAL INFORMATION\n∂\n∂wni I(ξ,y)\n= ∂\n∂wni 〈∑ y pW(y|ξ) ln pW(y|ξ) pW(y) 〉 p(ξ)\n= 〈∑ y ( ∂ ∂wni pW(y|ξ) ) ln pW(y|ξ) pW(y) 〉 p(ξ)\n+ 〈∑ y pW(y|ξ) ( ∂ ∂wni ln pW(y|ξ) )〉 p(ξ)︸ ︷︷ ︸\n= 〈∑\ny ∂ ∂wn i pW(y|ξ) 〉 p(ξ) = ∂ ∂wn i 1=0\n− 〈∑ y pW(y|ξ) ( ∂ ∂wni ln pW(y) )〉 p(ξ)︸ ︷︷ ︸\n= ∑\ny ∂\n∂wn i pW(y)=\n∂ ∂wn\ni 1=0\n= 〈∑ j ( ∂ ∂wni fj(W, ξ) ) ln fj(W, ξ) f̄j(W) 〉 p(ξ) .\n(35)"
    }, {
      "heading" : "Acknowledgements",
      "text" : "This study was supported by the DFG, Emmy Noether grant BR4164/1-1."
    } ],
    "references" : [ {
      "title" : "Theory of Games and Economic Behavior",
      "author" : [ "J von Neumann", "O Morgenstern" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1944
    }, {
      "title" : "Computational rationality: a converging paradigm for intelligence in brains",
      "author" : [ "S J Gershman", "E J Horvitz", "J B Tenenbaum" ],
      "venue" : "minds, and machines. Science, 349(6245):273–278",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Theories of bounded rationality",
      "author" : [ "H A Simon" ],
      "venue" : "Decision and Organization, 1:161–176",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1972
    }, {
      "title" : "Bounded rationality",
      "author" : [ "T Genewein", "F Leibfried", "J Grau-Moya", "D A Braun" ],
      "venue" : "abstraction and hierarchical decision-making: an information-theoretic optimality principle. Frontiers in Robotics and AI, 2(27)",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Information-theoretic bounded rationality",
      "author" : [ "P A Ortega", "D A Braun", "J Dyer", "K-E Kim", "N Tishby" ],
      "venue" : "arXiv preprint arXiv:1512.06789",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Thermodynamics as a theory of decision-making with informationprocessing costs",
      "author" : [ "P A Ortega", "D A Braun" ],
      "venue" : "Proceedings of the Royal Society A, 469",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2153
    }, {
      "title" : "Rational inattention and monetary economics",
      "author" : [ "C A Sims" ],
      "venue" : "Handbook of Monetary Economics, volume 3, chapter 4. Elsevier",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Information theory - the bridge connecting bounded rational game theory and statistical physics",
      "author" : [ "D H Wolpert" ],
      "venue" : "Complex Engineered Systems, chapter 12. Springer",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Probabilistic choice and procedurally bounded rationality",
      "author" : [ "L G Mattsson", "J W Weibull" ],
      "venue" : "Games and Economic Behavior, 41(1):61–78",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Weight uncertainty in neural networks",
      "author" : [ "C Blundell", "J Cornebise", "K Kavukcuoglu", "D Wierstra" ],
      "venue" : "Proceedings of the 32nd International Conference on Machine Learning",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Lossy is lazy",
      "author" : [ "S Still" ],
      "venue" : "Workshop on Information Theoretic Methods in Science and Engineering, pages 17–21",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Ratedistortion auto-encoders",
      "author" : [ "L G Sanchez Giraldo", "J C Principe" ],
      "venue" : " arXiv preprint arXiv:1312.7381",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Optimal control as a graphical model inference problem",
      "author" : [ "H J Kappen", "V Gómez", "M Opper" ],
      "venue" : "Machine Learning, 87(2):159–182",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "On stochastic optimal control and reinforcement learning by approximate inference",
      "author" : [ "K Rawlik", "M Toussaint", "S Vijayakumar" ],
      "venue" : "Proceedings Robotics: Science and Systems",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Trading value and information in MDPs",
      "author" : [ "J Rubin", "O Shamir", "N Tishby" ],
      "venue" : "Decision Making with Imperfect Decision Makers, chapter 3. Springer",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Information theory of decisions and actions",
      "author" : [ "N Tishby", "D Polani" ],
      "venue" : "Perception-Action Cycle, chapter 19. Springer",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "The free-energy principle: a unified brain theory? Nature Reviews Neuroscience",
      "author" : [ "K Friston" ],
      "venue" : "11(2):127– 138",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Relative entropy policy search",
      "author" : [ "J Peters", "K Muelling", "Y Altun" ],
      "venue" : "Proceedings of the National Conference on Artificial Intelligence",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Information-theoretic approach to interactive learning",
      "author" : [ "S Still" ],
      "venue" : "Europhysics Letters, 85(2):28005",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Efficient computation of optimal actions",
      "author" : [ "E Todorov" ],
      "venue" : "Proceedings of the National Academy of Sciences of the United States of America, 106(28):11478–11483",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A reward-maximizing spiking neuron as a bounded rational decision maker",
      "author" : [ "F Leibfried", "D A Braun" ],
      "venue" : "Neural Computation, 27(8):1686–720",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Learning in neural networks by reinforcement of irregular spiking",
      "author" : [ "X Xie", "H S Seung" ],
      "venue" : "Physical Review E, 69(4 Pt 1):041909",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "The information bottleneck method",
      "author" : [ "N Tishby", "F C Pereira", "W Bialek" ],
      "venue" : "Proceedings of the 37th Annual Allerton Conference on Communication, Control and Computing, pages 368–377",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Coding theorems for a discrete source with a fidelity criterion",
      "author" : [ "C E Shannon" ],
      "venue" : "Institute of Radio Engineers, International Convention Record, 7:142–163",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 1959
    }, {
      "title" : "An algorithm for computing the capacity of arbitrary discrete memoryless channels",
      "author" : [ "S Arimoto" ],
      "venue" : "IEEE Transactions on Information Theory, 18(1):14–20",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 1972
    }, {
      "title" : "Computation of channel capacity and ratedistortion functions",
      "author" : [ "R Blahut" ],
      "venue" : "IEEE Transactions on Information Theory, 18(4):460–473",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 1972
    }, {
      "title" : "On the computation of rate-distortion functions",
      "author" : [ "I. Csiszar" ],
      "venue" : "IEEE Transactions on Information Theory, 20(1):122–124",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 1974
    }, {
      "title" : "Efficient backprop",
      "author" : [ "Y LeCun", "L Bottou", "G B Orr", "K R Müller" ],
      "venue" : "Neural Networks: Tricks of the Trade, volume 1524, chapter 2, pages 9–50. Springer Berlin Heidelberg",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Deep sparse rectifier neural networks",
      "author" : [ "X Glorot", "A Bordes", "Y Bengio" ],
      "venue" : "Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pages 315–323",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Torch7: a matlab-like environment for machine learning",
      "author" : [ "R Collobert", "K Kavukcuoglu", "C Farabet" ],
      "venue" : "BigLearn, NIPS Workshop. No. EPFL-CONF- 192376",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Best practices for convolutional neural networks applied to visual document analysis",
      "author" : [ "P Y Simard", "D Steinkraus", "J C Platt" ],
      "venue" : "Proceedings of the Seventh International Conference on Document Analysis and Recognition",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Regularization of neural networks using dropconnect",
      "author" : [ "L Wan", "M Zeiler", "S Zhang", "Y LeCun", "R Fergus" ],
      "venue" : "Proceedings of the 30th International Conference on Machine Learning",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Dropout : a simple way to prevent neural networks from overfitting",
      "author" : [ "N Srivastava", "G E Hinton", "A Krizhevsky", "I Sutskever", "R Salakhutdinov" ],
      "venue" : "Journal of Machine Learning Research, 15:1929–1958",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "An important idea, originating from the foundations of decision theory, is the principle of maximum expected utility [1].",
      "startOffset" : 117,
      "endOffset" : 120
    }, {
      "referenceID" : 1,
      "context" : "One way of taking computational resources into account is to study optimal decisionmaking under information-processing constraints [2, 3].",
      "startOffset" : 131,
      "endOffset" : 137
    }, {
      "referenceID" : 2,
      "context" : "One way of taking computational resources into account is to study optimal decisionmaking under information-processing constraints [2, 3].",
      "startOffset" : 131,
      "endOffset" : 137
    }, {
      "referenceID" : 3,
      "context" : "In this study, we use an information-theoretic model of bounded rational decision-making [4, 5, 6] that has precursors in the economic literature [7, 8, 9] and that is closely related to recent advances harnessing information theory for machine learning and perception-action systems [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20].",
      "startOffset" : 89,
      "endOffset" : 98
    }, {
      "referenceID" : 4,
      "context" : "In this study, we use an information-theoretic model of bounded rational decision-making [4, 5, 6] that has precursors in the economic literature [7, 8, 9] and that is closely related to recent advances harnessing information theory for machine learning and perception-action systems [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20].",
      "startOffset" : 89,
      "endOffset" : 98
    }, {
      "referenceID" : 5,
      "context" : "In this study, we use an information-theoretic model of bounded rational decision-making [4, 5, 6] that has precursors in the economic literature [7, 8, 9] and that is closely related to recent advances harnessing information theory for machine learning and perception-action systems [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20].",
      "startOffset" : 89,
      "endOffset" : 98
    }, {
      "referenceID" : 6,
      "context" : "In this study, we use an information-theoretic model of bounded rational decision-making [4, 5, 6] that has precursors in the economic literature [7, 8, 9] and that is closely related to recent advances harnessing information theory for machine learning and perception-action systems [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20].",
      "startOffset" : 146,
      "endOffset" : 155
    }, {
      "referenceID" : 7,
      "context" : "In this study, we use an information-theoretic model of bounded rational decision-making [4, 5, 6] that has precursors in the economic literature [7, 8, 9] and that is closely related to recent advances harnessing information theory for machine learning and perception-action systems [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20].",
      "startOffset" : 146,
      "endOffset" : 155
    }, {
      "referenceID" : 8,
      "context" : "In this study, we use an information-theoretic model of bounded rational decision-making [4, 5, 6] that has precursors in the economic literature [7, 8, 9] and that is closely related to recent advances harnessing information theory for machine learning and perception-action systems [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20].",
      "startOffset" : 146,
      "endOffset" : 155
    }, {
      "referenceID" : 9,
      "context" : "In this study, we use an information-theoretic model of bounded rational decision-making [4, 5, 6] that has precursors in the economic literature [7, 8, 9] and that is closely related to recent advances harnessing information theory for machine learning and perception-action systems [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20].",
      "startOffset" : 284,
      "endOffset" : 328
    }, {
      "referenceID" : 10,
      "context" : "In this study, we use an information-theoretic model of bounded rational decision-making [4, 5, 6] that has precursors in the economic literature [7, 8, 9] and that is closely related to recent advances harnessing information theory for machine learning and perception-action systems [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20].",
      "startOffset" : 284,
      "endOffset" : 328
    }, {
      "referenceID" : 11,
      "context" : "In this study, we use an information-theoretic model of bounded rational decision-making [4, 5, 6] that has precursors in the economic literature [7, 8, 9] and that is closely related to recent advances harnessing information theory for machine learning and perception-action systems [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20].",
      "startOffset" : 284,
      "endOffset" : 328
    }, {
      "referenceID" : 12,
      "context" : "In this study, we use an information-theoretic model of bounded rational decision-making [4, 5, 6] that has precursors in the economic literature [7, 8, 9] and that is closely related to recent advances harnessing information theory for machine learning and perception-action systems [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20].",
      "startOffset" : 284,
      "endOffset" : 328
    }, {
      "referenceID" : 13,
      "context" : "In this study, we use an information-theoretic model of bounded rational decision-making [4, 5, 6] that has precursors in the economic literature [7, 8, 9] and that is closely related to recent advances harnessing information theory for machine learning and perception-action systems [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20].",
      "startOffset" : 284,
      "endOffset" : 328
    }, {
      "referenceID" : 14,
      "context" : "In this study, we use an information-theoretic model of bounded rational decision-making [4, 5, 6] that has precursors in the economic literature [7, 8, 9] and that is closely related to recent advances harnessing information theory for machine learning and perception-action systems [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20].",
      "startOffset" : 284,
      "endOffset" : 328
    }, {
      "referenceID" : 15,
      "context" : "In this study, we use an information-theoretic model of bounded rational decision-making [4, 5, 6] that has precursors in the economic literature [7, 8, 9] and that is closely related to recent advances harnessing information theory for machine learning and perception-action systems [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20].",
      "startOffset" : 284,
      "endOffset" : 328
    }, {
      "referenceID" : 16,
      "context" : "In this study, we use an information-theoretic model of bounded rational decision-making [4, 5, 6] that has precursors in the economic literature [7, 8, 9] and that is closely related to recent advances harnessing information theory for machine learning and perception-action systems [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20].",
      "startOffset" : 284,
      "endOffset" : 328
    }, {
      "referenceID" : 17,
      "context" : "In this study, we use an information-theoretic model of bounded rational decision-making [4, 5, 6] that has precursors in the economic literature [7, 8, 9] and that is closely related to recent advances harnessing information theory for machine learning and perception-action systems [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20].",
      "startOffset" : 284,
      "endOffset" : 328
    }, {
      "referenceID" : 18,
      "context" : "In this study, we use an information-theoretic model of bounded rational decision-making [4, 5, 6] that has precursors in the economic literature [7, 8, 9] and that is closely related to recent advances harnessing information theory for machine learning and perception-action systems [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20].",
      "startOffset" : 284,
      "endOffset" : 328
    }, {
      "referenceID" : 19,
      "context" : "In this study, we use an information-theoretic model of bounded rational decision-making [4, 5, 6] that has precursors in the economic literature [7, 8, 9] and that is closely related to recent advances harnessing information theory for machine learning and perception-action systems [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20].",
      "startOffset" : 284,
      "endOffset" : 328
    }, {
      "referenceID" : 20,
      "context" : "Previously, this information-theoretic bounded rationality model was applied to derive a synaptic weight update rule for a single reward-maximizing spiking neuron [21].",
      "startOffset" : 163,
      "endOffset" : 167
    }, {
      "referenceID" : 21,
      "context" : "The bounded rational weight update rule furthermore generalizes the synaptic weight update rule for an ordinary reward-maximizing spiking neuron as presented for example in [22].",
      "startOffset" : 173,
      "endOffset" : 177
    }, {
      "referenceID" : 9,
      "context" : "Mathematically, this informational cost is given by the Kullback-Leibler divergence DKL(p(y)||p0(y)) ≤ B between prior and posterior strategy, where computational resources are modeled as an upper bound B ≥ 0 [10, 5, 6, 13, 14, 15, 17, 18, 19, 20, 8, 9].",
      "startOffset" : 209,
      "endOffset" : 253
    }, {
      "referenceID" : 4,
      "context" : "Mathematically, this informational cost is given by the Kullback-Leibler divergence DKL(p(y)||p0(y)) ≤ B between prior and posterior strategy, where computational resources are modeled as an upper bound B ≥ 0 [10, 5, 6, 13, 14, 15, 17, 18, 19, 20, 8, 9].",
      "startOffset" : 209,
      "endOffset" : 253
    }, {
      "referenceID" : 5,
      "context" : "Mathematically, this informational cost is given by the Kullback-Leibler divergence DKL(p(y)||p0(y)) ≤ B between prior and posterior strategy, where computational resources are modeled as an upper bound B ≥ 0 [10, 5, 6, 13, 14, 15, 17, 18, 19, 20, 8, 9].",
      "startOffset" : 209,
      "endOffset" : 253
    }, {
      "referenceID" : 12,
      "context" : "Mathematically, this informational cost is given by the Kullback-Leibler divergence DKL(p(y)||p0(y)) ≤ B between prior and posterior strategy, where computational resources are modeled as an upper bound B ≥ 0 [10, 5, 6, 13, 14, 15, 17, 18, 19, 20, 8, 9].",
      "startOffset" : 209,
      "endOffset" : 253
    }, {
      "referenceID" : 13,
      "context" : "Mathematically, this informational cost is given by the Kullback-Leibler divergence DKL(p(y)||p0(y)) ≤ B between prior and posterior strategy, where computational resources are modeled as an upper bound B ≥ 0 [10, 5, 6, 13, 14, 15, 17, 18, 19, 20, 8, 9].",
      "startOffset" : 209,
      "endOffset" : 253
    }, {
      "referenceID" : 14,
      "context" : "Mathematically, this informational cost is given by the Kullback-Leibler divergence DKL(p(y)||p0(y)) ≤ B between prior and posterior strategy, where computational resources are modeled as an upper bound B ≥ 0 [10, 5, 6, 13, 14, 15, 17, 18, 19, 20, 8, 9].",
      "startOffset" : 209,
      "endOffset" : 253
    }, {
      "referenceID" : 16,
      "context" : "Mathematically, this informational cost is given by the Kullback-Leibler divergence DKL(p(y)||p0(y)) ≤ B between prior and posterior strategy, where computational resources are modeled as an upper bound B ≥ 0 [10, 5, 6, 13, 14, 15, 17, 18, 19, 20, 8, 9].",
      "startOffset" : 209,
      "endOffset" : 253
    }, {
      "referenceID" : 17,
      "context" : "Mathematically, this informational cost is given by the Kullback-Leibler divergence DKL(p(y)||p0(y)) ≤ B between prior and posterior strategy, where computational resources are modeled as an upper bound B ≥ 0 [10, 5, 6, 13, 14, 15, 17, 18, 19, 20, 8, 9].",
      "startOffset" : 209,
      "endOffset" : 253
    }, {
      "referenceID" : 18,
      "context" : "Mathematically, this informational cost is given by the Kullback-Leibler divergence DKL(p(y)||p0(y)) ≤ B between prior and posterior strategy, where computational resources are modeled as an upper bound B ≥ 0 [10, 5, 6, 13, 14, 15, 17, 18, 19, 20, 8, 9].",
      "startOffset" : 209,
      "endOffset" : 253
    }, {
      "referenceID" : 19,
      "context" : "Mathematically, this informational cost is given by the Kullback-Leibler divergence DKL(p(y)||p0(y)) ≤ B between prior and posterior strategy, where computational resources are modeled as an upper bound B ≥ 0 [10, 5, 6, 13, 14, 15, 17, 18, 19, 20, 8, 9].",
      "startOffset" : 209,
      "endOffset" : 253
    }, {
      "referenceID" : 7,
      "context" : "Mathematically, this informational cost is given by the Kullback-Leibler divergence DKL(p(y)||p0(y)) ≤ B between prior and posterior strategy, where computational resources are modeled as an upper bound B ≥ 0 [10, 5, 6, 13, 14, 15, 17, 18, 19, 20, 8, 9].",
      "startOffset" : 209,
      "endOffset" : 253
    }, {
      "referenceID" : 8,
      "context" : "Mathematically, this informational cost is given by the Kullback-Leibler divergence DKL(p(y)||p0(y)) ≤ B between prior and posterior strategy, where computational resources are modeled as an upper bound B ≥ 0 [10, 5, 6, 13, 14, 15, 17, 18, 19, 20, 8, 9].",
      "startOffset" : 209,
      "endOffset" : 253
    }, {
      "referenceID" : 3,
      "context" : "Limited computational resources are modeled through an upper bound B ≥ 0 on the expected Kullback-Leibler divergence 〈DKL(p(y|x)||p0(y))〉p(x) ≤ B between the strategies p(y|x) and a common prior p0(y), averaged over all possible environments described by the distribution p(x) [4, 15].",
      "startOffset" : 277,
      "endOffset" : 284
    }, {
      "referenceID" : 14,
      "context" : "Limited computational resources are modeled through an upper bound B ≥ 0 on the expected Kullback-Leibler divergence 〈DKL(p(y|x)||p0(y))〉p(x) ≤ B between the strategies p(y|x) and a common prior p0(y), averaged over all possible environments described by the distribution p(x) [4, 15].",
      "startOffset" : 277,
      "endOffset" : 284
    }, {
      "referenceID" : 22,
      "context" : "It can be shown that the most economic prior p0(y) is given by the marginal distribution p0(y) = p(y) = ∑ x p(y|x)p(x), because the marginal distribution minimizes the expected KullbackLeibler divergence for a given set of conditional distributions p(y|x)—see [23].",
      "startOffset" : 260,
      "endOffset" : 264
    }, {
      "referenceID" : 3,
      "context" : "In this case, the expected KullbackLeibler divergence becomes identical to the mutual information I(x, y) between the environment x and the action y [4, 21, 11, 12, 7, 16].",
      "startOffset" : 149,
      "endOffset" : 171
    }, {
      "referenceID" : 20,
      "context" : "In this case, the expected KullbackLeibler divergence becomes identical to the mutual information I(x, y) between the environment x and the action y [4, 21, 11, 12, 7, 16].",
      "startOffset" : 149,
      "endOffset" : 171
    }, {
      "referenceID" : 10,
      "context" : "In this case, the expected KullbackLeibler divergence becomes identical to the mutual information I(x, y) between the environment x and the action y [4, 21, 11, 12, 7, 16].",
      "startOffset" : 149,
      "endOffset" : 171
    }, {
      "referenceID" : 11,
      "context" : "In this case, the expected KullbackLeibler divergence becomes identical to the mutual information I(x, y) between the environment x and the action y [4, 21, 11, 12, 7, 16].",
      "startOffset" : 149,
      "endOffset" : 171
    }, {
      "referenceID" : 6,
      "context" : "In this case, the expected KullbackLeibler divergence becomes identical to the mutual information I(x, y) between the environment x and the action y [4, 21, 11, 12, 7, 16].",
      "startOffset" : 149,
      "endOffset" : 171
    }, {
      "referenceID" : 15,
      "context" : "In this case, the expected KullbackLeibler divergence becomes identical to the mutual information I(x, y) between the environment x and the action y [4, 21, 11, 12, 7, 16].",
      "startOffset" : 149,
      "endOffset" : 171
    }, {
      "referenceID" : 23,
      "context" : "which is mathematically equivalent to the rate distortion problem from information theory [24].",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 24,
      "context" : "This procedure is known as Blahut-Arimoto algorithm [25, 26] and is guaranteed to converge to a global optimum [27] presupposed that q(y) does not assign zero probability mass to any y.",
      "startOffset" : 52,
      "endOffset" : 60
    }, {
      "referenceID" : 25,
      "context" : "This procedure is known as Blahut-Arimoto algorithm [25, 26] and is guaranteed to converge to a global optimum [27] presupposed that q(y) does not assign zero probability mass to any y.",
      "startOffset" : 52,
      "endOffset" : 60
    }, {
      "referenceID" : 26,
      "context" : "This procedure is known as Blahut-Arimoto algorithm [25, 26] and is guaranteed to converge to a global optimum [27] presupposed that q(y) does not assign zero probability mass to any y.",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 20,
      "context" : "We therefore assume a parameterized form of the strategy pw(y|x), from which the decision-maker can draw samples y for a given sample of the environment x, and optimize the rate distortion objective from Equation (6) with help of gradient ascent [21]—also referred to as policy gradient in the reinforcement learning literature [22].",
      "startOffset" : 246,
      "endOffset" : 250
    }, {
      "referenceID" : 21,
      "context" : "We therefore assume a parameterized form of the strategy pw(y|x), from which the decision-maker can draw samples y for a given sample of the environment x, and optimize the rate distortion objective from Equation (6) with help of gradient ascent [21]—also referred to as policy gradient in the reinforcement learning literature [22].",
      "startOffset" : 328,
      "endOffset" : 332
    }, {
      "referenceID" : 20,
      "context" : "Previously, Equation (11) was applied to a single spiking neuron that was stochastic [21].",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 20,
      "context" : "A stochastic neuron may be considered as a bounded rational decision-maker [21]: the neuron’s presynaptic input is",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 20,
      "context" : "where τ is a constant defining the time horizon [21].",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 20,
      "context" : "1 [21].",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 20,
      "context" : "By averaging over the binary quantity y, a more concise weight update rule is derived [21]:",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 27,
      "context" : "The derivative of the utility function with respect to the weight ∂ ∂wn i U(ξ , f(W, ξ)) can be straightforwardly derived via ordinary backpropagation [28].",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 27,
      "context" : "Note that the derivative of the rate distortion objective ∂ ∂wn i L(W) takes a convenient form which can be easily computed by extending ordinary backpropagation [28].",
      "startOffset" : 162,
      "endOffset" : 166
    }, {
      "referenceID" : 28,
      "context" : "For all our simulations, we used a network with two hidden layers of rectified linear units [29] and a top layer of 10 softmax units implemented in Lua with Torch [30].",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 29,
      "context" : "For all our simulations, we used a network with two hidden layers of rectified linear units [29] and a top layer of 10 softmax units implemented in Lua with Torch [30].",
      "startOffset" : 163,
      "endOffset" : 167
    }, {
      "referenceID" : 30,
      "context" : "We chose as optimization criterion the negative cross entropy between the class labels and the network output [31]",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 9,
      "context" : "Table 1: Classification Errors on the MNIST Test Set in the Permutation Invariant Setup Method #neu Error [%] Bayes by backprop [10] 1200 1.",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 31,
      "context" : "32 Dropout [32] 800 1.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 31,
      "context" : "28 Dropconnect [32] 800 1.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 32,
      "context" : "20 Dropout [33] 4096 1.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 32,
      "context" : "01% [33] and 1.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 31,
      "context" : "28% [32]), dropconnect (1.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 31,
      "context" : "20% [32]) and Bayes by backprop (1.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 9,
      "context" : "32% [10]).",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 31,
      "context" : "2) to a convolutional neural network with an architecture according to [32]—see Section B.",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 31,
      "context" : "2 in [32]—attaining an error of 0.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 31,
      "context" : "59% [32]) and dropconnect (0.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 31,
      "context" : "63% [32]).",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 32,
      "context" : "In line with [33], we preprocessed the input with ZCA whitening and added a max-norm regularizer to limit the size of presynaptic weight vectors to at most 3.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 31,
      "context" : "Table 2: Classification Errors on the MNIST Test Set in the Permutation Non-Invariant Setup Method Error [%] Conv net + Dropconnect [32] 0.",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 31,
      "context" : "61 Conv net + Dropout [32] 0.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 20,
      "context" : "Previously, a synaptic weight update rule for a single reward-maximizing spiking neuron was devised, where the neuron was interpreted as a bounded rational decisionmaker under limited computational resources with help of rate distortion theory [21].",
      "startOffset" : 244,
      "endOffset" : 248
    }, {
      "referenceID" : 32,
      "context" : "These results are robust as we conducted experiments with different network architectures achieving performance competitive with other recent techniques like dropout [33], dropconnect [32] and Bayes by backprop [10] for both ordinary and convolutional networks.",
      "startOffset" : 166,
      "endOffset" : 170
    }, {
      "referenceID" : 31,
      "context" : "These results are robust as we conducted experiments with different network architectures achieving performance competitive with other recent techniques like dropout [33], dropconnect [32] and Bayes by backprop [10] for both ordinary and convolutional networks.",
      "startOffset" : 184,
      "endOffset" : 188
    }, {
      "referenceID" : 9,
      "context" : "These results are robust as we conducted experiments with different network architectures achieving performance competitive with other recent techniques like dropout [33], dropconnect [32] and Bayes by backprop [10] for both ordinary and convolutional networks.",
      "startOffset" : 211,
      "endOffset" : 215
    }, {
      "referenceID" : 11,
      "context" : "Parameterized policies that optimize the rate distortion objective have been previously applied to unsupervised density estimation tasks with autoencoder networks [12].",
      "startOffset" : 163,
      "endOffset" : 167
    } ],
    "year" : 2016,
    "abstractText" : "Bounded rational decision-makers transform sensory input into motor output under limited computational resources. Mathematically, such decision-makers can be modeled as informationtheoretic channels with limited transmission rate. Here, we apply this formalism for the first time to multilayer feedforward neural networks. We derive synaptic weight update rules for two scenarios, where either each neuron is considered as a bounded rational decision-maker or the network as a whole. In the update rules, bounded rationality translates into information-theoretically motivated types of regularization in weight space. In experiments on the MNIST benchmark classification task for handwritten digits, we show that such information-theoretic regularization successfully prevents overfitting across different architectures and attains results that are competitive with other recent techniques like dropout, dropconnect and Bayes by backprop, for both ordinary and convolutional neural networks.",
    "creator" : "LaTeX with hyperref package"
  }
}