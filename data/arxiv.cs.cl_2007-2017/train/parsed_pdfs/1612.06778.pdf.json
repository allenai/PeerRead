{
  "name" : "1612.06778.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Text classification with sparse composite document vectors",
    "authors" : [ "Dheeraj Mekala", "Vivek Gupta", "Harish Karnick" ],
    "emails" : [ "dheerajm@iitk.ac.in", "t-vigu@microsoft.com", "hk@iitk.ac.in" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Text classification and text clustering are widely used in various information retrieval and natural language processing tasks (Moulinier et al., 1996). Various machine learning algorithms are used to perform these tasks. They require text data to be represented as a fixed dimension D floating point vector. Some of the most common techniques used are described in the next section along with their problems."
    }, {
      "heading" : "2 Related Work",
      "text" : "Le. and Mikolov et. al.(Le and Mikolov, 2014) proposed two models for Distributional representation of document as fixed dimensional vectors\n*Represents equal contribution Submitted to proceedings of EACL 2017\ncalled Distributed Memory Model Paragraph Vectors (PV-DM) (Le and Mikolov, 2014) and Distributed BoWs paragraph vectors (PV-DBoW) (Le and Mikolov, 2014). However, they do not perform very well in document categorization due to multiple reasons as stated in (Vivek Gupta, 2016)."
    }, {
      "heading" : "2.1 Composite Document Representation",
      "text" : "Recently much efforts has gone into composing a document vector from word vectors. Some of the important methods are described below:"
    }, {
      "heading" : "2.1.1 Weighted Average Embedding",
      "text" : "Mukerjee et al. (Pranjal Singh, 2015) proposed a weighted average composite document vectors in which vectors for words appearing in the document are added after weighting them with idf (Robertson, 2004) values from the training set. This method tries to capture the relative importance of words by varying weights.\nThis method assumes that all words within a document have the same semantic topic. Intuitively a paragraph often has words coming from multiple semantically different topics."
    }, {
      "heading" : "2.1.2 Topical Word Embedding",
      "text" : "(Liu et al., 2015b) proposed three novel composite document representations called Topical word embedding (TWE-1,TWE-2 and TWE-3). For each word in the vocabulary, word-topic assignments are obtained through Latent Dirichlet Allocation (Blei et al., 2003). TWE-1 learns word and topic embedding by considering each topic as pseudo word and builds the topical word embedding for each word-topic assignment. TWE-2 learns topical word embedding of each word-topic assignment directly by considering each word-topic pair as a pseudo word. For each word and each topic, TWE-3 builds distinct embeddings for the topic and word separately and for each word-topic asar X\niv :1\n61 2.\n06 77\n8v 1\n[ cs\n.C L\n] 2\n0 D\nec 2\n01 6\nsignment, the corresponding word embedding and topic embedding are concatenated to form a topical word embedding which is used for further learning. Architecture for same is shown in Figure 1 TWE-1 outperforms other embedding methods on 20 newsgroup data-set.\n(Liu et al., 2015a) proposed an architecture called Neural tensor skip-gram model (NTSG1, NTSG-2, NTSG-3, NTSG-4), to learn multiprototype word embeddings and uses a tensor layer to model the interaction of words and topics. NTSG outperforms other embedding methods like TWE-1 on 20 newsgroup data-set.\nIn TWE-1, the interaction between a word and the topic to which it is assigned is not considered. In TWE-2, the interaction between a word and its assigned topic is considered. As each word is differentiated into different topics, there are sparsity issues. TWE-3 stands between both TWE-1 and TWE-2 with respect to differentiation and sparsity. In TWE-3, the word embeddings are influenced by the corresponding topic embeddings, making words in the same topic less discriminative. Since, TWE uses LDA it suffers from computational issues like large training and prediction time and requires more storage."
    }, {
      "heading" : "2.1.3 Graded weighted Bag of Word Vectors",
      "text" : "(Vivek Gupta, 2016) proposed a method to form a composite document vector using word vectors and tf-idf values called Graded Weighted Bag of Words Vector (gwBoWV). In gwBoWV, each document is represented by a vector of dimension D = K ∗ d + K, where K represents number of clusters, d is the dimension of the word\nvectors. The basic idea is, semantically different words belong to different clusters and their word vectors should not be averaged, hence concatenated. Further, gwBoWV concatenates inverse cluster frequency of each cluster (icf) which is computed by idf values of all words in a cluster to capture the importance of words across documents. (Vivek Gupta, 2016) shows gwBoWV outperforms paragraph vector models on a hierarchical product classification task.\ngwBoWV is a non-sparse high dimensional continuous vector and thus suffers computational issues like large training and prediction time and takes considerable storage. This makes them impractical for many large datasets."
    }, {
      "heading" : "3 Sparse Document Vectors",
      "text" : "In this section we would present the main contributions of the paper. We modify gwBoWV to incorporate sparsity, leading to faster computation time and better results. We also modify the weighting scheme by directly multiplying idf values with word vectors leading to off-store computation and significant improvement in time and space complexity. We call our modified document representation as Sparse Document Vector (sdv). Compared to gwBoWV (Vivek Gupta, 2016), sdv has the following changes and major advantages:\n1. We replace hard clustering (K-means) with soft clustering algorithm(GMM) so that each word can belong to multiple topics, thus handling polysemy. Lots of work had been done earlier to handle polysemy for word embeddings (Huang et al., 2012) but not on its effects in document representation.\n2. We introduce sparsity in the feature vector by zeroing attribute values that are < 5% of average attribute values, which leads to reduction in feature vector size, training and prediction time (Linear SVM) by significant factors as shown in Figure 2\n3. Instead of separately concatenating cluster frequency as in gwBoWV, we weight each word vector of a word by its inverse document frequency while forming cluster vectors. This leads to significant reduction in feature formation time due to off-loop precomputation.\nThe full details are in Algorithm 1.\nAlgorithm 1: Sparse Document Vector Data: Documents Dn, n = 1 . . . N Result: Document vectors ~sdvDn , n = 1 . . . N\n1 Obtain word vector ( ~wvi), for each word wi; 2 Calculate idf values, idf(wi), i = 1..|V | ; /* |V | is vocabulary size */ 3 Cluster word vectors ~wv using GMM clustering into K clusters; 4 Obtain soft assignment P (ck|wi) for word wi and cluster ck; /* Loop 5-10 can be pre-computed */ 5 for each word wi in vocabulary V do 6 for each cluster ck do 7 ~wcvik = ~wvi × P (ck|wi); 8 end 9 ~wtvi = idf(wi) × ⊕K k=1 ~wcvik ;\n/* ⊕ is concatenation */\n10 end 11 for n ∈ (1..N) do 12 Initialize document vector ~dvDn = ~0; 13 for word wi in Dn do 14 ~dvDn += ~wtvi; 15 end 16 ~sdvDn = make-sparse( ~dvDn);\n/* as mentioned in sec 3 */\n17 end\nWord vectors are clustered using soft clustering algorithms (e.g. GMM), thus each word belongs to every cluster with some probability. For each word wi, we create K word-cluster-vectors ( ~wcvik) by multiplying word vector with the probability of word belonging to each cluster. For each word wi, we concatenate all word-cluster vectors ( ~wcvik) and weight it with idf of wi to form a word-topics vector ( ~wtvi).\nWe use idf values from training corpus directly for the test corpus for weighting. Finally, for each word wi appearing in the document Dn, we sum word-topics vector ~wtvi to obtain the document vector ~dvDn . We make document vector ~dvDn sparse as explained in point 2 above resulting in ~sdvDn .\nDuring implementation, for each word wi in the vocabulary V , we can pre-compute the word topics vector ~wtvi which results in significant reduction in computation time as described in Algorithm 1 (line 5-10)."
    }, {
      "heading" : "4 Experiments",
      "text" : "We perform multiple experiments to show the effectiveness of our representation for multi-class and multi-label text classification. We run the multi-class experiments on 20NewsGroup dataset (Lang, ) and multi-label classification experiments on Reuters-21578 dataset (Lewis, ).\nExperimental conditions are as follows: preprocessing: remove stop words; word vector dimension: 200; GMM components: 60 with same spherical co-variance matrix for all components; classifier multi-class: linear SVM; classifier multilabel: logistic regression, OneVsRest setting; platform: Intel(R) Xeon(R) CPU E5-2670 v2 @ 2.50GHz, 40 working cores, 128GB RAM with Linux Ubuntu 14.4, use multiple cores for one-vsrest classifier, Word2vec, and Doc2Vec training in all other cases only a single core.\n4.0.4 Baseline We consider the following baselines, bag-ofwords (BOW) model, Paragraph vector models (Mikolov), Topical word embeddings (TWE) (Liu et al., 2015b), Average word-vector model, where we build document vector by averaging all word vectors in a document and tf-idf weighted average word-vector model, where we build document vector by tf-idf weighted averaging all word vectors in a document. The dimension of word vectors in both Average word-vector and tf-idf weighted word-vector model is 200. The dimension of document vector in paragraph vector models is 400. We use the source code provided by (Liu et al., 2015b) with same experimental parameter settings for TWE models, the number of topics are set to 80 and the dimension of word and topic embeddings is 400. We use gensim(Řehůřek and Sojka, 2010) for implementing Doc2Vec and Word2Vec. For all baselines, we use LinearSVM for multi-class classification and Logistic regres-\nsion in OneVsRest approach for multi-label classification. During training we tune all parameters for baselines and the performance reported is with the best parameters. Code is available here 1."
    }, {
      "heading" : "4.1 Dataset Description",
      "text" : "Multi Class : The training:test samples breakup on 20NewsGroup dataset is 11314 : 7532. Multi Label : The training:test samples breakup on Reuters-21578 dataset is 13734 : 5887. We utilize the script provided by Eustache 2 for preprocessing the Reuters-21578 dataset. Dataset contains 5 main categories and 445 categories in total. Each article is assigned to several categories, thus a multi-label classification."
    }, {
      "heading" : "5 Results",
      "text" : ""
    }, {
      "heading" : "5.1 Multi-class classification",
      "text" : "We evaluate performance using standard metrics like accuracy, macro-averaging precision, recall and F-measure for comparison. Table 1 shows results for 20Newsgroup and table 2 shows the comparison between SDV and NTSG, the current state-of-art on the 20Newsgroup dataset. Com-\npared to TWE, SDV manages to reduce training and prediction times significantly. Table 3 shows a comparison of training and prediction times between gwBoWV, SDV and TWE models. Table 4 shows the space comparison between gwBoWV\n1https://dheeraj7596.github.io/SDV/ 2 https://gist.github.com/herrfz/7967781\nand SDV models. The pictographic comparison between training and prediction times are shown in Figure 3 and 4.\nWe observe that SDV outperforms paragraph vector, average word-vector, tf-idf weighted average word-vector, bag-of-words models by reasonable margins. With respect to accuracy, SDV performs as well as NTSG and NTSG outperforms SDV on precision, recall and f-measure. As topic model learning and document vector formation in NTSG is similar to TWE, SDV is faster than NTSG. Compared to gwBoWV, there is significant reduction in training and prediction times and also in document feature space (by 80%)."
    }, {
      "heading" : "5.2 Multi-label classification",
      "text" : "We evaluate performance using Precision@K, nDCG@k, Coverage error, Label ranking average precision score (LRAPS), weighted F-measure. The first two are taken from Extreme learning repository (Bhatia et al., ) and the next four are defined in Scikit-Learn multilabel loss function class (Pedregosa et al., 2011).\nAs we split data randomly into train and test sets, weighted F-measure is an appropriate metric for multi-label classification as it considers label sample biases as well. Table 5 and 6 shows the evaluation results on multi-label text classification on Reuters-21578 dataset.\nModel Coverage Error LRAPS Weighted F1-Score SDV 7.49 92.06 80.13 gwBoWV 8.16 91.46 79.16 TWE-1 9.03 89.25 74.76 PV-DM 13.15 86.21 70.24 PV-DBOW 11.28 87.43 73.68 AvgVec 9.67 87.28 71.91 tfidf AvgVec 9.42 87.90 71.97\nTable 6: Performance on various metrics II"
    }, {
      "heading" : "6 Time Complexity Analysis",
      "text" : "Let, W = size of vocabulary, N = number of documents, T = number of topics or clusters, C = window size, M = corpus length, KW = word vector length, KT = topic vector length, I = number of\niterations in topic modeling, D = feature vector dimension. LDA time complexity: O(W 2NT ) GMM time complexity: O(NT 2D). Since W 2 >> TD, thus better time complexity. Detailed time complexity comparison is shown in Table 7."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper we modified gwBoWV and reduced overall feature vector computation time by 8.5×, prediction time by 20× and space by 4× on 20newsgroup multi-class dataset. We experimentally demonstrated that in multi-class classification on 20newsgroup dataset, our model outperforms TWE-1 and performs equally well as NTSG (accuracy). Compared to TWE-1 and NTSG, our model is faster by 6× during feature vector computation and by 3× during test class prediction on 20newsgroup. In multi-label classification on Reuters, our model outperforms on every metric by reasonable margins. Overall, we improved gwBoWV by making it simple and efficient and obtained better results on standard multi-class and multi-label datasets."
    } ],
    "references" : [ {
      "title" : "Latent dirichlet allocation",
      "author" : [ "Blei et al.2003] David M. Blei", "Andrew Y. Ng", "Michael I. Jordan" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Blei et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Blei et al\\.",
      "year" : 2003
    }, {
      "title" : "Improving word representations via global context and multiple word prototypes",
      "author" : [ "Huang et al.2012] Eric Huang", "Richard Socher", "Christopher Manning", "Andrew Ng" ],
      "venue" : "In Proceedings of the 50th Annual Meeting of the Association",
      "citeRegEx" : "Huang et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2012
    }, {
      "title" : "Distributed representations of sentences and documents. arXiv preprint arXiv:1405.4053",
      "author" : [ "Le", "Mikolov2014] Quoc V Le", "Tomas Mikolov" ],
      "venue" : null,
      "citeRegEx" : "Le et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Le et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning context-sensitive word embeddings with neural tensor skip-gram model",
      "author" : [ "Liu et al.2015a] Pengfei Liu", "Xipeng Qiu", "Xuanjing Huang" ],
      "venue" : "In Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence",
      "citeRegEx" : "Liu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2015
    }, {
      "title" : "Topical word embeddings",
      "author" : [ "Liu et al.2015b] Yang Liu", "Zhiyuan Liu", "Tat-Seng Chua", "Maosong Sun" ],
      "venue" : null,
      "citeRegEx" : "Liu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2015
    }, {
      "title" : "Text categorization: a symbolic approach",
      "author" : [ "Gailius Raskinis", "J Ganascia" ],
      "venue" : "In proceedings of the fifth annual symposium on document analysis and information retrieval,",
      "citeRegEx" : "Moulinier et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Moulinier et al\\.",
      "year" : 1996
    }, {
      "title" : "Words are not equal: Graded weighting model for building composite document vectors",
      "author" : [ ],
      "venue" : "In Proceedings of the twelfth International Conference on Natural Language Processing (ICON-",
      "citeRegEx" : "Singh.,? \\Q2015\\E",
      "shortCiteRegEx" : "Singh.",
      "year" : 2015
    }, {
      "title" : "Software Framework for Topic Modelling with Large Corpora",
      "author" : [ "Řehůřek", "Sojka2010] Radim Řehůřek", "Petr Sojka" ],
      "venue" : "In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks,",
      "citeRegEx" : "Řehůřek et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Řehůřek et al\\.",
      "year" : 2010
    }, {
      "title" : "Understanding inverse document frequency: on theoretical arguments for idf",
      "author" : [ "Stephen Robertson" ],
      "venue" : "Journal of documentation,",
      "citeRegEx" : "Robertson.,? \\Q2004\\E",
      "shortCiteRegEx" : "Robertson.",
      "year" : 2004
    }, {
      "title" : "Product classification in e-commerce using distributional semantics",
      "author" : [ "Harish Karnick" ],
      "venue" : "In Proceedings of COLING 2016,",
      "citeRegEx" : "Gupta and Karnick.,? \\Q2016\\E",
      "shortCiteRegEx" : "Gupta and Karnick.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "Text classification and text clustering are widely used in various information retrieval and natural language processing tasks (Moulinier et al., 1996).",
      "startOffset" : 127,
      "endOffset" : 151
    }, {
      "referenceID" : 8,
      "context" : "(Pranjal Singh, 2015) proposed a weighted average composite document vectors in which vectors for words appearing in the document are added after weighting them with idf (Robertson, 2004) values from the training set.",
      "startOffset" : 170,
      "endOffset" : 187
    }, {
      "referenceID" : 0,
      "context" : "For each word in the vocabulary, word-topic assignments are obtained through Latent Dirichlet Allocation (Blei et al., 2003).",
      "startOffset" : 105,
      "endOffset" : 124
    }, {
      "referenceID" : 1,
      "context" : "Lots of work had been done earlier to handle polysemy for word embeddings (Huang et al., 2012) but not on its effects in document representation.",
      "startOffset" : 74,
      "endOffset" : 94
    } ],
    "year" : 2017,
    "abstractText" : "In this work, we present a modified feature formation technique gradedweighted Bag of Word Vectors (gwBoWV) by (Vivek Gupta, 2016) for faster and better composite document feature representation. We propose a very simple feature construction algorithm that potentially overcomes many weaknesses in current distributional vector representations and other composite document representation methods widely used for text representation. Through extensive experiments on multi-class classification on 20newsgroup dataset and multi-label text classification on Reuters-21578, we achieve better performance results and also significant reduction in training and prediction time compared to composite document representation methods gwBoWV and TWE(Liu et al., 2015b).",
    "creator" : "LaTeX with hyperref package"
  }
}