{
  "name" : "1302.3283.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "StructBoost: Boosting Methods for Predicting Structured Output Variables",
    "authors" : [ "Chunhua Shen", "Guosheng Lin", "Anton van den Hengel" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms—Boosting, AdaBoost, structured learning, conditional random field, image segmentation, object tracking.\nF\nCONTENTS"
    }, {
      "heading" : "1 Introduction 2",
      "text" : "1.1 Main contributions . . . . . . . . . . . . 2 1.2 Related work . . . . . . . . . . . . . . . 2 1.3 Notation . . . . . . . . . . . . . . . . . . 3"
    }, {
      "heading" : "2 Structured boosting 3",
      "text" : "2.1 1-slack formulation for fast optimization 4 2.2 Cutting-plane optimization for solving\nthe 1-slack primal . . . . . . . . . . . . . 5"
    }, {
      "heading" : "3 Special cases of StructBoost 6",
      "text" : "3.1 Binary classification . . . . . . . . . . . . 6 3.2 Multi-class boosting . . . . . . . . . . . 6 3.3 Hierarchical multi-class classification . . 7 3.4 Ordinal regression and AUC optimiza-\ntion . . . . . . . . . . . . . . . . . . . . . 7 3.5 Optimization of the Pascal image overlap criterion . . . . . . . . . . . . . . . . 7 3.6 StructBoost for CRF parameter learning 8"
    }, {
      "heading" : "4 Experiments 9",
      "text" : "4.1 Binary classification . . . . . . . . . . . . 9 4.2 Ordinal regression and AUC optimiza-\ntion . . . . . . . . . . . . . . . . . . . . . 9 4.3 Multi-class classification . . . . . . . . . 10 4.4 Hierarchical multi-class classification . . 10 4.5 Visual tracking by optimizing the im-\nage area overlap criterion . . . . . . . . 10\nThe authors are with School of Computer Science, and Australian Center for Visual Technologies, The University of Adelaide, Adelaide, SA 5005, Australia. Email: chunhua.shen@adelaide.edu.au.\n4.6 CRF parameter learning for image segmentation . . . . . . . . . . . . . . . . . 14"
    }, {
      "heading" : "5 Conclusion 14",
      "text" : "References 14\nar X\niv :1\n30 2.\n32 83\nv1 [\ncs .L\nG ]\n1 4\nFe b\n20 13\n2"
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Structured learning has attracted extensive attention recently in machine learning and computer vision [1]–[4]. Conventional supervised learning such as classification and regression is the problem of learning a function that predicts the best value for a response variable y ∈ R for an input x by making use of a sample of input-output pairs. In many applications, however, the outputs are often complex and cannot be well represented by a scalar because the classes may have inter-class dependencies, or the classes are objects (vectors, sequences, trees, etc.). These problems are referred to as structured output prediction. Structured support vector machines (SSVM) [4] generalize the multiclass SVM of [5] and [6] to the much broader problem of learning for interdependent and structured outputs. SSVM uses discriminant functions that take advantage of the dependencies and structure of outputs. In SSVM, the general form of the learned discriminant function is F (x,y;w) : X × Y 7→ R over input-output pairs and the prediction is achieved by maximizing F (x,y;w) over all possible y ∈ Y. As in standard SVM, here F (x,y;w) is usually defined by a feature mapping function that is only available in the format of inner production, unless the feature mapping function is linear.\nBoosting algorithms linearly combine a set of moderately accurate weak learners to form a highly accurate strong predictor. Recently, Shen and Hao proposed a direct formulation for multi-class boosting using the loss functions of multi-class SVM [5], [6]. Inspired by the general boosting framework of [7], they implemented multi-class boosting with the column generation technique. Here we go further by generalizing multi-class boosting of Shen and Hao to broad structured out prediction problems. The advantage of the proposed StructBoost over SSVM might be that in some cases, one wants to learn sparse and explicit features for a particular problem. The feature space induced by a nonlinear kernel in SVM—either the standard SVM or SSVM—is usually of large (or even infinite) dimensionality. When the data can be separated with a few features, a kernel-induced feature scheme may still have to use all the features due to the lack of feature selection capability. In contrast, boosting with appropriate weak learners, e.g., decision stumps or decision tress, can select relevant features. In this case, the learning procedure of boosting is also a procedure of feature induction. Moreover, it is in general difficult to derive explicit expressions for kernel-induced features, while boosting’s feature induction procedure explicitly introduces nonlinear features into the learned model. The model learned by boosting is usually simpler and computationally more efficient. This is very important for real-time vision applications like object detection and tracking."
    }, {
      "heading" : "1.1 Main contributions",
      "text" : "Overall, the main contributions of this work are four-fold. • To our knowledge, our StructBoost is the first prac-\ntical boosting method for predicting a broad range of structured outputs. We discuss special cases of\nthis general structured learning framework, including multi-class classification, ordinal regression, optimization of complex measure such as the Pascal image overlap criterion and conditional random field (CRF) parameters learning for image segmentation. • To implement StructBoost, we adapt the efficient cutting-plane method—originally designed for efficient linear SVM training [8]—for our purpose. We equivalently reformulate the m-slack optimization to 1-slack optimization. We demonstrate that even conventional LPBoost [9] can benefit from this reformulation to gain significant speedup in training. • We also introduce a new formulation of multi-class boosting, which can be easily implemented by StructBoost. Experiments show encouraging accuracy with faster training time. Also for the first time, we train multi-class boosting classifiers by considering the hierarchical category structure and optimizing the tree loss. This has potential application in object recognition on datasets like ImageNet1. • We apply the proposed StructBoost to some computer vision applications and show that our StructBoost can indeed advance some important computer vision problems. In particular, we demonstrate a state-ofthe-art object tracker trained by our StructBoost. We also demonstrate an application for CRF and superpixel based image segmentation. We use StructBoost together with graph cuts for CRF parameter learning.\nSince our StructBoost builds upon the fully corrective boosting of Shen and Li [7], it inherits the desirable properties of column generation based boosting, such as a fast convergence rate and a clear explanation from the primal-dual convex optimization perspective."
    }, {
      "heading" : "1.2 Related work",
      "text" : "The two state-of-the-art structured learning methods are CRF [10] and SSVM [4], which captures the interdependency among output variables. The significance of CRF is in the global training for structured prediction as a convex optimization problem. SSVM follows this path but employs a different loss function (hinge loss) and optimization methods. Our StructBoost is directly inspired by SSVM. StructBoost is an extension of boosting methods to structured prediction. It therefore builds upon the work of column generation boosting [7] and the direct formulation for multi-class boosting [11]. Indeed, we show the multiclass boosting of [11] is a special case of the general framework presented here.\nCRF and SSVM have been applied to various problems in machine learning and computer vision mainly because the learned models can easily integrate prior knowledge given a problem of interest. For example, the linear chain CRF widely used in natural language processing estimates sequences of labels for sequences of input samples due to the fact that CRF can take context into account [10], [12].\n1. http://www.image-net.org/\n3 SSVM achieves so based on the joint feature maps over the input-output pairs, where features can be represented equivalently as in CRF [8]. CRF is particularly of interest in computer vision for its success in semantic image segmentation [13]. A critical issue of semantic image segmentation is to integrate local and global features for the prediction of local pixel/segment labels. Semantical segmentation is achieved by exploiting the class information with a CRF model. SSVM can also be used for similar purposes as demonstrated in [14]. Blaschko and Lampert [3] trained SSVM models to predict the bounding box of objects in a given image, by optimizing the Pascal bounding box overlap score. The work in [1] introduced structured learning to real-time object detection and tracking, which also optimizes the Pascal box overlap score. SSVM has also been used to learn statistics that capture the spatial arrangements of various object classes in images [15]. The trained model can then simultaneously predict a structured labeling of the entire image. Based on the idea of large-margin learning in SSVM, Szummer et al. [16] learned optimal parameters of a CRF, avoiding tedious cross validation. The survey of [2] has provided a comprehensive review of structured learning and its application in computer vision. Next we review some boosting attempts to structured prediction.\nThere are a few structured boosting methods in the literature. As we discuss here, none of them is as general and practical as ours. Ratliff et al. [17] proposed boosting for imitation learning based on structured prediction called maximum margin planning (MMP). In the MMPBoost of [17], a demonstrated policy is provided as example behavior for training and the purpose is to learn a function over features of the environment that produce policies with similar behavior. Although MMPBoost is structured learning in that the output is a vector, it differs ours fundamentally. First, MMPBoost is heuristic because the optimization procedure is not directly defined on the joint function F (x,y;w). Second, MMPBoost is based on the idea of gradient descent boosting [18], and our StructBoost is built upon fully corrective boosting of Shen and Li [7]. Most importantly, MMPBoost is specifically designed for the planning problem in robotics. It remains unclear how MMPBoost can be extended to other general structured learning problems.\nParker [19] developed a margin-based structured perceptron update and showed that it can incorporate general notions of misclassification cost as well as kernels. Although it is called structured boosting, Parker assumed that the dictionary (weak learners) are known a priori, and the only variable to optimize is the coefficient w. No weak learner training is involved. Therefore the method in [19] is essentially an online version of SSVM. Wang et al. [20] learned a local predictor using standard methods, e.g., SVM, but then achieved improved structured classification by exploiting the influence of misclassified components after structured prediction, and iteratively re-training the local predictor. Again, this approach is heuristic and it is more like a post-processing procedure—it does not directly optimize the structured learning objective."
    }, {
      "heading" : "1.3 Notation",
      "text" : "A bold lowercase letter (u, v) denotes a column vector. An element-wise inequality between two vectors or matrices like u ≥ v means ui ≥ vi for all i. Let (xi; yi) ∈ X × Y, with X ⊂ Rd. Unlike classification (Y = {1, 2, . . . , k}) or regression (Y = R) problems where yi is either a discrete or real-valued scalar. We are interested in the case where elements of Y are structured variables, e.g., vectors, strings, graphs. We denote F a set of weak learners (dictionary); the size of F can be infinite. Each ~j(·, ·) ∈ F, j = 1 . . . n, is a function that maps an input-output (x,y) pair to {−1,+1}. Although our discussion works for the general case that ~(·, ·) can be any real value, we consider binary weak learners here. Clearly ~(·, ·) plays the same role as the feature representation of inputs and outputs Φ(x,y) in SSVM. We define column vectors h(x,y) = [~1(x,y), ~2(x,y), · · · , ~n(x,y)]> to be the outputs of all weak learners on the training datum x and label y. The discriminant function that we want to learn is then F : X× Y 7→ R over input-output pairs, which has the form of\nF (x,y;w) = w>h(x,y) = ∑ jwj~j(x,y), (1)\nwith w > 0. Analogue to SSVM, the inference step is to maximize the joint compatibility function over the output y:\ny? = argmax y F (x,y;w) = argmax y w>h(x,y). (2)\nWe denote by 1 a column vector of all 1’s, whose dimension should be clear from the context. ‖x‖1 and ‖x‖2 denote the `1 and `2 norms in the vector space, respectively. Next, we explain how StructBoost works in Section 2, including how to efficiently solve the resulting optimization problem. We then highlight a few applications in various domains in Section 3. Experimental results are shown in Section 4 and we conclude the paper in the last section."
    }, {
      "heading" : "2 STRUCTURED BOOSTING",
      "text" : "Before we present the proposed general structured boosting framework, we introduce the general loss for structured learning and then we take a look at some special instances: classification, ordinal regression, optimizing special criteria such as area under the ROC curve and the Pascal image area overlap ratio, and learning CRF parameters using StructBoost.\nTo measure the accuracy of a prediction, as in SSVM, we want to learn with arbitrary loss functions ∆ : Y × Y 7→ R. ∆(y,y′) calculates the loss associated with a prediction y′ against the true label value y. Note that in general we assume ∆(y,y) = 0 and ∆(y,y′) > 0 for any y′ 6= y. We also assume that the loss is upper bounded.\nThe formulation of StructBoost can be written as (m-slack\n4 primal) with the model defined in (1):\nmin w,ξ\n‖w‖1 + Cm 1 >ξ (3a)\ns.t. : w> [ h(xi,yi)− h(xi,y) ] ≥ ∆(yi,y)− ξi,\n∀i = 1, . . . ,m; and ∀y ∈ Y\\yi, (3b) w ≥ 0; ξ ≥ 0. (3c)\nHere we have used the `1 norm as the regularization function to control the complexity of the learned model. To simplify the notation, we introduce δhi(y) = h(xi,yi) − h(xi,y); and the constraints can be re-written as:w>δhi(y) ≥ ∆(yi,y)−ξi. There are two major obstacles to solve problem (3). First, as in conventional boosting, because the possibility of weak learners ~(·, ·) can be exponentially large or even infinite, the dimension of w can be exponentially large or infinite. So in general we are not able to directly solve for w. Second, same as in SSVM, the number of constraints (3b) can be extremely (or infinitely) large. For example, in the case of multi-label or multi-class classification, the label yi can be represented as a binary vector (or string) and clearly the possible number of y such that y 6= yi is exponential in the length of the vector, which is 2|Y|. In other words, problem (3) can have an extremely or infinitely large number of variables as well as constraints. This is much more challenging than solving standard boosting or SSVM from the viewpoint of optimization. In standard boosting, one has a large number of variables and in SSVM, one has a large number of constraints.\nFor the time being, let us put aside the difficulty of the large number of constraints, and focus on how to iteratively solve for w using column generation as in [7], [9]. The Lagrangian of the m-slack primal problem (3) can be written as:\nL = ‖w‖1 + Cm 1 >ξ − ∑ i,y 6=yi λ(i,y) · { w> [ h(xi,yi)−\nh(xi,y) ] −∆(yi,y) + ξi } − ν>w − β>ξ, (4)\nwhere λ,ν,β are Lagrange multipliers: λ ≥ 0,ν ≥ 0,β ≥ 0. We denote by λ(i,y) the Lagrange dual multiplier associated with the margin constraints (3b) for label y 6= yi and training pair (xi,yi). At optimum, the first derivative of the Lagrangian w.r.t. the primal variables must vanish,\n∂L ∂ξi = 0 =⇒ C m − ∑ y 6=yi λ(i,y) − βi = 0\n=⇒ 0 ≤ ∑ y 6=yi λ(i,y) ≤ Cm ;\nand,\n∂L ∂w = 0 =⇒ 1− ∑ i,y 6=yi λ(i,y)δhi(y)− ν = 0\n=⇒ ∑\ni,y 6=yi\nλ(i,y)δhi(y) ≤ 1.\nBy putting them back into the Lagrangian (4) and we can obtain the dual problem of the m-slack formulation in (3):\nmax λ ∑ i,y 6=yi λ(i,y)∆(yi,y) (5a)\ns.t. : ∑ i,y 6=yi λ(i,y)δhi(y) ≤ 1, (5b)\n0 ≤ ∑ y 6=yi λ(i,y) ≤ C m ,∀i = 1, . . . ,m. (5c)\nThe idea of column generation is to split the original problem into two problems: the master problem and the subproblem. The master problem is the original problem with only a subset of variables being considered. The subproblem’s task is to add new variables into the master problem. Usually the objective function of the subproblem is the reduced cost of the new variable with respect to the current dual variables. At each iteration, the master problem is solved and we obtain dual variables. With the dual variables we solve the subproblem to generate a new weak learner which corresponds to a new variable in the primal, and we re-solve the master problem until convergence. With the primal-dual pair of (3) and (5) and following the general framework of column generation based boosting [7], [9], we can obtain our StructBoost as follows:\nIterate the following three steps until converge:\n1) Solve the subproblem which finds the best weak learner by finding the most violated constraint in the dual:\n~?(·, ·) = argmax ~(·,·) ∑ i,y 6=yi λ(i,y) [ ~(xi,yi)− ~(xi,y) ] (6)\n2) Add the selected weak learner into the master problem and re-solve for w. 3) Update the dual variable λ (using KKT conditions, for example).\nThis approach, however, may not be practical because it is difficult to solve the master problem (the reduced problem of (3)), which still can have extremely many constraints due to the set of {y ∈ Y|y 6= yi}. We show the poor scalability of this approach in the experiment section, even for special cases of binary classification. The direct formulation for multi-class boosting in [11] can be seen as a specific instance of this approach, which is in general very slow.\n2.1 1-slack formulation for fast optimization\nInspired by the cutting-plane method for fast training of linear SVM [8], we can equivalently rewrite the above problem into a “1-slack” form so that an efficient cuttingplane method can be employed to solve the optimization\n5 problem in (3):\nmin w,ξ\n‖w‖1 + Cξ (7a)\ns.t. : 1 m w> [ m∑ i=1 ci · δhi(y) ] ≥ 1 m m∑ i=1 ci∆(yi,y)− ξ,\n∀c ∈ {0, 1}m;∀y ∈ Y\\yi, i = 1, · · · ,m, (7b) w ≥ 0; ξ ≥ 0. (7c)\nThe following theorem shows the equivalence of problems (3) and (7).\nTheorem 2.1. A solution of problem (7) is also a solution of problem (3) and vice versa. The connections are: w?(7) = w ? (3) and ξ?(7) = 1 m1 >ξ?(3).\nProof: The proof adapts the proof in [8]. Given a fixed w, the only variable ξ(3) in (3) can be solved by\nξi,(3) = max y,y 6=yi\n{ 0,∆(yi,y)−w>δhi(y) } ,∀i.\nFor (7), the optimal ξ(7) given a w can be computed as:\nξ(7) = 1\nm max c,y 6=yi { m∑ i=1 ci∆(yi,y)−w> [ m∑ i=1 ciδhi(y) ]}\n= 1\nm m∑ i=1 { max ci∈{0,1},y 6=yi ci∆(yi,y)− ciw>δhi(y) }\n= 1\nm m∑ i=1 max y 6=yi { 0,∆(yi,y)−w>δhi(y) } = 1\nm 1>ξ(3).\nNote that c ∈ {0, 1}m in the above equalities. Clearly the objective functions of both problems coincide for any fixed w and the optimal ξ(3) and ξ(7).\nAs demonstrated in [8], cutting-plane methods can be used to solve the 1-slack primal problem (7) efficiently. This 1-slack formulation has been used to train linear SVM in linear time. When solving for w, (7) is similar to `1- norm regularized SVM—except the extra non-negativeness constraint on w in our case.\nIn order to utilize column generation for designing boosting methods, we need to derive the Lagrange dual of the above 1-slack optimization problem. The Lagrangian of the 1-slack primal problem in (7) can be written as:\nL =‖w‖1 + Cξ − ∑ c,y 6=yi λ(c,y) · { 1 m w> [ m∑ i=1 ci · δhi(y) ] −\n1\nm m∑ i=1 ci∆(yi,y) + ξ } − ν>w − βξ, (8)\nwhere λ,ν, β are Lagrange multipliers: λ ≥ 0,ν ≥ 0, β ≥ 0. We denote by λ(c,y) the Lagrange multiplier associated with the inequality constraints for c ∈ {0, 1}m and y 6= yi, i = 1 . . .m. Again, at optimum, the first derivative of\nthe Lagrangian w.r.t. the primal variables must be zeros,\n∂L ∂ξ = 0 =⇒ C − ∑ c,y 6=yi λ(c,y) − β = 0\n=⇒ 0 ≤ ∑ c,y 6=yi λ(c,y) ≤ C;\nand,\n∂L ∂w = 0 =⇒ 1− 1 m ∑ c,y 6=yi λ(c,y) · [ m∑ i=1 ci · δhi(y) ] = ν.\n=⇒ 1 m ∑ c,y 6=yi λ(c,y) · [ m∑ i=1 ci · δhi(y) ] ≤ 1. (9)\nThe dual problem of (7) can be written as:\nmax λ ∑ c,y 6=yi λ(c,y) m∑ i=1 ci∆(yi,y) (10a)\ns.t. : 1\nm ∑ c,y 6=yi λ(c,y) [ m∑ i=1 ci · δhi(y) ] ≤ 1, (10b)\n0 ≤ ∑ c,y 6=yi λ(c,y) ≤ C. (10c)\nHere c enumerates all possible c ∈ {0, 1}m. So in practice, we solve the 1-slack formulation (primal (7) and dual (10)). The subproblem to find the most violated constraint in the dual form for generating weak learners is:\n~?(·, ·) = argmax ~(·,·) ∑ c,y 6=yi λ(c,y) ∑ i ci [ ~(xi,yi)− ~(xi,y) ] = argmax\n~(·,·) ∑ i,y 6=yi ∑ c\nλ(c,y)ci︸ ︷︷ ︸ :=µ(i,y)\n[ ~(xi,yi)− ~(xi,y) ] .\n(11)\nWe have changed the order of summation to have a similar form as in the m-slack case.\n2.2 Cutting-plane optimization for solving the 1-slack primal\nDespite the extra nonnegative-ness constraint w ≥ 0 in our case, it is easy to modify the cutting-plane method in [8] for solving our problem (7). For the analysis of the cuttingplane method for optimizing the 1-slack primal, readers may refer to [8] for details.\nAlgorithm 2 summarizes how the original optimization problem (3) can be solved using cutting planes.\nIn the experiment section, we empirically show that solving (7) using cutting planes can be orders of magnitude faster than solving (3).\nIn theory, improved cutting-plane methods such as [21] can also be adapted for solving our optimization problem at each column generation.\nThe algorithmic implementation of our StructBoost is summarized in Algorithm 1. Line 4 finds the most violated constraint and add a new weak learner to the master problem. Here µ(i,y) defined in (11) plays the role as the sample weights associated to each training sample in conventional\n6 Algorithm 1 Column generation for StructBoost 1: Input: training examples (x1;y1), (x2;y2), · · · ; parameter C; termination threshold cg, and the maximum iteration number. 2: Initialize: for each i, (i = 1, . . . ,m), randomly pick any y(0)i ∈ Y, initialize µ(i,y) = C/m for y = y (0) i , and µ(i,y) = 0 for all y ∈ Y\\y (0) i . 3: Repeat 4: − Find and add a new weak learner ~?(·, ·) by solving:\n~?(·, ·) = argmax ~(·,·) ∑ i,y 6=yi µ(i,y) [ ~(xi,yi)− ~(xi,y) ] .\n5: − Call Algorithm 2 to obtain w, ξ; λ, and W. 6: − Update µ(i,y) = ∑ c λ(c,y)ci. 7: Until either 1 m ∑ c,y 6=yi λ(c,y) [∑m i=1 ci · δ~?i (y) ] ≤ 1− cg, or the maximum iteration is reached. 8: Output: the discriminant function F (x,y;w) = w>h(x,y).\nAlgorithm 2 Cutting planes for solving the 1-slack primal 1: Input: cutting-plane termination threshold cp, and inputs from Algorithm 1. 2: Initialize: working set W ← ∅; ci = 1, y′i ← any element in Y, for i = 1, . . . ,m. 3: Repeat 4: − W←W ∪ {(c1, . . . , cm,y′1, . . . ,y′m)}. 5: − Obtain primal and dual solutions w, ξ; λ by solving\nmin w≥0,ξ≥0\n‖w‖1 + Cξ\ns.t. : ∀(c1, . . . , cm,y′1, . . . ,y′m) ∈W :\n1 m w> [ m∑ i=1 ci · δhi(y′i) ] ≥ 1 m m∑ i=1 ci∆(yi,y ′ i)− ξ.\n6: − For i = 1, . . . ,m 7: y′i = argmax y ∆(yi,y)−w>δhi(y); 8: ci = { 1 ∆(yi,y ′ i)−w>δhi(y′i) > 0 0 otherwise 9: End for 10: Until 1\nm w> [ m∑ i=1 ciδhi(y ′ i) ] ≥ 1 m m∑ i=1 ci∆(yi,y ′ i)− ξ − cp.\n11: Output: w, ξ; λ,W.\nboosting such as AdaBoost. Lines 4 and 5 then solve the primal problem and update the variables. We can see that the training loop is almost identical to these conventional boosting methods. The following theorem shows the convergence property of Algorithm 1.\nTheorem 2.2. Algorithm 1 makes progress at each column generation iteration; i.e., the objective value decreases at each iteration. Hence, in the limit, Algorithm 1 globally solves the optimization problem (3) (or (7) due to Theorem 2.1) to a prescribed accuracy.\nProof: Let us assume that the current solution is a finite subset of weak learners and their corresponding coefficients are w. When we add a weak learner that is not in the current subset and resolve the problem and the corresponding ŵ is zero, then the objective value and the solution keep unchanged. In this case, we can draw a conclusion that the current selected weak learner and the solution w are optimal.\nNow let us assume that the optimality condition is violated. We want to show that we can find a weak learner ~̂(·, ·) that is not in the current set of weak learners, such that its corresponding coefficient ŵ > 0 holds. Assume\nthat ~̂(·, ·) is found by solving (11), and the convergence condition 1m ∑ c,y 6=yi λ(c,y) [∑m i=1 ci · δ~̂i(y) ] ≤ 1 does not\nhold. In other words, we have 1m ∑ c,y 6=yi λ(c,y) [∑m i=1 ci ·\nδ~̂i(y) ] > 1.\nNow if this ~̂(·, ·) is added into the master problem and the primal solution is not changed; i.e., ŵ = 0, then we know that in (9), ν = 1 − 1m ∑ c,y 6=yi λ(c,y) [∑m i=1 ci ·\nδ~̂i(y) ] < 0. This contradicts the fact that the Lagrange multiplier ν must be nonnegative. Therefore, after this weak learner is added into the master problem, its corresponding coefficient ŵ must be a nonzero positive value. It means that one more free variable is added into the master problem and re-solving the it must reduce the objective value. That means, a strict decrease in the objective is assured. Hence Algorithm 1 makes progress at each iteration. Moreover, since the optimization problem is convex in w, a local solution is global."
    }, {
      "heading" : "3 SPECIAL CASES OF STRUCTBOOST",
      "text" : "We consider a few special cases of the proposed general structured boosting in this section."
    }, {
      "heading" : "3.1 Binary classification",
      "text" : "Clearly the standard binary classification LPBoost can be seen as a special case of multi-class classification and of StructBoost as well. We write the 1-slack formulation of LPBoost and solve the 1-slack primal using cutting-plane. The primal is:\nmin w,ξ ‖w‖1 + Cξ (12a)\ns.t. : 1\nm w> [ m∑ i=1 ciyih ′(xi) ] ≥ 1 m m∑ i=1 ci − ξ, (12b)\n∀c ∈ {0, 1}m,∀i = 1, . . . ,m; w ≥ 0; ξ ≥ 0. (12c)\nHere yi ∈ {−1, 1} and we define the symbol\nh′(x) = [~1(x) · · · ~n(x)]>, (13)\nwhich is the outputs of all binary weak classifiers on example x. The dual problem of (12) can be easily derived. We show in the experiments that at each iteration of LPBoost, solving (12) is much faster than solving the m-slack primal or dual as shown in [9]."
    }, {
      "heading" : "3.2 Multi-class boosting",
      "text" : "We first show the MultiBoost algorithm in Shen and Hao [11] can be implemented by the StructBoost framework as follows. We then introduce a new multi-class boosting algorithm. Let Y = {1, 2, . . . , k} and w = w1 · · · wk. Here stacks two vectors. As in [11], wy is the model parameter associated with the y-th class. The multi-class discriminant function in [11] writes F (x, y;w) = w>yh\n′(x). Now let us define the orthogonal label coding vector:\nΓ(y) = [II(y, 1), II(y, 2), · · · , II(y, k)]> ∈ {0, 1}k. (14)\n7 Here II(y, k) is the indicator function defined as:\nII(y, k) = { 1 if y = k, 0 if y 6= k.\n(15)\nThen h(x, y) = h′(x) ⊗ Γ(y) recovers the StructBoost formulation (3) for multi-class boosting. The operator ⊗ calculates the tensor product.\nNow we propose a new multi-class boosting algorithm. Instead of learning k model parameter (one wr for each class) as in Shen and Hao [11], we learn a single parameter w. Classes are distinguished by augmenting the data with the label. Let us define label-augmented data as x′y = x⊗ Γ(y), with x the original input data. Clearly the label-augmented data x′y have the same number of nonzero entries as the original data x. This is desirable since the label-augmented data do not increase the computation complexity much by using the sparse data structure. So we formulate the multi-class learning as\nmin w,ξ\n‖w‖1 + Cm 1 >ξ (16a)\ns.t. : w> [ h′(x′i,yi)− h ′(x′i,y) ] ≥ 1− ξi,\n∀i = 1, . . . ,m; and ∀y ∈ {1, . . . , k}\\yi, (16b) w ≥ 0; ξ ≥ 0; (16c)\nwith h′(·) defined in (13). So we only need to set h(x, y) = h′(x′y) = h\n′(x⊗ Γ(y)) and ∆(y, y′) = 1 to implement this new multi-class boosting in the StructBoost framework. The main difference between (16) and MultiBoost in [11] is that here w ∈ Rn, while w ∈ Rn×k for MultiBoost, with n being the number of weak learners. We compare the performance of this new multi-class boosting in the experiment section."
    }, {
      "heading" : "3.3 Hierarchical multi-class classification",
      "text" : "The flexibility of StructBoost allows us to train a multiclass classifier that optimizes the complex tree loss. In many applications such as object categorization, classes of objects are organized in taxonomies or hierarchies. For example, The ImageNet dataset has organized all the classes according to the tree structures of WordNet. This problem is a classification example that the output space has interdependent structures. An example tree structure of image categories is shown in Figure 1.\nSimilar to [4], here we consider the tree loss: ∆tree(y, y′). Given a class tree structure T with a height of τ (i.e., T has τ levels), ∆tree(y, y′) is the height of the first common parent node of class y and y′ in the tree structure from the bottom to the top. All we need is to redefine the orthogonal coding vector Γ(y) in (14), and the algorithmic implementation remains identical as the standard multi-class case that we discussed. We define:\nΓ(y) = Γ′(y(1)) Γ′(y(2)) · · · Γ′(y(τ)) (17)\nHere stacks two vectors. We define kl to be the number of classes in the l-th level of the tree structure. y(l) is parent label of y on the l-th level of the tree structure (with y(1) =\nScene\nIndoor Outdoor Landscape\nShopping Mall Library Museum River Lake Mountain\ny), and Γ′(y(l)) ∈ {0, 1}kl is the orthogonal label coding vector:\nΓ′(y(l)) = [II(y(l), 1), II(y(l), 2), · · · , II(y(l), kl)]>. (18)\nII(·, ·) is defined in (15). The original Γ(y) is flat in that the inner product Γ(y)>Γ(y′) = 0 always holds. With the tree loss, Γ(y)>Γ(y′) counts the number of common predecessors in the label tree. We have run some experiments on the SUN scene recognition dataset in the experiment section."
    }, {
      "heading" : "3.4 Ordinal regression and AUC optimization",
      "text" : "In ordinal regression, labels of the training data are ranks. Let us assume that the label y ∈ R indicates an ordinal scale, and pairs (i, j) in the set S has the relationship of example i being ranked higher than j, i.e., yi > yj . The primal can be written as\nmin w,ξ\n‖w‖1 + Cm ∑ (i,j)∈Sξij (19a)\ns.t. : w> [ h′(xi)− h′(xj) ] ≥ 1− ξij ,∀(i, j) ∈ S, (19b)\nw ≥ 0; ξ ≥ 0; (19c)\nNote that (19) also optimizes the area under the receiver operating characteristic (ROC) curve (AUC) criterion. In general, The number of constraints is quadratic in the number of training examples. Directly solving (19) can only solve problems with up to a few thousand training examples. We can reformulate (19) into an equivalent 1-slack problem, same as in (12); and the StructBoost framework can be applied to solve large-scale problems."
    }, {
      "heading" : "3.5 Optimization of the Pascal image overlap criterion",
      "text" : "Object detection/localization has used the image area overlap as the loss function [1]–[3], e.g, in the PASCAL object detection challenges:\n∆(y,y′) = 1− area(y ∩ y ′)\narea(y ∪ y′) , (20)\n8 with y,y′ being the bounding box coordinates. y ∩ y′ and y∪y′ are the box intersection and union. In this application, we train the weak learner h(x,y) with the image features extracted from the image patch defined by y. For example, we can extract histograms of oriented gradients (HOG) from the image patch y and train a decision stump with the extracted HOG features. This naturally fits into StructBoost.\nNote that in this case, to find the most violated constraint in the training step as well as the inference for prediction is in general highly non-convex and it is difficult to find a global solution. In [3], a branch-and-bound search has been employed to find the global optimum. In our visual tracking application, we simplify this problem using discrete sampling. That is to say, only a certain number of sampled image patches are evaluated to find the most violated constraint at each column generation iteration. It is also the case for the final inference step for prediction. This simple search strategy has been used in [1]."
    }, {
      "heading" : "3.6 StructBoost for CRF parameter learning",
      "text" : "CRF has found many applications in computer vision such as image segmentation. However, the parameter learning of CRF remains an issue in many applications. Most work uses tedious cross-validation to find the optimal values for a small number of parameters. Recently, structured SVM [14], [16] and a tree-based graph learning method [23] have been proposed to learn these parameters in a principled way. We demonstrate CRF parameter learning using StructBoost in the application of image segmentation. Later we run some experiments on the Graz-02 image segmentation dataset.\nTo speed up computation, super-pixels rather than pixels have been widely adopted in image segmentation. We define x as an image, y as the segmentation labels of all super-pixels in an image.\nWe consider the energy E of an image x and segmentation labels y over the nodes N and edges S, which takes the following form:\nE(x,y;w) = ∑ p∈N w(1)h(1) (U(yp,x))\n+ ∑\n(p,q)∈S\nw(2)h(2) (V(yp, yq,x)) . (21)\nHere p and q are the super-pixel indexes; and yp, yq are the labels of the super-pixels p, q. U is a set of unary potential functions: U = [U1, U2, . . . ]>. V is a set of pairwise potential functions: V = [V1, V2, . . . ]>. Details about how to obtain U and V are postponed to the experiment section. w(1) and w(2) are the CRF parameters that we want to learn. h(1)(·) and h(2)(·) are two sets of weak learners for the unary part and pairwise part respectively: h(1)(·) = [ ~(1)1 (·), ~ (1) 2 (·), . . . , ~ (1) n (·) ]>, h(2)(·) = [ ~(2)1 (·), ~ (2) 2 (·), . . . , ~ (2) n (·) ]>. In our experiments, we use discrete weak learners and a weak learner ~(·) here maps a vector to {0, 1}, which is different from other experiments. Thus the energy value is always positive: E ≥ 0. Note that our setting (21) differs most CRF learning settings such as [16]. These traditional CRF methods often use a linear\nmodel [16]. Until recently, Bertelli et al. presented an image segmentation approach that uses nonlinear kernel for the unary energy term in the CRF model [14]. In our model (21), nonlinearity is introduced by applying weak learners on the potential functions’ outputs U and V. This is in spirit same as the fact that an SVM introduces nonlinearity via the socalled kernel trick and boosting learns a nonlinear model with nonlinear weak learners.\nTo predict the segmentation labels y? of an unknown image x is to solve an energy minimization problem:\ny? = argmin y E(x,y;w), (22)\nwhich can be solved efficiently by graph cuts [16], [24]. To learn the parameters in StructBoost framework, we define w = −(w(1) w(2)) and the function h(·, ·):\nh(x,y) = ∑ p∈N h(1) (U(yp,x)) ∑ (p,q)∈S h(2) (V(yp, yq,x)) .\n(23)\nRecall that stacks two vectors. With this definition, we have the relation: w>h(x,y) = −E(x,y;w). By substituting it into our StructBoost in (3), the CRF parameter learning problem can be written as:\nmin w,ξ ‖w‖1 + Cm ∑ i ξi (24a) s.t. : E(xi,y;w)− E(xi,yi;w) ≥ ∆(yi,y)− ξi, ∀i = 1, . . . ,m; and ∀y ∈ Y\\yi. w ≥ 0; ξ ≥ 0.\n(24b)\nHere i indexes images. Intuitively, the optimization in (24) is to encourage the energy of the ground truth label E(xi,yi) to be lower then any other incorrect labels E(xi,y) by at least a margin ∆(yi,y), ∀y 6= yi. This optimization can be solved in the StructBoost framework using the one-slack algorithm which we discussed before. We use decision stumps for function ~(1) and ~(2). In each column generation iteration (Algorithm 1), two new weak learners (~(1)? and ~(2)?) are generated and added to unary weak learner set h(1) and pairwise weak learner set h(2) respectively by solving the argmax problem defined in (11), which can be written as:\n~(1)?(·, ·) = argmax ~(·,·) ∑ i,y 6=yi ∑ c λ(c,y)ci\n· ∑ p∈N [ ~(1) (U(yp,xi))− ~(1) (U(ypi ,xi) ] ; (25)\nand,\n~(2)?(·, ·) = argmax ~(·,·) ∑ i,y 6=yi ∑ c λ(c,y)ci\n· ∑\n(p,q)∈S\n[ ~(2) (V(yp, yq,xi))− ~(2) (V(ypi , y q i ,xi)) ] .\n(26)\nConsidering the maximization to find the most violated constraint corresponding to xi in line 7 of Algorithm 2:\ny′i = argmax y ∆(yi,y)−w>δhi(y). (27)\n9 Solving (27) is to solve the inference:\ny′i = argmin y E(xi,y)−∆(yi,y), (28)\nwhich is similar to the label prediction inference in (22), and the only difference is that the labeling loss term: ∆(yi,y) is involved in (28). We simply define ∆(yi,y) using Hamming loss, which is the sum of the differences between the ground truth label yi and the label y over super-pixels:\n∆(yi,y) = ∑ p (1− II(ypi , y p)). (29)\nII(·, ·) is an indicator function defined in (15). With this definition, the term ∆(yi,y) can be absorbed into the unary term of the energy function defined in (21). The inference in (28) can be written as:\ny′i = argmin y ∑ p∈N [ w(1)h(1) (U(·))− (1− II(ypi , y p)) ] +\n∑ (p,q)∈S w(2)h(2) (V(·)) . (30)\nSimilar to [16], the minimization (30) still can be solved efficiently by graph cuts."
    }, {
      "heading" : "4 EXPERIMENTS",
      "text" : "In this section, we run various experiments on applications including binary classification, ordinal regression, multiclass image classification, hierarchical image classification, visual tracking and image segmentation. We use UCI datasets in binary classification and ordinal regression for training time comparison, we have randomly chosen 75% data as training data and the rest 25% for test. For each dataset we run 10 times and report the average results. We use 4-fold cross validation on the entire dataset to determine the regularization parameter. The value of the regularization parameter C is chosen from 22 to 26. For all the experiments, we have set the cutting-plane cp to 0.01. The threshold of the column generation stopping criterion cg is 0.01. Maximum column generation iteration is set to 200. The CPU time is obtained on a desktop with an AMD CPU 2.20GHz. The code is in Matlab that calls some C mex files."
    }, {
      "heading" : "4.1 Binary classification",
      "text" : "We run experiments on some UCI machine learning datasets to compare our StructBoost formulation of binary boosting against the standard LPBoost [9]. Table 1 reports the experiment results on binary classification data sets (we use one class as positive data and the rest classes as negative if the original datasets have multiple labels). It is easy to see that the 1-slack formulation is orders of magnitude faster than the standard LPBoost.\ndataset method CPU time (s) training % test %"
    }, {
      "heading" : "4.2 Ordinal regression and AUC optimization",
      "text" : "The details of StructBoost for AUC optimization are described in Section 3.4. We run AUC optimization with the m-slack formulation of StructBoost and (solving (3) or its dual) 1-slack formulation of StructBoost (solving (7)). To create imbalanced data, we have used one class of the multi-class UCI datasets as positive data and all the rest labels as negative data. Table 2 reports the results. We can see that the 1-slack formulation of StructBoost is much faster with similar performance.\nNote that RankBoost may also be applied to this problem [25]. RankBoost has been designed for solving ranking problems and it is not a general structured boosting method.\n10"
    }, {
      "heading" : "4.3 Multi-class classification",
      "text" : "The details of StructBoost for multi-class are described in Section 3.2. We run our multi-class boosting on two image datasets: MNIST2 and Scene15 [26]. Here we have used the linear `1 SVM as weak classifiers. We set the tradeoff parameter as C = 106/(number of examples). To avoid over-fitting, at each boosting iteration, we first sort the data weights and select top p percentage of the weighted positive and negative examples to train the SVM (p = 60% for MNIST and 80% for Scene15).\nFor MNIST, we randomly select 100 samples from each class as training sets and use the original test sets of 10, 000 samples. We have repeated this procedure for 5 times and reported the average test error. Spatial pyramid HOG features [27] are used here. For Scene15, we randomly sample 100 examples of each class to generate training data, and the rest as testing data. So the total number of training examples is 1500, and the number of test examples is 2985. The reported result is the average of 5 runs. We generate histograms of code words as features. The code book size is 200. An image is divided into 31 sub-windows in a spatial hierarchy manner [28]. We generate histograms in each sub-windows, so the histogram feature dimension is 6200. CENTRIST [29] is used as the feature descriptor. In each train/test split, a visual codebook is generated using only training images. Both training and test images are then transformed into histograms of code words.\nFor comparison, we also run two standard multi-class boosting methods: AdaBoost.ECC [30] and AdaBoost.MH [31]. We use decision stumps for AdaBoost.MH and AdaBoost.ECC. Figure 2 shows the convergence curves. The observations are: 1) linear SVM as weak classifiers seems to converge faster than decision stumps. 2) Our StructBoost converges faster than other competitors, although the final accuracy is not significantly different from others."
    }, {
      "heading" : "4.4 Hierarchical multi-class classification",
      "text" : "The details of hierarchical multi-class are described in Section 3.3. We have constructed two hierarchical image datasets from the SUN dataset [22]. The first dataset contains 6 classes of scenes, it has two category levels. For each scene class, we use the top first 200 images from the original SUN dataset. So there are 1200 images in total. The second dataset is larger which contains 15 classes of scenes, and there are 3000 images in total. We have used the HOG features as described in [22]. The detail of the hierarchical structure of these two dataset is show in the Figure 1.\nFor each dataset, we randomly select 50% examples for training, and the rest for testing. The reported results are computed on 8 random splits. We heuristically set the regularization parameter for the StructBoost in this experiment. The maximum boosting iteration is set to 500.\nTable 3 reports the results. Here we have also run standard multi-class boosting, AdaBoost.ECC, and AdaBoost.MH. Two observations can be made: 1) Hierarchical\n2. http://yann.lecun.com/exdb/mnist/\nmulti-class boosting indeed has the minimum tree loss over all the compared methods because it directly minimizes the tree loss; 2) Hierarchical multi-class boosting improves its standard multiclass counterpart (the second column in Table 3) in terms of both classification accuracy and the tree loss, demonstrating its usefulness."
    }, {
      "heading" : "4.5 Visual tracking by optimizing the image area overlap criterion",
      "text" : "In [1], a visual tracking method, termed Struck, was introduced based on SSVM. The core idea is to train a tracker by optimizing the Pascal image overlap score using SSVM. Here we follow the same general setting of this structured tracking method, but with our StructBoost, instead of SSVM. We use decision stumps as the weak learner. More details are described in Section 3.5.\nWe use an on-line tracking setting for StructBoost tracker in our experiment. We only use the first 3 labeled frames for initialization and training our StructBoost tracker. We then update our tracker by re-training the model with sequent frames during the course of tracking. In the i-th frame (represented by xi), we first perform a prediction step to output the detection box, then collect training data for tracker update. In the prediction step, we solve the inference in (2) to output the prediction box (represented by yi) of current frame. For solving the inference in (2), we simply sample about 2000 bounding boxes around the prediction bounding box of last frame (represented by yi−1), one sampled bounding box is denoted by y, and search the most confident bounding box over all sampled boxes y as the prediction yi. In the first 3 labelled frames for initialization, we use the labelled bounding box as yi.\n11\n12\n13\nAfter the prediction, we collect training data by sampling about 200 bounding boxes around the current prediction yi. We use the training data in recent 60 frames to re-train the tracker for every 2 frames. We search over those sampled bounding boxes for finding the most violated constraint of each frame in the training process, which analogue to the prediction inference.\nFor StructBoost, the maximum number of weak learners is set to 300. The regularization parameter is selected from 100.5 to 102. We use the down-scaled gray-scale raw pixels and HOG as image features. For HOG feature, we use the code in [32]. For comparison, we also run the AdaBoost trackers using the same setting as our StructBoost tracker. For AdaBoost training, the maximum number of weak learners is set to 500. The AdaBoost tracker is a simple binary model. When updating (or initializing) AdaBoost tracker, we collect positive training boxes that significantly overlap with the predicted bounding box of the current frame (overlap above 0.8), and negative training boxes with small overlap (overlap lower or equal to 0.3).\nWe also compare our trackers with a few state-of-the-art tracking methods, including Struck [1] (with a buffer size\nof 50), multi-instance tracking (MIL) [33], fragment tracking (Frag) [34], online AdaBoost tracking (OAB) [35], and visual tracking decomposition (VTD) [36]. OAB has two versions with two different settings (r = 1 means only one positive example per frame and r = 5 means five positive examples per frame for training. They are referred to as OAB1 and OAB5 here. See [33].). The test video sequences “coke, tiger1, tiger2, david, girl and sylv” were used in [1]. The sequences “shaking, singer” are obtained from [36], and the rest sequences are from [37].\nTable 4 reports the Pascal overlap score of various tracking methods on testing video sequences. Our StructBoost tracker performs best on most test sequences. Compared with the binary AdaBoost tracker, StructBoost has a significantly higher score. Note that here Struck uses Haar features. When Struck uses a Gaussian kernel defined on raw pixels, the performance is slightly different [1], and ours still outperforms Struck in most cases. This might be due to the fact that our StructBoost selects relevant features (300 features selected here), and the SSVM of [1] uses all the image patch information which may contain noises.\nFigure 3 plots the Pascal overlap scores frame by frame on several video sequences. It clearly shows that StructBoost outperform other methods in most cases. Compared to AdaBoost, StructBoost performs better at almost all\n14\nframes. The main reason is that StructBoost directly maximizes the overlap, while AdaBoost is trained by optimizing the classification error, which is not directly related to the Pascal overlap score.\nThe central location errors of compared methods are shown in Table 5. Our method also achieve the best results in most cases, which reveals that optimizing the overlap score also helps minimize the central location errors. We also plot the central location errors of different methods frame by frame on several sequences in Figure 4. These results prove the superior performance of StructBoost for tracking.\nSome tracking examples are shown in Figure 5. In our experiments, the output space of StructBoost is the bounding box’s coordinates and the scale is fixed. However, it is easy to incorporate scale changes, rotation and transforms into the output space due to the flexibility of StructBoost."
    }, {
      "heading" : "4.6 CRF parameter learning for image segmentation",
      "text" : "In this experiment, we extend the super-pixels based segmentation method [24] with CRF parameter learning. More details are described in Section 3.6. We use the Graz-02 dataset3 in this experiment, which contains 3 categories (bike, car and person). Each image only contains one category. For each category, we use first 300 labeled images. Images with the odd indices are for training and the rest for testing. We generate super-pixels and features same as in [24]: the neighborhood size is set to 2; histogram of visual words features are generated for each superpixel; code book size is 200. For StructBoost, we use two unary potentials: U = [U1, U2 ]> and 2 pairwise potentials: V = [V1, V2 ]\n>. We only use randomly sampled 50 training images for the training of StructBoost to learn CRF parameters. In binary classifier training for the unary potential, we use all training images.\nTwo unary potentials: U1, U2 are constructed using two AdaBoost classifiers; one is trained on the visual word histogram features [24], and the other is trained on color histogram together with the thumbnail feature [38]. We define F ′ as the discriminant function of AdaBoost. Then the unary potential function can be written as:\nU(x, yp) = −ypF ′(x). (31)\nFor the two pairwise potentials, V1 is constructed using color difference, and V2 is constructed using shared boundary length between two neighboring super-pixels\n3. http://www.emt.tugraz.at/∼pinz/\n[24], which is able to discourage small isolated segments. Recall that II(·, ·) is an indicator function defined in (15). ‖xp − xq‖2 calculates the `2 norm of the color difference between two super-pixels in the LUV color-space; `(xp,xq) is the shared boundary length between two super-pixels, as in [24]. Then V1, V2 can be written as:\nV1(·) = exp(−‖xp − xq‖2) [ 1− II(yp, yq) ] , (32)\nV2(·) =`(xp,xq) [ 1− II(yp, yq) ] . (33)\nFor comparison, we also run AdaBoost and SVM for segmentation, which are binary classifiers trained on foreground and background super-pixels using the same visual word histogram features as our method. As [24], we use the precision = recall point and intersection-union score to evaluation our method. Results are shown in Table 6. Some segmentation examples are shown in Figures 6 and 7. The results show that StructBoost with the efficient inference method (graph cuts) gains performance improvement, and also show that StructBoost is able to learn the CRF parameters for combining different potential functions in a principled way."
    }, {
      "heading" : "5 CONCLUSION",
      "text" : "We have presented a boosting method for structural learning, as an alternative to SSVM [4] and CRF [10]. Analogues to SSVM, where the discriminant function is learned over a joint feature space of inputs and outputs, the discriminant function of the proposed StructBoost is a linear combination of weak learners defined over a joint space of input-output pairs.\nMoreover, StructBoost is flexible in its ability to optimize specific loss functions. To efficiently solve the resulting optimization problems, we have introduced a cutting-plane method, which was originally proposed for fast training of linear SVM. Our extensive experiments demonstrate that indeed the proposed algorithm is computationally tractable. We also show that the test accuracy of our StructBoost is at least comparable or sometimes exceeds conventional approaches for a wide range of applications such as multi-class classification, AUC optimization, image segmentation with CRF parameter learning. In particular, we have used StructBoost to train a visual tracker by optimizing the Pascal image overlap score. Experiments show its state-of-the-art tracking accuracy, compared with a few recent tracking methods. Future work will focus on more applications of this general StructBoost framework."
    } ],
    "references" : [ {
      "title" : "Struck: Structured output tracking with kernels",
      "author" : [ "S. Hare", "A. Saffari", "P. Torr" ],
      "venue" : "Proc. IEEE Int. Conf. Comp. Vis., 2011.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Structured learning and prediction in computer vision",
      "author" : [ "S. Nowozin", "C.H. Lampert" ],
      "venue" : "Foundations & Trends in Computer Graphics & Vision, 2011.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Learning to localize objects with structured output regression",
      "author" : [ "M.B. Blaschko", "C.H. Lampert" ],
      "venue" : "Proc. Eur. Conf. Comp. Vis., 2008, pp. 2–15.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Support vector machine learning for interdependent and structured output spaces",
      "author" : [ "I. Tsochantaridis", "T. Hofmann", "T. Joachims", "Y. Altun" ],
      "venue" : "Proc. Int. Conf. Mach. Learn., 2004, pp. 104–111.  15",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Multi-class support vector machines",
      "author" : [ "J. Weston", "C. Watkins" ],
      "venue" : "Proc. Euro. Symp. Artificial Neural Networks, 1999.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "On the algorithmic implementation of multiclass kernel-based vector mchines",
      "author" : [ "K. Crammer", "Y. Singer" ],
      "venue" : "J. Mach. Learn. Res., vol. 2, pp. 265–292, 2001.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "On the dual formulation of boosting algorithms",
      "author" : [ "C. Shen", "H. Li" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell., vol. 32, no. 12, pp. 2216–2231, 2010.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Training linear SVMs in linear time",
      "author" : [ "T. Joachims" ],
      "venue" : "Proc. ACM SIGKDD Int. Conf. Knowledge discovery & data mining, 2006, pp. 217–226.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Linear programming boosting via column generation",
      "author" : [ "A. Demiriz", "K.P. Bennett", "J. Shawe-Taylor" ],
      "venue" : "Mach. Learn., vol. 46, no. 1-3, pp. 225–254, 2002.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "author" : [ "J. Lafferty", "A. McCallum", "F. Pereira" ],
      "venue" : "Proc. Int. Conf. Mach. Learn., 2001, pp. 282–289.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "A direct formulation for totally-corrective multi-class boosting",
      "author" : [ "C. Shen", "Z. Hao" ],
      "venue" : "Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2011.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "An introduction to conditional random fields",
      "author" : [ "C. Sutton", "A. McCallum" ],
      "venue" : "Foundations and Trends in Machine Learning, 2012. [Online]. Available: http://arxiv.org/abs/1011.4088",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Multi-class image segmentation using conditional random fields and global classification",
      "author" : [ "N. Plath", "M. Toussaint", "S. Nakajima" ],
      "venue" : "Proc. Int. Conf. Mach. Learn., 2009.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Kernelized structural SVM learning for supervised object segmentation",
      "author" : [ "L. Bertelli", "T. Yu", "D. Vu", "B. Gokturk" ],
      "venue" : "Proc. IEEE Conf. Comp. Vis. Patt. Recogn. IEEE, 2011, pp. 2153–2160.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Discriminative models for multi-class object layout",
      "author" : [ "C. Desai", "D. Ramanan", "C.C. Fowlkes" ],
      "venue" : "Int. J. Comp. Vis., vol. 95, no. 1, pp. 1–12, 2011.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Learning CRFs using graph cuts",
      "author" : [ "M. Szummer", "P. Kohli", "D. Hoiem" ],
      "venue" : "Proc. Eur. Conf. Comp. Vis., 2008, pp. 582–595.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Boosting structured prediction for imitation learning",
      "author" : [ "N. Ratliff", "D. Bradley", "J.A. Bagnell", "J. Chestnutt" ],
      "venue" : "Proc. Adv. Neural Inf. Process. Syst., 2007.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Boosting algorithms as gradient descent",
      "author" : [ "L. Mason", "J. Baxter", "P.L. Bartlett", "M.R. Frean" ],
      "venue" : "Proc. Adv. Neural Inf. Process. Syst., 1999, pp. 512–518.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Structured gradient boosting",
      "author" : [ "C. Parker" ],
      "venue" : "2007, PhD thesis, Oregon State University. [Online]. Available: http://hdl.handle. net/1957/6490",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Simple training of dependency parsers via structured boosting",
      "author" : [ "Q. Wang", "D. Lin", "D. Schuurmans" ],
      "venue" : "Proc. Int. Joint Conf. Artificial Intell., 2007, pp. 1756–1762.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Optimized cutting plane algorithm for support vector machines",
      "author" : [ "V. Franc", "S. Sonnenburg" ],
      "venue" : "Proc. Int. Conf. Mach. Learn., New York, NY, USA, 2008, pp. 320–327. [Online]. Available: http://doi.acm.org/10.1145/1390156.1390197",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "SUN database: Large-scale scene recognition from abbey to zoo",
      "author" : [ "J. Xiao", "J. Hays", "K. Ehinger", "A. Oliva", "A. Torralba" ],
      "venue" : "Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2010.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "On parameter learning in CRF-based approaches to object class image segmentation",
      "author" : [ "S. Nowozin", "P.V. Gehler", "C.H. Lampert" ],
      "venue" : "Proc. Eur. Conf. Comp. Vis., 2010, pp. 98–111.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Class segmentation and object localization with superpixel neighborhoods",
      "author" : [ "B. Fulkerson", "A. Vedaldi", "S. Soatto" ],
      "venue" : "Proc. Int. Conf. Comp. Vis., 2009.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "An efficient boosting algorithm for combining preferences",
      "author" : [ "Y. Freund", "R. Iyer", "R.E. Schapire", "Y. Singer" ],
      "venue" : "J. Mach. Learn. Res., vol. 4, pp. 933–969, 2003.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories",
      "author" : [ "S. Lazebnik", "C. Schmid", "J. Ponce" ],
      "venue" : "Proc. IEEE Conf. Comp. Vis. Patt. Recogn., vol. 2, 2006, pp. 2169 – 2178.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Fast and accurate digit classification",
      "author" : [ "S. Maji", "J. Malik" ],
      "venue" : "EECS Department, University of California, Berkeley, Tech. Rep. UCB/EECS-2009-159, Nov 2009. [Online]. Available: http://www. eecs.berkeley.edu/Pubs/TechRpts/2009/EECS-2009-159.html",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Scene classification using a hybrid generative/discriminative approach",
      "author" : [ "A. Bosch", "A. Zisserman", "X. Munoz" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell., vol. 30, no. 4, pp. 712 – 727, 2008.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "CENTRIST: A visual descriptor for scene categorization",
      "author" : [ "J. Wu", "J.M. Rehg" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell., vol. 33, no. 8, pp. 1489–1501, 2011.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Multiclass learning, boosting, and error-correcting codes",
      "author" : [ "V. Guruswami", "A. Sahai" ],
      "venue" : "Proc. Annual Conf. Computational Learning Theory. ACM, 1999, pp. 145–155.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Improved boosting algorithms using confidence-rated predictions",
      "author" : [ "R.E. Schapire", "Y. Singer" ],
      "venue" : "Mach. Learn., 1999, pp. 80–91.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Object detection with discriminatively trained part-based models",
      "author" : [ "P.F. Felzenszwalb", "R.B. Girshick", "D. McAllester", "D. Ramanan" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell., vol. 32, no. 9, pp. 1627– 1645, September 2010.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Visual tracking with online multiple instance learning",
      "author" : [ "B. Babenko", "M.-H. Yang", "S. Belongie" ],
      "venue" : "Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2009.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Robust fragments-based tracking using the integral histogram",
      "author" : [ "A. Adam", "E. Rivlin", "I. Shimshoni" ],
      "venue" : "Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2006, pp. 798–805.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Real-time tracking via on-line boosting",
      "author" : [ "H. Grabner", "M. Grabner", "H. Bischof" ],
      "venue" : "Proc. British Mach. Vis. Conf., 2006, pp. 47–56.",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Visual tracking decomposition",
      "author" : [ "J. Kwon", "K.M. Lee" ],
      "venue" : "Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2010, pp. 1269–1276.",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Superpixel tracking",
      "author" : [ "S. Wang", "H. Lu", "F. Yang", "M.-H. Yang" ],
      "venue" : "Proc. Int. Conf. Comp. Vis., pp. 1323–1330, 2011.",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Superparsing: Scalable nonparametric image parsing with superpixels",
      "author" : [ "J. Tighe", "S. Lazebnik" ],
      "venue" : "Proc. Eur. Conf. Comp. Vis., 2010, pp. 352–365.",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "1 INTRODUCTION Structured learning has attracted extensive attention recently in machine learning and computer vision [1]–[4].",
      "startOffset" : 118,
      "endOffset" : 121
    }, {
      "referenceID" : 3,
      "context" : "1 INTRODUCTION Structured learning has attracted extensive attention recently in machine learning and computer vision [1]–[4].",
      "startOffset" : 122,
      "endOffset" : 125
    }, {
      "referenceID" : 3,
      "context" : "Structured support vector machines (SSVM) [4] generalize the multiclass SVM of [5] and [6] to the much broader problem of learning for interdependent and structured outputs.",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 4,
      "context" : "Structured support vector machines (SSVM) [4] generalize the multiclass SVM of [5] and [6] to the much broader problem of learning for interdependent and structured outputs.",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 5,
      "context" : "Structured support vector machines (SSVM) [4] generalize the multiclass SVM of [5] and [6] to the much broader problem of learning for interdependent and structured outputs.",
      "startOffset" : 87,
      "endOffset" : 90
    }, {
      "referenceID" : 4,
      "context" : "Recently, Shen and Hao proposed a direct formulation for multi-class boosting using the loss functions of multi-class SVM [5], [6].",
      "startOffset" : 122,
      "endOffset" : 125
    }, {
      "referenceID" : 5,
      "context" : "Recently, Shen and Hao proposed a direct formulation for multi-class boosting using the loss functions of multi-class SVM [5], [6].",
      "startOffset" : 127,
      "endOffset" : 130
    }, {
      "referenceID" : 6,
      "context" : "Inspired by the general boosting framework of [7], they implemented multi-class boosting with the column generation technique.",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 7,
      "context" : "• To implement StructBoost, we adapt the efficient cutting-plane method—originally designed for efficient linear SVM training [8]—for our purpose.",
      "startOffset" : 126,
      "endOffset" : 129
    }, {
      "referenceID" : 8,
      "context" : "We demonstrate that even conventional LPBoost [9] can benefit from this reformulation to gain significant speedup in training.",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 6,
      "context" : "Since our StructBoost builds upon the fully corrective boosting of Shen and Li [7], it inherits the desirable properties of column generation based boosting, such as a fast convergence rate and a clear explanation from the primal-dual convex optimization perspective.",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 9,
      "context" : "2 Related work The two state-of-the-art structured learning methods are CRF [10] and SSVM [4], which captures the interdependency among output variables.",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 3,
      "context" : "2 Related work The two state-of-the-art structured learning methods are CRF [10] and SSVM [4], which captures the interdependency among output variables.",
      "startOffset" : 90,
      "endOffset" : 93
    }, {
      "referenceID" : 6,
      "context" : "It therefore builds upon the work of column generation boosting [7] and the direct formulation for multi-class boosting [11].",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 10,
      "context" : "It therefore builds upon the work of column generation boosting [7] and the direct formulation for multi-class boosting [11].",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 10,
      "context" : "Indeed, we show the multiclass boosting of [11] is a special case of the general framework presented here.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 9,
      "context" : "For example, the linear chain CRF widely used in natural language processing estimates sequences of labels for sequences of input samples due to the fact that CRF can take context into account [10], [12].",
      "startOffset" : 193,
      "endOffset" : 197
    }, {
      "referenceID" : 11,
      "context" : "For example, the linear chain CRF widely used in natural language processing estimates sequences of labels for sequences of input samples due to the fact that CRF can take context into account [10], [12].",
      "startOffset" : 199,
      "endOffset" : 203
    }, {
      "referenceID" : 7,
      "context" : "SSVM achieves so based on the joint feature maps over the input-output pairs, where features can be represented equivalently as in CRF [8].",
      "startOffset" : 135,
      "endOffset" : 138
    }, {
      "referenceID" : 12,
      "context" : "CRF is particularly of interest in computer vision for its success in semantic image segmentation [13].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 13,
      "context" : "SSVM can also be used for similar purposes as demonstrated in [14].",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 2,
      "context" : "Blaschko and Lampert [3] trained SSVM models to predict the bounding box of objects in a given image, by optimizing the Pascal bounding box overlap score.",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 0,
      "context" : "The work in [1] introduced structured learning to real-time object detection and tracking, which also optimizes the Pascal box overlap score.",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 14,
      "context" : "SSVM has also been used to learn statistics that capture the spatial arrangements of various object classes in images [15].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 15,
      "context" : "[16] learned optimal parameters of a CRF, avoiding tedious cross validation.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 1,
      "context" : "The survey of [2] has provided a comprehensive review of structured learning and its application in computer vision.",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 16,
      "context" : "[17] proposed boosting for imitation learning based on structured prediction called maximum margin planning (MMP).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "In the MMPBoost of [17], a demonstrated policy is provided as example behavior for training and the purpose is to learn a function over features of the environment that produce policies with similar behavior.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 17,
      "context" : "Second, MMPBoost is based on the idea of gradient descent boosting [18], and our StructBoost is built upon fully corrective boosting of Shen and Li [7].",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 6,
      "context" : "Second, MMPBoost is based on the idea of gradient descent boosting [18], and our StructBoost is built upon fully corrective boosting of Shen and Li [7].",
      "startOffset" : 148,
      "endOffset" : 151
    }, {
      "referenceID" : 18,
      "context" : "Parker [19] developed a margin-based structured perceptron update and showed that it can incorporate general notions of misclassification cost as well as kernels.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 18,
      "context" : "Therefore the method in [19] is essentially an online version of SSVM.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 19,
      "context" : "[20] learned a local predictor using standard methods, e.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 6,
      "context" : "For the time being, let us put aside the difficulty of the large number of constraints, and focus on how to iteratively solve for w using column generation as in [7], [9].",
      "startOffset" : 162,
      "endOffset" : 165
    }, {
      "referenceID" : 8,
      "context" : "For the time being, let us put aside the difficulty of the large number of constraints, and focus on how to iteratively solve for w using column generation as in [7], [9].",
      "startOffset" : 167,
      "endOffset" : 170
    }, {
      "referenceID" : 6,
      "context" : "With the primal-dual pair of (3) and (5) and following the general framework of column generation based boosting [7], [9], we can obtain our StructBoost as follows: Iterate the following three steps until converge:",
      "startOffset" : 113,
      "endOffset" : 116
    }, {
      "referenceID" : 8,
      "context" : "With the primal-dual pair of (3) and (5) and following the general framework of column generation based boosting [7], [9], we can obtain our StructBoost as follows: Iterate the following three steps until converge:",
      "startOffset" : 118,
      "endOffset" : 121
    }, {
      "referenceID" : 10,
      "context" : "The direct formulation for multi-class boosting in [11] can be seen as a specific instance of this approach, which is in general very slow.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 7,
      "context" : "Inspired by the cutting-plane method for fast training of linear SVM [8], we can equivalently rewrite the above problem into a “1-slack” form so that an efficient cuttingplane method can be employed to solve the optimization",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 7,
      "context" : "Proof: The proof adapts the proof in [8].",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 7,
      "context" : "As demonstrated in [8], cutting-plane methods can be used to solve the 1-slack primal problem (7) efficiently.",
      "startOffset" : 19,
      "endOffset" : 22
    }, {
      "referenceID" : 7,
      "context" : "2 Cutting-plane optimization for solving the 1-slack primal Despite the extra nonnegative-ness constraint w ≥ 0 in our case, it is easy to modify the cutting-plane method in [8] for solving our problem (7).",
      "startOffset" : 174,
      "endOffset" : 177
    }, {
      "referenceID" : 7,
      "context" : "For the analysis of the cuttingplane method for optimizing the 1-slack primal, readers may refer to [8] for details.",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 20,
      "context" : "In theory, improved cutting-plane methods such as [21] can also be adapted for solving our optimization problem at each column generation.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 8,
      "context" : "We show in the experiments that at each iteration of LPBoost, solving (12) is much faster than solving the m-slack primal or dual as shown in [9].",
      "startOffset" : 142,
      "endOffset" : 145
    }, {
      "referenceID" : 10,
      "context" : "2 Multi-class boosting We first show the MultiBoost algorithm in Shen and Hao [11] can be implemented by the StructBoost framework as follows.",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 10,
      "context" : "As in [11], wy is the model parameter associated with the y-th class.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 10,
      "context" : "The multi-class discriminant function in [11] writes F (x, y;w) = wyh ′(x).",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 10,
      "context" : "Instead of learning k model parameter (one wr for each class) as in Shen and Hao [11], we learn a single parameter w.",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 10,
      "context" : "The main difference between (16) and MultiBoost in [11] is that here w ∈ R, while w ∈ Rn×k for MultiBoost, with n being the number of weak learners.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 3,
      "context" : "Similar to [4], here we consider the tree loss: ∆(y, y′).",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 21,
      "context" : "1: The hierarchical structures of two selected subsets of the SUN dataset [22] used in our experiments for hierarchical image classification.",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 0,
      "context" : "5 Optimization of the Pascal image overlap criterion Object detection/localization has used the image area overlap as the loss function [1]–[3], e.",
      "startOffset" : 136,
      "endOffset" : 139
    }, {
      "referenceID" : 2,
      "context" : "5 Optimization of the Pascal image overlap criterion Object detection/localization has used the image area overlap as the loss function [1]–[3], e.",
      "startOffset" : 140,
      "endOffset" : 143
    }, {
      "referenceID" : 2,
      "context" : "In [3], a branch-and-bound search has been employed to find the global optimum.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 0,
      "context" : "This simple search strategy has been used in [1].",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 13,
      "context" : "Recently, structured SVM [14], [16] and a tree-based graph learning method [23] have been proposed to learn these parameters in a principled way.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 15,
      "context" : "Recently, structured SVM [14], [16] and a tree-based graph learning method [23] have been proposed to learn these parameters in a principled way.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 22,
      "context" : "Recently, structured SVM [14], [16] and a tree-based graph learning method [23] have been proposed to learn these parameters in a principled way.",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 15,
      "context" : "Note that our setting (21) differs most CRF learning settings such as [16].",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 15,
      "context" : "These traditional CRF methods often use a linear model [16].",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 13,
      "context" : "presented an image segmentation approach that uses nonlinear kernel for the unary energy term in the CRF model [14].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 15,
      "context" : "which can be solved efficiently by graph cuts [16], [24].",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 23,
      "context" : "which can be solved efficiently by graph cuts [16], [24].",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 15,
      "context" : "Similar to [16], the minimization (30) still can be solved efficiently by graph cuts.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 8,
      "context" : "We run experiments on some UCI machine learning datasets to compare our StructBoost formulation of binary boosting against the standard LPBoost [9].",
      "startOffset" : 144,
      "endOffset" : 147
    }, {
      "referenceID" : 8,
      "context" : "We compare the 1-slack StructBoost formulation of binary boosting agaisnt standard LPBoost [9] (i.",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 24,
      "context" : "Note that RankBoost may also be applied to this problem [25].",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 25,
      "context" : "We run our multi-class boosting on two image datasets: MNIST2 and Scene15 [26].",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 26,
      "context" : "Spatial pyramid HOG features [27] are used here.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 27,
      "context" : "An image is divided into 31 sub-windows in a spatial hierarchy manner [28].",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 28,
      "context" : "CENTRIST [29] is used as the feature descriptor.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 29,
      "context" : "ECC [30] and AdaBoost.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 30,
      "context" : "MH [31].",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 21,
      "context" : "We have constructed two hierarchical image datasets from the SUN dataset [22].",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 21,
      "context" : "We have used the HOG features as described in [22].",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 29,
      "context" : "ECC [30] and AdaBoost.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 30,
      "context" : "MH [31] on two image multi-class classification datasets: MNIST and Scene15.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 0,
      "context" : "5 Visual tracking by optimizing the image area overlap criterion In [1], a visual tracking method, termed Struck, was introduced based on SSVM.",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 0,
      "context" : "Struck50 is structured SVM tracking with a buffer size of 50 [1].",
      "startOffset" : 61,
      "endOffset" : 64
    }, {
      "referenceID" : 0,
      "context" : "Structured SVM of [1] is the second best, which confirms the usefulness of structured training.",
      "startOffset" : 18,
      "endOffset" : 21
    }, {
      "referenceID" : 0,
      "context" : "Struck50 is structured SVM tracking with a buffer size of 50 [1].",
      "startOffset" : 61,
      "endOffset" : 64
    }, {
      "referenceID" : 0,
      "context" : "We observe similar results as in Table 4: Our StructBoost outperforms other methods on all the sequences, and structured SVM of [1] is the second best.",
      "startOffset" : 128,
      "endOffset" : 131
    }, {
      "referenceID" : 31,
      "context" : "For HOG feature, we use the code in [32].",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 0,
      "context" : "We also compare our trackers with a few state-of-the-art tracking methods, including Struck [1] (with a buffer size (a) Testing images",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 32,
      "context" : "of 50), multi-instance tracking (MIL) [33], fragment tracking (Frag) [34], online AdaBoost tracking (OAB) [35], and visual tracking decomposition (VTD) [36].",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 33,
      "context" : "of 50), multi-instance tracking (MIL) [33], fragment tracking (Frag) [34], online AdaBoost tracking (OAB) [35], and visual tracking decomposition (VTD) [36].",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 34,
      "context" : "of 50), multi-instance tracking (MIL) [33], fragment tracking (Frag) [34], online AdaBoost tracking (OAB) [35], and visual tracking decomposition (VTD) [36].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 35,
      "context" : "of 50), multi-instance tracking (MIL) [33], fragment tracking (Frag) [34], online AdaBoost tracking (OAB) [35], and visual tracking decomposition (VTD) [36].",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 32,
      "context" : "See [33].",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 0,
      "context" : "The test video sequences “coke, tiger1, tiger2, david, girl and sylv” were used in [1].",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 35,
      "context" : "The sequences “shaking, singer” are obtained from [36], and the rest sequences are from [37].",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 36,
      "context" : "The sequences “shaking, singer” are obtained from [36], and the rest sequences are from [37].",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 0,
      "context" : "When Struck uses a Gaussian kernel defined on raw pixels, the performance is slightly different [1], and ours still outperforms Struck in most cases.",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 0,
      "context" : "This might be due to the fact that our StructBoost selects relevant features (300 features selected here), and the SSVM of [1] uses all the image patch information which may contain noises.",
      "startOffset" : 123,
      "endOffset" : 126
    }, {
      "referenceID" : 23,
      "context" : "The precision=recall point [24] and intersection-union score are used to evaluation our method.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 23,
      "context" : "6 CRF parameter learning for image segmentation In this experiment, we extend the super-pixels based segmentation method [24] with CRF parameter learning.",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 23,
      "context" : "We generate super-pixels and features same as in [24]: the neighborhood size is set to 2; histogram of visual words features are generated for each superpixel; code book size is 200.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 23,
      "context" : "Two unary potentials: U1, U2 are constructed using two AdaBoost classifiers; one is trained on the visual word histogram features [24], and the other is trained on color histogram together with the thumbnail feature [38].",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 37,
      "context" : "Two unary potentials: U1, U2 are constructed using two AdaBoost classifiers; one is trained on the visual word histogram features [24], and the other is trained on color histogram together with the thumbnail feature [38].",
      "startOffset" : 216,
      "endOffset" : 220
    }, {
      "referenceID" : 23,
      "context" : "at/∼pinz/ [24], which is able to discourage small isolated segments.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 23,
      "context" : "‖x − x‖2 calculates the `2 norm of the color difference between two super-pixels in the LUV color-space; `(x,x) is the shared boundary length between two super-pixels, as in [24].",
      "startOffset" : 174,
      "endOffset" : 178
    }, {
      "referenceID" : 23,
      "context" : "As [24], we use the precision = recall point and intersection-union score to evaluation our method.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 3,
      "context" : "5 CONCLUSION We have presented a boosting method for structural learning, as an alternative to SSVM [4] and CRF [10].",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 9,
      "context" : "5 CONCLUSION We have presented a boosting method for structural learning, as an alternative to SSVM [4] and CRF [10].",
      "startOffset" : 112,
      "endOffset" : 116
    } ],
    "year" : 2017,
    "abstractText" : "Structured learning has found many applications in computer vision recently. Analogues to structured support vector machines (SSVM), here we propose boosting algorithms for predicting multivariate or structured outputs, which is referred to as StructBoost. As SSVM generalizes SVM, our StructBoost generalizes standard boosting such as AdaBoost, or LPBoost to structured learning. AdaBoost, LPBoost and many other conventional boosting methods arise as special cases of StructBoost. The resulting optimization problem of StructBoost is more challenging than SSVM in the sense that the problem of StructBoost can involve exponentially many variables and constraints. In contrast, for SSVM one usually has an exponential number of constraints and a cutting-plane method is used. In order to efficiently solve StructBoost, we propose an equivalent 1-slack formulation and solve it using a combination of cutting planes and column generation. We show the versatility and usefulness of StructBoost on a few problems such as hierarchical multi-class classification, robust visual tracking and image segmentation. In particular, we train a tracking-by-detection based object tracker using the proposed structured boosting. Tracking is implemented as structured output prediction by maximizing the Pascal image area overlap criterion. We show that the structural tracker not only significantly outperforms conventional classification based trackers that do not directly optimize the Pascal image overlap criterion, but also outperforms many other state-of-the-art trackers on the tested videos.",
    "creator" : "TeX"
  }
}