{
  "name" : "1611.00740.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Why and When Can Deep – but Not Shallow – Networks Avoid the Curse of Dimensionality",
    "authors" : [ "Tomaso Poggio", "Hrushikesh Mhaskar", "Lorenzo Rosasco", "Brando Miranda", "Qianli Liao" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "This work was supported by the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF - 1231216. H.M. is supported in part by ARO Grant W911NF-15-10385.\n1\nar X\niv :1\nWhy and When Can Deep – but Not Shallow – Networks Avoid the Curse of Dimensionality\nTomaso Poggio1 Hrushikesh Mhaskar2 Lorenzo Rosasco1 Brando Miranda1 Qianli Liao1 1Center for Brains, Minds, and Machines, McGovern Institute for Brain Research, Massachusetts Institute of Technology, Cambridge, MA, 02139.\n2Department of Mathematics, California Institute of Technology, Pasadena, CA 91125; Institute of Mathematical Sciences, Claremont Graduate University, Claremont, CA 91711\nAbstract: The paper reviews an emerging body of theoretical results on deep learning including the conditions under which it can be exponentially better than shallow learning. Deep convolutional networks represent an important special case of these conditions, though weight sharing is not the main reason for their exponential advantage. Explanation of a few key theorems is provided together with new results, open problems and conjectures.\nKeywords: Deep and Shallow Networks, Convolutional Neural Networks, Function Approximation, Deep Learning"
    }, {
      "heading" : "1 A theory of deep learning",
      "text" : ""
    }, {
      "heading" : "1.1 Introduction",
      "text" : "There are at least three main sets of theory questions about Deep Neural Networks. The first set of questions is about the power of the architecture – which classes of functions can it approximate and learn well? The second set of questions is about the learning process: why is SGD (Stochastic Gradient Descent) so unreasonably efficient, at least in appearance? Do good local minima exist in deep networks? Are they easier to find in deep rather than in shallow networks? The third set of questions is about generalization properties of deep learning, since it appears that deep networks suffer much less from overfitting than classical shallow networks."
    }, {
      "heading" : "1.2 Relevance for hackers",
      "text" : "In this paper we summarize our recent results that have appeared over the last few months in several online papers (CBMM Memos 35, 37, 41, 45, 54 [Anselmi et al., 2015b],[Poggio et al., 2015c],[Poggio et al., 2015a] ,[Mhaskar et al., 2016],[Mhaskar and Poggio, 2016]) and that include answers to why and when deep networks are better than shallow. We then describe several corollaries of the main theorems as well as a few conjectures and open questions.\nThe main implications likely to be relevant in practice are:\n1. among the deep learning architectures used in practice, for all practical purposes only deep convolutional architectures – but not densely connected deep networks – are guaranteed to be much better than one layer architectures such as kernel machines (for specific problems, see next);\n2. the problems for which deep networks are exponentially better than other nets correspond to input-output mappings that are compositional functions composed of hierarchically local constituent functions: an example is f(x1, · · · , x8) = h3(h21(h11(x1, x2), h12(x3, x4)), h22(h13(x5, x6), h14(x7, x8))). The compositional function f requires only “local” computations (here with just dimension 2) in each of its constituent functions h;\n3. the key aspect of convolutional network that gives them an exponential advantage is not weight sharing but locality at each level of the hierarchy."
    }, {
      "heading" : "2 Previous theoretical work",
      "text" : "Deep Learning references start with Hinton’s backpropagation and with Lecun’s convolutional networks (see for a nice review [LeCun et al., 2015]). Of course, multilayer convolutional networks have been around at least as far back as the optical processing era of the 70s. The Neocognitron ([Fukushima, 1980]) was a convolutional neural network that was trained to recognize characters. The property of compositionality was a main motivation for hierarchical models of visual cortex such as HMAX which can be regarded as a pyramid of AND and OR layers ([Riesenhuber and Poggio, 1999]), that is a sequence of conjunctions and disjunctions. Several papers in the ’80s focused on the approximation power and learning properties of one-hidden layer networks (called shallow networks here). Very little appeared on multilayer networks, (but see [Mhaskar, 1993a, Chui et al., 1994, Chui et al., 1996]), mainly because one hidden layer nets performed empirically as well as deeper networks. On the theory side, a review by Pinkus in 1999 ([Pinkus, 1999]) concludes that “...there seems to be reason to conjecture that the two hidden layer model may be significantly more promising than the single hidden layer model...”. A version of the questions about the importance of hierarchies was asked in [Poggio and Smale, 2003] as follows: “A comparison with real brains offers another, and probably related, challenge to learning theory. The “learning algorithms” we have described in this paper correspond to one-layer architectures. Are hierarchical architectures with more layers justifiable in terms of learning theory? It seems that the learning theory of the type we have outlined does not offer any general argument in favor of hierarchical learning machines for regression or classification. This is somewhat of a puzzle since the organization of cortex – for instance visual cortex – is strongly hierarchical. At the same time, hierarchical learning systems show superior performance in several engineering applications.” Because of the great empirical success of deep learning over the last three years, several papers addressing the question of why hierarchies have appeared. SumProduct networks, which are equivalent to polynomial networks (see [B. Moore and Poggio, 1998, Livni et al., 2013]), are a simple case of a hierarchy that was analyzed ([Delalleau and Bengio, 2011]) but did not provide any useful insight. Montufar and Bengio[Montufar et al., 2014] showed that the number of linear regions that can be synthesized by a deep network with ReLU nonlinearities is much larger than by a shallow network. However, they did not study the conditions under which this property yields better learning performance. In fact we will show later that the power of a deep network can be exploited for certain classes of functions but not for general functions. Examples of specific functions that cannot be represented efficiently by shallow networks have been given very recently by [Telgarsky, 2015]. Most relevant to the papers reviewed here is the work on hierarchical quadratic networks ([Livni et al., 2013]), together with function approximation results ([Mhaskar, 1993b, Pinkus, 1999]). Also relevant is\nthe conjecture by Shashua (see [Cohen et al., 2015]) on a connection between deep learning networks and the hierarchical Tucker representations of tensors. Our theorems show for which class of functions the conjecture holds. This paper describes and extends results presented in [Anselmi et al., 2014, Anselmi et al., 2015a, Poggio et al., 2015b] [Mhaskar et al., 2016] and in [Liao and Poggio, 2016]."
    }, {
      "heading" : "3 Main results",
      "text" : "In this section, we state theorems about the approximation properties of shallow and deep networks."
    }, {
      "heading" : "3.1 Degree of approximation",
      "text" : "The general paradigm is as follows. We are interested in determining how complex a network ought to be to theoretically guarantee approximation of an unknown target function f up to a given accuracy > 0. To measure the accuracy, we need a norm ‖ · ‖ on some normed linear space X. As we will see the norm used in the results of this paper is the sup norm in keeping with the standard choice in approximation theory. Notice, however, that from the point of view of machine learning, the relevant norm is the L2 norm. In this sense, several of our results are stronger than needed. On the other hand, our main results on compositionality require the sup norm in order to be immediately useful in a machine learning context.\nLet VN be the be set of all networks of a given kind with complexity N which we take here to be the total number of units in the network (e.g., all shallow networks with N units in the hidden layer). It is assumed that the class of networks with a higher complexity include those with a lower complexity; i.e., VN ⊆ VN+1. The degree of approximation is defined by\ndist(f, VN ) = inf P∈VN\n‖f − P‖. (1)\nFor example, if dist(f, VN ) = O(N−γ) for some γ > 0, then a network with complexity N = O( − 1 γ ) will be sufficient to guarantee an approximation with accuracy at least . Since f is unknown, in order to obtain theoretically proved upper bounds, we need to make some assumptions on the class of functions from which the unknown target function is chosen. This a priori information is codified by the statement that f ∈W for some subspace W ⊆ X. This subspace is usually a smoothness class characterized by a smoothness parameter m. Here it will be generalized to a smoothness and compositional class, characterized by the parameters m and d (d = 2 in the example of Figure 1)."
    }, {
      "heading" : "3.2 Previous main results",
      "text" : "This section characterizes conditions under which deep networks are “better” than shallow network in approximating functions. Thus we compare shallow (one-hidden layer) networks with deep networks as shown in Figure 1. Both types of networks use the same small set of operations – dot products, linear combinations, a fixed nonlinear function of one variable, possibly convolution and pooling. In particular each node in the network contains a certain number of units. A unit is a neuron which computes\n| 〈x,w〉+ b|+, (2)\nwhere w is the vector of weights on the vector input x. Both t and the real number b are parameters tuned by learning. Notice that\nthe linear combination of r such nodes ∑r i=1 ci|(〈x, ti〉 + bi)|+ corresponds to ridge approximation which is universal.\nThe logic of our theorems is as follows.\n• Both shallow (a) and deep (b) networks are universal, that is they can approximate arbitrarily well any continuous function of n variables on a compact domain. The result for shallow networks is classical. For constrained deep networks, such as the binary tree architectures considered here, consider appropriate linear combination of units at the top node.\n• We consider a special class of functions of n variables on a compact domain that are a hierarchical compositions of functions such as\nf(x1, · · · , x8) = h3(h21(h11(x1, x2), h12(x3, x4)), h22(h13(x5, x6), h14(x7, x8))) (3)\nThe structure of the function in equation 3 is represented by a graph of the binary tree type. This is the simplest example of compositional functions, reflecting dimensionality d = 2 for the constituent functions h. In general, d is arbitrary but fixed and independent of the dimensionality n of the compositional function f . In our results we will often think of n increasing while d is fixed.\n• The approximation of functions with a compositional structure – can be achieved with the same degree of accuracy by deep and shallow networks but that the number of parameters are much smaller for the deep networks than for the shallow network with equivalent approximation accuracy. It is intuitive that a hierarchical network matching the structure of a compositional function should be “better” at approximating it than a generic shallow network but universality of shallow networks aks for non-obvious characterization of “better”. Our result makes clear that the intuition is indeed correct and provides proofs and quantitative bounds.\nWe assume that the shallow networks do not have any structural information on the function to be learned (here its compositional structure), because they cannot represent it directly. Deep networks with standard architectures on the other hand do represent compositionality and can be adapted to the details of such prior information.\nWe approximate functions of n variables of the form of Equation (3) with networks in which the activation nonlinearity is a smoothened version of the so called ReLU, originally called ramp by Breiman and given by σ(x) = x+ = max(0, x) . The architecture of the deep networks reflects Equation (3) with each node hi being a ridge function, comprising one or more neurons.\nLet In = [−1, 1]n, X = C(In) be the space of all continuous functions on In, with ‖f‖ = maxx∈In |f(x)|. Let SN denote the class of all shallow networks with N units of the form\nx 7→ N∑ k=1 akσ(〈wk, x〉+ bk),\nwhere wk ∈ Rn, bk, ak ∈ R. The number of trainable parameters here is (n+2)N ∼ n. Letm ≥ 1 be an integer, andWnm be the set of all functions of n variables with continuous partial derivatives of orders up to m <∞ such that ‖f‖+ ∑ 1≤|k|1≤m ‖D\nkf‖ ≤ 1, where Dk denotes the partial derivative indicated by the multi– integer k ≥ 1, and |k|1 is the sum of the components of k.\nFor the hierarchical binary tree network, the analogous spaces are defined by considering the compact set Wn,2m to be the class of all compositional functions f of n variables with a binary tree architecture and constituent functions h in W 2m. We define the corresponding class of deep networks DN,2 to be the set of all deep networks with a binary tree architecture, where each of the constituent nodes is in S2. We note that in the case when n is an integer power of 2, the total number of parameters involved in a deep network in DN,2 – that is, weights and biases, is 4N .\nThe following two theorems estimate the degree of approximation for shallow and deep networks. Two observations are critical to understand the meaning of our results:\n• compositional functions of n variables are a subset of functions of n variables, that is Wnm ⊇Wn,2m . Deep networks can exploit in their architecture the special structure of compositional functions, whereas shallow networks are blind to it. Thus from the point of view of shallow networks, functions in Wn,2m are just functions in Wnm; this is not the case for deep networks.\n• the deep network does not need to have exactly the same compositional architecture as the compositional function to be approximated. It is sufficient that the acyclical graph representing the structure of the function is a subgraph of the graph representing the structure of the deep network. The degree of approximation estimates depend on the graph associated with the network and are thus an upper bound on what could be achieved by a network exactly matched to the function architecture.\nThe first theorem [Mhaskar, 1996] is about shallow networks.\nTheorem 1. Let σ : R→ R be infinitely differentiable, and not a polynomial. For f ∈Wnm the complexity of shallow networks that provide accuracy at least is\nN = O( −n/m) and is the best possible. (4)\nNotes In [Mhaskar, 1996, Theorem 2.1], the theorem is stated under the condition that σ is infinitely differentiable, and there exists b ∈ R such that σ(k)(b) 6= 0 for any integer k ≥ 0. It is proved in [Corominas and Balaguer, 1954] that the second condition is equivalent to σ not being a polynomial. The proof in [Mhaskar, 1996] relies on the fact that under these conditions on σ, the algebraic polynomials in n variables of (total or coordinatewise) degree < q are in the uniform closure of the span ofO(qn) functions of the form x 7→ σ(w·x+b) (see Appendix 4.1). The estimate itself is an upper bound on the degree of approximation by such polynomials. Since it is based on the approximation of the polynomial space contained in the ridge functions implemented by shallow networks, one may ask whether it could be improved by using a different approach. The answer relies on the concept of nonlinear n–width of the compact set Wnm (cf. [DeVore et al., 1989, Mhaskar et al., 2016]). The n-width results imply that the estimate in Theorem (1) is the best possible among all reasonable [DeVore et al., 1989] methods of approximating arbitrary functions in Wnm.\nThe condition of a smooth activation function can be relaxed to prove density (see [Pinkus, 1999], Proposition 3.7):\nProposition 1. Let σ =: R→ R be in C0, and not a polynomial. Then shallow networks are dense in C0.\nIn particular, ridge functions using ReLUs of the form∑r i=1 ci(〈wi, x〉+ bi)+, with wi, x ∈ R\n2, ci, bi ∈ R are dense in C.\nNetworks with non-smooth activation functions are expected to do relatively poorly in approximating smooth functions such as polynomials in the sup norm. “Good” degree of approximation rates (modulo a constant) have been proved in the L2 norm. Define B the unit ball in Rn. Call Cm(Bn) the set of all continuous functions with continuous derivative up to degree m defined on the unit ball. We define the Sobolev space Wmp as the completion of Cm(Bn) with respect to the Sobolev norm p (see for details [Pinkus, 1999] page 168). We define the space Bmp = {f : f ∈ Wmp , ‖f‖m,p ≤ 1} and the approximation errorE(Bm2 ;H;L2) = supf∈Bm2\n= E(f ;H;L2) = infg∈H ‖f − g‖L2 . It is shown in [Pinkus, 1999, Corollary 6.10] that\nProposition 2. ForMr : f(x) = ∑r i=1 ci|(〈wi, x〉+bi)+ it holds E(Bm2 ;Mr;L2) ≤ Cr− m n for m = 1, · · · , n+3 2 .\nOur second and main theorem is about deep networks with smooth activations and is new (preliminary versions appeared in [Poggio et al., 2015a, Poggio et al., 2015c, Mhaskar et al., 2016]). We formulate it in the binary tree case for simplicity but it extends immediately to functions that are compositions of constituent functions of a fixed number of variables d instead than of d = 2 variables as in the statement of the theorem.\nTheorem 2. For f ∈ Wn,2m consider a deep network with the same compositonal architecture and with an activation function σ : R→ R which is infinitely differentiable, and not a polynomial. The complexity of the network to provide approximation with accuracy at least is\nN = O((n− 1) −2/m). (5)\nProof To prove Theorem 2, we observe that each of the constituent functions being in W 2m, (1) applied with n = 2 implies that each of these functions can be approximated from SN,2 up to accuracy = cN−m/2. Our assumption that f ∈WN,2m implies that each of these constituent functions is Lipschitz continuous. Hence, it is easy to deduce that, for example, if P , P1, P2 are approximations to the constituent functions h, h1, h2, respectively within an accuracy of , then since ‖h − P‖ ≤ , ‖h1 − P1‖ ≤ and ‖h2 − P2‖ ≤ , then ‖h(h1, h2) − P (P1, P2)‖ = ‖h(h1, h2) − h(P1, P2) + h(P1, P2) − P (P1, P2)‖ ≤ ‖h(h1, h2) − h(P1, P2)‖ + ‖h(P1, P2)− P (P1, P2)‖ ≤ c by Minkowski inequality. Thus\n‖h(h1, h2)− P (P1, P2)‖ ≤ c , for some constant c > 0 independent of the functions involved. This, together with the fact that there are (n− 1) nodes, leads to (5).\nDefinition 1. The effective dimension of the class W of functions (for a given norm) is said to be d if for every > 0, any function in W can be recovered within an accuracy of (as measured by the norm) using a network (shallow or deep) with −d parameters.\nThus, the effective dimension for the class Wnm is n/m, that of Wn,2m is 2/m.\nRemarks\n1. Both shallow and deep representations may or may not reflect invariance to group transformations of the inputs of the function ([Soatto, 2011, Anselmi et al., 2015a]). Invariance – also called weight sharing – decreases the complexity of the network. Since we are interested in the comparison of shallow vs deep architectures, here we consider the generic case of networks\n(and functions) for which invariance is not assumed. In fact, the key advantage of deep vs. shallow network – as shown by the proof of the theorem – in learning compositional functions is the associated hierarchical locality (the constituent functions in each node are local that is have a small dimensionality) and not invariance (which designates shared weights that is nodes at the same level sharing the same function). Invariance of course helps but not exponentially as hierarchical locality does.\n2. The estimates on the n–width imply that there is some function in eitherWnm (theorem 1) orWn,2m (theorem 2) for which the approximation cannot be better than that suggested by the theorems above. This is of the course the guarantee we want.\n3. The constants involved inO in the theorems will depend upon the norms of the derivatives of f as well as σ.\n4. When the only a priori assumption on the target function is about the number of derivatives, then to guarantee an accuracy of , we need a shallow network with O( −n/m) trainable parameters. If we assume a hierarchical structure on the target function as in Theorem 2, then the corresponding deep network yields a guaranteed accuracy of with O( −2/m) trainable parameters.\n5. Theorem 2 applies to all f with a compositional architecture given by a graph which correspond to, or is a subgraph of, the graph associated with the deep network – in this case the graph corresponding to Wn,dm .\n6. The assumptions on σ in the theorems are not satisfied by the ReLU function x 7→ x+, but they are satisfied by smoothing the function in an arbitrarily small interval around the origin. This suggests that the result of the theorem should be valid also for the non-smooth ReLU but the constant in the estimate can be quite different.\n7. Similar – actually stronger – results (see [Mhaskar and Poggio, 2016]) hold for networks where each channel evaluates a Gaussian non–linearity; i.e., Gaussian networks of the form\nG(x) = N∑ k=1 ak exp(−|x− wk|2), x ∈ Rd (6)\nwhere the approximation is on the entire Euclidean space.\n8. In a machine learning context, minimization over a training set of a loss function such as the square loss yields an empirical approximation of the regression function p(y/x). Our hypothesis of compositionality becomes an hypothesis about the structure of the conditional probability function."
    }, {
      "heading" : "3.3 Extension: functions composed by a hierarchy of functions with bounded effective dimensionality",
      "text" : "The main class of functions we considered in previous papers consists of functions as in Figure 1b that have been called compositional functions. The term “compositionality” was used with the meaning it has in language and vision, where higher level concepts are composed of a small number of lower level ones, objects are composed of parts, sentences are composed of words and words are composed of syllables. Notice that this meaning of compositionality is narrower than the mathematical meaning of composition of functions. The compositional functions we have described in previous papers may be more precisely called functions composed of hierarchically local functions.\nHere we generalize our previous results to a broader class of compositional functions by restating formally a few comments and examples in previous papers. To motivate this discussion, let\nQ(x, y) = (Ax2y2 +Bx2y\n+Cxy2 +Dx2 + 2Exy\n+Fy2 + 2Gx+ 2Hy + I)2 10 .\nSince Q is nominally a polynomial of coordinatewise degree 211, [Mhaskar, 1996, Lemma 3.2] shows that a shallow network with 211 + 1 units is able to approximate Q arbitrarily well on Id. However, because of the hierarchical structure of Q, [Mhaskar, 1996, Lemma 3.2] shows also that a hierarchical network with 9 units can approximate the quadratic expression, and 10 further layers, each with 3 units can approximate the successive powers. Thus, a hierarchical network with 11 layers and 39 units can approximate Q arbitrarily well. We note that even if Q is nominally of degree 211, each of the monomial coefficients in Q is a function of only 9 variables, A, · · · , I .\nA different example is\nQ(x, y) = |x2 − y2|. (7)\nThis is obviously a Lipschitz continuous function of 2 variables. The effective dimension of this class is 2, and hence, a shallow network would require at least c −2 parameters to approximate it within . However, the effective dimension of the class of univariate Lipschitz continuous functions is 1. Hence, if we take into account the fact that Q is a composition of a polynomial of degree 2 in 2 variables and the univariate Lipschitz continuous function t 7→ |t|, then it is easy to see that the same approximation can be achieved by using a two layered network with O( −1) parameters.\nTo formulate our most general result, we first define formally a compositional function in terms of a directed acyclic graph. Let G be a directed acyclic graph (DAG), with the set of nodes V . A G–function is defined as follows. Each of the source node obtains an input from R. Each in-edge of every other node represents an input real variable, and the node itself represents a function of these input real variables, called a constituent function. The out-edges fan out the result of this evaluation. We assume that there is only one sink node, whose output is the G-function. Thus, ignoring the compositionality of this function, it is a function of n variables, where n is the number of source nodes in G.\nTheorem 3. Let G be a DAG,n be the number of source nodes, and for each v ∈ V , let dv be the number of in-edges of v. Let f : Rn 7→ R be a compositional G-function, where each of the constitutent function is in W dvmv . Consider shallow and deep networks with infinitely smooth activation function as in Theorem 1. Then deep networks – with an associated graph that corresponds to the graph of f – avoid the curse of dimensionality in approximating f for increasing n, whereas shallow networks cannot directly avoid the curse. In particular, the complexity of the best approximating shallow network is exponential in n\nNs = O( − n m ), (8)\nwhere m = minv∈V mv , while the complexity of the deep network is Nd = O( ∑ v∈V −dv/mv ). (9)\nLet us define dv/mv as the effective dimension of function v. Then, deep networks can avoid the curse of dimensionality if the constituent functions of a compositional function have a small effective\ndimension; i.e., have fixed, “small” dimensionality or fixed, “small” “roughness. A different interpretation of Theorem 3 is the following.\nProposition 3. If a family of functions f : Rn 7→ R of smoothness m has an effective dimension < n/m, then the functions are compositional in a manner consistent with the estimates in Theorem 3."
    }, {
      "heading" : "3.4 Approximation results for shallow and deep networks with standard, non-smooth ReLUs",
      "text" : "The results we described so far use smooth activation functions. We already mentioned why relaxing the smoothness assumption should not change our results in a fundamental way. While studies on the properties of neural networks with smooth activation abound, the results on non-smooth activation functions are much more sparse. Here we briefly recall some of them. Approximation results with respect to the L2 norm are classical, see Corollary 6.10 in [Pinkus, 1999], but they cannot be applied to derive bounds for compositional networks. Indeed, in the latter case, as we remarked already, estimates in the uniform norm are needed to control the propagation of the errors from one layer to the next, see Theorem 2. Results in this direction are given in [Mhaskar, 2004], and more recently in [Bach, 2014] and [Mhaskar and Poggio, 2016]. In particular, using a result in [Bach, 2014] and following the proof strategy of Theorem 2 it is possible to derive the following results on the approximation of Lipshitz continuous functions with deep and shallow ReLU networks that mimics our Theorem 2:\nTheorem 4. Let f be a L-Lipshitz continuous function of n variables. Then, the complexity of a network which is a linear combination of ReLU providing an approximation with accuracy at least is\nNs = O ((\nL\n)−n) ,\nwheres that of a deep compositional architecture is Nd = O ((\nn− 1)( L\n)−2) .\nThe above result is an example of how the analysis of smooth activation functions can be adapted to ReLU. Indeed, it shows how deep compositional networks can avoid the curse of dimensionality. In the above results, the regularity of the function class is quantified by the magnitude of Lipshitz constant. Whether the latter is best notion of smoothness for ReLU based networks, and if the above estimates can be improved, are interesting questions that we defer to a future work. A result that is more intuitive and may reflect what networks actually do is described in Appendix 4.3. The construction described there, however, provides approximation in the L2 norm but not in the sup norm.\nFigure 2, Figure 3, Figure4 and Figure 5 provide a sanity check and empirical support for our main results and for the claims in section 1.2."
    }, {
      "heading" : "3.5 Generalization bounds",
      "text" : "Our estimate of the number of units and parameters needed for a deep network to approximate compositional functions with an error G allow the use of one of several available bounds for the generalization error of the network to derive sample complexity bounds. As an example consider theorem 16.2 in [Anthony and Bartlett, 2002] which provides the following sample bound for a generalization error G with probability at least 1 − δ in a network in which the W parameters (weights and biases) which are supposed to minimize the empirical error (the theorem is stated in the standard ERM setup) are expressed in terms of k bits:\nM( G, δ) ≤ 2\n2G (kW log 2 + log(\n2 δ )) (10)\nThis suggests the following comparison between shallow and deep compositional (here binary tree-like networks). Assume a network size that ensure the same approximation error .\nThen in order to achieve the same generalization error G, the sample size Mshallow of the shallow network must be much larger than the sample size Mdeep of the deep network:\nMdeep Mshallow ≈ n. (11)\nThis implies that for largish n there is a (large) range of training set sizes between Mdeep and Mshallow for which deep networks will not overfit (corresponding to small G) but shallow networks will (for dimensionality n ≈ 104 and ≈ 0.1 Equation 11 yields mshallow ≈ 1010 4 mdeep).\nA similar comparison is derived if one considers the best possible expected error obtained by a deep and a shallow network. Such an error is obtained finding the architecture with the best trade-off between the approximation and the estimation error. The latter is essentially of the same order as the generalization bound implied by inequality (10), and is essentially the same for deep and shallow networks, that is\nrn√ M ,\nwhere we denoted by M the number of samples. For shallow networks, the number of parameters corresponds to r units of n dimensional vectors (plus off-sets), whereas for deep compositional networks the number of parameters correspond to r units of 2 dimensional vectors (plus off-sets) in each of the n − 1 units. The\nnumber of units giving the best approximation/estimation trade-off is\nrshallow ≈ (√ M\nn\n) n m+n\nand rdeep ≈ (√ M ) 2 m+2\nfor shallow and deep networks, respectively. The corresponding (excess) expected errors E are\nEdeep ≈ (\nn√ M\n) m m+n\nfor shallow networks and Eshallow ≈ n (\n1√ M\n) m m+2\nfor deep networks. For the expected error, as for the generalization error, deep networks appear to achieve an exponential gain. The above observations hold under the assumption that the optimization process during training finds the optimum parameters values for both deep and shallow networks. Taking into account optimization, e.g. by stochastic gradient descent, requires considering a further error term, but we expect that the overall conclusions about generalization properties for deep vs. shallow networks should still hold true."
    }, {
      "heading" : "4 Connections with the theory of Boolean functions",
      "text" : "The approach followed in our main theorems suggest the following considerations (see Appendix 1 for a brief introduction). The structure of a deep network is reflected in polynomials that are best\napproximated by it – for instance generic polynomials or sparse polynomials (in the coefficients) in d variables of order k. The tree structure of the nodes of a deep network reflects the structure of a specific sparse polynomial. Generic polynomial of degree k in d variables are difficult to learn because the number of terms, trainable parameters and associated VC-dimension are all exponential in d. On the other hand, functions approximated well by sparse polynomials can be learned efficiently by deep networks with a tree structure that matches the polynomial. We recall that in a similar way several properties of certain Boolean functions can be “read out” from the terms of their Fourier expansion corresponding to “large” coefficients, that is from a polynomial that approximates well the function.\nClassical results [Hastad, 1987] about the depth-breadth tradeoff in circuits design show that deep circuits are more efficient in representing certain Boolean functions than shallow circuits. Hastad proved that highly-variable functions (in the sense of having high frequencies in their Fourier spectrum), in particular the parity function cannot even be decently approximated by small constant depth circuits (see also [Linial et al., 1993]). A closely related result follow immediately from our main theorem since functions of real variables of the form x1x2...xd have the compositional form of the binary tree (for d even). Restricting the values of the variables to −1,+1 yields\nProposition 4. The family of parity functions x1x2...xd with xi ∈ {−1,+1} and i = 1, · · · , xd can be represented with exponentially fewer units by a deep than a shallow network.\nNotice that Hastad’s results on Boolean functions have been often quoted in support of the claim that deep neural networks can represent functions that shallow networks cannot. For instance Bengio and LeCun [Bengio and LeCun, 2007] write “We claim that most functions that can be represented compactly by deep architectures cannot be represented by a compact shallow architecture”.”.\nFinally, we want to mention a few other observations on Boolean functions that shows an interesting connection with our approach. It is known that within Boolean functions the AC0 class of polynomial size constant depth circuits is characterized by Fourier transforms where most of the power spectrum is in the low order coefficients. Such functions can be approximated well by a polynomial of low degree and can be learned well by considering only such coefficients. In general, two algorithms [Mansour, 1994] seems to allow learning of certain Boolean function classes:\n1. the low order algorithm that approximates functions by considering their low order Fourier coefficients and\n2. the sparse algorithm which learns a function by approximating its significant coefficients.\nDecision lists and decision trees can be learned by the first algorithm. Functions with small L1 norm can be approximated well by the second algorithm. Boolean circuits expressing DNFs can be approximated by the first one but even better by the second. In fact, in many cases a function can be approximated by a small set of coefficients but these coefficients do not correspond to low-order terms. All these cases are consistent with the notes about sparse functions in section 5."
    }, {
      "heading" : "5 Notes on a theory of compositional computation",
      "text" : "The key property of the theory of compositional functions sketched here is that deep networks can learn them avoiding the curse of dimensionality because of the blessing of effective compositional dimension.\nWe state here informally several comments and conjectures.\n1. General comments\n• We have briefly mentioned in the previous section that the approximating deep network does not need to exactly match the architecture of the compositional function as long as the graph or tree associated with the function is contained in the graph associated with the network. It may otherwise be difficult to estimate the exact compositional architecture of a new function to be learned. We have shown that for a given class of compositional functions characterized by an associated graph there exist a deep network that approximates such a function better than a shallow network. The same network approximates well functions characterized by subgraphs of the original class.\n• It is obvious that linear combination of compositional functions are universal in the sense that they can approximate any function and that deep networks with a number of units that increases exponentially with layers can approximate any function. In a similar way, a binary tree with squaring nodes and enough channels per node can represent exactly any polynomial in d inputs of degree d. The proof consists of noticing that the number of monomials in a polynomial in 2 variables with total degree ≤ 2 is ( 2+2 2 ) = 6 and can be\nwritten as a linear combination of 6 terms of the form (〈w, x〉+ b)2. In particular, each node can synthesize the monomials x2, xy, · · · . Thus the top node can represent any polynomial of degree d in the d variables. These arguments suggest that the number of layers and\nthe number of nodes per layer and units per node – that is, the design of the architecture – constrains the type of approximation, effectively specifying the type of sparsity of the polynomial represented by network to approximate the function to be learned from supervised data.\n• In our main theorem we have proved that deep nets can be exponentially better than shallow networks for learning compositional functions. We suspect the following deep shortcoming: Deep networks are no better than shallow ones for functions that are not compositional.\n• The simplest compositional function – addition – is trivial in the sense that it offers no approximation advantage to deep networks. A key function is multiplication which is for us the prototypical compositional functions. It is not an accident that in the case of Boolean variables the parity function f(x1, ...xn) = x1 · · ·xn is at the core of the original Hastad result ([Hastad, 1987]).\n• Function compositionality can be expressed in terms of DAGs, with binary trees providing the simplest example. There is an analogy with n-dimensional tables decomposable in hierarchical 2-dimensional tables: this has in turn obvious connections with HVQ (Hierarchical Vector Quantization), dicussed in the Appendix of the Arxiv version of this paper.\n• Our results apply well beyond binary trees network architectures to most deep convolutional architectures. In particular, they apply to the very deep convolutional networks of the ResNet type.\n• We have used polynomials (but see Appendix 4.3) to prove results about complexity of approximation in the case of neural networks. Neural network learning with SGD may or may not synthesize polynomial, depending on the activation function and on the target. This is not a problem for theoretically establishing bounds on the degree of convergence because results using the framework on nonlinear width guarantee the “polynomial” bounds are optimal.\n• What is the relation of these results with itheory[Anselmi and Poggio, 2016].? Our theorems may explain why hierarchical networks are better than shallow ones for learning compositional functions. The original core of i-theory describes how pooling can provide either shallow or deep networks with invariance and selectivity properties. Invariance can reduce further the complexity of a network but compositionality play the key role.\n• There are several properties that follow from the theory here which are attractive from the point of view of neuroscience. A main one is the robustness of the results with respect to the choice of nonlinearities (linear rectifiers, sigmoids, Gaussians etc.) and pooling.\n• A recent theorem by Telgarsky [Telgarsky, 2015] can be summarized as saying that a certain family of classification problems with real-valued inputs cannot be approximated well by shallow networks with fewer than exponentially many nodes whereas a deep network achieves zero error. This is a special case of our results and corresponds to high-frequency, sparse trigonometric polynomials. The result can be proved directly from our main theorem by considering the real-valued polynomials x1x2...xd defined on the cube (−1, 1)d. This is a compositional functions for which our main results holds in terms of shallow vs deep networks.\n• Recently, several authors have noticed that the last hidden unit layer contains information about tasks different from the training one (e.g. [Yamins et al., 2014]). This is to be expected. The last layer of HBF is rather independent of the training target and mostly depends on the input\npart of the training set (see theory and gradient descent equations in Poggio and Girosi, [Poggio and Girosi, 1989] for the one-hidden layer case). This is exactly true for onehidden layer RBF networks and holds approximatively for HBFs. The weights from the last hidden layer to the output are instead task/target dependent.\n2. Spline approximations and Boolean functions\n• Consider again the case of section 4 of a multivariate function f : [0, 1]d → R. Suppose to discretize it by a set of piecewise constant splines and their tensor products. Each coordinate is effectively replaced by n boolean variables.This results in a d-dimensional table with N = nd entries. This in turn corresponds to a boolean function f : {0, 1}N → R. Here, the assumption of compositionality corresponds to compressability of a d-dimensional table in terms of a hierarchy of d− 1 2-dimensional tables. Instead of nd entries there are (d− 1)n2 entries. It seems that assumptions of the compositionality type may be at least as effective as assumptions about function smoothness in countering the curse of dimensionality in learning and approximation.\n• As Appendix 4.3 shows, every function f can be approximated by an epsilon-close binary function fB . Binarization of f : Rn → R is done by using k partitions for each variable xi and indicator functions. Thus f 7→ fB : {0, 1}kn → R and sup|f − fB | ≤ , with depending on k and bounded Df .\n• If the function f is compositional the associated Boolean functions fB is sparse; the converse is not true.\n• fB can be written as a polynomial (a Walsh decomposition) fB ≈ pB . It is always possible to associate a pb to any f , given .\n• The binarization argument suggests a direct way to connect results on function approximation by neural nets with older results on Boolean functions. The latter are special cases of the former results.\n3. Sparsity\n• We suggest to define binary sparsity of f , in terms of the sparsity of the boolean function pB ; binary sparsity implies that an approximation to f can be learned by non-exponential deep networks via binarization.\n• Sparsity of Fourier coefficients is a a more general constraint for learning than Tikhonov-type smoothing priors. The latter is usually equivalent to cutting high order Fourier coefficients. The two priors can be regarded as two different implementations of sparsity, one more general than the other. Both reduce the number of terms, that is trainable parameters, in the approximating trigonometric polynomial – that is the number of Fourier coefficients.\n• A set of functions may be defined to be sparse in a specific basis when the number of parameters necessary for its -approximation increases less than exponentially with the dimensionality. An open question is the appropriate definition of sparsity. The notion of sparsity we suggest here is the effective r in Equation 21. For a general function r ≈ kn; we may define sparse functions those for which r << kn in\nf(x) ≈ P ∗k (x) = r∑ i=1 pi(〈wi, x〉). (12)\nwhere P ∗ is a specific polynomial that approximates f(x) within the desired . Notice that the polynomial P ∗k can be a sum of monomials or a sum of, for instance,\northogonal polynomials with a total of r parameters. In general, sparsity depends on the basis and one needs to know the basis and the type of sparsity to exploit it in learning, for instance with a deep network with appropriate activation functions and architecture. There are function classes that are sparse in every bases. Examples are compositional functions described by a binary tree graph.\n4. Tensors\n• One can think about tensors in terms of d-dimensional tables. The framework of hierarchical decompositions of tensors – in particular the Hierarchical Tucker format – seems closely connected to our notion of compositionality. Interestingly, the hierarchical Tucker decomposition has been the subject of recent papers on Deep Learning (for instance see [Cohen et al., 2015]). This work, as well more classical papers [Grasedyck, 2010], does not seem to consider directly the class of functions for which these decompositions are effective. Our results provide a rigorous grounding to the tensor work related to deep learning. There is obviously a wealth of interesting connections with approximation theory. The idea of hierarchical set of tables is related to hierarchical VQ. It is also indirectly related to the separation rank of a tensor which is almost identical with the effective r in Equation 21. Notice that for a boolean function a full hierarchy of tables ends with a 2-by-2 binary table or with n-dimensional binarization of the outp in R.\n• Notice that tensor decompositions assume that the sum of polynomial functions of order d is sparse (see eq. at top of page 2030 of [Grasedyck, 2010]).\n5. Theory of computation, locality and compositionality\n• Our Theorems are closely related to the compression argument of [Poggio et al., 2015c], see Appendices.\n• Approximation properties can be related to the notion of connectivity of a network. Connectivity is a key property in network computations. Local processing may be a key constraint also in neuroscience. One of the natural measures of connectivity that can be introduced is the order of a node defined as the number of its distinct inputs. The order of a network is then the maximum order among its nodes. The term order dates back to the Perceptron book ([Minsky and Papert, 1972], see also [Poggio and Reichardt, 1980]). From the previous observations, it follows that a hierarchical network of order at least 2 can be universal.\n• In the Perceptron book many interesting visual computations have low order (e.g. recognition of isolated figures). The message is that they can be implemented in a single layer by units that have a small number of inputs. More complex visual computations require inputs from the full visual field. A hierarchical network can achieve effective high order at the top using units with low order.\n• The network architecture of Figure 1 b) has low order: each node in the intermediate layers is connected to just 2 other nodes, rather than (say) all nodes in the previous layer (notice that the connections in the trees of the figures may reflect linear combinations of the input units).\n• Low order may be a key constraint for cortex. If it captures what is possible in terms of connectivity between neurons, it may determine by itself the hierarchical architecture of the ventral stream.\n• On the other hand, a low-order hierarchy may be the computational consequence of the compositionality of the visual world and the related effectiveness of learning first common parts or features and more complex objects later in the hierarchy. This is particulary important when a deep network is trained on a large multiclass problem: since simple parts are likely to be common across many different classes the first layers of the network can leverage a large set of examples independently of the label.\n• The idea of functions that are compositions of “simpler” functions extends in a natural way to recurrent computations and recursive functions. For instance h(f (t)g((x))) represents t iterations of the algorithm f (h and g match input and output dimensions to f )."
    }, {
      "heading" : "6 Why are compositional functions so common or important?",
      "text" : "First, let us formalize the requirements on the algorithms of local compositionality is to define scalable computations as a subclass of nonlinear discrete operators, mapping vectors from Rn into Rd (for simplicity we put in the following d = 1). Informally we call an algorithm Kn : Rn 7→ R scalable if it maintains the same “form” when the input vectors increase in dimensionality; that is, the same kind of computation takes place when the size of the input vector changes. This motivates the following construction. Consider a “layer” operator H2m : R2m 7→ R2m−2 for m ≥ 1 with a special structure that we call “shift invariance”.\nDefinition 2. For integer m ≥ 2, an operator H2m is shiftinvariant if H2m = H ′m ⊕H ′′m where R2m = Rm⊕Rm, H ′ = H ′′ and H ′ : Rm 7→ Rm−1. An operator K2M : R2M → R is called scalable and shift invariant if K2M = H2 ◦ · · ·H2M where each H2k, 1 ≤ k ≤M , is shift invariant.\nWe observe that scalable shift-invariant operators K : R2m 7→ R have the structure K = H2 ◦ H4 ◦ H6 · · · ◦ H2m, with H4 = H ′2 ⊕H ′2, H6 = H ′′2 ⊕H ′′2 ⊕H ′′2 , etc..\nThus the structure of a shift-invariant, scalable operator consists of several layers; each layer consists of identical blocks; each block is an operator H : R2 7→ R: see Figure 6 . We note also that an alternative weaker constraint on H2m in Definition 2, instead of shift invariance, is mirror symmetry, that is H ′′ = R ◦H ′, where R is a reflection. Obviously, shift-invariant scalable operator are\nequivalent to shift-invariant compositional functions. Obviously the definition can be changed in several of its details. For instance for two-dimensional images the blocks could be operators H : R5 → R mapping a neighborhood around each pixel into a real number.\nThe final step in the argument uses the results of previous sections to claim that a nonlinear node with two inputs and enough units (that is, channels) can approximate arbitrarily well each of the H2 blocks. This leads to conclude that deep convolutional neural networks are natural approximators of scalable, shift-invariant operators.\nLet us provide a couple of very simple examples of compositional functions. Addition is compositional but degree of approximation does not improve by decomposing addition in different layers of network; all linear operators are compositional with no advantage for deep networks; multiplication as well as the AND operation (for Boolean variables) is the prototypical compositional function that provides an advantage to deep networks.\nThis line of arguments defines a class of algorithms that is universal and can be optimal for a large set of problems. It does not however explain why problems encountered in practice should match this class of algorithms. Though we and others have argued that the explanation may be in either the physics or the neuroscience of the brain, these arguments (see Appendix 2) are not (yet) rigorous."
    }, {
      "heading" : "Acknowledgment",
      "text" : "This work was supported by the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF – 1231216. HNM was supported in part by ARO Grant W911NF-15-1-0385."
    }, {
      "heading" : "1 Boolean Functions",
      "text" : "One of the most important tools for theoretical computer scientists for the study of functions of n Boolean variables, their related circuit design and several associated learning problems, is the Fourier transform over the Abelian group Zn2 . This is known as Fourier analysis over the Boolean cube {−1, 1}n. The Fourier expansion of a Boolean function f : {−1, 1}n → {−1, 1} or even a real-valued Boolean function f : {−1, 1}n → [−1, 1] is its representation as a real polynomial, which is multilinear because of the Boolean nature of its variables. Thus for Boolean functions their Fourier representation is identical to their polynomial representation. In this paper we use the two terms interchangeably. Unlike functions of real variables, the full finite Fourier expansion is exact, instead of an approximation. There is no need to distingush between trigonometric and real polynomials. Most of the properties of standard harmonic analysis are otherwise preserved, including Parseval theorem. The terms in the expansion correspond to the various monomials; the low order ones are parity functions over small subsets of the variables and correspond to low degrees and low frequencies in the case of polynomial and Fourier approximations, respectively, for functions of real variables."
    }, {
      "heading" : "2 Does Physics or Neuroscience imply compositionality?",
      "text" : "It has been often argued that not only text and speech are compositional but so are images. There are many phenomena in nature that have descriptions along a range of rather different scales. An extreme case consists of fractals which are infinitely self-similar, iterated mathematical constructs. As a reminder, a self-similar object is similar to a part of itself (i.e. the whole is similar to one or more of the parts). Many objects in the real world are statistically self-similar, showing the same statistical properties at many scales: clouds, river networks, snow flakes, crystals and neurons branching. A relevant point is that the shift-invariant scalability of image statistics follows from the fact that objects contain smaller clusters of similar surfaces in a selfsimilar fractal way. Ruderman [Ruderman, 1997] analysis shows that image statistics reflects what has been known as the property of compositionality of objects and parts: parts are themselves objects, that is selfsimilar clusters of similar surfaces in the physical world. Notice however that, from the point of view of this paper, it is misleading to say that an image is compositional: in our terminology a function on an image may be compositional but not its argument. In fact, functions to be learned may or may not be compositional even if their input is an image since they depend on the input but also on the task (in the supervised case of deep learning networks all weights depend on x and y). Conversely, a network may be given a function which can be written in a compositional form, independently of the nature of the input vector such as the function “multiplication of all scalar inputs’ components”. Thus a more reasonable statement is that “many natural questions on images correspond to algorithms which are compositional”. Why this is the case is an interesting open question. An answer inspired by the condition of “locality” of the constituent functions in our theorems and by the empirical success of deep convolutional networks has attracted some attention. The starting observation is that in the natural sciences– physics, chemistry, biology – many phenomena seem to be described well by processes that that take place at a sequence of increasing scales and are local at each scale, in the sense that they can be described\nwell by neighbor-to-neighbor interactions.\nNotice that this is a much less stringent requirement than renormalizable physical processes [Lin, 2016] where the same Hamiltonian (apart from a scale factor) is required to describe the process at each scale (in the obsevation above the renormalized Hamiltonian only needs to be local as the previous one)1. As discussed previously [Poggio et al., 2015a] hierarchical locality may be related to properties of basic physics that imply local interactions at each level in a sequence of scales, possibly different at each level. To complete the argument one would have then to assume that several different questions on sets of natural images may share some of the initial inference steps (first layers in the associated deep network) and thus share some of features computed by intermediate layers of a deep network. In any case, at least two open questions remain that require formal theoretical results in order to explain the connection between hierarchical, local functions and physics:\n• can hierarchical locality be derived from the Hamiltonians of physics? In other words, under which conditions does coarse graining lead to local Hamiltonians?\n• is it possible to formalize how and when the local hierarchical structure of computations on images is related to the hierarchy of local physical process that describe the physical world represented in the image?\nIt seems to us that the above set of arguments is unsatisfactory and unlikely to provide the answer. One of the reasons is that iterated local functions (from Rn to Rn with n increasing without bound) can be Turing universal and thus can simulate any physical phenomenon (as shown by the game Life which is local and Turing universal). Of course, this does not imply that the simulation will be efficient but it weakens the physics-based argument. An alternative hypothesis in fact is that locality across levels of explanation originates from the structure of the brain – wired, say, similarly to convolutional deep networks – which is then forced to use local algorithms of the type shown in Figure 6. Such local algorithms allow the organism to survive because enough of the key problems encountered during evolution can be solved well enough by them. So instead of claiming that all questions on the physical world are local because of its physics we believe that local algorithms are good enough over the distribution of evolutionary relevant problems. From this point of view locality of algorithms follows from the need to optimize local connections and to reuse computational elements. Despite the high number of synapses on each neuron it would be impossible for a complex cell to pool information across all the simple cells needed to cover an entire image, as needed by a single hidden layer network."
    }, {
      "heading" : "3 Splines: some observations",
      "text" : ""
    }, {
      "heading" : "3.1 Additive and Tensor Product Splines",
      "text" : "Additive and tensor product splines are two alternatives to radial kernels for multidimensional function approximation. It is well known that the three techniques follow from classical Tikhonov regularization and correspond to one-hidden layer networks with either the square loss or the SVM loss.\n1Tegmark and Lin [Lin, 2016] have also suggested that a sequence of generative processes can be regarded as a Markov sequence that can be inverted to provide an inference problem with a similar compositional structure. The resulting compositionality they describe does not, however, correspond to our notion of hierarchical locality and thus our theorems cannot be used to support their claims.\nWe recall the extension of classical splines approximation techniques to multidimensional functions. The setup is due to Jones et al. (1995) [Girosi et al., 1995]."
    }, {
      "heading" : "3.1.1 Tensor product splines",
      "text" : "The best-known multivariate extension of one-dimensional splines is based on the use of radial kernels such as the Gaussian or the multiquadric radial basis function. An alternative to choosing a radial function is a tensor product type of basis function, that is a function of the form\nK(x) = Πdj=1k(xj)\nwhere xj is the j-th coordinate of the vector x and k(x) is the inverse Fourier transform associated with a Tikhonov stabilizer (see [Girosi et al., 1995]).\nWe notice that the choice of the Gaussian basis function for k(x) leads to a Gaussian radial approximation scheme with K(x) = e−‖x‖ 2 ."
    }, {
      "heading" : "3.1.2 Additive splines",
      "text" : "Additive approximation schemes can also be derived in the framework of regularization theory. With additive approximation we mean an approximation of the form\nf(x) = d∑ µ=1 fµ(x µ) (13)\nwhere xµ is the µ-th component of the input vector x and the fµ are one-dimensional functions that will be defined as the additive components of f (from now on Greek letter indices will be used in association with components of the input vectors). Additive models are well known in statistics (at least since Stone, 1985) and can be considered as a generalization of linear models. They are appealing because, being essentially a superposition of one-dimensional functions, they have a low complexity, and they share with linear models the feature that the effects of the different variables can be examined separately. The resulting scheme is very similar to Projection Pursuit Regression. We refer to [Girosi et al., 1995] for references and discussion of how such approximations follow from regularization.\nGirosi et al. [Girosi et al., 1995] derive an approximation scheme of the form (with i corresponding to spline knots – which are free parameters found during learning as in free knots splines - and µ corresponding to new variables as linear combinations of the original components of x):\nf(x) = d′∑ µ=1 n∑ i=1 cµiK(〈t µ, x〉−bµ) = ∑ µ=1 ∑ i=1 cµiK(〈t µ, x〉−bµi ) .\n(14)\nNote that the above can be called spline only with a stretch of the imagination: not only the wµ but also the tµ depend on the data in a very nonlinear way. In particular, the tµ may not correspond at all to actual data point. The approximation could be called ridge approximation and is related to projection pursuit. When the basis\nfunction K is the absolute value that is K(x − y) = |x − y| the network implements piecewise linear splines."
    }, {
      "heading" : "3.2 Hierarchical Splines",
      "text" : "Consider an additive approximation scheme (see subsection) in which a function of d variables is approximated by an expression such as\nf(x) = d∑ i φi(x i) (15)\nwhere xi is the i-th component of the input vector x and the φi are one-dimensional spline approximations. For linear piecewise splines φi(xi) = ∑ j cij |x\ni − bji |. Obviously such an approximation is not universal: for instance it cannot approximate the function f(x, y) = xy. The classical way to deal with the problem is to use tensor product splines. The new alternative that we propose here is hierarchical additive splines, which in the case of a 2-layers hierarchy has the form\nf(x) = K∑ j φj( d∑ i φi(x i)). (16)\nand which can be clearly extended to an arbitrary depth. The intuition is that in this way, it is possible to obtain approximation of a function of several variables from functions of one variable because interaction terms such as xy in a polynomial approximation of a function f(x, y) can be obtained from terms such as elog(x)+log(y).\nWe start with a lemma about the relation between linear rectifiers, which do not correspond to a kernel, and absolute value, which is a kernel.\nLemma 1 Any given superposition of linear rectifiers∑ i c ′ i|x − b ′i|+ with c′i, b ′i given, can be represented over a finite interval in terms of the absolute value kernel with appropriate weights. Thus there exist ci, bi such that∑ i c ′ i|x− b ′i|+ = ∑ i ci|x− b i|.\nThe proof follows from the facts that a) the superpositions of ramps is a piecewiselinear function, b) piecewise linear functions can be represented in terms of linear splines and c) the kernel corresponding to linear splines in one dimension is the absolute value K(x, y) = |x− y|.\nNow consider two layers in a network in which we assume degenerate pooling for simplicity of the argument. Because of Lemma 1, and because weights and biases are arbitrary we assume that the the nonlinearity in each edge is the absolute value. Under this assumption, unit j in the first layer, before the non linearity, computes\nf j(x) = ∑ i=1 cji | 〈 ti, x 〉 − bi|, (17)\nwhere x and w are vectors and the ti are real numbers. Then the second layer output can be calculated with the nonlinearity | · · · | instead of (·)2.\nIn the case of a network with two inputs x, y the effective output after pooling at the first layer may be φ(1)(x, y) = t1|x + b1| + t2|y + b2|, that is the linear combination of two “absolute value” functions. At the second layer terms like φ(2)(x, y) = |t1|x+b1|+ t2|y + b2| + b3| may appear. The output of a second layer still consists of hyperplanes, since the layer is a kernel machine with an output which is always a piecewise linear spline.\nNetworks implementing tensor product splines are universal in the sense that they approximate any continuous function in an interval, given enough units. Additive splines on linear combinations of the input variables of the form in eq. (17) are also universal (use Theorem 3.1 in [Pinkus, 1999]). However additive splines on the individual variables are not universal while hierarchical additive splines are:\nTheorem Hierarchical additive splines networks are universal."
    }, {
      "heading" : "4 On multivariate function approximation",
      "text" : "Consider a multivariate function f : [0, 1]d → R discretized by tensor basis functions:\nφ(i1,...,id)(x1, ..., xd) := d∏ µ=1 φiµ(xµ), (18)\nwith φiµ : [0, 1]→ R, 1 ≤ iµ ≤ nµ, 1 ≤ µ ≤ d\nto provide\nf(x1, ..., xd) = n1∑ i1=1 · · · nd∑ id=1 c(i1, ..., id)φ(i1, ..., id)(x1, ..., xd).\n(19)\nThe one-dimensional basis functions could be polynomials (as above), indicator functions, polynomials, wavelets, or other sets of basis functions. The total number N of basis functions scales exponentially in d as N = ∏d µ=1 nµ for a fixed smoothness class m (it scales as d m ).\nWe can regard neural networks as implementing some form of this general approximation scheme. The problem is that the type of operations available in the networks are limited. In particular, most of the networks do not include the product operation (apart from “sum-product” networks also called “algebraic circuits”) which is needed for the straightforward implementation of the tensor product approximation described above. Equivalent implementations can be achieved however. In the next two sections we describe how networks with a univariate ReLU nonlinearity may perform multivariate function approximation with a polynomial basis and with a spline basis respectively. The first result is known and we give it for completeness. The second is simple but new."
    }, {
      "heading" : "4.1 Neural Networks: polynomial approach",
      "text" : "One of the choices listed above leads to polynomial basis functions. The standard approach to prove degree of approximations uses\npolynomials. It can be summarized in three steps:\n1. Let us denote with Hk the linear space of polynomials of degree k in Rn and with Pk = ⋃k s=0Hs the linear space\nof polynomials of degree at most k in n variables. Set r =( n+k k ) = dimHk and denote by πk the space of univariate polynomials of degree at most k We recall that the number of monomials in a polynomial in d variables with total degree ≤ N is ( d+N d ) and can be written as a linear combination of the same number of terms of the form (〈w, x〉+ b)N . We first prove that\nPk(x) = span(( 〈 wi, x 〉 )s : i = 1, · · · , r, s = 1, · · · , k\n(20) and with, pi ∈ πk,\nPk(x) = r∑ i=1 pi(〈wi, x〉). (21)\nNotice that the effective r, as compared with the theoretical r which is of the order r ≈ kn, is closely related to the separation rank of a tensor.\n2. Second, we prove that each univariate polynomial can be approximated on any finite interval from\nN (σ) = span{σ(λt− θ)}, λ, θ ∈ R (22)\nin an appropriate norm\n3. the last step is to use classical results about approximation error of polynomials for the target function such as\nE(Bmp ;Pk;Lp) ≤ Ck−m (23)\nwhere Bmp is the Sobolev space of functions supported on the unit ball in Rn\nThe key step from the point of view of possible implementations by a deep neural network with ReLU units is step number 2. A univariate polynomial can be synthesized – in principle – via the linear combination of ReLUs units as follows. The limit of the linear combination σ((a+h)x+b)−σ(ax+b)\nh contains the monomial\nx (assuming the derivative of σ is nonzero). In a similar way one shows that the set of shifted and dilated ridge functions has the following property. Consider for ci, bi, λi ∈ R the space of univariate functions\nNm(σ) = { r∑ i=1 ciσ(λix− bi) } . (24)\nThe following (see Propositions 3.6 and 3.8 in [Pinkus, 1999]) holds\nProposition 5. If σ ∈ C(R) is not a polynomial and σ ∈ C∞, the closure ofN contains the linear space of algebraic polynomial of degree at most r − 1.\nThis implies that a few units – in the order of 2kr – can represent any polynomial of order k in n variables. Since r ≈ kn, equation 23 gives\nE(Bmp ;Pk;Lp) ≤ Cr− m n . (25)"
    }, {
      "heading" : "4.2 Neural Networks: splines and look-up tables",
      "text" : "Another choice of basis functions for discretization consists of splines. In particular, we focus for simplicity on indicator functions on partitions of [0, 1], that is piecewise constant splines. Another attractive choice are Haar basis functions. If we focus on the binary case section 4.3 tells the full story that does not need to be repeated here. We just add a note on establishing a partition\nSuppose that a = x1 < x2 · · · < xm = b are given points, and set ∆x the maximum separation between any two points.\n• If f ∈ C[a, b] then for every > 0 there is a δ > 0 such that if ∆x < δ, then |f(x)−Sf(x)| < for all x ∈ [a, b], where Sf is the spline interpolant of f .\n• if f ∈ C2[a, b] then for all x ∈ [a, b]\n|f(x)− Sf(x)| ≤ 1 8 (∆x)2maxa≤≤b|f ′′(z)|\nThe first part of the Proposition states that piecewise linear interpolation of a continuous function converges to the function when the distance between the data points goes to zero. More specifically, given a tolerance, we can make the error less than the tolerance by choosing ∆x sufficiently small. The second part gives an upper bound for the error in case the function is smooth, which in this case means that f and its first two derivatives are continuous."
    }, {
      "heading" : "4.3 Non-smooth ReLUs: how deep nets may work in reality",
      "text" : "Our main theorem (2) in this paper is based on polynomial approximation. Because of the n-width result other approaches to approximation cannot yield better rates than polynomial approximation. It is, however, interesting to consider other kinds of approximation that may better capture what deep neural network with the ReLU activation functions implement in practice as a consequence of minimizing the empirical risk.\nOur construction shows that a network with non-smooth ReLU activation functions can approximate any continuous function with a rate similar to our other results in this paper. The logic of the argument is simple:\n• Consider the constituent functions of the binary tree, that is functions of two variables such as g(x1, x2). Assume that g is Lipschitz with Lipschitz constant L. Then for any it is possible to set a partition of x1, x2 on the unit square that allows piecewise constant approximation of g with accuracy at least in the sup norm.\n• We show then that a multilayer network of ReLU units can compute the required partitions in the L2 norm and perform piecewise constant approximation of g.\nNotice that partitions of two variables x and y can in principle be chosen in advance yielding a finite set of points 0 =: x0 < x1 < · · · < xk := 1 and an identical set 0 =: y0 < y1 < · · · < yk := 1. In the extreme, there may be as little as one partition – the binary case. In practice, the partitions can be assumed to be set by the architecture of the network and optimized during learning. The simple way to choose partitions is to choose an interval on a regular grid. The other way is an irregular grid optimized to the local\nsmoothness of the function. As we will mention later this is the difference between fixed-knots splines and free-knots splines.\nWe describe next a specific construction.\nHere is how a linear combination of ReLUs creates a unit that is active if x1 ≤ x ≤ x2 and y0 ≤ y ≤ y1. Since the ReLU activation t+ is a basis for piecewise linear splines, an approximation to an indicator function (taking the value 1 or 0, with knots at x1, x1 +η, x2 x2 + η, ) for the interval between x1 and x2 can be synthesized using at most 4 units in one layer. A similar set of units creates an approximate indicator function for the second input y. A set of 3 ReLU’s can then perform a min operations between the x and the y indicator functions, thus creating an indicator function in two dimensions.\nIn greater detail, the argument is as follows: For any > 0, 0 ≤ x0 < x1 < 1, it is easy to construct an ReLU network Rx0,x1 with 4 units as described above so that\n‖χ[x0,x1) −R‖L2[0,1] ≤ .\nWe define another ReLU network with two inputs and 3 units by\nφ(x1, x2) := (x1)+ − (−x1)+ − (x1 − x2)+ = min(x1, x2)\n= x1 + x2 2 + |x1 − x2| 2 .\nThen, with I = [x0, x1)×[y0, y1), we define a two layered network with 11 units total by\nΦI(x, y) = φ(Rx0,x1(x), Ry0,y1(y)).\nThen it is not difficult to deduce that\n‖χI − ΦI‖2L2([0,1]2) = ∫ 1 0 ∫ 1 0\n|min(χ[x0,x1)(x), χ[y0,y1)(y))− min(Rx0,x1(x), Ry0,y1(y))| 2dxdy ≤ c 2.\nNotice that in this case dimensionality is n = 2; notice that in general the number of units is proportional to kn which is of the same order as ( n+k k ) which is the number of parameters in a polynomial in n variables of degree k. The layers we described compute the entries in the 2D table corresponding to the bivariate function g. One node in the graph (there are n− 1 nodes in a binary tree with n inputs) contains O(k2) units; the total number of units in the network is (n− 1)O(k2). This construction leads to the following results.\nProposition 6. Compositional functions on the unit cube with an associated binary tree graph structure and constituent functions that are Lipschitz can be approximated by a deep network of ReLU units within accuracy in the L2 norm with a number of units in the order of O((n − 1)L −2), where L is the worse – that is the max – of the Lipschitz constant among the constituent functions.\nNotice that the number of partitions in each of two variables that are input to each node in the graph is k = L where L is the Lipschitz\nconstant associated with the function g approximated by the node. Here the role of smoothness is clear: the smaller L is, the smaller is the number of variables in the approximating Boolean function. Notice that if g ∈W 21 , that is g has bounded first derivatives, then g is Lipschitz. However, higher order smoothness beyond the bound\non the first derivative cannot be exploited by the network because of the non-smooth activation function2.\nWe conjecture that the construction above that performs piecewise constant approximation is qualitatively similar to what deep networks may represent after training. Notice that the partitions we used correspond to a uniform grid set a priori depending on global properties of the function such as a Lipschitz bound. In supervised training of deep network the location of each partition is likely to be optimized in a greedy way as a function of the performance on the training set and therefore as a function of inputs and output. Our Theorem 2 and Theorem 6 are obtained under this assumption. In any case, their proofs suggest two different ways of how deep networks could perform function approximations, the first by using derivatives and the second by using piecewise linear splines. In the latter case, optimizing the partition as a function of the input-output examples correspond to free-knots splines. The case in which the partitions depend on the inputs but not the target, correspond to classical fixed-knots splines. As an aside, one expects that networks with smooth ReLUs will perform much better than networks with non-smooth ReLUs in the approximation of very smooth functions."
    }, {
      "heading" : "5 Vector Quantization and Hierarchical Vector Quantization",
      "text" : "Let us start with the observation that a network of radial Gaussian-like units become in the limit of σ → 0 a look-up table with entries corresponding to the centers. The network can be described in terms of soft Vector Quantization (VQ) (see section 6.3 in Poggio and Girosi, [Poggio and Girosi, 1989]). Notice that hierarchical VQ (dubbed HVQ) can be even more efficient than VQ in terms of storage requirements (see e.g. [Mihalik, 1992]). This suggests that a hierarchy of HBF layers may be similar (depending on which weights are determined by learning) to HVQ. Note that compression is achieved when parts can be reused in higher level layers as in convolutional networks. Notice that the center of one unit at level n of a “convolutional” hierarchy is a combinations of parts provided by each of the lower units feeding in it. This may even happen without convolution and pooling as shown in the following extreme example.\nExample Consider the case of kernels that are in the limit delta-like functions (such as Gaussian with very small variance). Suppose that there are four possible quantizations of the input x: x1, x2, x3, x4. One hidden layer would consist of four units δ(x − xi), i = 1, · · · , 4. But suppose that the vectors x1, x2, x3, x4 can be decomposed in terms of two smaller parts or features x′ and x”, e.g. x1 = x′⊕x”, x2 = x′⊕x′, x3 = x”⊕x” and x4 = x” ⊕ x′. Then a two layer network could have two types of units in the first layer δ(x − x′) and δ(x − x”); in the second layer four units will detect the conjunctions of x′ and x” corresponding to x1, x2, x3, x4. The memory requirements will go from 4N to 2N/2 + 8 where N is the length of the quantized vectors; the latter is much smaller for large N . Memory compression for HVQ vs VQ – that is for multilayer networks vs one-layer networks – increases with the number of (reusable) parts. Thus for problems that are compositional, such as text and images, hierarchical architectures of HBF modules minimize memory requirements.\nClassical theorems (see refrences in [Girosi and Poggio, 1989, Girosi and Poggio, 1990] show that one hidden layer networks can approximate arbitrarily well rather general\n2In the case of univariate approximation on the interval [−1, 1], piecewise linear functions with inter-knot spacing h gives an accuracy of (h2/2)M , where M is the max absolute value of f ′′ . So, a higher derivative does lead to better approximation : we need√\n2M/ units to give an approximation of . This is a saturation though. Even higher smoothness does not help.\nclasses of functions. A possible advantage of multilayer vs one-layer networks that emerges from the analysis of this paper is memory efficiency which can be critical for large data sets and is related to generalization rates."
    }, {
      "heading" : "6 Pooling and invariance",
      "text" : "This section considers the important case of deep networks with translation symmetry which corresponds to convolutional networks – the best performing neural networks so far. In particular, we characterize the theoretical properties of the key step of pooling and subsampling in convolutional networks and in the corresponding binary trees.\nThe networks we have discussed in the previous section do not need to have any “weight” sharing and do not need to be convolutional. If we assume shift symmetry of the network, all weights in each layer are “shared”, that is the weigths in one node are just shifts gi of t in another node. The layer performs a convolution indexed by i (in our one dimensional networks of this paper). Thus the convolution layer computes for all i\n| 〈x, git〉+ b|+, (26)\nwhere git is the vector t transformed by gi ∈ G.\nAccording to i-theory[Anselmi and Poggio, 2016], pooling corresponds to a higher unit averaging several neurons according to\n∑ i | 〈x, git〉+ bi|+, i ∈ N , (27)\nwhereN is the neighborhood over which pooling is performed. In 2D this is usually a 3× 3 kernel. In our 1D figures we consider the immediate left and right neighboring nodes.\nThe results proven in [Anselmi et al., 2014] characterize pooling and the associated invariance and selectivity. They say – informally – that an invariant and selective signature of an image I can be obtained as\nµkh(I) = 1\n|G| |G|∑ i=1 ηh( 〈 I, git k 〉 ) (28)\nwhere h denotes subchammels of channel k and ηh, h = 1, · · · , H is a set of nonlinear functions, tk, k = 1, · · · ,K are a set of randomly chosen images called templates and we suppose G to be a discrete finite group. We call ~µ(I) ∈ RHK the signature of image I . It can be proven that the signature is an empirical proxy for the probability distribution PI under the group G. In particular it has the following properties:\nInvariance theorem The distributions represented by equation (28) are invariant, that is for each h, k\nµkh(I) = µ k h(gI) (29)\nfor any g in G, where G is the (compact) group of transformations labeled gi. The result follows simply observing that, for fixed h, k, I and tk, ηh( 〈 I, gtk 〉 ) ≡ f(g) is a function on the group and eq. (28) is the group average over f . The signature in eq. (28) is also selective since it is a proxy of the probability distribution PI . This result follows in two steps: 1) eq.\nConvolutional Deep Networks with pooling are ~ (binary) trees\n∼x1 x2 x3 x4 x5 x6 x7 x8 x1 x2 x3 x4 x5 x6 x7 x8\nAccording to i-theory:\nconvolution + (average)\npooling + subsampling\nIn practice: convolution + subsampling\n+ + ++++ +\n++\nPoggio, 2016\nFigure 7 Shift invariance of the compositional function imply h11 = h12 = h13 = h14 and h21 = h22. These constraints (which correspond to weight sharing in convolutional networks) reduce the number of parameters in the network. Under shift invariance the reduction in number of nodes at each layer of the binary tree of Figure 1b can be interpreted as pooling neighboring nodes and subsampling. In other words the binary tree is an approximation of taking the more complex tree above and assume that at the first layer neighboring nodes are averaged t gether or subsampled. On the tree above we can use the results of i-theory about invariance and selectivity properties of average pooling of the form ∑ i | 〈x, git〉+ bi|+.\n(28) gives a proxy of the probability distribution of 〈 I, gtk 〉 (the\nsituation is particularly simple when the nonlinear functions are indicator functions of width ∆ centered in h; in this case µkh is a bin of the histogram of the distribution of 〈 I, gtk 〉 ) 2) the theorem\nbelow (based on the Cramer-Wold theorem, [Cramer and Wold, 1936], see [Anselmi et al., 2014] for further details) assures that a probability distribution PI can be almost uniquely characterized by K one-dimensional probability distributions P〈I,tk〉 induced by the results of projections 〈 I, tk 〉 . More precisely we have:\nSelectivity theorem For (compact) groups of transformations, the distributions can achieve any desired selectivity for an image among N images in the sense that they can -approximate the true distance between each pair with probability 1− δ, provided that\nK > c 2 ln N\nδ (30)\nwhere c is a universal constant. Thus selectivity could be achieved (up to ) via empirical proxy of the one-dimensional distribution P〈I,tk〉 of projections of the image onto a finite number of templates tk, k = 1, ...,K under the action of the group."
    }, {
      "heading" : "7 Optimization of compositional functions and Bezout theorem",
      "text" : "Consider minimization of the square loss of a network descriibed by a binary tree graph with a polynomial activation function, such as the square activation function. The function computed by the network is a polynomial in the input variables.The critical points, obtained by setting the gradient wrt the weights to zero, are solutions to polynomial equations in the weights.\nAn interesting observation by others is that when the activation is the ReLU function and the network is deep (in particular compositional), then the critical points are solutions of polynomial equations in the weights wi,j .\nIn all these cases, Bezout theorem provides an estimate of the number of local minima as a function of the architecture of the network. The number of distinct zeros (counting points at infinity, using projective space, assigning an appropriate multiplicity to each intersection point, and excluding a degenerate case) of the gradient of the loss functions is exactly equal to the product of the degrees of each of the equilibrium equations obtained putting to zero the gradient wrt weights of the loss function.\nSquare activation function\nLet us look at the following specific network with x = (x1, x2) as input, the square activation function, 2 hidden layers each with 2 units and a final output layer. The space of functions spanned by the network consist of homogeneous polynomials of order 4 in 2 variables parametrized by 10 real numbers:\nf(x1, x2) = C(A(ax1 + bx2) 2\n+B(a′x1 + b ′x2) 2)2\n+C′(A′(ax1 + bx2) 2\n+B′(a′x1 + b ′x2) 2)2.\nNotice that f is not only a polynomial in x1, x2 but also a homogeneous polynomial in the 10 coefficients. We recall that the number of monomials in a polynomial in d variables with total degree≤ N is ( d+N d ) .\nThe optimization process looks for values of the parameters – such as C,C′, A,A′, B,B′, a, a′, b, b′ that we collectively collect as the vector w – that solve the equations\nN∑ i=1 (yi − f(xi))∇wfw(xi) = 0, (31)\nwhere (yi, xi), i = 1, · · · , N are the training examples.\nIf f is a polynomial of degree d1, · · · , dn in n variables it will be a polynomial of degree k1, · · · , km in the m unknown coefficients.\nNotice that adding one layer at the top increases the number of values of the first layer coefficients corresponding to minima – that is they make it easier to find fits of the parameters of the first layer to the training data. The other layers also become easier with a positive effect decreasing with increasing layer level.\nReLU activation function\nConsider optimization on the training set of a network with ReLUs activation approximating a function of four variables f(x1, x2, x3, x4) = g(h1(x1, x2), h2(x3, x4)). We assume a quadratic loss function L = ∑Q i=1(yi − f(xi) 2\nWe conjecture the following\nProposition 7. If the number of parameters N ≤ 9Q where Q is the number of examples then there is always a global minimum with empirical error = 0.\nA possible proof considers the approximating polynomial realized by the network and check that if number of terms is large enough (for instance via degree) the polynomial will interpolate. Another proof, exploiting the ReLUs properties, is in the margins of my notebook. Assume a quadratic loss function L = ∑Q i=1(yi − f(xi)\n2 and consider optimization on the training set of a network with ReLUs activation approximating a function of four variables with the following structure, equivalent to a single channel in the second layer:\nf(x1, x2, x3, x4) = g(h(x 1, x2), h(x3, x4)) (32)\nwith g(h, h) = |Ah(x1, x2) +Bh(x3, x4) + C|+ (33)\nand\nh(x1, x2) = |ax1 + bx2 + c|+, h(x3, x4) = |ax3 + bx4 + c|+. (34)\nThe effect of the ReLU nonlinearity is to select a subset of the training points that satisfy positivity conditions. The selection is discontinuous with respect to the parameters values. Consider now the case of fewer parameters than data, in which in general one does not expect the empirical error to be zero. Critical differentiable points – corresponding to zero of the gradient of the parameters - must satisfy nonlinear equations in 6 unknowns (in our case of one unit per node) of degree 3, yielding a total number of critical points that are, under the conditions of validity of Bezout theorem = 36 = 729. For the binary tree case (assuming one unit per node and shared weights) the number of critical points according to a Bezout estimate is (2l − 1)3l, where l is the number of layers.\nRecall again that in the overparametrized case there are global minima with zero empirical error. Also notice that for large data sets\nthe increase in generalization error for increasing overparametrization may be very slow (see the bounds in Niyogi and Girosi [Niyogi and Girosi, 1996], Figure 3)."
    }, {
      "heading" : "7.1 Convergence of greedy techniques in deep networks",
      "text" : "Theorem 4.2 in [Poggio, 1975] may be adapted to prove convergence of an iteration method that optimizes the best correction at layer m until the last layer and then iterates.\nThe steps are: I) The optimum approximation of zero degree and the sequences of optimal corrections up to the layer n are calculated. II) The optimum corrections to the result of step are computed for all degrees, starting again from the zero order degree. III)\nThe iteration results in a series of layers (i = 1, 2, · · · , n) and in the associated mean square errors. The iteration algorithm outlined here (adapted from Katzenelson et al., 1964) gives at each step a meaningful result. Convergence of the iteration algorithm, as well as the uniqueness of the optimum estimator up to an estimator which does not affect the mean square error, are proved in the next theorem.\nTheorem 4.2 The iterative algorithm a) has a limit n-th order estimator; b) the limit estimator is the optimal n-th order estimator in the least square sense; c) the limit estimator is unique up to an estimator of the same degree which does not affect the mean square error. 8 Approximating compositional functions"
    } ],
    "references" : [ {
      "title" : "TensorFlow: Large-scale machine learning on heterogeneous systems. Software available from tensorflow.org",
      "author" : [ "P. Tucker", "V. Vanhoucke", "V. Vasudevan", "F. Viégas", "O. Vinyals", "P. Warden", "M. Wattenberg", "M. Wicke", "Y. Yu", "X. Zheng" ],
      "venue" : null,
      "citeRegEx" : "Tucker et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Tucker et al\\.",
      "year" : 2015
    }, {
      "title" : "Unsupervised learning of invariant representations with low sample complexity: the magic of sensory cortex or a new framework for machine learning",
      "author" : [ "Anselmi et al", "F. 2014] Anselmi", "J. Leibo", "L. Rosasco", "J. Mutch", "A. Tacchetti", "T. Poggio" ],
      "venue" : "Center for Brains, Minds and Machines (CBMM) Memo No",
      "citeRegEx" : "al. et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2014
    }, {
      "title" : "Unsupervised learning of invariant representations",
      "author" : [ "Anselmi et al", "F. 2015a] Anselmi", "J.Z. Leibo", "L. Rosasco", "J. Mutch", "A. Tacchetti", "T. Poggio" ],
      "venue" : "Theoretical Computer Science",
      "citeRegEx" : "al. et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2015
    }, {
      "title" : "Visual Cortex and Deep Networks",
      "author" : [ "Anselmi", "Poggio", "F. 2016] Anselmi", "T. Poggio" ],
      "venue" : null,
      "citeRegEx" : "Anselmi et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Anselmi et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep convolutional network are hierarchical kernel machines. Center for Brains, Minds and Machines (CBMM) Memo No",
      "author" : [ "Anselmi et al", "F. 2015b] Anselmi", "L. Rosasco", "C. Tan", "T. Poggio" ],
      "venue" : null,
      "citeRegEx" : "al. et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural Network Learning - Theoretical Foundations",
      "author" : [ "Anthony", "Bartlett", "M. 2002] Anthony", "P. Bartlett" ],
      "venue" : null,
      "citeRegEx" : "Anthony et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Anthony et al\\.",
      "year" : 2002
    }, {
      "title" : "Representations properties of multilayer feedforward networks",
      "author" : [ "B. Moore", "Poggio", "B. 1998] B. Moore", "T. Poggio" ],
      "venue" : "Abstracts of the First annual INNS meeting,",
      "citeRegEx" : "Moore et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Moore et al\\.",
      "year" : 1998
    }, {
      "title" : "Scaling learning algorithms towards ai",
      "author" : [ "Bengio", "LeCun", "Y. 2007] Bengio", "Y. LeCun" ],
      "venue" : null,
      "citeRegEx" : "Bengio et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2007
    }, {
      "title" : "Random search for hyper-parameter optimization",
      "author" : [ "Bergstra", "Bengio", "J. 2012] Bergstra", "Y. Bengio" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Bergstra et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bergstra et al\\.",
      "year" : 2012
    }, {
      "title" : "Neural networks for localized approximation",
      "author" : [ "Chui et al", "C.K. 1994] Chui", "X. Li", "H.N. Mhaskar" ],
      "venue" : "Mathematics of Computation,",
      "citeRegEx" : "al. et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 1994
    }, {
      "title" : "Limitations of the approximation capabilities of neural networks with one hidden layer",
      "author" : [ "Chui et al", "C.K. 1996] Chui", "X. Li", "H.N. Mhaskar" ],
      "venue" : "Advances in Computational Mathematics,",
      "citeRegEx" : "al. et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 1996
    }, {
      "title" : "On the expressive power of deep learning: a tensor analysis",
      "author" : [ "Cohen et al", "N. 2015] Cohen", "O. Sharir", "A. Shashua" ],
      "venue" : null,
      "citeRegEx" : "al. et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2015
    }, {
      "title" : "Condiciones para que una funcion infinitamente derivable sea un polinomio",
      "author" : [ "Corominas", "Balaguer", "E. 1954] Corominas", "F.S. Balaguer" ],
      "venue" : "Revista matemática hispanoamericana,",
      "citeRegEx" : "Corominas et al\\.,? \\Q1954\\E",
      "shortCiteRegEx" : "Corominas et al\\.",
      "year" : 1954
    }, {
      "title" : "Some theorems on distribution functions",
      "author" : [ "Cramer", "Wold", "H. 1936] Cramer", "H. Wold" ],
      "venue" : "J. London Math. Soc.,",
      "citeRegEx" : "Cramer et al\\.,? \\Q1936\\E",
      "shortCiteRegEx" : "Cramer et al\\.",
      "year" : 1936
    }, {
      "title" : "Y",
      "author" : [ "O. Delalleau", "Bengio" ],
      "venue" : "(2011). Shallow vs. deep sum-product networks. In Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems",
      "citeRegEx" : "Delalleau and Bengio. 2011",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Optimal nonlinear approximation",
      "author" : [ "DeVore et al", "R.A. 1989] DeVore", "R. Howard", "C.A. Micchelli" ],
      "venue" : "Manuscripta mathematica,",
      "citeRegEx" : "al. et al\\.,? \\Q1989\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 1989
    }, {
      "title" : "Regularization theory and neural networks architectures",
      "author" : [ "Girosi et al", "F. 1995] Girosi", "M. Jones", "T. Poggio" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "al. et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 1995
    }, {
      "title" : "Representation properties of networks: Kolmogorov’s theorem is irrelevant",
      "author" : [ "Girosi", "Poggio", "F. 1989] Girosi", "T. Poggio" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Girosi et al\\.,? \\Q1989\\E",
      "shortCiteRegEx" : "Girosi et al\\.",
      "year" : 1989
    }, {
      "title" : "Networks and the best approximation property",
      "author" : [ "Girosi", "Poggio", "F. 1990] Girosi", "T. Poggio" ],
      "venue" : "Biological Cybernetics,",
      "citeRegEx" : "Girosi et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "Girosi et al\\.",
      "year" : 1990
    }, {
      "title" : "Adam: A method for stochastic optimization. CoRR, abs/1412.6980",
      "author" : [ "Kingma", "Ba", "D.P. 2014] Kingma", "J. Ba" ],
      "venue" : null,
      "citeRegEx" : "Kingma et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2014
    }, {
      "title" : "Bridging the gap between residual learning, recurrent neural networks and visual cortex. Center for Brains, Minds and Machines",
      "author" : [ "Liao", "Poggio", "Q. 2016] Liao", "T. Poggio" ],
      "venue" : "(CBMM) Memo No. 47,",
      "citeRegEx" : "Liao et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Liao et al\\.",
      "year" : 2016
    }, {
      "title" : "Why does deep and cheap learning work so well",
      "author" : [ "Lin", "H. 2016] Lin", "M. Tegmark" ],
      "venue" : null,
      "citeRegEx" : "Lin et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2016
    }, {
      "title" : "Constant depth circuits, fourier transform, and learnability",
      "author" : [ "al. Linial et", "Linial N", "Y. M" ],
      "venue" : "Journal of the ACM,",
      "citeRegEx" : "et et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "et et al\\.",
      "year" : 1993
    }, {
      "title" : "A provably efficient algorithm for training deep networks. CoRR, abs/1304.7045",
      "author" : [ "Livni et al", "R. 2013] Livni", "S. Shalev-Shwartz", "O. Shamir" ],
      "venue" : null,
      "citeRegEx" : "al. et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2013
    }, {
      "title" : "Learning real and boolean functions: When is deep better than shallow? Center for Brains, Minds and Machines",
      "author" : [ "Mhaskar et al", "H. 2016] Mhaskar", "Q. Liao", "T. Poggio" ],
      "venue" : "(CBMM) Memo No. 45,",
      "citeRegEx" : "al. et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep versus shallow networks: an approximation theory perspective. Center for Brains, Minds and Machines (CBMM",
      "author" : [ "Mhaskar", "Poggio", "H. 2016] Mhaskar", "T. Poggio" ],
      "venue" : "Memo No. 54,",
      "citeRegEx" : "Mhaskar et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Mhaskar et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural networks for localized approximation of real functions",
      "author" : [ "Mhaskar", "H.N. 1993b] Mhaskar" ],
      "venue" : "In Neural Networks for Processing",
      "citeRegEx" : "Mhaskar and Mhaskar,? \\Q1993\\E",
      "shortCiteRegEx" : "Mhaskar and Mhaskar",
      "year" : 1993
    }, {
      "title" : "Perceptrons: An Introduction to Computational Geometry. The MIT Press, ISBN 0-26263022-2",
      "author" : [ "Minsky", "Papert", "M. 1972] Minsky", "S. Papert" ],
      "venue" : null,
      "citeRegEx" : "Minsky et al\\.,? \\Q1972\\E",
      "shortCiteRegEx" : "Minsky et al\\.",
      "year" : 1972
    }, {
      "title" : "On the number of linear regions of deep neural networks",
      "author" : [ "Montufar et al", "G.F. 2014] Montufar", "R. Pascanu", "K. Cho", "Y. Bengio" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "al. et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2014
    }, {
      "title" : "On the relationship between generalization error, hypothesis complexity and sample complexity in regularization networks",
      "author" : [ "Niyogi", "Girosi", "P. 1996] Niyogi", "F. Girosi" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Niyogi et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Niyogi et al\\.",
      "year" : 1996
    }, {
      "title" : "Itheory on depth vs width: hierarchical function composition",
      "author" : [ "Poggio et al", "T. 2015a] Poggio", "F. Anselmi", "L. Rosasco" ],
      "venue" : "CBMM memo 041",
      "citeRegEx" : "al. et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2015
    }, {
      "title" : "A theory of networks for approximation and learning",
      "author" : [ "Poggio", "Girosi", "T. 1989] Poggio", "F. Girosi" ],
      "venue" : "Laboratory, Massachusetts Institute of Technology, A.I. memo n1140",
      "citeRegEx" : "Poggio et al\\.,? \\Q1989\\E",
      "shortCiteRegEx" : "Poggio et al\\.",
      "year" : 1989
    }, {
      "title" : "On the representation of multi-input systems: Computational properties of polynomial algorithms",
      "author" : [ "Poggio", "Reichardt", "T. 1980] Poggio", "W. Reichardt" ],
      "venue" : "Biological Cybernetics,",
      "citeRegEx" : "Poggio et al\\.,? \\Q1980\\E",
      "shortCiteRegEx" : "Poggio et al\\.",
      "year" : 1980
    }, {
      "title" : "Notes on hierarchical splines, dclns and i-theory",
      "author" : [ "Poggio et al", "T. 2015b] Poggio", "L. Rosaco", "A. Shashua", "N. Cohen", "F. Anselmi" ],
      "venue" : "CBMM memo 037",
      "citeRegEx" : "al. et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2015
    }, {
      "title" : "Notes on hierarchical splines, dclns and i-theory",
      "author" : [ "Poggio et al", "T. 2015c] Poggio", "L. Rosasco", "A. Shashua", "N. Cohen", "F. Anselmi" ],
      "venue" : "Technical report,",
      "citeRegEx" : "al. et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2015
    }, {
      "title" : "The mathematics of learning: Dealing with data. Notices of the American Mathematical Society (AMS), 50(5):537–544",
      "author" : [ "Poggio", "Smale", "T. 2003] Poggio", "S. Smale" ],
      "venue" : null,
      "citeRegEx" : "Poggio et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Poggio et al\\.",
      "year" : 2003
    }, {
      "title" : "Hierarchical models of object recognition in cortex",
      "author" : [ "Riesenhuber", "Poggio", "M. 1999] Riesenhuber", "T. Poggio" ],
      "venue" : "Nature Neuroscience,",
      "citeRegEx" : "Riesenhuber et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Riesenhuber et al\\.",
      "year" : 1999
    }, {
      "title" : "M",
      "author" : [ "Telgarsky" ],
      "venue" : "(2015). Representation benefits of deep feedforward networks. arXiv preprint arXiv:1509.08101v2 [cs.LG] 29 Sep",
      "citeRegEx" : "Telgarsky. 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Performance-optimized hierarchical models predict neural responses in higher visual cortex",
      "author" : [ "Yamins et al", "D. 2014] Yamins", "H. Hong", "C. Cadieu", "E. Solomon", "D. Seibert", "J. DiCarlo" ],
      "venue" : "Proceedings of the National Academy of Sciences",
      "citeRegEx" : "al. et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 25,
      "context" : ", 2015b] [Mhaskar et al., 2016] and in [Liao and Poggio, .",
      "startOffset" : 9,
      "endOffset" : 31
    }, {
      "referenceID" : 37,
      "context" : "• A recent theorem by Telgarsky [Telgarsky, 2015] can be summarized as saying that a certain family of classification problems with real-valued inputs cannot be approximated well by shallow networks with fewer than exponentially many nodes whereas a deep network achieves zero error.",
      "startOffset" : 32,
      "endOffset" : 49
    } ],
    "year" : 2016,
    "abstractText" : "The paper reviews an emerging body of theoretical results on deep learning including the conditions under which it can be exponentially better than shallow learning. Deep convolutional networks represent an important special case of these conditions, though weight sharing is not the main reason for their exponential advantage. Explanation of a few key theorems is provided together with new results, open problems and conjectures. This work was supported by the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF 1231216. H.M. is supported in part by ARO Grant W911NF-15-10385. 1 ar X iv :1 61 1. 00 74 0v 1 [ cs .L G ] 2 N ov 2 01 6 Why and When Can Deep – but Not Shallow – Networks Avoid the Curse of Dimensionality Tomaso Poggio Hrushikesh Mhaskar Lorenzo Rosasco Brando Miranda Qianli Liao Center for Brains, Minds, and Machines, McGovern Institute for Brain Research, Massachusetts Institute of Technology, Cambridge, MA, 02139. Department of Mathematics, California Institute of Technology, Pasadena, CA 91125; Institute of Mathematical Sciences, Claremont Graduate University, Claremont, CA 91711 Abstract: The paper reviews an emerging body of theoretical results on deep learning including the conditions under which it can be exponentially better than shallow learning. Deep convolutional networks represent an important special case of these conditions, though weight sharing is not the main reason for their exponential advantage. Explanation of a few key theorems is provided together with new results, open problems and conjectures. The paper reviews an emerging body of theoretical results on deep learning including the conditions under which it can be exponentially better than shallow learning. Deep convolutional networks represent an important special case of these conditions, though weight sharing is not the main reason for their exponential advantage. Explanation of a few key theorems is provided together with new results, open problems and conjectures.",
    "creator" : "TeX"
  }
}