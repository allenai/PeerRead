{
  "name" : "1312.3020.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Sparse Allreduce: Efficient Scalable Communication for Power-Law Data",
    "authors" : [ "Huasha Zhao" ],
    "emails" : [ "hzhao@cs.berkeley.edu", "jfc@cs.berkeley.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords-Allreduce; butterfly network; fault tolerant; big data;\nI. INTRODUCTION\nPower-law statistics are the norm for most behavioural datasets, i.e. data generated by people, including the web graph, social networks, text data, clickthrough data etc. By power-law, we mean that the probability distributions of elements in one or both (row and column) dimensions of these matrices follow a function of the form\np ∝ d−α (1)\nwhere d is the degree of that feature (the number of nonzeros in the corresponding row or column). These datasets are large: 40 billion vertices for the web graph, terabytes for social media logs and news archives, and petabytes for large portal logs. Many groups are developing tools to analyze these datasets on clusters [1]–[10]. While cluster approaches have produced useful speedups, they have generally not leveraged single-machine performance either through CPUaccelerated libraries (such as Intel MKL) or using GPUs. Recent work has shown that very large speedups are possible on single nodes [4], [11], and in fact for many common\nmachine learning problems single node benchmarks now dominate the cluster benchmarks that have appeared in the literature [11].\nIts natural to ask if we can further scale single-node performance on clusters of full-accelerated nodes. However, this requires proportional improvements in network primitives if the network is not to be a bottleneck. In this work we are looking to obtain one to two orders of magnitude improvement in the throughput of the Allreduce operation.\nAllreduce is a rather general primitive that is integral to many distributed graph mining and machine learning algorithms. In an Allreduce, data from each node, which can be represented as a vector vi for node i, is reduced in some fashion (say via a sum) to produce an aggregate\nv = ∑\ni=1,...,M\nvi\nand this aggregate is then shared across all the nodes. In many applications, and in particular when the shared data is large, the vectors vi are sparse. And furthermore, each cluster node may not require all of the sum v but only a sparse subset of it. We call a primitive which provides this capability a Sparse Allreduce. By communicating only those values that are needed by the nodes Sparse Allreduce can achieve orders-of-magnitude speedups over dense approaches.\nThe aim of this paper is to develop a general Sparse Allreduce primitive, and tune it to be as efficient as possible for a given problem. We next show how Sparse Allreduce naturally arises in algorithms such as PageRank, Spectral Clustering, Diameter Estimation, and machine learning algorithms that train on blocks (mini-batches) of data, e.g. those that use Stochastic Gradient Descent(SGD) or Gibbs samplers."
    }, {
      "heading" : "A. Applications",
      "text" : "1) MiniBatch Machine Learning Algortihms: Recently there has been considerable progress in sub-gradient algorithms [12], [13] which partition a large dataset into minibatches and update the model using sub-gradients, illustrated in Figure 1. Such models achieve many model updates in a single pass over the dataset, and several benchmarks on\nar X\niv :1\n31 2.\n30 20\nv1 [\ncs .D\nC ]\n1 1\nD ec\n2 01\n3\nlarge datasets show convergence in a single pass [12]. While sub-gradient algorithms have relatively slow theoretical convergence, in practice they often reach a desired loss level much sooner than other methods for problems including Regression, Support Vector Machines, factor models, and several others.\nFinally, MCMC algorithms such as Gibbs samplers involve updates to a model on every sample. In practice to reduce communication overhead, the sample updates are batched in very similar fashion to sub-gradient updates [14].\nAll these algorithms have a common property in terms of the input mini-batch: if the mini-batch involves a subset of features {f1, . . . , fn}, then a gradient update commonly uses input only from, and only makes updates to, the subset of the model that is projected onto those features. This is easily seen for factor and regression models whose loss function has the form\nl = f(AX)\nwhere X is the input mini-batch, A is a matrix which partly parametrizes the model, and f is in general a non-linear function. The derivative of loss, which defines the SGD update, has the form\ndl/dA = f ′(AX)XT\nfrom which we can see that the update is a scaled copy of X , and therefore involves the same non-zero features.\n2) Iterative Matrix Product: Many graph mining algorithms use repeated matrix-matrix/vector multiplication. Here are some representative examples.\nPageRank Given the adjacency matrix G of a graph on n vertices with normalized columns (to sum to 1), and P a vector of vertex scores, the PageRank iteration in matrix form is:\nP ′ = 1\nn + n− 1 n GP (2)\nDiameter Estimation In the HADI [15] algorithm for diameter estimation, the number of neighbours within hop h is encoded in a probabilistic bit-string vector bh. The vector is updated as follows:\nbh+1 = G×or bh. (3)\nAgain G is the adjacency matrix and operation ×or replaces addition in matrix-vector product is replaced by bitwise OR operation.\nSpectral Graph Algorithms Spectral methods make use of the eigen-spectrum (some leading set of eigenvalues and eigenvectors) of the graph adjacency matrix. Almost all eigenvalue algorithms use repeated matrix-vector products with the matrix.\nTo present one of these examples in a bit more detail: PageRank provides an ideal motivation for Sparse Allreduce. The dominant step is computing the matrix-vector product\nG P . We assume that edges of adjacency matrix G are distributed across machines with Gi being the share on machine i, and that vertices P are also distributed (usually redundantly) across machines as Pi on machine i. At each iteration, every machine first acquires a sparse input subset Pi corresponding to non-zero columns of its share Gi - for a sparse graph such as a web graph this will be a small fraction of all the columns. It then computes the product Qi = GiPi. This product vector is also sparse, and its nonzeros correspond to non-zero rows of Gi. The input vertices Pi and the output vertices Qi are passed to a sparse (sum) Allreduce, and the result loaded into the vectors P ′i on the next iteration will be the appropriate share of the matrix product GP . Thus a requirement for Sparse Allreduce is that we be able to specify a vertex subset going in, and a different vertex set going out (i.e. whose values are to be computed and returned)."
    }, {
      "heading" : "B. Summary of Work and Contributions",
      "text" : "In this paper, we describe Sparse Allreduce, a communication primitive that supports high performance distributed machine learning on sparse data. Our Sparse Allreduce has the following properties:\n1) Each network node specifies a sparse vector of output values, and a vector of input indices whose values it wants to obtain from the protocol. 2) Index calculations (configuration) can be separated from value calculations and only computed once for problems where the indices are fixed (e.g. Pagerank iterations). 3) The Sparse Allreduce network is a nested, heterogeneous butterfly. By heterogeneous we mean that the butterfly degree k differs from one layer of the network to another. By nested, we mean that values pass “down” through the network to implement an scatter-reduce, and then back up through the same nodes to implement an allgather.\nSparse Allreduce is modular and easy to run, and requires only a mapping from node ids to IP addresses. Our current implementation is in pure Java, making it easy to integrate with Java-based cluster systems like Hadoop, HDFS, Spark, etc.\nThe key contributions of this paper are the following: • Sparse Allreduce, a programming primitive that sup-\nports efficient parallelization of a wide range of iterative algorithms, including PageRank, diameter estimation, and mini-batch gradient algorithms for Machine Learning, and others. • A number of experiments on large datasets with billions of edges. Experimental results suggest that Sparse Allreduce significantly improves over prior work, by factors of 5-30. • A replication scheme that provides a high-degree of fault-tolerance with modest overhead. We demonstrate that Allreduce with our replication scheme can tolerate about √ M node failures on an M -node network, and\nthat node failures themselves do not slow down the operation.\nThe rest of the paper is organized as follows. Section II reviews existing Allreduce primitives, and highlights their difficulties when applied to large, sparse datasets. Section III introduces Sparse Allreduce, its essential features, and an example network. Section IV and V describe its optimized implementation and fault tolerance respectively. Experimental results are presented in Section VI. We summarize related works in Section VII, and finally Section VIII concludes the paper."
    }, {
      "heading" : "II. BACKGROUND: ALLREDUCE ON CLUSTERS",
      "text" : "Allreduce is a common operation to aggregate local model update in distributed machine learnings. This section reviews the practice of data partition across processors, and popular Allreduce implementations and their limitations."
    }, {
      "heading" : "A. AllReduce",
      "text" : "When data is partitioned across processors, local updates must then be combined by an additive or average reduction, and then redistributed to the hosts. This process is commonly known as Allreduce. Allreduce is commonly implemented with 1) tree structure [16], 2) simple round-robin in fullmesh networks or 3) butterfly topologies [17].\n1) Tree Reduce: The tree reduce topology is illustrated in Figure 2(a). The topology uses the lowest overall bandwidth for atomic messages, although it effectively maximizes latency since the delay is set by the slowest path in the tree. It is a reasonable solution for small, dense (fixedsize) messages. A serious limitation for Sparse Allreduce applications is that the entire sum is held and distributed by the bottom half of the network. i.e. the length of the sparse sums is increasing as one goes down the layers of this network, and eventually encompasses the entire sum. Thus the time to compute sums is increasing layer-by-layer and one effectively loses the advantage of parallelism. It is not practical for the problems of interest to us, and we will not discuss it further.\n2) Round-Robin Reduce: In round-robin reduce, each processor communicate with all other processors in a circular order, as presented in Figure 2(b). Round-robin reduce achieves asymptotically optimal bandwidth, and optimal latency when packets are sufficiently large to mask setupteardown times. In practice though, this requirement is often not satisfied, and there is no way to tune the network to avoid this problem. Also, the very large (quadratic in M) number of messages make this network more prone to failures due to packet corruption, and sensitive to latency outliers.\nIn our experiment setup of a 64-node Amazon EC2 cluster with 10Gb/s inter-connect, the optimal packets size is 1M10M to mask message sending overhead. As illustrated in Figure 3, for smaller packets, latency dominates the communication process, so the runtime per node will goes up when distributing data to larger clusters. In many problems of interest, and e.g. the Twitter follower’s graph and Yahoo’s web graph, the packet size in each round of communication under a round-robin network is much smaller than optimal. This causes significant inefficiencies.\n3) Butterfly Network: In a butterfly network, every node computes some function of the values from its in neighbours (including its own) and outputs to its out neighbours. In the binary case, the neighbours at layer d lie on the edges of hypercube in dimension d with nodes as vertices. The\ncardinality of the neighbour set is called the degree of that layer. Figure 2 demonstrates a 2 × 2 butterfly network and Figure 4 shows a 3 × 2 network. A binary butterfly gives the lowest latency for Allreduce operations when messages have fixed cost.\nFaults in the basic butterfly network affect the outputs, but on a subset of the nodes. Simple recovery strategies (failover to the sibling just averaged with) can produce complete recovery since every value is computed at two nodes. However, butterfly networks involve higher bandwidth.\nWhile these networks are “optimal” in various ways for dense data, they have the problems listed above for sparse data. However, we can hybridize butterfly and round-robin in a way that gives us the good properties of each. Our solution is a d-layer butterfly where each layer has degree k1, . . . , kd. Communication within each group of ki will use the Allreduce pattern. We adjust ki for each layer to the largest value that avoids saturation (packet sizes below the practical minimum discussed earlier). Because the sum of message lengths decreases as we do down layers of the network, the optimal k-values will also typically decrease."
    }, {
      "heading" : "B. Partitions of Power-Law Data",
      "text" : "As shown in [2], edge partitioning is much more effective for large, power-law datasets than vertex partitioning. The paper [2] describes two edge partitioning schemes, one random and one greedy. Here we will only use random edge partitioning - we feel this is more typically the case for data that is “sitting in the network” although results should be similar for other partitioning schemes."
    }, {
      "heading" : "III. SPARSE ALLREDUCE",
      "text" : "In this section, we describe a typical work flow of distributed machine learning, and introduce Sparse Allreduce. Example usage of Sparse Allreduce is discussed in Section III-B."
    }, {
      "heading" : "A. Overview of Sparse AllReduce",
      "text" : "A typical distributed learning task starts with graph/data partitioning, followed by a sequence of alternating model update and model Allreduce. The “data” may directly represent a graph, or may be a data matrix whose adjacency graph is the structure to be partitioned. Canonical examples\nare PageRank and other matrix power calculations, or largemodel machine learning algorithms such as LDA.\nTo avoid clustering of high-degree vertices with similar indices, we first apply a random hash to the vertex indices (which will effect a random permutation). We then sort and thereafter maintain indices in sorted order - this is part of the data structure creation and we assume it is done before everything else.\nThen the vertex set in a group of k nodes is split into k ranges. Because of the initial index permutation, we are effectively partitioning vertices into k sets randomly, but it is much more efficient to do using the sorted index sets. The partitioning is literally splitting the data into contiguous intervals as show in figure 4, using a linear-time, memorystreaming operation. Each range is sent to one of the neighbours (or to self) in the group.\nEach node in the layer below receives sparse vectors in one of the sub-ranges and sums them. For performance reasons, we implement the sums of k vectors using a tree - direct addition of vectors to a cumulative sum has quadratic complexity. Hashing has very bad memory coherence and is about an order of magnitude slower than coherent addition of sorted indices. For the tree addition, the input vectors form the leaves of the tree. The leaves are grouped together in pairs to form parent nodes, and each parent nodes holds a sum of its children. We continue summing siblings up to a root node. This approach has O(Nlogk) complexity (N is total length of all vectors) if there were no index collisions. But thanks to the high frequency of such collisions for power-law data, the total lengths of vectors decreases as we go up the tree. This is bounded by a multiplicative factor less than one, so the practical complexity is O(N) for this stage. In terms of constants, it was about 5x faster overall than a hash table approach.\nThis stage also produces a very helpful compression of the data: i.e. many indices of input vertices collide, and the total length of all vectors across the cluster at the second layer is a fraction of the amount at the first layer.\nThe same process is repeated at the layer below, and continues until we reach the bottom layer of the network. At this point, we will have the sum of all input vectors, and it will be split into narrow and distinct sub-ranges representing R/M where R is the original range of indices, and M is the number of machines.\nFrom here on, the algorithm involves only distribution of results (allgather). Each layer passes up the values that were requested by a parent (and whose indices were saved during the configuration step) to that specific parent. The indices of those values are sorted and they lie in distinct ranges, and the parent has only to concatenate them to produce its final output sparse vector."
    }, {
      "heading" : "B. Use Case Examples",
      "text" : "We provide two methods config and reduce, for the programmers. Configuration involves passing down the outbound indices ( an array of vertex indices to be reduced (outbound) and inbound indices (an array of indices to collect). After configuration, the reduce function is called to obtain the vertex values for the next iteration. The reduce function takes in the vertex values to be reduced (corresponding to outbound vertices) and returns the vertex values for the next update (corresponding to inbound vertices). The following code examples show the usage of our primitive to run the PageRank algorithm and mini-batch update algorithm.\nPageRank:\nvar out = outbound(G) var in = inbound(G) config(out.indices, in.indices) for(i <-0 until iter){\nin.values = reduce(out.values) out.values = matrix_vec_multi(G,in.values)\n}\nIn PageRank, the graph is static, so only one call of config is required at the beginning of iterations until convergence.\nMini-Batch Algorithm:\nfor(i <-0 until iter){ var Di = D(i*b until (i+1)*b) var out = outbound(Di) var in = inbound(Di) config(out.indices, in.indices) in.values = reduce(out.values) out.values = model_update(Di,in.values) }\nAt the beginning of each iteration, a mini-batch of data of size b is loaded to compute the new update. The graph of each mini-batch is dynamic, as a result, config need to be called before every reduce and model update.\nIV. IMPLEMENTATION WITH A BUTTERFLY OF HETEROGENEOUS DEGREES\nWe have implemented Sparse Allreduce on a butterfly network with a layered design, as illustrated in Figure 4. Each layer is characterized by two sets of neighbours the processor receive/send packets from/to and a set of indices/values to be exchanged. In a butterfly network, each processor sums vectors of indices from above, and partitions and transmits the sum to below."
    }, {
      "heading" : "A. Layered Config and Reduce",
      "text" : "Sparse Allreduce has two phases: config step and reduce step.\nIn the config phase, each processor computes a map for each input vector. This map maps indices from the input vector to the sparse sum of all input vectors. The maps facilitate addition of values from above, and then the allgather stage going up.\nIn the reduce phase, processors exchange the vertex values to be reduced. Only vertex values are communicated, because vertex indices are already hard-coded in the maps.\nWe described here config and reduce methods separately, since this simplifies the explanation and the code does include these operations. We also provide a combined config-reduce method that performs both operations in a single round of communication at each layer. i.e. the indices and values during the down phase are sent with the same messages.\nCode for config and reduce: //D: total levels of the butterfly //setup indices public void config(IVec downi, IVec upi) {\nIVec [] outputs = new IVec[2]; for (int d = 0; d < D; i++) { layers[d].config(downi, upi, outputs); downi = outputs[0]; upi = outputs[1]; } finalMap = mapInds(upi, downi);\n} //reduce values public Vec reduce(Vec downv) {\nfor (int d = 0; d < D; d++) { downv = layers[d].reduceDown(downv); } Vec upv = downv.mapFrom(finalMap); for (int d = D-1; d >= 0; d--) { upv = layers[d].reduceUp(upv); } return upv;\n}\nReduce layers are nested instead of cascaded as in the dense Allreduce butterfly fly network. The reduce-scatter (down) computes the reduced values and the allgather (up) redistributes the final values to hosts. If we used a traditional layered butterfly without passing back up through the same nodes, it would be necessary to push the inbound indices along all the way through the network along with their values. This would increase the overall size of configuration messages by about 50%."
    }, {
      "heading" : "B. Heterogeneous Butterfly Network",
      "text" : "In a heterogeneous butterfly network, the degree of in and out neighbours is different from layer to layer. Figure 4 illustrates a 3× 2 network, where each processor talks to 3 neighbours in layer 1 and 2 in layer 2. The benefit of using heterogeneous degree is work balance.\nThe heterogeneous butterfly network is a hybrid of pure round-robin network and standard (degree 2) butterfly. In\npure round-robin, packet size in each round of communication is constrained to be C/M2 where C is the total dataset size, and M is number of machines. This may be too small - smaller than the packet overhead. For example, in the Twitter followers’ graph, the packet size is around 0.5 MB in a 64 node round-robin network. Our tests suggested the effective packet floor for these EC2 nodes is 2-4 MB. On the other hand, in a degree 2 butterfly, the large number of layers leads to much higher overall communication.\nThe heterogeneity of layer degrees allows us to tailor packet size (which becomes C/M/k) with k. We can also deal with issues like latency outliers across the network. Smaller k values will reduce the effects of latency outliers.\nLarger k values are desirable, so long as they do not reduce message sizes below the effective floor. Larger k values leave less work to be done in subsequent layers, and also reduce the total vector size in the next layer because of index collisions. The more vectors that are summed (and the number will be k) in the layer below, the more collisions of matching indices will occur, and each collision implies a reduction of the number of indices below.\nBecause of the reduction of total vector lengths in the layer below, the optimal k value will also be smaller (or we will hit the packet size floor again)."
    }, {
      "heading" : "C. Multi-Threading and Latency Hiding",
      "text" : "Scientific computing systems typically maintain a high degree of synchrony between nodes running a calculation. In cluster environments, we have to allow for many sources of variability in node timing, latency and throughput. While our network conceptually uses synchronized messages to different destinations to avoid congestion, in practice this does not give best performance. Instead we use multithreading and communicate opportunistically. i.e. we start threads to send all messages concurrently, and spawn a thread to process each message that is received. In the case of replicated messages, once the first message of a\nreplicate group is received, the other threads listening for duplicates are terminated and those copies discarded. Still, the network interface itself is a shared resource, so we have to be careful that excessive threading does not hurt performance through switching of the active message thread. The effects of varying the thread count is shown in Figure 7."
    }, {
      "heading" : "D. Language and Networking Libraries",
      "text" : "Sparse Allreduce is currently implemented using standard Java sockets. We explored several other options including OpenMPI-Java, MPJexpress, and Java NIO. Unfortunately the MPI implementations lacked key features that we needed to support multi-threading, asynchronous messaging, cancellation etc., or these features did not work through the Java interface. Java NIO was simply more complex to use without a performance advantage. All of the features we needed were easily implemented with sockets, and ultimately they were a better match for the level of abstraction and configurability that we needed.\nWe acknowledge that the network interface could be considerably improved. The ideal choice would be RDMA over Ethernet (RoCE), and even better RoCE directly from the GPUs. This feature in fact already exists (as GPUdirect for NVIDIA CUDA GPUs). But it currently only available for infiniband networks. Other benchmarks of this technology suggest a 4- 5-fold improvement should be possible."
    }, {
      "heading" : "V. FAULT TOLERANCE",
      "text" : "Machine failure is a reality of large clusters. We introduce in this section a simple but relatively efficient fault tolerance mechanism to deal with multiple node failures."
    }, {
      "heading" : "A. Data Duplication",
      "text" : "Our approach is to replicate by a replication factor r, the data on each node, and all messages. Thus data on machine i also appears on the replicas M+ i through i+(r−1)∗M .\nSimilarly every config and reduce message targeted at node j is also sent to replicas M + j through j + (r − 1) ∗M . When receiving a message expected from node j, the other replicas are also listened to. The first message received is used, and the other listeners are cancelled.\nThis protocol completes unless all the replicas in a group are dead. e.g. when the replication factor is two, the probability of this happening with random failures on an M -node network is about √ M (birthday paradox)."
    }, {
      "heading" : "B. Packets Racing",
      "text" : "Replication by r increases per-node communication by r in the worst case (cancellations will reduce it somewhat). There is some performance loss because of this, as shown in the next section. On the other hand, replication offers potential gains on networks with high latency or throughput variance, because they create a race for the fastest response (in contrast to the non-replicate network which is instead driven by the slowest path in the network."
    }, {
      "heading" : "VI. EXPERIMENTS",
      "text" : "In this section, we evaluate the performance and scalability of Sparse Allreduce, in comparison with other popular systems including Hadoop, Spark and PowerGraph. Three datasets are primarily studied in this section. • Twitter follower’s graph. The graph consists of 60\nmillion vertices and 1.5 billion edges. Figure 1 in [2] shows the power-law property of this graph. • Yahoo! Altavista web graph. This is one of the largest publicly available web graphs with 1.4 billion vertices and 6 billion edges. • Twitter document-term graph. The dataset consists of 1 billion unique tweets with 40 million uni-gram bagof-words features each. The data is collected using Twitter API during the March of 2013, which provides 10% sampled “gardenhose” twitter stream in the XML format.\nAll the experiments are conducted on the Amazon EC2 cluster consists of Cluster Compute nodes (cc1.4xlarge). Each node is equipped with 8 virtualized cores and they are interconnected by 10Gb/s Ethernet."
    }, {
      "heading" : "A. Sparsity of the Datasets",
      "text" : "Table I demonstrates the sparsity of the partitioned datasets. The Twitter followers’ graph and Yahoo web graph are partitioned across 64 processors using random edge partition, and the model size is the total number of vertices in the graph. While the Twitter document-term graph are partitioned by hour ot tweet; each partition is one “minibatch” to feed into some sub-gradient/online method. The model dimension is the number of uni-gram features.\nAs illustrated in the Table I, all dataset demonstrates strong sparsity after partition. The Yahoo web graph is the biggest one in terms of model size, and it is also the most\nsparse one among the three, each partition only holds 3 percent of all the vertices."
    }, {
      "heading" : "B. Optimal System Parameters",
      "text" : "We described the trade-offs between round-robin and binary butterfly earlier. In this section, we empirically determine the optimal configuration of the butterfly degrees to deliver the best Sparse Allreduce performance.\nFigure 5 plots the packet sizes at different level of butterfly network for different configurations in a 64-node cluster, which holds the random (edge) partitioned Twitter followers’ graph. As illustrated from the figure, the 64 round-robin topology sends 0.5MB of packets each round which unlikely to fully utilize the bandwidth. Also for the butterfly configuration, although the packet size is decaying with depth into the network, the more layer we have, the more duplicated message we send. The full butterfly with degree 2 ends up sending packets of 17MB for each machine in the first round of communication.\nFigure 6 plots the average reduce time per iteration and throughput for different configuration, for both Twitter followers’ graph and Yahoo web graph. Throughput is measured in terms of total billions of input values reduced per second. From the figure, we can see the best configurations for both graph is 16 × 4. This has already been hinted in Figure 5, for topology 16 × 4, communication is almost evenly distributed across two layers of the network; this balance prevents under-utilization of bandwidth by small packets.\nIt’s not surprising to see that the round robin is closer to the optimal in the Web graph. The Web graph is much bigger in size, so latency is less of a problem when distributed to 64-node networks. However, round-robin may get into scaling issues when distributed to more machines."
    }, {
      "heading" : "C. Effect of Multi-Threading",
      "text" : "We compare the Allreduce runtime for different thread levels in Figure 7. All the results are run under the 16 × 4 configuration. Significant performance improvement can be observed by increasing from single thread up to 4 threads, and it is also clear from the figure that the benefit of adding\nTable I: Sparsity of the partitioned datasets\nData Set Twitter follower’s graph Yahoo web graph Twitter document-term graph Partition # of vertices 12.1M 48M 5.1M\nTotal # of vertices 60M 1.6B 40M Percentage of total vertices 0.21 0.03 0.12\n(a) Twitter followers’ graph (b) Yahoo web graph\nFigure 6: Allreduce time per iteration and throughput\nthread level is marginal beyond 8 threads (remember we are running on 8-core machines). However, there is no penalty to add more threads: resources are just being shared among the thread pool."
    }, {
      "heading" : "D. Cost of Fault Tolerance",
      "text" : "Table II demonstrates the overhead of data replication in terms of runtime. We compare a 8 × 4 network with replication with a 16 × 4 network and a 8 × 4 network with no replication. The 8× 4 network with error-tolerance consumes the same amount resource as the 16× 4 network: both requires 64 machines. The data is partitioned into 32 pieces and each piece is hosted by 2 machines. It doubles the resource requirement in comparison with 8× 4 network with no fault tolerance.\nAs shown in the table, the impact of data duplication on runtime is moderate. In the 8 × 4 network, the replication version is only 10-15% slower than the no replication version. Given 64 machines, the error-tolerance runs 50-60 % slower than without error-tolerance.\nWe also compare the runtime for different number of node failures. With no replication, the system cannot compute the correct reduce results. The replicated version is able to compute the correct result with node failures most of the time unless all the nodes in a replication set are lost. For replication by two, the expected number of failures to cause this is about √ M for M total machines (birthday paradox)."
    }, {
      "heading" : "E. Performance and Scalability",
      "text" : "In this section, we show the performance and scalability of Sparse Allreduce performance and scalability by running PageRank algorithm on clusters of different size and different systems. PageRank is implemented on top of BIDMat, an interactive matrix library written in Scala that fully utilize hardware accelerations (Intel MKL). So the computation is already an order of magnitude faster than pure Java. the Twitter follower’s graph and Yahoo web graph.\nThe scaling of Sparse Allreduce is illustrated in Figure 8. The figure plots the total runtime in the first 10 iteration against cluster size. The configuration is optimally tuned individually for different cluster size. We also present the runtime breakdown (into computation and communication). As shown in the figure, the system scales well up to 64 nodes. However, communication starts to dominate the runtime for larger clusters. Particularly, for the 64 node cluster, communication takes up to 80% of overall runtime.\nIt is also worth pointing out that the overall achieved bandwidth is around 2Gb/s on EC2 which is much smaller than the rated 10Gb/s of the network. This is not a bad number for the communication technology used (pure Java sockets). Socket performance in HPC has been extensively studied, and it is well-known that Java sockets achieve a maximum of a few Gbits/sec. There are several technologies available which would better this figure, however at this time\nFigure 8: Sparse Allreduce scaling and compute/comm break down. v\nFigure 9: PageRank runtime comparison (log scale).\nthere are barriers to their use on commodity clusters. RDMA over Converged Ethernet would be an ideal technology. This technology bypasses copies in several layers of the TCP/IP stack and uses direct memory-to-memeory copies to delivers throughputs much closer to the limits of the network itself. It is available currently for GPU as GPUdirect (which communicates directly from on GPU’s memory to another over a network), and in Java as Sockets Direct. However, at this time both these technologies are only available for Infiniband networks. We will monitor these technologies, and we also plan to experiment with some open source projects like RoCE (RDMA over Converged Ethernet) which offer more modest gains.\nFinally, we compare our system with other popular distributed learning systems: Hadoop/Pegasus, GraphX and GraphLab. Figure 9 plots the first 10 iteration runtime for different systems. There’s no data available for Mahout and GraphX for the Yahoo dataset. The y-axis of the plot is logscale. Sparse Allreduce spends 6 seconds for 10 PageRank\niterations on the Twitter followers’ graph and 23 seconds for the Yahoo graph. As seen from the figure, each system provide half to one order magnitude improvement from right to left.\nIt also worth mentioning that PowerGraph uses greedily partitioned graph which produces shorter vertex lists (and communication) on each node. Our benchmarks use random partitioning, and should improve by about 15-20 % using greedy partition."
    }, {
      "heading" : "VII. RELATED WORKS",
      "text" : "Many other distributed learning systems are under active development at this time [1]–[6]. Our work is closest to the GraphLab project which also has a focus on PowerLaw graphs and matrices. [1] improves upon the Hadoop MapReduce infrastructure by expressing asynchronous iterative algorithms with sparse computational dependencies. PowerGraph is a improved version of GraphLab, where the concerns about power-law graph in the context of graph mining has been first proposed. We have taken a somewhat more modular approach, isolating the Allreduce primitive from matrix and machine learning modules. The Pegasos project proposed GIM-V, a primitive generalizable to a variety of graph mining tasks. We further generalize the primitive to mini-batch update algorithms which covers regressions, factor model, topic models etc. There are a variety of other similar distributed data mining systems [10], [15], [20], [21] built on top of Hadoop that however, the disk-caching and disk-buffering philosophy of Hadoop, along with heavy reliance on reflection and serialization, cause such approaches to fall orders of magnitude behind the other approaches discussed here.\nOur work is also related with research in distributed SpMV (Sparse Matrix Vector multiplication) algorithms [22]–[24] in the parallel/scientific computing community. However, they usually deal with matrices with regular shapes (tri-diagonal) or desirable partition properties such as small surface-to-volume ratio. We also distinguish our work by concentrating on studying the performance tradeoff on commodity hardwares, such as on Amazon EC2, as opposed to scientific clusters featuring extremely fast network connections, high synchronization and exclusive (non-virtal) machine use."
    }, {
      "heading" : "VIII. CONCLUSION",
      "text" : "In this paper, we describe Sparse Allreduce for efficient and scalable distributed machine learning. The primitive is\nparticularly well-adapted to the power-law data common in machine learning and graph analysis. We showed that the best approach is a hybrid between butterfly and round-robin topologies, using a nested communication pattern and nonhomogeneous layer degrees. We added a replication layer to the network which provides a high degree of fault tolerance with modest overhead. Finally, we presented a number of experiments exploring the performance of Sparse Allreduce primitive. We showed that it is significantly faster than other primitives, and is limited at this time by the performance of the underlying technology, Java Sockets. In the future we hope to achieve further gains by using more advanced network layers that use RDMA over Converged Ethernet (RoCE). Our code is open-source and freely-available, and is currently in pure Java. It is distributed as part of the BIDMat suite, but can be run standalone without other BIDMat features."
    } ],
    "references" : [ {
      "title" : "Graphlab: A new framework for parallel machine learning",
      "author" : [ "Y. Low", "J. Gonzalez", "A. Kyrola", "D. Bickson", "C. Guestrin", "J.M. Hellerstein" ],
      "venue" : "arXiv preprint arXiv:1006.4990, 2010.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Powergraph: Distributed graph-parallel computation on natural graphs",
      "author" : [ "J.E. Gonzalez", "Y. Low", "H. Gu", "D. Bickson", "C. Guestrin" ],
      "venue" : "Proc. of the 10th USENIX conference on Operating systems design and implementation, OSDI, vol. 12.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 0
    }, {
      "title" : "Pregel: a system for large-scale graph processing",
      "author" : [ "G. Malewicz", "M.H. Austern", "A.J. Bik", "J.C. Dehnert", "I. Horn", "N. Leiser", "G. Czajkowski" ],
      "venue" : "Proceedings of the 2010 international conference on Management of data. ACM, 2010, pp. 135–146.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Butterfly mixing: Accelerating incremental-update algorithms on clusters",
      "author" : [ "H. Zhao", "J. Canny" ],
      "venue" : "SIAM International Conference on Data Mining, 2013.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Pegasus: A peta-scale graph mining system implementation and observations",
      "author" : [ "U. Kang", "C.E. Tsourakakis", "C. Faloutsos" ],
      "venue" : "Data Mining, 2009. ICDM’09. Ninth IEEE International Conference on. IEEE, 2009, pp. 229–238.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Dryad: distributed data-parallel programs from sequential building blocks",
      "author" : [ "M. Isard", "M. Budiu", "Y. Yu", "A. Birrell", "D. Fetterly" ],
      "venue" : "ACM SIGOPS Operating Systems Review, vol. 41, no. 3, pp. 59–72, 2007.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Graphsig: A scalable approach to mining significant subgraphs in large graph databases",
      "author" : [ "S. Ranu", "A.K. Singh" ],
      "venue" : "Data Engineering, 2009. ICDE’09. IEEE 25th International Conference on. IEEE, 2009, pp. 844–855.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "gapprox: Mining frequent approximate patterns from a massive network",
      "author" : [ "C. Chent", "X. Yan", "F. Zhu", "J. Han" ],
      "venue" : "Data Mining, 2007. ICDM 2007. Seventh IEEE International Conference on. IEEE, 2007, pp. 445–450.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "gspan: Graph-based substructure pattern mining",
      "author" : [ "X. Yan", "J. Han" ],
      "venue" : "Data Mining, 2002. ICDM 2003. Proceedings. 2002 IEEE International Conference on. IEEE, 2002, pp. 721–724.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Disco: Distributed co-clustering with map-reduce: A case study towards petabyte-scale end-toend mining",
      "author" : [ "S. Papadimitriou", "J. Sun" ],
      "venue" : "Data Mining, 2008. ICDM’08. Eighth IEEE International Conference on. IEEE, 2008, pp. 512–521.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Big data analytics with small footprint: Squaring the cloud",
      "author" : [ "J. Canny", "H. Zhao" ],
      "venue" : "ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), 2013.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Large scale online learning",
      "author" : [ "L.B.Y. Le Cun", "L. Bottou" ],
      "venue" : "Advances in neural information processing systems, vol. 16, p. 217, 2004.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "J. Duchi", "E. Hazan", "Y. Singer" ],
      "venue" : "The Journal of Machine Learning Research, vol. 999999, pp. 2121–2159, 2011.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "An architecture for parallel topic models",
      "author" : [ "A. Smola", "S. Narayanamurthy" ],
      "venue" : "Proceedings of the VLDB Endowment, vol. 3, no. 1-2, pp. 703–710, 2010.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Fast diameter estimation and mining in massive graphs with hadoop",
      "author" : [ "U. Kang", "C. Tsourakakis", "A.P. Appel", "C. Faloutsos", "J. Leskovec", "Hadi" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2008
    }, {
      "title" : "Vowpal wabbit online learning project",
      "author" : [ "J. Langford", "L. Li", "A. Strehl" ],
      "venue" : "2007.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Bandwidth optimal all-reduce algorithms for clusters of workstations",
      "author" : [ "P. Patarasuk", "X. Yuan" ],
      "venue" : "Journal of Parallel and Distributed Computing, vol. 69, no. 2, pp. 117–124, 2009.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Benchmarking amazon ec2 for high-performance scientific computing",
      "author" : [ "E. Walker" ],
      "venue" : "Usenix Login, vol. 33, no. 5, pp. 18–23, 2008.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "A performance analysis of ec2 cloud computing services for scientific computing",
      "author" : [ "S. Ostermann", "A. Iosup", "N. Yigitbasi", "R. Prodan", "T. Fahringer", "D. Epema" ],
      "venue" : "Cloud Computing. Springer, 2010, pp. 115–131.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Haloop: Efficient iterative data processing on large clusters",
      "author" : [ "Y. Bu", "B. Howe", "M. Balazinska", "M.D. Ernst" ],
      "venue" : "Proceedings of the VLDB Endowment, vol. 3, no. 1-2, pp. 285–296, 2010.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Avoiding communication in sparse matrix computations",
      "author" : [ "J. Demmel", "M. Hoemmen", "M. Mohiyuddin", "K. Yelick" ],
      "venue" : "Parallel and Distributed Processing, 2008. IPDPS 2008. IEEE International Symposium on. IEEE, 2008, pp. 1–12.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Communication-avoiding krylov subspace methods",
      "author" : [ "M. Hoemmen" ],
      "venue" : "Ph.D. dissertation, University of California, 2010.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Communication-avoiding parallel and sequential qr factorizations",
      "author" : [ "J. Demmel", "L. Grigori", "M. Hoemmen", "J. Langou" ],
      "venue" : "CoRR abs/0806.2159, 2008.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Many groups are developing tools to analyze these datasets on clusters [1]–[10].",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 9,
      "context" : "Many groups are developing tools to analyze these datasets on clusters [1]–[10].",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 3,
      "context" : "Recent work has shown that very large speedups are possible on single nodes [4], [11], and in fact for many common machine learning problems single node benchmarks now dominate the cluster benchmarks that have appeared in the literature [11].",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 10,
      "context" : "Recent work has shown that very large speedups are possible on single nodes [4], [11], and in fact for many common machine learning problems single node benchmarks now dominate the cluster benchmarks that have appeared in the literature [11].",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 10,
      "context" : "Recent work has shown that very large speedups are possible on single nodes [4], [11], and in fact for many common machine learning problems single node benchmarks now dominate the cluster benchmarks that have appeared in the literature [11].",
      "startOffset" : 237,
      "endOffset" : 241
    }, {
      "referenceID" : 11,
      "context" : "1) MiniBatch Machine Learning Algortihms: Recently there has been considerable progress in sub-gradient algorithms [12], [13] which partition a large dataset into minibatches and update the model using sub-gradients, illustrated in Figure 1.",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 12,
      "context" : "1) MiniBatch Machine Learning Algortihms: Recently there has been considerable progress in sub-gradient algorithms [12], [13] which partition a large dataset into minibatches and update the model using sub-gradients, illustrated in Figure 1.",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 11,
      "context" : "large datasets show convergence in a single pass [12].",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 13,
      "context" : "In practice to reduce communication overhead, the sample updates are batched in very similar fashion to sub-gradient updates [14].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 14,
      "context" : "Diameter Estimation In the HADI [15] algorithm for diameter estimation, the number of neighbours within hop h is encoded in a probabilistic bit-string vector b.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 15,
      "context" : "Allreduce is commonly implemented with 1) tree structure [16], 2) simple round-robin in fullmesh networks or 3) butterfly topologies [17].",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 16,
      "context" : "Allreduce is commonly implemented with 1) tree structure [16], 2) simple round-robin in fullmesh networks or 3) butterfly topologies [17].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 1,
      "context" : "As shown in [2], edge partitioning is much more effective for large, power-law datasets than vertex partitioning.",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 1,
      "context" : "The paper [2] describes two edge partitioning schemes, one random and one greedy.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 1,
      "context" : "IVec [] outputs = new IVec[2];",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 0,
      "context" : "upi = outputs[1];",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 1,
      "context" : "Figure 1 in [2] shows the power-law property of this graph.",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 0,
      "context" : "Many other distributed learning systems are under active development at this time [1]–[6].",
      "startOffset" : 82,
      "endOffset" : 85
    }, {
      "referenceID" : 5,
      "context" : "Many other distributed learning systems are under active development at this time [1]–[6].",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 0,
      "context" : "[1] improves upon the Hadoop MapReduce infrastructure by expressing asynchronous iterative algorithms with sparse computational dependencies.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "There are a variety of other similar distributed data mining systems [10], [15], [20], [21] built on top of Hadoop that however, the disk-caching and disk-buffering philosophy of Hadoop, along with heavy reliance on reflection and serialization, cause such approaches to fall orders of magnitude behind the other approaches discussed here.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 14,
      "context" : "There are a variety of other similar distributed data mining systems [10], [15], [20], [21] built on top of Hadoop that however, the disk-caching and disk-buffering philosophy of Hadoop, along with heavy reliance on reflection and serialization, cause such approaches to fall orders of magnitude behind the other approaches discussed here.",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 19,
      "context" : "There are a variety of other similar distributed data mining systems [10], [15], [20], [21] built on top of Hadoop that however, the disk-caching and disk-buffering philosophy of Hadoop, along with heavy reliance on reflection and serialization, cause such approaches to fall orders of magnitude behind the other approaches discussed here.",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 20,
      "context" : "Our work is also related with research in distributed SpMV (Sparse Matrix Vector multiplication) algorithms [22]–[24] in the parallel/scientific computing community.",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 22,
      "context" : "Our work is also related with research in distributed SpMV (Sparse Matrix Vector multiplication) algorithms [22]–[24] in the parallel/scientific computing community.",
      "startOffset" : 113,
      "endOffset" : 117
    } ],
    "year" : 2013,
    "abstractText" : "Many large datasets exhibit power-law statistics: The web graph, social networks, text data, clickthrough data etc. Their adjacency graphs are termed natural graphs, and are known to be difficult to partition. As a consequence most distributed algorithms on these graphs are communicationintensive. Many algorithms on natural graphs involve an Allreduce: a sum or average of partitioned data which is then shared back to the cluster nodes. Examples include PageRank, spectral partitioning, and many machine learning algorithms including regression, factor (topic) models, and clustering. In this paper we describe an efficient and scalable Allreduce primitive for power-law data. We point out scaling problems with existing butterfly and round-robin networks for Sparse Allreduce, and show that a hybrid approach improves on both. Furthermore, we show that Sparse Allreduce stages should be nested instead of cascaded (as in the dense case). And that the optimum throughput Allreduce network should be a butterfly of heterogeneous degree where degree decreases with depth into the network. Finally, a simple replication scheme is introduced to deal with node failures. We present experiments showing significant improvements over existing systems such as PowerGraph and Hadoop. Keywords-Allreduce; butterfly network; fault tolerant; big data;",
    "creator" : "LaTeX with hyperref package"
  }
}