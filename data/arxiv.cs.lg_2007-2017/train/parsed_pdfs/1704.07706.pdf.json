{
  "name" : "1704.07706.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Automatic Anomaly Detection in the Cloud Via Statistical Learning",
    "authors" : [ "Jordan Hochenbaum", "Owen S. Vallis", "Arun Kejariwal" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "To this end, we developed two novel statistical techniques for automatically detecting anomalies in cloud infrastructure data. Specifically, the techniques employ statistical learning to detect anomalies in both application, and system metrics. Seasonal decomposition is employed to filter the trend and seasonal components of the time series, followed by the use of robust statistical metrics – median and median absolute deviation (MAD) – to accurately detect anomalies, even in the presence of seasonal spikes. We demonstrate the efficacy of the proposed techniques from three different perspectives, viz., capacity planning, user behavior, and supervised learning. In particular, we used production data for evaluation, and we report Precision, Recall, and F-measure in each case."
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "Big Data is characterized by the increasing volume (on the order of zetabytes), and the velocity of data generation [1, 2]. It is projected that the market size of Big Data shall climb up from the current market size of $5.1 billion [3] to $53.7 billion by 2017. In a recent report [4], EMC Corporation stated: “A major factor behind the expansion of the digital universe is the growth of machine generated data, increasing from 11% of the digital universe in 2005 to over 40% in 2020.” In the context of social networks, analysis of User Big Data is key to building an engaging social network; in a similar fashion, analysis of Machine Big Data is key to building an efficient and performant underlying cloud computing platform.\nAnomalies in Big Data can potentially result in losses to the business – in both revenue [5], as well as in long term reputation [6]. To this end, several enterprise-wide monitor-\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Copyright 200Z ACM X-XXXXX-XX-X/XX/XX ...$5.00.\ning initiatives [7, 8] have been undertaken. Likewise, there has been an increasing emphasis on developing techniques for detection, and root cause analysis, of performance issues in the cloud [9, 10, 11, 12, 13, 14, 15].\nA lot of research has been done in the context of anomaly detection in various domains such as, but not limited to, statistics, signal processing, finance, econometrics, manufacturing, and networking [16, 17, 18, 19]. In a recent survey paper Chandola et al. highlighted that anomalies are contextual in nature [20] and remarked the following:\nA data instance might be a contextual anomaly in a given context, but an identical data instance (in terms of behavioral attributes) could be considered normal in a different context. This property is key in identifying contextual and behavioral attributes for a contextual anomaly detection technique.\nDetection of anomalies in the presence of seasonality, and an underlying trend – which are both characteristic of the time series data of social networks – is non-trivial. Figure 1 illustrates the presence of both positive and negative anomalies – corresponding to the circled data points – in time series data obtained from production. From the figure we note that the time series has a very conspicuous seasonality, and that there are multiple modes within a seasonal period. Existing techniques for anomaly detection (overviewed indepth in Section 5) are not amenable for time series data with the aforementioned characteristics. To this end, we developed novel techniques for automated anomaly detection in the cloud via statistical learning. In particular, the main contributions of the paper are as follows:\nr First, we propose novel statistical learning based techniques to detect anomalies in the cloud. The proposed techniques can be used to automatically detect anomalies in time series data of both application metrics such as Tweets Per Sec (TPS) and system metrics such as CPU utilization etc. Specifically, we propose the following:\nz Seasonal ESD (S-ESD): This techniques employs time series decomposition to determine the seasonal component of a given time series. S-ESD then applies ESD [21, 22] on the resulting time series to detect the anomalies.\nz Seasonal Hybrid ESD (S-H-ESD): In the case of some time series (obtained from production) we observed a relatively high percentage of anomalies. To address such cases, coupled with the fact that mean and standard deviation (used by ESD) are highly sensitive to a large number anomalies [23, 24], we extended S-ESD to use the robust statistics median [25] and median absolute deviation (MAD) to detect anomalies [26]. Compu-\nar X\niv :1\n70 4.\n07 70\n6v 1\n[ cs\n.L G\n] 2\n4 A\npr 2\n01 7\ntationally, S-H-ESD is more expensive than SESD, but is more robust to a higher percentage of anomalies.\nr Second, we present a detailed evaluation of the proposed techniques using production data. In particular, we present the evaluation from three different perspectives:\nz Capacity planning: Occurrence of anomalies can potentially result in violation of service level agreement (SLA), and/or impact end-user experience. To mitigate the impact of anomalies, timely detection of anomalies is paramount and may require provisioning additional capacity. Given a threshold for a system metric (corresponding to the SLA), we evaluate the efficacy of the proposed techniques to detect anomalies greater than the specified threshold.\nz User behavior: Change in user behavior – which may result due to, but not limited to, events such as the Superbowl – at times manifests itself as anomalies in time series data. Thus, anomaly detection can guide the study of change in user behavior. To this end, we evaluated the efficacy of the proposed techniques with respect to change in user behavior.\nz Supervised Learning: Given the velocity, volume, and real-time nature of cloud infrastructure data, it is not practical to obtain time series data with “true” anomalies labeled. To address this limitation, we injected anomalies in a randomized fashion in smoothed, using B-spline, production data. The randomization was done along three dimensions – time of injection, magnitude, and width of the anomaly. We evaluated the efficacy of the proposed techniques with respect to detection of the injected anomalies.\nThe rest of the paper is organized as follows: Section 2 lays out the notation used in the rest of the paper, and briefly overviews statistical background for completeness and better understanding of the rest of the paper.\nSection 3 details the techniques proposed in this paper, viz., S-ESD and S-H-ESD. Section 4 presents a detailed evaluation of the aforementioned techniques using production data.\nPrevious work is discussed in Section 5. Finally, in Section 6 we conclude with directions for future work."
    }, {
      "heading" : "2. BACKGROUND",
      "text" : "In this section we describe the notation used in the rest of the paper. Additionally, we present a brief statistical background for completeness, and for better understanding of the rest of the paper.\nA time series refers to a set of observations collected sequentially in time. Let xt denote the observation at time t, where t = 0, 1, 2, . . ., and let X denote the set of all observations constituting the time series."
    }, {
      "heading" : "2.1 Grubbs Test and ESD",
      "text" : "In this subsection, we briefly overview the most widely used existing techniques for anomaly detection. In essence, these techniques employ statistical hypothesis testing, for a given significance level [27], to determine whether a datum is anomalous. In other words, the test is used to evaluate the rejection of the null hypothesis (H0), for a pre-specified level of significance, in favor of the alternative hypothesis (H1).\n2.1.1 Grubbs Test Grubbs test [28, 29] was developed for detecting the largest anomaly within a univariate sample set. The test assumes that the the underlying data distribution is normal. Grubbs’ test is defined for the hypothesis:\nH0 :There are no outliers in the data set (1)\nH1 :There is at least one outlier in the data set (2)\nThe Grubbs’ test statistic is defined as follows:\nC = maxt | xt − x |\ns (3)\nwhere, x and s denote the mean and variance of the time series X. For the two-sided test, the hypothesis of no outliers is rejected at significance level α if\nC > (N − 1)√\nN\n√ (tα/(2N),N−2)2\nN − 2 + (tα/(2N),N−2)2 (4)\nwhere tα/(2N),N−2 denotes the upper critical values of the tdistribution with N−2 degrees of freedom and a significance level of α/(2N). For one-sided tests, α/(2N) becomes α/N [30]. The largest data point in the time series that is greater than the test statistic is labeled as an anomaly.\nIn practice, we observe that there is more than one anomaly in the time series data obtained from production. Conceivably, one can iteratively apply Grubbs’ test to detect mul-\ntiple anomalies. Removal of the largest anomaly at each iteration reduces the value of N ; however, Grubbs’ test does not update the value obtained from the t-distribution tables. Consequently, Grubbs’ test is not suited for detecting multiple outliers in a given time series data.\nSeveral other approaches, such as the Tietjen-Moore test1 [31], and the extreme Studentized deviate (ESD) test [21, 22] have been proposed to address the aforementioned issue. Next, we briefly overview ESD.\n2.1.2 Extreme Studentized Deviate (ESD) The Extreme Studentized Deviate test (ESD) [21] (and its generalized version [22]) can also be used to detect multiple anomalies in the given time series. Unlike the TietjenMoore test, it only requires an upper bound on the number of anomalies (k) to be specified. In the worst case, the number of anomalies can be at most 49.9% of the total number of data points in the given time series. In practice, our observation, based on production data, has been that the number of anomalies is typically less than 1% in the context of application metrics and less than 5% in the context of system metrics.\nESD computes the following test statistic for the k most extreme values in the data set.\nCk = maxk | xk − x |\ns (5)\nThe test statistic is then compared with a critical value, computed using Equation 6, to determine whether a value is anomalous. If the value is indeed anomalous, it is removed from the data set, and the critical value is recalculated from the remaining data.\nλk = (n− k)tp,n−k−1√\n(n− k − 1 + t2p,n−k−1 )(n− k + 1) (6)\nESD repeats this process k times, with the number of anomalies equal to the largest k such that Ck > λk. In practice, Ck may swing above and below λk multiple times before permanently becoming less then λk.\nIn the case of Grubbs’ test, the above would cause the test to prematurely exit; however, ESD will continue until the test has run for k outliers."
    }, {
      "heading" : "2.2 Median and Median Absolute Deviation",
      "text" : "The techniques discussed in the previous subsection use mean and standard deviation. It is well known that these metrics are sensitive to anomalous data [25, 32]. As such, the use of the statistically robust median, and the median absolute deviation (MAD), has been proposed to address these issues.\nThe sample mean x̄ can be distorted by a single anomaly, with the distortion increasing as xt → ±∞. By contradistinction, the sample median is robust against such distortions, and can tolerate up to 50% of the data being anomalous. Thus, the sample mean is said to have a breakdown point [33, 34] of 0, while the sample median is said to have a breakdown point of 0.5.\nFor a univariate data set X1, X2, ..., Xn, MAD is defined as the median of the absolute deviations from the sample median. Formally,\n1Although the Tietjen-Moore test can be used to detect multiple anomalies, it requires the number of anomalies to detect to be pre-specified. This is not practical in the current context.\nMAD = mediani(|Xi −medianj(Xj)|) (7)\nUnlike standard deviation, MAD is robust against anomalies in the input data [26, 25]. Furthermore, MAD can be used to estimate standard deviation by scaling MAD by a constant factor b.\nσ̂ = b ·MAD (8)\nwhere b = 1.4826 is used for normally distributed data (irrespective of non-normality introduced by outliers) [35]. When another underlying distribution is assumed, Leyes et al. suggest b = 1\nQ(0.75) , where Q(0.75) is the 0.75 quantile of\nthe underlying distribution [23]."
    }, {
      "heading" : "2.3 Precision, Recall, and F-Measure",
      "text" : "As mentioned earlier in Section 1, the efficacy of the proposed techniques were evaluated from three different perspectives. In each case we report the following metrics – Precision, Recall, and F-measure (these metrics are commonly used to report the efficacy of an algorithm in data mining, information retrieval, et cetera).\nIn the context of anomaly detection, Precision is defined as follows:\nPrecision = |{S} ∩ {G}| |{S}| = tp tp+ fp (9)\nwhere S is the set of detected anomalies, and G is the set of ground-truth anomalies. In other words, Precision is the ratio of true positives (tp) over the sum of true positives (tp) and false positives (fp) that have actually been detected.\nIn the context of anomaly detection, Recall is defined as follows:\nRecall = |{S} ∩ {G}| |{G}| = tp tp+ fn (10)\nwhere fn denotes false negatives. Lastly, F-measure is defined as follows:\nF = 2× precision× recall precision + recall\n(11)\nSince Precision and Recall are weighted equally in Equation 11, the measure is also referred as the F1-measure, or the balanced F-score. Given that there is a trade-off between Precision and Recall, the F-measure is often generalized as follows:\nFβ = (1 + β 2)× precision× recall\nβ2 × precision + recall where β ≥ 0\nIf β > 1, F is said to become more recall-oriented and if β < 1, F is said to become more precision-oriented [36]."
    }, {
      "heading" : "3. TECHNIQUES",
      "text" : "In this section we detail our approaches Seasonal ESD (SESD) and Seasonal Hybrid ESD (S-H-ESD). Note that both the approaches are currently deployed to automatically detect anomalies in production data on a daily basis. We employed an incremental approach in developing the two approaches. In particular, we started with evaluating the “rule of thumb” (the Three Sigma Rule) using production\ndata and progressively learned the limitations of using the existing techniques for automatically detecting anomalies in production data. In the rest of this section we walk the reader through the core steps of our aforementioned incremental approach."
    }, {
      "heading" : "3.1 Three-Sigma Rule",
      "text" : "As a first cut, the 3 · σ “rule” is commonly used to detect anomalies in a given data set. Specifically, data points with values more than 3 times the sample standard deviation are deemed anomalous. The rule can potentially be used for capturing large global anomalies, but is ill-suited for detecting seasonal anomalies. This is exemplified by Figure 2, wherein the seasonal anomalies that could not be captured using the 3 · σ “rule” are annotated with circles.\nConceivably, one can potentially segment the input time series into multiple windows and apply the aforementioned rule on each window using their respective σ. The intuition behind segmentation being that the input time series is non-stationary and consequently σ varies over time. The variation in σ for different window lengths is shown in Figure 3.\nFrom Figure 2 we note that applying the rule on a perwindow basis – the time series was segmented into two windows – does not facilitate capturing of the seasonal anomalies.\nThe 3 · σ “rule” assumes that the underlying data distribution is normal. However, our experience has been that the time series data obtained from production is seldom, if at all, normal (see Figure 4 for an example).\nIn light of the aforementioned limitations, we find that the 3 ·σ “rule” is not applicable in the current context. Next, we explored the efficacy of using moving average for anomaly detection."
    }, {
      "heading" : "3.2 Moving Averages",
      "text" : "One of the key aspects associated with anomaly detection is to mitigate the impact of the presence of white noise. To this end, the use of moving averages has been proposed to filter (/smooth) out white noise.\nThe most common moving average is the simple moving average (SMA), which is defined as:\nSMAt = xt + xt−1 + . . .+ xt−(n−1)\nn (12)\nNote that SMA weighs each of the previous n data points equally. This may not desirable given the dynamic nature of the data stream observed in production and from a recency perspective. To this end, the use of the exponentially weighted moving average (EWMA), defined by Equation 13, has been proposed [37].\nEWMAT ≡\n{ yt = xt, t = 1\nyt = α(xt) + (1− α)yt−1, t > 1 (13)\nIn [38], Carter and Streilein argue that in the context of streaming time series data EWMA can potentially be “volatile to abrupt transient changes, losing utility for appropriately detecting anomalies”. To address this, they proposed the Probabilistic Exponentially Weighted Moving Average (PEWMA), wherein the weighting parameter α is adapted by (1−βPt), Pt is the probability of xt evaluated under some modeled distribution, and β is the weight placed on Pt. The SMA, EWMA and PEWMA for the time series of Figure 1 is shown in Figure 5a. From the figure we note that the SMA and EWMA do not fare well with respect to capturing the daily seasonality. Although PEWMA traces the raw time series\nbetter (than SMA and EWMA), it fails to capture both global as well as seasonal anomalies.\nWe evaluated the efficacy of using SMA, EWMA, and PEWMA as an input for anomaly detection. Our experience, using production data, was that the respective moving averages filter out most of the seasonal anomalies and consequently are ill-suited for the current context. This is illustrated by Figure 5b from which we note that application of ESD on PEWMA fails to detect “true” anomalies (annotated on the raw time series with circles).\nFurther, given that a moving average is a lagging indicator, it is not suited for real time anomaly detection.\nArguably, one can use shorter window length, when computing a moving average, to better trace the input time series. However, as shown in [39], the standard error of σ (∝ 1√\nn−1 ), increases as the window length decreases. This\nis illustrated in Figure 3 (window length decreases from right to left)."
    }, {
      "heading" : "3.3 Seasonality and STL",
      "text" : "As discussed in the previous section, Twitter data (obtained from production) exhibits heavy seasonality. Further, in most cases, the underlying distribution exhibits a multimodal distribution, as exemplified by Figure 4. This limits the applicability of existing anomaly detection techniques such as Grubbs and ESD as the existing techniques assume a normal data distribution. Specifically, we learned based on data analysis that the presence of multiple modes yields a higher value of standard deviation (the increase can be as much as up to 5%) which in turn leads to masking of detection of some of the “true” anomalies.\nTo this end, we employ time series decomposition wherein a given time series (X) is decomposed into three – seasonal (SX), trend (TX), and residual (RX) – components. The residual has a unimodal distribution that is amenable to the application of anomaly detection techniques such as ESD. Before moving on, let us first define sub-cycle series [40]:\nDefinition 1. A sub-cycle series comprises of values at each position of a seasonal cycle. For example, if the series is monthly with a yearly periodicity, then the first sub-cycle series is the January values, the second is the February values, and so forth.\nTime series decomposition approaches can be either additive or multiplicative [41], with additive decomposition being appropriate for seasonal data that has a constant magnitude as is the case in the current context. The algorithm first derives TX using a moving average filter, and subsequently subtracts it from the X. Once the trend is removed, SX is then estimated as the average of the data points within the corresponding sub-cycle series. Note that the number of data points per period is an input to the algorithm in order to create the sub-cycle series. In the current context, we set the number of data points per period to be a function of the data granularity. The residual RX is estimated as follows:\nRX = X − TX − SX (14) The residual component computed above can be potentially corrupted by extreme anomalies in the input data.\nThis issue is addressed by STL [40], a robust approach to decomposition that uses LOESS[42] to estimate the seasonal component. The algorithm consists of an inner loop that derives the trend, seasonal, and residual components and an outer loop that increases the robustness of the algorithm with respect to anomalies. Akin to classical seasonal decomposition, the inner loop derives the trend component using a moving average filter, removes the trend from the data and then smoothes the sub-cycle series to derive the seasonal component; however, STL uses LOESS to derive the seasonality, allowing the decomposition to fit more complex functions than either the additive or multiplicative approaches. Additionally, STL iteratively converges on the decomposition, repeating the process several times, or until the difference in iterations is smaller then some specified threshold ( ). Optionally, STL can be made more robust against the influence of anomalies by weighting data points in an outer loop. The outer loop uses the decomposed components to assign a robustness weight to each datum as:\nWeightt = B\n( |RXt |\n6×median|RXt | ) where B is the bisquare function defined as:\nB(u) ≡\n{ (1− u2)2 for u ≤ 0 < 1\n0 for u > 1\nThese weights are then used to converge closer to the “true” decomposed components in the next iteration of the inner loop."
    }, {
      "heading" : "3.4 Seasonal ESD (S-ESD)",
      "text" : "To recap, the applicability of the existing techniques (overviewed in Section 2) is limited by the following:\nz Presence of seasonality in the Twitter time series data (exemplified by Figure 1).\nz Multimodal data distribution of Twitter time series data (illustrated by Figure 4).\nTo this end, we propose a novel algorithm, referred to as Seasonal-ESD (S-ESD), to automatically detect anomalies in Twitter’s production data. The algorithm uses a modified STL decomposition (discussed in subsubsection 3.4.1) to extract the residual component of the input time series and then applies ESD to detect anomalies.\nThis two step process allows S-ESD to detect both global anomalies that extend beyond the expected seasonal minimum and maximum and local anomalies that would otherwise be masked by the seasonality.\nA formal description of the algorithm is presented in Algorithm 1.\nIn the rest of this subsection, we detail the modified STL algorithm, elaborate S-ESD’s ability to detect global and local anomalies, as well as the limitations of the algorithm.\nSection 4 presents the efficacy of S-ESD using production data – both core drivers (or business metrics) and system metrics.\n3.4.1 STL Variant Applying STL decomposition to time series of system metrics yielded, in some cases, spurious anomalies (i.e., anomalies not present in the original time series) in the residual component. For example, let us consider the time series shown in Figure 6a wherein we observe a region of continuous anomalies in the raw data. On applying STL decomposition, we observed a breakout (ala a pulse) in the trend component. On deriving the residual component using Equation 14, we observed an inverse breakout, highlighted with a red rectangle in Figure 6a, which in turn yielded spurious anomalies.\nTo address the above, we use the median of the time series to represent the “stable” trend value which is in turn used to compute the residual component as follows:\nAlgorithm 1 S-ESD Algorithm\nInput:\nX = A time series\nn = number of observations in X\nk = max anomalies (iterations in ESD)\nOutput:\nXA = An anomaly vector wherein each element is a tuple (timestamp, observed value)\nRequire:\nk ≤ (n× .49) 1. Extract seasonal component SX using STL Variant\n2. Compute median X̃\n/* Compute residual */\n3. RX = X − SX − X̃ /* Detect anomalies vector XA using ESD */\n4. XA = ESD(R, k)\nreturn XA\nRX = X − SX − X̃ (15)\nwhere X is the raw time series, SX is the seasonal component as determined by STL, and X̃ is the median of the raw time series. Replacing the trend with the median eliminates the spurious anomalies in the residual component as exemplified by Figure 6b. From the figure we note that the region highlighted with a green rectangle does not have any spurious anomalies, unlike the corresponding region in Figure 6a.\n3.4.2 Global and Local Anomalies Unlike the techniques overviewed in Section 2, S-ESD can detect local anomalies that would otherwise be masked by seasonal data. These local anomalies are bound between the seasonal minimum and maximum and may not not appear to be anomalous from a global perspective. However, they are indeed anomalous and it is important to detect these as they represent a deviation from the historical pattern. For instance, local anomalies found in Twitter data\nand other social network data, may reflect changes in user behavior, or the systems in the data center/cloud. Figure 7 illustrates the use of STL variant to expose both global and local anomalies.\n3.4.3 S-ESD Limitations Although S-ESD can be used for detection of both global and local anomalies, S-ESD does not fare well when applied to data sets that have a high percentage of anomalies. This is exemplified by Figure 8a wherein S-ESD does not capture the anomalies corresponding to the region highlighted by the red rectangle.\nAs discussed earlier in subsection 2.2, a single large value can inflate both the mean and the standard deviation. This makes ESD conservative in tagging anomalies and results in a large number of false negatives. In the following section we detail the application of robust statistics as a further refinement of S-ESD."
    }, {
      "heading" : "3.5 Seasonal Hybrid ESD (S-H-ESD)",
      "text" : "Seasonal Hybrid ESD (S-H-ESD) builds upon the S-ESD algorithm described in the previous subsection. In particular, S-H-ESD uses the robust statistical techniques and metrics discussed in subsection 2.2 to enable a more consistent measure of central tendency of a time series with a high percentage of anomalies. For example, let us consider the time\nseries shown in Figure 8a. From the graph we observe that the seasonal component is apparent in the middle region of the time series; however, a significantly large portion of the time series is anomalous. This can inflate the mean and standard deviation, resulting in true anomalies being mislabeled as not anomalous and consequently yielding a high number of false negatives.\nWe addressed the above by replacing the mean and standard deviation used in ESD with more robust statistical measures during the calculation of the test statistic (refer to Equation 5); in particular, we use the median and MAD, as these metrics exhibit a higher breakdown point (discussed earlier in Section 2.2).\nTable 1 lists the aforementioned metrics for the time series in Figure 8. The anomalies induce a small difference between the mean and median (≈ 1.2%), however the standard deviation is > 2× the median absolute deviation. This results in S-ESD detecting only 1.11% of the data as “anomalous”,\nwhereas S-H-ESD correctly detects 29.68% of the input time series as anomalous – contrast the two graphs shown in Figure 8b.\nNote that using the median and MAD requires sorting the data and consequently the run time of S-H-ESD is higher than that of S-ESD. Therefore, in cases where the time series under consideration is large but with a relatively low anomaly count, it is advisable to use S-ESD. A detailed performance comparison between the two approaches is presented in Section 4.3."
    }, {
      "heading" : "4. EVALUATION",
      "text" : "In this section we outline our methodology for evaluating the proposed techniques, discuss the deployment in production, and last but not the least, present results to demonstrate the efficacy of the proposed techniques."
    }, {
      "heading" : "4.1 Methodology",
      "text" : "The efficacy of S-ESD and S-H-ESD was evaluated using a wide corpus of time series data obtained from production. The time series corresponded to both low-level system metrics and higher-level core drivers (business metrics). For example, but not limited to, the following metrics were used:\nr System Metrics\nz CPU utilization\nz Heap usage\nz Time spent in GC (garbage collection)\nz Disk writes\nr Application Metrics\nz Request rate\nz Latency\nr Core Drivers\nz Tweets per minute (TPM)\nz Retweets per minute (RTPM)\nz Unique Photos Per Minute (UPPM)\nMore than 20 data sets were used for evaluation. The system metrics ranged from two-week long periods to four-week long periods, with hour granularity. The core drivers metrics were all four-week periods, with minute granularity."
    }, {
      "heading" : "4.2 Production",
      "text" : "Increasingly, machine-generated BigData is being used to drive performance and efficiency of data centers/cloud computing platforms. BigData is often characterized by volume and velocity [43, 44]. Given the multitude of services in our service-oriented-architecture (SOA), and the fact that each service monitors a large set of metrics, it is imperative to automatically detect anomalies2 in the time series of each metric. We have deployed the proposed techniques for automatic detection of anomalies in production data for a wide set of services.\nOne can also set a threshold to refine the set of anomalies detected (using the proposed techniques) based on the specific requirements of the service owner. For example, it is\n2A manual approach would be prohibitive from a cost perspective and would also be error-prone.\npossible for capacity engineers to set a threshold such that only anomalies greater than the specified threshold are reported.\nBased on extensive experimentation and analysis, S-HESD with α = 0.05 (95% confidence) was selected for detecting anomalies in the metrics mentioned in the previous subsection. For each metric, S-H-ESD is run over a time series containing the last 14 days worth of data and an e-mail report is sent out if one or more anomalies were detected the previous day. The anomalies and time series are plotted using an in-house data visualization and analytics framework called Chiffchaff. Chiffchaff uses the ggplot2 plotting environment in R to produce graphs similar to the graphs shown in this paper. Additionally, CSVs with the metric, timestamps and magnitude of any anomalies detected is also attached to the email report."
    }, {
      "heading" : "4.3 Efficacy",
      "text" : "In this section we detail the efficacy of S-ESD and S-HESD using the metrics mentioned earlier in this section. In particular, the following – precision, recall, and F-measure – described previously in subsection 2.3, are reported.\n4.3.1 Perspectives The performance of S-ESD and S-H-ESD was investigated from three different perspectives –\n(a) Capacity engineering (CapEng), (b) User behavior (UB) and (c) Supervised learning (Inj), wherein anomalies were injected in the input time series to obtain labeled data.\nIn the first two cases, service owners set a threshold that was then used to categorize the detected anomalies into true positives (TP) and false positives (FP).\nSpecifically,\nr The CapEng perspective was motivated by the fact that in capacity planning, a primary goal is to effectively scale the system resources to handle the normal operating levels of the traffic, e.g., the expected daily maximum, while maintaining enough headroom to absorb anomalies in input traffic.\nThus, the objective in the current context is to detect anomalies that have a magnitude greater than a prespecified threshold (which is in turn determined via load testing).\nr From a user behavior (UB) perspective, the local intraday anomalies serve as potential signals of change in user behavior. To this end, the service owners set the threshold on the residual component of the time series of interest.\nNote that setting a threshold as mentioned above is intrinsically directed toward detection of only positive anomalies. In other words, anomalies in the right tail of the underlying data distribution are detected.\nr Lastly, the Inj perspective is aimed to assess the efficacy of the proposed techniques in the presence of ground-truth (or labeled data). Given the volume, velocity, and real-time nature of production data, it is not practically feasible to obtain labeled anomalies. To alleviate this limitation, we first fit a smooth spline (B-spline) curve to a time series (obtained from production) to derive a time series which had the same characteristics – trend and seasonality – as of the original time series. Subsequently, positive anomalies were\ninjected in the derived time series, with varying magnitudes, widths, and frequency. The positions and magnitudes of the injected anomalies were recorded and used to compare against the anomalies detected by both S-ESD and S-H-ESD.\n4.3.2 Results We computed Precision, Recall, and F-measure (refer to Section 2.3) to assess the efficacy of S-ESD and S-H-ESD from the three perspectives.\nRight-tailed (/positive) anomalies were detected at both 95% and 99.9% confidence levels. The metrics are reported in Tables 2 (CapEng), 3 (UB), and 4 (Inj).\nFrom the tables we note the following: across both system metrics and core drivers, precision increases from about 75% in the CapEng perspective to 100% in the UB perspective for S-ESD, and about 59% to 95% for S-H-ESD. The threshold set in the CapEng perspective results in labeling of the intra-day or off-peak anomalies as false positives, thereby lowering the precision. In contrast, in the UB perspective, the threshold is set for the residual component which removes the seasonality effect. This results in S-H-ESD’s higher recall rates, which improve from 47.5%\n(S-ESD) to 77% (S-H-ESD) for CapEng and from 31.5% (S-ESD) to 65% (S-H-ESD) for UB.\nComparative analysis of the F-measure reported in Tables 2 and 3 highlights that the F-measure is better in the latter case. This can be attributed, in part, to the fact that in case of the latter the service-owners set the thresholds over the residual component (recall that the time series decomposition removes the trend and seasonal component). This makes anomaly detection independent of the time at which they occurred, e.g., on-peak on a daily max, or off-peak on a daily trough. The low values of F-measure in the CapEng perspective are reasoned at the end of this section. Table 4 reports the efficacy of S-ESD and S-H-ESD from the Inj (Ground-Truth) perspective. From the table we note that the precision achieved was 100%, meaning that all anomalies detected were true anomalies (there were no false positives). Also, the recall was very high, achieving about 96% and 97% (for S-ESD and S-H-ESD respectively) at the 95% confidence level and about 94% and 95% recall at the 99.9% confidence level.\nOn further analysis we noted that false negatives (anomalies that were not detected) were within the boundaries of\nthe anomalies detected, normally at the tail ends (at the beginning or end of the sustained anomalous behavior).\nLastly, the results reported Table 4 correspond to injection sets containing anomalies with increasing magnitudes from 0.75σ to 6σ. At 0.75σ, the F-measure of S-ESD and S-HESD begins to degrade (0.84 (S-ESD) and 0.88 (S-H-ESD) at the 95% confidence level and 0.76 (S-ESD) 0.82 (S-HESD) at the 99.9% confidence level). At 1.5σ, the F-measure was about 0.97 on an average. Injected anomalies with a magnitude of 3σ or greater achieved an F-measure of 1.00.\nFigure 9 summarizes the overall F-measure average for the three perspectives at the 95% confidence level. From the figure we note that in each case S-H-ESD outperformed S-ESD; in particular, the F-measure increased by 17.5%, 29.5% and 0.62% for CapEng, UB, and Inj respectively. This stems from the fact that median and MAD, unlike mean and standard deviation, are robust against a large number of outliers.\nThe F-measure for the CapEng and UB perspectives is\nsignificantly lower than the Inj perspective. This can be ascribed to the following: Capacity planning engineers typically determine the capacity needed to withstand the typical daily peaks (with additional headroom), thus the serviceowners tend to set the threshold around the maximum daily peaks. Consequently, anomalies which occur during the off-peak hours of the day, such as in the daily troughs or other intra-day locations, would be marked as false positives (even though they might still be anomalous from S-ESD/SH-ESD’s point of view). This is illustrated in Figure 10 wherein the anomalies (detected using S-H-ESD) above the pre-specified threshold are annotated by # and the rest are annotated by 4. The latter, albeit “true” anomalies from a statistical standpoint, are tagged as false positives which adversely impact precision (refer to Equation 9)."
    }, {
      "heading" : "5. PREVIOUS WORK",
      "text" : "In this section, we overview prior work in the context of anomaly detection. A lot of anomaly detection research has been done in various domains such as, but not limited to, statistics, signal processing, finance, econometrics, manufacturing, and networking. For a detailed coverage of the same, the reader is referred to books and survey papers [16, 17, 18, 19].\nFor better readability, we have partitioned this section into subsections on a per domain basis. As mentioned in a recent survey on anomaly detection [20], anomaly detection is highly contextual in nature. Based on our in-depth literature survey we find that the techniques discussed in the rest of this section cater to different type of data sets (than cloud infrastructure data) and hence are complementary to the techniques proposed in this paper.\nManufacturing Anomaly detection manifests itself in manufacturing in the form of determining if a particular process is in a state of normal and stable behavior. To this end, Statistical Process Control (SPC) was proposed in the early 1920s to monitor and control the reliability of a manufacturing process [45, 46]. Control charts are one of the key tools used in SPC.\nIn essence, the premise of SPC is that a certain amount of variation can occur at any one point of a production chain. The variation is “common” if it is controlled and within normal or expected limits. However, the variation is “assignable” if it is not present in the causal system of the process at all times (i.e., falls outside of the normal limits). Identifying and removing the assignable sources which have impact on the manufacturing process is thus crucial to ensure the expected operation and quality of the manufacturing process.\nA traditional control chart includes points representing\na statistical measurement (such as the mean) of a quality characteristic in samples taken over a period of time. The mean of this characteristic is calculated over all samples, and plotted as the center line. The standard deviation is calculated over all samples, and the upper control limit (UCL) and lower control limit (LCL) defined as the threshold at which the process is considered statistically unlikely to occur (typically set at 3 standard deviations, denoted by 3σ, about the mean/center line). When the process is “in control”, 99.73% of all points are within the upper and lower control limits. A signal may be generated when observations fall outside of these control limits, signifying the introduction of some other source of variation outside of the normal expected behavior.\nMethodologies for improving the performance of control charts have been investigated since Shewhart’s early work. Roberts proposed the geometrical moving average control chart – also known as the EWMA (exponentially weighted moving average) control chart – which weights the most recent samples more highly than older samples [47]. The EWMA chart tends to detect small shifts (1-2 σ) in the sample mean more efficiently; however, the Shewhart chart tends to detect larger shift (3 σ) more efficiently. In case the quality characteristic follows a Poisson distribution, alternatives to the EWMA chart have been proposed [48]. Other types of control charts, such as the CUSUM (cumulative sum) chart [49], have been proposed. In [50], Lowry and Montgomery present a review on control charts for multivariate quality control. For further details on SPC and control charts, the reader is referred to the surveys by Woodall and Montgomery [51] and by Stoumbos et al. [52]. Lastly, a recent survey by Tsung et al. presents a comprehensive survey of statistical control methods for multistage manufacturing and service operations [53].\nFinance Behavioral economics and finance study the causal relationships between social, cognitive, and emotional factors on the economic decisions of individuals and institutions, and their effect on the economic decisions such as market prices and returns.\nThe interplay of the aforementioned factors often result in, but not limited to, seasonal stock market anomalies. These seasonal anomalies, also referred to as “calendar effects”, take many forms. Haugen and Jorion describe the January Effect – stocks, especially small stocks, historically generate abnormally high returns during the month of January – as “... perhaps the best-known example of anomalous behavior in security markets throughout the world” [54]. This effect is normally attributed to a theory which states that the investors who hold a disproportionate amount of small stocks sell their stocks for tax reasons at the end of the year (to claim capital loss), and then reinvest at the start of the new year. On the other hand, the January effect is also attributed to the end of year bonuses which are paid in January and used to purchase stocks (thus driving up prices).\nIn [55], Angelovska reports that early studies on stock market anomalies began in the late 1920’s, where Kelly’s reports [56] showed the existence of a so-called “Monday effect” – US markets have low and negative Monday returns. The Monday effect in the US sock market was actively researched in the 1980s [57] [58] [59]. In [60], Coutts and Hayes showed that the Monday effect exists albeit not as strongly as previous work demonstrated. Wang et al. show that between 1962 and 1993, the Monday effect is strongly evident\nin the last two weeks of the month, for a wide array of stock indices. For a full literature review on the Monday effect, refer to [61].\nNumerous studies support the existence of calendar effects in stock markets, as well as others such as “turn of the month” effects. For instance, Hensel and Ziemba examined S&P 500 returns over a 65 year period from 1928 to 1993, and reported that U.S. large-cap stocks consistently show higher returns at the turn of the month [62]. In contrast, Sullivan et al. argue against the case, stating that there is no statistically significant evidence supporting the claim, and that such periodicities in stock market behavior are the result of data dredging [63]. Subsequently, Hansen et al. attempted to use sound statistical approaches to evaluate the significance of calendar effects, by tightly controlling the testing to avoid data mining biases [64]. In their study of 27 stock indices from 10 countries, calendar effects were found to be significant in most returns series, with the end-of-the-year effect producing the largest anomalies and the most convincing evidence supporting calendar effects in small-cap indices.\nSignal Processing Techniques from signal processing such as, but not limited to, spectral analysis, have been adopted for anomaly detection. For instance, in [65], Cheng et al. employed spectral analysis to complement existing DoS defense mechanisms that focus on identifying attack traffic, by ruling out normal TCP traffic, thereby reducing false positives.\nSimilarly, wavelet packets and wavelet decomposition have been used for detecting anomalies in network traffic [66, 67, 68, 69, 70]. Benefits of wavelet-based techniques include the ability to accurately detect anomalies at various frequencies (due to the inherent time-frequency property of decomposing signals into different components at several frequencies), with relatively fast computation.\nIn [71], Gao et al. proposed a speed optimization for realtime use using sliding windows. Recently, Lu et al. proposed an approach consisting of three components: (1) feature analysis, (2) normal daily traffic modeling based on wavelet approximation and ARX (AutoRegressive with eXogenous), and intrusion decision [72]. An overview of signal processing techniques for network anomaly detection, including PSD (Power Spectral Density) and wavelet-based approaches, is presented in [73].\nAdditionally, Kalman filtering and Principle Component Analysis (PCA) based approaches have been proposed in the signal processing domain for anomaly detection. In [74], Ndong and Salamatian reported that PCA-based approaches exhibit improved performance when coupled with Karuhen-Loeve expansion (KL); on the other hand, Kalman filtering approaches, when combined with statistical methods such as Gaussian mixture and Hidden Markov models, outperformed the PCA-KL method.\nNetwork Traffic With the Internet of Things (IoT) paradigm [75, 76] increasingly becoming ubiquitous3, there is an increasing concern about security. In a post the FTC said on its website [78]: “At the same time, the data collection and sharing that smart devices and greater connectivity enable pose privacy\n3According to a study by ABI Research [77], it is estimated that 10 billion devices are currently connected to one another by wired or wireless Internet. By the year 2020, that number is expected to exceed 30 billion.\nand security risks. Over the years, various anomaly detection techniques have been proposed for detection network intrusion.\nFor example, in [79], Denning proposed a rule-based technique wherein both network (system) and user data was used to detect different types of abnormal behavior, by comparing audit-trails to different anomalous profiles or models.\nIn [80], Lazarevic et al. presented a comparative study of several anomaly detection schemes for network intrusion detection.\nIn [81], Garcia-Teodoro et al. presented an overview of the pros and cons of various approaches for anomaly detection in network intrusion systems such as statistical techniques, knowledge-based techniques (finite state machines, Bayesian networks, expert systems, etc.), and learning based classification of patterns (Markov models, neural networks, fuzzy logic, clustering, et cetera). Recently, Gogoi et al. presented a comprehensive survey on outlier detection for network anomaly detection in [82]; in particular, the authors classified the approaches into three categories: (1) distancebased, (2) density-based, and (3) machine learning or softcomputing based.\nThe reader is referred to the survey of intrusion detection techniques by Yu for further reading [83].\nStatistics Anomaly detection has been actively researched for over five decades in the domain of statistics [84, 85, 86, 16, 87]. Recent surveys include the ones from Hodge and Jim [18] and Chandola et al. [88].\nThe key focus of prior work has been to determine whether a single value is statistically anomalous with respect to an underlying distribution. Work by Markov and Chebyshev provided bounds on the probability of a random value with respect to the expected value of the distribution. The Markov inequality states that for any non-negative random variable X, the following holds true P (X > α) ≤ E[X]/α, while the more general Chebyshev inequality states that P (|X − E[X]| > α) ≤ V ar[X]/α2, and shows that values equal to or greater then K standard deviations from the expected value constitute no more then 1/k2 of the total distribution.\nThese bounds can be used as a threshold for determining the “outlierness” of a random value, indicating that a value does not fit the underlying distribution [17]; however, the Markov and Chebyshev inequalities are non-parametric, and create relatively weak bounds that may miss potential outliers in the data [19]. The Chernoff bound and the Hoeffding inequality attempt to create tighter bounds by making assumptions about the underlying distribution. While these tail inequalities provided a closer bounds for testing the outlierness of a data point, their assumptions regarding the distribution make them unsuitable for use when the distribution doesn’t follow their underlying assumptions (as in the current context).\nFurther, the Box plot may be applied as a robust means of determining if a data point is anomalous with respect to the underlying distribution [89]. A Box plot divides the data into five groups: the minimum non-anomalous value (min), the lower quartile (Q1), the median, the upper quartile (Q3) and the maximum non-anomalous value (max). Data that is 1.5× lower then (Q1) or 1.5× greater then (Q3) are typically considered anomalous."
    }, {
      "heading" : "6. CONCLUSION",
      "text" : "In this paper we presented two novel statistical techniques for automatically detecting anomalies in cloud infrastructure data. Although there exists a large body of research in anomaly detection, the seasonal (and trending) nature of cloud infrastructure data limits the application of techniques. To this end, we proposed a method called SeasonalESD (S-ESD), which combines seasonal decomposition and the Generalized ESD test, for anomaly detection. The second method, Seasonal-Hybrid-ESD (S-H-ESD), builds on SESD to enable robust anomaly detection when a significant portion (up to 50%) of the underlying data is anomalous. This is achieved by extending the original ESD algorithm with robust statistical measures, median and median absolute deviation (MAD).\nThe efficacy of both S-ESD and S-H-ESD was evaluated using both core metrics such as Tweets Per Sec (TPS), system metrics such as CPU and heap usage and application metrics. The evaluation was carried out from three different perspectives, viz., capacity engineering (CapEng), user behavior (UB), and supervised learning (Inj). Precision, Recall, and F-measure in each case. Overall, S-H-ESD outperformed S-ESD, with F-Measure increasing by 17.5%, 29.5% and 0.62% for CapEng, UB, and Inj respectively.\nIn light of the fact that S-H-ESD more computationally expensive than S-ESD (recall that the former requires sorting of the data), it is recommended to use S-ESD in cases where the time series under consideration is large but with a relatively low anomaly count.\nAs future work, we plan to extend the proposed techniques for detecting anomalies in long time series. The challenge in this regard is that capturing the underlying trend,4 which in our observation is predominant in the case of long time series, is non-trivial in the presenece of anomalies. To this end, we plan to explore the use of qunatile regression [90] and/or robust regression [91, 92]."
    }, {
      "heading" : "7. REFERENCES",
      "text" : "[1] Federal Government Big Data Rollout.\nhttp://www.nsf.gov/news/news_videos.jsp?cntn_id=123607&media_id=72174&org=NSF, 2012.\n[2] J. Manyika, M. Chui, B. Brown, J. Bughin, R. Dobbs, C. Roxburgh, and A. H. Byers. Big data: The next frontier for innovation, competition, and productivity. http://www.mckinsey.com/Insights/MGI/Research/Technology_and_ Innovation/Big_data_The_next_frontier_for_innovation, May 2011. [3] Big Data Market Size and Vendor Revenues. http://wikibon.org/wiki/v/Big_Data_Market_Size_and_Vendor_Revenues. [4] New Digital Universe Study Reveals Big Data Gap: Less Than 1% of World’s Data is Analyzed; Less Than 20% is Protected. http://www.emc.com/about/news/press/2012/20121211-01.htm. [5] Web Startups Crumble under Amazon S3 Outage. http://www.theregister.co.uk/2008/02/15/amazon_s3_outage_feb_20%08/. [6] How Much is the Reputation of Your SaaS Provider Worth? http://cloudsecurity.org/2009/03/13/ how-much-is-the-reputation-of-your-saas-provider-worth. [7] S. Agarwala and K. Schwan. Sysprof: Online distributed behavior diagnosis through fine-grain system monitoring. In Proceedings of the 26th IEEE International Conference on Distributed Computing Systems, pages 8–8, 2006. [8] G. Ren, E. Tune, T. Moseley, Y. Shi, S. Rus, and R. Hundt. Google-wide profiling: A continuous profiling infrastructure for data centers. IEEE Micro, 30(4):65–79, July 2010. [9] Mike Y. Chen, Emre Kiciman, Eugene Fratkin, Armando Fox, and Eric Brewer. Pinpoint: Problem determination in large, dynamic internet services. In Proceedings of the 2002 International Conference on Dependable Systems and Networks, pages 595–604, 2002. [10] Anton Babenko, Leonardo Mariani, and Fabrizio Pastore. Ava: automated interpretation of dynamically detected anomalies. In Proceedings of the eighteenth international symposium on Software testing and analysis, pages 237–248, 2009. [11] J. P. Magalh aes and Luis Moura Silva. Root-cause analysis of performance anomalies in web-based applications. In Proceedings of the 2011 ACM Symposium on Applied Computing, pages 209–216, TaiChung, Taiwan, 2011. [12] Hui Kang, Xiaoyun Zhu, and Jennifer L. Wong. Dapa: diagnosing application performance anomalies for virtualized infrastructures. In Proceedings of the 2nd USENIX conference on Hot Topics in Management of Internet, Cloud, and Enterprise Networks and Services, pages 8–8, 2012. [13] Mona Attariyan, Michael Chow, and Jason Flinn. X-ray: automating root-cause diagnosis of performance anomalies in production software. In Proceedings of the 10th USENIX conference on Operating Systems Design and Implementation, pages 307–320, Hollywood, CA, 2012. [14] C. Wang, V. Talwar, K. Schwan, and P. Ranganathan. Online detection of utility cloud anomalies using metric distributions. In Network Operations and\n4Capturing the trend is required to minimize the number of false positives.\nManagement Symposium (NOMS), 2010 IEEE, pages 96–103, 2010.\n[15] C. Wang, Krishnamurthy Viswanathan, Choudur Lakshminarayan, Vanish Talwar, Wade Satterfield, and Karsten Schwan. Statistical techniques for online anomaly detection in data centers. In Proceedings of Integrated Network Management, pages 385–392, 2011. [16] Douglas M. Hawkins. Identification of outliers, volume 11. Chapman and Hall London, 1980. [17] Vic Barnett and Toby Lewis. Outliers in statistical data, volume 3. Wiley New York, 1994. [18] Victoria J. Hodge and Jim Austin. A survey of outlier detection methodologies. Artificial Intelligence Review, 22(2):85–126, 2004. [19] Charu C. Aggarwal. Outlier analysis. Springer, 2013. [20] Varun Chandola, Arindam Banerjee, and Vipin Kumar. Anomaly detection: A survey. ACM Computing Surveys, 41(3):15:1–15:58, July 2009. [21] Bernard Rosner. On the detection of many outliers. Technometrics, 17(2):221–227, 1975. [22] Bernard Rosner. Percentage points for a generalized ESD many-outlier procedure. Technometrics, 25(2):165–172, 1983. [23] Christophe Leys, Christophe Ley, Olivier Klein, Philippe Bernard, and Laurent Licata. Detecting outliers: Do not use standard deviation around the mean, use absolute deviation around the median. Journal of Experimental Social Psychology, 2013. [24] George W Snedecor and William G Cochran. Statistical methods. Iowa State University Press, Ames, 1989. [25] Peter J Huber and Elvezio Ronchetti. Robust statistics. Wiley, Hoboken, N.J., 1981. [26] Frank R. Hampel. The influence curve and its role in robust estimation. Journal of the American Statistical Association, 69(346):383–393, 1974. [27] R.A. Fisher. Statistical methods for research workers. Edinburgh Oliver & Boyd, 1925. [28] F. E. Grubbs. Sample criteria for testing outlying observations. Ann. Math. Statistics, 21:27–58, 1950. [29] Frank E. Grubbs. Procedures for detecting outlying observations in samples. Technometrics, 11(1):1–21, 1969. [30] Francisco Augusto Alcaraz Garcia. Tests to identify outliers in data series. Pontifical Catholic University of Rio de Janeiro, Industrial Engineering Department, Rio de Janeiro, Brazil, 2012. [31] Gary L. Tietjen and Roger H. Moore. Some grubbs-type statistics for the detection of several outliers. Technometrics, 14(3):583–597, 1972. [32] Frank R Hampel, Elvezio Ronchetti, Peter J. Rousseeuw, and Werner A. Stahel. Robust statistics: the approach based on influence functions. Wiley, New York, 1986. [33] Frank Rudolf Hampel. Contributions to the theory of robust estimation. University of California, 1968. [34] David L. Donoho and Peter J. Huber. The notion of breakdown point. A Festschrift for Erich L. Lehmann, pages 157–184, 1983. [35] Peter J. Rousseeuw and Christophe Croux. Alternatives to the median absolute deviation. Journal of the American Statistical Association, 88(424):1273–1283, 1993. [36] Yutaka Sasaki. The truth of the f-measure. Teach Tutor mater, pages 1–5, 2007. [37] James M. Lucas and Michael S. Saccucci. Exponentially weighted moving average control schemes: properties and enhancements. Technometrics, 32(1):1–12, 1990. [38] Kevin M. Carter and William W. Streilein. Probabilistic reasoning for streaming anomaly detection. In Statistical Signal Processing Workshop (SSP), 2012 IEEE, pages 377–380, 2012. [39] S. Ahn and J. A. Fessler. Standard Errors of Mean, Variance, and Standard Deviation Estimators. http://web.eecs.umich.edu/~fessler/papers/files/tr/stderr.pdf, 2003. [40] Robert B. Cleveland, William S. Cleveland, Jean E. McRae, and Irma Terpenning. STL: a seasonal-trend decomposition procedure based on loess. Journal of Official Statistics, 6(1):3–73, 1990. [41] A. Stuart, M. Kendall, and J. Keith Ord. The advanced theory of statistics. Vol. 3: Design and analysis and time-series. Griffin, 1983. [42] William S. Cleveland. Robust locally weighted regression and smoothing scatterplots. Journal of the American statistical association, 74(368):829–836, 1979. [43] Volume, Velocity, Variety: What You Need to Know About Big Data. http://www.forbes.com/sites/oreillymedia/2012/01/19/ volume-velocity-variety-what-you-need-to-know-about-big-data/, 2012. [44] The Four V’s of Big Data. http://dashburst.com/infographic/big-data-volume-variety-velocity/, 2012. [45] Walter A. Shewhart. Quality control charts. Bell System Technical Journal, 5(4):593–603, 1926. [46] W. A. Shewart. Economic control of Quality of Manufactured Product. Van Nostrand Reinhold Co., 1931. [47] S. W. Roberts. Control chart tests based on geometric moving averages. Technometrics, 1(3):239–250, 1959. [48] Douglas C. Montgomery. Introduction to statistical quality control. Wiley. com, 2007. [49] E. S. Page. Continuous inspection schemes. Biometrika, 41(1/2):100–115, 1954. [50] Cynthia A. Lowry and Douglas C. Montgomery. A review of multivariate control charts. IIE transactions, 27(6):800–810, 1995. [51] William H. Woodall and Douglas C. Montgomery. Research issues and ideas in statistical process control. Journal of Quality Technology, 31(4), 1999. [52] Zachary G. Stoumbos, Marion R. Reynolds Jr, Thomas P. Ryan, and William H. Woodall. The state of statistical process control as we proceed into the 21st century. Journal of the American Statistical Association, 95(451):992–998, 2000. [53] Fugee Tsung, Yanting Li, and Ming Jin. Statistical process control for multistage manufacturing and service operations: a review and some extensions. International Journal of Services Operations and Informatics, 3(2):191–204, 2008. [54] Robert A. Haugen and Philippe Jorion. The january effect: still there after all these years. Financial Analysts Journal, pages 27–31, 1996. [55] Julijana Angelovska. An econometric analysis of market anomaly-day of the week effect on a small emerging market. International Journal of Academic Research in Accounting, Finance and Management Sciences, 3(1):314–322, 2013. [56] Fred C. Kelly. Why you win or lose: The psychology of speculation. Courier Dover Publications, 1930. [57] Kenneth R. French. Stock returns and the weekend effect. Journal of financial economics, 8(1):55–69, 1980. [58] Michael R. Gibbons and Patrick Hess. Day of the week effects and asset returns. Journal of business, pages 579–596, 1981. [59] Richard J. Rogalski. New findings regarding day-of-the-week returns over trading and non-trading periods: A note. The Journal of Finance, 39(5):1603–1614, 1984. [60] J. Andrew Coutts and Peter A. Hayes. The weekend effect, the stock exchange account and the financial times industrial ordinary shares index: 1987-1994. Applied Financial Economics, 9(1):67–71, 1999. [61] Glenn N. Pettengill. A survey of the monday effect literature. Quarterly Journal of Business and Economics, pages 3–27, 2003. [62] Chris R. Hensel and William T. Ziemba. Investment results from exploiting turn-of-the-month effects. The Journal of Portfolio Management,\n22(3):17–23, 1996.\n[63] Ryan Sullivan, Allan Timmermann, and Halbert White. Dangers of data mining: The case of calendar effects in stock returns. Journal of Econometrics, 105(1):249–286, 2001. [64] Peter Hansen, Asger Lunde, and James Nason. Testing the significance of calendar effects. Federal Reserve Bank of Atlanta Working Paper, (2005-02), 2005. [65] Chen-Mou Cheng, H. T. Kung, and Koan-Sin Tan. Use of spectral analysis in defense against DoS attacks. In Global Telecommunications Conference, 2002. GLOBECOM’02. IEEE, volume 3, pages 2143–2148, 2002. [66] Paul Barford, Jeffery Kline, David Plonka, and Amos Ron. A signal analysis of network traffic anomalies. In Proceedings of the 2nd ACM SIGCOMM Workshop on Internet measurment, pages 71–82, 2002. [67] Vicente Alarcon-Aquino and Javier A. Barria. Anomaly detection in communication networks using wavelets. IEE Proceedings-Communications, 148(6):355–362, 2001. [68] Lan Li and Gyungho Lee. DDoS attack detection and wavelets. Telecommunication Systems, 28(3-4):435–451, 2005. [69] Seong Soo Kim, AL Narasimha Reddy, and Marina Vannucci. Detecting traffic anomalies through aggregate analysis of packet header data. In NETWORKING 2004. Networking Technologies, Services, and Protocols; Performance of Computer and Communication Networks; Mobile and Wireless Communications, pages 1047–1059. Springer, 2004. [70] Anu Ramanathan. WADeS: A tool for distributed denial of service attack detection. PhD thesis, Texas A&M University, 2002. [71] Jun Gao, Guangmin Hu, Xingmiao Yao, and Rocky KC Chang. Anomaly detection of network traffic based on wavelet packet. In Communications, 2006. APCC’06. Asia-Pacific Conference on, pages 1–5, 2006. [72] Wei Lu and Ali A. Ghorbani. Network anomaly detection based on wavelet analysis. EURASIP Journal on Advances in Signal Processing, 2009:4, 2009. [73] Lingsong Zhang. Signal processing methods for network anomaly detection. 2005. [74] Joseph Ndong and Kavé Salamatian. Signal processing-based anomaly detection techniques: A comparative analysis. In INTERNET 2011, The Third International Conference on Evolving Internet, pages 32–39, 2011. [75] Disruptive Civil Technologies Six Technologies with Potential Impacts on US Interests out to 2025 . http://www.fas.org/irp/nic/disruptive.pdf, 2008. [76] Luigi Atzori, Antonio Iera, and Giacomo Morabito. The internet of things: A survey. Computer Networks, 54(15):2787–2805, October 2010. [77] Internet of Everything. https://www.abiresearch.com/research/service/internet-of-everything. [78] FTC Seeks Input on Privacy and Security Implications of the Internet of Things. http://www.ftc.gov/opa/2013/04/internetthings.shtm. [79] Dorothy E. Denning. An intrusion-detection model. Software Engineering, IEEE Transactions on, (2):222–232, 1987. [80] Aleksandar Lazarevic, Levent Ertoz, Vipin Kumar, Aysel Ozgur, and Jaideep Srivastava. A comparative study of anomaly detection schemes in network intrusion detection. Proc. SIAM, 2003. [81] Pedro Garcia-Teodoro, J. Diaz-Verdejo, Gabriel Maciá-Fernández, and Enrique Vázquez. Anomaly-based network intrusion detection: Techniques, systems and challenges. computers & security, 28(1):18–28, 2009. [82] Prasanta Gogoi, D. K. Bhattacharyya, Bhogeswar Borah, and Jugal K. Kalita. A survey of outlier detection methods in network anomaly identification. The Computer Journal, 54(4):570–588, 2011. [83] Yingbing Yu. A survey of anomaly intrusion detection techniques. J. Comput. Sci. Coll., 28(1):9–17, October 2012. [84] F. J. Anscombe and Irwin Guttman. Rejection of outliers. Biometrika, 2:123–147, 1960. [85] D. Bernoulli. The most probable choice between several discrepant observations and the formation therefrom of the most likely induction. Biometrika, 48:3–18, 1961. [86] Vic Barnett. The study of outliers: purpose and model. Applied Statistics, pages 242–250, 1978. [87] R. J. Beckman and R. D. Cook. Outlier. . . . . . . . .ṡ. Technometrics, 25(2):119–149, 1983. [88] Varun Chandola, Arindam Banerjee, and Vipin Kumar. Anomaly detection: A survey. ACM Computing Surveys (CSUR), 41(3):15, 2009. [89] Jorma Laurikkala, Martti Juhola, Erna Kentala, N. Lavrac, S. Miksch, and B. Kavsek. Informal identification of outliers in medical data. In Proceedings of the 5th International Workshop on Intelligent Data Analysis in Medicine and Pharmacology, pages 20–24, 2000. [90] R. Koenker and G. Bassett Jr. Regression quantiles. Econometrica, 46:33—50, 1978. [91] P. J. Huber. Robust regression: Asymptotics, conjectures and monte carlo. Annals of Statistics, 1:799–821, 1973. [92] P. J. Rousseeuw and A. M. Leroy. Robust Regression and Outlier Detection. 2003."
    } ],
    "references" : [ {
      "title" : "Big data: The next frontier for innovation, competition, and productivity. http://www.mckinsey.com/Insights/MGI/Research/Technology_and_ Innovation/Big_data_The_next_frontier_for_innovation",
      "author" : [ "J. Manyika", "M. Chui", "B. Brown", "J. Bughin", "R. Dobbs", "C. Roxburgh", "A.H. Byers" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2011
    }, {
      "title" : "Sysprof: Online distributed behavior diagnosis through fine-grain system monitoring",
      "author" : [ "S. Agarwala", "K. Schwan" ],
      "venue" : "Proceedings of the 26th IEEE International Conference on Distributed Computing Systems, pages 8–8",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Google-wide profiling: A continuous profiling infrastructure for data centers",
      "author" : [ "G. Ren", "E. Tune", "T. Moseley", "Y. Shi", "S. Rus", "R. Hundt" ],
      "venue" : "IEEE Micro,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2010
    }, {
      "title" : "Pinpoint: Problem determination in large, dynamic internet services",
      "author" : [ "Mike Y. Chen", "Emre Kiciman", "Eugene Fratkin", "Armando Fox", "Eric Brewer" ],
      "venue" : "In Proceedings of the 2002 International Conference on Dependable Systems and Networks,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2002
    }, {
      "title" : "Ava: automated interpretation of dynamically detected anomalies",
      "author" : [ "Anton Babenko", "Leonardo Mariani", "Fabrizio Pastore" ],
      "venue" : "In Proceedings of the eighteenth international symposium on Software testing and analysis,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2009
    }, {
      "title" : "Root-cause analysis of performance anomalies in web-based applications",
      "author" : [ "J.P. Magalh aes", "Luis Moura Silva" ],
      "venue" : "In Proceedings of the 2011 ACM Symposium on Applied Computing,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2011
    }, {
      "title" : "Dapa: diagnosing application performance anomalies for virtualized infrastructures",
      "author" : [ "Hui Kang", "Xiaoyun Zhu", "Jennifer L. Wong" ],
      "venue" : "In Proceedings of the 2nd USENIX conference on Hot Topics in Management of Internet, Cloud, and Enterprise Networks and Services,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2012
    }, {
      "title" : "X-ray: automating root-cause diagnosis of performance anomalies in production software",
      "author" : [ "Mona Attariyan", "Michael Chow", "Jason Flinn" ],
      "venue" : "In Proceedings of the 10th USENIX conference on Operating Systems Design and Implementation,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2012
    }, {
      "title" : "Online detection of utility cloud anomalies using metric distributions",
      "author" : [ "C. Wang", "V. Talwar", "K. Schwan", "P. Ranganathan" ],
      "venue" : "Network Operations and Capturing the trend is required to minimize the number of false positives.  https://github.com/twitter/AnomalyDetection/ Management Symposium (NOMS), 2010 IEEE, pages 96–103",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Krishnamurthy Viswanathan",
      "author" : [ "C. Wang" ],
      "venue" : "Choudur Lakshminarayan, Vanish Talwar, Wade Satterfield, and Karsten Schwan. Statistical techniques for online anomaly detection in data centers. In Proceedings of Integrated Network Management, pages 385–392",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Identification of outliers, volume 11",
      "author" : [ "Douglas M. Hawkins" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1980
    }, {
      "title" : "Outliers in statistical data, volume",
      "author" : [ "Vic Barnett", "Toby Lewis" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1994
    }, {
      "title" : "A survey of outlier detection methodologies",
      "author" : [ "Victoria J. Hodge", "Jim Austin" ],
      "venue" : "Artificial Intelligence Review,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2004
    }, {
      "title" : "Anomaly detection: A survey",
      "author" : [ "Varun Chandola", "Arindam Banerjee", "Vipin Kumar" ],
      "venue" : "ACM Computing Surveys,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2009
    }, {
      "title" : "On the detection",
      "author" : [ "Bernard Rosner" ],
      "venue" : "of many outliers. Technometrics,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1975
    }, {
      "title" : "Percentage points for a generalized ESD many-outlier",
      "author" : [ "Bernard Rosner" ],
      "venue" : "procedure. Technometrics,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1983
    }, {
      "title" : "Detecting outliers: Do not use standard deviation around the mean, use absolute deviation around the median",
      "author" : [ "Christophe Leys", "Christophe Ley", "Olivier Klein", "Philippe Bernard", "Laurent Licata" ],
      "venue" : "Journal of Experimental Social Psychology,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2013
    }, {
      "title" : "Statistical methods",
      "author" : [ "George W Snedecor", "William G Cochran" ],
      "venue" : null,
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 1989
    }, {
      "title" : "Robust statistics",
      "author" : [ "Peter J Huber", "Elvezio Ronchetti" ],
      "venue" : null,
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 1981
    }, {
      "title" : "The influence curve and its role in robust estimation",
      "author" : [ "Frank R. Hampel" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 1974
    }, {
      "title" : "Statistical methods for research workers",
      "author" : [ "R.A. Fisher" ],
      "venue" : "Edinburgh Oliver & Boyd",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 1925
    }, {
      "title" : "Sample criteria for testing outlying observations",
      "author" : [ "F.E. Grubbs" ],
      "venue" : "Ann. Math. Statistics, 21:27–58",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 1950
    }, {
      "title" : "Procedures for detecting outlying observations in samples",
      "author" : [ "Frank E. Grubbs" ],
      "venue" : "Technometrics, 11(1):1–21,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 1969
    }, {
      "title" : "Tests to identify outliers in data series",
      "author" : [ "Francisco Augusto Alcaraz Garcia" ],
      "venue" : "Pontifical Catholic University of Rio de Janeiro, Industrial Engineering Department, Rio de Janeiro,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2012
    }, {
      "title" : "Some grubbs-type statistics for the detection",
      "author" : [ "Gary L. Tietjen", "Roger H. Moore" ],
      "venue" : "of several outliers. Technometrics,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 1972
    }, {
      "title" : "Robust statistics: the approach based on influence functions",
      "author" : [ "Frank R Hampel", "Elvezio Ronchetti", "Peter J. Rousseeuw", "Werner A. Stahel" ],
      "venue" : null,
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 1986
    }, {
      "title" : "Contributions to the theory of robust estimation",
      "author" : [ "Frank Rudolf Hampel" ],
      "venue" : "University of California,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 1968
    }, {
      "title" : "The notion of breakdown point. A Festschrift for Erich L",
      "author" : [ "David L. Donoho", "Peter J. Huber" ],
      "venue" : null,
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 1983
    }, {
      "title" : "Alternatives to the median absolute deviation",
      "author" : [ "Peter J. Rousseeuw", "Christophe Croux" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 1993
    }, {
      "title" : "The truth of the f-measure",
      "author" : [ "Yutaka Sasaki" ],
      "venue" : "Teach Tutor mater, pages",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2007
    }, {
      "title" : "Exponentially weighted moving average control schemes: properties and enhancements",
      "author" : [ "James M. Lucas", "Michael S. Saccucci" ],
      "venue" : null,
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 1990
    }, {
      "title" : "Probabilistic reasoning for streaming anomaly detection",
      "author" : [ "Kevin M. Carter", "William W. Streilein" ],
      "venue" : "In Statistical Signal Processing Workshop (SSP),",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2012
    }, {
      "title" : "Standard Errors of Mean",
      "author" : [ "S. Ahn", "J.A. Fessler" ],
      "venue" : "Variance, and Standard Deviation Estimators. http://web.eecs.umich.edu/~fessler/papers/files/tr/stderr.pdf",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "STL: a seasonal-trend decomposition procedure based on loess",
      "author" : [ "Robert B. Cleveland", "William S. Cleveland", "Jean E. McRae", "Irma Terpenning" ],
      "venue" : "Journal of Official Statistics,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 1990
    }, {
      "title" : "The advanced theory of statistics",
      "author" : [ "A. Stuart", "M. Kendall", "J. Keith Ord" ],
      "venue" : "Vol. 3: Design and analysis and time-series. Griffin",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 1983
    }, {
      "title" : "Robust locally weighted regression and smoothing scatterplots",
      "author" : [ "William S. Cleveland" ],
      "venue" : "Journal of the American statistical association,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 1979
    }, {
      "title" : "Shewhart. Quality control charts",
      "author" : [ "A. Walter" ],
      "venue" : "Bell System Technical Journal,",
      "citeRegEx" : "45",
      "shortCiteRegEx" : "45",
      "year" : 1926
    }, {
      "title" : "Economic control of Quality of Manufactured Product",
      "author" : [ "W.A. Shewart" ],
      "venue" : "Van Nostrand Reinhold Co.",
      "citeRegEx" : "46",
      "shortCiteRegEx" : null,
      "year" : 1931
    }, {
      "title" : "Control chart tests based on geometric moving averages",
      "author" : [ "S.W. Roberts" ],
      "venue" : "Technometrics, 1(3):239–250",
      "citeRegEx" : "47",
      "shortCiteRegEx" : null,
      "year" : 1959
    }, {
      "title" : "Introduction to statistical quality control",
      "author" : [ "Douglas C. Montgomery" ],
      "venue" : "Wiley. com,",
      "citeRegEx" : "48",
      "shortCiteRegEx" : "48",
      "year" : 2007
    }, {
      "title" : "Continuous inspection schemes",
      "author" : [ "E.S. Page" ],
      "venue" : "Biometrika, 41(1/2):100–115",
      "citeRegEx" : "49",
      "shortCiteRegEx" : null,
      "year" : 1954
    }, {
      "title" : "A review of multivariate control charts",
      "author" : [ "Cynthia A. Lowry", "Douglas C. Montgomery" ],
      "venue" : "IIE transactions,",
      "citeRegEx" : "50",
      "shortCiteRegEx" : "50",
      "year" : 1995
    }, {
      "title" : "Research issues and ideas in statistical process control",
      "author" : [ "William H. Woodall", "Douglas C. Montgomery" ],
      "venue" : "Journal of Quality Technology,",
      "citeRegEx" : "51",
      "shortCiteRegEx" : "51",
      "year" : 1999
    }, {
      "title" : "The state of statistical process control as we proceed into the 21st century",
      "author" : [ "Zachary G. Stoumbos", "Marion R. Reynolds Jr.", "Thomas P. Ryan", "William H. Woodall" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "52",
      "shortCiteRegEx" : "52",
      "year" : 2000
    }, {
      "title" : "Statistical process control for multistage manufacturing and service operations: a review and some extensions",
      "author" : [ "Fugee Tsung", "Yanting Li", "Ming Jin" ],
      "venue" : "International Journal of Services Operations and Informatics,",
      "citeRegEx" : "53",
      "shortCiteRegEx" : "53",
      "year" : 2008
    }, {
      "title" : "The january effect: still there after all these years",
      "author" : [ "Robert A. Haugen", "Philippe Jorion" ],
      "venue" : "Financial Analysts Journal,",
      "citeRegEx" : "54",
      "shortCiteRegEx" : "54",
      "year" : 1996
    }, {
      "title" : "An econometric analysis of market anomaly-day of the week effect on a small emerging market",
      "author" : [ "Julijana Angelovska" ],
      "venue" : "International Journal of Academic Research in Accounting, Finance and Management Sciences,",
      "citeRegEx" : "55",
      "shortCiteRegEx" : "55",
      "year" : 2013
    }, {
      "title" : "Why you win or lose: The psychology of speculation",
      "author" : [ "Fred C. Kelly" ],
      "venue" : "Courier Dover Publications,",
      "citeRegEx" : "56",
      "shortCiteRegEx" : "56",
      "year" : 1930
    }, {
      "title" : "Stock returns and the weekend effect",
      "author" : [ "Kenneth R. French" ],
      "venue" : "Journal of financial economics,",
      "citeRegEx" : "57",
      "shortCiteRegEx" : "57",
      "year" : 1980
    }, {
      "title" : "Day of the week effects and asset returns",
      "author" : [ "Michael R. Gibbons", "Patrick Hess" ],
      "venue" : "Journal of business,",
      "citeRegEx" : "58",
      "shortCiteRegEx" : "58",
      "year" : 1981
    }, {
      "title" : "New findings regarding day-of-the-week returns over trading and non-trading periods: A note",
      "author" : [ "Richard J. Rogalski" ],
      "venue" : "The Journal of Finance,",
      "citeRegEx" : "59",
      "shortCiteRegEx" : "59",
      "year" : 1984
    }, {
      "title" : "The weekend effect, the stock exchange account and the financial times industrial ordinary shares index: 1987-1994",
      "author" : [ "J. Andrew Coutts", "Peter A. Hayes" ],
      "venue" : "Applied Financial Economics,",
      "citeRegEx" : "60",
      "shortCiteRegEx" : "60",
      "year" : 1999
    }, {
      "title" : "A survey of the monday effect literature",
      "author" : [ "Glenn N. Pettengill" ],
      "venue" : "Quarterly Journal of Business and Economics,",
      "citeRegEx" : "61",
      "shortCiteRegEx" : "61",
      "year" : 2003
    }, {
      "title" : "Investment results from exploiting turn-of-the-month effects",
      "author" : [ "Chris R. Hensel", "William T. Ziemba" ],
      "venue" : "The Journal of Portfolio Management,",
      "citeRegEx" : "62",
      "shortCiteRegEx" : "62",
      "year" : 1996
    }, {
      "title" : "Dangers of data mining: The case of calendar effects in stock returns",
      "author" : [ "Ryan Sullivan", "Allan Timmermann", "Halbert White" ],
      "venue" : "Journal of Econometrics,",
      "citeRegEx" : "63",
      "shortCiteRegEx" : "63",
      "year" : 2001
    }, {
      "title" : "Testing the significance of calendar effects",
      "author" : [ "Peter Hansen", "Asger Lunde", "James Nason" ],
      "venue" : "Federal Reserve Bank of Atlanta Working Paper,",
      "citeRegEx" : "64",
      "shortCiteRegEx" : "64",
      "year" : 2005
    }, {
      "title" : "Use of spectral analysis in defense against DoS attacks",
      "author" : [ "Chen-Mou Cheng", "H.T. Kung", "Koan-Sin Tan" ],
      "venue" : "In Global Telecommunications Conference,",
      "citeRegEx" : "65",
      "shortCiteRegEx" : "65",
      "year" : 2002
    }, {
      "title" : "A signal analysis of network traffic anomalies",
      "author" : [ "Paul Barford", "Jeffery Kline", "David Plonka", "Amos Ron" ],
      "venue" : "In Proceedings of the 2nd ACM SIGCOMM Workshop on Internet measurment,",
      "citeRegEx" : "66",
      "shortCiteRegEx" : "66",
      "year" : 2002
    }, {
      "title" : "Anomaly detection in communication networks using wavelets",
      "author" : [ "Vicente Alarcon-Aquino", "Javier A. Barria" ],
      "venue" : "IEE Proceedings-Communications,",
      "citeRegEx" : "67",
      "shortCiteRegEx" : "67",
      "year" : 2001
    }, {
      "title" : "DDoS attack detection and wavelets",
      "author" : [ "Lan Li", "Gyungho Lee" ],
      "venue" : "Telecommunication Systems,",
      "citeRegEx" : "68",
      "shortCiteRegEx" : "68",
      "year" : 2005
    }, {
      "title" : "Detecting traffic anomalies through aggregate analysis of packet header data",
      "author" : [ "Seong Soo Kim", "AL Narasimha Reddy", "Marina Vannucci" ],
      "venue" : "NETWORKING",
      "citeRegEx" : "69",
      "shortCiteRegEx" : "69",
      "year" : 2004
    }, {
      "title" : "WADeS: A tool for distributed denial of service attack detection",
      "author" : [ "Anu Ramanathan" ],
      "venue" : "PhD thesis, Texas A&M University,",
      "citeRegEx" : "70",
      "shortCiteRegEx" : "70",
      "year" : 2002
    }, {
      "title" : "Anomaly detection of network traffic based on wavelet packet",
      "author" : [ "Jun Gao", "Guangmin Hu", "Xingmiao Yao", "Rocky KC Chang" ],
      "venue" : "In Communications,",
      "citeRegEx" : "71",
      "shortCiteRegEx" : "71",
      "year" : 2006
    }, {
      "title" : "Network anomaly detection based on wavelet analysis",
      "author" : [ "Wei Lu", "Ali A. Ghorbani" ],
      "venue" : "EURASIP Journal on Advances in Signal Processing,",
      "citeRegEx" : "72",
      "shortCiteRegEx" : "72",
      "year" : 2009
    }, {
      "title" : "Signal processing methods for network anomaly detection",
      "author" : [ "Lingsong Zhang" ],
      "venue" : null,
      "citeRegEx" : "73",
      "shortCiteRegEx" : "73",
      "year" : 2005
    }, {
      "title" : "Signal processing-based anomaly detection techniques: A comparative analysis",
      "author" : [ "Joseph Ndong", "Kavé Salamatian" ],
      "venue" : "In INTERNET",
      "citeRegEx" : "74",
      "shortCiteRegEx" : "74",
      "year" : 2011
    }, {
      "title" : "The internet of things: A survey",
      "author" : [ "Luigi Atzori", "Antonio Iera", "Giacomo Morabito" ],
      "venue" : "Computer Networks,",
      "citeRegEx" : "76",
      "shortCiteRegEx" : "76",
      "year" : 2010
    }, {
      "title" : "An intrusion-detection model",
      "author" : [ "Dorothy E. Denning" ],
      "venue" : "Software Engineering, IEEE Transactions on,",
      "citeRegEx" : "79",
      "shortCiteRegEx" : "79",
      "year" : 1987
    }, {
      "title" : "A comparative study of anomaly detection schemes in network intrusion detection",
      "author" : [ "Aleksandar Lazarevic", "Levent Ertoz", "Vipin Kumar", "Aysel Ozgur", "Jaideep Srivastava" ],
      "venue" : "Proc. SIAM,",
      "citeRegEx" : "80",
      "shortCiteRegEx" : "80",
      "year" : 2003
    }, {
      "title" : "Anomaly-based network intrusion detection: Techniques, systems and challenges. computers",
      "author" : [ "Pedro Garcia-Teodoro", "J. Diaz-Verdejo", "Gabriel Maciá-Fernández", "Enrique Vázquez" ],
      "venue" : null,
      "citeRegEx" : "81",
      "shortCiteRegEx" : "81",
      "year" : 2009
    }, {
      "title" : "A survey of outlier detection methods in network anomaly identification",
      "author" : [ "Prasanta Gogoi", "D.K. Bhattacharyya", "Bhogeswar Borah", "Jugal K. Kalita" ],
      "venue" : "The Computer Journal,",
      "citeRegEx" : "82",
      "shortCiteRegEx" : "82",
      "year" : 2011
    }, {
      "title" : "A survey of anomaly intrusion detection techniques",
      "author" : [ "Yingbing Yu" ],
      "venue" : "J. Comput. Sci. Coll.,",
      "citeRegEx" : "83",
      "shortCiteRegEx" : "83",
      "year" : 2012
    }, {
      "title" : "The most probable choice between several discrepant observations and the formation therefrom of the most likely induction",
      "author" : [ "D. Bernoulli" ],
      "venue" : "Biometrika, 48:3–18",
      "citeRegEx" : "85",
      "shortCiteRegEx" : null,
      "year" : 1961
    }, {
      "title" : "The study of outliers: purpose and model",
      "author" : [ "Vic Barnett" ],
      "venue" : "Applied Statistics,",
      "citeRegEx" : "86",
      "shortCiteRegEx" : "86",
      "year" : 1978
    }, {
      "title" : "Outlier",
      "author" : [ "R.J. Beckman", "R.D. Cook" ],
      "venue" : ". . . . . . . .ṡ. Technometrics, 25(2):119–149",
      "citeRegEx" : "87",
      "shortCiteRegEx" : null,
      "year" : 1983
    }, {
      "title" : "Anomaly detection: A survey",
      "author" : [ "Varun Chandola", "Arindam Banerjee", "Vipin Kumar" ],
      "venue" : "ACM Computing Surveys (CSUR),",
      "citeRegEx" : "88",
      "shortCiteRegEx" : "88",
      "year" : 2009
    }, {
      "title" : "Informal identification of outliers in medical data",
      "author" : [ "Jorma Laurikkala", "Martti Juhola", "Erna Kentala", "N. Lavrac", "S. Miksch", "B. Kavsek" ],
      "venue" : "In Proceedings of the 5th International Workshop on Intelligent Data Analysis in Medicine and Pharmacology,",
      "citeRegEx" : "89",
      "shortCiteRegEx" : "89",
      "year" : 2000
    }, {
      "title" : "Regression quantiles",
      "author" : [ "R. Koenker", "G. Bassett Jr" ],
      "venue" : "Econometrica, 46:33—50",
      "citeRegEx" : "90",
      "shortCiteRegEx" : null,
      "year" : 1978
    }, {
      "title" : "Robust regression: Asymptotics",
      "author" : [ "P.J. Huber" ],
      "venue" : "conjectures and monte carlo. Annals of Statistics, 1:799–821",
      "citeRegEx" : "91",
      "shortCiteRegEx" : null,
      "year" : 1973
    }, {
      "title" : "Robust Regression and Outlier Detection",
      "author" : [ "P.J. Rousseeuw", "A.M. Leroy" ],
      "venue" : null,
      "citeRegEx" : "92",
      "shortCiteRegEx" : "92",
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Big Data is characterized by the increasing volume (on the order of zetabytes), and the velocity of data generation [1, 2].",
      "startOffset" : 116,
      "endOffset" : 122
    }, {
      "referenceID" : 1,
      "context" : "ing initiatives [7, 8] have been undertaken.",
      "startOffset" : 16,
      "endOffset" : 22
    }, {
      "referenceID" : 2,
      "context" : "ing initiatives [7, 8] have been undertaken.",
      "startOffset" : 16,
      "endOffset" : 22
    }, {
      "referenceID" : 3,
      "context" : "Likewise, there has been an increasing emphasis on developing techniques for detection, and root cause analysis, of performance issues in the cloud [9, 10, 11, 12, 13, 14, 15].",
      "startOffset" : 148,
      "endOffset" : 175
    }, {
      "referenceID" : 4,
      "context" : "Likewise, there has been an increasing emphasis on developing techniques for detection, and root cause analysis, of performance issues in the cloud [9, 10, 11, 12, 13, 14, 15].",
      "startOffset" : 148,
      "endOffset" : 175
    }, {
      "referenceID" : 5,
      "context" : "Likewise, there has been an increasing emphasis on developing techniques for detection, and root cause analysis, of performance issues in the cloud [9, 10, 11, 12, 13, 14, 15].",
      "startOffset" : 148,
      "endOffset" : 175
    }, {
      "referenceID" : 6,
      "context" : "Likewise, there has been an increasing emphasis on developing techniques for detection, and root cause analysis, of performance issues in the cloud [9, 10, 11, 12, 13, 14, 15].",
      "startOffset" : 148,
      "endOffset" : 175
    }, {
      "referenceID" : 7,
      "context" : "Likewise, there has been an increasing emphasis on developing techniques for detection, and root cause analysis, of performance issues in the cloud [9, 10, 11, 12, 13, 14, 15].",
      "startOffset" : 148,
      "endOffset" : 175
    }, {
      "referenceID" : 8,
      "context" : "Likewise, there has been an increasing emphasis on developing techniques for detection, and root cause analysis, of performance issues in the cloud [9, 10, 11, 12, 13, 14, 15].",
      "startOffset" : 148,
      "endOffset" : 175
    }, {
      "referenceID" : 9,
      "context" : "Likewise, there has been an increasing emphasis on developing techniques for detection, and root cause analysis, of performance issues in the cloud [9, 10, 11, 12, 13, 14, 15].",
      "startOffset" : 148,
      "endOffset" : 175
    }, {
      "referenceID" : 10,
      "context" : "A lot of research has been done in the context of anomaly detection in various domains such as, but not limited to, statistics, signal processing, finance, econometrics, manufacturing, and networking [16, 17, 18, 19].",
      "startOffset" : 200,
      "endOffset" : 216
    }, {
      "referenceID" : 11,
      "context" : "A lot of research has been done in the context of anomaly detection in various domains such as, but not limited to, statistics, signal processing, finance, econometrics, manufacturing, and networking [16, 17, 18, 19].",
      "startOffset" : 200,
      "endOffset" : 216
    }, {
      "referenceID" : 12,
      "context" : "A lot of research has been done in the context of anomaly detection in various domains such as, but not limited to, statistics, signal processing, finance, econometrics, manufacturing, and networking [16, 17, 18, 19].",
      "startOffset" : 200,
      "endOffset" : 216
    }, {
      "referenceID" : 13,
      "context" : "highlighted that anomalies are contextual in nature [20] and remarked the following:",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 14,
      "context" : "S-ESD then applies ESD [21, 22] on the resulting time series to detect the anomalies.",
      "startOffset" : 23,
      "endOffset" : 31
    }, {
      "referenceID" : 15,
      "context" : "S-ESD then applies ESD [21, 22] on the resulting time series to detect the anomalies.",
      "startOffset" : 23,
      "endOffset" : 31
    }, {
      "referenceID" : 16,
      "context" : "To address such cases, coupled with the fact that mean and standard deviation (used by ESD) are highly sensitive to a large number anomalies [23, 24], we extended S-ESD to use the robust statistics median [25] and median absolute deviation (MAD) to detect anomalies [26].",
      "startOffset" : 141,
      "endOffset" : 149
    }, {
      "referenceID" : 17,
      "context" : "To address such cases, coupled with the fact that mean and standard deviation (used by ESD) are highly sensitive to a large number anomalies [23, 24], we extended S-ESD to use the robust statistics median [25] and median absolute deviation (MAD) to detect anomalies [26].",
      "startOffset" : 141,
      "endOffset" : 149
    }, {
      "referenceID" : 18,
      "context" : "To address such cases, coupled with the fact that mean and standard deviation (used by ESD) are highly sensitive to a large number anomalies [23, 24], we extended S-ESD to use the robust statistics median [25] and median absolute deviation (MAD) to detect anomalies [26].",
      "startOffset" : 205,
      "endOffset" : 209
    }, {
      "referenceID" : 19,
      "context" : "To address such cases, coupled with the fact that mean and standard deviation (used by ESD) are highly sensitive to a large number anomalies [23, 24], we extended S-ESD to use the robust statistics median [25] and median absolute deviation (MAD) to detect anomalies [26].",
      "startOffset" : 266,
      "endOffset" : 270
    }, {
      "referenceID" : 20,
      "context" : "In essence, these techniques employ statistical hypothesis testing, for a given significance level [27], to determine whether a datum is anomalous.",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 21,
      "context" : "Grubbs test [28, 29] was developed for detecting the largest anomaly within a univariate sample set.",
      "startOffset" : 12,
      "endOffset" : 20
    }, {
      "referenceID" : 22,
      "context" : "Grubbs test [28, 29] was developed for detecting the largest anomaly within a univariate sample set.",
      "startOffset" : 12,
      "endOffset" : 20
    }, {
      "referenceID" : 23,
      "context" : "For one-sided tests, α/(2N) becomes α/N [30].",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 24,
      "context" : "Several other approaches, such as the Tietjen-Moore test [31], and the extreme Studentized deviate (ESD) test [21, 22] have been proposed to address the aforementioned issue.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 14,
      "context" : "Several other approaches, such as the Tietjen-Moore test [31], and the extreme Studentized deviate (ESD) test [21, 22] have been proposed to address the aforementioned issue.",
      "startOffset" : 110,
      "endOffset" : 118
    }, {
      "referenceID" : 15,
      "context" : "Several other approaches, such as the Tietjen-Moore test [31], and the extreme Studentized deviate (ESD) test [21, 22] have been proposed to address the aforementioned issue.",
      "startOffset" : 110,
      "endOffset" : 118
    }, {
      "referenceID" : 14,
      "context" : "The Extreme Studentized Deviate test (ESD) [21] (and its generalized version [22]) can also be used to detect multiple anomalies in the given time series.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 15,
      "context" : "The Extreme Studentized Deviate test (ESD) [21] (and its generalized version [22]) can also be used to detect multiple anomalies in the given time series.",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 18,
      "context" : "It is well known that these metrics are sensitive to anomalous data [25, 32].",
      "startOffset" : 68,
      "endOffset" : 76
    }, {
      "referenceID" : 25,
      "context" : "It is well known that these metrics are sensitive to anomalous data [25, 32].",
      "startOffset" : 68,
      "endOffset" : 76
    }, {
      "referenceID" : 26,
      "context" : "Thus, the sample mean is said to have a breakdown point [33, 34] of 0, while the sample median is said to have a breakdown point of 0.",
      "startOffset" : 56,
      "endOffset" : 64
    }, {
      "referenceID" : 27,
      "context" : "Thus, the sample mean is said to have a breakdown point [33, 34] of 0, while the sample median is said to have a breakdown point of 0.",
      "startOffset" : 56,
      "endOffset" : 64
    }, {
      "referenceID" : 19,
      "context" : "Unlike standard deviation, MAD is robust against anomalies in the input data [26, 25].",
      "startOffset" : 77,
      "endOffset" : 85
    }, {
      "referenceID" : 18,
      "context" : "Unlike standard deviation, MAD is robust against anomalies in the input data [26, 25].",
      "startOffset" : 77,
      "endOffset" : 85
    }, {
      "referenceID" : 28,
      "context" : "4826 is used for normally distributed data (irrespective of non-normality introduced by outliers) [35].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 16,
      "context" : "the underlying distribution [23].",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 29,
      "context" : "If β > 1, F is said to become more recall-oriented and if β < 1, F is said to become more precision-oriented [36].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 30,
      "context" : "To this end, the use of the exponentially weighted moving average (EWMA), defined by Equation 13, has been proposed [37].",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 31,
      "context" : "In [38], Carter and Streilein argue that in the context of streaming time series data EWMA can potentially be “volatile to abrupt transient changes, losing utility for appropriately detecting anomalies”.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 32,
      "context" : "However, as shown in [39], the standard error of σ (∝ 1 √ n−1 ), increases as the window length decreases.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 33,
      "context" : "Before moving on, let us first define sub-cycle series [40]:",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 34,
      "context" : "Time series decomposition approaches can be either additive or multiplicative [41], with additive decomposition being appropriate for seasonal data that has a constant magnitude as is the case in the current context.",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 33,
      "context" : "This issue is addressed by STL [40], a robust approach to decomposition that uses LOESS[42] to estimate the seasonal component.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 35,
      "context" : "This issue is addressed by STL [40], a robust approach to decomposition that uses LOESS[42] to estimate the seasonal component.",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 10,
      "context" : "For a detailed coverage of the same, the reader is referred to books and survey papers [16, 17, 18, 19].",
      "startOffset" : 87,
      "endOffset" : 103
    }, {
      "referenceID" : 11,
      "context" : "For a detailed coverage of the same, the reader is referred to books and survey papers [16, 17, 18, 19].",
      "startOffset" : 87,
      "endOffset" : 103
    }, {
      "referenceID" : 12,
      "context" : "For a detailed coverage of the same, the reader is referred to books and survey papers [16, 17, 18, 19].",
      "startOffset" : 87,
      "endOffset" : 103
    }, {
      "referenceID" : 13,
      "context" : "As mentioned in a recent survey on anomaly detection [20], anomaly detection is highly contextual in nature.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 36,
      "context" : "To this end, Statistical Process Control (SPC) was proposed in the early 1920s to monitor and control the reliability of a manufacturing process [45, 46].",
      "startOffset" : 145,
      "endOffset" : 153
    }, {
      "referenceID" : 37,
      "context" : "To this end, Statistical Process Control (SPC) was proposed in the early 1920s to monitor and control the reliability of a manufacturing process [45, 46].",
      "startOffset" : 145,
      "endOffset" : 153
    }, {
      "referenceID" : 38,
      "context" : "Roberts proposed the geometrical moving average control chart – also known as the EWMA (exponentially weighted moving average) control chart – which weights the most recent samples more highly than older samples [47].",
      "startOffset" : 212,
      "endOffset" : 216
    }, {
      "referenceID" : 39,
      "context" : "In case the quality characteristic follows a Poisson distribution, alternatives to the EWMA chart have been proposed [48].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 40,
      "context" : "Other types of control charts, such as the CUSUM (cumulative sum) chart [49], have been proposed.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 41,
      "context" : "In [50], Lowry and Montgomery present a review on control charts for multivariate quality control.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 42,
      "context" : "For further details on SPC and control charts, the reader is referred to the surveys by Woodall and Montgomery [51] and by Stoumbos et al.",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 43,
      "context" : "[52].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 44,
      "context" : "presents a comprehensive survey of statistical control methods for multistage manufacturing and service operations [53].",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 45,
      "context" : "perhaps the best-known example of anomalous behavior in security markets throughout the world” [54].",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 46,
      "context" : "In [55], Angelovska reports that early studies on stock market anomalies began in the late 1920’s, where Kelly’s reports [56] showed the existence of a so-called “Monday effect” – US markets have low and negative Monday returns.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 47,
      "context" : "In [55], Angelovska reports that early studies on stock market anomalies began in the late 1920’s, where Kelly’s reports [56] showed the existence of a so-called “Monday effect” – US markets have low and negative Monday returns.",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 48,
      "context" : "The Monday effect in the US sock market was actively researched in the 1980s [57] [58] [59].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 49,
      "context" : "The Monday effect in the US sock market was actively researched in the 1980s [57] [58] [59].",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 50,
      "context" : "The Monday effect in the US sock market was actively researched in the 1980s [57] [58] [59].",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 51,
      "context" : "In [60], Coutts and Hayes showed that the Monday effect exists albeit not as strongly as previous work demonstrated.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 52,
      "context" : "For a full literature review on the Monday effect, refer to [61].",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 53,
      "context" : "large-cap stocks consistently show higher returns at the turn of the month [62].",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 54,
      "context" : "argue against the case, stating that there is no statistically significant evidence supporting the claim, and that such periodicities in stock market behavior are the result of data dredging [63].",
      "startOffset" : 191,
      "endOffset" : 195
    }, {
      "referenceID" : 55,
      "context" : "attempted to use sound statistical approaches to evaluate the significance of calendar effects, by tightly controlling the testing to avoid data mining biases [64].",
      "startOffset" : 159,
      "endOffset" : 163
    }, {
      "referenceID" : 56,
      "context" : "For instance, in [65], Cheng et al.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 57,
      "context" : "Similarly, wavelet packets and wavelet decomposition have been used for detecting anomalies in network traffic [66, 67, 68, 69, 70].",
      "startOffset" : 111,
      "endOffset" : 131
    }, {
      "referenceID" : 58,
      "context" : "Similarly, wavelet packets and wavelet decomposition have been used for detecting anomalies in network traffic [66, 67, 68, 69, 70].",
      "startOffset" : 111,
      "endOffset" : 131
    }, {
      "referenceID" : 59,
      "context" : "Similarly, wavelet packets and wavelet decomposition have been used for detecting anomalies in network traffic [66, 67, 68, 69, 70].",
      "startOffset" : 111,
      "endOffset" : 131
    }, {
      "referenceID" : 60,
      "context" : "Similarly, wavelet packets and wavelet decomposition have been used for detecting anomalies in network traffic [66, 67, 68, 69, 70].",
      "startOffset" : 111,
      "endOffset" : 131
    }, {
      "referenceID" : 61,
      "context" : "Similarly, wavelet packets and wavelet decomposition have been used for detecting anomalies in network traffic [66, 67, 68, 69, 70].",
      "startOffset" : 111,
      "endOffset" : 131
    }, {
      "referenceID" : 62,
      "context" : "In [71], Gao et al.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 63,
      "context" : "proposed an approach consisting of three components: (1) feature analysis, (2) normal daily traffic modeling based on wavelet approximation and ARX (AutoRegressive with eXogenous), and intrusion decision [72].",
      "startOffset" : 204,
      "endOffset" : 208
    }, {
      "referenceID" : 64,
      "context" : "An overview of signal processing techniques for network anomaly detection, including PSD (Power Spectral Density) and wavelet-based approaches, is presented in [73].",
      "startOffset" : 160,
      "endOffset" : 164
    }, {
      "referenceID" : 65,
      "context" : "In [74], Ndong and Salamatian reported that PCA-based approaches exhibit improved performance when coupled with Karuhen-Loeve expansion (KL); on the other hand, Kalman filtering approaches, when combined with statistical methods such as Gaussian mixture and Hidden Markov models, outperformed the PCA-KL method.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 66,
      "context" : "Network Traffic With the Internet of Things (IoT) paradigm [75, 76] increasingly becoming ubiquitous, there is an increasing concern about security.",
      "startOffset" : 59,
      "endOffset" : 67
    }, {
      "referenceID" : 67,
      "context" : "For example, in [79], Denning proposed a rule-based technique wherein both network (system) and user data was used to detect different types of abnormal behavior, by comparing audit-trails to different anomalous profiles or models.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 68,
      "context" : "In [80], Lazarevic et al.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 69,
      "context" : "In [81], Garcia-Teodoro et al.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 70,
      "context" : "presented a comprehensive survey on outlier detection for network anomaly detection in [82]; in particular, the authors classified the approaches into three categories: (1) distancebased, (2) density-based, and (3) machine learning or softcomputing based.",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 71,
      "context" : "The reader is referred to the survey of intrusion detection techniques by Yu for further reading [83].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 72,
      "context" : "Statistics Anomaly detection has been actively researched for over five decades in the domain of statistics [84, 85, 86, 16, 87].",
      "startOffset" : 108,
      "endOffset" : 128
    }, {
      "referenceID" : 73,
      "context" : "Statistics Anomaly detection has been actively researched for over five decades in the domain of statistics [84, 85, 86, 16, 87].",
      "startOffset" : 108,
      "endOffset" : 128
    }, {
      "referenceID" : 10,
      "context" : "Statistics Anomaly detection has been actively researched for over five decades in the domain of statistics [84, 85, 86, 16, 87].",
      "startOffset" : 108,
      "endOffset" : 128
    }, {
      "referenceID" : 74,
      "context" : "Statistics Anomaly detection has been actively researched for over five decades in the domain of statistics [84, 85, 86, 16, 87].",
      "startOffset" : 108,
      "endOffset" : 128
    }, {
      "referenceID" : 12,
      "context" : "Recent surveys include the ones from Hodge and Jim [18] and Chandola et al.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 75,
      "context" : "[88].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "These bounds can be used as a threshold for determining the “outlierness” of a random value, indicating that a value does not fit the underlying distribution [17]; however, the Markov and Chebyshev inequalities are non-parametric, and create relatively weak bounds that may miss potential outliers in the data [19].",
      "startOffset" : 158,
      "endOffset" : 162
    }, {
      "referenceID" : 76,
      "context" : "Further, the Box plot may be applied as a robust means of determining if a data point is anomalous with respect to the underlying distribution [89].",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 77,
      "context" : "To this end, we plan to explore the use of qunatile regression [90] and/or robust regression [91, 92].",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 78,
      "context" : "To this end, we plan to explore the use of qunatile regression [90] and/or robust regression [91, 92].",
      "startOffset" : 93,
      "endOffset" : 101
    }, {
      "referenceID" : 79,
      "context" : "To this end, we plan to explore the use of qunatile regression [90] and/or robust regression [91, 92].",
      "startOffset" : 93,
      "endOffset" : 101
    } ],
    "year" : 2017,
    "abstractText" : "Performance and high availability have become increasingly important drivers, amongst other drivers, for user retention in the context of web services such as social networks, and web search. Exogenic and/or endogenic factors often give rise to anomalies, making it very challenging to maintain high availability, while also delivering high performance. Given that service-oriented architectures (SOA) typically have a large number of services, with each service having a large set of metrics, automatic detection of anomalies is nontrivial. Although there exists a large body of prior research in anomaly detection, existing techniques are not applicable in the context of social network data, owing to the inherent seasonal and trend components in the time series data. To this end, we developed two novel statistical techniques for automatically detecting anomalies in cloud infrastructure data. Specifically, the techniques employ statistical learning to detect anomalies in both application, and system metrics. Seasonal decomposition is employed to filter the trend and seasonal components of the time series, followed by the use of robust statistical metrics – median and median absolute deviation (MAD) – to accurately detect anomalies, even in the presence of seasonal spikes. We demonstrate the efficacy of the proposed techniques from three different perspectives, viz., capacity planning, user behavior, and supervised learning. In particular, we used production data for evaluation, and we report Precision, Recall, and F-measure in each case.",
    "creator" : "pdflatex"
  }
}