{
  "name" : "1204.1467.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Mining Fuzzy β-Certain and β-Possible rules from incomplete quantitative data by rough sets",
    "authors" : [ "L. Asadzadeh" ],
    "emails" : [ "Ali_soultanmohammadi@yahoo.com", "l_asadzadeh@pnu.ac.ir", "D_dashchi_rezaee@yahoo.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords-component: Fuzzy set; Rough set; Data mining; Certain rule; Possible rule; Quantitative value; incomplete data\nI. Introduction\nMachine learning and data mining techniques have recently been developed to find implicitly meaningful pat-terns and ease the knowledge-acquisition bottleneck. Among these approaches, deriving inference or association rules from training examples is the most common [9,12,13]. Given a set of examples and counterexamples of a concept, the learning program tries to induce general rules that describe all or most of the positive training instances and none or few of the counterexamples [6,7]. If the training instances belong to more than two classes, the learning program tries to induce general rules that\ndescribe each class. Recently, the rough-set theory has been used in reasoning and knowledge acquisition for expert systems [4,14]. It was proposed by Pawlak in 1982 [19], with the concept of equivalence classes as its basic principle. Several applications and extensions of the rough-set theory have also been proposed. Examples are [14,18] reasoning with incomplete information,[2] knowledge-base reduction, [10] data mining, [16] rule discovery. Due to the success of the rough-set theory to knowledge acquisition, many researchers in database and machine learning fields are interested in this new research topic because it offers opportunities to discover useful information in training examples. [6,17] mentioned that the main issue in the rough-set approach was the formation of good rules.\nII. Review of the variable precision rough-set model\nThe rough-set theory, proposed by Pawlak in 1982, can serve as a new mathematical tool to deal with data classification problems [19]. It adopts the concept of equivalence classes to partition the training instances according to some criteria. Two kinds of partitions are formed in the mining process: lower approximations and upper approximations, from which certain and possible rules are easily derived. Let X be an arbitrary subset of the universe U, and B be an arbitrary subset of the attribute set A. The lower and the upper approximations for B on X, denoted B* (X) and B*\nB\n(X) respectively, are defined as follows:\n* (1) (X) = {x|x ϵ U, B(X)⊆ X} , B* (2) (X) = {x|x ϵ U and B(X) ∩ X ≠Ø} . Elements in B* (x) can be classified using attribute set B as members of the set X with full certainty, and B* (x) is thus called a lower approximation of X. On the other hand, elements in B (x) can be classified using attribute set B as members of the set X only with partial certainty, and B* (x) is thus called an upper approximation of X. The original rough-set model is quite sensitive to noisy data [7]. When noisy data exists, the lower and the upper approximations cannot normally be formed [17]. Let two sets X and Y be non-empty subsets of the universal set (X, Y,\nU). The rough inclusion function is then defined as follows:\n)( Y)card(X Y)(X, Xcard ∩ =µ (3)\nIf the rough inclusion value equals to 1, then X is totally included in Y (X,Y). Otherwise, the rough inclusion value ranges between 0 to 1, and X is said partially included in Y. Also, the relative degree of misclassification of the set X with respect to set Y is defined as :\n)( Y)card(X 1 Y)(X, Xcard c ∩−= (4)\nBased on the relative degree of misclassification, in the[17], generalized the lower and upper approximations of the original rough-set model with a majority inclusion threshold β. The β-lower and the β-upper approximations are defined as follows:\nB*β (5) (X) = {x|x ϵ U , c(B(x),X) ≤ β } B*β (6) (X) = {x|x ϵ U , c(B(x),X) <1- β } .\nIII. Incomplete data sets\nData sets can be roughly classified into two classes: complete and incomplete data sets. All the objects in a complete data set have known attribute values. If at least one object in a data set has a missing value, the data set is incomplete. the symbol ‘*’ denotes an unknown attribute value. Learning rules from incomplete data sets is usually more difficult than from complete data sets. In the past, several methods were proposed to handle the problem of incomplete data sets [21]. For example, incomplete data sets may be transformed into complete data sets by similarity measures or by removing objects with unknown values before learning programs begin. Incomplete data sets may also be directly processed in a particular way to get the rules [20,22]. In the [20] proposed a rough-set approach to directly learn rules from incomplete data sets without guessing unknown attribute values. In the [22] modified [20] approach by introducing the rough entropy to distinguish the power of the attribute subsets that have the same partition for similarity relations.\nIV. Notation\nNotation used in this paper is described as follows: U universe of all objects β tolerance degree of noise and misclassification n total number of training examples (objects) in U Obj(i) A set of all attributes describing U ith training example (object), 1 ≤i ≤n m total number of attributes in A B an arbitrary subset of A\nAj |A j th attribute, 1≤ j≤ m j| number of fuzzy regions for A R j\njk k th fuzzy region of Aj,1 ≤k ≤|Aj v | j (i) quantitative value of Aj for Obj\nf (i) j (i)\nf fuzzy set converted from v jk (i)\nC set of classes to be determined membership value of v in region R c total number of classes in C Xl B(Obj l th class, 1 ≤ l≤ c (i)) the fuzzy incomplete equivalence classes in which Obj(i) B exists k\nc(Obj(i)) the certain part of the kth fuzzy incomplete equivalence class in B(Obj(i) B ) *\nB\n(X) the fuzzy incomplete lower approximation for B on X\n*\n(X) the fuzzy incomplete upper approximation for B on X\nWhen the same linguistic term RRjkR of an attribute ARjR exists in two fuzzy objects ObjP(i) P and ObjP(r) P with membership values fRjkRP(i) P and fRjkRP(r) P larger than zero, ObjP(i) P and ObjP(r) P are said to have a fuzzy indiscernibility relation (or fuzzy equivalence relation) on attribute ARjR with a membership value equal to min (fRjkRP(i) P ∩ fRjkRP(r) P ). Also, if the same linguistic terms of an attribute subset B exist in both ObjP(i) Pand ObjP(r) P with membership values larger than zero, Obj P(i) P and ObjP(r) P are said to have a fuzzy indiscernibility relation (or a fuzzy equivalence relation) on attribute subset B with a membership value equal to the minimum of all the membership values .These fuzzy equivalence relations thus partition the fuzzy object set U into several fuzzy subsets that may overlap, and the result is denoted by U/B. The set of fuzzy partitions, based on B and including ObjP(i) P , is denoted B(ObjP(i) P). Thus, B(ObjP(i) P)= {((BR1R(ObjP(i) P, µ RB1 R(ObjP(i) P)) … ((BRrR(ObjP(r) P, µ RBrR(ObjP(r) P)) , where r is the number of partitions included in B(ObjP(i) P) , BRjR(ObjP(i) P) is the jth partition in B(ObjP(i) P) ,and µ RBjR(ObjP(r) P) is the membership value of the jth partition.\nSince an incomplete quantitative data set contains unknown attribute values, each object ObjP(i) P is thus represented as a tuple (ObjP(i) P ,symbol), where the symbol may be certain (c) or uncertain (u). If an object ObjP(i) P has an uncertain value for attribute ARjR, then (ObjP(i) P ,u) is put in each fuzzy equivalence class of attribute ARjR. The fuzzy sets formed in this way are called fuzzy incomplete equivalence classes, which are not necessarily equivalence classes.The fuzzy incomplete lower and upper approximations for B on X, denoted BR*R(X) and BP*P(X) respectively, are defined as follows: BR*R(XRlR) = {(BRkR(ObjP(i) P),µRBkR(ObjP(i) P))| 1≤i ≤n , objP(i) P ϵ XRlR , BRkRPc P(ObjP(i)P)⊆ Xl , 1≤k≤|B(ObjP(i)P)|}. (7)\nBP*P(XRlR) = {(BRkR(ObjP(i) P),µRBkR(ObjP(i) P))| 1≤i ≤n , BRkRPc P(ObjP(i)P)∩ Xl ≠ Ø , BRkRPc P(ObjP(i)P)⊄ X RlR 1≤k≤|B(ObjP(i)P|}. (8) Here, the definition of the fuzzy incomplete upper approximation has the constraint BRkRPc P(ObjP(i) P)⊄ X to exclude the objects in the fuzzy incomplete lower approximation for avoiding redundant calculation. The fu zzy β-lo wer and the fu zzy β-upper approximations for B on X, denoted BR*βR(X) and BP*PRβ R(X) respectively, are defined as follows:\nBR*β R(XRlR) = {(BRkR(x),µRBkR(x))| x ϵ U , c(BRkR(x), X)≤ β , 1≤k≤|B(x)|} (9)\nBP*PRβ R(XRlR) = {(BRkR(x),µRBkR(x))| x ϵ U , β< c(B RkR(x), X)≤1- β , 1≤k≤|B(x)|}. (10) After the fu zy β-lower and the fu zy β-upper approximations are found, both β-certain and β-uncertain rules can thus be derived.\nV. The proposed algorithm for incomplete quantitative data sets\nIn the section, a learning algorithm based on rough sets is proposed, which can simultaneously estimate the missing values and derive fuzzy certain and possible rules from incomplete quantitative data sets. The proposed fuzzy learning algorithm first transforms each quantitative value into a fuzzy set of linguistic terms using membership functions. The details of the proposed fuzzy learning algorithm are described as follows.\nThe fuzzy mining algorithm for β-certain and βpossible rules:\nInput: A incomplete quantitative data set with n objects ,each with m attribute values, β is tolerance degree of noise and misclassification ,and a set of membership functions.\nOutput: A set of maximally general fuzzy β-certain and fuzzy β-possible rules.\nStep 1: Partition the object set into disjoint subsets according to class labels. Denote each set of objects belonging to the same class CRlR as XRlR.\nStep 2: Transform the quantitative value vRjRP(iPP) P of each object ObjP(i) P ;i =1 to n, for each attribute ARjR; j = 1 to m , into a fuzzy set fRjRP(i) P , represented as,\njl\ni jl\nj\ni j\nj\ni j\nR f R f R f )(\n2\n)( 2\n1\n)( 1 ...+++\n(11)\nusing the given membership functions, where RRjkR is the kth fuzzy region of attribute ARjR ; fRjkRP(i) P is vRjRP(i) P’s fuzzy membership value in region RRjkR, and l (= |ARjR|) is the number of fuzzy regions for ARjR. If ObjP(i) P has a missing value for ARjR, keep it with a missing value (*).\nStep 3: Find the fuzzy incomplete elementary sets of singleton attributes; That is, if an object ObjP(i) P has a certain fuzzy membership value fRjkRP(i) P for attribute ARjR, put (ObjP(i) P ,c) into the fuzzy incomplete equivalence class from ARjR = RRjk R; If ObjP(i) P has a missing value for ARjR, put (ObjP(i) P ,u) into each fuzzy incomplete equivalence class from ARjR; The membership value µRAjkR of a fuzzy incomplete class for ARjR=RRjkR is calculated as:\nf (i)jkjkA Min =µ (12)\nwhere ObjP(i) P is certain and fRjkRP(i) P≠ 0.\nStep 4: Initialize q = 1, where q is used to count the number of attributes currently being processed for fuzzy incomplete lower approximations.\nStep 5: Compute the fuzzy incomplete lower approximations of each subset B with q attributes for each class XRlR as:\nBR*R(XRlR) = {(BRkR(ObjP(i) P),µRBkR(ObjP(i) P))| 1≤i ≤n , objP(i) P ϵ XRlR , BRkRPc P(ObjP(i)P)⊆ Xl , 1≤k≤|B(ObjP(i)P)|}. (13)\nwhere B(ObjP(i) P) is the set of fuzzy incomplete equivalence classes including ObjP(i) P and derived from attribute subset B, BRkRPc P(ObjP(i) P) is the certain part of the kth fuzzy incomplete equivalence class in B(ObjP(i) P) .\nStep 6: If ObjP(i) P exists in fuzzy incomplete equivalence class BRkRPc P(ObjP(i) P) of the kth region combination RRBRPkP from attribute subset B in a fuzzy incomplete lower approximation, assign the uncertain value of ObjP(i) P as:\n∑\n∑\n∈\n∈\n)(\n)( )(\n)()(\n)()(\n)()( .\nic k r\nic k r\nobjBobj\nr jk\nobjBobj\nr jk r j\nf\nfv (14)\nwhere vRjRP(r) P is the quantitative value of ObjP(r) P for attribute ARjR and fRjkRP(r) P is vRjRP(r) P’s fuzzy membership value in RRBRPkP . Also transform the estimated ObjP(i) P value into a fuzzy set, remove (ObjP(i) P ,u)’s with membership values equal to zero from the fuzzy incomplete equivalence classes, change (ObjP(i) P ,u)’s with membership values not equal to zero into (ObjP(i) P ,c)’s and re-calculate the membership values of the fuzzy incomplete equivalence classes including them by the\nminimum operation. Besides, backtrack to the previously found fuzzy incomplete lower approximations for doing the same actions on Obj(i) Step 7: Set q = q+1 and repeat Steps 5–7 until q > m. .\nStep 8: Initialize h = 1, where h is used to count the number of attributes currently being processed for fuzzy incomplete upper approximations.\nStep 9: Compute the fuzzy incomplete upper approximations of each subset B with q attributes for each class Xl\nBP*P(XRlR) = {(BRkR(ObjP(i)P),µRBkR(ObjP(i) P))| 1≤i ≤n , BRkRPc P(ObjP(i)P)∩ Xl ≠ Ø , BRkRPc P(ObjP(i)P)⊄ XRlR 1≤k≤|B(ObjP(i)P|}. (15)\nwhere B(ObjP(i) P) is the set of fuzzy incomplete equivalence classes including ObjP(i) P and derived from attribute subset B, BRkRPc P(ObjP(i) P) is the certain part of the kth fuzzy incomplete equivalence class in B(ObjP(i) P) . Step 10: Do the following sub steps for each uncertain instance ObjP(i) P in the fuzzy incomplete upper approximations:\na) If ObjP(i) P exists in fuzzy incomplete equivalence class BRkRPc P(ObjP(i) P) of the kth region combination RRB RPk P from attribute subset B in a fuzzy incomplete lower approximation, assign the uncertain value of ObjP(i) P as:\n∑\n∑\n∈∈\n∈∈\nl ric k r\nl ric k r\nXobjobjBobj\nr jk\nXobjobjBobj\nr jk r j\nf\nfv\n)()()(\n)()()(\n&)(\n)( &)(\n)()( . (16)\nwhere vRjRP(r) P is the quantitative value of ObjP(r) P for attribute ARjR and fRjkRP(r) P is vRjRP(r) P’s fuzzy membership value in RRBRPkP . Also transform the estimated ObjP(i) P value into a fuzzy set, remove (ObjP(i) P ,u)’s with membership values equal to zero from the fuzzy incomplete equivalence classes, change (ObjP(i) P ,u)’s with membership values not equal to zero into (ObjP(i) P ,c)’s and re-calculate the membership values of the fuzzy incomplete equivalence classes including them by the minimum operation. Besides, backtrack to the previously found fuzzy incomplete lower approximations for doing the same actions on ObjP(i) P . b) If an object ObjP(i) P still exists in more than one fuzzy incomplete equivalence class in a fuzzy incomplete upper approximation, use the equivalence class with the maximum plausibility measure to estimate the uncertain value of ObjP(i) P . The estimation and processing are the same as those stated in Step 10(a). Calculate the plausibility measures of each fuzzy incomplete equivalence class in an upper approximation for each class XRlR as:\n∑\n∑\n⊂\n∈∈=\n)(\n)( &)(\n)(\n)(\n)()(\n)()()( ))((\nic k r\nl ric k r\nobjBobj\nr jk\nXobjobjBobj\nr jk\nic k objBP\nµ\nµ\n(17)\nc) If an object ObjP(i) P exists in more than one fuzzy incomplete equivalence class in a fuzzy incomplete upper approximation and them plausibility measure together are equal, define for determine uncertain value of ObjP(i) Pin it equivalence class that number certain objects that more of classes other. If number certain objects classes are equal, hence define that class than are include least label.\nStep 11: Set h=h+1 and repeat Steps 9–11until h > m.\nStep 12 : Initialize l= 1, where l is used to count the number of classes being processed.\nStep 13: Calculate the relative degree of misclassification of each attribute subset BRkR for class XRlR as:\n∑\n∑\n∈\n∩∈−=\n)(\n))((\n)(\n)(\n1)),((\nxBy B\nXxBy B\nlk\nk k\nlk k\ny\ny\nXxBc µ\nµ (18)\nStep 14: Calculate the modified fuzzy β-lower and βupper approximation of each attribute subset B for class XRlR as equation “(9)” and “(10)”. Step 15: Derive the β-certain rules from the fuzzy βlower approximation BR*β R(XRlR) of any subset B, set the membership values in the β-lower approximation as the effectiveness for future data, and calculate the plausibility measure of each rule for BRkR(x) as :\n)),((1 lk XxBc− (19) Step 16: Remove the β-certain rules with the condition parts more specific and effectiveness measure equal to or smaller than those of some other β-certain rules.\nStep 17: Derive the β-possible rules from the fuzzy βupper approximation BP*PRβ R(X ) of any subset B, set the membership values in the β-upper approximation as the effectiveness for future data, and calculate the plausibility measure of each rule for BRkR(x) as equation “(19)” .\nStep 18: Remove the β-possible rules with the condition parts more specific and both the effectiveness and plausibility equal to or smaller than those of some other β-certain or β-possible rules.\nStep 19: Set l=l+1and Repeat Steps 13 to19 until l≤ c.\nStep 20 : Output the β-certain and β-possible fuzzy rules .\nas:\nVI. an example\nIn this section, an example is given to show how the proposed algorithm can be used to generate maximally general fuzzy β-certain and fuzzy βpossible rules from incomplete quantitative data.\nStep 1: Since three classes exist in the data set, three partitions are thus formed as follows:\nStep 2: The quantitative values of each object are transformed into fuzzy sets. Take the attribute Systolic Pressure (SP) in ObjP(2) P as an example. The value ‘‘155’’ is converted into a fuzzy set (0.1/N + 0.75/H) using the given membership functions. Results for all the objects are shown in Table 2.\nStep 3: The elementary fuzzy sets of the singleton attributes SP and DP are found as follows:\nu),0.5}},c)(obj,c)(obj,{(obj\nu),0.5},c)(obj,c)(obj,{(obj\nu),0.1},c)(obj,c)(obj,(obj\nc),c)(obj,c)(obj,{{(obj = U/{SP}\n(5)(7)(4)\n(5)(6)(2)\n(5)(7)(6)\n(3)(2)(1)\nand\nu),0.5}},c)(obj,{(obj\nu),0.4},c)(obj,c)(obj,c)(obj,{(obj\nc),0.16},u)(obj,c)(obj,(obj\nc),c)(obj,{{(obj = U/{DP}\n(7)(4)\n(7)(6)(5)(3)\n(5)(7)(3)\n(2)(1)\nStep 4: q is initially set at 1, where q is used to count the number of the attributes currently being processed for fuzzy incomplete lower approximations.\nStep 5: The fuzzy incomplete lower approximation of one attribute for XRHR={ObjP(2) P , ObjP(5) P , ObjP(6) P } is first calculated. Since only the certain part (ObjP(2) P,c) of the first incomplete equivalence class for attribute SP is included in XRH Rand the uncertain instances ObjP(5) Pbelong to XRHR, thus:\nu),0.5},c)(obj,c)(obj,{(obj = )(XSP (5)(6)(2)H*\nSince the certain part of each fuzzy incomplete equivalence class for attribute DP is not included in XRHR, thus:\nφ = )(XDP H*\nSimilarly, the fuzzy incomplete lower approximations of single attributes for XRNR and XRLR are found as follows:\nu),1},c)(obj,{(obj = )(XDP\nc),0.5},c)(obj,{(obj = )(XSP\n= )(XDP = )(XSP\n(7)(4) L*\n(7)(4) L*\nN*\nN* φ φ\nStep 6: Each uncertain object in the above fuzzy incomplete lower approximations is checked for change to certain objects. For example in SP*(XH), since Obj(5) exist in only one fuzzy incomplete equivalence class of SP = H, their values can then be estimated from the certain objects in the same equivalence class. Since only one certain object Obj(2) and Obj(6) exists in the fuzzy incomplete equivalence class of SP = H, the estimated value of Obj(5) is then (155*0.75+150*0.5)/1.25 (=153), where 153 is the quantitative value of Obj(2) and Obj(6) for attribute SP and 0.75 , 0.5 are there fuzzy membership values for the region of SP = H. The estimated values of Obj(5) is then transformed as the fuzzy set (0.2/N+0.65/H) . (Obj(5) ,u) is then changed as (Obj(5)\nThe modified SP\n,c).\n*(XH\nc),0.5}},c)(obj,{(obj\nc),0.5},c)(obj,c)(obj,{(obj\nc),0.1},c)(obj,c)(obj,(obj\nc),c)(obj,c)(obj,{{(obj = U/{SP}\nc),0.5},c)(obj,c)(obj,{(obj = )(XSP\n(7)(4)\n(5)(6)(2)\n(5)(7)(6)\n(3)(2)(1)\n(5)(6)(2) H*\n) and The fuzzy incomplete elementary set of attribute SP are then:\nSimilarly , Since the uncertain object Obj(7) in DP*(XL) exist in only the fuzzy incomplete equivalence class of DP = L, the estimated value of Obj(7) is then (68*1)/1 (=68)), where 68 is the quantitative value of Obj(4) for attribute DP and 1 is its fuzzy membership value of DP = L. The estimated value of Obj(7) is then transformed as the fuzzy set (1/L) for attribute DP. Also, (Obj(7) ,u) is then changed as (Obj(7) ,c). The modified DP*(XH\nc),0.5}},c)(obj,{(obj\nc),0.4},c)(obj,c)(obj,{(obj\nc),0.16},c)(obj,(obj\nc),c)(obj,{{(obj = U/{DP}\nc),1},c)(obj,{(obj = )(XDP\n(7)(4)\n(6)(5)(3)\n(5)(3)\n(2)(1)\n(7)(4) L*\n) and The fuzzy incomplete elementary set of attribute DP are then:\nStep 7: q = q+1, and Steps 5–7 are repeated. Until the fuzzy incomplete elementary set of attributes {SP, DP} is found as follows:\nc),0.5}},c)(obj,{(obj\nc),0.1},bjc),0.5}{(o,c)(obj,{(obj\nc),0.16},c)(obj,{(obj\nc),0.2},c)(obj,c)(obj,{(obj\nc),0.1},c)(obj,(obj\nc),c)(obj,{{(obj = DP}U/{SP,\n(6)(5)\n(7)(7)(4)\n(5)(2)\n(6)(5)(3)\n(5)(3)\n(2)(1)\nBecause , It is requirement for next steps .\nStep 8: Since all objects in the fuzzy incomplete lower approximations have become certain, go to the step12 is executed.\nStep 12 : Initialize l=l+1, where l is used to count the number of classes then begin finding β-certain and β-possible rules processed.\nwith the relative degree β = 0.2 of misclassification can be calculated as follows:\nfor XN\n28.0 85.01.03.02.01.09.0 85.09.01= )X(x),SP c( NN =+++++ + −\n:\n1 0-1 )X(x),SP c( NH == 1 0-1 )X(x),SP c( NL ==\n31.0 0.160.30.90.4 0.390.-1 )X(x),DP c( NN =+++ + =\n8.0 0.410.6 40.-1 )X(x),DP c( NH =++ = 10-1 )X(x),DP c( NL ==\n18.0 0.30.160.10.9 0.390.-1 )X(x),DPSP, c( NNN =+++ + = 1 0-1 )X(x),DPSP, c( NHN ==\n55.0 0.40.30.2 4.0-1 )X(x),DPSP, c( NNH =++ = 1 0-1 )X(x),DPSP, c( NLL == 1 0-1 )X(x),DPSP, c( NNL == 1 0-1 )X(x),DPSP, c( NHH ==\nStep 1 3 : Assume β =0.2. The fu zzy β-lower and βupper approximation for class XN=({Obj(1) ,Obj(3) }) is first calculated. Since only {Obj(1) ,Obj(3) } is included in XN\nSP,DP\n, thus:\n*0.2(XN) = {{( Obj(1) ,Obj(2), Obj(3), Obj(5)\nSimilarly, the modified fuzzy β-upper approximations of single attributes for class X\n),0.1}\nN is then calculated . Since only {Obj(1) ,Obj(3) } is included in XN\nSP\n, thus:\n* 0.2(XN) = {( Obj(1) ,Obj(2), Obj(3), Obj(5) ,Obj(6),\nObj(7) ),0.1}\nDP*0.2(XN) = {{( Obj(1) ,Obj(2), Obj(3) ,Obj(5) {( Obj ),0.16} (3) ,Obj(5) ,Obj(6)\nSP,DP ),0.4}} * 0.2(XN) = {{( Obj(3) ,Obj(5) ,Obj(6) ),0.2}\nStep 1 5 : Each partition in the fu zy β-lower approximation is used to derived a β-certain rule with plausibility measure and future effectiveness measure. From SP* 0.2(XN\n1. If Systolic Pressure = Normal and Diastolic Pressure = Normal Then Blood Pressure = Normal , with plausibility=0.82 and future effectiveness = 0.1 .\n), the following rule is derived:\nSimilarly, not exist rules are derived for DP*0.2(XN) and SP,DP*0.2(XN\nStep 16: Since the condition part of this rule is not more specific than any other rule in the β-certain rule sets, it is then kept in the fuzzy β-certain rule set.\n).\nStep 1 7 : Each partition in the fu zy β-upper approximation is used to derived a β-possible rule with plausibility measure and future effectiveness measure. From SP* 0.2(XN\n2. If Systolic Pressure = Normal Then Blood Pressure = Normal , with plausibility=0.72 and future effectiveness = 0.1 .\n), the following rule is derived:\nSimilarly, the following rules are derived for DP*0.2(XN) and SP,DP*0.2(XN\n3. If Diastolic Pressure = Normal Then Blood Pressure = Normal, with plausibility =0.6 and future effectiveness = 0.16.\n) :\n4. If Diastolic Pressure = High Then Blood Pressure =Normal, with plausibility =0.2 and future effectiveness = 0.4. 5. If Systolic Pressure = Normal and Diastolic Pressure = High Then Blood Pressure = Normal , with plausibility =0.45 and future effectiveness = 0.2 .\nStep 18: Since the condition part of this rule is not more specific than any other rule in the β-possible rule sets, it is then k ept in the fu zzy β-possible rule set.\nStep 19: Steps 13–19 are repeated to find rules for classes XL and XH\nSimilarly , perform for X\nuntil l ≤ c .\nL and XH\n6. If Systolic Pressure = Low Then Blood Pressure = Low, with plausibility =1 and future effectiveness = 0.5.\n. thus,\n7. If Diastolic Pressure = Low Then Blood Pressure = Low, with plausibility = 1 and future effectiveness = 1;\n8. If Systolic Pressure =Low and Diastolic Pressure = Low Then Blood Pressure = Low, with plausibility =1 and future effectiveness = 0.5. 9. If Systolic Pressure = Normal and Diastolic Pressure = Low Then Blood Pressure = Low , with plausibility=1 and future effectiveness = 0.1. 10. If Systolic Pressure = High Then Blood Pressure = High, with plausibility =1 and future effectiveness = 0.5. 11. If Diastolic Pressure = High Then Blood Pressure = High, with plausibility = 0.8 and future effectiveness = 0.4; 12. If Systolic Pressure = High and Diastolic Pressure = Normal Then Blood Pressure = High, with plausibility =1 and future effectiveness = 0.16 . 13. If Systolic Pressure = High and Diastolic Pressure = High Then Blood Pressure = High , with plausibility=1 and future effectiveness = 0.5 . 14. If Diastolic Pressure = Normal Then Blood Pressure = High, with plausibility = 0.32 and future effectiveness = 0.16; 15. If Systolic Pressure = Normal and Diastolic Pressure = High Then Blood Pressure = High, with plausibility =0.56 and future effectiveness = 0.2 . 16. If Systolic Pressure = Normal Then Blood Pressure = High , with plausibility=0.25 and future effectiveness = 0.1. Since the condition part of the eightieth rule is more specific and both its plausibility (1) and effectiveness (0.5) is equal to those of the sixth rule, this rule is thus removed from the β-certain rule set. Similarly, rules 9,12 and 13 are also removed by fuzzy rules 7 and 10 .\nStep 20 : all the fuzzy β-certain and β-possible rules are shown below:\nFuzzy β-certain rules :\n1. If Systolic Pressure = Normal and Diastolic Pressure = Normal Then Blood Pressure = Normal , with plausibility=0.82 and future effectiveness = 0.1 . 2. If Systolic Pressure = Low Then Blood Pressure = Low, with plausibility =1 and future effectiveness = 0.5. 3. If Diastolic Pressure = Low Then Blood Pressure = Low, with plausibility = 1 and future effectiveness = 1; 4. If Systolic Pressure = High Then Blood Pressure = High, with plausibility =1 and future effectiveness = 0.5. 5. If Diastolic Pressure = High Then Blood Pressure = High, with plausibility = 0.8 and future effectiveness = 0.4. Fuzzy β-possible rules : 1. If Systolic Pressure = Normal Then Blood Pressure = Normal , with plausibility=0.72 and future effectiveness = 0.1 . 2. If Diastolic Pressure = Normal Then Blood Pressure = Normal, with plausibility =0.6 and future effectiveness = 0.16 . 3. If Diastolic Pressure = High Then Blood Pressure = Normal, with plausibility =0.2 and future effectiveness = 0.4 .\n4. If Systolic Pressure = Normal and Diastolic Pressure = High Then Blood Pressure = Normal , with plausibility =0.45 and future effectiveness = 0.2. 5. If Diastolic Pressure = Normal Then Blood Pressure = High, with plausibility = 0.32 and future effectiveness = 0.16; 6. If Systolic Pressure = Normal and Diastolic Pressure = High Then Blood Pressure = High, with plausibility =0.56 and future effectiveness = 0.2 . 7. If Systolic Pressure = Normal Then Blood Pressure = High , with plausibility=0.25 and future effectiveness = 0.1.\nVII. Discussion and conclusion\nIn this paper, we have proposed a novel data mining algorithm, which can process incomplete quantitative data with a predefined tolerance degree of uncertainty and misclassification. The interaction between data and approximations helps derive βcertain and β-possible rules from fuzzy incomplete data sets and estimate appropriate unknown values. The fuzzy β-certain rules with misclassification degrees smaller than β and the fuzzy β-possible rules with misclassification degrees smaller than 1-β are derived. Noisy training examples (as outliers) may then be omitted. The rules thus mined exhibit fuzzy quantitative regularity in databases and can be used to provide some suggestions to appropriate supervisors. The proposed algorithm can also solve conventional incomplete quantitative data problems by using degraded membership functions. The selection of β values is remarked here. When β =0, the proposed approach will be reduced to the one for the original rough-set model with incomplete quantitative data. A larger β value represents a larger tolerance of uncertainty and misclassification, but with a smaller gap between lower approximations and upper approximations. The selection of an appropriate β value then depends on given training instances.\nAcknowledgement\nThis research was supported by the affiliation must be Islamic Azad University, Arak Branch of Iran.\nReferences\n[1] T.P,Hong, T.T,Wang, S.L,Wang,” Mining fuzzy b-certain and b-possible rules from quantitative data based on the variable precision rough-set mode”, Expert System with Application, 32,pp. 223–232, 2007. [2] L.T,Germano, P,Alexandre ,”Knowledge-base reduction based on rough set techniques”, Canadian conference on electrical and computer engineering , pp. 278–281, 1996. [3] I.Graham, P.L,Jones ,”Expert systems—knowledge ,uncertainty and decision”, Boston: Chapman and Computing, pp. 117–158 ,1988.\n[4] J. W,Grzymala-Busse,” Knowledge acquisition under uncertainty: A rough set approach”, Journal of Intelligent Robotic Systems, 1,pp. 3–16, 1988. [5] T. P,Hong,C. S,Kuo,, S. C,Chi, ”Mining association rules from quantitative data”, Intelligent Data Analysis, 3, pp.363–376, 1999. [6] T. P,Hong, C.Y,Lee, ”.Induction of fuzzy rules and membership functions from training examples”, Fuzzy Sets and Systems, 84, pp.33–47, 1996. [7] T. P,Hong, S.S,Tseng, “A generalized version space learning algorithm for noisy and uncertain data”,IEEE Transactions on Knowledge and Data Engineering, 9, pp.336–340,1997 [8] T. P,Hong, T.T,Wang, S.L,Wang,” Knowledge acquisition from quantitative data using the rough-set theory”, Intelligent Data Analysis, 4, pp.289–304, 2000. [9] Y,Kodratoff, R.S,Michalski, “Machine learning: An artificial intelligence artificial intelligence approach”, 3. San Mateo, CA: Morgan Kaufmann Publishers, 1983 . [10] P.J,Lingras, Y.Y,Yao,“Data mining using extensions of the rough set model”, Journal of the American Society for Information Science, 49(5), pp.415–422, 1998. [11] T.P,Hong, L.H,Tseng, S.L,Wang,”Learning rules from incomplete training examples by rough sets”, Expert System with Application, 22,pp. 285–293, 2002. [12] R.S,Michalski, J.G, Carbonell, T.M,Mitchell,”Machine Learning: An Artificial Intelligence Approach 1”, Los Altos, CA: Morgan Kaufmann Publishers, 1983. [13] R.S,Michalski,J.G, Carbonell, T.M,Mitchell,“Machine learning: An artificial intelligence approach 2”, Los Altos, CA: Morgan Kaufmann Publishers,1983. [14] E,Orlowska,“Reasoning with incomplete information: rough set based information logics”, In V. Alagar, S. Bergler, & F. Q. Dong (Eds.), Incompleteness and uncertainty in information systems, pp. 16–33,1993. [15] Y,Yuan, Y., & M.J,Shaw, “Induction of fuzzy decision trees”, Fuzzy Sets and Systems, 69, pp.125–139,1995. [16] N,Zhong, J.Z,Dong, S,Ohsuga, T.Y,Lin,” An incremental, probabilistic rough set approach to rule discovery”, IEEE International Conference on Fuzzy Systems, 2,pp. 933–938,1998. [17] W,Ziarko, “Variable precision rough set model”, Journal of Computer and System Sciences, 46, pp.39–59,1993. [18] T.P,Hong, L.H,Tseng, B.C,Chien,“Mining from incomplete quantitative data by fuzzy rough sets”, Expert System with Application, 37, pp.2644–2653,2010. [19] Z. Pawlak, “Rough set,” International Journal of Computer and Information Sciences, pp.341-356,1983. [20] M. Kryszkiewicz, “Rough set approach to incomplete informationsystems”, Information Science, Vol. 112, pp. 39- 49,1998. [21] M. R,Chmielewski, J. W,Grzymala-Busse, N. W,Peterson, S,Than,“The rule induction system LERS – a version for sonal computers”, Foundations of Computing and Decision Sciences, Vol.18, pp. 181-212,1993. [22] J,Liang , Z,Xu ,”Uncertainty measures of roughness of knowledge and rough sets in incomplete information systems”, The third world congress on intelligent control and automation , pp.2526-2529, 2000 ."
    } ],
    "references" : [ {
      "title" : "Mining fuzzy b-certain and b-possible rules from quantitative data based on the variable precision rough-set mode",
      "author" : [ "T.P", "Hong", "T.T", "Wang", "S.L" ],
      "venue" : "Expert System with Application,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2007
    }, {
      "title" : "”Knowledge-base reduction based on rough set techniques”, Canadian conference on electrical and computer engineering",
      "author" : [ "L.T", "P Germano", "Alexandre" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1996
    }, {
      "title" : "”Expert systems—knowledge ,uncertainty and decision",
      "author" : [ "I.Graham", "P.L", "Jones" ],
      "venue" : "Boston: Chapman and Computing,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1988
    }, {
      "title" : "Knowledge acquisition under uncertainty: A rough set approach",
      "author" : [ "J. W", "Grzymala-Busse" ],
      "venue" : "Journal of Intelligent Robotic Systems,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1988
    }, {
      "title" : "Mining association rules from quantitative data",
      "author" : [ "T. P", "Hong", "C. S", "Kuo", "S. C", "Chi" ],
      "venue" : "Intelligent Data Analysis,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1999
    }, {
      "title" : ".Induction of fuzzy rules and membership functions from training examples",
      "author" : [ "T. P", "Hong", "C.Y", "Lee" ],
      "venue" : "Fuzzy Sets and Systems, 84,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1996
    }, {
      "title" : "A generalized version space learning algorithm for noisy and uncertain data”,IEEE",
      "author" : [ "T. P", "Hong", "S.S", "Tseng" ],
      "venue" : "Transactions on Knowledge and Data Engineering,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1997
    }, {
      "title" : "Knowledge acquisition from quantitative data using the rough-set theory",
      "author" : [ "T. P", "Hong", "T.T", "Wang", "S.L" ],
      "venue" : "Intelligent Data Analysis,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2000
    }, {
      "title" : "Machine learning: An artificial intelligence artificial intelligence approach",
      "author" : [ "Kodratoff", "R.S", "Michalski" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1983
    }, {
      "title" : "S.L,Wang,”Learning rules from incomplete training examples by rough sets",
      "author" : [ "T.P", "Hong", "L.H", "Tseng" ],
      "venue" : "Expert System with Application,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2002
    }, {
      "title" : "Learning: An Artificial Intelligence Approach 1",
      "author" : [ "R.S", "Michalski", "J.G", "Carbonell", "T.M", "Mitchell", "”Machine" ],
      "venue" : "Los Altos, CA: Morgan Kaufmann Publishers,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1983
    }, {
      "title" : "T.M,Mitchell,“Machine learning: An artificial intelligence approach 2",
      "author" : [ "R.S", "Michalski", "J.G", "Carbonell" ],
      "venue" : "Los Altos, CA: Morgan Kaufmann Publishers,1983",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1983
    }, {
      "title" : "Induction of fuzzy decision trees",
      "author" : [ "Y. Yuan", "M.J", "Shaw" ],
      "venue" : "Fuzzy Sets and Systems,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1995
    }, {
      "title" : "An incremental, probabilistic rough set approach to rule discovery",
      "author" : [ "Zhong", "J.Z", "S Dong", "Ohsuga", "T.Y", "Lin" ],
      "venue" : "IEEE International Conference on Fuzzy Systems,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1998
    }, {
      "title" : "B.C,Chien,“Mining from incomplete quantitative data by fuzzy rough sets",
      "author" : [ "T.P", "Hong", "L.H", "Tseng" ],
      "venue" : "Expert System with Application,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2010
    }, {
      "title" : "Rough set",
      "author" : [ "Z. Pawlak" ],
      "venue" : "International Journal of Computer and Information Sciences, pp.341-356,1983.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 1983
    }, {
      "title" : "Rough set approach to incomplete informationsystems",
      "author" : [ "M. Kryszkiewicz" ],
      "venue" : "Information Science,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1998
    }, {
      "title" : "S,Than,“The rule induction system LERS – a version for sonal computers",
      "author" : [ "M. R", "Chmielewski", "J. W", "Grzymala-Busse", "N. W", "Peterson" ],
      "venue" : "Foundations of Computing and Decision Sciences,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1993
    }, {
      "title" : "”Uncertainty measures of roughness of knowledge and rough sets in incomplete information systems”, The third world congress on intelligent control and automation",
      "author" : [ "Z Liang", "Xu" ],
      "venue" : null,
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2000
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "Among these approaches, deriving inference or association rules from training examples is the most common [9,12,13].",
      "startOffset" : 106,
      "endOffset" : 115
    }, {
      "referenceID" : 10,
      "context" : "Among these approaches, deriving inference or association rules from training examples is the most common [9,12,13].",
      "startOffset" : 106,
      "endOffset" : 115
    }, {
      "referenceID" : 11,
      "context" : "Among these approaches, deriving inference or association rules from training examples is the most common [9,12,13].",
      "startOffset" : 106,
      "endOffset" : 115
    }, {
      "referenceID" : 5,
      "context" : "Given a set of examples and counterexamples of a concept, the learning program tries to induce general rules that describe all or most of the positive training instances and none or few of the counterexamples [6,7].",
      "startOffset" : 209,
      "endOffset" : 214
    }, {
      "referenceID" : 6,
      "context" : "Given a set of examples and counterexamples of a concept, the learning program tries to induce general rules that describe all or most of the positive training instances and none or few of the counterexamples [6,7].",
      "startOffset" : 209,
      "endOffset" : 214
    }, {
      "referenceID" : 3,
      "context" : "Recently, the rough-set theory has been used in reasoning and knowledge acquisition for expert systems [4,14].",
      "startOffset" : 103,
      "endOffset" : 109
    }, {
      "referenceID" : 15,
      "context" : "It was proposed by Pawlak in 1982 [19], with the concept of equivalence classes as its basic principle.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 14,
      "context" : "Examples are [14,18] reasoning with incomplete information,[2] knowledge-base reduction, [10] data mining, [16] rule discovery.",
      "startOffset" : 13,
      "endOffset" : 20
    }, {
      "referenceID" : 1,
      "context" : "Examples are [14,18] reasoning with incomplete information,[2] knowledge-base reduction, [10] data mining, [16] rule discovery.",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 13,
      "context" : "Examples are [14,18] reasoning with incomplete information,[2] knowledge-base reduction, [10] data mining, [16] rule discovery.",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 5,
      "context" : "[6,17] mentioned that the main issue in the rough-set approach was the formation of good rules.",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 15,
      "context" : "The rough-set theory, proposed by Pawlak in 1982, can serve as a new mathematical tool to deal with data classification problems [19].",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 6,
      "context" : "The original rough-set model is quite sensitive to noisy data [7].",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 17,
      "context" : "In the past, several methods were proposed to handle the problem of incomplete data sets [21].",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 16,
      "context" : "Incomplete data sets may also be directly processed in a particular way to get the rules [20,22].",
      "startOffset" : 89,
      "endOffset" : 96
    }, {
      "referenceID" : 18,
      "context" : "Incomplete data sets may also be directly processed in a particular way to get the rules [20,22].",
      "startOffset" : 89,
      "endOffset" : 96
    }, {
      "referenceID" : 16,
      "context" : "In the [20] proposed a rough-set approach to directly learn rules from incomplete data sets without guessing unknown attribute values.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 18,
      "context" : "In the [22] modified [20] approach by introducing the rough entropy to distinguish the power of the attribute subsets that have the same partition for similarity relations.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 16,
      "context" : "In the [22] modified [20] approach by introducing the rough entropy to distinguish the power of the attribute subsets that have the same partition for similarity relations.",
      "startOffset" : 21,
      "endOffset" : 25
    } ],
    "year" : 2012,
    "abstractText" : "The rough-set theory proposed by Pawlak, has been widely used in dealing with data classification problems. The original rough-set model is, however, quite sensitive to noisy data. Tzung thus proposed deals with the problem of producing a set of fuzzy certain and fuzzy possible rules from quantitative data with a predefined tolerance degree of uncertainty and misclassification. This model allowed , which combines the variable precision rough-set model and the fuzzy set theory, is thus proposed to solve this problem. This paper thus deals with the problem of producing a set of fuzzy certain and fuzzy possible rules from incomplete quantitative data with a predefined tolerance degree of uncertainty and misclassification. A new method, incomplete quantitative data for rough-set model and the fuzzy set theory, is thus proposed to solve this problem. It first transforms each quantitative value into a fuzzy set of linguistic terms using membership functions and then finding incomplete quantitative data with lower and the fuzzy upper approximations. It second calculates the fuzzy β-lower and the fuzzy β-upper approximations. The certain and possible rules are then generated based on these fuzzy approximations. These rules can then be used to classify unknown objects. Keywords-component: Fuzzy set; Rough set; Data mining; Certain rule; Possible rule; Quantitative value; incomplete data",
    "creator" : "Acrobat PDFMaker 9.0 for Word"
  }
}