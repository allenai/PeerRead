{
  "name" : "1312.0512.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "dingwc@bu.edu", "pi@bu.edu", "srv@bu.edu", "wckarl@bu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n31 2.\n05 12\nv2 [\ncs .L\nG ]\n1 3\nM ar\n2 01\nIndex Terms— Sensing model, Kernel method, SVM, Bag of Words, Supervised Classification"
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "This paper presents a new provably optimum method for designing kernels for SVMs that explicitly incorporates information about the structure of the underlying data generating process. We consider the typical classification task where the observed data point x ∈ X and label y follow some joint distribution p(x, y). In many real world problems, however, observations x are only indirectly related through some generative process to latent variables z ∈ Z via a sensing model p(x|z) and the latent variables have different distributions conditioned on the underlying label y. An example of such a situation is medical tomography, where the acquired data x consists of X-ray based projections but the diagnosis is done on reconstructed cross-sections of the body z to determine the presence or absence of a disease y. Another example is the classic “Bag-of-Word” (BoW) modeling paradigm widely-used to model text documents, images, etc. [1][2], where the observed document x is modeled as a collection of iid drawings of words from a latent document-specific distribution z over the vocabulary. This z is, in turn, related to the document category y through an unknown p(z|y).\nThis article is based upon work supported by the U.S. AFOSR and the U.S. NSF under award numbers ♯FA9550-10-1-0458 (subaward ♯A1795) and ♯1218992 respectively.\nFor supervised classification, a generative approach would make further assumptions on p(z|y) which may either not hold or lead to an intractable posterior inference problem [3]. On the other hand, the classical distribution-free discriminative paradigm would ignore knowledge of the sensing model p(x|z) and build classifiers using labeled training data [4, 5]. Yet another approach is a decoupled two-step process where the latent states are first estimated as ẑ(x) using the sensing model and then a classifier is built based on (ẑ, y) pairs. Computing estimates, however, can be costly and can introduce confounding artifacts when data is limited or noisy, and may be unnecessary if the ultimate goal is a simple decision.\nMotivated by such problems, we start by examining the structure of the Bayes-optimum classifier, focusing on binary classification, and then connect it to kernel SVMs via the max-margin principle. Our proposed “sensing-aware” kerneldesign arises as the natural consequence of such a procedure. We illustrate the approach on the BoW model and demonstrate its merits on document and image classification tasks."
    }, {
      "heading" : "2. OPTIMUM SENSING-AWARE KERNEL",
      "text" : "Let y ∈ {−1,+1}, z ∈ Z , and x ∈ X denote the class label, latent variable, and observation respectively. x and y are independent conditioned on z and the sensing model p(x|z) is known. We make the following technical assumptions on the joint distribution which are satisfied in many applications. Assumption 1: (Square-integrability)∀x, y, p(x|z), p(z, y) ∈ L2(Z) as functions of z. Assumption 2: (Uniform boundedness) There exists R ∈ [0,∞) such that supx,z |p(x|z)| ≤ R. Further, let 〈f, g〉Z := ∫\nZ f(z)g(z)dz denote the usual inner\nproduct in L2(Z) which induces the norm ‖f‖2 Z , 〈f, f〉Z . Let ‖f‖1 , ∫ Z |f(z)|dz denote the ℓ1-norm in L2(Z).\nWhen full knowledge of the generative model p(x, y) is available, it is well known that the Maximum Aposteriori Probability (MAP) estimator of the class label minimizes the error probability, i.e., the Bayes risk. For binary classification, this reduces to a simple likelihood ratio test (LRT):\np(y = 1|x) 1 ≷ −1 p(y = −1|x) (1)\nUsing p(x, z, y) = p(x|z)p(z, y) and marginalizing over z the LRT (1) reduces to,\n〈p(x|z), w(z)〉Z 1\n≷ −1 0 (2)\nwhere w(z) := p(z, y = 1)− p(z, y = −1). Note that w(z) satisfies the following constraint\n‖w(z)‖1 ≤ ‖p(z, y = 1)‖1 + ‖p(z, y = −1)‖1 = 1.\nTheorem: If we interpret p(x|z) and w(z) as vectors (functions of z with x held fixed) in L2(Z), then the Bayes-optimal rule (2) is a linear classifier in L2(Z) defined by the separating hyperplane w(z).\nWhen a set of training samples {xi, yi}Mi=1 that are generated from p(x, y) in an iid fashion are given, and w(z) is unknown, the popular max-margin principle advocates using the hyperplane w∗(z) that maximizes the separation, i.e., a (soft) margin, between the two classes. This max-margin hyperplane w∗(z) is the optimal solution of the following (infinite-dimensional) constrained optimization problem\nw∗(z) = argmin w(z)∈L2(Z),ξi\n1 2 ‖w(z)‖2Z + C\nN ∑\ni=1\nξi\nsuch that yi〈p(xi|z), w(z)〉 ≥ 1− ξi, ξi ≥ 0\n‖w(z)‖1 ≤ 1 (3)\nAnd the optimum classifier is then given by\nf∗(x) = sign (〈p(x|z), w∗(z)〉) .\nIf we write w∗(z) = M ∑\ni=1\nβip(x i|z) + w⊥(z), where w⊥ is\northogonal to the linear span of {p(xi|z)}Mi=1, and ignore the constraint ‖w(z)‖ ≤ 1, the optimization problem (3) reduces to\nmin βi,ξi,w⊥\n1\n2\nM ∑\ni,j=1\nβiβjK(x i,xj) + ‖w⊥‖22 + C\nN ∑\ni=1\nξi\nsuch that yi M ∑\nj=1\nβjK(x i,xj) ≥ 1− ξi, ξi ≥ 0\n(4) where\nK(x,x′) , 〈p(x|z), p(x′|z)〉 = ∫\nZ\np(x|z)p(x′|z)dz. (5)\nNote that the optimal solution of (4) has w⊥ = 0 making it a finite-dimensional constrained optimization problem. Equation (4) is, in fact, a kernel SVM problem with the kernel defined by equation (5). We will refer to (5) as the “sensingaware kernel” in what follows. The optimal classifier f∗(x) corresponding to the solution to 4 has the following form\nf(x) = sign\n(\nn ∑\ni=1\nβ∗i K(x,x i)\n)\n. (6)\nThe sensing-aware kernel thus provides a principled way to incorporate the partial information about the generative model p(x|z). For a wide range of p(x|z), K(x,x′) has a\nclosed form expression and can be calculated efficiently. In this paper, we focus on the BoW generative model.\nRemark 1: If t(x) is a sufficient statistic for z, i.e., x − t(x) − z is a Markov chain, then t(x) can replace x in the above analysis and the resulting kernel SVM will have the kernel K(x,x′) = ∫\nZ p(t(x)|z)p(t(x′)|z)dz.\nRemark 2: There is a vast literature dedicated to kernel methods for classification[5]. While the standard RBF kernel works well in many problems, various kernels have been designed for specific tasks. Among them, a line of work on model-based kernels is related to our approach in the sense that knowledge of a generative model is utilized. The Probability Product Kernel (PPK) proposed in [6] first estimates latent variable ẑi using observation xi and defines the kernel in terms of the estimate as K(xi,xj) := ∫\nX p(x|ẑi)p(x|ẑj)dx.\nHeat or Diffusion kernels (Diff) that exploit the Fisher information metric on the probability manifold were proposed in [7]. Kernels based on the KL divergence were proposed in [8]. Although each approach has its own strength, unlike ours, the aforementioned probabilistic kernels are not designed to directly minimize the classification error. Moreover, they require full model knowledge which is unreasonable for large and complex datasets like text or images."
    }, {
      "heading" : "3. KERNEL FOR BAG OF WORDS MODELS",
      "text" : "In the “Bag of words” (BoW), or “Bag of Features” (BoF) model, there is a collection of documents composed of words from a vocabulary of size W . Each document is modeled as being generated by N i.i.d drawings of words from a latent W × 1 document word-distribution vector z. Since the ordering is ignored, a document is represented as a W×1 empirical word-count vector x = (x1, . . . , xW )T . Therefore, p(x|z) is a multinomial distribution\np(x|z) =\n(\nN\nx1, . . . , xW\n) W ∏\nw=1\nzxww\nFrom this it can be shown that the sensing kernel for two documents xi,xj defined in 5 has the following form,\nK(xi,xj) =\nW ∏\nw=1\nΓ(xiw + x j w + 1)\nΓ(xiw + 1)Γ(x j w + 1)\nΓ(Ni + 1)Γ(Nj + 1)\nΓ(Ni +Nj +W )\n= W ∏\nw=1\n(xiw + x j w)! (xiw)!(x j w)!\nNi!Nj!\n(Ni +Nj +W − 1)!\n(7) where Ni is the number of words in the i-th document and Γ(t) is the ordinary Gamma function. One practical concern is the size of the factorials. Typically, W is a large constant and Ni varies across documents. Therefore, the product of factorials will have a large dynamic range and lead to overflow errors (even for small W ). The product is sensitive to the differences in the Ni values across documents. These issues can be addressed by using one of the following approximations to the original kernel 7.\n• Sensing 0 kernel: K0(xi,xj) := log(K(xi,xj)).\n• Sensing 1 kernel: Following (7),\nK1n(x i,xj) := log\n{\nW ∏\nw=1\nΓ(nx̄iw + nx̄ j w + 1)\nΓ(nx̄iw + 1)Γ(nx̄ j w + 1)\n}\nwhere x̄i := xi/Ni is the word-frequency (normalized word-count) and n is set to be some constant.\n• Sensing 2 kernel: K2N(x i,xj) = log(K(x̂iN , x̂ j N ))\nwhere x̂iN are the words counts obtained by randomly resampling the i-th document, uniformly, N times.\nIn general, these approximations balance the number of words of documents and avoid numerical issues by a log-mapping that compresses the dynamic range. In typical experimental settings, we have found that these approximations yield similar performance."
    }, {
      "heading" : "4. DOCUMENT CLASSIFICATION",
      "text" : "In this section, we consider the problem of text document classification. In this application context, the terms “word”, “vocabulary”, and “document” have their natural meanings. We selected the 20 Newsgourps dataset which contains 18774 news postings from 20 newsgroups (classes) to test performance. The average number of words per document in this dataset is 117. Following standard practice [9], we removed a standard list of stop words from the vocabulary."
    }, {
      "heading" : "4.1. Binary Classification",
      "text" : "We chose to distinguish postings in the newsgroup alt.atheism from talk.religion.misc. This is a difficult task due to the similarity of content in these two groups [3, 10]. The training set contains 856 documents and the test set contains 569 documents. The average number of words per document is 132.\nWe used LIBSVM [11] to train our kernel SVMs. We report results only for the Sensing 1 and Sensing 2 approximations of our sensing-aware kernel since in this dataset, the number of words Ni varies significantly across documents. Table 1 compares the Correct Classification Rate (CCR %) of the proposed sensing-aware kernels against two modelbased kernels, PPK and Diff, and the baseline RBF kernel. Table 1 also shows results for a discriminative Latent Dirichlet Allocation method (denoted by DiscLDA) [3] and a recent method that is based on a max-margin supervised topic model (denoted by G-MedLDA) [10]. Both of these methods posit more complex models and represent the current stateof-the-art. The free model parameters such as n and N in\nsensing kernels 1 and 2, t in Diff, and σ in the RBF kernel, were tuned using a 5 fold cross-validation. The CCR for the Gibbs MedLDA method is for the best number of topics k as in [10]. Words in the test set that were not in the training set were dropped.\nWe can see that the proposed Sensing 1 and 2 kernels outperform the RBF kernel and the model-based kernels PPK and Diff. Surprisingly, the performance of our very simple method is better than that of DiscLDA and is almost the same as that of G-MedLDA (with only 1 less document correctly classified). These state-of-the-art methods make use of fairly complex models and require sophisticated inference algorithms while our sensing-aware kernel SVM makes minimal modeling assumptions and has much lower time complexity."
    }, {
      "heading" : "4.2. Multi-class Classification",
      "text" : "We next studied multi-class classification in the 20 Newsgroups dataset with all 20 classes. We adopted a widely used training/test split where the training set consists of 11,269 documents, and the test set consists of 7,505 documents. We used the “one-versus-all” strategy following [10] to do multiclass classification with binary classifiers. We followed the same settings as in the binary case. For the Sensing 1 and 2 kernels, we used n = N = 150. Table 2 shows the CCRs for the proposed sensing-aware kernels, the RBF baseline, the two model-based kernels PPK and Diff, and the G-MedLDA [10] algorithm. Also shown is the CCR for a recently developed deep learning method based on a Restricted Boltzmann Machine (RBM) [12] which outperforms the standard RBF SVM. Deep learning has recently emerged as a powerful generic approach and has attained state-of-the-art performance in many applications. Since [12] uses the same training settings, we simply quote the results reported in that paper. Observe that the approximate Sensing kernels 1 and 2\noutperform all other kernel SVMs and the RBM. As in binary classification, their performance is close to the G-MedLDA method that uses complex models."
    }, {
      "heading" : "5. IMAGE CLASSIFICATION",
      "text" : "In this section, we consider the problem of recognizing image scene categories. We use the Natural Scene category dataset first introduced in [2, 13]. This dataset consists of 15 scene categories, e.g., office, street view, forests, etc., with 200-400 grayscale images in each category (total 4485 images) and an average image size of 300× 250 pixels."
    }, {
      "heading" : "5.1. Modeling Images as Bags of Features",
      "text" : "There is extensive literature on BOF models for images. A typical model consists of 1) local feature extraction, 2) visual-vocabulary construction, and 3) image representation as BoFs. In order to highlight the effect of kernels, we follow the baseline approach proposed in [2, 13, 14]. 1) Local feature extraction. For each image, the SIFT descriptors of all the 16 × 16 image patches centered at grid points spaced 8 pixels apart are computed. 2) Visual vocabulary construction. D patches are randomly selected from training images. W -means clustering is performed over the corresponding D SIFT descriptors. The W mean vectors form a visual vocabulary, labeled as 1, . . . ,W . 3) BoF representation. For each image, the SIFT descriptor of each patch is assigned the label of its nearest neighbor in the visual vocabulary. An image is then a collection of visual words from the vocabulary and the spatial correlation is ignored.\nTo incorporate spatial structure, we use the pyramid matching scheme proposed in [13]. This uses a sequence of coarse-to-fine grids. At level L, an image is split into 2L × 2L non-overlapping cells, and each cell is treated as a separate “document”. Hence, at level L, each image xi is represented as a collection of 22L word-frequency vectors {xiL,c, c = 1, . . . , 2\n2L}, ordered according to the spatial position of the cell. The kernel at level L for two images xi,xj is defined as KL(xi,xj) = ∑22L\nc=1 κ(x i L,c,x j L,c), where κ is\nthe kernel function for two BoF documents. The above kernel is only for a single level. The pyramid kernel K for two images is defined as a weighted sum of single-level kernels: K(xi,xj) = ∑L\nl=0 α lK l(xi,xj). We\nuse the same weighting factors αl as in [13]."
    }, {
      "heading" : "5.2. Experimental Results",
      "text" : "We follow the settings in [13, 2]. We set the vocabulary size as W = 400. We randomly select 100 images per class for training and leave the rest for testing. We repeat experiments 10 times with a randomly selected training/testing split for each run. We use the “one-versus-all” strategy for multi-class classification and report the average CCR over 10 random runs.\nFollowing [13], we consider three setups. “L = 0, single” uses K0 as the kernel for classification. This simply views the entire image as a document. Similarly, “L = 2, single” uses K2 as the kernel for classification. In this case, the image is divided into 22L = 16 regions. “L = 2, Pyramid” uses K =\n∑2 l=0 α lK l as the kernel value. We compare our Sensing 0 and Sensing 2 kernels with the probabilistic model-based PPK and Diff kernels and the standard RBF kernel. We also compare with the classic Spatial Pyramid (SP) kernel of [13]. This kernel is specially crafted for this problem and has good empirical performance in several Computer Vision tasks. We also consider a recent work called Reconfiguration BoW (RBoW), which makes additional modeling assumptions on p(z|y). Finally, we con-\nsider a recently developed deep learning algorithm [15] (denoted by MRF) which achieves state-of-the-art performance on the same task.\nFor the Sensing 2 kernel, we simply set N = 500. As in text classification, all the other free parameters are tuned by a 5-fold cross-validation. We measure performance using the overall classification accuracy (CCR %). The results are summarized in Table 3. Since the settings are identical, we simply quote the results for MRF and RBoW as reported in [15] and [16] respectively.\nWe can see that in the “L = 2, single-level” setting, the proposed Sensing kernel 2 clearly improves over the classic kernel SP and outperforms other kernels (RBF, PPK and Diff) as well as the more complex RBoW method. However in the “L = 0 single-level” setting, the sensing-aware kernels do perform worse than PPK, Diff, and SP. This could be because L = 0 corresponds to the whole image. It has been strongly suggested in the Computer Vision literature that modeling the entire image as a BoF is unrealistic in many datasets [14] [13]. Our proposed sensing-aware approach only makes sense if the data is actually generated from the sensing structure. Our experimental results accord with the fact that BoF is a reasonable model for local regions, but not for the entire image.\nWe also did experiments for the “L = 2, pyramid” setting. We can see that the Sensing 2 kernel achieves almost the same performance as the state-of-the-art SP and MRF. The improvement is not as significant as in the “L = 2 singlelevel” case. This could be because it involves the L = 0 level kernel values, which does not fit the BoF model well."
    }, {
      "heading" : "6. CONCLUSION",
      "text" : "In this paper, we proposed a novel kernel design principle to incorporate partial model information. On two distinct types of data, we showed that with minimal modeling assumptions, the sensing-aware kernel improves upon other standard kernels and handcrafted kernels for specific domains. We also observed that the sensing-aware kernel can match the performance of the state-of-the-art approaches."
    }, {
      "heading" : "7. REFERENCES",
      "text" : "[1] D. Blei, “Probabilistic topic models,” Commun. of the ACM, vol. 55, no. 4, pp. 77–84, 2012.\n[2] Fei-Fei Li and P. Perona, “A bayesian hierarchical model for learning natural scene categories,” in Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05), San Diego, CA, Jun. 2005, pp. 524–531.\n[3] S. Lacoste-Julien, F. Sha, and M. Jordan, “Disclda: Discriminative learning for dimensionality reduction and classification,” in Advances in Neural Information Processing Systems 21(NIPS’09), pp. 897–904. 2009.\n[4] V. N. Vapnik, The nature of statistical learning theory, Springer-Verlag New York, Inc., New York, NY, USA, 1995.\n[5] T. Hastie, R. Tibshirani, and J. Friedman, Elements of Statistical Learning, Springer-Verlag New York, Inc., New York, NY, USA, 2009.\n[6] T. Jebara, R. Kondor, and A. Howard, “Probability product kernels,” J. Mach. Learn. Res., vol. 5, pp. 819–844, Dec. 2004.\n[7] J. Lafferty and G. Lebanon, “Diffusion kernels on statistical manifolds,” J. Mach. Learn. Res., vol. 6, pp. 129–163, Dec. 2005.\n[8] Pedro J. Moreno, Purdy P. Ho, and Nuno Vasconcelos, “A kullback-leibler divergence based kernel for svm classification in multimedia applications,” in Advances in Neural Information Processing Systems 16 (NIPS’03). 2004.\n[9] D. D. Lewis, Y. Yang, T. G. Rose, and F. Li, “Rcv1: A new benchmark collection for text categorization research,” J. Mach. Learn. Res., vol. 5, pp. 361–397, Dec. 2004.\n[10] J. Zhu, N. Chen, H. Perkins, and B. Zhang, “Gibbs maxmargin topic models with fast sampling algorithms,” in the 30th Int. Conf. on Machine Learning (ICML’13), Atlanta, GA, Jun. 2013.\n[11] C. Chang and C. Lin, “LIBSVM: A library for support vector machines,” ACM Transactions on Intelligent Systems and Technology, vol. 2, pp. 27:1–27:27, 2011.\n[12] H. Larochelle, M. Mandel, R. Pascanu, and Y. Bengio, “Learning algorithms for the classification restricted boltzmann machine,” J. Mach. Learn. Res., vol. 13, pp. 643–669, Mar. 2012.\n[13] S. Lazebnik, C. Schmid, and J. Ponce, “Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories,” in Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06), New York, NY, Jun. 2006, pp. 2169–2178.\n[14] K. Grauman and T. Darrell, “The pyramid match kernel: Discriminative classification with sets of image features,” in Proc. the 10th IEEE International Conference on Computer Vision(ICCV’05), Beijing, China, Oct. 2005, pp. 1458–1465.\n[15] M. Ranzato, V. Mnih, J.M. Susskind, and G.E. Hinton, “Modeling natural images using gated mrfs,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 35, no. 9, pp. 2206–2222, 2013.\n[16] Sobhan Naderi Parizi, John Oberlin, and Pedro F. Felzenszwalb, “Reconfigurable models for scene recognition,” in Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2012, pp. 2775–2782."
    } ],
    "references" : [ {
      "title" : "Probabilistic topic models",
      "author" : [ "D. Blei" ],
      "venue" : "Commun. of the ACM, vol. 55, no. 4, pp. 77–84, 2012.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A bayesian hierarchical model for learning natural scene categories",
      "author" : [ "Fei-Fei Li", "P. Perona" ],
      "venue" : "Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05), San Diego, CA, Jun. 2005, pp. 524–531.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Disclda: Discriminative learning for dimensionality reduction and classification",
      "author" : [ "S. Lacoste-Julien", "F. Sha", "M. Jordan" ],
      "venue" : "Advances in Neural Information Processing Systems 21(NIPS’09), pp. 897–904. 2009.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "The nature of statistical learning theory, Springer-Verlag",
      "author" : [ "V.N. Vapnik" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1995
    }, {
      "title" : "Probability product kernels",
      "author" : [ "T. Jebara", "R. Kondor", "A. Howard" ],
      "venue" : "J. Mach. Learn. Res., vol. 5, pp. 819–844, Dec. 2004.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Diffusion kernels on statistical manifolds",
      "author" : [ "J. Lafferty", "G. Lebanon" ],
      "venue" : "J. Mach. Learn. Res., vol. 6, pp. 129–163, Dec. 2005.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "A kullback-leibler divergence based kernel for svm classification in multimedia applications",
      "author" : [ "Pedro J. Moreno", "Purdy P. Ho", "Nuno Vasconcelos" ],
      "venue" : "Advances in Neural Information Processing Systems 16 (NIPS’03). 2004.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Rcv1: A new benchmark collection for text categorization research",
      "author" : [ "D.D. Lewis", "Y. Yang", "T.G. Rose", "F. Li" ],
      "venue" : "J. Mach. Learn. Res., vol. 5, pp. 361–397, Dec. 2004.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Gibbs maxmargin topic models with fast sampling algorithms",
      "author" : [ "J. Zhu", "N. Chen", "H. Perkins", "B. Zhang" ],
      "venue" : "the 30th Int. Conf. on Machine Learning (ICML’13), Atlanta, GA, Jun. 2013.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "LIBSVM: A library for support vector machines",
      "author" : [ "C. Chang", "C. Lin" ],
      "venue" : "ACM Transactions on Intelligent Systems and Technology, vol. 2, pp. 27:1–27:27, 2011.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Learning algorithms for the classification restricted boltzmann machine",
      "author" : [ "H. Larochelle", "M. Mandel", "R. Pascanu", "Y. Bengio" ],
      "venue" : "J. Mach. Learn. Res., vol. 13, pp. 643–669, Mar. 2012.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories",
      "author" : [ "S. Lazebnik", "C. Schmid", "J. Ponce" ],
      "venue" : "Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06), New York, NY, Jun. 2006, pp. 2169–2178.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "The pyramid match kernel: Discriminative classification with sets of image features",
      "author" : [ "K. Grauman", "T. Darrell" ],
      "venue" : "Proc. the 10th IEEE International Conference on Computer Vision(ICCV’05), Beijing, China, Oct. 2005, pp. 1458–1465.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Modeling natural images using gated mrfs",
      "author" : [ "M. Ranzato", "V. Mnih", "J.M. Susskind", "G.E. Hinton" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell., vol. 35, no. 9, pp. 2206–2222, 2013.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Reconfigurable models for scene recognition",
      "author" : [ "Sobhan Naderi Parizi", "John Oberlin", "Pedro F. Felzenszwalb" ],
      "venue" : "Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2012, pp. 2775–2782.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "[1][2], where the observed document x is modeled as a collection of iid drawings of words from a latent document-specific distribution z over the vocabulary.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "[1][2], where the observed document x is modeled as a collection of iid drawings of words from a latent document-specific distribution z over the vocabulary.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 2,
      "context" : "For supervised classification, a generative approach would make further assumptions on p(z|y) which may either not hold or lead to an intractable posterior inference problem [3].",
      "startOffset" : 174,
      "endOffset" : 177
    }, {
      "referenceID" : 3,
      "context" : "On the other hand, the classical distribution-free discriminative paradigm would ignore knowledge of the sensing model p(x|z) and build classifiers using labeled training data [4, 5].",
      "startOffset" : 176,
      "endOffset" : 182
    }, {
      "referenceID" : 4,
      "context" : "The Probability Product Kernel (PPK) proposed in [6] first estimates latent variable ẑ using observation x and defines the kernel in terms of the estimate as K(x,x) := ∫",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 5,
      "context" : "Heat or Diffusion kernels (Diff) that exploit the Fisher information metric on the probability manifold were proposed in [7].",
      "startOffset" : 121,
      "endOffset" : 124
    }, {
      "referenceID" : 6,
      "context" : "Kernels based on the KL divergence were proposed in [8].",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 7,
      "context" : "Following standard practice [9], we removed a standard list of stop words from the vocabulary.",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 2,
      "context" : "This is a difficult task due to the similarity of content in these two groups [3, 10].",
      "startOffset" : 78,
      "endOffset" : 85
    }, {
      "referenceID" : 8,
      "context" : "This is a difficult task due to the similarity of content in these two groups [3, 10].",
      "startOffset" : 78,
      "endOffset" : 85
    }, {
      "referenceID" : 2,
      "context" : "35 DiscLDA [3] 83.",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 4,
      "context" : "00 PPK SVM[6] 81.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 8,
      "context" : "02 G-MedLDA [10] 83.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 5,
      "context" : "57 Diff SVM [7] 79.",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 9,
      "context" : "We used LIBSVM [11] to train our kernel SVMs.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 2,
      "context" : "Table 1 also shows results for a discriminative Latent Dirichlet Allocation method (denoted by DiscLDA) [3] and a recent method that is based on a max-margin supervised topic model (denoted by G-MedLDA) [10].",
      "startOffset" : 104,
      "endOffset" : 107
    }, {
      "referenceID" : 8,
      "context" : "Table 1 also shows results for a discriminative Latent Dirichlet Allocation method (denoted by DiscLDA) [3] and a recent method that is based on a max-margin supervised topic model (denoted by G-MedLDA) [10].",
      "startOffset" : 203,
      "endOffset" : 207
    }, {
      "referenceID" : 8,
      "context" : "The CCR for the Gibbs MedLDA method is for the best number of topics k as in [10].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 8,
      "context" : "We used the “one-versus-all” strategy following [10] to do multiclass classification with binary classifiers.",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 8,
      "context" : "Table 2 shows the CCRs for the proposed sensing-aware kernels, the RBF baseline, the two model-based kernels PPK and Diff, and the G-MedLDA [10] algorithm.",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 10,
      "context" : "Also shown is the CCR for a recently developed deep learning method based on a Restricted Boltzmann Machine (RBM) [12] which outperforms the standard RBF SVM.",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 10,
      "context" : "Since [12] uses the same training settings, we simply quote the results reported in that paper.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 10,
      "context" : "5 RBM [12] 76.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 4,
      "context" : "2 PPK SVM[6] 78.",
      "startOffset" : 9,
      "endOffset" : 12
    }, {
      "referenceID" : 8,
      "context" : "2 G-MedLDA [10] 80.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 5,
      "context" : "9 Diff SVM [7] 78.",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 1,
      "context" : "We use the Natural Scene category dataset first introduced in [2, 13].",
      "startOffset" : 62,
      "endOffset" : 69
    }, {
      "referenceID" : 11,
      "context" : "We use the Natural Scene category dataset first introduced in [2, 13].",
      "startOffset" : 62,
      "endOffset" : 69
    }, {
      "referenceID" : 1,
      "context" : "In order to highlight the effect of kernels, we follow the baseline approach proposed in [2, 13, 14].",
      "startOffset" : 89,
      "endOffset" : 100
    }, {
      "referenceID" : 11,
      "context" : "In order to highlight the effect of kernels, we follow the baseline approach proposed in [2, 13, 14].",
      "startOffset" : 89,
      "endOffset" : 100
    }, {
      "referenceID" : 12,
      "context" : "In order to highlight the effect of kernels, we follow the baseline approach proposed in [2, 13, 14].",
      "startOffset" : 89,
      "endOffset" : 100
    }, {
      "referenceID" : 11,
      "context" : "To incorporate spatial structure, we use the pyramid matching scheme proposed in [13].",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 11,
      "context" : "We use the same weighting factors α as in [13].",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 11,
      "context" : "We follow the settings in [13, 2].",
      "startOffset" : 26,
      "endOffset" : 33
    }, {
      "referenceID" : 1,
      "context" : "We follow the settings in [13, 2].",
      "startOffset" : 26,
      "endOffset" : 33
    }, {
      "referenceID" : 11,
      "context" : "Following [13], we consider three setups.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 11,
      "context" : "We also compare with the classic Spatial Pyramid (SP) kernel of [13].",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 13,
      "context" : "Finally, we consider a recently developed deep learning algorithm [15] (denoted by MRF) which achieves state-of-the-art performance on the same task.",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 13,
      "context" : "Since the settings are identical, we simply quote the results for MRF and RBoW as reported in [15] and [16] respectively.",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 14,
      "context" : "Since the settings are identical, we simply quote the results for MRF and RBoW as reported in [15] and [16] respectively.",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 4,
      "context" : "3 PPK [6] 73.",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 5,
      "context" : "6 Diff [7] 74.",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 11,
      "context" : "5 SP [13] 74.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 14,
      "context" : "4 RBOW [16] N/A 78.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 13,
      "context" : "MRF [15] 81.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 12,
      "context" : "It has been strongly suggested in the Computer Vision literature that modeling the entire image as a BoF is unrealistic in many datasets [14] [13].",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 11,
      "context" : "It has been strongly suggested in the Computer Vision literature that modeling the entire image as a BoF is unrealistic in many datasets [14] [13].",
      "startOffset" : 142,
      "endOffset" : 146
    } ],
    "year" : 2014,
    "abstractText" : "We propose a novel approach for designing kernels for support vector machines (SVMs) when the class label is linked to the observation through a latent state and the likelihood function of the observation given the state (the sensing model) is available. We show that the Bayes-optimum decision boundary is a hyperplane under a mapping defined by the likelihood function. Combining this with the maximum margin principle yields kernels for SVMs that leverage knowledge of the sensing model in an optimal way. We derive the optimum kernel for the bag-of-words (BoWs) sensing model and demonstrate its superior performance over other kernels in document and image classification tasks. These results indicate that such optimum sensing-aware kernel SVMs can match the performance of rather sophisticated state-of-the-art approaches.",
    "creator" : "dvips(k) 5.991 Copyright 2011 Radical Eye Software"
  }
}