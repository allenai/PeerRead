{
  "name" : "1606.01219.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning Stylometric Representations for Authorship Analysis",
    "authors" : [ "STEVEN H. H. DING", "BENJAMIN C. M. FUNG", "FARKHUND IQBAL", "WILLIAM K. CHEUNG" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "0 Learning Stylometric Representations for Authorship Analysis\nSTEVEN H. H. DING, School of Information Studies, McGill University, Canada BENJAMIN C. M. FUNG, School of Information Studies, McGill University, Canada FARKHUND IQBAL, College of Technological Innovation, Zayed University, UAE WILLIAM K. CHEUNG, Department of Computer Science, Hong Kong Baptist University, Hong Kong\nAuthorship analysis (AA) is the study of unveiling the hidden properties of authors from a body of exponentially exploding textual data. It extracts an author’s identity and sociolinguistic characteristics based on the reflected writing styles in the text. It is an essential process for various areas, such as cybercrime investigation, psycholinguistics, political socialization, etc. However, most of the previous techniques critically depend on the manual feature engineering process. Consequently, the choice of feature set has been shown to be scenario- or dataset-dependent. In this paper, to mimic the human sentence composition process using a neural network approach, we propose to incorporate different categories of linguistic features into distributed representation of words in order to learn simultaneously the writing style representations based on unlabeled texts for authorship analysis. In particular, the proposed models allow topical, lexical, syntactical, and character-level feature vectors of each document to be extracted as stylometrics. We evaluate the performance of our approach on the problems of authorship characterization and authorship verification with the Twitter, novel, and essay datasets. The experiments suggest that our proposed text representation outperforms the bag-of-lexical-n-grams, Latent Dirichlet Allocation, Latent Semantic Analysis, PVDM, PVDBOW, and word2vec representations.\nCategories and Subject Descriptors: K.4.1 [Computers and Society]: Public Policy Issues—Abuse and crime involving computers; I.7.5 [Document and Text Processing]: Document Capture—Document analysis; I.2.7 [Natural Language Processing]: Text analysis\nGeneral Terms: Design, Algorithms, Experimentation\nAdditional Key Words and Phrases: Authorship analysis, computational linguistics, feature learning, text mining"
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "The prevalence of the computer information system, personal computational devices, and the globalizing Internet have fundamentally transformed our daily lives and reshaped the way we generate and digest information. Countless pieces of textual snippets and documents are generated every millisecond: This is the era of infobesity. Authorship analysis (AA) is one of the critical approaches to turn the burden of a vast amount of data into practical, useful knowledge. By looking into the reflected linguistic trails, AA is a study to unveil an underlying author’s identity and sociolinguistic characteristics. The advancement of authorship analysis backed up by stylometric techniques has a fundamental impact on various areas:\n— Cybercrime investigation. The distributed nature of cyber space provides an ideal anonymous channel for computer-mediated malicious activities, e.g., phishing scams, spamming, ransom messages, harassment, money laundering, illegal material distribution, etc., because the network-based origins such as IP address can be easily repudiated. Several authorship identification techniques have been developed for the purpose of cyber investigation on SMS text-messaging slips [Ragel et al. 2014], personal e-mails [Iqbal et al. 2013; Ding et al. 2015], and social blogs [Yang and Chow 2014; Stolerman et al. 2014]. Stylometric techniques have been used as evidence in the form of expert knowledge in the courts of the UK, the US, and Australia [Juola 2006; Brennan et al. 2012]. In a well-known case in the UK, linguistic experts showed\nAuthors’ addresses: Benjamin C. M. Fung (corresponding author), School of Information Studies, McGill University, Montreal, QC, Canada H3A 1X1; email: ben.fung@mcgill.ca\nar X\niv :1\n60 6.\n01 21\n9v 1\n[ cs\n.C L\n] 3\nJ un\n2 01\n6\nthat a series of text messages sent from Danielle’s phone after she had disappeared were actually not written by her but by her uncle, Stuart Campbell [Grant 2008]. For more recent information and definitions about linguistic evidence, please refer to the home page of the Institute for Linguistic Evidence (ILE)1.\n— Marketing, political socialization, and social network analysis. By inferring the hidden attributes of social network users, businesses and political parties can get a deeper understanding of their audiences and the related discussions. AA techniques have been applied to understand audiences’ political preferences [Golbeck and Hansen 2014; Pennacchiotti and Popescu 2011; Golbeck and Hansen 2011], networks of political communication [Conover et al. 2011b], and political learnings of prominent tweeters and media [Wong et al. 2013]. Understanding the audiences on social networks helps in planning political strategy [Conover et al. 2011a] and corrects bias for predicting election results [Wong et al. 2013]. From the perspective of marketing, it is profitable to identify potential customers by characterizing them and understanding their reviews [Weren et al. 2014]. Pennacchiotti and Popescu [2011] present an AA approach to distinguish whether or not a Twitter user is a fan of Starbucks, which in general can be applied to other types of businesses. Moreover, most of social network analyses are based on some attributes of the user inside the network, and AA techniques can provide more hidden labels for these analyses [Cohen and Ruths 2013].\n— Literary science and education. Authorship analysis also has a significant impact in the fields related to literary science. Many AA techniques have been developed to infer the disputed authorship of historical documents such as Civil War letters [Klarreich 2003], Shakespeare’s plays [Klarreich 2003], the Federalist Papers [Oakes 2004; Tschuggnall and Specht 2014; Nasir et al. 2014], and the classic French literary mystery: “Le Roman de Violette” [Boukhaled and Ganascia 2014]. AA techniques are also used to quantify the performance of literary translators, since the best translators will not have their own writing style reflected in the translated works [Almishari et al. 2014]. Moreover, AA techniques have been used to understand the personality of students [Almishari et al. 2014], first languages [Almishari et al. 2014; Torney et al. 2012], and self-reported names [Liu and Ruths 2013]. Furthermore, AA techniques are fundamental to the detection of plagiarism in academic works [Cerra et al. 2014; Hansen et al. 2014].\nStudies of authorship analysis backed up by computational stylometric techniques can be dated back to the 19th century. Many customized approaches focusing on different sub-problems and scenarios have been proposed [Stamatatos 2009]. It has been a successful line of research [Brennan et al. 2012]. Research problems in authorship analysis can be broadly categorized into three types: authorship identification (i.e., identify the most plausible author given a set of candidates [Iqbal et al. 2013; Ding et al. 2015]), authorship verification (i.e., verify whether or not a given candidate is the actual author of the given text [Stamatatos et al. 2014]), and authorship characterization (i.e., infer the sociolinguistic characteristics of the author of the given text [Rangel et al. 2014]). Both the problems of authorship identification and authorship characterization can be formulated as a one-class text classification problem. For the authorship attribution problem, the classification label is the identity of the anonymous text snippet; for the authorship attribution problem, the label can be the hidden properties of the anonymous author, such as age and gender. There exists other variances of the authorship attribution problems, such as the open-set problem, the closed-set problem, and the attribution-with-probability-output problem [Stamatatos 2009; Iqbal et al. 2013; Ding et al. 2015].\n1Institute for Linguistic Evidence (ILE): http://linguisticevidence.org/\nRegardless of the studied authorship problems, the existing solutions in previous AA studies typically consist of three major processes, as shown in the upper flowchart of Figure 1): the feature engineering process, the solution design process, and the experimental evaluation process. In the first, a set of features are manually chosen by the researchers to represent each unit of textual data as a numeric vector. In the second process, a classification model is carefully adopted or designed. At the end, the entire solution is evaluated based on the specific datasets. Representative solutions are Burger et al. [2011], Nirkhi and Dharaskar [2014], and Cavalcante et al. [2014]. Exceptions are few recent applications of the topic models that actually combine these two process into one [Pratanwanich and Liò 2014; Savoy 2013a; Seroussi et al. 2014]. Still, the two-processes-based studies on authorship analysis problems dominate [Rangel et al. 2014].\nDuring the feature engineering process, given the available dataset and application scenario, authorship analysts manually select a broad set of features based on the hypotheses or educated guesses, and then refine the selection based on the experimental feedback. As demonstrated by previous research [Savoy 2012; Zamani et al. 2014a; Savoy 2013b; Ding et al. 2015], the choice of the feature set (i.e., the feature selection method) is a crucial indicator of the prediction result, and it requires explicit knowledge in computational linguistics and tacit experiences in analyzing the textual data. Manual feature engineering is a time-consuming and labor-intensive task. The hand-crafted feature representations have very limited generalizability on different data and scenarios. We have shown that the amount of data and the complexity of the problem to be solved have a strong implication on the analysis result [Ding et al. 2015] even when a full set of n-grams is employed. Thus, the generalizability and sensitivity to different application scenarios is critical.\nManual feature engineering to a high degree limits the potentials of the whole available feature space because it makes a strong simplification on the language model. For example, the bag of words (BoW) model, in which the text is represented as an unordered list of words with their respective frequency value, ignores the dependency between words and the sequential information. The lexical n-gram model describes a feature as a consecutive n lexical token; it can capture the co-occurrence relationship within an n-length neighbor. Still, it is unable to capture the contextual relationship between words over long sentences. Also, most feature selection methods trim the original space based on a specific metric, for example, by picking the top-k frequent lexical n-gram. In our previous study [Ding et al. 2015], we showed that using a full set of n-gram features greatly promotes the accuracy of authorship identification, which indicates the reduced classification power of feature selection for authorship analysis.\nInspired by the recent development of the unsupervised representation learning in deep learning [Mikolov et al. 2013], we raise two new research questions for authorship analysis. (1) Given the unlabeled textual data, can we automatically come up with a vectorized numeric representation of the writing style? (2) Can we contribute to the AA field by discovering and generalizing new, special, and interpretable linguistic features from the available data instead of validating the educated guess and hypotheses on the data?\nIn this paper, we present a stylometric representation learning approach for authorship analysis (AA). Refer to the lower flowchart in Figure 1. The goal is to learn an effective vector representation of writing styles of different linguistic modalities to mitigate the aforementioned issue in AA study. Following the previous work [Solorio et al. 2011; Sapkota et al. 2013; Ding et al. 2015], we use the concept linguistic modalities to denote the categories of linguistic features [Solorio et al. 2011]. We broadly categorize them into four modalities: the topical modality, the lexical modality, the character-level modality, and the syntactic modality. It is noted that the term “modality” used here is different from the term “multi-modality” in machine learning. The first one denotes a category of linguistic features, and the latter denotes a combination of different ways in which information is presented, such as text, image, rating, etc. In this paper, “modality” and “linguistic modality” are used interchangeably to denote the categories of linguistic features. Also, we use the term representation and embedding interchangeably to describe the vectorized representation of feature. In the first stage, we learn the stylometric representation for different linguistic modalities based on the unlabeled textual data. At the second stage, an authorship analyst can select the modality according to his or her needs. If the scenario requires the least interference from the topic-related information, the analyst can discard the topical modality, or, more strictly, both the topical and lexical modality. Such a design inherits the flexibility of the original hand-crafted stylometric features while it enables the learned representation to fit into the available data.\nThe basic idea of our proposed solution is to simulate how people construct a sentence based on different linguistic modalities. The proposed approach follows the recent ideas and models of estimating word embedding [Mikolov et al. 2013] and paragraph embedding [Le and Mikolov 2014] to efficiently approximate the factorization of different co-occurrence matrices. The proposed approach can be applied to any authorship analysis studies that involve feature engineering processes.\nTo the best of our knowledge, this is the very first work attempting to automate the feature engineering process and discover the stylometric representations for authorship analysis. Specifically, our major contributions are summarized as follows:\n— We propose a different solution flow for authorship analysis. Instead of manually engineering the stylometric features, we learn the representation of the writing style\nbased on the available unlabeled texts according to different linguistic modalities. The user/researcher can pick the modalities based on their needs and interests in the context of the authorship analysis problem. For example, political socialization researchers are interested in content, so they may choose topic-modality. In contrast, cybercrime investigators would prefer avoiding topic-related features since given a harassment letter, the candidate authors may not have previously written anything on such a topic. — We propose a joint learning model that can learn simultaneously the distributed word representation as well as the topical bias and lexical bias representations of each document based on unlabeled texts. The joint learning model simulates the sentence composition process and captures the joint effects of topical bias and lexical bias in picking a specific word to form the sentence. The learned topical vector representation of a document is able to capture the global topical context, while the learned lexical representation of a document is able to capture the personal bias in choosing words under the given global context. — Using a similar learning representation approach, we propose how the characterlevel and syntactic-level representations of each document can be learned. The character-level model captures the morphological and phonemes bias of an author when he/she is composing a lexical token. It models the probability of how a character element is chosen to form a given lexical token in the sentence. The syntactic-level model captures the syntactic/grammatical bias of an author when he/she is putting words together to construct a sentence. It models a prediction path that maximally avoids the dependency information introduced by the Part-of-Speech (POS) tagger. To the best of our knowledge, this paper proposes the first model that learns a characterlevel representation and the first model that learns a syntactic-level representation. — We evaluate the effectiveness of the learned representations as stylometrics via extensive experiments and show its superiority over the state-of-the-art representations and algorithms for author verification and author characterization tasks. Without using any labels we achieve the best result on the PAN 2014 [Stamatatos et al. 2014] English authorship verification problem with respect to the Area Under Receiver Operating Characteristic curve (AUROC). By using a simple logistic regression classifier over the learned representations, we achieve the best result on the ICWSM 2012 [Al Zamal et al. 2012] authorship characterization dataset without any social network related structural information. The characteristics include age range, gender, and political orientation.\nThe rest of this paper is organized as follows: Section 2 elaborates our stylometric learning models and discusses their relation to the existing stylometric features according to different linguistic modalities. Section 3 elaborates our evaluation of the proposed models on the authorship verification problem with the PAN 2014 dataset. Section 4 presents our evaluation of the proposed models on the problem of authorship characterization with the ICWSM 2012 dataset. Relevant works are situated throughout the discussions in this paper. Finally Section 5 concludes this paper and explores future directions."
    }, {
      "heading" : "2. MINING STYLOMETRIC REPRESENTATIONS FOR AUTHORSHIP ANALYSIS",
      "text" : "In this section, we present the proposed models for learning the stylometric representations on unlabeled training data and estimating the representations for unseen testing data. To start with, we will define several key concepts and the studied stylometric learning problem.\nTo be consistent in terminology, text dataset refers to the union of available labeled and unlabeled text; document and writing sample are used interchangeably to refer to\nthe minimum unit of text data to be analyzed. A writing sample consists of a list of sentences, and a sentence consists of a sequence of lexical tokens. Each lexical token has its respective POS tag in the corresponding sentence.\nThis section corresponds to the first process of the lower flowchart in Figure 1, where only unlabeled text data are available. In this process we learn the representation of each chosen unit of text into four vectorized numeric representations, respectively, for four linguistic modalities. We formally define the stylometric feature learning problem as follows:\nDefinition 2.1. (stylometric representation learning) The given text dataset is denoted by D, and each document is formulated as ω ∈ D. A document ω consists of a list of ordered sentences S(ω) = s[1 : a], where sa represents one of them. Each sentence consists of an ordered list of lexical tokens T (sa) = t[1 : b], where tb represents the token at index b. P(tb) denotes the Part-of-Speech tag for token tb. Given D, the task is to learn four vector representations ~θtpω ∈ RD(tp), ~θlxω ∈ RD(lx), ~θchω ∈ RD(ch), and ~θsyω ∈ RD(sy), respectively, for topical modality tp, lexical modality lx, characterlevel modality ch, and syntactic modality sy for each document ω ∈ D. D(·) denotes the dimensionality for a given modality.\nWe argue that the division of the whole feature space according to linguistic modalities is necessary because different application scenarios have different requirements of the features. Moreover, even for manual feature engineering, writing styles that correspond to different linguistic dimensions are constructed differently by humans [Solorio et al. 2011; Torney et al. 2012]; therefore, they need to be grouped together.\nTake the typical dynamic bag-of-lexical-n-gram model, with ranking by frequency, as a lexical modality representation example. Given a text dataset ω ∈ D, the top k lexical n-grams G(ω) = g[1 : c] are selected based on their occurring frequency. gc represents one of them. For each document ω, a lexical level modality representation is constructed as ~θkω ∈ Rk, where ~θkω[c] is the frequency value of gc in ω. In the following section, we describe the proposed models for stylometric learning according to different linguistic modalities."
    }, {
      "heading" : "2.1. Joint learning model for topical modality and lexical modality",
      "text" : "In this section we are interested in both the topical modality and the lexical modality; both operate on the lexical tokens of a document. To start with, we look into their nature and the existing AA solutions that involve these two modalities.\n2.1.1. Topical modality. The topical modality concerns the differences of interested topics reflected from the plain text. For example, in web blogs males may talk more about the information technologies, while females may talk more about fashion and cosmetics. It is widely acknowledged that the topics reflected from text actually depend on the distribution of words or combination of words [Fung et al. 2003].\nTypical LDA-based AA techniques [Savoy 2013a; Pratanwanich and Liò 2014; Seroussi et al. 2014] construct the topics by drawing a distribution over the lexical tokens and then represent the writing style as the distribution over topics. For this modality it is intuitive to directly use the LDA or the LSA topic models and represent the constructed distribution over topics as the stylometric representation. However, it has been shown that the representation of words learned by a neural network performs significantly better than the LSA model, and the LDA model becomes computationally infeasible on large datasets [Mikolov et al. 2013]. Thus, we seek to customize existing embedding learning neural network models to combine the co-occurrence relationship between words and document and the co-occurrence relationship between words.\n2.1.2. Lexical modality. The lexical modality is concerned with the specific choice of words, given the context. Different from the topical modality, in which the stylometric representation captures the relationship between word and document, the lexical modality captures the difference in the co-occurrence relationship between words reflected by the text.\nThe most widely employed lexical features are function words. They have been shown to be effective for capturing the differences in writing styles [Koppel et al. 2002]. Usually the frequency value of the function words are used to represent the features [Baron 2014; Halvani and Steinebach 2014; HaCohen-Kerner and Margaliot 2014]. They are effective for identifying the first language of the authors [Torney et al. 2012; Argamon et al. 2009], identifying the actual author of French literature [Boukhaled and Ganascia 2014], and characterizing the gender of e-mails [Corney et al. 2002]. Especially for the first language detection, the effect of language transfer affects the use of function words in the secondary language [Torney et al. 2012]. These feature representations can be regarded as the typical “bag-of-words” model. Segarra et al. [2014] present the function words differently. They construct a function word adjacent matrix to capture the relationships among function words, and model the probability by regarding it as a Markov chain. Their approach considers the co-occurring relationship among function words.\nAnother useful type of lexical features are lexical n-grams, which denote a sequence of consecutive words of length n. Lexical n-grams are becoming popular as they are shown to be more effective than character n-grams and syntactic n-grams when all the possible n-grams are used as features [Ding et al. 2015]. Moreover, it has been shown to be effective in identifying the gender of tweeters [Burger et al. 2011]. However, the study presented by Rao and Yarowsky [2010] shows that the socio-linguistic features outperform the lexical n-gram approach (n ∈ 1, 2) when characterizing gender and age, but when characterizing the region of the Twitter user, the n-gram-based approach out-performs the socio-linguistic feature. This is possibly because people in different regions discuss different topics.\nThe lexical n-gram approach has two problems. First, the current bag − of − ngram features fail to capture the co-occurring relationship between words in a longer context due to the limit of the parameter n and the independence assumption of the n-grams. Simple unigram (i.e., n = 1) and bigram (i.e., n = 2) features can hardly capture the relationship among nouns across the whole sentence, and the relationship between each bigram/trigram is considered independent. Second, the current n-gram approach heavily depends on the feature selection method [Zamani et al. 2014a; Savoy 2013a; Pavlyshenko 2014]. The space of the complete n-gram (n ∈ N) features is indeed sparse and can be greatly compressed for the problem of authorship analysis. Most of the AA studies simply discard some n-gram features based on the threshold of a specific measure, such as occurring frequency; a better way is to analyze the principal component or conduct the factorization of the co-occurrence matrix. However, it is computationally infeasible to do a full factorization for a large dataset.\n2.1.3. Joint modeling of topical and lexical modalities. Both the topical modality and the lexical modality operate on the lexical tokens. A text document ω can be considered to be generated by the author under a mixture effect of topical bias and lexical bias. In the bag-of-n-grams approaches for AA studies, it is difficult to distinguish whether an n-gram selected to form a sentence is mainly due to the holistic topics of the document or the personal lexical preference. The LDA-based and LSA-based approaches fail to consider the lexical preference; they only considered the co-occurrence relationship between documents and words instead of the co-occurrence relationship between words.\nIn order to best separate the mixed effects of topical modality and lexical modality, and to address the aforementioned issues, we propose a joint learning model in which a document is considered as a lexical token picking process and the author picks tokens from his/her vocabulary in sequence to construct sentences and express what his/her interests are. We consider three factors in this token picking process: the topical bias, the local contextual bias, and the lexical bias.\n— Topical bias. Based on the certain holistic meaning (i.e., topics) to be conveyed through the text, the author is limited to a vague set of possible thematic tokens. For example, if the previously picked tokens are mostly about Microsoft, then the author will have a higher chance of picking the word “Windows” in the rest of the document because they are probably under a similar topic. Given the topics of the document, the author’s selection of the next token in a sentence is influenced by the related vocabulary under these topics. — Local contextual bias. A document has both holistic topics and local contexts. Both influence how the next word is chosen in a sentence. For example, a document about Microsoft may consist of several parts that cover its different software products. Moreover, the context can be irrelevant to the topic. For example, a web blog may have an opening about weather that has nothing to do with the holistic topic in the following text. — Lexical bias. Given the topics and their related vocabularies, the author has different choices for picking the next token to convey a similar meaning. For example, if the author wants to talk about the good weather, He or she may pick the adjective “nice” to describe the word “day”. Alternatively, the author can pick other words such as “great”, “wonderful”, “fantastic”, or “fabulous”, etc. The variations in choosing different words to convey a similar meaning introduce the lexical bias for an author to construct the document.\nThe word picking process is a sequence of individual decision problems influenced by the individual topical bias, contextual bias, and lexical bias; therefore, it is natural to jointly learn the topical representation and lexical representation in the same model. It has the advantage of modeling their joint effects simultaneously and at best of minimizing the interference between the learned representations.\n2.1.4. The proposed joint model. This section introduces our proposed joint learning model for the topical modality and lexical modality. The goal is to estimate ~θtpω ∈ RD(tp) and ~θlxω ∈ RD(lx) in Definition 2.1.\nFigure 2 depicts the model, which is a single-layer neural network with two output layers. The input is the joint effect of topical bias, local contextual bias, and lexical bias. Recall that the contextual bias concerns the local information surrounding the token to be picked. We represent the vectorized local contextual bias surrounding token tb in its corresponding sentence sa as θ C(tb) sa . The first output is the prediction probability of the targeted word to be chosen by the author. The model tries to maximumize the log probability for the first output:\nargmaxJ1(θ) = argmax 1\n|D| D∑ ω S(ω)∑ sa T (sa)∑ tb log P(tb| ~θtpω︸︷︷︸ topical , ~θlxω︸︷︷︸ lexical , θC(tb)sa︸ ︷︷ ︸ contextual ) (1)\nSimilar to the other neural-network-based paragraph/word embedding learning models [Bengio et al. 2006; Mikolov et al. 2013; Le and Mikolov 2014], this model maps each lexical token tb into two vectors: ~wtbin ∈ Rdw (the yellow rectangles in Figure 2) and\n~wtbout ∈ Rdw (the blue rectangles in Figure 2) where dw denotes the dimensionality. ~w tb in is used to construct the input of contextual bias for the neural network, and ~wtbout is used for the multi-class prediction output of the neural network. They are all model parameters to be estimated on the text data.\nThe local context of a token is represented by its surrounding tokens within the window size. Given a token tb in a sentence sa with a sliding window of sizeW(tp), the context of tb is formulated as C(tb, sa) = {tb−W(tp), . . . , tb−1, tb, tb+1, . . . , tb+W(tp)} where C(tb, sa) ⊆ T (sa). The contextual bias input to the neural network is defined as the average over the input mapped vectors of C(tb). We define 〈·〉 as the vector elementwise average function:\nθC(tb)sa = 〈C(tb,sa)∑ t ~wtin 〉 (2)\nThe other two inputs to the model are the topical bias ~θtpω ∈ RD(tp) and the lexical bias ~θlxω ∈ RD(lx). In order to have the model working properly, we need to set D(lx), D(tp), and dw equal to d1, where d1 is the parameter of the whole model that indicates the dimensionality for both the lexical modality representation and topical modality representation. With these three input vectors we further take their average as joint input vector θtbin since it is costly to have a fully connected layer to have the model trained in a reasonable time.\n~θtbin = 〈 ~θtpω︸︷︷︸\ntopical + ~θlxω︸︷︷︸ lexical + ~θC(tb)sa︸ ︷︷ ︸ contextual\n〉 (3)\nExample 2.2. Consider a simple sentence: ta = “it is a great day !!” in Figure 2. For each token {tb|b ∈ [1, 6]} we pass forward the neural network. We take b = 4 and tb =’great’ for example. The training process is the same for other values of b. Given a windows size of 2, we construct the local context of t4 as C(t4, sa) = {t2, t3, t5, t6} = {‘is’, ‘a’, ‘day’, ‘!!’}. We map these two tokens into their corresponding vector representations ~wt2in, ~w t3 in, ~w t5 in and ~w t6 in. With ~θ tp ω and ~θlxω , we calculate ~θ t4 in using Equation 3.\nSuppose that we use the typical soft-max multi-class output layer. The first output of this model captures the probability of picking a word tb based on the joint bias input θtbin as follows:\nP(tb| ~θtpω︸︷︷︸ topical , ~θlxω︸︷︷︸ lexical , ~θC(tb)sa︸ ︷︷ ︸ contextual ) = P(~wtbout|~θ tb in) =\nf(~wtbout, ~θtbin)∑V\nt f(~w t out, ~θtbin)\nf(~wtout, ~θtbin) = Uh((~w t out) T × ~θtbin)\n(4)\nV denotes the whole vocabulary constructed upon the text dataset D. Uh(·) denotes the element-wise sigmoid function. It corresponds to the red circle under the first output in the Figure 2. This function scales the output to the range of [0, 1], so its output can be interpreted as probability. ~wtout is the mapped out vector for the lexical token t.\nBy substituting the log probability in Equation 1 with Equation 4 and taking derivatives respectively on ~wtout and ~θ tb in, we have the gradients to be updated for each tb at each mini-batch in the back propagation algorithm that is used to train this model:\n∂\n∂ ~wtout J(θ)1 =\n( Jt == tbK−P(~wtout|~θtbin) ) × ~θtbin\n∂\n∂θtbin J(θ)1 = ~w\ntb out − V∑ t P(~wtout|~θ tb in)× ~w t out\n(5)\nJ·K is an identity function. If the expression inside this function is evaluated to be true, then it outputs 1; otherwise 0. For example, J1 + 2 == 3K = 1 and J1 + 1 == 3K = 0.\nHowever, using a full soft-max layer is costly and inefficient because for each tb we need to update d1× (|V |+2+W(tp)× 2) parameters, where |V | can be large. Following recent development of an efficient word embedding learning approach [Mikolov et al. 2013], we use the negative sampling method to approximate the complete soft-max layer:\nlog P(tb| ~θtpω︸︷︷︸ topical , ~θlxω︸︷︷︸ lexical , ~θC(tb)sa︸ ︷︷ ︸ contextual ) = log P(~wtbout|~θ tb in)\n≈ log f(~wtbout, ~θ tb in) + k∑ i=1 EtvPn(tb) ( Jt 6= tbKlog f(−1× ~wtout, ~θtbin) ) f(~wtout, ~θtbin) = Uh((~w t out)\nT × ~θtbin) (6)\nThe negative sampling algorithm tries to distinguish the correct guess tb with k randomly selected negative samples {t|t 6= tb} using k + 1 logistic regressions. EtvPn(t) is a sampling function that samples a token v from the vocabulary V according to the noise distribution Pn(t) of V . By substituting the log probability in Equation 1 with Equation 6 and taking derivatives, respectively, on ~wtout and ~θ tb in, we have the gradients to be updated:\n∂\n∂ ~wtout J(θ)ng1 =\n( Jt == tbK− f(~wtout, ~θtbin) ) × ~θtbin\n∂\n∂~θtbin J(θ)ng1 = k∑ i EtvPn(t) (( Jt == tbK− f(~wtout, ~θtbin) ) × ~wtout ) (7) The superscript ng of J(θ)ng1 indicates that the log probability of the objective function is substituted by negative sampling rather than a complete soft-max. For each tb to be predicted, we only need to update d1 × (k + 1 +W(tp) × 2) parameters for the second output. k is contributed by the negative sampling. W(tp) × 2 is contributed by the contextual input. We equally propagate the error to each lexical token in the context C(tb, sa) of tb. The constant 1 is contributed by the lexical bias term ~θlxω . We do not propagate the errors from the first output to the topical modality ~θtpω since the topical bias is determined by the holistic distribution of vocabulary and is not determined by the specific token selection on the local level. We update ~θtpω in the second output, which will be described below.\nExample 2.3. Continue from Example 2.2. This example shows how to train the model for the first output. We map t4 into its output vector ~wt4out. Next we can calculate P(~wt4out|~θ t4 in) using negative sampling (Equation 6). After that we can calculate the gradients w.r.p.t ~wt4out and ~θ t4 in using Equation 7. We update ~w t4 out according to its gradient with a pre-specified learning rate. We also update ~wt2in, ~w t3 in, ~w t5 in, ~w t6 in, and ~θ lx ω equally according to the gradient of ~θt4in.\nThe second output of this model captures the topical bias reflected on the document ω. The topics reflected from the text can be interpreted as the union of effects of all the local context in the sentence. Thus, the second output of this single-layer feed-forward neural network (see the left part of Figure 2) is a multi-class prediction of each word in the sentence sa, which is denoted by T (sa) in Definition 2.1. The goal is to maximumize the log probability on ~θtpω of document ω for each of its sentences S(ω):\nargmaxJ2(θ) = argmax 1\n|D| D∑ ω S(ω)∑ sa T (sa)∑ tb log P(tb| ~θtpω︸︷︷︸ topical ) (8)\nSimilar to the first output of this model, we map each lexical token at the output to a numeric vector ~wtbout (the yellow rectangles in Figure 2). Suppose that we use the typical soft-max multi-class output layer. The second output of this model captures the probability of picking a word tb based on the topics θtbin as follows:\nP(tb| ~θtpω︸︷︷︸ topical ) = P(~wtbout|~θtpω ) = f(~wtbout, ~θtpω )∑V t f(~w t out, ~θtbω )\nf(~wtout, ~θtpω ) = Uh((~w t out) T × ~θtpω )\n(9)\nThe total number of parameters to be estimated is (|V |+ 1)× d1. However, the term |V | is too large. Similar to the first output of this model, we use the k negative sampling approach to approximate the log probability:\nP(tb| ~θtpω︸︷︷︸ topical ) = P(~wtbout|~θtpω )\n≈ log f(~wtbout, ~θtpω ) + k∑\ni=1\nEtvPn(tb) ( Jt 6= tbKlog f(−1× ~wtout, ~θtpω ) ) (10) By substituting the log probability in Equation 8 with Equation 10, and taking the derivatives respectively over ~wtbout and ~θtpω , we have the derivatives to be updated for each tb:\n∂\n∂ ~wtout J(θ)ng2 =\n( Jt == tbK− f(~wtout, ~θtpω ) ) × ~θtpω\n∂\n∂~θtpω J(θ)ng2 = k∑ i EtvPn(t) (( Jt == tbK− f(~wtout, ~θtpω ) ) × ~wtout ) (11) The total number of parameters now becomes (k + 1) × d1 for each tb. Constant k is contributed by k negative samples, and constant 1 is contributed by the update of ~θtpω . Basically, the second output of this model is an approximation to the full factorization of the document-term co-occurrence matrix.\nExample 2.4. Continue from Example 2.2. This example shows how to train the model for the second output. For the second output of the model we map each token into a numeric vector ~wtbout, where tb ∈ {‘it’, ‘is’, ‘a’, ‘great’, ‘day’, ‘!!’}. For each of the vectors we calculate P(~wtbout|~θtpω ) in Equation 10 using negative sampling. Then we calculate the derivatives for each ~wtbout and ~θtpω by using Equation 11, and update them accordingly by multiplying the gradients with a pre-specified learning rate.\nIn this model, we count punctuation marks as lexical tokens. Consequently, the information related to the punctuation marks is also included. Punctuation marks carry information of intonation in linguistics and are useful for authorship analysis [Torney et al. 2012]. After training the model on a given text dataset D, we have a topical modality vector representation ~θtpω ∈ Rd1 and a lexical modality vector representation ~θlxω ∈ Rd1 for each document ω ∈ D. Also, for each lexical token tb ∈ V we have a vectorized representation ~wtbin ∈ Rd1 .\nFor an unseen document ω ′ /∈ D that does not belong to the training text data, we fix all the ~wtbin ∈ Rd1 and ~w tb out ∈ Rd1 in the trained model and only propagate errors to ~θlx ω′ ∈ Rd1 and ~θtp ω′ ∈ Rd1 . At the end, we also have both ~θlx ω′ and ~θd1 ω′ for ω ′ .\nThe PVCBOW model and PVDM model for paragraph embedding in [Le and Mikolov 2014] are trained in a similar way, and all of them operate on the lexical tokens. How-\never, the proposed joint network structure in this paper is customized for authorship analysis. We jointly model the topical bias and the local contextual bias in a single neural network. Thus, it is very different from the PVCBOW and PVDM models."
    }, {
      "heading" : "2.2. The character-level modality",
      "text" : "Features of the character-level modality are concerned with the morphology and phonemes biases in the process of constructing/spelling a single lexical word [Torney et al. 2012]. The typical feature used for character modality is the character n-gram feature. Character n-gram is a sequence of consecutive characters. Usually the character bigrams, trigrams, and four-grams (i.e., n ∈ 2, 3, 4) with their respective frequency or tf×idf values are used for AA studies. They have been shown to be effective for authorship verification [Halvani and Steinebach 2014; Potha and Stamatatos 2014], attribution [Nasir et al. 2014], and characterization [Burger et al. 2011]. Sapkota et al. [2014] show that character n-gram features are robust and perform well even in the condition where the training data and testing data are on different topics. In contrast, our previous studies [Ding et al. 2015] show that the lexical n-gram approach still outperforms the character n-gram approach even on e-mail data where topics vary across different documents. This is possibly because the documents used by Sapkota et al. [2014] are newspapers, which on average are more formal and longer than e-mail data; therefore, the cumulative effect of character n-gram is more stable and robust.\nHowever, similar to the lexical n-gram, the problems related to the character n-gram are two-fold. First it is difficult to determine the parameter n, and the choice actually depends on the data. For formal writings such as theses, academic papers, or newspapers, bigrams and trigrams appear to be sufficient; however, for informal writing such as tweets, some special grams are ignored if only n ∈ 2, 3, 4 is considered. For example, a lexical token containing repeated alphabets (e.g., “niceeeeee”) are popular on social networks and they are important for authorship analysis as a socio-linguistic feature [Rao and Yarowsky 2010]. However using only short n-grams cannot really distinguish the style of “niceeeeee” from the style of the repeating lexical tokens in “engineer” and “IEEE” because all of them reflect the frequent usage of bigram “ee”. The question is: Can we model the relationship between characters directly from the text instead of choosing the parameter n? This example illustrates that the character modality has a strong relationship with the lexical token. Can we also model the variation of the morphological relationship between them? Second, the number of possible character n-grams is large and it is difficult to determine the choice of feature selection method and the hyper parameter k. They have a strong dependency on the available dataset, as shown in our experiment. The question is: Can we get rid of these dataset-dependent configurations?\nIn order to address the aforementioned issues of character-based stylometric features, we propose a neural-network-based model to learn the character modality representation on the plain text data. This model consists of a single neural layer and captures the morphological differences in constructing and spelling lexical tokens across different documents. Refer to Figure 3. The input of this model is one of the character bigrams generated by a sliding window over a lexical token tb with the character-level bias. The output of this model is the vectorized representation of the token tb. The purpose is to learn ~θchω ∈ RD(ch) for each document ω ∈ D such that vector ~θchω captures the morphological differences in constructing lexical tokens. Let CH(tb) = bg[1 : c] denote the list of character bigrams of a given token tb, and bg is one of them. The goal is to maximize the following log probability on the given dataset D:\nargmaxJ(θ) = argmax 1\n|D| D∑ ω S(ω)∑ sa T (sa)∑ tb CH(tb)∑ bg log P(tb| ~θchω︸︷︷︸ char-level , ~bgin) (12)\nWe use a character bigram instead of unigram to increase the character-level vocabulary size. Similar to the previous lexical model, we map each lexical token tb into a numeric vector ~wbtout, which is used to output a multi-class prediction. We also map each character bigram into a numeric vector ~bgin, which is used for the network input. Both are model parameters to be estimated. The input vectors of this model are ~bg bt\nin\nand ~θchω . Both of them have the same dimensionality d2. After taking an average, it is fed into the neural network, as depicted in Figure 3, to predict its corresponding lexical token tb. We considered adding a soft-max layer to predict tb:\n~θbgin = 〈 ~θchω , ~bgin 〉 P(tb| ~θchω︸︷︷︸\nchar-level\n, ~bgin) = P(~w tb out|~θ bg in) = f(~wtbout, ~θbgin)∑V\nt f(~w t out, ~θbgin)\nf(~wtout, ~θbgin) = Uh((~w t out) T × ~θbgin)\n(13)\nAgain, there are O(V ) parameters to be updated for each pass of the neural network, which is not efficient. Thus, we use the negative sampling approach to approximate\nthe log probability:\nP(tb| ~θchω︸︷︷︸ char-level , bg) = P(~wtbout|~θ bg in)\n≈ log f(~wtbout, ~θ bg in) + k∑ i=1 EtvPn(tb) ( Jt 6= tbKlog f(−1× ~wtout, ~θbgin) ) (14) Similar to the previous model, we have the following derivatives by using negative sampling as indicated by the superscript ng:\n∂\n∂ ~wtout J(θ)ng =\n( Jt == tbK− f(~wtout, ~θbgin) ) × ~θbgin)\n∂\n∂~θbgin J(θ)ng = k∑ i EtvPn(t) (( Jt == tbK− f(~wtout, ~θbgin) ) × ~wtout ) (15) The number of parameters to be updated for each bigram bg of token tb is (k+2)×d2. The constant k is contributed by the negative sampling function, and the constant 2 is contributed by ~θchω and ~bgin. To learn ~θchω′ , for ω ′ /∈ D we fix all ~wtbout and ~bgin and only propagate errors to ~θch ω′ .\nExample 2.5. Consider a simple sentence: ta = “Fantastic day !!” in Figure 3. For each token {tb|b ∈ [1, 3]} we extract its character bigrams. Suppose the word in the target is t1 = ‘fantastic’, and its bigrams are CH(t4) = {bgc|c ∈ 1, 2, 3, 4, 5, 6, 7, 8} = {‘fa’, ‘an’, ‘nt’, ‘ta’, ‘as’, ‘st’, ‘ti’, ‘ic’}. The training process is the same for other words in this sentence. Let us take a bigram bg1 =‘fa’ as an example. First, we map bg1 to its vectorized representation ~bgin and map t1 to its representation ~w t1 out. In combination with ~θchω , we calculate ~θ bg in according to the first formula in Equation 13. Then we calculate the forward log probability for P(~wt1out|~θ bg in) in Equation 14. Then we calculate the corresponding gradients in Equation 15 and update the respective parameters. The training pass for bigram bg1 = ‘fa’ is completed, and we move to the next bigram bg2=‘an’ following the sample procedure. After all the bigrams are completed, we move to the next token t2 = ‘day’."
    }, {
      "heading" : "2.3. The syntactic modality",
      "text" : "Syntactic features are usually considered as deep linguistic features that are comparatively more difficult to consciously manipulate [Gamon 2004]. Typically, the Part-ofSpeech (POS) tags n-grams are used as the features for this modality [Boukhaled and Ganascia 2014; Baron 2014; Qian et al. 2014]. Given a token tb, its Part-of-Speech (POS) tag represents its grammatical role in the sentence. The number of total unique tags is much smaller than the number of the character n-grams and the number of the lexical n-grams. Due to its limited feature space, it is less effective than the other linguistic modalities; however, it is more robust than the others [Ding et al. 2015], especially for the cross-language authorship attribution problem [Bogdanova and Lazaridou 2014]. However, by re-constructing the POS n-grams based on the traversing order of the parsed tree structure of the sentence, [Posadas-Duran et al. 2014] show that the complete POS tag or Syntatic Relation (SR) tag n-grams can actually outperform the character n-grams when given the same number of features. However, based on their experimental result, we notice that as the limited number of features increases, the performance of character n-grams increases steadily without reaching the peak value, and their maximum limit on the number of features appears to limit the performance\nof the character n-gram approach. Recently, Feng and Hirst [2014] present the syntactic information in a different way by considering the sequence of grammatical roles of the entities recognized from the text, e.g., the entity is the subject, the object, or neither. Another alternative to the POS tags n-gram is the fully parsed POS dependent tree of the sentence [Tschuggnall and Specht 2014; Posadas-Duran et al. 2014]. Syntactic tree-based approaches have larger feature space than POS n-grams. However, we find that for most online casual text snippets and informal writings the accuracy of such a parsing approach drops significantly. Moreover, it is inefficient and computationally infeasible to parse a large dataset into full syntactic trees.\nInstead of looking at the POS n-grams, we seek another alternative to maximize the degree of variations that we can gain from the POS tags. First, we look into the stateof-the-art tagger models. POS taggers are pre-trained models that take a list of tokens as input and output a list of tags. Suppose we have a sentence sa with its tokens tb ∈ T (sa). Recall that P(tb) denotes the POS tag for the token tb in the sentence. Refer to Definition 2.1. To assign a tag P(tb) to a token tb, there are three typical structures/models [Toutanova et al. 2003]:\n— Left-to-Right structure. This structure tries to maximize P(P(tb)|tb,P(tb−1)). The tag for token tb is determined by both the lexical token itself and the next tag P(tb−1). Strong dependencies exist between P(tb−1) and P(tb) and between P(tb) and tb. See Figure 4a. The solid lines indicate the dependencies. — Right-to-Left structure. This structure tries to maximize P(P(tb)|tb,P(tb+1)). The tag for token tb is determined by both the lexical token itself and the previous tag P(tb+1). Strong dependencies exist between P(tb+1) and P(tb) and between P(tb) and tb. See Figure 4b. The solid lines indicate the dependencies. — Bidirectional structure This structure is a combination of the previous two. It maximizes P(P(tb)|tb,P(tb+1),P(tb−1)). The tag for token tb is determined by both the lexical token itself and the surrounding tags P(tb+1) and P(tb−1). Strong dependencies exist between P(tb+1) and P(tb), between P(tb) and P(tb−1), and between P(tb) and tb. See Figure 4c. The solid lines indicate the dependencies.\nFor all of these three structures, there exists a strong dependency between contiguous POS tags, as well as between the actual lexical token and its tag. Using POS tags n-grams as a stylometric feature is less effective than using character n-grams and lexical n-grams because the strong dependencies between contiguous POS tags introduced by the POS taggers are shared between different documents. An n-gram-based\nmodel enlarges the feature space using the contiguous gram dependencies, while for POS n-grams it is weakened by the taggers.\nTherefore, we seek another way that has fewer dependencies introduced by the POS tagger. In Figure 4c, strong dependencies introduced by the tagger are shown as solid lines. We select two weak dependency links from tb to P(tb+1) and from tb to P(tb−1), as indicated by the dashed lines. The tagger only introduces indirect dependencies on these two paths. Thus, these two paths have more variations across different documents than the others, as indicated by solid lines. Formally, our model tries to maximize P (P(tb−1),P(tb+1)|tb), which is different from the typical structures for the taggers.\nThe number of unique POS tags is quite limited, so we use the bigrams of POS tags. See Figure 5. Let P2(tb) be a POS tag bigram [P(tb),P(tb+1)], and pgb = PG(tb) = {P2(tb−3),P2(tb−2),P2(tb+1),P2(tb+2)} be the neighbor POS bigrams of token tb. The goal of this model is to maximize:\nargmaxJ(θ) = argmax 1\n|D| D∑ ω S(ω)∑ sa T (sa)∑ tb PG(tb)∑ pgb log P(pgb| ~θsyω︸︷︷︸ syntactic , ~wtbin) (16)\nSimilar to the previous models, this model maps each lexical token tb into a numeric vector ~wtbin, and each of its neighbor POS bigrams maps into an numeric vector ~pg b out. The input of the model, denoted by ~θpgin , is the average of ~w tb in and ~θ sy ω , and the prediction is one of the target token tb’s neighbor POS tag bigrams, as shown in Figure 5. ~wtbin and ~θsyω share the same dimensionality d3. The prediction can be implemented as a soft-max\nlayer:\n~θpgin = 〈 ~θsyω , ~w tb in 〉 P(pgb| ~θsyω︸︷︷︸\nsyntactic\n, tb) = P( ~pg b out|~θ pg in ) = f( ~pgbout, ~θpgin )∑Vpg\npg f( ~pgout, ~θpgin )\nf( ~pgout, ~θpgin )) = Uh(( ~pgout) T × ~θpgin ))\n(17)\nwhere Vpg denotes the union of all distinct POS bigrams, and the number of parameters to be updated for each pgb is bounded by Vpg, which is around a few hundreds. It is still computationally feasible to directly use the soft-max layer. It is possible to use the negative sampling as well:\nP(pgb| ~θsyω︸︷︷︸ syntactic , tb) = P( ~pg b out|~θ pg in )\n≈ log f( ~pgbout, ~θ pg in ) + k∑ i=1 EpgvPn(pgb) ( Jpg 6= pgbKlog f(−1× ~pgout, ~θpgin ) ) (18)\nwhere Pn(pgb) denotes the negative sampling function for Vpg. Accordingly, we have the following derivatives for back propagation:\n∂\n∂ ~pgbout J(θ)ng =\n( Jpg == pgbK− f( ~pgbout, ~θpgin ) ) × ~θpgin )\n∂\n∂~θpgin J(θ)ng = k∑ i EpgvPn(pgb) (( Jpg == pgbK− f( ~pgout, ~θpgin ) ) × ~pgout ) (19) At the end of the training, we have ~θsyω for each document ω ∈ D. To estimate ~θ sy ω′ for ω ′ /∈ D, we fix all ~wtbin and ~pgout and only propagate errors to ~θ sy ω′ .\nExample 2.6. Consider a simple sentence and its corresponding sequence of POS tags in Figure 5. For each token {tb|b ∈ [1, 10]} we extract its POS neighbor bigrams. Suppose the word in target is t5 =‘contains’, and its POS neighbor bigrams are PG(t5) = {‘DT JJ’, ‘JJ NN’, ‘RB RB’, ‘RB JJ’} given a window size of 2. The training process is the same for other lexical tokens in the sentence. Let us take one of its (t5’s) POS neighbor bigrams pg5 =‘DT JJ’ as an example. First we map pg5 to its vectorized representation ~pg5in and map t5 to its representation ~w t5 in. With ~θ sy ω , we calculate ~θ pg in according to the first formula in Equation 17. In combination with ~pg5in, we calculate the forward log probability for P( ~pg5in|~θ pg in ) in Equation 18. Then we calculate the corresponding gradients in Equation 19 and update the respective parameters. The training pass for bigram pg5 =’DT JJ’ is completed, and we move to the next bigram ‘JJ NN’ following the same procedure. After all the bigrams are processed, we move to the next token t6."
    }, {
      "heading" : "2.4. Making the model deterministic",
      "text" : "Deterministic and reproducible results are important requirements for most authorship applications, especially in the area of cyber forensic and linguistic evidence [Iqbal et al. 2013; Ding et al. 2015]. We could not show that a piece of document is written\nby an author in the first run, and later show that it is authored by another author in the subsequent runs with the same inputs and settings. Unfortunately, the proposed stylometric representation learning approach is based on the stochastic gradient descend back-propagation algorithm, which involves a high degree of randomness in its nature. In order to make the proposed model deterministic, we need to enforce specific modifications to the aforementioned models on the implementation level:\n— Initializing the parameters to be estimated. To start with, we need to initialize all the neural network input layer parameters of all the models to small random values around zero. In order to have the same sequence of the random number generated for the parameters, we fix the starting random seed as 0. In this way, we make sure that the gradient descend process always starts from the same point at the feature space.\n— Using the single-thread implementation. A multi-threaded implementation of the stochastic gradient descent can greatly reduce the runtime for learning stylometric representation; however, the behavior of each thread can hardly be controlled. Even the parameter space is sparse; there is a high chance for different threads to update the parameters of the frequent terms at the same time. To avoid the unpredictable behavior of multi-threading, we choose a single-threaded implementation for both the parameter initialization and stochastic gradient descent. Thus, using the negative sampling approach is critical for all the models to keep the training efficient."
    }, {
      "heading" : "3. EVALUATION ON UNSUPERVISED AUTHORSHIP VERIFICATION PROBLEM",
      "text" : "In this section, we evaluate the proposed models on the authorship verification problem. The problem is to verify whether or not two anonymous text documents are written by the same author. Unlike the authorship identification problem, where a set of candidate authors are available for comparison, the authorship verification problem has only one target document to be compared. The solution to this problem should provide a confidence value that indicates how likely the two given text documents are written by the same author. Authorship identification is a closed-set classification problem and authorship verification is an open-set classification problem [Stamatatos et al. 2014]. Authorship verification is more difficult to solve than authorship identification.\nWe further divide authorship verification into two types: supervised verification and unsupervised verification. In the supervised authorship verification problem, the ground-truth data is available in the training set. The ground-truth data consists of a list of authors with their respective written documents. The ground-truth data has similar properties to the two anonymous documents to be verified. For example, all of them are e-mails. Learning-to-rank-based classification schemes fit into this category: Given a vector, the classification model learns to output a value ranging from 0 to 1 based on the training vectors. Typically, a SVM model or a logistic regression model is employed.\nIn the unsupervised authorship verification problem, no such ground-truth data is available. However, a list of documents that share similar properties to the two anonymous documents to be verified is available. For example, all of them are e-mails. The authors of this list of documents are unknown. The unsupervised authorship verification problem is more difficult than the supervised authorship verification problem. In this section, we focus on the unsupervised authorship verification problem."
    }, {
      "heading" : "3.1. The solution",
      "text" : "To solve the targeted problem with our proposed stylometric representation learning models, we first train the three models mentioned in Section 2 on the unlabeled text data, and then we estimate the stylometric representations ~θtpω ∈ Rd1 , ~θlxω ∈ Rd1 ,\n~θchω ∈ Rd2 , and ~θsyω ∈ Rd3 , respectively, for the two anonymous documents ω1, ω2 in the testing data that is previously unseen by the models. The verification score is a simple cosine distance measure between the given two documents’ stylometric representations. Formally, given two anonymous documents ω1, ω2, the solution outputs the similarity value:\nQ(ω1, ω2) = (~θvω1) T × ~θvω2 |~θvω1 | × |~θvω2 |\nv ∈ {tp, lx, ch, sy} (20)\nwhere v denotes the selected modality. It could be tp topical modality, lx lexical modality, ch character-level modality, sy syntactic modality, or their combinations. If more than one modality is selected, we concatenate their ~θvω into a single one for each ω.\nTo measure the performance of the proposed approaches and the baselines on this problem, we use the Area Under Receiver Operating Characteristic curve (AUROC) [Fawcett 2006]. It is a well-known evaluation measure for binary classifiers where both positive labels and negative labels are equally important. Since changing the threshold of the similarity value results in different accuracy and recall measures, the AUROC measure captures the overall performance of the classifier when the threshold is varied. A 1.0 AUROC value indicates an excellent performance, and a 0.5 AUROC value implies a worthless random guess."
    }, {
      "heading" : "3.2. The English novel and essay dataset",
      "text" : "We choose the PAN2014 authorship verification English dataset as the benchmark dataset. PAN provides a series of shared tasks on digital text forensics. In this way we can directly compare our results with other studies. The latest available dataset (both training and testing) for the authorship verification problem is from PAN20142. At the moment of writing this paper, PAN2015 has only published the training dataset for this problem. Currently, we only focus on English text data, even though the aforementioned models can be adopted for different languages.\nRefer to Table I. This dataset provides both the training data and testing data. The training data consists of 300 verification cases. Each verification case consists of two sets of documents and a label. The label can be true, which indicates that two sets of\n2PAN2014 Authorship Verification. Available at http://pan.webis.de/clef14/pan14-web/author-identification. html\ndocuments are written by the same author, or false, vice versa. 200 of them are essays, and 100 are novels. The test data follows the same format. It contains 400 verification cases; 200 of them are essays, and 200 are novels. Table I shows that the numbers of essay verification cases for both training and testing data are comparable, while there are more verification cases in testing data than in training data for novels. Figure 6 shows the empirical distribution function over the document length in terms of the size of lexical tokens for essays and novels. It is apparent that documents in the novel dataset are longer than those in the essay dataset. We expect that the proposed model performs better on the novel dataset than the essay dataset based on our previous studies on the factors that influence the quality of AA results [Ding et al. 2015].\nWe preprocess the data by removing extra spaces and non-ASCII lexical tokens. We also pre-tokenize texts, detect sentence boundaries, and generate POS tags for the dataset using the Stanford tagger [Toutanova et al. 2003]. This tagger has a bidirectional structure, discussed in Section 2.3.\nAs we focus on the unsupervised authorship verification problem, we treat all the training data as unlabeled data (all the ground-truth labels are stripped for training). Only a small portion of randomly sampled problems from the training dataset with their ground-truth labels are selected as a validation set for tuning the hyperparameters for the proposed models and all the baseline models."
    }, {
      "heading" : "3.3. Baselines",
      "text" : "We choose several of the most relevant approaches to compare with our proposed models on the authorship verification problem.\n— Style. This approach represents a document as a numeric vector under a typical 302 static stylometric features, which has been widely studied. Table II provides a summary of these features. It is shown to be effective coupled with classifiers for the authorship identification problem. It is called a static feature set since the features do not change across different datasets. The similarity between two documents is based on their normalized cosine distance.\n— Style+[k-freq-ngram]. This approach represents a document as a numeric vector under the 302 static features in Table II as well as k dynamic features constructed based on the training dataset. The k dynamic features are constructed by picking the top-k n-grams ranked by their occurring frequency in the training set. n-grams include lexical unigrams, lexical bigrams, lexical trigrams, character unigrams, character bigrams, character trigrams, POS tag unigrams, POS tag bigrams, and POS tag trigrams. In this experiment, we pick k ∈ {100, 200, 500, 1000, 2000, 5000}. The value for each top-k selected n-gram is calculated using tf × idf . The similarity between two documents is based on their normalized cosine distance.\n— Style+[k-info-ngram]. This approach is the same as Style+[k-freq-ngram] except that the top-k n-grams are selected based on information gain rather than the frequency value in the training dataset. Even though previous AA research already experimentally demonstrates that frequency value carries enough stylistic information and outperforms the information gain scheme [Stamatatos 2009], we include it in the baselines. We pick k ∈ {100, 200, 500, 1000, 2000, 5000}.\n— LDA. The Latent Dirichlet allocation (LDA) is a generative model that learns a latent semantic representation between the documents and the words. The latent semantic operation is learned through Gibbs Sampling. This approach represents each document as a numeric vector under the document-to-latent-topic distribution learned from the dataset. The numeric vector has k elements and each of them corresponds to one of the latent topics in the LDA model. We validate the number of iterations and the number of k for the LDA model on the validation set. The similarity between two documents is the cosine distance between their vectorized representation. We pick k = 500, which achieves the best result on the validation set.\n— LSA. Latent semantic analysis (LSA) is a technique for analyzing the relationship between words and documents. Given a set of documents, we can represent it as a sparse matrix, where a row denotes a document and a column represents a word. A word is represented as its occurrences in different documents and a document is represented as a set of words with their corresponding occurrence in this document. Given such a sparse matrix, LSA learns the latent representation between document and word by factorizing the sparse matrix using Single Value Decomposition (SVD). After SVD, each document can be represented as weights over the k singular values in SVD. The similarity between two documents is the cosine distance between their vectorized representation. We pick k = 200 by maximizing its performance on the validation set.\n— w2v-skipgram. w2v-skipgram is a neural network language model that learns the vectorized embedding for words in a text dataset [Mikolov et al. 2013]. It is efficient and has been adapted in various data mining problems. It is well known for the word analogy task by mathematically manipulating the vectorized representation of words. It is shown that this model is an approximated factorization of the co-occurrence matrix between words. By converting each word of a document into a vector and taking their average, we can obtain an vectorized representation of the document. Finally we use cosine similarity to measure the distance between vectors.\n— w2v-cbow. w2v-cbow is another neural network language model that learns the vectorized embedding for words in a text dataset [Mikolov et al. 2013]. It is more scalable to larger dataset than the w2v-skipgram model. Following the previous w2vskipgram approach, we obtain a vectorized representation of a document by converting each word of the document into a vector and taking their average. At the end we use cosine similarity to measure the distance between vectors.\n— PVDBOW. PVDBOW is a recently proposed model that learns a vectorized document representation based on the neural network language model [Le and Mikolov 2014]. It captures the occurrences relationship between words. It has been shown to be effective in sentiment analysis [Le and Mikolov 2014]. Our proposed model uses a similar neural network approach but the whole structure and the motivation are different. We validate hyper-parameters for PVDBOW on the validation set. We pick k = 400 with sub-sampling enabled and a window size of 4 accordingly to maximize its performance on the validation set.\n— PVDM. PVDM is another recently proposed model that learns a vectorized document representation based on the neural network language model [Le and Mikolov 2014]. It captures the document-to-word occurrences relationship. It has been shown to be more effective than PVDBOW in sentiment analysis [Le and Mikolov 2014]. We pick k = 400 with sub-sampling enabled and a window size of 4 according to its performance on the validation set.\n— Other approaches are reported in PAN2014. For comparison, we have selected the top 10 approaches from the other studies reported in PAN2014. PAN2014 also has a meta-classifier, called META-CLASSIFIER-PAN2014, which combines all the submitted approaches.\nThese baselines cover both the recent development in text embedding learning models and authorship verification solutions. We also include several combinations such as w2v-cbow+skipgram. Following the same procedure for the baseline approaches, we train our proposed three models on the training set and choose its hyper-parameters based on the validation set. We set b1 = 400, b2 = 400, b3 = 400 and select a window size of 4 for the joint model of lexical and topical modality. Evaluation results are reported based on the performance on the test dataset."
    }, {
      "heading" : "3.4. Performance comparison",
      "text" : "In this section, we present our evaluation result on the English authorship verification dataset with respect to the AUROC measure with all the baselines mentioned above. As indicated in Table III, our proposed Modality models achieve the highest AUROC score on this authorship verification problem. Specifically, on average the first-rated model is the joint learning model for lexical modality and the topical modality described in Section 2.1. This model also outperforms all the others on the essay dataset. The runner-up is the lexical modality representation that is learned in the joint learning model. Character-level modality achieves the highest score on the novel dataset. It also outperforms all the aforementioned baselines on average. The syntactic modality does not perform as well as the lexical, topical, and character-level modalities; however, it still achieves better AUROC than the PVDBOW, LSA, LDA, and other dynamic n-gram approaches. It is noted that our proposed syntactic modality representation outperforms the other POS-tags-based approach, such as [Harvey 2014] and n-gram approaches, that involve POS tags.\nOur proposed models perform better than LSA and LDA approaches, and the LSA approaches outperform the LDA approaches. Probably it is because our model is a joint effect of document-to-word co-occurrence relationship and co-occurrence relationship between words, while LSA and LDA only consider the direct relationship between document and word. The PVDBOW and PVDM approaches also outperform LSA and LDA. In general, the neural-network-based model achieves better performance. Table III also shows that our proposed lexical modality representation outperforms the dynamic n-gram-based feature representation with a lower degree of dimensionality.\nThe w2v-related approaches, which learn document embedding by averaging the word embedding, do not perform as well as our proposed approaches and the PVDMrelated approaches that directly learn the document embedding. We also see that the overall performance on the novel dataset is better than that on the essay dataset, which is consistent with our expectation described in Section 3.2 and the observation in our previous work [Ding et al. 2015].\nConsidering the feature selection criteria for dynamic n-gram-based approaches, in this scenario the information gain measure outperforms the frequency measure, which is contradictory to the results reported in the survey [Stamatatos 2009].\nThe information-gain-based feature selection method consistently outperforms the frequency-based measure for the authorship verification problem."
    }, {
      "heading" : "4. EVALUATION ON AUTHORSHIP CHARACTERIZATION PROBLEM",
      "text" : "We evaluate the proposed models on another important problem in authorship analysis: authorship characterization. The problem is to identify the socio-linguistic characteristics of the author based on the given text. As discussed in Section 1, it has wide applications in marketing, political socialization, digital forensics, and social network analysis.\nThis problem can be described as follows: Given a set of labeled documents, where each document is assigned a class label such as the gender of its author, the problem is to identify the class label for a document whose author remains unknown. A classifier is trained on the labeled documents, and it assigns one of the labels to the targeted document. All labels are considered non-overlapping. For example, the labels for age ranges can be 18-23 and 23+.\nTo have the classifier understand the text data we need to represent these data in numeric form. To have the proposed models in this paper be able to solve this problem, we first learn the models based on the training text data by treating them as unlabeled data. After that we estimate the stylometric representations for all the documents in the training set and pad the available labels into the vectorized representations of the documents. Then an arbitrary classification model can be trained based on these vectors. Given the unseen document data, the models estimate their representations and feed them into the classifier to have the predicted labels.\nIn this section, we evaluate different stylometric representations for the authorship characterization problem. We represent the text documents in different forms based on the selected model and feed the learned representation of the documents into a simple logistic regression model to predict the characteristics of a text’s author."
    }, {
      "heading" : "4.1. The Twitter characterization dataset",
      "text" : "We choose the ICWSM 2012 labeled Twitter dataset [Al Zamal et al. 2012] in our experiment. This dataset consists of three categories of labels, and it is publicly available. Due to the limitation of Twitter’s policy, the actual content of tweets were not included with the dataset; however, the Twitter users’ identification numbers as well as their tweet IDs are available. We retrieve all the data using Twitter API according to the available information.\nTo preprocess the dataset, we remove all the non-ASCII characters and replace all the URLs with a special lexical token. We also pre-tokenize the tweets and assign POS tags for each token in each tweet using the pre-trained tagger from [Owoputi et al. 2013]. In this dataset there is other social-network-based information, such as the target user’s friends, and the friends’ tweets, etc. Since we only want to model the writing style of the Twitter user, we omit this information as well as those tweets that\nare re-tweeted by the given author. We attempt to include only the tweets that are authentically authored by the labeled Twitter user.\nThe labels in this dataset are generated semi-automatically and manually inspected [Al Zamal et al. 2012]. This dataset consists of three categories of labels for Twitter users: age, gender, and political orientation. The cleaned dataset is summarized in Table IV. There are 1170 Twitter users in total.\n— Gender. The labels for this category can be either male or female. The labels are automatically generated based on the Twitter user’s name with a name-gender database, and then labels are manually inspected to ensure the labels are correct.\n— Age.This dataset only distinguishes individuals of age ranges in 18-23 or 25-30. It frames the age prediction into a binary classification problem. The labels are constructed by looking at the tweets about birthday announcement, e.g., “Happy birthday to me”.\n— Political orientation. This dataset provides political Twitter users with a label: either Democrat or Republican. Twitter users are collected from the wefollow Twitter directory [Al Zamal et al. 2012].\nFigure 7 shows the empirical distribution, kernel density and histogram on the tweets’ length in terms of lexical token size. In general, tweets are very short text snippets. 90 percent of tweets have less than 25 tokens and most have a length of around 10 tokens. We combine all tweets of a single user into a single document and treat each tweet as an individual sentence.\nTo proceed with the experiment we conduct a 10-fold cross validation on the Twitter dataset and collect the accuracy measure for each characterization approach. First we convert each document into its numeric vector representation using different stylometric representations in our proposed models or the baselines, and then we feed them into a simple logistic classifier to predict the label of the document.\n4.1.1. Baselines. We inherit the same set of baselines as in the previous experiment on the authorship verification problem, except for those studies reported in PAN2014 [Rangel et al. 2014] since we do not have the available result for direct comparison. The baselines are configured to have the same hyper-parameter setting as the previous experiment. Additionally, we include several additional baselines:\n— LDA. In addition to picking the empirical optimal value k = 500, which represents the latent topics, we include the performance of k ∈ {100, 200, 500, 800} for comparison.\n— LSA. Likewise, for the LSA model we also include the performance of k ∈ {100, 200, 500, 800} in addition to the original k = 200. Recall that k for LSA represents the number of singular values.\n— Moreover, we include two evaluation results that were presented by Al Zamal et al. [2012] since we follow the same setup and use the same dataset for the experiment. The target user info approach is a SVM-based model trained on the features that are constructed on the user’s tweets and other information. These features include textual features (e.g., stemmed n-grams and hash tags, etc.) and socio-linguistic features (e.g., re-tweeting tendency, neighborhood size, and mention frequency, etc.). The all info approach is another SVM-based model trained on the features that adds additional social-network features (e.g., average of the neighborhood’s feature vectors).\nWe notice that the baseline measures adopted from [Al Zamal et al. 2012] have more advantages to our proposed approaches and our baseline approaches. First, they use a SVM model that typically outperforms a simple logistic regression model when the same data is given. Second, our approaches only consider the information reflected from the text, i.e., stylometric information. Other socio-linguistic, behavioral,\nand social-network-related information is discarded. Given the advantages of their approach we expect that probably they will outperform the others. However, our experiments show that our proposed model achieves even better accuracy, which will be described in the following section.\n4.1.2. Performance comparison. The performance of our proposed models, as well as all the baselines, is listed in Table V. It shows that our first proposed model, which jointly learns the representation for the lexical modality and the topical modality, achieves the highest accuracy value. The runner-up is the topical modality, and the characterlevel modality does not perform as well as the other two. The proposed lexical/topical modality model and the character-level modality model also outperform the PVDMrelated models, w2v-related models, and other dynamic n-gram-based models. Unlike the results for the authorship verification problem, the w2v-related baselines perform fairly well. They achieve a higher accuracy value than PVDM, PVDBOW, LSA, and LDA.\nEven the (target user only) approach and the (all info) approach are given advantage; however, it achieves a lower accuracy value than our proposed joint model for lexical and topical modality, which is contradictory to our expectation. It shows that our proposed approach better models the writing variation than the n-gram language model that is used in both of these two baselines.\nTable V also shows that the proposed syntactic representation learning model does not perform well on the ICWSM 2012 dataset, which is different from the previous authorship verification problem. This is because the tweet text data are relatively more casual than essay and novel, which does not introduce much variation in the grammatical bias.\nRegarding the feature selection measure, in this scenario the frequency-based selection approach outperforms the information-gain-based selection-based approach. Even the top-100 frequency-ranked n-grams outperform top-1500 information-gain-ranked n-grams, which is completely different from the result shown in previous authorship verification experiments. Such a difference further confirms our argument that feature selection measures are scenario-dependent/data-dependent. Even the feature set is dynamically constructed based on a different dataset, the measurement for the selection process is data-dependent. A language model over text data is better than a feature-selection-based model."
    }, {
      "heading" : "5. CONCLUSIONS AND FUTURE DIRECTIONS",
      "text" : "In this article, we present our three models for learning the vectorized stylometric representations of different linguistic modalities for authorship analysis. To the best of our knowledge, it is the very first work introducing the problem of stylometric representation learning into the authorship analysis field. Our proposed models are designed to effectively capture the differences of writing styles of different modalities when an author is composing text. By using the proposed feature learning scheme, guided by the selected linguistic modality, we attempt to mitigate the issues related to the feature engineering process in current authorship study. Our experiments on the publicly available PAN 2014 and the ICWSM 2012 Twitter datasets, respectively, for the authorship verification problem and the authorship characterization problem, demonstrate that our proposed models are effective and robust on both different datasets and AA problems.\nOur future research will focus on exploring better models to capture writing styles and proposing models for other languages. Currently the representation learning models are simple one-layer neural networks. A recurrent neural network with long-short term memory is more suitable for capturing the contextual relationship over long text. For learning the syntactic modality representation, a recursive neural network that operates on the fully parsed syntactic tree will fit more into the nature of grammatical variations than the current one. Moreover, this work only focuses on capturing the variations in English writing. Additional changes need to be applied for text in other languages."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "The research is supported in part by the Discovery Grants (356065-2013) from the Natural Sciences and Engineering Research Council of Canada (NSERC), Canada Research Chairs Program (950-230623), and the Research Incentive Fund grant (RIF13059) from the Zayed University."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "Authorship analysis (AA) is the study of unveiling the hidden properties of authors from a body of exponentially exploding textual data. It extracts an author’s identity and sociolinguistic characteristics based on the reflected writing styles in the text. It is an essential process for various areas, such as cybercrime investigation, psycholinguistics, political socialization, etc. However, most of the previous techniques critically depend on the manual feature engineering process. Consequently, the choice of feature set has been shown to be scenarioor dataset-dependent. In this paper, to mimic the human sentence composition process using a neural network approach, we propose to incorporate different categories of linguistic features into distributed representation of words in order to learn simultaneously the writing style representations based on unlabeled texts for authorship analysis. In particular, the proposed models allow topical, lexical, syntactical, and character-level feature vectors of each document to be extracted as stylometrics. We evaluate the performance of our approach on the problems of authorship characterization and authorship verification with the Twitter, novel, and essay datasets. The experiments suggest that our proposed text representation outperforms the bag-of-lexical-n-grams, Latent Dirichlet Allocation, Latent Semantic Analysis, PVDM, PVDBOW, and word2vec representations.",
    "creator" : "LaTeX with hyperref package"
  }
}