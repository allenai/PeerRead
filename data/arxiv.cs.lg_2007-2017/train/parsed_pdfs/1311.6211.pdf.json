{
  "name" : "1311.6211.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "xfern}@eecs.oregonstate.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n31 1.\n62 11\nv1 [\ncs .L\nG ]\n2 5\nN ov\n2 01\nIndex Terms— Novelty detection, multi-instance multilabel, kernel method"
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "Novelty detection is the identification of new or unknown data that is not labeled during training [1]. In the traditional setting, only training examples from a nominal distribution are provided and the goal is to determine for a new example whether it comes from the nominal distribution or not. Much work has been done in this field. Early work is generally divided into two categories [1, 2]. One category includes statistical approaches such as some density estimation methods. The other category consists of neural network based approaches, e.g., multi-layer perceptrons. Several new approaches have been introduced in recent years. In [3], geo-\nThis work was partially supported by the National Science Foundation grant CCF-1254218.\nmetric entropy minimization is introduced for anomaly detection. An efficient anomaly detection method using bipartite k-NN graphs is presented in [4]. In [5], an anomaly detection algorithm is proposed based on score functions. Each point gets scores from its nearest neighbors. This algorithm can be directly applied to novelty detection. In [6], SVMs are applied to novelty detection to learn a function f that is positive on a subset S of the input space and negative outside S.\nIn this paper, we consider novelty detection in a new setting where the data follows a multi-instance multi-label (MIML) format. The MIML framework has been primarily studied for supervised learning [7] and widely used in applications where data is associated with multiple classes and can be naturally represented as bags of instances (i.e., collections of parts). For example, a document can be viewed as a bag of words and associated with multiple tags. Similarly, an image can be represented as a bag of pixels or patches, and associated with multiple classes corresponding to the objects that it contains. Formally speaking, the training data in MIML consists of a collection of labeled bags {(X1, Y1), (X2, Y2), . . . , (XN , YN )}, where Xi ⊂ X is a set of instances and Yi ⊂ Y is a set of labels. In the traditional MIML applications the goal is to learn a bag-level classifier f : 2X → 2Y that can reliably predict the label set of a previously unseen bag.\nIt is commonly assumed in MIML that every instance we observe in the training set belongs to one of the known classes. However, in many applications, this assumption is violated. For example, in a collection of tagged images, the tag may only cover a subset of objects present in the images. The goal of novelty detection in the MIML setting is to determine whether a given instance comes from an unknown class given only a set of bags labeled with the known classes. This setup has several advantages compared to the more wellknown setup in novelty detection: First, the labeled bags allow us to apply an approach that takes into account the presence of multiple known classes. Second, frequently the training set would contain some novel class instances. The presence of such instances, although never explicitly labeled as novel instances, can in a way serve as “implicit” negative examples for the known classes, which can be helpful for identifying novel instances in new bags.\n978-1-4799-1180-6/13/$31.00 c©2013 IEEE\nThe work presented in this paper is inspired by a real world bioacoustics application. In this application, the annotation of individual bird vocalization is often a time consuming task. As an alternative, experts identify from a list of focal bird species the ones that they recognize in a given recording. Such labels are associated with the entire recording and not with a specific vocalization in the recording. Based on a collection of such labeled recordings, the goal is to annotate each vocalization in a new recording [8]. An implicit assumption here is that each vocalization in the recording must come from one of the focal species, which can be incomplete. Under this assumption, vocalizations of new species outside of the focal list will not be discovered. Instead, such vocalizations will be annotated with a label from the existing species list. The setup proposed in this paper allows for novel instances to be observed in the training data without being explicitly labeled, and hence should enable the annotation of vocalizations from novel species. In turn, such novel instances can be presented back to the experts for further inspection.\nTo the best of our knowledge, novelty detection in the MIML setting has not been investigated. Our main contributions are: (i) We propose a new problem – novelty detection in the MIML setting. (ii) We offer a framework based on score functions to solve the problem. (iii) We illustrate the efficacy of our method on a real-world MIML bioacoustics data."
    }, {
      "heading" : "2. PROPOSED METHODS",
      "text" : "Suppose we are given a collection of labeled bags {(X1, Y1), (X2, Y2), . . . , (XN , YN )}, where the ith bag Xi ⊂ X is a set of instances from the feature space X ⊂ Rd, and Yi is a subset of the know label set Y = ⋃N i=1 Yi. For any label yim ∈ Yi, there is at least one instance xin ∈ Xi belonging to this class. We consider the scenario where an instance in Xi has no label in Yi related to it, which extends the traditional MIML learning framework. Our goal is to determine for a given instance x ∈ X whether it belongs to a known class in Y or not.\nTo illustrate the intuition behind our general strategy, consider the toy problem shown in Table 1. The known label set is {I,II}. We have four labeled bags available. According to the principle that one instance must belong to one class and one bag-level label must have at least one corresponding instance, we conclude that △ is drawn from class I, belongs to class II, and ♦ doesn’t come from the existing classes. ▽ cannot be fully determined based on current data.\nTo express this observation mathematically, we calculate the rate of co-occurrence of an instance and a label. For example, △ appears with label I together in bag 1, 2, 4 and they are both missing in bag 3. So, the co-occurrence rate p(△, I) = 1. All the other rates are listed in Table 2. If we detect an instance based on the maximal co-occurrence rate with respect to all classes and set a threshold to be 3/4, we will reach a result that can generally reflect our previous observation.\nThis example inspires us to devise a general strategy for detection. We introduce a set of score functions, each of which corresponds to one class, i.e., for each label c ∈ Y , we assign a function fc to class c. Generally, for an instance from a specific known class, the value of the score function corresponding to this class should be large. If all scores of an instance are below a prescribed threshold, it would not be considered to belong to any known class. The decision principle is: If maxc∈{1,...,|Y |} fc(x) < ε then return ‘unknown’, otherwise return ‘known’.\nThere are many possible choices for the set of score functions. Generally, the score functions are expected to enable us to achieve a high true positive rate with a given false positive (Type I error) rate, which can be measured by the area under the curve (AUC) of ROC."
    }, {
      "heading" : "2.1. Kernel Based Scoring Functions",
      "text" : "We define the score function for class c as follows:\nfc(x) = ∑\nxl∈ ⋃\ni\nXi\nαclk(x, xl)\n= αTc k(x)\n(1)\nwhere Xi’s are training bags, xl’s are training instances from training bags, k(·, ·) is the kernel function such that k(x) = (k(x, x1), . . . , k(x, xL))\nT, and αcl’s are the components of the weight vector αc = (αc1, . . . , αcL)T.\nWe encourage fc to take positive values on instances in class c and negative values on instances from other classes. Hence, we define the objective function OBJ as\nλ\n2\n|Y |∑\nc=1\nαTc Kαc + 1\nN |Y |\nN∑\ni=1\n|Y |∑\nc=1\nFc(Xi) (2)\nwhere\nFc(Xi) = max{0, 1− yic max xij∈Xi fc(xij)}, yic ∈ {−1,+1}\nλ is a regularization parameter, K is the kernel matrix with (i, j)-th entry k(xi, xj), xi, xj ∈ ⋃ k Xk, and yic = +1 if and only if Yi contains the label for class c. In fact, we define an objective function for each class separately and sum over all these objective functions to construct OBJ . The first term of OBJ controls model complexity. Fc(·) in the second term of OBJ can be viewed as a baglevel hinge loss for class c, which is a generalization of the single-instance case. If c is a bag-level label of bag Xi, we expect max\nxij∈Xi fc(xij) to give a high score because there is at\nleast one instance in Xi is from class c. Other loss functions such as rank loss [8] have already been introduced for MIML learning.\nOur goal is to minimize the objective function which is unfortunately non-convex. However, if we fix the term max\nxij∈Xi fc(xij), i.e., find the support instance xic such that xic = argmaxxij∈Xi αc Tk(xij) and substitute back to the objective function, the resulted objective function OBJ ∗ will be convex with respect to αc’s. To solve this convex problem, we deploy the L-BFGS [9] algorithm. The subgradient along αc used in L-BFGS is computed as follows:\n∇c = λKαc − 1\nN |Y |\nN∑\ni=1\nyick(xic)1{1−yicfc(xic)>0} (3)\nDetails can be found in Algorithm 1. This descent method can be applied to any choice of kernel function and according to our experience it works very well (usually converges within 30 steps). Note that many algorithms [8,10] for MIML learning that attempt to learn an instance-level score functions including the proposed approach are based on a non-convex objective. Consequently, no global optimum is guaranteed. To reduce the effect induced by randomness, we usually rerun the algorithm multiple times with independent random initializations and adopt the result with the smallest value of the objective function.\nAlgorithm 1 Descent Method Require: {(X1, Y1), (X2, Y2), . . . , (XN , YN )}, λ, T .\nRandomly initialize all αc’s s.t. ‖α1c‖ = 1 for t = 1 to T do\nSet xtic = argmaxxij∈Xi (α t c) T k(xij),\n1 t ic = 1{1−yicfc(xtic)>0},\n∇tc = λKαc − 1\nN |Y | N∑ i=1 yick(x t ic)1 t ic.\nPlug {xtic} into OBJ to get a convex surrogate OBJ t ∗. Run L-BFGS with inputs OBJ t∗, ∇ t c to return {α t+1 c } and OBJ t+1\nend for return {αT+1c } and OBJ T+1."
    }, {
      "heading" : "2.2. Parameter Tuning",
      "text" : "In our experiment, we use Gaussian kernel, i.e., k(xi, xj) = e−γ‖xi−xj‖ 2\n, where ‖·‖ is the Euclidean norm. The parameter γ controls the bandwidth of the kernel. Hence, there are a pair of parameters λ and γ in the objective function required to be determined.\nWhile training, we search in a wide range of values for the parameter pair, and select the pair with corresponding αc’s that minimizes\nN∑\ni=1\n|Y |∑\nc=1\ng(yic max xij∈Xi fc(xij))\nwhere g(x) = 1x<0 is the zero-one loss function. Note that 1x<0 is a lower bound of the hinge loss max{0, 1− x}.\nWe vary the value of threshold to generate ROCs while testing. The values of threshold are derived from training examples."
    }, {
      "heading" : "3. EXPERIMENTAL RESULTS",
      "text" : "In this section, we provide a number of experimental results based on both synthetic data and real-world data to show the effectiveness of our algorithm. Additionally, we present a comparison to one-class SVM, a notable anomaly detection algorithm."
    }, {
      "heading" : "3.1. MNIST Handwritten Digits Dataset",
      "text" : "We generated the synthetic data based on the MNIST handwritten digits data set1. Each image in the data set is a 28 by 28 bitmap, i.e., a vector of 784 dimensions. By using PCA, we reduced the dimension of instances to 20.\nWe created training and testing bags from the MNIST instances. Some examples for handwritten digits bags are\n1Available on-line http://www.cs.nyu.edu/˜roweis/data.html\nshown in Table 3. Two processes for generating bags are listed in Algorithm 2 and Algorithm 3. The only difference between these two procedures is that Algorithm 3 rules out the possibility of a label set for a bag being empty, i.e., a bag including purely novel examples. For Dirichlet process used in our simulation, we assigned relatively small concentration parameters β = (β1, β2, . . . , β10) to the Dirichlet distribution in order to encourage a sparse label set for a bag, which is common in real-world scenarios. We set all βi = 0.1 and the bag size M = 20. Typical examples of bags generated from Dirichlet distribution are shown in Table 4.\nAlgorithm 2 Bag generation procedure for handwritten digits data Require: N , M , Y , β.\nfor i = 1 to N do Draw M instances {xij} according to the proportion given by Dirichlet (β) distribution. Extract labels from xij ’s to form Y ′ i and set Yi = Y ∩Y ′\ni . end for\nAlgorithm 3 Bag generation procedure with filtration for handwritten digits data. Require: N , M , Y , β.\nfor i = 1 to N do Set Yi = ∅. while Yi == ∅ do\nDraw M instances {xij} according to the proportion given by Dirichlet (β) distribution. Extract labels from xij ’s to form Y ′\ni and set Yi = Y ∩ Y ′\ni . end while\nend for\nWe provided our method with bags generated in two different ways:\n1. Generate both training and testing bags according to Algorithm 2.\n2. Generate training bags according to Algorithm 3 while generate testing bags by applying Algorithm 2.\nIn our experiments, we consider various sizes of known label sets and different combinations of labels in these two settings. Two typical examples of ROCs from the two setting are shown in Figure 1.\nTable 5 shows the average AUCs of ROCs over multiple runs from the first setting. We observe that average AUCs are all above 0.85 for the known label sets of size 4. For the known label sets of size 8, the average AUCs are all larger than 0.8. The results are fairly stable with different combinations of labels. This demonstrates the effectiveness of our algorithm.\nTable 6 shows the average AUCs of ROCs for the setting which does not contain bags with an empty label set. The label sets in these two tables are the same. The results in the two tables are comparable but those in Table 5 are always better. This demonstrates that it is beneficial to include bags with an empty label set. The reason could be that those bags contain purely novel examples and hence training on those\nbags is very reliable."
    }, {
      "heading" : "3.2. HJA Birdsong Dataset",
      "text" : "We tested our algorithm on the real-world dataset - HJA birdsong dataset2, which has been used in [11, 12]. This dataset consists of 548 bags, each of which contains several 38-dimensional instances. The bag size, i.e., the number of instances in a bag, varies vary from 1 to 26, the average of which is approximately 9. The dataset includes 4998 instances from 13 species. Species names and the numbers of instances for those species are listed in Table 7. Each species corresponds to a class in the complete label set {1, 2, . . . , 13}. We took a subset of the complete label set as the known label set and conducted experiment with various choices of the known label set. Table 8 shows the average AUCs of different known label sets. Specifically, we intentionally made each species appear at least once in those known sets. From Table 8, we observe that most all of the values of AUCs are above 0.85 and some even reach 0.9. The results are quite stable with different label settings despite the imbalance in the instance population of the species. These results illustrate the potential of the approach as a utility for novel species discovery.\n2Available on-line http://web.engr.oregonstate.edu/˜briggsf/kdd2012datasets/hja_birdsong/"
    }, {
      "heading" : "Y AUC Y AUC",
      "text" : ""
    }, {
      "heading" : "3.3. Comparison with One-Class SVM",
      "text" : "Our algorithm deals with detection problem with MIML setting, which is different from the traditional setting for anomaly detection. We argue that traditional anomaly detection algorithms cannot be directly applied to our problem. To make comparison, we adopt one-class SVM [13–15], a well known algorithm for anomaly detection. To apply one-class SVM, we construct a normal class training data consisting of examples from the known label set. The parameter ν vary from 0 to 1 with step size 0.02 to generate ROCs. The Gaussian kernel is used for one-class SVM. We search the parameter γ for the kernel in a wide range and select the best one for on-class SMV post-hoc. We present this unfair advantage to one-class SVM for two reasons: (i) It is unclear how to optimize the parameter in the absence of novel instances. (ii) We would like to illustrate the point that even given such unfair advantage, one-class SVM cannot outperform our algorithm.\nTable 9 and 10 show the average AUCs for handwritten digits data and birdsong data respectively. Compared to Table 5 and 8, the proposed algorithm outperforms 1-class SVM\nin terms of AUC not only in absolute value but also in stability. This also demonstrates that training with unlabeled instances are beneficial to the detection."
    }, {
      "heading" : "Y AUC Y AUC",
      "text" : ""
    }, {
      "heading" : "4. CONCLUSION",
      "text" : "In this paper, we proposed a new problem – novelty detection in the MIML setting and offered a framework based on score functions to solve the problem. A large number of simulations show that our algorithm not only works well on synthetic data but also on real-world data. We also demonstrate that the presence of unlabeled examples in the training set is useful to detect new class examples while testing. We present the advantage in the MIML setting for novelty detection. Even though positive examples for the novelty that are not directly labeled, their presence provides a clear advantage over methods that rely on data that does not include novel class examples.\nThere are many relative problems call for investigation. One will be on how to use the information of bag-level labels in detection if bag-level labels are available, which will possibly improve the performance of our algorithm since we did not make use of such information in our experiment."
    }, {
      "heading" : "5. REFERENCES",
      "text" : "[1] Markos Markou and Sameer Singh, “Novelty detection: A review - part 1: Statistical approaches,” Signal Processing, vol. 83, pp. 2481–2497, 2003.\n[2] Markos Markou and Sameer Singh, “Novelty detection:\nA review - part 2: Neural network based approaches,” Signal Processing, vol. 83, pp. 2499–2521, 2003.\n[3] Alfred O. Hero, “Geometric entropy minimization (gem) for anomaly detection and localization,” in NIPS. 2006, pp. 585–592, MIT Press.\n[4] Kumar Sricharan and Alfred O. Hero, “Efficient anomaly detection using bipartite k-nn graphs,” in NIPS, 2011, pp. 478–486.\n[5] Manqi Zhao and Venkatesh Saligrama, “Anomaly detection with score functions based on nearest neighbor graphs,” in NIPS, 2009, pp. 2250–2258.\n[6] Bernhard Schölkopf, Robert C. Williamson, Alex J. Smola, John Shawe-Taylor, and John C. Platt, “Support vector method for novelty detection,” in NIPS, 1999, pp. 582–588.\n[7] Zhi-Hua Zhou, Min-Ling Zhang, Sheng-Jun Huang, and Yu-Feng Li, “Multi-instance multi-label learning,” Artif. Intell., vol. 176, no. 1, pp. 2291–2320, 2012.\n[8] Forrest Briggs, Xiaoli Z. Fern, and Raviv Raich, “Rankloss support instance machines for miml instance annotation,” in KDD, 2012.\n[9] Richard H. Byrd, Jorge Nocedal, and Robert B. Schnabel, “Representations of quasi-newton matrices and their use in limited memory methods,” 1994.\n[10] Oksana Yakhnenko and Vasant Honavar, “Multiinstance multi-label learning for image classification with large vocabularies,” in Proceedings of the British Machine Vision Conference. 2011, pp. 59.1–59.12, BMVA Press.\n[11] Forrest Briggs, Xiaoli Z. Fern, Raviv Raich, and Qi Lou, “Instance annotation for multi-instance multilabel learning,” Transactions on Knowledge Discovery from Data (TKDD), 2012.\n[12] Li-Ping Liu and Thomas G. Dietterich, “A conditional multinomial mixture model for superset label learning,” in NIPS, 2012, pp. 557–565.\n[13] Bernhard Schölkopf, John C. Platt, John Shawe-taylor, Alex J. Smola, and Robert C. Williamson, “Estimating the support of a high-dimensional distribution,” 1999.\n[14] Chih-Chung Chang and Chih-Jen Lin, “LIBSVM: A library for support vector machines,” ACM Transactions on Intelligent Systems and Technology, vol. 2, pp. 27:1– 27:27, 2011.\n[15] Larry M. Manevitz, Malik Yousef, Nello Cristianini, John Shawe-taylor, and Bob Williamson, “One-class svms for document classification,” Journal of Machine Learning Research, vol. 2, pp. 139–154, 2001."
    } ],
    "references" : [ {
      "title" : "Novelty detection: A review - part 1: Statistical approaches",
      "author" : [ "Markos Markou", "Sameer Singh" ],
      "venue" : "Signal Processing, vol. 83, pp. 2481–2497, 2003.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Novelty detection:  A review - part 2: Neural network based approaches",
      "author" : [ "Markos Markou", "Sameer Singh" ],
      "venue" : "Signal Processing, vol. 83, pp. 2499–2521, 2003.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Geometric entropy minimization (gem) for anomaly detection and localization",
      "author" : [ "Alfred O. Hero" ],
      "venue" : "NIPS. 2006, pp. 585–592, MIT Press.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Efficient anomaly detection using bipartite k-nn graphs",
      "author" : [ "Kumar Sricharan", "Alfred O. Hero" ],
      "venue" : "NIPS, 2011, pp. 478–486.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Anomaly detection with score functions based on nearest neighbor graphs",
      "author" : [ "Manqi Zhao", "Venkatesh Saligrama" ],
      "venue" : "NIPS, 2009, pp. 2250–2258.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Support vector method for novelty detection",
      "author" : [ "Bernhard Schölkopf", "Robert C. Williamson", "Alex J. Smola", "John Shawe-Taylor", "John C. Platt" ],
      "venue" : "NIPS, 1999, pp. 582–588.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Multi-instance multi-label learning",
      "author" : [ "Zhi-Hua Zhou", "Min-Ling Zhang", "Sheng-Jun Huang", "Yu-Feng Li" ],
      "venue" : "Artif. Intell., vol. 176, no. 1, pp. 2291–2320, 2012.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Rankloss support instance machines for miml instance annotation",
      "author" : [ "Forrest Briggs", "Xiaoli Z. Fern", "Raviv Raich" ],
      "venue" : "KDD, 2012.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Representations of quasi-newton matrices and their use in limited memory methods",
      "author" : [ "Richard H. Byrd", "Jorge Nocedal", "Robert B. Schnabel" ],
      "venue" : "1994.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Multiinstance multi-label learning for image classification with large vocabularies",
      "author" : [ "Oksana Yakhnenko", "Vasant Honavar" ],
      "venue" : "Proceedings of the British Machine Vision Conference. 2011, pp. 59.1–59.12, BMVA Press.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Instance annotation for multi-instance multilabel learning",
      "author" : [ "Forrest Briggs", "Xiaoli Z. Fern", "Raviv Raich", "Qi Lou" ],
      "venue" : "Transactions on Knowledge Discovery from Data (TKDD), 2012.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A conditional multinomial mixture model for superset label learning",
      "author" : [ "Li-Ping Liu", "Thomas G. Dietterich" ],
      "venue" : "NIPS, 2012, pp. 557–565.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Estimating the support of a high-dimensional distribution",
      "author" : [ "Bernhard Schölkopf", "John C. Platt", "John Shawe-taylor", "Alex J. Smola", "Robert C. Williamson" ],
      "venue" : "1999.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "LIBSVM: A library for support vector machines",
      "author" : [ "Chih-Chung Chang", "Chih-Jen Lin" ],
      "venue" : "ACM Transactions on Intelligent Systems and Technology, vol. 2, pp. 27:1– 27:27, 2011.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "One-class svms for document classification",
      "author" : [ "Larry M. Manevitz", "Malik Yousef", "Nello Cristianini", "John Shawe-taylor", "Bob Williamson" ],
      "venue" : "Journal of Machine Learning Research, vol. 2, pp. 139–154, 2001.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2001
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Novelty detection is the identification of new or unknown data that is not labeled during training [1].",
      "startOffset" : 99,
      "endOffset" : 102
    }, {
      "referenceID" : 0,
      "context" : "Early work is generally divided into two categories [1, 2].",
      "startOffset" : 52,
      "endOffset" : 58
    }, {
      "referenceID" : 1,
      "context" : "Early work is generally divided into two categories [1, 2].",
      "startOffset" : 52,
      "endOffset" : 58
    }, {
      "referenceID" : 2,
      "context" : "In [3], geo-",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 3,
      "context" : "An efficient anomaly detection method using bipartite k-NN graphs is presented in [4].",
      "startOffset" : 82,
      "endOffset" : 85
    }, {
      "referenceID" : 4,
      "context" : "In [5], an anomaly detection algorithm is proposed based on score functions.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 5,
      "context" : "In [6], SVMs are applied to novelty detection to learn a function f that is positive on a subset S of the input space and negative outside S.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 6,
      "context" : "The MIML framework has been primarily studied for supervised learning [7] and widely used in applications where data is associated with multiple classes and can be naturally represented as bags of instances (i.",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 7,
      "context" : "Based on a collection of such labeled recordings, the goal is to annotate each vocalization in a new recording [8].",
      "startOffset" : 111,
      "endOffset" : 114
    }, {
      "referenceID" : 7,
      "context" : "Other loss functions such as rank loss [8] have already been introduced for MIML learning.",
      "startOffset" : 39,
      "endOffset" : 42
    }, {
      "referenceID" : 8,
      "context" : "To solve this convex problem, we deploy the L-BFGS [9] algorithm.",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 7,
      "context" : "Note that many algorithms [8,10] for MIML learning that attempt to learn an instance-level score functions including the proposed approach are based on a non-convex objective.",
      "startOffset" : 26,
      "endOffset" : 32
    }, {
      "referenceID" : 9,
      "context" : "Note that many algorithms [8,10] for MIML learning that attempt to learn an instance-level score functions including the proposed approach are based on a non-convex objective.",
      "startOffset" : 26,
      "endOffset" : 32
    }, {
      "referenceID" : 10,
      "context" : "We tested our algorithm on the real-world dataset - HJA birdsong dataset2, which has been used in [11, 12].",
      "startOffset" : 98,
      "endOffset" : 106
    }, {
      "referenceID" : 11,
      "context" : "We tested our algorithm on the real-world dataset - HJA birdsong dataset2, which has been used in [11, 12].",
      "startOffset" : 98,
      "endOffset" : 106
    }, {
      "referenceID" : 12,
      "context" : "To make comparison, we adopt one-class SVM [13–15], a well known algorithm for anomaly detection.",
      "startOffset" : 43,
      "endOffset" : 50
    }, {
      "referenceID" : 13,
      "context" : "To make comparison, we adopt one-class SVM [13–15], a well known algorithm for anomaly detection.",
      "startOffset" : 43,
      "endOffset" : 50
    }, {
      "referenceID" : 14,
      "context" : "To make comparison, we adopt one-class SVM [13–15], a well known algorithm for anomaly detection.",
      "startOffset" : 43,
      "endOffset" : 50
    } ],
    "year" : 2013,
    "abstractText" : "Novelty detection plays an important role in machine learning and signal processing. This paper studies novelty detection in a new setting where the data object is represented as a bag of instances and associated with multiple class labels, referred to as multi-instance multi-label (MIML) learning. Contrary to the common assumption in MIML that each instance in a bag belongs to one of the known classes, in novelty detection, we focus on the scenario where bags may contain novel-class instances. The goal is to determine, for any given instance in a new bag, whether it belongs to a known class or a novel class. Detecting novelty in the MIML setting captures many realworld phenomena and has many potential applications. For example, in a collection of tagged images, the tag may only cover a subset of objects existing in the images. Discovering an object whose class has not been previously tagged can be useful for the purpose of soliciting a label for the new object class. To address this novel problem, we present a discriminative framework for detecting new class instances. Experiments demonstrate the effectiveness of our proposed method, and reveal that the presence of unlabeled novel instances in training bags is helpful to the detection of such instances in testing stage.",
    "creator" : "LaTeX with hyperref package"
  }
}