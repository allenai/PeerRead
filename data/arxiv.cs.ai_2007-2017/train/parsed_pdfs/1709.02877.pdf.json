{
  "name" : "1709.02877.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Variable Annealing Length and Parallelism in Simulated Annealing",
    "authors" : [ "Vincent A. Cicirello" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "For many scheduling and optimization problems, metaheuristics such as simulated annealing (SA), genetic algorithms (GA), ant colony optimization (ACO), tabu search, etc, offer a means of trading off guarantees of optimality in favor of efficiently finding high quality solutions. Such algorithms often exhibit anytime behavior, providing increasingly better solutions with increases in available time.\nMetaheuristic behavior is usually controlled by several parameters. GAs have mutation and crossover rates, among other parameters. SA has parameters that control the annealing schedule. ACO has parameters that balance the relative influence of heuristic guidance and the learned pheromone trails. Control parameters can either be tuned beforehand by some process (automated or otherwise) or adapted dynamically using search feedback. For example, there exist parameter control approaches for GA and other forms of evolutionary computation (Eiben, Hinterding, and Michalewicz 1999; Wessing, Preuss, and Rudolph 2011; Aleti and Moser 2013),\nCopyright c© 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nadaptive annealing schedules for SA (Lam and Delosme 1988; Swartz 1993; Boyan 1998), among others.\nParallel implementation of many metaheuristics is straightforward, such as population-based algorithms like GA and ACO, whose motivations come from naturally occurring distributed behavior. While for others, parallel implementation is less obvious. In this paper, we propose a new approach to parallel SA. We execute several independent runs, with restarts, in parallel of an adaptive SA using the Modified Lam (Swartz 1993; Boyan 1998) annealing schedule. The Modified Lam annealing schedule is nearly parameter-free, requiring only knowledge of the annealing length. In our proposed parallel SA, we eliminate this last parameter with restarts following a schedule of annealing lengths that better balance the risk associated with errors in estimating available computation time. The initial and restart annealing lengths increase exponentially. The result is a parallel parameter-free SA with improved anytime behavior.\nWe validate our approach using an NP-Hard scheduling problem with sequence-dependent setups. We begin our experiments with the sequential case. Although for any a priori known fixed time limit, a single run of that length outperforms our restart schedule at run end, our proposed restart schedule exhibits greatly improved anytime behavior during the run. We continue our experiments in parallel, demonstrating our parallel variable-length SA to significantly dominate parallel fixed-length restarts early in the run.\nThe paper is organized as follows. In Section 2, we discuss related work on parallel SA. We provide details of our parallel parameter-free SA in Section 3. Then, in Section 4, we validate our approach experimentally with an NPHard scheduling problem, consisting of sequence-dependent setup constraints, and offer conclusions in Section 5."
    }, {
      "heading" : "2 Background",
      "text" : "SA is typically described in a very sequential manner, and not parallelized as obviously as other metaheuristics. There are two major categories of parallel SA (Rudolph 1993): namely, approaches that implement neighbor evaluations in parallel, and approaches that are essentially parallel implementations of multistart SA. An example of the first case is the work of Ludwin and Betz who use a parallel SA for FPGA placement to optimize critical path length (Ludwin and Betz 2011). They execute multiple move evaluations in\nar X\niv :1\n70 9.\n02 87\n7v 1\n[ cs\n.N E\n] 8\nS ep\n2 01\n7\nparallel using what they call speculative moves. The second type can be referred to as parallel multistart. Although restarting some algorithms, such as hill climbers, offers a means of countering large numbers of local optima; many have shown sequential multistart SA to be ineffective. One long run of SA the length of the available time is typically more effective than taking the best solution from a set of shorter runs. Therefore, it is not surprising that approaches to parallel SA that execute multiple runs of SA in parallel rarely involve independent runs. More commonly, parallel multistart SA involves sharing information among the parallel runs. For example, Ram et al’s approach (for job shop scheduling) periodically exchanges the best solution among the parallel runs, each continuing its search from there (Ram, Sreenivas, and Subramaniam 1996). More recently, in Jha and Menon’s approach, at intervals called “beats” the best solution is shared among threads (Jha and Menon 2014). Jha and Menon developed their approach for general purpose computation on graphics processing units (GPGPU) for a sports league scheduling problem.\nOthers apply SA in parallel to optimize a set of subproblems, each SA instance solving a different sub-problem. For example, Rahimian et al’s approach to graph partitioning, specifically for large social network graphs, distributes the problem, and individual distributed instances of SA optimize portions of the problem (Rahimian et al. 2015). They apply this to both edge-cut and vertex-cut partitioning.\nOther forms of search often rely on restarts, quite effectively. For example, in constraint satisfaction, satisfiability, and other similar problems, backtracking search using randomized variable-ordering and value-ordering heuristics often exhibit heavy-tailed runtime distributions (Gomes et al. 2000). Using an effective restart strategy, one can try to abandon the runs that are likely in the heavy-tail, restarting in an attempt to more directly solve the problem. The Luby restart schedule is the most widely known (Luby, Sinclair, and Zuckerman 1993), and has been parallelized (Cire, Kadioglu, and Sellmann 2014). The first several restart lengths of the Luby schedule (in number of backtracks) are as follows: [1, 1, 2, 1, 1, 2, 4, 1, 1, 2, 1, 1, 2, 4, 8, . . .]. You begin with a restart sequence, [1, 1, 2], then repeat the entire prior sequence from the beginning followed by a restart double the length of the last, etc."
    }, {
      "heading" : "3 Technical Approach",
      "text" : ""
    }, {
      "heading" : "3.1 Modified Lam Simulated Annealing",
      "text" : "The foundation of our approach is an existing sequential simulated annealer with an adaptive annealing schedule. SA operates via a mechanism modeled after the process of heating a metal and allowing it to cool slowly. Heating enables the material to be shaped as desired, while cooling at a slow rate minimizes internal stress thus enabling greater stability in the final state.\nIn SA, search is controlled by a temperature parameter. The most basic form of SA begins with a high temperature and then “cools” at some rate, with both initial temperature and cooling rate as system parameters. The Modified Lam annealing schedule (Swartz 1993; Boyan 1998) eliminates\nModified Lam Annealing S ← GenerateRandomInitialState T ← 0.5 AcceptRate← 0.5 for i = 1 to MaxEvals do S′ ← random selection from η(S) if Cost(S′) ≤ Cost(S) or Rand ∈ [0, 1) < e(Cost(S)−Cost(S\n′))/T then S ← S′ AcceptRate← 1500 (499 · AcceptRate + 1)\nelse AcceptRate← 1500 (499 · AcceptRate) if i/MaxEvals < 0.15 then\nLamRate← 0.44 + 0.56 · 560−i/MaxEvals/0.15 else if 0.15 ≤ i/MaxEvals < 0.65 then\nLamRate← 0.44 else if 0.65 ≤ i/MaxEvals then\nLamRate← 0.44 · 440−(i/MaxEvals−0.65)/0.35 if AcceptRate > LamRate then T ← 0.999 · T else T ← T/0.999\nreturn Best solution found during run\nFigure 1: SA with the Modified Lam annealing schedule.\nthese parameters, by dynamically adjusting temperature using search feedback. It’s based on results of Lam and Delosme (Lam and Delosme 1988), where they showed that the ideal run of SA accepts neighboring states at a rate of 0.44, which formed the basis for an annealing schedule that tracks this acceptance rate. Lam and Delosme’s version originally used a monotonically decreasing temperature schedule, and adjusted the size of the neighborhood to maintain the acceptance rate as near 0.44 as possible—e.g., they increased the size of the local neighborhood to decrease the acceptance rate, and decreased the size of the local neighborhood to increase the acceptance rate. They relied on the common assumption that nearby search states are of similar quality; and thus, a smaller local neighborhood implies smaller difference between current fitness and neighbor fitness, which leads to higher probability of neighbor acceptance.\nSwartz later made additional observations on Lam and Delosme’s annealing schedule (Swartz 1993), which were then refined by Boyan into the Modified Lam schedule (Boyan 1998). Specifically, Swartz observed that at the beginning of the search, the acceptance rate is near 1.0 (i.e., random search) and decreases at an exponential rate during the first 15% of the run when it reaches the target acceptance rate of 0.44, continues at that rate for 50% of the run, and then declines exponentially to the end of the run (i.e., end of run converges to a stochastic hill climb). Rather than adjusting the size of the local neighborhood, Swartz’s and Boyan’s Modified Lam schedule varies the temperature— increasing temperature to increase acceptance rate, and decreasing temperature to decrease acceptance rate. Figure 1 shows SA with the Modified Lam schedule. In the pseudocode, η(S) refers to the set of neighboring states of S (i.e., our neighborhood function)."
    }, {
      "heading" : "3.2 Parallel Variable Length Runs",
      "text" : "The Modified Lam annealing schedule is nearly parameterfree. However, it requires the annealing length, referred to as MaxEvals in the pseudocode of Figure 1. It is not always practical to accurately predict the time available for search. If the run is shorter than you anticipate, the search would have spent too much time randomly exploring, and insufficient time exploiting observed high quality portions of the search space. If the run is much longer than you expected, it will get stuck too early in a local optimum, failing to effectively utilize the unexpected extra time.\nFor sequential SA, many have shown that one longer run of SA is typically much better than restarting shorter runs. Extending this to parallel SA with independent instances, one would expect better performance if all parallel instances were single runs of the available time. However, the available time may not be known, and may be difficult to accurately predict. Our approach attempts to balance the risk associated with incorrectly estimating time available, using restarts with a schedule of increasing annealing lengths.\nAdditionally, we define our restart schedule to support both sequential and parallel implementations. Specifically, we propose a parallel SA, that executes several SA instances with the Modified Lam annealing schedule. Each SA instance has a different initial value of MaxEvals. As each SA instance completes its initial run, it restarts with a new longer run. The SA instances operate independently, sharing no data, and each restart begins anew with randomly generated initial solutions. The best solution found among all parallel runs is returned. The approach is essentially a parallel implementation of a multistart SA, but where the length of the restarts varies and increases.\nVariable Annealing Length (VAL): In the sequential case, our annealing length schedule, VAL, is as follows. Restart r (r = 0 is the initial run) is of length:\nMaxEvals(r) = 1000 ∗ 2r. (1)\nThus, the multistart SA follows a sequence of annealing lengths {1000, 2000, 4000, 8000, 16000, 32000, . . .}.\nParallel Variable Annealing Length Version 0 (PVAL-0): Assume that we execute N instances, {SA0,SA1, . . . ,SAN−1}, of multistart Modified Lam SA in parallel. The length, MaxEvalsi(r), for restart r of instance SAi is:\nMaxEvalsi(r) = 1000 ∗ 2i+r∗N . (2)\nIn the case of N = 1, there is a single instance SA0 and thus P-VAL reduces to VAL.\nConsiderN = 3 as an example. SA0 has a sequence of annealing lengths {1000, 8000, 64000, . . .}, SA1 has annealing lengths {2000, 16000, 128000, . . .}, and SA2 has annealing lengths {4000, 32000, 256000, . . .}.\nParallel Variable Annealing Length (P-VAL): There are deficiencies in P-VAL-0 related to parallel speedup, for\nN > 4, which we discuss later in Section 4.5. We resolve those deficiencies with the following schedule of annealing lengths MaxEvalsi(r), for restart r of instance SAi:\nMaxEvalsi(r) = 1000 ∗ 2(i mod 4)+r∗min{N,4}. (3) WhenN ≤ 4, P-VAL is identical to P-VAL-0. WhenN > 4, {SA0,SA4,SA8, . . .} all have a sequence of annealing lengths {1000, 16000, 256000, . . .}, {SA1,SA5,SA9, . . .} have annealing lengths {2000, 32000, 512000, . . .}, {SA2,SA6,SA10, . . .} have annealing lengths {4000, 64000, 1024000, . . .}, and {SA3,SA7,SA11, . . .} have annealing lengths {8000, 128000, 2048000, . . .}."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Scheduling with Sequence-Dependent Setups",
      "text" : "To validate our approach, we consider an NP-Hard single machine scheduling problem, characterized by sequencedependent setups, with an objective of minimizing weighted tardiness. The problem is NP-Hard even if setups are independent of job ordering (Morton and Pentico 1993), and the sequence-dependent setups magnify computational difficulty by inducing a non-order-preserving property of the evaluation function (Sen and Bagchi 1996).\nThe problem is defined as follows, and consists ofN jobs, J = {j1, j2, . . . , jN}. Each job jk has weight wk, duedate dk, and processing time pk. Setup times si,k, required prior to processing job jk if it immediately follows job ji, depend on the job ordering, and are asymmetric (i.e., si,k 6= sk,i); and s0,k is the initial setup time required if job jk is processed first. The jobs J must be sequenced to minimize:\nT = N∑ k=1 wkTk = N∑ k=1 wk max(ck − dk, 0), (4)\nwhere Tk is the tardiness of job jk. The completion time ck of jk is the sum of the processing and setup times of jk and of all jobs that preceed jk. If π(k) is the position of job jk in the sequence, then define ck as:\nck = ∑\nπ(x)≤π(k),π(x)=π(y)+1\n(px + sy,x). (5)\nIn our experiments, we use the standard benchmark set for the problem (Cicirello 2003a; Cicirello 2003b), which consists of 120 instances, 40 each of loose, medium, and tight due dates. Of these, 22 loose duedate instances have an optimal weighted tardiness of 0.\nThe best available exact solver, Tanaka and Araki’s Successive Sublimation Dynamic Programming, can solve all of the available benchmark instances, but requires over two weeks of memory-intensive CPU time to solve the hardest instances (Tanaka and Araki 2013). Therefore, metaheuristics are a more practical approach. A variety of algorithms have been proposed for the problem, such as dynamic programming (Tanaka and Araki 2013), neighborhood search (Liao, Tsou, and Huang 2012), iterated local search (Xu, Lü, and Cheng 2014), value-biased stochastic sampling (Cicirello and Smith 2005), ACO (Liao and Juan 2007), GA (Cicirello 2015; Cicirello 2006), SA (Cicirello 2007), etc.\nWe preprocess the instances transforming them as suggested by others (Tanaka and Araki 2013; Cicirello 2015). To minimize the impact of setup times, we increase the processing time of each job, jk, by its minimum setup time, and reduce all setup times accordingly (Cicirello 2015):\nsmink = min 0≤i≤N,i6=k si,k, (6)\npk = pk + s min k , (7)\nsi,k = si,k − smink ,∀i, i 6= k, 0 ≤ i ≤ N. (8) We also eliminate jobs jk, such that wk = 0, if ∀x∀y, x 6= y, sx,k + pk + sk,y ≥ sx,y (Tanaka and Araki 2013)."
    }, {
      "heading" : "4.2 Experimental Design",
      "text" : "We conduct our experiments on an Ubuntu 14.04 Server, with 32GB memory and two Intel Xeon L5520 Quad-Core CPUs (2.27GHz). The L5520 supports hyper-threading with two threads per core, so our server has a total of 16 logical cores. We implement our experiments with Java 8 and the Java HotSpot 64-bit Server VM.\nWe conduct experiments in both the sequential case (N = 1) as well as in parallel. For the parallel runs, we consider both N = 4 and N = 8 parallel instances. We record the best solution found at 1 second intervals over 60 seconds.\nWe compare our VAL and P-VAL to multistart SA with fixed annealing length (FAL and P-FAL in parallel). We consider several fixed annealing lengths. FAL-1 and P-FAL-1 use an annealing length tuned to the total available time (60 seconds), specifically runs of 108 million SA evaluations. This threshold was determined based on the total number of evaluations that VAL was able to do in 60 seconds. Likewise, FAL-1/2, FAL-1/4, FAL-1/8, use annealing lengths that are 1/2, 1/4, and 1/8 of the available 60 second limit (54 million, 27 million, and 13.5 million SA evaluations, respectively), restarting at that same length as long as time remains. P-FAL-1, P-FAL-1/2, P-FAL-1/4, P-FAL-1/8 are equivalent to a best of N , 2N , 4N , and 8N independent runs, respectively, with approximately the same total cost.\nWe represent solutions as permutations, and use Insertion Mutation as our neighborhood function. This operator removes a random element, and reinserts it at a different random position. Several existing metaheuristics for this scheduling problem use this operator, and it has been shown\neffective for permutation optimization problems characterized by asymmetric edges and general position within the permutation (Cicirello 2016). This problem has both: sequence-dependent setups (asymmetric edges), and due dates influence general position within permutation.\nWe use the following commonly employed metrics in the analysis of our experiments for this scheduling problem. Most commonly reported is the average percentage deviation from the optimal solutions, averaged only across the 98 instances with non-zero optimal values:\n%∆Opt = 100\nN N∑ i=1 (Si −Oi) Oi , (9)\nwhere Si andOi are the value of the solution found for problem instance i and its optimal solution, respectively. One issue with this metric is that it ignores the 22 instances whose optimal solutions have weighted tardiness of 0. Thus, we also report the percentage deviation of the sum across all 120 instances relative to the sum of the optimal solutions:\n%∆OptSum = 100 ∑N i=1 Si − ∑N i=1Oi∑N\ni=1Oi . (10)\nUsing each algorithm, we optimize each instance 10 times. We use t-tests to test the significance of the %∆OptSum results. We use the Wilcoxon signed rank test to test the significance of the %∆Opt. Since %∆Opt is an average across multiple problem instances with values of varying scale, the t-test’s normality requirement is not met."
    }, {
      "heading" : "4.3 Sequential Results",
      "text" : "The results for sequential SA are summarized in Figures 2 and 3, which show average %∆Opt and %∆OptSum, respectively, throughout the duration of the 60 second runs. Early in the run, VAL dominates, and then performance approximately tracks that of each progressively longer fixed annealing length.\nOn the %∆Opt metric, VAL dominates early in the run, but the FAL variations overtake it at approximately the time corresponding to the annealing length for which they were tuned. However, as VAL’s longer runs complete, VAL’s performance then matches that of the fixed length restarts. For\nexample, consider VAL versus FAL-1/8. For the first 6 seconds, VAL strongly dominates at extremely significant levels (p-values: < 10−18). FAL-1/8 then outperforms VAL for approximately 2 seconds at significant levels (p-value at second 8 is < 0.0001). From that point onward, there are periods where VAL’s increasingly longer runs enable it to gain a performance advantage over FAL-1/8 (at significant levels), and stretches with no significant performance difference. Next, consider VAL versus FAL-1, fixed annealing length as long as the experiment. FAL-1 catches up to VAL in performance (on %∆Opt) at second 50, and its end of run performance is best among all variations considered. However, the end of run differences are not significant. At one second intervals, from second 48 to the end of the run, pvalues (from Wilcoxon signed rank test) between VAL and FAL-1 are no lower than 0.06 and are as high as 0.86.\nThe results on %∆OptSum are similar. For example, for the first 48 seconds, VAL outperforms FAL-1 at significant levels (t-test p-values from near zero early on to p = 0.018 at second 48). However, from second 49 to the end of the run, the difference in performance between VAL and FAL-1 is not significant (p-values > 0.35).\nFigure 4 shows the number of optimal solutions found out of 1200 runs as a function of time. VAL dominates early in the run, finding more optimal solutions than the others. At the end of the run, the FAL variations all find optimal solutions slightly more often than VAL.\nIn the sequential case, the restart schedule of annealing lengths enables SA to approximate the performance of long runs near the end of the run, while simultaneously obtaining huge performance gains earlier in the run."
    }, {
      "heading" : "4.4 Parallel Results: 4 Parallel Instances",
      "text" : "Figures 5 and 6 show average %∆Opt and %∆OptSum, respectively, for the duration of the 60 second runs for N = 4 parallel instances. Among the P-FAL variations, P-FAL-1 has the best performance on both metrics at the end of the run. However, the end of run differences among the P-FAL variations are not statistically significant (p-values > 0.13 for %∆Opt and p-values > 0.08 for %∆OptSum). Early in the run, P-VAL dominates relative to any fixed annealing length. For 90% of the run, P-VAL strongly dominates PFAL-1, and dominates P-FAL-1/2 for nearly half the run, on\nboth metrics, and similarly for P-FAL-1/4 and P-FAL-1/8. But late in the run, P-VAL does not match the performance of fixed annealing length as well as it did in the sequential case. Once the fixed annealing length is reached, P-FAL outperforms to end of run, while P-VAL exhibits superior performance during the run. All differences in performance between P-VAL and the P-FAL variations at each 1 second interval, except where the P-FAL curves cross the P-VAL curve, are statistically significant (p-values < 0.0001).\nAlthough in the sequential case, VAL approximates the end of run performance of long fixed length runs, in parallel P-VAL is outperformed at the end of the run by the long fixed length runs. P-VAL’s advantage, however, is in improved anytime performance early in the run."
    }, {
      "heading" : "4.5 P-VAL and P-VAL-0: 8 Parallel Instances",
      "text" : "Earlier, we indicated that deficiencies exist in P-VAL-0 when N > 4. We consider this further here. First, examine the performance of P-VAL-0 relative to P-VAL in the case of 8 parallel instances. Figures 7 and 8 show average %∆Opt and %∆OptSum, respectively, for the duration of the 60 second runs. P-VAL strongly dominates P-VAL-0 throughout the run, on both metrics, at extremely statistically significant levels (p-values < 0.00001 at every one second interval).\nWhy is this the case? For P-VAL-0, restart r of SAi is of length 1000∗2i+r∗N , and completes at time proportional to:\nCi(r) = 1000 r∑ j=0 2i+j∗N = 1000∗2i∗2 N(r+1) − 1 2N − 1 , (11)\nwhich is the sum of the annealing lengths up to and including restart r. The restart, r0, of the sequential VAL that is of length 1000 ∗ 2i+r∗N is r0 = i + r ∗ N , and completes at time proportional to:\nC0(r0) = 1000 i+r∗N∑ j=0 2j = 1000 ∗ (2i+r∗N+1 − 1), (12)\nthe sum of the run lengths up to and including restart r0. If parallel speedup was impacted only by completing longer runs sooner, then the expected speedup factor is:\nC0(r0)\nCi(r) = (2i+r∗N+1 − 1)(2N − 1) 2i(2N(r+1) − 1) . (13)\nIn the limit as the number of restarts r grows large, the speedup due to completing longer runs earlier is:\nlim r→∞\nC0(r0)\nCi(r) = 2N − 1 2N−1 . (14)\nFor N = 4, the anticipated speedup from completing longer runs earlier is 1.875, and for N = 8 is approximately 1.992. In fact, in the limit as the number of parallel instances N → ∞, the speedup factor approaches 2.0. Specifically, the longest completed restart finishes in half the time relative to the sequential VAL. With N = 4, P-VAL-0 is already approaching this limiting behavior, maximizing the benefit from shortening the time for the longest runs to complete.\nWe experimentally examine this in Figures 9 and 10, which show average %∆Opt and %∆OptSum, respectively,\nfor a single sequential instance of VAL, P-VAL for both N = 4 and N = 8 parallel instances, as well as P-VAL-0 for N = 8 (recall that P-VAL and P-VAL-0 are identical for N ≤ 4). Visually, it is impossible to distinguish the P-VAL0 results with N = 8 from the N = 4 case for both metrics. The performance differences between these two cases are not significant. Using more than 4 parallel instances with P-VAL-0 does not improve performance.\nThe time for P-VAL with N = 4 to reach a given %∆Opt (and likewise %∆OptSum) is approximately one-half to one-third the time taken by the sequential VAL (speedup factor between 2 and 3), rather than the one-fourth we would expect from linear speedup. This is consistent with the analysis above of completion time of the longest runs. The speedup factor for P-VAL with N = 8 is approximately 4 (the sequential VAL takes approximately 4 times as long to reach equivalent levels of %∆Opt and %∆OptSum).\nAs you can see, as we increase N from 1 to 4 and then to 8, the difference in performance for P-VAL relative to VAL is consistent throughout the run. The speedup, through parallelization, is sublinear, but not limited by number of parallel instances."
    }, {
      "heading" : "4.6 Parallel Results: 8 Parallel Instances",
      "text" : "Now consider N = 8 parallel instances. Figures 11 and 12 show average %∆Opt and %∆OptSum, respectively, for the duration of the 60 second runs. For most of the run, P-VAL very strongly dominates. Each of the P-FAL variations eventually overtake P-VAL as they near their tuned annealing\nlengths. For example, P-FAL-1/2 overtakes P-VAL at second 26 on %∆OptSum and second 28 on %∆Opt, and PFAL-1 overtakes P-VAL at second 51 on %∆OptSum and second 56 on %∆Opt. Though not evident from the scale of the graphs, P-FAL-1 outperforms all others by end of run, at statistically significant levels, on both metrics. We should expect P-FAL-1 to perform best at the end, as P-FAL-1 executes multiple long runs in parallel, providing the Modified Lam schedule with the actual experimental run length. This is also consistent with other findings that show a long run of SA is usually better than multiple independent short runs.\nHowever, for 90% of the run, P-VAL strongly dominates P-FAL-1, and dominates P-FAL-1/2 for nearly half the run, on both metrics, and similarly for P-FAL-1/4 and P-FAL1/8. P-VAL exhibits superior performance during the run.\nThe %∆OptSum differences between P-VAL and P-FAL1 are statistically significant (t-test p-values< 0.04) at every one-second interval; and the %∆Opt results are significant (Wilcoxon signed rank test, p-values ranging from near-zero to 0.008), except where the curves cross (p = 0.57). The differences between P-VAL and each of P-FAL-1/2, P-FAL1/4, and P-FAL-1/8 are statistically significant at every 1 second interval."
    }, {
      "heading" : "4.7 On the Efficacy of Reannealing",
      "text" : "Thus far, all experimental results use independent restarts from random starting solutions, with no data sharing among parallel instances. We now explore what, if any, benefit is gained from reannealing prior solutions. Specifically, we\nconsider VAL(R), where each restart reanneals the best of run solution rather than a random one. P-VAL(R) reanneals the current best of run solution across all parallel instances. Likewise, FAL-1/8(R) is FAL-1/8, but with reannealing of the best of run solution (similarly for P-FAL-1/8(R)).\nThe sequential results are found in Figures 13 and 14, and the parallel results (for N = 8) are in Figures 15 and 16. For variable annealing lengths, both sequential and parallel, the differences in performance between reannealing and random starting solutions are not statistically significant, except for the parallel case during the last 15-20 seconds of the run, where reannealing leads to marginally better results at statistically significant levels (p-values less than 0.01). Visually, in the graphs, it is virtually impossible to distinguish these. For fixed annealing length, both sequential and parallel, the differences in performance with and without reannealing are not statistically significant. Reannealing good solutions does not provide any benefit over independent runs in either the sequential or parallel case."
    }, {
      "heading" : "5 Conclusions",
      "text" : "In this paper, we proposed a restart schedule for SA using the modified Lam annealing schedule. Our restart schedule eliminates the need to know the annealing length a priori. Relying on the often demonstrated property of SA that single long runs typically outperform multiple short runs, our restart schedule increases the annealing length at an exponential rate. The shorter runs at the beginning enable quickly finding “good” solutions, while the increasing an-\nnealing lengths of the restarts enable approximating the end of run performance of a single long run of SA.\nOur restart schedule supports parallel implementation, using parallel independent SA instances that vary in initial annealing length, and with exponentially increasing restart lengths. The initial annealing lengths are staggered to ensure that longer runs are already in progress as shorter runs complete. Our aim is to balance the risk associated with errors in determining the time available for problem solving.\nThe end-of-run behavior observed in experiments, both sequential as well as in parallel, with a sequence-dependent scheduling problem confirm the commonly found property that a longer SA run outperforms restarts of a shorter run. However, performance during the run is often overlooked. For example, although FAL-1 performs best at the end of run, FAL-1/2 achieved better results at the mid-way point. Our annealing length schedule VAL, and in parallel P-VAL, exhibited stronger anytime behavior throughout the run."
    } ],
    "references" : [ {
      "title" : "and Moser",
      "author" : [ "A. Aleti" ],
      "venue" : "I.",
      "citeRegEx" : "Aleti and Moser 2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "J",
      "author" : [ "Boyan" ],
      "venue" : "A.",
      "citeRegEx" : "Boyan 1998",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "S",
      "author" : [ "V.A. Cicirello", "Smith" ],
      "venue" : "F.",
      "citeRegEx" : "Cicirello and Smith 2005",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Boosting Stochastic Problem Solvers Through Online Self-Analysis of Performance",
      "author" : [ "V.A. Cicirello 2003a] Cicirello" ],
      "venue" : null,
      "citeRegEx" : "Cicirello,? \\Q2003\\E",
      "shortCiteRegEx" : "Cicirello",
      "year" : 2003
    }, {
      "title" : "Weighted tardiness scheduling with sequence-dependent setups: A benchmark library",
      "author" : [ "V.A. Cicirello 2003b] Cicirello" ],
      "venue" : "Tech. report, ICL Lab, CMU. http://www. cicirello.org/datasets/wtsds/",
      "citeRegEx" : "Cicirello,? \\Q2003\\E",
      "shortCiteRegEx" : "Cicirello",
      "year" : 2003
    }, {
      "title" : "2006",
      "author" : [ "Cicirello", "V. A" ],
      "venue" : "Non-wrapping order crossover: An order preserving crossover operator that respects absolute position. In Proc. GECCO’06. ACM. 1125–",
      "citeRegEx" : "Cicirello 2006",
      "shortCiteRegEx" : null,
      "year" : 1131
    }, {
      "title" : "V",
      "author" : [ "Cicirello" ],
      "venue" : "A.",
      "citeRegEx" : "Cicirello 2007",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "V",
      "author" : [ "Cicirello" ],
      "venue" : "A.",
      "citeRegEx" : "Cicirello 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "V",
      "author" : [ "Cicirello" ],
      "venue" : "A.",
      "citeRegEx" : "Cicirello 2016",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "A",
      "author" : [ "Cire" ],
      "venue" : "A.; Kadioglu, S.; and Sellmann, M.",
      "citeRegEx" : "Cire. Kadioglu. and Sellmann 2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A",
      "author" : [ "Eiben" ],
      "venue" : "E.; Hinterding, R.; and Michalewicz, Z.",
      "citeRegEx" : "Eiben. Hinterding. and Michalewicz 1999",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "C",
      "author" : [ "Gomes" ],
      "venue" : "P.; Selman, B.; Crato, N.; and Kautz, H.",
      "citeRegEx" : "Gomes et al. 2000",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "and Menon",
      "author" : [ "S. Jha" ],
      "venue" : "V.",
      "citeRegEx" : "Jha and Menon 2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "and Delosme",
      "author" : [ "J. Lam" ],
      "venue" : "J.",
      "citeRegEx" : "Lam and Delosme 1988",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "and Juan",
      "author" : [ "Liao", "C.-J." ],
      "venue" : "H.-C.",
      "citeRegEx" : "Liao and Juan 2007",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Neighborhood search procedures for single machine tardiness scheduling with sequencedependent setups",
      "author" : [ "Tsou Liao", "C.-J. Huang 2012] Liao", "H.-H. Tsou", "K.-L. Huang" ],
      "venue" : "Theoretical Comp. Sci",
      "citeRegEx" : "Liao et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Liao et al\\.",
      "year" : 2012
    }, {
      "title" : "Optimal speedup of las vegas algorithms. Information Processing Letters 47(4):173–180",
      "author" : [ "Sinclair Luby", "M. Zuckerman 1993] Luby", "A. Sinclair", "D. Zuckerman" ],
      "venue" : null,
      "citeRegEx" : "Luby et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Luby et al\\.",
      "year" : 1993
    }, {
      "title" : "and Betz",
      "author" : [ "A. Ludwin" ],
      "venue" : "V.",
      "citeRegEx" : "Ludwin and Betz 2011",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "D",
      "author" : [ "T.E. Morton", "Pentico" ],
      "venue" : "W.",
      "citeRegEx" : "Morton and Pentico 1993",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "A",
      "author" : [ "Rahimian, F.", "Payberah" ],
      "venue" : "H.; Girdzijauskas, S.; Jelasity, M.; and Haridi, S.",
      "citeRegEx" : "Rahimian et al. 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "K",
      "author" : [ "D.J. Ram", "T.H. Sreenivas", "Subramaniam" ],
      "venue" : "G.",
      "citeRegEx" : "Ram. Sreenivas. and Subramaniam 1996",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "and Bagchi",
      "author" : [ "A.K. Sen" ],
      "venue" : "A.",
      "citeRegEx" : "Sen and Bagchi 1996",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "W",
      "author" : [ "Swartz" ],
      "venue" : "P.",
      "citeRegEx" : "Swartz 1993",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "and Araki",
      "author" : [ "S. Tanaka" ],
      "venue" : "M.",
      "citeRegEx" : "Tanaka and Araki 2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "When parameter tuning actually is parameter control",
      "author" : [ "Preuss Wessing", "S. Rudolph 2011] Wessing", "M. Preuss", "G. Rudolph" ],
      "venue" : "In Proc. GECCO,",
      "citeRegEx" : "Wessing et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Wessing et al\\.",
      "year" : 2011
    }, {
      "title" : "T",
      "author" : [ "H. Xu", "Z. Lü", "Cheng" ],
      "venue" : "C.",
      "citeRegEx" : "Xu. Lü. and Cheng 2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    } ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "In this paper, we propose: (a) a restart schedule for an adaptive simulated annealer, and (b) parallel simulated annealing, with an adaptive and parameter-free annealing schedule. The foundation of our approach is the Modified Lam annealing schedule, which adaptively controls the temperature parameter to track a theoretically ideal rate of acceptance of neighboring states. A sequential implementation of Modified Lam simulated annealing is almost parameter-free. However, it requires prior knowledge of the annealing length. We eliminate this parameter using restarts, with an exponentially increasing schedule of annealing lengths. We then extend this restart schedule to parallel implementation, executing several Modified Lam simulated annealers in parallel, with varying initial annealing lengths, and our proposed parallel annealing length schedule. To validate our approach, we conduct experiments on an NP-Hard scheduling problem with sequence-dependent setup constraints. We compare our approach to fixed length restarts, both sequentially and in parallel. Our results show that our approach can achieve substantial performance gains, throughout the course of the run, demonstrating our approach to be an effective anytime algorithm.",
    "creator" : "LaTeX with hyperref package"
  }
}