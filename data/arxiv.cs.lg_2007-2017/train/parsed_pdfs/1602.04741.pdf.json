{
  "name" : "1602.04741.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Delay and Cooperation in Nonstochastic Bandits",
    "authors" : [ "Nicolò Cesa-Bianchi", "Claudio Gentile", "Yishay Mansour", "MANSOUR MINORA" ],
    "emails" : [ "NICOLO.CESA-BIANCHI@UNIMI.IT", "CLAUDIO.GENTILE@UNINSUBRIA.IT", "MANSOUR@TAU.AC.IL", "AMINORA@UNINSUBRIA.IT" ],
    "sections" : [ {
      "heading" : null,
      "text" : "at most of order √( d+ 1 + KN α≤d ) (T lnK), where α≤d is the independence number of the d-th power of the communication graphG. We then show that for any connected graph, for d = √ K the regret bound is K1/4 √ T , strictly better than the minimax regret √ KT for noncooperating agents. More informed choices of d lead to bounds which are arbitrarily close to the full information minimax regret √ T lnK when G is dense. When G has sparse components, we show that a variant of EXP3-COOP, allowing agents to choose their parameters according to their centrality in G, strictly improves the regret. Finally, as a by-product of our analysis, we provide the first characterization of the minimax regret for bandit learning with delay."
    }, {
      "heading" : "1. Introduction",
      "text" : "Delayed feedback naturally arises in many sequential decision problems. For instance, a recommender system typically learns the utility of a recommendation by detecting the occurrence of certain events (e.g., a user conversion), which may happen with a variable delay after the recommendation was issued. Other examples are the communication delays experienced by interacting learning agents. Concretely, consider a network of geographically distributed ad servers using real-time bidding to sell their inventory. Each server sequentially learns how to set the auction parameters (e.g., reserve price) in order to maximize the network’s overall revenue, and shares feedback information with other servers in order to speed up learning. However, the rate at which information is exchanged through the communication network is slower than the typical rate at which ads are\nc© 2016. Cesa-Bianchi, C. Gentile, Y. Mansour & A. Minora.\nar X\niv :1\n60 2.\n04 74\n1v 2\n[ cs\n.L G\n] 1\nserved. This causes each learner to acquire feedback information from other servers with a delay that depends on the network’s structure.\nMotivated by the ad network example, we consider networks of learning agents that cooperate to solve the same nonstochastic bandit problem, and study the impact of delay on the global performance of these agents. We introduce the EXP3-COOP algorithm, a distributed and cooperative version of the EXP3 algorithm of Auer et al. (2002). EXP3-COOP works within a distributed and synced model where each agent runs an instance of the same bandit algorithm (EXP3). All bandit instances are initialized in the same way irrespective to the agent’s location in the network (that is, agents have no preliminary knowledge of the network), and we assume the information about an agent’s actions is propagated through the network with a unit delay for each crossed edge. In each round t, each agent selects an action and incurs the corresponding loss (which is the same for all agents that pick that action in round t). Besides observing the loss of the selected action, each agent obtains the information previously broadcast by other agents with a delay equal to the shortest-path distance between the agents. Namely, at time t an agent learns what the agents at shortest-path distance s did at time t− s for each s = 1, . . . , d, where d is a delay parameter. In this scenario, we aim at controlling the growth of the regret averaged over all agents (the so-called average welfare regret).\nIn the noncooperative case, when agents ignore the information received from other agents, the average welfare regret grows like √ KT (the minimax rate for standard bandit setting), whereK is the number of actions and T is the time horizon. We show that, using cooperation, N agents with com-\nmunication graph G can achieve an average welfare regret of order √( d+ 1 + KN α≤d ) (T lnK). Here α≤d denotes the independence number of the d-th power of G (i.e., the graph G augmented with all edges between any two pair of nodes at shortest-path distance less than or equal to d). When d = √ K this bound is at mostK1/4 √ T lnK+ √ K(lnT ) for any connected graph —see Remark 7 in Section 4.1— which is asymptotically better than √ KT .\nNetworks of nonstochastic bandits were also investigated by Awerbuch and Kleinberg (2008) in a setting where the distribution over actions is shared among the agents without delay. Awerbuch and Kleinberg (2008) prove a bound on the average welfare regret of order √( 1 + KN ) T ignoring polylog factors.1 We recover the same bound as a special case of our bound when G is a clique and\nd = 1. In the clique case our bound is also similar to the bound √\nK N (T lnK) achieved by Seldin\net al. (2014) in a single-agent bandit setting where, at each time step, the agent can choose a subset ofN ≤ K actions and observe their loss. In the case whenN = 1 (single agent), our analysis can be applied to the nonstochastic bandit problem where the player observes the loss of each played action with a delay of d steps. In this case we improve on the previous result of √ (d+ 1)KT by Neu et al. (2010, 2014), and give the first characterization (up to logarithmic factors) of the minimax regret, which is of order √ (d+K)T .\nIn principle, the problem of delays in online learning could be tackled by simple reductions. Yet, these reductions give rise to suboptimal results. In the single agent setting, where the delay is constant and equal to d, one can use the technique of Weinberger and Ordentlich (2002) and run d+1 in-\n1. The rate proven in (Awerbuch and Kleinberg, 2008, Theorem 2.1) has a worse dependence on T , but we believe this is due to the fact that their setting allows for dishonest agents and agent-specific loss vectors.\nstances of an online algorithm for the nondelayed case, where each instance is used every d+1 steps. This delivers a suboptimal regret bound of √ (d+ 1)KT . In the case of multiple delays, like in our multi-agent setting, one can repeat the same action for d+ 1 steps while accumulating information from the other agents, and then perform an update on scaled-up losses. The resulting (suboptimal)\nbound on the average welfare regret would be of the form √ (d+ 1) ( 1 + KN α≤d ) (T lnK).\nRather than using reductions, the analysis of EXP3-COOP rests on quantifying the performance of suitable importance weighted estimates. In fact, in the single-agent setting with delay parameter d, using EXP3-COOP reduces to running the standard EXP3 algorithm performing an update as soon a new loss becomes available. This implies that at any round t > d, EXP3 selects an action without knowing the losses incurred during the last d rounds. The resulting regret is bounded by relating the standard analysis of EXP3 to a detailed quantification of the extent to which the distribution maintained by EXP3 can drift in d steps.\nIn the multi-agent case, the importance weighted estimate of EXP3-COOP is designed in such a way that at each time t > d the instance of the algorithm run by an agent v updates all actions that were played at time t − d by agent v or by other agents not further away than d from v. Compared to the single agent case, here each agent can exploit the information circulated by the other agents. However, in order to compute the importance weighted estimates used locally by each agent, the probabilities maintained by the agents must be propagated together with the observed losses. Here, further concerns may show up, like the amount of communication, and the location of each agent within the network. In particular, when G has sparse components, we show that a variant of EXP3COOP, allowing agents to choose their parameters according to their centrality within G, strictly improves on the regret of EXP3-COOP."
    }, {
      "heading" : "2. Additional Related Work",
      "text" : "Many important ideas in delayed online learning, including the observation that the effect of delays can be limited by controlling the amount of change in the agent strategy, were introduced by Mesterharm (2005) —see also (Mesterharm, 2007, Chapter 8). A more recent investigation on delayed online learning is due to Neu et al. (2010, 2014), who analyzed exponential weights with delayed feedbacks. Furher progress is made by Joulani et al. (2013), who also study delays in the general partial monitoring setting. Additional works (Joulani et al., 2016; Quanrud and Khashabi, 2015) prove regret bounds for the full-information case of the form √ (D + T ) lnK, where D is the total delay experienced over the T rounds. In the stochastic case, bandit learning with delayed feedback was considered by Dudı́k et al. (2011); Joulani et al. (2013).\nTo the best of our knowledge, the first paper about nonstochastic cooperative bandit networks is (Awerbuch and Kleinberg, 2008). More papers analyze the stochastic setting, and the closest one to our work is perhaps (Szorenyi et al., 2013). In that paper, delayed loss estimates in a network of cooperating stochastic bandits are analyzed using a dynamic P2P random networks as communication model. A more recent paper is (Landgren et al., 2015), where the communication network is a fixed graph and a cooperative version of the UCB algorithm is introduced which uses a distributed consensus algorithm to estimate the mean rewards of the arms. The main result is an individual (per-agent) regret bound that depends on the network structure without taking delays into account.\nAnother interesting paper about cooperating bandits in a stochastic setting is (Kar et al., 2011). Similar to our model, agents sit on the nodes of a communication network. However, only one designated agent observes the rewards of actions he selects, whereas the others remain in the dark. This designated agent broadcasts his sampled actions through the networks to the other agents, who must learn their policies relying only on this indirect feedback. The paper shows that in any connected network this information is sufficient to achieve asymptotically optimal regret. Cooperative bandits with asymmetric feedback are also studied by Barrett and Stone (2011). In their model, an agent must teach the reward distribution to another agent while keeping the discounted regret under control. Tekin and van der Schaar (2015) investigate a stochastic contextual bandit model where each agent can either privately select an action or have another agent select an action on his behalf. In a related paper, Tekin et al. (2014) look at a stochastic bandit model with combinatorial actions in a distributed recommender system setting, and study incentives among agents who can now recommend items taken from other agents’ inventories. Another line of relevant work involves problems of decentralized bandit coordination. For example, Stranders et al. (2012) consider a bandit coordination problem where the the reward function is global and can be represented as a factor graph in which each agent controls a subset of the variables. A parallel thread of research concerns networks of bandits that compete for shared resources. A paradigmatic application domain is that of cognitive radio networks, in which a number of channels are shared among many users and any two or more users interfere whenever they simultaneously try to use the same channel. The resulting bandit problem is one of coordination in a competitive environment, because every time two or more agents select the same action at the same time step they both get a zero reward due to the interference —see (Rosenski et al., 2015) for recent work on stochastic competitive bandits and (Kleinberg et al., 2009) for a study of more general congestion games in a game-theoretic setting. Finally, there exists an extensive literature on the adaptation of gradient descent and related algorithms to distributed computing settings, where asynchronous processors naturally introduce delays —see, e.g., (Zinkevich et al., 2009; Agarwal and Duchi, 2011; Li et al., 2013; McMahan and Streeter, 2014; Quanrud and Khashabi, 2015; Liu et al., 2015; Duchi et al., 2015). However, none of these works considers bandit settings, which are an essential ingredient for our analysis."
    }, {
      "heading" : "3. Preliminaries",
      "text" : "We now establish our notation, along with basic assumptions and preliminary facts related to our algorithms. Notation and setting here both refer to the single agent case. The cooperative setting with multiple agents (and notation thereof) will be introduced in Section 4. Proofs of all the results stated here can be found in (Cesa-Bianchi et al., 2016).\nLet A = {1, . . . ,K} be the action set. A learning agent runs an exponentially-weighted algorithm with weights wt(i), and learning rate η > 0. Initially, w1(i) = 1 for all i ∈ A. At each time step t = 1, 2, . . . , the agent draws action It with probability P(It = i) = pt(i) = wt(i)/Wt, where Wt = ∑ j∈Awt(j). After observing the loss `t(It) ∈ [0, 1] associated with the chosen action It, and possibly some additional information, the agent computes, for each i ∈ A, nonnegative loss estimates ̂̀t(i), and performs the exponential update\nwt+1(i) = pt(i) exp ( −η ̂̀t(i)) (1)\nto these weights. The following two lemmas are general results that control the evolution of the probability distributions in the exponentially-weighted algorithm. As we said in the introduction, bounding the extent to which the distribution used by our algorithms can drift in d steps is key to controlling regret in a delayed setting. The first result bounds the additive change in the probability of any action, and it holds no matter how ̂̀t(i) is defined. Lemma 1 Under the update rule (1), for all t ≥ 1 and for all i ∈ A,\n−η pt(i)̂̀t(i) ≤ pt+1(i)− pt(i) ≤ η pt+1(i)∑ j∈A pt(j)̂̀t(j) holds deterministically with respect to the agent’s randomization.\nThe second result delivers a multiplicative bound on the change in the probability of any action when the loss estimates ̂̀t(i) are of the following form:\n̂̀ t(i) =  `t−d(i) qt−d(i) Bt−d(i) if t > d,\n0 otherwise , (2)\nwhere d ≥ 0 is a delay parameter, Bt−d(i) ∈ {0, 1}, for i ∈ A, are indicator functions, and qt−d(i) ≥ pt−d(i) for all i and t > d. In all later sections, Bt−d(i) will be instantiated to the indicator function of the event that action i has been played at time t−d by some agent, and qt−d(i) will be the (conditional) probability of this event. Lemma 2 Let ̂̀t(i) be of the form (2) for each t ≥ 1 and i ∈ A. If η ≤ 1Ke(d+1) in the update rule (1), then\npt+1(i) ≤ ( 1 + 1\nd\n) pt(i)\nholds for all t ≥ 1 and i ∈ A, deterministically with respect to the agent’s randomization.\nAs we said in Section 1, the idea of controlling the drift of the probabilities in order to bound the effects of delayed feedback is not new. In particular, variants of Lemma 1 were already derived in the work of Neu et al. (2010, 2014). However, Lemma 2 appears to be new, and this is the key result to achieving our improvements."
    }, {
      "heading" : "4. The Cooperative Setting on a Communication Network",
      "text" : "In our multi-agent bandit setting, there are N agents sitting on the vertices of a connected and undirected communication graph G = (V,E), with V = {1, . . . , N}. The agents cooperate to solve the same instance of a nonstochastic bandit problem while limiting the communication among them. Let Ns(v) be the set of nodes v′ ∈ V whose shortest-path distance distG(v, v′) from v in G is exactly s. At each time step t = 1, 2, . . . , each agent v ∈ V draws an action It(v) from the common action set A. Note that each action i ∈ A delivers the same loss `t(i) ∈ [0, 1] to all agents v such that It(v) = i. At the end of round t, each agent v observes his own loss `t ( It(v) ) , and sends to his neighbors in G the message\nmt(v) = 〈 t, v, It(v), `t ( It(v) ) ,pt(v) 〉\nwhere pt(v) = ( pt(1, v), . . . , pt(K, v) ) is the distribution of It(v). Moreover, v also receives from his neighbors a variable number of messages mt−s(v′). Each message mt−s(v′) that v receives from a neighbor is used to update pt(v) and then forwarded to the other neighbors only if s < d, otherwise it is dropped.2 Here d is the maximum delay, a parameter of the communication protocol. Therefore, at the end of round t, each agent v receives one message mt−s(v′) for each agent v′ such that distG(v, v′) = s, where s ∈ {1, . . . , d}. Graph G can thus be seen as a synchronous multi-hop communication network where messages are broadcast, each hop causing a delay of one time step. Our learning protocol is summarized in Figure 1, while Figure 2 contains a pictorial example.\nOur model is similar to the LOCAL communication model in distributed computing (Linial, 1992; Suomela, 2013), where the output of a node depends only on the inputs of other nodes in a constantsize neighborhood of it, and the goal is to derive algorithms whose running time is independent of the network size. (The main difference is that the task here has no completion time, however, also in our model influence on a node is only through a constant-size neighborhood of it.)\nOne aspect deserving attention is that, apart from the common delay parameter d, the agents need not share further information. In particular, the agents need not know neither the topology of the graph G nor the total number of agents N . In Section 5, we show that our distributed algorithm can also be analyzed when each agent v uses a personalized delay d(v), thus doing away with the need of a common delay parameter, and guaranteeing a generally better performance.\nFurther graph notation is needed at this point. Given G as above, let us denote by G≤d the graph (V,E≤d) where (u, v) ∈ E≤d if and only if the shortest-path distance between agents u and v in G is at most d (hence G≤1 = G). Graph G≤d is sometimes called the d-th power of G. We also use G0 to denote the graph (V, ∅). Recall that an independent set of G is any subset T ⊆ V such that no two i, j ∈ T are connected by an edge in E. The largest size of an independent set is the independence number of G, denoted by α(G). Let dG be the diameter of G (maximal length over all possible shortest paths between all pairs of nodes); then G≤dG is a clique, and one can easily see that N = α(G0) > α(G) ≥ α(G≤2) ≥ · · · ≥ α(G≤dG) = 1. We show in Section 4.1 that the\n2. Dropping messages older than d rounds is clearly immaterial with respect to proving bandit regret bounds. We added this feature just to prove a point about the message complexity of the protocol. See Remark 8 in Section 5 for further discussion.\ncollective performance of our algorithms depends on α(G≤d). If the graph G under consideration is directed (see Section 5), then α(G) is the independence number of the undirected graph obtained from G by disregarding edge orientation.\nThe adversary generating losses is oblivious: loss vectors `t = ( `t(1), . . . , `t(K) ) ∈ [0, 1]K do not depend on the agents’ internal randomization. The agents’ goal is to control the average welfare regret RcoopT , defined as\nRcoopT =\n( 1\nN ∑ v∈V E [ T∑ t=1 `t ( It(v) )] −min i∈A T∑ t=1 `t(i) ) ,\nthe expectation being with respect to the internal randomization of each agent’s algorithm. In the sequel, we write Et[·] to denote the expectation w.r.t. the product distribution ∏ v∈V pt(v), conditioned on I1(v), . . . , It−1(v), v ∈ V ."
    }, {
      "heading" : "4.1. The Exp3-Coop algorithm",
      "text" : "Our first algorithm, called EXP3-COOP (Cooperative Exp3) is described in Figure 3. The algorithm works in the learning protocol of Figure 1. Each agent v ∈ V runs the exponentially-weighted algorithm (1), combined with a “delayed” importance-weighted loss estimate ̂̀t(i, v) that incorporates the delayed information sent by the other agents. Specifically, denote by N≤d(v) = ⋃ s≤dNs(v) the set of nodes in G whose shortest-path distance from v is at most d, and note that, for all v, {v} = N≤0(v) ⊆ N≤1(v) ⊆ N≤2(v) ⊆ · · · . If any of the agents in N≤d(v) has played at time t − d action i (that is, Bd,t−d(i, v) = 1 in Eq. in (3)), then the corresponding loss `t−d(i) is incorporated by v into ̂̀t(i, v). The denominator qd,t−d(i, v) is simply, conditioned on the history, the probability of Bd,t−d(i, v) = 1, i.e., qd,t−d(i, v) = Et[Bd,t−d(i, v)]. Observe that {v} ⊆ N≤d(v) for all d ≥ 0 implies qd,t−d(i, v) ≥ pt−d(i, v), as required by (2). It is also worth mentioning that, despite this is not strictly needed by our learning protocol, each agent v actually exploits the loss information gathered from playing action It(v) only d time steps later. A relevant special case of this learning mode is when we only have a single bandit agent receiving delayed feedback (Section 6).\nBy their very definition, the loss estimates ̂̀t(·, ·) at time t are determined by the realizations of Is(·), for s = 1, . . . , t − d. This implies that the numbers pt(·, ·) defining qd,t−d(·, ·), are determined by the realizations of Is(·) for s = 1, . . . , t − d − 1 (because the probabilities pt(v) at time t are\ndetermined by the loss estimates up to time t − 1, see (1)). We have, for all t > d, i ∈ A, and v ∈ V , Et−d [̂̀ t(i, v) ] = `t−d(i) . (4)\nFurther, because of what we just said about pt(·, ·) and qd,t−d(·, ·) being determined by I1(·), . . . , It−d−1(·), we also have\nEt−d [ pt(i, v)̂̀t(i, v)] = pt(i, v)`t−d(i) , Et−d[pt(i, v)̂̀t(i, v)2] = pt(i, v) `t−d(i)2\nqd,t−d(i, v) . (5)\nThe following theorem quantifies the behavior of EXP3-COOP in terms of a free parameter γ in the learning rate, the tuning of which will be addressed in the subsequent Theorem 4.\nTheorem 3 The regret of EXP3-COOP run over a network G = (V,E) of N agents, each using delay d and learning rate η = γ /( Ke(d+ 1) ) , for γ ∈ (0, 1], satisfies\nRcoopT ≤ 2d+ Ke(d+ 1) lnK\nγ + γ\n( α(G≤d)\n2(1− e−1)(d+ 1)N +\n3\nKe\n) T .\nWith this bound handy, we might be tempted to optimize for γ. However, this is not a legal learning rate setting in a distributed scenario, for the optimized value of γ would depend on the global\nquantities N and α(G≤d). Thus, instead of this global tuning, we let each agent set its own learning rate γ through a “doubling trick” played locally. The doubling trick3 works as follows. For each v ∈ V , we let γr(v) = Ke(d+1) √ (lnK)/2r for each r = r0, r0+1, . . . , where r0 = ⌈ log2 lnK+\n2 log2(Ke(d + 1)) ⌉\nis chosen in such a way that γr(v) ≤ 1 for all r ≥ r0. Let Tr be the random set of consecutive time steps where the same γr(v) was used. Whenever the local algorithm at v is running with γr(v) and detects ∑ s∈Tr Qs(v) > 2\nr, then we restart this algorithm with γ(v) = γr+1(v).\nWe have the following result.\nTheorem 4 The regret of EXP3-COOP run over a networkG = (V,E) ofN agents, each using delay d, and an individual learning rate η(v) = γ(v)/ ( Ke(d+ 1) ) , where γ(v) ∈ (0, 1] is adaptively selected by each agent through the above doubling trick, satisfies, when T grows large,4\nRcoopT = O\n(√ (lnK) ( d+ 1 + K\nN α(G≤d)\n) T + d log T ) .\nRemark 5 Theorem 4 shows a natural trade-off between delay and information. To make it clear, suppose N ≈ K. In this case, the regret bound becomes of order √( d+ α(G≤d) ) T lnK + d lnT .\nNow, if d is as big as the diameter dG of G, then α(G≤d) = 1. This means that at every time step all N ≈ K agents observe (with some delay) the losses of each other’s actions. This is very much reminiscent of a full information scenario, and in fact our bound becomes of order√\n(dG + 1)T lnK + dG lnT , which is close to the full information minimax rate √\n(d+ 1)T lnK when feedback has a constant delay d (Weinberger and Ordentlich, 2002). When G is sparse (i.e., dG is likely to be large, say dG ≈ N ), then agents have no advantage in taking d = dG since dG ≈ N ≈ K. In this case, agents may even give up cooperation (choosing d = 0 in Figure 3), and fall back on the standard bandit bound √ TK lnK, which corresponds to running EXP3-COOP on the edgeless graph G0. (No doubling trick is needed in this case, hence no extra log T term appears.)\nRemark 6 When d = dG, each neighborhood N≤d(v) used in the loss estimate (3) is equal to V , hence all agents receive the same feedback. Because they all start off from the same initial weights, the agents end up computing the same updates. This in turn implies that: (1) the individual regret incurred by each agent is the same as the average welfare regretRcoopT ; (2) the messages exchanged by the agents (see Figure 1) may be shortened by dropping the distribution part pt−s(v ′).\nRemark 7 An interesting question is whether the agents can come up with a reasonable choice for the value of d even when they lack any information whatsoever about the global structure of G. A partial answer to this question follows. It is easy to show that the choice d = √ K in Theorem 4 yields a bound on the average welfare regret of the form K1/4 √ T lnK + √ K(lnT ) for all G (and irrespective to the value of N = |V |), provided G is connected. This holds because, for any\n3. There has been some recent work on adaptive learning rate tuning applied to nonstochastic bandit algorithms (Kocák et al., 2014; Neu, 2015). One might wonder whether the same techniques may apply here as well. Unfortunately, the specific form of our update (1) makes this adaptation nontrivial, and this is why we resorted to a more traditional “doubling trick”. 4. The big-oh notation here hides additive terms that are independent of T and do depend polynomially on the other parameters.\nconnected graph G, the independence number α(G≤d) is always bounded by5 ⌈ 2N / (d + 2) ⌉ . To see why this latter statement is true, observe that the neighborhood N≤d/2(v) of any node v in G≤d/2 contains at least d/2 + 1 nodes (including v), and any pair of nodes v′, v′′ ∈ N≤d/2(v) are adjacent in G≤d. Therefore, no independent set of G≤d can have size bigger than d2N / (d+ 2) ⌉ . A more detailed bound is contained, e.g., in (Firby and Haviland, 1997)."
    }, {
      "heading" : "5. Extensions: Cooperation with Individual Parameters",
      "text" : "In this section, we analyze a modification of EXP3-COOP that allows each agent v in the network to use a delay parameter d(v) different from that of the other agents. We then show how such individual delays may improve the average welfare regret of the agents. In the previous setting, where all agents use the same delay parameter d, messages have an implicit time-to-live equal to d. In this setting, however, agents may not have a detailed knowledge of the delay parameters used by the other agents. For this reason we allow an agent v to generate messages with a time-to-live ttl(v) possibly different from the delay parameter d(v). Note that the role of the two parameters d(v) and ttl(v) is inherently different. Whereas d(v) rules the extent to which v uses the messages received from the other agents, ttl(v) limits the number of times a message from v is forwarded to the other agents, thereby limiting the message complexity of the algorithm. In order to accomodate this additional parameter, we are required to modify the cooperative bandit protocol of Figure 1. As in Section 4, we have an undirected communication network G = (V,E) over the agents. However, in this new protocol the message that at the end of round t each agent v sends to his neighbors in G has the format\nmt(v) = 〈 t, v, ttl(v), It(v), `t ( It(v) ) ,pt(v) 〉 where ttl(v) is the time-to-live parameter of agent v. Each message mt−s(v′), which v receives from a neighbor, first has its time-to-leave decremented by one. If the resulting value is positive, the message is forwarded to the other neighbors, otherwise it is dropped. Moreover, v uses this message to update pt(v) only if s ≤ d(v). Hence, at time t an agent v uses the message sent at time t − s by v′ if and only if distG(v′, v) = s with s ≤ min{d(v), ttl(v′)}, where distG(v, v′) is the shortest-path distance from v′ to v in G.\nBased on the collection P = {d(v), ttl(v)}v∈V of individual parameters, we define the directed graph GP = (V,EP) as follows: arc (v′, v) ∈ EP if and only if distG(v, v′) ≤ min{d(v), ttl(v′)}. The in-neighborhood N−P (v) of v thus contains the set of all v\n′ ∈ V whose distance from v is not larger than min{d(v), ttl(v′)}. Notice that, with this definition, v ∈ N−P (v), so that (V,EP) includes all self-loops (v, v). Figure 4(a) illustrates these concepts through a simple pictorial example.\nRemark 8 It is important to remark that the communication structure encoded by P is an exogenous parameter of the regret minimization problem, and so our algorithms cannot trade it off against regret. In addition to that, the parameterization P = {d(v), ttl(v)}v∈V defines a simple and static communication graph which makes it relatively easy to express regret as a function of the amount of available communication. This would not be possible if we had each individual node\n5. Because it holds for a worst-case (connected) G, this upper bound on α(G≤d) can be made tighter when specific graph topologies are considered.\nv decide whether to forward a message based, say, on its own local delay parameter d(v). To see why, consider the situation where nodes v and v′ are along the route of a message that is reaching v before v′. The decision of v to drop the message may clash with the willingness of v′ to receive it, and this may clearly happen when d(v) < d(v′). The structure of the communication graph resulting from this individual behavior of the nodes would be rather complicated. On the contrary, the time-to-live-based parametrization, which is commonly used in communication networks to control communication complexity, does not have this issue.\nFigure 5 contains our algorithm (called EXP3-COOP2) for this setting. EXP3-COOP2 is a strict generalization of EXP3-COOP, and so is its analysis. The main difference between the two algorithms is that EXP3-COOP2 deals with directed graphs. This fact prevents us from using the same techniques of Section 4.1 in order to control the regret. Intuitively, adding orientations to the edges reduces the information available to the agents and thus increases the variance of their loss estimates. Thus, in order to control this variance, we need a lower bound6 on the probabilities pt(i, v). From Figure 5, one can easily see that\n1 = ∑ i∈A wt(i, v) Wt(v) ≤ P̃t(v) ≤ ∑ i∈A ( wt(i, v) Wt(v) + δ K ) = 1 + δ (6)\nimplying the lower bound pt(i, v) ≥ δK(1+δ) , holding for all i, t, and v.\nThe following theorem is the main result of this section.\n6. We find it convenient to derive this lower bound without mixing with the uniform distribution over A —see, e.g., (Auer et al., 2002)— but in a slightly different manner. This facilitates our delayed feedback analysis.\nTheorem 9 The regret of EXP3-COOP2 run over a network G = (V,E) of N agents, each agent v using individual delay d(v), individual time-to-leave ttl(v), exploration parameter δ = 1/T , and learning rate η such that η → 0 as T →∞ satisfies, when T grows large,\nRcoopT = O ( lnK η + η ( d̄V + K N α (GP) ln(TNK) ) T ) , where d̄V = 1 N ∑ v∈V d(v) .\nUsing a doubling trick in much the same way we used it to prove Theorem 4, we can prove the following result.\nCorollary 10 The regret of EXP3-COOP2 run over a networkG = (V,E) ofN agents, each agent v using individual delay d(v), individual time-to-leave ttl(v), exploration parameter δ = 1/T , and individual learning rate η(v) adaptively selected by each agent through a doubling trick, satisfies,\nwhen T grows large\nRcoopT = O\n(√ (lnK) ( d̄V + 1 + K\nN α(GP) ln(TNK)\n) T + d̄V ( lnT + ln ln(TNK) )) .\nTo illustrate the advantage of having individual delays as opposed to sharing the same delay value, it suffices to consider a communication network including regions of different density. Concretely, consider the graph in Figure 4(b) with a large densely connected region (red agents) and a small sparsely connected region black agents). In this example, the black agents prefer a large value of their individual delay so as to receive more information from nearby agents, but this comes at the price of a larger bias for their estimators ̂̀t(i, v). On the contrary, information from nearby agents is readily available to the red agents, so that they do not gain any regret improvement from a large delay parameter. A similar argument applies here to the individual time-to-live values: red agents v will set a small ttl(v) to reduce communication. Black agents v′ may decide to set ttl(v′) depending on their intention to reach the red nodes. But because the red agents have set a small d(v), any effort made by v′ trying to reach them would be a communication waste. Hence, it is reasonable for a black agent v′ to set a moderately large value for ttl(v′), but perhaps not so large as to reach the red agents. One can read this off the bounds in both Theorem 9 and Corollary 10, as explained next. Suppose for simplicity that K ≈ N so that, disregarding log factors, these bounds depend on parameters P only through the quantity H = d̄V + α (GP). Now, in the case of a common delay parameter d (Section 4.1), it is not hard to see that the best setting for d in order to minimize H is of the form d = N1/4, resulting in H = Θ(N1/4). On the other hand, the best setting for the individual delays is d(v) = 1 when v is red, and d(v) = √ N when v is black, resulting in H = Θ(1).\nThe time-to-live parameters ttl(v) affect the regret bound only through α (GP), but they clearly play the additional role of bounding the message complexity of the algorithm. In our example of Figure 4(b), we essentially have d(v) ≈ ttl(v) for all v. A typical scenario where agents may have d(v) 6= ttl(v) is illustrated in Figure 4(c). In this case, we have star-like graph where a central agent is connected through long rays to all others agents. The center v prefers to set a small d(v), since it has a large degree, but also a large ttl(v) in order to reach the green peripheral nodes. The green nodes v′ are reasonably doing the opposite: a large d(v′) in order to gather information from other nodes, but also a smaller time-to-live than the center, for the information transmitted by v′ is comparatively less valuable to the whole network than the one transmitted by the center.\nAgents can set their individual parameters in a topology-dependent manner using any algorithm for assessing the centrality of nodes in a distributed fashion —e.g., (Wehmuth and Ziviani, 2013), and references therein. This can be done at the beginning in a number of rounds which only depends on the network topology (but not on T ). Hence, this initial phase would affect the regret bound only by an additive constant."
    }, {
      "heading" : "6. Delayed Losses (for a Single Agent)",
      "text" : "EXP3-COOP can be specialized to the setting where a single agent is facing a bandit problem in which the loss of the chosen action is observed with a fixed delay d. In this setting, at the end of\neach round t the agent incurs loss `t(It) and observes `t−d(It−d), if t > d, and nothing otherwise. The regret is defined in the usual way,\nRT = E [ T∑ t=1 `t(It) ] − min i=1,...,K T∑ t=1 `t(i) .\nThis problem was studied by Weinberger and Ordentlich (2002) in the full information case, for which they proved that √ (d+ 1)T lnK is the optimal order for the minimax regret. The result was extended to the bandit case by Neu et al. (2010, 2014) —see also Joulani et al. (2013)— whose techniques can be used to obtain a regret bound of order √ (d+ 1)KT . Yet, no matching lower bound was available for the bandit case.\nAs a matter of fact, the upper bound √\n(d+ 1)KT for the bandit case is easily obtained: just run in parallel d + 1 instances of the minimax optimal bandit algorithm for the standard (no delay) setting, achieving RT ≤ √ KT (ignoring constant factors). At each time step t = (d + 1)r + s (for r = 0, 1, . . . and s = 0, . . . , d), use instance s + 1 for the current play. Hence, the no-delay bound applies to every instance and, assuming d + 1 divides T , we immediately obtain RT ≤∑d+1\ns=1 √ K Td+1 ≤ √ (d+ 1)KT , again, ignoring constant factors.\nNext, we show that the machinery we developed in Section 4.1 delivers an improved upper bound on the regret for the bandit problem with delayed losses, and then we complement this result by providing a lower bound matching the upper bound up to log factors, thereby characterizing (up to log factors) the minimax regret for this problem.\nCorollary 11 In the nonstochastic bandit setting with K ≥ 2 actions and delay d ≥ 0, where at the end of each round t the predictor has access to the losses `1(I1), . . . , `s(Is) ∈ [0, 1]K for s = max{1, t− d}, the minimax regret is of order √ (K + d)T , ignoring logarithmic factors."
    }, {
      "heading" : "7. Conclusions and Ongoing Research",
      "text" : "We have investigated a cooperative and nonstochastic bandit scenario where cooperation comes at the price of delayed information. We have proven average welfare regret bounds that exhibit a natural tradeoff between amount cooperation and delay, the tradeoff being ruled by the underlying communication network topology. As a by-product of our analysis, we have also provided the first characterization to date of the regret of learning with (constant) delayed feedback in an adversarial bandit setting. There are a number of possible extensions which we are currently considering:\n1. So far our analysis only delivers average welfare regret bounds. It would be interesting to show simultaneous regret bounds that hold for each agent individually. We conjecture that the individual regret bound of an agent v is of the form √ (lnK) ( d+ K|N≤d(v)| ) T , where\n|N≤d(v)| is the degree of v in G≤d (plus one). Such bound would in fact imply, e.g., the one in Theorem 4. A possible line of attack to solve this problem could be the use of graph sparsity along the lines of (Pan et al., 2015; Duchi et al., 2013; Mania et al., 2015; McMahan and Streeter, 2014).\n2. It would be nice to characherize the average welfare regret by complementing our upper bounds with suitable lower bounds: Is the upper bound of Theorem 4 optimal in the communication model considered here?\n3. The two algorithms we designed do not use the loss information in the most effective way, for they both postpone the update step by d (Figure 3) or d(v) ((Figure 5) time steps. In fact, we do have generalized versions of both algorithms where all losses `t−s(i) coming from agents at distance s from any given agent v are indeed used at time t by agent v i.e., as soon as these losses become available to v. The resulting regret bounds mix delays and independence numbers of graphs at different levels of delay. (Details will be given in the full version of this paper.) More ambitiously, it is natural to think of ways to adaptively tune our algorithms so as to automatically determine the best delay parameter d. For instance, disregarding message complexity, is there a way for each agent to adaptively tune d locally so to minimize the bound in Theorem 4?\n4. Our messages mt(v) contain both action/loss information and distribution information. Is it possible to drop the distribution information and still achieve average welfare regret bounds similar to those in Theorems 3 and 4? 5. Even for the single-agent setting, we do not know whether regret bounds of the form√ (D + T ) lnK, where D is the total delay experienced over the T rounds, could be proven\n—see (Joulani et al., 2016; Quanrud and Khashabi, 2015) for similar results in the fullinformation setting. In general, the study of learning on a communication network with time-varying delays, and its impact on the regret rates, is a topic which is certainly worth of attention."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank the anonymous reviewers for their careful reading, and for their thoughtful suggestions that greatly improved the presentation of this paper. Yishay Mansour is supported in part by the Israeli Centers of Research Excellence (I-CORE) program, (Center No. 4/11), by a grant from the Israel Science Foundation (ISF), by a grant from United States-Israel Binational Science Foundation (BSF) and by a grant from the Len Blavatnik and the Blavatnik Family Foundation."
    }, {
      "heading" : "Appendix A. Proofs from Section 3",
      "text" : "Proof of Lemma 1.\nProof Directly from the definition of the update (1), wt+1(i) ≤ pt(i) for all i ∈ A, so that Wt+1 ≤ 1, which in turn implies wt+1(i) ≤ wt+1(i)/Wt+1 = pt+1(i). Therefore\npt+1(i)− pt(i) ≥ wt+1(i)− pt(i) = pt(i) ( e−η ̂̀ t(i) − 1 ) ≥ −η pt(i)̂̀t(i) ,\nthe last inequality using 1− e−x ≤ x for x ≥ 0. Similarly,\npt+1(i)− pt(i) ≤ pt+1(i)− wt+1(i) = pt+1(i)− pt+1(i)Wt+1 = pt+1(i)\n∑ j∈A ( pt(j)− wt+1(j) ) = pt+1(i)\n∑ j∈A pt(j) ( 1− e−η ̂̀t(j)) ≤ η pt+1(i)\n∑ j∈A pt(j)̂̀t(j) concluding the proof.\nProof of Lemma 2. Proof We proceed by induction over t. For all t ≤ d, ̂̀t(·) = 0. Hence pt(·) = 1/K, and the lemma trivially holds. For t > d we can write∑\ni∈A pt(i)̂̀t(i) = ∑ i∈A pt(i) `t−d(i) qt−d(i) Bt−d(i)\n≤ ∑ i∈A pt(i) qt−d(i) (because Bt−d(i)`t−d(i) ≤ 1)\n≤ ∑ i∈A ( 1 + 1 d )d pt−d(i) qt−d(i)\n(by the inductive hypothesis)\n≤ ( 1 + 1\nd\n)d K (because qt−d(i) ≥ pt−d(i))\n≤ Ke .\nHence, using Lemma 1,\npt+1(i) ( 1− η Ke ) ≤ pt+1(i) 1− η ∑ j∈A pt(j)̂̀t(j)  ≤ pt(i)\nwhich implies pt+1(i) ≤ ( 1 + 1d ) pt(i) whenever η ≤ 1Ke(d+1) ."
    }, {
      "heading" : "Appendix B. Proofs from Section 4.1",
      "text" : "The next lemma relates the variance of the estimates (3) to the structure of the communication graph G. The lemma is stated for a generic undirected communication graph G, but our application of it actually involves graph G≤d.\nLemma 12 Let G = (V,E) be an undirected graph with independence number α(G). For each v ∈ V , let N≤1(v) be the neighborhood of node v (including v itself), and p(v) =( p(1, v), . . . , p(K, v) ) be a probability distribution over A = {1, . . . ,K}. Then, for all i ∈ A,\n∑ v∈V p(i, v) q(i, v) ≤ 1 1− e−1\n( α(G) +\n∑ v∈V p(i, v)\n) where q(i, v) = 1−\n∏ v′∈N≤1(v) ( 1− p(i, v′) ) .\nProof Fix i ∈ A and set for brevity P (i, v) = ∑\nv′∈N≤1(v) p(i, v ′). We can write\n∑ v∈V p(i, v) q(i, v) = ∑ v∈V :P (i,v)≥1 p(i, v)\nq(i, v)︸ ︷︷ ︸ (I)\n+ ∑\nv∈V :P (i,v)<1\np(i, v)\nq(i, v)︸ ︷︷ ︸ (II)\n,\nand proceed by upper bounding the two terms (I) and (II) separately. Let r(v) be the cardinality of N≤1(v). We have, for any given v ∈ V ,\nmin q(i, v) : ∑ v′∈N≤1(v) p(i, v′) ≥ 1  = 1− ( 1− 1 r(v) )r(v) ≥ 1− e−1 .\nThe equality is due to the fact that the minimum is achieved when p(i, v′) = 1r(v) for all v ′ ∈ N≤1(v), and the inequality comes from r(v) ≥ 1 (for, v ∈ N≤1(v)). Hence\n(I) ≤ ∑\nv∈V :P (i,v)≥1\np(i, v) 1− e−1 ≤ ∑ v∈V p(i, v) 1− e−1 .\nAs for (II), using the inequality 1− x ≤ e−x, x ∈ [0, 1], with x = p(i, v′), we can write\nq(i, v) ≥ 1− exp − ∑ v′∈N≤1(v) p(i, v′)  = 1− exp (−P (i, v)) . In turn, because P (i, v) < 1 in terms (II), we can use the inequality 1− e−x ≥ (1− e−1)x, holding when x ∈ [0, 1], with x = P (i, v), thereby concluding that\nq(i, v) ≥ (1− e−1)P (i, v)\nThus\n(II) ≤ ∑\nv∈V :P (i,v)<1\np(i, v) (1− e−1)P (i, v) ≤ 1 1− e−1 ∑ v∈V p(i, v) P (i, v) ≤ α(G) 1− e−1 ,\nwhere in the last step we used (Alon et al., 2014, Lemma 10). Notice that despite the statement of this lemma refers to a directed graph and its maximum acyclic subgraph, in the special case of undirected graphs, the size of the maximum acyclic subgraph coincides with the independence number. Moreover, observe that p(i, 1), . . . , p(i,N) ≥ 0 need not sum to one in order for this\nlemma to hold.\nProof of Theorem 3.\nProof The standard analysis of the exponentially-weighted algorithm with importance-sampling estimates (see, e.g., the proof of (Alon et al., 2014, Lemma 1)) gives for each agent v and each action k the deterministic bound\nT∑ t=1 K∑ i=1 pt(i, v)̂̀t(i, v) ≤ T∑ t=1 ̂̀ t(k, v) + lnK η + η 2 T∑ t=1 K∑ i=1 pt(i, v)̂̀t(i, v)2 . (7) We take expectations of the three (double) sums in (7) separately. As for the first sum, notice that an iterative application of Lemma 1 gives, for t > d,\npt(i, v) ≥ pt−d(i, v)− η d∑ s=1 pt−s(i, v)̂̀t−s(i, v) , so that, setting for brevity At(i, v) = ∑d s=1 pt−s(i, v) ̂̀ t−s(i, v), we have\nT∑ t=1 K∑ i=1 pt(i, v)̂̀t(i, v) ≥ T∑ t=2d+1 K∑ i=1 pt(i, v)̂̀t(i, v) ≥\nT∑ t=2d+1 K∑ i=1 pt−d(i, v)̂̀t(i, v)− η T∑ t=2d+1 K∑ i=1 At(i, v) ̂̀t(i, v) . Hence\nE [ T∑ t=1 K∑ i=1 pt(i, v)̂̀t(i, v)] ≥ E[ T∑ t=2d+1 K∑ i=1 pt−d(i, v)̂̀t(i, v) ] − η E [ T∑ t=2d+1 K∑ i=1 At(i, v) ̂̀t(i, v)]\n= E\n[ T∑\nt=2d+1 K∑ i=1 pt−d(i, v)Et−d [̂̀ t(i, v)\n]]\n− η E\n[ T∑\nt=2d+1 K∑ i=1 At(i, v)Et−d [̂̀ t(i, v) ]] (since pt(i, v) is determined by I1(·), . . . , It−d−1(·))\n= E\n[ T∑\nt=2d+1 K∑ i=1 pt−d(i, v) `t−d(i)\n] − η E [ T∑\nt=2d+1 K∑ i=1 At(i, v) `t−d(i) ] (using (4))\n≥ E [ T∑ t=1 K∑ i=1 pt(i, v) `t(i) ] − 2d− η T d .\nThe last step uses\nE [ K∑ i=1 At(i, v) `t−d(i) ] ≤ E [ K∑ i=1 At(i, v) ]\n= E [ K∑ i=1 d∑ s=1 pt−s(i, v)̂̀t−s(i, v)]\n= E [ K∑ i=1 d∑ s=1 pt−s(i, v)`t−s−d(i) ]\n≤ E [ K∑ i=1 d∑ s=1 pt−s(i, v) ] = d\nholding for t ≥ 2d+ 1. Similarly, for the second sum in (7), we have\nE [ T∑ t=1 ̂̀ t(k, v) ] = T∑ t=d+1 `t−d(k) ≤ T∑ t=1 `t(k) .\nFinally, for the third sum in (7), an iterative application of Lemma 2 yields, for t > d, pt(i, v) ≤ ( 1 + 1\nd\n)d pt−d(i, v) ≤ e pt−d(i, v) ,\nso that we can write\nE [ T∑ t=1 K∑ i=1 pt(i, v)̂̀t(i, v)2] = E[ T∑ t=d+1 K∑ i=1 Et−d [ pt(i, v)̂̀t(i, v)2]]\n≤ E\n[ T∑\nt=d+1 K∑ i=1 pt(i, v) qd,t−d(i, v)\n] (using (5) and `t(·) ≤ 1)\n≤ eE\n[ T∑\nt=d+1 K∑ i=1 pt−d(i, v) qd,t−d(i, v)\n] ,\nthe last inequality being due to an iterative application of Lemma 2, and the observation that( 1 + 1d )d ≤ e. Hence, summing over all agents v, dividing by N , and using Lemma 12 on G≤d gives\n1\nN E [ T∑ t=1 K∑ i=1 ∑ v∈V pt(i, v)̂̀t(i, v)2] ≤ e N E [ T∑ t=d+1 K∑ i=1 ∑ v∈V pt−d(i, v) qd,t−d(i, v) ]\n≤ e (1− e−1)N E\n[ T∑\nt=d+1 K∑ i=1\n( α(G≤d) +\n∑ v∈V pt−d(i, v)\n)]\n≤ e 1− e−1 T\n( K\nN α(G≤d) + 1\n) .\nFinally, putting together as in (7), setting η = γ /( Ke(d + 1) ) , and overapproximating, we obtain the desired bound.\nProof of Theorem 4.\nProof We start off from first part of the proof of Theorem 3 which, after rearranging terms, gives the following bound for each agent v:\nE [ T∑ t=1 K∑ i=1 pt(i, v)`t(i) ] − T∑ t=1 `t(k)\n≤ 2d+ E\n[ lnK\nη(v) + η(v) d2 + η(v) T∑ t=d+1\n( d+ e\n2 K∑ i=1 pt−d(i, v) qd,t−d(i, v)\n)]\n≤ 3d+ E Ke(d+ 1) lnKγ(v) + γ(v)Ke(d+ 1) T∑ t=1 ( I{t > d} d+ e 2 ( K∑ i=1 pt−d(i, v) qd,t−d(i, v) ) I{t > d} ) ︸ ︷︷ ︸\nQt(v)\n . (8)\nNote that the optimal tuning of γ(v) depends on the random quantity\nQT (v) = T∑ t=1 Qt(v) .\nWe now apply the doubling trick to each instance of EXP3-COOP. Recall that, for each v ∈ V , we let γr(v) = Ke(d + 1) √ (lnK)/2r for each r = r0, r0 + 1, . . . , where r0 = ⌈ log2 lnK +\n2 log2(Ke(d + 1)) ⌉\nis chosen in a way that γr(v) ≤ 1 for all r ≥ r0. Let Tr be the random set of consecutive time steps where the same γr(v) was used. Whenever the algorithm is running with γr(v) and detects ∑ s∈Tr Qs(v) > 2\nr, then we restart the algorithm with γ(v) = γr+1(v). The largest r = r(v) we need is ⌈ log2QT (v) ⌉ and⌈\nlog2QT (v) ⌉∑\nr=r0\n2r/2 < 5 √ QT (v) .\nBecause of (8), the regret agent v suffers when using γr(v) within Tr is at most 3d+ 2 √\n(lnK)2r. Now, since we pay at most regret d at each restart, we have\nE [ T∑ t=1 ∑ i pt(i, v)`t(i) ] − T∑ t=1 `t(k) ≤ 3d+ 4Ke(d+ 1) lnK\n+ E [ 10 √ (lnK)QT (v) + 3d ⌈ log2QT (v) ⌉] .\nThe term 3d + 4Ke(d + 1) lnK bounds the regret when the algorithm is never restarted implying that only γr0(v) is used.\nTaking averages with respect to v, using Jensen’s inequality multiple times, and applying the (deterministic) bound\n1\nN ∑ v∈V QT (v) ≤ ( d+\ne 2(1− e−1) K (α(G≤d) + 1) N\n) T\nderived with the aid of Lemma 12 at the end of the proof of Theorem 3, gives\nRcoopT ≤ 3d+ 4Ke(d+ 1) lnK\n+ 10 √√√√(lnK)E[ 1 N ∑ v∈V QT (v) ] + 3d log2 ( E [ 1 N ∑ v∈V QT (v) ])\n≤ 10 √ (lnK) ( d+\ne 2(1− e−1) K (α(G≤d) + 1) N\n) T + 3d log2 T + C ,\nwhereC is independent of T and depends polynomially on the other parameters. Hence, as T grows large,\nRcoopT = O\n(√ (lnK) ( d+ 1 + K\nN α(G≤d)\n) T + d log T ) ,\nas claimed."
    }, {
      "heading" : "Appendix C. Proofs from Section 5",
      "text" : "We first need to adapt the preliminary Lemmas 1 and 2 to the new update rule of EXP3-COOP2 contained in Figure 5.\nLemma 13 Under the update rule contained in Figure 5, for all t ≥ 1, for all i ∈ A, and for all v ∈ V\n−pt(i, v) ( η̂̀t(i, v) + δ) ≤ pt+1(i, v)− pt(i, v)\n≤ pt+1(i, v) K∑ j=1 pt(j, v) ( 1− I{p̃t+1(i, v) > δ/K} ( 1− η ̂̀t(i, v)))\nholds deterministically with respect to the agents’ randomization.\nProof For the lower bound, we have\npt+1(i, v)− pt(i, v) = p̃t+1(i, v)\nP̃t+1(v) − pt(i, v) ≥\nwt+1(i, v)\nWt+1(v) P̃t+1(v) − pt(i, v) .\nSince Wt+1(v) = ∑ i∈A pt(i, v)e −η̂̀t(i,v) ≤ ∑i∈A pt(i, v) = 1, and P̃t+1(v) ≤ 1 + δ by (6), we can write\npt+1(i, v)− pt(i, v) ≥ wt+1(i, v)\n1 + δ − pt(i, v)\n= pt(i, v)\n( e−η ̂̀ t(i,v)\n1 + δ − 1\n)\n≥ pt(i, v)\n( 1− η̂̀t(i, v)\n1 + δ − 1\n) (using e−x ≥ 1− x)\n≥ pt(i, v) ( −δ − η̂̀t(i, v))\nas claimed. As for the upper bound, we first claim that\nwt+1(i, v)\nWt+1(v) ≥ pt+1(i, v)I{p̃t+1(i, v) > δ/K} . (9)\nTo prove (9), we recall that p̃t+1(i, v) = max { wt+1(i,v) Wt+1(v) , δK } . Then we distinguish two cases:\n1. If wt+1(i,v)Wt+1(v) ≤ δ K , then p̃t+1(i, v) = δ/K, and wt+1(i, v)/Wt+1(v) > 0 by definition, hence\n(9) holds;\n2. If wt+1(i,v)Wt+1(v) > δ K then p̃t+1(i, v) = wt+1(i,v) Wt+1(v) , so that pt+1(i, v) ≤ pt+1(i, v) P̃t+1(v) = p̃t+1(i, v) and (9) again holds.\nThen, setting for brevity C = I{p̃t+1(i, v) > δ/K}, we can write\npt+1(i, v)− pt(i, v) ≤ pt+1(i, v)− wt+1(i, v) (from the update (1)) ≤ pt+1(i, v)−Wt+1(v)pt+1(i, v)C (using (9)) = pt+1(i, v) ( 1−Wt+1(v)C\n) = pt+1(i, v)\n∑ j∈A ( pt(j, v)− C wt+1(j, v) ) = pt+1(i, v)\n∑ j∈A pt(j, v) ( 1− C e−η ̂̀t(j,v)) ≤ pt+1(i, v)\n∑ j∈A pt(j, v) ( 1− C(1− η ̂̀t(j, v))) where in the last step we again used e−x ≥ 1− x. This concludes the proof.\nLemma 14 Under the update rule contained in Figure 5, if δ ≤ 1/d(v) and η ≤ 1Ke(d(v)+1) , then\npt+1(i, v) ≤ ( 1 + 1\nd(v)\n) pt(i, v) (10)\nholds for all t ≥ 1 and i ∈ A, deterministically with respect to the agents’ randomization.\nProof If p̃t+1(i, v) = δ/K then, from (6), we have δ/K = pt+1(i, v)P̃t+1(v) ≥ pt+1(i, v), and pt(i, v) ≥ δK(1+δ) . Hence, pt+1(i,v) pt(i,v) ≤ δ/Kδ/(K(1+δ)) = 1 + δ, so the claim follows from δ ≤ 1\nd(v) . On the other hand, if p̃t+1(i, v) > δ/K, then the proof is exactly the same as the proof of Lemma 2, for the second inequality in the statement of Lemma 13 turns out to be exactly the same as the corresponding inequality in the statement in Lemma 1.\nNext, we generalize Lemma 12 to the case of directed graphs. This is where we need a lower bound on the probabilities pt(i, v). If G = (V,E) is a directed graph, then for each v ∈ V let N−≤1(v) be the in-neighborhood of node v (i.e., the set of v′ ∈ V such that arc (v′, v) ∈ E), including v itself.\nLemma 15 Let G = (V,E) be a directed graph with independence number α(G). Let p(v) = ( p(1, v), . . . , p(K, v) ) be a probability distribution over A = {1, . . . ,K} such that p(i, v) ≥ δK(1+δ) . Then, for all i ∈ A,\n∑ v∈V p(i, v) q(i, v) ≤ 1 1− e−1\n( 6α(G) ln ( 1 + N2K(1 + δ)\nδ\n) + ∑ v∈V p(i, v) ) ,\nwhere q(i, v) = 1− ∏ v′∈N−≤1(v) ( 1− p(i, v′) ) .\nProof We follow the notation and the proof of Lemma 12, where it is shown that∑ v∈V p(i, v) q(i, v) ≤ 1 1− e−1 ∑ v∈V ( p(i, v) P (i, v) + p(i, v) ) . In order to bound from above the sum ∑\nv∈V p(i,v) P (i,v) , we combine (Alon et al., 2014, Lemma 14\nand 16) and derive the upper bound\n∑ v∈V p(i, v) P (i, v) ≤ 6α(G) ln ( 1 + N2K(1 + δ) δ )\nholding when p(i, v) ≥ δK(1+δ) . Again, the probabilities p(i, 1), . . . , p(i,N) ≥ 0 need not sum to one in order for this lemma to apply.\nWith the above three lemmas handy, we are ready to prove Theorem 9.\nProof [Theorem 9] This proof is similar to the proof of Theorem 3, hence we only emphasize the differences between the two.\nFrom the update rule in Figure 5, we have, for each v ∈ V ,\nWT+1(v) = K∑ i=1 p̃T (i) P̃T (v) e−η ̂̀ T (i,v)\n≥ K∑ i=1 wT (i, v) WT (v)P̃T (v) e−η ̂̀ T (i,v) (since p̃T (i) ≥ wT (i, v)/WT (v))\n= K∑ i=1 p̃T−1(i, v)e −η̂̀T−1(i,v)e−η̂̀T (i,v) WT (v)P̃T−1(v)P̃T (v)\n...\n≥ K∑ i=1\nw1(i, v) e −η\n∑T t=1 ̂̀ t(i,v)\nW1(v) · · ·WT (v)P̃1(v) · · · P̃T (v) .\nNow, because w1(i, v) = 1, W1(v) = K, and P̃t(v) ≤ 1 + δ for all t, see (6), the above chain of inequalities implies that, for any fixed action k ∈ A,\n(1 + δ)T K ( T∏ t=1 Wt+1(v) ) ≥ e−η ∑T t=1 ̂̀ t(k,v) . (11)\nAs usual, the quantity Wt+1(v) can be upper bounded as\nWt+1(v) = K∑ i=1 pt(i, v)e −η̂̀t(i,v)\n≤ K∑ i=1 pt(i, v) ( 1− η̂̀t(i, v) + η2 2 ̂̀ t(i, v) 2 ) (from e−x ≤ 1− x+ x2/2 for all x ≥ 0) = 1− η K∑ i=1 pt(i, v)̂̀t(i, v) + η2 2 K∑ i=1\npt(i, v)̂̀t(i, v)2 . Plugging back into (11) and taking logs of both sides gives\nT ln(1+δ)+lnK+ T∑ t=1 ln\n( 1− η\nK∑ i=1 pt(i, v)̂̀t(i, v) + η2 2 K∑ i=1 pt(i, v)̂̀t(i, v)2) ≥ −η K∑ i=1 ̂̀ t(k, v) .\nFinally, using ln(1 + x) ≤ x, dividing by η, using δ = 1/T , and rearranging yields\nT∑ t=1 K∑ i=1 pt(i, v)̂̀t(i, v) ≤ 1 + lnK η + T∑ t=1 ̂̀ t(k, v) + η 2 T∑ t=1 K∑ i=1 pt(i, v)̂̀t(i, v)2 (12) hence arriving at the counterpart to (7).\nFrom this point on, we proceed as in the proof of Theorem 3 by taking expectation on the three sums in (12). Notice that we do still have, for all v ∈ V , t > d(v), and i ∈ A,\nEt−d(v) [̂̀ t(i, v) ] = `t−d(v)(i)\nEt−d(v) [ pt(i, v)̂̀t(i, v)] = pt(i, v)`t−d(v)(i)\nEt−d(v) [ pt(i, v)̂̀t(i, v)2] = pt(i, v) `t−d(v)(i)2\nqP,t−d(v)(i, v) .\nWe can write\nE [ T∑ t=1 K∑ i=1 pt(i, v)̂̀t(i, v)] ≥ E[ T∑ t=1 K∑ i=1 pt(i, v) `t(i) ] − 2d(v)− (η + δ)T d(v)\nE [ T∑ t=1 ̂̀ t(k, v) ] ≤ T∑ t=1 `t(k)\nand, as in the proof of Theorem 3,\nE [ T∑ t=1 K∑ i=1 pt(i, v)̂̀t(i, v)2] ≤ eE  T∑ t=d(v)+1 K∑ i=1 pt−d(v)(i, v) qP,t−d(v)(i, v)  . Summing over all agents v, dividing by N , and applying Lemma 15 to the directed graph GP , the latter inequality gives\n1\nN E [ T∑ t=1 K∑ i=1 ∑ v∈V pt(i, v)̂̀t(i, v)2] ≤ e 1− e−1 T ( 6K N α (GP) ln ( 1 + 2TN2K ) + 1 ) .\nCombining as in (12), recalling that δ = 1/T , and setting for brevity d̄V = 1N ∑\nv∈V d(v), we have thus obtained that the average welfare regret of EXP3-COOP2 satisfies\nRcoopT ≤ 3d̄V + η T d̄V + 1 + lnK\nη +\neη\n2(1− e−1) T\n( 6K\nN α (GP) ln\n( 1 + 2TN2K ) + 1 ) = O ( η T d̄V + lnK\nη + η TK N α (GP) ln (TNK) ) as T grows large. This concludes the proof."
    }, {
      "heading" : "Appendix D. Proofs regarding Section 6",
      "text" : "Proof of Corollary 11.\nProof In order to prove the upper bound, we use the exponentially-weighted algorithm with Estimate (3) specialized to the case of one agent only, namely Bt−d(i) = I{It−d = i} and\nqd,t−d(i) = pt−d(i). Notice that this amounts to running the standard Exp3 algorithm performing an update as soon a new loss becomes available. In this case, because N = α(G≤d) = 1, the bound of Theorem 3, with a suitable choice of γ (which depends on T , K, and d) reduces to\nRT = O ( d+ √ (K + d)T lnK ) .\nWe now prove a lower bound matching our upper bound up to logarithmic factors. The proof hinges on combining the known lower bound Ω (√ KT ) for bandits without delay of Auer et al. (2002) with the following argument by Weinberger and Ordentlich (2002) that provides a lower bound for the full information case with delay. The proof of the latter bound is by contradiction: we show that a low-regret full information algorithm for delay d > 0 can be used to design a low-regret full information algorithm for the d = 0 (no delay) setting. We then apply the known lower bound for the minimax regret in the no-delay setting to derive a lower bound for the setting with delay.\nFix d > 0 and let A be a predictor for the full-information online prediction problem with delay d. Let pt be the probability distribution used by A at time t. We now apply algorithm A to design a new algorithm A′ for a full information online prediction problem with arbitrary loss vectors `′1, . . . , ` ′ B ∈ [0, 1]K and no delay. More specifically, we create a sequence `1, . . . , `T ∈ [0, 1]K of\nloss vectors such that T = (d+1)B and `t = `′b where b = ⌈ t/(d+1) ⌉ . At each time b = 1, . . . , B algorithm A′ uses the distribution\np′b = 1\nd+ 1 d+1∑ s=1 p(d+1)(b−1)+s\nwhere pt = ( 1 K , . . . , 1 K ) for all t ≤ 1. Note that p′b is defined using p(d+1)(b−1)+1, . . . ,p(d+1)b. These are in turn defined using the same loss vectors `′1, . . . , ` ′ b−1 since, by definition, each pt+1\nuses `1, . . . , `t−d, and ⌈ (t− d)/(d+ 1) ⌉ = b− 1 for all t = (d+ 1)(b− 1), . . . , (d+ 1)b− 1. So A′ is a legitimate full-information online algorithm for the problem `′1, . . . , `′B with no delay. As a consequence,\nT∑ t=1 K∑ i=1 `t(i)pt(i) = B∑ b=1 d+1∑ s=1 K∑ i=1 `′b(i)p(d+1)(b−1)+s(i)\n= (d+ 1) B∑ b=1 K∑ i=1 1 d+ 1 d+1∑ s=1 `′b(i)p(d+1)(b−1)+s(i)\n= (d+ 1) B∑ b=1 K∑ i=1 `′b(i)p ′ b(i) .\nMoreover,\nmin k∈A T∑ t=1 `t(k) = (d+ 1) min k∈A B∑ b=1 `′b(k) .\nSince we know that for any predictor A′ there exists a loss sequence `′1, `′2, . . . such that the regret of A′ is at least ( 1 − o(1) )√ (T/2) lnK, where o(1) → 0 for K,B → ∞, we have that the regret of A is at least\n(d+ 1)RT/(d+1)(A′) = ( 1− o(1) ) (d+ 1)\n√ T\n2(d+ 1) lnK =\n( 1− o(1) )√ (d+ 1) T\n2 lnK ,\nwhere RT/(d+1)(A′) is the regret of A′ over T/(d + 1) time steps. The proof is completed by observing that that the regret of any predictor in the bandit setting with delay d cannot be smaller than the regret of the predictor in the bandit setting with no delay or smaller than the regret of the predictor in the full information setting with delay d. Hence, the minimax regret in the bandit setting with delay d must be at least of order\nmax {√ KT, √ (d+ 1)T lnK } = Ω (√ (K + d)T ) ."
    } ],
    "references" : [ {
      "title" : "Distributed delayed stochastic optimization",
      "author" : [ "Alekh Agarwal", "John C Duchi" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Agarwal and Duchi.,? \\Q2011\\E",
      "shortCiteRegEx" : "Agarwal and Duchi.",
      "year" : 2011
    }, {
      "title" : "Nonstochastic multi-armed bandits with graph-structured feedback",
      "author" : [ "Noga Alon", "Nicolò Cesa-Bianchi", "Claudio Gentile", "Shie Mannor", "Yishay Mansour", "Ohad Shamir" ],
      "venue" : "arXiv preprint arXiv:1409.8428,",
      "citeRegEx" : "Alon et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Alon et al\\.",
      "year" : 2014
    }, {
      "title" : "The nonstochastic multiarmed bandit problem",
      "author" : [ "Peter Auer", "Nicolò Cesa-Bianchi", "Yoav Freund", "Robert E Schapire" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "Auer et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "Competitive collaborative learning",
      "author" : [ "Baruch Awerbuch", "Robert Kleinberg" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Awerbuch and Kleinberg.,? \\Q2008\\E",
      "shortCiteRegEx" : "Awerbuch and Kleinberg.",
      "year" : 2008
    }, {
      "title" : "Ad hoc teamwork modeled with multi-armed bandits: An extension to discounted infinite rewards",
      "author" : [ "Samuel Barrett", "Peter Stone" ],
      "venue" : "In Proceedings of 2011 AAMAS Workshop on Adaptive and Learning Agents,",
      "citeRegEx" : "Barrett and Stone.,? \\Q2011\\E",
      "shortCiteRegEx" : "Barrett and Stone.",
      "year" : 2011
    }, {
      "title" : "Delay and cooperation in nonstochastic bandits",
      "author" : [ "Nicolo’ Cesa-Bianchi", "Claudio Gentile", "Yishay Mansour", "Alberto Minora" ],
      "venue" : "arXiv preprint,",
      "citeRegEx" : "Cesa.Bianchi et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Cesa.Bianchi et al\\.",
      "year" : 2016
    }, {
      "title" : "Estimation, optimization, and parallelism when data is sparse",
      "author" : [ "John Duchi", "Michael I Jordan", "Brendan McMahan" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Duchi et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2013
    }, {
      "title" : "Asynchronous stochastic convex optimization",
      "author" : [ "John C Duchi", "Sorathan Chaturapruek", "Christopher Ré" ],
      "venue" : "arXiv preprint arXiv:1508.00882,",
      "citeRegEx" : "Duchi et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2015
    }, {
      "title" : "Efficient optimal learning for contextual bandits",
      "author" : [ "Miroslav Dudı́k", "Daniel J. Hsu", "Satyen Kale", "Nikos Karampatziakis", "John Langford", "Lev Reyzin", "Tong Zhang" ],
      "venue" : "UAI",
      "citeRegEx" : "Dudı́k et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Dudı́k et al\\.",
      "year" : 2011
    }, {
      "title" : "Independence and average distance in graphs",
      "author" : [ "P. Firby", "J. Haviland" ],
      "venue" : "Discrete Applied Mathematics,",
      "citeRegEx" : "Firby and Haviland.,? \\Q1997\\E",
      "shortCiteRegEx" : "Firby and Haviland.",
      "year" : 1997
    }, {
      "title" : "Online learning under delayed feedback",
      "author" : [ "Pooria Joulani", "András György", "Csaba Szepesvári" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning",
      "citeRegEx" : "Joulani et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Joulani et al\\.",
      "year" : 2013
    }, {
      "title" : "Delay-tolerant online convex optimization: Unified analysis and adaptive-gradient algorithms",
      "author" : [ "Pooria Joulani", "András György", "Csaba Szepesvári" ],
      "venue" : "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, February 12-17,",
      "citeRegEx" : "Joulani et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Joulani et al\\.",
      "year" : 2016
    }, {
      "title" : "Bandit problems in networks: Asymptotically efficient distributed allocation rules",
      "author" : [ "Soummya Kar", "H Vincent Poor", "Shuguang Cui" ],
      "venue" : "In 50th IEEE Conference on Decision and Control and European Control Conference (CDC-ECC),",
      "citeRegEx" : "Kar et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Kar et al\\.",
      "year" : 2011
    }, {
      "title" : "Multiplicative updates outperform generic no-regret learning in congestion games",
      "author" : [ "Robert Kleinberg", "Georgios Piliouras", "Éva Tardos" ],
      "venue" : "In Proceedings of the forty-first annual ACM symposium on Theory of computing,",
      "citeRegEx" : "Kleinberg et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Kleinberg et al\\.",
      "year" : 2009
    }, {
      "title" : "Efficient learning by implicit exploration in bandit problems with side observations",
      "author" : [ "Tomáš Kocák", "Gergely Neu", "Michal Valko", "Remi Munos" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Kocák et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kocák et al\\.",
      "year" : 2014
    }, {
      "title" : "On distributed cooperative decision-making in multiarmed bandits",
      "author" : [ "Peter Landgren", "Vaibhav Srivastava", "Naomi Ehrich Leonard" ],
      "venue" : "arXiv preprint arXiv:1512.06888,",
      "citeRegEx" : "Landgren et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Landgren et al\\.",
      "year" : 2015
    }, {
      "title" : "Distributed delayed proximal gradient methods",
      "author" : [ "Mu Li", "David G. Andersen", "Alexander Smola" ],
      "venue" : "In NIPS Workshop on Optimization for Machine Learning,",
      "citeRegEx" : "Li et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2013
    }, {
      "title" : "Locality in distributed graph algorithms",
      "author" : [ "Nathan Linial" ],
      "venue" : "SIAM J. Comput.,",
      "citeRegEx" : "Linial.,? \\Q1992\\E",
      "shortCiteRegEx" : "Linial.",
      "year" : 1992
    }, {
      "title" : "An asynchronous parallel stochastic coordinate descent algorithm",
      "author" : [ "Ji Liu", "Stephen J Wright", "Christopher Ré", "Victor Bittorf", "Srikrishna Sridhar" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Liu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2015
    }, {
      "title" : "Perturbed iterate analysis for asynchronous stochastic optimization",
      "author" : [ "Horia Mania", "Xinghao Pan", "Dimitris Papailiopoulos", "Benjamin Recht", "Kannan Ramchandran", "Michael I Jordan" ],
      "venue" : "arXiv preprint arXiv:1507.06970,",
      "citeRegEx" : "Mania et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mania et al\\.",
      "year" : 2015
    }, {
      "title" : "Delay-tolerant algorithms for asynchronous distributed online learning",
      "author" : [ "Brendan McMahan", "Matthew Streeter" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "McMahan and Streeter.,? \\Q2014\\E",
      "shortCiteRegEx" : "McMahan and Streeter.",
      "year" : 2014
    }, {
      "title" : "On-line learning with delayed label feedback. In Algorithmic Learning Theory, pages 399–413",
      "author" : [ "Chris Mesterharm" ],
      "venue" : null,
      "citeRegEx" : "Mesterharm.,? \\Q2005\\E",
      "shortCiteRegEx" : "Mesterharm.",
      "year" : 2005
    }, {
      "title" : "Improving Online Learning",
      "author" : [ "Chris Mesterharm" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "Mesterharm.,? \\Q2007\\E",
      "shortCiteRegEx" : "Mesterharm.",
      "year" : 2007
    }, {
      "title" : "Explore no more: Improved high-probability regret bounds for non-stochastic bandits",
      "author" : [ "Gergely Neu" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Neu.,? \\Q2015\\E",
      "shortCiteRegEx" : "Neu.",
      "year" : 2015
    }, {
      "title" : "Online Markov decision processes under bandit feedback",
      "author" : [ "Gergely Neu", "Andras Antos", "András György", "Csaba Szepesvári" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Neu et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Neu et al\\.",
      "year" : 2010
    }, {
      "title" : "Online markov decision processes under bandit feedback",
      "author" : [ "Gergely Neu", "Andras Gyorgy", "Csaba Szepesvari", "Andras Antos" ],
      "venue" : "Automatic Control, IEEE Transactions on,",
      "citeRegEx" : "Neu et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Neu et al\\.",
      "year" : 2014
    }, {
      "title" : "Parallel correlation clustering on big graphs",
      "author" : [ "Xinghao Pan", "Dimitris Papailiopoulos", "Samet Oymak", "Benjamin Recht", "Kannan Ramchandran", "Michael I Jordan" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Pan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Pan et al\\.",
      "year" : 2015
    }, {
      "title" : "Online learning with adversarial delays",
      "author" : [ "Kent Quanrud", "Daniel Khashabi" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Quanrud and Khashabi.,? \\Q2015\\E",
      "shortCiteRegEx" : "Quanrud and Khashabi.",
      "year" : 2015
    }, {
      "title" : "Szlak. Multi-player bandits – a musical chairs approach",
      "author" : [ "Jonathan Rosenski", "Ohad Shamir", "Liran" ],
      "venue" : "CoRR, abs/1512.02866,",
      "citeRegEx" : "Rosenski et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Rosenski et al\\.",
      "year" : 2015
    }, {
      "title" : "Prediction with limited advice and multiarmed bandits with paid observations",
      "author" : [ "Yevgeny Seldin", "Peter Bartlett", "Koby Crammer", "Yasin Abbasi-Yadkori" ],
      "venue" : "In Proceedings of The 31st International Conference on Machine Learning,",
      "citeRegEx" : "Seldin et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Seldin et al\\.",
      "year" : 2014
    }, {
      "title" : "Dcops and bandits: Exploration and exploitation in decentralised coordination",
      "author" : [ "Ruben Stranders", "Long Tran-Thanh", "Francesco M Delle Fave", "Alex Rogers", "Nicholas R Jennings" ],
      "venue" : "In Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems-Volume",
      "citeRegEx" : "Stranders et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Stranders et al\\.",
      "year" : 2012
    }, {
      "title" : "Survey of local algorithms",
      "author" : [ "Jukka Suomela" ],
      "venue" : "ACM Computing Surveys,",
      "citeRegEx" : "Suomela.,? \\Q2013\\E",
      "shortCiteRegEx" : "Suomela.",
      "year" : 2013
    }, {
      "title" : "Gossip-based distributed stochastic bandit algorithms",
      "author" : [ "Balazs Szorenyi", "Róbert Busa-Fekete", "István Hegedüs", "Róbert Ormándi", "Márk Jelasity", "Balázs Kégl" ],
      "venue" : "In 30th International Conference on Machine Learning (ICML 2013),",
      "citeRegEx" : "Szorenyi et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Szorenyi et al\\.",
      "year" : 2013
    }, {
      "title" : "Distributed online learning via cooperative contextual bandits",
      "author" : [ "Cem Tekin", "Mihaela van der Schaar" ],
      "venue" : "IEEE Transactions on Signal Processing,",
      "citeRegEx" : "Tekin and Schaar.,? \\Q2015\\E",
      "shortCiteRegEx" : "Tekin and Schaar.",
      "year" : 2015
    }, {
      "title" : "Distributed online learning in social recommender systems",
      "author" : [ "Cem Tekin", "Simpson Z. Zhang", "Mihaela van der Schaar" ],
      "venue" : "J. Sel. Topics Signal Processing,",
      "citeRegEx" : "Tekin et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Tekin et al\\.",
      "year" : 2014
    }, {
      "title" : "Daccer: Distributed assessment of the closeness centrality ranking in complex networks",
      "author" : [ "Klaus Wehmuth", "Artur Ziviani" ],
      "venue" : "Computer Networks,",
      "citeRegEx" : "Wehmuth and Ziviani.,? \\Q2013\\E",
      "shortCiteRegEx" : "Wehmuth and Ziviani.",
      "year" : 2013
    }, {
      "title" : "On delayed prediction of individual sequences",
      "author" : [ "Marcelo J Weinberger", "Erik Ordentlich" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Weinberger and Ordentlich.,? \\Q2002\\E",
      "shortCiteRegEx" : "Weinberger and Ordentlich.",
      "year" : 2002
    }, {
      "title" : "Slow learners are fast",
      "author" : [ "Martin Zinkevich", "John Langford", "Alex J. Smola" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Zinkevich et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Zinkevich et al\\.",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "We introduce the EXP3-COOP algorithm, a distributed and cooperative version of the EXP3 algorithm of Auer et al. (2002). EXP3-COOP works within a distributed and synced model where each agent runs an instance of the same bandit algorithm (EXP3).",
      "startOffset" : 101,
      "endOffset" : 120
    }, {
      "referenceID" : 2,
      "context" : "We introduce the EXP3-COOP algorithm, a distributed and cooperative version of the EXP3 algorithm of Auer et al. (2002). EXP3-COOP works within a distributed and synced model where each agent runs an instance of the same bandit algorithm (EXP3). All bandit instances are initialized in the same way irrespective to the agent’s location in the network (that is, agents have no preliminary knowledge of the network), and we assume the information about an agent’s actions is propagated through the network with a unit delay for each crossed edge. In each round t, each agent selects an action and incurs the corresponding loss (which is the same for all agents that pick that action in round t). Besides observing the loss of the selected action, each agent obtains the information previously broadcast by other agents with a delay equal to the shortest-path distance between the agents. Namely, at time t an agent learns what the agents at shortest-path distance s did at time t− s for each s = 1, . . . , d, where d is a delay parameter. In this scenario, we aim at controlling the growth of the regret averaged over all agents (the so-called average welfare regret). In the noncooperative case, when agents ignore the information received from other agents, the average welfare regret grows like √ KT (the minimax rate for standard bandit setting), whereK is the number of actions and T is the time horizon. We show that, using cooperation, N agents with communication graph G can achieve an average welfare regret of order √( d+ 1 + KN α≤d ) (T lnK). Here α≤d denotes the independence number of the d-th power of G (i.e., the graph G augmented with all edges between any two pair of nodes at shortest-path distance less than or equal to d). When d = √ K this bound is at mostK1/4 √ T lnK+ √ K(lnT ) for any connected graph —see Remark 7 in Section 4.1— which is asymptotically better than √ KT . Networks of nonstochastic bandits were also investigated by Awerbuch and Kleinberg (2008) in a setting where the distribution over actions is shared among the agents without delay.",
      "startOffset" : 101,
      "endOffset" : 1988
    }, {
      "referenceID" : 2,
      "context" : "We introduce the EXP3-COOP algorithm, a distributed and cooperative version of the EXP3 algorithm of Auer et al. (2002). EXP3-COOP works within a distributed and synced model where each agent runs an instance of the same bandit algorithm (EXP3). All bandit instances are initialized in the same way irrespective to the agent’s location in the network (that is, agents have no preliminary knowledge of the network), and we assume the information about an agent’s actions is propagated through the network with a unit delay for each crossed edge. In each round t, each agent selects an action and incurs the corresponding loss (which is the same for all agents that pick that action in round t). Besides observing the loss of the selected action, each agent obtains the information previously broadcast by other agents with a delay equal to the shortest-path distance between the agents. Namely, at time t an agent learns what the agents at shortest-path distance s did at time t− s for each s = 1, . . . , d, where d is a delay parameter. In this scenario, we aim at controlling the growth of the regret averaged over all agents (the so-called average welfare regret). In the noncooperative case, when agents ignore the information received from other agents, the average welfare regret grows like √ KT (the minimax rate for standard bandit setting), whereK is the number of actions and T is the time horizon. We show that, using cooperation, N agents with communication graph G can achieve an average welfare regret of order √( d+ 1 + KN α≤d ) (T lnK). Here α≤d denotes the independence number of the d-th power of G (i.e., the graph G augmented with all edges between any two pair of nodes at shortest-path distance less than or equal to d). When d = √ K this bound is at mostK1/4 √ T lnK+ √ K(lnT ) for any connected graph —see Remark 7 in Section 4.1— which is asymptotically better than √ KT . Networks of nonstochastic bandits were also investigated by Awerbuch and Kleinberg (2008) in a setting where the distribution over actions is shared among the agents without delay. Awerbuch and Kleinberg (2008) prove a bound on the average welfare regret of order √( 1 + KN ) T ignoring polylog factors.",
      "startOffset" : 101,
      "endOffset" : 2109
    }, {
      "referenceID" : 2,
      "context" : "We introduce the EXP3-COOP algorithm, a distributed and cooperative version of the EXP3 algorithm of Auer et al. (2002). EXP3-COOP works within a distributed and synced model where each agent runs an instance of the same bandit algorithm (EXP3). All bandit instances are initialized in the same way irrespective to the agent’s location in the network (that is, agents have no preliminary knowledge of the network), and we assume the information about an agent’s actions is propagated through the network with a unit delay for each crossed edge. In each round t, each agent selects an action and incurs the corresponding loss (which is the same for all agents that pick that action in round t). Besides observing the loss of the selected action, each agent obtains the information previously broadcast by other agents with a delay equal to the shortest-path distance between the agents. Namely, at time t an agent learns what the agents at shortest-path distance s did at time t− s for each s = 1, . . . , d, where d is a delay parameter. In this scenario, we aim at controlling the growth of the regret averaged over all agents (the so-called average welfare regret). In the noncooperative case, when agents ignore the information received from other agents, the average welfare regret grows like √ KT (the minimax rate for standard bandit setting), whereK is the number of actions and T is the time horizon. We show that, using cooperation, N agents with communication graph G can achieve an average welfare regret of order √( d+ 1 + KN α≤d ) (T lnK). Here α≤d denotes the independence number of the d-th power of G (i.e., the graph G augmented with all edges between any two pair of nodes at shortest-path distance less than or equal to d). When d = √ K this bound is at mostK1/4 √ T lnK+ √ K(lnT ) for any connected graph —see Remark 7 in Section 4.1— which is asymptotically better than √ KT . Networks of nonstochastic bandits were also investigated by Awerbuch and Kleinberg (2008) in a setting where the distribution over actions is shared among the agents without delay. Awerbuch and Kleinberg (2008) prove a bound on the average welfare regret of order √( 1 + KN ) T ignoring polylog factors.1 We recover the same bound as a special case of our bound when G is a clique and d = 1. In the clique case our bound is also similar to the bound √ K N (T lnK) achieved by Seldin et al. (2014) in a single-agent bandit setting where, at each time step, the agent can choose a subset ofN ≤ K actions and observe their loss.",
      "startOffset" : 101,
      "endOffset" : 2395
    }, {
      "referenceID" : 2,
      "context" : "We introduce the EXP3-COOP algorithm, a distributed and cooperative version of the EXP3 algorithm of Auer et al. (2002). EXP3-COOP works within a distributed and synced model where each agent runs an instance of the same bandit algorithm (EXP3). All bandit instances are initialized in the same way irrespective to the agent’s location in the network (that is, agents have no preliminary knowledge of the network), and we assume the information about an agent’s actions is propagated through the network with a unit delay for each crossed edge. In each round t, each agent selects an action and incurs the corresponding loss (which is the same for all agents that pick that action in round t). Besides observing the loss of the selected action, each agent obtains the information previously broadcast by other agents with a delay equal to the shortest-path distance between the agents. Namely, at time t an agent learns what the agents at shortest-path distance s did at time t− s for each s = 1, . . . , d, where d is a delay parameter. In this scenario, we aim at controlling the growth of the regret averaged over all agents (the so-called average welfare regret). In the noncooperative case, when agents ignore the information received from other agents, the average welfare regret grows like √ KT (the minimax rate for standard bandit setting), whereK is the number of actions and T is the time horizon. We show that, using cooperation, N agents with communication graph G can achieve an average welfare regret of order √( d+ 1 + KN α≤d ) (T lnK). Here α≤d denotes the independence number of the d-th power of G (i.e., the graph G augmented with all edges between any two pair of nodes at shortest-path distance less than or equal to d). When d = √ K this bound is at mostK1/4 √ T lnK+ √ K(lnT ) for any connected graph —see Remark 7 in Section 4.1— which is asymptotically better than √ KT . Networks of nonstochastic bandits were also investigated by Awerbuch and Kleinberg (2008) in a setting where the distribution over actions is shared among the agents without delay. Awerbuch and Kleinberg (2008) prove a bound on the average welfare regret of order √( 1 + KN ) T ignoring polylog factors.1 We recover the same bound as a special case of our bound when G is a clique and d = 1. In the clique case our bound is also similar to the bound √ K N (T lnK) achieved by Seldin et al. (2014) in a single-agent bandit setting where, at each time step, the agent can choose a subset ofN ≤ K actions and observe their loss. In the case whenN = 1 (single agent), our analysis can be applied to the nonstochastic bandit problem where the player observes the loss of each played action with a delay of d steps. In this case we improve on the previous result of √ (d+ 1)KT by Neu et al. (2010, 2014), and give the first characterization (up to logarithmic factors) of the minimax regret, which is of order √ (d+K)T . In principle, the problem of delays in online learning could be tackled by simple reductions. Yet, these reductions give rise to suboptimal results. In the single agent setting, where the delay is constant and equal to d, one can use the technique of Weinberger and Ordentlich (2002) and run d+1 in1.",
      "startOffset" : 101,
      "endOffset" : 3197
    }, {
      "referenceID" : 11,
      "context" : "Additional works (Joulani et al., 2016; Quanrud and Khashabi, 2015) prove regret bounds for the full-information case of the form √ (D + T ) lnK, where D is the total delay experienced over the T rounds.",
      "startOffset" : 17,
      "endOffset" : 67
    }, {
      "referenceID" : 27,
      "context" : "Additional works (Joulani et al., 2016; Quanrud and Khashabi, 2015) prove regret bounds for the full-information case of the form √ (D + T ) lnK, where D is the total delay experienced over the T rounds.",
      "startOffset" : 17,
      "endOffset" : 67
    }, {
      "referenceID" : 3,
      "context" : "To the best of our knowledge, the first paper about nonstochastic cooperative bandit networks is (Awerbuch and Kleinberg, 2008).",
      "startOffset" : 97,
      "endOffset" : 127
    }, {
      "referenceID" : 32,
      "context" : "More papers analyze the stochastic setting, and the closest one to our work is perhaps (Szorenyi et al., 2013).",
      "startOffset" : 87,
      "endOffset" : 110
    }, {
      "referenceID" : 15,
      "context" : "A more recent paper is (Landgren et al., 2015), where the communication network is a fixed graph and a cooperative version of the UCB algorithm is introduced which uses a distributed consensus algorithm to estimate the mean rewards of the arms.",
      "startOffset" : 23,
      "endOffset" : 46
    }, {
      "referenceID" : 16,
      "context" : "Additional Related Work Many important ideas in delayed online learning, including the observation that the effect of delays can be limited by controlling the amount of change in the agent strategy, were introduced by Mesterharm (2005) —see also (Mesterharm, 2007, Chapter 8).",
      "startOffset" : 218,
      "endOffset" : 236
    }, {
      "referenceID" : 8,
      "context" : "Furher progress is made by Joulani et al. (2013), who also study delays in the general partial monitoring setting.",
      "startOffset" : 27,
      "endOffset" : 49
    }, {
      "referenceID" : 7,
      "context" : "In the stochastic case, bandit learning with delayed feedback was considered by Dudı́k et al. (2011); Joulani et al.",
      "startOffset" : 80,
      "endOffset" : 101
    }, {
      "referenceID" : 7,
      "context" : "In the stochastic case, bandit learning with delayed feedback was considered by Dudı́k et al. (2011); Joulani et al. (2013). To the best of our knowledge, the first paper about nonstochastic cooperative bandit networks is (Awerbuch and Kleinberg, 2008).",
      "startOffset" : 80,
      "endOffset" : 124
    }, {
      "referenceID" : 12,
      "context" : "Another interesting paper about cooperating bandits in a stochastic setting is (Kar et al., 2011).",
      "startOffset" : 79,
      "endOffset" : 97
    }, {
      "referenceID" : 28,
      "context" : "The resulting bandit problem is one of coordination in a competitive environment, because every time two or more agents select the same action at the same time step they both get a zero reward due to the interference —see (Rosenski et al., 2015) for recent work on stochastic competitive bandits and (Kleinberg et al.",
      "startOffset" : 222,
      "endOffset" : 245
    }, {
      "referenceID" : 13,
      "context" : ", 2015) for recent work on stochastic competitive bandits and (Kleinberg et al., 2009) for a study of more general congestion games in a game-theoretic setting.",
      "startOffset" : 62,
      "endOffset" : 86
    }, {
      "referenceID" : 37,
      "context" : ", (Zinkevich et al., 2009; Agarwal and Duchi, 2011; Li et al., 2013; McMahan and Streeter, 2014; Quanrud and Khashabi, 2015; Liu et al., 2015; Duchi et al., 2015).",
      "startOffset" : 2,
      "endOffset" : 162
    }, {
      "referenceID" : 0,
      "context" : ", (Zinkevich et al., 2009; Agarwal and Duchi, 2011; Li et al., 2013; McMahan and Streeter, 2014; Quanrud and Khashabi, 2015; Liu et al., 2015; Duchi et al., 2015).",
      "startOffset" : 2,
      "endOffset" : 162
    }, {
      "referenceID" : 16,
      "context" : ", (Zinkevich et al., 2009; Agarwal and Duchi, 2011; Li et al., 2013; McMahan and Streeter, 2014; Quanrud and Khashabi, 2015; Liu et al., 2015; Duchi et al., 2015).",
      "startOffset" : 2,
      "endOffset" : 162
    }, {
      "referenceID" : 20,
      "context" : ", (Zinkevich et al., 2009; Agarwal and Duchi, 2011; Li et al., 2013; McMahan and Streeter, 2014; Quanrud and Khashabi, 2015; Liu et al., 2015; Duchi et al., 2015).",
      "startOffset" : 2,
      "endOffset" : 162
    }, {
      "referenceID" : 27,
      "context" : ", (Zinkevich et al., 2009; Agarwal and Duchi, 2011; Li et al., 2013; McMahan and Streeter, 2014; Quanrud and Khashabi, 2015; Liu et al., 2015; Duchi et al., 2015).",
      "startOffset" : 2,
      "endOffset" : 162
    }, {
      "referenceID" : 18,
      "context" : ", (Zinkevich et al., 2009; Agarwal and Duchi, 2011; Li et al., 2013; McMahan and Streeter, 2014; Quanrud and Khashabi, 2015; Liu et al., 2015; Duchi et al., 2015).",
      "startOffset" : 2,
      "endOffset" : 162
    }, {
      "referenceID" : 7,
      "context" : ", (Zinkevich et al., 2009; Agarwal and Duchi, 2011; Li et al., 2013; McMahan and Streeter, 2014; Quanrud and Khashabi, 2015; Liu et al., 2015; Duchi et al., 2015).",
      "startOffset" : 2,
      "endOffset" : 162
    }, {
      "referenceID" : 3,
      "context" : "Cooperative bandits with asymmetric feedback are also studied by Barrett and Stone (2011). In their model, an agent must teach the reward distribution to another agent while keeping the discounted regret under control.",
      "startOffset" : 65,
      "endOffset" : 90
    }, {
      "referenceID" : 3,
      "context" : "Cooperative bandits with asymmetric feedback are also studied by Barrett and Stone (2011). In their model, an agent must teach the reward distribution to another agent while keeping the discounted regret under control. Tekin and van der Schaar (2015) investigate a stochastic contextual bandit model where each agent can either privately select an action or have another agent select an action on his behalf.",
      "startOffset" : 65,
      "endOffset" : 251
    }, {
      "referenceID" : 3,
      "context" : "Cooperative bandits with asymmetric feedback are also studied by Barrett and Stone (2011). In their model, an agent must teach the reward distribution to another agent while keeping the discounted regret under control. Tekin and van der Schaar (2015) investigate a stochastic contextual bandit model where each agent can either privately select an action or have another agent select an action on his behalf. In a related paper, Tekin et al. (2014) look at a stochastic bandit model with combinatorial actions in a distributed recommender system setting, and study incentives among agents who can now recommend items taken from other agents’ inventories.",
      "startOffset" : 65,
      "endOffset" : 449
    }, {
      "referenceID" : 3,
      "context" : "Cooperative bandits with asymmetric feedback are also studied by Barrett and Stone (2011). In their model, an agent must teach the reward distribution to another agent while keeping the discounted regret under control. Tekin and van der Schaar (2015) investigate a stochastic contextual bandit model where each agent can either privately select an action or have another agent select an action on his behalf. In a related paper, Tekin et al. (2014) look at a stochastic bandit model with combinatorial actions in a distributed recommender system setting, and study incentives among agents who can now recommend items taken from other agents’ inventories. Another line of relevant work involves problems of decentralized bandit coordination. For example, Stranders et al. (2012) consider a bandit coordination problem where the the reward function is global and can be represented as a factor graph in which each agent controls a subset of the variables.",
      "startOffset" : 65,
      "endOffset" : 778
    }, {
      "referenceID" : 5,
      "context" : "Proofs of all the results stated here can be found in (Cesa-Bianchi et al., 2016).",
      "startOffset" : 54,
      "endOffset" : 81
    }, {
      "referenceID" : 17,
      "context" : "Our model is similar to the LOCAL communication model in distributed computing (Linial, 1992; Suomela, 2013), where the output of a node depends only on the inputs of other nodes in a constantsize neighborhood of it, and the goal is to derive algorithms whose running time is independent of the network size.",
      "startOffset" : 79,
      "endOffset" : 108
    }, {
      "referenceID" : 31,
      "context" : "Our model is similar to the LOCAL communication model in distributed computing (Linial, 1992; Suomela, 2013), where the output of a node depends only on the inputs of other nodes in a constantsize neighborhood of it, and the goal is to derive algorithms whose running time is independent of the network size.",
      "startOffset" : 79,
      "endOffset" : 108
    }, {
      "referenceID" : 36,
      "context" : "This is very much reminiscent of a full information scenario, and in fact our bound becomes of order √ (dG + 1)T lnK + dG lnT , which is close to the full information minimax rate √ (d+ 1)T lnK when feedback has a constant delay d (Weinberger and Ordentlich, 2002).",
      "startOffset" : 231,
      "endOffset" : 264
    }, {
      "referenceID" : 14,
      "context" : "There has been some recent work on adaptive learning rate tuning applied to nonstochastic bandit algorithms (Kocák et al., 2014; Neu, 2015).",
      "startOffset" : 108,
      "endOffset" : 139
    }, {
      "referenceID" : 23,
      "context" : "There has been some recent work on adaptive learning rate tuning applied to nonstochastic bandit algorithms (Kocák et al., 2014; Neu, 2015).",
      "startOffset" : 108,
      "endOffset" : 139
    }, {
      "referenceID" : 9,
      "context" : ", in (Firby and Haviland, 1997).",
      "startOffset" : 5,
      "endOffset" : 31
    }, {
      "referenceID" : 2,
      "context" : ", (Auer et al., 2002)— but in a slightly different manner.",
      "startOffset" : 2,
      "endOffset" : 21
    }, {
      "referenceID" : 35,
      "context" : ", (Wehmuth and Ziviani, 2013), and references therein.",
      "startOffset" : 2,
      "endOffset" : 29
    }, {
      "referenceID" : 31,
      "context" : "This problem was studied by Weinberger and Ordentlich (2002) in the full information case, for which they proved that √ (d+ 1)T lnK is the optimal order for the minimax regret.",
      "startOffset" : 28,
      "endOffset" : 61
    }, {
      "referenceID" : 10,
      "context" : "(2010, 2014) —see also Joulani et al. (2013)— whose techniques can be used to obtain a regret bound of order √ (d+ 1)KT .",
      "startOffset" : 23,
      "endOffset" : 45
    }, {
      "referenceID" : 26,
      "context" : "A possible line of attack to solve this problem could be the use of graph sparsity along the lines of (Pan et al., 2015; Duchi et al., 2013; Mania et al., 2015; McMahan and Streeter, 2014).",
      "startOffset" : 102,
      "endOffset" : 188
    }, {
      "referenceID" : 6,
      "context" : "A possible line of attack to solve this problem could be the use of graph sparsity along the lines of (Pan et al., 2015; Duchi et al., 2013; Mania et al., 2015; McMahan and Streeter, 2014).",
      "startOffset" : 102,
      "endOffset" : 188
    }, {
      "referenceID" : 19,
      "context" : "A possible line of attack to solve this problem could be the use of graph sparsity along the lines of (Pan et al., 2015; Duchi et al., 2013; Mania et al., 2015; McMahan and Streeter, 2014).",
      "startOffset" : 102,
      "endOffset" : 188
    }, {
      "referenceID" : 20,
      "context" : "A possible line of attack to solve this problem could be the use of graph sparsity along the lines of (Pan et al., 2015; Duchi et al., 2013; Mania et al., 2015; McMahan and Streeter, 2014).",
      "startOffset" : 102,
      "endOffset" : 188
    }, {
      "referenceID" : 11,
      "context" : "Even for the single-agent setting, we do not know whether regret bounds of the form √ (D + T ) lnK, where D is the total delay experienced over the T rounds, could be proven —see (Joulani et al., 2016; Quanrud and Khashabi, 2015) for similar results in the fullinformation setting.",
      "startOffset" : 179,
      "endOffset" : 229
    }, {
      "referenceID" : 27,
      "context" : "Even for the single-agent setting, we do not know whether regret bounds of the form √ (D + T ) lnK, where D is the total delay experienced over the T rounds, could be proven —see (Joulani et al., 2016; Quanrud and Khashabi, 2015) for similar results in the fullinformation setting.",
      "startOffset" : 179,
      "endOffset" : 229
    }, {
      "referenceID" : 2,
      "context" : "The proof hinges on combining the known lower bound Ω (√ KT ) for bandits without delay of Auer et al. (2002) with the following argument by Weinberger and Ordentlich (2002) that provides a lower bound for the full information case with delay.",
      "startOffset" : 91,
      "endOffset" : 110
    }, {
      "referenceID" : 2,
      "context" : "The proof hinges on combining the known lower bound Ω (√ KT ) for bandits without delay of Auer et al. (2002) with the following argument by Weinberger and Ordentlich (2002) that provides a lower bound for the full information case with delay.",
      "startOffset" : 91,
      "endOffset" : 174
    } ],
    "year" : 2016,
    "abstractText" : "We study networks of communicating learning agents that cooperate to solve a common nonstochastic bandit problem. Agents use an underlying communication network to get messages about actions selected by other agents, and drop messages that took more than d hops to arrive, where d is a delay parameter. We introduce EXP3-COOP, a cooperative version of the EXP3 algorithm and prove that with K actions and N agents the average per-agent regret after T rounds is at most of order √( d+ 1 + KN α≤d ) (T lnK), where α≤d is the independence number of the d-th power of the communication graphG. We then show that for any connected graph, for d = √ K the regret bound is K √ T , strictly better than the minimax regret √ KT for noncooperating agents. More informed choices of d lead to bounds which are arbitrarily close to the full information minimax regret √ T lnK when G is dense. When G has sparse components, we show that a variant of EXP3-COOP, allowing agents to choose their parameters according to their centrality in G, strictly improves the regret. Finally, as a by-product of our analysis, we provide the first characterization of the minimax regret for bandit learning with delay.",
    "creator" : "LaTeX with hyperref package"
  }
}