{
  "name" : "1312.3393.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Relative Upper Confidence Bound for the K-Armed Dueling Bandit Problem",
    "authors" : [ "Masrour Zoghi", "Shimon Whiteson" ],
    "emails" : [ "M.Zoghi@uva.nl", "S.A.Whiteson@uva.nl", "remi.munos@inria.fr", "M.deRijke@uva.nl" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "In this paper, we propose and analyze a new algorithm, called Relative Upper Confidence Bound (RUCB), for the K-armed dueling bandit problem (Yue et al., 2012), a variation on the K-armed bandit problem, where the feedback comes in the form of pairwise preferences. We assess the performance of this algorithm using one of the main current applications of the K-armed dueling bandit problem, ranker evaluation (Hofmann et al., 2013; Joachims, 2002; Yue & Joachims, 2011), which is used in information retrieval, ad placement and recommender systems, among others.\nProceedings of the 31 st International Conference on Machine Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).\nThe K-armed dueling bandit problem is part of the general framework of preference learning (Fürnkranz & Hüllermeier, 2010; Fürnkranz et al., 2012), where the goal is to learn, not from real-valued feedback, but from relative feedback, which specifies only which of two alternatives is preferred. Developing effective preference learning methods is important for dealing with domains in which feedback is naturally qualitative (e.g., because it is provided by a human) and specifying real-valued feedback instead would be arbitrary or inefficient (Fürnkranz et al., 2012).\nOther algorithms proposed for this problem are Interleaved Filter (IF) (Yue et al., 2012), Beat the Mean (BTM) (Yue & Joachims, 2011), and SAVAGE (Urvoy et al., 2013). All of these methods were designed for the finite-horizon setting, in which the algorithm requires as input the exploration horizon, T , the time by which the algorithm needs to produce the best arm. The algorithm is then judged based upon either the accuracy of the returned best arm or the regret accumulated in the exploration phase.1 All three of these algorithms use the exploration horizon to set their internal parameters, so for each T , there is a separate algorithm IFT , BTMT and SAVAGET . By contrast, RUCB does not require this input, making it more useful in practice, since a good exploration horizon is often difficult to guess. Nonetheless, RUCB outperforms these algorithms in terms of the accuracy and regret metrics used in the finite-horizon setting.\nThe main idea of RUCB is to maintain optimistic estimates of the probabilities of all possible pairwise out-\n1These terms are formalized in Section 2.\nar X\niv :1\n31 2.\n33 93\nv1 [\ncs .L\nG ]\n1 2\ncomes, and (1) use these estimates to select a potential champion, which is an arm that has a chance of being the best arm, and (2) select an arm to compare to this potential champion by performing regular Upper Confidence Bound (Auer et al., 2002) relative to it.\nWe prove a finite-time high-probability bound of O(log t) on the cumulative regret of RUCB, from which we deduce a bound on the expected cumulative regret. These bounds rely on substantially less restrictive assumptions on the K-armed dueling bandit problem than IF and BTM and have better multiplicative constants than those of SAVAGE. Furthermore, our bounds are the first explicitly non-asymptotic results for the K-armed dueling bandit problem.\nMore importantly, The main distinction of our result is that it holds for all time steps. By contrast, given an exploration horizon T , the results for IF, BTM and SAVAGE bound only the regret accumulated by IFT , BTMT and SAVAGET in the first T time steps.\nFinally, we evaluate our method empirically using real data from an information retrieval application. The results show that RUCB can learn quickly and effectively and greatly outperforms BTM and SAVAGE.\nThe main contributions of this paper are as follows:\n• A novel algorithm for the K-armed dueling bandit problem that is more broadly applicable than existing algorithms,\n• More comprehensive theoretical results that make less restrictive assumptions than those of IF and BTM, have better multiplicative constants than the results of SAVAGE, and apply to all time steps, and\n• Experimental results, based on a real-world application, demonstrating the superior performance of our algorithm compared to existing methods."
    }, {
      "heading" : "2. Problem Setting",
      "text" : "The K-armed dueling bandit problem (Yue et al., 2012) is a modification of the K-armed bandit problem (Auer et al., 2002): the latter considers K arms {a1, . . . , aK} and at each time-step, an arm ai can be pulled, generating a reward drawn from an unknown stationary distribution with expected value µi. The K-armed dueling bandit problem is a variation, where instead of pulling a single arm, we choose a pair (ai, aj) and receive one of the two as the better choice, with the probability of ai being picked equal to a constant pij and that of aj equal to pji = 1 − pij . We define the preference matrix P = [pij ], whose ij entry is pij .\nIn this paper, we assume that there exists a Condorcet winner (Urvoy et al., 2013): an arm, which without\nloss of generality we label a1, such that p1i > 1 2 for all i > 1. Given a Condorcet winner, we define regret for each time-step as follows (Yue et al., 2012): if arms ai and aj were chosen for comparison at time t, then regret at that time is set to be rt := ∆1i+∆1j\n2 , with ∆k := p1k − 12 for all k ∈ {1, . . . ,K}. Thus, regret measures the average advantage that the Condorcet winner has over the two arms being compared against each other. Given our assumption on the probabilities p1k, this implies that r = 0 if and only if the best arm is compared against itself. We define cumulative regret up to time T to be RT = ∑T t=1 rt.\nThe Condorcet winner is different in a subtle but important way from the Borda winner (Urvoy et al., 2013), which is an arm ab that satisfies ∑ j pbj ≥∑\nj pij , for all i = 1, . . . ,K. In other words, when averaged across all other arms, the Borda winner is the arm with the highest probability of winning a given comparison. In the K-armed dueling bandit problem, the Condorcet winner is sought rather than the Borda winner, for two reasons. First, in many applications, including the ranker evaluation problem addressed in our experiments, the eventual goal is to adapt to the preferences of the users of the system. Given a choice between the Borda and Condorcet winners, those users prefer the latter in a direct comparison, so it is immaterial how these two arms fare against the others. Second, in settings where the Borda winner is more appropriate, no special methods are required: one can simply solve the K-armed bandit algorithm with arms {a1, . . . , aK}, where pulling ai means choosing an index j ∈ {1, . . . ,K} randomly and comparing ai against aj . Thus, research on the K-armed dueling bandit problem focuses on finding the Condorcet winner, for which special methods are required to avoid mistakenly choosing the Borda winner.\nThe goal of a bandit algorithm can be formalized in several ways. In this paper, we consider two standard settings:\n1. The finite-horizon setting : In this setting, the algorithm is told in advance the exploration horizon, T , i.e., the number of time-steps that the evaluation process is given to explore before it has to produce a single arm as the best, which will be exploited thenceforth. In this setting, the algorithm can be assessed on its accuracy, the probability that a given run of the algorithm reports the Condorcet winner as the best arm (Urvoy et al., 2013), which is related to expected simple regret : the regret associated with the algorithm’s choice of the best arm, i.e., rT+1 (Bubeck et al., 2009). Another measure of success in this setting is the amount of regret accumulated during the exploration phase, as for-\nmulated by the explore-then-exploit problem formulation (Yue et al., 2012).\n2. The horizonless setting : In this setting, no horizon is specified and the evaluation process continues indefinitely. Thus, it is no longer sufficient for the algorithm to maximize accuracy or minimize regret after a single horizon is reached. Instead, it must minimize regret across all horizons by rapidly decreasing the frequency of comparisons involving suboptimal arms, particularly those that fare worse in comparison to the best arm. This goal can be formulated as minimizing the cumulative regret over time, rather than with respect to a fixed horizon (Lai & Robbins, 1985).\nAs we describe in Section 3, all existing K-armed dueling bandit methods target the finite-horizon setting. However, we argue that the horizonless setting is more relevant in practice for the following reason: finitehorizon methods require a horizon as input and often behave differently for different horizons. This poses a practical problem because it is typically difficult to know in advance how many comparisons are required to determine the best arm with confidence and thus how to set the horizon. If the horizon is set too long, the algorithm is too exploratory, increasing the number of evaluations needed to find the best arm. If it is set too short, the best arm remains unknown when the horizon is reached and the algorithm must be restarted with a longer horizon.\nMoreover, any algorithm that can deal with the horizonless setting can easily be modified to address the finite-horizon setting by simply stopping the algorithm when it reaches the horizon and returning the best arm. By contrast, for the reverse direction, one would have to resort to the “doubling trick” (Cesa-Bianchi & Lugosi, 2006, Section 2.3), which leads to substantially worse regret results: this is because all of the upper bounds proven for methods addressing the finitehorizon setting so far are in O(log T ) and applying the doubling trick to such results would lead to regret bounds of order (log T )2, with the extra log factor coming from the number of partitions.\nTo the best of our knowledge, RUCB is the first Karmed dueling bandit algorithm that can function in the horizonless setting without resorting to the doubling trick. We show in Section 4 how it can be adapted to the finite-horizon setting."
    }, {
      "heading" : "3. Related Work",
      "text" : "In this section, we briefly survey existing methods for the K-armed dueling bandit problem.\nThe first method for the K-armed dueling bandit problem is interleaved filter (IF) (Yue et al., 2012), which was designed for a finite-horizon scenario and which proceeds by picking a reference arm to compare against the rest and using it to eliminate other arms, until the reference arm is eliminated by a better arm, in which case the latter becomes the reference arm and the algorithm continues as before. The algorithm terminates either when all other arms are eliminated or if the exploration horizon T is reached.\nMore recently, the beat the mean (BTM) algorithm has been shown to outperform IF (Yue & Joachims, 2011), while imposing less restrictive assumptions on the K-armed dueling bandit problem. BTM focuses exploration on the arms that have been involved in the fewest comparisons. When it determines that an arm fares on average too poorly in comparison to the remaining arms, it removes it from consideration. More precisely, BTM considers the performance of each arm against the mean arm by averaging the arm’s scores against all other arms and uses these estimates to decide which arm should be eliminated.\nBoth IF and BTM require the comparison probabilities pij to satisfy certain conditions that are difficult to verify without specific knowledge about the dueling bandit problem at hand and, moreover, are often violated in practice (see the supplementary material for a more thorough discussion and analysis of these assumptions). Under these conditions, theoretical results have been proven for IF and BTM in (Yue et al., 2012) and (Yue & Joachims, 2011). More precisely, both algorithms take the exploration horizon T as an input and so for each T , there are algorithms IFT and BTMT ; the results then state the following: for large T , in the case of IFT , we have the expected regret bound\nE [ RIFTT ] ≤ C K log T\nminKj=2 ∆j ,\nand, in the case of BTMT , the high probability regret bound\nRBTMTT ≤ C ′ γ7K log T\nminKj=2 ∆j with high probability,\nwhere arm a1 is assumed to be the best arm, and we define ∆j := p1j − 12 , and C and C ′ are constants independent of the specific dueling bandit problem.\nThe first bound matches a lower bound proven in (Yue et al., 2012, Theorem 4). However, as pointed out in (Yue & Joachims, 2011), this result holds for a very restrictive class of K-armed dueling bandit problems.\nIn an attempt to remedy this issue, the second bound was proven for BTM, which includes a relaxation parameter γ that allows for a broader class of problems, as discussed in the supplementary material. The difficulty with this result is that the parameter γ, which depends on the probabilities pij and must be passed to the algorithm, can be very large. Since it is raised to the power of 7, this makes the bound very loose. For instance, in the three-ranker evaluation experiments discussed in Section 6, the values for γ are 4.85, 11.6 and 47.3 for the 16-, 32- and 64-armed examples.\nIn contrast to the above limitations and loosenesses, in Section 5 we provide explicit bounds on the regret accumulated by RUCB that do not depend on γ and require only the existence of a Condorcet winner for their validity, which makes them much more broadly applicable.\nSensitivity Analysis of VAriables for Generic Exploration (SAVAGE) (Urvoy et al., 2013) is a recently proposed algorithm that outperforms both IF and BTM by a wide margin when the number of arms is of moderate size. Moreover, one version of SAVAGE, called Condorcet SAVAGE, makes the Condorcet assumption and performed the best experimentally (Urvoy et al., 2013). Condorcet SAVAGE compares pairs of arms uniformly randomly until there exists a pair for which one of the arms beats another by a wide margin, in which case the loser is removed from the pool of arms under consideration. We show in this paper that our proposed algorithm for ranker evaluation substantially outperforms Condorcet SAVAGE.\nThe theoretical result proven for Condorcet SAVAGE has the following form (Urvoy et al., 2013, Theorem 3). First, let us assume that a1 is the Condorcet winner and let T̂CSAVAGET denote the number of iterations the Condorcet SAVAGE algorithm with exploration horizon T requires before terminating and returning the best arm; then, given δ > 0, with probability 1−δ, we have for large T\nT̂CSAVAGET ≤ C ′′ K−1∑\nj=1\nj · log ( KT δ )\n∆2j+1 ,\nwith the indices j arranged such that ∆2 ≤ · · · ≤ ∆K and ∆j = p1j− 12 as before, and C ′′ a problem independent constant. This bound is very similar in spirit to our high probability result, with the important distinction that, unlike the above bound, the multiplicative factors in our result (i.e., the Dij in Theorem 2 below) do not depend on δ. Moreover, in (Urvoy et al., 2013, Appendix B.1), the authors show that for large T we have the following expected regret bound:\nE [ RCSAVAGETT ] ≤ C ′′ K∑\nj=2\nj · log ( KT 2 )\n∆2j + 1.\nThis is similar to our expected regret bound in Theorem 3, although for difficult problems where the ∆j are small, Theorem 3 yields a tighter bound due to the presence of the ∆j in the numerator of the second summand.\nAn important advantage that our result has over the results reviewed here is an explicit expression for the additive constant, which was left out of the analyses of IF, BTM and SAVAGE.\nFinally, note that all of the above results bound only RT , where T is the predetermined exploration horizon, since IF, BTM and SAVAGE were designed for the finite-horizon setting. By contrast, in Section 5, we bound the cumulative regret of each version of our algorithm for all time steps."
    }, {
      "heading" : "4. Method",
      "text" : "We now introduce Relative Upper Confidence Bound (RUCB), which is applicable to any K-armed dueling bandit problem with a Condorcet winner.\nAlgorithm 1 Relative Upper Confidence Bound Input: α > 12 , T ∈ {1, 2, . . .} ∪ {∞} 1: W = [wij ] ← 0K×K // 2D array of wins: wij is\nthe number of times ai beat aj 2: for t = 1, . . . , T do\n3: U := [uij ] = W W+WT + √ α ln t W+WT // All operations are element-wise; x0 := 1 for any x. 4: uii ← 12 for each i = 1, . . . ,K. 5: Pick any c satisfying ucj ≥ 12 for all j. If no such c, pick c randomly from {1, . . . ,K}. 6: d← arg maxj ujc 7: Compare arms ac and ad and increment wcd or\nwdc depending on which arm wins. 8: end for Return: An arm ac that beats the most arms, i.e., c\nwith the largest count # { j| wcjwcj+wjc > 1 2 } .\nIn each time-step, RUCB, shown in Algorithm 1, goes through the following three stages:\n(1) RUCB puts all arms in a pool of potential champions. Then, it compares each arm ai against all other arms optimistically: for all i 6= j, we compute the upper bound uij(t) = µij(t) + cij(t), where µij(t) is the frequentist estimate of pij at time t and cij(t) is an optimism bonus that increases with t and decreases with the number of comparisons between i and j (Line 3).\nIf we have uij < 1 2 for any j, then ai is removed from the pool. Next, a champion arm ac is chosen randomly from the remaining potential champions (Line 5).\n(2) Regular UCB is performed using ac as a benchmark (Line 6), i.e., UCB is performed on the set of arms a1c . . . aKc. Specifically, we select the arm d = arg maxj ujc. When c 6= j, ujc is defined as above. When c = j, since pcc = 1 2 , we set ucc = 1 2 (Line 4).\n(3) The pair (ac, ad) are compared and the score sheet is updated as appropriate (Line 7).\nNote that in stage (1) the comparisons are based on ucj , i.e., ac is compared optimistically to the other arms, making it easier for it to become the champion. By contrast, in stage (2) the comparisons are based on ujc, i.e., ac is compared to the other arms pessimistically, making it more difficult for ac to be compared against itself. This is important because comparing an arm against itself yields no information. Thus, RUCB strives to avoid auto-comparisons until there is great certainty that ac is indeed the Condorcet winner.\nEventually, as more comparisons are conducted, the estimates µ1j tend to concentrate above 1 2 and the optimism bonuses c1j(t) will become small. Thus, both stages of the algorithm will increasingly select a1, i.e., ac = ad = a1. Since comparing a1 to itself is optimal, rt declines over time.\nNote that Algorithm 1 is a finite-horizon algorithm if T <∞ and a horizonless one if T =∞, in which case the for loop never terminates."
    }, {
      "heading" : "5. Theoretical Results",
      "text" : "In this section, we prove finite-time high-probability and expected regret bounds for RUCB. We first state Lemma 1 and use it to prove a high-probability bound in Theorem 2, from which we deduce an expected regret bound in Theorem 3.\nTo simplify notation, we assume without loss of generality that a1 is the optimal arm in the following. Moreover, given any K-armed dueling bandit algorithm, we define wij(t) to be the number of times arm ai has beaten aj in the first t iterations of the algorithm. We also define uij(t) := wij(t) wij(t)+wji(t) + √ α ln t wij(t)+wji(t) , for any given α > 0, and set lij(t) := 1−uji(t). Moreover, for any δ > 0, define C(δ) := ( (4α−1)K2 (2α−1)δ ) 1 2α−1 .\nLemma 1. Let P := [pij ] be the preference matrix of a K-armed dueling bandit problem with arms {a1, . . . , aK}, satisfying p1j > 12 for all j > 1 (i.e. a1 is the Condorcet winner). Then, for any dueling\nbandit algorithm and any α > 12 and δ > 0, we have\nP ( ∀ t > C(δ), i, j, pij ∈ [lij(t), uij(t)] ) > 1− δ.\nProof. See the supplementary material.\nThe idea behind this lemma is depicted in Figure 1, which illustrates the two phenomena that make it possible: first, as long as arms ai and aj are not compared against each other, the interval [lij(t), uij(t)] will grow in length as √ log t, hence approaching pij ; second, as the number of comparisons between ai and aj increases, the estimated means µij approach pij , hence increasing the probability that the interval [lij(t), uij(t)] will contain pij .\nLet us now turn to our high probability bound:\nTheorem 2. Given a preference matrix P = [pij ] and δ > 0 and α > 12 , define C(δ) := ( (4α−1)K2 (2α−1)δ ) 1 2α−1 and Dij := 4α\nmin{∆2i ,∆2j} for each i, j = 1, . . . ,K with i 6= j,\nwhere ∆i := 1 2 − pi1, and set Dii = 0 for all i. Then, if we apply Algorithm 1 to the K-armed dueling bandit problem defined by P, given any pair (i, j) 6= (1, 1), the number of comparisons between arms ai and aj performed up to time t, denoted by Nij(t), satisfies\nP ( ∀ t, Nij(t) ≤ max { C(δ), Dij ln t }) > 1− δ. (1)\nMoreover, we have the following high probability bound for the regret accrued by the algorithm:\nP ( ∀ t, Rt ≤ C(δ)∆∗ + ∑\ni>j\nDij∆ij ln t\n) > 1− δ, (2)\nwhere ∆∗ := maxi ∆i and ∆ij := ∆i+∆j\n2 , while Rt is the cumulative regret as defined in Section 2.\nProof. Given Lemma 1, we know with probability 1−δ that pij ∈ [lij(t), uij(t)] for all t > C(δ). Let us first deal with the easy case when i = j 6= 1: when t > C(δ) holds, ai cannot be played against itself, since if we get c = i in Algorithm 1, then by Lemma 1 and the fact that a1 is the Condorcet winner we have\nuii(t) = 1\n2 < p1i ≤ u1i(t),\nand so d 6= i. Now, let us assume that distinct arms ai and aj have been compared against each other more than Dij ln t times and that t > C(δ). If s is the last time ai and aj were compared against each other, we must have\nuij(s)− lij(s) = 2 √ α ln s\nNij(t) (3)\n≤ 2 √ α ln t\nNij(t) < 2\n√√√√ α ln t 4α ln t\nmin{∆2i ,∆2j} = min{∆i,∆j}.\nOn the other hand, for ai to have been compared against aj at time s, one of the following two scenarios must have happened:\nI. In Algorithm 1, we had c = i and d = j, in which case both of the following inequalities must hold:\na. uij(s) ≥ 12 , since otherwise c could not have been set to i by Line 5 of Algorithm 1, and b. lij(s) = 1− uji(s) ≤ 1− p1i = pi1, since we know that p1j ≤ u1i(t), by Lemma 1 and the fact that t > C(δ), and for d = j to be satisfied, we must have u1i(t) ≤ uji(t) by Line 6 of Algorithm 1.\nFrom these two inequalities, we can conclude\nuij(s)− lij(s) ≥ 1\n2 − pi1 = ∆i. (4)\nThis inequality is illustrated using the lower right confidence interval in the (ai, aj) block of Figure 2, where the interval shows [lij(s), uij(s)] and the distance between the dotted lines is 12 − pi1.\nII. In Algorithm 1, we had c = j and d = i, in which case swapping i and j in the above argument gives\nuji(s)− lji(s) ≥ 1\n2 − pj1 = ∆j . (5)\nSimilarly, this is illustrated using the lower left confidence interval in the (aj , ai) block of Figure 2, where the interval shows [lji(s), uji(s)] and the distance between the dotted lines is 12 − pj1.\nPutting (4) and (5) together with (3) yields a contradiction, so with probability 1 − δ we cannot have Nij be larger than both C(δ) and Dij ln t.\nThis gives us (1), from which (2) follows by allowing for the largest regret, ∆∗, to occur in each of the first C(δ) steps of the algorithm and adding the regret accrued by Dij ln t comparisons between ai and aj .\nNext, we prove our expected regret bound:\nTheorem 3. Given α > 1, the expected regret accumulated by RUCB after t iterations is bounded by\nE[Rt] ≤ ∆∗ (\n(4α− 1)K2 2α− 1\n) 1 2α−1 2α− 1\n2α− 2\n+ ∑\ni>j\n2α ∆i + ∆j\nmin{∆2i ,∆2j} ln t. (6)\nProof. We can obtain the bound in (6) from (2) by integrating with respect to δ from 0 to 1. This is because given any one-dimensional random variable X with\nCDF FX , we can use the identity E[X] = ∫ 1 0 F−1X (q)dq. In our case, X = Rt for a fixed time t and, as illustrated in Figure 3, we can deduce from (2) that FRt(r) > H −1 t (r), which gives the bound\nF−1Rt (q) < Ht(q) = C(1− q)∆ ∗ +\n∑\ni>j\nDij∆ij ln t.\nNow, assume that α > 1. To derive (6) from the above inequality, we need to integrate the righthand side, and since it is only the first term in the summand that depends on q, that is all we need to integrate. To do so, recall that C(δ) := (\n(4α−1)K2 (2α−1)δ\n) 1 2α−1\n, so to simplify\nnotation, we define L := (\n(4α−1)K2 2α−1\n) 1 2α−1\n. Now, we\ncan carry out the integration as follows, beginning by using the substitution 1− q = δ, dq = −dδ: ∫ 1\nq=0\nC(1− q)dq = ∫ 0\nδ=1\n−C(δ)dδ\n=\n∫ 1\n0\n( (4α− 1)K2 (2α− 1)δ ) 1 2α−1 dδ = L ∫ 1\n0\nδ− 1 2α−1 dδ\n= L\n[ δ1− 1 2α−1\n1− 12α−1\n]1\n0\n=\n( (4α− 1)K2\n2α− 1\n) 1 2α−1 2α− 1\n2α− 2 .\nRemark 4. Note that RUCB uses the upperconfidence bounds (Line 3 of Algorithm 1) introduced in the original version of UCB (Auer et al., 2002) (up to the α factor). Recently refined upper-confidence bounds (such as UCB-V (Audibert et al., 2009) or KL-UCB (Cappé et al., 2013)) have improved performance for the regular K-armed bandit problem. However, in our setting the arm distributions are Bernoulli and the comparison value is 1/2. Thus, since we have 2∆2i ≤ kl(p1,i, 1/2) ≤ 4∆2i (where kl(a, b) =\na log ab + (1− a) log 1−a1−b is the KL divergence between Bernoulli distributions with parameters a and b), we deduce that using KL-UCB instead of UCB does not improve the leading constant in the logarithmic term of the regret by a numerical factor of more than 2."
    }, {
      "heading" : "6. Experiments",
      "text" : "To evaluate RUCB, we apply it to the problem of ranker evaluation from the field of information retrieval (IR) (Manning et al., 2008). A ranker is a function that takes as input a user’s search query and ranks the documents in a collection according to their relevance to that query. Ranker evaluation aims to determine which among a set of rankers performs best. One effective way to achieve this is to use interleaved comparisons (Radlinski et al., 2008), which interleave the documents proposed by two different rankers and presents the resulting list to the user, whose resulting click feedback is used to infer a noisy preference for one of the rankers. Given a set of K rankers, the problem of finding the best ranker can then be modeled as a K-armed dueling bandit problem, with each arm corresponding to a ranker.\nOur experimental setup is built on real IR data, namely the LETOR NP2004 dataset (Liu et al., 2007). Using this data set, we create a set of 64 rankers, each corresponding to a ranking feature provided in the data set, e.g., PageRank. The ranker evaluation task thus corresponds to determining which single feature constitutes the best ranker (Hofmann et al., 2013).\nTo compare a pair of rankers, we use probabilistic interleave (PI) (Hofmann et al., 2011), a recently developed method for interleaved comparisons. To model the user’s click behavior on the resulting interleaved lists, we employ a probabilistic user model (Craswell et al., 2008; Hofmann et al., 2011) that uses as input the manual labels (classifying documents as relevant or not for given queries) provided with the LETOR NP2004 dataset. Queries are sampled randomly and clicks are generated probabilistically by conditioning on these assessments in a way that resembles the behavior of an actual user (Guo et al., 2009a;b).\nFollowing (Yue & Joachims, 2011), we first used the above approach to estimate the comparison probabilities pij for each pair of rankers and then used these probabilities to simulate comparisons between rankers. More specifically, we estimated the full preference matrix by performing 4000 interleaved comparisons on each pair of the 64 feature rankers included in the LETOR dataset.\nWe evaluated RUCB, Condorcet SAVAGE and BTM\nusing randomly chosen subsets from the pool of 64 rankers, yielding K-armed dueling bandit problems with K ∈ {16, 32, 64}. For each set of rankers, we performed 100 independent runs of each algorithm for a maximum of 4.5 million iterations. For RUCB we set α = 0.51, which approaches the limit of our highprobability theoretical results, i.e., α > 0.5 as in Theorem 2. We did not include an evaluation of IF, since both BTM and Condocet SAVAGE were shown to outperform it (Urvoy et al., 2013; Yue & Joachims, 2011).\nSince BTM and SAVAGE require the exploration horizon as input, we ran BTMT and CSAVAGET for various horizons T ranging from 1000 to 4.5 million. In the top row of plots in Figure 4, the markers on the green and the blue curves show the regret accumulated by BTMT and CSAVAGET in the first T iteration of the algorithm for each of these horizons. Thus, each marker corresponds, not to the continuation of the runs that produced the previous marker, but to new runs conducted with a larger T .\nSince RUCB is horizonless, we ran it for 4.5 million iterations and plotted the cumulative regret, as shown using the red curves in the same plots. In the case of all three algorithms, the solid line shows the expected cumulative regret averaged across all 100 runs and the dotted lines show the minimum and the maximum cu-\nmulative regret that was observed across runs. Note that these plots are in log-log scale.\nThe bottom plots in Figure 4 show the accuracy of all three algorithms across 100 runs, computed at the same times as the exploration horizons used for BTM and SAVAGE in the regret plots. Note that these plots are in lin-log scale.\nThese results clearly demonstrate that RUCB identifies the best arm more quickly, since it asymptotically accumulates 5 to 10 times less regret than Condorcet SAVAGE, while reaching higher levels of accuracy in roughly 20% of the time as Condorcet SAVAGE, all without knowing the horizon T . The contrast is even more stark when comparing to BTM."
    }, {
      "heading" : "7. Conclusions",
      "text" : "This paper proposed a new method called Relative Upper Confidence Bound (RUCB) for the K-armed dueling bandit problem that extends the Upper Confidence Bound (UCB) algorithm to the relative setting by using optimistic estimates of the pairwise probabilities to choose a potential champion and conducting regular UCB with the champion as the benchmark.\nWe proved finite-time high-probability and expected\nregret bounds of order O(log t) for our algorithm and evaluated it empirically in an information retrieval application. Unlike existing results, our regret bounds hold for all time steps, rather than just a specific horizon T input to the algorithm. Furthermore, they rely on less restrictive assumptions or have better multiplicative constants than existing methods. Finally, the empirical results showed that RUCB greatly outperforms state-of-the-art methods.\nIn future work, we will consider two extensions to this research. First, building off extensions of UCB to the continuous bandit setting (Bubeck et al., 2011; de Freitas et al., 2012; Munos, 2011; Srinivas et al., 2010; Valko et al., 2013), we aim to extend RUCB to the continuous dueling bandit setting, without a convexity assumption as in (Yue & Joachims, 2009). Second, building off Thompson Sampling (Agrawal & Goyal, 2012; Kauffmann et al., 2012; Thompson, 1933), an elegant and effective sampling-based alternative to UCB, we will investigate whether a sampling-based extension to RUCB would be amenable to theoretical analysis. Both these extensions involve overcoming not only the technical difficulties present in the regular bandit setting, but also those that arise from the two-stage nature of RUCB."
    }, {
      "heading" : "8. Appendix",
      "text" : "Here we provide some details that were alluded to in the main body of the paper."
    }, {
      "heading" : "8.1. The Condorcet Assumption",
      "text" : "As mentioned in Section 3, IF and BTM require the comparison probabilities pij to satisfy certain difficult to verify conditions. Specifically, IF and BTM require a total ordering {a1, . . . , aK} of the arms to exist such that pij > 1 2 for all i < j. Here we provide evidence that this assumption is often violated in practice. By contrast, the algorithm we propose in Section 4 makes only the Condorcet assumption, which is implied by the total ordering assumption of IF and BTM.\nIn order to test how stringent an assumption the existence of a Condorcet winner is compared the total ordering assumption, we estimated the probability of each assumption holding in our ranker evaluation application. Using the same preference matrix as in our experiments in Section 6, we computed for each K = 1, . . . , 64 the probability PK that a given Karmed dueling bandit problem obtained from considering K of our 64 feature rankers would have a Condorcet winner as follows: first, we calculated the number of K-armed dueling bandit that have a Condorcet winner by calculating for each feature ranker r how many K-armed duelings bandits it can be the Condorcet winner of: for each r, this is equal to ( Nr K ) , where Nr is the number rankers that r beats; next, we divided this total number of K-armed dueling bandit\nwith a Condorcet winner by (\n64 K\n) , which is the number\nof all K-armed dueling bandit that one could construct from these 64 rankers.\nThe probabilities PK , plotted as a function of K in Figure 5 (the red curve), were all larger than 0.97. The same plot also shows an estimate of the probability that the total ordering assumption holds for a given K (the blue curve), which was obtained by randomly selecting 100, 000 K-armed bandits and searching for ones that satisfy the total ordering assumption. As can be seen from Figure 5, as K grows the probability that the total ordering assumption holds decreases rapidly. This is because there exist cyclical relationships between these feature rankers and as soon as the chosen subset of feature rankers contains one of these cycles, it fails to satisfy the total ordering condition. By contrast, the Condorcet assumption will still be satisfied as long as the cycle does not include the Condorcet winner. Moreover, because of the presence of these cycles, the probability that the Condorcet assumption holds decreases initially as K increases, but then increases again because the number of all possible K-armed dueling bandit decreases asK approaches 64.\nFurthermore, in addition to the total ordering assumption, IF and BTM each require a form of stochastic transitivity. In particular, IF requires strong stochastic transitivity ; for any triple (i, j, k), with i < j < k, the following condition needs to be satisfied:\npik ≥ max{pij , pjk}.\nBTM requies the less restrictive relaxed stochastic transitivity, i.e., that there exists a number γ ≥ 1 such that for all pairs (j, k) with 1 < j < k, we have\nγp1k ≥ max{p1j , pjk}.\nAs pointed out in (Yue & Joachims, 2011), strong stochastic transitivity is often violated in practice, a phenomenon also observed in our experiments: for instance, all of the K-armed dueling bandit on which we experimented require γ > 1.\nEven though BTM permits a broader class of K-armed dueling bandit problems, it requires γ to be explicitly passed to it as a parameter, which poses substantial difficulties in practice. If γ is underestimated, the algorithm can in certain circumstances be misled with high probability into choosing the Borda winner instead of the Condorcet winner, e.g., when the Borda winner has a larger average advantage over the remaining arms than the Condorcet winner. On the other hand, though overestimating γ does not cause the algorithm to choose the wrong arm, it nonetheless results in a severe penalty, since it makes the algorithm\nMoreover, for each time-step, the area of the shaded region under the vertical graphs is the bound given by the ChernoffHoeffding (CH) bound on the probability that the confidence interval will not contain pij . Note that the CH bound has the form e−(x−µ ij n ) 2 and so in order for this number to be the area under a graph (hence making it easier to illustrate in a figure), we have drawn the derivative of this function, f ijn (x) := |x− µijn |e−(x−µ ij n ) 2\n, which is why the graphs are equal to 0 in the middle. Note that this does not mean that µijn has very low probability of being close to pij : the graphs drawn here are not the PDFs of the posteriors, but simply a manifestation of the bound given by the Chernoff-Hoeffding bound.\nMore specifically, the property that they satisfy is that P ( pij /∈ [lij(t), uij(t)] ) ≤ ∫ lij(t) −∞ f ij Nij(t) (x)dx+ ∫∞ uij(t) f ijNij(t)(x)dx.\nmuch more exploratory, yielding the γ7 term in the upper bound on the cumulative regret, as discussed in Section 3."
    }, {
      "heading" : "8.2. Proof of Lemma 1",
      "text" : "In this section, we prove Lemma 1, whose statement is repeated here for convenience. Recall from Section 5 that we assume without loss of generality that a1 is the optimal arm. Moreover, given any K-armed dueling bandit algorithm, we define wij(t) to be the number of times arm ai has beaten aj in the first t iterations of the algorithm. We also define uij(t) :=\nwij(t) wij(t)+wji(t)\n+ √\nα ln t wij(t)+wji(t) , where α is any positive\ncontant, and lij(t) := 1 − uji(t). Moreover, for any δ > 0, define C(δ) := ( (4α−1)K2 (2α−1)δ ) 1 2α−1 .\nLemma 1. Let P := [pij ] be the preference matrix of a K-armed dueling bandit problem with arms {a1, . . . , aK}, satisfying p1j > 12 for all j > 1 (i.e., a1 is the Condorcet winner). Then, for any dueling bandit algorithm and any α > 12 and δ > 0, we have\nP ( ∀ t > C(δ), i, j, pij ∈ [lij(t), uij(t)] ) > 1− δ. (7)\nProof. To decompose the lefthand side of (7), we introduce the notation Gij(t) for the “good” event that at time t we have pij ∈ [lij(t), uij(t)], which satisfies the following:\n(i) Gij(t) = Gji(t) because of the triple of equalities( pji, lji(t), uji(t) ) = ( 1− pij , 1− uij(t), 1− lij(t) ) .\n(ii) Gii(t) always holds, since (pii, lii(t), uii(t)) =( 1 2 , 1 2 , 1 2 ) . Together with (i), this means that we only\nneed to consider Gij(t) for i < j.\n(iii) Define τ ijn to be the iteration at which arms i and j were compared against each other for the nth time. If Gij ( τ ijn + 1 ) holds, then the events Gij(t) hold for\nall t ∈ ( τ ijn , τ ij n+1 ] because when t ∈ ( τ ijn , τ ij n+1 ] , wij and wji remain constant and so in the expressions for uij(t) and uji(t) only the ln t changes, which is a monotonically increasing function of t. So, we have\nlij(t) ≤ lij(τ ijn + 1) ≤ pij ≤ uij(τ ijn + 1) ≤ uij(t).\nMoreover, the same statement holds with τ ijn re-\nplaced by any T ∈ ( τ ijn , τ ij n+1 ] , i.e., if we know that Gij(T ) holds, then Gij(t) also holds for all t ∈ ( T, τ ijn+1 ] . This is illustrated in Figure 6.\nNow, given the above three facts, we have for any T\nP ( ∀ t ≥ T, i, j, Gij(t) ) (8)\n= P ( ∀ i > j, Gij(T ) and ∀n s.t. τ ijn > T, Gij(τ ijn ) ) .\nLet us now flip things around and look at the complement of these events, i.e. the “bad” event Bij(t) that pij /∈ [lij(t), uij(t)] occurs. Then, subtracting both sides of Equation (8) from 1 and using the union bound gives P ( ∃ t > T, i, j s.t. Bij(t) )\n≤ ∑\ni<j\n[ P ( Bij(T ) ) + P ( ∃n : τ ijn > T and Bij(τ ijn ) )] .\nFurther decomposing the righthand side using union bounds and making the condition explicit, we get P ( ∃ t > T, i, j s.t. Bij(t) )\n≤ ∑\ni>j\n[ P (∣∣∣pij − µijNij(T ) ∣∣∣ > √ α lnT\nNij(T )\n) +\nP  ∃n ≤ T s.t. τ ijn > T and ∣∣pij − µijn ∣∣ > √ α ln τ ijn n  \n+ P  ∃n > T s.t. ∣∣pij − µijn ∣∣ > √ α ln τ ijn n   ] ,\nsince T < n < τ ijn . Here, µ ij n :=\nwij(τ ij n )\nwij(τ ij n )+wji(τ ij n )\nis the\nfrequentist estimate of pij after n comparisons between arms ai and aj .\nNow, in the above sum, we can upper-bound the first term by looking at the higher probability event that Bij(T ) happens for any possible number of comparisons between ai and aj , and since we know that\nNij(T ) ≤ T , we can replace Nij(T ) with a variable n that can take values between 0 and T . For the second term, we know that τ ijn > T , so we can replace τ ijn with T and remove the condition τ ij n > T and look at all n ≤ T . For the third term, since we always have that n < τ ijn , we can replace τ ij n with n and get a higher probability event. Putting all of this together we get the looser bound P ( ∃ t > T, i, j s.t. Bij(t) )\n≤ ∑\ni<j\n[ P ( ∃n ∈ {0, . . . , T} : ∣∣pij − µijn ∣∣ > √ α lnT\nn\n)\n+ P ( ∃n ∈ {0, . . . , T} : ∣∣pij − µijn ∣∣ > √ α lnT\nn\n)\n+ P ( ∃n > T s.t. ∣∣pij − µijn ∣∣ > √ α lnn\nn\n)]\n≤ ∑\ni<j\n[ 2 T∑\nn=0\nP ( ∣∣pij − µijn ∣∣ > √ α lnT\nn\n)\n+\n∞∑\nn=T+1\nP ( ∣∣pij − µijn ∣∣ > √ α lnn\nn\n)] . (9)\nTo bound the expression on line (9), we apply the Chernoff-Hoeffding bound, which in its simplest form states that given i.i.d. random variables X1, . . . , Xn, whose support is contained in [0, 1] and whose expectation satisfies E[Xk] = p, and defining µn := X1+···+Xnn , we have P (|µn − p| > a) ≤ 2e−2na 2 . This gives us P ( ∃ t > T, i, j s.t. Bij(t) )\n≤ ∑\ni<j\n 2 T∑\nn=1\n2e −2 n\nα lnT\nn +\n∞∑\nn=T+1\n2e −2 n\nα lnn\nn  \n= K(K − 1)\n2\n[ T∑\nn=1\n4\nT 2α +\n∞∑\nn=T+1\n2\nn2α\n]\n≤ 2K 2\nT 2α−1 +K2\n∫ ∞\nT\ndx\nx2α , since\n1\nx2α is decreasing.\n≤ 2K 2\nT 2α−1 +K2\n∫ ∞\nT\ndx\nx2α =\n2K2\nT 2α−1 +\nK2 (1− 2α)x2α−1 ∣∣∣∣ ∞\nT\n= (4α− 1)K2\n(2α− 1)T 2α−1 . (10)\nNow, since C(δ) = (\n(4α−1)K2 (2α−1)δ\n) 1 2α−1\nfor each δ > 0,\nthe bound in (10) gives us (7)."
    } ],
    "references" : [ {
      "title" : "Analysis of thompson sampling for the multi-armed bandit problem",
      "author" : [ "S. Agrawal", "N. Goyal" ],
      "venue" : "In Conference on Learning Theory, pp",
      "citeRegEx" : "Agrawal and Goyal,? \\Q2012\\E",
      "shortCiteRegEx" : "Agrawal and Goyal",
      "year" : 2012
    }, {
      "title" : "Exploration-exploitation tradeoff using variance estimates in multi-armed bandits",
      "author" : [ "Audibert", "J.-Y", "R. Munos", "C. Szepesvári" ],
      "venue" : null,
      "citeRegEx" : "Audibert et al\\.,? \\Q1876\\E",
      "shortCiteRegEx" : "Audibert et al\\.",
      "year" : 1876
    }, {
      "title" : "Finite-time analysis of the multiarmed bandit problem",
      "author" : [ "P. Auer", "N. Cesa-Bianchi", "P. Fischer" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Auer et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "Pure exploration in multi-armed bandits problems",
      "author" : [ "S. Bubeck", "R. Munos", "G. Stoltz" ],
      "venue" : "In Algorithmic Learning Theory,",
      "citeRegEx" : "Bubeck et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bubeck et al\\.",
      "year" : 2009
    }, {
      "title" : "Kullback-Leibler upper confidence bounds for optimal sequential allocation",
      "author" : [ "O. Cappé", "A. Garivier", "Maillard", "O.-A", "R. Munos", "G. Stoltz" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "Cappé et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Cappé et al\\.",
      "year" : 2013
    }, {
      "title" : "Prediction, Learning, and Games",
      "author" : [ "N. Cesa-Bianchi", "G. Lugosi" ],
      "venue" : null,
      "citeRegEx" : "Cesa.Bianchi and Lugosi,? \\Q2006\\E",
      "shortCiteRegEx" : "Cesa.Bianchi and Lugosi",
      "year" : 2006
    }, {
      "title" : "An experimental comparison of click position-bias models",
      "author" : [ "N. Craswell", "O. Zoeter", "M. Taylor", "B. Ramsey" ],
      "venue" : "In WSDM",
      "citeRegEx" : "Craswell et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Craswell et al\\.",
      "year" : 2008
    }, {
      "title" : "Exponential regret bounds for Gaussian process bandits with deterministic observations",
      "author" : [ "N. de Freitas", "A. Smola", "M. Zoghi" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Freitas et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Freitas et al\\.",
      "year" : 2012
    }, {
      "title" : "Towards preference-based reinforcement learning",
      "author" : [ "J. Fürnkranz", "E. Hüllermeier", "W. Cheng", "S.H. Park" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Fürnkranz et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Fürnkranz et al\\.",
      "year" : 2012
    }, {
      "title" : "Efficient multiple-click models in web search",
      "author" : [ "F. Guo", "C. Liu", "Y. Wang" ],
      "venue" : "In WSDM",
      "citeRegEx" : "Guo et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2009
    }, {
      "title" : "A probabilistic method for inferring preferences from clicks",
      "author" : [ "K. Hofmann", "S. Whiteson", "M. de Rijke" ],
      "venue" : "In CIKM",
      "citeRegEx" : "Hofmann et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Hofmann et al\\.",
      "year" : 2011
    }, {
      "title" : "Optimizing search engines using clickthrough data",
      "author" : [ "T. Joachims" ],
      "venue" : "In KDD",
      "citeRegEx" : "Joachims,? \\Q2002\\E",
      "shortCiteRegEx" : "Joachims",
      "year" : 2002
    }, {
      "title" : "Asymptotically efficient adaptive allocation rules",
      "author" : [ "T.L. Lai", "H. Robbins" ],
      "venue" : "Advances in Applied Mathematics,",
      "citeRegEx" : "Lai and Robbins,? \\Q1985\\E",
      "shortCiteRegEx" : "Lai and Robbins",
      "year" : 1985
    }, {
      "title" : "Letor: Benchmark dataset for research on learning to rank for information retrieval",
      "author" : [ "Liu", "T.-Y", "J. Xu", "T. Qin", "W. Xiong", "H. Li" ],
      "venue" : "In LR4IR ’07,",
      "citeRegEx" : "Liu et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2007
    }, {
      "title" : "Introduction to Information Retrieval",
      "author" : [ "C. Manning", "P. Raghavan", "H. Schütze" ],
      "venue" : null,
      "citeRegEx" : "Manning et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Manning et al\\.",
      "year" : 2008
    }, {
      "title" : "Optimistic optimization of a deterministic function without the knowledge of its smoothness",
      "author" : [ "R. Munos" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Munos,? \\Q2011\\E",
      "shortCiteRegEx" : "Munos",
      "year" : 2011
    }, {
      "title" : "How does clickthrough data reflect retrieval quality",
      "author" : [ "F. Radlinski", "M. Kurup", "T. Joachims" ],
      "venue" : "In CIKM",
      "citeRegEx" : "Radlinski et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Radlinski et al\\.",
      "year" : 2008
    }, {
      "title" : "Gaussian process optimization in the bandit setting: No regret and experimental design",
      "author" : [ "N. Srinivas", "A. Krause", "S.M. Kakade", "M. Seeger" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Srinivas et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Srinivas et al\\.",
      "year" : 2010
    }, {
      "title" : "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples",
      "author" : [ "W.R. Thompson" ],
      "venue" : null,
      "citeRegEx" : "Thompson,? \\Q1933\\E",
      "shortCiteRegEx" : "Thompson",
      "year" : 1933
    }, {
      "title" : "Generic exploration and k-armed voting bandits",
      "author" : [ "T. Urvoy", "F. Clerot", "R. Féraud", "S. Naamane" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Urvoy et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Urvoy et al\\.",
      "year" : 2013
    }, {
      "title" : "Stochastic simultaneous optimistic optimization",
      "author" : [ "M. Valko", "A. Carpentier", "R. Munos" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Valko et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Valko et al\\.",
      "year" : 2013
    }, {
      "title" : "Interactively optimizing information retrieval systems as a dueling bandits problem",
      "author" : [ "Y. Yue", "T. Joachims" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Yue and Joachims,? \\Q2009\\E",
      "shortCiteRegEx" : "Yue and Joachims",
      "year" : 2009
    }, {
      "title" : "Beat the mean bandit",
      "author" : [ "Y. Yue", "T. Joachims" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Yue and Joachims,? \\Q2011\\E",
      "shortCiteRegEx" : "Yue and Joachims",
      "year" : 2011
    }, {
      "title" : "The K-armed dueling bandits problem",
      "author" : [ "Y. Yue", "J. Broder", "R. Kleinberg", "T. Joachims" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Yue et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Yue et al\\.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 23,
      "context" : "In this paper, we propose and analyze a new algorithm, called Relative Upper Confidence Bound (RUCB), for the K-armed dueling bandit problem (Yue et al., 2012), a variation on the K-armed bandit problem, where the feedback comes in the form of pairwise preferences.",
      "startOffset" : 141,
      "endOffset" : 159
    }, {
      "referenceID" : 11,
      "context" : "We assess the performance of this algorithm using one of the main current applications of the K-armed dueling bandit problem, ranker evaluation (Hofmann et al., 2013; Joachims, 2002; Yue & Joachims, 2011), which is used in information retrieval, ad placement and recommender systems, among others.",
      "startOffset" : 144,
      "endOffset" : 204
    }, {
      "referenceID" : 8,
      "context" : "The K-armed dueling bandit problem is part of the general framework of preference learning (Fürnkranz & Hüllermeier, 2010; Fürnkranz et al., 2012), where the goal is to learn, not from real-valued feedback, but from relative feedback, which specifies only which of two alternatives is preferred.",
      "startOffset" : 91,
      "endOffset" : 146
    }, {
      "referenceID" : 8,
      "context" : ", because it is provided by a human) and specifying real-valued feedback instead would be arbitrary or inefficient (Fürnkranz et al., 2012).",
      "startOffset" : 115,
      "endOffset" : 139
    }, {
      "referenceID" : 23,
      "context" : "Other algorithms proposed for this problem are Interleaved Filter (IF) (Yue et al., 2012), Beat the Mean (BTM) (Yue & Joachims, 2011), and SAVAGE (Urvoy et al.",
      "startOffset" : 71,
      "endOffset" : 89
    }, {
      "referenceID" : 19,
      "context" : ", 2012), Beat the Mean (BTM) (Yue & Joachims, 2011), and SAVAGE (Urvoy et al., 2013).",
      "startOffset" : 64,
      "endOffset" : 84
    }, {
      "referenceID" : 2,
      "context" : "comes, and (1) use these estimates to select a potential champion, which is an arm that has a chance of being the best arm, and (2) select an arm to compare to this potential champion by performing regular Upper Confidence Bound (Auer et al., 2002) relative to it.",
      "startOffset" : 229,
      "endOffset" : 248
    }, {
      "referenceID" : 23,
      "context" : "The K-armed dueling bandit problem (Yue et al., 2012) is a modification of the K-armed bandit problem (Auer et al.",
      "startOffset" : 35,
      "endOffset" : 53
    }, {
      "referenceID" : 2,
      "context" : ", 2012) is a modification of the K-armed bandit problem (Auer et al., 2002): the latter considers K arms {a1, .",
      "startOffset" : 56,
      "endOffset" : 75
    }, {
      "referenceID" : 19,
      "context" : "In this paper, we assume that there exists a Condorcet winner (Urvoy et al., 2013): an arm, which without loss of generality we label a1, such that p1i > 1 2 for all i > 1.",
      "startOffset" : 62,
      "endOffset" : 82
    }, {
      "referenceID" : 23,
      "context" : "Given a Condorcet winner, we define regret for each time-step as follows (Yue et al., 2012): if arms ai and aj were chosen for comparison at time t, then regret at that time is set to be rt := ∆1i+∆1j 2 , with ∆k := p1k − 12 for all k ∈ {1, .",
      "startOffset" : 73,
      "endOffset" : 91
    }, {
      "referenceID" : 19,
      "context" : "The Condorcet winner is different in a subtle but important way from the Borda winner (Urvoy et al., 2013), which is an arm ab that satisfies ∑ j pbj ≥ ∑ j pij , for all i = 1, .",
      "startOffset" : 86,
      "endOffset" : 106
    }, {
      "referenceID" : 19,
      "context" : "In this setting, the algorithm can be assessed on its accuracy, the probability that a given run of the algorithm reports the Condorcet winner as the best arm (Urvoy et al., 2013), which is related to expected simple regret : the regret associated with the algorithm’s choice of the best arm, i.",
      "startOffset" : 159,
      "endOffset" : 179
    }, {
      "referenceID" : 3,
      "context" : ", rT+1 (Bubeck et al., 2009).",
      "startOffset" : 7,
      "endOffset" : 28
    }, {
      "referenceID" : 23,
      "context" : "mulated by the explore-then-exploit problem formulation (Yue et al., 2012).",
      "startOffset" : 56,
      "endOffset" : 74
    }, {
      "referenceID" : 23,
      "context" : "The first method for the K-armed dueling bandit problem is interleaved filter (IF) (Yue et al., 2012), which was designed for a finite-horizon scenario and which proceeds by picking a reference arm to compare against the rest and using it to eliminate other arms, until the reference arm is eliminated by a better arm, in which case the latter becomes the reference arm and the algorithm continues as before.",
      "startOffset" : 83,
      "endOffset" : 101
    }, {
      "referenceID" : 23,
      "context" : "Under these conditions, theoretical results have been proven for IF and BTM in (Yue et al., 2012) and (Yue & Joachims, 2011).",
      "startOffset" : 79,
      "endOffset" : 97
    }, {
      "referenceID" : 19,
      "context" : "Sensitivity Analysis of VAriables for Generic Exploration (SAVAGE) (Urvoy et al., 2013) is a recently proposed algorithm that outperforms both IF and BTM by a wide margin when the number of arms is of moderate size.",
      "startOffset" : 67,
      "endOffset" : 87
    }, {
      "referenceID" : 19,
      "context" : "Moreover, one version of SAVAGE, called Condorcet SAVAGE, makes the Condorcet assumption and performed the best experimentally (Urvoy et al., 2013).",
      "startOffset" : 127,
      "endOffset" : 147
    }, {
      "referenceID" : 2,
      "context" : "Note that RUCB uses the upperconfidence bounds (Line 3 of Algorithm 1) introduced in the original version of UCB (Auer et al., 2002) (up to the α factor).",
      "startOffset" : 113,
      "endOffset" : 132
    }, {
      "referenceID" : 4,
      "context" : ", 2009) or KL-UCB (Cappé et al., 2013)) have improved performance for the regular K-armed bandit problem.",
      "startOffset" : 18,
      "endOffset" : 38
    }, {
      "referenceID" : 14,
      "context" : "To evaluate RUCB, we apply it to the problem of ranker evaluation from the field of information retrieval (IR) (Manning et al., 2008).",
      "startOffset" : 111,
      "endOffset" : 133
    }, {
      "referenceID" : 16,
      "context" : "One effective way to achieve this is to use interleaved comparisons (Radlinski et al., 2008), which interleave the documents proposed by two different rankers and presents the resulting list to the user, whose resulting click feedback is used to infer a noisy preference for one of the rankers.",
      "startOffset" : 68,
      "endOffset" : 92
    }, {
      "referenceID" : 13,
      "context" : "Our experimental setup is built on real IR data, namely the LETOR NP2004 dataset (Liu et al., 2007).",
      "startOffset" : 81,
      "endOffset" : 99
    }, {
      "referenceID" : 10,
      "context" : "To compare a pair of rankers, we use probabilistic interleave (PI) (Hofmann et al., 2011), a recently developed method for interleaved comparisons.",
      "startOffset" : 67,
      "endOffset" : 89
    }, {
      "referenceID" : 6,
      "context" : "To model the user’s click behavior on the resulting interleaved lists, we employ a probabilistic user model (Craswell et al., 2008; Hofmann et al., 2011) that uses as input the manual labels (classifying documents as relevant or not for given queries) provided with the LETOR NP2004 dataset.",
      "startOffset" : 108,
      "endOffset" : 153
    }, {
      "referenceID" : 10,
      "context" : "To model the user’s click behavior on the resulting interleaved lists, we employ a probabilistic user model (Craswell et al., 2008; Hofmann et al., 2011) that uses as input the manual labels (classifying documents as relevant or not for given queries) provided with the LETOR NP2004 dataset.",
      "startOffset" : 108,
      "endOffset" : 153
    }, {
      "referenceID" : 19,
      "context" : "We did not include an evaluation of IF, since both BTM and Condocet SAVAGE were shown to outperform it (Urvoy et al., 2013; Yue & Joachims, 2011).",
      "startOffset" : 103,
      "endOffset" : 145
    }, {
      "referenceID" : 15,
      "context" : "First, building off extensions of UCB to the continuous bandit setting (Bubeck et al., 2011; de Freitas et al., 2012; Munos, 2011; Srinivas et al., 2010; Valko et al., 2013), we aim to extend RUCB to the continuous dueling bandit setting, without a convexity assumption as in (Yue & Joachims, 2009).",
      "startOffset" : 71,
      "endOffset" : 173
    }, {
      "referenceID" : 17,
      "context" : "First, building off extensions of UCB to the continuous bandit setting (Bubeck et al., 2011; de Freitas et al., 2012; Munos, 2011; Srinivas et al., 2010; Valko et al., 2013), we aim to extend RUCB to the continuous dueling bandit setting, without a convexity assumption as in (Yue & Joachims, 2009).",
      "startOffset" : 71,
      "endOffset" : 173
    }, {
      "referenceID" : 20,
      "context" : "First, building off extensions of UCB to the continuous bandit setting (Bubeck et al., 2011; de Freitas et al., 2012; Munos, 2011; Srinivas et al., 2010; Valko et al., 2013), we aim to extend RUCB to the continuous dueling bandit setting, without a convexity assumption as in (Yue & Joachims, 2009).",
      "startOffset" : 71,
      "endOffset" : 173
    }, {
      "referenceID" : 18,
      "context" : "Second, building off Thompson Sampling (Agrawal & Goyal, 2012; Kauffmann et al., 2012; Thompson, 1933), an elegant and effective sampling-based alternative to UCB, we will investigate whether a sampling-based extension to RUCB would be amenable to theoretical analysis.",
      "startOffset" : 39,
      "endOffset" : 102
    } ],
    "year" : 2017,
    "abstractText" : "This paper proposes a new method for the K-armed dueling bandit problem, a variation on the regular K-armed bandit problem that offers only relative feedback about pairs of arms. Our approach extends the Upper Confidence Bound algorithm to the relative setting by using estimates of the pairwise probabilities to select a promising arm and applying Upper Confidence Bound with the winner as a benchmark. We prove a finite-time regret bound of order O(log t). In addition, our empirical results using real data from an information retrieval application show that it greatly outperforms the state of the art.",
    "creator" : "LaTeX with hyperref package"
  }
}