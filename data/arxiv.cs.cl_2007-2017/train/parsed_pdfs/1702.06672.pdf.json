{
  "name" : "1702.06672.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Calculating Probabilities Simplifies Word Learning",
    "authors" : [ "Aida Nematzadeh", "Barend Beekhuizen", "Shanshan Huang32AndSuzanne" ],
    "emails" : [ "1nematzadeh@berkeley.edu", "2barend@cs.toronto.edu", "3sunny.huang@mail.utoronto.ca", "4suzanne@cs.toronto.edu" ],
    "sections" : [ {
      "heading" : "Introduction",
      "text" : "How do people acquire the meanings of words as they begin to learn a language? A well-supported proposal is crosssituational learning (e.g., Pinker, 1989), which suggests that people are sensitive to the regularities that repeat in different situations, and use such evidence to identify the commonalities, from which they can infer word meanings. As an example, when a child hears what a cute kitty, be nice to the kitty, etc., she/he could infer that the word kitty refers to the common referent in all these situations, i.e., a cat. Recent word learning experiments confirm that both adults and infants keep track of cross-situational statistics across learning trials, and infer the correct word–meaning mappings even in highly ambiguous conditions (e.g., Yu & Smith, 2007; Smith & Yu, 2008; Yurovsky, Fricker, Yu, & Smith, 2014).\nDespite empirical evidence for statistical cross-situational learning, the exact mechanisms in play are still not fully understood. In this paper, we focus on the first step of a crosssituational framework – the learning that occurs on each observation of a word, which we call in-the-moment learning. Given the words in an utterance and their potential meanings in the accompanying situation, there are many possible ways to associate words and meanings, but only some of these associations are correct. We refer to these in-the-moment as-\n1nematzadeh@berkeley.edu 2barend@cs.toronto.edu 3sunny.huang@mail.utoronto.ca 4suzanne@cs.toronto.edu\nsociations of words and meanings as alignments, and consider different strategies for assessing the strength of these alignments, drawing on the evolving knowledge of word meanings. We note that previous research has considered “hard” (or binary) in-the-moment learning strategies, where an alignment is either considered by the learner or not (e.g., Trueswell, Medina, Hafri, & Gleitman, 2013); we instead examine “soft” strategies where alignments have strengths between zero and one.\nWe formulate various in-the-moment learning mechanisms that introduce different kinds of competition – i.e., the way in which the strength of a word–meaning alignment depends on and interacts with other possible alignments. Each mechanism corresponds to certain statistics of the word learning input, such as the weighted frequency of word–meaning pairs or their conditional probabilities. We show that the different types of competition during the in-the-moment learning lead to various kinds of mutual exclusivity behaviours. Mutual exclusivity has been proposed as an explicit bias, in which children assume each word has a single meaning (e.g., Markman, 1987; Markman & Wachtel, 1988). Here, mutual exclusivity of words and/or meanings arises from competition in a way that focuses learning.\nWe take a computational modeling approach to investigate the effectiveness of these mechanisms in overall acquisition of word meanings in various long-term word learning scenarios. Using a computational model enables us to explore the impact of different learning mechanisms in a variety of conditions, and to examine the role of one factor (e.g., frequency) while controlling for another one (e.g., utterance length). We find that the mechanism that maximizes the use of the accumulated knowledge of learned meanings performs the best. Interestingly, the performance gap between this mechanism and others is most significant in more difficult learning conditions, such as learning of low frequency words given long utterances. This shows that using conditional probabilities (as opposed to counts) and introducing competition (leading to a mutual exclusivity bias) improves overall word learning and might be necessary to guide learning in the presence of ambiguity or little data.\nar X\niv :1\n70 2.\n06 67\n2v 1\n[ cs\n.C L\n] 2\n2 Fe\nb 20\n17"
    }, {
      "heading" : "A Cross-situational Word Learning Framework",
      "text" : "There has been an increased interest in the last decade in developing computational models as tools to study word learning in people. Of particular interest are cross-situational learners that are incremental (e.g., Siskind, 1996; Fazly, Alishahi, & Stevenson, 2010; Kachergis, Yu, & Shiffrin, 2012), which is necessary in studying developmental learning patterns. Notably, the model of Fazly et al. (2008; 2010) (henceforth FAS) is the first probabilistic model that robustly predicts a range of observed behavior in child word learning. Moreover, this model has been adopted and extended by a series of successive work (e.g., Nematzadeh, Fazly, & Stevenson, 2012a; Grant, Nematzadeh, & Stevenson, 2016), demonstrating its robustness in accounting for empirical data. We adopt the FAS word learning framework to examine various in-the-moment learning mechanisms."
    }, {
      "heading" : "The FAS Model",
      "text" : "Word learning input and output. The model’s input is a sequence of utterance–scene pairs simulating what the child hears and perceives, respectively. Each utterance is a set of words (ignoring their order), and the corresponding scene is a set of semantic features that represents possible meanings of words in the utterance. Word meanings are represented by multiple features, which exposes the model to naturalistic commonalities among the words.\nUtterance: { Joel, eats } Scene: { PERSON, JOEL, ACT, CONSUME, ... } (1)\nThe output of the model, at each step in learning, is the current representation of the meaning of each word w as a probability distribution, p(·|w), over all possible semantic features f that the model has observed in the input scenes. The word learning problem. Given a corpus of utterance– scene pairs, the goal of the model is to learn the meaning probability distribution, p(·|w), for all words w. Prior to receiving any input, all features f are equally likely for a word. As the model processes each input pair, the probability is adjusted to reflect the cross-situational evidence in the corpus, in two steps: (a) in-the-moment learning on this input pair and (b) update of the word meaning probabilities using the accumulated evidence over all inputs. In-the-moment learning. Given an utterance and a scene, which features in the scene are part of a word’s meaning? There are different possible ways to determine whether a semantic feature is associated with a word in the input pair, and the corresponding strength of that association. FAS assumes that each feature f in scene St at time t, independently of the other features, is aligned to all the words w in the utterance Ut with a particular strength (see Figure 1a):\nat(w|f ) = pt(f |w)\n∑ w′∈Ut\npt(f |w′) (2)\nThe alignment strength between a feature f and word w depends on the current probability that f is part of the meaning\nof w – i.e., pt( f |w) – as well as the probabilities that f is part of the meaning of other words in the utterance (the denominator above).\nIn this way, Eqn. (2) has words in the utterance “compete” to be associated with a given feature: a higher alignment strength of one word with a feature necessarily results in a lower alignment strength for other words with that feature. This can be interpreted as a directional mutual exclusivity bias: the alignment formulation limits the number of words a feature can be strongly associated with, but does not directly limit the number of features a word can be associated with. Updating the word meanings. How is the information learned from an input pair incorporated into a learner’s longterm knowledge of word meanings? The learner incrementally accumulates the alignment strengths between each w and f in an overall association score assoc(w, f ), which is updated at each time t that w and f co-occur in an input pair:\nassoct(w, f ) = assoct−1(w, f )+at( f |w) (3)\nwhere assoct−1(w, f ) = 0 if w and f have not co-occurred prior to t.\nAfter updating the association scores, the meaning probability p(·|w) of each word w in the current input is adjusted using a smoothed version of this formula:\npt+1(f |w) = assoct(f , w)\n∑ fj∈M\nassoct(fj, w) (4)\nwhere M is the set of all features observed up to time t. In Eqn. (4), the probability of a feature given a word is a normalization of their association score over all possible features, which introduces another source of competition, this time, among features for a given word. This competition can be thought of as a mutual exclusivity bias in the reversed direction of the alignment score in Eqn. (2); here a word can only be strongly associated to a limited number of features."
    }, {
      "heading" : "Grouping Sets of Features into Referents",
      "text" : "In FAS, an input scene is the set union of all meaning features for all words in the corresponding utterance. This representation lacks information that would be apparent to a child concerning how groups of features belong to a single entity or event – e.g., PERSON and JOEL, or ACT and CONSUME in Ex. 1. However, replacing the sets of features with a single symbol corresponding to the meaning would prevent the model from learning semantic similarities among the words (e.g., Nematzadeh, Fazly, & Stevenson, 2012b). Instead, following Alishahi, Fazly, Koehne, and Crocker (2012), we group the semantic features in a scene into referents that correspond to something referred to by a word in the utterance, as in Ex. 5:5\n5We use the term referent to denote anything referred to by a word – an object or event, or set of semantic properties (e.g., { INDEFINITE, SINGULAR } for an).\nUtterance { Joel, eats, an, apple } Scene: { {PERSON, JOEL}, {ACT, CONSUME, ...}, {SINGULAR, INDEFINITE, DETERMINER, ... }, {APPLE, FRUIT, FOOD, ...} }\n(5)\nA scene is now a set of referents, each of which is a set of semantic features. In the FAS model, calculation of alignment strength between a word w and feature f at time t uses the meaning probability pt( f |w). Now, aligning words with referents (as in 5) requires consideration of strength of alignment of a word with a set of features. In calculating alignment strength for a word w and a referent r at time t, we change the FAS model to consider sim(vt(w), v(r)), the similarity between the word’s current meaning representation and the representation of the referent, where v(r) and vt(w) are vectors in which the elements are meaning features. For vt(w), the value for each component feature f is pt( f |w). (I.e, vt(w) is a vector corresponding to pt(·|w).) For v(r), the element values are 1 for features present in the definition of r and 0 otherwise. In this way, alignment strength for word w and referent r is influenced by the strength of the meaning probabilities p( f |w) for all features f that are part of the representation of r. In the remainder of the paper, we explore variations in how the alignment process actually does this, in ways that implement different types of mutual exclusivity biases."
    }, {
      "heading" : "In-the-Moment Learning Mechanisms",
      "text" : "Competition in the model. We observed above that the alignment strength calculation in Eqn. (2) instantiates a form of mutual exclusivity bias, because words are competing to be strongly associated with a feature during this in-the-moment learning process. With the change of aligning words to referents instead of to features, we have the opportunity to explore various ways to formulate competition in determining the strength of alignments. The three alignment formulations explored here implement (1) no competition among words or referents, (2) competition of referents for a word (as in Alishahi et al., 2012), and (3) competition of words for a referent (analogous to the competition of words for a feature in FAS). Each of these ways of viewing competition implements a different approach to mutual exclusivity in the model, and we will explore the resulting impact on word learning in the results. No competition. The no-competition mechanism (henceforth, no-comp) serves as a baseline for comparison to the other two. It assumes no mutual exclusivity bias – all the alignments between words and referents are calculated independently, and the value of one alignment does not effect any of the others (see Figure 1b). We formulate such an alignment between a word w and a referent r as simply the similarity between vt(w) and v(r) as described above:\nat(w, r) = sim(vt(w), v(r)) (6)\nThis formulation can be seen as a simple weighted count, where each feature relevant to r (valued 1 in v(r)) contributes\nto the overall alignment strength proportionally to the model’s prior knowledge of its meaning probability with that word. Referent competition. Here we adopt the alignment formulation of Alishahi et al. (2012), which we call “ref-comp” because referents compete for alignment with a word. This mechanism implements a directed mutual exclusivity bias in which each word has a preference to be strongly associated with one referent. In other words, referents in the scene compete for a given word, while the alignments of words are independent of each other (see Figure 1c). This preference can be implemented by normalizing the sim(vt(w), v(r)) over all the referents in the scene:\nat(r|w) = sim(vt(w), v(r))\n∑ r′∈St\nsim(vt(w), v(r′)) (7)\nBy normalizing the weighted count of sim(vt(w), v(r)), this alignment formulation can be interpreted as the conditional probability of r given w, rather than a simple count. Word competition. Here, we consider a competition that is instead analogous to the competition of words for a feature in FAS; “word-comp” is the reverse of ref-comp, because here words compete for a referent. This leads to a directed mutual exclusivity bias, but in the opposite direction to ref-comp. The word-comp mechanism asserts a preference for each referent to be strongly associated with a single word, by having words compete for a referent, while the alignments of referents are independent of each other (see Figure 1d). This bias is formulated by normalizing the sim(vt(w), v(r)) over the words in the utterance (as FAS did):\nat(w|r) = sim(vt(w), v(r))\n∑ w′∈Ut\nsim(vt(w′), v(r)) (8)\nThis formulation also yields a conditional probability, but here of w given r. The association score. We note one final change to the FAS model to deal with referents: We must modify Eqn. (3) to keep track of associations between a word w and all the features of a referent r. Since a feature f can occur in more than one referent in scene S, which can have multiple alignment scores, we use the maximum alignment score of a referent that contains the feature in updating the feature’s association score:\nassoct(w, f ) = assoct−1(w, f )+ max r′∈S: f∈r′\nat(w, r′) (9)\nThe meaning probabilities in the model continue to be calculated between individual features and a word. Recall that the meaning probability distribution p(·|w), as a conditional probability over semantic features, enforces a competition among them for the probability mass.\nExperiments"
    }, {
      "heading" : "Set-up",
      "text" : "The utterances in the input are child-directed speech taken from the Manchester corpus (Theakston, Lieven, Pine, &\nRowland, 2001) in CHILDES (MacWhinney, 2000). To create the associated scene representations, each word in the corpus is entered into a gold-standard lexicon with a set of semantic features representing its gold-standard meaning, following the procedure of Fazly et al. (2008). The referents shown in Ex. 1 correspond to the gold-standard meanings of each of those words. (The word–mapping in the lexicon is only used to generate scenes, and is not seen by the model.) The model is trained on 20K utterance–scene pairs, at which point behaviour is stable.\nIn the following experiments, we examine the quality of the individual learned word representations in two ways: the average acquisition score of all words observed by the model, and the proportion of observed words that is learned. The acquisition score of each word w is obtained by comparing the word meaning representation v(w) with a gold standard representation of the word gold(w) using cosine similarity:\nacq(w) = sim(v(w),gold(w)) (10)\nwhere gold(w) is a vector over all semantic features, with value 1 for features part of the gold-standard meaning of w and 0 otherwise. An observed word counts as “learned” if its acq score is higher than some threshold θ, here set to 0.7."
    }, {
      "heading" : "Results",
      "text" : ""
    }, {
      "heading" : "Overall Learning Patterns",
      "text" : "Over time, all models converge to high average acq scores (Figure 2a) and proportions of words learned (Figure 2b), but with substantial differences between them. Notably, we find\nthat on the average acq score, the word-comp formulation performs better than the original FAS (.96 vs. .86), while the ref-comp and no-comp models do not learn the representations as well (both .83).\nTwo factors may underlie the varying performance of the models: the semantic grouping of features into referents (distinguishing our models from FAS), and the type of in-themoment competition (and resulting type of mutual exclusivity). For the first factor, the word-comp mechanism provides the most direct comparison to FAS: it uses the same direction of bias – in which words compete to align with the elements of the scene – but using referents instead of features. The grouping into referents appears to improve learning. When aligning features individually as in FAS, the correct features for a word may be aligned more or less strongly (depending on competition for each from other words), so that the overall meaning probability vector may not converge as easily to the full set of correct features. By contrast, when a word has a strong alignment with the correct referent – which corresponds to the gold-standard meaning of the word – all features of the referent are boosted in the meaning probability of the word, yielding improved learning in word-comp over FAS.\nSecond, we find an interesting asymmetry between the two mechanisms involving competition between the words for a referent (word-comp) and between the referents for a word (ref-comp). Each imposes a conditional probability formulation of competition, but word-comp performs much better, with ref-comp behaving no better than the no-comp model. In fact, the advantage of using referents instead of individual features is completely eliminated in both the no-comp and the ref-comp mechanisms, as both perform worse than FAS. This is especially surprising given that Alishahi et al. (2012) used the ref-comp alignment mechanism in their work that modeled human behaviour in a language learning task. (We return to this point below.)\nThe source of this asymmetry, we believe, is the deployment of learned knowledge by the model. In both the nocomp and the ref-comp model (Figure 1(b), (c)), a learned word meaning is compared to the referents in isolation from the learned meanings of the other words in the utterance. In this set-up, the knowledge of other word meanings cannot help to guide the model to determine how good a word’s alignment to some referent is. By contrast, the word-comp model (Figure 1(d)) tunes the alignments by comparing how\nsimilar various learned word meanings are to a referent. One might expect that mutual exclusivity in the reverse direction (as in the ref-comp model) would achieve the same effects: Tuning the similarity between a word meaning and a referent by the similarity between that word meaning and all other referents should guide the model to correct associations more quickly than not doing so. However, we do not find this effect. We will return to the reason for this lack of effect in the section on the role of frequency.\nCompetition is clearly important in focusing alignments and facilitating learning, but only in the context of appropriately constraining information: the most effective learning occurs when the competition draws on the maximal amount of learned knowledge in the model, in the form of the developing meaning probabilities. In what follows, we consider the impact of increased ambiguity in forming alignments, or decreased knowledge about words, to see how these factors impact these various mechanisms. Because the proportion of words learned shows similar relative behaviours to the acq score, in the remaining analysis we focus on comparing acq scores of each of the models after 20K inputs."
    }, {
      "heading" : "The Role of Frequency",
      "text" : "Children are able to learn word meanings in various conditions, sometimes after only a few observations. Previous research suggests that children use biases such as mutual exclusivity to guide their learning. Learning low-frequency words is also a challenge for computational models, and understanding the mechanisms that improve learning from little evidence can shed light on how children address this issue. The type of competition in the various models under study here plays an important role in their performance on low-frequency words. Figure 3a shows that for the two models with competition over words – the FAS and word-comp models – there is no decrease in performance for words of low frequency (< 5) compared to high frequency (> 10), while for the other two models, no-comp and ref-comp, there is a dramatic drop off in learning.\nSpecifically, the competition among words in the FAS and word-comp models – which maximizes the use of learned knowledge in focusing alignments – appears to play a crucial role in enabling these models to learn low-frequency words. Comparing the alignments in Figure 1c and Figure 1d in the face of a novel word and its novel referent (as an extreme case of low frequency) will clarify the utility of the learned meaning probabilities. In the word-comp model (Figure 1d),\nthe meaning probabilities of previously-seen words competing for a new referent will not have a very good match to the feature vector for the new referent (since their probabilities will have been adjusted to better fit referents they have been seen with). The novel word will have uniform meaning probabilities that will enable it to better match the new referent, and thus will have a stronger alignment than previously-seen words. By contrast, in the ref-comp model (Figure 1c), the uniform probabilities of the new word will equally match all the referents competing for it, whether they have been seen before or not. There is no prior knowledge in the model in this competition that indicates the previously-seen referents have a better fit with other words. Thus a competition among words works well for novel or low-frequency words by drawing on the fact that previously-seen words will not compete as strongly for a new(er) referent. In short: a new word can in principle go equally well with any referent in the situation, but a new referent not equally well with any word in the utterance."
    }, {
      "heading" : "The Role of Utterance Length",
      "text" : "Above, we found that the different types of competition gave more pronounced results for low-frequency words than for high-frequency ones. Similarly, we can explore whether there is a differential impact of utterance length on the different models. To simulate this, we manipulated the input generation procedure so that the model was trained only on utterances of length 5 or higher (long-corpus), or 3 and lower (short-corpus). Looking at Figure 3b, we observe that the acquisition scores are globally lower when the models are trained on long sentences only, likely due to the fact that there is more uncertainty about which words and which referents belong together.\nHere we see that the word-comp model is the only one to not substantially decline in performance when comparing learning on the short-corpus and long-corpus. While the competition over words seems to help the FAS and word-comp models equally in dealing with low-frequency words, here the bundling of features into referents as in word-comp is also necessary for performance to be robust to the added ambiguity of long utterances. The FAS model cannot “scale up” to deal with the very long unstructured lists of features in the long-corpus input. We can also now suggest why the Alishahi et al. (2012) model (the ref-comp approach) worked well in their experiments but not here: the utterances they used all had two words, unlike the naturalistic data we train on above,\nindicating that ref-comp also cannot scale effectively. Interestingly, as shown in Figure 3c, we see that the word-comp model is particularly robust to the challenge of learning lowfrequency words in the corpus of longer utterances, with a very small decrease in performance compared to the other models."
    }, {
      "heading" : "The Role of Referential Uncertainty",
      "text" : "To explore the impact of referential uncertainty – the occurrence of many more potential referents in a scene than there are words – we create a subcorpus that uses every ith utterance from our full corpus, and uses the utterances in between those to generate “extra” referents in the scenes for utterances in the subcorpus. Here we report results on 20K inputs with referents added to each scene Si from 0, 1, or 2 utterances in addition to referents taken from utterance Ui. Figure 4 presents the results for no referential uncertainty, along with the two added levels of uncertainty. As we expect, the learning performance of all models degrades with higher referential uncertainty. However, in contrast to our previous results, here there is little benefit from either word-based competition or feature bundling. The high degree of ambiguity introduced by these levels of referential uncertainty may be better dealt with by attentional mechanisms that focus joint attention on a likely subset of relevant referents prior to alignment."
    }, {
      "heading" : "Conclusions and Future Work",
      "text" : "Previous research shows that children are sensitive to the cross-situational statistics of their environment: i.e., they can use the regularities across different situations to learn word meanings. However, the detailed mechanisms responsible for cross-situational word learning are still not fully understood, such as precisely what information is used from each observation in identifying the correct word meaning, and how this information is incorporated in the accumulated knowledge about a word. Moreover, children are good at learning word meanings in a variety of situations: they can learn a novel word from a few example and also acquire words from ambiguous/noisy conditions. Previous research has suggested that children are equipped with biases that guide them in word learning by reducing the difficulty/ambiguity of a learning situation. The necessity of these biases in children, and whether they are innate or learnable, are issues that have been debated among cognitive scientists.\nHere, we show that one such bias – the mutual exclusivity bias that limits the number of meanings a word takes – can be modeled as a competition mechanism during in-the-\nmoment learning. The competition exists when the model assesses possible word and referent associations with conditional probabilities as opposed to counts. In other words, the bias or competition is a learning mechanism that is able to condition in-the-moment learning to the learned knowledge of word meanings. We observe that the role of the bias is particularly significant when the learning is more challenging: for example, for learning low-frequency words or from longer utterances. Previous research has investigated how cognitive processes such as memory and attention interact with crosssituational word learning (e.g., Nematzadeh et al., 2012a). Future work should study how these cognitive processes affect the in-moment-learning."
    } ],
    "references" : [ {
      "title" : "Sentence-based attentional mechanisms in word learning: evidence from a computational model",
      "author" : [ "A. Alishahi", "A. Fazly", "J. Koehne", "M.W. Crocker" ],
      "venue" : "Frontiers in psychology,",
      "citeRegEx" : "Alishahi et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Alishahi et al\\.",
      "year" : 2012
    }, {
      "title" : "A probabilistic incremental model of word learning in the presence of referential uncertainty",
      "author" : [ "A. Fazly", "A. Alishahi", "S. Stevenson" ],
      "venue" : "In CogSci Proceedings",
      "citeRegEx" : "Fazly et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Fazly et al\\.",
      "year" : 2008
    }, {
      "title" : "A probabilistic computational model of cross-situational word learning",
      "author" : [ "A. Fazly", "A. Alishahi", "S. Stevenson" ],
      "venue" : "Cognitive Science,",
      "citeRegEx" : "Fazly et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Fazly et al\\.",
      "year" : 2010
    }, {
      "title" : "The interaction of memory and attention in novel word generalization: A computational investigation",
      "author" : [ "E. Grant", "A. Nematzadeh", "S. Stevenson" ],
      "venue" : "In Proceedings of the 38th annual conference of the cognitive science society",
      "citeRegEx" : "Grant et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Grant et al\\.",
      "year" : 2016
    }, {
      "title" : "An associative model of adaptive inference for learning word–referent mappings",
      "author" : [ "G. Kachergis", "C. Yu", "R. Shiffrin" ],
      "venue" : "Psychonomic Bulletin and Review,",
      "citeRegEx" : "Kachergis et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Kachergis et al\\.",
      "year" : 2012
    }, {
      "title" : "The CHILDES project: Tools for analyzing talk",
      "author" : [ "B. MacWhinney" ],
      "venue" : "(3rd ed.,",
      "citeRegEx" : "MacWhinney,? \\Q2000\\E",
      "shortCiteRegEx" : "MacWhinney",
      "year" : 2000
    }, {
      "title" : "How children constrain the possible meanings of words. In U. Neisser (Ed.), Concepts and conceptual development: Ecological and intellectual factors in categorization",
      "author" : [ "E.M. Markman" ],
      "venue" : null,
      "citeRegEx" : "Markman,? \\Q1987\\E",
      "shortCiteRegEx" : "Markman",
      "year" : 1987
    }, {
      "title" : "Children’s use of mutual exclusivity to constrain the meanings of words",
      "author" : [ "E.M. Markman", "G.F. Wachtel" ],
      "venue" : "Cognitive Psychology,",
      "citeRegEx" : "Markman and Wachtel,? \\Q1988\\E",
      "shortCiteRegEx" : "Markman and Wachtel",
      "year" : 1988
    }, {
      "title" : "A computational model of memory, attention, and word learning",
      "author" : [ "A. Nematzadeh", "A. Fazly", "S. Stevenson" ],
      "venue" : "In CMCL Proceedings (pp. 80–89)",
      "citeRegEx" : "Nematzadeh et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Nematzadeh et al\\.",
      "year" : 2012
    }, {
      "title" : "Interaction of word learning and semantic category formation in late talking",
      "author" : [ "A. Nematzadeh", "A. Fazly", "S. Stevenson" ],
      "venue" : "In CogSci Proceedings (pp. 2085–2090)",
      "citeRegEx" : "Nematzadeh et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Nematzadeh et al\\.",
      "year" : 2012
    }, {
      "title" : "Learnability and cognition: The acquisition of argument structure",
      "author" : [ "S. Pinker" ],
      "venue" : null,
      "citeRegEx" : "Pinker,? \\Q1989\\E",
      "shortCiteRegEx" : "Pinker",
      "year" : 1989
    }, {
      "title" : "A computational study of crosssituational techniques for learning word-to-meaning",
      "author" : [ "J.M. Siskind" ],
      "venue" : "mappings. Cognition,",
      "citeRegEx" : "Siskind,? \\Q1996\\E",
      "shortCiteRegEx" : "Siskind",
      "year" : 1996
    }, {
      "title" : "Infants rapidly learn wordreferent mappings via cross-situational",
      "author" : [ "L.B. Smith", "C. Yu" ],
      "venue" : "statistics. Cognition,",
      "citeRegEx" : "Smith and Yu,? \\Q2008\\E",
      "shortCiteRegEx" : "Smith and Yu",
      "year" : 2008
    }, {
      "title" : "The role of performance limitations in the acquisition of verb–argument structure: An alternative account",
      "author" : [ "A.L. Theakston", "E.V. Lieven", "J.M. Pine", "C.F. Rowland" ],
      "venue" : "Journal of Child Language,",
      "citeRegEx" : "Theakston et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Theakston et al\\.",
      "year" : 2001
    }, {
      "title" : "Propose but verify: Fast mapping meets crosssituational word learning",
      "author" : [ "J.C. Trueswell", "T.N. Medina", "A. Hafri", "L.R. Gleitman" ],
      "venue" : "Cognitive Psychology,",
      "citeRegEx" : "Trueswell et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Trueswell et al\\.",
      "year" : 2013
    }, {
      "title" : "Rapid word learning under uncertainty via cross-situational statistics",
      "author" : [ "C. Yu", "L.B. Smith" ],
      "venue" : "Psychological Science,",
      "citeRegEx" : "Yu and Smith,? \\Q2007\\E",
      "shortCiteRegEx" : "Yu and Smith",
      "year" : 2007
    }, {
      "title" : "The role of partial knowledge in statistical word learning",
      "author" : [ "D. Yurovsky", "D.C. Fricker", "C. Yu", "L.B. Smith" ],
      "venue" : "Psychonomic Bulletin and Review,",
      "citeRegEx" : "Yurovsky et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Yurovsky et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Here we adopt the alignment formulation of Alishahi et al. (2012), which we call “ref-comp” because referents compete for alignment with a word.",
      "startOffset" : 43,
      "endOffset" : 66
    }, {
      "referenceID" : 5,
      "context" : "Rowland, 2001) in CHILDES (MacWhinney, 2000).",
      "startOffset" : 26,
      "endOffset" : 44
    }, {
      "referenceID" : 1,
      "context" : "To create the associated scene representations, each word in the corpus is entered into a gold-standard lexicon with a set of semantic features representing its gold-standard meaning, following the procedure of Fazly et al. (2008). The referents shown in Ex.",
      "startOffset" : 211,
      "endOffset" : 231
    }, {
      "referenceID" : 0,
      "context" : "This is especially surprising given that Alishahi et al. (2012) used the ref-comp alignment mechanism in their work that modeled human behaviour in a language learning task.",
      "startOffset" : 41,
      "endOffset" : 64
    }, {
      "referenceID" : 0,
      "context" : "We can also now suggest why the Alishahi et al. (2012) model (the ref-comp approach) worked well in their experiments but not here: the utterances they used all had two words, unlike the naturalistic data we train on above,",
      "startOffset" : 32,
      "endOffset" : 55
    } ],
    "year" : 2017,
    "abstractText" : "Children can use the statistical regularities of their environment to learn word meanings, a mechanism known as crosssituational learning. We take a computational approach to investigate how the information present during each observation in a cross-situational framework can affect the overall acquisition of word meanings. We do so by formulating various in-the-moment learning mechanisms that are sensitive to different statistics of the environment, such as counts and conditional probabilities. Each mechanism introduces a unique source of competition or mutual exclusivity bias to the model; the mechanism that maximally uses the model’s knowledge of word meanings performs the best. Moreover, the gap between this mechanism and others is amplified in more challenging learning scenarios, such as learning from few examples.",
    "creator" : "TeX"
  }
}