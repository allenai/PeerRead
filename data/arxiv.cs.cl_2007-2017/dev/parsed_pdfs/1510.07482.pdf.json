{
  "name" : "1510.07482.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Edge-Linear First-Order Dependency Parsing with Undirected Minimum Spanning Tree Inference",
    "authors" : [ "Effi Levi", "Roi Reichart", "Ari Rappoport" ],
    "emails" : [ "efle@cs.huji.ac.il", "arir@cs.huji.ac.il", "roiri@ie.technion.ac.il" ],
    "sections" : [ {
      "heading" : null,
      "text" : "The run time complexity of state-of-theart inference algorithms in graph-based dependency parsing is super-linear in the number of input words (n). Recently, pruning algorithms for these models have shown to cut a large portion of the graph edges, with minimal damage to the resulting parse trees. Solving the inference problem in run time complexity determined solely by the number of edges (m) is hence of obvious importance.\nWe propose such an inference algorithm for first-order models, which encodes the problem as a minimum spanning tree (MST) problem in an undirected graph. This allows us to utilize state-of-the-art undirected MST algorithms whose run time is O(m) at expectation and with a very high probability. A directed parse tree is then inferred from the undirected MST and is subsequently improved with respect to the directed parsing model through local greedy updates, both steps running inO(n) time. In experiments with 18 languages, a variant of the first-order MSTParser (McDonald et al., 2005b) that employs our algorithm performs very similarly to the original parser that runs an O(n2) directed MST inference."
    }, {
      "heading" : "1 Introduction",
      "text" : "Dependency parsers are major components of a large number of NLP applications. As application models are applied to constantly growing amounts of data, efficiency becomes a major consideration.\nIn graph-based dependency parsing models (Eisner, 2000; McDonald et al., 2005a; McDonald et al., 2005b; Carreras, 2007; Koo and Collins, 2010b), given an n word sentence and a model order k, the run time of exact inference is O(n3) for k = 1 and O(nk+1) for k > 1 in the projective case (Eisner, 1996; McDonald and Pereira, 2006). In the non-projective case it is O(n2) for k = 1 and NP-hard for k ≥ 2 (McDonald and Satta, 2007). 1 Consequently, a number of approximated parsers have been introduced, utilizing a variety of techniques: the Eisner algorithm (McDonald and Pereira, 2006), belief propagation (Smith and Eisner, 2008), dual decomposition (Koo and Collins, 2010b; Martins et al., 2013) and multi-commodity flows (Martins et al., 2009; Martins et al., 2011). The run time of all these approximations is superlinear in n.\nRecent pruning algorithms for graph-based dependency parsing (Rush and Petrov, 2012; Riedel et al., 2012; Zhang and McDonald, 2012) have shown to cut a very large portion of the graph edges, with minimal damage to the resulting parse trees. For example, Rush and Petrov (2012) demonstrated that a single O(n) pass of vinepruning (Eisner and Smith, 2005) can preserve > 98% of the correct edges, while ruling out > 86% of all possible edges. Such results give strong motivation to solving the inference problem in a run time complexity that is determined solely by the number of edges (m). 2\n1We refer to parsing approaches that produce only projective dependency trees as projective parsing and to approaches that produce all types of dependency trees as non-projective parsing.\n2Some pruning algorithms require initial construction of the full graph, which requires exactly n(n − 1) edge weight computations. Utilizing other techniques, such as lengthdictionary pruning, graph construction and pruning can be\nar X\niv :1\n51 0.\n07 48\n2v 4\n[ cs\n.C L\n] 7\nJ un\n2 01\n6\nIn this paper we propose to formulate the inference problem in first-order (arc-factored) dependency parsing as a minimum spanning tree (MST) problem in an undirected graph. Our formulation allows us to employ state-of-the-art algorithms for the MST problem in undirected graphs, whose run time depends solely on the number of edges in the graph. Importantly, a parser that employs our undirected inference algorithm can generate all possible trees, projective and non-projective.\nParticularly, the undirected MST problem (§ 2) has a randomized algorithm which is O(m) at expectation and with a very high probability ((Karger et al., 1995)), as well as an O(m · α(m,n)) worst-case deterministic algorithm (Pettie and Ramachandran, 2002), where α(m,n) is a certain natural inverse of Ackermann’s function (Hazewinkel, 2001). As the inverse of Ackermann’s function grows extremely slowly 3 the deterministic algorithm is in practice linear in m (§ 3). In the rest of the paper we hence refer to the run time of these two algorithms as practically linear in the number of edges m.\nOur algorithm has four steps (§ 4). First, it encodes the first-order dependency parsing inference problem as an undirected MST problem, in up to O(m) time. Then, it computes the MST of the resulting undirected graph. Next, it infers a unique directed parse tree from the undirected MST. Finally, the resulting directed tree is greedily improved with respect to the directed parsing model. Importantly, the last two steps take O(n) time, which makes the total run time of our algorithm O(m) at expectation and with very high probability. 4\nWe integrated our inference algorithm into the first-order parser of (McDonald et al., 2005b) and compared the resulting parser to the original parser which employs the Chu-Liu-Edmonds algorithm (CLE, (Chu and Liu, 1965; Edmonds, 1967)) for inference. CLE is the most efficient exact inference algorithm for graph-based first-order nonprojective parsers, running at O(n2) time.5\njointly performed in O(n) steps. We therefore do not include initial graph construction and pruning in our complexity computations.\n3α(m,n) is less than 5 for any practical input sizes (m,n).\n4The output dependency tree contains exactly n−1 edges, therefore m ≥ n− 1, which makes O(m)+O(n) = O(m).\n5CLE has faster implementations: O(m+nlogn) (Gabow et al., 1986) as well as O(mlogn) for sparse graphs (Tarjan, 1977), both are super-linear in n for connected graphs. We re-\nWe experimented (§ 5) with 17 languages from the CoNLL 2006 and 2007 shared tasks on multilingual dependency parsing (Buchholz and Marsi, 2006; Nilsson et al., 2007) and in three English setups. Our results reveal that the two algorithms perform very similarly. While the averaged unlabeled attachment accuracy score (UAS) of the original parser is 0.97% higher than ours, in 11 of 20 test setups the number of sentences that are better parsed by our parser is larger than the number of sentences that are better parsed by the original parser.\nImportantly, in this work we present an edge-linear first-order dependency parser which achieves similar accuracy to the existing one, making it an excellent candidate to be used for efficient MST computation in k-best trees methods, or to be utilized as an inference/initialization subroutine as a part of more complex approximation frameworks such as belief propagation. In addition, our model produces a different solution compared to the existing one (see Table 2), paving the way for using methods such as dual decomposition to combine these two models into a superior one.\nUndirected inference has been recently explored in the context of transition based parsing (Gómez-Rodrı́guez and Fernández-González, 2012; Gómez-Rodrı́guez et al., 2015), with the motivation of preventing the propagation of erroneous early edge directionality decisions to subsequent parsing decisions. Yet, to the best of our knowledge this is the first paper to address undirected inference for graph based dependency parsing. Our motivation and algorithmic challenges are substantially different from those of the earlier transition based work."
    }, {
      "heading" : "2 Undirected MST with the Boruvka Algorithm",
      "text" : "In this section we define the MST problem in undirected graphs. We then discuss the Burovka algorithm (Boruvka, 1926; Nesetril et al., 2001) which forms the basis for the randomized algorithm of (Karger et al., 1995) we employ in this paper. In the next section we will describe the Karger et al. (1995) algorithm in more details.\nProblem Definition. For a connected undirected graph G(V,E), where V is the set of n vertices\nfer here to the classical implementation employed by modern parsers (e.g. (McDonald et al., 2005b; Martins et al., 2013)).\nandE the set ofmweighted edges, the MST problem is defined as finding the sub-graph ofGwhich is the tree (a connected acyclic graph) with the lowest sum of edge weights. The opposite problem – finding the maximum spanning tree – can be solved by the same algorithms used for the minimum variant by simply negating the graph’s edge weights.\nGraph Contraction. In order to understand the Boruvka algorithm, let us first define the Graph Contraction operation. For a given undirected graph G(V,E) and a subset Ẽ ⊆ E, this operation creates a new graph, GC(VC , EC). In this new graph, VC consists of a vertex for each connected component in G̃(V, Ẽ) (these vertices are referred to as super-vertices). EC , in turn, consists of one edge, (û, v̂), for each edge (u, v) ∈ E \\ Ẽ, where û, v̂ ∈ VC correspond to G̃’s connected components to which u and v respectively belong. Note that this definition may result in multiple edges between two vertices in VC (denoted repetitive edges) as well as in edges from a vertex in VC to itself (denoted self edges).\nAlgorithm 1 The basic step of the Boruvka algorithm for the undirected MST problem.\nContract graph Input: a graph G(V,E), a subset Ẽ ⊆ E C ← connected components of G̃(V, Ẽ) return GC(C,E \\ Ẽ) Boruvka-step Input: a graph G(V,E)\n1: for all (u, v) ∈ E do 2: if w(u, v) < w(u.minEdge) then 3: u.minEdge← (u, v) 4: end if 5: if w(u, v) < w(v.minEdge) then 6: v.minEdge← (u, v) 7: end if 8: end for 9: for all v ∈ V do\n10: Em ← Em ∪ {v.minEdge} 11: end for 12: GB(VB, EB)← Contract graph(G(V,E),Em) 13: Remove fromEB self edges and non-minimal\nrepetitive edges 14: return GB(VB, EB), Em\nThe Boruvka-Step. Next, we define the basic step of the Borukva algorithm (see example in Fig-\nure 1 and pseudocode in Algorithm 1). In each such step, the algorithm creates a subset Em ⊂ E by selecting the minimally weighted edge for each vertex in the input graph G(V,E) (Figure 1 (a,b) and Algorithm 1 (lines 1-11)). Then, it performs the contraction operation on the graph G and Em to receive a new graph GB(VB, EB) (Figure 1 (c) and Algorithm 1 (12)). Finally, it removes from EB all self-edges and repetitive edges that are not the minimal edges between the vertices VB’s which they connect (Figure 1 (d) and Algorithm 1 (13)). The set Em created in each such step is guaranteed to consist only of edges that belong to G’s MST and is therefore also returned by the Boruvka step.\nThe Boruvka algorithm runs successive Boruvka-steps until it is left with a single supervertex. The MST of the original graph G is given by the unification of the Em sets returned in each step. The resulting computational complexity is O(m log n) (Nesetril et al., 2001). We now turn to describe how the undirected MST problem can be solved in a time practically linear in the number of graph edges."
    }, {
      "heading" : "3 Undirected MST in Edge Linear Time",
      "text" : "There are two algorithms that solve the undirected MST problem in time practically linear in the number of edges in the input graph. These algorithms are based on substantially different approaches: one is deterministic and the other is randomized 6.\n6Both these algorithms deal with a slightly more general case where the graph is not necessarily connected, in which case the minimum spanning forest (MSF) is computed. In our case, where the graph is connected, the MSF reduces to an MST.\nThe complexity of the first, deterministic, algorithm (Chazelle, 2000; Pettie and Ramachandran, 2002) isO(m ·α(m,n)), where α(m,n) is a natural inverse of Ackermann’s function, whose value for any practical values of n and m is lower than 5. As this algorithm employs very complex datastructures, we do not implement it in this paper.\nThe second, randomized, algorithm (Karger et al., 1995) has an expected run time of O(m + n) (which for connected graphs is O(m)), and this run time is achieved with a high probability of 1− exp(−Ω(m)). 7 In this paper we employ only this algorithm for first-order graph-based parsing inference, and hence describe it in details in this section.\nDefinitions and Properties. We first quote two properties of undirected graphs (Tarjan, 1983): (1) The cycle property: The heaviest edge in a cycle in a graph does not appear in the MSF; and (2) The cut property: For any proper nonempty subset V ′ of the graph vertices, the lightest edge with exactly one endpoint in V ′ is included in the MSF.\nWe continue with a number of definitions and observations. Given an undirected graph G(V,E) with weighted edges, and a forest F in that graph, F (u, v) is the path in that forest between u and v (if such a path exists), and sF (u, v) is the maximum weight of an edge in F (u, v) (if the path does not exist then sF (u, v) =∞). An edge (u, v) ∈ E is called F-heavy if s(u, v) > sF (u, v), otherwise it is called F-light. An alternative equivalent definition is that an edge is F-heavy if adding it to F creates a cycle in which it is the heaviest edge. An important observation (derived from the cycle property) is that for any forest F , no F-heavy edge can possibly be a part of an MSF for G. It has been shown that given a forest F , all the F-heavy edges in G can be found in O(m) time (Dixon et al., 1992; King, 1995).\nAlgorithm. The randomized algorithm can be outlined as follows (see pseudocode in algorithm 2): first, two successive Boruvka-steps are applied to the graph (line 4, Boruvka-step2 stands for two successive Boruvka-steps), reducing the number of vertices by (at least) a factor of 4 to receive a contracted graph GC and an edge set Em (§ 2). Then, a subgraph Gs is randomly constructed, such that each edge in GC , along with\n7This complexity analysis is beyond the scope of this paper.\nAlgorithm 2 Pseudocode for the Randomized MSF algorithm of(Karger et al., 1995).\nRandomized MSF Input: a graph G(V,E)\n1: if E is empty then 2: return ∅ 3: end if 4: GC(VC , EC), Em ← Boruvka-step2(G) 5: for all (u, v) ∈ EC do 6: if coin-flip == head then 7: Es ← Es ∪ {(u, v)} 8: Vs ← Vs ∪ {u, v} 9: end if\n10: end for 11: F ← Randomized MSF(Gs(Vs, Es)) 12: remove all F-heavy edges from GC(VC , EC) 13: FC ← Randomized MSF(GC(VC , EC)) 14: return FC ∪ Em\nthe vertices which it connects, is included in Gs with probability 12 (lines 5-10). Next, the algorithm is recursively applied toGs to obtain its minimum spanning forest F (line 11). Then, all Fheavy edges are removed from GC (line 12), and the algorithm is recursively applied to the resulting graph to obtain a spanning forest FC (line 13). The union of that forest with the edges Em forms the requested spanning forest (line 14).\nCorrectness. The correctness of the algorithm is proved by induction. By the cut property, every edge returned by the Boruvka step (line 4), is part of the MSF. Therefore, the rest of the edges in the original graph’s MSF form an MSF for the contracted graph. The removed F-heavy edges are, by the cycle property, not part of the MSF (line 12). By the induction assumption, the MSF of the remaining graph is then given by the second recursive call (line 13)."
    }, {
      "heading" : "4 Undirected MST Inference for Dependency Parsing",
      "text" : "There are several challenges in the construction of an undirected MST parser: an MST parser that employs an undirected MST algorithm for inference.8 These challenges stem from the mismatch between the undirected nature of the inference algorithm and the directed nature of the resulting\n8Henceforth, we refer to an MST parser that employs a directed MST algorithm for inference as directed MST parser.\nparse tree. The first problem is that of undirected encoding. Unlike directed MST parsers that explicitly encode the directed nature of dependency parsing into a directed input graph to which an MST algorithm is applied (McDonald et al., 2005b), an undirected MST parser needs to encode directionality information into an undirected graph. In this section we consider two solutions to this problem.\nThe second problem is that of scheme conversion. The output of an undirected MST algorithm is an undirected tree while the dependency parsing problem requires finding a directed parse tree. In this section we show that for rooted undirected spanning trees there is only one way to define the edge directions under the constraint that the root vertex has no incoming edges and that each nonroot vertex has exactly one incoming edge in the resulting directed spanning tree. As dependency parse trees obey the first constraint and the second constraint is a definitive property of directed trees, the output of an undirected MST parser can be transformed into a directed tree using a simple O(n) time procedure.\nUnfortunately, as we will see in § 5, even with our best undirected encoding method, an undirected MST parser does not produce directed trees of the same quality as its directed counterpart. At the last part of this section we therefore present a simple, O(n) time, local enhancement procedure, that improves the score of the directed tree generated from the output of the undirected MST parser with respect to the edge scores of a standard directed MST parser. That is, our procedure improves the output of the undirected MST parser with respect to a directed model without having to compute the MST of the latter, which would take O(n2) time.\nWe conclude this section with a final remark stating that the output class of our inference algorithm is non-projective. That is, it can generate all possible parse trees, projective and non-projective.\nUndirected Encoding Our challenge here is to design an encoding scheme that encodes directionality information into the graph of the undirected MST problem. One approach would be to compute directed edge weights according to a feature representation scheme for directed edges (e.g. one of the schemes employed by existing directed MST parsers) and then transform these directed weights into undirected ones.\nSpecifically, given two vertices u and v with directed edges (u, v) and (v, u), weighted with sd(u, v) and sd(v, u) respectively, the goal is to compute the weight su( ˆu, v) of the undirected edge ( ˆu, v) connecting them in the undirected graph. We do this using a pre-determined function f : R×R→ R, such that f(sd(u, v), sd(v, u)) = su( ˆu, v). f can take several forms including mean, product and so on. In our experiments the mean proved to be the best choice.\nTraining with the above approach is implemented as follows. w, the parameter vector of the parser, consists of the weights of directed features. At each training iteration, w is used for the computation of sd(u, v) = w · φ(u, v) and sd(v, u) = w · φ(v, u) (where φ(u, v) and φ(v, u) are the feature representations of these directed edges). Then, f is applied to compute the undirected edge score su( ˆu, v). Next, the undirected MST algorithm is run on the resulting weighted undirected graph, and its output MST is transformed into a directed tree (see below). Finally, this directed tree is used for the update of w with respect to the gold standard (directed) tree.\nAt test time, the vector w which resulted from the training process is used for sd computations. Undirected graph construction, undirected MST computation and the undirected to directed tree conversion process are conducted exactly as in training. 9\nUnfortunately, preliminary experiments in our development setup revealed that this approach yields parse trees of much lower quality compared to the trees generated by the directed MST parser that employed the original directed feature set. In § 5 we discuss these results in details.\nAn alternative approach is to employ an undirected feature set. To implement this approach, we employed the feature set of the MST parser ((McDonald et al., 2005a), Table 1) with one difference: some of the features are directional, distinguishing between the properties of the source (parent) and the target (child) vertices. We stripped those features from that information, which resulted in an undirected version of the feature set.\nUnder this feature representation, training with undirected inference is simple. w, the parameter vector of the parser, now consists of the weights\n9In evaluation setup experiments we also considered a variant of this model where the training process utilized directed MST inference. As this variant performed poorly, we exclude it from our discussion in the rest of the paper.\nof undirected features. Once the undirected MST is computed by an undirected MST algorithm, w can be updated with respect to an undirected variant of the gold parse trees. At test time, the algorithm constructs an undirected graph using the vector w resulted from the training process. This graph’s undirected MST is computed and then transformed into a directed tree.\nInterestingly, although this approach does not explicitly encode edge directionality information into the undirected model, it performed very well in our experiments (§ 5), especially when combined with the local enhancement procedure described below.\nScheme Conversion Once the undirected MST is found, we need to direct its edges in order for the end result to be a directed dependency parse tree. Following a standard practice in graph-based dependency parsing (e.g. (McDonald et al., 2005b)), before inference is performed we add a dummy\nroot vertex to the initial input graph with edges connecting it to all of the other vertices in the graph. Consequently, the final undirected tree will have a designated root vertex. In the resulting directed tree, this vertex is constrained to have only outgoing edges. As observed by GómezRodrı́guez and Fernández-González (2012), this effectively forces the direction for the rest of the edges in the tree.\nGiven a root vertex that follows the above constraint, and together with the definitive property of directed trees stating that each non-root vertex in the graph has exactly one incoming edge, we can direct the edges of the undirected tree using a simple BFS-like algorithm (Figure 2). Starting with the root vertex, we mark its undirected edges as outgoing, mark the vertex itself as done and its descendants as open. We then recursively repeat the same procedure for each open vertex until there are no such vertices left in the tree, at which point we have a directed tree. Note that given the constraints on the root vertex, there is no other way to direct the undirected tree edges. This procedure runs in O(n) time, as it requires a constant number of operations for each of the n−1 edges of the undirected spanning tree.\nIn the rest of the paper we refer to the directed tree generated by the undirected and directed MST parsers as du-tree and dd-tree respectively.\nLocal Enhancement Procedure As noted above, experiments in our development setup (§ 5) revealed that the directed parser performs somewhat better than the undirected one. This motivated us to develop a local enhancement procedure that improves the tree produced by the undirected model with respect to the directed model without compromising our O(m) run time. Our enhancement procedure is motivated by development experiments, revealing the much smaller gap between the quality of the du-tree and dd-tree of the same sentence under undirected evaluation compared to directed evaluation (§ 5 demonstrates this for test results).\nFor a du-tree that contains the vertex u and the edges (t, u) and (u, v), we therefore consider the replacement of (u, v) with (v, u). Note that after this change our graph would no longer be a directed tree, since it would cause u to have two parents, v and t, and v to have no parent. This, however, can be rectified by replacing the edge (t, u) with the edge (t, v).\nIt is easy to infer whether this change results in a better (lower weight) spanning tree under the directed model by computing the equation: gain = sd(t, u) + sd(u, v)− (sd(t, v) + sd(v, u)), where sd(x, y) is the score of the edge (x, y) according to the directed model. This is illustrated in Figure 3.\nGiven the du-tree, we traverse its edges and compute the above gain for each. We then choose the edge with the maximal positive gain, as this forms the maximal possible decrease in the directed model score using modifications of the type we consider, and perform the corresponding modification. In our experiments we performed this procedure five times per inference problem.10 This procedure performs a constant number of operations for each of the n − 1 edges of the du-tree, resulting in O(n) run time.\nOutput Class. Our undirected MST parser is non-projective. This stems from the fact that the undirected MST algorithms we discuss in § 3 do not enforce any structural constraint, and particularly the non-crossing constraint, on the resulting undirected MST. As the scheme conversion (edge directing) and the local enhancement procedures described in this section do not enforce any such constraint as well, the resulting tree can take any possible structure."
    }, {
      "heading" : "5 Experiments and Results",
      "text" : "Experimental setup We evaluate four models: (a) The original directed parser (D-MST, (McDonald et al., 2005b)); (b) Our undirected MST parser with undirected features and with the local enhancement procedure (U-MST-uf-lep);11 (c) Our undirected MST parser with undirected features but without the local enhancement procedure (UMST-uf); and (d) Our undirected MST parser with directed features (U-MST-df). All models are implemented within the MSTParser code12.\nThe MSTParser does not prune its input graphs. To demonstrate the value of undirected parsing for sparse input graphs, we implemented the lengthdictionary pruning strategy which eliminates all edges longer than the maximum length observed\n10This hyperparameter was estimated once on our English development setup, and used for all 20 multilingual test setups.\n11The directed edge weights for the local enhancement procedure (sd in § 4) were computed using the trained DMST parser.\n12http://www.seas.upenn.edu/˜strctlrn/ MSTParser/MSTParser.html\nfor each directed head-modifier POS pair in the training data. An undirected edge ˆ(u, v) is pruned iff both directed edges (u, v) and (v, u) are to be pruned according to the pruning method. To estimate the accuracy/graph-size tradeoff provided by undirected parsing (models (b)-(d)), we apply the pruning strategy only to these models leaving the the D-MST model (model (a)) untouched. This way D-MST runs on a complete directed graph with n2 edges.\nOur models were developed in a monolingual setup: training on sections 2-21 of WSJ PTB (Marcus et al., 1993) and testing on section 22. The development phase was devoted to the various decisions detailed throughout this paper and to the tuning of the single hyperparameter: the number of times the local enhancement procedure is executed.\nWe tested the models in 3 English and 17 multilingual setups. The English setups are: (a) PTB: training on sections 2-21 of the WSJ PTB and testing on its section 23; (b) GENIA: training with a random sample of 90% of the 4661 GENIA corpus (Ohta et al., 2002) sentences and testing on the other 10%; and (c) QBank: a setup identical to (b) for the 3987 QuestionBank (Judge et al., 2006) sentences. Multilingual parsing was performed with the multilingual datasets of the CoNLL 2006 (Buchholz and Marsi, 2006) and 2007 (Nilsson et al., 2007) shared tasks on multilingual dependency parsing, following their standard train/test split. Following previous work, punctuation was excluded from the evaluation.\nLength-dictionary pruning reduces the number of undirected edges by 27.02% on average across our 20 setups (std = 11.02%, median = 23.85%), leaving an average of 73.98% of the edges in the undirected graph. In 17 of 20 setups the reduction is above 20%. Note that the number of edges in a complete directed graph is twice the number in its undirected counterpart. Therefore, on average, the number of input edges in the pruned undirected models amounts to 73.98%2 = 36.49% of the number of edges in the complete directed graphs. In fact, every edge-related operation (such as feature extraction) in the undirected model is actually performed on half of the number of edges compared to the directed model, saving run-time not only in the MST-inference stage but in every stage involving these operations. In addition, some pruning methods, such as length-dictionary pruning (used\nin this work) perform feature extraction only for existing (un-pruned) edges, meaning that any reduction in the number of edges also reduces feature extraction operations.\nFor each model we report the standard directed unlabeled attachment accuracy score (D-UAS). In addition, since this paper explores the value of undirected inference for a problem that is directed in nature, we also report the undirected unlabeled attachment accuracy score (U-UAS), hoping that these results will shed light on the differences between the trees generated by the different models.\nResults Table 1 presents our main results. While the directed MST parser (D-MST) is the best performing model across almost all test sets and evaluation measures, it outperforms our best model, U-MST-uf-lep, by a very small margin.\nParticularly, for D-UAS, D-MST outperforms U-MST-uf-lep by up to 1% in 14 out of 20 setups (in 6 setups the difference is up to 0.5%). In 5 other setups the difference between the models is between 1% and 2%, and only in one setup it is above 2% (2.6%). Similarly, for U-UAS, in 2 setups the models achieve the same performance, in 15 setups the difference is less than 1% and in the other setups the differences is 1.1% - 1.5%. The average differences are 0.97% and 0.67% for DUAS and U-UAS respectively.\nThe table further demonstrates the value of the local enhancement procedure. Indeed, U-MSTuf-lep outperforms U-MST in all 20 setups in D-\nUAS evaluation and in 15 out of 20 setups in UUAS evaluation (in one setup there is a tie). However, the improvement this procedure provides is much more noticeable for D-UAS, with an averaged improvement of 2.35% across setups, compared to an averaged U-UAS improvement of only 0.26% across setups. While half of the changes performed by the local enhancement procedure are in edge directions, its marginal U-UAS improvement indicates that almost all of its power comes from edge direction changes. This calls for an improved enhancement procedure.\nFinally, moving to directed features (the UMST-df model), both D-UAS and U-UAS substantially degrade, with more noticeable degradation in the former. We hypothesize that this stems from the idiosyncrasy between the directed parameter update and the undirected inference in this model.\nTable 2 reveals the complementary nature of our U-MST-uf-lep model and the classical D-MST: each of the models outperforms the other on an average of 22.2% of the sentences across test setups. An oracle model that selects the parse tree of the best model for each sentence would improve DUAS by an average of 1.2% over D-MST across the test setups.\nThe results demonstrate the power of first-order graph-based dependency parsing with undirected inference. Although using a substantially different inference algorithm, our U-MST-uf-lep model performs very similarly to the standard MST parser which employs directed MST inference."
    }, {
      "heading" : "6 Discussion",
      "text" : "We present a first-order graph-based dependency parsing model which runs in edge linear time at expectation and with very high probability. In extensive multilingual experiments our model performs very similarly to a standard directed firstorder parser. Moreover, our results demonstrate the complementary nature of the models, with our model outperforming its directed counterpart on an average of 22.2% of the test sentences.\nBeyond its practical implications, our work provides a novel intellectual contribution in demonstrating the power of undirected graph based methods in solving an NLP problem that is directed in nature. We believe this contribution has the potential to affect future research on additional NLP problems.\nThe potential embodied in this work extends to a number of promising research directions:\n• Our algorithm may be used for efficient MST computation in k-best trees methods which are instrumental in margin-based training algorithms. For example, McDonald et al. (2005b) observed that k calls to the CLU algorithm might prove to be too inefficient; our more efficient algorithm may provide the remedy.\n• It may also be utilized as an inference/initialization subroutine as a part of more complex approximation frameworks such as belief propagation (e.g. Smith and Eisner (2008), Gormley et al. (2015)).\n• Finally, the complementary nature of the directed and undirected parsers motivates the development of methods for their combination, such as dual decomposition (e.g. Rush et al. (2010), Koo et al. (2010a)). Particularly, we have shown that our undirected inference algorithm converges to a different solution than the standard directed solution while still maintaining high quality (Table 2). Such techniques can exploit this diversity to produce a higher quality unified solution.\nWe intend to investigate all of these directions in future work. In addition, we are currently exploring potential extensions of the techniques presented in this paper to higher order, projective and non-projective, dependency parsing."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The second author was partly supported by a GIF Young Scientists’ Program grant No. I-2388407.6/2015 - Syntactic Parsing in Context."
    } ],
    "references" : [ {
      "title" : "Conll-x shared task on multilingual dependency parsing",
      "author" : [ "Buchholz", "Marsi2006] Sabine Buchholz", "Erwin Marsi" ],
      "venue" : "In Proceedings of the Tenth Conference on Computational Natural Language Learning,",
      "citeRegEx" : "Buchholz et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Buchholz et al\\.",
      "year" : 2006
    }, {
      "title" : "Experiments with a higher-order projective dependency parser",
      "author" : [ "Xavier Carreras" ],
      "venue" : "In Proc. of CoNLL",
      "citeRegEx" : "Carreras.,? \\Q2007\\E",
      "shortCiteRegEx" : "Carreras.",
      "year" : 2007
    }, {
      "title" : "A minimum spanning tree algorithm with inverse-ackermann type complexity",
      "author" : [ "Bernard Chazelle" ],
      "venue" : "J. ACM,",
      "citeRegEx" : "Chazelle.,? \\Q2000\\E",
      "shortCiteRegEx" : "Chazelle.",
      "year" : 2000
    }, {
      "title" : "On the shortest arborescence of a directed graph",
      "author" : [ "Chu", "Liu1965] Y.J. Chu", "T.H. Liu" ],
      "venue" : "Science Sinica,",
      "citeRegEx" : "Chu et al\\.,? \\Q1965\\E",
      "shortCiteRegEx" : "Chu et al\\.",
      "year" : 1965
    }, {
      "title" : "Verification and sensitivity analysis of minimum spanning trees in linear time",
      "author" : [ "Dixon et al.1992] Brandon Dixon", "Monika Rauch", "Robert", "Robert E. Tarjan" ],
      "venue" : "SIAM J. Comput,",
      "citeRegEx" : "Dixon et al\\.,? \\Q1992\\E",
      "shortCiteRegEx" : "Dixon et al\\.",
      "year" : 1992
    }, {
      "title" : "Optimum branchings",
      "author" : [ "J. Edmonds" ],
      "venue" : "Journal of Research of the National Bureau of Standards,",
      "citeRegEx" : "Edmonds.,? \\Q1967\\E",
      "shortCiteRegEx" : "Edmonds.",
      "year" : 1967
    }, {
      "title" : "Parsing with soft and hard constraints on dependency length",
      "author" : [ "Eisner", "Smith2005] Jason Eisner", "Noah Smith" ],
      "venue" : "In Proc. IWPT",
      "citeRegEx" : "Eisner et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Eisner et al\\.",
      "year" : 2005
    }, {
      "title" : "Three new probabilistic models for dependency parsing: An exploration",
      "author" : [ "Jason Eisner" ],
      "venue" : "In Proc. of COLING",
      "citeRegEx" : "Eisner.,? \\Q1996\\E",
      "shortCiteRegEx" : "Eisner.",
      "year" : 1996
    }, {
      "title" : "Bilexical grammars and their cubic-time parsing algorithms",
      "author" : [ "Jason Eisner" ],
      "venue" : "Advances in Probabilistic and Other Parsing Technologies",
      "citeRegEx" : "Eisner.,? \\Q2000\\E",
      "shortCiteRegEx" : "Eisner.",
      "year" : 2000
    }, {
      "title" : "Efficient algorithms for finding minimum spanning trees in undirected and directed graphs",
      "author" : [ "Gabow et al.1986] Harold N Gabow", "Zvi Galil", "Thomas Spencer", "Robert E Tarjan" ],
      "venue" : null,
      "citeRegEx" : "Gabow et al\\.,? \\Q1986\\E",
      "shortCiteRegEx" : "Gabow et al\\.",
      "year" : 1986
    }, {
      "title" : "Undirected dependency parsing",
      "author" : [ "GómezRodrı́guez", "Daniel Fernández-González", "Vı́ctor Manuel Darriba Bilbao" ],
      "venue" : "Computational Intelligence,",
      "citeRegEx" : "GómezRodrı́guez et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "GómezRodrı́guez et al\\.",
      "year" : 2015
    }, {
      "title" : "Approximation-aware dependency parsing by belief propagation. Transactions of the Association for Computational Linguistics, 3:489–501",
      "author" : [ "Mark Dredze", "Jason Eisner" ],
      "venue" : null,
      "citeRegEx" : "Gormley et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gormley et al\\.",
      "year" : 2015
    }, {
      "title" : "Questionbank: Creating a corpus of parse-annotated questions",
      "author" : [ "Judge et al.2006] John Judge", "Aoife Cahill", "Josef Van Genabith" ],
      "venue" : "In Proceedings of ACL-COLING,",
      "citeRegEx" : "Judge et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Judge et al\\.",
      "year" : 2006
    }, {
      "title" : "A randomized linear-time algorithm to find minimum spanning trees",
      "author" : [ "Karger et al.1995] David Karger", "Philip Klein", "Robert Tarjan" ],
      "venue" : "J. ACM,",
      "citeRegEx" : "Karger et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Karger et al\\.",
      "year" : 1995
    }, {
      "title" : "A simpler minimum spanning tree verification algorithm",
      "author" : [ "Valerie King" ],
      "venue" : null,
      "citeRegEx" : "King.,? \\Q1995\\E",
      "shortCiteRegEx" : "King.",
      "year" : 1995
    }, {
      "title" : "Efficient third-order dependency parsers",
      "author" : [ "Koo", "Collins2010b] Terry Koo", "Michael Collins" ],
      "venue" : "In Proc. of ACL",
      "citeRegEx" : "Koo et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Koo et al\\.",
      "year" : 2010
    }, {
      "title" : "Dual decomposition for parsing with non-projective head automata",
      "author" : [ "Koo et al.2010a] T. Koo", "A.M. Rush", "M. Collins", "T. Jaakkola", "D. Sontag" ],
      "venue" : "In Proc. of EMNLP",
      "citeRegEx" : "Koo et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Koo et al\\.",
      "year" : 2010
    }, {
      "title" : "Building a large annotated corpus of english: The penn treebank",
      "author" : [ "Mary Ann Marcinkiewicz", "Beatrice Santorini" ],
      "venue" : null,
      "citeRegEx" : "Marcus et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Marcus et al\\.",
      "year" : 1993
    }, {
      "title" : "Concise integer linear programming formulations for dependency parsing",
      "author" : [ "A.F.T. Martins", "N.A. Smith", "E.P. Xing" ],
      "venue" : "In Proc. of ACL",
      "citeRegEx" : "Martins et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Martins et al\\.",
      "year" : 2009
    }, {
      "title" : "Dual decomposition with many overlapping components",
      "author" : [ "A.F.T. Martins", "N.A. Smith", "P.M.Q. Aguiar", "M.A.T. Figueiredo" ],
      "venue" : "In Proc. of EMNLP",
      "citeRegEx" : "Martins et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Martins et al\\.",
      "year" : 2011
    }, {
      "title" : "Turning on the turbo: Fast third-order non-projective turbo parsers",
      "author" : [ "A.F.T. Martins", "Miguel Almeida", "N.A. Smith" ],
      "venue" : "In Proc. of ACL",
      "citeRegEx" : "Martins et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Martins et al\\.",
      "year" : 2013
    }, {
      "title" : "Online learning of approximate dependency parsing algorithms",
      "author" : [ "McDonald", "Pereira2006] Ryan McDonald", "Fernando Pereira" ],
      "venue" : "In Proc. of EACL",
      "citeRegEx" : "McDonald et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "McDonald et al\\.",
      "year" : 2006
    }, {
      "title" : "On the complexity of nonprojective data-driven dependency parsing",
      "author" : [ "McDonald", "Satta2007] Ryan McDonald", "Giorgio Satta" ],
      "venue" : "In Proc. of IWPT",
      "citeRegEx" : "McDonald et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "McDonald et al\\.",
      "year" : 2007
    }, {
      "title" : "Online large-margin training of dependency parsers",
      "author" : [ "Koby Crammer", "Giorgio Satta" ],
      "venue" : "In Proc. of ACL",
      "citeRegEx" : "McDonald et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "McDonald et al\\.",
      "year" : 2005
    }, {
      "title" : "Nonprojective dependency parsing using spanning tree algorithms",
      "author" : [ "Fernando Pereira", "Kiril Ribarov", "Jan Hajic" ],
      "venue" : "In Proc. of HLT-EMNLP",
      "citeRegEx" : "McDonald et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "McDonald et al\\.",
      "year" : 2005
    }, {
      "title" : "Otakar boruvka on minimum spanning tree problem translation of both the 1926 papers, comments, history",
      "author" : [ "Eva Milková", "Helena Nesetrilová" ],
      "venue" : "Discrete Mathematics,",
      "citeRegEx" : "Nesetril et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Nesetril et al\\.",
      "year" : 2001
    }, {
      "title" : "The conll 2007 shared task on dependency parsing",
      "author" : [ "Nilsson et al.2007] Jens Nilsson", "Sebastian Riedel", "Deniz Yuret" ],
      "venue" : "In Proceedings of the CoNLL shared task session of EMNLP-CoNLL,",
      "citeRegEx" : "Nilsson et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Nilsson et al\\.",
      "year" : 2007
    }, {
      "title" : "The genia corpus: An annotated research abstract corpus in molecular biology domain",
      "author" : [ "Ohta et al.2002] Tomoko Ohta", "Yuka Tateisi", "JinDong Kim" ],
      "venue" : "In Proceedings of the second international conference on Human Language Technology",
      "citeRegEx" : "Ohta et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Ohta et al\\.",
      "year" : 2002
    }, {
      "title" : "An optimal minimum spanning tree algorithm",
      "author" : [ "Pettie", "Ramachandran2002] Seth Pettie", "Vijaya Ramachandran" ],
      "venue" : "J. ACM,",
      "citeRegEx" : "Pettie et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Pettie et al\\.",
      "year" : 2002
    }, {
      "title" : "Parse, price and cut – delayed column and row generation for graph based parsers",
      "author" : [ "David Smith", "Andrew McCallum" ],
      "venue" : "In Proc. of EMNLP-CoNLL",
      "citeRegEx" : "Riedel et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Riedel et al\\.",
      "year" : 2012
    }, {
      "title" : "Vine pruning for efficient multi-pass dependency parsing",
      "author" : [ "Rush", "Petrov2012] Alexander Rush", "Slav Petrov" ],
      "venue" : "In Proc. of NAACL",
      "citeRegEx" : "Rush et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Rush et al\\.",
      "year" : 2012
    }, {
      "title" : "On dual decomposition and linear programming relaxations for natural language processing",
      "author" : [ "David Sontag", "Michael Collins", "Tommi Jaakkola" ],
      "venue" : "In Proceedings of the 2010 Conference on Empirical Meth-",
      "citeRegEx" : "Rush et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Rush et al\\.",
      "year" : 2010
    }, {
      "title" : "Dependency parsing by belief propagation",
      "author" : [ "Smith", "Eisner2008] David Smith", "Jason Eisner" ],
      "venue" : "In Proc. of EMNLP",
      "citeRegEx" : "Smith et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Smith et al\\.",
      "year" : 2008
    }, {
      "title" : "Data Structures and Network Algorithms",
      "author" : [ "Robert Endre Tarjan" ],
      "venue" : "Society for Industrial and Applied Mathematics,",
      "citeRegEx" : "Tarjan.,? \\Q1983\\E",
      "shortCiteRegEx" : "Tarjan.",
      "year" : 1983
    }, {
      "title" : "Generalized higher-order dependency parsing with cube pruning",
      "author" : [ "Zhang", "McDonald2012] Hao Zhang", "Ryan McDonald" ],
      "venue" : "In Proc. of EMNLP-CoNLL",
      "citeRegEx" : "Zhang et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : ", 2005b; Carreras, 2007; Koo and Collins, 2010b), given an n word sentence and a model order k, the run time of exact inference is O(n3) for k = 1 and O(nk+1) for k > 1 in the projective case (Eisner, 1996; McDonald and Pereira, 2006).",
      "startOffset" : 192,
      "endOffset" : 234
    }, {
      "referenceID" : 20,
      "context" : "Pereira, 2006), belief propagation (Smith and Eisner, 2008), dual decomposition (Koo and Collins, 2010b; Martins et al., 2013) and multi-commodity flows (Martins et al.",
      "startOffset" : 80,
      "endOffset" : 126
    }, {
      "referenceID" : 18,
      "context" : ", 2013) and multi-commodity flows (Martins et al., 2009; Martins et al., 2011).",
      "startOffset" : 34,
      "endOffset" : 78
    }, {
      "referenceID" : 19,
      "context" : ", 2013) and multi-commodity flows (Martins et al., 2009; Martins et al., 2011).",
      "startOffset" : 34,
      "endOffset" : 78
    }, {
      "referenceID" : 29,
      "context" : "Recent pruning algorithms for graph-based dependency parsing (Rush and Petrov, 2012; Riedel et al., 2012; Zhang and McDonald, 2012) have shown to cut a very large portion of the graph edges, with minimal damage to the resulting parse trees.",
      "startOffset" : 61,
      "endOffset" : 131
    }, {
      "referenceID" : 27,
      "context" : "Recent pruning algorithms for graph-based dependency parsing (Rush and Petrov, 2012; Riedel et al., 2012; Zhang and McDonald, 2012) have shown to cut a very large portion of the graph edges, with minimal damage to the resulting parse trees. For example, Rush and Petrov (2012) demonstrated that a single O(n) pass of vinepruning (Eisner and Smith, 2005) can preserve > 98% of the correct edges, while ruling out > 86% of all possible edges.",
      "startOffset" : 85,
      "endOffset" : 277
    }, {
      "referenceID" : 13,
      "context" : "Particularly, the undirected MST problem (§ 2) has a randomized algorithm which is O(m) at expectation and with a very high probability ((Karger et al., 1995)), as well as an O(m ·",
      "startOffset" : 137,
      "endOffset" : 158
    }, {
      "referenceID" : 5,
      "context" : "which employs the Chu-Liu-Edmonds algorithm (CLE, (Chu and Liu, 1965; Edmonds, 1967)) for inference.",
      "startOffset" : 50,
      "endOffset" : 84
    }, {
      "referenceID" : 9,
      "context" : "CLE has faster implementations: O(m+nlogn) (Gabow et al., 1986) as well as O(mlogn) for sparse graphs (Tarjan, 1977), both are super-linear in n for connected graphs.",
      "startOffset" : 43,
      "endOffset" : 63
    }, {
      "referenceID" : 26,
      "context" : "the CoNLL 2006 and 2007 shared tasks on multilingual dependency parsing (Buchholz and Marsi, 2006; Nilsson et al., 2007) and in three English setups.",
      "startOffset" : 72,
      "endOffset" : 120
    }, {
      "referenceID" : 25,
      "context" : "We then discuss the Burovka algorithm (Boruvka, 1926; Nesetril et al., 2001) which forms the basis for the randomized algorithm of (Karger et al.",
      "startOffset" : 38,
      "endOffset" : 76
    }, {
      "referenceID" : 13,
      "context" : ", 2001) which forms the basis for the randomized algorithm of (Karger et al., 1995) we employ in this paper.",
      "startOffset" : 62,
      "endOffset" : 83
    }, {
      "referenceID" : 13,
      "context" : ", 2001) which forms the basis for the randomized algorithm of (Karger et al., 1995) we employ in this paper. In the next section we will describe the Karger et al. (1995) algorithm in more details.",
      "startOffset" : 63,
      "endOffset" : 171
    }, {
      "referenceID" : 20,
      "context" : "(McDonald et al., 2005b; Martins et al., 2013)).",
      "startOffset" : 0,
      "endOffset" : 46
    }, {
      "referenceID" : 25,
      "context" : "The resulting computational complexity is O(m log n) (Nesetril et al., 2001).",
      "startOffset" : 53,
      "endOffset" : 76
    }, {
      "referenceID" : 2,
      "context" : "rithm (Chazelle, 2000; Pettie and Ramachandran, 2002) isO(m ·α(m,n)), where α(m,n) is a natural inverse of Ackermann’s function, whose value for any practical values of n and m is lower than 5.",
      "startOffset" : 6,
      "endOffset" : 53
    }, {
      "referenceID" : 13,
      "context" : "The second, randomized, algorithm (Karger et al., 1995) has an expected run time of O(m + n) (which for connected graphs is O(m)), and this run time is achieved with a high probability of 1− exp(−Ω(m)).",
      "startOffset" : 34,
      "endOffset" : 55
    }, {
      "referenceID" : 33,
      "context" : "We first quote two properties of undirected graphs (Tarjan, 1983): (1) The cycle property: The heaviest edge in a cycle in a graph does not appear in the MSF; and (2) The cut property: For any proper nonempty subset V ′",
      "startOffset" : 51,
      "endOffset" : 65
    }, {
      "referenceID" : 4,
      "context" : "edges in G can be found in O(m) time (Dixon et al., 1992; King, 1995).",
      "startOffset" : 37,
      "endOffset" : 69
    }, {
      "referenceID" : 14,
      "context" : "edges in G can be found in O(m) time (Dixon et al., 1992; King, 1995).",
      "startOffset" : 37,
      "endOffset" : 69
    }, {
      "referenceID" : 13,
      "context" : "Algorithm 2 Pseudocode for the Randomized MSF algorithm of(Karger et al., 1995).",
      "startOffset" : 58,
      "endOffset" : 79
    }, {
      "referenceID" : 17,
      "context" : "Our models were developed in a monolingual setup: training on sections 2-21 of WSJ PTB (Marcus et al., 1993) and testing on section 22.",
      "startOffset" : 87,
      "endOffset" : 108
    }, {
      "referenceID" : 27,
      "context" : "pus (Ohta et al., 2002) sentences and testing on the other 10%; and (c) QBank: a setup identical to (b) for the 3987 QuestionBank (Judge et al.",
      "startOffset" : 4,
      "endOffset" : 23
    }, {
      "referenceID" : 12,
      "context" : ", 2002) sentences and testing on the other 10%; and (c) QBank: a setup identical to (b) for the 3987 QuestionBank (Judge et al., 2006) sentences.",
      "startOffset" : 114,
      "endOffset" : 134
    }, {
      "referenceID" : 26,
      "context" : "CoNLL 2006 (Buchholz and Marsi, 2006) and 2007 (Nilsson et al., 2007) shared tasks on multilingual dependency parsing, following their standard train/test split.",
      "startOffset" : 47,
      "endOffset" : 69
    }, {
      "referenceID" : 21,
      "context" : "For example, McDonald et al. (2005b) observed that k calls to the CLU algorithm might prove to be too inefficient; our more efficient algorithm may provide the",
      "startOffset" : 13,
      "endOffset" : 37
    }, {
      "referenceID" : 28,
      "context" : "Rush et al. (2010), Koo et al.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 15,
      "context" : "(2010), Koo et al. (2010a)).",
      "startOffset" : 8,
      "endOffset" : 27
    } ],
    "year" : 2016,
    "abstractText" : "The run time complexity of state-of-theart inference algorithms in graph-based dependency parsing is super-linear in the number of input words (n). Recently, pruning algorithms for these models have shown to cut a large portion of the graph edges, with minimal damage to the resulting parse trees. Solving the inference problem in run time complexity determined solely by the number of edges (m) is hence of obvious importance. We propose such an inference algorithm for first-order models, which encodes the problem as a minimum spanning tree (MST) problem in an undirected graph. This allows us to utilize state-of-the-art undirected MST algorithms whose run time is O(m) at expectation and with a very high probability. A directed parse tree is then inferred from the undirected MST and is subsequently improved with respect to the directed parsing model through local greedy updates, both steps running inO(n) time. In experiments with 18 languages, a variant of the first-order MSTParser (McDonald et al., 2005b) that employs our algorithm performs very similarly to the original parser that runs an O(n2) directed MST inference.",
    "creator" : "LaTeX with hyperref package"
  }
}