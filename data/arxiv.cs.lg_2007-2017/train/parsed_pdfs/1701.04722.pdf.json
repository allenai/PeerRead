{
  "name" : "1701.04722.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks",
    "authors" : [ "Lars Mescheder", "Andreas Geiger" ],
    "emails" : [ "LARS.MESCHEDER@TUEBINGEN.MPG.DE", "SEBASTIAN.NOWOZIN@MICROSOFT.COM", "ANDREAS.GEIGER@TUEBINGEN.MPG.DE" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Generative models in machine learning are models that can be trained on an unlabeled dataset and are capable of generating new data points after training is completed. As generating new content requires a good understanding of the training data at hand, such models are often regarded as a key ingredient to unsupervised learning. In fact, it can be argued that generating new content is also extremely important to the human learning process: in order to fully\ngrasp a subject, students usually have to work through numerous exercises on their own and thereby learn how to “generate” new content.\nIn recent years, generative models have become more and more powerful. While many model classes such as PixelRNNs (van den Oord et al., 2016b), PixelCNNs (van den Oord et al., 2016a), real NVP (Dinh et al., 2016) and Plug & Play generative networks (Nguyen et al., 2016) have been introduced and studied, the two most prominent ones are Variational Autoencoders (VAEs) (Kingma & Welling, 2013; Rezende et al., 2014) and Generative Adversarial Networks (GANs) (Goodfellow et al., 2014).\nBoth VAEs and GANs have their own advantages and disadvantages: while GANs generally yield visually sharper results when applied to images, VAEs are attractive because they naturally yield both a generative model and an inference model. Moreover, it was reported, that VAEs often lead to better log-likelihoods (Wu et al., 2016), making them interesting for scenarios where visual realism is not the main goal. The recently introduced BiGANs (Donahue et al., 2016; Dumoulin et al., 2016) add an inference model to GANs. However, it was observed that the reconstruction results often only vaguely resemble the input and often do so only semantically and not in a quantitative way.\nThe failure of VAEs to generate sharp images is often attributed to the fact that the inference models used during training are usually not expressive enough to capture the true posterior distribution. Indeed, recent work shows that using more expressive model classes can lead to substantially better results (Kingma et al., 2016), both visually and in terms of log-likelihood-bounds.\nIn this paper, we present Adversarial Variational Bayes (AVB), a technique for training Variational Autoencoders with arbitrarily flexible inference models parameterized by\nar X\niv :1\n70 1.\n04 72\n2v 1\n[ cs\n.L G\n] 1\n7 Ja\nneural networks. We can show that in the nonparametric limit we obtain a maximum-likelihood assignment for the generative model together with the correct posterior distribution.\nWhile there were some attempts at combining VAEs and GANs (Makhzani et al., 2015; Larsen et al., 2015), most of these attempts are not motivated from a maximumlikelihood point of view and therefore usually do not lead to maximum-likelihood assignments. For example, in Adversarial Autoencoders (AAEs) (Makhzani et al., 2015) the Kullback-Leibler-regularization term that appears in the training objective for VAEs is replaced with an adversarial loss that encourages the aggregated posterior to be close to the prior over the latent variables. Even though AAEs do not maximize a lower bound to the maximum-likelihoodobjective, we show in Section 5.2 that AAEs can be interpreted as an approximation to our approach, thereby establishing a connection of AAEs to maximum-likelihood learning.\nOur contributions are as follows:\n• we enable the usage of arbitrarily complex inference models for Variational Autoencoders using adversarial training\n• we give theoretical insight into our method, showing that in the nonparametric limit our method recovers the true posterior distribution as well as a true maximum-likelihood assignment for the parameters of the generative model\n• we demonstrate experimentally that our model is able to learn rich posterior distributions and show that the model is able to generate compelling samples for complex data sets"
    }, {
      "heading" : "2. Background",
      "text" : "As our model is an extension of Variational Autoencoders (VAEs) (Kingma & Welling, 2013; Rezende et al., 2014), we start with a brief review of VAEs.\nVAEs are specified by a parametric generative model pθ(x | z) of the visible variables given the latent variables, a prior p(z) over the latent variables and an approximate inference model qφ(z | x) over the latent variables given the visible variables. It can be shown that\nlog p(x) ≥ −KL(qφ(z | x), p(z)) + Eqφ(z|x) log pθ(x | z). (2.1)\nThe right hand side of (2.1) is called the variational lower bound or evidence lower bound (ELBO). If there is φ such\nthat qφ(z | x) = pθ(z | x), we would have\nlog p(x) = max φ −KL(qφ(z | x), p(z))\n+ Eqφ(z|x) log pθ(x | z). (2.2)\nHowever, in general this is not true, so that we only have an inequality in (2.2).\nWhen performing maximum-likelihood training, our goal is to optimize the marginal log-likelihood\nEpD(x) log pθ(x), (2.3)\nwhere pD is the data distribution. Unfortunately, computing log pθ(x) requires marginalizing out z in pθ(x, z) which is usually intractable. Variational Bayes uses inequality (2.1) to rephrase the intractable problem of optimizing (2.3) into\nmax θ max φ EpD(x)\n[ −KL(qφ(z | x), p(z))\n+ Eqφ(z|x) log pθ(x | z) ] . (2.4)\nDue to inequality (2.1), we still optimize a lower bound to the true maximum-likelihood-objective (2.3).\nNaturally, the quality of this lower bound depends on the expressiveness of the inference model qφ(z | x). Usually, qφ(z | x) is taken to be a Gaussian distribution with diagonal covariance matrix whose mean and variance vectors are parameterized by neural networks with x as input (Kingma & Welling, 2013; Rezende et al., 2014). While this model\nis very flexible in its dependence on x, its dependence on z is very restrictive, potentially limiting the quality of the resulting generative model. Indeed, it was observed that applying standard Variational Autoencoders to natural images often results in blurry images (Larsen et al., 2015)."
    }, {
      "heading" : "3. Method",
      "text" : "In this work we show how we can instead use a black-box inference model qφ(z | x) and use adversarial training to obtain an approximate maximum likelihood assignment θ∗ to θ and a close approximation qφ∗(z | x) to the true posterior pθ∗(z | x). This is visualized in Figure 1: on the left hand side the structure of a typical Variational Autoencoder is shown. The right hand side shows our flexible black-box inference model."
    }, {
      "heading" : "3.1. Derivation",
      "text" : "To derive our method, we rewrite the optimization problem in (2.4) as\nmax θ max φ\nEpD(x)Eqφ(z|x) ( log p(z)\n− log qφ(z | x) + log pθ(x | z) ) . (3.1)\nWhen we have an explicit representation of qφ(z | x), (3.1) can be optimized using the reparameterization trick (Kingma & Welling, 2013) and stochastic gradient descent. Unfortunately, this is not the case when we define qφ(z | x) by a black-box procedure like in Figure 1b.\nThe idea of our approach is to circumvent this problem by representing the term\nlog p(z)− log qφ(z | x) (3.2)\nindirectly as the optimal value of an additional real-valued discriminative network T (x, z) that we introduce to the problem.\nMore specifically, consider the following objective for the discriminator T (x, z):\nmax T EpD(x)Eqφ(z|x) log σ(T (x, z))\n+ EpD(x)Ep(z) log (1− σ(T (x, z))) . (3.3)\nHere, σ(t) := (1 + e−t)−1 denotes the sigmoid-function. Intuitively, T (x, z) tries to distinguish pairs (x, z) that were sampled independently using the distribution pD(x)p(z) from those that were sampled using the current inference model, i.e., using pD(x)qφ(z | x).\nTo simplify the theoretical analysis, we assume that the model T (x, z) is flexible enough to represent any function of the two variables x and z. This assumption is often\nreferred to as the nonparametric limit (Goodfellow et al., 2014) and is justified by the fact that deep neural networks are universal function approximators (Hornik et al., 1989).\nAs it turns out, the optimal discriminator T ∗(x, z) according to the objective in (3.3) is given by the negative of (3.2).\nProposition 1. For pθ(x | z) and qφ(z | x) fixed, the optimal discriminator T ∗ according to the objective in (3.3) is given by\nT ∗(x, z) = log qφ(z | x)− log p(z). (3.4)\nProof. The proof is analogous to the proof of Proposition 1 in Goodfellow et al. (2014). See the Appendix for details.\nTogether with (3.1), Proposition 1 allows us to write the optimization objective in (2.4) as\nmax θ,φ\nEpD(x)Eqφ(z|x) ( − T ∗(x, z) + log pθ(x | z) ) , (3.5)\nwhere T ∗(x, z) is defined as the function that maximizes (3.3).\nTo optimize (3.5), we need the gradients of (3.5) with respect to θ and φ. While taking the gradient with respect to θ is straightforward, taking the gradient with respect to φ is complicated by the fact that we have defined T ∗(x, z) indirectly as the solution of an auxiliary optimization problem which itself depends on φ. The following Proposition shows that taking the gradient with respect to the explicit occurrence of φ in T ∗(x, z) is not necessary:\nProposition 2. We have\nEqφ(z|x) (∇φT ∗(x, z)) = 0. (3.6)\nProof. By Proposition 1,\nEqφ(z|x) (∇φT ∗(x, z))\n= Eqφ(z|x) (∇φ log qφ(z | x)) . (3.7)\nFor an arbitrary family of probability densities qφ we have\nEqφ (∇φ log qφ) = ∫ qφ(z)\n∇φqφ(z) qφ(z) dz\n= ∇φ ∫ qφ(z)dz = ∇φ1 = 0. (3.8)\nTogether with (3.7), this implies (3.6).\nUsing the reparameterization trick (Kingma & Welling, 2013; Rezende et al., 2014), (3.5) can be rewritten in the\nform\nmax θ,φ\nEpD(x)E ( − T ∗(x, zφ(x, ))\n+ log pθ(x | zφ(x, )) ) (3.9)\nfor a suitable function zφ(x, ). Together with Proposition 1, (3.9) allows us to take unbiased estimates of the gradients of (3.5) with respect to φ and θ."
    }, {
      "heading" : "3.2. Algorithm",
      "text" : "Algorithm 1 Adversarial Variational Bayes (AVB) 1: i = 0 2: while not converged do 3: Samplem examples {x(1), . . . , x(m)} from data dis-\ntribution pD(x). 4: Sample m examples {z(1), . . . , z(m)} from prior distribution p(z). 5: Sample m noise examples { (1), . . . , (m)} from N (0, 1). 6: Compute θ-gradient (eq. 3.9):\ngθ = ∇θ 1m ∑m i=1 log pθ ( x(i) | zφ ( x(i), (i) )) 7: Compute φ-gradient (eq. 3.9):\ngφ = ∇φ 1m ∑m i=1 [ −Tψ ( x(i), zφ(x (i), (i)) )\n+ log pθ ( x(i) | zφ(x(i), (i)) )] .\n8: Compute ψ-gradient (eq. 3.3) : gψ = ∇ψ 1m ∑m i=1 [ log ( σ(Tψ(x (i), zφ(x (i), (i)))) ) + log ( 1− σ(Tψ(x(i), z(i)) )] .\n9: Perform SGD-updates for θ, φ and ψ: θ = θ + hi gθ, φ = φ+ hi gφ, ψ = ψ − hi gψ .\n10: i = i+ 1 11: end while\nIn theory, Propositions 1 and 2 allow us apply stochastic gradient descent directly to the objective in (2.4). However, this requires keeping T ∗(x, z) optimal which is computationally challenging. We therefore regard the optimization problems in (3.3) and (3.9) as a two-player game. Propositions 1 and 2 show that any Nash-equilibrium of this game yields a stationary point of the objective in (2.4).\nIn practice, we try to find a Nash-equilibrium by applying stochastic-gradient-descent (SGD) jointly to (3.3) and (3.9), see Algorithm 1. Here, we parameterize the neural network T with a vector ψ. Even though we have no guarantees that this algorithm converges, any fix point of this algorithm yields a stationary point of the objective in (2.4).\nNote that optimizing (3.5) with respect to φ while keeping θ and T fixed makes the encoder network collapse to a deterministic function. This is also a common problem for\nregular GANs (Radford et al., 2015). It is therefore crucial to keep the discriminative T network close to optimality while optimizing (3.5)."
    }, {
      "heading" : "3.3. Theoretical results",
      "text" : "In Sections 3.1 we derived AVB as a way of performing stochastic gradient descent on the variational lower bound in (2.4). In this section, we analyze the properties of Algorithm 1 from a game theoretical point of view.\nAs the next proposition shows, global Nash-equilibria of Algorithm 1 yield global optima of the objective in (2.4):\nProposition 3. If (θ∗, φ∗, T ∗) defines a Nash-equilibrium of the two-player game defined by (3.3) and (3.9), then\nT ∗(x, z) = log qφ∗(z | x)− log p(z) (3.10)\nand (θ∗, φ∗) is a global optimum of the variational lower bound in (2.4).\nProof. The proof can be found in the Appendix.\nConsider now the case where T and qφ(z | x) are both flexible enough, so that T can represent any function of two variables and qφ(z | x) can represent any family of positive probability densities on the latent space. In this case we have\nCorollary 4. Assume T can represent any function of two variables and qφ(z | x) can represent any family of positive probability densities on the latent space. If (θ∗, φ∗, T ∗) defines a Nash-equilibrium for the game defined by (3.3) and (3.9), then\n1. θ∗ is a maximum-likelihood assignment\n2. qφ∗(z | x) is equal to the true posterior pθ∗(z | x)\n3. T ∗ is the pointwise mutual information between x and z, i.e.\nT ∗(x, z) = log pθ∗(x, z)\npθ∗(x)p(z) . (3.11)\nProof. This is a straightforward consequence of Proposition 3, as in this case (θ∗, φ∗) optimizes the variational lower bound in (2.4) if and only if 1 and 2 hold. Inserting the result from 2 into (3.10) yields 3."
    }, {
      "heading" : "4. Experiments",
      "text" : "We tested our method both on shallow fully connected and a deep convolutional neural networks.\nThe CNN implementation is based on Radford et al. (2015) who show that deep convolutional networks can be used to parameterize the generator and discriminator for GANs. In\nparticular, Radford et al. demonstrate that their network architecture is capable of generating sharp images that are visually close to the training data. Our method also leads to sharp images, but in addition also yields an approximation to the posterior distribution that can be used for inference.\nSynthetic example As a first test, we trained a fully connected neural network on a simple synthetic dataset containing only the 4 data points shown in Figure 2 and a 2- dimensional latent space. Both the encoder and decoder are parameterized by 2-layer fully connected neural networks with 512 hidden units each. The encoder network takes as input a data point x and a vector of Gaussian random noise and produces a latent code z. The decoder network takes as input a latent code z and produces the parameters for four independent Bernoulli-distributions, one for each pixel of the output image. The adversary is parameterized by two neural networks with two 512-dimensional hidden layers each, acting on x and z respectively, whose 512- dimensional outputs are combined using an inner product.\nWe compare our method to a Variational Autoencoder with a diagonal Gaussian posterior distribution. The encoder and decoder networks are parameterized similarly as above, but the encoder network does not take the noise as input and produces a mean and variance vector instead of a single\nsample.\nWe visualize the resulting division of the latent space in Figure 3, where each color corresponds to one state in the x-space. Whereas the Variational Autoencoder divides the space into a mixture of 4 Gaussians, the Adversarial Variational Autoencoder learns a complex posterior distribution. Quantitatively this can be verified by computing the KLdivergence between the prior p(z) and the aggregated posterior qφ(z) := ∫ qφ(z | x)pD(x)dx, which we estimate using the ITE-package (Szabo, 2013), see Table 1. Note that the variations for different colors in Figure 3 are solely due to noise used in the inference model.\nThe ability of AVB to learn more complex posterior models leads to improved performance as Table 1 shows. In particular, AVB leads to a much higher likelihood score that is close to the optimal value of − log(4) compared to a standard VAE that struggles with the fact that it cannot divide the latent space appropriately. Moreover, we see that the reconstruction error given by the mean cross-entropy between an input x and its reconstruction using the encoder and decoder networks is much lower when using AVB instead of a VAE with diagonal Gaussian inference model. We also observe that the estimated variational lower bound is close to the true log-likelihood, indicating that the adversary has learned the correct function.\nMNIST In addition, we trained deep convolutional networks based on the DC-GAN-architecture (Radford et al., 2015) on the MNIST-dataset (LeCun et al., 1998). We use a similar architecture as for the synthetic example, but use 5-layer deep convolutional neural networks instead of fully connected neural networks acting on x. In the encoder network, we add the noise via a learned projection matrix to each hidden layer of the neural network. For the adversary, we keep the fully connected neural network with one hidden layer acting on z, but replace the network acting on x with a deep convolutional neural network. Moreover, we use batch normalization both in the encoder and adversary but not in the decoder, as we found that this decreases re-\nconstruction quality.\nSome random samples for MNIST are shown in Figure 4. We see that our model produces random samples that are visually close the training set.\ncelebA We also trained a deep convolutional network on the celebA-dataset (Liu et al., 2015). We use the same architecture as for MNIST except that we replace the adversary with a single deep convolutional neural network acting on x. We add the latent code z to each hidden layer via a learned projection matrix. Moreover, in the encoder and decoder we replace every hidden layer by three RESNETunits (He et al., 2015).\nThe samples for celebA are shown in Figure 5. We see that our model produces visually sharp images of faces. To demonstrate that the model has indeed learned an abstract representation of the data, we show reconstruction results and the result of linearly interpolating the z-vector in the latent space in Figure 6. We see that the reconstructions are reasonably sharp and the model produces realistic images for all interpolated z-values."
    }, {
      "heading" : "5. Related Work",
      "text" : ""
    }, {
      "heading" : "5.1. Connection to Variational Autoencoders",
      "text" : "Adversarial Variational Bayes strives to optimize the same objective as a standard Variational Autoencoder (Kingma & Welling, 2013; Rezende et al., 2014), but approximates the Kullback-Leibler divergence using an adversary instead of relying on a closed-form formula.\nSubstantial work has focused on making the the class of approximate inference models more expressive. Normalizing flows (Rezende & Mohamed, 2015; Kingma et al., 2016) make the posterior more complex by composing a simple Gaussian posterior with an invertible smooth mapping for which the determinant of the Jacobian is tractable. Auxiliary Variable VAEs (Maaløe et al., 2016) add auxiliary variables to the posterior to make it more flexible. However, no other approach we are aware of allows to use black-box inference models parameterized by a general neural network that takes as input a data point and a noise vector and produces a sample from the approximate posterior.\nAs neural network are universal function approximators (Hornik et al., 1989), our approach recovers the true posterior in the nonparametric limit. In the nonparametric limit, this implies that we obtain a true maximum-likelihood assignment for the parameters of the generative model. Of course, in practice this is only true if the adversary is sufficiently expressive to represent the optimal discriminator. However, in practice we found that our method is able to generate high quality images also if the discriminative network is of limited capacity."
    }, {
      "heading" : "5.2. Connection to Adversarial Autoencoders",
      "text" : "Makhzani et al. (Makhzani et al., 2015) introduced the concept of Adversarial Autoencoders. The idea is to replace the term\nKL(qφ(z | x), p(z)) (5.1)\nin (2.4) with an adversarial loss that tries to enforce that upon convergence\n∫ qφ(z | x)pD(x)dx ≈ p(z). (5.2)\nWhile related to our approach, the approach by Makhzani et al. modifies the variational objective while our approach retains the objective.\nInterestingly, the approach by Makhzani et al. can be regarded as an approximation to our approach, where T (x, z) is restricted to the class of functions that do not depend on x. Indeed, an ideal discriminator that only depends on z\nmaximizes∫ ∫ pD(x)q(z | x) log σ(T (z))dxdz\n+ ∫ ∫ pD(x)p(z) log (1− σ(T (z)))dxdz (5.3)\nwhich is the case if and only if\nT (z) = log ∫ q(z | x)pD(x)dx− log p(z). (5.4)\nClearly, this simplification is a crude approximation to our formulation from Section 3, but Makhzani et al. (2015) show that this method can still lead to good sampling results. In theory, restricting T (x, z) in this way ensures that upon convergence we approximately have∫\nqφ(z | x)pD(x)dx = p(z), (5.5)\nbut qφ(z | x) need not be close to the true posterior pθ(z | x). Intuitively, while mapping pD(x) through qφ(z | x) results in the correct marginal distribution, the contribution of each x to this distribution can be very inaccurate. When using this model as a generative model, the resulting sampler therefore has no guarantee to generate samples according to the true probability of the data point in the data distribution, even if the encoder and decoder networks are allowed to be arbitrarily complex."
    }, {
      "heading" : "5.3. Connection to f-GANs",
      "text" : "Nowozin et al. (Nowozin et al., 2016) proposed to generalize Generative Adversarial Networks (Goodfellow et al., 2014) to f-divergences (Ali & Silvey, 1966) based on results by Nguyen et al. (Nguyen et al., 2010). In this paragraph we show that f-divergences allow to represent AVB as a zero-sum two-player game.\nThe family of f-divergences is given by Df (p‖q) = Epf ( q(x)\np(x)\n) . (5.6)\nfor some convex functional f : R→ R∞ with f(1) = 0.\nNguyen et al. (2010) show that using the convex conjugate f∗ of f (Hiriart-Urruty & Lemaréchal, 2013)\nDf (p‖q) = sup T Eq(x) [T (x)]− Ep(x) [f∗(T (x))] (5.7)\nwhere T is a real-valued function. In particular, this is true for the reverse Kullback-Leibler divergence with f(t) = t log t. We therefore obtain\nKL(q(z | x), p(z)) = Df (p(z), q(z | x)) = sup\nT Eq(z|x)T (x, z)− Ep(z)f∗(T (x, z)) (5.8)\nwith f∗(ξ) = exp(ξ − 1) the convex conjugate of f(t) = t log t.\nAll in all, this yields\nmax θ EpD(x) log pθ(x)\n= max θ,q min T\nEpD(x)Ep(z)f ∗(T (x, z))\n+ EpD(x)Eq(z|x)(log pθ(x | z)− T (x, z)) (5.9)\nBy replacing the objective (3.3) for the discriminator with\nmin T EpD(x)\n[ Ep(z)e T (x,z)−1 − Eq(z|x)T (x, z) ] , (5.10)\nwe can reformulate the maximum-likelihood-problem as a min-max zero-sum game. In fact, the derivations from Section 3 remain valid for any f -divergence that we use to train the discriminator. This is similar to the approach taken by Poole et al. (Poole et al., 2016) to improve the GAN-objective. In practice, we observed that the objective (5.10) results in unstable training. We therefore used the standard GAN-objective (3.3), which corresponds to the Jensen-Shannon-divergence."
    }, {
      "heading" : "5.4. Connection to BiGANs",
      "text" : "BiGANs (Donahue et al., 2016; Dumoulin et al., 2016) are a recent extension of generative adversarial networks with the goal to add an inference network to the generative model. Similarly to our approach, the authors introduce an adversary that acts on pairs (x, z) of data points and latent codes. However, whereas in BiGANs the adversary is used to optimize the generative and inference networks separately, our approach optimizes the generative and inference model jointly. As a result, our approach obtains good reconstructions of the input data, whereas for BiGANs we obtain these reconstruction only indirectly."
    }, {
      "heading" : "6. Conclusion",
      "text" : "We presented a new training procedure for Variational Autoencoders based on adversarial training. This allows us to make the inference model much more flexible, effectively allowing it to represent almost any family of conditional distributions over the latent variables.\nWe believe that further progress can be made by investigating the class of neural network architectures used for the adversary and the encoder and decoder networks."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work was supported by Microsoft Research through its PhD Scholarship Programme."
    } ],
    "references" : [ {
      "title" : "A general class of coefficients of divergence of one distribution from another",
      "author" : [ "Ali", "Syed Mumtaz", "Silvey", "Samuel D" ],
      "venue" : "Journal of the Royal Statistical Society. Series B (Methodological),",
      "citeRegEx" : "Ali et al\\.,? \\Q1966\\E",
      "shortCiteRegEx" : "Ali et al\\.",
      "year" : 1966
    }, {
      "title" : "Density estimation using real nvp",
      "author" : [ "Dinh", "Laurent", "Sohl-Dickstein", "Jascha", "Bengio", "Samy" ],
      "venue" : "arXiv preprint arXiv:1605.08803,",
      "citeRegEx" : "Dinh et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Dinh et al\\.",
      "year" : 2016
    }, {
      "title" : "Adversarial feature learning",
      "author" : [ "Donahue", "Jeff", "Krähenbühl", "Philipp", "Darrell", "Trevor" ],
      "venue" : "arXiv preprint arXiv:1605.09782,",
      "citeRegEx" : "Donahue et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Donahue et al\\.",
      "year" : 2016
    }, {
      "title" : "Adversarially learned inference",
      "author" : [ "Dumoulin", "Vincent", "Belghazi", "Ishmael", "Poole", "Ben", "Lamb", "Alex", "Arjovsky", "Martin", "Mastropietro", "Olivier", "Courville", "Aaron" ],
      "venue" : "arXiv preprint arXiv:1606.00704,",
      "citeRegEx" : "Dumoulin et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Dumoulin et al\\.",
      "year" : 2016
    }, {
      "title" : "Generative adversarial nets",
      "author" : [ "Goodfellow", "Ian", "Pouget-Abadie", "Jean", "Mirza", "Mehdi", "Xu", "Bing", "Warde-Farley", "David", "Ozair", "Sherjil", "Courville", "Aaron", "Bengio", "Yoshua" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian" ],
      "venue" : "arXiv preprint arXiv:1512.03385,",
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Convex analysis and minimization algorithms I: fundamentals, volume 305",
      "author" : [ "Hiriart-Urruty", "Jean-Baptiste", "Lemaréchal", "Claude" ],
      "venue" : "Springer science & business media,",
      "citeRegEx" : "Hiriart.Urruty et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Hiriart.Urruty et al\\.",
      "year" : 2013
    }, {
      "title" : "Multilayer feedforward networks are universal approximators",
      "author" : [ "Hornik", "Kurt", "Stinchcombe", "Maxwell", "White", "Halbert" ],
      "venue" : "Neural networks,",
      "citeRegEx" : "Hornik et al\\.,? \\Q1989\\E",
      "shortCiteRegEx" : "Hornik et al\\.",
      "year" : 1989
    }, {
      "title" : "Auto-encoding variational bayes",
      "author" : [ "Kingma", "Diederik P", "Welling", "Max" ],
      "venue" : "arXiv preprint arXiv:1312.6114,",
      "citeRegEx" : "Kingma et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2013
    }, {
      "title" : "Improving variational inference with inverse autoregressive flow",
      "author" : [ "Kingma", "Diederik P", "Salimans", "Tim", "Welling", "Max" ],
      "venue" : "arXiv preprint arXiv:1606.04934,",
      "citeRegEx" : "Kingma et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2016
    }, {
      "title" : "Autoencoding beyond pixels using a learned similarity metric",
      "author" : [ "Larsen", "Anders Boesen Lindbo", "Sønderby", "Søren Kaae", "Winther", "Ole" ],
      "venue" : "arXiv preprint arXiv:1512.09300,",
      "citeRegEx" : "Larsen et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Larsen et al\\.",
      "year" : 2015
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "LeCun", "Yann", "Bottou", "Léon", "Bengio", "Yoshua", "Haffner", "Patrick" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "LeCun et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1998
    }, {
      "title" : "Deep learning face attributes in the wild",
      "author" : [ "Liu", "Ziwei", "Luo", "Ping", "Wang", "Xiaogang", "Tang", "Xiaoou" ],
      "venue" : "In Proceedings of International Conference on Computer Vision (ICCV),",
      "citeRegEx" : "Liu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2015
    }, {
      "title" : "Auxiliary deep generative models",
      "author" : [ "Maaløe", "Lars", "Sønderby", "Casper Kaae", "Søren Kaae", "Winther", "Ole" ],
      "venue" : "arXiv preprint arXiv:1602.05473,",
      "citeRegEx" : "Maaløe et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Maaløe et al\\.",
      "year" : 2016
    }, {
      "title" : "Plug & play generative networks: Conditional iterative generation of images in latent space",
      "author" : [ "Nguyen", "Anh", "Yosinski", "Jason", "Bengio", "Yoshua", "Dosovitskiy", "Alexey", "Clune", "Jeff" ],
      "venue" : "arXiv preprint arXiv:1612.00005,",
      "citeRegEx" : "Nguyen et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2016
    }, {
      "title" : "Estimating divergence functionals and the likelihood ratio by convex risk minimization",
      "author" : [ "Nguyen", "XuanLong", "Wainwright", "Martin J", "Jordan", "Michael I" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Nguyen et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2010
    }, {
      "title" : "f-gan: Training generative neural samplers using variational divergence minimization",
      "author" : [ "Nowozin", "Sebastian", "Cseke", "Botond", "Tomioka", "Ryota" ],
      "venue" : "arXiv preprint arXiv:1606.00709,",
      "citeRegEx" : "Nowozin et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Nowozin et al\\.",
      "year" : 2016
    }, {
      "title" : "Improved generator objectives for gans",
      "author" : [ "Poole", "Ben", "Alemi", "Alexander A", "Sohl-Dickstein", "Jascha", "Angelova", "Anelia" ],
      "venue" : "arXiv preprint arXiv:1612.02780,",
      "citeRegEx" : "Poole et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Poole et al\\.",
      "year" : 2016
    }, {
      "title" : "Unsupervised representation learning with deep convolutional generative adversarial networks",
      "author" : [ "Radford", "Alec", "Metz", "Luke", "Chintala", "Soumith" ],
      "venue" : "arXiv preprint arXiv:1511.06434,",
      "citeRegEx" : "Radford et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2015
    }, {
      "title" : "Variational inference with normalizing flows",
      "author" : [ "Rezende", "Danilo Jimenez", "Mohamed", "Shakir" ],
      "venue" : "arXiv preprint arXiv:1505.05770,",
      "citeRegEx" : "Rezende et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Rezende et al\\.",
      "year" : 2015
    }, {
      "title" : "Stochastic backpropagation and approximate inference in deep generative models",
      "author" : [ "Rezende", "Danilo Jimenez", "Mohamed", "Shakir", "Wierstra", "Daan" ],
      "venue" : "arXiv preprint arXiv:1401.4082,",
      "citeRegEx" : "Rezende et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Rezende et al\\.",
      "year" : 2014
    }, {
      "title" : "Information theoretical estimators (ite) toolbox",
      "author" : [ "Szabo", "Zoltán" ],
      "venue" : null,
      "citeRegEx" : "Szabo and Zoltán.,? \\Q2013\\E",
      "shortCiteRegEx" : "Szabo and Zoltán.",
      "year" : 2013
    }, {
      "title" : "Conditional image generation with pixelcnn decoders",
      "author" : [ "van den Oord", "Aaron", "Kalchbrenner", "Nal", "Espeholt", "Lasse", "Vinyals", "Oriol", "Graves", "Alex" ],
      "venue" : "In Advances In Neural Information Processing Systems,",
      "citeRegEx" : "Oord et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Oord et al\\.",
      "year" : 2016
    }, {
      "title" : "Pixel recurrent neural networks",
      "author" : [ "van den Oord", "Aaron van den", "Kalchbrenner", "Nal", "Kavukcuoglu", "Koray" ],
      "venue" : "arXiv preprint arXiv:1601.06759,",
      "citeRegEx" : "Oord et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Oord et al\\.",
      "year" : 2016
    }, {
      "title" : "On the quantitative analysis of decoder-based generative models",
      "author" : [ "Adversarial Variational Bayes Wu", "Yuhuai", "Burda", "Yuri", "Salakhutdinov", "Ruslan", "Grosse", "Roger" ],
      "venue" : "arXiv preprint arXiv:1611.04273,",
      "citeRegEx" : "Wu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : ", 2016a), real NVP (Dinh et al., 2016) and Plug & Play generative networks (Nguyen et al.",
      "startOffset" : 19,
      "endOffset" : 38
    }, {
      "referenceID" : 14,
      "context" : ", 2016) and Plug & Play generative networks (Nguyen et al., 2016) have been introduced and studied, the two most prominent ones are Variational Autoencoders (VAEs) (Kingma & Welling, 2013; Rezende et al.",
      "startOffset" : 44,
      "endOffset" : 65
    }, {
      "referenceID" : 20,
      "context" : ", 2016) have been introduced and studied, the two most prominent ones are Variational Autoencoders (VAEs) (Kingma & Welling, 2013; Rezende et al., 2014) and Generative Adversarial Networks (GANs) (Goodfellow et al.",
      "startOffset" : 106,
      "endOffset" : 152
    }, {
      "referenceID" : 4,
      "context" : ", 2014) and Generative Adversarial Networks (GANs) (Goodfellow et al., 2014).",
      "startOffset" : 51,
      "endOffset" : 76
    }, {
      "referenceID" : 24,
      "context" : "Moreover, it was reported, that VAEs often lead to better log-likelihoods (Wu et al., 2016), making them interesting for scenarios where visual realism is not the main goal.",
      "startOffset" : 74,
      "endOffset" : 91
    }, {
      "referenceID" : 2,
      "context" : "The recently introduced BiGANs (Donahue et al., 2016; Dumoulin et al., 2016) add an inference model to GANs.",
      "startOffset" : 31,
      "endOffset" : 76
    }, {
      "referenceID" : 3,
      "context" : "The recently introduced BiGANs (Donahue et al., 2016; Dumoulin et al., 2016) add an inference model to GANs.",
      "startOffset" : 31,
      "endOffset" : 76
    }, {
      "referenceID" : 9,
      "context" : "Indeed, recent work shows that using more expressive model classes can lead to substantially better results (Kingma et al., 2016), both visually and in terms of log-likelihood-bounds.",
      "startOffset" : 108,
      "endOffset" : 129
    }, {
      "referenceID" : 10,
      "context" : "While there were some attempts at combining VAEs and GANs (Makhzani et al., 2015; Larsen et al., 2015), most of these attempts are not motivated from a maximumlikelihood point of view and therefore usually do not lead to maximum-likelihood assignments.",
      "startOffset" : 58,
      "endOffset" : 102
    }, {
      "referenceID" : 20,
      "context" : "As our model is an extension of Variational Autoencoders (VAEs) (Kingma & Welling, 2013; Rezende et al., 2014), we start with a brief review of VAEs.",
      "startOffset" : 64,
      "endOffset" : 110
    }, {
      "referenceID" : 20,
      "context" : "Usually, qφ(z | x) is taken to be a Gaussian distribution with diagonal covariance matrix whose mean and variance vectors are parameterized by neural networks with x as input (Kingma & Welling, 2013; Rezende et al., 2014).",
      "startOffset" : 175,
      "endOffset" : 221
    }, {
      "referenceID" : 10,
      "context" : "Indeed, it was observed that applying standard Variational Autoencoders to natural images often results in blurry images (Larsen et al., 2015).",
      "startOffset" : 121,
      "endOffset" : 142
    }, {
      "referenceID" : 4,
      "context" : "This assumption is often referred to as the nonparametric limit (Goodfellow et al., 2014) and is justified by the fact that deep neural networks are universal function approximators (Hornik et al.",
      "startOffset" : 64,
      "endOffset" : 89
    }, {
      "referenceID" : 7,
      "context" : ", 2014) and is justified by the fact that deep neural networks are universal function approximators (Hornik et al., 1989).",
      "startOffset" : 100,
      "endOffset" : 121
    }, {
      "referenceID" : 4,
      "context" : "The proof is analogous to the proof of Proposition 1 in Goodfellow et al. (2014). See the Appendix for details.",
      "startOffset" : 56,
      "endOffset" : 81
    }, {
      "referenceID" : 20,
      "context" : "Using the reparameterization trick (Kingma & Welling, 2013; Rezende et al., 2014), (3.",
      "startOffset" : 35,
      "endOffset" : 81
    }, {
      "referenceID" : 18,
      "context" : "This is also a common problem for regular GANs (Radford et al., 2015).",
      "startOffset" : 47,
      "endOffset" : 69
    }, {
      "referenceID" : 18,
      "context" : "The CNN implementation is based on Radford et al. (2015) who show that deep convolutional networks can be used to parameterize the generator and discriminator for GANs.",
      "startOffset" : 35,
      "endOffset" : 57
    }, {
      "referenceID" : 18,
      "context" : "MNIST In addition, we trained deep convolutional networks based on the DC-GAN-architecture (Radford et al., 2015) on the MNIST-dataset (LeCun et al.",
      "startOffset" : 91,
      "endOffset" : 113
    }, {
      "referenceID" : 11,
      "context" : ", 2015) on the MNIST-dataset (LeCun et al., 1998).",
      "startOffset" : 29,
      "endOffset" : 49
    }, {
      "referenceID" : 12,
      "context" : "celebA We also trained a deep convolutional network on the celebA-dataset (Liu et al., 2015).",
      "startOffset" : 74,
      "endOffset" : 92
    }, {
      "referenceID" : 5,
      "context" : "Moreover, in the encoder and decoder we replace every hidden layer by three RESNETunits (He et al., 2015).",
      "startOffset" : 88,
      "endOffset" : 105
    }, {
      "referenceID" : 20,
      "context" : "Connection to Variational Autoencoders Adversarial Variational Bayes strives to optimize the same objective as a standard Variational Autoencoder (Kingma & Welling, 2013; Rezende et al., 2014), but approximates the Kullback-Leibler divergence using an adversary instead of relying on a closed-form formula.",
      "startOffset" : 146,
      "endOffset" : 192
    }, {
      "referenceID" : 9,
      "context" : "Normalizing flows (Rezende & Mohamed, 2015; Kingma et al., 2016) make the posterior more complex by composing a simple Gaussian posterior with an invertible smooth mapping for which the determinant of the Jacobian is tractable.",
      "startOffset" : 18,
      "endOffset" : 64
    }, {
      "referenceID" : 13,
      "context" : "Auxiliary Variable VAEs (Maaløe et al., 2016) add auxiliary variables to the posterior to make it more flexible.",
      "startOffset" : 24,
      "endOffset" : 45
    }, {
      "referenceID" : 7,
      "context" : "As neural network are universal function approximators (Hornik et al., 1989), our approach recovers the true posterior in the nonparametric limit.",
      "startOffset" : 55,
      "endOffset" : 76
    }, {
      "referenceID" : 16,
      "context" : "(Nowozin et al., 2016) proposed to generalize Generative Adversarial Networks (Goodfellow et al.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 4,
      "context" : ", 2016) proposed to generalize Generative Adversarial Networks (Goodfellow et al., 2014) to f-divergences (Ali & Silvey, 1966) based on results by Nguyen et al.",
      "startOffset" : 63,
      "endOffset" : 88
    }, {
      "referenceID" : 15,
      "context" : "(Nguyen et al., 2010).",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 14,
      "context" : "Nguyen et al. (2010) show that using the convex conjugate f∗ of f (Hiriart-Urruty & Lemaréchal, 2013) Df (p‖q) = sup T Eq(x) [T (x)]− Ep(x) [f∗(T (x))] (5.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 17,
      "context" : "(Poole et al., 2016) to improve the GAN-objective.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 2,
      "context" : "BiGANs (Donahue et al., 2016; Dumoulin et al., 2016) are a recent extension of generative adversarial networks with the goal to add an inference network to the generative model.",
      "startOffset" : 7,
      "endOffset" : 52
    }, {
      "referenceID" : 3,
      "context" : "BiGANs (Donahue et al., 2016; Dumoulin et al., 2016) are a recent extension of generative adversarial networks with the goal to add an inference network to the generative model.",
      "startOffset" : 7,
      "endOffset" : 52
    } ],
    "year" : 2017,
    "abstractText" : "Variational Autoencoders (VAEs) are expressive latent variable models that can be used to learn complex probability distributions from training data. However, the quality of the resulting model crucially relies on the expressiveness of the inference model used during training. We introduce Adversarial Variational Bayes (AVB), a technique for training Variational Autoencoders with arbitrarily expressive inference models. We achieve this by introducing an auxiliary discriminative network that allows to rephrase the maximum-likelihood-problem as a two-player game, hence establishing a principled connection between VAEs and Generative Adversarial Networks (GANs). We show that in the nonparametric limit our method yields an exact maximumlikelihood assignment for the parameters of the generative model, as well as the exact posterior distribution over the latent variables given an observation. Contrary to competing approaches which combine VAEs with GANs, our approach has a clear theoretical justification, retains most advantages of standard Variational Autoencoders and is easy to implement.",
    "creator" : "LaTeX with hyperref package"
  }
}