{
  "name" : "1210.2771.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Cost-Sensitive Tree of Classifiers",
    "authors" : [ "Zhixiang (Eddie", "Matt J. Kusner", "Kilian Q. Weinberger" ],
    "emails" : [ "xuzx@cse.wustl.edu", "mkusner@wustl.edu", "kilian@wustl.edu", "mchen@wustl.edu" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Machine learning algorithms are widely used in many real-world applications, ranging from emailspam (Weinberger et al., 2009) and adult content filtering (Fleck et al., 1996), to web-search engines (Zheng et al., 2008). As machine learning transitions into these industry fields, managing the CPU cost at testtime becomes increasingly important. In applications of such large scale, computation must be budgeted and accounted for. Moreover, reducing energy wasted on unnecessary computation can lead to monetary sav-\nings and reductions of greenhouse gas emissions.\nThe test-time cost consists of the time required to evaluate a classifier and the time to extract features for that classifier, where the extraction time across features is highly variable. Imagine introducing a new feature to an email spam filtering algorithm that requires 0.01 seconds to extract per incoming email. If a web-service receives one billion emails (which many do daily), it would require 115 extra CPU days to extract just this feature. Although this additional feature may increase the accuracy of the filter, the cost of computing it for every email is prohibitive. This introduces the problem of balancing the test-time cost and the classifier accuracy. Addressing this trade-off in a principled manner is crucial for the applicability of machine learning.\nIn this paper, we propose a novel algorithm, CostSensitive Tree of Classifiers (CSTC). A CSTC tree (illustrated schematically in Fig. 1) is a tree of classifiers that is carefully constructed to reduce the average testtime complexity of machine learning algorithms, while maximizing their accuracy. Different from prior work, which reduces the total cost for every input (Efron et al., 2004) or which stages the feature extraction into linear cascades (Viola & Jones, 2004; Lefakis & Fleuret, 2010; Saberian & Vasconcelos, 2010; Pujara et al., 2011; Chen et al., 2012), a CSTC tree incorporates input-dependent feature selection into training and dynamically allocates higher feature budgets for infrequently traveled tree-paths. By introducing a probabilistic tree-traversal framework, we can compute the exact expected test-time cost of a CSTC tree. CSTC is trained with a single global loss function, whose test-time cost penalty is a direct relaxation of this expected cost. This principled approach leads to unmatched test-cost/accuracy tradeoffs as it naturally divides the input space into sub-regions and extracts expensive features only when necessary.\nWe make several novel contributions: 1. We introduce\nar X\niv :1\n21 0.\n27 71\nv3 [\nst at\n.M L\n] 2\n2 A\npr 2\n01 3\nthe meta-learning framework of CSTC trees and derive the expected cost of an input traversing the tree during test-time. 2. We relax this expected cost with a mixed-norm relaxation and derive a single global optimization problem to train all classifiers jointly. 3. We demonstrate on synthetic data that CSTC effectively allocates features to classifiers where they are most beneficial and show on large-scale real-world websearch ranking data that CSTC significantly outperforms the current state-of-the-art in test-time costsensitive learning—maintaining the performance of the best algorithms for web-search ranking at a fraction of their computational cost."
    }, {
      "heading" : "2. Related Work",
      "text" : "A basic approach to control test-time cost is the use of l1-norm regularization (Efron et al., 2004), which results in a sparse feature set, and can significantly reduce the feature cost during test-time (as unused features are never computed). However, this approach fails to address the fact that some inputs may be successfully classified by only a few cheap features, whereas others strictly require expensive features for correct classification.\nThere is much previous work that extends single classifiers to classifier cascades (mostly for binary classification) (Viola & Jones, 2004; Lefakis & Fleuret, 2010; Saberian & Vasconcelos, 2010; Pujara et al., 2011; Chen et al., 2012). In these cascades, several classifiers are ordered into a sequence of stages. Each classifier can either reject inputs (predicting them), or pass them on to the next stage, based on the prediction of each input. To reduce the test-time cost, these cascade algorithms enforce that classifiers in early stages use very few and/or cheap features and reject many easily-classified inputs. Classifiers in later stages, however, are more expensive and cope with more difficult inputs. This linear structure is particularly effective for applications with highly skewed class imbalance and generic features. One celebrated example is face detection in images, where the majority of all image regions do not contain faces and can often be easily rejected based on the response of a few simple Haar features (Viola & Jones, 2004). The linear cascade model is however less suited for learning tasks with balanced classes and specialized features. It cannot fully capture the scenario where different partitions of the input space require different expert features, as all inputs follow the same linear chain.\nGrubb & Bagnell (2012) and Xu et al. (2012) focus on training a classifier that explicitly trades-off test-time cost and accuracy. Instead of optimizing the trade-\noff by building a cascade, they push the cost trade-off into the construction of the weak learners. It should be noted that, in spite of the high accuracy achieved by these techniques, the algorithms are based heavily on stage-wise regression (gradient boosting) (Friedman, 2001), and are less likely to work with more general weak learners.\nGao & Koller (2011) use locally weighted regression during test time to predict the information gain of unknown features. Different from our algorithm, their model is learned during test-time, which introduces an additional cost especially for large data sets. In contrast, our algorithm learns and fixes a tree structure in training and has a test-time complexity that is constant with respect to the training set size.\nKarayev et al. (2012) use reinforcement learning to dynamically select features to maximize the average precision over time in an object detection setting. In this case, the dataset has multi-labeled inputs and thus warrants a different approach than ours.\nHierarchical Mixture of Experts (HME) (Jordan & Jacobs, 1994) also builds tree-structured classifiers. However, in contrast to CSTC, this work is not motivated by reductions in test-time cost and results in fundamentally different models. In CSTC, each classifier is trained with the test-time cost in mind and each test-input only traverses a single path from the root down to a terminal element, accumulating pathspecific costs. In HME, all test-inputs traverse all paths and all leaf-classifiers contribute to the final prediction, incurring the same cost for all test-inputs.\nRecent tree-structured classifiers include the work of Deng et al. (2011), who speed up the training and evaluation of label trees (Bengio et al., 2010), by avoiding many binary one-vs-all classifier evaluations. Differently, we focus on problems in which feature extraction time dominates the test-time cost which motivates different algorithmic setups. Dredze et al. (2007) combine the cost to select a feature with the mutual information of that feature to build a decision tree that reduces the feature extraction cost. Different from this work, they do not directly minimize the total test-time cost of the decision tree or the risk. Possibly most similar to our work are (Busa-Fekete et al., 2012), who learn a directed acyclic graph via a Markov decision process to select features for different instances, and (Wang & Saligrama, 2012), who adaptively partition the feature space and learn local region-specific classifiers. Although each work is similar in motivation, the algorithmic frameworks are very different and can be regarded complementary to ours."
    }, {
      "heading" : "3. Cost-sensitive classification",
      "text" : "We first introduce our notation and then formalize our test-time cost-sensitive learning setting. Let the training data consist of inputs D={x1, . . . ,xn} ⊂ Rd with corresponding class labels {y1, . . . , yn} ⊆ Y, where Y = R in the case of regression (Y could also be a finite set of categorical labels—because of space limitations we do not focus on this case in this paper).\nNon-linear feature space. Throughout this paper, we focus on linear classifiers but in order to allow non-linear decision boundaries we map the input into a non-linear feature space with the “boosting trick” (Friedman, 2001; Chapelle et al., 2011), prior to our optimization. In particular, we first train gradient boosted regression trees with a squared loss penalty (Friedman, 2001), H ′(xi) = ∑T t=1 ht(xi), where each function ht(·) is a limited-depth CART tree (Breiman, 1984). We then apply the mapping xi → φ(xi) to all inputs, where φ(xi) = [h1(xi), . . . , hT (xi)]\n>. To avoid confusion between CART trees and the CSTC tree, we refer to CART trees ht(·) as weak learners. Risk minimization. At each node in the CSTC tree we propose to learn a linear classifier in this feature space, H(xi) = φ(xi)\n>β with β ∈ RT , which is trained to explicitly reduce the CPU cost during testtime. We learn the weight-vector β by minimizing a convex empirical risk function `(φ(xi)\n>β, yi) with l1 regularization, |β|. In addition, we incorporate a cost term c(β), which we derive in the following subsection, to restrict test-time cost. The combined test-time costsensitive loss function becomes\nL(β) = ∑\ni\n`(φ(xi) >β, yi) + ρ|β|\n︸ ︷︷ ︸ regularized risk\n+ λ c(β)︸︷︷︸ test-cost , (1)\nwhere λ is the accuracy/cost trade-off parameter, and ρ controls the strength of the regularization.\nTest-time cost. There are two factors that contribute to the test-time cost of each classifier. The weak learner evaluation cost of all active ht(·) (with |βt|>0) and the feature extraction cost for all features used in these weak learners. We assume that features are computed on demand with the cost c the first time they are used, and are free for future use (as feature values can be cached). We define an auxiliary matrix F ∈ {0, 1}d×T with Fαt = 1 if and only if the weak learner ht uses feature fα. Let et > 0 be the cost to evaluate a ht(·), and cα be the cost to extract feature fα. With this notation, we can formulate the total\ntest-time cost for an instance precisely as\nc(β) = ∑\nt et‖βt‖0 ︸ ︷︷ ︸ evaluation cost\n+ ∑\nα\ncα ∥∥∥∥∥ ∑\nt\n|Fαtβt| ∥∥∥∥∥\n0︸ ︷︷ ︸ feature extraction cost\n, (2)\nwhere the l0 norm for scalars is defined as ‖a‖0∈{0, 1} with ‖a‖0 =1 if and only if a 6=0. The first term assigns cost et to every weak learner used in β, the second term assigns cost cα to every feature that is extracted by at least one of such weak learners.\nTest-cost relaxation. The cost formulation in (2) is exact but difficult to optimize as the l0 norms are non-continuous and non-differentiable. As a solution, throughout this paper we use the mixed-norm relaxation of the l0 norm over sums,\n∑\nj\n∥∥∥∥∥ ∑\ni\n|aij | ∥∥∥∥∥\n0\n→ ∑\nj\n√∑\ni\n(aij)2, (3)\ndescribed by (Kowalski, 2009). Note that for a single element this relaxation relaxes the l0 norm to the l1 norm, ‖aij‖0 → √ (aij)2 = |aij |, and recovers the commonly used approximation to encourage sparsity (Efron et al., 2004; Schölkopf & Smola, 2001). We plug the cost-term (2) into the loss in (1) and apply the relaxation (3) to all l0 norms to obtain\n∑\ni `i+ρ|β| ︸ ︷︷ ︸\nregularized loss\n+λ\n( ∑\nt et|βt| ︸ ︷︷ ︸\nev. cost penalty\n+ ∑\nα\ncα\n√∑\nt\n(Fαtβt)2\n︸ ︷︷ ︸ feature cost penalty\n) ,\n(4)\nwhere we abbreviate `i=`(φ(xi) >β, yi) for simplicity. While (4) is cost-sensitive, it is restricted to a single linear classifier. In the next section we describe how to expand this formulation into a cost-effective treestructured model."
    }, {
      "heading" : "4. Cost-sensitive tree",
      "text" : "We begin by introducing foundational concepts regarding the CSTC tree and derive a global loss function (5). Similar to the previous section, we first derive the exact cost term and then relax it with the mixed-norm. Finally, we describe how to optimize this function efficiently and to undo some of the inaccuracy induced by the mixed-norm relaxations.\nCSTC nodes. We make the assumption that instances with similar labels can utilize similar features.1\n1For example, in web-search ranking, features generated by browser statistics are typically predictive only for highly relevant pages as they require the user to spend significant time on the page and interact with it.\nmin β1,θ1,...,β|V |,θ|V |\n∑\nvk∈V\n( 1\nn\nn∑\ni=1\npki ` k i +ρ|βk|\n)\n︸ ︷︷ ︸ regularized risk\n+λ ∑\nvl∈L pl\n[ ∑\nt\net\n√∑\nvj∈πl (βjt ) 2\n︸ ︷︷ ︸ evaluation cost penalty\n+ ∑\nα\ncα\n√∑\nvj∈πl\n∑\nt\n(Fαtβ j t ) 2\n︸ ︷︷ ︸ feature cost penalty\n] (5)\nWe therefore design our tree algorithm to partition the input space based on classifier predictions. Classifiers that reside deep in the tree become experts for a small subset of the input space and intermediate classifiers determine the path of instances through the tree. We distinguish between two different elements in a CSTC tree (depicted in Figure 1): classifier nodes (white circles) and terminal elements (black squares). Each classifier node vk is associated with a weight vector βk and a threshold θk. Different from cascade approaches, these classifiers not only classify inputs using βk, but also branch them by their threshold θk, sending inputs to their upper child if φ(xi)\n>βk > θk, and to their lower child otherwise. Terminal elements are “dummy” structures and are not classifiers. They return the predictions of their direct parent classifier nodes—essentially functioning as a placeholder for an exit out of the tree. The tree structure may be a full balanced binary tree of some depth (eg. figure 1), or can be pruned based on a validation set (eg. figure 4, left).\nDuring test-time, inputs are first applied to the root node v0. The root node produces predictions φ(xi) >β0 and sends the input xi along one of two different paths,\ndepending on whether φ(xi) >β0 > θ0. By repeatedly branching the test-inputs, classifier nodes sitting deeper in the tree only handle a small subset of all inputs and become specialized towards that subset of the input space."
    }, {
      "heading" : "4.1. Tree loss",
      "text" : "We derive a single global loss function over all nodes in the CSTC tree.\nSoft tree traversal. Training the CSTC tree with hard thresholds leads to a combinatorial optimization problem, which is NP-hard. Therefore, during training, we softly partition the inputs and assign traversal probabilities p(vk|xi) to denote the likelihood of input xi traversing through node v\nk. Every input xi traverses through the root, so we define p(v0|xi) = 1 for all i. We use the sigmoid function to define a soft belief that an input xi will transition from classifier node vk to its upper child vj as p(vj |xi, vk) = σ(φ(xi)>βk − θk).2 The probability of reaching child vj from the root is, recursively, p(vj |xi) = p(vj |xi, vk)p(vk|xi), because each node has exactly one parent. For a lower child vl of parent vk we naturally obtain p(vl|xi) = [ 1 − p(vj |xi, vk) ] p(vk|xi). In the following paragraphs we incorporate this probabilistic framework into the single-node risk and cost terms of eq. (4) to obtain the corresponding expected tree risk and tree cost.\nExpected tree risk. The expected tree risk can be obtained byWg over all nodes V and inputs and weighing the risk `(·) of input xi at node vk by the probability pki =p(v k|xi),\n1\nn\nn∑\ni=1\n∑\nvk∈V pki `(φ(xi) >βk, yi). (6)\nThis has two effects: 1. the local risk for each node focusses more on likely inputs; 2. the global risk attributes more weight to classifiers that serve many inputs.\nExpected tree costs. The cost of a test-input is the cumulative cost across all classifiers along its path through the CSTC tree. Figure 1 illustrates an exam-\n2The sigmoid function is defined as σ(a) = 1 1+exp(−a) and takes advantage of the fact that σ(a) ∈ [0, 1] and that σ(·) is strictly monotonic.\nple of a CSTC tree with all paths highlighted in color. Every test-input must follow along exactly one of the paths from the root to a terminal element. Let L denote the set of all terminal elements (e.g., in figure 1 we have L={v7, v8, v9, v10}), and for any vl∈L let πl denote the set of all classifier nodes along the unique path from the root v0 before terminal element vl (e.g., π9 = {v0, v2, v5}). The evaluation and feature cost of this unique path is exactly\ncl= ∑\nt\net ∥∥∥∥∥ ∑\nvj∈πl |βjt | ∥∥∥∥∥ 0︸ ︷︷ ︸\nevaluation cost\n+ ∑\nα\ncα ∥∥∥∥∥ ∑\nvj∈πl\n∑\nt\n|Fαtβjt | ∥∥∥∥∥\n0︸ ︷︷ ︸ feature cost\n.\nThis term is analogous to eq. (2), except the cost et of the weak learner ht is paid if any of the classifiers v\nj in path πl use this tree (i.e. assign βjt non-zero weight). Similarly, the cost cα of a feature fα is paid exactly once if any of the weak learners of any of the classifiers along πl require it. Once computed, a feature or weak learner can be reused by all classifiers along the path for free (as the computation can be cached very efficiently).\nGiven an input xi, the probability of reaching terminal element vl ∈ L (traversing along path πl) is pli = p(v\nl|xi). Therefore, the marginal probability that a training input (picked uniformly at random from the training set) reaches vl is pl = ∑ i p(v\nl|xi)p(xi) = 1 n ∑n i=1 p l i. With this notation, the expected cost for an input traversing the CSTC tree becomes E[cl] =∑ vl∈L p lcl. Using our l0-norm relaxation in eq. (3) on both l0 norms in c l gives the final expected tree cost penalty\n∑ vl∈L pl\n[∑\nt\net\n√∑\nvj∈πl (βjt )\n2 + ∑\nα\ncα\n√∑\nvj∈πl\n∑\nt\n(Fαtβ j t ) 2\n] ,\nwhich naturally encourages weak learner and feature re-use along paths through the CSTC tree.\nOptimization problem. We combine the risk (6) with the cost penalties and add the l1-regularization term (which is unaffected by our probabilistic splitting) to obtain the global optimization problem (5). (We abbreviate the empiWisk at node vk as `ki = `(φ(xi) >βk, yi).)"
    }, {
      "heading" : "4.2. Optimization Details",
      "text" : "There are many techniques to minimize the loss in (5). We use a cyclic optimization procedure, solving\n∂L ∂(βk,θk) for each classifier node vk one at a time, keeping all other nodes fixed. For a given classifier node vk, the traversal probabilities pji of a descendant node\nvj and the probability of an instance reaching a terminal element pl also depend on βk and θk (through its recursive definition) and must be incorporated into the gradient computation.\nTo minimize (5) with respect to parameters βk, θk, we use the lemma below to overcome the nondifferentiability of the square-root terms (and l1 norms) resulting from the l0-relaxations (3).\nLemma 1. Given any g(x) > 0, the following holds:\n√ g(x) = min\nz>0\n1\n2\n[ g(x)\nz + z\n] . (7)\nThe lemma can be proved as z = √ g(x) minimizes the function on the right hand side. Further, it is shown in (Boyd & Vandenberghe, 2004) that the right hand side is jointly convex in x and z, so long as g(x) is convex.\nFor each square-root or l1 term we introduce an auxiliary variable (i.e., z above) and alternate between minimizing the loss in (5) with respect to βk, θk and the auxiliary variables. The former is performed with conjugate gradient descent and the latter can be computed efficiently in closed form. This pattern of blockcoordinate descent followed by a closed form minimization is repeated until convergence. Note that the loss is guaranteed to converge to a fixed point because each iteration decreases the loss function, which is bounded below by 0.\nInitialization. The minimization of eq. (5) is nonconvex and therefore initialization dependent. However, minimizing eq. (5) with respect to the parameters of leaf classifier nodes is convex, as the loss function, after substitutions based on lemma 1, becomes jointly convex (because of the lack of descendant nodes). We therefore initialize the tree top-to-bottom, starting at v0, and optimize over βk by minimizing (5) while considering all descendant nodes of vk as “cut-off” (thus pretending node vk is a leaf).\nTree pruning. To obtain a more compact model and to avoid overfitting, the CSTC tree can be pruned with the help of a validation set. As each node is a classifier, we can apply the CSTC tree on a validation set and compute the validation error at each node. We prune away nodes that, upon removal, do not decrease the performance of CSTC on the validation set (in the case of ranking data, we even can use validation NDCG as our pruning criterion).\nFine-tuning. The relaxation in (3) makes the exact l0 cost terms differentiable and is well suited to approximate which dimensions in a vector βk should be\nassigned non-zero weights. The mixed-norm does however impact the performance of the classifiers because (different from the l0 norm) larger weights in β incur larger penalties in the loss. We therefore introduce a post-processing step to correct the classifiers from this unwanted regularization effect. We re-optimize all predictive classifiers (classifiers with terminal element children, i.e. classifiers that make final predictions), while clamping all features with zero-weight to strictly remain zero.\nmin β̄k\n∑\ni\npki `(φ(xi) >β̄ k , yi) + ρ|β̄k|\nsubject to: β̄kt = 0 if β k t = 0. (8)\nThe final CSTC tree uses these re-optimized weight vectors β̄ k for all predictive classifier nodes vk."
    }, {
      "heading" : "5. Results",
      "text" : "In this section, we first evaluate CSTC on a carefully constructed synthetic data set to test our hypothesis that CSTC learns specialized classifiers that rely on different feature subsets. We then evaluate the performance of CSTC on the large scale Yahoo! Learning to Rank Challenge data set and compare it with state-ofthe-art algorithms."
    }, {
      "heading" : "5.1. Synthetic data",
      "text" : "We construct a synthetic regression dataset, sampled from the four quadrants of the X,Z-plane, where X = Z = [−1, 1]. The features belong to two categories: cheap features, sign(x), sign(z) with cost c=1, which can be used to identify the quadrant of an input; and four expensive features y++, y+−, y−+, y−− with cost c = 10, which represent the exact label of an input if it is from the corresponding region (a random number otherwise). Since in this synthetic data set we do not transform the feature space, we have φ(x) =x, and F (the weak learner feature-usage variable) is the 6×6 identity matrix. By design, a perfect classifier can use the two cheap features to identify the sub-region of an instance and then extract the correct expensive feature to make a perfect prediction. The minimum feature cost of such a perfect classifier is exactly c=12 per instance. The labels are sampled from Gaussian distributions with quadrant-specific means µ++, µ−+, µ+−, µ−− and variance 1. Figure 2 shows the CSTC tree and the predictions of test inputs made by each node. In every path along the tree, the first two classifiers split on the two cheap features and identify the correct sub-region of the input. The final classifier extracts a single expensive feature to predict the labels. As such, the mean squared error of the training and testing data both approach 0."
    }, {
      "heading" : "5.2. Yahoo! Learning to Rank",
      "text" : "To evaluate the performance of CSTC on real-world tasks, we test our algorithm on the public Yahoo! Learning to Rank Challenge data set3 (Chapelle & Chang, 2011). The set contains 19,944 queries and 473,134 documents. Each query-document pair xi consists of 519 features. An extraction cost, which takes on a value in the set {1, 5, 20, 50, 100, 150, 200}, is associated with each feature4. The unit of these values is the time required to evaluate a weak learner ht(·). The label yi ∈ {4, 3, 2, 1, 0} denotes the relevancy of a document to its corresponding query, with 4 indicating a perfect match. In contrast to Chen et al. (2012), we do not inflate the number of irrelevant documents (by counting them 10 times). We measure the performance using NDCG@5 (Järvelin & Kekäläinen, 2002), a preferred ranking metric when multiple levels of relevance are available. Unless otherwise stated, we restrict CSTC to a maximum of 10 nodes. All results are obtained on a desktop with two 6-core Intel i7 CPUs. Minimizing the global objective requires less than 3 hours to complete, and fine-tuning the classifiers takes about 10 minutes.\nComparison with prior work. Figure 3 shows a comparison of CSTC with several recent algorithms for test-time cost-sensitive learning. We show NDCG\n3http://learningtorankchallenge.yahoo.com 4The extraction costs were provided by a Yahoo! em-\nployee.\nversus cost (in units of weak learner evaluations). The plot shows different stages in our derivation of CSTC: the initial cost-insensitive ensemble classifier H ′(·) (Friedman, 2001) from section 3 (stage-wise regression), a single cost-sensitive classifier as described in eq. (4), the CSTC tree (5) and CSTC tree with fine-tuning (8). We obtain the curves by varying the accuracy/cost trade-off parameter λ (and perform early stopping based on the validation data, for finetuning). For CSTC tree we evaluate six settings, λ= { 13 , 12 , 1, 2, 3, 4, 5, 6}. In the case of stage-wise regression, which is not cost-sensitive, the curve is simply a function of boosting iterations.\nFor competing algorithms, we include Early exit (Cambazoglu et al., 2010) which improves upon stagewise regression by short-circuiting the evaluation of unpromising documents at test-time, reducing the overall test-time cost. The authors propose several criteria for rejecting inputs early and we use the best-performing method “early exits using proximity threshold”. For Cronus (Chen et al., 2012), we use a cascade with a maximum of 10 nodes. All hyper-parameters (cascade length, keep ratio, discount, early-stopping) were set based on a validation set. The cost/accuracy curve was generated by varying the corresponding trade-off parameter, λ.\nAs shown in the graph, CSTC significantly improves the cost/accuracy trade-off curve over all other algorithms. The power of Early exit is limited in this case as the test-time cost is dominated by feature extraction, rather than the evaluation cost. Compared with Cronus, CSTC has the ability to identify features that are most beneficial to different groups of inputs. It is this ability, which allows CSTC to maintain the high NDCG significantly longer as the cost-budget is reduced.\nNote that CSTC with fine-tuning only achieves very tiny improvement over CSTC without it. Although the fine-tuning step decreases the mean squared error on the test-set, it has little effect on NDCG, which is only based on the relative ranking of the documents (as opposed to their exact predictions). Moreover, because we fine-tune prediction nodes until validation NDCG decreases, for the majority of λ values, only a small amount of fine-tuning occurs.\nInput space partition. Figure 4 (left) shows a pruned CSTC tree (λ = 4) for the Yahoo! data set. The number above each node indicates the average label of theWg inputs passing through that node. We can observe that different branches aim at different parts of the input domain. In general, the upper branches focus on correctly classifying higher ranked documents, while the lower branches target low-rank documents. Figure 4 (right) shows the Jaccard matrix of the predictive classifiers (v3, v4, v5, v6, v14) from the same CSTC tree. The matrix shows a clear trend that the Jaccard coefficients decrease monotonically away from the diagonal. This indicates that classifiers share fewer features in common if their average labels are further apart—the most different classifiers v3 and v14 have only 64% of their features in common—and validates that classifiers in the CSTC tree extract different features in different regions of the tree.\nFeature extraction. We also investigate the features extracted in individual classifier nodes. Figure 5 shows the fraction of features, with a particular cost, extracted at different depths of the CSTC tree for the Yahoo! data. We observe a general trend that as depth increases, more features are being used. However, cheap features (c ≤ 5) are fully extracted early-\non, whereas expensive features (c ≥ 20) are extracted by classifiers sitting deeper in the tree, where each individual classifier only copes with a small subset of inputs. The expensive features are used to classify these subsets of inputs more precisely. The only feature that has cost 200 is extracted at all depths—which seems essential to obtain high NDCG (Chen et al., 2012)."
    }, {
      "heading" : "6. Conclusions",
      "text" : "We introduce Cost-Sensitive Tree of Classifiers (CSTC), a novel learning algorithm that explicitly addresses the trade-off between accuracy and expected test-time CPU cost in a principled fashion. The CSTC tree partitions the input space into sub-regions and identifies the most cost-effective features for each one of these regions—allowing it to match the high accuracy of the state-of-the-art at a small fraction of the cost. We obtain the CSTC algorithm by formulating the expected test-time cost of an instance passing through a tree of classifiers and relax it into a continuous cost function. This cost function can be minimized while learning the parameters of all classifiers in the tree jointly. By making the test-time cost vs. accuracy tradeoff explicit we enable high performance classifiers that fit into computational budgets and can reduce unnecessary energy consumption in large-scale industrial applications. Further, engineers can design highly specialized features for particular edges-cases of their input domain and CSTC will automatically incorporate them on-demand into its tree structure.\nAcknowledgements KQW, ZX, MK, and MC are supported by NIH grant U01 1U01NS073457-01 and NSF grants 1149882 and 1137211. The authors thank John P. Cunningham for clarifying discussions and suggestions."
    } ],
    "references" : [ {
      "title" : "Label embedding trees for large multi-class tasks",
      "author" : [ "S. Bengio", "J. Weston", "D. Grangier" ],
      "venue" : null,
      "citeRegEx" : "Bengio et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2010
    }, {
      "title" : "Classification and regression trees",
      "author" : [ "L. Breiman" ],
      "venue" : "Chapman & Hall/CRC,",
      "citeRegEx" : "Breiman,? \\Q1984\\E",
      "shortCiteRegEx" : "Breiman",
      "year" : 1984
    }, {
      "title" : "Fast classification using sparse decision dags",
      "author" : [ "R. Busa-Fekete", "D. Benbouzid", "B Kégl" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Busa.Fekete et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Busa.Fekete et al\\.",
      "year" : 2012
    }, {
      "title" : "Early exit optimizations for additive machine learned ranking systems",
      "author" : [ "B.B. Cambazoglu", "H. Zaragoza", "O. Chapelle", "J. Chen", "C. Liao", "Z. Zheng", "J. Degenhardt" ],
      "venue" : "In WSDM’3,",
      "citeRegEx" : "Cambazoglu et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Cambazoglu et al\\.",
      "year" : 2010
    }, {
      "title" : "Yahoo! learning to rank challenge overview",
      "author" : [ "O. Chapelle", "Y. Chang" ],
      "venue" : "In JMLR: Workshop and Conference Proceedings,",
      "citeRegEx" : "Chapelle and Chang,? \\Q2011\\E",
      "shortCiteRegEx" : "Chapelle and Chang",
      "year" : 2011
    }, {
      "title" : "Boosted multi-task learning",
      "author" : [ "O. Chapelle", "P. Shivaswamy", "S. Vadrevu", "K. Weinberger", "Y. Zhang", "B. Tseng" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "Chapelle et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Chapelle et al\\.",
      "year" : 2011
    }, {
      "title" : "Classifier cascade for minimizing feature evaluation cost",
      "author" : [ "M. Chen", "Z. Xu", "K.Q. Weinberger", "O. Chapelle" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Chen et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2012
    }, {
      "title" : "Fast and balanced: Efficient label tree learning for large scale object recognition",
      "author" : [ "J. Deng", "S. Satheesh", "A.C. Berg", "L. Fei-Fei" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Deng et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Deng et al\\.",
      "year" : 2011
    }, {
      "title" : "Learning fast classifiers for image spam",
      "author" : [ "M. Dredze", "R. Gevaryahu", "A. Elias-Bachrach" ],
      "venue" : "In proceedings of the Conference on Email and Anti-Spam (CEAS),",
      "citeRegEx" : "Dredze et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Dredze et al\\.",
      "year" : 2007
    }, {
      "title" : "Least angle regression",
      "author" : [ "B. Efron", "T. Hastie", "I. Johnstone", "R. Tibshirani" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Efron et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Efron et al\\.",
      "year" : 2004
    }, {
      "title" : "Finding naked people",
      "author" : [ "M. Fleck", "D. Forsyth", "C. Bregler" ],
      "venue" : "ECCV, pp. 593–602,",
      "citeRegEx" : "Fleck et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Fleck et al\\.",
      "year" : 1996
    }, {
      "title" : "Greedy function approximation: a gradient boosting machine",
      "author" : [ "J.H. Friedman" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Friedman,? \\Q2001\\E",
      "shortCiteRegEx" : "Friedman",
      "year" : 2001
    }, {
      "title" : "Active classification based on value of classifier",
      "author" : [ "T. Gao", "D. Koller" ],
      "venue" : "In NIPS, pp",
      "citeRegEx" : "Gao and Koller,? \\Q2011\\E",
      "shortCiteRegEx" : "Gao and Koller",
      "year" : 2011
    }, {
      "title" : "Speedboost: Anytime prediction with uniform near-optimality",
      "author" : [ "A. Grubb", "J.A. Bagnell" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Grubb and Bagnell,? \\Q2012\\E",
      "shortCiteRegEx" : "Grubb and Bagnell",
      "year" : 2012
    }, {
      "title" : "Cumulated gain-based evaluation of IR techniques",
      "author" : [ "K. Järvelin", "J. Kekäläinen" ],
      "venue" : "ACM TOIS,",
      "citeRegEx" : "Järvelin and Kekäläinen,? \\Q2002\\E",
      "shortCiteRegEx" : "Järvelin and Kekäläinen",
      "year" : 2002
    }, {
      "title" : "Hierarchical mixtures of experts and the em algorithm",
      "author" : [ "M.I. Jordan", "R.A. Jacobs" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Jordan and Jacobs,? \\Q1994\\E",
      "shortCiteRegEx" : "Jordan and Jacobs",
      "year" : 1994
    }, {
      "title" : "Timely object recognition",
      "author" : [ "S. Karayev", "T. Baumgartner", "M. Fritz", "T. Darrell" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Karayev et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Karayev et al\\.",
      "year" : 2012
    }, {
      "title" : "Sparse regression using mixed norms",
      "author" : [ "M. Kowalski" ],
      "venue" : "Applied and Computational Harmonic Analysis,",
      "citeRegEx" : "Kowalski,? \\Q2009\\E",
      "shortCiteRegEx" : "Kowalski",
      "year" : 2009
    }, {
      "title" : "Joint cascade optimization using a product of boosted classifiers",
      "author" : [ "L. Lefakis", "F. Fleuret" ],
      "venue" : "In NIPS, pp",
      "citeRegEx" : "Lefakis and Fleuret,? \\Q2010\\E",
      "shortCiteRegEx" : "Lefakis and Fleuret",
      "year" : 2010
    }, {
      "title" : "Using classifier cascades for scalable e-mail classification",
      "author" : [ "J. Pujara", "H. Daumé III", "L. Getoor" ],
      "venue" : "In CEAS,",
      "citeRegEx" : "Pujara et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Pujara et al\\.",
      "year" : 2011
    }, {
      "title" : "Boosting classifier cascades",
      "author" : [ "M. Saberian", "N. Vasconcelos" ],
      "venue" : null,
      "citeRegEx" : "Saberian and Vasconcelos,? \\Q2010\\E",
      "shortCiteRegEx" : "Saberian and Vasconcelos",
      "year" : 2010
    }, {
      "title" : "Learning with kernels: Support vector machines, regularization, optimization, and beyond",
      "author" : [ "B. Schölkopf", "A.J. Smola" ],
      "venue" : "MIT press,",
      "citeRegEx" : "Schölkopf and Smola,? \\Q2001\\E",
      "shortCiteRegEx" : "Schölkopf and Smola",
      "year" : 2001
    }, {
      "title" : "Robust real-time face detection",
      "author" : [ "P. Viola", "M.J. Jones" ],
      "venue" : "IJCV, 57(2):137–154,",
      "citeRegEx" : "Viola and Jones,? \\Q2004\\E",
      "shortCiteRegEx" : "Viola and Jones",
      "year" : 2004
    }, {
      "title" : "Local supervised learning through space partitioning",
      "author" : [ "J. Wang", "V. Saligrama" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Wang and Saligrama,? \\Q2012\\E",
      "shortCiteRegEx" : "Wang and Saligrama",
      "year" : 2012
    }, {
      "title" : "Feature hashing for large scale multitask learning",
      "author" : [ "K.Q. Weinberger", "A. Dasgupta", "J. Langford", "A. Smola", "J. Attenberg" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Weinberger et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Weinberger et al\\.",
      "year" : 2009
    }, {
      "title" : "The greedy miser: Learning under test-time budgets",
      "author" : [ "Z. Xu", "K. Weinberger", "O. Chapelle" ],
      "venue" : "In ICML, pp",
      "citeRegEx" : "Xu et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2012
    }, {
      "title" : "A general boosting method and its application to learning ranking functions for web search",
      "author" : [ "Z. Zheng", "H. Zha", "T. Zhang", "O. Chapelle", "K. Chen", "G. Sun" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Zheng et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 24,
      "context" : "Machine learning algorithms are widely used in many real-world applications, ranging from emailspam (Weinberger et al., 2009) and adult content filtering (Fleck et al.",
      "startOffset" : 100,
      "endOffset" : 125
    }, {
      "referenceID" : 10,
      "context" : ", 2009) and adult content filtering (Fleck et al., 1996), to web-search engines (Zheng et al.",
      "startOffset" : 36,
      "endOffset" : 56
    }, {
      "referenceID" : 26,
      "context" : ", 1996), to web-search engines (Zheng et al., 2008).",
      "startOffset" : 31,
      "endOffset" : 51
    }, {
      "referenceID" : 9,
      "context" : "Different from prior work, which reduces the total cost for every input (Efron et al., 2004) or which stages the feature extraction into linear cascades (Viola & Jones, 2004; Lefakis & Fleuret, 2010; Saberian & Vasconcelos, 2010; Pujara et al.",
      "startOffset" : 72,
      "endOffset" : 92
    }, {
      "referenceID" : 19,
      "context" : ", 2004) or which stages the feature extraction into linear cascades (Viola & Jones, 2004; Lefakis & Fleuret, 2010; Saberian & Vasconcelos, 2010; Pujara et al., 2011; Chen et al., 2012), a CSTC tree incorporates input-dependent feature selection into training and dynamically allocates higher feature budgets for infrequently traveled tree-paths.",
      "startOffset" : 68,
      "endOffset" : 184
    }, {
      "referenceID" : 6,
      "context" : ", 2004) or which stages the feature extraction into linear cascades (Viola & Jones, 2004; Lefakis & Fleuret, 2010; Saberian & Vasconcelos, 2010; Pujara et al., 2011; Chen et al., 2012), a CSTC tree incorporates input-dependent feature selection into training and dynamically allocates higher feature budgets for infrequently traveled tree-paths.",
      "startOffset" : 68,
      "endOffset" : 184
    }, {
      "referenceID" : 9,
      "context" : "A basic approach to control test-time cost is the use of l1-norm regularization (Efron et al., 2004), which results in a sparse feature set, and can significantly reduce the feature cost during test-time (as unused features are never computed).",
      "startOffset" : 80,
      "endOffset" : 100
    }, {
      "referenceID" : 19,
      "context" : "There is much previous work that extends single classifiers to classifier cascades (mostly for binary classification) (Viola & Jones, 2004; Lefakis & Fleuret, 2010; Saberian & Vasconcelos, 2010; Pujara et al., 2011; Chen et al., 2012).",
      "startOffset" : 118,
      "endOffset" : 234
    }, {
      "referenceID" : 6,
      "context" : "There is much previous work that extends single classifiers to classifier cascades (mostly for binary classification) (Viola & Jones, 2004; Lefakis & Fleuret, 2010; Saberian & Vasconcelos, 2010; Pujara et al., 2011; Chen et al., 2012).",
      "startOffset" : 118,
      "endOffset" : 234
    }, {
      "referenceID" : 11,
      "context" : "It should be noted that, in spite of the high accuracy achieved by these techniques, the algorithms are based heavily on stage-wise regression (gradient boosting) (Friedman, 2001), and are less likely to work with more general weak learners.",
      "startOffset" : 163,
      "endOffset" : 179
    }, {
      "referenceID" : 24,
      "context" : "Grubb & Bagnell (2012) and Xu et al. (2012) focus on training a classifier that explicitly trades-off test-time cost and accuracy.",
      "startOffset" : 27,
      "endOffset" : 44
    }, {
      "referenceID" : 0,
      "context" : "(2011), who speed up the training and evaluation of label trees (Bengio et al., 2010), by avoiding many binary one-vs-all classifier evaluations.",
      "startOffset" : 64,
      "endOffset" : 85
    }, {
      "referenceID" : 2,
      "context" : "Possibly most similar to our work are (Busa-Fekete et al., 2012), who learn a directed acyclic graph via a Markov decision process to select features for different instances, and (Wang & Saligrama, 2012), who adaptively partition the feature space and learn local region-specific classifiers.",
      "startOffset" : 38,
      "endOffset" : 64
    }, {
      "referenceID" : 5,
      "context" : "Recent tree-structured classifiers include the work of Deng et al. (2011), who speed up the training and evaluation of label trees (Bengio et al.",
      "startOffset" : 55,
      "endOffset" : 74
    }, {
      "referenceID" : 0,
      "context" : "(2011), who speed up the training and evaluation of label trees (Bengio et al., 2010), by avoiding many binary one-vs-all classifier evaluations. Differently, we focus on problems in which feature extraction time dominates the test-time cost which motivates different algorithmic setups. Dredze et al. (2007) combine the cost to select a feature with the mutual information of that feature to build a decision tree that reduces the feature extraction cost.",
      "startOffset" : 65,
      "endOffset" : 309
    }, {
      "referenceID" : 11,
      "context" : "Throughout this paper, we focus on linear classifiers but in order to allow non-linear decision boundaries we map the input into a non-linear feature space with the “boosting trick” (Friedman, 2001; Chapelle et al., 2011), prior to our optimization.",
      "startOffset" : 182,
      "endOffset" : 221
    }, {
      "referenceID" : 5,
      "context" : "Throughout this paper, we focus on linear classifiers but in order to allow non-linear decision boundaries we map the input into a non-linear feature space with the “boosting trick” (Friedman, 2001; Chapelle et al., 2011), prior to our optimization.",
      "startOffset" : 182,
      "endOffset" : 221
    }, {
      "referenceID" : 11,
      "context" : "In particular, we first train gradient boosted regression trees with a squared loss penalty (Friedman, 2001), H ′(xi) = ∑T t=1 ht(xi), where each function ht(·) is a limited-depth CART tree (Breiman, 1984).",
      "startOffset" : 92,
      "endOffset" : 108
    }, {
      "referenceID" : 1,
      "context" : "In particular, we first train gradient boosted regression trees with a squared loss penalty (Friedman, 2001), H ′(xi) = ∑T t=1 ht(xi), where each function ht(·) is a limited-depth CART tree (Breiman, 1984).",
      "startOffset" : 190,
      "endOffset" : 205
    }, {
      "referenceID" : 17,
      "context" : "described by (Kowalski, 2009).",
      "startOffset" : 13,
      "endOffset" : 29
    }, {
      "referenceID" : 9,
      "context" : "Note that for a single element this relaxation relaxes the l0 norm to the l1 norm, ‖aij‖0 → √ (aij) = |aij |, and recovers the commonly used approximation to encourage sparsity (Efron et al., 2004; Schölkopf & Smola, 2001).",
      "startOffset" : 177,
      "endOffset" : 222
    }, {
      "referenceID" : 11,
      "context" : "Stage−wise regression (Friedman, 2001) Single cost−sensitive classifier Early exit s=0.",
      "startOffset" : 22,
      "endOffset" : 38
    }, {
      "referenceID" : 6,
      "context" : "In contrast to Chen et al. (2012), we do not inflate the number of irrelevant documents (by counting them 10 times).",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 11,
      "context" : "The plot shows different stages in our derivation of CSTC: the initial cost-insensitive ensemble classifier H ′(·) (Friedman, 2001) from section 3 (stage-wise regression), a single cost-sensitive classifier as described in eq.",
      "startOffset" : 115,
      "endOffset" : 131
    }, {
      "referenceID" : 3,
      "context" : "For competing algorithms, we include Early exit (Cambazoglu et al., 2010) which improves upon stagewise regression by short-circuiting the evaluation of unpromising documents at test-time, reducing the overall test-time cost.",
      "startOffset" : 48,
      "endOffset" : 73
    }, {
      "referenceID" : 6,
      "context" : "For Cronus (Chen et al., 2012), we use a cascade with a maximum of 10 nodes.",
      "startOffset" : 11,
      "endOffset" : 30
    }, {
      "referenceID" : 6,
      "context" : "The only feature that has cost 200 is extracted at all depths—which seems essential to obtain high NDCG (Chen et al., 2012).",
      "startOffset" : 104,
      "endOffset" : 123
    } ],
    "year" : 2013,
    "abstractText" : "Recently, machine learning algorithms have successfully entered large-scale real-world industrial applications (e.g. search engines and email spam filters). Here, the CPU cost during test-time must be budgeted and accounted for. In this paper, we address the challenge of balancing the test-time cost and the classifier accuracy in a principled fashion. The test-time cost of a classifier is often dominated by the computation required for feature extraction—which can vary drastically across features. We decrease this extraction time by constructing a tree of classifiers, through which test inputs traverse along individual paths. Each path extracts different features and is optimized for a specific subpartition of the input space. By only computing features for inputs that benefit from them the most, our cost-sensitive tree of classifiers can match the high accuracies of the current state-of-the-art at a small fraction of the computational cost.",
    "creator" : "LaTeX with hyperref package"
  }
}