{
  "name" : "1608.06049.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Local Binary Convolutional Neural Networks",
    "authors" : [ "Felix Juefei-Xu", "Vishnu Naresh Boddeti", "Marios Savvides" ],
    "emails" : [ "felixu@cmu.edu", "naresh@cmu.edu", "msavvid@ri.cmu.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Deep learning has been overwhelmingly successful in a broad range of applications, such as computer vision, speech recognition / natural language processing, machine translation, bio-medical data analysis, and many more. Deep convolutional neural networks (CNN), in particular, have enjoyed huge success in tackling many computer vision problems over the past few years. Convolutional neural network architectures have seen tremendous development, AlexNet [1], VGG [2], Inception [3], ResNet [4, 5]. However, training these networks end-to-end with fully learnable convolutional kernels (as is standard practice) is (1) computationally very expensive, (2) results in large model size in terms of both memory usage and disk space, and (3) prone to over-fitting due to the large number of parameters.\nOn the other hand, there is a growing need for deploying these systems on resource constrained platforms like, autonomous cars, robots, smart-phones, smart cameras, smart wearable devices, etc. To address these drawbacks, several binary versions of CNNs have been proposed [6, 7, 8] that approximate the dense real-valued weights with binary weights. Binary weights bear dramatic computational savings through efficient implementations of binary convolutions. Complete binarization of CNNs, though, leads to performance loss in comparison to real-valued network weights.\nIn this paper, we present an alternative approach to reducing the computational complexity of CNNs while performing as well as standard CNNs. We introduce the local binary convolution (LBC) layer that approximates the non-linearly activated response of a standard convolutional layer. The LBC layer comprises of fixed sparse binary filters, a non-linear activation function and a set of learnable linear weights that computes weighted combinations of the activated convolutional response maps. Learning reduces to optimizing the linear weights, as opposed to optimizing the convolutional filters.\nar X\niv :1\n60 8.\n06 04\n9v 1\n[ cs\n.L G\n] 2\n2 A\nug 2\n01 6\nParameter savings of at least 9×, 25×, 49×, 81×, 121× or even 169× can be realized during the learning stage depending on the spatial dimensions of the convolutional filters (3× 3, 5× 5, 7× 7, 9× 9, 11× 11, or 13× 13 sized filters respectively), as well as computational and memory savings due to the sparse nature of the binary filters. CNNs with LBC layers, called local binary convolutional neural networks (LBCNN)1, have much lower model complexity and are as such less prone to over-fitting and are well suited for resource-constrained environments.\nOur theoretical analysis shows that the LBC layer is a good approximation for the non-linear activations of standard convolutional layers. We also demonstrate empirically that CNNs with LBC layers achieves state-of-the-art performance on a range of visual datasets (MNIST, SVHN, and CIFAR-10) while enjoying significant savings in terms of the number of parameters during training, computations, as well as memory requirements due to the sparse and pre-defined nature of our binary filters in comparison to learnable dense real-valued filters.\nRelated Work: The idea of using binary filters for convolutional layers is not new. BinaryConnect [6] has been proposed to approximate the real-valued weights in neural networks with binary weights. Given any real-valued weight, it stochastically assigns +1 with probability p that is taken from the hard sigmoid output of the real-valued weight, and -1 with probability 1 − p. Weights are only binarized during the forward and backward propagation, but not during the parameter update step, in which high-precision real-valued weights are necessary for the SGD during weight update. Therefore, BinaryConnect alternates between binarized weights and real-valued ones during the network training process. A follow-up work of [6] is called BinaryNet, and later Binarized Neural Network (BNN) [7] where both the weights and activations are constrained to binary. The main contribution is that by doing so, run-time efficiency can be drastically improved because most of the 32-bit floating point multiply-accumulations are replaced by 1-bit XNOR-count operations.\nBoth BinaryConnect and BNN experiment on MNIST, CIFAR-10, and SVHN dataset. Lately, XNORNet [8] has been proposed on top of BNN that can deal with larger-scale image recognition tasks such as ImageNet. XNOR-Net inherits the design principles from BNN, where both weights and activations are binary. Both of them require high-precision real-valued weights during weight update, and achieve efficient implementations using XNOR bit count. XNOR-Net differs from BNN in the binarization method and the network structure.\nOur proposed LBCNN is quite different from these methods in nature. We are exhibiting that, with fixed sparse LB filters, a learnable way of combining them can lead to on par performance with real-valued traditional CNN, while enjoying significant savings in the number of parameters during learning process, as well as memory and computation. On the other hand, [6, 7, 8] do not save the number of parameters needed to be learned, and are still based of the traditional CNN because each binarization step still depends on the real-valued weights, with on-the-fly binarization to improve run-time efficiency. Many other network quantization and compression methods can be found in [9, 10, 11, 12, 13, 14, 15, 16]. The performances of the quantized or compressed models are usually upper bounded by the un-compressed counterparts."
    }, {
      "heading" : "2 Forming Local Binary Patterns with Convolutional Filters",
      "text" : "Local binary patterns (LBP) descriptor is a simple yet very powerful image descriptor rooted from face recognition community, and later has been widely used in many computer vision, pattern recognition, and image processing applications as consolidated in [17].\nThe traditional LBP operator [18, 19, 20] operates on image patches of size 3×3, 5×5, etc. The LBP descriptor is formed by sequentially compare the neighboring pixel intensity to that of the center pixel within the patch. Neighbors having higher intensity value than the center pixel will be assigned 1 and 0 otherwise. In the end, the bit string is read sequentially and converted to a decimal number using base 2, which is the feature assigned to the center pixel, characterizing local texture. The LBP for the center pixel (xc, yc) within a patch can be represented as LBP(xc, yc) = ∑L−1 n=0 s(in − ic) · 2n where in denotes the intensity of the nth surrounding pixel, ic denotes the intensity of the center pixel, L is the length of the sequence, and s = 1 if in ≥ ic, otherwise, s = 0. In the case of a N ×N neighborhood, there are N2 − 1 surrounding pixels, so the bit string is of length N2 − 1. Figure 1 shows the examples of 3× 3 and 5× 5 LBP encoding.\n1Implementation and future updates will be available at http://www.xujuefei.com/lbcnn.html.\nThere are many parameters and configurations that one can tune during the LBP formulation, and result in totally different LBP. Here is a list of things one might consider to generalize the LBP descriptor:\n• Varying base: One can vary the base for forming the decimal number. Instead of using base 2 as traditionally practiced, one can choose other bases. Lifting the restriction of using only base 2 for decimal conversion, much more diversity can be achieved when encoding LBP. Moreover, allowing combining weights to take any real-valued number gives more freedom.\n• Varying center: The physical center of the neighborhood is usually chosen as the center for thresholding neighbors. However, one can vary the center. Different choices of pivot (thresholding center) returns different LBP. One can even allow multiple pivots for a greater range of thresholding comparisons among neighboring pixels.\n• Varying ordering: If the neighborhood size and the thresholding center are both fixed, different ordering of the neighbors (or the weighting of each bit) gives different decimal outputs. One can easily vary the ordering of the neighbors and thus lead to different LBP.\nAll the aforementioned variations are usually determined empirically for certain applications, i.e., the choice of center point, the base, and the ordering of the neighbors. Being able to generalize on these factors and make them all learnable is one of the motivations behind LBCNN to be discussed.\nFirst, let us reformulate the LBP encoding using convolutional filters, which is much more efficient. The traditional way of encoding LBP feature is to use a 3 × 3 window to scan through the entire image in an overlapping fashion. At each 3× 3 patch, the encoding involves (1) getting difference between pairs of pixels, (2) thresholding to binary numbers, and (3) obtaining a weighed sum.\nNow, a simple convolution of the entire image with eight 3×3 convolutional filters, followed by simple binarization can achieve the same goal, as shown in Figure 2. Each convolution filter is a 2-sparse difference filter. The 8 resulting bit maps after binarization are also shown. The traditional LBP is simply a weighted sum of all the bit maps using the pre-defined weight vector v = [27, 26, 25, 24, 23, 22, 21, 20]. Therefore, the reformulation of the LBP can be shown as y = ∑8 i=1 σ(bi ∗ x) · vi, where x ∈ Rd is vectorized version of the original image, bi’s are the sparse convolutional filters, σ is the binarization operator, which is a Heaviside step function, and y ∈ Rd is the resulting LBP image. Note that only the binarization is non-linear operation.\nThe varying base and ordering can be done by specifying the combining weights properly, and the varying center can be done by specifying the locations of the non-zero (+1 and -1) support in the convolutional filters. These two specifications are the major design principles to be borrowed for the creation of the LBCNN."
    }, {
      "heading" : "3 Local Binary Convolutional Neural Networks",
      "text" : ""
    }, {
      "heading" : "3.1 Design of the Basic Module",
      "text" : "Somewhat surprisingly, the reformulation of the traditional LBP descriptor has indeed contained all the main components required by the modern convolutional neural networks. For example, in LBP, there is a step that the image is filtered by a filter bank of convolutional filters, and there is also a step of non-linearity, through Heaviside step function. Finally, the obtained bit maps are linearly combined to arrive at the final LBP glyph, which can be further passed onto the next module for further processing.\nAll of these motivate us for designing an alternative for the deep CNN architecture, which we name local binary convolutional neural networks (LBCNN)2. As shown in Figure 3, the basic module of LBCNN starts with m pre-defined non-learnable convolutional filters bi, i ∈ [m]. The input image xl is filtered by these LB filters and becomes m difference maps, which are then passed through a non-linear activation gate and become m bit maps. To make the activation step differentiable, we use sigmoid function in place of the Heaviside step function, as was in the original LBP. Finally, the m bit maps are lineally combined by m learnable weights Vl,i, i ∈ [m], which leads to the feature map. It is also the input xl+1 for the next layer. This process is shown as\nxl+1 = m∑ i=1 σ(bi ∗ xl) · Vl,i (1)\nIt is worth noting that the final step involving the weighted sum is essentially another convolution operation with filters of size 1 × 1. This means that each LBCNN module is comprised of two convolutional layers. The weights in the first convolutional layer are fixed and non-learnable, and the weights in the second convolutional layer are learnable.\nCompared to the CNN module under the same LBCNN structure (with 1 × 1 convolutions) , the number of learnable parameters is significantly smaller in LBCNN. Let us assume that the number of input and output channels are p and q. Therefore, the size of each 3D filter in both CNN and LBCNN is p · h · w, where h and w are the spatial dimensions of the filter, and there are m such filters. The 1 × 1 convolutions act on the m filters and create the q-channel output. For CNN, the number of learnable weights is p · h ·w ·m+m · q. For LBCNN, the learnable weights only reside in the 1× 1 convolution step, so the number of learnable weights is m · q. For simplicity let us assume p = q, which is usually the case for multi-layer CNN architecture. Then we have\n# parameters in CNN # parameters in LBCNN = p · h · w ·m+m · q m · q = h · w + 1 (2)\nIf we do not include the 1× 1 convolutions for CNN, and thus make m = q = p, readers can verify that the parameter ratio becomes h · w. Numerically, LBCNN saves at least 9×, 25×, 49×, 81×, 121×, and 169× parameters during learning for 3 × 3, 5 × 5, 7 × 7, 9 × 9, 11 × 11, and 13 × 13 convolutional filters respectively."
    }, {
      "heading" : "3.2 Training of the LBCNN",
      "text" : "The training of the LBCNN is quite straightforward. Backpropagation is the same for the learnable and the anchor weights (weights in the LB filters that do not update). However in learning, the learnable 1× 1 filters are updated while the anchor weights are unaffected. Gradients can and do get propagated through the fixed LB filters just like they would with learnable filters. This is similar to propagating gradients through layers without learnable parameters (e.g., ReLU, Max Pooling etc.). However, we do not compute the gradient w.r.t. fixed filters nor update them during the training\n2In this paper we assume convolutional filters do not have bias terms.\nprocess. The 3D non-learnable filter banks of size p×h×w×m (assuming a total ofm filters in each layer) in LBCNN can either be generated deterministically (as practiced in LBP) or stochastically. We use the latter for our experiments. Specifically, we first determine a sparsity level in terms of percentage of the weights that can bear non-zero values, and then randomly assign 1 or -1 to these weights with equal probability (Bernoulli distribution). This is essentially a generalization of the traditional LBP because we allow multiple neighbors to be compared to multiple pivots, which is similar to the 3D LBP formulation can be found in spatial-temporal applications [17]. As shown in Figure 4, pink locations bear value 1 and black location -1. Remaining green locations are still 0. From left to right, the sparsity level increases. Stochastically generating the LB filters allows us to place more diversified filters at each layer, and also makes sparsity control easier.\n3.3 Advantages of the LBCNN\n• Provides savings: LBCNN not only saves 9× to 169× parameters during training, but also enjoys memory / computation saving during inference since the local binary weights are binary and sparse. Exact theoretical saving / speed-up factors can be computed on a caseby-case basis. Binary weights instead of floating point weights can provide 9× to 169× savings from the perspective of model size. Bearing much fewer parameters allows LBCNN to train much faster. With the same amount of parameter and computing budget, one can go much deeper using LBCNN architecture, allowing better representations to be learned through deeper architecture [2, 4, 5]. Lastly, with LBCNN, there is a potential of sharing convolutional parameters (pre-defined weights) across all layers, which leads to even more efficient implementation, especially on the embedded systems.\n• Prevents over-fitting: LBCNN, being a simpler model with much fewer learnable parameters compared to CNN, can effectively prevent over-fitting. Methods such as Dropout [21], DropConnect [22], and Maxout [23] have been introduced to regularize the network during training to avoid over-fitting, as a further aid to the data augmentation measures. However, the number of parameters is huge in the high capacity models such as deep CNNs, and by conforming to the original design principles of CNNs, there has not been much success in reducing the model complexity of CNN while maintaining high performance. LBCNN, on the contrary, is a groundup re-design of the CNN module and allows the model to only learn how to optimally combine the responses of the pre-defined filters, instead of learning the filters themselves. As opposed to preventing over-fitting in the fully connected layers [21, 22, 24], it is also worth noting that LBCNN prevents the over-fitting directly in the convolutional layers, which is also quite important as discussed in [21, 25]. Figure 5 shows a toy example training on a subset of CIFAR-10 dataset. The training subset randomly picks 25% images (5000× 0.25 = 1250) per class and keep the testing set intact. We set both LBCNN and CNN models to be of high capacity in the same way, each following the best-performing architecture on CIFAR-10, to be discussed in Section 4. It can be seen that LBCNN trains faster and does not suffer from over-fitting.\n• Provides de-correlation: We know that Dropout [21] prevents the co-adaptation of neuron activations, and Batch Normalization [26] tries to reduce internal co-variate shift. They, working as regularizers, are all trying to remove the redundancies in the deep neural networks. Lately, another\nwork [24] has been proposed to explicitly de-correlate and minimize the cross-covariance of hidden activations, for improved performance and over-fitting prevention. It encourages diverse or nonredundant representations. LBCNN naturally provides de-correlation for the activations because the convolutional filters are randomly generated sparse Bernoulli filters. Figure 6 shows the amount of de-correlation in both LBCNN and CNN filters for the first 5 layers of the best-performing architecture on CIFAR-10, to be discussed in Section 4. The metric is the Frobenius norm squared of the covariance matrix subtracting the `2-norm squared of the diagonal elements, and normalized (‖Σ‖2F − ‖diag(Σ)‖22)/‖Σ‖2F . The smaller the value, the more de-correlated they are.\n3.4 Theoretical Analysis\nWe now show how the proposed LBCNN method can very well approximate the convolution block, followed by ReLU non-linearity in the traditional CNN. At layer l, let x ∈ R(p·h·w)×1 be a vectorized single patch from the p-channel input maps, where h and w are the spatial sizes of the convolutional filter. Let w ∈ R(p·h·w)×1 be a vectorized single convolution filter from the convolutional filter banks W ∈ Rp×h×w×m with m learnable filters at layer l. We drop the layer subscription l for brevity.\nSo in the traditional CNN, this patch x is projected onto the filter w, followed by the non-linear activation, and yield one number d on the output map. Each slice of the output feature map is a direct result of convolving the input map with just one convolutional filter, and this one element d receives no other contributions than from this x and w. This microscopic process can be shown as\nd = σrelu(w >x) (3)\nFor the proposed LBCNN approach, on the contrary, each element on the output feature map is a linear combination of multiple elements from the intermediate bit maps through 1× 1 convolution. And each slice of the bit map is obtained by convolving the input map with a set of m pre-defined, non-learnable convolutional filters B ∈ Rm×p×h×w, and followed by a non-linear activation. In order to generate the counterpart of d for the LBCNN, which we call it d′, the m bit maps are convolved with m convolutional filters of size 1× 1, each has a parameter: v1, v2, . . . , vm. Therefore, this process can be written as\nd′ = σsigmoid(Bx︸︷︷︸ m×1 )> v︸︷︷︸ m×1 = c>sigmoidv (4)\nwhere B is now a 2D matrix of size m × (p · h · w) with m filters stacked as rows, with a slight abuse of notation. v = [v1, . . . , vm]> ∈ Rm×1. From Equation 3, we are certain that d ≥ 0 due to ReLU activation. From Equation 4, we know that any element in the vector csigmoid = σsigmoid(Bx) is always ∈ (0, 1), due to sigmoid activation. Therefore, we can always obtain a v such that c>sigmoidv = d\n′ = d. More issue comes when we choose to use ReLU for LBCNN, where Equation 4 now becomes\nd′ = σrelu(Bx) >v = c>reluv (5)\nFor the case when d = 0, since crelu = σrelu(Bx) ≥ 0, there is always going to exist a vector v ∈ Rm×1 such that d′ = d. However, for the case when d > 0, it is obvious that vector crelu cannot be all 0’s. Next we will show under what conditions crelu can have positive elements with provable bound, to ensure d′ = d in this case. Definition 3.1 (subgaussian random variable). A random variable X is called subgaussian if there exist constants β, κ > 0, such that P(|X| ≥ t) ≤ βe−κt2 for all t > 0. Lemma 3.1. Let X be a subgaussian random variable with E[X] = 0, then there exists a constant c that only depends on β and κ > 0 such that E[exp(θX)] ≤ exp(cθ2) for all θ ∈ R. Conversely, if the above inequality holds, then E[X] = 0 and X is subgaussian with parameters β = 2 and κ = 1/(4c).\nDefinition 3.2 (isotropic random vector). Let be a random vector on RN . If E[|〈 ,x〉|2] = ‖x‖22 for all x ∈ RN , then is called an isotropic random vector. Definition 3.3 (subgaussian random vector). Let be a random vector on RN . If for all x ∈ RN with ‖x‖2 = 1, the random variable 〈 ,x〉 is subgaussian with subgaussian parameter c being independent of x, that is\nE[exp(θ〈 ,x〉)] ≤ exp(cθ2), for all θ ∈ R, ‖x‖ = 1 (6) then is called a subgaussian random vector.\nLemma 3.2. Bernoulli random matrices are subgaussian matrices. Lemma 3.3. Bernoulli random vectors are isotropic. Lemma 3.4. Let B be an m×N random matrix with independent, isotropic, and subgaussian rows with the same subgaussian parameter c in (6). Then, for all x ∈ RN and every t ∈ (0, 1),\nP (∣∣∣∣ 1m‖Bx‖22 − ‖x‖22 ∣∣∣∣ ≥ t‖x‖22) ≤ 2 exp(−c̃t2m) (7) where c̃ only depends on c.\nTheorem 3.5. Let B ∈ Rm×N be a Bernoulli random matrix with the same subgaussian parameter c in (6), and x ∈ RN be a fixed vector and ‖x‖2 > 0, with N = p · h · w. Let ξ = Bx ∈ Rm. Then, for all t ∈ (0, 1), there exists a matrix B and an index i ∈ [m] such that\nP ξi ≥√(1− t)‖x‖2︸ ︷︷ ︸ >0  ≥ 1− 2 exp(−c̃t2m) (8) The above shown derivation is true for the case where a single image patch is convolved with CNN and LBCNN filters. Imagine there are a total of τ patches in an image, then the scalar d in Equation 3 is now generalized to a vector d ∈ Rτ with each element di, i ∈ [τ ] being the scalar output for i-th patch in the CNN. Similarly, for the LBCNN, Equation 5 is generalized to d′ = C>reluv, where Crelu ∈ Rm×τ and each column in Crelu corresponds to the m bit maps from each of the τ image patches. We notice that the same vector v will now need to accommodate for all τ columns in Crelu, in order to get the τ -vector d′. When τ ≤ m, we are still able to solve for a vector v such that d′ = C>reluv exactly. However, when τ > m, it has become an over-determined system and a least-square error solution ṽ can be found through ṽ = (CC>)−1Cd′, such that d′ ≈ C>reluṽ. Therefore, we should always use more filters, such that m is relatively high.\nEmpirically we can measure how far d′ is from d by adopting the normalized mean square error (NMSE), which is defined as ‖d′ − d‖22/‖d‖22. We take the entire 50,000 32 × 32 images from CIFAR-10 training set, and measure the NMSE, as shown in Figure 7. For the CNN, dense realvalued filters are independently generated as Gaussian random filters, for each individual image. For the LBCNN, the sparse LB filters are also independently generated for each individual image. Experiments are repeated for 10 levels of sparsity (10%, 20%, . . . , 100%) and 3 choices of number of LB filters, 64, 128 and 512. We can see that the approximation is better using more filters, and with higher sparsity, with the exception of sparsity being 100%. We conjecture that this may be due to that fact that d is actually sparse, due to ReLU activation, and enforcing no sparsity level at all in the LB filters B actually makes the approximation a harder task."
    }, {
      "heading" : "4 Experimental Results",
      "text" : "Datasets: We have experimented with 4 visual datasets, MNIST, SVHN, CIFAR-10, and a subset of ILSVRC-2012 ImageNet classification challenge. The MNIST contains a training set of 60K and a testing set of 10K 32× 32 gray-scale images showing hand-written digits from 0 to 9. SVHN is also a widely used dataset for classifying digits, house number digits from street view images in this case. It contains a training set of 604K and a testing set of 26K 32× 32 color images showing house number digits. CIFAR-10 is an image classification dataset containing a training set of 50K and a testing set of 10K 32× 32 color images, which are across the following 10 classes: airplanes, automobiles, birds, cats, deers, dogs, frogs, horses, ships, and trucks. The ImageNet ILSVRC-2012\nclassification dataset [27] consists of 1000 classes, with 1.28 million images in the training set and 50K images in the validation set, where we use for testing as commonly practiced. For faster roll-out, we randomly select 100 classes with the largest number of images (1300 training images in each class, with a total of 130K training images and 5K testing images.), and report top-1 accuracy on this subset. Full ImageNet experimental results will be reported in the updated version.\nImplementation Details: Conceptually LBCNN can be easily implemented in any existing deep learning framework. Since the convolutional weights are fixed, we do not have to compute the gradients nor update the weights. This leads to savings both from a computational point of view and memory as well. Furthermore, since the weights are binary the convolution operation can be performed purely through additions and subtractions. We base the model architectures we evaluate in this paper on ResNet [5], with default 3× 3 filter size. Our basic module is the LBCNN module shown in Figure 3 along with an identity connection as in ResNet. We experiment with different numbers of LBCNN units, 10, 20 and 75, which is equivalent to 20, 40, and 150 convolutional layers. For LBCNN the convolutional weights are generated following the procedure described in Section 3.2. We use 512 randomly generated anchor weights, with a sparsity of 0.1, 0.5 or 0.9, for all of our experiments. Spatial average pooling is adopted after the convolution layers to\nreduce the spatial dimensions of the image to 6× 6. We use a learning rate of 1e-3 and following the learning rate decay schedule from [5]. We use ReLU in place of sigmoid because we find that ReLU activations train faster. One trick is not to apply ReLU prior to the local binary convolutional layer, which is usually done right after batch normalization [26]. The reason is that contiguous 0-valued region is not suitable for LB filters because each LB filter gives a flat response in this region and once the responses are linearly combined, the loss of information accumulates.\nBaselines: For a fair comparison and to quantify the exact difference between our LBCNN approach and traditional CNN, we compare ours against the exact corresponding network architecture with dense and learnable convolutional weights. We also use the exact same data and hyper-parameters in terms of the number of convolutional weights, initial learning rate and the learning rate schedule. In this sense, LBCNN enjoys 10×, 26×, 50×, 82×, 122×, or 170× savings in the number of learnable parameters because the baseline CNNs also have the 1× 1 convolutional layer.\nResults on MNIST, SVHN, and CIFAR-10: Table 1 compares the accuracy on CIFAR-10 achieved by various LBCNN architecture as well as their CNN counterparts. We can see that for a fixed number of convolution layers and LB filters, the more intermediate channels leads to higher performance. Also, LBCNN is on par with the CNN counterpart, while saves 10× parameters. The best performing LBCNN models are:\n• For MNIST: 150 convolutional layers (75 LBCNN modules), 512 LB filters, 16 intermediate channels, 0.5 sparsity, 128 hidden units in the fully connected layer.\n• For SVHN: 80 convolutional layers (40 LBCNN modules), 512 LB filters, 16 intermediate channels, 0.9 sparsity, 512 hidden units in the fully connected layer.\n• For CIFAR-10: 100 convolutional layers (50 LBCNN modules), 512 LB filters, 384 intermediate channels, 0.1 sparsity, 512 hidden units in the fully connected layer.\nTable 2 consolidates the images classification accuracies from our experiments. The best performing LBCNNs are compared to their particular baselines, as well as the state-of-the-art methods such as BinaryConnect [6], Binarized Neural Networks (BNN) [7], ResNet [4], Maxout Network [23], Network in Network (NIN) [28].\nSharing LBC across All Layers: We can further save the LBCNN model size by sharing the local binary convolutional filters across all layers, as opposed to randomly generate new ones at each layer. By doing this, we further save the model size roughly by a factor of D where D is the depth of the network. It turns out, the strategy of sharing the LBC module across all layers does not shy at all. As can be seen from the second row in Table 1 and in Figure 8, the weight sharing strategy is quite comparable to the standard LBCNN.\nNetEverest: With at least 9× parameter reduction, one can now train much deeper networks, going roughly from 100 to 1,000 layers, or from 1,000 to 10,000 layers. LBC module allows us to train extremely deep convolutional neural networks efficiently with 8,848 convolutional layers (4,424 LBC modules) which is termed as NetEverest using a single nVidia Titan X GPU, and reaches the highest accuracy on CIFAR-10 among our experiments as shown in Table 2. The architecture of NetEverest: 8848 convolutional layers (4424 LBC modules), 32 LB filters, 32 intermediate channels, 0.1 sparsity, 512 hidden units in the fully connected layer.\nResults on 100-Class ImageNet: We report the top-1 accuracy on 100-Class subset of ImageNet 2012 classification challenge dataset in Table 3. The input images of ImageNet is much larger than those of MNIST, SVHN, and CIFAR-10, which allows us to experiments with the LBC filter sizes. Both the LBCNN and our baseline share the same architecture: 48 convolutional layers (24 LBC modules),\n512 LB filters, 512 intermediate channels, 0.9 sparsity, 4096 hidden units in the fully connected layer."
    }, {
      "heading" : "LB Filter Size 3× 3 5× 5 7× 7 9× 9 11× 11 13× 13",
      "text" : "Discussions: We have shown the effectiveness of the proposed LBCNN. Not only can it achieve on-par performance with the state-of-the-art, but also enjoy a significant utility savings."
    }, {
      "heading" : "5 Conclusions and Future Work",
      "text" : "Motivated by LBP, in this paper, we proposed the LBC layer as an alternative to the convolutional layer. The LBC module enjoys significant savings in the number of parameters to be learned at training, at least 9× to 169×. CNNs with LBC layers have much lower model complexity compared CNNs with standard convolutional layers while still exhibiting state-of-the-art performance on multiple object recognition datasets. We demonstrate, both theoretically and empirically, that the LBC module is a good approximation of a standard convolutional layer. The proposed LBCNN demonstrates performance on par with the state-of-the-art architectures on 3 image recognition datasets."
    } ],
    "references" : [ {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "NIPS, pages 1097–1105,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "K. Simonyan", "A. Zisserman" ],
      "venue" : "ICLR,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Going deeper with convolutions",
      "author" : [ "Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2015
    }, {
      "title" : "Deep Residual Learning for Image Recognition",
      "author" : [ "K. He", "X. Zhang", "S. Ren", "J. Sun" ],
      "venue" : "CVPR,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Identity Mappings in Deep Residual Networks",
      "author" : [ "K. He", "X. Zhang", "S. Ren", "J. Sun" ],
      "venue" : "arXiv:1603.05027,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "BinaryConnect: Training Deep Neural Networks with binary weights during propagations",
      "author" : [ "M. Courbariaux", "Y. Bengio", "J.-P. David" ],
      "venue" : "NIPS, pages 3105–3113,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "BinaryNet: Training deep neural networks with weights and activations constrained to +1 or -1",
      "author" : [ "M. Courbariaux", "Y. Bengio" ],
      "venue" : "arXiv:1602.02830,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks",
      "author" : [ "M. Rastegari", "V. Ordonez", "J. Redmon", "A. Farhadi" ],
      "venue" : "arXiv:1603.05279,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <1MB model size",
      "author" : [ "F.N. Iandola", "M.W. Moskewicz", "K. Ashraf", "S. Han", "W.J. Dally", "K. Keutzer" ],
      "venue" : "arXiv:1602.07360,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Quantized Convolutional Neural Networks for Mobile Devices",
      "author" : [ "J. Wu", "C. Leng", "Y. Wang", "Q. Hu", "J. Cheng" ],
      "venue" : "arXiv:1512.06473,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding",
      "author" : [ "S. Han", "H. Mao", "W.J. Dally" ],
      "venue" : "arXiv:1510.00149,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Compressing neural networks with the hashing trick",
      "author" : [ "W. Chen", "J.T. Wilson", "S. Tyree", "K.Q. Weinberger", "Y. Chen" ],
      "venue" : "arXiv:1504.04788,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Predicting parameters in deep learning",
      "author" : [ "M. Denil", "B. Shakibi", "L. Dinh", "M. Ranzato", "N. de Freitas" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2013
    }, {
      "title" : "Learning both weights and connections for efficient neural network",
      "author" : [ "S. Han", "J. Pool", "J. Tran", "W. Dally" ],
      "venue" : "NIPS, pages 1135–1143,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Expectation backpropagation: parameter-free training of multilayer neural networks with continuous or discrete weights",
      "author" : [ "D. Soudry", "I. Hubara", "R. Meir" ],
      "venue" : "NIPS, pages 963–971,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Backpropagation for energy-efficient neuromorphic computing",
      "author" : [ "S.K. Esser", "R. Appuswamy", "P. Merolla", "J.V. Arthur", "D.S. Modha" ],
      "venue" : "NIPS, pages 1117–1125,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Computer Vision Using Local Binary Patterns",
      "author" : [ "M. Pietikäinen", "A. Hadid", "G. Zhao", "T. Ahonen" ],
      "venue" : "Springer,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Subspace-based discrete transform encoded local binary patterns representations for robust periocular matching on nist’s face recognition grand challenge",
      "author" : [ "F. Juefei-Xu", "M. Savvides" ],
      "venue" : "IEEE Transactions on Image Processing, 23(8):3490–3505, Aug",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A comparative study of texture measures with classification based on featured distributions",
      "author" : [ "T. Ojala", "M. Pietikäinen", "D. Harwood" ],
      "venue" : "Pattern Recognition, 29(1):51–59,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Learning to Invert Local Binary Patterns",
      "author" : [ "F. Juefei-Xu", "M. Savvides" ],
      "venue" : "27th BMVC, Sept",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Dropout: A simple way to prevent neural networks from overfitting",
      "author" : [ "N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov" ],
      "venue" : "JMLR, 15(1):1929–1958,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Regularization of neural networks using dropconnect",
      "author" : [ "L. Wan", "M. Zeiler", "S. Zhang", "Y. LeCun", "R. Fergus" ],
      "venue" : "ICML, pages 1058–1066,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Maxout networks",
      "author" : [ "I.J. Goodfellow", "D. Warde-Farley", "M. Mirza", "A. Courville", "Y. Bengio" ],
      "venue" : "arXiv:1302.4389,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Reducing Overfitting in Deep Networks by Decorrelating Representations",
      "author" : [ "M. Cogswell", "F. Ahmed", "R. Girshick", "L. Zitnick", "D. Batra" ],
      "venue" : "ICLR,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)",
      "author" : [ "D.-A. Clevert", "T. Unterthiner", "S. Hochreiter" ],
      "venue" : "ICLR,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "S. Ioffe", "C. Szegedy" ],
      "venue" : "arXiv:1502.03167,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Imagenet large scale visual recognition challenge",
      "author" : [ "Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein" ],
      "venue" : null,
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2015
    }, {
      "title" : "Network in network",
      "author" : [ "M. Lin", "Q. Chen", "S. Yan" ],
      "venue" : "ICLR,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Convolutional neural network architectures have seen tremendous development, AlexNet [1], VGG [2], Inception [3], ResNet [4, 5].",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 1,
      "context" : "Convolutional neural network architectures have seen tremendous development, AlexNet [1], VGG [2], Inception [3], ResNet [4, 5].",
      "startOffset" : 94,
      "endOffset" : 97
    }, {
      "referenceID" : 2,
      "context" : "Convolutional neural network architectures have seen tremendous development, AlexNet [1], VGG [2], Inception [3], ResNet [4, 5].",
      "startOffset" : 109,
      "endOffset" : 112
    }, {
      "referenceID" : 3,
      "context" : "Convolutional neural network architectures have seen tremendous development, AlexNet [1], VGG [2], Inception [3], ResNet [4, 5].",
      "startOffset" : 121,
      "endOffset" : 127
    }, {
      "referenceID" : 4,
      "context" : "Convolutional neural network architectures have seen tremendous development, AlexNet [1], VGG [2], Inception [3], ResNet [4, 5].",
      "startOffset" : 121,
      "endOffset" : 127
    }, {
      "referenceID" : 5,
      "context" : "To address these drawbacks, several binary versions of CNNs have been proposed [6, 7, 8] that approximate the dense real-valued weights with binary weights.",
      "startOffset" : 79,
      "endOffset" : 88
    }, {
      "referenceID" : 6,
      "context" : "To address these drawbacks, several binary versions of CNNs have been proposed [6, 7, 8] that approximate the dense real-valued weights with binary weights.",
      "startOffset" : 79,
      "endOffset" : 88
    }, {
      "referenceID" : 7,
      "context" : "To address these drawbacks, several binary versions of CNNs have been proposed [6, 7, 8] that approximate the dense real-valued weights with binary weights.",
      "startOffset" : 79,
      "endOffset" : 88
    }, {
      "referenceID" : 5,
      "context" : "BinaryConnect [6] has been proposed to approximate the real-valued weights in neural networks with binary weights.",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 5,
      "context" : "A follow-up work of [6] is called BinaryNet, and later Binarized Neural Network (BNN) [7] where both the weights and activations are constrained to binary.",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 6,
      "context" : "A follow-up work of [6] is called BinaryNet, and later Binarized Neural Network (BNN) [7] where both the weights and activations are constrained to binary.",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 7,
      "context" : "Lately, XNORNet [8] has been proposed on top of BNN that can deal with larger-scale image recognition tasks such as ImageNet.",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 5,
      "context" : "On the other hand, [6, 7, 8] do not save the number of parameters needed to be learned, and are still based of the traditional CNN because each binarization step still depends on the real-valued weights, with on-the-fly binarization to improve run-time efficiency.",
      "startOffset" : 19,
      "endOffset" : 28
    }, {
      "referenceID" : 6,
      "context" : "On the other hand, [6, 7, 8] do not save the number of parameters needed to be learned, and are still based of the traditional CNN because each binarization step still depends on the real-valued weights, with on-the-fly binarization to improve run-time efficiency.",
      "startOffset" : 19,
      "endOffset" : 28
    }, {
      "referenceID" : 7,
      "context" : "On the other hand, [6, 7, 8] do not save the number of parameters needed to be learned, and are still based of the traditional CNN because each binarization step still depends on the real-valued weights, with on-the-fly binarization to improve run-time efficiency.",
      "startOffset" : 19,
      "endOffset" : 28
    }, {
      "referenceID" : 8,
      "context" : "Many other network quantization and compression methods can be found in [9, 10, 11, 12, 13, 14, 15, 16].",
      "startOffset" : 72,
      "endOffset" : 103
    }, {
      "referenceID" : 9,
      "context" : "Many other network quantization and compression methods can be found in [9, 10, 11, 12, 13, 14, 15, 16].",
      "startOffset" : 72,
      "endOffset" : 103
    }, {
      "referenceID" : 10,
      "context" : "Many other network quantization and compression methods can be found in [9, 10, 11, 12, 13, 14, 15, 16].",
      "startOffset" : 72,
      "endOffset" : 103
    }, {
      "referenceID" : 11,
      "context" : "Many other network quantization and compression methods can be found in [9, 10, 11, 12, 13, 14, 15, 16].",
      "startOffset" : 72,
      "endOffset" : 103
    }, {
      "referenceID" : 12,
      "context" : "Many other network quantization and compression methods can be found in [9, 10, 11, 12, 13, 14, 15, 16].",
      "startOffset" : 72,
      "endOffset" : 103
    }, {
      "referenceID" : 13,
      "context" : "Many other network quantization and compression methods can be found in [9, 10, 11, 12, 13, 14, 15, 16].",
      "startOffset" : 72,
      "endOffset" : 103
    }, {
      "referenceID" : 14,
      "context" : "Many other network quantization and compression methods can be found in [9, 10, 11, 12, 13, 14, 15, 16].",
      "startOffset" : 72,
      "endOffset" : 103
    }, {
      "referenceID" : 15,
      "context" : "Many other network quantization and compression methods can be found in [9, 10, 11, 12, 13, 14, 15, 16].",
      "startOffset" : 72,
      "endOffset" : 103
    }, {
      "referenceID" : 16,
      "context" : "Local binary patterns (LBP) descriptor is a simple yet very powerful image descriptor rooted from face recognition community, and later has been widely used in many computer vision, pattern recognition, and image processing applications as consolidated in [17].",
      "startOffset" : 256,
      "endOffset" : 260
    }, {
      "referenceID" : 17,
      "context" : "The traditional LBP operator [18, 19, 20] operates on image patches of size 3×3, 5×5, etc.",
      "startOffset" : 29,
      "endOffset" : 41
    }, {
      "referenceID" : 18,
      "context" : "The traditional LBP operator [18, 19, 20] operates on image patches of size 3×3, 5×5, etc.",
      "startOffset" : 29,
      "endOffset" : 41
    }, {
      "referenceID" : 19,
      "context" : "The traditional LBP operator [18, 19, 20] operates on image patches of size 3×3, 5×5, etc.",
      "startOffset" : 29,
      "endOffset" : 41
    }, {
      "referenceID" : 1,
      "context" : "The traditional LBP is simply a weighted sum of all the bit maps using the pre-defined weight vector v = [2, 2, 2, 2, 2, 2, 2, 2].",
      "startOffset" : 105,
      "endOffset" : 129
    }, {
      "referenceID" : 1,
      "context" : "The traditional LBP is simply a weighted sum of all the bit maps using the pre-defined weight vector v = [2, 2, 2, 2, 2, 2, 2, 2].",
      "startOffset" : 105,
      "endOffset" : 129
    }, {
      "referenceID" : 1,
      "context" : "The traditional LBP is simply a weighted sum of all the bit maps using the pre-defined weight vector v = [2, 2, 2, 2, 2, 2, 2, 2].",
      "startOffset" : 105,
      "endOffset" : 129
    }, {
      "referenceID" : 1,
      "context" : "The traditional LBP is simply a weighted sum of all the bit maps using the pre-defined weight vector v = [2, 2, 2, 2, 2, 2, 2, 2].",
      "startOffset" : 105,
      "endOffset" : 129
    }, {
      "referenceID" : 1,
      "context" : "The traditional LBP is simply a weighted sum of all the bit maps using the pre-defined weight vector v = [2, 2, 2, 2, 2, 2, 2, 2].",
      "startOffset" : 105,
      "endOffset" : 129
    }, {
      "referenceID" : 1,
      "context" : "The traditional LBP is simply a weighted sum of all the bit maps using the pre-defined weight vector v = [2, 2, 2, 2, 2, 2, 2, 2].",
      "startOffset" : 105,
      "endOffset" : 129
    }, {
      "referenceID" : 1,
      "context" : "The traditional LBP is simply a weighted sum of all the bit maps using the pre-defined weight vector v = [2, 2, 2, 2, 2, 2, 2, 2].",
      "startOffset" : 105,
      "endOffset" : 129
    }, {
      "referenceID" : 1,
      "context" : "The traditional LBP is simply a weighted sum of all the bit maps using the pre-defined weight vector v = [2, 2, 2, 2, 2, 2, 2, 2].",
      "startOffset" : 105,
      "endOffset" : 129
    }, {
      "referenceID" : 16,
      "context" : "This is essentially a generalization of the traditional LBP because we allow multiple neighbors to be compared to multiple pivots, which is similar to the 3D LBP formulation can be found in spatial-temporal applications [17].",
      "startOffset" : 220,
      "endOffset" : 224
    }, {
      "referenceID" : 1,
      "context" : "With the same amount of parameter and computing budget, one can go much deeper using LBCNN architecture, allowing better representations to be learned through deeper architecture [2, 4, 5].",
      "startOffset" : 179,
      "endOffset" : 188
    }, {
      "referenceID" : 3,
      "context" : "With the same amount of parameter and computing budget, one can go much deeper using LBCNN architecture, allowing better representations to be learned through deeper architecture [2, 4, 5].",
      "startOffset" : 179,
      "endOffset" : 188
    }, {
      "referenceID" : 4,
      "context" : "With the same amount of parameter and computing budget, one can go much deeper using LBCNN architecture, allowing better representations to be learned through deeper architecture [2, 4, 5].",
      "startOffset" : 179,
      "endOffset" : 188
    }, {
      "referenceID" : 20,
      "context" : "Methods such as Dropout [21], DropConnect [22], and Maxout [23] have been introduced to regularize the network during training to avoid over-fitting, as a further aid to the data augmentation measures.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 21,
      "context" : "Methods such as Dropout [21], DropConnect [22], and Maxout [23] have been introduced to regularize the network during training to avoid over-fitting, as a further aid to the data augmentation measures.",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 22,
      "context" : "Methods such as Dropout [21], DropConnect [22], and Maxout [23] have been introduced to regularize the network during training to avoid over-fitting, as a further aid to the data augmentation measures.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 20,
      "context" : "As opposed to preventing over-fitting in the fully connected layers [21, 22, 24], it is also worth noting that LBCNN prevents the over-fitting directly in the convolutional layers, which is also quite important as discussed in [21, 25].",
      "startOffset" : 68,
      "endOffset" : 80
    }, {
      "referenceID" : 21,
      "context" : "As opposed to preventing over-fitting in the fully connected layers [21, 22, 24], it is also worth noting that LBCNN prevents the over-fitting directly in the convolutional layers, which is also quite important as discussed in [21, 25].",
      "startOffset" : 68,
      "endOffset" : 80
    }, {
      "referenceID" : 23,
      "context" : "As opposed to preventing over-fitting in the fully connected layers [21, 22, 24], it is also worth noting that LBCNN prevents the over-fitting directly in the convolutional layers, which is also quite important as discussed in [21, 25].",
      "startOffset" : 68,
      "endOffset" : 80
    }, {
      "referenceID" : 20,
      "context" : "As opposed to preventing over-fitting in the fully connected layers [21, 22, 24], it is also worth noting that LBCNN prevents the over-fitting directly in the convolutional layers, which is also quite important as discussed in [21, 25].",
      "startOffset" : 227,
      "endOffset" : 235
    }, {
      "referenceID" : 24,
      "context" : "As opposed to preventing over-fitting in the fully connected layers [21, 22, 24], it is also worth noting that LBCNN prevents the over-fitting directly in the convolutional layers, which is also quite important as discussed in [21, 25].",
      "startOffset" : 227,
      "endOffset" : 235
    }, {
      "referenceID" : 20,
      "context" : "• Provides de-correlation: We know that Dropout [21] prevents the co-adaptation of neuron activations, and Batch Normalization [26] tries to reduce internal co-variate shift.",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 25,
      "context" : "• Provides de-correlation: We know that Dropout [21] prevents the co-adaptation of neuron activations, and Batch Normalization [26] tries to reduce internal co-variate shift.",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 23,
      "context" : "work [24] has been proposed to explicitly de-correlate and minimize the cross-covariance of hidden activations, for improved performance and over-fitting prevention.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 26,
      "context" : "classification dataset [27] consists of 1000 classes, with 1.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 4,
      "context" : "We base the model architectures we evaluate in this paper on ResNet [5], with default 3× 3 filter size.",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 4,
      "context" : "We use a learning rate of 1e-3 and following the learning rate decay schedule from [5].",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 25,
      "context" : "One trick is not to apply ReLU prior to the local binary convolutional layer, which is usually done right after batch normalization [26].",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 5,
      "context" : "The best performing LBCNNs are compared to their particular baselines, as well as the state-of-the-art methods such as BinaryConnect [6], Binarized Neural Networks (BNN) [7], ResNet [4], Maxout Network [23], Network in Network (NIN) [28].",
      "startOffset" : 133,
      "endOffset" : 136
    }, {
      "referenceID" : 6,
      "context" : "The best performing LBCNNs are compared to their particular baselines, as well as the state-of-the-art methods such as BinaryConnect [6], Binarized Neural Networks (BNN) [7], ResNet [4], Maxout Network [23], Network in Network (NIN) [28].",
      "startOffset" : 170,
      "endOffset" : 173
    }, {
      "referenceID" : 3,
      "context" : "The best performing LBCNNs are compared to their particular baselines, as well as the state-of-the-art methods such as BinaryConnect [6], Binarized Neural Networks (BNN) [7], ResNet [4], Maxout Network [23], Network in Network (NIN) [28].",
      "startOffset" : 182,
      "endOffset" : 185
    }, {
      "referenceID" : 22,
      "context" : "The best performing LBCNNs are compared to their particular baselines, as well as the state-of-the-art methods such as BinaryConnect [6], Binarized Neural Networks (BNN) [7], ResNet [4], Maxout Network [23], Network in Network (NIN) [28].",
      "startOffset" : 202,
      "endOffset" : 206
    }, {
      "referenceID" : 27,
      "context" : "The best performing LBCNNs are compared to their particular baselines, as well as the state-of-the-art methods such as BinaryConnect [6], Binarized Neural Networks (BNN) [7], ResNet [4], Maxout Network [23], Network in Network (NIN) [28].",
      "startOffset" : 233,
      "endOffset" : 237
    }, {
      "referenceID" : 5,
      "context" : "LBCNN Baseline BinaryConnect [6] BNN [7] ResNet [4] Maxout [23] NIN [28]",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 6,
      "context" : "LBCNN Baseline BinaryConnect [6] BNN [7] ResNet [4] Maxout [23] NIN [28]",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 3,
      "context" : "LBCNN Baseline BinaryConnect [6] BNN [7] ResNet [4] Maxout [23] NIN [28]",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 22,
      "context" : "LBCNN Baseline BinaryConnect [6] BNN [7] ResNet [4] Maxout [23] NIN [28]",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 27,
      "context" : "LBCNN Baseline BinaryConnect [6] BNN [7] ResNet [4] Maxout [23] NIN [28]",
      "startOffset" : 68,
      "endOffset" : 72
    } ],
    "year" : 2016,
    "abstractText" : "We propose local binary convolution (LBC), an efficient alternative to convolutional layers in standard convolutional neural networks (CNN). The design principles of LBC are motivated by local binary patterns (LBP). The LBC layer comprises of a set of fixed sparse pre-defined binary convolutional filters that are not updated during the training process, a non-linear activation function and a set of learnable linear weights. The linear weights combine the activated filter responses to approximate the corresponding activated filter responses of a standard convolutional layer. The LBC layer affords significant parameter savings, 9× to 169× in the number of learnable parameters compared to a standard convolutional layer. Furthermore, due to lower model complexity and sparse and binary nature of the weights also results in up to 9× to 169× savings in model size compared to a standard convolutional layer. We demonstrate both theoretically and experimentally that our local binary convolution layer is a good approximation of a standard convolutional layer. Empirically, CNNs with LBC layers, called local binary convolutional neural networks (LBCNN), reach state-of-the-art performance on a range of visual datasets (MNIST, SVHN, CIFAR-10, and a subset of ImageNet) while enjoying significant computational savings.",
    "creator" : "LaTeX with hyperref package"
  }
}