{
  "name" : "1512.05919.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Planning based Framework for Essay Generation",
    "authors" : [ "Bing Qin", "Duyu Tang", "Xinwei Geng", "Dandan Ning", "Jiahao Liu", "Ting Liu" ],
    "emails" : [ "jhliu@ir.hit.edu.cn", "tliu@ir.hit.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Generating an article automatically with computer program is a challenging task in artificial intelligence and natural language processing. In this paper, we target at essay generation, which takes as input a topic word in mind and generates an organized article under the theme of the topic. We follow the idea of text planning (Reiter and Dale, 1997) and develop an essay generation framework. The framework consists of three components, including topic understanding, sentence extraction and sentence reordering. For each component, we studied several statistical algorithms and empirically compared between them in terms of qualitative or quantitative analysis. Although we run experiments on Chinese corpus, the method is language independent and can be easily adapted to other language. We lay out the remaining challenges and suggest avenues for future research."
    }, {
      "heading" : "1 Introduction",
      "text" : "In general, natural language processing tasks could be divided into natural language understanding and natural language generation (Manning and Schütze, 1999; Jurafsky and Martin, 2000). The former takes as input a piece of text and outputs the syntactic/semantic/sentimental information involved in the text. The latter in contrast focuses on generating a piece of text from an idea in mind or from a large collection of text corpus. In this paper, we focus on natural language generation (Reiter et al., 2000).\n∗ indicates equal contribution.\nSpecifically, we formulate the task as essay generation from mind, namely taking the input as a topic word1 in mind and outputing an organized article (a document) with several paragraphs under the theme of the topic. The task is challenging as it requires the generator to deeply understand the way human beings write articles. Hopefully, solving this problem contributes to making progress towards Artificial Intelligence.\nWe argue that generating a well organized article is a challenging task. The first challenge is how to understand and represent the meaning of a topic word in mind. This is extremely important as telling the computer what we want to write is the first step we need to do before generating an article. Computer program does not have background like human beings, so that it does not understand a “cellphone” is an electronic product including battery and it can be used to chat with others. After understanding the meaning of a topic word, the following challenge is how to generate a topic focused article, e.g. how to collect topic-specific “fuel” (e.g. sentences) and how to organize them to form an organized article. This is of great importance as an article is not a set of sentences chaotically. Natural language is structured (Mann and Thompson, 1988; Jurafsky and Martin, 2000). The coherence/discourse relationship (Prasad et al., 2008; Li and Hovy, 2014; Li et al., 2014) between sentences is a crucial element to improve the readability of a document and to guarantee the structured nature of a document in terms of lexicalization and semantic.\nIn this paper, we develop a planning based frame-\n1Supposing the input topic word is unambiguous.\nar X\niv :1\n51 2.\n05 91\n9v 2\n[ cs\n.C L\n] 6\nJ an\n2 01\nwork (Reiter and Dale, 1997; Reiter et al., 2000) for essay generation. The framework consists of three steps: topic understanding, sentence selecting and sentence organizing. Firstly, it represents the topic word in semantic vector space and automatically recognizes several arguments in order to support the topic. Each argument is represented as a list of supporting words which are semantically related to the topic word from a certain perspective. Afterwards, a set of semantically related sentences are extracted/ranked for each argument given the list of supporting words. Finally, the chaotic sentences with regard to each argument are organized to output an article by taking into account of the discourse and semantic relatedness between sentences. Furthermore, in order to find new evidences (e.g. words) to better support an argument, we add a feedback component to find new words from the extracted sentences to expand the existing evidence set.\nWe conduct a case study on a Chinese corpus. For each component in the framework, we explore several strategies and empirically compare between them in terms of qualitative or quantitative analysis. We analyse the pros and cons of each approach, lay out the remaining challenges and suggest avenues for future research."
    }, {
      "heading" : "2 The Framework",
      "text" : "We describe the planning based framework in this section. As illustrated in Figure 1, the framework consists of three steps: topic understanding, sentence selecting and sentence organizing. We also add a feedback mechanism to enrich the supporting\nwords of each argument. We describe the details of these components, respectively."
    }, {
      "heading" : "2.1 Topic Understanding",
      "text" : "When a person writes an article under the theme of a certain topic, he/she typically finds some arguments to support his main idea. For example, an article about “cellphone” might have three paragraphs, stating the evaluations towards “call quality”, “appearance” and “battery life”, respectively. These arguments are some important characteristics of the topic from some aspects. The evidences about each argument make the whole article cohesive. Based on these considerations, we regard topic understanding as the first component of the framework. Given a topic word as input, topic understanding analyzes its semantic meaning and outputs several arguments to support the topic. Each argument is represented as a collection of words, each of which is semantically related with the topic from some aspect.\nWe consider topic understanding as two cascaded steps: topic expansion and topic clustering. An illustration is given in Figure 2. The former step finds a collection of words having similar semantic meanings with the topic word. The latter step separate\nsimilar words into several clusters, each of which share some properties with the topic word in terms of some aspects.\nSpecifically, for the topic expansion component, we exploit thesaurus based, topic model based and word embedding based approaches. Using external thesaurus is a natural choice as thesaurus like WordNet (or HowNet in Chinese) mostly contain the synonyms, antonyms, hypernym relationships. Accordingly, we can use heuristic rules to find more semantically related words as candidates. Straight forward rules include the synonym/hypernym of a word or the antonym of an antonym. Since some results might have noises, we can design a scoring function (e.g. the number of times a word occurs) to filter out some words with lower confidence. We also try a propagation strategy, where the extracted word set is further regarded as seeds and used to find more related words. Topic modeling and word embedding approaches represent a word as a continuous vector in semantic vector space. Let us take word embedding as an example. Words with similar semantic meanings and grammatical usages will be mapped into closed vectors in the embedding space (Mikolov et al., 2013). Therefore, to find some semantically similar words with the topic word, we could collect the neighboring words of the topic word in the vector space in terms of some similarity criterion like cosine or Euclidean distance with the topic word. We set a threshold to remain the most confident k words and filter out the others.\nAfter find a set of related words, we use standard clustering algorithms like K-Means and Affinity Propagation (Frey and Dueck, 2007) to separate words to several clusters, each of which represents an argument to support the topic. For topic model and word embedding approaches, the inputs of a clustering algorithm are the continuous representation of words. We pre-define the number of clusters of K-Means, while AP approach could automatically decide the number of clusters."
    }, {
      "heading" : "2.2 Sentence Selecting",
      "text" : "After obtaining several clusters of words, each of which supports the topic word from an aspect/argument, we select a number of sentences for every argument. We can reuse a sentence selecting module several times to find evidences for each\nargument, respectively. Formally, for each argument, sentence selecting takes as input a collection of words and outputs a list of sentences with regard to the semantics of these words. The selected sentences will be used to compose a paragraph with sentence organizing, which is described in the following subsection.\nSentence selecting could be regarded as a retrieval problem, namely selecting the sentences with high similarities with a collection of words. Accordingly, defining a good scoring function f(W, s) plays an important role to obtain a good result, where W is a collection of words and s is a sentence to be scored. We explore two kinds of methods in this work, a counting based method and an embedding based method. Counting based approach is a straight forward way to score each sentence, which is similar with the word matching strategy in information retrieval (Manning et al., 2008). The assumption is that a sentence s containing more words in W should be semantically closer to W . The scoring function f(W, s) is the number of words in W occurs in the sentence s, the higher the better. However, it is commonly accepted that a word typically has different semantic meanings and one meaning could be expressed by different word surfaces. Therefore, it is more desirable to develop a semantic driven approach, where W and s are mapped in the semantic vector space. Towards this goal, we consider an embedding-based approach, which maps W and s in a latent semantic space. Accordingly, we can compute the similarity between W and s in the embedding space. We use a simple average method as a case study in this work. We first map each word in a low-dimensional word embedding (Mikolov et al., 2013). Afterwards, the representation of W is the average of the vectors of words w ∈W .\nFor sentence s, we use semantic compositional method to calculate sentence representation from the representations of the word it contains. This is on the basis of the principal of compositionality (Frege, 1892), which states that the meaning of a longer expression (e.g. a sentence) comes from the meanings of its constituents. Since we do not have plenty of task-specific training data, unsupervised compositional approach is preferred to make the system scalable. As a case study, we also use average as the compositional function. We leave more sophisticated unsupervised approaches like Deep Boltzmann Machine (Hinton and Salakhutdinov, 2006), Denoising Autoencoder (Glorot et al., 2011), Unfolding Recursive Autoencoder (Socher et al., 2011a), LSTM Encoder (Li et al., 2015) as future work.\nExtension 1 It is worth noting that the purpose of sentence selecting is to obtain some “fuel” which can be used in the Sentence Organizing part to form an article. Based on this consideration, we believe that tagging each sentence with a discourse/semantic tag will help to organize the sentences with more evidences. Therefore, we use an automatically discourse element labeling algorithm (Song et al., 2015) to decide what role does a sentence acts as, such as “Introduction”, “Prompt” and “Conclusion”. For example, “Introduction” sentence introduces the background and/or grabs readers’ attention and “Conclusion” sentence concludes the whole essay or one of the main ideas.\nExtension 2 Given a list of words for one argument, sentence selecting part of outputs a collection of sentences where the input words come from the “Topic Understanding” part. These selected sentences are semantically related to the argument, and might contain some supporting words which do not covered in the input word set. Based on this consideration, we add a “feedback” mechanism to extract some new words from the sentences and add them to the input word set as an expansion. In this way, the framework could work in a bootstrapping fashion. We regard extracting new word w from a collection of sentences S as a ranking problem. What we need to do is designing a scoring function f(w,W,S), where w is a candidate word in the sentence collection S, W is the input word set for one\nargument. After getting the score of each candidate w ∈ S, we rank them and select the top ranked k words which are not contained in W . We explore two methods for extracting new words from the outputted sentences: a counting method and an embedding method. In counting based method, f(w,W,S) is the number of w occurs in S. In embedding based method, f(w,W,S) is the similarity between vecw and vecW , where vecW is the averaging of vectors of words in W ."
    }, {
      "heading" : "2.3 Sentence Organizing",
      "text" : "In this part, we describe the sentence organizing part which organizes a set of chaotic sentences into an organized article. This can be considered as a structure prediction problem, and the objective is to predict a desirable structure of a collection of sentences.\nTo this end, a natural choice is to greedily get the order of a list of sentence from left to right, one sentence at a time. That is to say, when we looking at a sentence, we only select which sentence is the most relevant one to be after the current sentence. This process could be done in a recursive way, so that a order could be generated greedily. An illustration of this idea is given in Figure 4. One important component in this setting is a scoring function f(s1, s2) to weight the semantic similarity between two sentences s1 and s2.\nWe explore four methods as relatedness scoring function to calculate the coherence of two sentence.\n• Bag-of-Word (Boolean). We represent each sentence as bag-of-words, whose values represent whether a word occurs in the sentence. We use cosine similarity between s1 and s2 as their similarity.\nAmong these four methods, the first three methods are similarity driven as they regard the cosine similarity between sentences as the scoring function. The last method is relatedness driven as there is an additional feed-forward neural network to encode the relatedness between sentences. The first three models do not contain external parameters, while the fourth model needs to be trained from data. In order to learn the parameters in recursive neural network and the the parameters in the feed-forward scoring function, we follow (Li and Hovy, 2014) and use a cross entropy loss function. The basic idea is that during training the category of a correct pair of sentence f(s1, s2) should be different from the category of a corrupted pair of sentence f(s1, s∗2). We use cross entropy as the loss function (Li and Hovy, 2014), where Pc(s1, s2) is the probability of predicting (s1, s2) as class c given by the sigmoid layer, P gc (s1, s2) indicates whether class c is the correct category, whose value is 1 or 0. The correct pair of sentence is a real case in the corpus, namely there is a sentence s2 occurring after sentence s1. The corrupted pair of sentence is artificially generally by replacing s2 with a randomly selected sentence s∗2.\n2One could also use recurrent neural network or convolutional neural network as alternatives.\nloss = − ∑\nsi,sj∈S\n∑ c=0,1 P gc (si, sj) · log(Pc(si, sj))\n(1)\n6\nWi1 Wi2 Wi3 Wi4 Wim Wj1 Wj2 Wj3 Wj4 Wjn\niS jS\nRNN RNN\n 0,1\n  sentanh W b \n  sigmoid U b \nThe greedy method mentioned above organized sentences in a local way. That is to say, the method processes a sentence by only seeing its previous sentence, without capturing global evidence or optimizing a global organizing result. To solve this problem, we use a global approach, which is also a natural choice inspired by the representative studies in sequence labeling like part-of speech tagging and named entity recognition (Manning and Schütze, 1999; Jurafsky and Martin, 2000). The basic idea is that, when we deal with a potential sentence at index i, we consider the preceding histories and calculate the best score of a result from the beginning of a sentence to the current index. We use dynamic programming to recursively calculate the scores and decode with standard Viterbi algorithm."
    }, {
      "heading" : "3 Experiment",
      "text" : "We compare between different methods for each component empirically. For “Topic Understanding” and “Sentence Selecting” parts, we only evaluate methods qualitatively as we do not have the ground truth. For “Sentence Organizing” part, we also evaluate different methods quantitatively as the original orders of sentences in a document could be viewed\nas a ground truth. We conduct experiments on a Chinese dataset we crawl from the web, which contains 6,683 documents with 193,210 sentences."
    }, {
      "heading" : "3.1 Evaluation on Topic Understanding",
      "text" : "We evaluate the effects of different algorithms for topic understanding. Since there are two steps in this part, we evaluate them separately. The results on youth (青春) are given in Table 1, where Thes is thesaurus-based method, TM means topic model approach, WE is word embedding based method. We uses Hownet3 as the external resource in Thes. The word embeddings used in WE are learned with Skipgram method (Mikolov et al., 2013). In thesaurus based methods, we filter out the words whose length are 1 because most of them do not have concrete meaning. Despite using this filtering rule, we find that the results of Thes are still worse than others. The supporting words are formal, not commonly used in user generated articles. Moreover, the meanings of supporting words are topic focused and do not go beyond the literal meaning of the input word. This is partly caused by the coverage of the thesaurus. We observe that the results of TM and WE are comparable and better than Thes in this example. For an noun “youth” (青春 in Chinese), TM and WE could find semantically related words which are more diverse and not restricted to the literal similarity. We take word embedding method as an example, and compare between K-Means and AP clustering algorithms to test their performances on “Topic Clustering”. According to our observations, K-Means performs better than AP clustering as the results in AP contain many clusters containing less than 3 words."
    }, {
      "heading" : "3.2 Evaluation on Sentence Selecting",
      "text" : "We evaluate the performances of counting and embedding methods for sentence selecting. We also take youth (青春) as a case study. The top selected results (in Chinese) are given in Table 2. We can find that the obtained sentences in counting based method are typically longer as it favors the sentences containing more key words. We believe that these results are more suitable to act as Prompt sentences because they include more specific evidences. On\n3http://www.keenage.com/\nthe contrary, the results of embedding based method are typically shorter and more cohesive, which is partly caused by the way we used for composing sentence vector. Such results might be regarded as Theme or Conclusion sentences which are more abstractive. For both methods, it is somewhat disappointing that they favour to selecting the sentences containing topic words such as “youth”, which is less diverse than we have expected."
    }, {
      "heading" : "3.3 Evaluation on Sentence Organizing",
      "text" : "In this part we use two experimental settings to compare between different four coherence functions. In the first setting, we take greedy framework as a case study and qualitatively evaluate them in the real system by showing the sentence orders generated from different coherence functions, including BOW (Boolean), BOW (Frequency), Embedding (Avg) and Recursive NN. In the second setting, we quantitatively evaluate them on a hold-out dataset consisting of several documents. The input for each coherence model is the same, namely the sentences of a document and the first sentence. The output is the orders generated from each coherence model. As we have the original sentence order, we can regard it as the ground truth, and evaluate the quantitative performance in terms of accuracy on bigrams of sentences, which is similar with Bleu (Papineni et al., 2002) in machine translation and Rough (Lin, 2004) in summarization.\nAn illustration of the different sentence organizing results are given in Table 3 and Table 4. We choose a relatively short document consisting of five sentences to show the performance. We can find that recursive neural network performs gets the correct order while all others have one or more mistakes. Different from the previous two components, we can quantitatively evaluate the performance of this part as the original sentence order of a document could\nbe regarded as the ground truth of the sentences it contains.\nExperimental results are given in Table 5. We can find that the performances of the four methods in greedy and DP setting are consistent. BOW (Boolean) outperforms BOW (Frequency) consistently. This indicates that whether a word occurs in two sentences could indicate the relatedness of two sentences and it does not need to consider how many times a word occurs in each sentence. Average based embedding method performs better than bag-of-word methods by considering the continuous word and sentence representation in some latent\nsemantic space. We can find that recursive neural network method performs best in each setting, outperforming three previous similarity based methods. This shows the effectiveness of a powerful semantic composition model as well as the necessary to model the relatedness between two sentences rather than a cosine based similarity measurement."
    }, {
      "heading" : "4 Related Work",
      "text" : "We briefly talk about some related works in literature about natural language generation and essay generation in this section.\nNatural language generation (NLG) is a fundamental and challenge task in natural language processing and computational linguistics (Manning and Schütze, 1999; Jurafsky and Martin, 2000; Reiter et al., 2000). The task of essay generation could be viewed as a special kind of natural language generation. Existing NLG approaches could be divided into three categories: template based meth-\nods, grammar based methods and statistical based methods. Template based methods typically use manually designed templates with some slots and replace words to generate new article. Grammar based methods go one step further by manually designing some structured templates and compose an article with computer program. Statistical based methods focus on learn the sophisticated patterns from the web and generate article in an automatically way. In this work, we follow (Reiter and Dale, 1997) and explore planning based approach. There also exists some related studies in text generation. For example, Belz (2007) generate weather report texts with probabilistic generation method. Jiang and Zhou (2008) generate Chinese couplets with statistical machine translation approach. Angeli et al. (2010) develop a domain-independent method with a sequence of local decisions, and evaluate the method on Robocup sportscasting and technical weather forecasts. Zhang and Lapata (2014) generate Chinese poetry with recurrent neural network. Shang et al. (2015) generate short-text conversation with neural responding machine. Li et al. (2015) generate a paragraph/document with attention based neural network. Rush et al. (2015) and Hu et al. (2015) use attention based recurrent neural network to generate abstractive summarization. Image captioning (Xu et al., 2015) can also be viewed as a kind of text generation which takes a picture as the input."
    }, {
      "heading" : "5 Conclusion and Future Directions",
      "text" : "In summary, we develop a planning based framework to generate an article by taking a topic word as input. The framework consists of three compo-\nnents: a topic understanding component, a sentence selecting component and a sentence organizing component. We also add a feedback mechanism to enhance the results of topic understanding. For each component, we explore several methods and conduct a case study on a Chinese corpus. We show that for topic understanding, topic model and word embedding based methods perform better than thesaurus based methods. Recursive neural network based model performs better than bag-of-word and embedding average based similarity driven methods for sentence organizing.\nThere remains plenty of challenges in this line of research. One direction is how to quantitatively evaluate the effectiveness of each internal component as well as the final generated article. In this work, we only quantitatively evaluate the sentence organizing part as the original sentence order could be used as gold standard. However, for other parts, it is impractical to build gold standard for each input topic word. It is desirable to find some automatic evaluation methods or to test the algorithms on some applications with automatically labeled gold standards. For the sentence selecting part, we find that the methods we tried prefers to select the sentences which contains the exact supporting words. The task requires us to choose more diverse sentences with different semantic roles to form a diversified document. From another perspective, the input of this work is a given topic word. However, in some situation there is only an idea or a description about what we want to write. We need to understand the idea/description and get the topic word. We leave this as another potential future work. Furthermore, the methods used in this work can be regarded as\nan extractive approach. We separate the whole task into several subtasks and develop algorithms to address each part. This might suffers from the problem of error propagation, and this could be to reduced to some extend if we build an end-to-end method like the emerging neural network approach. This is also a very interesting future work."
    } ],
    "references" : [ {
      "title" : "A simple domain-independent probabilistic approach to generation",
      "author" : [ "Angeli et al.2010] Gabor Angeli", "Percy Liang", "Dan Klein" ],
      "venue" : "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Angeli et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Angeli et al\\.",
      "year" : 2010
    }, {
      "title" : "Probabilistic generation of weather forecast texts. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics",
      "author" : [ "Anja Belz" ],
      "venue" : "Proceedings of the Main Conference,",
      "citeRegEx" : "Belz.,? \\Q2007\\E",
      "shortCiteRegEx" : "Belz.",
      "year" : 2007
    }, {
      "title" : "Clustering by passing messages between data points",
      "author" : [ "Frey", "Dueck2007] Brendan J Frey", "Delbert Dueck" ],
      "venue" : null,
      "citeRegEx" : "Frey et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Frey et al\\.",
      "year" : 2007
    }, {
      "title" : "Domain adaptation for largescale sentiment classification: A deep learning approach",
      "author" : [ "Glorot et al.2011] Xavier Glorot", "Antoine Bordes", "Yoshua Bengio" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Glorot et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Glorot et al\\.",
      "year" : 2011
    }, {
      "title" : "Reducing the dimensionality of data with neural networks",
      "author" : [ "Hinton", "Salakhutdinov2006] G.E. Hinton", "R.R. Salakhutdinov" ],
      "venue" : "Science,",
      "citeRegEx" : "Hinton et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2006
    }, {
      "title" : "Lcsts: A large scale chinese short text summarization dataset",
      "author" : [ "Hu et al.2015] Baotian Hu", "Qingcai Chen", "Fangze Zhu" ],
      "venue" : "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Hu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2015
    }, {
      "title" : "Generating chinese couplets using a statistical MT approach",
      "author" : [ "Jiang", "Zhou2008] Long Jiang", "Ming Zhou" ],
      "venue" : "In Proceedings of the 22nd International Conference on Computational Linguistics (Coling",
      "citeRegEx" : "Jiang et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2008
    }, {
      "title" : "Speech & language processing. Pearson Education India",
      "author" : [ "Jurafsky", "Martin2000] Dan Jurafsky", "James H Martin" ],
      "venue" : null,
      "citeRegEx" : "Jurafsky et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Jurafsky et al\\.",
      "year" : 2000
    }, {
      "title" : "A model of coherence based on distributed sentence representation",
      "author" : [ "Li", "Hovy2014] Jiwei Li", "Eduard Hovy" ],
      "venue" : "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Li et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2014
    }, {
      "title" : "Recursive deep models for discourse parsing",
      "author" : [ "Li et al.2014] Jiwei Li", "Rumeng Li", "Eduard Hovy" ],
      "venue" : "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Li et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2014
    }, {
      "title" : "A hierarchical neural autoencoder for paragraphs and documents",
      "author" : [ "Li et al.2015] Jiwei Li", "Thang Luong", "Dan Jurafsky" ],
      "venue" : "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference",
      "citeRegEx" : "Li et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2015
    }, {
      "title" : "Rouge: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin" ],
      "venue" : "In Text summarization branches out: Proceedings of the ACL-04 workshop,",
      "citeRegEx" : "Lin.,? \\Q2004\\E",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Rhetorical structure theory: Toward a functional theory of text organization",
      "author" : [ "Mann", "Thompson1988] William C Mann", "Sandra A Thompson" ],
      "venue" : "TextInterdisciplinary Journal for the Study of Discourse,",
      "citeRegEx" : "Mann et al\\.,? \\Q1988\\E",
      "shortCiteRegEx" : "Mann et al\\.",
      "year" : 1988
    }, {
      "title" : "Foundations of statistical natural language processing",
      "author" : [ "Manning", "Hinrich Schütze" ],
      "venue" : null,
      "citeRegEx" : "Manning et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Manning et al\\.",
      "year" : 1999
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean" ],
      "venue" : null,
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Salim Roukos", "Todd Ward", "Wei-Jing Zhu" ],
      "venue" : "In Proceedings of the 40th annual meeting on association for computational linguistics,",
      "citeRegEx" : "Papineni et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "The penn discourse treebank 2.0",
      "author" : [ "Prasad et al.2008] Rashmi Prasad", "Nikhil Dinesh", "Alan Lee", "Eleni Miltsakaki", "Livio Robaldo", "Aravind K Joshi", "Bonnie L Webber" ],
      "venue" : "In LREC. Citeseer",
      "citeRegEx" : "Prasad et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Prasad et al\\.",
      "year" : 2008
    }, {
      "title" : "Building applied natural language generation systems",
      "author" : [ "Reiter", "Dale1997] Ehud Reiter", "Robert Dale" ],
      "venue" : "Natural Language Engineering,",
      "citeRegEx" : "Reiter et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Reiter et al\\.",
      "year" : 1997
    }, {
      "title" : "Building natural language generation systems, volume 33",
      "author" : [ "Reiter et al.2000] Ehud Reiter", "Robert Dale", "Zhiwei Feng" ],
      "venue" : null,
      "citeRegEx" : "Reiter et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Reiter et al\\.",
      "year" : 2000
    }, {
      "title" : "A neural attention model for abstractive sentence summarization",
      "author" : [ "Sumit Chopra", "Jason Weston" ],
      "venue" : "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Rush et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Rush et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural responding machine for shorttext conversation",
      "author" : [ "Shang et al.2015] Lifeng Shang", "Zhengdong Lu", "Hang Li" ],
      "venue" : "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Nat-",
      "citeRegEx" : "Shang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Shang et al\\.",
      "year" : 2015
    }, {
      "title" : "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection",
      "author" : [ "Eric H Huang", "Jeffrey Pennington", "Andrew Y Ng", "Christopher D Manning" ],
      "venue" : "The Conference on Neural Information Processing Sys-",
      "citeRegEx" : "Socher et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2011
    }, {
      "title" : "Parsing natural scenes and natural language with recursive neural networks",
      "author" : [ "Cliff C Lin", "Andrew Ng", "Chris Manning" ],
      "venue" : "In Proceedings of the International Conference on Machine Learning,",
      "citeRegEx" : "Socher et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2011
    }, {
      "title" : "Discourse element identification in student essays based on global and local cohesion",
      "author" : [ "Song et al.2015] Wei Song", "Ruiji Fu", "Lizhen Liu", "Ting Liu" ],
      "venue" : "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Song et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2015
    }, {
      "title" : "Show, attend and tell: Neural image caption generation",
      "author" : [ "Xu et al.2015] Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Xu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    }, {
      "title" : "Chinese poetry generation with recurrent neural networks",
      "author" : [ "Zhang", "Lapata2014] Xingxing Zhang", "Mirella Lapata" ],
      "venue" : "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Zhang et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 18,
      "context" : "In this paper, we focus on natural language generation (Reiter et al., 2000).",
      "startOffset" : 55,
      "endOffset" : 76
    }, {
      "referenceID" : 16,
      "context" : "The coherence/discourse relationship (Prasad et al., 2008; Li and Hovy, 2014; Li et al., 2014) between sentences is a crucial element to improve the readability of a document and to guarantee the structured nature of a document in terms of lexicalization and semantic.",
      "startOffset" : 37,
      "endOffset" : 94
    }, {
      "referenceID" : 8,
      "context" : "The coherence/discourse relationship (Prasad et al., 2008; Li and Hovy, 2014; Li et al., 2014) between sentences is a crucial element to improve the readability of a document and to guarantee the structured nature of a document in terms of lexicalization and semantic.",
      "startOffset" : 37,
      "endOffset" : 94
    }, {
      "referenceID" : 18,
      "context" : "work (Reiter and Dale, 1997; Reiter et al., 2000) for essay generation.",
      "startOffset" : 5,
      "endOffset" : 49
    }, {
      "referenceID" : 14,
      "context" : "Words with similar semantic meanings and grammatical usages will be mapped into closed vectors in the embedding space (Mikolov et al., 2013).",
      "startOffset" : 118,
      "endOffset" : 140
    }, {
      "referenceID" : 14,
      "context" : "We first map each word in a low-dimensional word embedding (Mikolov et al., 2013).",
      "startOffset" : 59,
      "endOffset" : 81
    }, {
      "referenceID" : 3,
      "context" : "We leave more sophisticated unsupervised approaches like Deep Boltzmann Machine (Hinton and Salakhutdinov, 2006), Denoising Autoencoder (Glorot et al., 2011), Unfolding Recursive Autoencoder (Socher et al.",
      "startOffset" : 136,
      "endOffset" : 157
    }, {
      "referenceID" : 10,
      "context" : ", 2011a), LSTM Encoder (Li et al., 2015) as future work.",
      "startOffset" : 23,
      "endOffset" : 40
    }, {
      "referenceID" : 23,
      "context" : "Therefore, we use an automatically discourse element labeling algorithm (Song et al., 2015) to decide what role does a sentence acts as, such as “Introduction”, “Prompt” and “Conclusion”.",
      "startOffset" : 72,
      "endOffset" : 91
    }, {
      "referenceID" : 14,
      "context" : "The word embeddings used in WE are learned with Skipgram method (Mikolov et al., 2013).",
      "startOffset" : 64,
      "endOffset" : 86
    }, {
      "referenceID" : 15,
      "context" : "As we have the original sentence order, we can regard it as the ground truth, and evaluate the quantitative performance in terms of accuracy on bigrams of sentences, which is similar with Bleu (Papineni et al., 2002) in machine translation and Rough (Lin, 2004) in summarization.",
      "startOffset" : 193,
      "endOffset" : 216
    }, {
      "referenceID" : 11,
      "context" : ", 2002) in machine translation and Rough (Lin, 2004) in summarization.",
      "startOffset" : 41,
      "endOffset" : 52
    }, {
      "referenceID" : 18,
      "context" : "Natural language generation (NLG) is a fundamental and challenge task in natural language processing and computational linguistics (Manning and Schütze, 1999; Jurafsky and Martin, 2000; Reiter et al., 2000).",
      "startOffset" : 131,
      "endOffset" : 206
    }, {
      "referenceID" : 24,
      "context" : "Image captioning (Xu et al., 2015) can also be viewed as a kind of text generation which takes a picture as the input.",
      "startOffset" : 17,
      "endOffset" : 34
    }, {
      "referenceID" : 0,
      "context" : "For example, Belz (2007) generate weather report texts with probabilistic generation method.",
      "startOffset" : 13,
      "endOffset" : 25
    }, {
      "referenceID" : 0,
      "context" : "For example, Belz (2007) generate weather report texts with probabilistic generation method. Jiang and Zhou (2008) generate Chinese couplets with statistical machine translation approach.",
      "startOffset" : 13,
      "endOffset" : 115
    }, {
      "referenceID" : 0,
      "context" : "Angeli et al. (2010) develop a domain-independent method with a sequence of local decisions, and evaluate the method on Robocup sportscasting and technical weather forecasts.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 0,
      "context" : "Angeli et al. (2010) develop a domain-independent method with a sequence of local decisions, and evaluate the method on Robocup sportscasting and technical weather forecasts. Zhang and Lapata (2014) generate Chinese poetry with recurrent neural network.",
      "startOffset" : 0,
      "endOffset" : 199
    }, {
      "referenceID" : 0,
      "context" : "Angeli et al. (2010) develop a domain-independent method with a sequence of local decisions, and evaluate the method on Robocup sportscasting and technical weather forecasts. Zhang and Lapata (2014) generate Chinese poetry with recurrent neural network. Shang et al. (2015) generate short-text conversation with neural responding machine.",
      "startOffset" : 0,
      "endOffset" : 274
    }, {
      "referenceID" : 0,
      "context" : "Angeli et al. (2010) develop a domain-independent method with a sequence of local decisions, and evaluate the method on Robocup sportscasting and technical weather forecasts. Zhang and Lapata (2014) generate Chinese poetry with recurrent neural network. Shang et al. (2015) generate short-text conversation with neural responding machine. Li et al. (2015) generate a paragraph/document with attention based neural network.",
      "startOffset" : 0,
      "endOffset" : 356
    }, {
      "referenceID" : 0,
      "context" : "Angeli et al. (2010) develop a domain-independent method with a sequence of local decisions, and evaluate the method on Robocup sportscasting and technical weather forecasts. Zhang and Lapata (2014) generate Chinese poetry with recurrent neural network. Shang et al. (2015) generate short-text conversation with neural responding machine. Li et al. (2015) generate a paragraph/document with attention based neural network. Rush et al. (2015) and Hu et al.",
      "startOffset" : 0,
      "endOffset" : 442
    }, {
      "referenceID" : 0,
      "context" : "Angeli et al. (2010) develop a domain-independent method with a sequence of local decisions, and evaluate the method on Robocup sportscasting and technical weather forecasts. Zhang and Lapata (2014) generate Chinese poetry with recurrent neural network. Shang et al. (2015) generate short-text conversation with neural responding machine. Li et al. (2015) generate a paragraph/document with attention based neural network. Rush et al. (2015) and Hu et al. (2015) use attention based recurrent neural network to generate abstractive summarization.",
      "startOffset" : 0,
      "endOffset" : 463
    } ],
    "year" : 2016,
    "abstractText" : "Generating an article automatically with computer program is a challenging task in artificial intelligence and natural language processing. In this paper, we target at essay generation, which takes as input a topic word in mind and generates an organized article under the theme of the topic. We follow the idea of text planning (Reiter and Dale, 1997) and develop an essay generation framework. The framework consists of three components, including topic understanding, sentence extraction and sentence reordering. For each component, we studied several statistical algorithms and empirically compared between them in terms of qualitative or quantitative analysis. Although we run experiments on Chinese corpus, the method is language independent and can be easily adapted to other language. We lay out the remaining challenges and suggest avenues for future research.",
    "creator" : "LaTeX with hyperref package"
  }
}